Proceedings of the ACL 2007 Demo and Poster Sessions, pages 57?60,
Prague, June 2007. c?2007 Association for Computational Linguistics
Support Vector Machines for Query-focused Summarization trained and
evaluated on Pyramid data
Maria Fuentes
TALP Research Center
Universitat Polite`cnica de Catalunya
mfuentes@lsi.upc.edu
Enrique Alfonseca
Computer Science Departament
Universidad Auto?noma de Madrid
Enrique.Alfonseca@gmail.com
Horacio Rodr??guez
TALP Research Center
Universitat Polite`cnica de Catalunya
horacio@lsi.upc.edu
Abstract
This paper presents the use of Support
Vector Machines (SVM) to detect rele-
vant information to be included in a query-
focused summary. Several SVMs are
trained using information from pyramids
of summary content units. Their per-
formance is compared with the best per-
forming systems in DUC-2005, using both
ROUGE and autoPan, an automatic scor-
ing method for pyramid evaluation.
1 Introduction
Multi-Document Summarization (MDS) is the task
of condensing the most relevant information from
several documents in a single one. In terms of the
DUC contests1, a query-focused summary has to
provide a ?brief, well-organized, fluent answer to a
need for information?, described by a short query
(two or three sentences). DUC participants have to
synthesize 250-word sized summaries for fifty sets
of 25-50 documents in answer to some queries.
In previous DUC contests, from 2001 to 2004, the
manual evaluation was based on a comparison with
a single human-written model. Much information
in the evaluated summaries (both human and auto-
matic) was marked as ?related to the topic, but not
directly expressed in the model summary?. Ideally,
this relevant information should be scored during the
evaluation. The pyramid method (Nenkova and Pas-
sonneau, 2004) addresses the problem by using mul-
tiple human summaries to create a gold-standard,
1http://www-nlpir.nist.gov/projects/duc/
and by exploiting the frequency of information in
the human summaries in order to assign importance
to different facts. However, the pyramid method re-
quires to manually matching fragments of automatic
summaries (peers) to the Semantic Content Units
(SCUs) in the pyramids. AutoPan (Fuentes et al,
2005), a proposal to automate this matching process,
and ROUGE are the evaluation metrics used.
As proposed by Copeck and Szpakowicz (2005),
the availability of human-annotated pyramids con-
stitutes a gold-standard that can be exploited in or-
der to train extraction models for the summary au-
tomatic construction. This paper describes several
models trained from the information in the DUC-
2006 manual pyramid annotations using Support
Vector Machines (SVM). The evaluation, performed
on the DUC-2005 data, has allowed us to discover
the best configuration for training the SVMs.
One of the first applications of supervised Ma-
chine Learning techniques in summarization was in
Single-Document Summarization (Ishikawa et al,
2002). Hirao et al (2003) used a similar approach
for MDS. Fisher and Roark (2006)?s MDS system is
based on perceptrons trained on previous DUC data.
2 Approach
Following the work of Hirao et al (2003) and
Kazawa et al (2002), we propose to train SVMs
for ranking the candidate sentences in order of rele-
vance. To create the training corpus, we have used
the DUC-2006 dataset, including topic descriptions,
document clusters, peer and manual summaries, and
pyramid evaluations as annotated during the DUC-
2006 manual evaluation. From all these data, a set
57
of relevant sentences is extracted in the following
way: first, the sentences in the original documents
are matched with the sentences in the summaries
(Copeck and Szpakowicz, 2005). Next, all docu-
ment sentences that matched a summary sentence
containing at least one SCU are extracted. Note that
the sentences from the original documents that are
not extracted in this way could either be positive (i.e.
contain relevant data) or negative (i.e. irrelevant for
the summary), so they are not yet labeled. Finally,
an SVM is trained, as follows, on the annotated data.
Linguistic preprocessing The documents from
each cluster are preprocessed using a pipe of general
purpose processors performing tokenization, POS
tagging, lemmatization, fine grained Named Enti-
ties (NE)s Recognition and Classification, anaphora
resolution, syntactic parsing, semantic labeling (us-
ing WordNet synsets), discourse marker annotation,
and semantic analysis. The same tools are used for
the linguistic processing of the query. Using these
data, a semantic representation of the sentence is
produced, that we call environment. It is a semantic-
network-like representation of the semantic units
(nodes) and the semantic relations (edges) holding
between them. This representation will be used to
compute the (Fuentes et al, 2006) lexico-semantic
measures between sentences.
Collection of positive instances As indicated be-
fore, every sentence from the original documents
matching a summary sentence that contains at least
one SCU is considered a positive example. We have
used a set of features that can be classified into three
groups: those extracted from the sentences, those
that capture a similarity metric between the sentence
and the topic description (query), and those that try
to relate the cohesion between a sentence and all the
other sentences in the same document or collection.
The attributes collected from the sentences are:
? The position of the sentence in its document.
? The number of sentences in the document.
? The number of sentences in the cluster.
? Three binary attributes indicating whether the
sentence contains positive, negative and neutral
discourse markers, respectively. For instance,
what?s more is positive, while for example and
incidentally indicate lack of relevance.
? Two binary attributes indicating whether
the sentence contains right-directed discourse
markers (that affect the relevance of fragment
after the marker, e.g. first of all), or discourse
markers affecting both sides, e.g. that?s why.
? Several boolean features to mark whether the
sentence starts with or contains a particular
word or part-of-speech tag.
? The total number of NEs included in the sen-
tence, and the number of NEs of each kind.
? SumBasic score (Nenkova and Vanderwende,
2005) is originally an iterative procedure that
updates word probabilities as sentences are se-
lected for the summary. In our case, word prob-
abilities are estimated either using only the set
of words in the current document, or using all
the words in the cluster.
The attributes that depend on the query are:
? Word-stem overlapping with the query.
? Three boolean features indicating whether the
sentence contains a subject, object or indirect
object dependency in common with the query.
? Overlapping between the environment predi-
cates in the sentence and those in the query.
? Two similarity metrics calculated by expanding
the query words using Google.
? SumFocus score (Vanderwende et al, 2006).
The cohesion-based attributes 2 are:
? Word-stem overlapping between this sentence
and the other sentences in the same document.
? Word-stem overlapping between this sentence
and the other sentences in the same cluster.
? Synset overlapping between this sentence and
the other sentences in the same document.
? Synset overlapping with other sentences in the
same collection.
Model training In order to train a traditional
SVM, both positive and negative examples are nec-
essary. From the pyramid data we are able to iden-
tify positive examples, but there is not enough ev-
idence to classify the remaining sentences as posi-
tive or negative. Although One-Class Support Vec-
tor Machine (OSVM) (Manevitz and Yousef, 2001)
can learn from just positive examples, according to
Yu et al (2002) they are prone to underfitting and
overfitting when data is scant (which happens in
2The mean, median, standard deviation and histogram of the
overlapping distribution are calculated and included as features.
58
this case), and a simple iterative procedure called
Mapping-Convergence (MC) algorithm can greatly
outperform OSVM (see the pseudocode in Figure 1).
Input: positive examples, POS, unlabeled examples U
Output: hypothesis at each iteration h?1, h?2, ..., h?k
1. Train h to identify ?strong negatives? in U :
N1 := examples from U classified as negative by h
P1 := examples from U classified as positive by h
2. Set NEG := ? and i := 1
3. Loop until Ni = ?,
3.1. NEG := NEG ? Ni
3.2. Train h?i from POS and NEG
3.3. Classify Pi by h?i:
Ni+1 = examples from Pi classified as negative
Pi+1 = examples from Pi classified as positive
5. Return {h?1, h?2, ..., h?k}
Figure 1: Mapping-Convergence algorithm.
The MC starts by identifying a small set of in-
stances that are very dissimilar to the positive exam-
ples, called strong negatives. Next, at each iteration,
a new SVM h?i is trained using the original positive
examples, and the negative examples found so far.
The set of negative instances is then extended with
the unlabeled instances classified as negative by h?i.
The following settings have been tried:
? The set of positive examples has been collected
either by matching document sentences to peer
summary sentences (Copeck and Szpakowicz,
2005) or by matching document sentences to
manual summary sentences.
? The initial set of strong negative examples for
the MC algorithm has been either built auto-
matically as described by Yu et al (2002), or
built by choosing manually, for each cluster, the
two or three automatic summaries with lowest
manual pyramid scores.
? Several SVM kernel functions have been tried.
For training, there were 6601 sentences from the
original documents, out of which around 120 were
negative examples and either around 100 or 500 pos-
itive examples, depending on whether the document
sentences had been matched to the manual or the
peer summaries. The rest were initially unlabeled.
Summary generation Given a query and a set of
documents, the trained SVMs are used to rank sen-
tences. The top ranked ones are checked to avoid re-
dundancy using a percentage overlapping measure.
3 Evaluation Framework
The SVMs, trained on DUC-2006 data, have been
tested on the DUC-2005 corpus, using the 20 clus-
ters manually evaluated with the pyramid method.
The sentence features were computed as described
before. Finally, the performance of each system
has been evaluated automatically using two differ-
ent measures: ROUGE and autoPan.
ROUGE, the automatic procedure used in DUC,
is based on n-gram co-occurrences. Both ROUGE-2
(henceforward R-2) and ROUGE-SU4 (R-SU4) has
been used to rank automatic summaries.
AutoPan is a procedure for automatically match-
ing fragments of text summaries to SCUs in pyra-
mids, in the following way: first, the text in the
SCU label and all its contributors is stemmed and
stop words are removed, obtaining a set of stem
vectors for each SCU. The system summary text is
also stemmed and freed from stop words. Next, a
search for non-overlapping windows of text which
can match SCUs is carried. Each match is scored
taking into account the score of the SCU as well as
the number of matching stems. The solution which
globally maximizes the sum of scores of all matches
is found using dynamic programming techniques.
According to Fuentes et al (2005), autoPan scores
are highly correlated to the manual pyramid scores.
Furthermore, autoPan also correlates well with man-
ual responsiveness and both ROUGE metrics.3
3.1 Results
Positive Strong neg. R-2 R-SU4 autoPan
peer pyramid scores 0.071 0.131 0.072
(Yu et al, 2002) 0.036 0.089 0.024
manual pyramid scores 0.025 0.075 0.024
(Yu et al, 2002) 0.018 0.063 0.009
Table 1: ROUGE and autoPan results using different SVMs.
Table 1 shows the results obtained, from which
some trends can be found: firstly, the SVMs
trained using the set of positive examples obtained
from peer summaries consistently outperform SVMs
trained using the examples obtained from the man-
ual summaries. This may be due to the fact that the
3In DUC-2005 pyramids were created using 7 manual sum-
maries, while in DUC-2006 only 4 were used. For that reason,
better correlations are obtained in DUC-2005 data.
59
number of positive examples is much higher in the
first case (on average 48,9 vs. 12,75 examples per
cluster). Secondly, generating automatically a set
with seed negative examples for the M-C algorithm,
as indicated by Yu et al (2002), usually performs
worse than choosing the strong negative examples
from the SCU annotation. This may be due to the
fact that its quality is better, even though the amount
of seed negative examples is one order of magnitude
smaller in this case (11.9 examples in average). Fi-
nally, the best results are obtained when using a RBF
kernel, while previous summarization work (Hirao
et al, 2003) uses polynomial kernels.
The proposed system attains an autoPan value of
0.072, while the best DUC-2005 one (Daume? III and
Marcu, 2005) obtains an autoPan of 0.081. The dif-
ference is not statistically significant. (Daume? III
and Marcu, 2005) system also scored highest in re-
sponsiveness (manually evaluated at NIST).
However, concerning ROUGE measures, the best
participant (Ye et al, 2005) has an R-2 score of
0.078 (confidence interval [0.073?0.080]) and an R-
SU4 score of 0.139 [0.135?0.142], when evaluated
on the 20 clusters used here. The proposed sys-
tem again is comparable to the best system in DUC-
2005 in terms of responsiveness, Daume? III and
Marcu (2005)?s R-2 score was 0.071 [0.067?0.074]
and R-SU4 was 0.126 [0.123?0.129] and it is better
than the DUC-2005 Fisher and Roark supervised ap-
proach with an R-2 of 0.066 and an R-SU4 of 0.122.
4 Conclusions and future work
The pyramid annotations are a valuable source of
information for training automatically text sum-
marization systems using Machine Learning tech-
niques. We explore different possibilities for apply-
ing them in training SVMs to rank sentences in order
of relevance to the query. Structural, cohesion-based
and query-dependent features are used for training.
The experiments have provided some insights on
which can be the best way to exploit the annota-
tions. Obtaining the positive examples from the an-
notations of the peer summaries is probably better
because most of the peer systems are extract-based,
while the manual ones are abstract-based. Also, us-
ing a very small set of strong negative example seeds
seems to perform better than choosing them auto-
matically with Yu et al (2002)?s procedure.
In the future we plan to include features from ad-
jacent sentences (Fisher and Roark, 2006) and use
rouge scores to initially select negative examples.
Acknowledgments
Work partially funded by the CHIL project, IST-2004506969.
References
T. Copeck and S. Szpakowicz. 2005. Leveraging pyramids. In
Proc. DUC-2005, Vancouver, Canada.
Hal Daume? III and Daniel Marcu. 2005. Bayesian summariza-
tion at DUC and a suggestion for extrinsic evaluation. In
Proc. DUC-2005, Vancouver, Canada.
S. Fisher and B. Roark. 2006. Query-focused summarization
by supervised sentence ranking and skewed word distribu-
tions. In Proc. DUC-2006, New York, USA.
M. Fuentes, E. Gonza`lez, D. Ferre?s, and H. Rodr??guez. 2005.
QASUM-TALP at DUC 2005 automatically evaluated with
the pyramid based metric autopan. In Proc. DUC-2005.
M. Fuentes, H. Rodr??guez, J. Turmo, and D. Ferre?s. 2006.
FEMsum at DUC 2006: Semantic-based approach integrated
in a flexible eclectic multitask summarizer architecture. In
Proc. DUC-2006, New York, USA.
T. Hirao, J. Suzuki, H. Isozaki, and E. Maeda. 2003. Ntt?s
multiple document summarization system for DUC2003. In
Proc. DUC-2003.
K. Ishikawa, S. Ando, S. Doi, and A. Okumura. 2002. Train-
able automatic text summarization using segmentation of
sentence. In Proc. 2002 NTCIR 3 TSC workshop.
H. Kazawa, T. Hirao, and E. Maeda. 2002. Ranking SVM and
its application to sentence selection. In Proc. 2002 Workshop
on Information-Based Induction Science (IBIS-2002).
L.M. Manevitz and M. Yousef. 2001. One-class SVM for docu-
ment classification. Journal of Machine Learning Research.
A. Nenkova and R. Passonneau. 2004. Evaluating content se-
lection in summarization: The pyramid method. In Proc.
HLT/NAACL 2004, Boston, USA.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical Report MSR-TR-
2005-101, Microsoft Research.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft research at DUC 2006: Task-focused summarization
with sentence simplification and lexical expansion. In Proc.
DUC-2006, New York, USA.
S. Ye, L. Qiu, and T.S. Chua. 2005. NUS at DUC 2005: Under-
standing documents via concept links. In Proc. DUC-2005.
H. Yu, J. Han, and K. C-C. Chang. 2002. PEBL: Positive
example-based learning for web page classification using
SVM. In Proc. ACM SIGKDD International Conference on
Knowledge Discovery in Databases (KDD02), New York.
60
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study on Similarity and Relatedness
Using Distributional and WordNet-based Approaches
Eneko Agirre? Enrique Alfonseca? Keith Hall? Jana Kravalova?? Marius Pas?ca? Aitor Soroa?
? IXA NLP Group, University of the Basque Country
? Google Inc.
? Institute of Formal and Applied Linguistics, Charles University in Prague
{e.agirre,a.soroa}@ehu.es {ealfonseca,kbhall,mars}@google.com
kravalova@ufal.mff.cuni.cz
Abstract
This paper presents and compares WordNet-
based and distributional similarity approaches.
The strengths and weaknesses of each ap-
proach regarding similarity and relatedness
tasks are discussed, and a combination is pre-
sented. Each of our methods independently
provide the best results in their class on the
RG and WordSim353 datasets, and a super-
vised combination of them yields the best pub-
lished results on all datasets. Finally, we pio-
neer cross-lingual similarity, showing that our
methods are easily adapted for a cross-lingual
task with minor losses.
1 Introduction
Measuring semantic similarity and relatedness be-
tween terms is an important problem in lexical se-
mantics. It has applications in many natural lan-
guage processing tasks, such as Textual Entailment,
Word Sense Disambiguation or Information Extrac-
tion, and other related areas like Information Re-
trieval. The techniques used to solve this problem
can be roughly classified into two main categories:
those relying on pre-existing knowledge resources
(thesauri, semantic networks, taxonomies or ency-
clopedias) (Alvarez and Lim, 2007; Yang and Pow-
ers, 2005; Hughes and Ramage, 2007) and those in-
ducing distributional properties of words from cor-
pora (Sahami and Heilman, 2006; Chen et al, 2006;
Bollegala et al, 2007).
In this paper, we explore both families. For the
first one we apply graph based algorithms to Word-
Net, and for the second we induce distributional
similarities collected from a 1.6 Terabyte Web cor-
pus. Previous work suggests that distributional sim-
ilarities suffer from certain limitations, which make
them less useful than knowledge resources for se-
mantic similarity. For example, Lin (1998b) finds
similar phrases like captive-westerner which made
sense only in the context of the corpus used, and
Budanitsky and Hirst (2006) highlight other prob-
lems that stem from the imbalance and sparseness of
the corpora. Comparatively, the experiments in this
paper demonstrate that distributional similarities can
perform as well as the knowledge-based approaches,
and a combination of the two can exceed the per-
formance of results previously reported on the same
datasets. An application to cross-lingual (CL) sim-
ilarity identification is also described, with applica-
tions such as CL Information Retrieval or CL spon-
sored search. A discussion on the differences be-
tween learning similarity and relatedness scores is
provided.
The paper is structured as follows. We first
present the WordNet-based method, followed by the
distributional methods. Section 4 is devoted to the
evaluation and results on the monolingual and cross-
lingual tasks. Section 5 presents some analysis, in-
cluding learning curves for distributional methods,
the use of distributional similarity to improve Word-
Net similarity, the contrast between similarity and
relatedness, and the combination of methods. Sec-
tion 6 presents related work, and finally, Section 7
draws the conclusions and mentions future work.
2 WordNet-based method
WordNet (Fellbaum, 1998) is a lexical database of
English, which groups nouns, verbs, adjectives and
adverbs into sets of synonyms (synsets), each ex-
pressing a distinct concept. Synsets are interlinked
with conceptual-semantic and lexical relations, in-
cluding hypernymy, meronymy, causality, etc.
Given a pair of words and a graph-based repre-
sentation of WordNet, our method has basically two
19
steps: We first compute the personalized PageR-
ank over WordNet separately for each of the words,
producing a probability distribution over WordNet
synsets. We then compare how similar these two dis-
crete probability distributions are by encoding them
as vectors and computing the cosine between the
vectors.
We represent WordNet as a graph G = (V,E) as
follows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets associated
to them by directed edges.
For each word in the pair we first compute a per-
sonalized PageRank vector of graph G (Haveliwala,
2002). Basically, personalized PageRank is com-
puted by modifying the random jump distribution
vector in the traditional PageRank equation. In our
case, we concentrate all probability mass in the tar-
get word.
Regarding PageRank implementation details, we
chose a damping value of 0.85 and finish the calcula-
tion after 30 iterations. These are default values, and
we did not optimize them. Our similarity method is
similar, but simpler, to that used by (Hughes and Ra-
mage, 2007), which report very good results on sim-
ilarity datasets. More details of our algorithm can be
found in (Agirre and Soroa, 2009). The algorithm
and needed resouces are publicly available1.
2.1 WordNet relations and versions
The WordNet versions that we use in this work are
the Multilingual Central Repository or MCR (At-
serias et al, 2004) (which includes English Word-
Net version 1.6 and wordnets for several other lan-
guages like Spanish, Italian, Catalan and Basque),
and WordNet version 3.02. We used all the rela-
tions in MCR (except cooccurrence relations and se-
lectional preference relations) and in WordNet 3.0.
Given the recent availability of the disambiguated
gloss relations for WordNet 3.03, we also used a
version which incorporates these relations. We will
refer to the three versions as MCR16, WN30 and
WN30g, respectively. Our choice was mainly moti-
vated by the fact that MCR contains tightly aligned
1http://http://ixa2.si.ehu.es/ukb/
2Available from http://http://wordnet.princeton.edu/
3http://wordnet.princeton.edu/glosstag
wordnets of several languages (see below).
2.2 Cross-linguality
MCR follows the EuroWordNet design (Vossen,
1998), which specifies an InterLingual Index (ILI)
that links the concepts across wordnets of differ-
ent languages. The wordnets for other languages in
MCR use the English WordNet synset numbers as
ILIs. This design allows a decoupling of the rela-
tions between concepts (which can be taken to be
language independent) and the links from each con-
tent word to its corresponding concepts (which is
language dependent).
As our WordNet-based method uses the graph of
the concepts and relations, we can easily compute
the similarity between words from different lan-
guages. For example, consider a English-Spanish
pair like car ? coche. Given that the Spanish Word-
Net is included in MCR we can use MCR as the
common knowledge-base for the relations. We can
then compute the personalized PageRank for each
of car and coche on the same underlying graph, and
then compare the similarity between both probabil-
ity distributions.
As an alternative, we also tried to use pub-
licly available mappings for wordnets (Daude et al,
2000)4 in order to create a 3.0 version of the Span-
ish WordNet. The mapping was used to link Spanish
variants to 3.0 synsets. We used the English Word-
Net 3.0, including glosses, to construct the graph.
The two Spanish WordNet versions are referred to
as MCR16 and WN30g.
3 Context-based methods
In this section, we describe the distributional meth-
ods used for calculating similarities between words,
and profiting from the use of a large Web-based cor-
pus.
This work is motivated by previous studies that
make use of search engines in order to collect co-
occurrence statistics between words. Turney (2001)
uses the number of hits returned by a Web search
engine to calculate the Pointwise Mutual Informa-
tion (PMI) between terms, as an indicator of syn-
onymy. Bollegala et al (2007) calculate a number
of popular relatedness metrics based on page counts,
4http://www.lsi.upc.es/?nlp/tools/download-map.php.
20
like PMI, the Jaccard coefficient, the Simpson co-
efficient and the Dice coefficient, which are com-
bined with lexico-syntactic patterns as model fea-
tures. The model parameters are trained using Sup-
port Vector Machines (SVM) in order to later rank
pairs of words. A different approach is the one taken
by Sahami and Heilman (2006), who collect snip-
pets from the results of a search engine and repre-
sent each snippet as a vector, weighted with the tf?idf
score. The semantic similarity between two queries
is calculated as the inner product between the cen-
troids of the respective sets of vectors.
To calculate the similarity of two words w1 and
w2, Ruiz-Casado et al (2005) collect snippets con-
taining w1 from a Web search engine, extract a con-
text around it, replace it with w2 and check for the
existence of that modified context in the Web.
Using a search engine to calculate similarities be-
tween words has the drawback that the data used will
always be truncated. So, for example, the numbers
of hits returned by search engines nowadays are al-
ways approximate and rounded up. The systems that
rely on collecting snippets are also limited by the
maximum number of documents returned per query,
typically around a thousand. We hypothesize that
by crawling a large corpus from the Web and doing
standard corpus analysis to collect precise statistics
for the terms we should improve over other unsu-
pervised systems that are based on search engine
results, and should yield results that are competi-
tive even when compared to knowledge-based ap-
proaches.
In order to calculate the semantic similarity be-
tween the words in a set, we have used a vector space
model, with the following three variations:
In the bag-of-words approach, for each word w
in the dataset we collect every term t that appears in
a window centered in w, and add them to the vector
together with its frequency.
In the context window approach, for each word
w in the dataset we collect every window W cen-
tered in w (removing the central word), and add it
to the vector together with its frequency (the total
number of times we saw windowW around w in the
whole corpus). In this case, all punctuation symbols
are replaced with a special token, to unify patterns
like , the <term> said to and ? the <term> said to.
Throughout the paper, when we mention a context
window of size N it means N words at each side of
the phrase of interest.
In the syntactic dependency approach, we parse
the entire corpus using an implementation of an In-
ductive Dependency parser as described in Nivre
(2006). For each word w we collect a template of
the syntactic context. We consider sequences of gov-
erning words (e.g. the parent, grand-parent, etc.) as
well as collections of descendants (e.g., immediate
children, grandchildren, etc.). This information is
then encoded as a contextual template. For example,
the context template cooks <term> delicious could
be contexts for nouns such as food, meals, pasta, etc.
This captures both syntactic preferences as well as
selectional preferences. Contrary to Pado and Lap-
ata (2007), we do not use the labels of the syntactic
dependencies.
Once the vectors have been obtained, the fre-
quency for each dimension in every vector is
weighted using the other vectors as contrast set, with
the ?2 test, and finally the cosine similarity between
vectors is used to calculate the similarity between
each pair of terms.
Except for the syntactic dependency approach,
where closed-class words are needed by the parser,
in the other cases we have removed stopwords (pro-
nouns, prepositions, determiners and modal and
auxiliary verbs).
3.1 Corpus used
We have used a corpus of four billion documents,
crawled from the Web in August 2008. An HTML
parser is used to extract text, the language of each
document is identified, and non-English documents
are discarded. The final corpus remaining at the end
of this process contains roughly 1.6 Terawords. All
calculations are done in parallel sharding by dimen-
sion, and it is possible to calculate all pairwise sim-
ilarities of the words in the test sets very quickly
on this corpus using the MapReduce infrastructure.
A complete run takes around 15 minutes on 2,000
cores.
3.2 Cross-linguality
In order to calculate similarities in a cross-lingual
setting, where some of the words are in a language l
other than English, the following algorithm is used:
21
Method Window size RG dataset WordSim353 dataset
MCR16 0.83 [0.73, 0.89] 0.53 (0.56) [0.45, 0.60]
WN30 0.79 [0.67, 0.86] 0.56 (0.58) [0.48, 0.63]
WN30g 0.83 [0.73, 0.89] 0.66 (0.69) [0.59, 0.71]
CW 1 0.83 [0.73, 0.89] 0.63 [0.57, 0.69]
2 0.83 [0.74, 0.90] 0.60 [0.53, 0.66]
3 0.85 [0.76, 0.91] 0.59 [0.52, 0.65]
4 0.89 [0.82, 0.93] 0.60 [0.53, 0.66]
5 0.80 [0.70, 0.88] 0.58 [0.51, 0.65]
6 0.75 [0.62, 0.84] 0.58 [0.50, 0.64]
7 0.72 [0.58, 0.82] 0.57 [0.49, 0.63]
BoW 1 0.81 [0.70, 0.88] 0.64 [0.57, 0.70]
2 0.80 [0.69, 0.87] 0.64 [0.58, 0.70]
3 0.79 [0.67, 0.86] 0.64 [0.58, 0.70]
4 0.78 [0.66, 0.86] 0.65 [0.58, 0.70]
5 0.77 [0.64, 0.85] 0.64 [0.58, 0.70]
6 0.76 [0.63, 0.85] 0.65 [0.58, 0.70]
7 0.75 [0.62, 0.84] 0.64 [0.58, 0.70]
Syn G1,D0 0.81 [0.70, 0.88] 0.62 [0.55, 0.68]
G2,D0 0.82 [0.72, 0.89] 0.55 [0.48, 0.62]
G3,D0 0.81 [0.71, 0.88] 0.62 [0.56, 0.68]
G1,D1 0.82 [0.72, 0.89] 0.62 [0.55, 0.68]
G2,D1 0.82 [0.73, 0.89] 0.62 [0.55, 0.68]
G3,D1 0.82 [0.72, 0.88] 0.62 [0.55, 0.68]
CW+ 4; G1,D0 0.88 [0.81, 0.93] 0.66 [0.59, 0.71]
Syn 4; G2,D0 0.87 [0.80, 0.92] 0.64 [0.57, 0.70]
4; G3,D0 0.86 [0.77, 0.91] 0.63 [0.56, 0.69]
4; G1,D1 0.83 [0.73, 0.89] 0.48 [0.40, 0.56]
4; G2,D1 0.83 [0.73, 0.89] 0.49 [0.40, 0.56]
4; G3,D1 0.82 [0.72, 0.89] 0.48 [0.40, 0.56]
Table 1: Spearman correlation results for the various WordNet-based
models and distributional models. CW=Context Windows, BoW=bag
of words, Syn=syntactic vectors. For Syn, the window size is actually
the tree-depth for the governors and descendants. For examples, G1
indicates that the contexts include the parents and D2 indicates that both
the children and grandchildren make up the contexts. The final grouping
includes both contextual windows (at width 4) and syntactic contexts in
the template vectors. Max scores are bolded.
1. Replace each non-English word in the dataset
with its 5-best translations into English using
state-of-the-art machine translation technology.
2. The vector corresponding to each Spanish word
is calculated by collecting features from all the
contexts of any of its translations.
3. Once the vectors are generated, the similarities
are calculated in the same way as before.
4 Experimental results
4.1 Gold-standard datasets
We have used two standard datasets. The first
one, RG, consists of 65 pairs of words collected by
Rubenstein and Goodenough (1965), who had them
judged by 51 human subjects in a scale from 0.0 to
4.0 according to their similarity, but ignoring any
other possible semantic relationships that might ap-
pear between the terms. The second dataset, Word-
Sim3535 (Finkelstein et al, 2002) contains 353 word
pairs, each associated with an average of 13 to 16 hu-
man judgements. In this case, both similarity and re-
5Available at http://www.cs.technion.ac.il/
?gabr/resources/data/wordsim353/wordsim353.html
Context RG terms and frequencies
ll never forget the * on his face when grin,2,smile,10
he had a giant * on his face and grin,3,smile,2
room with a huge * on her face and grin,2,smile,6
the state of every * will be updated every automobile,2,car,3
repair or replace the * if it is stolen automobile,2,car,2
located on the north * of the Bay of shore,14,coast,2
areas on the eastern * of the Adriatic Sea shore,3,coast,2
Thesaurus of Current English * The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2
wizard,4,glass,4,crane,5,smile,5
implement,5,oracle,2,lad,2
food,3,car,2,madhouse,3,jewel,3
asylum,4,tool,8,journey,6,etc.
be understood that the * 10 may be designed crane,3,tool,3
a fight between a * and a snake and bird,3,crane,5
Table 2: Sample of context windows for the terms in the RG dataset.
latedness are annotated without any distinction. Sev-
eral studies indicate that the human scores consis-
tently have very high correlations with each other
(Miller and Charles, 1991; Resnik, 1995), thus val-
idating the use of these datasets for evaluating se-
mantic similarity.
For the cross-lingual evaluation, the two datasets
were modified by translating the second word in
each pair into Spanish. Two humans translated
simultaneously both datasets, with an inter-tagger
agreement of 72% for RG and 84% for Word-
Sim353.
4.2 Results
Table 1 shows the Spearman correlation obtained on
the RG and WordSim353 datasets, including the in-
terval at 0.95 of confidence6.
Overall the distributional context-window ap-
proach performs best in the RG, reaching 0.89 corre-
lation, and both WN30g and the combination of con-
text windows and syntactic context perform best on
WordSim353. Note that the confidence intervals are
quite large in both RG and WordSim353, and few of
the pairwise differences are statistically significant.
Regarding WordNet-based approaches, the use of
the glosses and WordNet 3.0 (WN30g) yields the
best results in both datasets. While MCR16 is close
to WN30g for the RG dataset, it lags well behind
on WordSim353. This discrepancy is further ana-
lyzed is Section 5.3. Note that the performance of
WordNet in the WordSim353 dataset suffers from
unknown words. In fact, there are nine pairs which
returned null similarity for this reason. The num-
6To calculate the Spearman correlations values are trans-
formed into ranks, and we calculate the Pearson correlation on
them. The confidence intervals refer to the Pearson correlations
of the rank vectors.
22
Figure 1: Effect of the size of the training corpus, for the best distributional similarity model in each dataset. Left: WordSim353 with bag-of-words,
Right: RG with context windows.
Dataset Method overall ? interval
RG MCR16 0.78 -0.05 [0.66, 0.86]
WN30g 0.74 -0.09 [0.61, 0.84]
Bag of words 0.68 -0.23 [0.53, 0.79]
Context windows 0.83 -0.05 [0.73, 0.89]
WS353 MCR16 0.42 (0.53) -0.11 (-0.03) [0.34, 0.51]
WN30g 0.58 (0.67) -0.07 (-0.02) [0.51, 0.64]
Bag of words 0.53 -0.12 [0.45, 0.61]
Context windows 0.52 -0.11 [0.44, 0.59]
Table 3: Results obtained by the different methods on the Span-
ish/English cross-lingual datasets. The ? column shows the perfor-
mance difference with respect to the results on the original dataset.
ber in parenthesis in Table 1 for WordSim353 shows
the results for the 344 remaining pairs. Section 5.2
shows a proposal to overcome this limitation.
The bag-of-words approach tends to group to-
gether terms that can have a similar distribution of
contextual terms. Therefore, terms that are topically
related can appear in the same textual passages and
will get high values using this model. We see this
as an explanation why this model performed better
than the context window approach for WordSim353,
where annotators were instructed to provide high
ratings to related terms. On the contrary, the con-
text window approach tends to group together words
that are exchangeable in exactly the same context,
preserving order. Table 2 illustrates a few exam-
ples of context collected. Therefore, true synonyms
and hyponyms/hyperonyms will receive high simi-
larities, whereas terms related topically or based on
any other semantic relation (e.g. movie and star) will
have lower scores. This explains why this method
performed better for the RG dataset. Section 5.3
confirms these observations.
4.3 Cross-lingual similarity
Table 3 shows the results for the English-Spanish
cross-lingual datasets. For RG, MCR16 and the
context windows methods drop only 5 percentage
points, showing that cross-lingual similarity is feasi-
ble, and that both cross-lingual strategies are robust.
The results for WordSim353 show that WN30g is
the best for this dataset, with the rest of the meth-
ods falling over 10 percentage points relative to the
monolingual experiment. A closer look at the Word-
Net results showed that most of the drop in perfor-
mance was caused by out-of-vocabulary words, due
to the smaller vocabulary of the Spanish WordNet.
Though not totally comparable, if we compute the
correlation over pairs covered in WordNet alne, the
correlation would drop only 2 percentage points. In
the case of the distributional approaches, the fall in
performance was caused by the translations, as only
61% of the words were translated into the original
word in the English datasets.
5 Detailed analysis and system
combination
In this section we present some analysis, including
learning curves for distributional methods, the use
of distributional similarity to improve WordNet sim-
ilarity, the contrast between similarity and related-
ness, and the combination of methods.
5.1 Learning curves for distributional methods
Figure 1 shows that the correlation improves with
the size of the corpus, as expected. For the re-
sults using the WordSim353 corpus, we show the
results of the bag-of-words approach with context
size 10. Results improve from 0.5 Spearman correla-
tion up to 0.65 when increasing the corpus size three
orders of magnitude, although the effect decays at
the end, which indicates that we might not get fur-
23
Method Without similar words With similar words
WN30 0.56 (0.58) [0.48, 0.63] 0.58 [0.51, 0.65]
WN30g 0.66 (0.69) [0.59, 0.71] 0.68 [0.62, 0.73]
Table 4: Results obtained replacing unknown words with their most
similar three words (WordSim353 dataset).
Method overall Similarity Relatedness
MCR16 0.53 [0.45, 0.60] 0.65 [0.56, 0.72] 0.33 [0.21, 0.43]
WN30 0.56 [0.48, 0.63] 0.73 [0.65, 0.79] 0.38 [0.27, 0.48]
WN30g 0.66 [0.59, 0.71] 0.72 [0.64, 0.78] 0.56 [0.46, 0.64]
BoW 0.65 [0.59, 0.71] 0.70 [0.63, 0.77] 0.62 [0.53, 0.69]
CW 0.60 [0.53, 0.66] 0.77 [0.71, 0.82] 0.46 [0.36, 0.55]
Table 5: Results obtained on the WordSim353 dataset and on the two
similarity and relatedness subsets.
ther gains going beyond the current size of the cor-
pus. With respect to results for the RG dataset, we
used a context-window approach with context radius
4. Here, results improve even more with data size,
probably due to the sparse data problem collecting
8-word context windows if the corpus is not large
enough. Correlation improves linearly right to the
end, where results stabilize around 0.89.
5.2 Combining both approaches: dealing with
unknown words in WordNet
Although the vocabulary of WordNet is very ex-
tensive, applications are bound to need the similar-
ity between words which are not included in Word-
Net. This is exemplified in the WordSim353 dataset,
where 9 pairs contain words which are unknown to
WordNet. In order to overcome this shortcoming,
we could use similar words instead, as provided by
the distributional thesaurus. We used the distribu-
tional thesaurus defined in Section 3, using context
windows of width 4, to provide three similar words
for each of the unknown words in WordNet. Results
improve for both WN30 and WN30g, as shown in
Table 4, attaining our best results for WordSim353.
5.3 Similarity vs. relatedness
We mentioned above that the annotation guidelines
of WordSim353 did not distinguish between simi-
lar and related pairs. As the results in Section 4
show, different techniques are more appropriate to
calculate either similarity or relatedness. In order to
study this effect, ideally, we would have two ver-
sions of the dataset, where annotators were given
precise instructions to distinguish similarity in one
case, and relatedness in the other. Given the lack
of such datasets, we devised a simpler approach in
order to reuse the existing human judgements. We
manually split the dataset in two parts, as follows.
First, two humans classified all pairs as be-
ing synonyms of each other, antonyms, iden-
tical, hyperonym-hyponym, hyponym-hyperonym,
holonym-meronym, meronym-holonym, and none-
of-the-above. The inter-tagger agreement rate was
0.80, with a Kappa score of 0.77. This anno-
tation was used to group the pairs in three cate-
gories: similar pairs (those classified as synonyms,
antonyms, identical, or hyponym-hyperonym), re-
lated pairs (those classified as meronym-holonym,
and pairs classified as none-of-the-above, with a hu-
man average similarity greater than 5), and unrelated
pairs (those classified as none-of-the-above that had
average similarity less than or equal to 5). We then
created two new gold-standard datasets: similarity
(the union of similar and unrelated pairs), and relat-
edness (the union of related and unrelated)7.
Table 5 shows the results on the relatedness and
similarity subsets of WordSim353 for the different
methods. Regarding WordNet methods, both WN30
and WN30g perform similarly on the similarity sub-
set, but WN30g obtains the best results by far on
the relatedness data. These results are congruent
with our expectations: two words are similar if their
synsets are in close places in the WordNet hierarchy,
and two words are related if there is a connection
between them. Most of the relations in WordNet
are of hierarchical nature, and although other rela-
tions exist, they are far less numerous, thus explain-
ing the good results for both WN30 and WN30g on
similarity, but the bad results of WN30 on related-
ness. The disambiguated glosses help find connec-
tions among related concepts, and allow our method
to better model relatedness with respect to WN30.
The low results for MCR16 also deserve some
comments. Given the fact that MCR16 performed
very well on the RG dataset, it comes as a surprise
that it performs so poorly for the similarity subset
of WordSim353. In an additional evaluation, we at-
tested that MCR16 does indeed perform as well as
MCR30g on the similar pairs subset. We believe
that this deviation could be due to the method used to
construct the similarity dataset, which includes some
pairs of loosely related pairs labeled as unrelated.
7Available at http://alfonseca.org/eng/research/wordsim353.html
24
Methods combined in the SVM RG dataset WordSim353 dataset WordSim353 similarity WordSim353 relatedness
WN30g, bag of words 0.88 [0.82, 0.93] 0.78 [0.73, 0.81] 0.81 [0.76, 0.86] 0.72 [0.65, 0.77]
WN30g, context windows 0.90 [0.84, 0.94] 0.73 [0.68, 0.79] 0.83 [0.78, 0.87] 0.64 [0.56, 0.71]
WN30g, syntax 0.89 [0.83, 0.93] 0.75 [0.70, 0.79] 0.83 [0.78, 0.87] 0.67 [0.60, 0.74]
WN30g, bag of words, context windows, syntax 0.96 [0.93, 0.97] 0.78 [0.73, 0.82] 0.83 [0.78, 0.87] 0.71 [0.65, 0.77]
Table 6: Results using a supervised combination of several systems. Max values are bolded for each dataset.
Concerning the techniques based on distributional
similarities, the method based on context windows
provides the best results for similarity, and the bag-
of-words representation outperforms most of the
other techniques for relatedness.
5.4 Supervised combination
In order to gain an insight on which would be the up-
per bound that we could obtain when combining our
methods, we took the output of three systems (bag
of words with window size 10, context window with
size 4, and the WN30g run). Each of these outputs is
a ranking of word pairs, and we implemented an or-
acle that chooses, for each pair, the rank that is most
similar to the rank of the pair in the gold-standard.
The outputs of the oracle have a Spearman correla-
tion of 0.97 for RG and 0.92 for WordSim353, which
gives as an indication of the correlations that could
be achieved by choosing for each pair the rank out-
put by the best classifier for that pair.
The previous results motivated the use of a su-
pervised approach to combine the output of the
different systems. We created a training cor-
pus containing pairs of pairs of words from the
datasets, having as features the similarity and rank
of each pair involved as given by the differ-
ent unsupervised systems. A classifier is trained
to decide whether the first pair is more simi-
lar than the second one. For example, a train-
ing instance using two unsupervised classifiers is
0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative
meaning that the similarities given by the first clas-
sifier to the two pairs were 0.001364 and 0.327515
respectively, which ranked them in positions 31 and
64. The second classifier gave them similarities of
0.084805 and 0.109061 respectively, which ranked
them in positions 57 and 59. The class negative in-
dicates that in the gold-standard the first pair has a
lower score than the second pair.
We have trained a SVM to classify pairs of pairs,
and use its output to rank the entries in both datasets.
It uses a polynomial kernel with degree 4. We did
Method Source Spearman (MC) Pearson (MC)
(Sahami et al, 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78]
(Chen et al, 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85]
(Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89]
(Leacock et al, 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91]
(Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90]
(Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92]
(Bollegala et al, 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92]
(Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93]
(Jarmasz, 2003) Roget?s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94]
(Patwardhan et al, 2006) WordNet n/a 0.91
(Alvarez and Lim, 2007) WordNet n/a 0.91
(Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96]
(Hughes et al, 2007) WordNet 0.90 n/a
Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a
Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93]
Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95]
Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87]
SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97]
Table 7: Comparison with previous approaches for MC.
not have a held-out set, so we used the standard set-
tings of Weka, without trying to modify parameters,
e.g. C. Each word pair is scored with the number
of pairs that were considered to have less similar-
ity using the SVM. The results using 10-fold cross-
validation are shown in Table 6. A combination of
all methods produces the best results reported so far
for both datasets, statistically significant for RG.
6 Related work
Contrary to the WordSim353 dataset, common prac-
tice with the RG dataset has been to perform the
evaluation with Pearson correlation. In our believe
Pearson is less informative, as the Pearson correla-
tion suffers much when the scores of two systems are
not linearly correlated, something which happens
often given due to the different nature of the tech-
niques applied. Some authors, e.g. Alvarez and Lim
(2007), use a non-linear function to map the system
outputs into new values distributed more similarly
to the values in the gold-standard. In their case, the
mapping function was exp (?x4 ), which was chosenempirically. Finding such a function is dependent
on the dataset used, and involves an extra step in the
similarity calculations. Alternatively, the Spearman
correlation provides an evaluation metric that is in-
dependent of such data-dependent transformations.
Most similarity researchers have published their
25
Word pair M&C SVM Word pair M&C SVM
automobile, car 3.92 62 crane, implement 1.68 26
journey, voyage 3.84 54 brother, lad 1.66 39
gem, jewel 3.84 61 car, journey 1.16 37
boy, lad 3.76 57 monk, oracle 1.1 32
coast, shore 3.7 53 food, rooster 0.89 3
asylum, madhouse 3.61 45 coast, hill 0.87 34
magician, wizard 3.5 49 forest, graveyard 0.84 27
midday, noon 3.42 61 monk, slave 0.55 17
furnace, stove 3.11 50 lad, wizard 0.42 13
food, fruit 3.08 47 coast, forest 0.42 18
bird, cock 3.05 46 cord, smile 0.13 5
bird, crane 2.97 38 glass, magician 0.11 10
implement, tool 2.95 55 rooster, voyage 0.08 1
brother, monk 2.82 42 noon, string 0.08 5
Table 8: Our best results for the MC dataset.
Method Source Spearman
(Strube and Ponzetto, 2006) Wikipedia 0.19?0.48
(Jarmasz, 2003) WordNet 0.33?0.35
(Jarmasz, 2003) Roget?s 0.55
(Hughes and Ramage, 2007) WordNet 0.55
(Finkelstein et al, 2002) Web corpus, WN 0.56
(Gabrilovich and Markovitch, 2007) ODP 0.65
(Gabrilovich and Markovitch, 2007) Wikipedia 0.75
SVM Web corpus, WN 0.78
Table 9: Comparison with previous work for WordSim353.
complete results on a smaller subset of the RG
dataset containing 30 word pairs (Miller and
Charles, 1991), usually referred to as MC, making it
possible to compare different systems using differ-
ent correlation. Table 7 shows the results of related
work on MC that was available to us, including our
own. For the authors that did not provide the de-
tailed data we include only the Pearson correlation
with no confidence intervals.
Among the unsupervised methods introduced in
this paper, the context window produced the best re-
ported Spearman correlation, although the 0.95 con-
fidence intervals are too large to allow us to accept
the hypothesis that it is better than all others meth-
ods. The supervised combination produces the best
results reported so far. For the benefit of future re-
search, our results for the MC subset are displayed
in Table 8.
Comparison on the WordSim353 dataset is eas-
ier, as all researchers have used Spearman. The
figures in Table 9) show that our WordNet-based
method outperforms all previously published Word-
Net methods. We want to note that our WordNet-
based method outperforms that of Hughes and Ram-
age (2007), which uses a similar method. Although
there are some differences in the method, we think
that the main performance gain comes from the use
of the disambiguated glosses, which they did not
use. Our distributional methods also outperform all
other corpus-based methods. The most similar ap-
proach to our distributional technique is Finkelstein
et al (2002), who combined distributional similar-
ities from Web documents with a similarity from
WordNet. Their results are probably worse due to
the smaller data size (they used 270,000 documents)
and the differences in the calculation of the simi-
larities. The only method which outperforms our
non-supervised methods is that of (Gabrilovich and
Markovitch, 2007) when based on Wikipedia, prob-
ably because of the dense, manually distilled knowl-
edge contained in Wikipedia. All in all, our super-
vised combination gets the best published results on
this dataset.
7 Conclusions and future work
This paper has presented two state-of-the-art dis-
tributional and WordNet-based similarity measures,
with a study of several parameters, including per-
formance on similarity and relatedness data. We
show that the use of disambiguated glosses allows
for the best published results for WordNet-based
systems on the WordSim353 dataset, mainly due to
the better modeling of relatedness (as opposed to
similarity). Distributional similarities have proven
to be competitive when compared to knowledge-
based methods, with context windows being better
for similarity and bag of words for relatedness. Dis-
tributional similarity was effectively used to cover
out-of-vocabulary items in the WordNet-based mea-
sure providing our best unsupervised results. The
complementarity of our methods was exploited by
a supervised learner, producing the best results so
far for RG and WordSim353. Our results include
confidence values, which, surprisingly, were not in-
cluded in most previous work, and show that many
results over RG and WordSim353 are indistinguish-
able. The algorithm for WordNet-base similarity
and the necessary resources are publicly available8.
This work pioneers cross-lingual extension and
evaluation of both distributional and WordNet-based
measures. We have shown that closely aligned
wordnets provide a natural and effective way to
compute cross-lingual similarity with minor losses.
A simple translation strategy also yields good results
for distributional methods.
8http://ixa2.si.ehu.es/ukb/
26
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proc. of EACL
2009, Athens, Greece.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling
of Semantic Similarity between Words. Proc. of the
Conference on Semantic Computing, pages 355?362.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning multi-
lingual central repository. In Proc. of Global WordNet
Conference, Brno, Czech Republic.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proceedings of WWW?2007.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness.
Computational Linguistics, 32(1):13?47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proceedings of COCLING/ACL 2006.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets using structural information. In Proceedings of
ACL?2000, Hong Kong.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press,
Cambridge, Mass.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Transactions on Information Systems, 20(1):116?131.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proc of IJCAI, pages 6?12.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02: Proceedings of the 11th international con-
ference on World Wide Web, pages 517?526.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
M. Jarmasz. 2003. Roget?s Thesuarus as a lexical re-
source for Natural Language Processing.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, volume 33. Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An Electronic Lexical Database,
49(2):265?283.
D. Lin. 1998a. An information-theoretic definition of
similarity. In Proc. of ICML, pages 296?304, Wiscon-
sin, USA.
D. Lin. 1998b. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of ACL-98.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
J. Nivre. 2006. Inductive Dependency Parsing, vol-
ume 34 of Text, Speech and Language Technology.
Springer.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Re-
latedness of Concepts. In Proceedings of the EACL
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Pycholinguistics Together,
pages 1?8, Trento, Italy.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. Proc. of IJCAI,
14:448?453.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
M Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
Using context-window overlapping in Synonym Dis-
covery and Ontology Extension. In Proceedings of
RANLP-2005, Borovets, Bulgaria,.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. Proc. of WWW, pages 377?386.
M. Strube and S.P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the AAAI-2006, pages 1419?1424.
P.D. Turney. 2001. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. Lecture Notes in Computer
Science, 2167:491?502.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. of ACL, pages 133?138, Las
Cruces, New Mexico.
D. Yang and D.M.W. Powers. 2005. Measuring semantic
similarity in the taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
27
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 253?256,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Decompounding query keywords from compounding languages
Enrique Alfonseca
Google Inc.
ealfonseca@google.com
Slaven Bilac
Google Inc.
slaven@google.com
Stefan Pharies
Google Inc.
stefanp@google.com
Abstract
Splitting compound words has proved to be
useful in areas such as Machine Translation,
Speech Recognition or Information Retrieval
(IR). Furthermore, real-time IR systems (such
as search engines) need to cope with noisy
data, as user queries are sometimes written
quickly and submitted without review. In this
paper we apply a state-of-the-art procedure for
German decompounding to other compound-
ing languages, and we show that it is possible
to have a single decompounding model that is
applicable across languages.
1 Introduction
Compounding languages (Krott, 1999), such as Ger-
man, Dutch, Danish, Norwegian, Swedish, Greek
or Finnish, allow the generation of complex words
by merging together simpler ones. So, for instance,
the flower bouquet can be expressed in German as
Blumenstra?u?e, made up of Blumen (flower) and
stra?u?e (bouquet), and in Finnish as kukkakimppu,
from kukka (flower) and kimppu (bunch, collection).
For many language processing tools that rely on lex-
icons or language models it is very useful to be able
to decompose compounds to increase their cover-
age and reduce out-of-vocabulary terms. Decom-
pounders have been used successfully in Informa-
tion Retrieval (Braschler and Ripplinger, 2004), Ma-
chine Translation (Brown, 2002; Koehn and Knight,
2003) and Speech Recognition (Adda-Decker et al,
2000). The Cross Language Evaluation Forum
(CLEF) competitions have shown that very simple
approaches can produce big gains in Cross Lan-
guage Information Retrieval (CLIR) for German and
Dutch (Monz and de Rijke, 2001) and for Finnish
(Adafre et al, 2004).
When working with web data, which has not nec-
essarily been reviewed for correctness, many of the
words are more difficult to analyze than when work-
ing with standard texts. There are more words with
spelling mistakes, and many texts mix words from
different languages. This problem exists to a larger
degree when handling user queries: they are writ-
ten quickly, not paying attention to mistakes. How-
ever, being able to identify that achzigerjahre should
be decompounded as achzig+jahre (where achzig is
a misspelled variation of achtzig) is still useful in
obtaining some meaning from the user query and
in helping the spelling correction system. This pa-
per evaluates a state-of-the-art procedure for Ger-
man splitting (Alfonseca et al, 2008), robust enough
to handle query data, on different languages, and
shows that it is possible to have a single decom-
pounding model that can be applied to all the lan-
guages under study.
2 Problem definition and evaluation
settings
Any set of query keywords contains a large amount
of noisy data, such as words in foreign languages
or misspelled words. In order to be robust enough
to handle this kind of corpus, we require the fol-
lowing for a decompounder: first, obviously, com-
pounds should be split, and non-compounds should
be left untouched. This also applies if they are mis-
spelled. Unknown words or words involving a part
253
in a foreign language are split if there is a plausi-
ble interpretation of them being a compound word.
An example is Turingmaschine (Turing machine) in
German, where Turing is an English word. Finally,
words that are not really grammatical compounds,
but due to the user forgetting to input the blankspace
between the words (like desktopcomputer) are split.
For the evaluation, we have built and manually
annotated gold standard sets for German, Dutch,
Danish, Norwegian, Swedish and Finnish from fully
anonymized search query logs. Because people
do not use capitalization consistently when writing
queries, all the query logs are lowercased. By ran-
domly sampling keywords we would get few com-
pounds (as their frequency is small compared to that
of non-compounds), so we have proceeded in the
following way to ensure that the gold-standards con-
tain a substantial amount of compounds: we started
by building a very naive decompounder that splits a
word in several parts using a frequency-based com-
pound splitting method (Koehn and Knight, 2003).
Using this procedure, we obtain two random sam-
ples with possibly repeated words: one with words
that are considered non-compounds, and the other
with words that are considered compounds by this
naive approach. Next, we removed all the dupli-
cates from the previous list, and we had them an-
notated manually as compounds or non-compounds,
including the correct splittings. The sizes of the final
training sets vary between 2,000 and 3,600 words
depending on the language. Each compound was
annotated by two human judges who had received
the previous instructions on when to consider that
a keyword is a compound. For all the languages
considered, exactly one of the two judges was a na-
tive speaker living in a country where it is the of-
ficial language1. Table 1 shows the percentage of
agreement in classifying words as compounds or
non-compounds (Compound Classification Agree-
ment, CCA) for each language and the Kappa score
(Carletta, 1996) obtained from it, and the percent-
age of words for which also the decomposition pro-
vided was identical (Decompounding Agreement,
DA). The most common source of disagreement
were long words that could be split into two or more
1This requisite is important because many queries contain
novel or fashionable words.
Language CCA Kappa DA
German 93% 0.86 88%
Dutch 96% 0.92 96%
Danish 89% 0.78 89%
Norwegian 93% 0.86 81%
Swedish 96% 0.92 95%
Finnish 92% 0.84 89%
Table 1: Inter-judge agreement metrics.
Language Morphemes
German ?,-e,+s,+e,+en,+nen,+ens,+es,+ns,+er
Dutch ?,-e,+s,+e,+en
Danish ?,+e,+s
Norwegian ?,+e,+s
Swedish ?,+o,+u,+e,+s
Finnish ?
Table 2: Linking morphemes used in this work.
parts.
The evaluation is done using the metrics preci-
sion, recall and accuracy, defined in the following
way (Koehn and Knight, 2003):
? Correct splits: no. of compounds that are split correctly.
? Correct non-splits: no. non-compounds that are not split.
? Wrong non-splits: no. of compounds and are not split.
? Wrong faulty splits: no. of compounds that are incor-
rectly split.
? Wrong splits: no. of non-compounds that are split.
Precision =
correct splits
correct splits+ wrong faulty splits+ wrong splits
Recall =
correct splits
correct splits+ wrong faulty splits+ wrong non-splits
Accuracy =
correct splits
correct splits+ wrong splits
3 Combining corpus-based features
Most approaches for decompounding can be consid-
ered as having this general structure: given a word
w, calculate every possible way of splitting w in
one or more parts, and score those parts according
to some weighting function. If the highest scoring
splitting contains just one part, it means that w is
not a compound.
For the first step (calculating every possible split-
ting), it is common to take into account that modi-
fiers inside compound words sometimes need link-
ing morphemes. Table 2 lists the ones used in our
system (Langer, 1998; Marek, 2006; Krott, 1999).
254
Method Precision Recall Accuracy
Never split - 0.00% 64.09%
Geometric mean of frequencies 39.77% 54.06% 65.58%
Compound probability 60.41% 80.68% 76.23%
Mutual Information 82.00% 48.29% 80.52%
Support-Vector Machine 83.56% 79.48% 87.21%
Table 3: Results of the several configurations.
Concerning the second step, there is some work
that uses, for scoring, additional information such
as rules for cognate recognition (Brown, 2002) or
sentence-aligned parallel corpora and a translation
model, as in the full system described by Koehn
and Knight (2003). When those resources are not
available, the most common methods used for com-
pound splitting are using features such as the geo-
metric mean of the frequencies of compound parts in
a corpus, as in Koehn and Knight (2003)?s back-off
method, or learning a language model from a cor-
pus and estimating the probability of each sequence
of possible compound parts (Schiller, 2005; Marek,
2006). While these methods are useful for sev-
eral applications, such as CLIR and MT, they have
known weaknesses, such as preferring a decompo-
sition if a compound part happens to be very fre-
quent by chance, in the case of the frequency-based
method, or the preference of decompositions with
the least possible number of parts, in the case of the
probability-based method.
Alfonseca et al (2008) describe an integration of
the previous methods, together with the Mutual In-
formation and additional features obtained fromweb
anchor texts to train a supervised German decom-
pounder that outperforms the previous methods used
as standalone. The geometric mean of the frequen-
cies of compound parts and the probability estimated
from the language model usually attain a high recall,
given they are based on unigram features which are
easy to collect, but they have some weaknesses, as
mentioned above. On the other hand, while Mutual
Information is a much more precise metric, it is less
likely to have evidence about every single possible
pair of compound parts from a corpus, so it suffers
from low recall. A combination of all these metrics
into a learning model is able to attain a high recall.
An ablation study, reported in that paper, indicated
that the contribution of the web anchor texts is mini-
mal, so in this study we have just kept the other three
metrics. Table 3 shows the results reported for Ger-
Language P R A
German 83.56% 79.48% 87.21%
Dutch 78.99% 76.18% 83.45%
Danish 81.97% 87.12% 85.36%
Norwegian 88.13% 93.05% 90.40%
Swedish 83.34% 92.98% 87.79%
Finnish 90.79% 91.21% 91.62%
Table 4: Results in all the different languages.
man, training (i.e. counting frequencies and learn-
ing the language model) on the query keywords, and
running a 10-fold cross validation of a SVM with a
polynomial kernel using the German gold-standard.
The supervised system improves over the single un-
supervised metrics, attaining simultaneously good
recall and precision metrics.
4 Experiments and evaluation
The first motivation of this work is to test whether
the results reported for German are easy to repro-
duce in other languages. The results, shown in
Table 4, are very similar across languages, having
precision and recall values over 80% for most lan-
guages. A notable exception is Dutch, for which
the inter-judge agreement was the highest, so we ex-
pected the set of words to be easier to classify. An
analysis of the errors reported in the 10-fold cross-
validation indicates that most errors in Dutch were
wrong non-splits (in 147 cases) and wrong splits (in
139 cases), with wrong faulty splits happening only
in 20 occasions. Many of the wrong splits are loca-
tion names and trademarks, like youtube, piratebay
or smallville.
While the supervised model gives much better
results than the unsupervised ones, it still requires
the construction of a goldstandard from which to
train, which is usually costly. Therefore, we ran
another experiment to check whether the models
trained from some languages are applicable to other
languages. Table 5 shows the results obtained in this
case, the last column indicating the results when the
model is trained from the training instances from
all the other languages together. For each row, the
highest value and those which are inside its 95%
confidence interval are highlighted. Interestingly,
apart from a few exceptions, the results are rather
good for all the pairs of training and test language.
255
Language for training
de nl da no sv fi others
de P:83.56 P:78.69 P:74.96 P:88.93 P:82.72 P:89.69 P:80.89
R:79.48 R:75.48 R:92.77 R:89.26 R:90.79 R:89.96 R:76.07
A:87.21 A:82.76 A:83.53 A:90.31 A:86.53 A:90.82 A:88.15
nl P:79.52 P:78.99 P:76.93 P:92.81 P:85.67 P:90.98 P:77.53
R:75.74 R:76.18 R:89.02 R:55.08 R:87.15 R:86.73 R:76.54
A:87.77 A:83.45 A:83.21 A:91.00 A:86.47 A:88.95 A:82.32
da P:82.21 P:90.86 P:81.97 P:90.61 P:85.52 P:92.65 P:76.28
R:45.01 R:42.94 R:87.12 R:80.25 R:81.41 R:82.46 R:94.84
A:78.95 A:74.78 A:85.36 A:89.30 A:83.70 A:87.55 A:84.60
no P:68.23 P:70.18 P:74.85 P:88.13 P:82.25 P:90.08 P:88.78
R:83.33 R:87.18 R:96.67 R:93.05 R:94.21 R:91.84 R:90.88
A:83.77 A:80.67 A:84.18 A:90.40 A:87.24 A:91.41 A:89.85
sv P:76.57 P:77.33 P:76.31 P:89.00 P:83.34 P:90.81 P:83.89
R:79.76 R:81.79 R:94.66 R:90.41 R:92.98 R:90.86 R:92.05
A:87.18 A:83.38 A:84.57 A:89.67 A:87.79 A:91.38 A:87.69
fi P:74.12 P:74.50 P:75.93 P:88.71 P:83.54 P:90.79 P:90.70
R:80.12 R:81.67 R:95.39 R:91.46 R:92.70 R:91.21 R:90.62
A:85.93 A:81.98 A:84.51 A:90.07 A:87.52 A:91.62 A:91.18
Table 5: Result training and testing in different lan-
guages.
Thus, the use of features like frequencies, proba-
bilities or mutual information of compound parts is
truly language-independent and the models learned
from one language can safely be applied for decom-
pounding a different language without the need of
annotating a gold-standard for it.
Still, some trends in the results can be observed:
training with the Danish corpus produced the best
results in terms of recall for all the languages, but
recall for Danish still improved when we trained on
data from all languages. We believe that this in-
dicates that the Danish dataset contains items with
a more varied sets of feature combinations, so that
the models trained from it have a good coverage on
different kinds of compounds, but models trained
in other languages are not able to identify many of
the compounds in the Danish dataset. Concerning
precision, training with either the Norwegian or the
Finnish data produced very good results for most
languages. This is consistent with the monolingual
experiments (see Table 4) in which these languages
had the best results. We believe these trends are
probably due to the quality of the training data. In-
terestingly, the size of the training data is not so rel-
evant, as most of the best results are not located at
the last column in the table.
5 Conclusions
This paper shows that a combination of several
corpus-based metrics for decompounding, previ-
ously applied to German, with big improvements
with respect to other state-of-the-art systems, is also
useful for other compounding languages. More in-
terestingly, models learned from a goldstandard cre-
ated for some language can be applied to other
languages, sometimes producing better results than
when a model is trained and tested in the same lan-
guage. This should alleviate the fact that the pro-
posed system is supervised, as there should just be
the need of creating a goldstandard in one language
in order to train a generic decompounder, thus facil-
itating the availability of decompounders for smaller
languages like Faroese. For future work, we plan to
investigate more deeply how the quality of the data
affects the results, with a more detailed error analy-
sis. Other open lines include exploring the addition
of new features to the trained models.
References
S.F. Adafre, W.R. van Hage, J. Kamps, G.L. de Melo, and
M. de Rijke. 2004. The University of Amsterdam at
CLEF 2004. CLEF 2004 Workshop, pages 91?98.
M. Adda-Decker, G. Adda, and L. Lamel. 2000. Inves-
tigating text normalization and pronunciation variants
for German broadcast transcription. In ICSLP-2000.
E. Alfonseca, S. Bilac, and S. Pharies. 2008. German
decompounding in a difficult corpus. In CICLING.
M. Braschler and B. Ripplinger. 2004. How effective
is stemming and decompounding for german text re-
trieval? Information Retrieval, 7:291?316.
R.D. Brown. 2002. Corpus-driven splitting of compound
words. In TMI-2002.
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistics. Computational Linguistics,
22(2):249?254.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In ACL-2003.
A. Krott. 1999. Linking elements in compounds. LIN-
GUIST, 7 Oct 1999. http://listserv.linguistlist.org/cgi-
bin/wa?A2=ind9910a&L=linguist&P=6009.
S. Langer. 1998. Zur Morphologie und Semantik von
Nominalkomposita. Tagungsband der 4. Konferenz
zur Verarbeitung naturlicher Sprache (KONVENS).
T. Marek. 2006. Analysis of german compounds using
weighted finite state transducers. Technical report, BA
Thesis, Universita?t Tbingen.
C. Monz and M. de Rijke. 2001. Shallow morpholog-
ical analysis in monolingual information retrieval for
Dutch, German and Italian. In CLEF-2001.
A. Schiller. 2005. German compound analysis with
wfsc. In Finite State Methods and NLP 2005.
256
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Rote Extractor with Edit Distance-based Generalisation and
Multi-corpora Precision Calculation
Enrique Alfonseca12 Pablo Castells1 Manabu Okumura2 Maria Ruiz-Casado12
1Computer Science Deptartment 2Precision and Intelligence Laboratory
Univ. Auto?noma de Madrid Tokyo Institute of Technology
Enrique.Alfonseca@uam.es enrique@lr.pi.titech.ac.jp
Pablo.Castells@uam.es oku@pi.titech.ac.jp
Maria.Ruiz@uam.es maria@lr.pi.titech.ac.jp
Abstract
In this paper, we describe a rote extrac-
tor that learns patterns for finding seman-
tic relationships in unrestricted text, with
new procedures for pattern generalization
and scoring. These include the use of part-
of-speech tags to guide the generalization,
Named Entity categories inside the pat-
terns, an edit-distance-based pattern gen-
eralization algorithm, and a pattern accu-
racy calculation procedure based on eval-
uating the patterns on several test corpora.
In an evaluation with 14 entities, the sys-
tem attains a precision higher than 50% for
half of the relationships considered.
1 Introduction
Recently, there is an increasing interest in auto-
matically extracting structured information from
large corpora and, in particular, from the Web
(Craven et al, 1999). Because of the difficulty of
collecting annotated data, several procedures have
been described that can be trained on unannotated
textual corpora (Riloff and Schmelzenbach, 1998;
Soderland, 1999; Mann and Yarowsky, 2005).
An interesting approach is that of rote extrac-
tors (Brin, 1998; Agichtein and Gravano, 2000;
Ravichandran and Hovy, 2002), which look for
textual contexts that happen to convey a certain re-
lationship between two concepts.
In this paper, we describe some contributions
to the training of Rote extractors, including a pro-
cedure for generalizing the patterns, and a more
complex way of calculating their accuracy. We
first introduce the general structure of a rote ex-
tractor and its limitations. Next, we describe the
proposed modifications (Sections 2, 3 and 4) and
the evaluation performed (Section 5).
1.1 Rote extractors
According to the traditional definition of rote ex-
tractors (Mann and Yarowsky, 2005), they esti-
mate the probability of a relationship r(p, q) given
the surrounding contextA1pA2qA3. This is calcu-
lated, with a training corpus T , as the number of
times that two related elements r(x, y) from T ap-
pear with that same context A1xA2yA3, divided
by the total number of times that x appears in that
context together with any other word:
P (r(p, q)|A1pA2qA3) =
P
x,yr c(A1xA2yA3)
P
x,z c(A1xA2zA3)
(1)
x is called the hook, and y the target. In order
to train a Rote extractor from the web, this proce-
dure is usually followed (Ravichandran and Hovy,
2002):
1. Select a pair of related elements to be used
as seed. For instance, (Dickens,1812) for the
relationship birth year.
2. Submit the query Dickens AND 1812 to a
search engine, and download a number of
documents to build the training corpus.
3. Keep all the sentences containing both ele-
ments.
4. Extract the set of contexts between them and
identify repeated patterns. This may just be
the m characters to the left or to the right,
(Brin, 1998), the longest common substring
of several contexts (Agichtein and Gravano,
2000), or all substrings obtained with a suf-
fix tree constructor (Ravichandran and Hovy,
2002).
5. Download a separate corpus, called hook cor-
pus, containing just the hook (in the example,
Dickens).
6. Apply the previous patterns to the hook cor-
pus, calculate the precision of each pattern
9
in the following way: the number of times it
identifies a target related to the hook divided
by the total number of times the pattern ap-
pears.
7. Repeat the procedure for other examples of
the same relationship.
To illustrate this process, let us suppose that we
want to learn patterns to identify birth years. We
may start with the pair (Dickens, 1812). From the
downloaded corpus, we extract sentences such as
Dickens was born in 1812
Dickens (1812 - 1870) was an English writer
Dickens (1812 - 1870) wrote Oliver Twist
The system identifies that the contexts of the last
two sentences are very similar and chooses their
longest common substring to produce the follow-
ing patterns:
<hook> was born in <target>
<hook> ( <target> - 1870 )
In order to measure the precision of the ex-
tracted patterns, a new corpus is downloaded us-
ing the hook Dickens as the only query word, and
the system looks for appearances of the patterns
in the corpus. For every occurrence in which the
hook of the relationship is Dickens, if the target
is 1812 it will be deemed correct, and otherwise
it will be deemed incorrect (e.g. in Dickens was
born in Portsmouth).
1.2 Limitations and new proposal
We have identified the following limitations in this
algorithm: firstly, to our knowledge, no Rote ex-
tractor allows for the insertion of wildcards (e.g.
*) in the extracted patterns. Ravichandran and
Hovy (2002) have noted that this might be dan-
gerous if the wildcard matches unrestrictedly in-
correct sentences. However, we believe that the
precision estimation that is performed at the last
step of the algorithm, using the hook corpus, may
be used to rule out the dangerous wildcards while
keeping the useful ones.
Secondly, we believe that the procedure for cal-
culating the precision of the patterns may be some-
what unreliable in a few cases. For instance,
Ravichandran and Hovy (2002) report the follow-
ing patterns for the relationships Inventor, Discov-
erer and Location:
Relation Prec. Pattern
Inventor 1.0 <target> ?s <hook> and
Inventor 1.0 that <target> ?s <hook>
Discoverer 0.91 of <target> ?s <hook>
Location 1.0 <target> ?s <hook>
In this case, it can be seen that the same pattern
(the genitive construction) may be used to indi-
cate several different relationships, apart from the
most common use indicating possession. How-
ever, they all receive very high precision values.
The reason is that the patterns are only evaluated
for the same hook for which they were extracted.
Let us suppose that we obtain the pattern for Loca-
tion using the pairs (New York, Chrysler Building).
The genitive construction can be extracted from
the context New York?s Chrysler Building. After-
ward, when evaluating it, only sentences contain-
ing <target>?s Chrysler Building are taken into
account, which makes it unlikely that the pattern
is expressing a relationship other than Location,
so the pattern will receive a high precision value.
For our purposes, however, we need to collect
patterns for several relations such as writer-book,
painter-picture, director-film, actor-film, and we
want to make sure that the obtained patterns are
only applicable to the desired relationship. Pat-
terns like <target> ?s <hook> are very likely to
be applicable to all of these relationships at the
same time, so we would like to be able to discard
them automatically.
Hence, we propose the following improvements
for a Rote extractor:
? A new pattern generalization procedure that
allows the inclusion of wildcards in the pat-
terns.
? The combination with Named Entity recogni-
tion, so people, locations, organizations and
dates are replaced by their entity type in the
patterns, in order to increase their degree of
generality. This is in line with Mann and
Yarowsky (2003)?s modification, consisting
in replacing all numbers in the patterns with
the symbol ####.
? A new precision calculation procedure, in a
way that the patterns obtained for a given re-
lationship are evaluated on the corpus for dif-
ferent relationships, in order to improve the
detection of over-general patterns.
2 Proposed pattern generalization
procedure
To begin with, for every appearance of a pair of
concepts, we extract a context around them. Next,
those contexts are generalized to obtain the parts
that are shared by several of them. The procedure
is detailed in the following subsections.
10
Birth year:
BOS/BOS <hook> (/( <target> -/- number/entity )/) EOS/EOS
BOS/BOS <hook> (/( <target> -/- number/entity )/) British/JJ writer/NN
BOS/BOS <hook> was/VBD born/VBN on/IN the/DT first/JJ of/IN time expr/entity ,/, <target> ,/, at/IN location/entity ,/, of/IN
BOS/BOS <hook> (/( <target> -/- )/) a/DT web/NN guide/NN
Birth place:
BOS/BOS <hook> was/VBD born/VBN in/IN <target> ,/, in/IN central/JJ location/entity ,/,
BOS/BOS <hook> was/VBD born/VBN in/IN <target> date/entity and/CC moved/VBD to/TO location/entity
BOS/BOS Artist/NN :/, <hook> -/- <target> ,/, location/entity (/( number/entity -/-
BOS/BOS <hook> ,/, born/VBN in/IN <target> on/IN date/entity ,/, worked/VBN as/IN
Author-book:
BOS/BOS <hook> author/NN of/IN <target> EOS/EOS
BOS/BOS Odysseus/NNP :/, Based/VBN on/IN <target> ,/, <hook> ?s/POS epic/NN from/IN Greek/JJ mythology/NN
BOS/BOS Background/NN on/IN <target> by/IN <hook> EOS/EOS
did/VBD the/DT circumstances/NNS in/IN which/WDT <hook> wrote/VBD "/?? <target> "/?? in/IN number/entity ,/, and/CC
Capital-country:
BOS/BOS <hook> is/VBZ the/DT capital/NN of/IN <target> location/entity ,/, location/entity correct/JJ time/NN
BOS/BOS The/DT harbor/NN in/IN <hook> ,/, the/DT capital/NN of/IN <target> ,/, is/VBZ number/entity of/IN location/entity
BOS/BOS <hook> ,/, <target> EOS/EOS
BOS/BOS <hook> ,/, <target> -/- organization/entity EOS/EOS
Figure 1: Example patterns extracted from the training corpus for each several kinds of relationships.
2.1 Context extraction procedure
After selecting the sentences for each pair of re-
lated words in the training set, these are pro-
cessed with a part-of-speech tagger and a module
for Named Entity Recognition and Classification
(NERC) that annotates people, organizations, lo-
cations, dates, relative temporal expressions and
numbers. Afterward, a context around the two
words in the pair is extracted, including (a) at most
five words to the left of the first word; (b) all the
words in between the pair words; (c) at most five
words to the right of the second word. The context
never jumps over sentence boundaries, which are
marked with the symbols BOS (Beginning of sen-
tence) and EOS (End of sentence). The two related
concepts are marked as <hook> and <target>.
Figure 1 shows several example contexts extracted
for the relationships birth year, birth place, writer-
book and capital-country.
Furthermore, for each of the entities in the re-
lationship, the system also stores in a separate file
the way in which they are annotated in the training
corpus: the sequences of part-of-speech tags of ev-
ery appearance, and the entity type (if marked as
such). So, for instance, typical PoS sequences for
names of authors are ?NNP?1 (surname) and ?NNP
NNP? (first name and surname). A typical entity
kind for an author is person.
2.2 Generalization pseudocode
In order to identify the portions in common be-
tween the patterns, and to generalize them, we ap-
ply the following pseudocode (Ruiz-Casado et al,
in press):
1All the PoS examples in this paper are done with Penn
Treebank labels (Marcus et al, 1993).
1. Store all the patterns in a set P .
2. Initialize a setR as an empty set.
3. While P is not empty,
(a) For each possible pair of patterns, cal-
culate the distance between them (de-
scribed in Section 2.3).
(b) Take the two patterns with the smallest
distance, pi and pj .
(c) Remove them from P , and add them to
R.
(d) Obtain the generalization of both, pg
(Section 2.4).
(e) If pg does not have a wildcard adjacent
to the hook or the target, add it to P .
4. ReturnR
At the end, R contains all the initial patterns
and those obtained while generalizing the previous
ones. The motivation for step (e) is that, if a pat-
tern contains a wildcard adjacent to either the hook
or the target, it will be impossible to know where
it starts or ends. For instance, when applying the
pattern <hook> wrote * <target> to a text, the
wildcard prevents the system from guessing where
the title of the book starts.
2.3 Edit distance calculation
So as to calculate the similarity between two pat-
terns, a slightly modified version of the dynamic
programming algorithm for edit-distance calcula-
tion (Wagner and Fischer, 1974) is used. The dis-
tance between two patterns A and B is defined as
the minimum number of changes (insertion, addi-
tion or replacement) that have to be done to the
first one in order to obtain the second one.
The calculation is carried on by filling a ma-
trix M, as shown in Figure 2 (left). At the same
11
A: wrote the well known novel
B: wrote the classic novel
M 0 1 2 3 4 D 0 1 2 3 4
0 0 1 2 3 4 0 I I I I
1 1 0 1 2 3 1 R E I I I
2 2 1 0 1 2 2 R R E I I
3 3 2 1 1 2 3 R R R U I
4 4 3 2 2 2 4 R R R R U
5 5 4 3 3 2 5 R R R R E
Figure 2: Example of the edit distance algorithm. A and B are two word patterns;M is the matrix in which the edit distance
is calculated, and D is the matrix indicating the choice that produced the minimal distance for each cell inM.
time that we calculate the edit distance matrix, it
is possible to fill in another matrix D, in which we
record which of the choices was selected at each
step: insertion, deletion, replacement or no edi-
tion. This will be used later to obtain the gener-
alized pattern. We have used the following four
characters:
? I means that it is necessary to insert a token
in the first pattern to obtain the second one.
? R means that it is necessary to remove a to-
ken.
? E means that the corresponding tokens are
equal, so no edition is required.
? U means that the corresponding tokens are
unequal, so a replacement has to be done.
Figure 2 shows an example for two patterns,
A and B, containing respectively 5 and 4 to-
kens. M(5, 4) has the value 2, indicating the dis-
tance between the two complete patterns. For in-
stance, the two editions would be replacing well
by classic and removing known.
2.4 Obtaining the generalized pattern
After calculating the edit distance between two
patterns A and B, we can use matrix D to obtain
a generalized pattern, which should maintain the
common tokens shared by them. The procedure
used is the following:
? Every time there is an insertion or a deletion,
the generalized pattern will contain a wild-
card, indicating that there may be anything in
between.
? Every time there is replacement, the general-
ized pattern will contain a disjunction of both
tokens.
? Finally, in the positions where there is no edit
operation, the token that is shared between
the two patterns is left unchanged.
The patterns in the example will produced the
generalized pattern
Wrote the well known novel
Wrote the classic novel
???????????
Wrote the well|classic * novel
The generalization of these two patterns pro-
duces one that can match a wide variety of sen-
tences, so we should always take care in order not
to over-generalize.
2.5 Considering part-of-speech tags and
Named Entities
If we consider the result in the previous example,
we can see that the disjunction has been made be-
tween an adverb and an adjective, while the other
adjective has been deleted. A more natural result,
with the same number of editing operations as the
previous one, would have been to delete the adverb
to obtain the following generalization:
Wrote the well known novel
Wrote the classic novel
???????????
Wrote the * known|classic novel
This is done taking into account part-of-speech
tags in the generalization process. In this way, the
edit distance has been modified so that a replace-
ment operation can only be done between words of
the same part-of-speech.2 Furthermore, replace-
ments are given an edit distance of 0. This favors
the choice of replacements with respect to dele-
tions and insertions. To illustrate this point, the
distance between known|classic/JJ and old/JJ
2Note that, although our tagger produces the very detailed
PennTreebank labels, we consider that all nouns (NN, NNS,
NNP and NNPS) belong to the same part-of-speech class, and
the same for adjectives, verbs and adverbs.
12
Hook Birth Death Birth place Author of Director of Capital of
Charles Dickens 1812 1870 Portsmouth
{Oliver Twist,
The Pickwick Papers,
Nicholas Nickleby,
David Copperfield...}
None None
Woody Allen 1935 None Brooklin None
{Bananas,
Annie Hall,
Manhattan, ... }
None
Luanda None None None None None Angola
Table 1: Example rows in the input table for the system.
will be set to 0, because both tokens are adjectives.
In other words, the d function is redefined as:
d(A[i], B[j]) =
(
0 if PoS(A[i]) = PoS(B[j])
1 otherwise
(2)
Note that all the entities identified by the NERC
module will appear with a PoS tag of entity,
so it is possible to have a disjunction such as
location|organization/entity in a general-
ized pattern (See Figure 1).
3 Proposed pattern scoring procedure
As indicated above, if we measure the precision of
the patterns using a hook corpus-based approach,
the score may be inadvertently increased because
they are only evaluated using the same terms with
which they were extracted. The approach pro-
posed herein is to take advantage of the fact that
we are obtaining patterns for several relationships.
Thus, the hook corpora for some of the patterns
can be used also to identify errors done by other
patterns.
The input of the system now is not just a list
of related pairs, but a table including several rela-
tionships for the same entities. We may consider
it as mini-biographies as in Mann and Yarowsky
(2005)?s system. Table 1 shows a few rows in the
input table for the system. The cells for which
no data is provided have a default value of None,
which means that anything extracted for that cell
will be considered as incorrect.
Although this table can be written by hand, in
our experiments we have chosen to build it auto-
matically from the lists of related pairs. The sys-
tem receives the seed pairs for the relationships,
and mixes the information from all of them in a
single table. In this way, if Dickens appears in
the seed list for the birth year, death year, birth
place and writer-book relationships, four of the
cells in its row will be filled in with values, and
all the rest will be set to None. This is probably a
very strict evaluation, because, for all the cells for
which there was no value in any of the lists, any re-
sult obtained will be judged as incorrect. However,
the advantage is that we can study the behavior of
the system working with incomplete data.
The new procedure for calculating the patterns?
precisions is as follows:
1. For every relationship, and for every hook,
collect a hook corpus from the Internet.
2. Apply the patterns to all the hook corpora
collected. Whenever a pattern extracts a re-
lationship from a sentence,
? If the table does not contain a row for
the hook, ignore the result.
? If the extracted target appears in the cor-
responding cell in the table, consider it
correct.
? If that cell contained different values, or
None, consider it incorrect.
For instance, the pattern <target> ?s <hook>
extracted for director-film may find, in the Dick-
ens corpus, book titles. Because these titles do not
appear in the table as films directed by Dickens,
the pattern will be considered to have a low accu-
racy.
In this step, every pattern that did not apply at
least three times in the test corpora was discarded.
4 Pattern application
Finally, given a set of patterns for a particular
relation, the procedure for obtaining new pairs is
straightforward:
1. For any of the patterns,
2. For each sentence in the test corpus,
(a) Look for the left-hand-side context in
the sentence.
(b) Look for the middle context.
(c) Look for the right-hand-side context.
(d) Take the words in between, and check
that either the sequence of part-of-
speech tags or the entity type had been
13
Applied Prec. Pattern
3 1.0 BOS/BOS On/IN time expr/entity TARGET HOOK was/VBD baptized|born/VBN EOS/EOS
15 1.0 "/?? HOOK (/( TARGET -/-
4 1.0 ,/, TARGET ,/, */* Eugne|philosopher|playwright|poet/NNP HOOK earned|was/VBD */* at|in/IN
23 1.0 -|--/- HOOK (/( TARGET -/-
12 1.0 AND|and|or/CC HOOK (/( TARGET -/-
48 1.0 By|about|after|by|for|in|of|with/IN HOOK TARGET -/-
4 1.0 On|of|on/IN TARGET ,/, HOOK emigrated|faced|graduated|grew|has|perjured|settled|was/VBD
12 1.0 BOS/BOS HOOK TARGET -|--/-
49 1.0 ABOUT|ALFRED|Amy|Audre|Authors|BY| (...) |teacher|writer/NNPS HOOK (/( TARGET -|--/-
7 1.0 BOS/BOS HOOK (/( born/VBN TARGET )/)
3 1.0 BOS/BOS HOOK ,/, */* ,/, TARGET ,/,
13 1.0 BOS/BOS HOOK ,|:/, TARGET -/-
132 0.98 BOS/BOS HOOK (/( TARGET -|--/-
18 0.94 By|Of|about|as|between|by|for|from|of|on|with/IN HOOK (/( TARGET -/-
33 0.91 BOS/BOS HOOK ,|:/, */* (/( TARGET -|--/-
10 0.9 BOS/BOS HOOK ,|:/, */* ,|:/, TARGET -/-
3 0.67 ,|:|;/, TARGET ,|:/, */* Birth|son/NN of/IN */* General|playwright/NNP HOOK ,|;/,
210 0.63 ,|:|;/, HOOK (/( TARGET -|--/-
7 0.29 (/( HOOK TARGET )/)
Table 3: Patterns for the relationship birth year.
.
Relation Seeds Extr. Gener. Filt.
Actor-film 133 480 519 10
Writer-book 836 3858 4847 171
Birth-year 492 2520 3220 19
Birth-place 68 681 762 5
Country-capital 36 932 1075 161
Country-president 56 1260 1463 119
Death-year 492 2540 3219 16
Director-film 1530 3126 3668 121
Painter-picture 44 487 542 69
Player-team 110 2903 3514 195
Table 2: Number of seed pairs for each relation,
and number of unique patterns after the extraction
and the generalization step, and after calculating
their accuracy and filtering those that did not apply
3 times on the test corpus.
seen in the training corpus for that role
(hook or target). If so, output the rela-
tionship.
5 Evaluation and results
The procedure has been tested with 10 different
relationships. For each pair in each seed list, a
corpus with 500 documents has been collected us-
ing Google, from which the patterns are extracted.
Table 2 shows the number of patterns obtained. It
is interesting to see that for some relations, such as
birth-year or birth-place, more than one thousand
patterns have been reduced to a few. Table 3 shows
the patterns obtained for the relationship birth-
year. It can also be seen that some of the patterns
with good precision contain the wildcard *, which
helped extract the correct birth year in roughly 50
occasions. Specially of interest is the last pattern,
(/( HOOK TARGET )/)
which resulted in an accuracy of 0.29 with the pro-
Relation Precision Incl. prec. Applied
Actor-film 0% 76.84% 95
Writer-book 6.25% 28.13% 32
Birth-year 79.67% 79.67% 477
Birth-place 14.56% 14.56% 103
Country-capital 72.43% 72.43% 599
Country-president 81.40% 81.40% 43
Death-year 96.71% 96.71% 152
Director-film 43.40% 84.91% 53
Painter-picture - - 0
Player-team 52.50% 52.50% 120
Table 4: Precision, inclusion precision and num-
ber of times that a pattern extracted information,
when applied to a test corpus.
cedure here indicated, but which would have ob-
tained an accuracy of 0.54 using the traditional
hook corpus approach. This is because in other
test corpora (e.g. in the one containing soccer
players and clubs) it is more frequent to find the
name of a person followed by a number that is not
his/her birth year, while that did not happen so of-
ten in the birth year test corpus.
For evaluating the patterns, a new test corpus
has been collected for fourteen entities not present
in the training corpora, again using Google. The
chosen entities are Robert de Niro and Natalie
Wood (actors), Isaac Asimov and Alfred Bester
(writers), Montevideo and Yaounde (capitals),
Gloria Macapagal Arroyo and Hosni Mubarak
(country presidents), Bernardo Bertolucci and
Federico Fellini (directors), Peter Paul Rubens and
Paul Gauguin (painters), and Jens Lehmann and
Thierry Henry (soccer players). Table 4 shows the
results obtained for each relationship.
We have observed that, for those relationships
in which the target does not belong to a Named
14
Entity type, it is common for the patterns to extract
additional words together with the right target. For
example, rather than extracting The Last Emperor,
the patterns may extract this title together with
its rating or its length, the title between quotes,
or phrases such as The classic The Last Emperor.
In the second column in the table, we measured
the percentage of times that a correct answer ap-
pears inside the extracted target, so these examples
would be considered correct. We call this metric
inclusion precision.
5.1 Comparison to related approaches
Although the above results are not comparable to
Mann and Yarowsky (2005), as the corpora used
are different, in most cases the precision is equal
or higher to that reported there. On the other hand,
we have rerun Ravichandran and Hovy (2002)?s
algorithm on our corpus. In order to assure a
fair comparison, their algorithm has been slightly
modified so it also takes into account the part-of-
speech sequences and entity types while extract-
ing the hooks and the targets during the rule ap-
plication. So, for instance, the relationship birth
date is only extracted between a hook tagged as
a person and a target tagged as either a date or
a number. The results are shown in Table 5. As
can be seen, our procedure seems to perform bet-
ter for all of the relations except birth place. It
is interesting to note that, as could be expected,
for those targets for which there is no entity type
defined (films, books and pictures), Ravichandran
and Hovy (2002)?s extracts many errors, because
it is not possible to apply the Named Entity Rec-
ognizer to clean up the results, and the accuracy
remains below 10%. On the other hand, that trend
does not seem to affect our system, which had
very poor results for painter-picture, but reason-
ably good for actor-film.
Other interesting case is that of birth places.
A manual observation of our generalized patterns
shows that they often contain disjunctions of verbs
such as that in (1), that detects not just the birth
place but also places where the person lived. In
this case, Ravichandran and Hovy (2002)?s pat-
terns resulted more precise as they do not contain
disjunctions or wildcards.
(1) HOOK ,/, returned|travelled|born/VBN
to|in/IN TARGET
It is interesting that, among the three relation-
ships with the smaller number of extracted pat-
terns, one of them did not produce any result, and
Ravichandran
Relation Our approach and Hovy?s
Actor-film 76.84% 1.71%
Writer-book 28.13% 8.55%
Birth-year 79.67% 49.49%
Birth-place 14.56% 88.66%
Country-capital 72.43% 24.79%
Country-president 81.40% 16.13%
Death-year 96.71% 35.35%
Director-film 84.91% 1.01%
Painter-picture - 0.85%
Player-team 52.50% 44.44%
Table 5: Inclusion precision on the same test cor-
pus for our approach and Ravichandran and Hovy
(2002)?s.
the two others attained a low precision. Therefore,
it should be possible to improve the performance
of the system if, while training, we augment the
training corpora until the number of extracted pat-
terns exceeds a given threshold.
6 Related work
Extracting information using Machine Learning
algorithms has received much attention since the
nineties, mainly motivated by the Message Un-
derstanding Conferences (MUC6, 1995; MUC7,
1998). From the mid-nineties, there are systems
that learn extraction patterns from partially an-
notated and unannotated data (Huffman, 1995;
Riloff, 1996; Riloff and Schmelzenbach, 1998;
Soderland, 1999).
Generalizing textual patterns (both manually
and automatically) for the identification of re-
lationships has been proposed since the early
nineties (Hearst, 1992), and it has been applied
to extending ontologies with hyperonymy and
holonymy relationships (Kietz et al, 2000; Cimi-
ano et al, 2004; Berland and Charniak, 1999),
with overall precision varying between 0.39 and
0.68. Finkelstein-Landau and Morin (1999) learn
patterns for company merging relationships with
exceedingly good accuracies (between 0.72 and
0.93).
Rote extraction systems from the web have
the advantage that the training corpora can be
collected easily and automatically. Several
similar approaches have been proposed (Brin,
1998; Agichtein and Gravano, 2000; Ravichan-
dran and Hovy, 2002), with various applications:
Question-Answering (Ravichandran and Hovy,
2002), multi-document Named Entity Corefer-
ence (Mann and Yarowsky, 2003), and generating
15
biographical information (Mann and Yarowsky,
2005).
7 Conclusions and future work
We have described here a new procedure for build-
ing a rote extractor from the web. Compared to
other similar approaches, it addresses several is-
sues: (a) it is able to generate generalized patterns
containing wildcards; (b) it makes use of PoS and
Named Entity tags during the generalization pro-
cess; and (c) several relationships are learned and
evaluated at the same time, in order to test each
one on the test corpora built for the others. The re-
sults, measured in terms of precision and inclusion
precision are very good in most of the cases.
Our system needs an input table, which may
seem more complicated to compile that the list of
related pairs used by previous approaches, but we
have seen that the table can be built automatically
from the lists, with no extra work. In any case,
the time to build the table is significantly smaller
than the time needed to write the extraction pat-
terns manually.
Concerning future work, we are currently trying
to improve the estimation of the patterns accuracy
for the pruning step. We also plan to apply the ob-
tained patterns in a system for automatically gen-
erating biographical knowledge bases from vari-
ous web corpora.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In Proceedings of ICDL, pages 85?94.
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of ACL-99.
S. Brin. 1998. Extracting patterns and relations from
the World Wide Web. In Proceedings of the WebDB
Workshop at the 6th International Conference on Ex-
tending Database Technology, EDBT?98.
P. Cimiano, S. Handschuh, and S. Staab. 2004. To-
wards the self-annotating web. In Proceedings of the
13th World Wide Web Conference, pages 462?471.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1999. Learn-
ing to construct knowledge bases from the world
wide web. Artificial Intelligence, 118(1?2):69?113.
M. Finkelstein-Landau and E. Morin. 1999. Extracting
semantic relationships between terms: supervised
vs. unsupervised methods. In Workshop on Ontolo-
gial Engineering on the Global Info. Infrastructure.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING-92.
S. Huffman. 1995. Learning information extraction
patterns from examples. In IJCAI-95 Workshop on
New Approaches to Learning for NLP.
J. Kietz, A. Maedche, and R. Volz. 2000. A method
for semi-automatic ontology acquisition from a cor-
porate intranet. In Workshop ?Ontologies and text?.
G. S. Mann and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In CoNLL-2003.
G. S. Mann and D. Yarowsky. 2005. Multi-field in-
formation extraction and cross-document fusion. In
ACL 2005.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
MUC6. 1995. Proceedings of the 6th Message Under-
standing Conference (MUC-6). Morgan Kaufman.
MUC7. 1998. Proceedings of the 7th Message Under-
standing Conference (MUC-7). Morgan Kaufman.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In
Proceedings of ACL-2002, pages 41?47.
E. Riloff and M. Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In
Proceedings of WVLC, pages 49?56.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In AAAI.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. in
press. Automatising the learning of lexical pat-
terns: an application to the enrichment of wordnet
by extracting semantic relationships from wikipedia.
Data and Knowledge Engineering.
S. Soderland. 1999. Learning information extraction
rules for semi-structured and free text. Machine
Learning, 34(1?3):233?272.
R. Wagner and M. Fischer. 1974. The string-to-
string correction problem. Journal of Association
for Computing Machinery, 21.
16
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 49?56,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Large-scale Non-taxonomic Relation Extraction: Estimating the
Precision of Rote Extractors?
Enrique Alfonseca?? Maria Ruiz-Casado??
?Precision and Intelligence Laboratory
Tokyo Institute of Techonology
enrique@lr.pi.titech.ac.jp
oku@pi.titech.ac.jp
Manabu Okumura? Pablo Castells?
?Computer Science Department
Universidad Autonoma de Madrid
enrique.alfonseca@uam.es
maria.ruiz@uam.es
pablo.castells@uam.es
Abstract
In this paper, we describe a rote extrac-
tor that learns patterns for finding seman-
tic relations in unrestricted text, with new
procedures for pattern generalisation and
scoring. An improved method for estimat-
ing the precision of the extracted patterns
is presented. We show that our method ap-
proximates the precision values as evalu-
ated by hand much better than the proce-
dure traditionally used in rote extractors.
1 Introduction
With the large growth of the information stored in
the web, it is necessary to have available automatic
or semi-automatic tools so as to be able to process
all this web content. Therefore, a large effort has
been invested in developing automatic or semi-
automatic techniques for locating and annotating
patterns and implicit information from the web,
a task known as Web Mining. In the particular
case of web content mining, the aim is automati-
cally mining data from textual web documents that
can be represented with machine-readable seman-
tic formalisms such as ontologies and semantic-
web languages.
Recently, there is an increasing interest in au-
tomatically extracting structured information from
large corpora and, in particular, from the Web
(Craven et al, 1999). Because of the character-
istics of the web, it is necessary to develop effi-
cient algorithms able to learn from unannotated
data (Riloff and Schmelzenbach, 1998; Soderland,
1999; Mann and Yarowsky, 2005). New types of
web content such as blogs and wikis, are also a
?This work has been sponsored by MEC, project number
TIN-2005-06885.
source of textual information that contain an un-
derlying structure from which specialist systems
can benefit.
Consequently, rote extractors (Brin, 1998;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002) have been identified as an appropri-
ate method to look for textual contexts that happen
to convey a certain relation between two concepts.
In this paper, we describe a new procedure for es-
timating the precision of the patterns learnt by a
rote extractor, and how it compares to previous ap-
proaches. The solution proposed opens new pos-
sibilities for improving the precision of the gener-
ated patterns, as described below.
This paper is structured as follows: Section 2
describe related work; Section 3 and 4 describe the
proposed procedure and its evaluation, and Sec-
tion 5 presents the conclusions and future work.
2 Related work
Extracting information using Machine Learning
algorithms has received much attention since
the nineties, mainly motivated by the Message
Understanding Conferences. From the mid-
nineties, there are systems that learn extraction
patterns from partially annotated and unannotated
data (Huffman, 1995; Riloff, 1996; Riloff and
Schmelzenbach, 1998; Soderland, 1999).
Generalising textual patterns (both manually
and automatically) for the identification of rela-
tions has been proposed since the early nineties
(Hearst, 1992), and it has been applied to extend-
ing ontologies with hyperonymy and holonymy re-
lations (Morin and Jacquemin, 1999; Kietz et al,
2000; Cimiano et al, 2004; Berland and Char-
niak, 1999). Finkelstein-Landau andMorin (1999)
learn patterns for company merging relations with
exceedingly good accuracies. Recently, kernel
49
methods are also becoming widely used for rela-
tion extraction (Bunescu and Mooney, 2005; Zhao
and Grishman, 2005).
Concerning rote extractors from the web, they
have the advantage that the training corpora can
be collected easily and automatically, so they
are useful in discovering many different relations
from text. Several similar approaches have been
proposed (Brin, 1998; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002), with vari-
ous applications: Question-Answering (Ravichan-
dran and Hovy, 2002), multi-document Named
Entity Coreference (Mann and Yarowsky, 2003),
and generating biographical information (Mann
and Yarowsky, 2005). Szpektor et al (2004) ap-
plies a similar, with no seed lists, to extract auto-
matically entailment relationships between verbs,
and Etzioni et al (2005) report very good results
extracting Named Entities and relationships from
the web.
2.1 Rote extractors
Rote extractors (Mann and Yarowsky, 2005) es-
timate the probability of a relation r(p, q) given
the surrounding context A1pA2qA3. This is cal-
culated, with a training corpus T , as the number
of times that two related elements r(x, y) from T
appear with that same contextA1xA2yA3, divided
by the total number of times that x appears in that
context together with any other word:
P (r(p, q)|A1pA2qA3) =
P
x,yr c(A1xA2yA3)
P
x,z c(A1xA2zA3)
(1)
x is called the hook, and y the target. In order to
train a Rote extractor from the web, this procedure
is mostly used (Ravichandran and Hovy, 2002):
1. Select a pair of related elements to be used
as seed. For instance, (Dickens,1812) for the
relation birth year.
2. Submit the query Dickens AND 1812 to a
search engine, and download a number of
documents to build the training corpus.
3. Keep all the sentences containing both ele-
ments.
4. Extract the set of contexts between them and
identify repeated patterns. This may just be
the m characters to the left or to the right
(Brin, 1998), the longest common substring
of several contexts (Agichtein and Gravano,
2000), or all substrings obtained with a suf-
fix tree constructor (Ravichandran and Hovy,
2002).
5. Download a separate corpus, called hook cor-
pus, containing just the hook (in the example,
Dickens).
6. Apply the previous patterns to the hook cor-
pus, calculate the precision of each pattern
in the following way: the number of times it
identifies a target related to the hook divided
by the total number of times the pattern ap-
pears.
7. Repeat the procedure for other examples of
the same relation.
To illustrate this process, let us suppose that we
want to learn patterns to identify birth years. We
may start with the pair (Dickens, 1812). From the
downloaded corpus, we extract sentences such as
Dickens was born in 1812
Dickens (1812 - 1870) was an English writer
Dickens (1812 - 1870) wrote Oliver Twist
The system identifies that the contexts of the last
two sentences are very similar and chooses their
longest common substring to produce the follow-
ing patterns:
<hook> was born in <target>
<hook> ( <target> - 1870 )
The rote extractor needs to estimate automati-
cally the precision of the extracted patterns, in or-
der to keep the best ones. So as to measure these
precision values, a hook corpus is now down-
loaded using the hook Dickens as the only query
word, and the system looks for appearances of the
patterns in this corpus. For every occurrence in
which the hook of the relation is Dickens, if the
target is 1812 it will be deemed correct, and oth-
erwise it will be deemed incorrect (e.g. in Dickens
was born in Portsmouth).
3 Our proposal
3.1 Motivation
In a rote extractor as described above, we believe
that the procedure for calculating the precision of
the patterns may be unreliable in some cases. For
example, the following patterns are reported by
Ravichandran and Hovy (2002) for identifying the
relations Inventor, Discoverer and Location:
Relation Prec. Pattern
Inventor 1.0 <target> ?s <hook> and
Inventor 1.0 that <target> ?s <hook>
Discoverer 0.91 of <target> ?s <hook>
Location 1.0 <target> ?s <hook>
In the particular application in which they are
used (relation extraction for Question Answering),
they are useful because there is initially a ques-
tion to be answered that indicates whether we are
50
looking for an invention, a discovery or a location.
However, if we want to apply them to unrestricted
relation extraction, we have the problem that the
same pattern, the genitive construction, represents
all these relations, apart from the most common
use indicating possession.
If patterns like these are so ambiguous, then
why do they receive so high a precision estimate?
One reason is that the patterns are only evalu-
ated for the same hook for which they were ex-
tracted. To illustrate this with an example, let
us suppose that we obtain a pattern for the rela-
tion located-at using the pairs (New York, Chrysler
Building). The genitive construction can be ex-
tracted from the context New York?s Chrysler
Building. Afterwards, when estimating the pre-
cision of this pattern, only sentences containing
<target>?s Chrysler Building are taken into ac-
count. Because of this, most of the pairs extracted
by this pattern may extract the target New York,
apart from a few that extract the name of the ar-
chitect that built it, van Allen. Thus we can expect
that the genitive pattern will receive a high preci-
sion estimate as a located-at pattern.
For our purposes, however, we want to collect
patterns for several relations such as writer-book,
painter-picture, director-film, actor-film, and we
want to make sure that the obtained patterns are
only applicable to the desired relation. Patterns
like <target> ?s <hook> are very likely to be ap-
plicable to all of these relations at the same time,
so we would like to be able to discard them auto-
matically by assigning them a low precision.
3.2 Suggested improvements
Therefore, we propose the following three im-
provements to this procedure:
1. Collecting not only a hook corpus but also a
target corpus should help in calculating the
precision. In the example of the Chrysler
building, we have seen that in most cases
that we look for the pattern ?s Chrysler build-
ing the previous words are New York, and
so the pattern is considered accurate. How-
ever, if we look for the pattern New York?s,
we shall surely find it followed by many dif-
ferent terms representing different relations,
and the precision estimate will decrease.
2. Testing the patterns obtained for one relation
using the hook and target corpora collected
for other relations. For instance, if the geni-
tive construction has been extracted as a pos-
sible pattern for the writer-book relation, and
we apply it to a corpus about painters, the rote
extractor can detect that it also extracts pairs
with painters and paintings, so that particular
pattern will not be very precise for that rela-
tion.
3. Many of the pairs extracted by the patterns
in the hook corpora were not evaluated at all
when the hook in the extracted pair was not
present in the seed lists. To overcome this,
we propose to use the web to check whether
the extracted pair might be correct, as shown
below.
3.3 Algorithm
In our implementation, the rote extractor starts
with a table containing some information about the
relations for which we want to learn patterns. This
procedure needs a little more information than just
the seed list, which is provided as a table in the
format displayed in Table 1. The data provided for
each relation is the following: (a) The name of the
relation, used for naming the output files contain-
ing the patterns; (b) the name of the file contain-
ing the seed list; (c) the cardinality of the relation.
For instance, given that many people can be born
on the same year, but for every person there is just
one birth year, the cardinality of the relation birth
year is n:1; (d) the restrictions on the hook and
the target. These can be of the following three cat-
egories: unrestricted, if the pattern can extract any
sequence of words as hook or target of the relation,
Entity, if the pattern can extract as hook or target
only things of the same entity type as the words
in the seed list (as annotated by the NERC mod-
ule), or PoS, if the pattern can extract as hook or
target any sequence of words whose sequence of
PoS labels was seen in the training corpus; and (e)
a sequence of queries that could be used to check,
using the web, whether an extracted pair is correct
or not.
We assume that the system has used the seed list
to extract and generalise a set of patterns for each
of the relations using training corpora (Ravichan-
dran and Hovy, 2002; Alfonseca et al, 2006a).
Our procedure for calculating the patterns? preci-
sions is as follows:
1. For every relation,
(a) For every hook, collect a hook corpus
from the web.
51
Relation name Seed-list Cardinality Hook-type Target-type Web queries
birth year birth-date.txt n:1 entity entity $1 was born in $2
death year death-date.txt n:1 entity entity $1 died in $2
birth place birth-place.txt n:1 entity entity $1 was born in $2
country-capital country-capital.txt 1:1 entity entity $2 is the capital of $1
author-book author-book.txt n:n entity unrestricted $1 is the author of $2
director-film director-film.txt 1:n entity unrestricted $1 directed $2, $2 directed by $1
Table 1: Example rows in the input table for the system.
(b) For every target, collect a target corpus
from the web.
2. For every relation r,
(a) For every pattern P , collected during
training, apply it to every hook and tar-
get corpora to extract a set of pairs.
For every pair p = (ph, pt),
? If it appears in the seed list of r, con-
sider it correct.
? If it appears in the seed list of other
relation, consider it incorrect.
? If the hook ph appears in the seed list
of r with a different target, and the
cardinality is 1:1 or n:1, consider it
incorrect.
? If the target pt appears in r?s seed list
with a different hook, and the cardi-
nality is 1:1 or 1:n, incorrect.
? Otherwise, the seed list does not
provide enough information to eval-
uate p, so we perform a test on the
web. For every query provided for r,
the system replaces $1 with ph and
$2 with pt, and sends the query to
Google. The pair is deemed correct
if and only if there is at least one an-
swer.
The precision of P is estimated as the
number of extracted pairs that are sup-
posedly correct divided by the total
number of pairs extracted.
In this step, every pattern that did not apply at
least twice in the hook and target corpora is also
discarded.
3.4 Example
After collecting and generalising patterns for
the relation director-film, we apply each pat-
tern to the hook and target corpora collected
for every relation. Let us suppose that we
want to estimate the precision of the pattern
<target> ?s <hook>
and we apply it to the hook and the target cor-
pora for this relation and for author-book. Pos-
sible pairs extracted are (Woody Allen, Bananas),
(Woody Allen, Without Fears), (Charles Dickens,
A Christmas Carol). Only the first one is correct.
The rote extractor proceeds as follows:
? The first pair appears in the seed list, so it is
considered correct.
? Although Woody Allen appears as hook in the
seed list andWithout Fears does not appear as
target, the second pair is still not considered
incorrect because the directed-by relation has
n:n cardinality.
? The third pair appears in the seed list for
writer-book, so it is directly marked as incor-
rect.
? Finally, because still the system has not made
a decision about the second pair, it queries
Google with the sequences
Woody Allen directed Without Fears
Without Fears directed by Woody Allen
Because neither of those queries provide any
answer, it is considered incorrect.
In this way, it can be expected that the patterns
that are equally applicable to several relations,
such as writer-book, director-film or painter-
picture will attain a low precision because they
will extract many incorrect relations from the cor-
pora corresponding to the other relations.
4 Experiment and results
4.1 Rote extractor settings
The initial steps of the rote extractor follows the
general approach: downloading a training cor-
pus using the seed list and extracting patterns.
The training corpora are processed with a part-
of-speech tagger and a module for Named Entity
Recognition and Classification (NERC) that anno-
tates people, organisations, locations, dates, rela-
tive temporal expressions and numbers (Alfonseca
et al, 2006b), so this information can be included
in the patterns. Furthermore, for each of the terms
in a pair in the training corpora, the system also
52
Birth year:
BOS/BOS <hook> (/( <target> -/- number/entity )/) EOS/EOS
BOS/BOS <hook> (/( <target> -/- number/entity )/) British/JJ writer/NN
BOS/BOS <hook> was/VBD born/VBN on/IN the/DT first/JJ of/IN time expr/entity ,/, <target> ,/, at/IN location/entity ,/, of/IN
BOS/BOS <hook> (/( <target> -/- )/) a/DT web/NN guide/NN
Birth place:
BOS/BOS <hook> was/VBD born/VBN in/IN <target> ,/, in/IN central/JJ location/entity ,/,
BOS/BOS <hook> was/VBD born/VBN in/IN <target> date/entity and/CC moved/VBD to/TO location/entity
BOS/BOS Artist/NN :/, <hook> -/- <target> ,/, location/entity (/( number/entity -/-
BOS/BOS <hook> ,/, born/VBN in/IN <target> on/IN date/entity ,/, worked/VBN as/IN
Author-book:
BOS/BOS <hook> author/NN of/IN <target> EOS/EOS
BOS/BOS Odysseus/NNP :/, Based/VBN on/IN <target> ,/, <hook> ?s/POS epic/NN from/IN Greek/JJ mythology/NN
BOS/BOS Background/NN on/IN <target> by/IN <hook> EOS/EOS
did/VBD the/DT circumstances/NNS in/IN which/WDT <hook> wrote/VBD "/?? <target> "/?? in/IN number/entity ,/, and/CC
Capital-country:
BOS/BOS <hook> is/VBZ the/DT capital/NN of/IN <target> location/entity ,/, location/entity correct/JJ time/NN
BOS/BOS The/DT harbor/NN in/IN <hook> ,/, the/DT capital/NN of/IN <target> ,/, is/VBZ number/entity of/IN location/entity
BOS/BOS <hook> ,/, <target> EOS/EOS
BOS/BOS <hook> ,/, <target> -/- organization/entity EOS/EOS
Figure 1: Example patterns extracted from the training corpus for each several kinds of relations.
stores in a separate file the way in which they are
annotated in the training corpus: the sequences of
part-of-speech tags of every appearance, and the
entity type (if marked as such). So, for instance,
typical PoS sequences for names of authors are
?NNP?1 (surname) and ?NNP NNP? (first name
and surname). A typical entity kind for an author
is person.
In the case that a pair from the seed list is found
in a sentence, a context around the two words in
the pair is extracted, including (a) at most five
words to the left of the first word; (b) all the
words in between the pair words; (c) at most five
words to the right of the second word. The context
never jumps over sentence boundaries, which are
marked with the symbols BOS (Beginning of sen-
tence) and EOS (End of sentence). The two related
concepts are marked as <hook> and <target>.
Figure 1 shows several example contexts extracted
for the relations birth year, birth place, writer-
book and country-capital city.
The approach followed for the generalisation
is the one described by (Alfonseca et al, 2006a;
Ruiz-Casado et al, in press), which has a few
modifications with respect to Ravichandran and
Hovy (2002)?s, such as the use of the wildcard * to
represent any sequence of words, and the addition
of part-of-speech and Named Entity labels to the
patterns.
The input table has been built with the fol-
lowing nineteen relations: birth year, death year,
birth place, death place, author?book, actor?
film, director?film, painter?painting, Employee?
organisation, chief of state, soccer player?team,
1All the PoS examples in this paper are done with Penn
Treebank labels.
Relation Seeds Extr. Gener. Filt.
Birth year 244 2374 4748 30
Death year 216 2178 4356 14
Birth place 169 764 1528 28
Death place 76 295 590 6
Author-book 198 8297 16594 283
Actor-film 49 739 1478 3
Director-film 85 6933 13866 200
Painter-painting 92 597 1194 15
Employee-organisation 62 1667 3334 6
Chief of state 55 1989 3978 8
Soccer player-team 194 4259 8518 39
Soccer team-city 185 180 360 0
Soccer team-manager 43 994 1988 9
Country/region-capital city 222 4533 9066 107
Country/region-area 226 762 1524 2
Country/region-population 288 318 636 3
Country-bordering country 157 6828 13656 240
Country-inhabitant 228 2711 5422 17
Country-continent 197 1606 3212 21
Table 2: Number of seed pairs for each relation,
and number of unique patterns in each step.
soccer team-city, soccer team-manager, country
or region?capital city, country or region?area,
country or region?population, country?bordering
country, country-name of inhabitant (e.g. Spain-
Spaniard), and country-continent. The time re-
quired to build the table and the seed lists was less
than one person-day, as some of the seed lists were
directly collected from web pages.
For each step, the following settings have been
set:
? The size of the training corpus has been set
to 50 documents for each pair in the original
seed lists. Given that the typical sizes of the
lists collected are between 50 and 300 pairs,
this means that several thousand documents
are downloaded for each relation.
? Before the generalisation step, the rote ex-
tractor discards those patterns in which the
hook and the target are too far away to each
other, because they are usually difficult to
generalise. The maximum allowed distance
53
No. Pattern Applied Prec1 Prec2 Real
1
Biography|Hymns|Infography|Life|Love|POETRY|Poetry|Quotations|
Search|Sketch|Woolf|charts|genius|kindness|poets/NN */*
OF|Of|about|by|for|from|like|of/IN <hook> (/( <target> -/-
6 1.00 1.00 1.00
2 "/?? <hook> (/( <target> -/- 4 1.00 1.00 1.00
3
[BOS]/[BOS] <hook> was/VBD born/VBN about|around|in/IN <target>
B.C.|B.C.E|BC/NNP at|in/IN
3 1.00 1.00 1.00
4
[BOS]/[BOS] <hook> was/VBD born/VBN about|around|in/IN <target>
B.C.|B.C.E|BC/NNP at|in/IN location/entity
3 1.00 1.00 1.00
5
[BOS]/[BOS] <hook> was/VBD born/VBN around/IN <target> B.C.E/NNP at/IN
location/entity ,/, a/DT
3 1.00 1.00 1.00
6
[BOS]/[BOS] <hook> was/VBD born/VBN around|in/IN <target> B.C.|B.C.E/NNP
at|in/IN location/entity ,/,
3 1.00 1.00 1.00
7
[BOS]/[BOS] */* ATTRIBUTION|Artist|Author|Authors|Composer|Details|
Email|Extractions|Myth|PAL|Person|Quotes|Title|Topic/NNP :/, <hook> (/(
<target> -/-
3 1.00 1.00 1.00
8
classical/JJ playwrights/NNS of/IN organisation/entity ,/, <hook> was/VBD
born/VBN near/IN location/entity in/IN <target> BCE/NNP ,/, in/IN the/DT
village/NN
3 1.00 1.00 1.00
9 [BOS]/[BOS] <hook> (/( <target> -/- )/) 2 1.00 1.00 1.00
10 [BOS]/[BOS] <hook> (/( <target> -|--/- )/) 2 1.00 1.00 1.00
11 [BOS]/[BOS] <hook> (/( <target> person/entity BC/NNP ;/, Greek/NNP :/, 2 1.00 1.00 1.00
12
ACCESS|AND|Alice|Author|Authors|BY|Biography|CARL|Dame|Don|ELIZABETH|
(...)|web|writer|writerMuriel|years/NNP <hook> (/( <target> -|- -/-
8 0.75 1.00
13 -/- <hook> (/( <target> -/- 3 0.67 1.00 0.67
14 -|--/- <hook> (/( <target> -/- 3 0.67 1.00 0.67
15 [BOS]/[BOS] <hook> (/( <target> -/- 60 0.62 1.00 0.81
16 [BOS]/[BOS] <hook> (/( <target> -/- */* )/) 60 0.62 1.00 0.81
17 [BOS]/[BOS] <hook> (/( <target> -|--/- 60 0.62 1.00 0.81
18 ,|:/, <hook> (/( <target> -/- 32 0.41 0.67 0.28
19 [BOS]/[BOS] <hook> ,/, */* (/( <target> -|--/- 15 0.40 1.00 0.67
20 ,|:|;/, <hook> (/( <target> -|--/- 34 0.38 0.67 0.29
21
AND|Alice|Authors|Biography|Dame|Don|ELIZABETH|Email|Fiction|Frances|
GEORGE|Home|I.|Introduction|Jean|L|Neben|PAL|PAULA|Percy|Playwrights|
Poets|Sir|Stanisaw|Stanislaw|W.|WILLIAM|feedback|history|writer/NNP <hook>
(/( <target> -/-
3 0.33 n/a 0.67
22 AND|Frances|Percy|Sir/NNP <hook> (/( <target> -/- 3 0.33 n/a 0.67
23
Alice|Authors|Biography|Dame|Don|ELIZABETH|Email|Fiction|Frances|
GEORGE|Home|I.|Introduction|Jean|L|Neben|PAL|PAULA|Percy|Playwrights|
Poets|Sir|Stanisaw|Stanislaw|W.|WILLIAM|feedback|history|writer/NN <hook>
(/( <target> -/-
3 0.33 n/a 0.67
24 [BOS]/[BOS] <hook> ,|:/, */* ,|:/, <target> -/- 7 0.28 0.67 0.43
25 [BOS]/[BOS] <hook> ,|:/, <target> -/- 36 0.19 1.00 0.11
26 [BOS]/[BOS] <hook> ,/, */* (/( <target> )/) 20 0.15 0.33 0.10
27 [BOS]/[BOS] <target> <hook> ,/, 18 0.00 n/a 0.00
28 In|On|on/IN <target> ,/, <hook> grew|was/VBD 17 0.00 0.00 0.00
29 In|On|on/IN <target> ,/, <hook> grew|was|went/VBD 17 0.00 0.00 0.00
30
[BOS]/[BOS] <hook> ,/, */* DE|SARAH|VON|dramatist|novelist|
playwright|poet/NNP (/( <target> -/-
3 0.00 n/a 1.0
TOTAL 436 0.46 0.84 0.54
Table 3: Patterns for the relation birth year, results extracted by each, precision estimated with this
procedure and with the traditional hook corpus approach, and precision evaluated by hand).
between them has been set to 8 words.
? At each step, the two most similar patterns
are generalised, and their generalisation is
added to the set of patterns. No pattern is dis-
carded at this step. This process stops when
all the patterns resulting from the generalisa-
tion of existing ones contain wildcards adja-
cent to either the hook or the target.
? For the precision estimation, for each pair in
the seed lists, 50 documents are collected for
the hook and other 50 for the target. Because
of time constraints, and given that the total
size of the hook and the target corpora ex-
ceeds 100,000 documents, for each pattern a
sample of 250 documents is randomly cho-
sen and the patterns are applied to it. This
sample is built randomly but with the fol-
lowing constraints: there should be an equal
amount of documents selected from the cor-
pora from each relationship; and there should
be an equal amount of documents from hook
corpora and from target corpora.
4.2 Output obtained
Table 2 shows the number of patterns obtained for
each relation. Note that the generalisation proce-
dure applied produces new (generalised) patterns
to the set of original patterns, but no original pat-
tern is removed, so they all are evaluated; this is
why the set of patterns increases after the gener-
alisation. The filtering criterion was to keep the
patterns that applied at least twice on the test cor-
pus.
It is interesting to see that for most relations the
reduction of the pruning is very drastic. This is
because of two reasons: Firstly, most patterns are
far too specific, as they include up to 5 words at
each side of the hook and the target, and all the
words in between. Only those patterns that have
generalised very much, substituting large portions
with wildcards or disjunctions are likely to apply
to the sentences in the hook and target corpora.
54
Secondly, the samples of the hook and target cor-
pora used are too small for some of the relations
to apply, so few patterns apply more than twice.
Note that, for some relations, the output of the
generalisation step contains less patterns that the
output of the initial extraction step: that is due to
the fact that the patterns in which the hook and
the target are not nearby were removed in between
these two steps.
Concerning the precision estimates, a full eval-
uation is provided for the birth-year relation. Ta-
ble 3 shows in detail the thirty patterns obtained.
It can also be seen that some of the patterns with
good precision contain the wildcard *. For in-
stance, the first pattern indicates that the presence
of any of the words biography, poetry, etc. any-
where in a sentence before a person name and a
date or number between parenthesis is a strong in-
dication that the target is a birth year.
The last columns in the table indicate the num-
ber of times that each rule applied in the hook and
target corpora, and the precision of the rule in each
of the following cases:
? As estimated by the complete program
(Prec1).
? As estimated by the traditional hook cor-
pus approach (Prec2). Here, cardinality is
not taken into account, patterns are evaluated
only on the hook corpora from the same rela-
tion, and those pairs whose hook is not in the
seed list are ignored.
? The real precision of the rule (real). In or-
der to obtain this metric, two different an-
notators evaluated the pairs applied indepen-
dently, and the precision was estimated from
the pairs in which they agreed (there was a
96.29% agreement, Kappa=0.926).
As can be seen, in most of the cases our procedure
produces lower precision estimates.
If we calculate the total precision of all the rules
altogether, shown in the last row of the table, we
can see that, without the modifications, the whole
set of rules would be considered to have a total
precision of 0.84, while that estimate decreases
sharply to 0.46 when they are used. This value
is nearer the precision of 0.54 evaluated by hand.
Although it may seem surprising that the precision
estimated by the new procedure is even lower than
the real precision of the patterns, as measured by
hand, that is due to the fact that the web queries
consider unknown pairs as incorrect unless they
Relation Prec1 Prec2 Real
Birth year 0.46 [0.41,0.51] 0.84 [0.81,0.87] 0.54 [0.49,0.59]
Death year 0.29 [0.24,0.34] 0.55 [0.41,0.69] 0.38 [0.31,0.44]
Birth place 0.65 [0.62,0.69] 0.36 [0.29,0.43] 0.84 [0.79,0.89]
Death place 0.82 [0.73,0.91] 1.00 [1.00,1.00] 0.96 [0.93,0.99]
Author-book 0.07 [0.07,0.07] 0.26 [0.19,0.33] 0.03 [0.00,0.05]
Actor-film 0.07 [0.01,0.13] 1.00 [1.00,1.00] 0.02 [0.00,0.03]
Director-film 0.03 [0.03,0.03] 0.26 [0.18,0.34] 0.01 [0.00,0.01]
Painter-painting 0.10 [0.07,0.12] 0.35 [0.23,0.47] 0.17 [0.12,0.22]
Employee-organisation 0.31 [0.22,0.40] 1.00 [1.00,1.00] 0.33 [0.26,0.40]
Chief of state 0.00 [0.00,0.00] - 0.00 [0.00,0.00]
Soccer player-team 0.07 [0.06,0.08] 1.00 [1.00,1.00] 0.08 [0.04,0.12]
Soccer team-city - - -
Soccer team-manager 0.61 [0.53,0.69] 1.00 [1.00,1.00] 0.83 [0.77,0.88]
Country/region-capital city 0.12 [0.11,0.13] 0.23 [0.22,0.24] 0.12 [0.07,0.16]
Country/region-area 0.09 [0.00,0.19] 1.00 [1.00,1.00] 0.06 [0.02,0.09]
Country/region-population 1.00 [1.00,1.00] 1.00 [1.00,1.00] 1.00 [1.00,1.00]
Country-bordering country 0.17 [0.17,0.17] 1.00 [1.00,1.00] 0.15 [0.10,0.20]
Country-inhabitant 0.01 [0.00,0.01] 0.80 [0.67,0.93] 0.01 [0.00,0.01]
Country-continent 0.16 [0.14,0.18] 0.07 [0.04,0.10] 0.00 [0.00,0.01]
Table 4: Precision estimates for the whole set of
extracted pairs by all rules and all relations.
appear in the web exactly in the format of the
query in the input table. Specially for not very
well-known people, we cannot expect that all of
them will appear in the web following the pattern
?X was born in date?, so the web estimates tend
to be over-conservative.
Table 4 shows the precision estimates for every
pair extracted with all the rules using both proce-
dures, with 0.95 confidence intervals. The real
precision has been estimating by sampling ran-
domly 200 pairs and evaluating them by hand, as
explained above for the birth year relation. As can
be observed, out of the 19 relations, the precision
estimate of the whole set of rules for 11 of them
is not statistically dissimilar to the real precision,
while that only holds for two relationships using
the previous approach.
Please note as well that the precisions indicated
in the table refer to all the pairs extracted by all the
rules, some of which are very precise, but some of
which are very imprecise. If the rules are to be
applied in an annotation system, only those with
a high precision estimate would be used, and ex-
pectedly much better overall results would be ob-
tained.
5 Conclusions and future work
We have described here a new procedure for es-
timating the precision of the patterns learnt by a
rote extractor that learns from the web. Compared
to other similar approaches, it has the following
improvements:
? For each pair (hook,target) in the seed list, a
target corpora is also collected (apart from
the hook corpora), and the evaluation is per-
formed using corpora from several relations.
55
This has been observed to improve the esti-
mate of the rule?s precision, given that the
evaluation pairs not only refer to the elements
in the seed list.
? The cardinality of the relations is taken into
consideration in the estimation process using
the seed list. This is important, for instance,
to be able to estimate the precision in n:n re-
lations like author-work, given that we can-
not assume that the only books written by
someone are those in the seed list.
? For those pairs that cannot be evaluated using
the seed list, a simple query to the Google
search engine is employed.
The precisions estimated with this procedure
are significantly lower than the precisions obtained
with the usual hook corpus approach, specially for
ambiguous patterns, and much near the precision
estimate when evaluated by hand.
Concerning future work, we plan to estimate the
precision of the patterns using the whole hook and
target corpora, rather than using a random sample.
A second objective we have in mind is not to throw
away the ambiguous patterns with low precision
(e.g. the possessive construction), but to train a
model so that we can disambiguate which is the
relation they are conveying in each context (Girju
et al, 2003).
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In Proceedings of ICDL, pages 85?94.
E. Alfonseca, P. Castells, M. Okumura, and M. Ruiz-
Casado. 2006a. A rote extractor with edit distance-
based generalisation and multi-corpora precision
calculation. In Poster session of ACL-2006.
E. Alfonseca, A. Moreno-Sandoval, J. M. Guirao, and
M. Ruiz-Casado. 2006b. The wraetlic NLP suite.
In Proceedings of LREC-2006.
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of ACL-99.
S. Brin. 1998. Extracting patterns and relations from
the World Wide Web. In Proceedings of the WebDB
Workshop at EDBT?98.
R. Bunescu and R. J. Mooney. 2005. A shortest path
dependency kernel for relation extraction. In Pro-
ceedings of the HLT Conference and EMNLP.
P. Cimiano, S. Handschuh, and S. Staab. 2004. To-
wards the self-annotating web. In Proceedings of the
13th World Wide Web Conference, pages 462?471.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1999. Learn-
ing to construct knowledge bases from the world
wide web. Artificial Intelligence, 118(1?2):69?113.
O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named entity extraction from
the web: An experimental study. Artificial Intelli-
gence, 165(1):91?134.
M. Finkelstein-Landau and E. Morin. 1999. Extracting
semantic relationships between terms: supervised
vs. unsupervised methods. In Workshop on Ontolo-
gial Engineering on the Global Info. Infrastructure.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning semantic constraints for the automatic dis-
covery of part-whole relations. In HLT-NAACL-03.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING-92.
S. Huffman. 1995. Learning information extraction
patterns from examples. In IJCAI-95 Workshop on
New Approaches to Learning for NLP.
J. Kietz, A. Maedche, and R. Volz. 2000. A method
for semi-automatic ontology acquisition from a cor-
porate intranet. In Workshop ?Ontologies and text?.
G. S. Mann and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In CoNLL-2003.
G. S. Mann and D. Yarowsky. 2005. Multi-field in-
formation extraction and cross-document fusion. In
Proceedings of ACL 2005.
E. Morin and C. Jacquemin. 1999. Projecting corpus-
based semantic links on a thesaurus. In ACL-99.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In
Proceedings of ACL-2002, pages 41?47.
E. Riloff and M. Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In
Proceedings of WVLC, pages 49?56.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In AAAI.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. in
press. Automatising the learning of lexical patterns:
an application to the enrichment of WordNet by ex-
tracting semantic relationships from the Wikipedia.
Data and Knowledge Engineering, in press.
S. Soderland. 1999. Learning information extraction
rules for semi-structured and free text. Machine
Learning, 34(1?3):233?272.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment rela-
tions. In Proceedings of EMNLP 2004.
S. Zhao and R. Grishman. 2005. Extracting relations
with integrated information using kernel methods.
In Proceedings of ACL-2005.
56
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1046?1055,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Gazpacho and summer rash:
lexical relationships from temporal patterns of web search queries
Enrique Alfonseca Massimiliano Ciaramita Keith Hall
Google
Z?urich, Switzerland
ealfonseca@google.com, massi@google.com, kbhall@google.com
Abstract
In this paper we investigate temporal pat-
terns of web search queries. We carry out
several evaluations to analyze the proper-
ties of temporal profiles of queries, reveal-
ing promising semantic and pragmatic re-
lationships between words. We focus on
two applications: query suggestion and
query categorization. The former shows
a potential for time-series similarity mea-
sures to identify specific semantic relat-
edness between words, which results in
state-of-the-art performance in query sug-
gestion while providing complementary
information to more traditional distribu-
tional similarity measures. The query cat-
egorization evaluation suggests that the
temporal profile alone is not a strong in-
dicator of broad topical categories.
1 Introduction
The temporal patterns of word occurrences in hu-
man communication carry an implicit measure of
their relationship to real-world events and behav-
ioral patterns. For example, when there is an event
affecting a given entity (such as a natural disaster
in a country), the entity name will turn up more
frequently in human conversation, newswire arti-
cles and web documents; and people will search
for it more often. Two entities that are closely
related in the real world, such as the name of a
country and a prominent region inside the coun-
try are likely to share common events and there-
fore be closely associated in human communica-
tion. Finally, two instances of the same class
are also likely to share common usage patterns.
For example, names of airlines or retail stores are
more likely to be used by day rather than by night
(Chien, 2005).
In this paper we explore the linguistic relation-
ship between phrases that are judged to be sim-
ilar based on their frequency time series correla-
tion in search query logs. For every phrase
1
avail-
able in WordNet 3.0
2
(Miller, 1995), we have ob-
tained its temporal signature from query logs, and
calculated all their pairwise correlations. Next,
we study the relationship in the top-ranked pairs
with respect to their distribution in WordNet and a
human-annotated labelling.
We also discuss possible applications of this
data to solve open problems and present the results
of two experiments: one where time series corre-
lations turned out to be highly discriminative; and
another where they were not particularly informa-
tive but shed some light on the nature of temporal
semantics and topical categorization:
? Query suggestion, i.e. given a query, generate
a ranked list of alternative queries in which
the user may be interested.
? Query categorization, i.e. given a predefined
set of categories, find the top categories to
which the query can be assigned.
Finally, we illustrate with an example another ap-
plication of time series in solving information ex-
traction problems.
Although query logs are typically proprietary
data, there are ongoing initiatives, like the Lemur
toolbar
3
, which make this kind of information
available for research purposes. Other work
(Bansal and Koudas, 2007b; Bansal and Koudas,
2007a) shows that temporal information can also
be extracted from public data, such as blogs. More
traditional types of text, such as news, are also typ-
ically associated with temporal labels; e.g., dates
and timestamps.
This paper is structured in the following way:
1
We use the term phrase to refer to any single word or
multi-word expression that belongs to a synset in WordNet.
Examples of phrases are person, causal entity or william
shakespeare. We focused on the nouns hierarchy only.
2
http://wordnet.princeton.edu
3
http://www.lemurproject.org/
querylogtoolbar/
1046
Section 2 summarizes the related work. Section 3
describes the correlation analysis between all pairs
of phrases from WordNet. Next, Section 4 de-
scribes the application to query suggestion, and
Section 5 the application to labelling queries in
topical categories. Section 7 summarizes the con-
clusions and outlines ideas for future research.
2 Related work
The study of query time series explores a particu-
lar instance of the so-called wisdom of the crowds
effect. Within this area, we can distinguish two
kinds of phenomena. Knowledge and resources
assembled by people explicitly, either individu-
ally, such as the case of blogs, or in a collabora-
tive way, as in forums or wikis. These resources
are valuable for human-consumption and can also
be exploited in order to learn computational re-
sources (Medelyan et al, 2008; Weld et al, 2008;
Zesch et al, 2008b; Zesch et al, 2008a). On
the other hand, it is possible to acquire useful re-
sources and knowledge from aggregating behav-
ioral patterns of large groups of people, even in
the absence of a conscious effort. There is exten-
sive ongoing research on the use of web search
usage patterns to develop knowledge resources.
Some examples are clustering co-click patterns to
learn semantically related queries (Beeferman and
Berger, 2000), combining co-click patterns with
hitting times (Mei et al, 2008), analyzing query
revisions made by users when querying search en-
gines (Jones et al, 2006), replacing query words
with other words that have the highest pointwise
mutual information (Terra and Clarke, 2004), or
using the temporal distribution of words in docu-
ments to improve ranking of search results (Jones
and Diaz, 2007).
Within this second category, an important area
is dedicated to the study of time-related features
of search queries. News aggregators use real-time
frequencies of user queries to detect spikes and
identify news shortly after the spikes occur (Mu-
rata, 2008). Web users? query patterns have also
proved useful for building a real-time surveillance
system that accurately estimates region-by-region
influenza activity with a lag of one day (Ginsberg
et al, 2009). Search engines specifically devel-
oped for real-time searches, like Twitter search,
will most likely provide new use cases and sce-
narios for quickly detecting trends in user search
query patterns.
Figure 1: Time series obtained for the queries
[gazpacho] and [summertime] (normalized
scales).
Our study builds upon the work of Chien
(2005), who observed that queries with highly-
correlated temporal usage patterns are typically
semantically related, and described a procedure
for calculating the correlations efficiently. We
have extended the analysis described in this work,
by performing a more extensive evaluation of the
kinds of semantic relationships that we can find
among temporally-similar queries. We also pro-
pose, to our knowledge for the first time, areas
of applications in solving well-established prob-
lems which shed some light on the nature of time-
based semantic similarity. This work is also re-
lated to the analysis of temporal properties of
information streams in data mining (Kleinberg,
2006) and information retrieval from time series
databases (Agrawal et al, 1993).
3 Time-based similarities between
phrases
Similarly to the method described in Chien (2005),
we take a time interval, divide it into equally
spaced subintervals, and represent each phrase of
interest as the sequence of frequencies with which
the phrase was observed in the subintervals. In
our experiments, we have used as source data
the set of fully anonymized query logs from the
Google search engine between January 1st, 2004
and March 1st, 2009.
4
.
These data have been aggregated on a daily ba-
sis so that we have the daily frequency of the
4
Part of this data is publicly available from http://
www.google.com/trends
1047
queries of interest for over five years. The frequen-
cies are then normalized with the total number of
queries that happened on that day. The normaliza-
tion is necessary to avoid daily and seasonal varia-
tions as there are typically more queries on week-
days than on weekends and fewer queries during
holiday seasons than in the rest of the year. It
also helps reducing the effect deriving from the
fact that the population with Internet access is still
monotonically growing, so we can expect that the
number of queries will become higher and higher
over time.
Given two phrases and their associated time se-
ries, the similarity metric used is the correlation
coefficient between the two series (Chien, 2005).
For illustration, Figure 1 shows the time series ob-
tained for two sample queries, gazpacho and sum-
mertime, whose time series yield a correlation of
0.92. Similar high correlations can be observed
with other queries related to phenomena that oc-
cur mainly in summer in the countries from which
most queries come, like summer rash.
3.1 WordNet-based evaluation
In this section, we describe a study carried out
with the purpose of discovering the traditional
lexico-semantic relationships which hold between
the queries that are most strongly related accord-
ing to their temporal profiles.
For this evaluation, we have taken the nomi-
nal phrases appearing in WordNet 3.0. Given that
users, when writing queries, typically do not pay
attention to punctuation and case, we have normal-
ized all phrases by lowercasing them and remov-
ing all punctuation. Next, we collected the time se-
ries for each phrase by computing the normalized
daily frequency of each of them as exact queries
in the query logs. The computation of the pair-
wise correlations was performed in parallel using
the MapReduce infrastructure running over 2048
cores with 500 MB of RAM each. The total ex-
ecution (including data shuffling and networking
time) took approximately three hours.
Next, we represented the data as a complete
graph where phrases are nodes and the edge be-
tween each pair of nodes is weighted by their time
series correlation. Using a simple graph-cut we
obtained clusters of related terms. A minimum
weight threshold equal to 0.9 was applied;
5
thus,
5
This threshold is the same used by Chien (2005), and was
confirmed after a manual inspection of a sample of the data
two phrases belong to the same cluster if there is
a path between them only via edges with weight
over 0.9.
The previous procedure produced a set of 604
clusters, with highly different sizes. The first ob-
servation is that 70% of the phrases in WordNet
do not have a correlation over 0.9 with any other
phrase, so they are placed alone in singleton clus-
ters. There are several reasons for this. The clus-
ters obtained are very specific: only phrases that
have a very strong temporal association have tem-
poral correlations exceeding the threshold. This is
combined with the fact that we are using a very
restricted vocabulary, namely the terms included
in WordNet, which is many orders of magnitude
smaller than the vocabulary of all possible queries
from the users. Few phrase pairs in WordNet
have a temporal association and popularity strong
enough to be clustered together. Finally, many of
the phrases in WordNet are rare, including scien-
tific names of animals and plants, genuses or fami-
lies, which are not commonly used. Therefore, the
clusters extracted here correspond to very salient
sets of phrases. If, instead of WordNet, we choose
a vocabulary from known user queries (cf. Sec-
tion 4), there would be many fewer singleton clus-
ters, as the options of similar phrases to choose
from would be much larger.
From the phrases that belong to clusters, 25%
of the WordNet phrases do not have strong daily
temporal profiles. The typical pattern for these
terms is an almost flat time series, usually with
small drops at summertime and Christmas (when
seasonal leisure-related queries dominate). There-
fore, these phrases were collected in just one clus-
ter containing them all. Typical examples of the
elements of this set are names of famous scientists
and mathematicians (Gauss, Isaac Newton, Al-
bert Einstein, Thomas Alva Edison, Hipprocrates,
Gregor Mendel, ...), common terms (fertilization,
famine, macroeconomics, genus, nationalism, ...),
numbers and common first names, among other
things. It is possible that using sub-day intervals
might help to discriminate within this cluster.
The items in this big cluster contrast with pe-
riodical events, which display recurring patterns
(e.g., queries related to elections or tax-returns),
and names of famous people and other entities
which appeared in the news in the past few years.
All of these are associated with irregular, spiky
time series. These constitute the final 5% of the
1048
Type Pairs Examples
Synonyms 283 (angel cake, angel food cake), (thames, river thames), (armistice day, Nov 11)
Hyponym/hyperonyms 86 (howard hughes, aviator), (muhammad, prophet), (olga korbut, gymnast)
Siblings in hyponym taxonomy 611 (hiroshima, nagasaki), (junior school, primary school), (aids, welt)
Meronym/holonyms 53 (tutsi, rwanda), (july 4, july), (pyongyang, north korea)
Siblings in meronymy taxonomy 7 (everglades, everglades national park), (mississipi, orleans)
Other paths 471 (maundy thursday, maundy money), (tap water, water tap), (gren party, liberal)
Not structurally related 1009 (poppy, veterans day), (olympic games, gimnast), (belmont park, horse racing)
Table 1: Relationships between pairs of WordNet phrases belonging to the same cluster.
phrases belonging to small, highly focused, clus-
ters.
Table 1 shows the relationships that hold be-
tween all pairs of phrases belonging to any of the
smaller clusters. Out of 2520 pairs, 283 belong
to the same synset, 697 are related via hyponymy
links, 60 via meronymy links, and 471 by alternat-
ing hyponymy and meronymy links in the path.
When the phrases were polysemous, the short-
est path between any of their meaning was used.
About 40% of the relations do not have a clear
structural interpretation in WordNet.
The majority of pairs are related via more or
less complex paths in the WordNet graph. Inter-
estingly, even the structurally unrelated terms are
characterized by transparent relations in terms of
world knowledge, as it is the case between poppy
and veteran day. Note as well that sometimes a
WordNet term is used with a meaning not present
in WordNet or in a different language, which may
explain why aids has a very high correlation with
welt (AIDS and welt are both hyponyms of health
problem, but the correlation may be explained bet-
ter by the AIDS World Day, Welt Aids Tag in Ger-
man), and it also has a very high correlation with
sida, defined in WordNet as a genus of tropical
herbs, but which is in fact the translation of AIDS
into Spanish. These observations motivated an ad-
ditional manual labelling of the extracted pairs.
3.2 Hand labelled evaluation
As can be seen in Table 2, most of the terms that
constitute a cluster are related to each other, al-
though the kinds of semantic relationships that
hold between them can vary significantly. Exam-
ples of the following kinds can be observed:
? True synonyms, as in the case of november
and nov, or architeuthis and giant squid.
? Variations of people names, especially if a
person?s first name or surname is typically
used to refer to that person, as in the case of
john lennon and lennon, or janis joplin and
joplin. Sometimes the variations include per-
sonal titles, as it is the case of president carter
and president nixon, which are highly corre-
lated with jimmy carter and richard nixon.
? Geographically-related terms, referring to
locations which are located close to each
other, as in the clusters {korea, north ko-
rean, south korea, pyongyang, north korea}
and {strasbourg, grenoble, toulouse, poitiers,
lyon, lille, nantes, reims}.
? Synonyms of location names, like bahrain
and bahrein.
? Derived words, like north korea and north
korean, or lebanese and lebanon.
? Generic word optionalizations, which hap-
pen when one word in a multi-word phrase
is very correlated to the phrase, as in the
case of spanish inquisition and inquisition,
or red bone marrow and red marrow, where
the most common interpretation for the short-
ened version of the phrase is the same as for
the long version.
? Word reordering, where the two related
phrases have the same words in a different or-
der, as in the case of maple sugar and sugar
maple, or oil palm and palm oil.
? Morphological variants: WordNet does not
contain many morphological variants in the
main dataset, but there are a few, like station
of the cross and stations of the cross.
? Acronyms, like federal emergency manage-
ment agency and fema.
? Hyperonym-hyponym, like fern and plant.
? Sibling terms in a taxonomy, as in the clus-
ter {lutheran, methodist, presbyterian, united
methodist church, lutheran church,methodist
church, presbyterian church,baptist, baptist
church}, which contains mostly names of
Christian denominations.
? Co-occurring events in time, as is the case
of hitch and pacifier, both titles of movies
which were launched at almost the same
1049
hydrant,fire hydrant
inauguration day,inauguration,swearing,investiture,inaugural address,inaugural,benediction,oath
indulgence,self indulgence
insulation,heating
interstate highway,interstate, intestine,small intestine
iq,iq test
irish people,irish,irish potato,irish gaelic,gaelic,irish soda bread,irish stew,st patrick,saint patrick,leprechaun,
march 17,irish whiskey,shillelagh
ironsides,old ironsides
james,joyce,james joyce
janis joplin,joplin
jesus christ,pilate,pontius pilate,passion of christ,passion,aramaic
jewish new year,rosh hashana,rosh hashanah,shofar
john lennon,lennon
julep,mint julep,kentucky derby,kentucky
keynote,keynote address
kickoff,time off
korea,north korean,south korea,pyongyang,north korea
l ron hubbard,scientology
leap,leap year,leap day,february 29
left brain,right brain
leftover,leftovers,turkey stew
linseed oil,linseed
listeria,listeriosis,maple leaf
lobster tail,lobster,tails
lohan,lindsay
loire,rhone,rhone alpes
looking,looking for
lutheran,methodist,presbyterian,united methodist church,lutheran church,methodist church,presbyterian church,
baptist,baptist church
mahatma gandhi,mahatma
malignant hyperthermia,hyperthermia
maple sugar,sugar maple
martin luther,martin luther king,luther,martin,martin luther king day
matzo,matzah,matzoh,passover,seder,matzo meal,pesach,haggadah,gefilte fish
mestizo,half blood,half and half
meteorology,weather bureau
moslem,muslim,prophet,mohammed,mohammad,muhammad,mahomet
movie star,star,revenge,film star,menace,george lucas
mt st helens,mount saint helens,mount st helens
myeloma,multiple myeloma
ness,loch ness,loch ness monster,loch,nessie
new guinea,papua new guinea,papua
november,nov
pacifier,hitch
papa,pope,vatican,vatican city,karol wojtyla,john paul ii,holy see,pius xii,papacy,paul vi,john xxiii,the holy see,
vatican ii,pontiff,gulp,pater,nostradamus,ii,pontifex
parietal lobe,glioma,malignant tumor
particle accelerator,atom smasher,hadron,large,tallulah bankhead,bankhead,tanner
pledge,allegiance
president carter,jimmy carter
president nixon,richard nixon,richard m nixon
sept 11,september 11,sep 11,twin towers,wtc,ground zero,world trade center
slum,millionaire,pinto
strasbourg,grenoble,toulouse,poitiers,lyon,lille,nantes,reims
valentine,valentine day,february 14,romantic
aeon,flux
alien,predator
anne hathaway,hathaway
architeuthis,giant squid
basal temperature,basal body temperature
execution,saddam hussein,hussein,saddam,hanging,husain
flood,flooding
george herbert walker bush,george walker bush
intifada,palestine
may 1,may day,maypole
Table 2: Sample of clusters obtained from the temporal correlations.
1050
Type Clusters
True synonyms 19
Variations of people names 42
People names with and without titles 4
First name and surname from the same person 4
Geographically-related terms 18
Synonyms of location names 4
Derived words 4
Word optionalizations 87
Word reordering 7
Morphological variants 1
Acronyms 1
Cross-language synonyms 3
Hyperonym/hyponym 10
Sibling terms 10
Co-ocurring events in time 8
Topically related 38
Unrelated 72
Table 3: Results of the manual annotation of 2-
item clusters.
time. A particular example of this is when
the two terms are part of a named entity, as in
the case of quantum and solace, which have
a similar correlation because they appear to-
gether in a movie title.
? Topically-related terms, as the cluster
{jesus christ, pilate, pontius pilate, passion of
christ, passion, aramaic}, or the cluster con-
taining popes and the Vatican. A similar ex-
ample, execution is highly correlated to sad-
dam hussein, because his execution attracted
more interest worldwide during this time pe-
riod than any other execution. Interestingly,
topical correlation emerges at very specific
granularity.
For the manual analysis of the results, we ran-
domly selected 332 clusters containing only two
items (so that 664 phrases were considered in to-
tal). Each of these pairs has been classified in one
of the previous categories. The results of this anal-
ysis are shown in Table 3.
4 Application to query suggestion
Query suggestion is a feature of search engines
that helps users reformulate queries in order to bet-
ter describe their information need with the pur-
pose of reducing the time needed to find the de-
sired information (Beeferman and Berger, 2000;
Kraft and Zien, 2004; Sahami and Heilman, 2006;
Cucerzan and White, 2007; Yih and Meek, 2008).
In this section, we explore the application of a sim-
ilarity metric based on time series correlations for
finding related queries to suggest to the users.
As a test set, we have used the query sugges-
Method P@1 P@3 P@5 mAP
Random 0.37 0.37 0.37 0.43
Web Kernel 0.51 0.47 0.42 0.51
Dist. simil. 0.72 0.63 0.60 0.64
Time series 0.74 0.63 0.53 0.67
Combination 0.79 0.68 0.60 0.69
Table 4: Results for the query suggestion task.
tion dataset from (Alfonseca et al, 2009). It con-
tains a set of 57 queries and an average of 22 can-
didate query suggestions for each of them. Each
suggestion was rated by two human raters using
the 5-point Likert scale defined in (Sahami and
Heilman, 2006), from irrelevant to highly relevant.
The task involves providing a ranking of the sug-
gestions that most closely resembles the human
scores. The evaluation is based on standard IR
metrics: precision at 1, 3 and 5, and mean average
precision. In order to compute the precision- and
recall-based metrics, we infer a binary distinction
from the ratings: related or not related. The inter-
annotator agreement for this dataset given the bi-
nary classification as computed by Cohen?s Kappa
is 0.6171.
We used three baselines: the average values that
would be produced by a random scorer of the can-
didate suggestions, Sahami and Heilman (2006)?s
system (based on calculating similarities between
the retrieved snippets), and a recent competitive
ranker based on calculating standard distributional
similarities (Alfonseca et al, 2009) between the
original query and the suggestion. Please refer to
the referenced work for details.
In order to produce the ranked lists of candi-
date suggestions for each query, due to the lack of
training data, we have opted for the unsupervised
procedure described in the previous section:
1. Collect the daily time series of each of the
queries and the candidate suggestions.
2. Calculate the correlation between the original
query and each of the candidate suggestions
provided for it, and use it as the candidate?s
score.
3. For each query, rank its candidate sugges-
tions in decreasing order of correlation.
Finally, taking into account that the source of
similarity is very different to the one used for dis-
tributional similarity, we tested the hypothesis that
1051
a combination of the two techniques would be ben-
eficial to capture different features of the queries
and suggestions. We have trained a linear mixture
model combining both scores (time series and dis-
tributional similarities), using 10-fold cross vali-
dation.
The results are displayed in Table 4. For eval-
uating the results, whenever a system produced a
tie between several suggestions, we generated 100
random orderings of the elements in the tie, and
report the average scores.
Using distributional similarities and the tempo-
ral series turned out to be indistinguishable for the
precision scores at 0.95 confidence, and both are
significantly better than the similarity metric based
on the web kernel. The combination produced an
improvement across all metrics, although not sta-
tistically significant at p=0.05.
This is quite a positive finding as the time series
method relies on stored information requiring only
simple and highly optimized lookups.
5 Application to query categorization
The results from the manual evaluation in Sec-
tion 3.2 support the conclusion that time series
from query logs provide powerful signals for clus-
tering at a fine-grained level, in some cases un-
covering synonyms (may 1st, may day) and even
causal relations (insulation, heating). A natural
question is if temporal information is correlated
with other types of categorizations. In this sec-
tion we carry out a preliminary exploration of the
relation between query time series and query cat-
egorization. To this extent we adapt the data from
the KDD 2005 CUP (Li et al, 2005), which pro-
vides a set of queries classified into 67 broad topi-
cal categories. Since the data is rather sparse (678
queries) we applied Fourier analysis to ?smooth?
the time series.
5.1 The KDD CUP data
The KDD Cup 2005
6
introduced a query catego-
rization task and dataset consisting of 800,000 un-
labeled queries for unsupervised training, and an
evaluation set of 911 queries, 111 for development
and 800 for the final evaluation. The systems sub-
mitted for this task can be quite complex and made
full use of the large unlabeled set. Our goal here is
not to provide a comparative evaluation, but only
6
http://www.sigkdd.org/kdd2005/kddcup.
html
?
101234567
TIME
STANDARDIZED FREQUENCY  
 
Figure 2: RDFT reconstruction for the query
?brush cutters? using the first 25 Fourier coeffi-
cients. The squares represent the original time
series datapoints, while the continuous line repre-
sents the reconstructed signal.
to use the labelled data
7
in a simplified manner to
better understand the semantic properties of query
time series. Each query in the dataset is assessed
by three editors who can assign multiple topic la-
bels from a set of 67 categories belonging to seven
broad topics: Computers, Entertainment, Informa-
tion, Living, Online Community, Shopping and
Sports. We merged the KDD Cup development
and test set, out of the 911 queries we were able to
retrieve significant temporal information for 678
queries. We joined the sets of labels from each as-
sessor for each query. On average, each query is
assigned five labels.
5.2 DFT analysis
Assessing the similarity of data represented as
time series has been addressed mostly my means
of Fourier analysis; e.g., Agrawal et al (1993) in-
troduce a method for efficiently retrieving time
series from databases based on Discrete Fourier
Transform (DFT). Several other methods have
been proposed, e.g., Discrete Wavelet Trans-
form (DWT), however DFT provide a competitive
benchmark approach (Wu et al, 2000).
We use DFT to generate the Fourier coefficients
of the time series and Reverse DFT (RDFT) to re-
construct the original signal using only a subset
of the coefficients. This analysis effectively com-
presses the time series producing a smoother ap-
proximate representation. DFT can be computed
efficiently via Fast Fourier Transform (FFT), with
7
The KDD Cup dataset is probably the only public query
log providing topical categorization information.
1052
Method Accuracy ? std-err
Random 0.107 0.03
MostFrequent 0.490 0.07
DFT-c10 0.425 0.06
DFT-c50 0.456 0.05
DFT-c100 0.502 0.05
DFT-c200 0.456 0.04
DFT-c400 0.506 0.05
DFT-c600 0.481 0.06
DFT-c800 0.478 0.04
DFT-c1000 0.466 0.05
Table 5: Results of the KDD dataset exploration.
complexityO(n log n) where n is the length of the
sequence. The approximate representation is use-
ful not only to address sparsity but can also be used
to efficiently estimate the similarity of two time
series using only a small subset of coefficients as
in (Agrawal et al, 1993). As an example, Fig-
ure 2 shows the original time series for the query
?brush cutters? and its reconstructed signal using
only the first 25 Fourier coefficients. The recon-
structed signal captures the essence of the period-
icity of the query and highlights the yearly peaks
registered for the query in spring and summer.
5.3 Experiment and discussion
To explore the correlation between the structured
temporal representation of queries provided by the
time series and topical categorization we run the
following experiment. Each KDD Cup query was
reconstructed via RDFT using a variable number
of coefficients. The set of 679 queries was parti-
tioned in 10 sets and a 10-fold evaluation was per-
formed. For each fold we trained a classifier on the
remaining 9 folds. We used an average multi-class
perceptron (Freund and Schapire, 1999) adapted to
multi-label learning (Crammer and Singer, 2003).
Each model was trained on a fixed number of 10
iterations. The accuracy of each model was eval-
uated as the fraction of test items for which the
selected highest scoring class was in the gold stan-
dard set provided by the editors. As a lower bound
we estimated the accuracy of randomly choosing
a label for each test instance, and as a baseline we
used the most frequent label. The latter is a pow-
erful predictor: baselines based on class frequency
outperformmost of the systems that participated in
the KDD Cup (Lin and Wu, 2009).
Table 5 reports the average accuracy over the
10 runs with relative standard errors. Each DFT-
based model is characterized by the number of co-
efficients used for the reconstruction. Two main
patterns are noticeable. First, none of the differ-
ences between the frequency-based baseline and
the DFT models is significant, this seems to indi-
cate that temporal structure alone is not a good dis-
criminator of topic, at least of broad categories. In
retrospect, this is somewhat predictable. The tem-
poral dimension is a basic semantic component of
lexical meaning and world knowledge which is not
necessarily associated with any broad, and to some
extent subjective, categorization. An inspection of
the patterns found in each category shows in fact
that similar patterns often emerge in different cat-
egories; e.g., ?Halloween costume? and ?cheese-
cake recipe? have a similar yearly periodical pat-
tern with spikes in early winter, while monotoni-
cally decaying patterns are shared across all cate-
gories; e.g., between computer hardware and kids
toys.
The second interesting finding is the trend of
the DFT system results, higher at low-intermediate
values, providing some initial promising evidence
that DFT analysis generates useful compressed
representations which could be indexed and ap-
plied efficiently. Notice that the sequences recon-
structed using 1,000 coefficients reproduce almost
identically the original signals.
6 Applications in information extraction
Time series from query logs are particularly rel-
evant for phrases that refer to entities which are
involved in recent events. Therefore, we expect
them to be useful for solving other applications
that require handling entities, such as named en-
tity recognition and classification, relation extrac-
tion or disambiguation.
To illustrate this point, we mention an example
of relation extraction between actors and movies:
movies usually have spikes when they are re-
leased, and then the frequency again drops sharply.
At the same times, when a movie is released, the
search engine users have a renewed interest in
their actors. Figure 3 displays the time series for
the five most recent movies by Jim Carrey (as of
march 2009), and the time series for Jim Carrey.
As can be seen, the spikes are at exactly the same
points in time. If we add up the series (a) through
(e) into a single series and calculate the correlation
with (f), it turns out to be very high (0.88).
1053
(a) (b) (c)
(d) (e) (f)
Figure 3: Time series obtained for the five most recent movies with Jim Carrey, and (f) time serie for the
query [jim carrey] (normalized scales).
System Precision Recall F-measure
Random 0.24 0.14 0.17
Time series 0.53 0.66 0.57
Table 6: Results for the query suggestion task.
To validate the hypothesis that this data should
be useful for identifying related entities, we have
performed a small experiment in the following
way: by choosing five popular actors
8
and the cin-
ema movies in which they appear since the year
2004, obtained from IMDB
9
. Using the time se-
ries, for each actor we choose the combination of
movies such that, by adding up the time series of
those movies, we maximise the correlation with
the actor?s time series. It has been implemented
with a greedy beam search, with a beam size of
100. The results are shown in Table 6. The random
baseline randomly associates the movies from the
dataset with the five actors.
We do not believe this to be a perfect feature as,
for example, actors may have a peak in the time se-
ries related to their personal lives, not necessarily
to movies. However, the high correlations that can
be obtained when the pairing between actors and
movies is correct, and the improvement with re-
spect a random baseline, indicates this is a feature
which can probably be integrated with other re-
lation extraction systems when handling relation-
ships between entities that have big temporal de-
pendencies.
8
Ben Stiller, Edward Norton, Jim Carrey, Leonardo Di-
caprio, and Tom Hanks.
9
www.imdb.com.
7 Conclusions and future work
This paper explores the relationships between
queries whose associated time series obtained
from query logs are highly correlated. The use
of time series in semantic similarity has been dis-
cussed by Chien (2005), but only a very prelimi-
nary evaluation was described, and, to our knowl-
edge, they had never been applied and evaluated
in solving existing problems. Our results indicate
that, for a substantial percentage of phrases in a
thesaurus, it is possible to find other highly-related
phrases; and we have categorized the kind of se-
mantic relationships that hold between them.
We have found that in a query suggestion
task, somewhat surprisingly, results are compara-
ble with other state-of-the-art techniques based on
distributional similarities. Furthermore, informa-
tion obtained from time series seems to be com-
plementary with them, as a simple combination of
similarity metrics produces an important increase
in performance..
From an analysis on a query categorization task
the initial evidence suggests that there is no strong
correlation between broad topics and temporal
profiles. This agrees with the intuition that time
provides a fundamental semantic dimension possi-
bly orthogonal to broad topical classification. This
issue however deserves further investigation. An-
other issue which is worth a deeper investigation
is the application of Fourier transform methods
which offer tools for studying the periodic struc-
ture of the temporal sequences.
1054
References
R. Agrawal, C. Faloutsos, and A.N. Swami. 1993. Ef-
ficient similarity search in sequence databases. In
Proceedings of the 4th International Conference on
Foundations of Data Organization and Algorithms,
pages 69?84.
E. Alfonseca, K. Hall, and S. Hartmann. 2009. Large-
scale computation of distributional similarities for
queries. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics
- Human Language Technologies conference.
N. Bansal and N. Koudas. 2007a. BlogScope: a sys-
tem for online analysis of high volume text streams.
In Proceedings of the 33rd international conference
on Very large data bases, pages 1410?1413.
N. Bansal and N. Koudas. 2007b. BlogScope: Spatio-
temporal analysis of the blogosphere. In Proceed-
ings of the 16th international conference on World
Wide Web, pages 1269?1270.
D. Beeferman and A. Berger. 2000. Agglomerative
clustering of a search engine query log. In Proceed-
ings of the sixth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 407?416.
S. Chien. 2005. Semantic similarity between search
engine queries using temporal correlation. In Pro-
ceedings of the 14th international conference on
World Wide Web, pages 2?11.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
S. Cucerzan and R.W. White. 2007. Query sugges-
tion based on user landing pages. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 875?876.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
J. Ginsberg, M.H. Mohebbi, R.S. Patel, L. Brammer,
M.S. Smolinski, and L. Brilliant. 2009. Detecting
influenza epidemics using search engine query data.
Nature, 457, February.
R. Jones and F. Diaz. 2007. Temporal profiles of
queries. ACM Transactions on Information Systems,
25(3):14.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In Proceedings of
the 15th international conference on World Wide
Web, pages 387?396.
J. Kleinberg. 2006. Temporal dynamics of on-line in-
formation streams. In Data Stream Management:
Processing High-Speed Data. Springer.
R. Kraft and J. Zien. 2004. Mining anchor text for
query refinement. In Proceedings of the 13th inter-
national conference on World Wide Web, pages 666?
674.
Y. Li, Z. Zheng, and H. Dai. 2005. KDD Cup-2005
report: Facing a grat challenge. SIGKDD Explor.
Newsl., 7(2):91?99.
D. Lin and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and the International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing.
O. Medelyan, C. Legg, D. Milne, and I.H. Witten.
2008. Mining meaning from Wikipedia. Dept. of
Computer Science, University of Waikato.
Q. Mei, D. Zhou, and K. Church. 2008. Query sug-
gestion using hitting time. In Proceeding of the
17th ACM conference on Information and knowl-
edge management, pages 469?478.
G.A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39?
41.
T. Murata. 2008. Detection of breaking news from
online web search queries. New Generation Com-
puting, 26(1):63?73.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international
conference on World Wide Web, pages 377?386.
E. Terra and C.L.A. Clarke. 2004. Scoring missing
terms in information retrieval tasks. In Proceedings
of the thirteenth ACM international conference on
Information and knowledge management, pages 50?
58.
D.S. Weld, F. Wu, E. Adar, S. Amershi, J. Fogarty,
R. Hoffmann, K. Patel, and M. Skinner. 2008. In-
telligence in Wikipedia. In Proceedings of the 23rd
Conference on Artificial Intelligence.
Y. Wu, D. Agrawal, and A. El Abbadi. 2000. A com-
parison of DFT and DWT based similarity search
in time-series databases. In Proceedings of the 9th
International ACM Conference on Information and
Knowledge Management, pages 488?495.
W. Yih and C. Meek. 2008. Consistent Phrase Rel-
evance Measures. Workshop on Data Mining and
Audience Intelligence for Advertising, page 37.
T. Zesch, C. Muller, and I. Gurevych. 2008a. Extract-
ing lexical semantic knowledge from Wikipedia and
Wiktionary. In Proceedings of the Conference on
Language Resources and Evaluation.
T. Zesch, C. Muller, and I. Gurevych. 2008b. Using
Wiktionary for computing semantic relatedness. In
Proceedings of the Conference on Artificial Intelli-
gence, pages 861?867.
1055
Proceedings of NAACL HLT 2009: Short Papers, pages 29?32,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Large-scale Computation of Distributional Similarities for Queries
Enrique Alfonseca
Google Research
Zurich, Switzerland
ealfonseca@google.com
Keith Hall
Google Research
Zurich, Switzerland
kbhall@google.com
Silvana Hartmann
University of Stuttgart
Stuttgart, Germany
silvana.hartmann@ims.uni-stuttgart.de
Abstract
We present a large-scale, data-driven approach
to computing distributional similarity scores
for queries. We contrast this to recent web-
based techniques which either require the off-
line computation of complete phrase vectors,
or an expensive on-line interaction with a
search engine interface. Independent of the
computational advantages of our approach, we
show empirically that our technique is more
effective at ranking query alternatives that the
computationally more expensive technique of
using the results from a web search engine.
1 Introduction
Measuring the semantic similarity between queries
or, more generally, between pairs of very short texts,
is increasingly receiving attention due to its many
applications. An accurate metric of query simi-
larities is useful for query expansion, to improve
recall in Information Retrieval systems; for query
suggestion, to propose to the user related queries
that might help reach the desired information more
quickly; and for sponsored search, where advertisers
bid for keywords that may be different but semanti-
cally equivalent to user queries.
In this paper, we study the problem of measuring
similarity between queries using corpus-based unsu-
pervised methods. Given a query q, we would like
to rank all other queries according to their similarity
to q. The proposed approach compares favorably to
a state-of-the-art unsupervised system.
2 Related work
Distributional similarity methods model the similar-
ity or relatedness of words using a metric defined
over the set of contexts in which the words appear
(Firth, 1957). One of the most common representa-
tions for contexts is the vector space model (Salton
et al, 1975). This is the basic idea of approaches
such as (Grefenstette, 1992; Bordag, 2008; Lin,
1998; Riloff and Shepherd, 1997), with some varia-
tions; e.g., whether syntactic information is used ex-
plicitly, or which weight function is applied. Most of
the existing work has focused on similarity between
single words or syntactically-correct multiword ex-
pressions. In this work, we adapt these techniques
to calculate similarity metrics between pairs of com-
plete queries, which may or may not be syntactically
correct.
Other approaches for query similarity use sta-
tistical translation models (Riezler et al, 2008),
analysing search engine logs (Jones et al, 2006),
looking for different anchor texts pointing to the
same pages (Kraft and Zien, 2004), or replacing
query words with other words that have the high-
est pointwise mutual information (Terra and Clarke,
2004).
Sahami and Helman (Sahami and Heilman, 2006)
define a web kernel function for semantic similarity
based on the snippets of the search results returned
by the queries. The algorithm used is the following:
(a) Issue a query x to a search engine and collect
the set of n snippets returned by the search engine;
(b) Compute the tf?idf vector vi for each document
snippet di; (c) Truncate each vector to include its m
29
highest weighted terms; (d) Construct the centroid
of the L2-normalized vectors vi; (e) Calculate the
similarity of two queries as the dot product of their
L2-normalized vectors, i.e. as the cosine of both
vectors.
This work was followed up by Yih and Meek (Yih
and Meek, 2007), who combine the web kernel with
other simple metrics of similarity between word vec-
tors (Dice Coefficient, Jaccard Coefficient, Overlap,
Cosine, KL Divergence) in a machine learning sys-
tem to provide a ranking of similar queries.
3 Proposed method
Using a search engine to collect snippets (Sahami
and Heilman, 2006; Yih and Meek, 2007; Yih and
Meek, 2008) takes advantage of all the optimizations
performed by the retrieval engine (spelling correc-
tion, relevance scores, etc.), but it has several disad-
vantages: first, it is not repeatable, as the code un-
derlying search engines is in a constant state of flux;
secondly, it is usually very expensive to issue a large
number of search requests; sometimes the APIs pro-
vided limit the number of requests. In this section,
we describe a method which overcomes these draw-
backs. The distributional methods we propose for
calculating similarities between words and multi-
word expressions profit from the use of a large Web-
based corpus.
The contextual vectors for a query can be col-
lected by identifying the contexts in which the query
appears. Queries such as [buy a book] and [buy
some books] are supposed to appear close to simi-
lar context words in a bag-of-words model, and they
should have a high similarity. However, there are
two reasons why this would yield poor results:
First, as the length of the queries grows, the prob-
ability of finding exact queries in the corpus shrinks
quickly. As an example, when issuing the queries
[Lindsay Lohan pets] and [Britney Spears pets] to
Google enclosed in double quotes, we obtain only
6 and 760 results, respectively. These are too few
occurrences in order to collect meaningful statistics
about the contexts of the queries.
Secondly, many user queries are simply a concate-
nation of keywords with weak or no underlying syn-
tax. Therefore, even if they are popular queries, they
may not appear as such in well-formed text found
in web documents. For example, queries like [hol-
lywood dvd cheap], enclosed in double quotes, re-
trieve less than 10 results. Longer queries, such as
[hotel cheap new york fares], are still meaningful,
but do not appear frequently in web documents.
In order to use of distributional similarities in the
query setting, we propose the following method.
Given a query of interest p = [w1, w2, ..., wn]:
1. For each word wi collect all words that appear
close to wi in the web corpus (i.e., a bag-fo-
words models). Empirically we have chosen
all the words whose distance to wi is less or
equal to 3. This gives us a vector of context
words and frequencies for each of the words in
the query, ~vi = (fi1, fi2, ..., fi|V |), where |V | is
the size of the corpus vocabulary.
2. Represent the query p with a vector of words,
and the weight associated to each word is the
geometric mean of the frequencies for the word
in the original vectors:
~qv =
0
B
@
0
@
|n|Y
i=1
fi1
1
A
1
n
,
0
@
|n|Y
i=1
fi2
1
A
1
n
, ...,
0
@
|n|Y
i=1
fi|V |
1
A
1
n
1
C
A
3. Apply the ?2 test as a weighting function test to
measure whether the query and the contextual
feature are conditionally independent.
4. Given two queries, use the cosine between their
vectors to calculate their similarity.
The motivations for this approach are: the geo-
metric mean is a way to approximate a boolean AND
operation between the vectors, while at the same
time keeping track of the magnitude of the frequen-
cies. Therefore, if two queries only differ on a very
general word, e.g. [books] and either [buy books]
or [some books], the vector associated to the general
words (buy or some in the example) will have non-
zero values for most of the contextual features, be-
cause they are not topically constrained; and the vec-
tors for the queries will have similar sets of features
with non-zero values. Equally relevant, terms that
are closely related will appear in the proximity of a
similar set of words and will have similar vectors.
For example, if the two queries are Sir Arthur Co-
nan Doyle books and Sir Arthur Conan Doyle nov-
els, given that the vectors for books and novels are
expected to have similar features, these two queries
30
Contextual word acid fast bacteria Query
acidogenicity 11 6 4 6.41506
auramin 2 5 2 2.71441
bacillae 3 10 4 4.93242
carbolfuchsin 1 28 2 8.24257
dehydrogena 5 3 3 3.55689
diphtheroid 5 9 92 16.05709
fuchsine 42 3 4 7.95811
glycosilation 3 2 3 2.62074
Table 1: Example of context words for the query [acid fast bacteria].
will receive a high similarity score.
On the other hand, this combination also helps in
reducing word ambiguity. Consider the query bank
account; the bag-of-words vector for bank will con-
tain words related to the various senses of the word,
but when combining it to account only the terms that
belong to the financial domain and are shared be-
tween the two vectors will be included in the final
query vector.
Finally, we note that the geometric mean provides
a clean way to encode the pair-wise similarities of
the individual words of the phrase. One can inter-
pret the cosine similarity metric as the magnitude of
the vector constructed by the scalar product of the
individual vectors. Our approach scales this up by
taking the scalar product of the vectors for all words
in the phrase and then scaling them by the number of
words (i.e., the geometric mean). Instead of comput-
ing the magnitude of this vector, we use it to com-
pute similarities for the entire phrase.
As an example of the proposed procedure, Table 1
shows a random sample of the contextual features
collected for the words in the query [acid fast bac-
teria], and how the query?s vector is generated by
using the geometric mean of the frequencies of the
features in the vectors for the query words.
4 Experiments and results
4.1 Experimental settings
To collect the contextual features for words and
phrases, we have used a corpus of hundreds of mil-
lions of documents crawled from the Web in August
2008. An HTML parser is used to extract text and
non-English documents are discarded. After pro-
cess, the remaining corpus contains hundreds of bil-
lions of words.
As a source of keywords, we have used the top
0 1 2 3 4
0 280 95 14 1 0
1 108 86 65 4 0
2 11 47 83 16 0
3 1 2 17 45 2
4 0 0 1 1 2
Table 2: Confusion matrix for the pairs in the goldstandard. Rows
represent first rater scores, and columns second rater scores.
one and a half million English queries sent to the
Google search engine after being fully anonymized.
We have calculated the pairwise similarity between
all queries, which would potentially return 2.25 tril-
lion similarity scores, but in practice returns a much
smaller number as many pairs have non-overlapping
contexts.
As a baseline, we have used a new implementa-
tion of the Web Kernel similarity (Sahami and Heil-
man, 2006). The parameters are set the same as re-
ported in the paper with the exception of the snip-
pet size; in their study, the size was limited to 1,000
characters and in our system, the normal snippet re-
turned by Google is used (around 160 characters).
In order to evaluate our system, we prepared a
goldstandard set of query similarities. We have ran-
domly sampled 65 queries from our full dataset, and
obtained the top 20 suggestions from both the Sa-
hami system and the distributional similarities sys-
tem. Two human raters have rated the original query
and the union of the sets of suggestions, using the
same 5-point Likert scale that Sahami used. Table 2
shows the confusion matrix of scores between the
two raters. Most of the disagreements are between
the scores 0 and 1, which means that probably it was
not clear enough whether the queries were unrelated
or only slightly related. It is also noteworthy that
in this case, very few rewritten queries were clas-
sified as being better than the original, which also
suggests to us that probably we could remove the
topmost score from the classifications scale.
We have evaluated inter-judge agreement in the
following two ways: first, using the weighted Kappa
score, which has a value of 0.7111. Second, by
grouping the pairs judged as irrelevant or slightly
relevant (scores 0 and 1) as a class containing nega-
tive examples, and the pairs judged as very relevant,
equal or better (scores 2 through 4) as a class con-
taining positive examples. Using this two-class clas-
31
Method Prec@1 Prec@3 Prec@5 mAP AUC
Web Kernel 0.39 0.35 0.32 0.49 0.22
Unigrams 0.47 0.53 0.47 0.57 0.26
N-grams 0.70 0.57 0.52 0.71 0.54
Table 3: Results. mAP is mean average precision, and AUC is the
area under the precision/recall curve.
sification, Cohen?s Kappa score becomes 0.6171.
Both scores indicates substantial agreement amongst
the raters.
The data set thus collected is a ranked list of sug-
gestions for each query1, and can be used to evaluate
any other suggestion-ranking system.
4.2 Experiments and results
As an evolution of the distributional similarities
approach, we also implemented a second version
where the queries are chunked into phrases. The
motivation for the second version is that, in some
queries, like [new york cheap hotel], it makes sense
to handle new york as a single phrase with a sin-
gle associated context vector collected from the web
corpus. The list of valid n-grams is collected by
combining several metrics, e.g. whether Wikipedia
contains an entry with that name, or whether they
appear quoted in query logs. The queries are then
chunked greedily always preferring the longer n-
gram from our list.
Table 3 shows the results of trying both systems
on the same set of queries. The original system is
the one called Unigrams, and the one that chunks
the queries is the one called N-grams. The distri-
butional similarity approaches outperform the web-
based kernel on all the metrics, and chunking queries
shows a good improvement over using unigrams.
5 Conclusions
This paper extends the vector-space model of dis-
tributional similarities to query-to-query similarities
by combining different vectors using the geometric
mean. We show that using n-grams to chunk the
queries improves the results significantly. This out-
performs the web-based kernel method, a state-of-
the-art unsupervised query-to-query similarity tech-
nique, which is particularly relevant as the corpus-
based method does not benefit automatically from
1We plan to make it available to the research community.
search engine features.
References
S. Bordag. 2008. A Comparison of Co-occurrence and
Similarity Measures as Simulations of Context. Lec-
ture Notes in Computer Science, 4919:52.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis, pages 1?32.
G. Grefenstette. 1992. Use of syntactic context to pro-
duce term association lists for text retrieval. In Pro-
ceedings of the 15th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 89?97. ACM New York, NY,
USA.
R. Jones, B. Rey, O. Madani, andW. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15th
international conference on World Wide Web, pages
387?396. ACM New York, NY, USA.
Reiner Kraft and Jason Zien. 2004. Mining anchor text
for query refinement. In WWW ?04: Proceedings of
the 13th international conference on World Wide Web,
pages 666?674, New York, NY, USA. ACM.
D. Lin. 1998. Extracting Collocations from Text Cor-
pora. In First Workshop on Computational Terminol-
ogy, pages 57?63.
Stefan Riezler, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Improved
Query Expansion. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING?08).
E. Riloff and J. Shepherd. 1997. A corpus-based ap-
proach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124. As-
sociation for Computational Linguistics.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international con-
ference on World Wide Web, pages 377?386.
G. Salton, A. Wong, and CS Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613?620.
Egidio Terra and Charles L.A. Clarke. 2004. Scoring
missing terms in information retrieval tasks. In CIKM
?04: Proceedings of the thirteenth ACM international
conference on Information and knowledge manage-
ment, pages 50?58, New York, NY, USA. ACM.
W. Yih and C. Meek. 2007. Improving Similarity Mea-
sures for Short Segments of Text. In Proceedings of
the Natural Conference on Artificial Intelligence, vol-
ume 2, page 1489. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999.
W. Yih and C. Meek. 2008. Consistent Phrase Relevance
Measures. Data Mining and Audience Intelligence for
Advertising (ADKDD 2008), page 37.
32
Coling 2010: Poster Volume, pages 819?827,
Beijing, August 2010
Instance Sense Induction from Attribute Sets
Ricardo Martin-Brualla
Google Inc
rmbrualla@gmail.com
Enrique Alfonseca
Google Inc
ealfonseca@google.com
Marius Pasca
Google Inc
mars@google.com
Keith Hall
Google Inc
kbhall@google.com
Enrique Robledo-Arnuncio
Google Inc
era@google.com
Massimiliano Ciaramita
Google Inc
massi@google.com
Abstract
This paper investigates the new problem
of automatic sense induction for instance
names using automatically extracted at-
tribute sets. Several clustering strategies
and data sources are described and eval-
uated. We also discuss the drawbacks of
the evaluation metrics commonly used in
similar clustering tasks. The results show
improvements in most metrics with re-
spect to the baselines, especially for pol-
ysemous instances.
1 Introduction
Recent work on information extraction increas-
ingly turns its attention to the automatic acqui-
sition of open-domain information from large
text collections (Etzioni et al, 2008). The ac-
quired information typically includes instances
(e.g. barack obama or hillary clinton), class la-
bels (e.g. politician or presidential candidate)
and relations and attributes of the instances (e.g.
president-country or date-of-birth) (Sekine, 2006;
Banko et al, 2007).
Within the larger area of relation extraction,
the acquisition of instance attributes (e.g. pres-
ident for instances of countries, or side effects
for instances of drugs) plays an important role,
since attributes may serve as building blocks in
any knowledge base constructed around open-
domain classes of instances. Thus, a variety
of attribute extraction methods mine textual data
sources ranging from unstructured (Tokunaga et
al., 2005) or structured (Cafarella et al, 2008) text
within Web documents, to human-compiled ency-
clopedia (Wu et al, 2008; Cui et al, 2009) and
Web search query logs (Pas?ca and Van Durme,
2007), attempting to extract, for a given class, a
ranked list of attributes that is as comprehensive
and accurate as possible.
Previous work on attribute extraction, however,
does not capture or address attributes of polyse-
mous instances. An instance may have differ-
ent meanings, and the extracted attributes may
not apply to all of them. For example, the
most salient meanings of darwin are the scientist
Charles Darwin, an Australian city, and an op-
erating system, plus many less-known meanings.
For these ambiguous instances, it is common for
the existing procedures to extract mixed lists of
attributes that belong to incompatible meanings,
e.g. {biography, population, hotels, books}.
This paper explores the problem of automati-
cally inducing instance senses from the learned
attribute lists, and describes several clustering so-
lutions based on a variety of data sources. For
that, it brings together research on attribute acqui-
sition and on word sense induction. Results show
that we can generate meaninful groupings of at-
tributes for polysemous instance names, while not
harming much the monosemous instance names
by generating unwanted clusters for them. The
results are much better than for a random base-
line, and are superior to the one-in-all and the all-
singleton baselines.
2 Previous Work
Previous work on attribute extraction uses a va-
riety of types of textual data as sources for mining
attributes. Some methods take advantage of struc-
tured and semi-structured text available within
Web documents. Examples of this are the use of
markup information in HTML documents to ex-
819
tract patterns and clues around attributes (Yoshi-
naga and Torisawa, 2007; Wong and Lam, 2009;
Ravi and Pas?ca, 2008), or the use of articles
within online encyclopedia as sources of struc-
tured text for attribute extraction (Suchanek et al,
2007; Nastase and Strube, 2008; Wu and Weld,
2008). Regarding unstructured text in Web docu-
ments, the method described in (Tokunaga et al,
2005) takes various class labels as input, and ap-
plies manually-created lexico-syntactic patterns to
document sentences to extract candidate attributes
ranked using several frequency statistics. In (Bel-
lare et al, 2007), the extraction is guided by a set
of seed instances and attributes rather than hand-
crafted patterns, with the purpose of generating
training data and extract new instance-attribute
pairs from text.
Web search queries have also been used as a
data source for attribute extraction, using lexico-
syntactic patterns (Pas?ca and Van Durme, 2007) or
seed attributes (Pas?ca, 2007) to guide the extrac-
tion, and leading to attributes of higher accuracy
than those extracted with equivalent techniques
from Web documents (Pas?ca et al, 2007).
Another related area to this work is the field of
word sense induction: the task of identifying the
possible senses of a word in a corpus using unsu-
pervised methods (Yarowsky, 1995), as opposed
to traditional disambiguation methods which rely
on the availability of a finite and static list of pos-
sible meanings. In (Agirre and Soroa, 2007) a
framework is proposed for evaluating such sys-
tems. Word sense induction can be naturally for-
mulated as a clustering task. This introduces
the complication of choosing the right number
of possible senses, hence a Bayesian approach to
WSI was proposed which deals with this problem
within a principled generative framework (Brody
and Lapata, 2009). Another related line of work
Turkey Attributes Darwin Attributes
maps1 capital1 maps1 definition1,3
recipes2 culture1 awards2 jobs1
pictures1,2 history1 shoes1 tourism1
calories2 tourism1 evolution3 biography3
facts1,2 nutrition facts2 theory3 attractions1
nutrition2 beaches1 weather1 hotels1
cooking time2 brands2 pictures1,3 ports4
religion1 language1 quotes3 population1
Table 1: Attributes extracted for the instances
Turkey and Darwin.
is the disambiguation of people names (Mann and
Yarowsky, 2003). In SEMEVAL-1, a shared task
was introduced dedicated to this problem, the Web
People Search task (Artiles et al, 2007; Artiles et
al., 2009). Disambiguating names is also often ap-
proached as a clustering problem. One challenge
shared by word sense induction and name disam-
biguation (and most unsupervised settings), is the
evaluation. In both tasks, simple baselines such as
predicting one single cluster tend to outperform
more sophisticated approaches (Agirre and Soroa,
2007; Artiles et al, 2007).
3 Instance Sense Induction
3.1 Problem description
This paper assumes the existence of an attribute
extraction procedure. Using those attributes, our
aim is to identify the coarse-grained meanings
with which each attribute is associated. As an
example, Table 1 shows the top 16 attributes ex-
tracted using the procedure described in (Pas?ca
and Van Durme, 2007). Salient meanings for
turkey are the country name (labeled as 1 in the
table), and the bird name (labeled as 2). Some at-
tributes are applicable to both meanings (pictures
and facts). The second example, darwin, can re-
fer to a city (sense 1), the Darwin Awards (sense
2), the person (sense 3), and an operating system
(sense 4).
Examples of applications that need to dis-
criminate between the several meanings of in-
stances are user-facing applications requiring the
attributes to be organized logically and informa-
tion extraction pipelines that depend on the ex-
tracted attributes to find values in documents.
The problem we are addressing is the automatic
induction of instance senses from the attribute
sets, by grouping together the attributes that can
be applied to a particular sense. As in related work
on sense induction (Agirre and Soroa, 2007; Ar-
tiles et al, 2007), we approach this as a clustering
problem: finding the right similarity metrics and
clustering procedures to identify sets of related at-
tributes in an instance. We propose a clustering
based on the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977), exploring differ-
ent parameters, similarity sources, and prior dis-
tributions.
820
3.2 Instance and attributes input data
The input data of instances and attributes has been
obtained, in a fully automated way, following
the method described in (Pas?ca and Van Durme,
2007). The input dataset is a set of fully anony-
mized set of English queries submitted to a popu-
lar (anonymized) search engine. The set contains
millions of unique isolated, individual queries that
are independent from one another. Each query
is accompanied by its frequency of occurrence
in the query logs. The sum of frequencies of
all queries in the dataset is hundreds of millions.
Other sources of similar data are available pub-
licly for research purposes (Gao et al, 2007). This
extraction method applies a few patterns (e.g., the
A of I, or I?s A, or A of I) to queries within
query logs, where an instance I is one of the most
frequent 5 million queries from the repository of
isolated queries, and A is a candidate attribute.
For each instance, the method extracts ranked lists
containing zero, one or more attributes, along with
frequency-based scores. For this work, only the
top 32 attributes of each instance were used, in or-
der to have an input set for the clustering with a
reasonable size, but to keep precision at high lev-
els.
3.3 Per-attribute clustering information
For each (instance, attribute) pair, the following
information is collected:
Search results: The top 20 search results (in-
cluding titles and snippets) returned by a popular
search engine for a query created by concatenat-
ing the instance and the attribute. The motivation
for this data source is that the attributes that re-
fer to the same meaning of the instance should
help the search engine in selecting web pages that
refer to that meaning. The titles and snippets of
these search results are expected to contain other
terms related to that meaning. For example, for
the queries [turkey maps] and [turkey culture] the
search results will contain information related to
the country, whereas [turkey recipes] and [turkey
nutritional value] should share many terms about
the poultry.
Query sessions: A query session is a series of
queries submitted by a single user within a small
range of time (Silverstein et al, 1999). Informa-
tion stored in the session logs may include the text
For each (instance, attribute) pair:
? Retrieve all the sessions that contained the query [in-
stance attribute].
? Collect the set of all the queries that appeared in the
same session and which are a superstring of instance.
? Remove instance from each of those queries, and out-
put the resulting set of query words.
Figure 1: Algorithm to collect session phrases as-
sociated to attributes.
of the queries and metadata, such as the time, the
type of query (e.g., using the normal or the ad-
vance form), and user settings such as the Web
browser used (Silverstein et al, 1999).
Users often search for related queries within
a session: queries on the culture of the coun-
try Turkey will tend to be surrounded by queries
about topics related to the country; similarly,
queries about turkey recipes will tend to be sur-
rounded by other queries on recipes. Therefore,
if two attributes refer to the same meaning of the
instance, the distributions of terms that co-occur
with them in the same search sessions is expected
to be similar. To ensure that the user did not
change intent during the session, we also require
the queries from which we extract phrases to con-
tain the instance of interest. The pseudocode of
the procedure is shown in Figure 1.
Class labels: As described in (Pas?ca and Van
Durme, 2008), we collect for each instance (e.g.,
turkey), a ranked list of class labels (e.g., country,
location, poultry, food). The procedure uses a col-
lection of Web documents and applies some IsA
extraction patterns selected from (Hearst, 1992).
Using the (instance, ranked-attributes) and the (in-
stance, ranked-class labels) lists, it is possible to
aggregate the two datasets to obtain, for each at-
tribute, the class labels that are most strongly as-
sociated to it (Figure 2).
3.4 EM clustering
We run a set of EM clusterings separately for the
attributes of each instance. The model imple-
mented is the following: given an instance, let
A = {a1, a2, ..., an} be the set of attributes as-
sociated with that instance. Let T be the vocabu-
lary for the terms found in the search results, S the
vocabulary of session log terms co-occurring with
821
For each attribute:
? Collect all the instances that contain that attribute.
? For each class label, average its ranks for those in-
stances. If an instance does not contain a particular
class label, use as rank the size of the longest list of
class labels plus one.
? Rank the class labels from smaller to larger average
rank.
Figure 2: Algorithm to collect class labels associ-
ated to attributes.
the attribute, and C be the set of all the possible
class labels. Let K be the cluster function which
assigns cluster indexes to the attributes.
We assume that the distributions for snippet
terms, session terms and class labels are condi-
tionally independent given the clustering. Further-
more, we assume that the distribution of terms for
queries in a cluster are also conditionally indepen-
dent given the cluster assignments:
p?(T |K,A) ?
Y
j
p?(tj |K,A)
p?(S|K,A) ?
Y
k
p?(sk|K,A)
p?(C|K,A) ?
Y
l
p?(cl|K,A)
The clustering model for each instance (the ex-
pectation step) is, therefore:
p?(KT SC|A,?) =
N?
i
p?(K|A)p?(T |K,A)p?(S|K,A)p?(C|K,A)
To estimate the parameters of the model, we must
be able to estimate the following distributions dur-
ing the maximization step:
? p?(tj |K,A) = E?(tj ,K|A)E?(K|A)
? p?(sk|K,A) = E?(sk,K|A)E?(K|A)
? p?(cl|K,A) = E?(cl,K|A)E?(K|A)One advantage of this approach is that it allows
using a subset of the available data sources to eval-
uate their relative influence on the clustering qual-
ity. In the experiments we have tried all possible
combinations of the three data sources to find the
settings that give the best results.
3.5 Initialization strategies
The initial assignment of attributes to clusters is
important, since a bad seed clustering can lead
EM to local optima. We have tried the following
two strategies:
Random assignment: the attributes are assigned
to clusters randomly. To make the results repeat-
able, for each instance we use the instance name
as the seed for the random number generator.
K-means: the initial assignments of attributes
to clusters is performed using K-means. In this
model, we use a simple vector-space-model in the
following way:
1. Each attribute is represented with a bag-of-
words of the snippets of the search results for
a concatenation of the instance name and the
attribute. This is the same data already col-
lected for EM.
2. Each of the snippet terms in these bag-of-
words is weighted using the tf ? idf score,
with inverse document frequencies estimated
from an English web corpus with hundreds
of millions of documents.
3. The cosine of the angle of the vectors is used
as the similarity metric between each pair of
attributes.
Several values of K have been tried in our exper-
iments, as mentioned in Section 4.
3.6 Post-processing
EM works with a fixed set of clusters. In order
to decide which is the optimal number of clusters,
we have run all the experiments with a number of
clusters K that is large enough to accommodate
most of the queries in our dataset, and we run a
post-processing step that merges clusters for in-
stances that have less than K meanings.
Since we have, for each attribute, a distribution
of the most likely class labels (Section 3.3), the
post-processing performs as follows:
1. Generate a list of class labels per cluster, by
combining the ranked lists of per-attribute
class labels as was done in Section 3.3.
2. Merge together all the clusters such that their
sets of top k class labels are the same.
The values ofK and k are chosen by doing several
runs with different values on the development set,
as described in Section 4.
822
4 Evaluation and Results
4.1 Evaluation metrics
There does not exist a fully agreed evaluation
metric for clustering tasks in NLP (Geiss, 2009;
Amigo? et al, 2009). Each metric has its own
idiosyncrasies, so we have chosen to compute
six different evaluation metrics as described in
(Amigo? et al, 2009). Empirical results show they
are highly correlated, i.e., tuning a parameter by
hill-climbing on F-score typically also improves
the B3 F-score.
Purity (Zhao and Karypis, 2002): Let C be
the clusters to evaluate, L the set of cate-
gories (the clusters in the gold-standard), and
N the number of clustered items. Purity is
the average of the precision values: Purity =?
i
|Ci|
N maxj Prec(Ci, Lj), where the precisionfor cluster Ci with respect to category Lj is
Prec(Ci, Lj) = |Ci?Lj ||Ci| . Purity is a precision met-ric. Inverting the roles of the categories L and the
clusters C gives a recall metric, inverse purity,
which rewards grouping items together. The two
metrics can be combined in an F-score.
B3 Precision (Bagga and Baldwin, 1998): Let
L(e) and C(e) denote the gold-standard-category
and the cluster of an item e. The correctness of the
relation between e and other element e? is defined
as
Correctness(e, e?) =
?
1 iffL(e) = L(e?)? C(e) = C(e?)
0 otherwise
The B3 Precision of an item is the proportion
of items in its cluster which belong to its cat-
egory, including itself. The total precision is
the average of the item precisions: B3 Prec =
avge[avge?:C(e)=C(e?)Correctness(e, e?)]
B3 Recall: is calculated in a similar way, inverting
the roles of clusters and categories. The B3 F-
score is obtained by combining B3 precision and
B3 recall.
4.2 Gold standards
We have built two annotated sets, one to be used
as a development set for adjusting the parame-
ters, and a second one as a test set. The evalu-
ation settings were chosen without knowledge of
Purity Inv. F-score B3 B3 B3
Purity Precision Recall F-score
0.94 0.95 0.92 0.90 0.92 0.91
Table 2: Inter-judge agreement scores.
Polysemous Main meanings
airplane machine, movie
apple fruit, company
armstrong unit, company, person
chain reaction company, film, band, chemistry
chf airport, currency, heart attack
darwin person, city
david copperfield book, performer, movie
delta letter, airways
Table 3: Examples of polysemous instances.
the test set. Each of the two sets contains 75 in-
stances chosen randomly from the complete set of
instances with ranked attributes (Section 3.2 de-
scribed the input data). For the random sampling,
the instances were weighted with their frequency
in the query logs as full queries, so that more
frequent instances have higher chance to be cho-
sen. This ensures that uncommon instances are
not overrepresented in the gold-standard.
The annotators contributed 50 additional in-
stances (25 for development and 25 for testing)
that they considered interesting to study, e.g., be-
cause of having several salient meanings.
Five human annotators were shown the top 32
attributes for each instance, and they were asked
to cluster them. We decided to start with a sim-
plified version of the problem by considering it a
hard clustering task.
Table 2 shows that the average agreement
scores between judge pairs, measured with the
same evaluation metrics used for the system out-
put, are quite high. In the first three metrics, the
F-score is not an average of precision and recall,
but a weighted average calculated separately for
each cluster, so it may have a value that is not be-
tween the values of precision and recall.
The annotated instances were classified as
monosemous/polysemous, depending on wether
or not they had more than one cluster with enough
(five) attributes. This classification allows to re-
port separate results for the whole set (where in-
stances with just one major sense dominate) and
for the subset of polysemous instances. Table 3
shows examples of polysemous instances. Exam-
823
All instances polysemous instances
Weights Purity Inv. F B3 B3 B3 F Purity Inv. F B3 B3 B3 F
Purity score Prec. Recall score Purity score Prec. Recall score
All-in-one 0.797 1.000 0.766 0.700 1.000 0.797 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242 1.000 0.205 0.266 1.000 0.205 0.333
Random 0.888 0.322 0.451 0.851 0.246 0.373 0.685 0.362 0.447 0.595 0.276 0.373
Random Only snippets 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
Init. Only sessions 0.797 0.948 0.728 0.700 0.944 0.753 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.798 0.983 0.760 0.701 0.969 0.785 0.561 0.990 0.541 0.415 0.981 0.574
No snippets 0.798 0.934 0.723 0.702 0.918 0.744 0.561 0.990 0.541 0.415 0.981 0.574
No sessions 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
No class labels 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
All 0.809 0.380 0.420 0.736 0.316 0.414 0.596 0.430 0.400 0.483 0.361 0.399
K-Means Only snippets 0.844 0.765 0.700 0.771 0.654 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Init. Only sessions 0.798 0.957 0.736 0.702 0.949 0.759 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.824 0.656 0.622 0.747 0.568 0.604 0.641 0.768 0.565 0.519 0.699 0.575
No snippets 0.824 0.655 0.622 0.748 0.562 0.598 0.640 0.768 0.565 0.518 0.698 0.574
No sessions 0.843 0.770 0.701 0.769 0.661 0.677 0.671 0.806 0.587 0.556 0.719 0.611
No class labels 0.844 0.762 0.698 0.771 0.651 0.673 0.671 0.806 0.587 0.556 0.719 0.611
All 0.843 0.767 0.699 0.770 0.657 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Table 4: Scores over all instances and over polysemous instances.
ples of monosemous instances are activision, am-
ctheaters, american airlines, ask.com, bebo, dis-
ney or einstein. 22% of the instances in the devel-
opment set and 13% of the instances in the test set
are polysemous.
4.3 Parameter tuning
We tuned the different parameters of the algorithm
using the development set. We performed several
EM runs including all three data sources, modi-
fying the following parameters: the smoothing 
added to the cluster soft-assignment in the Maxi-
mization step (Manning et al, 2008), the number
K of clusters for K-Means and EM, and the num-
ber k of top ranked class labels that two clusters
need to have in common in order to be merged
at the post-processing step. The best results were
obtained with  = 0.4, K = 5 and k = 1. These
are the values used in the experiments mentioned
from now on.
4.4 EM initialization and data sources
Table 4 shows the results after running EM over
the development set, using every possible combi-
nation of data sources, and the two initialization
strategies (random and K-Means). Several obser-
vations can be drawn from this table:
First, as mentioned in Section 2, the evalua-
tion metrics are biased towards the all-in-one solu-
tion. This is worsened by the fact that the majority
of the instances in our dataset are monosemous.
Therefore, the highest F-scores and B3 F-scores
are obtained by the all-in-one baseline, although
it is not the most useful clustering.
When using only class labels, EM tends to pro-
duce results similar to the all-in-one baseline This
can be explained by the limited class vocabulary
which makes most of the attributes share class la-
bels. The bad results when using only sessions are
caused by the presence of attributes with no ses-
sion terms, due to insufficient data.
The random clustering baseline (third line in
Table 4) tends to give smaller clusters than EM,
because it distributes instances uniformly across
the clusters. This leads to better precision scores,
and much worse recall and F-score metrics.
From these results, we conclude that snippet
terms are the most useful resource for clustering.
The other data sources do not provide a signifi-
cant improvement over it. The best results overall
for the polysemous instances, and the highest re-
sults for the whole dataset (excluding the outliers
that are too similar to the all-in-one baseline) are
obtained using snippet terms. For these configura-
tions, as we expected, the K-Means initialization
does a better job in avoiding local optima during
EM than the random one.
4.5 Post-processing
Table 5 includes the results on the development
set after post-processing, using the best configu-
ration for EM (K-Means initialization and snippet
terms for EM). Post-processing slightly hurts the
B3 F-score for polysemous terms, but it improves
results for the whole dataset, as it merges many
clusters for the monosemous instances.
824
Data Method Purity Inv. Purity F-score B3 Prec. B3 Recall B3 F-score
All instances All-in-one 0.797 1.000 0.766 0.700 1.000 0.797
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242
K-Means + EM (snippets) 0.844 0.765 0.700 0.771 0.654 0.675
K-Means + EM (snippets) + postprocessing 0.825 0.837 0.728 0.743 0.761 0.722
Polysemous All-in-one 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.205 0.266 1.000 0.205 0.333
K-Means + EM (snippets) 0.671 0.806 0.587 0.556 0.719 0.611
K-Means + EM (snippets) + postprocessing 0.644 0.846 0.592 0.518 0.777 0.607
Table 5: Scores only over all and polysemous instances, without and with postprocessing.
K-Means output EM output Post-processing
pictures, family, logo, biography pictures, biography, inauguration pictures, biography, inauguration
inauguration, song, lyrics, foods, song, lyrics, foods, timeline, song, lyrics, goods, timeline,
quotes, timeline, shoes, health care camping, shoes, maps, art, history, camping, shoes, maps, art, history
maps, art, kids, history, speeches official website, facts, speeches official website, facts, speeches
official website, facts, scandal scandal, blog, music scandal, blog, music, family, kids
economy, blog, music, flag, camping approval rating, health care, daughters
approval rating economy approval rating, health care,
daughters family, kids, daughters economy
symbol logo, quotes, symbol, flag logo, quotes, symbol, definition
definition, religion, definition, religion, slogan, books religion, slogan, books, flag
slogan, books
Table 6: Attributes extracted for the monosemous instance obama, using snippet terms for EM.
4.6 Clustering examples
Tables 6 and 7 show examples of clustering results
for three instances chosen as representatives of the
monosemous and the polysemous subsets. These
show that the output of the K-Means initialization
can uncover some meaningful clusters, but tends
to generate a dominant cluster and a few small or
singleton clusters. EM distributes the attributes
more evenly across clusters, combining attributes
that are closely related.
For monosemous instances like obama, EM
generates small clusters of highly related at-
tributes (e.g, family, kids and daughters). Post-
processing merges some of the clusters together,
but it fails to merge all into a single cluster.
For darwin, two of the small clusters given by
K-Means are actually good, as ports is the only at-
tribute of the operating system, and lyrics is one of
the two attributes referring to a song titled Darwin.
EM again redistributes the attributes, creating two
large and mostly correct clusters.
For david copperfield, EM creates two clusters
for the performer, one for the book, one for the
movie, and one for tattoo (off-topic for this in-
stance). The two clusters referring to the per-
former are merged in the post-processing, with
some errors remaining, e.g, trailer and second
wife are in the wrong cluster.
4.7 Results on the test set
Table 8 show the results of the EM clustering and
the postprocessing step when executed on the test
set. The settings are those that produced the best
results on the development set: using EM initial-
ized with K-Means, and using only snippet terms
for the generative model.
As mentioned above, the test set has a higher
proportion of monosemous queries than the de-
velopment set, so the all-in-one baseline pro-
duces better results than before. Still, we can see
the same trend happening: for the whole dataset
the F-score metrics are somewhat worse than the
best baseline, given that the evaluation metrics all
overvalue the all-in-one baseline, but this can be
considered an artifact of the metrics. As with the
development set, using EM produces the best pre-
cision scores (except for the all-singletons base-
line), and the postprocessing improves precision
and F-score over the all-in-one baseline. The
whole system improves considerably the F-score
for the polysemous terms.
5 Conclusions
This paper investigates the new task of inducing
instance senses using ranked lists of attributes as
input. It describes a clustering procedure based
on the EM model, capable of integrating differ-
825
Instance K-Means output EM output Post-processing
Darwin maps, shoes, logo, awards, maps, shoes, logo, maps, shoes, logo,
weather pictures, quotes, weather jobs, tourism weather jobs, tourism
definition, jobs, tourism, hotels, attractions, hotels, attractions,
biography, hotels, beaches, accommodation, beaches, accommodation,
attractions, beaches, tv show, clothing, tv show, clothing,
accommodation, tv show, postcode, music, review postcode, music, review
clothing, postcode, music side effects, airlines, side effects, airlines,
facts, review, history prices, lighting prices, lighting
side effects, airlines, awards, ports definition, population
prices, lighting evolution, theory, quotes awards, ports
ports pictures, biography, evolution, theory, quotes
evolution, theory, books facts, history, books pictures, biography,
lyrics lyrics facts, history, books
population definition, population lyrics
David Copperfield summary, biography, pictures, biography, pictures, quotes, biography, pictures, girlfriend
quotes, strokes, book review, strokes, tricks, tour dates, quotes, strokes, tricks, tattoo
tricks, tour dates, characters, lyrics, dating, logo, tour dates, secrets, lyrics,
lyrics, plot, synopsis, dating, filmography, cast members, wives, music, dating, logo,
logo, themes, author, official website, trailer, filmography, blog, cast members,
filmography, cast members, setting, religion official website, trailer,
official website, trailer, book review, review, house, setting, religion
setting, religion reviews book review, review, house,
house, reviews tattoo reviews
tattoo summary, second wife, summary, second wife,
second wife characters, plot, synopsis, characters, plot, synopsis,
girlfriend, secrets, wives, themes, author themes, author
review, music, blog girlfriend, secrets, wives,
music, blog
Table 7: Attributes extracted for three polysemous instances, using snippet terms for EM.
Set Solution Purity Inverse Purity F-score B3 Precision B3 Recall B3 F-score
All All-in-one 0.907 1.000 0.892 0.858 1.000 0.908
All-singletons 1.000 0.076 0.114 1.000 0.076 0.136
Random 0.936 0.325 0.463 0.914 0.243 0.377
EM 0.927 0.577 0.664 0.896 0.426 0.561
EM+postprocessing 0.919 0.806 0.804 0.878 0.717 0.764
Polysemous All-in-one 0.588 1.000 0.586 0.457 1.000 0.613
All-singletons 1.000 0.141 0.210 1.000 0.141 0.239
Random 0.643 0.382 0.441 0.549 0.288 0.369
EM 0.706 0.631 0.556 0.626 0.515 0.547
EM+postprocessing 0.675 0.894 0.650 0.564 0.842 0.661
Table 8: Scores in the test set.
ent data sources, and explores cluster initializa-
tion and post-processing strategies. The evalu-
ation shows that the most important of the con-
sidered data sources is the snippet terms obtained
from search engine results to queries made by
concatenating the instance and the attribute. A
simple post-processing that merges attribute clus-
ters that have common class labels can improve
recall for monosemous queries. The results show
improvements across most metrics with respect to
a random baseline, and F-score improvements for
polysemous instances.
Future work includes extending the generative
model to be applied across the board, linking the
clustering models of different instances with each
other. We also intend to explore applications of
the clustered attributes in order to perform extrin-
sic evaluations on these data.
References
Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007, pages 7?12.
Association for Computational Linguistics.
Amigo?, E., J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Retrieval,
12(4):461?486.
Artiles, Javier, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 WePS evaluation: Establishing a
benchmark for the web people search task. In Proceed-
ings of SemEval-2007, pages 64?69.
Artiles, J., J. Gonzalo, and S. Sekine. 2009. Weps 2 evalua-
tion campaign: overview of the web people search cluster-
ing task. In 2nd Web People Search Evaluation Workshop
(WePS 2009), 18th WWW Conference.
Bagga, A. and B. Baldwin. 1998. Entity-based cross-
document co-referencing using the vector space model,
Proceedings of the 17th international conference on Com-
putational linguistics. In Proceedings of ACL-98.
826
Banko, M., Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information extraction
from the Web. In Proceedings of IJCAI-07, pages 2670?
2676, Hyderabad, India.
Bellare, K., P.P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. In NIPS 2007 Workshop
on Machine Learning for Web Search.
Brody, Samuel and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of EACL ?09, pages 103?
111.
Cafarella, M.J., A. Halevy, D.Z. Wang, and Y. Zhang. 2008.
Webtables: Exploring the Power of Tables on the Eeb.
Proceedings of the VLDB Endowment archive, 1(1):538?
549.
Cui, G., Q. Lu, W. Li, and Y. Chen. 2009. Automatic Acqui-
sition of Attributes for Ontology Construction. In Pro-
ceedings of the 22nd International Conference on Com-
puter Processing of Oriental Languages, pages 248?259.
Springer.
Dempster, A.P., N.M. Laird, D.B. Rubin, et al 1977. Max-
imum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society. Series B
(Methodological), 39(1):1?38.
Etzioni, O., M. Banko, S. Soderland, and S. Weld. 2008.
Open Information Extraction from the Web. Communica-
tions of the ACM, 51(12), December.
Gao, W., C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and
H. Hon. 2007. Cross-lingual query suggestion using
query logs of different languages. In Proceedings of
SIGIR-07, pages 463?470, Amsterdam, The Netherlands.
Geiss, J. 2009. Creating a Gold Standard for Sentence Clus-
tering in Multi-Document Summarization. ACL-IJCNLP
2009.
Hearst, M. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of COLING-92, pages
539?545, Nantes, France.
Mann, Gideon S. and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings of
HLT-NAACL 2003, pages 33?40. Association for Compu-
tational Linguistics.
Manning, C.D., P. Raghavan, and H. Schtze. 2008. Intro-
duction to Information Retrieval. Cambridge University
Press New York, NY, USA.
Nastase, V. and M. Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In Proceedings of
AAAI-08, pages 1219?1224, Chicago, Illinois.
Pas?ca, M. and B. Van Durme. 2007. What you seek is what
you get: Extraction of class attributes from query logs. In
Proceedings of IJCAI-07, pages 2832?2837, Hyderabad,
India.
Pas?ca, M. and B. Van Durme. 2008. Weakly-supervised ac-
quisition of open-domain classes and class attributes from
web documents and query logs. In Proceedings of ACL-
08, pages 19?27, Columbus, Ohio.
Pas?ca, M., B. Van Durme, and N. Garera. 2007. The role of
documents vs. queries in extracting class attributes from
text. In Proceedings of CIKM-07, pages 485?494, Lis-
bon, Portugal.
Pas?ca, M. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of WWW-07, pages 101?110,
Banff, Canada.
Ravi, S. and M. Pas?ca. 2008. Using Structured Text for
Large-Scale Attribute Extraction. In CIKM. ACM New
York, NY, USA.
Sekine, S. 2006. On-Demand Information Extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731?738. Association for Compu-
tational Linguistics Morristown, NJ, USA.
Silverstein, C., H. Marais, M. Henzinger, and M. Moricz.
1999. Analysis of a very large web search engine query
log. In ACM SIGIR Forum, pages 6?12. ACM New York,
NY, USA.
Suchanek, F., G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of WWW-07, pages 697?706,
Banff, Canada.
Tokunaga, K., J. Kazama, and K. Torisawa. 2005. Au-
tomatic discovery of attribute words from Web docu-
ments. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP-05),
pages 106?118, Jeju Island, Korea.
Wong, T.L. and W. Lam. 2009. An Unsupervised Method
for Joint Information Extraction and Feature Mining
Across Different Web Sites. Data & Knowledge Engi-
neering, 68(1):107?125.
Wu, F. and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of WWW-
08, pages 635?644, Beijing, China.
Wu, F., R. Hoffmann, and D. Weld. 2008. Information
extraction from Wikipedia: Moving down the long tail.
In Proceedings of KDD-08, pages 731?739, Las Vegas,
Nevada.
Yarowsky, David. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings of
ACL-95, pages 189?196. Association for Computational
Linguistics.
Yoshinaga, N. and K. Torisawa. 2007. Open-Domain
Attribute-Value Acquisition from Semi-Structured Texts.
In Proceedings of the Workshop on Ontolex, pages 55?66.
Zhao, Y. and G. Karypis. 2002. Criterion functions for docu-
ment clustering. Technical report, Experiments and Anal-
ysis University of Minnesota, Department of Computer
Science/Army HPC Research Center.
827
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 214?223,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
DualSum: a Topic-Model based approach for update summarization
Jean-Yves Delort
Google Research
Brandschenkestrasse 110
8002 Zurich, Switzerland
jydelort@google.com
Enrique Alfonseca
Google Research
Brandschenkestrasse 110
8002 Zurich, Switzerland
ealfonseca@google.com
Abstract
Update summarization is a new challenge
in multi-document summarization focusing
on summarizing a set of recent documents
relatively to another set of earlier docu-
ments. We present an unsupervised proba-
bilistic approach to model novelty in a doc-
ument collection and apply it to the genera-
tion of update summaries. The new model,
called DUALSUM, results in the second or
third position in terms of the ROUGE met-
rics when tuned for previous TAC competi-
tions and tested on TAC-2011, being statis-
tically indistinguishable from the winning
system. A manual evaluation of the gen-
erated summaries shows state-of-the art re-
sults for DUALSUM with respect to focus,
coherence and overall responsiveness.
1 Introduction
Update summarization is the problem of extract-
ing and synthesizing novel information in a col-
lection of documents with respect to a set of doc-
uments assumed to be known by the reader. This
problem has received much attention in recent
years, as can be observed in the number of partic-
ipants to the special track on update summariza-
tion organized by DUC and TAC since 2007. The
problem is usually formalized as follows: Given
two collections A and B, where the documents in
A chronologically precede the documents in B,
generate a summary of B under the assumption
that the user of the summary has already read the
documents in A.
Extractive techniques are the most common
approaches in multi-document summarization.
Summaries generated by such techniques consist
of sentences extracted from the document collec-
tion. Extracts can have coherence and cohesion
problems, but they generally offer a good trade-
off between linguistic quality and informative-
ness.
While numerous extractive summarization
techniques have been proposed for multi-
document summarization (Erkan and Radev,
2004; Radev et al 2004; Shen and Li, 2010; Li et
al., 2011), few techniques have been specifically
designed for update summarization. Most exist-
ing approaches handle it as a redundancy removal
problem, with the goal of producing a summary of
collection B that is as dissimilar as possible from
either collection A or from a summary of collec-
tionA. A problem with this approach is that it can
easily classify as redundant sentences in which
novel information is mixed with existing informa-
tion (from collection A). Furthermore, while this
approach can identify sentences that contain novel
information, it cannot model explicitly what the
novel information is.
Recently, Bayesian models have successfully
been applied to multi-document summarization
showing state-of-the-art results in summarization
competitions (Haghighi and Vanderwende, 2009;
Jin et al 2010). These approaches offer clear and
rigorous probabilistic interpretations that many
other techniques lack. Furthermore, they have the
advantage of operating in unsupervised settings,
which can be used in real-world scenarios, across
domains and languages. To our best knowledge,
previous work has not used this approach for up-
date summarization.
In this article, we propose a novel nonpara-
metric Bayesian approach for update summariza-
tion. Our approach, which is a variation of Latent
214
Dirichlet Allocation (LDA) (Blei et al 2003),
aims to learn to distinguish between common in-
formation and novel information. We have eval-
uated this approach on the ROUGE scores and
demonstrate that it produces comparable results
to the top system in TAC-2011. Furthermore, our
approach improves over that system when evalu-
ated manually in terms of linguistic quality and
overall responsiveness.
2 Related work
2.1 Bayesian approaches in Summarization
Most Bayesian approaches to summarization are
based on topic models. These generative mod-
els represent documents as mixtures of latent top-
ics, where a topic is a probability distribution over
words. In TOPICSUM (Haghighi and Vander-
wende, 2009), each word is generated by a sin-
gle topic which can be a corpus-wide background
distribution over common words, a distribution
of document-specific words or a distribution of
the core content of a given cluster. BAYESSUM
(Daume? and Marcu, 2006) and the Special Words
and Background model (Chemudugunta et al
2006) are very similar to TOPICSUM.
A commonality of all these models is the use of
collection and document-specific distributions in
order to distinguish between the general and spe-
cific topics in documents. In the context of sum-
marization, this distinction helps to identify the
important pieces of information in a collection.
Models that use more structure in the repre-
sentation of documents have also been proposed
for generating more coherent and less redun-
dant summaries, such as HIERSUM (Haghighi
and Vanderwende, 2009) and TTM (Celikyilmaz
and Hakkani-Tur, 2011). For instance, HIERSUM
models the intuitions that first sentences in docu-
ments should contain more general information,
and that adjacent sentences are likely to share
specic content vocabulary. However, HIERSUM,
which builds upon TOPICSUM, does not show
a statistically signicant improvement in ROUGE
over TOPICSUM.
A number of techniques have been proposed to
rank sentences of a collection given a word distri-
bution (Carbonell and Goldstein, 1998; Goldstein
et al 1999). The Kullback-Leibler divergence
(KL) is a widely used measure in summarization.
Given a target distribution T that we want a sum-
mary S to approximate, KL is commonly used as
the scoring function to select the subset of sen-
tences S? that minimizes the KL divergence with
T :
S? = argmin
S
KL(T, S) =
?
w?V
pT (w) log
pT (w)
pS(w)
where w is a word from the vocabulary V. This
strategy is called KLSum. Usually, a smoothing
factor ? is applied on the candidate distribution S
in order to avoid the divergence to be undefined1.
This objective function selects the most repre-
sentative sentences of the collection, and at the
same time it also diversifies the generated sum-
mary by penalizing redundancy. Since the prob-
lem of finding the subset of sentences from a
collection that minimizes the KL divergence is
NP-complete, a greedy algorithm is often used in
practice2. Some variations of this objective func-
tion can be considered, such as penalizing sen-
tences that contain document-specific topics (Ma-
son and Charniak, 2011) or rewarding sentences
appearing closer to the beginning of the docu-
ment.
Wang et al(2009) propose a Bayesian ap-
proach for summarization that does not use KL
for reranking. In their model, Bayesian Sentence-
based Topic Models, every sentence in a docu-
ment is assumed to be associated to a unique la-
tent topic. Once the model parameters have been
calculated, a summary is generated by choosing
the sentence with the highest probability for each
topic.
While hierarchical topic modeling approaches
have shown remarkable effectiveness in learning
the latent topics of document collections, they are
not designed to capture the novel information in
a collection with respect to another one, which is
the primary focus of update summarization.
2.2 Update Summarization
The goal of update summarization is to generate
an update summary of a collection B of recent
documents assuming that the users already read
earlier documents from a collection A. We refer
1In our experiments we set ? = 0.01.
2In our experiments, we follow the same approach as in
(Haghighi and Vanderwende, 2009) by greedily adding sen-
tences to a summary so long as they decrease KL divergence.
215
to collection A as the base collection and to col-
lection B as the update collection.
Update summarization is related to novelty de-
tection which can be defined as the problem of
determining whether a document contains new in-
formation given an existing collection (Soboroff
and Harman, 2005). Thus, while the goal of nov-
elty detection is to determine whether some infor-
mation is new, the goal of update summarization
is to extract and synthesize the novel information.
Update summarization is also related to con-
trastive summarization, i.e. the problem of jointly
generating summaries for two entities in order to
highlight their differences (Lerman and McDon-
ald, 2009). The primary difference here is that
update summarization aims to extract novel or up-
dated information in the update collection with re-
spect to the base collection.
The most common approach for update sum-
marization is to apply a normal multi-document
summarizer, with some added functionality to re-
move sentences that are redundant with respect
to collection A. This can be achieved using sim-
ple filtering rules (Fisher and Roark, 2008), Max-
imal Marginal Relevance (Boudin et al 2008), or
more complex graph-based algorithms (Shen and
Li, 2010; Wenjie et al 2008). The goal here is
to boost sentences in B that bring out completely
novel information. One problem with this ap-
proach is that it is likely to discard as redundant
sentences in B containing novel information if it
is mixed with known information from collection
A.
Another approach is to introduce specific fea-
tures intended to capture the novelty in collection
B. For example, comparing collections A and B,
FastSum derives features for the collection B such
as number of named entities in the sentence that
already occurred in the old cluster or the number
of new content words in the sentence not already
mentioned in the old cluster that are subsequently
used to train a Support Vector Machine classifier
(Schilder et al 2008). A limitation with this ap-
proach is there are no large training sets available
and, the more features it has, the more it is af-
fected by the sparsity of the training data.
3 DualSum
3.1 Model Formulation
The input for DUALSUM is a set of pairs of collec-
tions of documents C = {(Ai,Bi)}i=1...m, where
Ai is a base document collection and Bi is an up-
date document collection. We use c to refer to a
collection pair (Ac,Bc).
In DUALSUM, documents are modeled as a bag
of words that are assumed to be sampled from a
mixture of latent topics. Each word is associated
with a latent variable that specifies which topic
distribution is used to generate it. Words in a doc-
ument are assumed to be conditionally indepen-
dent given the hidden topic.
As in previous Bayesian works for summariza-
tion (Daume? and Marcu, 2006; Chemudugunta
et al 2006; Haghighi and Vanderwende, 2009),
DUALSUM not only learns collection-specific dis-
tributions, but also a general background distri-
bution over common words, ?G and a document-
specific distribution ?cd for each document d in
collection pair c, which is useful to separate the
specific aspects from the general aspects of c. The
main novelty is that DUALSUM introduces spe-
cific machinery for identifying novelty.
To capture the differences between the base and
the update collection for each pair c, DUALSUM
learns two topics for every collection pair. The
joint topic, ?Ac captures the common information
between the two collections in the pair, i.e. the
main event that both collections are discussing.
The update topic, ?Bc focuses on the specific as-
pects that are specific of the documents inside the
update collection.
In the generative model,
? For a document d in a collection Ac, words
can be originated from one of three differ-
ent topics: ?G, ?cd and ?Ac , the last one of
which captures the main topic described in
the collection pair.
? For a document d in a collection Bc, words
can be originated from one of four different
topics: ?G, ?cd, ?Ac and ?Bc . The last one
will capture the most important updates to
the main topic.
To make this representation easier, we can also
state that both collections are generated from the
four topics, but we constrain the topic probability
216
1. Sample ?G ? Dir(?G)
2. For each collection pair c = (Ac,Bc):
? Sample ?Ac ? Dir(?A)
? Sample ?Bc ? Dir(?B)
? For each document d of type ucd ? {A,B}:
- Sample ?cd ? Dir(?D)
- If (ucd = A) sample ?cd ? Dir(?A)
- If (ucd = B) sample ?cd ? Dir(?B)
- For each word w in document d:
(a) Sample a topic z ? Mult(?cd), z ?
{G, cd,Ac,Bc}
(b) Sample a word w ?Mult(?z)
Figure 1: Generative model in DUALSUM.
w
z
?D
?
?B
?B
?D ?G
?G
?B
u
?A
?A
?A
Figure 2: Graphical model representation of DUAL-
SUM.
for ?Bc to be always zero when generating a base
document.
We denote ucd ? {A,B} the type of a docu-
ment d in pair c. This is an observed, Boolean
variable stating whether the document d belongs
to the base or the update collection inside the pair
c.
The generation process of documents in DU-
ALSUM is described in Figure 1, and the plate
diagram corresponding to this generative story
is shown in Figure 2. DUALSUM is an LDA-
like model, where topic distributions are multi-
nomial distributions over words and topics that
are sampled from Dirichlet distributions. We use
? = (?G, ?D, ?A, ?B) as symmetric priors for the
Dirichlet distributions generating the word distri-
butions. In our experiments, we set ?G = 0.1 and
?D = ?A = ?B = 0.001. A greater value is as-
signed to ?G in order to reflect the intuition that
there should be more words in the background
than in the other distributions, so the mass is ex-
pected to be shared on a larger number of words.
Unlike for the word distributions, mixing prob-
abilities are drawn from a Dirichlet distribution
with asymmetric priors. The prior knowledge
about the origin of words in the base and up-
date collections is again encoded at the level the
hyper-parameters. For example, if we set ?A =
(5, 3, 2, 0), this would reflect the intuition that,
on average, in the base collections, 50% of the
words originate from the background distribution,
30% from the document-specific distribution, and
20% from the joint topic. Similarly, if we set
?B = (5, 2, 2, 1), the prior reflects the assumption
that, on average, in the update collections, 50% of
the words originate from the background distri-
bution, 20% from the document-specific distribu-
tion, 20% from the joint topic, and 10% from the
novel, update topic3. The priors we have actually
used are reported in Section 4.
3.2 Learning and inference
In order to find the optimal model parameters, the
following equation needs to be computed:
p(z, ?, ?|w,u) =
p(z, ?, ?,w,u)
p(w,u)
Omitting hyper-parameters for notational sim-
plicity, the joint distribution over the observed
variables is:
p(w,u) = p(?G)?
?
c
p(?Ac)p(?Bc)?
?
d
p(ucd)p(?
cd)
?
?
p(?cd|ucd)d?
cd ?
?
n
?
cdn
p(wcdn|zcdn)p(zcdn|?
cd)
where ? denotes the 4-dimensional simplex4.
Since this equation is intractable, we need to per-
form approximate inference in order to estimate
the model parameters. A number of Bayesian sta-
tistical inference techniques can be used to ad-
dress this problem.
3To highlight the difference between asymmetric and
symmetric priors we put the indices in superscript and sub-
script respectively.
4Remember that, for base documents, words cannot
be generated by the update topic, so ? denotes the 3-
dimensional simplex for base documents.
217
Variational approaches (Blei et al 2003) and
collapsed Gibbs sampling (Griffiths and Steyvers,
2004) are common techniques for approximate in-
ference in Bayesian models. They offer different
advantages: the variational approach is arguably
faster computationally, but the Gibbs sampling
approach is in principal more accurate since it
asymptotically approaches the correct distribution
(Porteous et al 2008). In this section, we pro-
vide details on a collapsed Gibbs sampling strat-
egy to infer the model parameters of DUALSUM
for a given dataset.
Collapsed Gibbs sampling is a particular case
of Markov Chain Monte Carlo (MCMC) that in-
volves repeatedly sampling a topic assignment for
each word in the corpus. A single iteration of the
Gibbs sampler is completed after sampling a new
topic for each word based on the previous assign-
ment. In a collapsed Gibbs sampler, the model
parameters are integrated out (or collapsed), al-
lowing to only sample z. Let us call wcdn the n-th
word in document d in collection c, and zcdn its
topic assignment. For Gibbs sampling, we need
to calculate p(zcdn|w,u, z?cdn) where z?cdn de-
notes the random vector of topic assignments ex-
cept the assignment zcdn.
p(zcdn = j|w,u, z?cdn, ?A, ?B, ?) ?
n(wcdn)?cdn,j + ?j
?V
v=1 n
(v)
?cdn,j + V ?j
n(cd)?cdn,j + ?
ucd
j
?
k?K(n
(cd)
?cdn,k + ?
ucd
k )
where K = {G, cd,Ac,Bc}, n
(v)
?cdn,j denotes the
number of times word v is assigned to topic j
excluding current assignment of word wcdn and
n(cd)?cdn,k denotes the number of words in document
d of collection c that are assigned to topic j ex-
cluding current assignment of word wcdn.
After each sampling iteration, the model pa-
rameters can be estimated using the following for-
mulas5.
?kw =
n(w)k + ?k
?V
v=1 n
(v)
k + V ?k
?cdk =
n(cd)k + ?k?
n(cd). + V ?k
5The interested reader is invited to consult (Wang, 2011)
for more details on using Gibbs sampling for LDA-like mod-
els
where k ? K, n(v)k denotes the number of times
word v is assigned to topic k, and n(cd)k denotes
the number of words in document d of collection
c that are assigned to topic k.
By the strong law of large numbers, the average
of sample parameters should converge towards
the true expected value of the model parameter.
Therefore, good estimates of the model parame-
ters can be obtained averaging over the sampled
values. As suggested by Gamerman and Lopes
(2006), we have set a lag (20 iterations) between
samples in order to reduce auto-correlation be-
tween samples. Our sampler also discards the first
100 iterations as burn-in period in order to avoid
averaging from samples that are still strongly in-
fluenced by the initial assignment.
4 Experiments in Update
Summarization
The Bayesian graphical model described in the
previous section can be run over a set of news
collections to learn the background distribution,
a joint distribution for each collection, an update
distribution for each collection and the document-
specific distributions. Once this is done, one of
the learned collections can be used to generate the
summary that best approximates this collection,
using the greedy algorithm described by Haghighi
and Vanderwende (2009). Still, there are some pa-
rameters that can be defined and which affects the
results obtained:
? DUALSUM?s choice of hyper-parameters af-
fects how the topics are learned.
? The documents can be represented with n-
grams of different lengths.
? It is possible to generate a summary that ap-
proximates the joint distribution, the update-
only distribution, or a combination of both.
This section describes how these parameters
have been tuned.
4.1 Parameter tuning
We use the TAC 2008 and 2009 update task
datasets as training set for tuning the hyper-
parameters for the model, namely the pseudo-
counts for the two Dirichlet priors that affects the
topic mix assignment for each document. By per-
forming a grid search over a large set of pos-
sible hyper-parameters, these have been fixed to
218
?A = (90, 190, 50, 0) and ?B = (90, 170, 45, 25)
as the values that produced the best ROUGE-2
score on those two datasets.
Regarding the base collection, this can be inter-
preted as setting as prior knowledge that roughly
27% of the words in the original dataset originate
from the background distribution, 58% from the
document-specific distributions, and 15% from
the topic of the original collection. We remind
the reader that the last value in ?A is set to zero
because, due to the problem definition, the origi-
nal collection must have no words generated from
the update topic, which reflects the most recent
developments that are still not present in the base
collections A.
Regarding the update set, 27% of the words are
assumed to originate again from the background
distribution, 51% from the document-specific dis-
tributions, 14% from an topic in common with
the original collection, and 8% from the update-
specific topic. One interesting fact to note from
these settings is that most of the words belong to
topics that are specific to single documents (58%
and 51% respectively for both sets A and B) and
to the background distribution, whereas the joint
and update topics generate a much smaller, lim-
ited set of words. This helps these two distribu-
tions to be more focused.
The other settings mentioned at the beginning
of this section have been tuned using the TAC-
2010 dataset, which we reserved as our develop-
ment set. Once the different document-specific
and collection-specific distributions have been ob-
tained, we have to choose the target distribu-
tion T to with which the possible summaries will
be compared using the KL metric. Usually, the
human-generated update summaries not only in-
clude the terms that are very specific about the last
developments, but they also include a little back-
ground regarding the developing event. There-
fore, we try, for KLSum, a simple mixture be-
tween the joint topic (?A) and the update topic
(?B).
Figure 3 shows the ROUGE-2 results obtained
as we vary the mixture weight between the joint
?A distribution and the update-specific ?B distri-
bution. As can be seen at the left of the curve, us-
ing only the update-specific model, which disre-
gards the generic words about the topic described,
produces much lower results. The results improve
as the relative weight of the joined topic model
Figure 3: Variation in ROUGE-2 score in the TAC-
2010 dataset as we change the mixture weight for the
joined topic model between 0 and 1.
Figure 4: Effect of the mixture weight in ROUGE-2
scores (TAC-2010 dataset). Results are reported us-
ing bigrams (above, blue), unigrams (middle, red) and
trigrams (below, yellow).
increases until it plateaus at a maximum around
roughly the interval [0.6, 0.8], and from that point
performance slowly degrades as at the right part
of the curve the update model is given very little
importance in generating the summary. Based on
these results, from this point onwards, the mixture
weight has been set to 0.7. Note that using only
the joint distribution (setting the mixture weight
to 1.0) also produces reasonable results, hinting
that it successfully incorporates the most impor-
tant n-grams from across the base and the update
collections at the same time.
A second parameter is the size of the n-grams
for representing the documents. The original
implementations of SUMBASIC (Nenkova and
Vanderwende, 2005) and TOPICSUM (Haghighi
and Vanderwende, 2009) were defined over sin-
219
gle words (unigrams). Still, Haghighi and Van-
derwende (2009) report some improvements in
the ROUGE-2 score when representing words as
a bag of bigrams, and Darling (2010) mention
similar improvements when running SUMBASIC
with bigrams. Figure 4 shows the effect on the
ROUGE-2 curve when we switch to using uni-
grams and trigrams. As stated in previous work,
using bigrams has better results than using uni-
grams. Using trigrams was worse than either of
them. This is probably because trigrams are too
specific and the document collections are small,
so the models are more likely to suffer from data
sparseness.
4.2 Baselines
DUALSUM is a modification of TOPICSUM de-
signed specifically for the case of update sum-
marization, by modifying TOPICSUM?s graphical
model in a way that captures the dependency be-
tween the joint and the update collections. Still, it
is important to discover whether the new graphi-
cal model actually improves over simpler applica-
tions of TOPICSUM to this task. The three base-
lines that we have considered are:
? Running TOPICSUM on the set of collections
containing only the update documents. We
call this run TOPICSUMB.
? Running TOPICSUM on the set of collections
containing both the base and the update doc-
uments. Contrary to the previous run, the
topic model for each collection in this run
will contain information relevant to the base
events. We call this run TOPICSUMA?B.
? Running TOPICSUM twice, once on the set
of collections containing the update docu-
ments, and the second time on the set of
collections containing the base documents.
Then, for each collection, the obtained base
and update models are combined in a mix-
ture model using a mixture weight between
zero and one. The weight has been tuned us-
ing TAC-2010 as development set. We call
this run TOPICSUMA+TOPICSUMB.
4.3 Automatic evaluation
DUALSUM and the three baselines6 have been
6Using the settings obtained in the previous section, hav-
ing been optimized on the datasets from previous TAC com-
petitions.
automatically evaluated using the TAC-2011
dataset. Table 1 shows the ROUGE results ob-
tained. Because of the non-deterministic nature
of Gibbs sampling, the results reported here are
the average of five runs for all the baselines and
for DUALSUM. DUALSUM outperforms two of
the baselines in all three ROUGE metrics, and it
also outperforms TOPICSUMB on two of the three
metrics.
The top three systems in TAC-2011 have been
included for comparison. The results between
these three systems, and between them and DU-
ALSUM, are all indistinguishable at 95% confi-
dence. Note that the best baseline, TOPICSUMB,
is quite competitive, with results that are indis-
tinguishable to the top participants in this year?s
evaluation. Note as well that, because we have
five different runs for our algorithms, whereas
we just have one output for the TAC participants,
the confidence intervals in the second case were
slightly bigger when checking for statistical sig-
nificance, so it is slightly harder for these systems
to assert that they outperform the baselines with
95% confidence. These results would have made
DUALSUM the second best system for ROUGE-
1 and ROUGE-SU4, and the third best system in
terms of ROUGE-2.
The supplementary materials contain a detailed
example of the the topic model obtained for the
background in the TAC-2011 dataset, and the base
and update models for collection D1110. As
expected, the top unigrams and bigrams are all
closed-class words and auxiliary verbs. Because
trigrams are longer, background trigrams actu-
ally include some content words (e.g. university
or director). Regarding the models for ?A and
?B, the base distribution contains words related
to the original event of an earthquake in Sichuan
province (China), and the update distribution fo-
cuses more on the official (updated) death toll
numbers. It can be noted here that the tokenizer
we used is very simple (splitting tokens separated
with white-spaces or punctuation) so that num-
bers such as 7.9 (the magnitude of the earthquake)
and 12,000 or 14,000 are divided into two tokens.
We thought this might be a for the bigram-based
system to produce better results, but we ran the
summarizers with a numbers-aware tokenizer and
the statistical differences between versions still
hold.
220
Method R-1 R-2 R-SU4
TOPICSUMB 0.3442 0.0868 0.1194
TOPICSUMA?B 0.3385 0.0809 0.1159
TOPICSUMA+TOPICSUMB 0.3328 0.0770 0.1125
DUALSUM 0.3575??? 0.0924?? 0.1285???
TAC-2011 best system (Peer 43) 0.3559?? 0.0958?? 0.1308???
TAC-2011 2nd system (Peer 25) 0.3582?? 0.0926? 0.1276??
TAC-2011 3rd system (Peer 17) 0.3558?? 0.0886 0.1279??
Table 1: Results on the TAC-2011 dataset. ?, ? and ? indicate that a result is significantly better than TOPICSUMB,
TOPICSUMA?B and TOPICSUMA+TOPICSUMB, respectively (p < 0.05).
4.4 Manual evaluation
While the ROUGE metrics provides an arguable
estimate of the informativeness of a generated
summary, it does not account for other important
aspects such as the readability or the overall re-
sponsiveness. To evaluate such aspects, a manual
evaluation is required. A fairly standard approach
for manual evaluation is through pairwise com-
parison (Haghighi and Vanderwende, 2009; Ce-
likyilmaz and Hakkani-Tur, 2011).
In this approach, raters are presented with pairs
of summaries generated by two systems and they
are asked to say which one is best with respect
to some aspects. We followed a similar approach
to compare DualSum with Peer 43 - the best sys-
tem with respect to ROUGE-2, on the TAC 2011
dataset. For each collection, raters were presented
with three summaries: a reference summary ran-
domly chosen from the model summaries, and the
summaries generated by Peer 43 and DualSum.
They were asked to read the summaries and say
which one of the two generated summaries is best
with respect to: 1) Overall responsiveness: which
summary is best overall (both in terms of content
and fluency), 2) Focus: which summary contains
less irrelevant details, 3) Coherence: which sum-
mary is more coherent and 4) Non-redundancy:
which summary repeats less the same informa-
tion. For each aspect, the rater could also reply
that both summary were of the same quality.
For each of the 44 collections in TAC-2011, 3
ratings were collected from raters7. Results are
reported in Table 2. DualSum outperforms Peer
43 in three aspects, including Overall Responsive-
ness, which aggregates all the other scores and
can be considered the most important one. Re-
7In total 132 raters participated to the task via our own
crowdsourcing platform, not mentioned yet for blind review.
Best system
Aspect Peer 43 Same DualSum
Overall Responsiveness 39 25 68
Focus 41 22 69
Coherence 39 30 63
Non-redundancy 40 53 39
Table 2: Results of the side-by-side manual evaluation.
garding Non-redundancy, DualSum and Peer 43
obtain similar results but the majority of raters
found no difference between the two systems.
Fleiss ? has been used to measure the inter-rater
agreement. For each aspect, we observe ? ? 0.2
which corresponds to a slight agreement; but if we
focus on tasks where the 3 ratings reflect a prefer-
ence for either of the two systems, then ? ? 0.5,
which indicates moderate agreement.
4.5 Efficiency and applicability
The running time for summarizing the TAC col-
lections with DualSum, averaged over a hundred
runs, is 4.97 minutes, using one core (2.3 GHz).
Memory consumption was 143 MB.
It is important to note as well that, while TOP-
ICSUM incorporates an additional layer to model
topic distributions at the sentence level, we noted
early in our experiments that this did not improve
the performance (as evaluated with ROUGE) and
consequently relaxed that assumption in Dual-
Sum. This resulted in a simplification of the
model and a reduction of the sampling time.
While five minutes is fast enough to be able
to experiment and tune parameters with the TAC
collections, it would be quite slow for a real-
time summarization system able to generate sum-
maries on request. As can be seen from the plate
diagram in Figure 2, all the collections are gen-
erated independently from each other. The only
exception, for which it is necessary to have all
221
the collections available at the same time dur-
ing Gibbs sampling, is the background distribu-
tion, which is estimated from all the collections
simultaneously, roughly representing 27% of the
words, that should appear distributed across all
documents.
The good news is that this background distri-
bution will contain closed-class words in the lan-
guage, which are domain-independent (see sup-
plementary material for examples). Therefore,
we can generate this distribution from one of
the TAC datasets only once, and then it can be
reused. Fixing the background distribution to a
pre-computed value requires a very simple mod-
ification of the Gibbs sampling implementation,
which just needs to adjust at each iteration the
collection and document-specific models, and the
topic assignment for the words.
Using this modified implementation, it is now
possible to summarize a single collection inde-
pendently. The summarization of a single col-
lection of the size of the TAC collections is re-
duced on average to only three seconds on the
same hardware settings, allowing the use of this
summarizer in an on-line application.
5 Conclusions
The main contribution of this paper is DUALSUM,
a new topic model that is specifically designed to
identify and extract novelty from pairs of collec-
tions.
It is inspired by TOPICSUM (Haghighi and
Vanderwende, 2009), with two main changes:
Firstly, while TOPICSUM can only learn the main
topic of a collection, DUALSUM focuses on the
differences between two collections. Secondly,
while TOPICSUM incorporates an additional layer
to model topic distributions at the sentence level,
we have found that relaxing this assumption and
modeling the topic distribution at document level
does not decrease the ROUGE scores and reduces
the sampling time.
The generated summaries, tested on the TAC-
2011 collection, would have resulted on the sec-
ond and third position in the last summarization
competition according to the different ROUGE
scores. This would make DUALSUM statistically
indistinguishable from the top system with 0.95
confidence.
We also propose and evaluate the applicability
of an alternative implementation of Gibbs sam-
pling to on-line settings. By fixing the back-
ground distribution we are able to summarize a
distribution in only three seconds, which seems
reasonable for some on-line applications.
As future work, we plan to explore the use of
DUALSUM to generate more general contrastive
summaries, by identifying differences between
collections whose differences are not of temporal
nature.
Acknowledgments
The research leading to these results has received
funding from the European Union?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement number 257790. We would also
like to thank Yasemin Altun and the anonymous
reviewers for their useful comments on the draft
of this paper.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alcation. J. Mach. Learn.
Res., 3:993?1022, March.
Florian Boudin, Marc El-Be`ze, and Juan-Manuel
Torres-Moreno. 2008. A scalable MMR approach
to sentence scoring for multi-document update sum-
marization. In Coling 2008: Companion volume:
Posters, pages 23?26, Manchester, UK, August.
Coling 2008 Organizing Committee.
J. Carbonell and J. Goldstein. 1998. The use of mmr,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the
21st annual international ACM SIGIR conference
on Research and development in information re-
trieval, pages 335?336. ACM.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2011. Dis-
covery of topically coherent sentences for extrac-
tive summarization. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
491?499, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS, pages 241?248.
W.M. Darling. 2010. Multi-document summarization
from first principles. In Proceedings of the third
Text Analysis Conference, TAC-2010. NIST.
Hal Daume?, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
222
of the Association for Computational Linguistics,
ACL-2006, pages 305?312, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22:457?479, De-
cember.
S. Fisher and B. Roark. 2008. Query-focused super-
vised sentence ranking for update summaries. In
Proceedings of the first Text Analysis Conference,
TAC-2008.
Dani Gamerman and Hedibert F. Lopes. 2006.
Markov Chain Monte Carlo: Stochastic Simulation
for Bayesian Inference. Chapman and Hall/CRC.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text docu-
ments: sentence selection and evaluation metrics.
In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ?99, pages
121?128, New York, NY, USA. ACM.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101(Suppl. 1):5228?5235, April.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 362?370. Association for Com-
putational Linguistics.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The
thu summarization systems at tac 2010. In Proceed-
ings of the third Text Analysis Conference, TAC-
2010.
Kevin Lerman and Ryan McDonald. 2009. Con-
trastive summarization: an experiment with con-
sumer reviews. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, NAACL-Short ?09, pages 113?116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xuan Li, Liang Du, and Yi-Dong Shen. 2011. Graph-
based marginal ranking for update summarization.
In Proceedings of the Eleventh SIAM International
Conference on Data Mining. SIAM / Omnipress.
Rebecca Mason and Eugene Charniak. 2011. Ex-
tractive multi-document summaries should explic-
itly not contain document-specific content. In Pro-
ceedings of the Workshop on Automatic Summariza-
tion for Different Genres, Media, and Languages,
WASDGML ?11, pages 49?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
A. Nenkova and L. Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling.
2008. Fast collapsed Gibbs sampling for latent
Dirichlet alcation. In KDD ?08: Proceeding of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 569?
577, New York, NY, USA, August. ACM.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys?,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Inf. Process. Manage.,
40:919?938, November.
Frank Schilder, Ravikumar Kondadadi, Jochen L. Lei-
dner, and Jack G. Conrad. 2008. Thomson reuters
at tac 2008: Aggressive filtering with fastsum for
update and opinion summarization. In Proceedings
of the first Text Analysis Conference, TAC-2008.
Chao Shen and Tao Li. 2010. Multi-document sum-
marization via the minimum dominating set. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
984?992, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ian Soboroff and Donna Harman. 2005. Novelty de-
tection: the trec experience. In Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT ?05, pages 105?112, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yi Wang. 2011. Distributed gibbs sampling of latent
dirichlet alcation: The gritty details.
Li Wenjie, Wei Furu, Lu Qin, and He Yanxiang. 2008.
Pnr2: ranking sentences with positive and nega-
tive reinforcement for query-oriented update sum-
marization. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics -
Volume 1, COLING ?08, pages 489?496, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
223
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54?59,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Pattern Learning for Relation Extraction with a Hierarchical Topic Model
Enrique Alfonseca Katja Filippova Jean-Yves Delort
Google Research
Brandschenkestrasse 110
8002 Zurich, Switzerland
{ealfonseca,katjaf,jydelort}@google.com
Guillermo Garrido?
NLP & IR Group, UNED
Juan del Rosal, 16.
28040 Madrid, Spain
ggarrido@lsi.uned.es
Abstract
We describe the use of a hierarchical topic
model for automatically identifying syntactic
and lexical patterns that explicitly state on-
tological relations. We leverage distant su-
pervision using relations from the knowledge
base FreeBase, but do not require any man-
ual heuristic nor manual seed list selections.
Results show that the learned patterns can be
used to extract new relations with good preci-
sion.
1 Introduction
The detection of relations between entities for the
automatic population of knowledge bases is very
useful for solving tasks such as Entity Disambigua-
tion, Information Retrieval and Question Answer-
ing. The availability of high-coverage, general-
purpose knowledge bases enable the automatic iden-
tification and disambiguation of entities in text
and its applications (Bunescu and Pasca, 2006;
Cucerzan, 2007; McNamee and Dang, 2009; Kwok
et al, 2001; Pasca et al, 2006; Weld et al, 2008;
Pereira et al, 2009; Kasneci et al, 2009).
Most early works in this area were designed
for supervised Information Extraction competitions
such as MUC (Sundheim and Chinchor, 1993) and
ACE (ACE, 2004; Doddington et al, 2004; Li et
al., 2011), which rely on the availability of anno-
tated data. Open Information Extraction (Sekine,
2006; Banko et al, 2007; Bollegala et al, 2010)
started as an effort to approach relation extraction in
?Work done during an internship at Google Zurich.
a completely unsupervised way, by learning regular-
ities and patterns from the web. Two example sys-
tems implementing this paradigm are TEXTRUN-
NER (Yates et al, 2007) and REVERB (Fader et al,
2011). These systems do not need any manual data
or rules, but the relational facts they extract are not
immediately disambiguated to entities and relations
from a knowledge base.
A different family of unsupervised methods for
relation extraction is unsupervised semantic pars-
ing, which aims at clustering entity mentions and
relation surface forms, thus generating a semantic
representation of the texts on which inference may
be used. Some techniques that have been used are
Markov Random Fields (Poon and Domingos, 2009)
and Bayesian generative models (Titov and Klemen-
tiev, 2011). These are quite powerful approaches
but have very high computational requirements (cf.
(Yao et al, 2011)).
A good trade-off between fully supervised and
fully unsupervised approaches is distant supervi-
sion, a semi-supervised procedure consisting of find-
ing sentences that contain two entities whose rela-
tion we know, and using those sentences as train-
ing examples for a supervised classifier (Hoffmann
et al, 2010; Wu and Weld, 2010; Hoffmann et al,
2011; Wang et al, 2011; Yao et al, 2011). A usual
problem is that two related entities may co-occur in
one sentence for many unrelated reasons. For ex-
ample, Barack Obama is the president of the United
States, but not every sentence including the two en-
tities supports and states this relation. Much of the
previous work uses heuristics, e.g. extracting sen-
tences only from encyclopedic entries (Mintz et al,
54
2009; Hoffmann et al, 2011; Wang et al, 2011), or
syntactic restrictions on the sentences and the entity
mentions (Wu and Weld, 2010). These are usually
defined manually and may need to be adapted to dif-
ferent languages and domains. Manually selected
seeds can also be used (Ravichandran and Hovy,
2002; Kozareva and Hovy, 2010).
The main contribution of this work is presenting
a variant of distance supervision for relation extrac-
tion where we do not use heuristics in the selection
of the training data. Instead, we use topic models to
discriminate between the patterns that are expressing
the relation and those that are ambiguous and can be
applied across relations. In this way, high-precision
extraction patterns can be learned without the need
of any manual intervention.
2 Unsupervised relational pattern learning
Similar to other distant supervision methods, our ap-
proach takes as input an existing knowledge base
containing entities and relations, and a textual cor-
pus. In this work it is not necessary for the corpus
to be related to the knowledge base. In what follows
we assume that all the relations studied are binary
and hold between exactly two entities in the knowl-
edge base. We also assume a dependency parser is
available, and that the entities have been automat-
ically disambiguated using the knowledge base as
sense inventory.
One of the most important problems to solve in
distant supervision approaches is to be able to dis-
tinguish which of the textual examples that include
two related entities, ei and ej , are supporting the re-
lation. This section describes a fully unsupervised
solution to this problem, computing the probability
that a pattern supports a given relation, which will
allow us to determine the most likely relation ex-
pressed in any sentence. Specifically, if a sentence
contains two entities, ei and ej , connected through a
pattern w, our model computes the probability that
the pattern is expressing any relation ?P (r|w)? for
any relation r defined in the knowledge base. Note
that we refer to patterns with the symbol w, as they
are the words in our topic models.
Preprocessing As a first step, the textual corpus
is processed and the data is transformed in the fol-
lowing way: (a) the input corpus is parsed and en-
Author-book
(Mark Twain, Adventures of Huckleberry
Finn)
ARG1
poss
,,
ARG2
ARG1
nn
,,
novels
nn
,,
ARG2
ARG1
nsubj
--
released ARG2dobj
qq
ARG2 ARG1
conj
rr
ARG1
nsubj
,,
wrote ARG2dobj
rr
ARG1
poss
,,
ARG2
...
(Jhumpa Lahiri, The Namesake)
ARG1
nn
--
ARG2
ARG2 by
prep
qq
ARG1
nn
uu
ARG1
nn
,,
novel
appos
--
ARG2
ARG2 by
prep
qq
ARG1
nn
uu
ARG2 by
prep
qq
ARG1
nn
uu
ARG1
poss
--
ARG1
...
(...)
Person-parent
(Liza Minneli, Judy Garland)
...
(Achilles, Peleus)
...
(...)
Person-death place
(Napoleon Bonaparte, Saint
Helena)
...
(Johann Christian Bach, Lon-
don)
...
(...)
Person-birth place
(Charles Darwin, Shrewsbury)
...
(Anthony Daniels, Salisbury)
...
(...)
Figure 1: Example of a generated set of document collec-
tions from a news corpus for relation extraction. Larger
boxes are document collections (relations), and inner
boxes are documents (entity pairs). Document contain
dependency patterns, which are words in the topic model.
tities are disambiguated; (b) for each relation r in
the knowledge base, a new (initially empty) docu-
ment collection Cr is created; (c) for each entity pair
(ei, ej) which are related in the knowledge base, a
new (initially empty) document Dij is created; (d)
for each sentence in the input corpus containing one
mention of ei and one mention of ej , a new term is
added to Dij consisting of the context in which the
two entities were seen in the document. This context
may be a complex structure, such as the dependency
path joining the two entities, but it is considered for
our purposes as a single term; (e) for each relation r
relating ei with ej , document Dij is added to collec-
tion Cr. Note that if the two entities are related in
different ways at the same time, an identical copy of
the document Dij will be added to the collection for
all those relations.
Figure 1 shows a set of document collections gen-
55
Figure 2: Plate diagram of the generative model used.
erated for three relations using this procedure. Each
relation r has associated a different document col-
lection, which contains one document associated to
each entity pair from the knowledge base which is
in relation r. The words in each document can be,
for example, all the dependency paths that have been
observed in the input textual corpus between the two
related entities. Each document will contain some
very generic paths (e.g. the two entities consecutive
in the text) and some more specific paths.
Generative model Once these collections are
built, we use the generative model from Figure 2
to learn the probability that a dependency path is
conveying some relation between the entities it con-
nects. This model is very similar to the one used
by Haghighi and Vanderwende (2009) in the con-
text of text summarization. w (the observed vari-
able) represents a pattern between two entities. The
topic model ?G captures general patterns that appear
for all relations. ?D captures patterns that are spe-
cific about a certain entity pair, but which are not
generalizable across all pairs with the same relation.
Finally ?A contains the patterns that are observed
across most pairs related with the same relation. ?A
is the topic model of interest for us.
We use Gibbs sampling to estimate the different
models from the source data. The topic assignments
(for each pattern) that are the output of this process
are used to estimate P (r|w): when we observe pat-
tern w, the probability that it conveys relation r.
3 Experiments and results
Settings We use Freebase as our knowledge base.
It can be freely downloaded1. text corpus used con-
tains 33 million English news articles that we down-
loaded between January 2004 and December 2011.
A random sample of 3M of them is used for building
the document collections on which to train the topic
models, and the remaining 30M is used for testing.
The corpus is preprocessed by identifying Freebase
entity mentions, using an approach similar to (Milne
and Witten, 2008), and parsing it with an inductive
dependency parser (Nivre, 2006).
From the three million training documents, a set
of document collections (one per relation) has been
generated, by considering the sentences that contain
two entities which are related in FreeBase through
any binary relation and restricting to high-frequency
200 relations. Two ways of extracting patterns have
been used: (a) Syntactic, taking the dependency
path between the two entities, and (b) Intertext,
taking the text between the two. In both cases, a
topic model has been trained to learn the probabil-
ity of a relation given a pattern w: p(r|w). For ?
we use symmetric Dirichlet priors ?G = 0.1 and
?D = ?A = 0.001, following the intuition that for
the background the probability mass across patterns
should be more evenly distributed. ? is set as (15,
15, 1), indicating in the prior that we expect more
patterns to belong to the background and entity-pair-
specific distributions due to the very noisy nature of
the input data. These values have not been tuned.
As a baseline, using the same training corpus, we
have calculated p(r|w) using the maximum likeli-
hood estimate: the number of times that a pattern w
has been seen connecting two entities for which r
holds divided by the total frequency of the pattern.
Extractions evaluation The patterns have been
applied to the 30 million documents left for testing.
For each pair of entities disambiguated as FreeBase
entities, if they are connected through a known pat-
tern, they are assigned argmaxr p(r|w). We have
randomly sampled 4,000 such extractions and sent
them to raters. An extraction is to be judged cor-
rect if both it is correct in real life and the sentence
from which it was extracted really supports it. We
1http://wiki.freebase.com/wiki/Data dumps
56
Figure 3: Evaluation of the extractions. X-axis has the threshold for p(r|w), and Y-axis has the precision of the extractions as a percentage.
have collected three ratings per example and taken
the majority decision. There was disagreement for
9.4% of the items on whether the sentence supports
the relation, and for 20% of the items on whether the
relation holds in the real world.
The results for different thresholds of p(r|w) are
shown in Figure 3. As can be seen, the MLE base-
lines (in red with syntactic patterns and green with
intertext) perform consistently worse than the mod-
els learned using the topic models (in pink and blue).
The difference in precision, aggregated across all re-
lations, is statistically significant at 95% confidence
for most of the thresholds.
Extractions aggregation We can take advantage
of redundancy on the web to calculate a support met-
ric for the extractions. In this experiment, for every
extracted relation (r, e1, e2), for every occurrence
of a pattern wi connecting e1 and e2, we add up
p(r|wi). Extractions that are obtained many times
and from high-precision patterns will rank higher.
Table 1 describes the results of this aggregation.
We have considered the top four highest-frequency
relations for people. After aggregating all the ex-
tracted relations and ranking them by support, we
have divided the evaluation set into two parts: (a)
for relations that were not already in FreeBase, we
evaluate the precision; (b) for extractions that were
already in FreeBase, we take the top-confidence sen-
tence identified and evaluate whether the sentence
is providing support to the relation. For each of
these, both syntactic patterns and intermediate-text
patterns have been evaluated.
The results are very interesting: using syntax,
Death place appears easy to extract new relations
and to find support. The patterns obtained are quite
unambiguous, e.g.
ARG1
subj
**
died at
prep
vv
home
pobj
ww
in
prep
uu
ARG2
pobj
ww
Relation Unknown relations Known relations
Correct relation P@50 Sentence support P@50
Syntax Intertext Syntax Intertext
Parent 0.58 0.38 1.00 1.00
Death place 0.90 0.68 0.98 0.94
Birth place 0.38 0.56 0.54 0.98
Nationality 0.86 0.78 0.34 0.40
Table 1: Evaluation on aggregated extractions.
On the other hand, birth place and nationality have
very different results for new relation acquisition
vs. finding sentence support for new relations. The
reason is that these relations are very correlated to
other relations that we did not have in our training
set. In the case of birth place, many relations re-
fer to having an official position in the city, such as
mayor; and for nationality, many of the patterns ex-
tract presidents or ministers. Not having mayor or
president in our initial collection (see Figure 1), the
support for these patterns is incorrectly learned. In
the case of nationality, however, even though the ex-
tracted sentences do not support the relation (P@50
= 0.34 for intertext), the new relations extracted are
mostly correct (P@50 = 0.86) as most presidents and
ministers in the real world have the nationality of the
country where they govern.
4 Conclusions
We have described a new distant supervision model
with which to learn patterns for relation extraction
with no manual intervention. Results are promising,
we could obtain new relations that are not in Free-
Base with a high precision for some relation types. It
is also useful to extract support sentences for known
relations. More work is needed in understanding
which relations are compatible or overlapping and
which ones can partially imply each other (such as
president-country or born in-mayor).
57
References
ACE. 2004. The automatic content extraction projects.
http://projects.ldc.upenn.edu/ace.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI?07.
D.T. Bollegala, Y. Matsuo, and M. Ishizuka. 2010. Rela-
tional duality: Unsupervised extraction of semantic re-
lations between entities on the web. In Proceedings of
the 19th international conference on World wide web,
pages 151?160. ACM.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 2007, pages 708?716.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The automatic
content extraction (ace) program?tasks, data, and eval-
uation. In Proceedings of LREC, volume 4, pages
837?840. Citeseer.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identify-
ing relations for open information extraction. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370. Association for Computational Lin-
guistics.
R. Hoffmann, C. Zhang, and D.S. Weld. 2010. Learning
5000 relational extractors. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 286?295. Association for Computa-
tional Linguistics.
R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and
D.S. Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 541?550. Asso-
ciation for Computational Linguistics.
G. Kasneci, M. Ramanath, F. Suchanek, and G. Weikum.
2009. The yago-naga approach to knowledge discov-
ery. ACM SIGMOD Record, 37(4):41?47.
Z. Kozareva and E. Hovy. 2010. Learning arguments
and supertypes of semantic relations using recursive
patterns. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1482?1491. Association for Computational Lin-
guistics.
C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling
question answering to the web. ACM Transactions on
Information Systems (TOIS), 19(3):242?262.
D. Li, S. Somasundaran, and A. Chakraborty. 2011. A
combination of topic models with max-margin learn-
ing for relation detection.
P. McNamee and H.T. Dang. 2009. Overview of the tac
2009 knowledge base population track. In Text Analy-
sis Conference (TAC).
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM conference
on Information and knowledge management, pages
509?518. ACM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2-Volume 2, pages 1003?
1011. Association for Computational Linguistics.
J. Nivre. 2006. Inductive dependency parsing. In
Text, Speech and Language Technology, volume 34.
Springer Verlag.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts-step one: the one-million fact extraction chal-
lenge. In Proceedings of the National Conference on
Artificial Intelligence, page 1400. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
F. Pereira, A. Rajaraman, S. Sarawagi, W. Tunstall-
Pedoe, G. Weikum, and A. Halevy. 2009. An-
swering web questions using structured data: dream
or reality? Proceedings of the VLDB Endowment,
2(2):1646?1646.
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 1-Volume 1, pages 1?10. Association for
Computational Linguistics.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 41?47. Associa-
tion for Computational Linguistics.
S. Sekine. 2006. On-demand information extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731?738. Association for Com-
putational Linguistics.
Beth M. Sundheim and Nancy A. Chinchor. 1993. Sur-
vey of the message understanding conferences. In
HLT?93.
58
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics.
C. Wang, J. Fan, A. Kalyanpur, and D. Gondek. 2011.
Relation extraction with relation topics. In Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing.
Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amershi,
James Fogarty, Raphael Hoffmann, Kayur Patel, and
Michael Skinner. 2008. Intelligence in wikipedia. In
Proceedings of the 23rd national conference on Artifi-
cial intelligence, pages 1609?1614. AAAI Press.
F. Wu and D.S. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127. Association for Computational
Linguistics.
L. Yao, A. Haghighi, S. Riedel, and A. McCallum. 2011.
Structured relation discovery using generative models.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-
head, and S. Soderland. 2007. Textrunner: Open in-
formation extraction on the web. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
59
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1243?1253,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
HEADY: News headline abstraction through event pattern clustering
Enrique Alfonseca
Google Inc.
ealfonseca@google.com
Daniele Pighin
Google Inc.
biondo@google.com
Guillermo Garrido?
NLP & IR Group at UNED
ggarrido@lsi.uned.es
Abstract
This paper presents HEADY: a novel, ab-
stractive approach for headline generation
from news collections. From a web-scale
corpus of English news, we mine syntac-
tic patterns that a Noisy-OR model gener-
alizes into event descriptions. At inference
time, we query the model with the patterns
observed in an unseen news collection,
identify the event that better captures the
gist of the collection and retrieve the most
appropriate pattern to generate a head-
line. HEADY improves over a state-of-the-
art open-domain title abstraction method,
bridging half of the gap that separates
it from extractive methods using human-
generated titles in manual evaluations, and
performs comparably to human-generated
headlines as evaluated with ROUGE.
1 Introduction
Motivation. News events are rarely reported
only in one way, from a single point of view. Dif-
ferent news agencies will interpret the event in dif-
ferent ways; various countries or locations may
highlight different aspects of it depending on how
they are affected; and opinions and in-depth anal-
yses will be written after the fact.
The variety of contents and styles is both an op-
portunity and a challenge. On the positive side, we
have the same events described in different ways;
this redundancy is useful for summarization, as
the information content reported by the majority
of news sources most likely represents the central
part of the event. On the other hand, variability
and subjectivity can be difficult to isolate. For
some applications it is important to understand,
given a collection of related news articles and re-
?Work done during an internship at Google Zurich.
? Carmelo and La La Party It Up with Kim and Ciara
? La La Vazquez and Carmelo Anthony: Wedding
Day Bliss
? Carmelo Anthony, actress LaLa Vazquez wed in
NYC
? Stylist to the Stars
? LaLa, Carmelo Set Off Celebrity Wedding Weekend
? Ciara rocks a sexy Versace Spring 2010 mini to
LaLa Vasquez and Carmelo Anthony?s wedding
(photos)
? Lala Vasquez on her wedding dress, cake, reality tv
show and fiance?, Carmelo Anthony (video)
? VAZQUEZ MARRIES SPORTS STAR AN-
THONY
? Lebron Returns To NYC For Carmelo?s Wedding
? Carmelo Anthony?s stylist dishes on the wedding
? Paul pitching another Big Three with ?Melo in
NYC?
? Carmelo Anthony and La La Vazquez Get Married
at Star-Studded Wedding Ceremony
Table 1: Headlines observed for a news collection
reporting the same wedding event.
ports, how to formulate in an objective way what
has happened.
As a motivating example, Table 1 shows the dif-
ferent headlines observed in news reporting the
wedding between basketball player Carmelo An-
thony and actress LaLa Vazquez. As can be seen,
there is a wide variety of ways to report the same
event, including different points of view, high-
lighted aspects, and opinionated statements on the
part of the reporter. When presenting this event to
a user in a news-based information retrieval or rec-
ommendation system, different event descriptions
may be more appropriate. For example, a user may
only be interested in objective, informative sum-
maries without any interpretation on the part of
the reporter. In this case, Carmelo Anthony, ac-
1243
tress LaLa Vazquez wed in NYC would be a good
choice.
Goal. Our final goal in this research is to build a
headline generation system that, given a news col-
lection, is able to describe it with the most com-
pact, objective and informative headline. In par-
ticular, we want the system to be able to:
? Generate headlines in an open-domain, unsu-
pervised way, so that it does not need to rely
on training data which is expensive to pro-
duce.
? Generalize across synonymous expressions
that refer to the same event.
? Do so in an abstractive fashion, to enforce
novelty, objectivity and generality.
In order to advance towards this goal, this paper
explores the following questions:
? What is a good way of using syntactic pat-
terns to represent events for generating head-
lines?
? Can we have satisfactory readability with an
open-domain abstractive approach, not rely-
ing on training data nor on manually pre-
defined generation templates?
? How far can we get in terms of informative-
ness, compared to the human-produced head-
lines, i.e., extractive approaches?
Contributions. In this paper we present
HEADY, which is at the same time a novel system
for abstractive headline generation, and a smooth
clustering of patterns describing the same events.
HEADY is fully open-domain and can scale to
web-sized data. By learning to generalize events
across the boundaries of a single news story or
news collection, HEADY produces compact and
effective headlines that objectively convey the
relevant information.
When compared to a state-of-the-art open-
domain headline abstraction system (Filippova,
2010), the new headlines are statistically signifi-
cantly better both in terms of readability and in-
formativeness. Also, automatic evaluations using
ROUGE, having objective headlines for the news
as references, show that the abstractive headlines
are on par with human-produced headlines.
2 Related work
Headline generation and summarization.
Most headline generation work in the past has
focused on the problem of single-document sum-
marization: given the main passage of a single
news article, generate a very short summary of
the article. From early in the field, it was pointed
out that a purely extractive approach is not good
enough to generate headlines from the body
text (Banko et al, 2000). Sometimes the most
important information is distributed across several
sentences in the document. More importantly,
quite often, the single sentence selected as the
most informative for the news collection is already
longer than the desired headline size. For this
reason, most early headline generation work fo-
cused on either extracting and reordering n-grams
from the document to be summarized (Banko et
al., 2000), or extracting one or two informative
sentences from the document and performing
linguistically-motivated transformations to them
in order to reduce the summary length (Dorr et
al., 2003). The first approach is not guaranteed
to produce grammatical headlines, whereas the
second approach is tightly tied to the actual
wording found in the document. Single-document
headline generation was also explored at the
Document Understanding Conferences between
2002 and 20041.
In later years, there has been more interest in
problems such as sentence compression (Galley
and McKeown, 2007; Clarke and Lapata, 2008;
Cohn and Lapata, 2009; Napoles et al, 2011;
Berg-Kirkpatrick et al, 2011), text simplification
(Zhu et al, 2010; Coster and Kauchak, 2011;
Woodsend and Lapata, 2011) and sentence fu-
sion (Barzilay and McKeown, 2005; Wan et al,
2007; Filippova and Strube, 2008; Elsner and San-
thanam, 2011). All of them have direct applica-
tions for headline generation, as it can be con-
strued as selecting one or a few sentences from
the original document(s), and then reducing them
to the target title size. For example, Wan et al
(2007) generate novel utterances by combining
Prim?s maximum-spanning-tree algorithm with an
n-gram language model to enforce fluency. Un-
like HEADY, the method by Wan and colleagues
is an extractive method that can summarize single
documents into a sentence, as opposed to generat-
ing a sentence that can stand for a whole collec-
1http://duc.nist.gov/
1244
tion of news. Filippova (2010) reports a system
that is very close to our settings: the input is a
collection of related news articles, and the system
generates a headline that describes the main event.
This system uses sentence compression techniques
and benefits from the redundancy in the collection.
One difference with respect to HEADY is that it
does not use any syntactic information aside from
part-of-speech tags, and it does not require a train-
ing step. We have used this approach as a baseline
for comparison.
There are not many fully abstractive systems for
news summarization. The few that exist, such as
the work by Genest and Lapalme (2012), rely on
manually written generation templates. In con-
trast, HEADY automatically learns the templates
or headline patterns automatically, which allows it
to work in open-domain settings without relying
on supervision or manual annotations.
Open-domain pattern learning. Pattern learn-
ing for relation extraction is an active area of re-
search that is very related to our problem of event
pattern learning for headline generation. TextRun-
ner (Yates et al, 2007), ReVerb (Fader et al, 2011)
and NELL (Carlson et al, 2010; Mohamed et al,
2011) are some examples of open-domain systems
that learn surface patterns that express relations
between pairs of entities. PATTY (Nakashole et
al., 2012) generalizes the patterns to also include
syntactic information and ontological (class mem-
bership) constraints. Our patterns are more similar
to the ones used by PATTY, which also produces
clusters of synonymous patterns. The main differ-
ences are that (a) HEADY is not limited to con-
sider patterns expressing relations between pairs
of entities; (b) we identify synonym patterns us-
ing a probabilistic, Bayesian approach that takes
advantage of the multiplicity of news sources re-
porting the same events. Chambers and Jurafsky
(2009) present an unsupervised method for learn-
ing narrative schemas from news, i.e., coherent
sets of events that involve specific entity types (se-
mantic roles). Similarly to them, we move from
the assumptions that 1) utterances involving the
same entity types within the same document (in
our case, a collection of related documents) are
likely describing aspects of the same event, and
2) meaningful representations of the underlying
events can be learned by clustering these utter-
ances in a principled way.
Noisy-OR networks. Noisy-OR Bayesian net-
works (Pearl, 1988) have been applied in the
past to a wide class of large-scale probabilis-
tic inference problems, from the medical do-
main (Middleton et al, 1991; Jaakkola and Jor-
dan, 1999; Onisko et al, 2001), to synthetic
image-decomposition and co-citation data analy-
sis (S?ingliar and Hauskrecht, 2006). By assum-
ing independence between the causes of the hid-
den variables, noisy-OR models tend to be reli-
able (Friedman and Goldszmidt, 1996) as they re-
quire a relatively small number of parameters to
be estimated (linear with the size of the network).
3 Headline generation
In this section, we describe the HEADY system for
news headline abstraction. Our approach takes as
input, for training, a corpus of news articles or-
ganized in news collections. Once the model is
trained, it can generate headlines for new collec-
tions. An outline of HEADY?s main components
follows (details of each component are provided
in Sections 3.1, 3.2 and 3.3):
Pattern extraction. Identify, in each of the news
collections, syntactic patterns connecting k enti-
ties, for k ? 1. These will be the candidate pat-
terns expressing events.
Training. Train a Noisy-OR Bayesian network
on the co-occurrence of syntactic patterns. Each
pattern extracted in the previous step is added as
an observed variable, and latent variables are used
to represent the hidden events that generate pat-
terns. An additional noise variable links to every
terminal node, allowing every terminal to be gen-
erated by language background (noise) instead of
by an actual event.
Inference. Generate a headline from an unseen
news collection. First, patterns are extracted using
the pattern extraction procedure mentioned above.
Given the patterns, the posterior probability of the
hidden event variables is estimated. Then, from
the activated hidden events, the likelihood of ev-
ery pattern can be estimated, even if they do not
appear in the collection. The single pattern with
the maximum probability is selected to generate a
new headline from it. Being the product of extra-
news collection generalization, the retrieved pat-
tern is more likely to be objective and informative
than patterns directly observed in the news collec-
tion.
1245
Algorithm 1 COLLECTIONTOPATTERNS?(N ):
N is a repository of news collections, ? is a set
of parameters controlling the extraction process.
R ? {}
for all N ? N do
PREPROCESSDATA(N)
E ? GETRELEVANTENTITIES(N ?)
for all Ei ? COMBINATIONS?(E) do
for all n ? N do
P ? EXTRACTPATTERNS?(n, Ei)
R{N,Ei} ? R{N,Ei} ? P
returnR
3.1 Pattern extraction
In this section we detail the process for obtain-
ing the event patterns that constitute the building
blocks of learning and inference.
Patterns are extracted from a large repository
N of news collections N1, . . . , N|N |. Each news
collection N = {ni} is an unordered collec-
tion of related news, each of which can be seen
as an ordered sequence of sentences, i.e.: n =
[s0, . . . s|n|].
Algorithm 1 presents a high-level view of the
pattern extraction process. The different steps are
described below:
PREPROCESSDATA: We start by preprocess-
ing all the news in the news collections with a
standard NLP pipeline: tokenization and sentence
boundary detection (Gillick, 2009), part-of-speech
tagging, dependency parsing (Nivre, 2006), co-
reference resolution (Haghighi and Klein, 2009)
and entity linking based on Wikipedia and Free-
base. Using the Freebase dataset, each entity is
annotated with all its Freebase types (class labels).
In the end, for each entity mentioned in the docu-
ment we have a unique identifier, a list with all its
mentions in the document and a list of class labels
from Freebase.
As a result of this process, we obtain for each
sentence in the corpus a representation as exem-
plified in Figure 1 (1). In this example, the men-
tions of three distinct entities have been identified,
i.e., e1, . . . , e3. In the Freebase list of types (class
labels), e1 is a person and a celebrity, and e3 is a
state and a location.
GETRELEVANTENTITIES: For each news col-
lection N we collect the set E of the entities men-
tioned most often within the collection. Next, we
generate the set COMBINATIONS?(E) consisting
NNP CC NNP TO VB IN NNP
Portia and Helen to marry in California
e1 e2 e3
person actress statecelebrity location
root
cc conj
nsubj
aux prep pobj
1
NNP NNP
e1 e2
person actresscelebrity
conj 2
NNP CC NNP TO VB
e1 and e2 to marry
person actresscelebrity
cc conj
nsubj
aux
3
NNP CC NNP TO VBperson and actress to marry
cc conj
nsubj
aux
4
NNP CC NNP TO VBcelebrity and actress to marry
cc conj
nsubj
aux
Figure 1: Pattern extraction process from an anno-
tated dependency parse. (1): an MST is extracted
from the entity pair e1, e2 (2); nodes are heuristi-
cally added to the MST to enforce grammaticality
(3); entity types are recombined to generate the fi-
nal patterns (4).
of non-empty subsets of E, without repeated en-
tities. The number of entities to consider in each
collection, and the maximum size for the subsets
of entities to consider are meta-parameters embed-
ded in ?.2
EXTRACTPATTERNS: For each subset of rel-
evant entities Ei, event patterns are mined from
the articles in the news collection. The process
by which patterns are extracted from a news is
explained in Algorithm 2 and exemplified graphi-
cally in Figure 1 (2?4).
GETMENTIONNODES: Using the dependency
parse T for a sentence s, we first identify the set
of nodes Mi that mention the entities in Ei. If
T does not contain exactly one mention of each
target entity in Ei, then the sentence is ignored.
Otherwise, we obtain the minimum spanning tree
for the nodeset Pi, i.e., the shortest path in the de-
pendency tree connecting all the nodes inMi (Fig-
ure 1, 2). Pi is the set of nodes around which the
patterns will be constructed.
APPLYHEURISTICS: With very high probabil-
ity, the MST Pi that we obtain does not constitute
a grammatical or useful extrapolation of the origi-
nal sentence s. For example, the MST for the en-
2As our objective is to generate very short titles (under
10 words), we only consider combinations of up to three ele-
ments of E.
1246
Algorithm 2 EXTRACTPATTERNS?(n, Ei): n is
the list of sentences in a news article. Sentences
are POS-tagged, dependency parsed and annotated
with respect to a set of entities E ? Ei
P ? ?
for all s ? n[0 : 2) do
T ? DEPPARSE(s)
Mi ? GETMENTIONNODES(t, Ei)
if ?e ? Ei, count(e,Mi) 6= 1 then continue
Pi ? GETMINIMUMSPANNINGTREE?(Mi)
APPLYHEURISTICS?(Pi) or continue
P ? P ? COMBINEENTITYTYPES?(Pi)
return P
tity pair ?e1, e2? in the example does not provide a
good description of the event as it is neither ade-
quate nor fluent. For this reason, we apply a set of
post-processing heuristic transformations that aim
at including a minimal set of meaningful nodes.
These include making sure that both the root of the
clause and its subject appear in the extracted pat-
tern, and that conjunctions between entities should
not be dropped (Figure 1, 3).
COMBINEENTITYTYPES: Finally, a distinct
pattern is generated from each possible combina-
tion of entity type assignments for the participat-
ing entities. (Figure 1, 4).
It is important to note that both at training and
test time, for pattern extraction we only consider
the title and the first sentence of the article body.
The reason is that we want to limit ourselves, in
each news collection, to the most relevant event
reported in the collection, which appears most of
the times in these two sentences. Unlike titles, first
sentences do not extensively use puns or rhetorics
as they tend to be grammatical and informative
rather than catchy.
The patterns mined from the same news collec-
tion and for the same set of entities are grouped
together, and constitute the building blocks of the
clustering algorithm which is described below.
3.2 Training
The extracted patterns are used to learn a Noisy-
OR (Pearl, 1988) model by estimating the prob-
ability that each (observed) pattern activates one
or many (hidden) events. Figure 2 represents the
two levels: the hidden event variables at the top,
and the observed pattern variables at the bottom.
An additional noise variable links to every termi-
e1 ... en noise
p3p2p1 ... pm
Figure 2: Probabilistic model. The associations
between latent event variables and observed pat-
tern variables are modeled by noisy-OR gates.
Events are assumed to be marginally independent,
and patterns conditionally independent given the
events.
nal node, allowing all terminals to be generated by
language background (noise) instead of by an ac-
tual event. The associations between latent events
and observed patterns are modeled by noisy-OR
gates.
In this model, the conditional probability of a
hidden event ei given a configuration of observed
patterns p ? {0, 1}|P| is calculated as:
P (ei = 0 | p) = (1? qi0)
?
j?pij
(1? qij)pj
= exp
?
???i0 ?
?
j?pii
?ijpj
?
? ,
where pii is the set of active events (i.e., pii =
?j{pj} | pj = 1), and qij = P (ei = 1 | pj = 1)
is the estimated probability that the observed pat-
tern pi can, in isolation, activate the event e. The
term qi0 is the so-called ?noise? term of the model,
and it accounts for the fact that an observed event
ei might be activated by some pattern that has
never been observed (Jaakkola and Jordan, 1999).
In Algorithm 1, at the end of the process we
group in R[N,Ei] all the patterns extracted from
the same news collection N and entity sub-set Ei.
These groups represent rough clusters of patterns,
that we can use to bootstrap the optimization of
the model parameters ?ij = ? log(1 ? qij). We
initiate the training process by randomly selecting
100,000 of these groups, and optimize the weights
of the model through 40 EM (Dempster et al,
1977) iterations.
3.3 Inference (generation of new headlines)
Given an unseen news collection N , the inference
component of HEADY generates a single headline
that captures the main event reported by the news
in N . In order to do so, we first need to select a
1247
single event-pattern p? that is especially relevant
for N . Having selected p?, in order to generate a
headline it is sufficient to replace the entity place-
holders in p? with the surface forms observed in
N .
To identify p?, we start from the assumption that
the most descriptive event encoded by N must de-
scribe an important situation in which some subset
of the relevant entities E in N are involved.
The basic inference algorithm is a two-
step random walk in the Bayesian network.
Given a set of entities E and sentences n,
EXTRACTPATTERNS?(n, E) collects patterns in-
volving those entities. By normalizing the fre-
quency of the extracted patterns, we get a prob-
ability distribution over the observed variables in
the network. A two-step random walk traversing
to the latent event nodes and back to the pattern
nodes allows us to generalize across events. We
call this algorithm INFERENCE(n, E).
In order to decide which is the most relevant set
of events that should appear in the headline, we
use the following procedure:
1. Given the set of entities E mentioned in the
news collection, we consider each entity sub-
set Ei ? E including up to three entities3.
For each Ei, we run INFERENCE(n, Ei),
which computes a distribution wi over pat-
terns involving the entities in Ei.
2. We invoke again INFERENCE, now using at
the same time all the patterns extracted for
every subset of Ei ? E. This computes a
probability distribution w over all patterns in-
volving any admissible subset of the entities
mentioned in the collection.
3. Third, we select the entity-specific distribu-
tion that approximates better the overall dis-
tribution
w? = arg max
i
cos(w,wi)
We assume that the corresponding set of en-
tities Ei are the most central entities in the
collection and therefore any headline should
make sure to mention them all.
3As we noted before, we impose this limitation to keep the
generated headlines relatively short and to limit data sparsity
issues.
4. Finally, we select the pattern with the highest
weight in w? as the pattern that better cap-
tures the main event reported in the news col-
lection:
p? = pj | wj = arg maxj w
?
j
The headline is then produced from p?, replac-
ing placeholders with the entities in the document
from which the pattern was extracted.
While in many cases information about entity
types would be sufficient to decide about the or-
der of the entities in the generated sentences (e.g.,
?[person] married in [location]? for the entity
set {ea = ?Mr. Brown?, eb = ?Los Angeles?}),
in other cases class assignment can be ambigu-
ous (e.g., ?[person] killed [person]? for {ea =
?Mr. A?, eb = ?Mr. B?}). To handle these cases,
when extracting patterns for an entity set {ea, eb},
we keep track of the alphabetical ordering of
the entities, e.g., from a news collection about
?Mr. B? killing ?Mr. A? we would produce
patterns such as ?[person:2] killed [person:1]? or
?[person:1] was killed by [person:2]? since ea =
?Mr. A? < eb = ?Mr. B?. At inference time,
when we query the model with such patterns we
can only activate events whose assignments are
compatible with the entities observed in the text,
making the replacement straightforward and un-
ambiguous.
4 Experiment settings
In our method we use patterns that are fully lex-
icalized (with the exception of entity placehold-
ers) and enriched with syntactic data. Under these
circumstances, the Noisy-OR can effectively gen-
eralize and learn meaningful clusters only if pro-
vided with large amounts of data. To our best
knowledge, available data sets for headline gen-
eration are not large enough to support this kind
of inference.
For this reason, we rely on a corpus of news
crawled from the web between 2008 and 2012
which have been clustered based on closeness in
time and cosine similarity, using the vector-space
model and tf.idf weights. News collections with
less than 5 documents are discarded4, and those
4There is a very long tail of singleton articles, which do
not offer useful examples of lexical or syntactic variation, and
many very small collections that tend to be especially noisy,
hence the decision to consider only collections with at least 5
documents.
1248
larger than 50 documents are capped, by randomly
picking 50 documents from the collection5. The
total number of news collections after clustering
is 1.7 million. From this set, we have set aside
a few hundred collections that will remain unseen
until the final evaluation.
As we have no development set, we have done
no tuning of the parameters for pattern extraction
nor for the Bayesian network training (100,000 la-
tent variables to represent the different events, 40
EM iterations, as mentioned in Section 3.2). The
EM iterations on the noisy-OR were distributed
across 30 machines with 16 GB of memory each.
4.1 Systems used
One of the questions we wanted to answer in
this research was whether it was possible to ob-
tain the same quality with automatically abstracted
headlines as with human-generated headlines. For
every news collection we have as many human-
generated headlines as documents. To decide
which human-generated headline should be used
in this comparison, we used three different meth-
ods that pick one of the collection headlines:
? Latest headline: selects the headline from
the latest document in the collection. Intu-
itively this should be the most relevant one
for news about sport matches and competi-
tions, where the earlier headlines offer pre-
views and predictions, and the later headlines
report who won and the final scores.
? Most frequent headline: some headlines
are repeated across the collection, and this
method chooses the most frequent one. If
there are several with the same frequency,
one is taken at random6.
? TopicSum: we use TopicSum (Haghighi and
Vanderwende, 2009), a 3-layer hierarchical
topic model, to infer the language model that
is most central for the collection. The news
title that has the smallest Kullback-Leibler
5Even though we did not run any experiment to find an
optimal value for this parameter, 50 documents seems like
a reasonable choice to avoid redundancy while allowing for
considerable lexical and syntactic variation.
6The most frequent headline only has a tie in 6 collections
in the whole test set. In 5 cases two headlines are tied at fre-
quencies around 4, and in one case three headlines are tied at
frequency 2. All six are large collections with 50 news arti-
cles, so this baseline is significantly different from a random
baseline.
R-1 R-2 R-SU4
HEADY 0.3565 0.1903 0.1966
Most frequent pattern 0.3560 0.1864 0.1959
TopicSum 0.3594 0.1821 0.1935
MSC 0.3470 0.1765 0.1855
Most frequent headline 0.3177 0.1401 0.1668
Latest headline 0.2814 0.1191 0.1425
Table 2: Results from the automatic evaluation,
sorted according to the ROUGE-2 and ROUGE-
SU4 scores.
divergence with respect the collection lan-
guage model is the one chosen.
A headline generation system that addresses
the same application as ours is (Filippova, 2010),
which generates a graph from the collection sen-
tences and selects the shortest path between the
begin and the end node traversing words in the
same order in which they were found in the orig-
inal documents. We have used this system, called
Multi-Sentence Compression (MSC), for compar-
isons.
Finally, in order to understand whether the
noisy-OR Bayesian network is useful for general-
izing across patterns into latent events, we added a
baseline that extracts all patterns from the test col-
lection following the same COLLECTIONTOPAT-
TERNS algorithm (including the application of the
linguistically motivated heuristics), and then pro-
duces a headline straightaway from the most fre-
quent pattern extracted. In other words, the only
difference with respect to HEADY is that in this
case no generalization through the Noisy-OR net-
work is carried out, and that headlines are gen-
erated from patterns directly observed in the test
news collections. We call this system Most fre-
quent pattern.
4.2 Annotation activities
In order to evaluate HEADY?s performance, we
carried out two annotation activities.
First, from the set of collections that we had
set aside at the beginning, we randomly chose 50
collections for which all the systems could gen-
erate an output, and we asked raters to manually
write titles for them. As this is not a simple task
to be crowdsourced, for this evaluation we relied
on eight trained raters. We collected between four
and five reference titles for each of the fifty news
collections, to be used to compare the headline
1249
Readability Informativeness
TopicSum 4.86 4.63
Most freq. headline ??4.61 ??34.43
Latest headline ??4.55 ? 4.00
HEADY ? 4.28 ? 3.75
Most freq. pattern ? 3.95 ? 3.82
MSC 3.00 3.05
Table 3: Results from the manual evaluation. At
95% confidence, TopicSum is significantly better
than all others for readability, and only indistin-
guishable from the most frequent pattern for in-
formativeness. For the rest, 3 means being signifi-
cantly better than HEADY, ? than the most frequent
pattern, and ? than MSC.
generation methods using automatic summariza-
tion metrics.
Then, we took the output of the systems for the
50 test collections and asked human raters to eval-
uate the headlines:
1. Raters were shown one headline and asked to
rate it in terms of readability on a 5-point
Likert scale. In the instructions, the raters
were provided with examples of ungrammat-
ical and grammatical titles to guide them in
this annotation.
2. After the previous rating is done, raters were
shown a selection of five documents from the
collection, and they were asked to judge the
informativeness of the previous headline for
the news in the collection, again on a 5-point
Likert scale.
This second annotation was carried out by inde-
pendent raters in a crowd-sourcing setting. The
raters did not have any involvement with the in-
ception of the model or the writing of the pa-
per. They did not know that the headlines they
were rating were generated according to differ-
ent methods. We measured inter-judge agreement
on the Likert-scale annotations using their Intra-
Class Correlation (ICC) (Cicchetti, 1994). The
ICC for readability is 0.76 (0.95 confidence in-
terval [0.71, 0.80]), and for informativeness it is
0.67 (0.95 confidence interval [0.60, 0.73]). This
means strong agreement for readability, and mod-
erate agreement for informativeness.
5 Results
The COLLECTIONTOPATTERNS algorithm was
run on the training set, producing a 230 million
event patterns. Patterns that were obtained from
the same collection and involving the same entities
were grouped together, for a total of 1.7 million
pattern collections. The pattern groups are used to
bootstrap the Noisy-OR model training. Training
the HEADY model that we used for the evaluation
took around six hours on 30 cores.
Table 2 shows the results of the comparison
of the headline generation systems using ROUGE
(R-1, R-2 and R-SU4) (Lin, 2004) with the col-
lected references. According to Owczarzak et
al. (2012), ROUGE is still a competitive met-
ric that correlates well with human judgements
for ranking summarizers. The significance tests
for ROUGE are performed using bootstrap resam-
pling and a graphical significance test (Minka,
2002). The human annotators that created the
references for this evaluation were explicitly in-
structed to write objective titles, which is the kind
of headlines that the abstractive systems aim at
generating. It is common to see real headlines
that are catchy, joking, or with a double mean-
ing, and therefore they use a different vocabulary
than objective titles that simply mention what hap-
pened. TopicSum sometimes selects objective ti-
tles amongst the human-made titles and that is
why it also scores very well with the ROUGE
scores. But the other two criteria for choosing
human-made headlines select non-objective titles
much more often, and this lowers their perfor-
mance when measured with ROUGE with respect
to the objective references.
Table 3 lists the results of the manual evaluation
of readability and informativeness of the generated
headlines. The first result that we can see is the
difference in the rankings between the two evalu-
ations. Part of this difference might be due to the
fact that ROUGE is not as good for discriminating
between human-made and automatic summaries.
In fact, in the DUC competitions, the gap between
human summaries and automatic summaries was
also more apparent in the manual evaluations than
using ROUGE. Another part of the observed dif-
ference may be due to the design of the evalua-
tion. The manual evaluation is asking raters to
judge whether real, human-written titles that were
actually used for those news are grammatical and
informative. As could be expected, as these are
published titles, the real titles score very good on
the manual evaluation.
Some other interesting results are:
1250
Model Generated title
TopicSum Modern Family?s Eric Stonestreet laughs off
Charlize Theron rumours
MSC Modern Family star Eric Stonestreet is dating
Charlize Theron.
Latest headline Eric laughs off Theron dating rumours
Frequent pattern Eric Stonestreet jokes about Charlize relationship
Frequent headline Charlize Theron dating Modern Family star
HEADY Eric Stonestreet not dating Charlize Theron
TopicSum McFadzean rescues point for Crawley Town
MSC Crawley side challenging for a point against Old-
ham Athletic.
Latest headline Reds midfielder victim of racist tweet
Frequent pattern Kyle McFadzean fired a equaliser Crawley were
made
Frequent headline Latics halt Crawley charge
HEADY Kyle McFadzean rescues point for Crawley Town
F.C.
TopicSum UCI to strip Lance Armstrong of his 7 Tour titles
MSC The international cycling union said today.
Latest headline Letters: elderly drivers and Lance Armstrong
Frequent pattern Lance Armstrong stripped of Tour de France ti-
tles
Frequent headline Today in the news: third debate is tonight
HEADY Lance Armstrong was stripped of Tour de France
titles
Table 4: A comparison of the titles generated by
the different models for three news collections.
? Amongst the automatic systems, HEADY per-
formed better than MSC, with statistical sig-
nificance at 95% for all the metrics. Head-
lines based on the most frequent patterns
were better than MSC for all metrics but
ROUGE-2.
? The most frequent pattern baseline and
HEADY have comparable performance across
all the metrics (not statistically significantly
different), although HEADY has slightly bet-
ter scores for all metrics except for informa-
tiveness.
While we do not take any step to explicitly
model stylistic variation, estimating the weights
of the Noisy-OR network turns out to be a very
effective way of filtering out sensational wording
to the advantage of plainer, more objective style.
This may not clearly emerge from the evaluation,
as we did not explicitly ask the raters to annotate
the items based on their objectivity, but a manual
inspection of the clusters suggests that the gener-
alization is working in the right direction.
Table 4 presents a selection of outputs produced
by the six models for three different news collec-
tions. The first example shows a news collection
containing news about a rumour that was imme-
diately denied. In the second example, HEADY
generalization improves over the most frequent
pattern. In the third case, HEADY generates a
good title from a noisy collection (containing dif-
ferent but related events). The examples also
show that TopicSum is very effective in selecting
a good human-generated headline for each collec-
tion. This opens the possibility of using TopicSum
to automatically generate ROUGE references for
future evaluations of abstractive methods.
6 Conclusions
We have presented HEADY, an abstractive head-
line generation system based on the generaliza-
tion of syntactic patterns by means of a Noisy-OR
Bayesian network. We evaluated the model both
automatically and through human annotations.
HEADY performs significantly better than a state-
of-the-art open domain abstractive model (Filip-
pova, 2010) in all evaluations, and is in par with
human-generated headlines in terms of ROUGE
scores. We have shown that it is possible to
achieve high quality generation of news headlines
in an open-domain, unsupervised setting by suc-
cessfully exploiting syntactic and ontological in-
formation. The system relies on a standard NLP
pipeline, requires no manual data annotation and
can effectively scale to web-sized corpora.
For feature work, we plan to improve all compo-
nents of HEADY in order to fill in the gap with the
human-generated titles in terms of readability and
informativeness. One of the directions in which
we plan to move is the removal of the syntac-
tic heuristics that currently enforce pattern well-
formedness and to automatically learn the neces-
sary transformations from the data.
Two other lines of work that we plan to explore
are the possibility of personalizing the headlines
to user interests (as stored in user profiles or ex-
pressed as user queries), and to investigate further
applications of the Bayesian network of event pat-
terns, such as its use for relation extraction and
knowledge base population.
Acknowledgments
The research leading to these results has received
funding from: the EU?s 7th Framework Pro-
gramme (FP7/2007-2013) under grant agreement
number 257790; the Spanish Ministry of Science
and Innovation?s project Holopedia (TIN2010-
21128-C02); and the Regional Government of
Madrid?s MA2VICMR (S2009/TIC1542). We
would like to thank Katja Filippova and the anony-
mous reviewers for their insightful comments.
1251
References
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?00, pages 318?325. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?
328.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481?490. Association for
Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010), pages 3?3.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and Their
Participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2 - Volume
2, pages 602?610.
Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical Assessment, 6(4):284.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31(1):399?429.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637?674.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using Wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9. Association for Computational
Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B.
Rubi. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1?38.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1?8. Association for Computational Lin-
guistics.
Micha Elsner and Deepak Santhanam. 2011. Learn-
ing to fuse disparate sentences. In Proceedings of
the Workshop on Monolingual Text-To-Text Gener-
ation, pages 54?63. Association for Computational
Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535?1545. Association for Computational
Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 177?185. As-
sociation for Computational Linguistics.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 322?330. Association
for Computational Linguistics.
Nir Friedman and Moises Goldszmidt. 1996. Learning
Bayesian networks with local structure. In Proceed-
ings of the Twelfth Conference Annual Conference
on Uncertainty in Artificial Intelligence (UAI-96),
pages 252?262, San Francisco, CA. Morgan Kauf-
mann.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. Proceedings of the North American Chap-
ter of the Association for Computational Linguistics,
pages 180?187.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics, short papers.
Association for Computational Linguistics.
Dan Gillick. 2009. Sentence boundary detection and
the problem with the us. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 241?244. Association for
Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1152?1161. Asso-
ciation for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362?370. Association for
Computational Linguistics.
1252
Tommi S. Jaakkola and Michael I. Jordan. 1999.
Variational probabilistic inference and the QMR-
DT Network. Journal of Artificial Intelligence Re-
search, 10:291?322.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81.
Blackford Middleton, Michael Shwe, David Hecker-
man, Max Henrion, Eric Horvitz, Harold Lehmann,
and Gregory Cooper. 1991. Probabilistic diag-
nosis using a reformulation of the INTERNIST-
1/QMR knowledge base. I. The probabilistic model
and inference algorithms. Methods of information in
medicine, 30(4):241?255, October.
Tom Minka. 2002. Judging Significance from Error
Bars. CM U Tech R eport.
Thahir P Mohamed, Estevam R Hruschka Jr, and
Tom M Mitchell. 2011. Discovering relations be-
tween noun categories. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1447?1455. Association for Com-
putational Linguistics.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84?90. Association for Computa-
tional Linguistics.
Joakim Nivre. 2006. Inductive Dependency Parsing,
volume 34 of Text, Speech and Language Technol-
ogy. Springer.
Agnieszka Onisko, Marek J. Druzdzel, and Hanna Wa-
syluk. 2001. Learning Bayesian network parame-
ters from small data sets: application of Noisy-OR
gates. International Journal of Approximated Rea-
soning, 27(2):165?182.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of the NAACL-HLT 2012 Work-
shop on Evaluation Metrics and System Comparison
for Automatic Summarization, pages 1?9. Associa-
tion for Computational Linguistics.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Mor-
gan Kaufmann.
Toma?s? S?ingliar and Milos? Hauskrecht. 2006. Noisy-or
component analysis and its application to link analy-
sis. J. Mach. Learn. Res., 7:2189?2213, December.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile
Paris. 2007. Global Revision in Summarisation:
Generating Novel Sentences with Prim?s Algorithm.
In Proceedings of PACLING 2007 - 10th Conference
of the Pacific Association for Computational Lin-
guistics.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 409?420. Association
for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: Open information
extraction on the web. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics, pages 1353?1361.
1253
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 892?901,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Modelling Events through Memory-based, Open-IE Patterns
for Abstractive Summarization
Daniele Pighin
Google Inc.
biondo@google.com
Marco Cornolti
?
University of Pisa, Italy
cornolti@di.unipi.it
Enrique Alfonseca
Google Inc.
ealfonseca@google.com
Katja Filippova
Google Inc.
katjaf@google.com
Abstract
Abstractive text summarization of news
requires a way of representing events, such
as a collection of pattern clusters in which
every cluster represents an event (e.g.,
marriage) and every pattern in the clus-
ter is a way of expressing the event (e.g.,
X married Y, X and Y tied the knot). We
compare three ways of extracting event
patterns: heuristics-based, compression-
based and memory-based. While the for-
mer has been used previously in multi-
document abstraction, the latter two have
never been used for this task. Compared
with the first two techniques, the memory-
based method allows for generating sig-
nificantly more grammatical and informa-
tive sentences, at the cost of searching a
vast space of hundreds of millions of parse
trees of known grammatical utterances. To
this end, we introduce a data structure and
a search method that make it possible to
efficiently extrapolate from every sentence
the parse sub-trees that match against any
of the stored utterances.
1 Introduction
Text summarization beyond extraction requires a
semantic representation that abstracts away from
words and phrases and from which a summary can
be generated (Mani, 2001; Sp?arck-Jones, 2007).
Following and extending recent work in semantic
parsing, information extraction (IE), paraphrase
generation and summarization (Titov and Klemen-
tiev, 2011; Alfonseca et al, 2013; Zhang and
Weld, 2013; Mehdad et al, 2013), the represen-
tation we consider in this paper is a large collec-
?
Work done during an internship at Google Zurich.
[John Smith] and 
[Mary Brown] wed 
in [Baltimore]...
[Smith] tied the 
knot with [Brown] 
this Monday...
#21: death
#22: divorce
#23: marriage
PER married PER
PER and PER wed
PER tied the knot with PER
PER has married PER
John Smith married Mary Brown
e1: J. Smith (PER)
e2: M. Brown (PER)
e3: Baltimore, MD (LOC)
Figure 1: An example of abstracting from input
sentences to an event representation and genera-
tion from that representation.
tion of clusters of event patterns. An abstractive
summarization system relying on such a represen-
tation proceeds by (1) detecting the most relevant
event cluster for a given sentence or sentence col-
lection, and (2) using the most representative pat-
tern from the cluster to generate a concise sum-
mary sentence. Figure 1 illustrates the summa-
rization architecture we are assuming in this pa-
per. Given input text(s) with resolved and typed
entity mentions, event mentions and the most rele-
vant event cluster are detected (first arrow). Then,
a summary sentence is generated from the event
and entity representations (second arrow).
However, the utility of such a representation for
summarization depends on the quality of pattern
clusters. In particular, event patterns must cor-
respond to grammatically correct sentences. In-
troducing an incomplete or incomprehensible pat-
tern (e.g., PER said PER) may negatively affect
both event detection and sentence generation. Re-
lated work on paraphrase detection and relation
extraction is mostly heuristics-based and has re-
lied on hand-crafted rules to collect such patterns
(see Sec. 2). A standard approach is to focus
on binary relations between entities and extract
892
EYent 
moGel
Pattern 
FlXVterinJ
1ewV 
FlXVterV
Pattern 
e[traFtion
1ewV 
artiFle
Pattern 
e[traFtion
,nIerenFe
$EVtraFtiYe
VXmmar\
Figure 2: A generic pipeline for event-driven ab-
stractive headline generation.
the dependency path between the two entities as
an event representation. An obvious limitation
of this approach is there is no guarantee that the
extracted pattern corresponds to a grammatically
correct sentence, e.g., that an essential preposi-
tional phrase is retained like in file for a divorce.
In this paper we explore two novel, data-driven
methods for event pattern extraction. The first,
compression-based method uses a robust sentence
compressor with an aggressive compression rate
to get to the core of the sentence (Sec. 3). The
second, memory-based method relies on a vast
collection of human-written headlines and sen-
tences to find a substructure which is known to
be grammatically correct (Sec. 4). While the lat-
ter method comes closer to ensuring perfect gram-
maticality, it introduces a problem of efficiently
searching the vast space of known well-formed
patterns. Since standard iterative approaches com-
paring every pattern with every sentence are pro-
hibitive here, we present a search strategy which
scales well to huge collections (hundreds of mil-
lions) of sentences.
In order to evaluate the three methods, we con-
sider an abstractive summarization task where the
goal is to get the gist of single sentences by recog-
nizing the underlying event and generating a short
summary sentence. To the best of our knowledge,
this is the first time that this task has been pro-
posed; it can be considered as abstractive sentence
compression, in contrast to most existing sentence
compression systems which are based on selecting
words from the original sentence or rewriting with
simpler paraphrase tables. An extensive evalua-
tion with human raters demonstrates the utility of
the new pattern extraction techniques. Our analy-
sis highlights advantages and disadvantages of the
three methods.
To better isolate the qualities of the three ex-
traction methodologies, all three methods use the
same training data and share components of the
Algorithm 1 HEURISTICEXTRACTOR(T,E): heuristi-
cally extract relational patterns for the dependency parse T
and the set of entities E.
1: /* Global constants /*
2: global V
p
, V
c
, N
p
, N
c
3: V
c
? {subj, nsubj, nsubjpass, dobj, iobj, xcomp,
4: acomp, expl, neg, aux, attr, prt}
5: V
p
? {xcomp}
6: N
c
? {det, predet, num, ps, poss, nc, conj}
7: N
p
? {ps, poss, subj, nsubj, nsubjpass, dobj, iobj}
8: /* Entry point /*
9: P ? ?
10: for all C ? COMBINATIONS(E) do
11: N ? MENTIONNODES(T,C)
12: N
?
? APPLYHEURISTICS(T, BUILDMST(T,N))
13: P ? P ? {BUILDPATTERN(T,N
?
)}
14: return P
15: /* Procedures /*
16: procedure APPLYHEURISTICS(T,N )
17: N
?
? N
18: while |N
?
| > 0 do
19: N
??
? ?
20: for all n ? N
?
do
21: if n.ISVERB() then
22: N
??
? N
??
? INCLUDECHILDREN(n, V
c
)
23: N
??
? N
??
? INCLUDEPARENT(n, V
p
)
24: else if n.ISNOUN() then
25: N
??
? N
??
? INCLUDECHILDREN(n,N
c
)
26: N
??
? N
??
? INCLUDEPARENT(n,N
p
)
27: N
?
? N
??
\N
?
28: procedure INCLUDECHILDREN(n,L)
29: R? ?
30: for all c ? n.CHILDREN() do
31: if c.PARENTEDGELABEL() ? L then
32: R? R ? {c}
33: return R
34: procedure INCLUDEPARENT(n,L)
35: if n.PARENTEDGELABEL() ? L then
36: return {n}
37: else return ?
very same summarization architecture, as shown
in Figure 2: an event model is constructed by clus-
tering the patterns extracted according to the se-
lected extraction method. Then, the same extrac-
tion method is used to collect patterns from sen-
tences in never-seen-before news articles. Finally,
the patterns are used to query the event model and
generate an abstractive summary. The three differ-
ent pattern extractors are detailed in the next three
sections.
2 Heuristics-based pattern extraction
In order to be able to work in an Open-IE man-
ner, applicable to different domains, most existing
pattern extraction systems are based on linguisti-
cally motivated heuristics. Zhang and Weld (2013)
is based on REVERB (Fader et al, 2011), which
uses a regular expression on part-of-speech tags
to produce the extractions. An alternative system,
893
OLLIE (Schmitz et al, 2012), uses syntactic de-
pendency templates to guide the pattern extraction
process.
The heuristics used in this paper are inspired by
Alfonseca et al (2013), who built well formed re-
lational patterns by extending minimum spanning
trees (MST) which connect entity mentions in a
dependency parse. Algorithm 1 details our re-
implementation of their method and the specific
set of rules that we rely on to enforce pattern gram-
maticality. We use the standard Stanford-style set
of dependency labels (de Marneffe et al, 2006).
The input to the algorithm are a parse tree T and
a set of target entities E. We first generate com-
binations of 1-3 elements of E (line 10), then for
each combination C we identify all the nodes in
T that mention any of the entities in C. We con-
tinue by constructing the MST of these nodes, and
finally apply our heuristics to the nodes in the
MST. The procedure APPLYHEURISTICS (:16) re-
cursively grows a nodeset N
?
by including chil-
dren and parents of noun and verb nodes in N
?
based on dependency labels. For example, we in-
clude all children of verbs in N
?
whose label is
listed in V
c
(:3), e.g., active or passive subjects,
direct or indirect objects, particles and auxiliary
verbs. Similarly, we include the parent of a noun
in N
?
if the dependency relation between the node
and its parent is listed in N
p
.
3 Pattern extraction by sentence
compression
Sentence compression is a summarization tech-
nique that shortens input sentences preserving the
most important content (Grefenstette, 1998; Mc-
Donald, 2006; Clarke and Lapata, 2008, inter
alia). While first attempts at integrating a com-
pression module into an extractive summarization
system were not particularly successful (Daum?e
III and Marcu, 2004, inter alia), recent work
has been very promising (Berg-Kirkpatrick et al,
2011; Wang et al, 2013). It has shown that drop-
ping constituents of secondary importance from
selected sentences ? e.g., temporal modifiers or
relative clauses ? results in readable and more in-
formative summaries. Unlike this related work,
our goal here is to compress sentences to obtain
an event pattern ? the minimal grammatical struc-
ture expressing an event. To our knowledge, this
application of sentence compressors is novel. As
in Section 2, we only consider sentences mention-
ing entities and require the compression (pattern)
to retain at least one such mention.
Sentence compression methods are abundant
but very few can be configured to produce out-
put satisfying certain constraints. For example,
most compression algorithms do not accept com-
pression rate as an argument. In our case, sen-
tence compressors which formulate the compres-
sion task as an optimization problem and solve it
with integer linear programming (ILP) tools un-
der a number of constraints are particularly attrac-
tive (Clarke and Lapata, 2008; Filippova and Al-
tun, 2013). They can be extended relatively easily
with both the length constraint and the constraint
on retaining certain words. The method of Clarke
and Lapata (2008) uses a trigram language model
(LM) to score compressions. Since we are inter-
ested in very short outputs, a LM trained on stan-
dard, uncompressed text would not be suitable. In-
stead, we chose to modify the method of Filippova
and Altun (2013) because it relies on dependency
parse trees and does not use any LM scoring.
Like other syntax-based compressors, the sys-
tem of Filippova and Altun (2013) prunes depen-
dency structures to obtain compression trees and
hence sentences. The objective function to maxi-
mize in an ILP problem (Eq. 1) is formulated over
weighted edges in a transformed dependency tree
and is subject to a number of constraints. Edge
weight is defined as a linear function over a fea-
ture set: w(e) = w ? f(e).
F (X) =
?
e?E
x
e
? w(e) (1)
In our reimplementation we followed the algo-
rithm as described by Filippova and Altun (2013).
The compression tree is obtained in two steps.
First, the input tree is transformed with determin-
istic rules, most of which aim at collapsing indis-
pensable modifiers with their heads (determiners,
auxiliary verbs, negation, multi-word expressions,
etc.). Then a sub-tree maximizing the objective
function is found under a number of constraints.
Apart from the structural constrains from the
original system which ensure that the output is a
valid tree, the constraints we add state that:
1. tree size in edges must be in [3, 6],
2. entity mentions must be retained,
3. subject of the clause must be retained,
4. the sub-tree must be covered by a single
clause ? exactly one finite verb must used.
894
Since we consider compressions with different
lengths as candidates, from this set we select the
one with the maximum averaged edge weight as
the final compression. Figure 3 illustrates the
use of the compressor for obtaining event pat-
terns. Dashed edges are dropped as a result of
constrained compression so that the output is John
Smith married Mary Brown and the event pattern
is PER married PER. Note that the root of a sub-
clause is allowed to be the top-level node in the
extracted compression.
Compared with patterns obtaines with heuris-
tics, compression patterns should retain preposi-
tional verb arguments whose removal would ren-
der the pattern ungrammatical. As an example
consider [C. Zeta-Jones] and [M. Douglas] filed
for divorce. The heuristics-based pattern is PER
and PER filed which is incomplete. Unlike it,
the compression-based method keeps the essential
prepositional phrase for divorce in the pattern be-
cause the average edge weight is greater for the
tree with the prepositional phrase.
4 Memory-based pattern extraction
Neither heuristics-based, nor compression-based
methods provide a guarantee that the extracted
pattern is grammatically correct. In this sec-
tion we introduce an extraction technique which
makes it considerably more likely because it only
extracts patterns which have been observed as
full sentences in a human-written text (Sec. 4.1).
However, this memory-based method also poses
a problem not encountered by the two previous
methods: how to search over the vast space of ob-
served headlines and sentences to extract a pattern
from a given sentence? Our trie-based solution,
which we present in the remainder of this sec-
tion, makes it possible to compare a dependency
graph against millions of observed grammatical
utterances in a fraction of a second.
4.1 A tree-trie to store them all. . .
Our objective is to construct a compact representa-
tion of hundreds of millions of observed sentences
that can fit in the memory of a standard worksta-
tion. This data structure should make it possible
to efficiently identify the sub-trees of a sentence
that match any complete utterance previously ob-
served. To this end, we build a trie of depen-
dency trees (which we call a tree-trie) by scan-
ning all the dependency parses in the news training
Algorithm 2 STORE(T, I): store the dependency tree T
in the tree-trie I .
1: /* Entry point /*
2: L? T.LINEARIZE()
3: STORERECURSION(I.ROOT(), L, 0)
4: return M
5: /* Procedures /*
6: procedure STORERECURSION(n,L, o)
7: if o == L.LENGTH() then
8: n.ADDTREESTRUCTURE(L.STRUCTURE())
9: return
10: if not n.HASCHILD(L.TOKEN(o)) then
11: n.ADDCHILD(L.TOKEN(o))
12: n
?
? n.GETCHILD(L.TOKEN(o))
13: STORERECURSION(n
?
, L, o+ 1)
data, and index each tree in the tree-trie accord-
ing to Algorithm 2. For better clarity, the process
is also described graphically in Figure 4. First,
each dependency tree (a) is linearized, resulting
in a data structure that consists of two aligned se-
quences (b). The first sequence (tokens) encodes
word/parent-relation pairs, while the second se-
quence (structure) encodes the offsets of parent
nodes in the linearized tree. As an example, the
first word ?The? is a determiner (?det?) for the sec-
ond node (offset 1) in the sequence, which is ?cat?.
In turn, ?cat? is the subject (?nsubj?) of the node
in position 2, i.e., ?sleeps?. As described in Algo-
rithm 2, we recursively store the token sequence
in the trie, each word/relation pair being stored in
a node. When the token sequence is completely
consumed, we store in the current trie node the
structure of the linearized tree. Combining struc-
tural information with the sequential information
encoded by each path in the trie makes it possi-
ble to rebuild a complete dependency graph. Fig-
ure 4(c) shows an example trie encoding 4 differ-
ent sentences. We highlighted in bold the path cor-
responding to the linearized form (b) of the exam-
ple parse tree (a).
The figure shows that the tree contains two
kinds of nodes: end-of-sentence (EOS) nodes
(red) and non-terminal nodes (in blue). EOS nodes
do not necessarily coincide with trie leaves, as it
is possible to observe complete sentences embed-
ded in longer ones. EOS nodes differ from non-
terminal nodes in that they store one or more struc-
tural sequences corresponding to different syntac-
tic representations of observed sentences with the
same tokens.
Space-complexity and generalization. Storing
all the observed sentences in a single trie requires
huge amounts of memory. To make it possible to
895
root Our sources report John Smith married Mary Brown in Baltimore yesterday
root
root
subj
subj
obj
in
tmod
Figure 3: Transformed dependency tree with a sub-tree expressing an event pattern.
The cat sleeps under the table
root
nsubj
det
prep
pobj
det
(a)
The
det
cat
nsubj
sleeps
ROOT
under
prep
the
det
table
pobj
1 2 -1 2 5 3
(b)
The
det
dog
nsubj
barked
ROOT
1,2,-1
cat
nsubj
sleeps
ROOT
1,2,-1
soundly
advmod
1,2,-1,2
under
prep
the
det
table
pobj
1,2,-1,2,5,3
(c)
Figure 4: A dependency tree (a), its linearized form (b) and the resulting path in a trie (c), in bold.
store a complete tree-trie in memory, we adopt the
following strategy. We replace the surface form of
entity nodes with the coarse entity type (e.g., PER,
LOC, ORG) of the entity. Similarly, we replace
proper nouns with the placeholder ?[P]?, thus sig-
nificantly reducing lexical sparsity. Then, we en-
code each distinct word/relation pair as a 32-bit
unsigned integer. Assuming a maximum tree size
of 255 nodes, we represent structure sequences as
vectors of type unsigned char (8 bit per element).
Finally, we store trie-node children as sorted vec-
tors instead of hash maps to reduce memory foot-
print. As a result, we are able to load a trie encod-
ing 400M input dependency parses, 170M distinct
nodes and 48M distinct sentence structures in un-
der 10GB of RAM.
4.2 . . . and in the vastness match them
At lookup time, we want to use the tree-trie to
identify all sub-graphs of an input dependency tree
T that match at least a complete observed sen-
tence. To do so, we need to identify all paths in
the trie that match any sub-sequence s of the lin-
earized sequence of T nodes. Whenever we en-
counter an EOS node e, we verify if any of the
structures stored at e matches the sub-tree gener-
ated by s. If so, then we have a positive match.
As a sentence might embed many shorter utter-
ances, each input T will generally yield multiple
matches. For example, querying the tree-trie in
Figure 4(c) with the input tree shown in (a) would
yield two results, as both The cat sleeps and The
cat sleeps under the table are complete utterances
stored in the trie.
Algorithm 3 LOOKUP(T, I): Lookup for matches of sub-
set of tree T in the trie index I .
1: /* Entry point /*
2: L? T.LINEARIZE()
3: M ? ?
4: LOOKUPRECURSIVE(T,L, 0, I.ROOT(), ?,M)
5: return M
6: /* Procedures /*
7: procedure LOOKUPRECURSIVE(T,L, o, n, P,M )
8: for all i ? [o, L.LENGTH()) do
9: if n.HASCHILD(L.TOKEN(i)) then
10: n
?
? n.GETCHILD(L.TOKEN(i))
11: P
?
? P ? {i}
12: for all S ? n
?
.TREESTRUCTURES() do
13: if L.ISCOMPATIBLE(S, P
?
) then
14: M ?M ? {T.GETNODES(P
?
)}
15: LOOKUPRECURSIVE(L, i, o+ 1, n
?
, P
?
,M)
Algorithm 3 describes the lookup process in
more detail. The first step consists in the lineariza-
tion of the input tree T . Then, we recursively tra-
verse the trie calling LOOKUPRECURSIVE. The
inputs of this procedure are: the input tree T , its
linearization L and an offset o (starting at 0), the
trie node currently being traversed n (starting with
the root), the set of offsets in L that constitute a
partial match P (initially empty) and the set of
complete matches found M . We recursively tra-
verse all the nodes in the trie that yield a partial
match with any sub-sequence of the linearized to-
kens of T . At each step, we scan all the tokens
in L in the range [o, L.LENGTH()) looking for to-
kens matching any of the children of n. If a match-
ing node is found, a new partial match P
?
is con-
structed by extending P with the matching token
896
Sheet6
Page 1
0 10 20 30 40 50 60 70 80 901.0E-05
1.0E-04
1.0E-03
1.0E-02
1.0E-01
1.0E+00
1.0E+01
f(x) = 1.9E-07 x^3.3E+00
Tree size (number of nodes)
Time (seconds)
Figure 5: Time complexity of lookup operations
for inputs of different sizes.
offset i (line 11), and the recursion continues from
the matching trie node n
?
and offset i (line 15).
Every time a partial match is found, we verify if
the partial match is compatible with any of the
tree structures stored in the matching node. If that
is the case, we identify the corresponding set of
matching nodes in T and add it to the result M
(lines 12-14). A pattern is generated from each
complete match returned by LOOKUP after apply-
ing a simple heuristic: for each verb node v in the
match, we enforce that negations and auxiliaries in
T depending from x are also included in the pat-
tern.
Time complexity of lookups. Let k be the max-
imum fan-out of trie nodes, d be the depth of
the trie and n be the size of an input tree (num-
ber of nodes). If trie node children are hashed
(which has a negative effect on space complex-
ity), then worst case complexity of LOOKUP() is
O(nk)
d?1
. If they are stored as sorted lists, as in
our memory-efficient implementation, theoretical
complexity becomes O(nk log(k))
d?1
. It should
be noted that worst case complexity can only be
observed under extremely unlikely circumstances,
i.e., that at every step of the recursion all the nodes
in the tail of the linearized tree match a child of
the current node. Also, in the actual trie used in
our experiments the average branching factor k is
very small. We observed that a trie storing 400M
sentences (170M nodes) has an average branching
factor of 1.02. While the root of the trie has unsur-
prisingly many children (210K, all the observed
first sentence words), already at depth 2 the aver-
age fan-out is 13.7, and at level 3 it is 4.9.
For an empirical analysis of lookup complexity,
Figure 5 plots, in black, wall-clock lookup time
as a function of tree size n for a random sample
of 1,600 inputs. As shown by the polynomial re-
gression curve (red), observed lookup complexity
is approximately cubic with a very small constant
factor. In general, we can see that for sentences of
common length (20-50 words) a lookup operation
can be completed in well under one second.
5 Evaluation
5.1 Experimental settings
All the models for the experiments that we present
have been trained using the same corpus of
news crawled from the web between 2008 and
2013. The news have been processed with a to-
kenizer, a sentence splitter (Gillick and Favre,
2009), a part-of-speech tagger and dependency
parser (Nivre, 2006), a co-reference resolution
module (Haghighi and Klein, 2009) and an entity
linker based on Wikipedia and Freebase (Milne
and Witten, 2008). We use Freebase types as fine-
grained named entity types, so we are also able to
label e.g. instances of sports teams as such instead
of the coarser label ORG.
Next, the news have been grouped based on
temporal closeness (Zhang and Weld, 2013) and
cosine similarity (using tf?idf weights). For each
of the three pattern extraction methods we used the
same summarization pipeline (as shown above in
Figure 2):
1. Run pattern extraction on the news.
2. For every news collection Coll and entity set
E, generate a set containing all the extracted
patterns from news in Coll mentioning all
the entities in E. These are patterns that are
likely to be paraphrasing each other.
3. Run a clustering algorithm to group together
patterns that typically co-occur in the sets
generated in the previous step. There are
many choices for clustering algorithms (Al-
fonseca et al, 2013; Zhang and Weld, 2013).
Following Alfonseca et al (2013) we use in
this work a Noisy-OR Bayesian Network be-
cause it has already been applied for abstrac-
tive summarization (albeit multi-document),
it provides an easily interpretable probabilis-
tic clustering, and training can be easily par-
allelized to be able to handle large training
sets. The hidden events in the Bayesian net-
work represent pattern clusters. When train-
ing is done, for each extraction pattern p
j
897
Original sentence Abstractive summary (method)
Two-time defending overall World Cup champion Marcel Hirscher won the
challenging giant slalom on the Gran Risa course with two solid runs Sunday
and attributed his victory to a fixed screw in his equipment setup.
Marcel Hirscher has won the giant
slalom. (C)
Zodiac Aerospace posted a 7.9 percent rise in first-quarter revenue, below mar-
ket expectations, but reaffirmed its full-year financial targets.
Zodiac Aerospace has reported a rise in
profits. (C)
Australian free-agent closer Grant Balfour has agreed to terms with the Balti-
more Orioles on a two-year deal, the Baltimore Sun reported on Tuesday citing
multiple industry sources.
Balfour will join the Baltimore Orioles.
(H)
Paul Rudd is ?Ant-Man?: 5 reasons he needs an ?Agents of SHIELD? appear-
ance.
Paul Rudd to play Ant-Man. (H)
Millwall defender Karleigh Osborne has joined Bristol City on a two-and-a-half
year deal after a successful loan spell.
Bristol City have signed Karleigh Os-
borne. (M)
Simon Hoggart, one of the Spectator?s best-loved columnists, died yesterday
after fighting pancreatic cancer for over three years.
Simon Hoggart passed away yesterday.
(M)
Table 1: Abstraction examples from compression (C), heuristic (H) and memory-based (M) patterns.
Method Extractions Abstractions
HEURISTIC 24,630 956
COMPRESSION 15,687 657
MEMORY-BASED 11,459 967
Table 2: Patterns extracted in each method, before
Noisy-OR inference.
and pattern cluster c
i
, the network provides
p(p
j
|c
i
) ?the probability that c
i
will gener-
ate p
j
? and p(c
i
|p
j
) ?the probability that,
given a pattern p
j
, c
i
was the hidden event
that generated it.
At generation time we proceed in the following
way:
1. Given the title or first sentence of a news ar-
ticle, run the same pattern extraction method
that was used in training and, if possible, ob-
tain a pattern p involving some entities.
2. Find the model clusters that contain this pat-
tern, C
p
= {c
i
such that p(c
i
|p) > 0}.
3. Return a ranked list of model patterns
output = {(p
j
, score(p
j
))}, scored as fol-
lows:
score(p
j
) =
?
c
i
?C
p
p(p
j
|c
i
)p(c
i
|p)
where p was the input pattern.
4. Replace the entity placeholders in the top-
scored patterns p
j
with the entities that were
actually mentioned in the input news article.
In all cases the parameters of the network were
predefined as 20,000 nodes in the hidden layer
(model clusters) and 40 Expectation Maximization
(EM) training iterations. Training was distributed
across 20 machines with 10 GB of memory each.
For testing we used 37,584 news crawled dur-
ing December 2013, which had not been used for
training the models. Table 3 shows one pattern
cluster example from each of the three trained
models. The table shows only the surface form
of the pattern for simplicity.
Pattern cluster (MEMORY-BASED)
organization
1
gets organization
0
nod for drug
organization
1
gets organization
0
nod for tablets
organization
0
approves organization
1
drug
organizations
0
approves organization
1
?s drug
organization
1
gets organization
0
nod for capsules
Pattern cluster (HEURISTIC)
organization
0
to buy organization
1
organization
0
to acquire organization
1
organization
0
buys organization
1
organization
0
acquires organization
1
organization
0
to acquire organizations
1
organization
0
buys organizations
1
organization
0
acquires organizations
1
organization
0
agrees to buy organization
1
organization
0
snaps up organization
1
organization
0
to purchase organizations
1
organization
0
is to acquire organization
1
organization
0
has agreed to buy organization
1
organization
0
announces acquisition of organizations
1
organization
0
may bid for organization
1
organization
1
sold to organization
0
organization
1
acquired by organization
0
Pattern cluster (COMPRESSION)
the sports team
1
have acquired person
0
from the sports team
2
the sports team
1
acquired person
0
from the sports team
2
the sports team
2
have traded person
0
to the sports team
1
sports team
1
acquired the rights to person
0
from sports team
2
sports team
2
acquired from sports team
1
in exchange for person
0
sports team
2
have acquired from the sports team
1
in exchange for person
0
Table 3: Examples of pattern clusters. In each
cluster c
i
, patterns are sorted by p(p
j
|c
i
).
898
5.2 Results
Table 2 shows the number of extracted patterns
from the test set, and the number of abstractive
event descriptions produced.
As expected, the number of extracted patterns
using the memory-based model is smaller than
with the two other models, which are based on
generic rules and are less restricted in what they
can generate. As mentioned, the memory-based
model can only extract previously-seen structures.
Compared to this model, with heuristics we can
obtain patterns for more than twice more news ar-
ticles. At the same time, looking at the number
of summary sentences generated they are com-
parable, meaning that a larger proportion of the
memory-based patterns actually appeared in the
pattern clusters and could be used to produce sum-
maries. This is also consistent with the fact that us-
ing heuristics the space of extracted patterns is ba-
sically unbounded and many new patterns can be
generated that were previously unseen ?and these
cannot generate abstractions. A positive outcome
is that restricting the syntactic structure of the ex-
tracted patterns to what has been observed in past
news does not negatively affect end-to-end cover-
age when generating the abstractive summaries.
Table 1 shows some of the abstractive sum-
maries generated with the different methods. For
manually evaluating their quality, a random sam-
ple of 100 original sentences was selected for each
method. The top ranked summary for each origi-
nal sentence was sent to human raters for evalua-
tion, and received three different ratings. None of
the raters had any involvement in the development
of the work or the writing of the paper, and a con-
straint was added that no rater could rate more than
50 abstractions. Raters were presented with the
original sentence and the compressed abstraction,
and were asked to rate it along two dimensions, in
both cases using a 5-point Likert scale:
? Readability: whether the abstracted com-
pression is grammatically correct.
? Informativeness: whether the abstracted
compression conveys the most important in-
formation from the original sentence.
Inter-judge agreement was measured using the
Intra-Class Correlation (ICC) (Shrout and Fleiss,
1979; Cicchetti, 1994). The ICC for readability
was 0.37 (95% confidence interval [0.32, 0.41]),
Method Readability Informativeness
HEURISTIC 3.95 3.07
COMPRESSION 3.98 2.35
MEMORY-BASED 4.20 3.70
Table 4: Results for the three methods when rating
the top-ranked abstraction.
and for informativeness it was 0.64 (95% confi-
dence interval [0,60, 0.67]), representing fair and
substantial reliability.
Table 4 shows the results when rating the top
ranked abstraction using either of the three dif-
ferent models for pattern extraction. The abstrac-
tions produced with the memory-based method are
more readable than those produced with the other
two methods (statistically significant with 95%
confidence).
Regarding informativeness, the differences be-
tween the methods are bigger, because the first two
methods have a proportionally larger number of
items with a high readability but a low informa-
tiveness score. For each method, we have man-
ually reviewed the 25 items where the difference
between readability and informativeness was the
largest, to understand in which cases grammatical,
yet irrelevant compressions are produced. The re-
sults are shown in Table 5. Be+adjective includes
examples where the pattern is of the form Entity is
Adjective, which the compression-based systems
extracts often represents an incomplete extraction.
Wrong inference contains the cases where patterns
that are related but not equivalent are clustered,
e.g. Person arrived in Country and Person arrived
in Country for talks. Info. missing represents cases
where very relevant information has been dropped
and the summary sentence is not complete. Pos-
sibility contains cases where the original sentence
described a possibility and the compression states
it as a fact, or vice versa. Disambiguation are en-
tity disambiguations errors, and Opposite contains
cases of patterns clustered together that are op-
posite along some dimension, e.g. Person quits
TV Program and Person to return to TV Program.
The method with the largest drop between the
readability and informativeness scores is COM-
PRESSION. As can be seen, many of these mis-
takes are due to relevant information being miss-
ing in the summary sentence. This is also the
largest source of errors for the HEURISTIC system.
For the MEMORY-BASED system, the drop in read-
899
Method Be+adjective Wrong inference Info. missing Possibility Disambiguation Opposite
HEURISTIC 0 7 14 3 1 0
COMPRESSION 3 10 10 0 0 2
MEMORY-BASED 0 17 4 2 0 2
Table 5: Sources of errors for the top 25 items with high readability and low informativeness.
Original sentence Pattern extracted (method) Abstraction
David Moyes is happy to use tough love on Adnan Januzaj
to ensure the Manchester United youngster fulfils his mas-
sive potential.
David Moyes is happy. (C)
Fortune will start to favour
David Moyes.
The Democratic People?s Republic of Korea will ?achieve
nothing by making threats or provocation,? the United
States said Friday.
The United States said Fri-
day. (C, H)
United States officials said
Friday.
EU targets Real and Barca over illegal state aid.
EU targets Real Madrid.
(H)
EU is going after Real
Madrid.
EU warns Israel over settlement construction
EU warns Israel. (M)
EU says Israel needs re-
forms.
Table 6: Examples of compression (C), heuristic (H) and memory-based (M) patterns that led to abstrac-
tions with high readability but a low informativeness score. Both incomplete summary sentences and
wrong inferences can be observed.
ability score is much smaller, so there were less of
these examples. And most of these examples be-
long to the class of wrong inferences (patterns that
are related but not equivalent, so we should not
abstract one of them from the other, but they were
clustered together in the model). Our conclusion
is that the examples with missing information are
not such a big problem with the MEMORY-BASED
system, as using the trie is an additional safeguard
that the generated titles are complete statements,
but the method is not preventing the wrong infer-
ence errors so this class of errors become the dom-
inant class by a large margin.
Some examples with high readability but low
informativeness are shown in Table 6.
6 Conclusions
Most Open-IE systems are based on linguistically-
motivated heuristics for learning patterns that ex-
press relations between entities or events. How-
ever, it is common for these patterns to be incom-
plete or ungrammatical, and therefore they are not
suitable for abstractive summary generation of the
relation or event mentioned in the text.
In this paper, we describe a memory-based ap-
proach in which we use a corpus of past news
to learn valid syntactic sentence structures. We
discuss the theoretical time complexity of look-
ing up extraction patterns in a large corpus of
syntactic structures stored as a trie and demon-
strate empirically that this method is effective in
practice. Finally, the evaluation shows that sum-
mary sentences produced by this method outper-
form heuristics and compression-based ones both
in terms of readability and informativeness. The
problem of generating incomplete summary sen-
tences, which was the main source of informative-
ness errors for the alternative methods, becomes a
minor problem with the memory-based approach.
Yet, there are some cases in which also the mem-
ory based approach extracts correct but misleading
utterances, e.g., a pattern like PER passed away
from the sentence PER passed the ball away. To
solve this class of problems, a possible research
direction would be the inclusion of more complex
linguistic features in the tree-trie, such as verb sub-
categorization frames.
As another direction for future work, more ef-
fort is needed in making sure that no incorrect in-
ferences are made with this model. These happen
when a more specific pattern is clustered together
with a less specific pattern, or when two non-
equivalent patterns often co-occur in news as two
events are somewhat correlated in real life, but it is
generally incorrect to infer one from the other. Im-
provements in the pattern-clustering model, out-
side the scope of this paper, will be required.
900
References
Enrique Alfonseca, Daniele Pighin, and Guillermo
Garrido. 2013. HEADY: News headline abstraction
through event pattern clustering. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, Sofia, Bulgaria, 4?9 August
2013, pages 1243?1253.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
OR, 19?24 June 2011.
Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical Assessment, 6(4):284.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Hal Daum?e III and Daniel Marcu. 2004. A tree-
position kernel for document compression. In Pro-
ceedings of the 2004 Document Understanding Con-
ference held at the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics,, Boston,
Mass., 6?7 May 2004.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006, pages 449?454.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, Edinburgh, UK, 27?29 July 2011, pages 1535?
1545.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, Seattle,
WA, USA, 18?21 October 2013, pages 1481?1491.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the ILP
for NLP Workshop, Boulder, CO, June 4 2009, pages
10?18.
Gregory Grefenstette. 1998. Producing intelligent
telegraphic text reduction to provide an audio scan-
ning service for the blind. In Working Notes of the
Workshop on Intelligent Text Summarization, Palo
Alto, Cal., 23 March 1998, pages 111?117.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, Singapore, 6-7 August 2009, pages 1152?1161.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins, Amsterdam, Philadelphia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Trento, Italy, 3?7 April 2006, pages 297?304.
Yashar Mehdad, Giuseppe Carenini, and Frank W.
Tompa. 2013. Abstractive meeting summariza-
tion with entailment and fusion. In Proceedings
of the 14th European Workshop on Natural Lan-
guage Generation, Sofia, Bulgaria, 8?9 August,
2013, pages 136?146.
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the AAAI
2008 Workshop on Wikipedia and Artificial Intelli-
gence, Chicago, IL, 13-14 July, 2008.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Michael Schmitz, Robert Bart, Stephen Soderland,
Oren Etzioni, et al 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Conference on Empirical Methods in Natu-
ral Language Processing, Jeju Island, Korea, 12?14
July 2012, pages 523?534.
Patrick E Shrout and Joseph L Fleiss. 1979. Intraclass
correlations: uses in assessing rater reliability. Psy-
chological bulletin, 86(2):420.
Karen Sp?arck-Jones. 2007. Automatic summaris-
ing: A review and discussion of the state of the art.
Technical Report UCAM-CL-TR-679, University of
Cambridge, Computer Laboratory, Cambridge, U.K.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
OR, 19?24 June 2011, pages 1445?1455.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics, Sofia, Bulgaria, 4?9 August
2013, pages 1384?1394.
Congle Zhang and Daniel S. Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, WA, USA, 18?21 October 2013,
pages 1776?1786.
901
