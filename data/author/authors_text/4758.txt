Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 774?782, Prague, June 2007. c?2007 Association for Computational Linguistics
Scalable Term Selection for Text Categorization
Jingyang Li
National Lab of Intelligent Tech. & Sys.
Department of Computer Sci. & Tech.
Tsinghua University, Beijing, China
lijingyang@gmail.com
Maosong Sun
National Lab of Intelligent Tech. & Sys.
Department of Computer Sci. & Tech.
Tsinghua University, Beijing, China
sms@tsinghua.edu.cn
Abstract
In text categorization, term selection is an
important step for the sake of both cate-
gorization accuracy and computational ef-
ficiency. Different dimensionalities are ex-
pected under different practical resource re-
strictions of time or space. Traditionally
in text categorization, the same scoring or
ranking criterion is adopted for all target
dimensionalities, which considers both the
discriminability and the coverage of a term,
such as ?2 or IG. In this paper, the poor ac-
curacy at a low dimensionality is imputed to
the small average vector length of the docu-
ments. Scalable term selection is proposed
to optimize the term set at a given dimen-
sionality according to an expected average
vector length. Discriminability and cover-
age are separately measured; by adjusting
the ratio of their weights in a combined cri-
terion, the expected average vector length
can be reached, which means a good com-
promise between the specificity and the ex-
haustivity of the term subset. Experiments
show that the accuracy is considerably im-
proved at lower dimensionalities, and larger
term subsets have the possibility to lower
the average vector length for a lower com-
putational cost. The interesting observations
might inspire further investigations.
1 Introduction
Text categorization is a classical text information
processing task which has been studied adequately
(Sebastiani, 2002). A typical text categorization pro-
cess usually involves these phases: document in-
dexing, dimensionality reduction, classifier learn-
ing, classification and evaluation. The vector space
model is frequently used for text representation
(document indexing); dimensions of the learning
space are called terms, or features in a general ma-
chine learning context. Term selection is often nec-
essary because:
? Many irrelevant terms have detrimental effect
on categorization accuracy due to overfitting
(Sebastiani, 2002).
? Some text categorization tasks have many rel-
evant but redundant features, which also hurt
the categorization accuracy (Gabrilovich and
Markovitch, 2004).
? Considerations on computational cost:
(i) Many sophisticated learning machines are
very slow at high dimensionalities, such as
LLSF (Yang and Chute, 1994) and SVMs.
(ii) In Asian languages, the term set is often
very large and redundant, which causes the
learning and the predicting to be really slow.
(iii) In some practical cases the computational
resources (time or space) are restricted, such as
hand-held devices, real-time applications and
frequently retrained systems. (iv) Some deeper
analysis or feature reconstruction techniques
rely on matrix factorization (e.g. LSA based
on SVD), which might be computationally in-
tractable while the dimensionality is large.
Sometimes an aggressive term selection might be
needed particularly for (iii) and (iv). But it is no-
table that the dimensionality is not always directly
774
connected to the computational cost; this issue will
be touched on in Section 6. Although we have
many general feature selection techniques, the do-
main specified ones are preferred (Guyon and Elis-
seeff, 2003). Another reason for ad hoc term se-
lection techniques is that many other pattern clas-
sification tasks has no sparseness problem (in this
study the sparseness means a sample vector has
few nonzero elements, but not the high-dimensional
learning space has few training samples). As a ba-
sic motivation of this study, we hypothesize that the
low accuracy at low dimensionalities is mainly due
to the sparseness problem.
Many term selection techniques were presented
and some of them have been experimentally tested
to be high-performing, such as Information Gain, ?2
(Yang and Pedersen, 1997; Rogati and Yang, 2002)
and Bi-Normal Separation (Forman, 2003). Every-
one of them adopt a criterion scoring and ranking
the terms; for a target dimensionality d, the term se-
lection is simply done by picking out the top-d terms
from the ranked term set. These high performing cri-
teria have a common characteristic ? both discrim-
inability and coverage are implicitly considered.
? discriminability: how unbalanced is the distri-
bution of the term among the categories.
? coverage: how many documents does the term
occur in.
(Borrowing the terminologies from document index-
ing, we can say the specificity of a term set corre-
sponds to the discriminability of each term, and the
exhaustivity of a term set corresponds to the cov-
erage of each term.) The main difference among
these criteria is to what extent the discriminability is
emphasized or the coverage is emphasized. For in-
stance, empirically IG prefers high frequency terms
more than ?2 does, which means IG emphasizes the
coverage more than ?2 does.
The problem is, these criteria are nonparametric
and do the same ranking for any target dimensional-
ity. Small term sets meet the specificity?exhaustivity
dilemma. If really the sparseness is the main rea-
son of the low performance of a small term set, the
specificity should be moderately sacrificed to im-
prove the exhaustivity for a small term set; that is
to say, the term selection criterion should consider
coverage more than discriminability. Contrariwise,
coverage could be less considered for a large term
set, because we need worry little about the sparse-
ness problem and the computational cost might de-
crease.
The remainder of this paper is organized as fol-
lows: Section 2 describes the document collections
used in this study, as well as other experiment set-
tings; Section 3 investigates the relation between
sparseness (measured by average vector length) and
categorization accuracy; Section 4 explains the basic
idea of scalable term selection and proposed a poten-
tial approach; Section 5 carries out experiments to
evaluate the approach, during which some empirical
rules are observed to complete the approach; Sec-
tion 6 makes some further observations and discus-
sions based on Section 5; Section 7 gives a conclud-
ing remark.
2 Experiment Settings
2.1 Document Collections
Two document collections are used in this study.
CE (Chinese Encyclopedia): This is from the
electronic version of the Chinese Encyclopedia. We
choose a Chinese corpus as the primary document
collection because Chinese text (as well as other
Asian languages) has a very large term set and a
satisfying subset is usually not smaller than 50000
(Li et al, 2006); on the contrary, a dimensional-
ity lower than 10000 suffices a general English text
categorization (Yang and Pedersen, 1997; Rogati
and Yang, 2002). For computational cost reasons
mentioned in Section 1, Chinese text categorization
would benefit more from an high-performing ag-
gressive term selection. This collection contains 55
categories and 71674 documents (9:1 split to train-
ing set and test set). Each documents belongs to
only one category. Each category contains 399?
3374 documents. This collection was also used by
Li et al (2006).
20NG (20 Newsgroups1): This classical English
document collection is chosen as a secondary in this
study to testify the generality of the proposed ap-
proach. Some figures about this collection are not
shown in this paper as the figures about CE, viz. Fig-
ure 1?4 because they are similar to CE?s.
1http://people.csail.mit.edu/jrennie/
20Newsgroups
775
2.2 Other Settings
For CE collection, character bigrams are chosen to
be the indexing unit for its high performance (Li et
al., 2006); but the bigram term set suffers from its
high dimensionality. This is exactly the case we tend
to tackle. For 20NG collection, the indexing units
are stemmed2 words. Both term set are df -cut by
the most conservative threshold (df ? 2). The sizes
of the two candidate term sets are |TCE| = 1067717
and |T20NG| = 30220.
Term weighting is done by tfidf (ti, dj) =
log(tf (ti, dj) + 1) ? log
( df (ti)+1
Nd
)
3, in which ti de-
notes a term, dj denotes a document, Nd denotes the
total document number.
The classifiers used in this study are support
vector machines (Joachims, 1998; Gabrilovich and
Markovitch, 2004; Chang and Lin, 2001). The ker-
nel type is set to linear, which is fast and enough
for text categorization. Also, Brank et al (2002)
pointed out that the complexity and sophistication of
the criterion itself is more important to the success
of the term selection method than its compatibility
in design with the classifier.
Performance is evaluated by microaveraged F1-
measure. For single-label tasks, microaveraged pre-
cision, recall and F1 have the same value.
?2 is used as the term selection baseline for its
popularity and high performance. (IG was also re-
ported to be good. In our previous experiments, ?2
is generally superior to IG.) In this study, features
are always selected globally, which means the maxi-
mum are computed for category-specific values (Se-
bastiani, 2002).
3 Average Vector Length (AVL)
In this study, vector length (how many different
terms does the document hold after term selection)
is used as a straightforward sparseness measure for a
document (Brank et al, 2002). Generally, document
sizes have a lognormal distribution (Mitzenmacher,
2003). In our experiment, vector lengths are also
found to be nearly lognormal distributed, as shown
in Figure 1. If the correctly classified documents
2Stemming by Porter?s Stemmer (http://www.
tartarus.org/ martin/PorterStemmer/).
3In our experiments this form of tfidf always outperforms
the basic tfidf (ti, dj) = tf (ti, dj) ? log
?
df (ti)+1
Nd
?
form.
1 10 100 1000
0.00
0.01
0.02
p
r
o
b
 
d
e
n
s
i
t
y
vector length
 correct
 wrong
 all
Figure 1: Vector Length Distributions (smoothed),
on CE Document Collection
1 10 100 1000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
e
r
r
o
r
 
r
a
t
e
vector length
Figure 2: Error Rate vs. Vector Length (smoothed),
on CE Collection, 5000 Dimensions by ?2
and the wrongly classified documents are separately
investigated, they both yield a nearly lognormal dis-
tribution.
Also in Figure 1, wrongly classified documents
shows a relatively large proportion at low dimen-
sionalities. Figure 2 demonstrates this with more
clarity. Thus the hypothesis formed in Section 1 is
confirmed: there is a strong correlation between the
sparseness degree and the categorization error rate.
Therefore, it is quite straightforward a thought to
measure the ?sparseness of a term subset? (or more
precisely, the exhaustivity) by the corresponding av-
erage vector length (AVL) of all documents.4 In the
4Due to the lognormal distribution of vector length, it seems
more plausible to average the logarithmic vector length. How-
ever, for a fixed number of documents , log
P |dj |
|D| should hold
a nearly fixed ratio to
P log |dj |
|D| , in which |D| denotes the doc-
ument number and |dj | denotes the document vector length.
776
remainder of this paper, (log) AVL is an important
metric used to assess and control the sparseness of a
term subset.
4 Scalable Term Selection (STS)
Since the performance droping down at low dimen-
sionalities is attributable to low AVLs in the previous
section, a scalable term selection criterion should
automatically accommodate its favor of high cov-
erage to different target dimensionalities.
4.1 Measuring Discriminability and Coverage
The first step is to separately measure the discrim-
inability and the coverage of a term. A basic guide-
line is that these two metrics should not be highly
(positive) correlated; intuitively, they should have a
slight negative correlation. The correlation of the
two metrics can be visually estimated by the joint
distribution figure. A bunch of term selection met-
rics were explored by Forman (2003). df (document
frequency) is a straightforward choice to measure
coverage. Since df follows the Zipf?s law (inverse
power law), log(df ) is adopted. High-performing
term selection criterion themselves might not be
good candidates for the discriminability metric be-
cause they take coverage into account. For exam-
ple, Figure 3 shows that ?2 is not satisfying. (For
readability, the grayness is proportional to the log
probability density in Figure 3, Figure 4 and Fig-
ure 12.) Relatively, probability ratio (Forman, 2003)
is a more straight metric of discriminability.
PR(ti, c) = P (ti|c+)P (ti|c?) =
df (ti, c+)/df (c+)
df (ti, c?)/df (c?)
It is a symmetric ratio, so log(PR) is likely to be
more appropriate. For multi-class categorization,
a global value can be assessed by PRmax(ti) =
maxc PR(ti, c), like ?2max for ?2 (Yang and Ped-
ersen, 1997; Rogati and Yang, 2002; Sebastiani,
2002); for brief, PR denotes PRmax hereafter. The
joint distribution of log(PR) and log(df ) is shown in
Figure 12. We can see that the distribution is quite
even and they have a slight negative correlation.
4.2 Combined Criterion
Now we have the two metrics: log(PR) for discrim-
inability and log(df ) for coverage, and a parametric
log(df )
?2
1.10 10.591.7
42033.0
Figure 3: (log(df ), ?2) Distribution, on CE
log(df )
log
(PR
)
1.10 10.590.40
9.46
Figure 4: (log(df ), log(PR)) Distribution, on CE
term selection criterion comes forth:
?(ti;?) =
( ?
log(PR(ti)) +
1? ?
log(df (ti))
)?1
A weighted harmonic averaging is adopted here be-
cause either metric?s being too small is a severe
detriment. ? ? [0, 1] is the weight for log(PR),
which denotes how much the discriminability is
emphasized. When the dimensionality is fixed, a
smaller ? leads to a larger AVL and a larger ? leads
to a smaller AVL. The optimal ? should be a function
777
of the expected dimensionality (k):
??(k) = argmax
?
F1(Sk(?))
in which the term subset Sk(?) ? T is selected by
?(?;?) , |Sk| = k, and F1 is the default evaluation
criterion. Naturally, this optimal ? leads to a corre-
sponding optimal AVL:
AVL?(k) ?? ??(k)
For a concrete implementation, we should have an
(empirical) function to estimate ?? or AVL?:
AVL?(k) .= AVL?(k)
In the next section, the values of AVL? (as well as ??)
for some k-s are figured out by experimental search;
then an empirical formula, AVL?(k), comes forth. It
is interesting and inspiring that by adding the ?cor-
pus AVL? as a parameter this formula is universal
for different document collections, which makes the
whole idea valuable.
5 Experiments and Implementation
5.1 Experiments
The expected dimensionalities (k) chosen for exper-
imentation are
CE: 500, 1000, 2000, 4000, . . . , 32000, 64000;
20NG: 500, 1000, 2000, . . . , 16000, 30220.5
For a given document collection and a given target
dimensionality, there is a corresponding AVL for a ?,
and vice versa (for the possible value range of AVL).
According to the observations in Section 5.2, AVL
other than ? is the direct concern because it is more
intrinsic, but ? is the one that can be tuned directly.
So, in the experiments, we vary AVL by tuning ? to
produce it, which means to calculate ?(AVL).
AVL(?) is a monotone function and fast to cal-
culate. For a given AVL, the corresponding ? can
be quickly found by a Newton iteration in [0,1]. In
fact, AVL(?) is not a continuous function, so ? is
only tuned to get an acceptable match, e.g. within
?0.1.
5STS is tested to the whole T on 20NG but not on CE, be-
cause (i) TCE is too large and time consuming for training and
testing, and (ii) ?2 was previously tested on larger k and the
performance (F1) is not stable while k > 64000.
For each k, by the above way of fitting ?,
we manually adjust AVL (only in integers) until
F1(Sk(?(AVL))) peaks. By this way, Figure 5?11
are manually tuned best-performing results as obser-
vations for figuring out the empirical formulas.
Figure 5 shows the F1 peaks at different dimen-
sionalities. Comparing to ?2, STS has a consid-
erable potential superiority at low dimensionalities.
The corresponding values of AVL? are shown in Fig-
ure 6, along with the AVLs of ?2-selected term sub-
sets. The dotted lines show the trend of AVL?; at the
overall dimensionality, |TCE| = 1067717, they have
the same AVL = 898.5. We can see that log(AVL?)
is almost proportional to log(k) when k is not too
large. The corresponding values of ?? are shown in
Figure 7; the relation is nearly linear between ?? and
log(k).
Now it is necessary to explain why an empirical
AVL?(k) derived from the straight line in Figure 6
can be used instead of AVL?(k) in practice. One
important but not plotted property is that the per-
formance of STS is not very sensitive to a small
value change of AVL. For instance, at k = 4000,
AVL? = 120 and the F1 peak is 85.8824%, and
for AVL = 110 and 130 the corresponding F1 are
85.8683% and 85.6583%; at the same k, the F1
of ?2 selection is 82.3950%. This characteristic of
STS guarantee that the empirical AVL?(k) has a very
close performance to AVL?(k); due to the limited
space, the performance curve of AVL?(k) will not
be plotted in Section 5.2.
Same experiments are done on 20NG and the re-
sults are shown in Figure 8, Figure 9 and Figure 10.
The performance improvements is not as signifi-
cant as on the CE collection; this will be discussed
in Section 6.2. The conspicuous relations between
AVL?, ?? and k remain the same.
5.2 Algorithm Completion
In Figure 6 and Figure 9, the ratios of log(AVL?(k))
to log(k) are not the same on CE and 20NG. Tak-
ing into account the corpus AVL (the AVL produced
by the whole term set): AVLTCE = 898.5286 and
AVLT20NG = 82.1605, we guess log(AVL
?(k))
log(AVLT ) is ca-
pable of keeping the same ratio to log(k) for both
CE and 20NG. This hypothesis is confirmed (not for
too high dimensionalities) by Figure 11; Section 6.2
778
100 1000 10000 100000
60
65
70
75
80
85
90
F
1
 
(
%
)
dimensionality (k)
 
2
 STS
Figure 5: Performance Comparison, on CE
1 10 100 1000 10000 100000 1000000
1
10
100
1000
 
2
 STS
A
V
L
*
dimensionality (k)
Figure 6: AVL Comparison, on CE
1 10 100 1000 10000 100000 1000000
0.00
0.02
0.04
0.06
0.08
0.10
0.12
dimensionality (k)
Figure 7: Optimal Weights of log(PR), on CE
100 1000 10000 100000
72
74
76
78
80
82
84
86
 
2
 STS
F
1
 
(
%
)
dimensionality (k)
Figure 8: Performance Comparison, on 20NG
1 10 100 1000 10000 100000
1
10
100
 
2
 STS
A
V
L
*
dimensionality (k)
Figure 9: AVL Comparison, on 20NG
1 10 100 1000 10000 100000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
dimensionality (k)
Figure 10: Optimal Weights of log(PR), on 20NG
779
1 10 100 1000 10000 100000 1000000
0.0
0.2
0.4
0.6
0.8
1.0
l
o
g
(
A
V
L
*
(
k
)
)
 
/
 
l
o
g
(
A
V
L
T
)
dimensionality (k)
 CE
 20NG
Figure 11: log(AVL?(k))log(AVLT ) , on Both CE and 20NG
contains some discussion on this.
From the figure, we get the value of this ratio (the
base of log is set to e):
? = log(AVL
?(k))/log(AVLT )
log(k)
?= 0.085
which should be a universal constant for all text cat-
egorization tasks.
So the empirical estimation of AVL?(k) is given
by
AVL?(k) = exp(? log(AVLT ) ? log(k))
= AVL ? log(k)T
and the final STS criterion is
?(ti, k) = ?(ti;?(AVL?(k)))
= ?(ti;?(AVL ? log(k)T ))
in which ?(?) can be calculated as in Section 5.1.
The target dimensionality, k, is involved as a param-
eter, so the approach is named scalable term selec-
tion. As stated in Section 5.1, AVL?(k) has a very
close performance to AVL?(k) and its performance
is not plotted here.
6 Further Observation and Discussion
6.1 Comparing the Selected Subsets
An investigation shows that for a quite large range
of ?, term rankings by ?(ti;?) and ?2(ti) have a
strong correlation (the Spearman?s rank correlation
coefficient is bigger than 0.999). In order to com-
log(df )
log
(PR
)
1.10 10.590.40
9.46
500100
0
200
0
400
0
800
0
160
00
320
00
640
00STS
?2
Figure 12: Selection Area Comparison of STS and
?2 on Various Dimensionalities, on CE
log(df )
log
(PR
)
1.10 9.140.11
8.04
500100
0
200
0
400
0
800
0
160
00STS
?2
Figure 13: Selection Area Comparison of STS and
?2 on Various Dimensionalities, on 20NG
pare the two criteria?s preferences for discriminabil-
ity and coverage, the selected subsets of different
dimensionalities are shown in Figure 12 (the cor-
responding term density distribution was shown in
Figure 4) and Figure 13. For different dimension-
780
alities, the selection areas of STS are represented by
boundary lines, and the selection areas of ?2 are rep-
resented by different grayness.
In Figure 12, STS shows its superiority at low di-
mensionalities by more emphasis on the coverage
of terms. In Figure 13, STS shows its superior-
ity at high dimensionalities by more emphasis on
the discriminability of terms; lower coverage yields
smaller index size and lower computational cost.
At any dimensionality, STS yields a relatively fixed
bound for either discriminability or coverage, other
than a compromise between them like ?2; this is at-
tributable to the harmonic averaging.
6.2 Adaptability of STS
There are actually two kinds of sparseness in a (vec-
torized) document collection:
collection sparseness: the high-dimensional learn-
ing space contains few training samples;
document sparseness: a document vector has few
nonzero dimensions.
In this study, only the document sparseness is inves-
tigated. The collection sparseness might be a back-
room factor influencing the actual performance on
different document collections. This might explain
why the explicit characteristics of STS are not the
same on CE to 20NG: (comparing with ?2, see Fig-
ure 5, Figure 6, Figure 8 and Figure 9)
CE. The significant F1 improvements at low di-
mensionalities sacrifice the short of AVL. In some
learning process implementations, it is AVL other
than k that determines the computational cost; in
many other cases, k is the determinant. Further
more, possible post-processing, like matrix factor-
ization, might benefit from a low k.
20NG. The F1 improvements at low dimension-
alities is not quite significant, but AVL remains a
lower level. For higher k, there is less difference in
F1, but the smaller AVL yield lower computational
cost than ?2.
Nevertheless, STS shows a stable behavior for
various dimensionalities and quite different docu-
ment collections. The existence of the universal
constant ? empowers it to be adaptive and practi-
cal. As shown in Figure 11, STS draws the rela-
tive log AVL?(k) to the same straight line, ? log(k),
for different document collections. This might
means that the relative AVL is an intrinsic demand
for the term subset size k.
7 Conclusion
In this paper, Scalable Term Selection (STS) is pro-
posed and supposed to be more adaptive than tra-
ditional high-performing criteria, viz. ?2, IG, BNS,
etc. The basic idea of STS is to separately measure
discriminability and coverage, and adjust the relative
importance between them to produce a optimal term
subset of a given size. Empirically, the constant re-
lation between target dimensionality and the optimal
relative average vector length is found, which turned
the idea into implementation.
STS showed considerable adaptivity and stability
for various dimensionalities and quite different doc-
ument collections. The categorization accuracy in-
creasing at low dimensionalities and the computa-
tional cost decreasing at high dimensionalities were
observed.
Some observations are notable: the loglinear rela-
tion between optimal average vector length (AVL?)
and dimensionality (k), the semi-loglinear relation
between weight ? and dimensionality, and the uni-
versal constant ?. For a future work, STS needs to be
conducted on more document collections to check if
? is really universal.
In addition, there could be other implementations
of the general STS idea, via other metrics of discrim-
inability and coverage, other weighted combination
forms, or other term subset evaluations.
Acknowledgement
The research is supported by the National Natural
Science Foundation of China under grant number
60573187, 60621062 and 60520130299.
References
Janez Brank, Marko Grobelnik, Natas?a Milic-
Fraylingand, and Dunjia Mladenic. 2002. Interaction
of feature selection methods and linear classifica-
tion models. Workshop on Text Learning held at
ICML-2002.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
781
George Forman. 2003. An extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3:1289?1305.
Evgeniy Gabrilovich and Shaul Markovitch. 2004. Text
categorization with many redundant features: using
aggressive feature selection to make svms competitive
with c4.5. In ICML ?04: Proceedings of the twenty-
first international conference on Machine learning,
page 41, New York, NY, USA. ACM Press.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures. In Proceedings of ECML ?98, number 1398,
pages 137?142. Springer Verlag, Heidelberg, DE.
Jingyang Li, Maosong Sun, and Xian Zhang. 2006. A
comparison and semi-quantitative analysis of words
and character-bigrams as features in chinese text cat-
egorization. In Proceedings of COLING-ACL ?06,
pages 545?552. Association for Computational Lin-
guistics, July.
Michael Mitzenmacher. 2003. A brief history of genera-
tive models for power law and lognormal distributions.
Internet Mathematics, 1:226?251.
Monica Rogati and Yiming Yang. 2002. High-
performing feature selection for text classification.
In Proceedings of CIKM ?02, pages 659?661. ACM
Press.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys
(CSUR), 34(1):1?47.
Yiming Yang and Christopher G. Chute. 1994. An
example-based mapping method for text categoriza-
tion and retrieval. ACM Transactions on Information
Systems (TOIS), 12(3):252?277.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Douglas H. Fisher, editor, Proceedings of ICML-
97, 14th International Conference on Machine Learn-
ing, pages 412?420, Nashville, US. Morgan Kauf-
mann Publishers, San Francisco, US.
782
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 545?552,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Comparison and Semi-Quantitative Analysis of Words and  
Character-Bigrams as Features in Chinese Text Categorization 
 
 
Jingyang Li                       Maosong Sun                     Xian Zhang 
National Lab. of Intelligent Technology & Systems, Department of Computer Sci. & Tech. 
Tsinghua University, Beijing 100084, China 
lijingyang@gmail.com  sms@tsinghua.edu.cn  kevinn9@gmail.com 
 
  
 
Abstract 
Words and character-bigrams are both 
used as features in Chinese text process-
ing tasks, but no systematic comparison 
or analysis of their values as features for 
Chinese text categorization has been re-
ported heretofore. We carry out here a 
full performance comparison between 
them by experiments on various docu-
ment collections (including a manually 
word-segmented corpus as a golden stan-
dard), and a semi-quantitative analysis to 
elucidate the characteristics of their be-
havior; and try to provide some prelimi-
nary clue for feature term choice (in most 
cases, character-bigrams are better than 
words) and dimensionality setting in text 
categorization systems. 
1 Introduction1 
Because of the popularity of the Vector Space 
Model (VSM) in text information processing, 
document indexing (term extraction) acts as a 
pre-requisite step in most text information proc-
essing tasks such as Information Retrieval 
(Baeza-Yates and Ribeiro-Neto, 1999) and Text 
Categorization (Sebastiani, 2002). It is empiri-
cally known that the indexing scheme is a non-
trivial complication to system performance, es-
pecially for some Asian languages in which there 
are no explicit word margins and even no natural 
semantic unit. Concretely, in Chinese Text Cate-
gorization tasks, the two most important index-
                                                 
1 This research is supported by the National Natural Science 
Foundation of China under grant number 60573187 and  
60321002, and the Tsinghua-ALVIS Project co-sponsored 
by the National Natural Science Foundation of China under 
grant number 60520130299 and EU FP6. 
ing units (feature terms) are word and character-
bigram, so the problem is: which kind of terms2 
should be chosen as the feature terms, words or 
character-bigrams? 
To obtain an all-sided idea about feature 
choice beforehand,  we review here the possible 
feature variants (or, options). First, at the word 
level, we can do stemming, do stop-word prun-
ing, include POS (Part of Speech) information, 
etc. Second, term combinations (such as ?word-
bigram?, ?word + word-bigram?, ?character-
bigram + character-trigram?3, etc.) can also be 
used as features (Nie et al, 2000). But, for Chi-
nese Text Categorization, the ?word or bigram? 
question is fundamental. They have quite differ-
ent characteristics (e.g. bigrams overlap each 
other in text, but words do not) and influence the 
classification performance in different ways. 
In Information Retrieval, it is reported that bi-
gram indexing schemes outperforms word 
schemes to some or little extent (Luk and Kwok, 
1997; Leong and Zhou 1998; Nie et al, 2000). 
Few similar comparative studies have been re-
ported for Text Categorization (Li et al, 2003) so 
far in literature. 
Text categorization and Information Retrieval 
are tasks that sometimes share identical aspects 
(Sebastiani, 2002) apart from term extraction 
(document indexing), such as tfidf term weight-
ing and performance evaluation. Nevertheless, 
they are different tasks. One of the generally ac-
cepted connections between Information Re-
trieval and Text Categorization is that an infor-
mation retrieval task could be partially taken as a 
binary classification problem with the query as 
the only positive training document. From this 
                                                 
2 The terminology ?term? stands for both word and charac-
ter-bigram. Term or  combination of terms (in word-bigram 
or other forms) might be chosen as ?feature?. 
3 The terminology ?character? stands for Chinese character, 
and ?bigram? stands for character-bigram in this paper. 
545
viewpoint, an IR task and a general TC task have 
a large difference in granularity. To better illus-
trate this difference, an example is present here. 
The words ????(film producer)? and ???
?(dubbed film)? should be taken as different 
terms in an IR task because a document with one 
would not necessarily be a good match for a 
query with the other, so the bigram ???(film 
production)? is semantically not a shared part of 
these two words, i.e. not an appropriate feature 
term. But in a Text Categorization task, both 
words might have a similar meaning at the cate-
gory level (?film? category, generally), which 
enables us to regard the bigram ???? as a se-
mantically acceptable representative word snip-
pet for them, or for the category. 
There are also differences in some other as-
pects of IR and TC. So it is significant to make a 
detailed comparison and analysis here on the 
relative value of words and bigrams as features 
in Text Categorization. The organization of this 
paper is as follows: Section 2 shows some ex-
periments on different document collections to 
observe the common trends in the performance 
curves of the word-scheme and bigram-scheme; 
Section 3 qualitatively analyses these trends; 
Section 4 makes some statistical analysis to cor-
roborate the issues addressed in Section 3; Sec-
tion 5 summarizes the results and concludes. 
2 Performance Comparison 
Three document collections in Chinese language 
are used in this study. 
The electronic version of Chinese Encyclo-
pedia (?CE?): It has 55 subject categories and 
71674 single-labeled documents (entries). It is 
randomly split by a proportion of 9:1 into a train-
ing set with 64533 documents and a test set with 
7141 documents. Every document has the full-
text. This data collection does not have much of 
a sparseness problem. 
The training data from a national Chinese 
text categorization evaluation4 (?CTC?): It has 
36 subject categories and 3600 single-labeled5 
documents. It is randomly split by a proportion 
of 4:1 into a training set with 2800 documents 
and a test set with 720 documents. Documents in 
this data collection are from various sources in-
cluding news websites, and some documents 
                                                 
4 The Annual Evaluation of  Chinese Text Categorization 
2004, by 863 National Natural Science Foundation. 
5 In the original document collection, a document might 
have a secondary category label. In this study, only the pri-
mary category label is reserved. 
may be very short. This data collection has a 
moderate sparseness problem. 
A manually word-segmented corpus from 
the State Language Affairs Commission 
(?LC?): It has more than 100 categories and 
more than 20000 single-labeled documents6. In 
this study, we choose a subset of 12 categories 
with the most documents (totally 2022 docu-
ments). It is randomly split by a proportion of 2:1 
into a training set and a test set. Every document 
has the full-text and has been entirely word-
segmented7 by hand (which could be regarded as 
a golden standard of segmentation). 
All experiments in this study are carried out at 
various feature space dimensionalities to show 
the scalability. Classifiers used in this study are 
Rocchio and SVM. All experiments here are 
multi-class tasks and each document is assigned 
a single category label. 
The outline of this section is as follows: Sub-
section 2.1 shows experiments based on the Roc-
chio classifier, feature selection schemes besides 
Chi and term weighting schemes besides tfidf to 
compare the automatic segmented word features 
with bigram features on CE and CTC, and both 
document collections lead to similar behaviors; 
Subsection 2.2 shows experiments on CE by a 
SVM classifier,  in which, unlike with the Roc-
chio method, Chi feature selection scheme and 
tfidf term weighting scheme outperform other 
schemes; Subsection 2.3 shows experiments by a 
SVM classifier with Chi feature selection and 
tfidf term weighting on LC (manual word seg-
mentation) to compare the best word features 
with bigram features. 
2.1 The Rocchio Method and Various Set-
tings 
The Rocchio method is rooted in the IR tradition, 
and is very different from machine learning ones 
(such as SVM) (Joachims, 1997; Sebastiani, 
2002). Therefore, we choose it here as one of the 
representative classifiers to be examined. In the 
experiment, the control parameter of negative 
examples is set to 0, so this Rocchio based classi-
fier is in fact a centroid-based classifier. 
Chimax is a state-of-the-art feature selection 
criterion for dimensionality reduction (Yang and 
Peterson, 1997; Rogati and Yang, 2002). Chi-
max*CIG (Xue and Sun, 2003a) is reported to be 
better in Chinese text categorization by a cen-
                                                 
6 Not completed. 
7 And POS (part-of-speech) tagged as well. But POS tags 
are not used in this study. 
546
troid based classifier, so we choose it as another 
representative feature selection criterion besides 
Chimax. 
Likewise, as for term weighting schemes, in 
addition to tfidf, the state of the art (Baeza-Yates 
and Ribeiro-Neto, 1999), we also choose 
tfidf*CIG (Xue and Sun, 2003b). 
Two word segmentation schemes are used for 
the word-indexing of documents. One is the 
maximum match algorithm (?mmword? in the 
figures), which is a representative of simple and 
fast word segmentation algorithms.  The other is 
ICTCLAS8 (?lqword? in the figures). ICTCLAS 
is one of the best word segmentation systems 
(SIGHAN 2003) and reaches a segmentation 
precision of more than 97%, so we choose it as a 
representative of state-of-the-art schemes for 
automatic word-indexing of document). 
For evaluation of single-label classifications,  
F1-measure, precision, recall and accuracy 
(Baeza-Yates and Ribeiro-Neto, 1999; Sebastiani, 
2002) have the same value by microaveraging9, 
and are labeled with ?performance? in the fol-
lowing figures. 
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
p
e
rf
o
rm
a
n
ce
mmword
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
lqword
p
e
rf
o
rm
a
n
ce
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
bigram
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf      
chicig-tfidfcig
 
Figure 1. chi-tfidf and chicig-tfidfcig on CE 
Figure 1 shows the performance-
dimensionality curves of the chi-tfidf approach 
and the approach with CIG, by mmword, lqword 
and bigram document indexing, on the CE 
document collection. We can see that the original 
chi-tfidf approach is better at low dimensional-
ities (less than 10000 dimensions), while the CIG 
version is better at high dimensionalities and 
reaches a higher limit.10 
                                                 
8 http://www.nlp.org.cn/project/project.php?proj_id=6 
9 Microaveraging is more prefered in most cases than 
macroaveraging (Sebastiani 2002). 
10 In all figures in this paper, curves might be truncated due 
to the large scale of dimensionality, especially the curves of 
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
p
e
rf
o
rm
a
n
ce
mmword
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
lqword
p
e
rf
o
rm
a
n
ce
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
bigram
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf      
chicig-tfidfcig
 
Figure 2. chi-tfidf and chicig-tfidfcig on CTC 
Figure 2 shows the same group of curves for 
the CTC document collection. The curves fluctu-
ate more than the curves for the CE collection 
because of sparseness; The CE collection is more 
sensitive to the additions of terms that come with 
the increase of dimensionality. The CE curves in 
the following figures show similar fluctuations 
for the same reason. 
For a parallel comparison among mmword, 
lqword and bigram schemes, the curves in  Fig-
ure 1 and Figure 2 are regrouped and shown in 
Figure 3 and Figure 4. 
2 4 6 8
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf
mmword
lqword
bigram
2 4 6 8
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
dimensionality
chicig-tfidfcig
mmword
lqword
bigram
 
Figure 3. mmword, lqword and bigram on CE 
 
1 2 3 4 5
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf
mmword
lqword
bigram
1 2 3 4 5
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
dimensionality
chicig-tfidfcig
mmword
lqword
bigram
 
Figure 4. mmword, lqword and bigram on CTC 
                                                                          
bigram scheme. For these kinds of figures, at least one of 
the following is satisfied: (a) every curve has shown its 
zenith; (b) only one curve is not complete and has shown a 
higher zenith than other curves; (c) a margin line is shown 
to indicate the limit of the incomplete curve. 
547
We can see that the lqword scheme outper-
forms the mmword scheme at almost any dimen-
sionality, which means the more precise the word 
segmentation the better the classification per-
formance. At the same time, the bigram scheme 
outperforms both of the word schemes on a high 
dimensionality, wherea the word schemes might 
outperform the bigram scheme on a low dimen-
sionality. 
Till now, the experiments on CE and CTC 
show the same characteristics despite the per-
formance fluctuation on CTC caused by sparse-
ness. Hence in the next subsections CE is used 
instead of both of them because its curves are 
smoother. 
2.2 SVM on Words and Bigrams 
As stated in the previous subsection, the lqword 
scheme always outperforms the mmword scheme; 
we compare here only the lqword scheme with 
the bigram scheme.  
Support Vector Machine (SVM) is one of the 
best classifiers at present (Vapnik, 1995; 
Joachims, 1998), so we choose it as the main 
classifier in this study. The SVM implementation 
used here is LIBSVM (Chang, 2001); the type of 
SVM is set to ?C-SVC? and the kernel type is set 
to linear, which means a one-with-one scheme is 
used in the multi-class classification. 
Because the CIG?s effectiveness on a SVM 
classifier is not examined in Xue and Sun (2003a, 
2003b)?s report, we make here the four combina-
tions of schemes with and without CIG in feature 
selection and term weighting. The experiment 
results are shown in Figure 5. The collection 
used is CE. 
 
1 2 3 4 5 6 7
x 10
4
0.6
0.65
0.7
0.75
0.8
0.85
0.9
p
e
rf
o
rm
a
n
ce
dimensionality
lqword
chi-tfidf      
chi-tfidfcig   
chicig-tfidf   
chicig-tfidfcig
1 2 3 4 5 6 7
x 10
4
0.6
0.65
0.7
0.75
0.8
0.85
0.9
dimensionality
bigram
chi-tfidf      
chi-tfidfcig   
chicig-tfidf   
chicig-tfidfcig
 
Figure 5. chi-tfidf and cig-involved approaches 
on lqword and bigram 
Here we find that the chi-tfidf combination 
outperforms any approach with CIG, which is the 
opposite of the results with the Rocchio method. 
And the results with SVM are all better than the 
results with the Rocchio method. So we find that 
the feature selection scheme and the term 
weighting scheme are related to the classifier, 
which is worth noting. In other words, no feature 
selection scheme or term weighting scheme is 
absolutely the best for all classifiers. Therefore, a 
reasonable choice is to select the best performing 
combination of feature selection scheme, term 
weighting scheme and classifier, i.e. chi-tfidf and 
SVM. The curves for the lqword scheme and the 
bigram scheme are redrawn in Figure 6 to make 
them clearer. 
1 2 3 4 5 6 7
x 10
4
0.75
0.8
0.85
0.9
p
e
rf
o
rm
a
n
c
e
dimensionality
lqword
bigram
 
Figure 6. lqword and bigram on CE 
The curves shown in Figure 6 are similar to 
those in Figure 3. The differences are: (a) a lar-
ger dimensionality is needed for the bigram 
scheme to start outperforming the lqword scheme; 
(b) the two schemes have a smaller performance 
gap. 
The lqword scheme reaches its top perform-
ance at a dimensionality of around 40000, and 
the bigram scheme reaches its top performance 
at a dimensionality of around 60000 to 70000, 
after which both schemes? performances slowly 
decrease. The reason is that the low ranked terms 
in feature selection are in fact noise and do not 
help to classification, which is why the feature 
selection phase is necessary. 
2.3 Comparing Manually Segmented 
Words and Bigrams 
0 1 2 3 4 5 6 7 8 9 10
x 10
4
72
74
76
78
80
82
84
86
88
dimansionality
p
e
rf
o
rm
a
n
c
e
word        
bigram      
bigram limit
 
Figure 7. word and bigram on LC 
548
Up to now, bigram features seem to be better 
than word ones for fairly large dimensionalities. 
But it appears that word segmentation precision 
impacts classification performance. So we 
choose here a fully manually segmented docu-
ment collection to detect the best performance a 
word scheme could  reach and compare it with 
the bigram scheme. 
Figure 7 shows such an experiment result on 
the LC document collection (the circles indicate 
the maximums and the dash-dot lines indicate the 
superior limit and the asymptotic interior limit of 
the bigram scheme). The word scheme reaches a 
top performance around the dimensionality of 
20000, which is a little higher than the bigram 
scheme?s zenith around 70000. 
Besides this experiment on 12 categories of 
the LC document collection, some experiments 
on fewer (2 to 6) categories of this subset were 
also done, and showed similar behaviors. The 
word scheme shows a better performance than 
the bigram scheme and needs a much lower di-
mensionality. The simpler the classification task 
is, the more distinct this behavior is. 
3 Qualitative Analysis 
To analyze the performance of words and bi-
grams as feature terms in Chinese text categori-
zation, we need to investigate two aspects as fol-
lows. 
3.1 An Individual Feature Perspective 
The word is a natural semantic unit in Chinese 
language and expresses a complete meaning in 
text. The bigram is not a natural semantic unit 
and might not express a complete meaning in 
text, but there are also reasons for the bigram to 
be a good feature term. 
First, two-character words and three-character 
words account for most of all multi-character 
Chinese words (Liu and Liang, 1986). A two-
character word can be substituted by the same 
bigram. At the granularity of most categorization 
tasks, a three-character words can often be sub-
stituted by one of its sub-bigrams (namely the 
?intraword bigram? in the next section)  without 
a change of meaning. For instance, ???? is a 
sub-bigram of the word ????(tournament)? 
and could represent it without ambiguity. 
Second, a bigram may overlap on two succes-
sive words (namely the ?interword bigram? in 
the next section), and thus to some extent fills the 
role of a word-bigram. The word-bigram as a 
more definite (although more sparse) feature  
surely helps the classification. For instance, ??
?? is a bigram overlapping on the two succes-
sive words ? ? ? (weather)? and ? ? ?
(forecast)?, and could almost replace the word-
bigram (also a phrase) ?????(weather fore-
cast)?, which is more likely to be a representative 
feature of the category ????(meteorology)? 
than either word. 
Third, due to the first issue, bigram features 
have some capability of identifying OOV (out-
of-vocabulary) words 11 , and help improve the 
recall of classification. 
The above issues state the advantages of bi-
grams compared with words. But in the first and 
second issue, the equivalence between bigram 
and word or word-bigram is not perfect. For in-
stance, the word ???(literature)? is a also sub-
bigram of the word ????(astronomy)?, but 
their meanings are completely different. So the 
loss and distortion of semantic information is a 
disadvantage of bigram features over word fea-
tures.  
Furthermore, one-character words cover about 
7% of words and more than 30% of word occur-
rences in the Chinese language; they are effev-
tive in the word scheme and are not involved in 
the above issues. Note that the impact of effec-
tive one-character words on the classification is 
not as large as their total frequency, because the 
high frequency ones are often too common to 
have a good classification power, for instance, 
the word ?? (of, ?s)?. 
3.2 A Mass Feature Perspective 
Features are not independently acting in text 
classification. They are assembled together to 
constitute a feature space. Except for a few mod-
els such as Latent Semantic Indexing (LSI) 
(Deerwester et al, 1990), most models assume 
the feature space to be orthogonal. This assump-
tion might not affect the effectiveness of the 
models, but the semantic redundancy and com-
plementation among the feature terms do impact 
on the classification efficiency at a given dimen-
sionality. 
According to the first issue addressed in the 
previous subsection, a bigram might cover for 
more than one word. For instance, the bigram 
???? is a sub-bigram of the words ???
(fabric)?,???? (cotton fabric)?, ????
(knitted fabric)?, and also a good substitute of 
                                                 
11 The ?OOV words? in this paper stand for the words that 
occur in the test documents but not in the training document. 
549
them. So, to a certain extent, word features are 
redundant with regard to the bigram features as-
sociated to them. Similarly, according to the sec-
ond issue addressed, a bigram might cover for 
more than one word-bigram. For instance, the 
bigram ???? is a sub-bigram of the word-
bigrams (phrases) ?????(short story)?, ??
???(novelette)?, ?????(novel)? and also 
a good substitute for them. So, as an addition to 
the second issue stated in the previous subsection, 
a bigram feature might even cover for more than 
one word-bigram. 
On the other hand, bigrams features are also 
redundant with regard to word features associ-
ated with them. For instance, the ???? and ??
?? are both sub-bigrams of the previously men-
tioned word ?????. In some cases, more than 
one sub-bigram can be a good representative of a 
word. 
We make a word list and a bigram list sorted 
by the feature selection criterion in a descending 
order. We now try to find how the relative re-
dundancy degrees of the word list and the bigram 
list vary with the dimensionality. Following is-
sues are elicited by an observation on the two 
lists (not shown here due to space limitations). 
The relative redundancy rate in the word list 
keeps even while the dimensionality varies to a 
certain extent, because words that share a com-
mon sub-bigram might not have similar statistics 
and thus be scattered in the word feature list. 
Note that these words are possibly ranked lower 
in the list than the sub-bigram because feature 
selection criteria (such as Chi) often prefer 
higher frequency terms to lower frequency ones, 
and every word containing the bigram certainly 
has a lower frequency than the bigram itself. 
The relative redundancy in the bigram list 
might be not as even as in the word list. Good 
(representative) sub-bigrams of a word are quite 
likely to be ranked close to the word itself. For 
instance, ???? and ???? are sub-bigrams of 
the word ????(music composer)?, both the 
bigrams and the word are on the top of the lists. 
Theretofore, the bigram list has a relatively large 
redundancy rate at low dimensionalities. The 
redundancy rate should decrease along with the 
increas of dimensionality for: (a) the relative re-
dundancy in the word list counteracts the redun-
dancy in the bigram list, because the words that 
contain a same bigram are gradually included as 
the dimensionality increases; (b) the proportion 
of interword bigrams increases in the bigram list 
and there is generally no redundancy between 
interword bigrams and intraword bigrams. 
Last, there are more bigram features than word 
features because bigrams can overlap each other 
in the text but words can not. Thus the bigrams 
as a whole should theoretically contain more in-
formation than the words as a whole. 
From the above analysis and observations, bi-
gram features are expected to outperform word 
features at high dimensionalities. And word fea-
tures are expected to outperform bigram features 
at low dimensionalities.  
4 Semi-Quantitative Analysis 
In this section, a preliminary statistical analysis 
is presented to corroborate the statements in the 
above qualitative analysis and expected to be 
identical with the experiment results shown in 
Section 1. All statistics in this section are based 
on the CE document collection and the lqword 
segmentation scheme (because the CE document 
collection is large enough to provide good statis-
tical characteristics). 
4.1 Intraword Bigrams and Interword Bi-
grams 
In the previous section, only the intraword bi-
grams were discussed together with the words. 
But every bigram may have both intraword oc-
currences and interword occurrences. Therefore 
we need to distinguish these two kinds of bi-
grams at a statistical level. For every bigram, the 
number of intraword occurrences and the number 
of interword occurrences are counted and we can 
use 
 
1
log
1
interword#
intraword#
+? ?? ?+? ?  
as a metric to indicate its natual propensity to be 
a intraword bigram. The probability density of 
bigrams about on this metric is shown in Figure 
8. 
-12 -10 -8 -6 -4 -2 0 2 4 6 8 10
0
0.05
0.1
0.15
0.2
0.25
log(intraword#/interword#)
p
ro
b
a
b
ili
ty
 d
e
n
s
it
y
 
Figure 8. Bigram Probability Density on 
log(intraword#/interword#) 
550
The figure shows a mixture of two Gaussian 
distributions, the left one for ?natural interword 
bigrams? and the right one for ?natural intraword 
bigrams?. We can moderately distinguish these 
two kinds of bigrams by a division at -1.4. 
4.2 Overall Information Quantity of a Fea-
ture Space 
The performance limit of a classification is re-
lated to the quantity of information used. So a 
quantitative metric of the information a feature 
space can provide is need. Feature Quantity (Ai-
zawa, 2000) is suitable for this purpose because 
it comes from information theory and is additive; 
tfidf was also reported as an appropriate metric of 
feature quantity (defined as ?probability ? infor-
mation?). Because of the probability involved as 
a factor, the overall information provided by a 
feature space can be calculated on training data 
by summation. 
The redundancy and complementation men-
tioned in Subsection 3.2 must be taken into ac-
count in the calculation of overall information 
quantity. For bigrams, the redundancy with re-
gard to words associated with them between two 
intraword bigrams is given by 
 { }
1,2
1 2( ) min ( ), ( )
b w
tf w idf b idf b
?
??  
in which b1 and b2 stand for the two bigrams and 
w stands for any word containing both of them. 
The overall information quantity is obtained by 
subtracting the redundancy between each pair of 
bigrams from the sum of all features? feature 
quantity (tfidf). Redundancy among more than 
two bigrams is ignored. For words, there is only 
complementation among words but not redun-
dancy, the complementation with regard to bi-
grams associated with them is given by 
 
{ } if  exists;
if  does not exists.
( ) min ( ) ,
( ) ( ),
b w
b
b
tf w idf b
tf w idf w
?
????
???
 
in which b is an intraword bigram contained by 
w. The overall information is calculated by 
summing the complementations of all words. 
4.3 Statistics and Discussion 
Figure 9 shows the variation of these overall in-
formation metrics on the CE document collection. 
It corroborates the characteristics analyzed in 
Section 3 and corresponds with the performance 
curves in Section 2.  
Figure 10 shows the proportion of interword 
bigrams at different dimensionalities, which also 
corresponds with the analysis in Section 3. 
0 2 4 6 8 10 12 14 16
x 10
4
0
2
4
6
8
10
12
14
16
x 10
7
dimensionality
o
v
e
ra
ll 
in
fo
rm
a
ti
o
n
 q
u
a
n
ti
ty
word  
bigram
 
Figure 9. Overall Information Quantity on CE 
The curves do not cross at exactly the same 
dimensionality as in the figures in Section 1, be-
cause other complications impact on the classifi-
cation performance: (a) OOV word identifying 
capability, as stated in Subsection 3.1; (b) word 
segmentation precision; (c) granularity of the 
categories (words have more definite semantic 
meaning than bigrams and lead to a better per-
formance for small category granularities); (d) 
noise terms, introduced in the feature space dur-
ing the increase of dimensionality. With these 
factors, the actual curves would not keep increas-
ing as they do in Figure 9. 
0 2 4 6 8 10 12 14 16
x 10
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
dimensionality
in
te
rw
o
rd
 b
ig
ra
m
 p
ro
p
o
rt
io
n
 
Figure 10. Interword Bigram Proportion on CE 
5 Conclusion 
In this paper, we aimed to thoroughly compare 
the value of words and bigrams as feature terms 
in text categorization, and make the implicit 
mechanism explicit. 
Experimental comparison showed that the Chi 
feature selection scheme and the tfidf term 
weighting scheme are still the best choices for 
(Chinese) text categorization on a SVM classifier. 
In most cases, the bigram scheme outperforms 
the word scheme at high dimensionalities and 
usually reaches its top performance at a dimen-
551
sionality of around 70000. The word scheme of-
ten outperforms the bigram scheme at low di-
mensionalities and reaches its top performance at 
a dimensionality of less than 40000. 
Whether the best performance of the word 
scheme is higher than the best performance 
scheme depends considerably on the word seg-
mentation precision and the number of categories. 
The word scheme performs better with a higher 
word segmentation precision and fewer (<10) 
categories. 
A word scheme costs more document indexing 
time than a bigram scheme does; however a bi-
gram scheme costs more training time and classi-
fication time than a word scheme does at the 
same performance level due to its higher dimen-
sionality. Considering that the document index-
ing is needed in both the training phase and the 
classification phase, a high precision word 
scheme is more time consuming as a whole than 
a bigram scheme. 
As a concluding suggestion: a word scheme is 
more fit for small-scale tasks (with no more than 
10 categories and no strict classification speed 
requirements) and needs a high precision word 
segmentation system; a bigram scheme is more 
fit for large-scale tasks (with dozens of catego-
ries or even more) without too strict training 
speed requirements (because a high dimensional-
ity and a large number of categories lead to a 
long training time). 
Reference 
Akiko Aizawa. 2000. The Feature Quantity: An In-
formation Theoretic Perspective of Tfidf-like 
Measures, Proceedings of ACM SIGIR 2000, 104-
111. 
Ricardo Baeza-Yates, Berthier Ribeiro-Neto. 1999. 
Modern Information Retrieval, Addison-Wesley 
Chih-Chung Chang, Chih-Jen Lin. 2001. LIBSVM: A 
Library for Support Vector Machines, Software 
available at http://www.csie.ntu.edu.tw/~cjlin/ 
libsvm 
Steve Deerwester, Sue T. Dumais, George W. Furnas, 
Richard Harshman. 1990. Indexing by Latent Se-
mantic Analysis, Journal of the American Society 
for Information Science, 41:391-407. 
Thorsten Joachims. 1997. A Probabilistic Analysis of 
the Rocchio Algorithm with TFIDF for Text Cate-
gorization, Proceedings of 14th International Con-
ference on Machine Learning (Nashville, TN, 
1997), 143-151. 
Thorsten Joachims. 1998. Text Categorization with 
Support Vector Machine: Learning with Many 
Relevant Features, Proceedings of the 10th Euro-
pean Conference on Machine Learning, 137-142. 
Mun-Kew Leong, Hong Zhou. 1998. Preliminary 
Qualitative Analysis of Segmented vs. Bigram In-
dexing in Chinese, The 6th Text Retrieval Confer-
ence (TREC-6), NIST Special Publication 500-240, 
551-557. 
Baoli Li, Yuzhong Chen, Xiaojing Bai, Shiwen Yu. 
2003. Experimental Study on Representing Units in 
Chinese Text Categorization, Proceedings of the 
4th International Conference on  Computational 
Linguistics and Intelligent Text Processing (CI-
CLing 2003), 602-614. 
Yuan Liu, Nanyuan Liang. 1986. Basic Engineering 
for Chinese Processing ? Contemporary Chinese 
Words Frequency Count, Journal of Chinese In-
formation Processing, 1(1):17-25. 
Robert W.P. Luk, K.L. Kwok. 1997. Comparing rep-
resentations in Chinese information retrieval. Pro-
ceedings of ACM SIGIR 1997, 34-41. 
Jianyun Nie, Fuji Ren. 1999. Chinese Information 
Retrieval: Using Characters or Words? Informa-
tion Processing and Management, 35:443-462. 
Jianyun Nie, Jianfeng Gao, Jian Zhang, Ming Zhou. 
2000. On the Use of Words and N-grams for Chi-
nese Information Retrieval, Proceedings of 5th In-
ternational Workshop on Information Retrieval 
with Asian Languages 
Monica Rogati, Yiming Yang. 2002. High-performing 
Feature Selection for Text Classification, Proceed-
ings of ACM Conference on Information and 
Knowledge Management 2002, 659-661. 
Gerard Salton, Christopher Buckley. 1988. Term 
Weighting Approaches in Automatic Text Retrieval, 
Information Processing and Management, 
24(5):513-523. 
Fabrizio Sebastiani. 2002. Machine Learning in 
Automated Text Categorization, ACM Computing 
Surveys, 34(1):1-47 
Dejun Xue, Maosong Sun. 2003a. Select Strong In-
formation Features to Improve Text Categorization 
Effectiveness, Journal of Intelligent Systems, Spe-
cial Issue. 
Dejun Xue, Maosong Sun. 2003b. A Study on Feature 
Weighting in Chinese Text Categorization, Pro-
ceedings of the 4th International Conference on  
Computational Linguistics and Intelligent Text 
Processing (CICLing 2003), 594-604. 
Vladimir Vapnik. 1995. The Nature of Statistical 
Learning Theory, Springer. 
Yiming Yang, Jan O. Pederson. 1997.  A Comparative 
Study on Feature Selection in Text Categorization, 
Proceedings of ICML 1997, 412-420. 
552
