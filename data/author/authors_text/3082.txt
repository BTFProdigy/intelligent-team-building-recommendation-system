Subcategorization Acquisition and Evaluation for Chinese Verbs 
Xiwu Han, Tiejun Zhao, Haoliang Qi, Hao Yu 
Department of Computer Science,  
Harbin Institute of Technology, 150001 Harbin, China 
{hxw, tjzhao, qhl, yh}@mtlab.hit.edu.cn 
 
Abstract 
This paper describes the technology and an ex-
periment of subcategorization acquisition for 
Chinese verbs. The SCF hypotheses are gener-
ated by means of linguistic heuristic information 
and filtered via statistical methods. Evaluation 
on the acquisition of 20 multi-pattern verbs 
shows that our experiment achieved the similar 
precision and recall with former researches. Be-
sides, simple application of the acquired lexicon 
to a PCFG parser indicates great potentialities of 
subcategorization information in the fields of 
NLP. 
Credits 
This research is sponsored by National Natural 
Science Foundation (Grant No. 60373101 and 
603750 19), and High-Tech Research and Devel-
opment Program (Grant No. 2002AA117010-09). 
Introduction 
Since (Brent 1991) there have been a consider-
able amount of researches focusing on verb lexi-
cons with respective subcategorization informa-
tion specified both in the field of traditional lin-
guistics and that of computational linguistics. As 
for the former, subcategory theories illustrating 
the syntactic behaviors of verbal predicates are 
now much more systemically improved, e.g. 
(Korhonen 2001). And for auto-acquisition and 
relevant application, researchers have made great 
achievements not only in English, e.g. (Briscoe 
and Carroll 1997), (Korhonen 2003), but also in 
many other languages, such as Germany (Schulte 
im Walde 2002), Czech (Sarkar and Zeman 
2000), and Portuguese (Gamallo et. al 2002). 
However, relevant theoretical researches on 
Chinese verbs are generally limited to case gram-
mar, valency, some semantic computation theo-
ries, and a few papers on manual acquisition or 
prescriptive designment of syntactic patterns. 
Due to irrelevant initial motivations, syntactic 
and semantic generalizabilities of the consequent 
outputs are not in such a harmony that satisfies 
the description granularity for SCF (Han and 
Zhao 2004). The only auto-acquisition work for 
Chinese SCF made by (Han and Zhao 2004) de-
scribes the predefinition of 152 general frames 
for all verbs in Chinese, but that experiment is 
not based on real corpus. After observing and 
analyzing quantity of subcategory phenomena in 
real Chinese corpus in the People?s Daily 
(Jan.~June, 1998), we removed from Han & 
Zhao?s predefinition 15 SCFs that are actually 
similar derivants of others, and then with this 
foundation and linguistic rules from (Zhao 2002) 
as heuristic information we generated SCF hy-
potheses from the corpus of People?s Daily 
(Jan.~June, 1998), and statistically filtered the 
hypotheses into a Chinese verb SCF lexicon. As 
far as we know, this is the first attempt of Chi-
nese SCF auto-acquisition based on real corpus. 
In the rest of this paper, the second section de-
scribes a comprehensive system that builds verb 
SCF lexicons from large real corpus, the respec-
tive operating principles, and the knowledge 
coded in our SCF. The third section analyzed the 
acquired lexicon with two experiments: one 
evaluated the acquisition results of 20 verbs with 
multi syntactic patterns against manual gold 
standard; the other checked the performance of 
the lexicon when applied in a PCFG parser. The 
forth section compares and contrasts this research 
with related works done by others. And at last, 
Section 5 concludes our present achievements, 
disadvantages and possible future focuses. 
 
1   SCF Acquisition 
1.1 The Acquisition Method 
There are generally 4 steps in the process of our 
auto-acquisition experiment. First, the corpus is 
processed with a cascaded HMM parser; second, 
every possible local patterns for verbs are ab-
stracted; and then, the verb patterns are classified 
into SCF hypotheses according to the predefined 
set; at last, hypotheses are filtered statistically 
and the respective frequencies are also recorded. 
The actual application program consists of 6 
parts as shown in the following paragraphs. 
a. Segmenting and tagging: The raw cor-
pus is segmented into words and tagged 
with POS by the comprehensive seg-
menting and tagging processor devel-
oped by MTLAB of Computer 
Department in Harbin Institute of Tech-
nology. The advantage of the POS defi-
nition is that it describes some subsets of 
nouns and verbs in Chinese. 
b. Parsing: The tagged sentences are parsed 
with a cascaded HMM parser1, devel-
oped by MTLAB of HIT, but only the 
intermediate parsing results are used. 
The training set of the parser is 20,000 
sentences in the Chinese Tree Bank2 of 
(Zhao 2002). 
c. Error-driven correction: Some key errors 
occurring in the former two parts are 
corrected according to manually ob-
tained error-driven rules, which are gen-
erally about words or POS in the corpus. 
d. Pattern abstraction: Verbs with largest 
governing ranges are regarded as predi-
cates, then local patterns, previous 
phrases and respective syntactic tags are 
abstracted, and isolated parts are com-
bined, generalized or omitted according 
to basic phrase rules in (Zhao 2002). 
e. Hypothesis generation: Based on lin-
guistic restraining rules, e.g. no more 
than two NP?s occurring in a series and 
no more than three in one pattern, and 
no PP TP MP occurring with NP before 
any predicates (Han and Zhao 2004), the 
patterns are coordinated and classified 
into the predefined SCF groups. In this 
part, about 5% unclassifiable patterns 
are removed. 
                                                           
1 When evaluated on auto-tagged open corpus, the parser?s 
phrase precision if 62.3%, and phrase recall is 60.9% (Meng, 
2003). 
2 A sample of the tree bank or relevant introduction could be 
found at http://mtlab.hit.edu.cn. 
f. Hypothesis filtering: According to the 
statistical reliability of each type of the 
SCF hypotheses and the linguistic prin-
ciple that arguments occur more fre-
quently with predicates than adjuncts do, 
the hypotheses are filtered by means of 
statistical methods, in this paper which 
are binomial hypotheses testing (BHT) 
and maximum likelihood estimation 
(MLE). 
 
Table 1: An Example of Auto-acquisition 
 
No Actions Results 
a) Input ????????????
???????? 
b) Tag and 
parse 
BNP[BMP[?/m ?/q ]?/ng ]
? /p NDE[ ? ? ? /r ?
/usde ]BVP[ ? ? /vg ?
/vq ]BVP[ ? ? /vg ?
/ut ]NP[??/nc ?/usde ??
/ng ]?/wj 
c) Correct 
errors 
BNP[BMP[?/m ?/q ]?/ng ]
?/p NDE[???/r ?/usde ?
?/vg ?/vq ]BVP[??/vg ?
/LE ]NP[??/nc ?/usde ??
/ng ]?/wj 
d) Abstract 
patterns 
BNP PP BVP[vg LE ] NP 
e) Generate 
hypothesis
NP v NP ?01000? 
f) Filter hy-
potheses 
NP v NP {01111}3 
 
In Table 1, for example, when acquiring SCF 
information for ???? (prove) and a related sen-
tence in the corpus is a), our tagger and parser 
will return b), and error-driven correction will 
return c) with errors of NDE and the 1st BVP cor-
rected4. Since the governing range of ???? is 
larger than that of ???? (ask), the other verb in 
this sentence, the program abstracts its local pat-
tern BVP[vg LE] and previous phrase BNP, gen-
                                                           
3  {01000} projects to the Chinese syntactic mor-
phemes {?????????}, 1 means the SCF 
may occur with the respective morpheme, while 0 
may not (Han & Zhao, 2004). 
4 Note that not all errors in this example have been corrected, 
but this doesn?t affect further procession. Also, for defini-
tions of NDE and BVP see (Zhao, 2002). 
eralizes BNP and NDE as NP, combines the sec-
ond NP with isolated part ??/p? into PP, and 
returns d). Then the hypothesis generator returns 
e) as the possible SCF in which the verb may 
occurs. Actually in the corpus there are 621 hy-
pothesis tokens generated, and among them 92 
ones are of same arguments with e), and thus e) 
can pass the hypothesis testing (See also Section 
1.2), so we obtain one SCF for ???? as f). 
1.2 Filtering Methods 
In researches of subcategorization acquisition, 
statistical methods for hypothesis filtering mainly 
include the BHT, the Log Likelihood Ratio 
(LLR), the T-test and the MLE, and the most 
popular one is the BHT. Since (Brent 1993) be-
gan to use the method, most researchers have 
agreed that the BHT results in better precision 
and recall with SCF hypotheses of high, medium 
and low frequencies. Only (Korhonen 2001) re-
ports 11.9% total performance of the MLE better 
than the BHT. Therefore, we applied the two sta-
tistical methods in our present experiment. This 
subsection chiefly illustrates the expressions of 
our methods and definitions of parameters in 
them, while performance comparison of the two 
will be introduced in Section 3. 
When applying the BHT method, it is nec-
essary to determine the probability of the primi-
tive event. As for SCF acquisition, the co-
occurrence of one predefined SCF scfi with one 
verb v is the relevant primitive event, and the 
concerned probability is p(v|scfi) here. However, 
the aim of filtering is to rule out those unreliable 
hypotheses, so it is the probability that one primi-
tive event doesn't occur that is often used for 
SCF hypothesis testing, i.e. the error probability: 
pe(v|scfi) = 1 p(v|scfi). (Brent 1993) estimated pe 
according to the acquisition system?s perform-
ance, while (Briscoe and Carroll 1997) calculated 
pe from the distribution of SCF types in ANLT 
and SCF tokens in Susanne as shown in the fol-
lowing equation. 
 
Brent?s method mainly depends on the related 
corpus and processing program, which may 
cause intolerable errors. Briscoe and Carroll?s 
method draws on both linguistic and statistical 
information thus leading to comparatively stable 
estimation, and therefore has been used by many 
latter researches, e.g. (Korhonen 2001). But there 
is no MRD proper for Chinese SCF description 
so we estimated pe from the 1,775 common verbs 
and SCF tokens in the related corpus of 43,000 
sentences used by (Han and Zhao 2004). We 
formed the equation as follows: 
 
Then the number of all hypotheses about verb 
vj is recorded as n, and the number of those for 
scfi as m. According to Bernoulli theory, the 
probability P that an event with probability p ex-
actly happens m times out of n such trials is:  
 
And the probability that the event happens m or 
more times is: 
 
In turn, P(m+, n, pe) is the probability that scfi 
wrongly occurs m or more times with a verb that 
doesn't match it. Therefore, a threshold of 0.05 
on this probability will yield a 95% confidence 
that a high enough proportion of hypotheses for 
scfi have been observed for the verb legitimately 
to be assigned scfi (Korhonen 2001). 
The MLE method is closely related to the general 
performance of the concerned SCF acquisition 
system. First, we randomly draw from the ap-
plied corpus a training set, which is large enough 
so as to ensure similar SCF frequency distribu-
tion. Then, the frequency of scfi occurring with a 
verb vj is recorded and used to estimate the actual 
probability p(scfi| vj). Thirdly, an empirical 
threshold is determined, such that it ensures 
maximum value of F measure on the training set. 
Finally, the threshold is used to filter out those 
SCF hypotheses with low frequencies from the 
total set. 
2    Experimental Evaluation 
2.1   Acquisition Performance 
 
Using the previously described theory and tech-
nology we have acquired an SCF lexicon for 
3,558 common Chinese verbs from the corpus of 
People?s Daily (Jan.~June, 1998). In the lexicon 
the minimum number of SCF tokens for a verb is 
30, and the maximum is 20,000. In order to check 
the acquisition performance of the used system, 
we evaluated a part of the lexicon against a man-
ual gold standard. The testing set includes 20 
verbs of multi syntactic patterns, and for each 
verb there are 503~2,000 SCF tokens with the 
total number of 18,316 (See Table 2). Table 3 
gives the evaluation results for different filtering 
methods, including non-filtering 5 , BHT, and 
MLE with thresholds of 0.001, 0.005, 0.008 and 
0.01. We calculated the type precision and recall 
by the following expressions as (Korhonen 2001) 
did:  
 
In here, true positives are correct SCF types 
proposed by the system, false positives are incor-
rect SCF types proposed by system, and false 
negatives are correct SCF types not proposed by 
the system. 
 
Table 2: Verbs in the Testing Set6 
 
Verbs English Tokens Verbs English Tokens
? Read 503 ?? Hope 620 
?? Find 529 ? See 645 
?? Reckon 543 ?? Invest 679 
? Pull 544 ?? Know 722 
?? Report 612 ? Send 800 
?? Develop 1,006 ?? Set up 1,186
?? Behave 1,007 ?? Insist 1,200
?? Decide 1,038 ? Think 1,200
?? End 1,140 ?? Require 1,200
?? Begin 1142 ? Write 2,000
 
According to Table 3, all other filtering meth-
ods outperform non-filtering, and MLE is better 
than BHT. Among the four MLE thresholds, 
0.008 achieves the best comprehensive perform-
ance but its F-measure is only 0.74 larger than 
that of 0.01 while its precision drops by 2.4 per-
cent. Hence, we chose 0.01 as the threshold for 
the whole experiment with purpose to meet the 
practical requirement of high precision and to 
avoid possible over-fit phenomena. Finally, with 
a confidence of 95% we can estimate the general 
performance of the acquisition system with preci-
sion of  60.6% +/- 2.39%, and recall of 51.3%+/-
2.45%. 
                                                           
5 Non-filtering means filtering with a zero threshold or not 
filtering at all. This method is used as baseline here.  
6 The English meanings given here are not intended to cover 
the whole semantic range of the respective verbs, on the 
contrary they are just for readers? reference. 
 
Table 3: System Performance for Different 
 Filtering Methods 
 
          Measures
Methods Precision Recall F-measure
Non-filtering 37.43% 85.9% 52.14 
BHT 50% 57.2% 53.36 
0.001 39.2% 85.9% 53.83 
0.005 40.3% 83.33% 54.33 
0.008 58.2% 54.5% 56.3 MLE
0.01 60.6% 51.3% 55.56 
 
2.2    Task-oriented Evaluation 
In order to further analyze the practicability of 
the previously described technology, we per-
formed a simple task-oriented evaluation, apply-
ing the acquired SCF lexicon in a PCFG parser 
helping to choose from the n-best parsing results. 
The concerned parser was trained from 10,000 
manually parsed Chinese sentences7. In this ex-
periment there are 664 verbs and their SCF in-
formation involved. The open testing set consists 
of 1,500 sentences, for each of which the PCFG 
parser outputs 5-best parsing results. Then SCF 
hypotheses are generated for each result by 
means of the formerly mentioned technology. 
Finally, the maximum likelihood between hy-
potheses and those SCF types for the related verb 
in the lexicon is calculated in the following way:  
 
where i ? 5, hi is one of the hypotheses generated 
for the parsing results, and scfj is the jth SCF type 
for the concerned verb. This calculation keeps the 
likelihood between 0 and 1. The parsing result 
                                                           
7 These sentences and the testing corpus mentioned latter are 
all taken from the Chinese Tree Bank developed by MTLAB 
of HIT, and a sample may be downloaded at 
http://mtlab.hit.edu.cn. 
with maximum likelyhood is then regarded as the 
final choice. When two or more hypotheses hold 
the same likelihood, the one with larger or largest 
PCFG probability will be chosen.  
Table 4 shows the phrase-based and sentence-
based evaluation results for the parser without 
and with SCF heuristic information. There are 
three cased included: a) The output is one-best; b) 
The output is 5-best and the best evaluation result 
is recorded; c) The 5-best output is checked again 
for the best syntactic tree by means of SCF in-
formation. The phrased-based evaluation follows 
the popular method for evaluating a parser, while 
the sentence-based depends on the intersection of 
the parsed trees and those in the gold standard. 
Since the PCFG parser output at least one syntac-
tic tree for every sentence in our testing corpus, 
the sentence-based precision and recall are equal 
to each other. 
 
Table 4: Parsing Evaluation 
 
Phrase-based Sentence-
based 
Parsing  
Methods 
Precision Recall Precision  
= Recall 
One-best 57.5% 55% 13.64% 
5-best 65.28% 64.59% 26.2% 
With SCF 62.86% 62.1% 21.66% 
 
Table 4 shows that SCF information remarka-
bly improved the performance of the PCFG 
parser: the phrase-based precision increased by 
5.36% and recall by 7.1%, while the sentence-
based precision and recall both increased by 
8.04%. However, this doesn?t reach the upper 
limit of the 5-best. The possible reasons are: a) 
the our present SCF lexicon remains to be im-
proved; b) our method of applying SCF informa-
tion to the parser is too simple, e.g. probabilities 
of PCFG parsing results haven?t been exploited 
thoroughly. 
 
3 Related Works 
As far as we know, this is the first attempt to 
automatically acquire SCF information from real 
Chinese corpus and the first trial to apply SCF 
lexicon to a Chinese parser. Our research draws a 
lot on related works from international researches, 
and for the purpose of crosslingual processing, 
our research is kept in consistency with SCF 
conventions as much as possible. 
Due to linguistic differences, nevertheless, not 
all theories, methods or experiences could adapt 
to Chinese. Generally, there are four aspects that 
our research differs from those of other lan-
guages. First, the SCF formalization of most 
former researches follows the Levin style, in 
which most SCFs omit NP before predicates, 
while Chinese SCFs need to depict arguments 
occurring before verbs. Second, except (Sarkar 
and Zeman 2000), most former researches are 
based on manual SCF predefinition, while our 
predefined SCF set is statistically acquired (See 
Han and Zhao 2004). Third, involved parsers of 
former researches are mostly better than Chinese 
parsers to some degree. Forth, our SCF informa-
tion also includes 5 syntactic morphemes (See 
also Section 1.1). 
Meanwhile, the basic purpose for Chinese SCF 
acquisition is also to determine the subcategory 
features for a verb via its argument distributions 
and then apply the lexicon to NLP tasks. There-
fore, under similar cases the respective evalua-
tions are comparable. And Table 5 gives the 
comparison between our research and the best 
English results without semantic backoff 8  in 
(Korhonen 2001). 
 
Table 5: Performance Comparison Between 
Chinese and English Researches 
 
                Filtering 
Measures    Non BHT MLE
Ours 37.43% 50% 58.2%Precision Korhonen 24.3% 50.3% 74.8%
Ours 85.9% 57.2% 54.5%Recall Korhonen 83.5% 56.6% 57.8%
Ours 52.14 53.36 56.3F-
measure Korhonen 37.6 53.3 65.2
 
The comparison shows that our nonfiltering re-
sult is better than Korhonen?s, both BHT results 
are similar, while our MLE result is much worse 
                                                           
8 Semantic backoff is a method of generating SCF hypothe-
ses according to the semantic classification of the concerned 
verb. Note that this paper doesn?t involve verb meanings for 
generating hypotheses. Besides, though the evaluation for 
English SCF acquisition is the best, it?s not the newest. For 
the newest, please refer to (Korhonen 2003), in which the 
precision is 71.8% and recall is 34.5%. 
than Korhonen?s. That means our hypothesis 
generator performs well but our filtering method 
remains to be improved. According to the analy-
sis of relevant corpus, we found the main cause 
might be that low frequency SCF types account 
for 32% in our corpus while those in (Korhonen 
2001) sum to nearly 21%. 
Further more, (Briscoe and Carroll 1997) ap-
plied their acquired English SCF lexicon to an 
intermediate parser, and reported a 7% improve-
ment of both phrase-based precision and recall. 
Our application of SCF lexicon to a PCFG parser 
leads to 5.36% improvement for phrase-based 
precision, 7.1% for recall, and 8.04% for sen-
tence-based precision and recall. 
 
4    Conclusion 
 
This paper for the first time describes a largescale 
experiment of automatically acquiring SCF lexi-
con from real Chinese corpus. Perfor mance eva-
luation shows that our technology and acquiring 
program have achieved similar performance 
compared with former researches of other lan-
guages. And the application of the acquired lexi-
con to a PCFG parser indicates great potentiali-
ties of SCF information in the field of NLP. 
However, there is still a large gap between 
Chinese subcategorization works and those of o-
ther languages. Our future work will focus on the 
optimization of linguistic heuristic information 
and filtering methods, the application of semantic 
backoff, and the exploitation of SCF lexicon for 
other NLP tasks. 
References  
Brent, M. R. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceed-
ings of the 29th Annual Meeting of the Association 
for Computational Linguistics, Berkeley, CA. 209-
214. 
Brent, M. 1993. From Grammar to Lexicon: un-
supervised learning of lexical syntax. Compu-
tational Linguistics 19.3. 243-262. 
Briscoe, Ted and John Carroll, 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th ACL Conference on Applied 
Natural Language Processing, Washington, DC. 
Dorr, B. J. Gina-Anne Levow, Dekang Lin, and Scott 
Thomas, 2000. Chinese-English Semantic Resource 
Construction, 2nd International Conference on 
Language Resources and Evaluation (LREC2000), 
Athens, Greece, pp. 757--760. 
Gamallo, P., Agustini, A. and Lopes Gabriel P., 2002. 
Using Co-Composition for Acquiring Syntactic 
and Semantic Subcategorisation, ACL-02.  
Han, Xiwu, Tiejun Zhao, 2004. FML-Based SCF Pre-
definition Learning for Chinese Verbs. Interna-
tional Joint Conference of NLP 2004. 
Jin, Guangjin, 2001. Semantic Computations for Mod-
ern Chinese Verbs. Beijing University Press, Bei-
jing. (in Chinese) 
Korhonen, Anna, 2001. Subcategorization Acquistion, 
Dissertation for Ph.D, Trinity Hall University of 
Cambridge. 29-77. 
Korhonen, Anna, 2003. Clustering Polysemic Sub-
categorization Frame Distributions Semantically. 
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, pp. 64-71. 
Meng, Yao, 2003. Research on Global Chinese Pars-
ing Model and Algorithm Based on Maximum En-
tropy. Dissertation for Ph.D. Computer Department, 
HIT. 33-34. 
Sabine Shulte im Walde, 2002. Inducing German Se-
mantic Verb Classes from Purely Syntactic Sub-
categorization Information. Proceedings of the 40st 
ACL, pp. 223-230. 
Sarkar, A. and Zeman, D. 2000. Automatic Ex-
traction of Subcategorization Frames for 
Czech. In Proceedings of the 19th Interna-
tional Conference on Computational Linguis-
tics, aarbrucken, Germany. 
Zhan Weidong, 2000. Valence Based Chinese Seman-
tic Dictionary, Language and Character Applica-
tions, Volume 1. (in Chinese) 
Zhao Tiejun, 2002. Knowledge Engineering Report 
for MTS2000.  
 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 331?336,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving English Subcategorization Acquisition with Diathesis Al-
ternations as Heuristic Information 
Xiwu Han 
Institute of Computational 
Linguistics 
Heilongjiang University 
Harbin City 150080 China 
hxw@hlju.edu.cn 
Tiejun Zhao 
School of Computer Science and 
Technology 
Harbin Institute of Technology 
Harbin City 150001 China 
tjzhao@mtlab.hit.edu.cn
Xingshang Fu 
Institute of Computational 
Linguistics 
Heilongjiang University 
Harbin City 150080 China
fxs@hlju.edu.cn 
 
  
 
Abstract 
Automatically acquired lexicons with 
subcategorization information have al-
ready proved accurate and useful enough 
for some purposes but their accuracy still 
shows room for improvement. By means 
of diathesis alternation, this paper pro-
poses a new filtering method, which im-
proved the performance of Korhonen?s 
acquisition system remarkably, with the 
precision increased to 91.18% and recall 
unchanged, making the acquired lexicon 
much more practical for further manual 
proofreading and other NLP uses. 
1 Introduction 
Subcategorization is the process that further clas-
sifies a syntactic category into its subsets. Chom-
sky (1965) defines the function of strict subcate-
gorization features as appointing a set of con-
straints that dominate the selection of verbs and 
other arguments in deep structure. Large sub-
categorized verbal lexicons have proved to be 
crucially important for many tasks of natural 
language processing, such as probabilistic pars-
ers (Korhonen, 2001, 2002) and verb classifica-
tions (Schulte im Walde, 2002; Korhonen, 2003).  
Since Brent (1993) a considerable amount of re-
search focusing on large-scaled automatic acqui-
sition of subcategorization frames (SCF) has met 
with some success not only in English but also in 
many other languages, including German 
(Schulte im Walde, 2002), Spanish (Chrupala, 
2003), Czech (Sarkar and Zeman, 2000), Portu-
guese (Gamallo et. al, 2002), and Chinese (Han 
et al 2004). The general objective of this re-
search is to acquire from a given corpus the SCF 
types and numbers for predicate verbs. Two typi-
cal steps during the process of automatic acquisi-
tion are hypothesis generation and selection. 
Usually based on heuristic rules, the first step 
generates SCF hypotheses for involved verbs; 
and the second selects reliable ones via statistical 
methods, such as BHT (binomial hypothesis test-
ing), LLR (log likelihood ratio) and MLE 
(maximum likelihood estimation). This second 
step is also called statistical filtering and has 
been widely regarded as problematic. English 
researchers have proposed some methods adjust-
ing the corpus hypothesis frequencies before or 
while filtering. These methods are often called 
backoff techniques for SCF acquisition. Some of 
them represent a remarkable improvement in the 
acquisition performance, for example diathesis 
alternation and semantic motivation (Korhonen, 
1998, 2001, 2002). 
For the convenience of comparison between 
performances of different SCF acquisition meth-
ods, we define absolute and relative recall in this 
paper. By absolute recall, we mean the figure 
computed against the background of input corpus, 
while relative recall is against the set of gener-
ated hypotheses.  
At present, automatically acquired verb lexi-
cons with SCF information have already proved 
accurate and useful enough for some NLP pur-
poses (Korhonen, 2001; Han et al 2004). As for 
English, Korhonen (2002) reported that semanti-
cally motivated SCF acquisition achieved a pre-
cision of 87.1%, an absolute recall of 71.2% and 
a relative recall of 85.27%, thus making the ac-
quired lexicon much more accurate and useful. 
However, the accuracy still shows room for im-
provement, especially for those SCF hypotheses 
with low frequencies. Detailed analysis on the 
acquisition system and some resulting data 
shows that three main causes should account for 
the comparatively unsatisfactory performance: a. 
the imperfect hypothesis generator, b. the Zipfian 
331
distribution of syntactic patterns, c. the incom-
plete partition over SCF types of a given verb. 
The first problem mainly comes from the inade-
quate parsing performance and noises existing in 
the corpus, while the other two problems are in-
herent to natural languages and should be solved 
in terms of acquisition techniques particularly 
during the process of hypothesis selection. 
2 Related Work 
The empirical background of this paper is the 
public resource for subcategorization acquisition 
of English verbs, provided by Anna Korhonen 
(2005) in her personal home page. The data in-
clude 30 verbs, as shown in Table 1, and their 
unfiltered SCF hypotheses, which were auto-
matically generated via Briscoe and Carroll?s 
(1997) SCF acquisition system, and the manually 
established standard.  
Precision  + Recall 
2 * Precision * Recall 
|True positives|+|False positives|
|True positives| 
|True positives|+|False negatives|
|True positives| 
Table 1. English Verbs in Use. 
add agree attach 
bring carry carve 
chop cling clip 
fly  cut travel 
drag communicate give 
lend lock marry 
meet mix move 
offer provide visit 
push sail send 
slice supply swing 
For each verb, there is a corpus of 1000 sen-
tences extracted from the BNC, and all together 
42 SCF types are involved in the corpus. The 
framework of Briscoe and Carroll?s system con-
sists of six overall components, which are ap-
plied in sequence to sentences containing a spe-
cific predicate in order to retrieve a set of SCFs 
for that verb: 
z A tagger, a first-order Hidden Markov 
Model POS and punctuation tag disam-
biguator. 
z A lemmatizer, an enhanced version of the 
General Architecture for Text Engineering 
project stemmer. 
z A probabilistic LR parser, trained on a 
tree-bank derived semi-automatically from 
the SUSANNE corpus, returns ranked 
analyses using a feature-based unification 
grammar. 
z A pattern extractor, which extracts 
subcategorization patterns, i.e. local 
syntactic frames, including the syntactic 
frames, including the syntactic categories 
and head lemmas. 
z A pattern classifier, which assigns patterns 
to SCFs or rejects them as unclassifiable. 
z A SCF filter, which evaluates sets of SCFs 
gathered for a predicate verb. 
Nowadays, in most related researches, the per-
formances of subcategorization acquisition sys-
tems are often evaluated in terms of precision, 
recall and F measure of SCF types (Korhonen, 
2001, 2002). Generally, precision is the percent-
age of SCFs that the system proposes correctly, 
while recall is the percentage of SCFs in the gold 
standard that the system proposes: 
 
Precision = 
 
 
Recall =  
 
 
F-measure =  
 
Here, true positives are correct SCF types pro-
posed by the system, false positives are incorrect 
SCF types proposed by system, and false nega-
tives are correct SCF types not proposed by the 
system. 
3 The MLE Filtering Method 
The present SCF acquisition system for English 
verbs employs a MLE filter to test the automati-
cally generated SCF hypotheses. Due to noises 
accumulated while tagging, lemmatizing and 
parsing the corpus, even though correction is im-
plemented for some typical errors when classify-
ing the extracted patterns, the hypothesis genera-
tor does not perform as efficiently as hoped. 
Sampling analysis on the unfiltered hypotheses 
in Korhonen?s evaluation corpus indicates that 
about 74% incorrectly proposed and rejected 
SCF types come from the defects of the MLE 
filtering method. 
Performance of the MLE filter is closely re-
lated to the actual distributions p(scfi|v) over 
predicates and SCF types in the input corpus. 
First, from the overall corpus a training set is 
drawn randomly; it must be large enough to en-
sure a similar distribution. Then, the frequency 
of a subcategorization frame scfi occurring with a 
verb v is recorded and used to estimate the prob-
ability p(scfi|v). Thirdly, an empirical threshold ? 
is determined, which ensures that a maximum 
332
value of the F-measure will result for the training 
set. Finally, the threshold is used to filter out 
from the total set those SCF hypotheses with fre-
quencies lower than ?.  
Therefore, the statistical foundation of this fil-
tering method is the assumption of independence 
among the SCFs that a verb enters, which can be 
probabilistically expressed in two formulas as 
follows: 
0),|(,,, =??? vscfscfpjiji ji ? (1) 
?
=
=
n
i
i vscfp
1
1)|(                          ? (2) 
Here, i and j are natural numbers, scfi and scfj are 
two SCF types that verb v enters, and variables in 
formulas henceforth will hold the same meanings. 
In actual application, the probability p(scfi|v) is 
estimated from the observed frequency f(scfi, v), 
and the conditional probability p(scfi|scfj, v) is 
assumed to be zero. This means any two SCF 
types entered by a given verb are taken for 
granted to be probabilistically independent from 
each other. However, this assumption can some-
times be far from appropriate. 
4 Diathesis Alternations and Filtering 
Much linguistic research focusing on child lan-
guage acquisition has revealed that many chil-
dren are able to produce new grammatical sen-
tences from what they have learned (Peters, 1983; 
Ellis, 1985). This implies that the widely-used 
independence assumption in the field of NLP 
may not be very appropriate, at least for syntactic 
patterns. If this assumption should be removed, a 
possible heuristic could be the information of 
diathesis alternations, which is also another con-
vincing counterargument. Diathesis alternations 
are generally regarded as alternative ways in 
which verbs express their arguments. Examples 
are as follows: 
a. He broke the glass. 
b. The glass broke. 
c. Ta1 chi1 le0 pin2guo3. 
? ? ? ???(         ) 
d. Ta1 ba3 pin2guo3 chi1 le01. 
?? ?? ???(         ) 
In the above examples, the English verb break 
takes the causative-inchoative alternation as 
shown in sentences a and b, while sentences c 
and d indicate that the Chinese verb chi1 ? ( , eat) 
may enter the ba-object-raising alternation where 
the object is shifted forward by the preposition 
ba3 ? ( ) to the location between the subject and 
the predicate, as illustrated in Figure 1. 
                                                 
1 The numbers in sentences c and d, which are pinyin nota-
tions, show tones of the Chinese syllables, and the two sen-
tences, in English, generally mean He ate an apple. 
 
 
 
 
 
 
Figure 1. Ba-object-raising Alternation. 
ba3 
Ta1 chi1 le0 pin2guo3. 
ba-object-raising 
Subcategorization of verbs has much to do 
with diathesis alternations, and most SCF re-
searchers regard information of diathesis alterna-
tion as an indispensable part of subcategorization 
(Korhonen, 2001; McCarthy, 2001). Therefore, 
one may conclude that, for subcategorization 
acquisition, the independence assumption sup-
porting the MLE filter is not as appropriate as 
previously thought.  
For a given verb, the assumption will be ap-
propriate and sufficient if and only if there is no 
diathesis alternation between all the SCFs it en-
ters, and formula (1) and (2) in Section 3 are ef-
ficient enough to serve as a foundation for the 
MLE filtering method. Otherwise, if there are 
diathesis alternations between some of the SCFs 
that a verb enters, then formula (1) and (2) must 
be modified as illustrated in formula (3) and (4). 
In either case, for the sake of convenience, it 
would be better to combine the formulas as 
shown in (5) and (6). 
0),|(,,, >??? vscfscfpjiji ji  ? (3) 
?
=
>
n
i
i vscfp
1
1)|(                           ? (4) 
0),|(,,, ???? vscfscfpjiji ji  ? (5) 
?
=
?
n
i
i vscfp
1
1)|(                            ? (6) 
For English verbs, previous research has 
achieved great progress in diathesis alternation 
and relative applications, such as the work of 
Levin (1993) and McCarthy (2001). Besides, 
Korhonen (1998) has proved that diathesis alter-
nation could be used as heuristic information for 
backoff estimates to improve the general per-
formance of subcategorization acquisition. How-
ever, determining where and how to seed the 
heuristic remains difficult. 
Korhonen (1998) employed diathesis alterna-
tions in Briscoe and Carroll?s system to improve 
the performance of their BHT filter. Although 
the precision rate increased from 61.22% to 
333
69.42% and the recall rate from 44.70% to 
50.81%, the results were still not accurate 
enough for possible practical NLP uses.  
Korhonen obtained her one-way diathesis al-
ternations from the ANLT dictionary (Boguraev 
and Briscoe, 1987), calculated the alternating 
probability p(scfj|scfi) according to the number of 
common verbs that took the alternation 
(scfi?scfj), and used formula (7) and (8), where 
w is an empirical weight, to adjust the previously 
estimated p(scfi|v): 
If p(scfi|scfj, v)>0,  
p(scfi|v) = p(scfi|v)?w(p(scfi|v)? 
 p(scfj| scfi))                  ?(7) 
If p(scfi|v)>0 & p(scfj|v)=0,  
p(scfi|v) = p(scfi|v)+w(p(scfi|v)? 
                p(scfj| scfi))                   ?(8)2 
Following the adjustment, a BHT filter with a 
confidence rate of 95% was used to check the 
SCF hypotheses. 
This method removes the assumption of inde-
pendence among SCF types but establishes an-
other assumption of independence between 
p(scfj|scfi) and certain verbs, which assumes that 
all verbs take each diathesis alternation with the 
same probability. Nevertheless, linguistic knowl-
edge tells us that verbs often enter different dia-
thesis alternations and can be classified accord-
ingly. Consider the following examples: 
e. He broke the glass. / The glass broke. 
f. The police dispersed the crowd.  
/ The crowd dispersed. 
g. Mum cut the bread. / *The bread cut. 
Both of the English verbs ?break? and ?disperse? 
can take the causative-inchoative alternation and, 
hence, may be classified together, while the verb 
?cut? does not take this alternation. Therefore, 
the newly established assumption doesn?t fit the 
actual situation either, and the probability sums 
?ip(scfi|v) and ?i,jp(scfi|scfj, v) neither need or 
can be normalized. 
Based on the above methodology, we formed a 
new filtering method with diathesis alternations 
as heuristic information, which is, in fact, de-
rived from the simple MLE filter and based on 
formula (5) and (6). The algorithm can be briefly 
expressed as shown in Table 2. 
 
                                                 
2 For the sake of consistency in this paper and for the con-
venience of understanding, formulae formats here are modi-
fied. They may look different from those of Korhonen 
(1998), but they are actually the same. 
Table 2. The New Filtering Method. 
For hypotheses of a given verb v, 
1. if p(scfi|v) > ?1,  
accept scfi into the output set S; 
2. else  
if p(scfi|v) > ?2, 
& p(scfi|scfj, v) > 0, 
& scfj?S, 
accept scfi into set S; 
3. Go to step 1 until S doesn?t increase.
In our method, two filters are employed. For 
each verb involved, first a common MLE filter is 
used, but it employs a threshold ?1 that is much 
higher than usual, and those SCF hypotheses that 
satisfy the requirement are accepted. Then, all of 
the remainder of the hypotheses are checked by 
another MLE filter seeded with diathesis alterna-
tions as heuristic information and equipped with 
a much lower threshold ?2. Any hypothesis scfi 
left out by the first filter will be accepted if its 
probability exceeds ?2 and it is an alternative of 
an SCF type scfj that has been accepted by the 
first filter, which means that p(scfi|scfj, v)>0 and 
scfj?S. The filtering process will be performed 
repeatedly for those unaccepted hypotheses until 
no more hypotheses can be accepted for the verb. 
5 Experimental Evaluation 
We implemented an acquisition experiment on 
Korhonen?s evaluation resources with the above-
mentioned filtering method.  
The diathesis alternations in use are also those 
provided by Korhonen, except that we used them 
in a two-way manner (scfi??scfj) instead of 
one-way (scfi?scfj), because the two involved 
SCF types are usually alternative pragmatic for-
mats of the concerned verb, as shown in exam-
ples in Section 3 and 4. 
In the experiment we empirically set ?1= 0.2, 
which is ten times of Korhonen?s threshold for 
her MLE filter; ?2= 0.002, which is one tenth of 
Korhonen?s. Thus, in a token set of hypotheses 
no more than 1000, an SCF type scfi will be ac-
cepted if it occurs two times or more and has a 
diathesis alternative type scfj already accepted for 
the verb. 
The gold standard was the manually analysed 
results by Korhonen. Precision, recall and F-
measure were calculated via expressions given in 
Section 2.  
Table 3 lists the performances of the baseline 
method of non-filtering (No_f), MLE filtering 
with ? = 0.02, and our filtering method on the 
334
evaluation corpus, and also gives the best results 
of Korhonen's method that is using extra seman-
tic information (Kor) to make a comparison. 
Here, Ab_R is the absolute recall ratio, Re_R the 
relative recall ratio, Ab_F the absolute F-
measure that is calculated from Precision and 
Ab_R, and Re_F the relative F-measure that is 
from Precision and Re_R. 
Table 3. Performance Comparison. 
Methods No-f MLE  ours Kor 
P(%) 47.85 67.89 91.18 87.1 
Ab_R(%) 34.62 32.52 32.52 71.2 
Re_R(%) 100 93.93 93.93 85.27
Ab_F 40.17 43.98 47.94 78.35
Re_F 64.73 78.81 92.53 86.18
The evaluation shows that our new filtering 
method improved the acquisition performance 
remarkably: a. Compared with MLE, precision 
increased by 23.29%, recall ratio remained un-
changed, absolute F-measure increased by 3.96, 
and relative F-measure increased by 13.72; b. 
Compared with Korhonen?s best results, preci-
sion, Re_R and Re_F also increased respec-
tively 3 . Thus, the general performance of our 
filtering method makes the acquired lexicon 
much more practical for further manual proof-
reading and other NLP uses. 
What?s more, the data shown in Table 3 im-
plies that there is little room left for improvement 
of the statistical filter, since the absolute recall 
ratio is only 2.1% lower than that of the non-
filtering method. Whereas, detailed analysis of 
the evaluation corpus shows that the hypothesis 
generator accounts for about 95% of those unre-
called and wrongly recalled SCF types, which 
indicates, for the present time, more improve-
ment efforts need to be made on the first step of 
subcategorization acquisition, i.e. hypothesis 
generation. 
6 Conclusion 
Our new filtering method removed the inappro-
priate assumptions and takes much more advan-
                                                 
3 Korhonen (2002) reported the non-filtering absolute recall 
ratio of her experiment was about 83.5%. She didn?t give 
any explanation with her evaluation resources why here 
non-filtering Ab_R was so much lower. Therefore, the 
Ab_R and Ab_F figures are not comparable here. 
tage of what can be observed in the corpus by 
drawing on the alternative relationship between 
SCF hypotheses with higher and lower frequen-
cies. Unlike the semantically motivated method 
(Korhonen, 2001, 2002), which is dependent on 
verb classifications that linguistic resources are 
able to provide, our filter needs no prior knowl-
edge other than reasonable diathesis alternation 
information and may work well for most verbs in 
other languages with sufficient predicative to-
kens. 
Our experimental results suggest that the pro-
posed technique improves the general perform-
ance of the English subcategorization acquisition 
system, and leaves only a little room for further 
improvement in statistical filtering methods. 
However, approaches that are more complicated 
still exist theoretically, for instance, some SCF 
types unseen by the hypothesis generator may be 
recalled by integrating semantic verb-
classification information into the system. 
More essential aspects of our future work, 
however, will focus on improving the perform-
ance of the hypothesis generator, and testing and 
applying the acquired subcategorization lexicons 
in some concrete NLP tasks. 
Acknowledgement This research has been 
jointly sponsored by the NSFC project No. 
60373101 and the post-doctor scholarship of for-
eign linguistics and literature in Heilongjiang 
University. And at the same time, our great 
thanks go to Dr. Anna Korhonen for her public 
evaluation resources, and Dr. Chrys Chrystello 
for his helpful advice on the English writing of 
this paper. 
References 
Boguraev B. K., E. J. Briscoe. Large lexicons for 
natural language processing utilizing the grammar 
coding system of the Longman Dictionary of Con-
temporary English. Computational Linguistics, 
1987: 219-240 
Brent, M., From Grammar to Lexicon: unsupervised 
learning of lexical syntax, Computational Linguis-
tics 19(3) 1993: 243-262. 
Briscoe, Ted and John Carroll, Automatic extraction 
of subcategorization from corpora, Proceedings of 
the 5th ACL Conference on Applied Natural Lan-
guage Processing, Washington, DC, 1997: 356-
363. 
Chomsky, Noam, Aspects of the Theory of Syntax, 
MIT Press, Cambridge, 1965. 
Chrupala, Grzegorz, Acquiring Verb Subcategoriza-
tion from Spanish Corpora, PhD program ?Cogni-
335
tive Science and Language?, Universitat de Barce-
lona, 2003: 67-68. 
Ellis, R?Understanding Second language Acquisi-
tion, Oxford University Press.1985 
Gamallo, P., Agustini, A. and Lopes Gabriel P., Using 
Co-Composition for Acquiring Syntactic and Se-
mantic Subcategorisation, Proceedings of the 
Workshop of the ACL Special Interest Group on 
the Lexicon (SIGLEX), Philadelphia, 2002: 34-41.  
Han, Xiwu, Tiejun Zhao, Haoliang Qi, and Hao Yu, 
Subcategorization Acquisition and Evaluation for 
Chinese Verbs, Proceedings of the COLING 2004, 
2004: 723-728. 
Korhonen, Anna, Automatic Extraction of Subcatego-
rization Frames from Corpora ?Improving Filtering 
with Diathesis Alternations, 1998. Please refer to 
http://www.folli.uva.nl/CD/1998/pdf/keller/korhon
en.pdf 
Korhonen, Anna, Subcategorization Acquisition, Dis-
sertation for PhD, Trinity Hall University of Cam-
bridge, 2001. 
Korhonen, Anna, Subcategorization Acquisition, 
Technical Report Number 530, Trinity Hall Uni-
versity of Cambridge, 2002. 
Korhonen, Anna, Yuval Krymolowski, Zvika Marx, 
Clustering Polysemic Subcategorization Frame 
Distributions Semantically, Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, 2003: 64-71. 
Korhonen, Anna. Subcategorization Evaluation Re-
sources. http://www.cl.cam.ac.uk/users/alk23/sub-
cat/subcat.html. 2005 
Levin, B., English Verb Classes and Alternations, 
Chicago University Press, Chicago, 1993. 
McCarthy, D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Sub-
categorization Frames and Selectional Preferences, 
PhD thesis, University of Sussex, 2001. 
Peters, A. The Unit of Language Acquisition, Cam-
bridge University Press. 1983. 
Sarkar, A. and Zeman, D., Automatic Extraction of 
Subcategorization Frames for Czech, Proceedings 
of the 19th International Conference on Computa-
tional Linguistics, Saarbrucken, Germany, 2000. 
Please refer to http://www.sfu.ca/~anoop/papers/ 
pdf/coling0final.pdf 
Shulte im Walde, Sabine, Inducing German Semantic 
Verb Classes from Purely Syntactic Subcategoriza-
tion Information, Proceedings of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics, 2002: 223-230. 
336
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 27?33,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Train the Machine with What It Can Learn 
? Corpus Selection for SMT 
Xiwu Han School of Computer Sci-ence and Technology, Heilongjiang University, Harbin City 150080 China 
hxw@hlju.edu.cn 
Hanzhang Li School of Computer Sci-ence and Technology, Heilongjiang University, Harbin City 150080 China 
lhj@hlju.edu.cn 
Tiejun Zhao School of Computer Science and Technology, Harbin Institute of Technology, Harbin City 150001 China 
tjzhao@mtlab.hit.edu.cn 
   
 
Abstract 
Statistical machine translation relies heavily on available parallel corpora, but SMT may not have the ability or intelligence to make full use of the training set. Instead of col-lecting more and more parallel training cor-pora, this paper aims to improve SMT performance by exploiting the full potential of existing parallel corpora. We first iden-tify literally translated sentence pairs via lexical and grammatical compatibility, and then use these data to train SMT models. One experiment indicates that larger train-ing corpora do not always lead to higher de-coding performance when the added data are not literal translations. And another ex-periment shows that properly enlarging the contribution of literal translation can im-prove SMT performance significantly. 
1 Introduction* 
Parallel corpora are generally considered indis-pensable for the training of a translation model in statistical machine translation (SMT). And most researchers tend to agree on the opinion that the more data is used to estimate the parameters of the translation model, the better it can approxi-mate the true translation probabilities, and in turn this will lead to a better translation performance. However, even if large corpora are easily avail-able, does an SMT system have the ability or intelligence to make full use of a training set?  Another aspect is that larger amounts of train-ing data also require larger computational re-
                                                           * This research is jointly supported by the National Natural Science Foundation of China under Grant No.60773069 and 60873169. 
sources. With increasing quantities of training data, the improvement of translation quality will become smaller and smaller. Therefore, while continuing to collect more and more parallel cor-pora, it is also important to seek effective ways of making better use of available parallel training data. Literal translation and free translation are two basic skills of human translation. A literal trans-lation is a translation that follows closely the form of the source language, also known as word-for-word translation (Larson 1984).  According to Mona Baker (1992) translation needs to maintain equivalence at different levels across languages. In bottom-up sequence, these levels are: the word level, the above word level, the grammatical level, the textual level and the pragmatic level. Lower levels of equivalence are often embedded in literal translation and easily maintained, whereas higher levels are very im-portant for free translation and very difficult to be achieved even for experienced translators be-cause this kind of equivalence more often than not calls for thorough analysis and understanding of the source language, which is obviously what an SMT system cannot be capable of. So from this perspective SMT may be regarded as a be-ginner in learning how to translate. The training of statistical machine translation mainly depends on the alignment probabilities estimated from certain frequencies observed in a parallel corpus. Thus, we may say that SMT translates according to its bilingual scanning ex-periences, and there is actually no deep compre-hension during the coding and decoding process. Since human learners of translation generally begin with the comparatively simpler techniques of literal translation, our efforts described in this paper are intended to discover whether a corpus 
27
of literal translations better suits the training of statistical machine translation.  In the following, section 2 introduces our cor-pus and proposes a combined method to recog-nize sentence pairs of literal translation. Section 3 describes our experiments with the acquired corpus on SMT training from two points of view. Section 4 analyzes the results from a linguistic point of view. And the conclusion is given in Section 5 with some suggestion for further work. 
2 Literal Translation Recognition 
Early machine translations were notorious for bad literal translations especially of idioms. However, good literal translation means to trans-late a sentence originally, and to keep the origi-nal message form, including the construction of the sentence, the meaning of the original words, use of metaphors and so on. Such a translation would be fluent and easy to comprehend by tar-get language readers. If we suppose that the training corpus for SMT is mainly constituted of good translations, our first task is to identify those literally translated sentence pairs. 
2.1 Our Corpus 
The corpus used for our experiment consists of 650,000 bilingual sentence pairs of English and Chinese, which were gathered either from public and free Internet resources or from our own translation works. The sentences are either trans-lated from Chinese to English or vice versa.  To facilitate the process of recognition, before the SMT experiment we preprocessed the corpus for the word and POS information, with English sentences parsed by (Collins 1999)?s head-driven parser and Chinese sentences by the head-driven parser of MI&TLAB at Harbin Institute of Tech-nology (Cao 2006). We define the literally translated sentence pairs as those that either embed enough word pairs which can be looked up in a bilingual dic-tionary, or share enough common grammatical categories. Hence, we invented two cross-lingual measures for the recognition of literal translation, i.e. lexical compatibility and grammatical com-patibility. 
2.2 Method of Lexical Compatibility 
The seed version of our bilingual dictionary is made up of 63,483 entries drawn from the bi-lingual dictionary for the rule-based Chinese-English machine translation system of CEMT2K 
developed by MI&TLAB at Harbin Institute of Technology (Zhao 2001). We extended the seed with synonyms from English WordNet v. 1.2 and Chinese Extended Tongyicicilin v. 1.0. The ex-tending algorithm is as follows. 
Input: The seed version dictionary SD, Chi-nese Extended Tongyicicilin CT, and English WordNet EW Output: An extended Chinese English dic-tionary ED Do: a. For each entry in SD, a) extend the Chinese part with all its synonyms found in CT; b) extend the English part with all its synonyms found in EW; c) accept the extended entry into ED. b. For each entry in ED, a) if its Chinese part is a subset of that of another entry, merge them; b) if its English part is a subset of that of another entry, merge them. 
An entry in our final extended dictionary in turn is organized as bilingual synonym classes, and there are altogether 43,820 entries including 212,367 Chinese and English lexical terms. By looking up Chinese-English word pairs in the extended dictionary, we defined the cross-lingual measure of lexical compatibility for a Chinese-English sentence pair as CL. 
wordsallofnumbertotalthe
uplookedpairswordofnumbertheCL =
 
For the recognition task, we employed a maxi-mum likelihood estimation filtering method with an empirical threshold of 0.85 on the lexical compatibility. Sentence pairs would be accepted as literal translation if their lexical compatibility CL > 0.85. Manual analysis on 15,000 sentence pairs showed that for this method the precision is 94.65% and the recall is only 16.84%. The low recall is obviously due to the limitations of our bilingual dictionary. 
2.3 Method of Grammatical Compatibility 
Although the diversity of grammatical categories tends to be great, some common word classes, such as nouns, pronouns, verbs, adjectives, etc, mainly constitute the vocabularies of most natu-ral languages. And our observations on English 
28
and Chinese parallel corpora show that the more literal a translation is, the more equivalent gram-matical categories the pair of sentences may share. We thus define the cross-lingual measure of grammatical compatibility as CG. 
?
= +
+= n
i ii
iiiG GCGEMax
GCGEMinC
1 1|)||,(|
1|)||,(|?  
GEi is an English grammatical category, |GEi| is the number it occurs in the English sentence, and GCi is the Chinese counterpart (see Table 1). n is the number of common grammatical catego-ries that make differences in the special task of recognizing literal translated sentence pairs. ?i is the weight for the respective category, which is trained by a simple gradient descent algorithm on a sample of 10,000 manually analysed sentence pairs. 
i Chinese English 1 noun noun 2 pronoun pronoun 3 verb verb 4 adjective adjective and adverb 
Table 1: Equivalent grammatical categories  For the recognition task, we also employed a maximum likelihood estimation filtering method with an empirical threshold of 0.82 on the gram-matical compatibility. Sentence pairs would be accepted as literal translation if their grammatical compatibility CG > 0.82. Evaluation on the held-out sample of 5,000 sentence pairs shows a precision ratio of 89.5% and a recall ratio of 42.34%. 
2.4 Combination of the Two Methods 
We simply combined the results of the two methods mentioned above to obtain a larger use-ful corpus. It is very interesting that the intersec-tion between the results of the two methods accounts only for a very small part, which is es-timated to be 17.2% of all the identified sentence pairs. The combined recognition results achieved a precision of 92.33% and a recall of 54.78% on the testing sample of 15,000 sentence pairs. And on the total corpus, our combined method ac-quired 201,062 sentence pairs that were classi-fied to be the results of literal translation. Further analysis on the sampled corpus shows that the wrongly unrecalled literally translated sentence pairs and the wrongly recalled ones are mainly due to bad segmentation of Chinese 
words or bad POS tagging results of both the Chinese and English parsers. In contrast, those sentence pairs correctly unrecalled are usually free transcriptions or bad translations. 
3 SMT Experiments 
3.1 Our Corpus and SMT System 
After excluding some too long sentence pairs, we got our final training corpus, which includes 200,000 Chinese-English sentence pairs of literal translation and 400,000 pairs of free translation1. Our evaluation corpus was drawn from the IWSLT Chinese-to-English MT test set of 2004, which includes 506 Chinese sentences and 16 English reference sentences for each Chinese one. Since our focus is not on a specific SMT ar-chitecture, we use the off-the-shelf phrase-based decoder Pharaoh (Koehn 2004). Pharaoh imple-ments a beam search decoder for phrase-based statistical models, and has the advantages of be-ing freely available and widely used. The phrase bilingual lexicon is derived from the intersection of bi-directional IBM Model 4 alignments, ob-tained with GIZA++ (Och and Ney 2003). For better comparison between experimental results, we kept all the system parameters as default, while only tuning our own parameters. 
3.2 Experiment on Incremental Training Corpora 
This experiment was designed to check whether it is true that larger training corpora always lead to better SMT decoding performance. We ran-domly segmented the 400,000 free translation sentence pairs into 4 subsets, with each of them including 100,000 pairs. A baseline SMT model was trained with the 200,000 literal translation sentence pairs, and then 4 other SMT models were trained on extended corpora, of which each later used corpus includes one more subset than the previous one.  The decoding performances in terms of BLEU and NIST scores of all 5 models are listed in the second and third column of Table 2, and the last column gives the numbers of out-of-vocabulary (OOV) words of each model on the test set. Curves in Figure 1 and 2, respectively, show the trajectories of BLEU and NIST scores in accor-dance with the sizes of extended training corpora. 
 
                                                           1  Note that ?free translations? are identified statistically using our recognition method for literal translations. 
29
Corpus Size BLEU NIST OOV 200,000 0.3835 7.0982 47 300,000 0.3695 6.9096 45 400,000 0.4113 7.1242 32 500,000 0.4194 7.1824 21 600,000 0.4138 7.1566 18 
Table 2: SMT performance with extended corpora  
 Figure 1: Trajectory of BLEU score  
 Figure 2: Trajectory of NIST score  A comparison between the different models? BLEU and NIST scores shows that a larger train-ing data set does not necessarily lead to better SMT decoding performance. Based on the literal translation data, when more and more free trans-lation data are added to the training set, the per-formance measures of the relevant SMT models fall at first, then rise, and at finally fall again. Furthermore, according to our manual analysis of the decoding results, free translation data have actually harmed the SMT model. It is just be-cause the much smaller numbers of OOV words have made up for the impairment that the per-formance measures have risen for two times. They, however, will fall when the decrease in OOV words fails to make it up. 
3.3 Experiment on Weighted Training Cor-pora 
This experiment was designed to exploit both the contribution of literal translation and the advan-tage of a large vocabulary from a larger corpus. To achieve such a goal, minor modifications need to be made towards the training corpus and the module of GIZA++.  We start with an SMT training data set X, which includes n bilingual sentence pairs, i.e. the 
input vector X = {x1, x2, x3, ?, xi, ?, xn-1, xn}. During the original training process, every sen-tence pair xi contributes in the same way to the estimation of parameters in the translation model since the corpus has not been weighted. Now we tried to adjust the contribution of xi according to our previous decision whether it is literal transla-tion or free translation. If we set the weight vec-tor to be W = {w1, w2, w3, ?, wi, ?, wn-1, wn}T, the weighted corpus would become X? = WX = {w1x1, w2 x2, w3x3, ?, wixi, ?, wn-1 xn-1, wn xn}, where     Hereby ? is an empirical weighting parameter in the range of 0<= ? <=1. The module of GIZA++ was modified to en-sure that the weights imposed on sentence pairs could be effectively transmitted to smaller trans-lation units. GIZA++ builds word alignments by means of counting occurrences of word pairs in the training corpus. Given a possibly translatable Chinese-English word pair D = <c, e>, the number N of its occurrences in our original train-ing corpus X can be calculated by summing up its occurrence number Nxi in each sentence pair, i.e. ? == ni xiNN 1  
Thus the weighted occurrence number N? of word pair D in the weighted training corpus can be calculated via the following equation. ? ?= =? ?== ni ni xiixiwi NwNN 1 1 )('  
Finally, GIZA++ estimates word alignment parameters on the basis of N?. Apart from this modification, all other parts of PHARAOH had been untouched to guarantee comparable ex-perimental results. We trained five SMT models of different weights on the previously mentioned corpora of free and literal translations. Table 3 lists both the training parameters and relevant decoding per-formances of the five models. Figures 3 and 4 show the trajectories of BLEU and NIST scores in accordance with the weight variable. We can see that the SMT model achieved the best per-formance when ? was set to be 0.67. 
wi  =    ? when xi is literal translation,            1 ? ? otherwise. 
30
 Corpus Size ? BLEU NIST OOV 400,000 0 0.4001 6.9082 23 600,000 0.5 0.4138 7.0796 18 600,000 0.67 0.4259 7.2997 26 600,000 0.8 0.4243 7.2706 39 200,000 1 0.3835 7.0982 47 
Table 3: SMT performances with weighted corpora  
 Figure 3: Trajectory of BLEU score  
 Figure 4: Trajectory of NIST score  Among the five models, that of ? = 0.5 is the baseline since here all sentence pairs contributed in the same way. Those of ? = 0 and 1 are two special cases designed to explore the isolated contribution of free and literal translation corpora in a contrastive way. Hereby the two models of ? = 0.67 and 0.8 are the central part of our ex-periment. According to the performance traject-ories it seems that a reasonable increase in the contribution of the corpus of literal translations effectively improves the decoding performance of the SMT system since the BLEU scores with ? = 0.67 and 0.8 are higher than that of the baseline which are 0.0121 and 0.0105, and of the NIST scores which are 0.2201 and 0.191. Our further analysis of the translation results and the related evaluation scores with different weight parameters showed that there exists some potential for literal translations to be used to im-prove SMT systems.  Our analysis indicates that two facts caused most of the out-of-vocabulary words (see Table 3). First, some OOV words never occurred in the training corpus; second, most others had been pruned off due to their much lower frequencies. Training corpora for ? = 0.67 and 0.8 have the 
same size of as that for ? = 0.5, but they resulted in much more OOV words than those for ? = 0.5 because the lower weight had decreased some related alignment probabilities very much. It seems that the large OOV increase must have counteracted the potential improvement to a cer-tain degree although it did not have a devastating effects in these two cases. Therefore, a proper selection of a corpus of literal translations as training data would contribute more to the im-provement of SMT models should some heuristic pruning methods be employed to avoid a possi-ble OOV increase. 
4 Related work  
There have been a lot of studies on SMT training data. Most of them are focused on parallel data collections. Some work tried to acquire more parallel sentences from the web (Nie et al 1999; Resnik and Smith 2003; Chen et al 2004). Oth-ers extracted parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). These works aim to collect more parallel training corpora, while our work aims to make better use of existing parallel corpora.  Some studies have also been conducted on parallel data selection and adaptation. Eck et al (2005) proposed a method to select more infor-mative sentences based on n-gram coverage. They used n-grams to estimate the importance of a sentence. The more previously unseen n-grams exist in the sentence, the more important the sen-tence is regarded. A TF-IDF weighting scheme was also tried in their method, but did not show improvements over n-grams. Their goal was to decrease the amount of training data to make SMT systems adaptable to small devices.  Some other works select training data accord-ing to domain information of the test set. Hildebrand et al (2005) used an information re-trieval method for translation model adaptation. They selected sentences similar to the test set from available in-of-domain and out-of-domain training data to form an adapted translation model. L? et al (2007) further used smaller adapted data to optimize the distribution of the whole training data. They took advantage both of larger data and adapted data.  Unlike all the above-mentioned studies, our method selected the training corpus according to basic theories of literal and free translation. This is somewhat similar to L? et al (2007), however, our weighting scheme also tried to make use of 
31
both larger and smaller data, which are free translations and literal translations in our case. Besides, there have also been some studies on language model adaptation in recent years, moti-vated by the fact hat large-scale monolingual corpora are easier to obtain than parallel corpora.. Examples are Zhao et al (2004), Eck et al (2004), Zhang et al (2006) and Mauser et al (2006). Since a language model is built for the target language in SMT, a one pass translation is usually needed to generate the n-best translation candidates in language model adaptation. The principle in our research could also be used for translation re-ranking to further improve SMT performance. 
5 Conclusions 
This paper presents a new method to improve statistical machine translation performance by making better use of the available parallel train-ing corpora. We at first identified literally trans-lated sentence pairs by means of lexical and grammatical compatibility, and then used these data to train SMT models. Experimental results show that literal and free translation corpora con-tribute differently to the training of SMT models. It seems that literal translation training data bet-ter suit SMT system at its present level of intelli-gence. The weighted training data can further improve translation performance by enlarging the contribution of literal translations while maintaining a larger vocabulary from the larger corpus of free translations. Detailed analysis shows that a literal translation corpus would con-tribute more to the improvement of SMT models if some heuristic pruning methods would be em-ployed to avoid possible OOV increase. In future work, we will improve our methods in several aspects. Currently, the recognition method for literal translations and the weighting schemes are very simple. It might work better by trying some supervised recognition techniques or using more complicated methods to determine the weights of sentence pairs with variant literal degree. What?s more, our present test corpus is an out-of-domain one, and this might have im-pacted the observations made in this work. Last, employing our method to the language model might also improve translation performance. 
Acknowledgments 
We are obliged to the authors of English Word-Net version 1.2 and Chinese Extended Tongy-icicilin version 1.0 for the free dictionary re-
sources they provided. We also thank the two reviewers for their constructive advices that we referred to when preparing the last version of this paper. 
References  
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the Transla-tion Model for Statistical Machine Translation based on Information Retrieval. Proceedings of EAMT 2005: 133-142. Arne Mauser, Richard Zens, Evgeny Matusov, Sasa Hasan, Hermann Ney. 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation. Proceedings of International Work-shop on Spoken Language Translation: 103-110. Bing Zhao, Matthias Eck, Stephan Vogel. 2004. Lan-guage Model Adaptation for Statistical Machine Translation with structured query models. COLING-2004. Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving Machine Translation Performance by Exploiting Comparable Corpora. Computational Linguistics, 31 (4): 477-504. Dragos Stefan Munteanu and Daniel Marcu. 2006. Extracting Parallel Sub-Sentential Fragments from Comparable Corpora. ACL-2006: 81-88. Franz Josef Och and Hermann Ney. 2003. A system-atic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19-52. Hailong Cao. 2006. Research on Chinese Syntactic Parsing Based on Lexicalized Statistical Model, Dissertation for PhD, Harbin Institute of Technol-ogy, Harbin. Jian-Yun Nie, Michel Simard, Pierre Isabelle, Richard Durand. 1999. Cross-Language Information Re-trieval based on Parallel Texts and Automatic Min-ing of Parallel Texts in the Web. SIGIR-1999: 74-81. Jisong Chen, Rowena Chau, Chung-Hsing Yeh. 2004. Discovering Parallel Text from the World Wide Web. ACSW Frontiers 2004: 157-161. Matthias Eck, Stephan Vogel, and Alex Waibel. 2004. Language Model Adaptation for Statistical Ma-chine Translation Based on Information Retrieval. Proceedings of Fourth International Conference on Language Resources and Evaluation: 327-330. Michael Collins. 1999. Head-Driven Statistical Mod-els for Natural Language Parsing. PhD Disserta-tion, University of Pennsylvania. Mona Baker. 2000. In Other Words: A Coursebook on Translaton, Foreign Language Teaching and Re-search Press, Beijing. Mildred L. Larson. 1984. Meaning-based translation: A guide to cross-language equivalence. Lanham, MD: University Press of America. Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In 6th Conference of the Association for 
32
Machine Translation in the Americas (AMTA), Washington, DC. Philip Resnik and Noah A. Smith. 2003. The Web as a Parallel Corpus. Computational Linguistics, 29(3): 349-380. Tiejun Zhao. 2001. Technical Reports for CEMT2K.  MI&TLAB, Harbin Institute of Technology, Harbin. Yajuan L?, Jin Huang and Qun Liu. 2007. Improving Statistical Machine Translation Performance by Training Data Selection and Optimization. Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing and Computational Natural Language Learning, pp. 343-350. Ying Zhang, Almut Silja Hildebrand, Stephan Vogel.  2006. Distributed Language Modeling for N-best List Re-ranking. EMNLP-2006: 216-223. 
 
33
Proceedings of the 8th International Natural Language Generation Conference, pages 133?137,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Latent User Models for Online River Information Tailoring 
 Xiwu Han1, Somayajulu Sripada1, Kit (CJA) Macleod2, and Antonio A. R. Ioris3 Department of Computing Sciences, University of Aberdeen, UK1 James Hutton Institute, Aberdeen; University of Exeter, Exeter, UK2	 ?School of GeoSciences, University of Edinburgh, UK3 {xiwuhan,yaji.sripada}@abdn.ac.uk  kit.macleod@hutton.ac.uk a.ioris@ed.ac.uk
Abstract 
This paper explores Natural Language Genera-tion techniques for online river information tailoring. To solve the problem of unknown users, we propose ?latent models?, which relate typical visitors to river web pages, river data types, and river related activities. A hierarchy is used to integrate domain knowledge and la-tent user knowledge, and serves as the search space for content selection, which triggers us-er-oriented selection rules when they visit a page. Initial feedback received from user groups indicates that the latent models deserve further research efforts.  1 Introduction Within recent decades, access to online river in-formation has increased exponentially thanks to great progresses in data collection and storage technologies employed by hydrological organiza-tions worldwide (Dixon, 2010). Local residents nearby rivers and those engaged in river related activities are now much better informed and more engaged with data providers than decades ago. However, organizations such as SEPA (Scottish Environment Protection Agency), CEH (Centre for Ecology and Hydrology), EA (Envi-ronment Agency) in UK, and quite a few Cana-dian and Australian ones are working to improve the presentation of river information further. Many of these data providers, who are mostly government agencies, provide descriptive texts along with archived data of flow, level, flood and temperature along with their graphs and/or ta-bles. A typical example of linguistic description from the EA website is shown below:    The river level at Morwick is 0.65 me-tres. This measurement was recorded at 08:45 on 23/01/2013. The typical river 
level range for this location is between 0.27 metres and 2.60 metres. The highest river level recorded at this location is 6.32 metres and the river level reached 6.32 me-tres on 07/09/2008.1    The above descriptive text could vary to some extent according to different river users. For in-stance, it may provide information perceived as good news by farmers whilst other users e.g. ca-noeists or paddlers may interpret the information as bad news for their activity. Such tailored in-formation provision promotes communication efficiency between stakeholders and the relevant government offices (Macleod et al., 2012). We explored data-to-text techniques (Reiter, 2007) in promoting online river information provision. Our engagement activities with river stakehold-ers showed that there could be great difficulties in specifying user groups for online river infor-mation tailoring. First, the relations between do-main knowledge and user knowledge are difficult to be acquired due to domain sensitive challeng-es. Second, for online communication, the issue that users themselves sometimes are not sure about their tasks further hinders user modeling. This paper proposes an alternative approach of latent user models, instead of directly asking us-ers to indicate what they are interested in. 2 User Modeling Problem It has long been argued in NLG research that contents of generated texts should be oriented to users? tasks and existing knowledge. User mod-els are usually employed for the tailoring task. However, user models may not be easily ac-quired. Reiter et al (2003a) claimed that no NLG system actually used detailed user models with non-trivial numbers of users. Most commercial                                                 1 http://www.environment-agency.gov.uk/homeandleisure/ floods/riverlevels/120694.aspx?stationId=8143 
133
NLG systems would rather do with very limited user models, and examples are STOP (Reiter et al., 2003b), SUMTIME-MOUSAM (Sripada et al., 2002), and GIRL (Williams, 2002).     Recent research on user modeling falls into roughly three categories, i.e. explicit, implicit and hybrid approaches 2 . All approaches start with knowledge acquisition. Explicit models then define a finite number of user groups, and finally generate tailored texts for users to choose from, or choose to generate for a unique group at each time, e.g. (Molina, 2011 and 2012). Implicit models, e.g. (Mairesse and Walker, 2011), then construct a framework of human computer inter-action to learn about the values of a finite set of features, and finally generate tailored texts ac-cording to the intersection between domain knowledge and feature values. Hybrid models, e.g. (Bouayad-Agha et al, 2012) and (Dannels et al, 2012), specify both a finite set of user groups and a human computer interaction framework, and finally classify online users into defined groups for tailored generation. 3 Latent User Models Online river information tailoring involves a website, such as SEPA?s, which provides map based (or text based) searchable river infor-mation 3. The NLG task is to generate user-oriented texts while users are navigating the website. Both explicit and implicit user models can be employed for online river information tailoring. A finite set of user groups could be defined according to river-related activities, such as flooding, fishing, canoeing, etc. along with a set of features such as level trends, temperature ranges, etc. Then an interactive navigation mech-anism could ask a user to either choose a group or tailor his/her own parameters, and relevant texts can be generated thereafter.     Unfortunately, our engagement activities with stakeholders showed that it is almost impossible to define user models using mappings from river-related activities to river data features. Further-more, frequent users are reluctant to spend time on specifying their preferences before viewing the river information. For such an NLG task, the uncertainty comes not only from a large variety of river users and stakeholders, but also from the issue that users themselves sometimes are not                                                 2 Note the difference between NLG and HCI user models. The former tailor the output of NLG systems, while the later tailor the systems themselves. 3 http://sepa.org.uk/water/river_levels/river_level_data.aspx 
sure of what data features are associated with making decisions about their activities.    Our efforts on dealing with NLG domain knowledge and user models brought about the idea of extending domain knowledge to statisti-cally cover user knowledge, without explicitly defining user groups or implicitly modeling po-tential users. We argue that non-trivial number of uncertain users can be dynamically and statisti-cally modeled by integrating a module for web mining and Google analytics into the NLG pipe-line system. We regard these statistically estab-lished models as latent since they are hidden be-neath the domain knowledge, and the latent vari-able of typical users is linked to river data types and river related activities. 
 Figure 1. Domain Knowledge with Latent Models    The domain knowledge and latent user models are constructed as a whole in a hierarchical struc-ture, as in Figure 1. We technically maintain this hierarchy as an ontology based on existing ap-proaches e.g. (Bontcheva, 2005; Bouayad-Agha et al, 2012). The general part of the main frame was extracted from hydrology or environment websites, such as SEPA, CEH and EA, with the view that these websites were deliberately estab-lished hierarchically by manual work of domain experts in the fields of hydrology, ecology and/or geology. This part serves as the center of our domain knowledge, which starts with a root node and branches to river catchments, rivers, river stations and river data, while river data consists of water level, water flow, water temperature, etc. There are also some non-hierarchical rela-tions embedded, namely the tributary relation between rivers, the upriver relation between river stations, and the relationship between certain river data and river related activities. In addition 
134
to the time series on the status of the rivers, other information is integrated offline. Then, the do-main knowledge was extended to cover potential users? knowledge and online visiting behaviors. The extended information, or the latent user models, as denoted in italic fonts in Figure 1, includes three parts, i.e. the webpage visiting frequency, the relevance degrees between certain river data and river related activities, and the ranking of popularities of river-related activities for each river station.    Our extension process includes three stages, i.e. web mining, Google analytics, and engage-ment activities. At first, basic and rough infor-mation about river stations was statistically gath-ered by using free or trial version web mining tools, such as spiders and crawlers, and corpus analysis tools. For all combinations of elements respectively from each pair of columns in Table 1, we simply count the tokens of co-occurrence within an empirical window of 10 words. For the co-occurring tokens between a given river station and related activities, the top five tokens were selected by filtering according to one threshold on co-occurrence frequencies and another threshold on frequency differences between ad-jacent ranked types. For the co-occurring tokens between a given activity and river data type, rel-evant tokens were chosen by only one threshold on the co-occurrence frequencies. Finally, the co-occurring types of river stations and river data with high frequencies were used to fine-tune the previously acquired results, supposing that some river stations seldom or never provide some types of river data. 
River Stations Related Activities River Data Type Aberlour Aberuchill Aberuthven Abington Alford Allnabad Almondell Alness Ancrum Anie Apigill Arbroath ? 
Farming Fishing Canoeing Swimming Kayaking Rowing Boating Research Education Hiking Cycling ? ? 
Level Flow Temperature Width Rainfall Wind Pollution Birds Animals Fishes ? ?  Table 1. Basic Domain Knowledge for Extension     We further had the statistically acquired re-sults complemented and modified by Google analytics data for river websites and engagement activities with domain experts and users. Google 
analytics provided us with webpage visiting fre-quencies for each hydrological station, and con-tributed to the ranking of river-related activity for a given station. Knowledge gathered from engagement activities, such as semi-structured interviews and focus groups, was mainly used to confirm the statistically gathered information during the first two stages (as well as refine our overall understanding of data demands, water-related activities and perception of existing communication tools). For example, flood warn-ing information was moved up in the ranks since over 5 million people in England and Wales live and work in properties that are at risk of flooding from rivers or the sea4 (Marsh and Hannaford, 2007). Our present research is limited to rivers in Scotland, involving 107 river catchments, 233 rivers, and 339 river stations. The webpage visit-ing frequencies for these stations were gathered from Google analytics data for the website of SEPA5. The page visiting frequency for each riv-er station is represented by a time series with yearly periodicity, and each period includes 12 numeric elements calculated by dividing the number of monthly visiting times of the station by the total number of monthly visiting times of all river stations. 4 NLG for Online Tailoring Our NLG pipeline system takes numeric data of a given river station as input, and outputs a tai-lored description for that river station. The sys-tem analyzes data of water level, flow, and tem-perature as similar to time series analysis tasks presented in (Turner et al., 2006). Then, the ana-lyzed patterns are interpreted into symbolic con-ceptual representations, including vague expres-sions, which might facilitate users? understand-ing (van Deemter, 2010). SEPA defines normal ranges for river levels and we use these defini-tions in our computations to generate vague ex-pressions. For content selection, we define five sets: S = {s1, s2, ?} the set of stations; A = {a1, a2, ?} the set of activities for a given station; D = {d1, d2, ?}= {{d11, d12, ?}, {d21, d22, ?}, ?} the set of river data sets for a given station; AD = {a1d1, a1d2, ?, a2d1, ?} where aidj refers to in-formation from the interpretation of an activity ai under the condition of data dj; and SAD an over-view on one station. For a river station, using the domain knowledge hierarchy, which embeds la-                                                4 http://www.environment-agency.gov.uk/homeandleisure/ floods/default.aspx. 5 http://www.sepa.org.uk. 
135
tent user models implicitly (Figure 1), we select A ? D ? AD ? SAD as the initial contents. 
 Figure 2. Statistical Schemas    A schema-based approach was employed for document planning. Each schema at the high lev-el is made up of three components: Introduction, Events and Summary. Each of these components has its own substructure as shown in examples in Figure 4. With the estimated probabilistic distri-bution we generate schemas for a station based on its popular activities. We then tailor the text by randomly selecting from users? favorite vo-cabulary, which was acquired from online corpus for different river-related activities. Other words for structural purposes are dependent on certain schemas. Realization was performed using the simpleNLG library (Gatt and Reiter, 2009), and some generated examples are listed in Table 2. 
Schema (1) 
The Tyne at Nungate boasts its excellent salm-on catches. Now with medium steady water level and comparatively low water temperature, many people want to fish some salmons in pools between the rapids or experience whitewater rafting within them, which makes the periphery of Nungate a hot spot. Schema (2) 
The periphery of Tyne at Nungate poses a hot spot now, where many people are fishing or canoeing while appreciating the medium steady water level and comparatively low water tem-perature. No wonder Nungate can boast one of the best salmon catching places. Schema (3) 
The Tyne at Nungate boasts its excellent salm-on catches. Many people may now fish or canoe there thanks to the medium steady water level and comparatively low water temperature, mak-ing the periphery of Nungate a hot spot. Table 2. Some Tailored NLG Examples (Italic fonts denote the tailored lexical realization) 5 Initial Feedback and Conclusion This research is still underway and a thorough evaluation is still pending. We have received valuable feedback from small user groups. Sup-portive examples are: a. An overview about pop-ular river stations can help users? further explora-tion of information to a significant extent; b. A general comprehension for a given river station can be more easily built up by simply reading the generated descriptions, than by solely reading the data and its related graphics; c. Along with the graphics, the generated descriptions can improve the communication efficiency by a large degree. Examples recommending further improve-ment/focus include: a. Schemas filled in with acquired vocabulary sometimes endow the gen-erated document a syntactically and/or semanti-cally unexpected flavor; b. Established users de-mand more linguistic varieties than new users.    Present feedback implicates that latent user models deserve further research. Our future ef-forts will focus on a. extending the domain knowledge to cover all river stations, b. develop-ing generic methodology for acquiring latent user models for other online NLG tasks (e.g. generat-ing descriptions of Census data), and c. integrat-ing an automatic update of latent models. Acknowledgement This research is supported by an award from the RCUK DE programme: EP/G066051/1. The au-thors are also grateful to Dr. Rene van der Wal, Dr. Koen Arts, and the three anonymous review-ers for improving the quality of this paper. 
136
References  K. Bontcheva. 2005. Generating Tailored Textual Summaries from Ontologies. The Semantic Web: Research and Applications, Lecture Notes in Com-puter Science, Vol. 3532, pages 531-545. Springer-Verlag. N. Bouayad-Agha, G. Casamayor, Simon Mille, et al. 2012.  From Ontology to NL: Generation of Multi-lingual User-Oriented Environmental Reports. Natural Language Processing and Information Systems, Lecture Notes in Computer Science Vol. 7337, pages 216-221. Springer-Verlag. Dana Dannells, Mariana Damova, Ramona Enache and Milen Chechev. 2012. Multilingual Online Generation from Semantic Web Ontologies. WWW 2012 ? European Projects Track, pages 239-242. H. Dixon. 2010. Managing national hydrometric data: from data to information. Global Change: Facing Risks and Threats to Water Resources. Walling-ford, UK, IAHS Press, pages 451-458. A. Gatt and Ehud Reiter. 2009. Simplenlg: A Realiza-tion Engine for Practical Applications. Proceedings ENLG-2009, pages 90-93. K. Macleod, S. Sripada, A. Ioris, K. Arts and R. Van der Wal. 2012. Communicating River Level Data and Information to Stakeholders with Different In-terests: the Participative Development of an Inter-active Online Service. International Environmental Modeling and Software Society (iEMSs): Interna-tional Congress on Environmental Modeling and Software Managing Resources of a Limited Planet, Sixth Biennial Meeting, Leipzig, Germany. R. Seppelt, A.A. Voinov, S. Lange, D. Bankamp (Eds.) pages 33-40. Francois Mairesse and Marilyn A. Walker. 2011. Controlling User Perceptions of Linguistic Style: Trainable Generation of Personality Traits. Compu-tational Linguistics, Volume 37 Issue 3, September 2011, pages 455-488. T. J. Marsh and J. Hannaford. 2007. The summer 2007 floods in England and Wales ? a hydrological appraisal. Centre for Ecology & Hydrology, UK. M. Molina. 2012. Simulating Data Journalism to Communicate Hydrological Information from Sen-sor Networks. Proceedings of IBERAMIA, pages 722-731. M. Molina, A. Stent, and E. Parodi. 2011. Generating Automated News to Explain the Meaning of Sen-sor Data. In: Gama, J., Bradley, E., Hollm?n, J. (eds.) IDA 2011. LNCS, vol. 7014, pages 282-293. Springer, Heidelberg. Ehud Reiter, Somayajulu Sripada, and Sandra Wil-liams. 2003a. Acquiring and Using Limited User Models in NLG. In Proceedings of the 9th Europe-
an Workshop on Natural Language Generation, pages 13-14, Budapest, Hungary. Ehud Reiter, Roma Robertson, and Liesl Osman. 2003b. Lessons from a failure: Generating tailored smoking cessation letters. Artificial Intelligence, 144(1-2), pages 41-58.  Ehud Reiter. 2007. An Architecture for Data-to-Text Systems. Proceedings of ENLG-2007, pages 97-104.  S. Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2002. Segmenting time series for weather fore- casting. Applications and Innovations in Intelli- gent Systems X, pages 105-118. Springer-Verlag. R. Turner, S. Sripada, E. Reiter and I. Davy. 2006. Generating Spatio-Temporal Descriptions in Pollen Forecasts. Proceedings of EACL06 poster session, pages 163-166. K. van Deemter. 2010. Vagueness Facilitates Search. Proceedings of the 2009 Amsterdam Colloquium, Springer Lecture Notes in Computer Science (LNCS). FoLLI LNAI 6042. Sandra Williams. 2002. Natural language generation of discourse connectives for different reading lev-els. In Proceedings of the 5th Annual CLUK Re-search Colloquium, Leeds, UK.  
137
