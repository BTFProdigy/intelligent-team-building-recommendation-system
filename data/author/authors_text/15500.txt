Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 293?303,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Predicting the Semantic Compositionality of Prefix Verbs
Shane Bergsma, Aditya Bhargava, Hua He, Grzegorz Kondrak
Department of Computing Science
University of Alberta
{bergsma,abhargava,hhe,kondrak}@cs.ualberta.ca
Abstract
In many applications, replacing a complex
word form by its stem can reduce sparsity, re-
vealing connections in the data that would not
otherwise be apparent. In this paper, we focus
on prefix verbs: verbs formed by adding a pre-
fix to an existing verb stem. A prefix verb is
considered compositional if it can be decom-
posed into a semantically equivalent expres-
sion involving its stem. We develop a clas-
sifier to predict compositionality via a range
of lexical and distributional features, includ-
ing novel features derived from web-scale N-
gram data. Results on a new annotated cor-
pus show that prefix verb compositionality can
be predicted with high accuracy. Our system
also performs well when trained and tested on
conventional morphological segmentations of
prefix verbs.
1 Introduction
Many verbs are formed by adding prefixes to exist-
ing verbs. For example, remarry is composed of a
prefix, re-, and a stem, marry. We present an ap-
proach to predicting the compositionality of prefix
verbs. The verb remarry is compositional; it means
to marry again. On the other hand, retire is gener-
ally non-compositional; it rarely means to tire again.
There is a continuum of compositionality in prefix
verbs, as in other complex word forms and multi-
word expressions (Bannard et al, 2003; Creutz and
Lagus, 2005; Fazly et al, 2009; Xu et al, 2009).
We adopt a definition of compositionality specifi-
cally designed to support downstream applications
that might benefit from knowledge of verb stems.
For example, suppose our corpus contains the fol-
lowing sentence: ?Pope Clement VII denied Henry
VIII permission to marry again before a decision
was given in Rome.? A user might submit the ques-
tion, ?Which pope refused Henry VIII permission to
remarry?? If we can determine that the meaning of
remarry could also be provided via the stem marry,
we could add marry to our search terms. This is
known as morphological query expansion (Bilotti et
al., 2004). Here, such an expansion leads to a better
match between question and answer.
Previous work has shown that ?full morpholog-
ical analysis provides at most very modest bene-
fits for retrieval? (Manning et al, 2008). Stem-
ming, lemmatization, and compound-splitting often
increase recall at the expense of precision, but the
results depend on the morphological complexity of
the text?s language (Hollink et al, 2004).
The lack of success in applying morphological
analysis in IR is unsurprising given that most pre-
vious systems are not designed with applications
in mind. For example, the objective of the influ-
ential Linguistica program is ?to produce an out-
put that matches as closely as possible the analy-
sis that would be given by a human morphologist?
(Goldsmith, 2001). Unsupervised systems achieve
this aim by exploiting learning biases such as min-
imum description length for lexicons (Goldsmith,
2001; Creutz and Lagus, 2007) and high entropy
across morpheme boundaries (Keshava and Pitler,
2006). Supervised approaches learn directly from
words annotated by morphologists (Van den Bosch
and Daelemans, 1999; Toutanova and Cherry, 2009),
often using CELEX, a lexical database that includes
293
morphological information (Baayen et al, 1996).
The conventional approach in morphology is to
segment words into separate morphemes even when
the words are not entirely compositional combina-
tions of their parts (Creutz and Lagus, 2005). For
example, while co- is considered a separate mor-
pheme in the verb cooperate, the meaning of coop-
erate is not simply to operate jointly. These forms
are sometimes viewed as perturbations of compo-
sition (de Marken, 1996). In practice, a user may
query, ?Which nations do not cooperate with the In-
ternational Criminal Court?? An expansion of the
query to include operate may have undesirable con-
sequences.
Rather than relying on conventional standards, we
present an algorithm whose objective is to find only
those prefix verbs that exhibit semantic composi-
tionality; i.e., prefix verbs that are fully meaning-
preserving, sums-of-their-parts. We produce a new
corpus, annotated according to this definition. We
use these annotated examples to learn a discrimina-
tive model of semantic compositionality.
Our classifier relies on a variety of features that
exploit the distributional patterns of verbs and stems.
We build on previous work that applies semantics
to morphology (Yarowsky and Wicentowski, 2000;
Schone and Jurafsky, 2001; Baroni et al, 2002), and
also on work that exploits web-scale data for seman-
tic analysis (Turney, 2001; Nakov, 2007; Kummer-
feld and Curran, 2008). For example, we measure
how often a prefix verb appears with a hyphen be-
tween the prefix and stem. We also look at the dis-
tribution of the stem as a separate word: we calculate
the probability of the prefix verb and the separated
stem?s co-occurrence in a segment of discourse; we
also calculate the distributional similarity between
the verb and the separated stem. High scores for
these measures indicate compositionality. We ex-
tract counts from a web-scale N-gram corpus, allow-
ing us to efficiently leverage huge volumes of unla-
beled text.
Our system achieves 93.6% accuracy on held-out
data, well above several baselines and comparison
systems. We also train and test our system on con-
ventional morphological segmentations. Our clas-
sifier remains reliable in this setting, making half
as many errors as the state-of-the-art unsupervised
Morfessor system (Creutz and Lagus, 2007).
2 Problem Definition and Setting
A prefix verb is a derived word with a bound mor-
pheme as prefix. While derivation can change both
the meaning and part-of-speech of a word (as op-
posed to inflection, which does not change ?referen-
tial or cognitive meaning? (Katamba, 1993)), here
the derived form remains a verb.
We define prefix-verb compositionality as a se-
mantic equivalence between a verb and a paraphrase
involving the verb?s stem. The stem must be used as
a verb in the paraphrase. Words can be introduced,
if needed, to account for the meaning contributed by
the prefix, e.g., outbuild?build more/better/faster
than. A bidirectional entailment between the prefix
verb and the paraphrase is required.
Words can have different meanings in different
contexts. For example, a nation might ?resort to
force,? (non-compositional) while a computer pro-
gram can ?resort a linked list? (compositional). We
therefore define prefix-verb compositionality as a
context-specific property of verb tokens rather than
a global property of verb types. However, it is worth
noting that we ultimately found the compositionality
of types to be very consistent across contexts (Sec-
tion 5.1.2), and we were unable to leverage contex-
tual information to improve classification accuracy;
our final system is essentially type-based. Other re-
cent morphological analyzers have also been type-
based (Keshava and Pitler, 2006; Poon et al, 2009).
Our system takes as input a verb token in unin-
flected form along with its sentence as context. The
verb must be divisible into an initial string and a fol-
lowing remainder such that the initial string is on
our list of prefixes and the remainder is on our list of
stems. Hyphenation is allowed, e.g., both re-enter
and reenter are acceptable inputs. The system deter-
mines whether the prefix/stem combination is com-
positional in the current context. For example, the
verb unionize in, ?The workers must unionize,? can
be divided into a prefix un- and a stem ionize. The
system should determine that here unionize is not a
compositional combination of these parts.
The algorithm requires a list of prefixes and stems
in a given language. For our experiments, we use
both dictionary and corpus-based methods to con-
struct these lists (Section 4).
294
3 Supervised Compositionality Detection
We use a variety of lexical and statistical informa-
tion when deciding whether a prefix verb is compo-
sitional. We adopt a discriminative approach. We
assume some labeled examples are available to train
a classifier. Relevant information is encoded in a
feature vector, and a learning algorithm determines
a set of weights for the features using the training
data. As compositionality is a binary decision, we
can adopt any standard package for binary classifi-
cation. In our experiments we use support vector
machines.
Our features include both local information that
depends only on the verb string (sometimes referred
to as lexical features) and also global information
that depends on the verb and the stem?s distribution
in text. Our approach can therefore be regarded as a
simple form of semi-supervised learning; we lever-
age both a small number of labeled examples and a
large volume of unlabeled text.
If a frequency or similarity is undefined in our cor-
pus, we indicate this with a separate feature; weights
on these features act as a kind of smoothing.
3.1 Features based on Web-Scale N-gram Data
We use web-scale N-gram data to extract distribu-
tional features. The most widely-used N-gram cor-
pus is the Google 5-gram Corpus (Brants and Franz,
2006). We use Google V2: a new N-gram corpus
(also with N-grams of length one-to-five) created
from the same one-trillion-word snapshot of the web
as the Google 5-gram Corpus, but with enhanced fil-
tering and processing of the source text (Lin et al,
2010). For Google V2, the source text was also part-
of-speech tagged, and the resulting part-of-speech
tag distribution is included for each N-gram. There
are 4.1 billion N-grams in the corpus.
The part-of-speech tag distributions are particu-
larly useful, as they allow us to collect verb-specific
counts. For example, while a string like reuse oc-
curs 1.1 million times in the web corpus, it is only
tagged as a verb 270 thousand times. Conflating the
noun/verb senses can lead to misleading scores for
certain features. E.g., the hyphenation frequency
of re-use would appear relatively low, even though
reuse is semantically compositional.
Lin et al (2010) also provide a high-coverage,
10-million-phrase set of clusters extracted from the
N-grams; we use these for our similarity features
(Section 3.1.3). There are 1000 clusters in total.
The data does not provide the context vectors for
each phrase; rather, each phrase is listed with its 20
most similar clusters, measured by cosine similar-
ity with the cluster centroid. We use these centroid
similarities as values in a 1000-dimensional cluster-
membership feature space. To calculate the similar-
ity between two verbs, we calculate the cosine simi-
larity between their cluster-membership vectors.
The feature classes in the following four subsec-
tions each make use of web-scale N-gram data.
3.1.1 HYPH features
Hyphenated verbs are usually compositional (e.g.,
re-elect). Of course, a particular instance of a com-
positional verb may or may not occur in hyphenated
form. However, across a large corpus, compositional
prefix verbs tend to occur in a hyphenated form more
often than do non-compositional prefix verbs. We
therefore provide real-valued features for how often
the verb was hyphenated and unhyphenated on the
web. For example, we collect counts for the fre-
quencies of re-elect (33K) and reelect (9K) in our
web corpus, and we convert the frequencies to log-
counts. We also give real-valued features for the hy-
phenated/unhyphenated log-counts using only those
occurrences of the verb that were tagged as a verb,
exploiting the tag distributions in our web corpus as
described above.
Nakov and Hearst (2005) previously used hy-
phenation counts as an indication of a syntactic re-
lationship between nouns. In contrast, we leverage
hyphenation counts as an indication of a semantic
property of verbs.
3.1.2 COOC features
COOC features, and also the SIM (Section 3.1.3)
and YAH (Section 3.2.2) features, concern the asso-
ciation in text between the prefix verb and its stem,
where the stem occurs as a separate word. We call
this the separated stem.
If a prefix verb is compositional, it is more likely
to occur near its separated stem in text. We often
see agree and disagree, read and reread, etc. occur-
ring in the same segment of discourse. We create
features for the association of the prefix verb and its
295
separated stem in a discourse. We include the log-
count of how often the verb and stem occur in the
same N-gram (of length 2-to-5) in our N-gram cor-
pus. Note that the 2-to-4-gram counts are not strictly
a subset of the 5-gram counts, since fewer 5-grams
pass the data?s minimum frequency threshold.
We also include a real-valued pointwise mutual
information (PMI) feature for the verb and separated
stem?s co-occurrence in an N-gram. For the PMI, we
regard occurrence in an N-gram as an event, and cal-
culate the probability that a verb and separated stem
jointly occur in an N-gram, divided by the probabil-
ity of their occurring in an N-gram independently.
3.1.3 SIM features
If a prefix verb is compositional, it should oc-
cur in similar contexts to its stem. The idea that
a stem and stem+affix should be semantically sim-
ilar has been exploited previously for morphological
analysis (Schone and Jurafsky, 2000). We include
a real-valued feature for the distributional similar-
ity of the verb and stem using Lin?s thesaurus (Lin,
1998). The coverage of this measure was low: it
was non-zero for only 93 of the 1000 prefix verbs in
our training set. We therefore also include distribu-
tional similarity calculated using the web-scale 10-
million-phrase clustering as described above. Us-
ing this data, similarity is defined for 615 of the
1000 training verbs. We also explored a variety of
WordNet-based similarity measures, but these ulti-
mately did not prove helpful on development data.
3.1.4 FRQ features
We include real-valued features for the raw fre-
quencies of the verb and the stem on the web. If
these frequencies are widely different, it may in-
dicate a non-compositional usage. Yarowsky and
Wicentowski (2000) use similar statistics to iden-
tify words related by inflection, but they gather their
counts from a much smaller corpus. In addition,
higher-frequency prefix verbs may be a priori more
likely to be non-compositional. A certain frequency
is required for an irregular usage to become famil-
iar to language speakers. The potential correlation
between frequency and non-compositionality could
thus also be exploited by the classifier via the FRQ
features.
3.2 Other Features
3.2.1 LEX features
We provide lexical features for various aspects
of a prefix verb. Binary features indicate the oc-
currence of particular verbs, prefixes, and stems,
and whether the prefix verb is hyphenated. While
hyphenated prefix verbs are usually compositional,
even non-compositional prefix verbs may be hy-
phenated if the prefix and stem terminate and be-
gin with a vowel, respectively. For example, non-
compositional uses of co-operate are often hyphen-
ated, whereas the compositional remarry is rarely
hyphenated. We therefore have indicator features
for the conjunction of the prefix and the first letter
of the stem (e.g., co-o), and also for the prefix con-
joined with a flag indicating whether the stem begins
with a vowel (e.g., co+vowel).
3.2.2 YAH features
While the COOC features capture many cases
where the verb and separated stem occur in close
proximity (especially, but not limited to, conjunc-
tions), there are many other cases where a longer
distance might separate a compositional verb and
its separated stem. For example, consider the sen-
tence, ?Brush the varnish on, but do not overbrush.?
Here, the verb and separated stem do not co-occur
within a 5-gram window, and their co-occurrence
will therefore not be recorded in our N-gram cor-
pus. As an approximation for co-occurrence counts
within a longer segment of discourse, we count the
number of pages on the web where the verb and sep-
arated stem co-occur. We use hit-counts returned
by the Yahoo search engine API.1 Similar to our
COOC features, we include a real-valued feature for
the pointwise mutual information of the prefix verb
and separated stem?s co-occurrence on a web page,
i.e., we use Turney?s PMI-IR (Turney, 2001).
Baroni et al (2002) use similar statistics to help
discover morphologically-related words. In contrast
to our features, however, their counts are derived
from source text that is several orders of magnitude
smaller in size.
1http://developer.yahoo.com/search/boss/
296
3.2.3 DIC features
One potentially useful resource, when available,
is a dictionary of the conventional morphological
segmentations of words in the language. Although
these segmentations have been created for a differ-
ent objective than that of our annotations, we hy-
pothesize that knowledge of morphology can help
inform our system?s predictions. For each prefix
verb, we include features for whether or not the pre-
fix and stem are conventionally segmented into sep-
arate morphemes, according to a morphological dic-
tionary. Similar to the count-based features, we in-
clude a DIC-undefined feature for the verbs that are
not in the dictionary; any precompiled dictionary
will have imperfect coverage of actual test examples.
Interestingly, DIC features are found to be among
our least useful features in the final evaluation.
4 Experiments
4.1 Resources
We use CELEX (Baayen et al, 1996) as our dictio-
nary for the DIC features. We also use CELEX to help
extract our lists of prefixes and stems. We take ev-
ery prefix that is marked in CELEX as forming a new
verb by attaching to an existing verb. For stems, we
use every verb that occurs in CELEX, but we also
extend this list by automatically collecting a large
number of words that were automatically tagged as
verbs in the NYT section of Gigaword (Graff, 2003).
To be included in the extra-verb list, a verb must oc-
cur more than ten times and be tagged as a verb more
than 70% of the time by a part-of-speech tagger. We
thereby obtain 43 prefixes and 6613 stems.2 We
aimed for an automatic, high-precision list for our
initial experiments. This procedure is also amenable
to human intervention; one could alternatively cast a
wider net for possible stems and then manually filter
false positives.
4.2 Annotated Data
We carried out a medium-scale annotation to provide
training and evaluation data for our experiments.3
2The 43 prefixes are: a- ab- ac- ad- as- be- circum- co- col-
com- con- cor- counter- cross- de- dis- e- em- en- ex- fore- im-
in- inter- ir- mis- out- over- per- photo- post- pre- pro- psycho-
re- sub- super- sur- tele- trans- un- under- with-
3Our annotated data is publicly available at:
http://www.cs.ualberta.ca/?ab31/verbcomp/
The data for our annotations also comes from the
NYT section of Gigaword. We first build a list of
possible prefix verbs. We include any verb that a) is
composed of a valid prefix and stem; and b) occurs
at least twice in the corpus.4 If the verb occurs less
than 50 times in the corpus, we also require that it
was tagged as a verb in at least 70% of cases. This
results in 2077 possible prefix verbs for annotation.
For each verb type in our list of possible prefix
verbs, we randomly select for annotation sentences
from Gigaword containing the verb. We take at most
three sentences for each verb type so that a few very
common types (such as become, understand, and im-
prove) do not comprise the majority of annotated ex-
amples. The resulting set of sentences includes a
small number of sentences with incorrectly-tagged
non-verbs; these are simply marked as non-verbs
by our annotators and excluded from our final data
sets. A graphical program was created for the an-
notation; the program automatically links to the on-
line Merriam-Webster dictionary entries for the pre-
fix verb and separated stem. When in doubt about
a verb?s meaning, our annotators adhere to the dic-
tionary definitions. A single annotator labeled 1718
examples, indicating for each sentence whether the
prefix verb was compositional. A second annota-
tor then labeled a random subset of 150 of these ex-
amples, and agreement was calculated. The annota-
tors agreed on 137 of the 150 examples. The Kappa
statistic (Jurafsky and Martin, 2000, page 315), with
P(E) computed from the confusion matrices, is 0.82,
above the 0.80 level considered to indicate good re-
liability.
For our experiments, the 1718 annotated exam-
ples are randomly divided into 1000 training, 359
development, and 359 held-out test examples.
4.3 Classifier Settings
We train a linear support vector machine classifier
using the efficient LIBLINEAR package (Fan et al,
2008). We use L2-loss and L2-regularization. We
4We found that the majority of single-occurrence verbs in
the Gigaword data were typos. We would expect true hapax
legomena to be largely compositional, and we could potentially
derive better statistics if we include them (Baayen and Sproat,
1996). One possible option, employed in previous work, is to
ensure words of interest are ?manually corrected for typing er-
rors before further analysis? (Baayen and Renouf, 1996).
297
optimize the choice of features and regularization
hyperparameter on development data, attaining a
maximum when C = 0.1.
4.4 Evaluation
We compare the following systems:
1. Base1: always choose compositional (the ma-
jority class).
2. Base2: for each prefix, choose the majority
class over the verbs having that prefix in train-
ing data.
3. Morf: the unsupervised Morfessor sys-
tem (Creutz and Lagus, 2007) (Categories-
ML, from 110K-word corpus). If Morfessor
splits the prefix and stem into separate mor-
phemes, we take the prediction as composi-
tional. If it does anything else, we take it as
non-compositional.
4. SCD: Supervised Compositionality Detection:
the system proposed in this paper.
We evaluate using accuracy: the percentage of ex-
amples classified correctly in held-out test data.
5 Results
We first analyze our annotations, gaining insight into
the relation between our definition and conventional
segmentations. We also note the consistency of our
annotations across contexts. We then provide the
main results of our system. Finally, we provide the
results of our system when trained and tested on con-
ventional morphological segmentations.
5.1 Analysis of Annotations
5.1.1 Annotation consistency with dictionaries
The majority of our examples are not present in
a morphological dictionary, even in one as compre-
hensive as CELEX. The prefix verbs are in CELEX
for only 670 of the 1718 total annotated instances.
For those that are in CELEX, Table 1 provides
the confusion matrix that relates the CELEX seg-
mentations to our annotations. The table shows
that the major difference between our annotations
and CELEX is that our definition of compositionality
is more strict than conventional morphological seg-
mentations. When CELEX does not segment the pre-
fix from the stem (case 0), our annotations agree in
CELEX segmentation
1 0
Compositionality 1 227 10
annotation 0 250 183
Table 1: Confusion matrix on the subset of prefix verb
annotations that are also in CELEX. 1 indicates that the
prefix and stem are segmented into separate morphemes,
0 indicates otherwise.
183 of 193 cases. When CELEX does split the prefix
from the stem (case 1), the meaning is semantically
compositional in less than half the cases. This is
a key difference between conventional morphology
and our semantic definition.
It is also instructive to analyze the 10 cases that
are semantically compositional but which CELEX
did not segment. Most of these are verbs that are
conventionally viewed as single morphemes because
they entered English as complete words. For exam-
ple, await comes from the Old North French await-
ier, itself from waitier. In practice, it is useful to
know that await is compositional, i.e. that it can be
rephrased as wait for. Downstream applications can
exploit the compositionality of await, but miss the
opportunity if using the conventional lack of seg-
mentation.
5.1.2 Annotation consistency across contexts
We next analyze our annotated data to determine
the consistency of compositionality across different
occurrences of the same prefix-verb type. There are
1248 unique prefix verbs in our 1718 labeled exam-
ples: 45 verbs occur three times, 380 occur twice
and 823 occur only once. Of the 425 verbs that oc-
cur multiple times, only 6 had different annotations
in different examples (i.e., six verbs occur in both
compositional and non-compositional usages in our
dataset). These six instances are subtle, debatable,
and largely uninteresting, depending on distinctions
like whether the proclaim sense of blazon can sub-
stitute for the celebrate sense of emblazon, etc.
It is easy to find clearer ambiguities online,
such as compositional examples of typically non-
compositional verbs (how to recover a couch, when
to redress a wound, etc.). However, in our data verbs
like recover and redress always occur in their more
dominant non-compositional sense. People may
298
Set # Base1 Base2 Morf SCD
Test 359 65.7 87.2 73.8 93.6
? CELEX 128 30.5 73.4 50.8 89.8
/? CELEX 231 85.3 94.8 86.6 95.7
? train 107 69.2 93.5 74.8 97.2
/? train 252 64.3 84.5 73.4 92.1
Table 2: Number of examples (#) and accuracy (%) on
test data, and on in-CELEX vs. not-in-CELEX, and in-
training-data vs. not-in-training splits.
consciously or unconsciously recognize the possi-
bility for confusion and systematically hyphenate
prefixes from the stem if a less-common composi-
tional usage is employed. For example, our data has
?repress your feelings? for the non-compositional
case but the hyphenated ?re-press the center? for the
compositional usage.5
Due to the consistency of compositionality across
contexts, context-based features may simply not be
very useful for classification. All the features we de-
scribe in Section 3 depend only on the prefix verb
itself and not the verb context. Various context-
dependent features did not improve accuracy on our
development data and were thus excluded from the
final system.
5.2 Main Results
The first row of Table 2 gives the results of all
systems on test data. SCD achieves 93.6% ac-
curacy, making one fifth as many errors as the
majority-class baseline (Base1) and half as many er-
rors as the more competitive prefix-based predictor
(Base2). The substantial difference between SCD
and Base2 shows that SCD is exploiting much infor-
mation beyond the trivial memorization of a deci-
sion for each prefix. Morfessor performs better than
Base1 but significantly worse than Base2. This indi-
cates that state-of-the-art unsupervised morpholog-
ical segmentation is not yet practical for semantic
preprocessing. Of course, Morfessor was also de-
signed with a different objective; in Section 5.3 we
compare Morfessor and SCD on conventional mor-
5Note that many examples like recover, repress and redress
are only ambiguous in text, not in speech. Pronunciation re-
duces ambiguity in the same way that hyphens do in text. Con-
versely, observe that knowledge of compositionality could po-
tentially help speech synthesis.
Prefix # Tot # Comp SCD
re- 166 147 95.8
over- 26 25 96.2
out- 23 18 91.3
de- 21 0 100.0
pre- 19 16 94.7
un- 17 1 94.1
dis- 10 0 90.0
under- 9 7 77.8
co- 7 6 100.0
en- 5 2 60.0
Table 3: Total number of examples (# Tot), number of
examples that are compositional (# Comp), and accuracy
(%) of SCD on test data, by prefix.
phological segmentations.
We further analyzed the systems by splitting the
test data two ways.
First, we separate verbs that occur in our mor-
phological dictionary (? CELEX) from those that
do not (/? CELEX). Despite using the dictionary
segmentation itself as a feature, the performance
of SCD is worse on the ? CELEX verbs (89.8%).
The comparison systems drop even more dramati-
cally on this subset. The ? CELEX verbs comprise
the more frequent, irregular verbs in English. Non-
compositionality is the majority class on the exam-
ples that are in the dictionary.
On the other hand, one would expect verbs that
are not in a comprehensive dictionary to be largely
compositional, and indeed most of the /? CELEX
verbs are compositional. However, there is still
much to be gained from applying SCD, which makes
a third as many errors as the system which always
assigns compositional (95.7% for SCD vs. 85.3%
for Base1).
Our second way of splitting the data is to divide
our test set into prefix verbs that also occurred in
training sentences (? train) and those that did not (/?
train). Over 70% did not occur in training. SCD
scores 97.2% accuracy on those that did. The clas-
sifier is thus able to exploit the consistency of anno-
tations across different contexts (Section 5.1.2). The
92.1% accuracy on the /?-train portion also shows
the features allow the system to generalize well to
new, previously-unseen verbs.
Table 3 gives the results of our system on sets of
299
-LEX -HYPH -COOC -SIM -YAH -FRQ -DIC
85.0 92.8 92.5 93.6 93.6 93.6 93.6
85.5 93.6 92.8 93.0 93.3 93.9
86.9 90.5 93.3 93.6 93.6
84.1 90.3 93.3 93.6
87.5 90.5 93.0
85.5 89.4
Table 4: Accuracy (%) of SCD as different feature classes
are removed. Performance with all features is 93.6%.
verbs divided according to their prefix. The table in-
cludes those prefixes that occurred at least 5 times
in the test set. Note that the prefixes have a long
tail: these ten prefixes cover only 303 of the 359
test examples. Accuracy is fairly high across all the
different prefixes. Note also that the three prefixes
de-, un-, and dis- almost always correspond to non-
compositional verbs. Each of these prefixes corre-
sponds to a subtle form of negation, and it is usually
difficult to paraphrase the negation using the stem.
For example, to demilitarize does not mean to not
militarize (or any other simple re-phrasing using the
stem as a verb), and so our annotation marks it as
non-compositional. Whether such a strict strategy is
ultimately best may depend on the target application.
Feature Analysis
We perform experiments to evaluate which features
are most useful for this task. Table 4 gives the ac-
curacy of our system as different feature classes are
removed. A similar table was previously used for
feature analysis in Daume? III and Marcu (2005).
Each row corresponds to performance with a group
of features; each entry is performance with a par-
ticular feature class individually removed the group.
We remove the least helpful feature class from each
group in succession moving group-to-group down
the rows.
We first remove the DIC features. These do not
impact performance on test data. The last row gives
the performance with only HYPH features (85.5, re-
moving LEX), and only LEX features (89.4, remov-
ing HYPH). These are found to be the two most ef-
fective features for this task, followed by the COOC
statistics. The other features, while marginally help-
ful on development data, are relatively ineffective on
the test set. In all cases, removing LEX features hurts
Base1 Base2 Morf SCD
76.0 79.6 72.4 86.4
Table 5: Accuracy (%) on CELEX.
the most. Removing LEX not only removes useful
stem, prefix, and hyphen information, but it also im-
pairs the ability of the classifier to use the other fea-
tures to separate the examples.
5.3 CELEX Experiments and Results
Finally, we train and test our system on prefix verbs
where the segmentation decisions are provided by
a morphological dictionary. We are interested in
whether the strong results of our system could trans-
fer to conventional morphological segmentations.
We extract all verbs in CELEX that are valid verbs
for our system (divisible into a prefix and verb stem),
and take the CELEX segmentation as the label; i.e.,
whether the prefix and stem are separated into dis-
tinct morphemes. We extract 1006 total verbs.
We take 506 verbs for training, 250 verbs as a
development set (to tune our classifier?s regulariza-
tion parameter) and 250 verbs as a final held-out test
set. We use the same features and classifier as in
our main results, except we remove the DIC features
which are now the instance labels.
Table 5 shows the performance of our two base-
line systems along with Morfessor and SCD. While
the majority-class baseline is much higher, the
prefix-based baseline is 7% lower, indicating that
knowledge of prefixes, and lexical features in gen-
eral, are less helpful for conventional segmentations.
In fact, performance only drops 2% when we re-
move the LEX features, showing that web-scale in-
formation alone can enable solid performance on
this task. Surprisingly, Morfessor performs worse
here, below both baselines and substantially below
the supervised system. We confirmed our Morfessor
program was generating the same segmentations as
the online demo. We also experimented with Lin-
guistica (Goldsmith, 2001), training on a large cor-
pus, but results were worse than with Morfessor.
Accurate segmentation of prefix verbs is clearly
part of the mandate of these systems; prefix verb
segmentation is simply a very challenging task. Un-
like other, less-ambiguous tasks in morphology, a
prefix/stem segmentation is plausible for all of our
300
input verbs, since the putative morphemes are by
definition valid morphemes in the language.
Overall, the results confirm and extend previous
studies that show semantic information is helpful in
morphology (Schone and Jurafsky, 2000; Yarowsky
and Wicentowski, 2000). However, we reiterate that
optimizing systems according to conventional mor-
phology may not be optimal for downstream ap-
plications. Furthermore, accuracy is substantially
lower in this setting than in our main results. Target-
ing conventional segmentations may be both more
challenging and less useful than focusing on seman-
tic compositionality.
6 Related Work
There is a large body of work on morphological
analysis of English, but most of this work does not
handle prefixes. Porter?s stemmer is a well-known
suffix-stripping algorithm (Porter, 1980), while
publicly-available lemmatizers like morpha (Min-
nen et al, 2001) and PC-KIMMO (Karp et al, 1992)
only process inflectional morphology. FreeLing (At-
serias et al, 2006) comes with a few simple rules
for deterministically stripping prefixes in some lan-
guages, but not English (e.g., only semi- and re- can
be stripped when analyzing OOV Spanish verbs).
A number of modern morphological analyzers use
supervised machine learning. These systems could
all potentially benefit from the novel distributional
features used in our model. Van den Bosch and
Daelemans (1999) use memory-based learning to
analyze Dutch. Wicentowski (2004)?s supervised
WordFrame model includes a prefixation compo-
nent. Results are presented on over 30 languages.
Erjavec and Dz?eroski (2004) present a supervised
lemmatizer for Slovene. Dreyer et al (2008) per-
form supervised lemmatization on Basque, English,
Irish and Tagalog; like us they include results when
the set of lemmas is given. Toutanova and Cherry
(2009) present a discriminative lemmatizer for En-
glish, Bulgarian, Czech and Slovene, but only han-
dle suffix morphology. Poon et al (2009) present an
unsupervised segmenter, but one that is based on a
log-linear model that can include arbitrary and in-
terdependent features of the type proposed in our
work. We see potential in combining the best el-
ements of both approaches to obtain a system that
does not need annotated training data, but can make
use of powerful web-scale features.
Our approach follows previous systems for mor-
phological analysis that leverage semantic as well
as orthographic information (Yarowsky and Wicen-
towski, 2000; Schone and Jurafsky, 2001; Baroni et
al., 2002). Similar problems also arise in core se-
mantics, such as how to detect the compositionality
of multi-word expressions (Lin, 1999; Baldwin et
al., 2003; Fazly et al, 2009). Our problem is sim-
ilar to the analysis of verb-particle constructions or
VPCs (e.g., round up, sell off, etc.) (Bannard et al,
2003). Web-scale data can be used for a variety of
problems in semantics (Lin et al, 2010), including
classifying VPCs (Kummerfeld and Curran, 2008).
We motivated our work by describing applications
in information retrieval, and here Google is clearly
the elephant in the room. It is widely reported that
Google has been using stemming since 2003; for ex-
ample, a search today for Porter stemming returns
pages describing the Porter stemmer, and the re-
turned snippets have words like stemming, stem-
mer, and stem in bold text. Google can of course
develop high-quality lists of morphological variants
by paying attention to how users reformulate their
queries. User query sessions have previously been
used to expand queries using similar terms, such as
substituting feline for cat (Jones et al, 2006). We
show that high-quality, IR-friendly stemming is pos-
sible even without query data. Furthermore, query
data could be combined with our other features for
highly discriminative word stemming in context.
Beyond information retrieval, suffix-based stem-
ming and lemmatization have been used in a range
of NLP applications, including text categorization,
textual entailment, and statistical machine transla-
tion. We believe accurate prefix-stripping can also
have an impact in these areas.
7 Conclusions and Future Work
We presented a system for predicting the semantic
compositionality of prefix verbs. We proposed a
new, well-defined and practical definition of compo-
sitionality, and we annotated a corpus of sentences
according to this definition. We trained a discrimina-
tive model to predict compositionality using a range
of lexical and web-scale statistical features. Novel
301
features include measures of the frequency of prefix-
stem hyphenation, and statistics for the likelihood of
the verb and stem co-occurring as separate words in
an N-gram. The classifier is highly accurate across a
range of prefixes, correctly predicting composition-
ality for 93.6% of examples.
Our preliminary results provide strong motiva-
tion for investigating and applying new distribu-
tional features in the prediction of both conventional
morphology and in task-directed semantic composi-
tionality. Our techniques could be used on a variety
of other complex word forms. In particular, many
of our features extend naturally to identifying stem-
stem compounds (like panfry or healthcare). Also, it
would be possible for our system to handle inflected
forms by first converting them to their lemmas us-
ing a morphological analyzer. We could also jointly
learn the compositionality of words across their in-
flections, along the lines of Yarowsky and Wicen-
towski (2000).
There are also other N-gram-derived features that
warrant further investigation. One source of in-
formation that has not previously been exploited is
the ?lexical fixedness? (Fazly et al, 2009) of non-
compositional prefix verbs. If prefix verbs are rarely
rephrased in another form, they are likely to be non-
compositional. For example, in our N-gram data,
the count of quest again is relatively low compared
to the count of request, indicating request is non-
compositional. On the other hand, marry again is
relatively frequent, indicating that remarry is com-
positional. Incorporation of these and other N-gram
counts could further improve classification accuracy.
References
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonza?lez, Llu??s Padro?, and Muntsa Padro?.
2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In LREC.
R. Harald Baayen and Antoinette Renouf. 1996. Chron-
icling the Times: Productive lexical innovations in an
English newspaper. Language, 72(1).
Harald Baayen and Richard Sproat. 1996. Estimating
lexical priors for low-frequency morphologically am-
biguous forms. Comput. Linguist., 22(2):155?166.
R. Harald Baayen, Richard Piepenbrock, and Leon
Gulikers. 1996. The CELEX2 lexical database.
LDC96L14.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In ACL 2003
Workshop on Multiword Expressions.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In ACL 2003 Workshop on Multiword Ex-
pressions.
Marco Baroni, Johannes Matiasek, and Harald Trost.
2002. Unsupervised discovery of morphologically re-
lated words based on orthographic and semantic sim-
ilarity. In ACL-02 Workshop on Morphological and
Phonological Learning (SIGPHON), pages 48?57.
Matthew W. Bilotti, Boris Katz, and Jimmy Lin. 2004.
What works better for question answering: Stemming
or morphological query expansion? In Information
Retrieval for Question Answering (IR4QA) Workshop
at SIGIR 2004.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In International and Interdisci-
plinary Conference on Adaptive Knowledge Represen-
tation and Reasoning.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4(1):1?
34.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT-EMNLP.
Carl de Marken. 1996. Linguistic structure as composi-
tion and perturbation. In ACL.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In EMNLP.
Tomaz? Erjavec and Sas?o Dz?eroski. 2004. Machine learn-
ing of morphosyntactic structure: Lemmatising un-
known Slovene words. Applied Artificial Intelligence,
18:17?41.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Comput. Linguist., 35(1):61?
103.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Comput. Linguist.,
27(2):153?198.
David Graff. 2003. English Gigaword. LDC2003T05.
302
Vera Hollink, Jaap Kamps, Christof Monz, and Maarten
de Rijke. 2004. Monolingual document retrieval for
European languages. IR, 7(1):33?52.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
WWW.
Daniel Jurafsky and James H. Martin. 2000. Speech and
language processing. Prentice Hall.
Daniel Karp, Yves Schabes, Martin Zaidel, and Dania
Egedi. 1992. A freely available wide coverage mor-
phological analyzer for English. In COLING.
Francis Katamba. 1993. Morphology. MacMillan Press.
Samarth Keshava and Emily Pitler. 2006. A simpler, in-
tuitive approach to morpheme induction. In 2nd Pas-
cal Challenges Workshop.
Jonathan K. Kummerfeld and James R. Curran. 2008.
Classification of verb particle constructions with the
Google Web1T Corpus. In Australasian Language
Technology Association Workshop.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
N-grams. In LREC.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat.
Lang. Eng., 7(3):207?223.
Preslav Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
plicit Training Set: Application to Noun Compound
Syntax and Semantics. Ph.D. thesis, University of Cal-
ifornia, Berkeley.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In HLT-NAACL.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Patrick Schone and Daniel Jurafsky. 2000. Knowledge-
free induction of morphology using latent semantic
analysis. In LLL/CoNLL.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
free induction of inflectional morphologies. In
NAACL.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In ACL-IJCNLP.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning.
Antal Van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In ACL.
Richard Wicentowski. 2004. Multilingual noise-robust
supervised morphological analysis using the word-
frame model. In ACL SIGPHON.
Ying Xu, Christoph Ringlstetter, and Randy Goebel.
2009. A continuum-based approach for tightness anal-
ysis of Chinese semantic units. In PACLIC.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL.
303
Proceedings of NAACL-HLT 2013, pages 325?334,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Massively Parallel Suffix Array Queries and On-Demand Phrase Extraction
for Statistical Machine Translation Using GPUs
Hua He
Dept. of Computer Science
University of Maryland
College Park, Maryland
huah@cs.umd.edu
Jimmy Lin
iSchool and UMIACS
University of Maryland
College Park, Maryland
jimmylin@umd.edu
Adam Lopez
HLTCOE
Johns Hopkins University
Baltimore, Maryland
alopez@cs.jhu.edu
Abstract
Translation models in statistical machine
translation can be scaled to large corpora
and arbitrarily-long phrases by looking up
translations of source phrases ?on the fly?
in an indexed parallel corpus using suffix
arrays. However, this can be slow because
on-demand extraction of phrase tables is
computationally expensive. We address this
problem by developing novel algorithms for
general purpose graphics processing units
(GPUs), which enable suffix array queries
for phrase lookup and phrase extraction to
be massively parallelized. Compared to
a highly-optimized, state-of-the-art serial
CPU-based implementation, our techniques
achieve at least an order of magnitude
improvement in terms of throughput. This
work demonstrates the promise of massively
parallel architectures and the potential
of GPUs for tackling computationally-
demanding problems in statistical machine
translation and language processing.
1 Introduction
Efficiently handling large translation models is a
perennial problem in statistical machine translation.
One particularly promising solution (?2) is to use
the parallel text itself as an implicit representation
of the translation model and extract translation units
?on the fly? when they are needed to decode new
input (Brown, 2004). This idea has been applied
to phrase-based (Callison-Burch et al, 2005; Zhang
and Vogel, 2005), hierarchical (Lopez, 2007; Lopez,
2008b; Lopez, 2008a), and syntax-based (Cromieres
and Kurohashi, 2011) models. A benefit of this
technique is that it scales to arbitrarily large models
with very little pre-processing. For instance, Lopez
(2008b) showed that a translation model trained on
a large corpus with sparse word alignments and
loose extraction heuristics substantially improved
Chinese-English translation. An explicit represen-
tation of the model would have required nearly a
terabyte of memory, but its implicit representation
using the parallel text required only a few gigabytes.
Unfortunately, there is substantial computational
cost in searching a parallel corpus for source
phrases, extracting their translations, and scoring
them on the fly. Since the number of possible
translation units may be quite large (for example,
all substrings of a source sentence) and their
translations are numerous, both phrase lookup and
extraction are performance bottlenecks. Despite
considerable research and the use of efficient
indexes like suffix arrays (Manber and Myers,
1990), this problem remains not fully solved.
We show how to exploit the massive parallelism
offered by modern general purpose graphics pro-
cessing units (GPUs) to eliminate the computational
bottlenecks associated with ?on the fly? phrase ex-
traction. GPUs have previously been applied to
DNA sequence matching using suffix trees (Schatz
et al, 2007) and suffix arrays (Gharaibeh and Ri-
peanu, 2010). Building on this work, we present
two novel contributions: First, we describe improved
GPU algorithms for suffix array queries that achieve
greater parallelism (?3). Second, we propose novel
data structures and algorithms for phrase extraction
(?4) and scoring (?5) that are amenable to GPU par-
325
allelization. The resulting implementation achieves
at least an order of magnitude higher throughput
than a state-of-the-art single-threaded CPU imple-
mentation (?6). Since our experiments verify that
the GPU implementation produces exactly the same
results as a CPU reference implementation on a full
extraction, we can simply replace that component
and reap significant performance advantages with no
impact on translation quality. To the best of our
knowledge, this is the first reported application of
GPU acceleration techniques for statistical machine
translation. We believe these results reveal a promis-
ing yet unexplored future direction in exploiting par-
allelism to tackle perennial performance bottlenecks
in state-of-the-art translation models.
2 Phrase Extraction On Demand
Lopez (2008b) provides the following recipe for
?translation by pattern matching?, which we use as
a guide for the remainder of this paper:
Algorithm 1 Translation by pattern matching
1: for each input sentence do
2: for each possible phrase in the sentence do
3: Find its occurrences in the source text
4: for each occurrence do
5: Extract its aligned target phrase (if any)
6: for each extracted phrase pair do
7: Compute feature values
8: Decode as usual using the scored rules
The computational bottleneck occurs in lines 2?7:
there are vast numbers of query phrases, matching
occurrences, and extracted phrase pairs to process in
the loops. In the next three sections, we attack each
problem in turn.
3 Finding Every Phrase
First, we must find all occurrences of each source
phrase in the input (line 3, Algorithm 1). This
is a classic application of string pattern matching:
given a short query pattern, the task is to find all
occurrences in a much larger text. Solving the
problem efficiently is crucial: for an input sentence
F of length |F |, each of its O(|F |2) substrings is a
potential query pattern.
3.1 Pattern Matching with Suffix Arrays
Although there are many algorithms for pattern
matching, all of the examples that we are aware
of for machine translation rely on suffix arrays.
We briefly review the classic algorithms of Manber
and Myers (1990) here since they form the basis
of our techniques and analysis, but readers who
are familiar with them can safely skip ahead to
additional optimizations (?3.2).
A suffix array represents all suffixes of a corpus
in lexicographical order. Formally, for a text T , the
ith suffix of T is the substring of the text beginning
at position i and continuing to the end of T . Each
suffix can therefore be uniquely identified by the
index i of its first word. A suffix array S(T )
of T is a permutation of these suffix identifiers
[1, |T |] arranged by the lexicographical order of the
corresponding suffixes?in other words, the suffix
array represents a sorted list of all suffixes in T .
With both T and S(T ) in memory, we can find any
query pattern Q in O(|Q| log |T |) time by compar-
ing pattern Q against the first |Q| characters of up to
log |T | different suffixes using binary search.
An inefficiency in this solution is that each com-
parison in the binary search algorithm requires com-
paring all |Q| characters of the query pattern against
some suffix of text T . We can improve on this using
an observation about the longest common prefix
(LCP) of the query pattern and the suffix against
which it is compared. Suppose we search for a query
pattern Q in the span of the suffix array beginning at
suffix L and ending at suffix R. For any suffix M
which falls lexicographically between those at L and
R, the LCP of Q and M will be at least as long as
the LCP of Q and L or Q and R. Hence if we know
the quantity h = MIN(LCP(Q,L), LCP(Q,R)) we
can skip comparisons of the first h symbols between
Q and the suffix M , since they must be the same.
The solution of Manber and Myers (1990) ex-
ploits this fact along with the observation that each
comparison in binary search is carried out accord-
ing to a fixed recursion scheme: a query is only
ever compared against a specific suffix M for a
single range of suffixes bounded by some fixed L
and R. Hence if we know the longest common
prefix between M and each of its corresponding
L and R according to the fixed recursions in the
326
algorithm, we can maintain a bound on h and reduce
the aggregate number of symbol comparisons to
O(|Q| + log |T |). To accomplish this, in addition
to the suffix array, we pre-compute two other arrays
of size |T | for both left and right recursions (called
the LCP arrays).
Memory use is an important consideration, since
GPUs have less memory than CPUs. For the algo-
rithms described here, we require four arrays: the
original text T , the suffix array S(T ), and the two
LCP arrays. We use a representation of T in which
each word has been converted to a unique integer
identifier; with 32-bit integers the total number of
bytes is 16|T |. As we will show, this turns out to be
quite modest, even for large parallel corpora (?6).
3.2 Suffix Array Efficiency Tricks
Previous work on translation by pattern matching
using suffix arrays on serial architectures has pro-
duced a number of efficiency optimizations:
1. Binary search bounds for longer substrings are
initialized to the bounds of their longest prefix.
Substrings are queried only if their longest
prefix string was matched in the text.
2. In addition to conditioning on the longest pre-
fix, Zhang and Vogel (2005) and Lopez (2007)
condition on a successful query for the longest
proper suffix.
3. Lopez (2007) queries each unique substring
of a sentence exactly once, regardless of how
many times it appears in an input sentence.
4. Lopez (2007) directly indexes one-word sub-
strings with a small auxiliary array, so that
their positions in the suffix array can be found
in constant time. For longer substrings, this
optimization reduces the log |T | term of query
complexity to log(count(a)), where a is the
first word of the query string.
Although these efficiency tricks are important in the
serial algorithms that serve as our baseline, not all
of them are applicable to parallel architectures. In
particular, optimizations (1), (2), and (3) introduce
order dependencies between queries; they are disre-
garded in our GPU implementation so that we can
fully exploit parallelization opportunities. We have
not yet fully implemented (4), which is orthogonal
to parallelization: this is left for future work.
3.3 Finding Every Phrase on a GPU
Recent work in computational biology has shown
that suffix arrays are particularly amenable to GPU
acceleration: the suffix-array-based DNA sequence
matching system MummurGPU++ (Gharaibeh and
Ripeanu, 2010) has been reported to outperform the
already fast MummurGPU 2 (Trapnell and Schatz,
2009), based on suffix trees (an alternative indexing
structure). Here, we apply the same ideas to ma-
chine translation, introducing some novel improve-
ments to their algorithms in the process.
A natural approach to parallelism is to perform
all substring queries in parallel (Gharaibeh and Ri-
peanu, 2010). There are no dependencies between
iterations of the loop beginning on line 2 of Algo-
rithm 1, so for input sentence F , we can parallelize
by searching for all O(|F |2) substrings concurrently.
We adopt this approach here.
However, na??ve application of query-level paral-
lelism leads to a large number of wasted threads,
since most long substrings of an input sentence will
not be found in the text. Therefore, we employ
a novel two-pass strategy: in the first pass, we
simply compute, for each position i in the input
sentence, the length j of the longest substring in F
that appears in T . These computations are carried
out concurrently for every position i. During this
pass, we also compute the suffix array bounds of the
one-word substring F [i], to be used as input to the
second pass?a variant of optimizations (1) and (4)
discussed in ?3.2. On the second pass, we search
for all substrings F [i, k] for all k ? [i + 1, i + j].
These computations are carried out concurrently for
all substrings longer than one word.
Even more parallelization is possible. As we saw
in ?3.1, each query in a suffix array actually requires
two binary searches: one each for the first and last
match in S(T ). The abundance of inexpensive
threads on a GPU permits us to perform both queries
concurrently on separate threads. By doing this in
both passes we utilize more of the GPU?s processing
power and obtain further speedups.
As a simple example, consider an input sentence
?The government puts more tax on its citizens?, and
suppose that substrings ?The government?, ?gov-
ernment puts?, and ?puts more tax? are found in
the training text, while none of the words in ?on
327
Initial Word Longest Match Substrings Threads
1st pass 2nd pass
The 2 The, The government 2 2
government 2 government, government puts 2 2
puts 3 puts, puts more, puts more tax 2 4
more 2 more, more tax 2 2
tax 1 tax 2 0
on 0 ? 2 0
its 0 ? 2 0
citizens 0 ? 2 0
Total Threads: 16 10
Table 1: Example of how large numbers of suffix array queries can be factored across two highly parallel passes on a
GPU with a total of 26 threads to perform all queries for this sample input sentence.
its citizens? are found. The number of threads
spawned is shown in Table 1: all threads during a
pass execute in parallel, and each thread performs a
binary search which takes no more than O(|Q| +
log |T |) time. While spawning so many threads
may seem wasteful, this degree of parallelization
still under-utilizes the GPU; the hardware we use
(?6) can manage up to 21,504 concurrent threads
in its resident occupancy. To fully take advantage
of the processing power, we process multiple input
sentences in parallel. Compared with previous
algorithms, our two-pass approach and our strategy
of thread assignment to increase the amount of
parallelism represent novel contributions.
4 Extracting Aligned Target Phrases
The problem at line 5 of Algorithm 1 is to extract the
target phrase aligned to each matching source phrase
instance. Efficiency is crucial since some source
phrases occur hundreds of thousands of times.
Phrase extraction from word alignments typically
uses the consistency check of Och et al (1999). A
consistent phrase is one for which no words inside
the phrase pair are aligned to words outside the
phrase pair. Usually, consistent pairs are computed
offline via dynamic programming over the align-
ment grid, from which we extract all consistent
phrase pairs up to a heuristic bound on phrase length.
The online extraction algorithm of Lopez (2008a)
checks for consistent phrases in a different manner.
Rather than finding all consistent phrase pairs in
a sentence, the algorithm asks: given a specific
source phrase, is there a consistent phrase pair
Figure 1: Source phrase f2f3f4 and target phrase
e2e3e4 are extracted as a consistent pair, since the back-
projection is contained within the original source span.
Figure 2: Source phrase f2f3f4 and target phrase e2e3e4
should not be extracted, since the back-projection is not
contained within the original source span.
of which it is one side? To answer this, it first
computes the projection of the source phrase in the
target sentence: the minimum span containing all
words that are aligned to any word of the source
span. It then computes the projection of the target
span back into the source; if this back-projection
is contained within the original source span, the
phrase pair is consistent, and the target span is
extracted as the translation of the source. Figure 1
shows a ?good? pair for source phrase f2f3f4, since
the back-projection is contained within the original
source span, whereas Figure 2 shows a ?bad? pair
for source phrase f2f3f4 since the back-projection
is not contained within the original source span.
328
4.1 Sampling Consistent Phrases
Regardless of how efficient the extraction of a single
target phrase is made, the fact remains that there
are many phrases to extract. For example, in our
Chinese Xinhua dataset (see ?6), from 8,000 input
query sentences, about 20 million source substrings
can be extracted. The standard solution to this
problem is to sample a set of occurrences of each
source phrase, and only extract translations for those
occurrences (Callison-Burch et al, 2005; Zhang and
Vogel, 2005). As a practical matter, this can be done
by sampling at uniform intervals from the matching
span of a suffix array. Lopez (2008a) reports a
sample size of 300; for phrases occurring fewer than
300 times, all translations are extracted.
4.2 GPU Implementation
We present novel data structures and an algorithm
for efficient phrase extraction, which together are
amenable to massive parallelization on GPUs. The
basic insight is to pre-compute data structures for
the source-to-target algnment projection and back-
projection procedure described by Lopez (2008a)
for checking consistent alignments.
Let us consider a single matching substring (from
the output of the suffix array queries), span [i, j] in
the source text T . For each k, we need to know the
leftmost and rightmost positions that it aligns to in
the target T ?. For this purpose we can define the
target span [i?, j?], along with leftmost and rightmost
arrays L and R as follows:
i? := min
k?[i,j]
L(k)
j? := max
k?[i,j]
R(k)
The arrays L and R are each of length |T |, in-
dexed by absolute corpus position. Each array
element contains the leftmost and rightmost extents
of the source-to-target algnments (in the target),
respectively. Note that in order to save space,
the values stored in the arrays are sentence-relative
positions (e.g., token count from the beginning of
each sentence), so that we only need one byte per
array entry. Thus, i? and j? are sentence-relative
positions (in the target).
Similarly, for the back-projection, we use two
arrays L? and R? on the target side (length |T ?|) to
keep track of the leftmost and rightmost positions
that k? in the target training text align to, as below:
i?? := min
k??[s?+i?,s?+j?]
L?(k?)
j?? := max
k??[s?+i?,s?+j?]
R?(k?)
The arrays L? and R? are indexed by absolute corpus
positions, but their contents are sentence relative
positions (on the source side). To index the arrays
L? and R?, we also need to obtain the corresponding
target sentence start position s?. Note that the back-
projected span [i??, j??] may or may not be the same
as the original span [i, j]. In fact, this is exactly what
we must check for to ensure a consistent alignment.
The suffix array gives us i, which is an ab-
solute corpus position, but we need to know the
sentence-relative position, since the spans computed
by R,L,R?, L? are all sentence relative. To solve
this, we introduce an array P (length |T |) that gives
the relative sentence position of each source word.
We then pack the three source side arrays (R, L,
and P ) into a single RLP array of 32-bit integers
(note that we are actually wasting one byte per array
element). Finally, since the end-of-sentence special
token is not used in any of R, L, or P , its position
in RLP can be used to store an index to the start
of the corresponding target sentence in the target
array T ?. Now, given a source phrase spanning
[i, j] (recall, these are absolute corpus positions), our
phrase extraction algorithm is as follows:
Algorithm 2 Efficient Phrase Extraction Algorithm
1: for each source span [i, j] do
2: Compute [i?, j?]
3: s := i? P [i]? 1
4: s? := RLP [s]
5: i?? := mink??[s?+i?,s?+j?] L
?(k?)
6: j?? := maxk??[s?+i?,s?+j?]R
?(k?)
7: If i? s = i?? and j ? s = j?? then
8: Extract T [i, j] with T ?[s? + i?, s? + j?]
where s is the source sentence start position of a
given source phrase and s? is the target sentence
start position. If the back-projected spans match the
original spans, the phrase pair T [i, j] and T ?[s? +
i?, s? + j?] is extracted.
In total, the data structures RLP , R?, and L?
require 4|T | + 2|T ?| bytes. Not only is this phrase
329
extraction algorithm fast?requiring only a few in-
direct array references?the space requirements for
the auxiliary data structures are quite modest.
Given sufficient resources, we would ideally par-
allelize the phrase table creation process for each
occurrence of the matched source substring. How-
ever, the typical number of source substring matches
for an input sentence is even larger than the number
of threads available on GPUs, so this strategy does
not make sense due to context switching overhead.
Instead, GPU thread blocks (groups of 512 threads)
are used to process each source substring. This
means that for substrings with large numbers of
matches, one thread in the GPU block would process
multiple occurrences. This strategy is widely used,
and according to GPU programming best practices
from NVIDIA, allocating more work to a single
thread maintains high GPU utilization and reduces
the cost of context switches.
5 Computing Every Feature
Finally, we arrive at line 7 in Algorithm 3, where
we must compute feature values for each extracted
phrase pair. Following the implementation of gram-
mar extraction used in cdec (Lopez, 2008a), we
compute several widely-used features:
1. Pair count feature, c(e, f).
2. The joint probability of all target-to-source
phrase translation probabilities, p(e|f)
= c(e, f)/c(f), where e is target phrase, f is
the source phrase.
3. The logarithm of the target-to-source lexical
weighting feature.
4. The logarithm of the source-to-target lexical
weighting feature.
5. The coherence probability, defined as the ratio
between the number of successful extractions
of a source phrase to the total count of the
source phrase in the suffix array.
The output of our phrase extraction is a large
collection of phrase pairs. To extract the above fea-
tures, aggregate statistics need to be computed over
phrase pairs. To make the solution both compact
and efficient, we first sort the unordered collection
of phrases from the GPU into an array, then the
aggregate statistics can be obtained in a single pass
over the array, since identical phrase pairs are now
grouped together.
6 Experimental Setup
We tested our GPU-based grammar extraction im-
plementation under the conditions in which it would
be used for a Chinese-to-English machine transla-
tion task, in particular, replicating the data condi-
tions of Lopez (2008b). Experiments were per-
formed on two data sets. First, we used the source
(Chinese) side of news articles collected from the
Xinhua Agency, with around 27 million words of
Chinese in around one million sentences (totaling
137 MB). Second, we added source-side parallel text
from the United Nations, with around 81 million
words of Chinese in around four million sentences
(totaling 561 MB). In a pre-processing phase, we
mapped every word to a unique integer, with two
special integers representing end-of-sentence and
end-of-corpus, respectively.
Input query data consisted of all sentences from
the NIST 2002?2006 translation campaigns, tok-
enized and integerized identically to the training
data. On average, sentences contained around 29
words. In order to fully stress our GPU algorithms,
we ran tests on batches of 2,000, 4,000, 6,000,
8,000, and 16,000 sentences. Since there are only
around 8,000 test sentences in the NIST data, we
simply duplicated the test data as necessary.
Our experiments used NVIDIA?s Tesla C2050
GPU (Fermi Generation), which has 448 CUDA
cores with a peak memory bandwidth 144 GB/s.
Note that the GPU was released in early 2010
and represents previous generation technology.
NVIDIA?s current GPUs (Kepler) boasts raw
processing power in the 1.3 TFlops (double
precision) range, which is approximately three
times the GPU we used. Our CPU is a 3.33 GHz
Intel Xeon X5260 processor, which has two cores.
As a baseline, we compared against the publicly
available implementation of the CPU-based algo-
rithms described by Lopez (2008a) found in the
pycdec (Chahuneau et al, 2012) extension of the
cdec machine translation system (Dyer et al, 2010).
Note that we only tested grammar extraction for
continuous pairs of phrases, and we did not test the
slower and more complex queries for hierarchical
330
Input Sentences 2,000 4,000 6,000 8,000 16,000
Number of Words 57,868 117,854 161,883 214,246 428,492
Xinhua
With Sampling (s300)
GPU (words/second)
3811
(21.9)
4723
(20.4)
5496
(32.1)
6391
(29.7)
12405
(36.0)
CPU (words/second) 200 (1.5)
Speedup 19? 24? 27? 32? 62?
No Sampling (s?)
GPU (words/second)
1917
(8.5)
2859
(11.1)
3496
(19.9)
4171
(23.2)
8186
(27.6)
CPU (words/second) 1.13 (0.02)
Speedup 1690? 2520? 3082? 3677? 7217?
Xinhua + UN
With Sampling (s300)
GPU (words/second)
2021
(5.3)
2558
(10.7)
2933
(13.9)
3439
(15.2)
6737
(29.0)
CPU (words/second) 157 (1.8)
Speedup 13? 16? 19? 22? 43?
No Sampling (s?)
GPU (words/second)
500.5
(2.5)
770.1
(3.9)
984.6
(5.8)
1243.8
(5.4)
2472.3
(12.0)
CPU (words/second) 0.23 (0.002)
Speedup 2194? 3375? 4315? 5451? 10836?
Table 2: Comparing the GPU and CPU implementations for phrase extraction on two different corpora. Throughput
is measured in words per second under different test set sizes; the 95% confidence intervals across five trials are given
in parentheses, along with relative speedups comparing the two implementations.
(gappy) patterns described by Lopez (2007). Both
our implementation and the baseline are written
primarily in C/C++.1
Our source corpora and test data are the same
as that presented in Lopez (2008b), and using the
CPU implementation as a reference enabled us to
confirm that our extracted grammars and features
are identical (modulo sampling). We timed our
GPU implementation as follows: from the loading
of query sentences, extractions of substrings and
grammar rules, until all grammars for all sentences
are generated in memory. Timing does not include
offline preparations such as the construction of the
suffix array on source texts and the I/O costs for
writing the per-sentence grammar files to disk. This
timing procedure is exactly the same for the CPU
1The Chahuneau et al (2012) implementation is in Cython,
a language for building Python applications with performance-
critical components in C. In particular, all of the suffix array
code that we instrumented for these experiments are compiled
to C/C++. The implementation is a port of the original code
written by Lopez (2008a) in Pyrex, a precursor to Cython.
Much of the code is unchanged from the original version.
baseline. We are confident that our results represent
a fair comparison between the GPU and CPU, and
are not attributed to misconfigurations or other flaws
in experimental procedures. Note that the CPU
implementation runs in a single thread, on the same
machine that hosts the GPU (described above).
7 Results
Table 2 shows performance results comparing our
GPU implementation against the reference CPU
implementation for phrase extraction. In one ex-
perimental condition, the sampling parameter for
frequently-matching phrases is set to 300, per Lopez
(2008a), denoted s300. The experimental condition
without sampling is denoted s?. Following stan-
dard settings, the maximum length of the source
phrase is set to 5 and the maximum length of the
target phrase is set to 15 (same for both GPU
and CPU implementations). The table is divided
into two sections: the top shows results on the
Xinhua data, and the bottom on Xinhua + UN
data. Columns report results for different numbers
331
# Sent. 2000 4000 6000 8000 16000
Speedup 9.6? 14.3? 17.5? 20.9? 40.9?
Phrases 2.1? 1.8? 1.7? 1.6? 1.6?
Table 3: Comparing no sampling on the GPU with sam-
pling on the CPU in terms of performance improvements
(GPU over CPU) and increases in the number of phrase
pairs extracted (GPU over CPU).
of input sentences. Performance is reported in terms
of throughput: the number of processed words per
second on average (i.e., total time divided by the
batch size in words). The results are averaged over
five trials, with 95% confidence intervals shown in
parentheses. Note that as the batch size increases,
we achieve higher throughput on the GPU since
we are better saturating its full processing power.
In contrast, performance is constant on the CPU
regardless of the number of sentences processed.
The CPU throughput on the Xinhua data is 1.13
words per second without sampling and 200 words
per second with sampling. On 16,000 test sentences,
we have mostly saturated the GPU?s processing
power, and observe a 7217? speedup over the CPU
implementation without sampling and 62? speedup
with sampling. On the larger (Xinhua + UN)
corpus, we observe 43? and 10836? speedup with
sampling and no sampling, respectively.
Interestingly, a run without sampling on the GPU
is still substantially faster than a run with sampling
on the CPU. On the Xinhua corpus, we observe
speedups ranging from nine times to forty times, as
shown in Table 3. Without sampling, we are able to
extract up to twice as many phrases.
In previous CPU implementations of on-the-fly
phrase extraction, restrictions were placed on the
maximum length of the source and target phrases
due to computational constraints (in addition to sam-
pling). Given the massive parallelism afforded by
the GPU, might we be able to lift these restrictions
and construct the complete phrase table? To answer
this question, we performed an experiment without
sampling and without any restrictions on the length
of the extracted phrases. The complete phrase
table contained about 0.5% more distinct pairs, with
negligible impact on performance.
When considering these results, an astute reader
might note that we are comparing performance
of a single-threaded implementation with a fully-
saturated GPU. To address this concern, we
conducted an experiment using a multi-threaded
version of the CPU reference implementation to
take full advantage of multiple cores on the CPU (by
specifying the -j option in cdec); we experimented
with up to four threads to fully saturate the
dual-core CPU. In terms of throughput, the CPU
implementation scales linearly, i.e., running on four
threads achieves roughly 4? throughput. Note that
the CPU and GPU implementations take advantage
of parallelism in completely different ways: cdec
can be characterized as embarrassingly parallel, with
different threads processing each complete sentence
in isolation, whereas our GPU implementation
achieves intra-sentential parallelism by exploiting
many threads to concurrently process each sentence.
In terms of absolute performance figures, even
with the 4? throughput improvement from fully
saturating the CPU, our GPU implementation
remains faster by a wide margin. Note that neither
our GPU nor CPU represents state-of-the-art
hardware, and we would expect the performance
advantage of GPUs to be even greater with latest
generation hardware, since the number of available
threads on a GPU is increasing faster than the
number of threads available on a CPU.
Since phrase extraction is only one part of an
end-to-end machine translation system, it makes
sense to examine the overall performance of the
entire translation pipeline. For this experiment, we
used our GPU implementation for phrase extrac-
tion, serialized the grammar files to disk, and used
cdec for decoding (on the CPU). The comparison
condition used cdec for all three stages. We used
standard phrase length constraints (5 on source side,
15 on target side) with sampling of frequent phrases.
Finally, we replicated the data conditions in Lopez
(2008a), where our source corpora was the Xinhua
data set and our development/test sets were the
NIST03/NIST05 data; the NIST05 test set contains
1,082 sentences.
Performance results for end-to-end translation are
shown in Table 4, broken down in terms of total
amount of time for each of the processing stages
for the entire test set under different conditions.
In the decoding stage, we varied the number of
CPU threads (note here we do not observe linear
332
Phrase Extraction I/O Decoding
GPU: 11.0
3.7
1 thread 55.7
2 threads 35.3
CPU: 166.5
3 threads 31.5
4 threads 26.2
Table 4: End-to-end machine translation performance:
time to process the NIST05 test set in seconds, broken
down in terms of the three processing stages.
speedup). In terms of end-to-end results, complete
translation of the test set takes 41 seconds with the
GPU for phrase extraction and CPU for decoding,
compared to 196 seconds using the CPU for both
(with four decoding threads in both cases). This rep-
resents a speedup of 4.8?, which suggests that even
selective optimizations of individual components in
the MT pipeline using GPUs can make a substantial
difference in overall performance.
8 Future Work
There are a number of directions that we have
identified for future work. For computational ef-
ficiency reasons, previous implementations of the
?translation by pattern matching? approach have
had to introduce approximations, e.g., sampling and
constraints on phrase lengths. Our results show that
the massive amounts of parallelism available in the
GPU make these approximations unnecessary, but
it is unclear to what extent they impact translation
quality. For example, Table 3 shows that we extract
up to twice as many phrase pairs without sampling,
but do these pairs actually matter? We have begun to
examine the impact of various settings on translation
quality and have observed small improvements in
some cases (which, note, come for ?free?), but so
far the results have not been conclusive.
The experiments in this paper focus primarily
on throughput, but for large classes of applications
latency is also important. One current limitation of
our work is that large batch sizes are necessary to
fully utilize the available processing power of the
GPU. This and other properties of the GPU, such as
the high latency involved in transferring data from
main memory to GPU memory, make low-latency
processing a challenge, which we hope to address.
Another broad future direction is to ?GPU-ify?
other machine translation models and other com-
ponents in the machine translation pipeline. An
obvious next step is to extend our work to the
hierarchical phrase-based translation model (Chi-
ang, 2007), which would involve extracting ?gappy?
phrases. Lopez (2008a) has tackled this problem
on the CPU, but it is unclear to what extent the
same types of algorithms he proposed can execute
efficiently in the GPU environment. Beyond phrase
extraction, it might be possible to perform decoding
itself in the GPU?not only will this exploit massive
amounts of parallelism, but also reduce costs in
moving data to and from the GPU memory.
9 Conclusion
GPU parallelism offers many promises for practical
and efficient implementations of language process-
ing systems. This promise has been demonstrated
for speech recognition (Chong et al, 2008; Chong
et al, 2009) and parsing (Yi et al, 2011), and we
have demonstrated here that it extends to machine
translation as well. We believe that explorations of
modern parallel hardware architectures is a fertile
area of research: the field has only begun to exam-
ine the possibilities and there remain many more
interesting questions to tackle. Parallelism is critical
not only from the perspective of building real-world
applications, but for overcoming fundamental com-
putational bottlenecks associated with models that
researchers are developing today.
Acknowledgments
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015; NSF
under award IIS-1144034. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors and do not necessarily
reflect views of the sponsors. The second author is
grateful to Esther and Kiri for their loving support
and dedicates this work to Joshua and Jacob. We
would like to thank three anonymous reviewers for
providing helpful suggestions and also acknowledge
Benjamin Van Durme and CLIP labmates for useful
discussions. We also thank UMIACS for providing
hardware resources via the NVIDIA CUDA Center
of Excellence, UMIACS IT staff, especially Joe
Webster, for excellent support.
333
References
R. D. Brown. 2004. A modified Burrows-Wheeler
Transform for highly-scalable example-based transla-
tion. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA 2004), pages 27?36.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005.
Scaling phrase-based statistical machine translation to
larger corpora and longer phrases. In Proceedings
of the 43rd Annual Meeting on Association for
Computational Linguistics (ACL 2005), pages 255?
262.
V. Chahuneau, N. A. Smith, and C. Dyer. 2012. pycdec:
A Python interface to cdec. In Proceedings of the 7th
Machine Translation Marathon (MTM 2012).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. Chong, Y. Yi, A. Faria, N. R. Satish, and K. Keutzer.
2008. Data-parallel large vocabulary continuous
speech recognition on graphics processors. In Pro-
ceedings of the Workshop on Emerging Applications
and Manycore Architectures.
J. Chong, E. Gonina, Y. Yi, and K. Keutzer. 2009. A fully
data parallel WFST-based large vocabulary continuous
speech recognition on a graphics processing unit.
In Proceedings of the 10th Annual Conference of
the International Speech Communication Association
(INTERSPEECH 2009), pages 1183?1186.
F. Cromieres and S. Kurohashi. 2011. Efficient retrieval
of tree translation examples for syntax-based machine
translation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2011, pages 508?518.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7?12.
A. Gharaibeh and M. Ripeanu. 2010. Size matters:
Space/time tradeoffs to improve GPGPU applications
performance. In Proceedings of the 2010 ACM/IEEE
International Conference for High Performance Com-
puting, Networking, Storage and Analysis (SC 2010),
pages 1?12.
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 976?985.
A. Lopez. 2008a. Machine translation by pattern
matching. Ph.D. dissertation, University of Maryland,
College Park, Maryland, USA.
A. Lopez. 2008b. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008), pages 505?512.
U. Manber and G. Myers. 1990. Suffix arrays: a new
method for on-line string searches. In Proceedings of
the First Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA ?90), pages 319?327.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proceedings of the 1999 Joint SIGDAT Conference
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 20?28.
M. Schatz, C. Trapnell, A. Delcher, and A. Varshney.
2007. High-throughput sequence alignment using
graphics processing units. BMC Bioinformatics,
8(1):474.
C. Trapnell and M. C. Schatz. 2009. Optimizing data
intensive GPGPU computations for DNA sequence
alignment. Parallel Computing, 35(8-9):429?440.
Y. Yi, C.-Y. Lai, S. Petrov, and K. Keutzer. 2011.
Efficient parallel CKY parsing on GPUs. In
Proceedings of the 12th International Conference on
Parsing Technologies, pages 175?185.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and
large corpora. In Proceedings of the Tenth Conference
of the European Association for Machine Translation
(EAMT-05).
334
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1312?1320,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identification of Speakers in Novels
Hua He? Denilson Barbosa ? Grzegorz Kondrak?
?Department of Computer Science ?Department of Computing Science
University of Maryland University of Alberta
huah@cs.umd.edu {denilson,gkondrak}@ualberta.ca
Abstract
Speaker identification is the task of at-
tributing utterances to characters in a lit-
erary narrative. It is challenging to auto-
mate because the speakers of the majority
of utterances are not explicitly identified in
novels. In this paper, we present a super-
vised machine learning approach for the
task that incorporates several novel fea-
tures. The experimental results show that
our method is more accurate and general
than previous approaches to the problem.
1 Introduction
Novels are important as social communication
documents, in which novelists develop the plot
by means of discourse between various charac-
ters. In spite of a frequently expressed opinion
that all novels are simply variations of a certain
number of basic plots (Tobias, 2012), every novel
has a unique plot (or several plots) and a different
set of characters. The interactions among charac-
ters, especially in the form of conversations, help
the readers construct a mental model of the plot
and the changing relationships between charac-
ters. Many of the complexities of interpersonal re-
lationships, such as romantic interests, family ties,
and rivalries, are conveyed by utterances.
A precondition for understanding the relation-
ship between characters and plot development in
a novel is the identification of speakers behind all
utterances. However, the majority of utterances
are not explicitly tagged with speaker names, as
is the case in stage plays and film scripts. In most
cases, authors rely instead on the readers? compre-
hension of the story and of the differences between
characters.
Since manual annotation of novels is costly, a
system for automatically determining speakers of
utterances would facilitate other tasks related to
the processing of literary texts. Speaker identifica-
tion could also be applied on its own, for instance
in generating high quality audio books without hu-
man lectors, where each character would be iden-
tifiable by a distinct way of speaking. In addi-
tion, research on spoken language processing for
broadcast and multi-party meetings (Salamin et
al., 2010; Favre et al, 2009) has demonstrated that
the analysis of dialogues is useful for the study of
social interactions.
In this paper, we investigate the task of speaker
identification in novels. Departing from previous
approaches, we develop a general system that can
be trained on relatively small annotated data sets,
and subsequently applied to other novels for which
no annotation is available. Since every novel has
its own set of characters, speaker identification
cannot be formulated as a straightforward tagging
problem with a universal set of fixed tags. Instead,
we adopt a ranking approach, which enables our
model to be applied to literary texts that are differ-
ent from the ones it has been trained on.
Our approach is grounded in a variety of fea-
tures that are easily generalizable across differ-
ent novels. Rather than attempt to construct com-
plete semantic models of the interactions, we ex-
ploit lexical and syntactic clues in the text itself.
We propose several novel features, including the
speaker alternation pattern, the presence of voca-
tives in utterances, and unsupervised actor-topic
features that associate speakers with utterances on
the basis of their content. Experimental evaluation
shows that our approach not only outperforms the
baseline, but also compares favorably to previous
approaches in terms of accuracy and generality,
even when tested on novels and authors that are
different from those used for training.
The paper is organized as follows. After dis-
cussing previous work, and defining the terminol-
ogy, we present our approach and the features that
it is based on. Next, we describe the data, the an-
1312
notation details, and the results of our experimen-
tal evaluation. At the end, we discuss an applica-
tion to extracting a set of family relationships from
a novel.
2 Related Work
Previous work on speaker identification includes
both rule-based and machine-learning approaches.
Glass and Bangay (2007) propose a rule gener-
alization method with a scoring scheme that fo-
cuses on the speech verbs. The verbs, such as
said and cried, are extracted from the communi-
cation category of WordNet (Miller, 1995). The
speech-verb-actor pattern is applied to the utter-
ance, and the speaker is chosen from the avail-
able candidates on the basis of a scoring scheme.
Sarmento and Nunes (2009) present a similar ap-
proach for extracting speech quotes from online
news texts. They manually define 19 variations of
frequent speaker patterns, and identify a total of
35 candidate speech verbs. The rule-based meth-
ods are typically characterized by low coverage,
and are too brittle to be reliably applied to differ-
ent domains and changing styles.
Elson and McKeown (2010) (henceforth re-
ferred to as EM2010) apply the supervised ma-
chine learning paradigm to a corpus of utterances
extracted from novels. They construct a single
feature vector for each pair of an utterance and
a speaker candidate, and experiment with various
WEKA classifiers and score-combination meth-
ods. To identify the speaker of a given utterance,
they assume that all previous utterances are al-
ready correctly assigned to their speakers. Our
approach differs in considering the utterances in
a sequence, rather than independently from each
other, and in removing the unrealistic assumption
that the previous utterances are correctly identi-
fied.
The speaker identification task has also been in-
vestigated in other domains. Bethard et al (2004)
identify opinion holders by using semantic pars-
ing techniques with additional linguistic features.
Pouliquen et al (2007) aim at detecting direct
speech quotations in multilingual news. Krestel
et al (2008) automatically tag speech sentences
in newspaper articles. Finally, Ruppenhofer et al
(2010) implement a rule-based system to enrich
German cabinet protocols with automatic speaker
attribution.
3 Definitions and Conventions
In this section, we introduce the terminology used
in the remainder of the paper. Our definitions are
different from those of EM2010 partly because we
developed our method independently, and partly
because we disagree with some of their choices.
The examples are from Jane Austen?s Pride and
Prejudice, which was the source of our develop-
ment set.
An utterance is a connected text that can be at-
tributed to a single speaker. Our task is to associate
each utterance with a single speaker. Utterances
that are attributable to more than one speaker are
rare; in such cases, we accept correctly identifying
one of the speakers as sufficient. In some cases, an
utterance may include more than one quotation-
delimited sequence of words, as in the following
example.
?Miss Bingley told me,? said Jane, ?that
he never speaks much.?
In this case, the words said Jane are simply a
speaker tag inserted into the middle of the quoted
sentence. Unlike EM2010, we consider this a sin-
gle utterance, rather than two separate ones.
We assume that all utterances within a para-
graph can be attributed to a single speaker. This
?one speaker per paragraph? property is rarely vi-
olated in novels ? we identified only five such
cases in Pride & Prejudice, usually involving one
character citing another, or characters reading let-
ters containing quotations. We consider this an
acceptable simplification, much like assigning a
single part of speech to each word in a corpus.
We further assume that each utterance is contained
within a single paragraph. Exceptions to this rule
can be easily identified and resolved by detecting
quotation marks and other typographical conven-
tions.
The paragraphs without any quotations are re-
ferred to as narratives. The term dialogue denotes
a series of utterances together with related narra-
tives, which provide the context of conversations.
We define a dialogue as a series of utterances and
intervening narratives, with no more than three
continuous narratives. The rationale here is that
more than three narratives without any utterances
are likely to signal the end of a particular dialogue.
We distinguish three types of utterances, which
are listed with examples in Table 1: explicit
speaker (identified by name within the paragraph),
1313
Category Example
Implicit
speaker
?Don?t keep coughing so, Kitty,
for heaven?s sake!?
Explicit
speaker
?I do not cough for my own
amusement,? replied Kitty.
Anaphoric
speaker
?Kitty has no discretion in her
coughs,? said her father.
Table 1: Three types of utterances.
anaphoric speaker (identified by an anaphoric ex-
pression), and implicit speaker (no speaker infor-
mation within the paragraph). Typically, the ma-
jority of utterances belong to the implicit-speaker
category. In Pride & Prejudice only roughly 25%
of the utterances have explicit speakers, and an
even smaller 15% belong to the anaphoric-speaker
category. In modern fiction, the percentage of ex-
plicit attributions is even lower.
4 Speaker Identification
In this section, we describe our method of extract-
ing explicit speakers, and our ranking approach,
which is designed to capture the speaker alterna-
tion pattern.
4.1 Extracting Speakers
We extract explicit speakers by focusing on the
speech verbs that appear before, after, or between
quotations. The following verbs cover most cases
in our development data: say, speak, talk, ask, re-
ply, answer, add, continue, go on, cry, sigh, and
think. If a verb from the above short list cannot be
found, any verb that is preceded by a name or a
personal pronoun in the vicinity of the utterance is
selected as the speech verb.
In order to locate the speaker?s name or
anaphoric expression, we apply a deterministic
method based on syntactic rules. First, all para-
graphs that include narrations are parsed with a
dependency parser. For example, consider the fol-
lowing paragraph:
As they went downstairs together, Char-
lotte said, ?I shall depend on hearing
from you very often, Eliza.?
The parser identifies a number of dependency rela-
tions in the text, such as dobj(went-3, downstairs-
4) and advmod(went-3, together-5). Our method
extracts the speaker?s name from the dependency
relation nsubj(said-8, Charlotte-7), which links a
speech verb with a noun phrase that is the syntac-
tic subject of a clause.
Once an explicit speaker?s name or an anaphoric
expression is located, we determine the corre-
sponding gender information by referring to the
character list or by following straightforward rules
to handle the anaphora. For example, if the utter-
ance is followed by the phrase she said, we infer
that the gender of the speaker is female.
4.2 Ranking Model
In spite of the highly sequential nature of the
chains of utterances, the speaker identification task
is difficult to model as sequential prediction. The
principal problem is that, unlike in many NLP
problems, a general fixed tag set cannot be de-
fined beyond the level of an individual novel.
Since we aim at a system that could be applied to
any novel with minimal pre-processing, sequential
prediction algorithms such as Conditional Ran-
dom Fields are not directly applicable.
We propose a more flexible approach that as-
signs scores to candidate speakers for each utter-
ance. Although the sequential information is not
directly modeled with tags, our system is able
to indirectly utilize the speaker alternation pat-
tern using the method described in the following
section. We implement our approach with SVM-
rank (Joachims, 2006).
4.3 Speaker Alternation Pattern
The speaker alternation pattern is often employed
by authors in dialogues between two charac-
ters. After the speakers are identified explicitly at
the beginning of a dialogue, the remaining odd-
numbered and even-numbered utterances are at-
tributable to the first and second speaker, respec-
tively. If one of the speakers ?misses their turn?, a
clue is provided in the text to reset the pattern.
Based on the speaker alternation pattern, we
make the following two observations:
1. The speakers of consecutive utterances are
usually different.
2. The speaker of the n-th utterance in a dia-
logue is likely to be the same as the speaker
of the (n? 2)-th utterance.
Our ranking model incorporates the speaker al-
ternation pattern by utilizing a feature expansion
scheme. For each utterance n, we first gener-
ate its own features (described in Section 5), and
1314
Features Novelty
Distance to Utterance No
Speaker Appearance Count No
Speaker Name in Utterance No
Unsupervised Actor-Topic Model Yes
Vocative Speaker Name Yes
Neighboring Utterances Yes
Gender Matching Yes
Presence Matching Yes
Table 2: Principal feature sets.
subsequently we add three more feature sets that
represent the following neighboring utterances:
n? 2, n? 1 and n+1. Informally, the features of
the utterances n? 1 and n+1 encode the first ob-
servation, while the features representing the utter-
ance n ? 2 encode the second observation. In ad-
dition, we include a set of four binary features that
are set for the utterances in the range [n?2, n+1]
if the corresponding explicit speaker matches the
candidate speaker of the current utterance.
5 Features
In this section, we describe the set of features used
in our ranking approach. The principal feature sets
are listed in Table 2, together with an indication
whether they are novel or have been used in previ-
ous work.
5.1 Basic Features
A subset of our features correspond to the features
that were proposed by EM2010. These are mostly
features related to speaker names. For example,
since names of speakers are often mentioned in
the vicinity of their utterances, we count the num-
ber of words separating the utterance and a name
mention. However, unlike EM2010, we consider
only the two nearest characters in each direction,
to reflect the observation that speakers tend to be
mentioned by name immediately before or after
their corresponding utterances. Another feature is
used to represent the number of appearances for
speaker candidates. This feature reflects the rela-
tive importance of a given character in the novel.
Finally, we use a feature to indicate the presence
or absence of a candidate speaker?s name within
the utterance. The intuition is that speakers are
unlikely to mention their own name.
Feature Example
start of utterance ?Kitty . . .
before period . . . Jane.
between commas . . . , Elizabeth, . . .
between comma & period . . . , Mrs. Hurst.
before exclamation mark . . . Mrs. Bennet!
before question mark . . . Lizzy?. . .
vocative phrase Dear . . .
after vocative phrase Oh! Lydia . . .
2nd person pronoun . . . you . . .
Table 3: Features for the vocative identification.
5.2 Vocatives
We propose a novel vocative feature, which en-
codes the character that is explicitly addressed in
an utterance. For example, consider the following
utterance:
?I hope Mr. Bingley will like it, Lizzy.?
Intuitively, the speaker of the utterance is neither
Mr. Bingley nor Lizzy; however, the speaker of the
next utterance is likely to be Lizzy. We aim at cap-
turing this intuition by identifying the addressee of
the utterance.
We manually annotated vocatives in about 900
utterances from the training set. About 25% of
the names within utterance were tagged as voca-
tives. A Logistic Regression classifier (Agresti,
2006) was trained to identify the vocatives. The
classifier features are shown in Table 3. The fea-
tures are designed to capture punctuation context,
as well as the presence of typical phrases that ac-
company vocatives. We also incorporate interjec-
tions like ?oh!? and fixed phrases like ?my dear?,
which are strong indicators of vocatives. Under
10-fold cross validation, the model achieved an F-
measure of 93.5% on the training set.
We incorporate vocatives in our speaker identi-
fication system by means of three binary features
that correspond to the utterances n? 1, n? 2, and
n ? 3. The features are set if the detected voca-
tive matches the candidate speaker of the current
utterance n.
5.3 Matching Features
We incorporate two binary features for indicating
the gender and the presence of a candidate speaker.
The gender matching feature encodes the gender
agreement between a speaker candidate and the
speaker of the current utterance. The gender in-
formation extraction is applied to two utterance
1315
groups: the anaphoric-speaker utterances, and the
explicit-speaker utterances. We use the technique
described in Section 4.1 to determine the gender
of a speaker of the current utterance. In contrast
with EM2010, this is not a hard constraint.
The presence matching feature indicates
whether a speaker candidate is a likely partic-
ipant in a dialogue. Each dialogue consists of
continuous utterance paragraphs together with
neighboring narration paragraphs as defined in
Section 3. The feature is set for a given character
if its name or alias appears within the dialogue.
5.4 Unsupervised Actor-Topic Features
The final set of features is generated by the unsu-
pervised actor-topic model (ACTM) (Celikyilmaz
et al, 2010), which requires no annotated train-
ing data. The ACTM, as shown in Figure 1, ex-
tends the work of author-topic model in (Rosen-
Zvi et al, 2010). It can model dialogues in a lit-
erary text, which take place between two or more
speakers conversing on different topics, as distri-
butions over topics, which are also mixtures of the
term distributions associated with multiple speak-
ers. This follows the linguistic intuition that rich
contextual information can be useful in under-
standing dialogues.
Figure 1: Graphical Representation of ACTM.
The ACTM predicts the most likely speakers of
a given utterance by considering the content of an
utterance and its surrounding contexts. The Actor-
Topic-Term probabilities are calculated by using
both the relationship of utterances and the sur-
rounding textual clues. In our system, we utilize
four binary features that correspond to the four top
ranking positions from the ACTM model.
Figure 2: Annotation Tool GUI.
6 Data
Our principal data set is derived from the text
of Pride and Prejudice, with chapters 19?26 as
the test set, chapters 27?33 as the development
set, and the remaining 46 chapters as the training
set. In order to ensure high-quality speaker anno-
tations, we developed a graphical interface (Fig-
ure 2), which displays the current utterance in con-
text, and a list of characters in the novel. After the
speaker is selected by clicking a button, the text
is scrolled automatically, with the next utterance
highlighted in yellow. The complete novel was
annotated by a student of English literature. The
annotations are publicly available1.
For the purpose of a generalization experiment,
we also utilize a corpus of utterances from the
19th and 20th century English novels compiled by
EM2010. The corpus differs from our data set in
three aspects. First, as discussed in Section 3, we
treat all quoted text within a single paragraph as
a single utterance, which reduces the total num-
ber of utterances, and results in a more realistic
reporting of accuracy. Second, our data set in-
cludes annotations for all utterances in the novel,
as opposed to only a subset of utterances from sev-
eral novels, which are not necessarily contiguous.
Lastly, our annotations come from a single expert,
while the annotations in the EM2010 corpus were
collected through Amazon?s Mechanical Turk, and
filtered by voting. For example, out of 308 utter-
ances from The Steppe, 244 are in fact annotated,
which raises the question whether the discarded
utterances tend to be more difficult to annotate.
Table 4 shows the number of utterances in all
1www.cs.ualberta.ca/?kondrak/austen
1316
IS AS ES Total
Pride & P. (all) 663 292 305 1260
Pride & P. (test) 65 29 32 126
Emma 236 55 106 397
The Steppe 93 39 112 244
Table 4: The number of utterances in various
data sets by the type (IS - Implicit Speaker; AS
- Anaphoric Speaker; ES - Explicit Speaker).
data sets. We selected Jane Austen?s Emma as
a different novel by the same author, and Anton
Chekhov?s The Steppe as a novel by a different au-
thor for our generalization experiments.
Since our goal is to match utterances to charac-
ters rather than to name mentions, a preprocess-
ing step is performed to produce a list of char-
acters in the novel and their aliases. For exam-
ple, Elizabeth Bennet may be referred to as Liz,
Lizzy, Miss Lizzy, Miss Bennet, Miss Eliza, and
Miss Elizabeth Bennet. We apply a name entity
tagger, and then group the names into sets of char-
acter aliases, together with their gender informa-
tion. The sets of aliases are typically small, except
for major characters, and can be compiled with
the help of web resources, such as Wikipedia, or
study guides, such as CliffsNotesTM . This pre-
processing step could also be performed automati-
cally using a canonicalization method (Andrews et
al., 2012); however, since our focus is on speaker
identification, we decided to avoid introducing an-
notation errors at this stage.
Other preprocessing steps that are required for
processing a new novel include standarizing the
typographical conventions, and performing POS
tagging, NER tagging, and dependency parsing.
We utilize the Stanford tools (Toutanova et al,
2003; Finkel et al, 2005; Marneffe et al, 2006).
7 Evaluation
In this section, we describe experiments conducted
to evaluate our speaker identification approach.
We refer to our main model as NEIGHBORS, be-
cause it incorporates features from the neighbor-
ing utterances, as described in Section 4.3. In
contrast, the INDIVIDUAL model relies only on
features from the current utterance. In an at-
tempt to reproduce the evaluation methodology of
EM2010, we also test the ORACLE model, which
has access to the gold-standard information about
the speakers of eight neighboring utterances in the
Pride & P. Emma Steppe
BASELINE 42.0 44.1 66.8
INDIVIDUAL 77.8 67.3 74.2
NEIGHBORS 82.5 74.8 80.3
ORACLE 86.5 80.1 83.6
Table 5: Speaker identification accuracy (in %) on
Pride & Prejudice, Emma, and The Steppe.
range [n ? 4, n + 4]. Lastly, the BASELINE ap-
proach selects the name that is the closest in the
narration, which is more accurate than the ?most
recent name? baseline.
7.1 Results
Table 5 shows the results of the models trained on
annotated utterances from Pride & Prejudice on
three test sets. As expected, the accuracy of all
learning models on the test set that comes from
the same novel is higher than on unseen novels.
However, in both cases, the drop in accuracy for
the NEIGHBORS model is less than 10%.
Surprisingly, the accuracy is higher on The
Steppe than on Emma, even though the differ-
ent writing style of Chekhov should make the
task more difficult for models trained on Austen?s
prose. The protagonists of The Steppe are mostly
male, and the few female characters rarely speak
in the novel. This renders our gender feature
virtually useless, and results in lower accuracy
on anaphoric speakers than on explicit speakers.
On the other hand, Chekhov prefers to mention
speaker names in the dialogues (46% of utterances
are in the explicit-speaker category), which makes
his prose slightly easier in terms of speaker identi-
fication.
The relative order of the models is the same
on all three test sets, with the NEIGHBORS
model consistently outperforming the INDIVID-
UAL model, which indicates the importance of
capturing the speaker alternation pattern. The per-
formance of the NEIGHBORS model is actually
closer to the ORACLE model than to the INDIVID-
UAL model.
Table 6 shows the results on Emma broken
down according to the type of the utterance. Un-
surprisingly, the explicit speaker is the easiest cat-
egory, with nearly perfect accuracy. Both the IN-
DIVIDUAL and the NEIGHBORS models do better
on anaphoric speakers than on implicit speakers,
which is also expected. However, it is not the
1317
IS AS ES Total
INDIVIDUAL 52.5 67.3 100.0 67.3
NEIGHBORS 63.1 76.4 100.0 74.8
ORACLE 74.2 69.1 99.1 80.1
Table 6: Speaker identification accuracy (in %) on
Austen?s Emma by the type of utterance.
case for the ORACLE model. We conjecture that
the ORACLE model relies heavily on the neighbor-
hood features (which are rarely wrong), and con-
sequently tends to downplay the gender informa-
tion, which is the only information extracted from
the anaphora. In addition, anaphoric speaker is the
least frequent of the three categories.
Table 7 shows the results of an ablation study
performed to investigate the relative importance of
features. The INDIVIDUAL model serves as the
base model from which we remove specific fea-
tures. All tested features appear to contribute to
the overall performance, with the distance features
and the unsupervised actor-topic features having
the most pronounced impact. We conclude that the
incorporation of the neighboring features, which
is responsible for the difference between the IN-
DIVIDUAL and NEIGHBORS models, is similar in
terms of importance to our strongest textual fea-
tures.
Feature Impact
Closest Mention -6.3
Unsupervised ACTM -5.6
Name within Utterance -4.8
Vocative -2.4
Table 7: Results of feature ablation (in % accu-
racy) on Pride & Prejudice.
7.2 Comparison to EM2010
In this section we analyze in more detail our re-
sults on Emma and The Steppe against the pub-
lished results of the state-of-the-art EM2010 sys-
tem. Recall that both novels form a part of the
corpus that was created by EM2010 for the devel-
opment of their system.
Direct comparison to EM2010 is difficult be-
cause they compute the accuracy separately for
seven different categories of utterances. For each
category, they experiment with all combinations
of three different classifiers and four score com-
bination methods, and report only the accuracy
Character
id name gender
. . .
9 Mr. Collins m
10 Charlotte f
11 Jane Bennet f
12 Elizabeth Bennet f
. . .
Relation
from to type mode
. . .
10 9 husband explicit
9 10 wife derived
10 12 friend explicit
12 10 friend derived
11 12 sister explicit
. . .
Figure 3: Relational database with extracted social
network.
achieved by the best performing combination on
that category. In addition, they utilize the ground
truth speaker information of the preceding utter-
ances. Therefore, their results are best compared
against our ORACLE approach.
Unfortunately, EM2010 do not break down their
results by novel. They report the overall ac-
curacy of 63% on both ?anaphora trigram? (our
anaphoric speaker), and ?quote alone? (similar to
our implicit speaker). If we combine the two cate-
gories, the numbers corresponding to our NEIGH-
BORS model are 65.6% on Emma and 64.4% on
The Steppe, while ORACLE achieves 73.2% and
70.5%, respectively. Even though a direct com-
parison is not feasible, the numbers are remarkable
considering the context of the experiment, which
strongly favors the EM2010 system.
8 Extracting Family Relationships
In this section, we describe an application of
the speaker identification system to the extraction
of family relationships. Elson et al (2010) ex-
tract unlabeled networks where the nodes repre-
sent characters and edges indicate their proxim-
ity, as indicated by their interactions. Our goal
is to construct networks in which edges are la-
beled by the mutual relationships between charac-
ters in a novel. We focus on family relationships,
but also include social relationships, such as friend
1318
INSERT INTO Relation (id1, id2, t, m)
SELECT r.to AS id1, r.from AS id2 , ?wife? AS t, ?derived? AS m
FROM Relation r
WHERE r.type=?husband? AND r.mode=?explicit? AND
NOT EXISTS(SELECT * FROM Relation r2
WHERE r2.from=r.to AND r2.to=r.from AND r2.type=t)
Figure 4: An example inference rule.
and attracted-to.
Our approach to building a social network from
the novel is to build an active database of relation-
ships explicitly mentioned in the text, which is ex-
panded by triggering the execution of queries that
deduce implicit relations. This inference process
is repeated for every discovered relationship until
no new knowledge can be inferred.
The following example illustrates how speaker
identification helps in the extraction of social re-
lations among characters. Consider, the following
conversation:
?How so? how can it affect them??
?My dear Mr. Bennet,? replied his wife,
?how can you be so tiresome!?
If the speakers are correctly identified, the utter-
ances are attributed to Mr. Bennet and Mrs. Ben-
net, respectively. Furthermore, the second utter-
ance implies that its speaker is the wife of the pre-
ceding speaker. This is an example of an explicit
relationship which is included in our database.
Several similar extraction rules are used to extract
explicit mentions indicating family and affective
relations, including mother, nephew, and fiancee.
We can also derive relationships that are not ex-
plicitly mentioned in the text; for example, that
Mr. Bennet is the husband of Mrs. Bennet.
Figure 3 shows a snippet of the relational
database of the network extracted from Pride &
Prejudice. Table Character contains all characters
in the book, each with a unique identifier and gen-
der information, while Table Relation contains all
relationships that are explicitly mentioned in the
text or derived through reasoning.
Figure 4 shows an example of an inference rule
used in our system. The rule derives a new re-
lationship indicating that character c1 is the wife
of character c2 if it is known (through an explicit
mention in the text) that c2 is the husband of c1.
One condition for the rule to be applied is that the
database must not already contain a record indi-
cating the wife relationship. This inference rule
would derive the tuple in Figure 3 indicating that
the wife or Mr. Collins is Charlotte.
In our experiment with Pride & Prejudice, a to-
tal of 55 explicitly indicated relationships were au-
tomatically identified once the utterances were at-
tributed to the characters. From those, another 57
implicit relationships were derived through infer-
ence. A preliminary manual inspection of the set
of relations extracted by this method (Makazhanov
et al, 2012) indicates that all of them are correct,
and include about 40% all personal relations that
can be inferred by a human reader from the text of
the novel.
9 Conclusion and Future Work
We have presented a novel approach to identifying
speakers of utterances in novels. Our system in-
corporates a variety of novel features which utilize
vocatives, unsupervised actor-topic models, and
the speaker alternation pattern. The results of our
evaluation experiments indicate a substantial im-
provement over the current state of the art.
There are several interesting directions for the
future work. Although the approach introduced
in this paper appears to be sufficiently general to
handle novels written in a different style and pe-
riod, more sophisticated statistical graphical mod-
els may achieve higher accuracy on this task. A re-
liable automatic generation of characters and their
aliases would remove the need for the preprocess-
ing step outlined in Section 6. The extraction of
social networks in novels that we discussed in Sec-
tion 8 would benefit from the introduction of ad-
ditional inference rules, and could be extended to
capture more subtle notions of sentiment or rela-
tionship among characters, as well as their devel-
opment over time.
We have demonstrated that speaker identifica-
tion can help extract family relationships, but the
converse is also true. Consider the following utter-
ance:
?Lizzy,? said her father, ?I have given
him my consent.?
1319
In order to deduce the speaker of the utterance,
we need to combine the three pieces of informa-
tion: (a) the utterance is addressed to Lizzy (voca-
tive prediction), (b) the utterance is produced by
Lizzy?s father (pronoun resolution), and (c) Mr.
Bennet is the father of Lizzy (relationship ex-
traction). Similarly, in the task of compiling a
list of characters, which involves resolving aliases
such as Caroline, Caroline Bingley, and Miss Bin-
gley, simultaneous extraction of family relation-
ships would help detect the ambiguity of Miss
Benett, which can refer to any of several sis-
ters. A joint approach to resolving speaker attri-
bution, relationship extraction, co-reference reso-
lution, and alias-to-character mapping would not
only improve the accuracy on all these tasks, but
also represent a step towards deeper understanding
of complex plots and stories.
Acknowledgments
We would like to thank Asli Celikyilmaz for col-
laboration in the early stages of this project, Su-
san Brown and Michelle Di Cintio for help with
data annotation, and David Elson for the attempt
to compute the accuracy of the EM2010 system
on Pride & Prejudice. This research was partially
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
References
Alan Agresti. 2006. Building and applying logistic re-
gression models. In An Introduction to Categorical
Data Analysis. John Wiley & Sons, Inc.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In EMNLP-CoNLL.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Auto-
matic extraction of opinion propositions and their
holders. In AAAI Spring Symposium on Exploring
Attitude and Affect in Text.
Asli Celikyilmaz, Dilek Hakkani-Tur, Hua He, Grze-
gorz Kondrak, and Denilson Barbosa. 2010. The
actor-topic model for extracting social networks in
literary narrative. In Proceedings of the NIPS 2010
Workshop - Machine Learning for Social Comput-
ing.
David K. Elson and Kathleen McKeown. 2010. Auto-
matic attribution of quoted speech in literary narra-
tive. In AAAI.
David K. Elson, Nicholas Dames, and Kathleen McKe-
own. 2010. Extracting social networks from literary
fiction. In ACL.
Sarah Favre, Alfred Dielmann, and Alessandro Vincia-
relli. 2009. Automatic role recognition in multi-
party recordings using social networks and proba-
bilistic sequential models. In ACM Multimedia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification in
fiction books. In Proceedings of the 18th Annual
Symposium of the Pattern Recognition.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In KDD.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the source: Automatic tagging of reported
speech in newspaper articles. In LREC.
Aibek Makazhanov, Denilson Barbosa, and Grzegorz
Kondrak. 2012. Extracting family relations from
literary fiction. Unpublished manuscript.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?41.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In RANLP.
Michal Rosen-Zvi, Chaitanya Chemudugunta,
Thomas L. Griffiths, Padhraic Smyth, and Mark
Steyvers. 2010. Learning author-topic models from
text corpora. ACM Trans. Inf. Syst., 28(1).
Josef Ruppenhofer, Caroline Sporleder, and Fabian
Shirokov. 2010. Speaker attribution in cabinet pro-
tocols. In LREC.
Hugues Salamin, Alessandro Vinciarelli, Khiet Truong,
and Gelareh Mohammadi. 2010. Automatic role
recognition based on conversational and prosodic
behaviour. In ACM Multimedia.
Luis Sarmento and Sergio Nunes. 2009. Automatic ex-
traction of quotes and topics from news feeds. In 4th
Doctoral Symposium on Informatics Engineering.
Ronald B. Tobias. 2012. 20 Master Plots: And How to
Build Them. Writer?s Digest Books, 3rd edition.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL-HLT.
1320
