Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 85?90,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Clavius: Bi-Directional Parsing for Generic Multimodal Interaction
Frank Rudzicz
Centre for Intelligent Machines
McGill University
Montre?al, Canada
frudzi@cim.mcgill.ca
Abstract
We introduce a new multi-threaded
parsing algorithm on unification grammars
designed specifically for multimodal
interaction and noisy environments.
By lifting some traditional constraints,
namely those related to the ordering
of constituents, we overcome several
difficulties of other systems in this
domain. We also present several criteria
used in this model to constrain the search
process using dynamically loadable
scoring functions. Some early analyses of
our implementation are discussed.
1 Introduction
Since the seminal work of Bolt (Bolt, 1980), the
methods applied to multimodal interaction (MMI)
have diverged towards unreconcilable approaches
retrofitted to models not specifically amenable to
the problem. For example, the representational
differences between neural networks, decision
trees, and finite-state machines (Johnston and
Bangalore, 2000) have limited the adoption of
the results using these models, and the typical
reliance on the use of whole unimodal sentences
defeats one of the main advantages of MMI - the
ability to constrain the search using cross-modal
information as early as possible.
CLAVIUS is the result of an effort to combine
sensing technologies for several modality types,
speech and video-tracked gestures chief among
them, within the immersive virtual environment
(Boussemart, 2004) shown in Figure 1. Its purpose
is to comprehend multimodal phrases such as
?put this ? here ? .?, for pointing gestures ?,
in either command-based or dialogue interaction.
CLAVIUS provides a flexible, and trainable
new bi-directional parsing algorithm on multi-
dimensional input spaces, and produces modality-
independent semantic interpretation with a low
computational cost.
Figure 1: The target immersive environment.
1.1 Graphical Models and Unification
Unification grammars on typed directed acyclic
graphs have been explored previously in MMI,
but typically extend existing mechanisms not
designed for multi-dimensional input. For
example, both (Holzapfel et al, 2004) and
(Johnston, 1998) essentially adapt Earley?s chart
parser by representing edges as sets of references
to terminal input elements - unifying these as new
edges are added to the agenda. In practice this
has led to systems that analyze every possible
subset of the input resulting in a combinatorial
explosion that balloons further when considering
the complexities of cross-sentential phenomena
such as anaphora, and the effects of noise and
uncertainty on speech and gesture tracking. We
will later show the extent to which CLAVIUS
reduces the size of the search space.
85
Directed graphs conveniently represent
both syntactic and semantic structure, and all
partial parses in CLAVIUS , including terminal-
level input, are represented graphically. Few
restrictions apply, except that arcs labelled
CAT and TIME must exist to represent the
grammar category and time spanned by the
parse, respectively1. Similarly, all grammar rules,
?i : LHS ?? RHS1 RHS2 ... RHSr, are
graphical structures, as exemplified in Figure 2.
Figure 2: ?1 : OBJECT REFERENCE ??
NP click {where(NP :: f1) = (click :: f1)}, with
NP expanded by ?2 : NP ?? DT NN.
1.2 Multimodal Bi-Directional Parsing
Our parsing strategy combines bottom-up and
top-down approaches, but differs from other
approaches to bi-directional chart parsing (Rocio,
1998) in several key respects, discussed below.
1.2.1 Asynchronous Collaborating Threads
A defining characteristic of our approach is
that edges are selected asynchronously by two
concurrent processing threads, rather than serially
in a two-stage process. In this way, we can
distribute processing across multiple machines,
or dynamically alter the priorities given to each
thread. Generally, this allows for a more dynamic
process where no thread can dominate the other. In
typical bi-directional chart parsing the top-down
component is only activated when the bottom-up
component has no more legal expansions (Ageno,
2000).
1.2.2 Unordered Constituents
Alhough evidence suggests that deictic
gestures overlap or follow corresponding spoken
pronomials 85-93% of the time (Kettebekov et al
1Usually this timespan corresponds to the real-time
occurrence of a speech or gestural event, but the actual
semantics are left to the application designer
2002), we must allow for all possible permutations
of multi-dimensional input - as in ?put ? this ?
here.? vs. ?put this ? here ? .?, for example.
We therefore take the unconvential approach
of placing no mandatory ordering constraints on
constituents, hence the rule ?abc : A ?? B C
parses the input ? C B?. We show how we can
easily maintain regular temporal ordering in ?3.5.
1.2.3 Partial Qualification
Whereas existing bi-directional chart parsers
maintain fully-qualified edges by incrementally
adding adjacent input words to the agenda,
CLAVIUS has the ability to construct parses that
instantiate only a subset of their constituents,
so ?abc also parses the input ?B?, for example.
Repercussions are discussed in ?3.4 and ?4.
2 The Algorithm
CLAVIUS expands parses according to a best-first
process where newly expanded edges are ordered
according to trainable criteria of multimodal
language, as discussed in ?3. Figure 3 shows a
component breakdown of CLAVIUS ?s software
architecture. The sections that follow explain
the flow of information through this system from
sensory input to semantic interpretation.
Figure 3: Simplified information flow between
fundamental software components.
2.1 Lexica and Preprocessing
Each unique input modality is asynchronously
monitored by one of T TRACKERS, each sending
an n-best list of lexical hypotheses to CLAVIUS for
any activity as soon as it is detected. For example,
a gesture tracker (see Figure 4a) parametrizes the
gestures preparation, stroke/point, and retraction
(McNeill, 1992), with values reflecting spatial
positions and velocities of arm motion, whereas
86
our speech tracker parametrises words with part-
of-speech tags, and prior probabilities (see Figure
4b). Although preprocessing is reduced to the
identification of lexical tokens, this is more
involved than simple lexicon lookup due to the
modelling of complex signals.
Figure 4: Gestural (a) and spoken (b) ?words?.
2.2 Data Structures
All TRACKERS write their hypotheses directly
to the first of three SUBSPACES that partition
all partial parses in the search space. The first
is the GENERALISER?s subspace, ?[G], which
is monitored by the GENERALISER thread -
the first part of the parser. All new parses
are first written to ?[G] before being moved to
the SPECIFIER?s active and inactive subspaces,
?[SAct], and ?[SInact], respectively. Subspaces are
optimised for common operations by organising
parses by their scores and grammatical categories
into depth-balanced search trees having the heap
property. The best partial parse in each subspace
can therefore be found in O(1) amortised time.
2.3 Generalisation
The GENERALISER monitors the best partial
parse, ?g, in ?[G], and creates new parses ?i
for all grammar rules ?i having CATEGORY(?g)
on the right-hand side. Effectively, these new
parses are instantiations of the relevant ?i, with
one constituent unified to ?g. This provides
the impetus towards sentence-level parses, as
simplified in Algorithm 1 and exemplified in
Figure 5. Naturally, if rule ?i has more than one
constituent (c > 1) of type CATEGORY(?g), then
c new parses are created, each with one of these
being instantiated.
Since the GENERALISER is activated as soon as
input is added to ?[G], the process is interactive
(Tomita, 1985), and therefore incorporates the
associated benefits of efficiency. This is contrasted
with the all-paths bottom-up strategy in GEMINI
(Dowding et al 1993) that finds all admissable
edges of the grammar.
Algorithm 1: Simplified Generalisation
Data: Subspace ?[G], grammar ?
while data remains in ?[G] do
?g := highest scoring graph in ?[G]
foreach rule ?i s.t. Cat (?g) ? RHS(?i)
do
?i := Unify (?i, [? ?RHS ? ? ?g])
if ??i then
Apply Score (?i) to ?i
Insert ?i into ?[G]
Move ?g into ?[SAct]
Figure 5: Example of GENERALISATION.
2.4 Specification
The SPECIFIER thread provides the impetus
towards complete coverage of the input, as
simplified in Algorithm 2 (see Figure 6). It
combines parses in its subspaces that have the
same top-level grammar expansion but different
instantiated constituents. The resulting parse
merges the semantics of the two original graphs
only if unification succeeds, providing a hard
constraint against the combination of incongruous
information. The result, ?, of specification must
be written to ?[G], otherwise ? could never appear
on the RHS of another partial parse. We show how
associated vulnerabilities are overcome in ?3.2
and ?3.4.
Specification is commutative and will always
provide more information than its constituent
graphs if it does not fail, unlike the ?overlay?
87
method of SMARTKOM (Alexandersson and
Becker, 2001), which basically provides a
subsumption mechanism over background
knowledge.
Algorithm 2: Simplified Specification
Data: Subspaces ?[SAct] and ?[SInact]
while data remains in ?[SAct] do
?s := highest scoring graph in ?[SAct]
?j := highest scoring graph in ?[SInact]
s.t. Cat (?j) = Cat (?s)
while ??j do
?i := Unify (?s,?j)
if ??i then
Apply Score (?i) to ?i
Insert ?i into ?[G]
?j := next highest scoring graph from
?[SInact] s.t. Cat (?j) = Cat (?s)
; // Optionally stop after I
iterations, for some I
Move ?s into ?[SInact]
Figure 6: Example of SPECIFICATION.
2.5 Cognition
The COGNITION thread monitors the best
sentence-level hypothesis, ?B , in ?[SInact],
and terminates the search process once ?B has
remained unchallenged by new competing parses
for some period of time.
Once found, COGNITION communicates ?B to
the APPLICATION. Both COGNITION and the
APPLICATION read state information from the
MySQL WORLD database, as discussed in ?3.5,
though only the latter can modify it.
3 Applying Domain-Centric Knowledge
Upon being created, all partial parses are assigned
a score approximating its likelihood of being part
of an accepted multimodal sentence. The score
of partial parse ?, SCORE(?) =
|S|?
i=0
?i?i(?),
is a weighted linear combination of independent
scoring modules (KNOWLEDGE SOURCES). Each
module presents a score function ?i : ? ? <[0..1]
according to a unique criterion of multimodal
language, weighted by ?i, also on <[0..1]. Some
modules provide ?hard constraints? that can
outright forbid unification, returning ?i = ??
in those cases. A subset of the criteria we have
explored are outlined below.
3.1 Temporal Alignment (?1)
By modelling the timespans of parses as
Gaussians, where ? and ? are determined by the
midpoint and 12 the distance between the two
endpoints, respectively - we can promote parses
whose constituents are closely related in time
with the symmetric Kullback-Leibler divergence,
DKL(?1,?2) =
(?21??
2
2)
2+((?1??2)(?21+?
2
2))
2
4?21?
2
2
.
Therefore, ?1 promotes more locally-structured
parses, and co-occuring multimodal utterances.
3.2 Ancestry Constraint (?2)
A consequence of accepting n-best lexical
hypotheses for each word is that we risk unifying
parses that include two competing hypotheses.
For example, if our speech TRACKER produces
hypotheses ?horse? and ?house? for ambiguous
input, then ?2 explicitly prohibits the parse ?the
horse and the house? with flags on lexical content.
3.3 Probabilistic Grammars (?3)
We emphasise more common grammatical
constructions by augmenting each grammar
rule with an associated probability, P (?i),
and assigning ?3(?) = P (RULE(?)) ??
?c=constituent of ?
?3(?c) where RULE is the
top-level expansion of ?.
Probabilities are trainable by maximum
likelihood estimation on annotated data. Within
the context of CLAVIUS , ?3 promotes the
processing of new input words and shallower
parse trees.
88
3.4 Information Content (?4), Coverage (?5)
The ?4 module partially orders parses by
preferring those that maximise the joint entropy
between the semantic variables of its constituent
parses. Furthermore, we use a shifted sigmoid
?5(?) = 2
1+e?
2
5 NUMWORDSIN(?)
?1, to promote parses
that maximise the number of ?words? in a parse.
These two modules together are vital in choosing
fully specified sentences.
3.5 Functional Constraints (?6)
Each grammar rule ?i can include constraint
functions f : ? ? <[0,1] parametrised by values
in instantiated graphs. For example, the function
T FOLLOWS(?1,?2) returns 1 if constituent ?2
follows ?1 in time, and ?? otherwise, thus
maintaining ordering constraints. Functions are
dynamically loaded and executed during scoring.
Since functions are embedded directly within
parse graphs, their return values can be directly
incorporated into those parses, allowing us to
utilise data in the WORLD. For example, the
function OBJECTAT(x, y,&o) determines if an
object exists at point (x, y), as determined by a
pointing gesture, and writes the type of this object,
o, to the graph, which can later further constrain
the search.
4 Early Results
We have constructed a simple blocks-world
experiment where a user can move, colour,
create, and delete geometric objects using speech
and pointing gestures with 74 grammar rules,
25 grammatical categories, and a 43-word
vocabulary. Ten users were recorded interacting
with this system, for a combined total of 2.5
hours of speech and gesture data, and 2304
multimodal utterances. Our randomised data
collection mechanism was designed to equitably
explore the four command types. Test subjects
were given no indication as to the types of phrases
we expected - but were rather shown a collection
of objects and were asked to replicate it, given the
four basic types of actions.
Several aspects of the parser have been tested at
this stage and are summarised below.
4.1 Accuracy
Table 1 shows three hand-tuned configurations of
the module weights ?i, with ?2 = 0.0, since ?2
provides a ?hard constraint? (?3.2).
Figure 7 shows sentence-level precision
achieved for each ?i on each of the four tasks,
where precision is defined as the proportion of
correctly executed sentences. These are compared
against the CMU Sphinx-4 speech recogniser
using the unimodal projection of the multimodal
grammar. Here, conjunctive phrases such as ?Put
a sphere here and colour it yellow? are classified
according to their first clause.
Presently, correlating the coverage and
probabilistic grammar constraints with higher
weights ( > 30%) appears to provide the best
results. Creation and colouring tasks appeared
to suffer most due to missing or misunderstood
head-noun modifiers (ie., object colour). In these
examples, CLAVIUS ranged from a ?51.7% to a
62.5% relative error reduction rate over all tasks.
Config ?1 ?
(?)
2 ?3 ?4 ?5 ?6
?1 0.4 0.0 0.3 0.1 0.1 0.1
?2 0.2 0.0 0.1 0.3 0.2 0.2
?3 0.1 0.0 0.3 0.3 0.15 0.15
Table 1: Three weight configurations.
Figure 7: Precision across the test tasks.
4.2 Work Expenditure
To test whether the best-first approach
compensates for CLAVIUS ? looser constraints
(?1.2), a simple bottom-up multichart parser
(?1.1) was constructed and the average number
of edges it produces on sentences of varying
length was measured. Figure 8 compares this
against the average number of edges produced
by CLAVIUS on the same data. In particular,
although CLAVIUS generally finds the parse it will
accept relatively quickly (?CLAVIUS - found?),
the COGNITION module will delay its acceptance
(?CLAVIUS - accepted?) for a time. Further tuning
will hopefully reduce this ?waiting period?.
89
Figure 8: Number of edges expanded, given
sentence length.
5 Remarks
CLAVIUS consistently ignores over 92% of
dysfluencies (eg. ?uh?) and significant noise
events in tracking, apparently as a result of the
partial qualifications discussed in ?1.2.3, which is
especially relevant in noisy environments. Early
unquantified observation also suggests that a
result of unordered constituents is that parses
incorporating lead words - head nouns, command
verbs and pointing gestures in particular - are
emphasised and form sentence-level parses early,
and are later ?filled in? with function words.
5.1 Ongoing Work
There are at least four avenues open to exploration
in the near future. First, applying the parser to
directed two-party dialogue will explore context-
sensitivity and a more complex grammar. Second,
the architecture lends itself to further parallelism
- specifically by permitting P > 1 concurrent
processing units to dynamically decide whether to
employ the GENERALISER or SPECIFIER, based
on the sizes of shared active subspaces.
We are also currently working on scoring
modules that incorporate language modelling
(with discriminative training), and prosody-based
co-analysis. Finally, we have already begun work
on automatic methods to train scoring parameters,
including the distribution of ?i, and module-
specific training.
6 Acknowledgements
Funding has been provided by la bourse de
maitrisse of the fonds que?be?cois de la recherche
sur la nature et les technologies.
References
Ageno, A., Rodriguez, H. 2000 Extending
Bidirectional Chart Parsing with a Stochastic
Model, in Proc. of TSD 2000, Brno, Czech
Republic.
Alexandersson, J. and Becker, T. 2001 Overlay as
the Basic Operation for Discourse Processing in a
Multimodal Dialogue System in Proc. of the 2nd
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Seattle, WA.
Bolt, R.A. 1980 ?Put-that-there?: Voice and gesture
at the graphics interface in Proc. of SIGGRAPH 80
ACM Press, New York, NY.
Boussemart, Y., Rioux, F., Rudzicz, F., Wozniewski,
M., Cooperstock, J. 2004 A Framework for 3D
Visualisation and Manipulation in an Immersive
Space using an Untethered Bimanual Gestural
Interface in Proc. of VRST 2004 ACM Press, Hong
Kong.
Dowding, J. et al 1993 Gemini: A Natural Language
System For Spoken-Language Understanding in
Meeting of the ACL, ACL, Morristown, NJ.
Holzapfel, H., Nickel, K., Stiefelhagen, R. 2004
Implementation and evaluation of a constraint-
based multimodal fusion system for speech and 3D
pointing gestures, in ICMI ?04: Proc. of the 6th intl.
conference on Multimodal interfaces, ACM Press,
New York, NY.
Johnston, M. 1998 Unification-based multimodal
parsing, in Proc. of the 36th annual meeting of the
ACL, ACL, Morristown, NJ.
Johnston, M., Bangalore, S. 2000 Finite-state
multimodal parsing and understanding in Proc. of
the 18th conference on Computational linguistics
ACL, Morristown, NJ.
Kettebekov, S., et al 2002 Prosody Based Co-
analysis of Deictic Gestures and Speech in Weather
Narration Broadcast, in Workshop on Multimodal
Resources and Multimodal System Evaluation.
(LREC 2002), Las Palmas, Spain.
McNeill, D. 1992 Hand and mind: What gestures
reveal about thought University of Chicago Press
and CSLI Publications, Chicago, IL.
Rocio, V., Lopes, J.G. 1998 Partial Parsing,
Deduction and Tabling in TAPD 98
Tomita, M. 1985 An Efficient Context-Free Parsing
Algorithm for Natural Languages, in Proc. Ninth
Intl. Joint Conf. on Artificial Intelligence, Los
Angeles, CA.
90
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549?557,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Summarizing multiple spoken documents: finding evidence from
untranscribed audio
Xiaodan Zhu, Gerald Penn and Frank Rudzicz
University of Toronto
10 King?s College Rd.,
Toronto, M5S 3G4, ON, Canada
{xzhu,gpenn,frank}@cs.toronto.edu
Abstract
This paper presents a model for summa-
rizing multiple untranscribed spoken doc-
uments. Without assuming the availabil-
ity of transcripts, the model modifies a
recently proposed unsupervised algorithm
to detect re-occurring acoustic patterns in
speech and uses them to estimate similari-
ties between utterances, which are in turn
used to identify salient utterances and re-
move redundancies. This model is of in-
terest due to its independence from spo-
ken language transcription, an error-prone
and resource-intensive process, its abil-
ity to integrate multiple sources of infor-
mation on the same topic, and its novel
use of acoustic patterns that extends pre-
vious work on low-level prosodic feature
detection. We compare the performance of
this model with that achieved using man-
ual and automatic transcripts, and find that
this new approach is roughly equivalent
to having access to ASR transcripts with
word error rates in the 33?37% range with-
out actually having to do the ASR, plus
it better handles utterances with out-of-
vocabulary words.
1 Introduction
Summarizing spoken documents has been exten-
sively studied over the past several years (Penn
and Zhu, 2008; Maskey and Hirschberg, 2005;
Murray et al, 2005; Christensen et al, 2004;
Zechner, 2001). Conventionally called speech
summarization, although speech connotes more
than spoken documents themselves, it is motivated
by the demand for better ways to navigate spoken
content and the natural difficulty in doing so ?
speech is inherently more linear or sequential than
text in its traditional delivery.
Previous research on speech summarization has
addressed several important problems in this field
(see Section 2.1). All of this work, however,
has focused on single-document summarization
and the integration of fairly simplistic acoustic
features, inspired by work in descriptive linguis-
tics. The issues of navigating speech content are
magnified when dealing with larger collections ?
multiple spoken documents on the same topic. For
example, when one is browsing news broadcasts
covering the same events or call-centre record-
ings related to the same type of customer ques-
tions, content redundancy is a prominent issue.
Multi-document summarization on written docu-
ments has been studied for more than a decade
(see Section 2.2). Unfortunately, no such effort
has been made on audio documents yet.
An obvious way to summarize multiple spo-
ken documents is to adopt the transcribe-and-
summarize approach, in which automatic speech
recognition (ASR) is first employed to acquire
written transcripts. Speech summarization is ac-
cordingly reduced to a text summarization task
conducted on error-prone transcripts.
Such an approach, however, encounters several
problems. First, assuming the availability of ASR
is not always valid for many languages other than
English that one may want to summarize. Even
when it is, transcription quality is often an issue?
training ASR models requires collecting and an-
notating corpora on specific languages, dialects,
or even different domains. Although recognition
errors do not significantly impair extractive sum-
marizers (Christensen et al, 2004; Zhu and Penn,
2006), error-laden transcripts are not necessarily
browseable if recognition errors are higher than
certain thresholds (Munteanu et al, 2006). In
such situations, audio summaries are an alterna-
tive when salient content can be identified directly
from untranscribed audio. Third, the underlying
paradigm of most ASR models aims to solve a
549
classification problem, in which speech is seg-
mented and classified into pre-existing categories
(words). Words not in the predefined dictionary
are certain to be misrecognized without excep-
tion. This out-of-vocabulary (OOV) problem is
unavoidable in the regular ASR framework, al-
though it is more likely to happen on salient words
such as named entities or domain-specific terms.
Our approach uses acoustic evidence from the
untranscribed audio stream. Consider text sum-
marization first: many well-known models such
as MMR (Carbonell and Goldstein, 1998) and
MEAD (Radev et al, 2004) rely on the reoccur-
rence statistics of words. That is, if we switch
any word w1 with another word w2 across an
entire corpus, the ranking of extracts (often sen-
tences) will be unaffected, because no word-
specific knowledge is involved. These mod-
els have achieved state-of-the-art performance in
transcript-based speech summarization (Zechner,
2001; Penn and Zhu, 2008). For spoken docu-
ments, such reoccurrence statistics are available
directly from the speech signal. In recent years, a
variant of dynamic time warping (DTW) has been
proposed to find reoccurring patterns in the speech
signal (Park and Glass, 2008). This method has
been successfully applied to tasks such as word
detection (Park and Glass, 2006) and topic bound-
ary detection (Malioutov et al, 2007).
Motivated by the work above, this paper ex-
plores the approach to summarizing multiple spo-
ken documents directly over an untranscribed au-
dio stream. Such a model is of interest because of
its independence from ASR. It is directly applica-
ble to audio recordings in languages or domains
when ASR is not possible or transcription quality
is low. In principle, this approach is free from the
OOV problem inherent to ASR. The premise of
this approach, however, is to reliably find reoccur-
ing acoustic patterns in audio, which is challeng-
ing because of noise and pronunciation variance
existing in the speech signal, as well as the dif-
ficulty of finding alignments with proper lengths
corresponding to words well. Therefore, our pri-
mary goal in this paper is to empirically determine
the extent to which acoustic information alone can
effectively replace conventional speech recogni-
tion with or without simple prosodic feature de-
tection within the multi-document speech summa-
rization task. As shown below, a modification of
the Park-Glass approach amounts to the efficacy
of a 33-37% WER ASR engine in the domain
of multiple spoken document summarization, and
also has better treatment of OOV items. Park-
Glass similarity scores by themselves can attribute
a high score to distorted paths that, in our context,
ultimately leads to too many false-alarm align-
ments, even after applying the distortion thresh-
old. We introduce additional distortion penalty
and subpath length constraints on their scoring to
discourage this possibility.
2 Related work
2.1 Speech summarization
Although abstractive summarization is more de-
sirable, the state-of-the-art research on speech
summarization has been less ambitious, focus-
ing primarily on extractive summarization, which
presents the most important N% of words,
phrases, utterances, or speaker turns of a spo-
ken document. The presentation can be in tran-
scripts (Zechner, 2001), edited speech data (Fu-
rui et al, 2003), or a combination of these (He
et al, 2000). Audio data amenable to summa-
rization include meeting recordings (Murray et al,
2005), telephone conversations (Zhu and Penn,
2006; Zechner, 2001), news broadcasts (Maskey
and Hirschberg, 2005; Christensen et al, 2004),
presentations (He et al, 2000; Zhang et al, 2007;
Penn and Zhu, 2008), etc.
Although extractive summarization is not as
ideal as abstractive summarization, it outperforms
several comparable alternatives. Tucker and Whit-
taker (2008) have shown that extractive summa-
rization is generally preferable to time compres-
sion, which speeds up the playback of audio doc-
uments with either fixed or variable rates. He et
al. (2000) have shown that either playing back im-
portant audio-video segments or just highlighting
the corresponding transcripts is significantly bet-
ter than providing users with full transcripts, elec-
tronic slides, or both for browsing presentation
recordings.
Given the limitations associated with ASR, it is
no surprise that previous work (He et al, 1999;
Maskey and Hirschberg, 2005; Murray et al,
2005; Zhu and Penn, 2006) has studied features
available in audio. The focus, however, is pri-
marily limited to prosody. The assumption is that
prosodic effects such as stress can indicate salient
information. Since a direct modeling of compli-
cated compound prosodic effects like stress is dif-
550
ficult, they have used basic features of prosody in-
stead, such as pitch, energy, duration, and pauses.
The usefulness of prosody was found to be very
limited by itself, if the effect of utterance length is
not considered (Penn and Zhu, 2008). In multiple-
spoken-document summarization, it is unlikely
that prosody will be more useful in predicating
salience than in single document summarization.
Furthermore, prosody is also unlikely to be appli-
cable to detecting or handling redundancy, which
is prominent in the multiple-document setting.
All of the work above has been conducted on
single-document summarization. In this paper
we are interested in summarizing multiple spo-
ken documents by using reoccurrence statistics of
acoustic patterns.
2.2 Multiple-document summarization
Multi-document summarization on written text
has been studied for over a decade. Compared
with the single-document task, it needs to remove
more content, cope with prominent redundancy,
and organize content from different sources prop-
erly. This field has been pioneered by early work
such as the SUMMONS architecture (Mckeown
and Radev, 1995; Radev and McKeown, 1998).
Several well-known models have been proposed,
i.e., MMR (Carbonell and Goldstein, 1998), multi-
Gen (Barzilay et al, 1999), and MEAD (Radev
et al, 2004). Multi-document summarization has
received intensive study at DUC. 1 Unfortunately,
no such efforts have been extended to summarize
multiple spoken documents yet.
Abstractive approaches have been studied since
the beginning. A famous effort in this direction
is the information fusion approach proposed in
Barzilay et al (1999). However, for error-prone
transcripts of spoken documents, an abstractive
method still seems to be too ambitious for the time
being. As in single-spoken-document summariza-
tion, this paper focuses on the extractive approach.
Among the extractive models, MMR (Carbonell
and Goldstein, 1998) and MEAD (Radev et al,
2004), are possibly the most widely known. Both
of them are linear models that balance salience and
redundancy. Although in principle, these mod-
els allow for any estimates of salience and re-
dundancy, they themselves calculate these scores
with word reoccurrence statistics, e.g., tf.idf,
and yield state-of-the-art performance. MMR it-
1http://duc.nist.gov/
eratively selects sentences that are similar to the
entire documents, but dissimilar to the previously
selected sentences to avoid redundancy. Its de-
tails will be revisited below. MEAD uses a redun-
dancy removal mechanism similar to MMR, but
to decide the salience of a sentence to the whole
topic, MEAD uses not only its similarity score
but also sentence position, e.g., the first sentence
of each new story is considered important. Our
work adopts the general framework of MMR and
MEAD to study the effectiveness of the acoustic
pattern evidence found in untranscribed audio.
3 An acoustics-based approach
The acoustics-based summarization technique
proposed in this paper consists of three consecu-
tive components. First, we detect acoustic patterns
that recur between pairs of utterances in a set of
documents that discuss a common topic. The as-
sumption here is that lemmata, words, or phrases
that are shared between utterances are more likely
to be acoustically similar. The next step is to com-
pute a relatedness score between each pair of ut-
terances, given the matching patterns found in the
first step. This yields a symmetric relatedness ma-
trix for the entire document set. Finally, the relat-
edness matrix is incorporated into a general sum-
marization model, where it is used for utterance
selection.
3.1 Finding common acoustic patterns
Our goal is to identify subsequences within acous-
tic sequences that appear highly similar to regions
within other sequences, where each sequence con-
sists of a progression of overlapping 20ms vec-
tors (frames). In order to find those shared pat-
terns, we apply a modification of the segmen-
tal dynamic time warping (SDTW) algorithm to
pairs of audio sequences. This method is similar
to standard DTW, except that it computes multi-
ple constrained alignments, each within predeter-
mined bands of the similarity matrix (Park and
Glass, 2008).2 SDTW has been successfully ap-
plied to problems such as topic boundary detec-
tion (Malioutov et al, 2007) and word detection
(Park and Glass, 2006). An example application
of SDTW is shown in Figure 1, which shows the
results of two utterances from the TDT-4 English
dataset:
2Park and Glass (2008) used Euclidean distance. We used
cosine distance instead, which was found to be better on our
held-out dataset.
551
I: the explosion in aden harbor killed seven-
teen u.s. sailors and injured other thirty
nine last month.
II: seventeen sailors were killed.
These two utterances share three words: killed,
seventeen, and sailors, though in different orders.
The upper panel of Figure 1 shows a matrix of
frame-level similarity scores between these two
utterances where lighter grey represents higher
similarity. The lower panel shows the four most
similar shared subpaths, three of which corre-
spond to the common words, as determined by the
approach detailed below.
Figure 1: Using segmental dynamic time warping
to find matching acoustic patterns between two ut-
terances.
Calculating MFCC
The first step of SDTW is to represent each utter-
ance as sequences of Mel-frequency cepstral coef-
ficient (MFCC) vectors, a commonly used repre-
sentation of the spectral characteristics of speech
acoustics. First, conventional short-time Fourier
transforms are applied to overlapping 20ms Ham-
ming windows of the speech amplitude signal.
The resulting spectral energy is then weighted
by filters on the Mel-scale and converted to 39-
dimensional feature vectors, each consisting of 12
MFCCs, one normalized log-energy term, as well
as the first and second derivatives of these 13 com-
ponents over time. The MFCC features used in
the acoustics-based approach are the same as those
used below in the ASR systems.
As in (Park and Glass, 2008), an additional
whitening step is taken to normalize the variances
on each of these 39 dimensions. The similarities
between frames are then estimated using cosine
distance. All similarity scores are then normalized
to the range of [0, 1], which yields similarity ma-
trices exemplified in the upper panel of Figure 1.
Finding optimal paths
For each similarity matrix obtained above, local
alignments of matching patterns need to be found,
as shown in the lower panel of Figure 1. A sin-
gle global DTW alignment is not adequate, since
words or phrases held in common between utter-
ances may occur in any order. For example, in Fig-
ure 1 killed occurs before all other shared words in
one document and after all of these in the other, so
a single alignment path that monotonically seeks
the lower right-hand corner of the similarity ma-
trix could not possibly match all common words.
Instead, multiple DTWs are applied, each starting
from different points on the left or top edges of the
similarity matrix, and ending at different points on
the bottom or right edges, respectively. The width
of this diagonal band is proportional to the esti-
mated number of words per sequence.
Given an M -by-N matrix of frame-level simi-
larity scores, the top-left corner is considered the
origin, and the bottom-right corner represents an
alignment of the last frames in each sequence. For
each of the multiple starting points p0 = (x0, y0)
where either x0 = 0 or y0 = 0, but not neces-
sarily both, we apply DTW to find paths P =
p0, p1, ..., pK that maximize
?
0? i? K sim(pi),
where sim(pi) is the cosine similarity score of
point pi = (xi, yi) in the matrix. Each point on the
path, pi, is subject to the constraint |xi ? yi| < T ,
where T limits the distortion of the path, as we
determine experimentally. The ending points are
pK = (xK , yK) with either xK = N or yK =
M . For considerations of efficiency, the multi-
ple DTW processes do not start from every point
on the left or top edges. Instead, they skip every
T such starting points, which still guarantees that
there will be no blind-spot in the matrices that are
inaccessible to all DTW search paths.
Finding optimal subpaths
After the multiple DTW paths are calculated, the
optimal subpath on each is then detected in or-
der to find the local alignments where the simi-
larity is maximal, which is where we expect ac-
tual matched phrases to occur. For a given path
P = p0, p2, ..., pK , the optimal subpath is defined
to be a continuous subpath, P ? = pm, pm+1..., pn
552
that maximizes
?
m?i?n sim(pi)
n?m+1 , 0 ? n ? m ? k,
and m ? n + 1 ? L. That is, the subpath is at
least as long as L and has the maximal average
similarity. L is used to avoid short alignments that
correspond to subword segments or short function
words. The value of L is determined on a devel-
opment set.
The version of SDTW employed by (Malioutov
et al, 2007) and Park and Glass (2008) employed
an algorithm of complexity O(Klog(L)) from
(Lin et al, 2002) to find subpaths. Lin et al (2002)
have also proven that the length of the optimal sub-
path is between L and 2L? 1, inclusively. There-
fore, our version uses a very simple algorithm?
just search and find the maximum of average simi-
larities among all possible subpaths with lengths
between L and 2L ? 1. Although the theoreti-
cal upper bound for this algorithm is O(KL), in
practice we have found no significant increase in
computation time compared with the O(Klog(L))
algorithm?L is actually a constant for both Park
and Glass (2008) and us, it is much smaller than
K, and the O(Klog(L)) algorithm has (constant)
overhead of calculating right-skew partitions.
In our implementation, since most of the time is
spent on calculating the average similarity scores
on candidate subpaths, all average scores are
therefore pre-calculated incrementally and saved.
We have also parallelized the computation of sim-
ilarities by topics over several computer clusters.
A detailed comparison of different parallelization
techniques has been conducted by Gajjar et al
(2008). In addition, comparing time efficiency
between the acoustics-based approach and ASR-
based summarizers is interesting but not straight-
forward since a great deal of comparable program-
ming optimization needs to be additionally consid-
ered in the present approach.
3.2 Estimating utterance-level similarity
In the previous stage, we calculated frame-level
similarities between utterance pairs and used these
to find potential matching patterns between the
utterances. With this information, we estimate
utterance-level similarities by estimating the num-
bers of true subpath alignments between two utter-
ances, which are in turn determined by combining
the following features associated with subpaths:
Similarity of subpath
We compute similarity features on each subpath.
We have obtained the average similarity score of
each subpath as discussed in Section 3.1. Based
on this, we calculate relative similarity scores,
which are computed by dividing the original sim-
ilarity of a given subpath by the average similar-
ity of its surrounding background. The motivation
for capturing the relative similarity is to punish
subpaths that cannot distinguish themselves from
their background, e.g., those found in a block of
high-similarity regions caused by certain acoustic
noise.
Distortion score
Warped subpaths are less likely to correspond to
valid matching patterns than straighter ones. In
addition to removing very distorted subpaths by
applying a distortion threshold as in (Park and
Glass, 2008), we also quantitatively measured the
remaining ones. We fit each of them with least-
square linear regression and estimate the residue
scores. As discussed above, each point on a sub-
path satisfies |xi ? yi| < T , so the residue cannot
be bigger than T . We used this to normalize the
distortion scores to the range of [0,1].
Subpath length
Given two subpaths with nearly identical average
similarity scores, we suggest that the longer of the
two is more likely to refer to content of interest
that is shared between two speech utterances, e.g.,
named entities. Longer subpaths may in this sense
therefore be more useful in identifying similarities
and redundancies within a speech summarization
system. As discussed above, since the length of a
subpath len(P ?) has been proven to fall between
L and 2L ? 1, i.e., L ? len(P ?) ? 2L ? 1,
given a parameter L, we normalize the path length
to (len(P ?) ? L)/L, corresponding to the range
[0,1).
The similarity scores of subpaths can vary widely
over different spoken documents. We do not use
the raw similarity score of a subpath, but rather
its rank. For example, given an utterance pair, the
top-1 subpath is more likely to be a true alignment
than the rest, even if its distortion score may be
higher. The similarity ranks are combined with
distortion scores and subpath lengths simply as
follows. We divide subpaths into the top 1, 3, 5,
and 10 by their raw similarity scores. For sub-
paths in each group, we check whether their dis-
tortion scores are below and lengths are above
553
some thresholds. If they are, in any group, then
the corresponding subpaths are selected as ?true?
alignments for the purposes of building utterance-
level similarity matrix. The numbers of true align-
ments are used to measure the similarity between
two utterances. We therefore have 8 threshold pa-
rameters to estimate, and subpaths with similarity
scores outside the top 10 are ignored. The rank
groups are checked one after another in a decision
list. Powell?s algorithm (Press et al, 2007) is used
to find the optimal parameters that directly mini-
mize summarization errors made by the acoustics-
based model relative to utterances selected from
manual transcripts.
3.3 Extractive summarization
Once the similarity matrix between sentences in a
topic is acquired, we can conduct extractive sum-
marization by using the matrix to estimate both
similarity and redundancy. As discussed above,
we take the general framework of MMR and
MEAD, i.e., a linear model combining salience
and redundancy. In practice, we used MMR in our
experiments, since the original MEAD considers
also sentence positions 3 , which can always been
added later as in (Penn and Zhu, 2008).
To facilitate our discussion below, we briefly re-
visit MMR here. MMR (Carbonell and Goldstein,
1998) iteratively augments the summary with ut-
terances that are most similar to the document
set under consideration, but most dissimilar to the
previously selected utterances in that summary, as
shown in the equation below. Here, the sim1 term
represents the similarity between a sentence and
the document set it belongs to. The assumption is
that a sentence having a higher sim1 would better
represent the content of the documents. The sim2
term represents the similarity between a candidate
sentence and sentences already in the summary. It
is used to control redundancy. For the transcript-
based systems, the sim1 and sim2 scores in this
paper are measured by the number of words shared
between a sentence and a sentence/document set
mentioned above, weighted by the idf scores of
these words, which is similar to the calculation of
sentence centroid values by Radev et al (2004).
3The usefulness of position varies significantly in differ-
ent genres (Penn and Zhu, 2008). Even in the news domain,
the style of broadcast news differs from written news, for
example, the first sentence often serves to attract audiences
(Christensen et al, 2004) and is hence less important as in
written news. Without consideration of position, MEAD is
more similar to MMR.
Note that the acoustics-based approach estimates
this by using the method discussed above in Sec-
tion 3.2.
Nextsent = argmax
tnr,j
(? sim1(doc, tnr,j)
?(1 ? ?)maxtr,ksim2(tnr,j, tr,k))
4 Experimental setup
We use the TDT-4 dataset for our evaluation,
which consists of annotated news broadcasts
grouped into common topics. Since our aim in this
paper is to study the achievable performance of the
audio-based model, we grouped together news sto-
ries by their news anchors for each topic. Then we
selected the largest 20 groups for our experiments.
Each of these contained between 5 and 20 articles.
We compare our acoustics-only approach
against transcripts produced automatically from
two ASR systems. The first set of transcripts
was obtained directly from the TDT-4 database.
These transcripts contain a word error rate of
12.6%, which is comparable to the best accura-
cies obtained in the literature on this data set.
We also run a custom ASR system designed to
produce transcripts at various degrees of accu-
racy in order to simulate the type of performance
one might expect given languages with sparser
training corpora. These custom acoustic mod-
els consist of context-dependent tri-phone units
trained on HUB-4 broadcast news data by se-
quential Viterbi forced alignment. During each
round of forced alignment, the maximum likeli-
hood linear regression (MLLR) transform is used
on gender-dependent models to improve the align-
ment quality. Language models are also trained on
HUB-4 data.
Our aim in this paper is to study the achievable
performance of the audio-based model. Instead
of evaluating the result against human generated
summaries, we directly compare the performance
against the summaries obtained by using manual
transcripts, which we take as an upper bound to
the audio-based system?s performance. This ob-
viously does not preclude using the audio-based
system together with other features such as utter-
ance position, length, speaker?s roles, and most
others used in the literature (Penn and Zhu, 2008).
Here, we do not want our results to be affected by
them with the hope of observing the difference ac-
curately. As such, we quantify success based on
ROUGE (Lin, 2004) scores. Our goal is to evalu-
554
ate whether the relatedness of spoken documents
can reasonably be gleaned solely from the surface
acoustic information.
5 Experimental results
We aim to empirically determine the extent to
which acoustic information alone can effectively
replace conventional speech recognition within the
multi-document speech summarization task. Since
ASR performance can vary greatly as we dis-
cussed above, we compare our system against
automatic transcripts having word error rates of
12.6%, 20.9%, 29.2%, and 35.5% on the same
speech source. We changed our language mod-
els by restricting the training data so as to obtain
the worst WER and then interpolated the corre-
sponding transcripts with the TDT-4 original au-
tomatic transcripts to obtain the rest. Figure 2
shows ROUGE scores for our acoustics-only sys-
tem, as depicted by horizontal lines, as well as
those for the extractive summaries given automatic
transcripts having different WERs, as depicted
by points. Dotted lines represent the 95% con-
fidence intervals of the transcript-based models.
Figure 2 reveals that, typically, as the WERs of au-
tomatic transcripts increase to around 33%-37%,
the difference between the transcript-based and the
acoustics-based models is no longer significant.
These observations are consistent across sum-
maries with different fixed lengths, namely 10%,
20%, and 30% of the lengths of the source docu-
ments for the top, middle, and bottom rows of Fig-
ure 2, respectively. The consistency of this trend is
shown across both ROUGE-2 and ROUGE-SU4,
which are the official measures used in the DUC
evaluation. We also varied the MMR parameter ?
within a typical range of 0.4?1, which yielded the
same observation.
Since the acoustics-based approach can be ap-
plied to any data domain and to any language
in principle, this would be of special interest
when those situations yield relatively high WER
with conventional ASR. Figure 2 also shows the
ROUGE scores achievable by selecting utterances
uniformly at random for extractive summarization,
which are significantly lower than all other pre-
sented methods and corroborate the usefulness of
acoustic information.
Although our acoustics-based method performs
similarly to automatic transcripts with 33-37%
WER, the errors observed are not the same, which
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=10% Rand=0.197
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=20%, Rand=0.340
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=30%, Rand=0.402
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=10%, Rand=0.176
R
O
UG
E?
2
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=20%, Rand=0.324
R
O
UG
E?
2
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=30%, Rand=0.389
R
O
UG
E?
2
Word error rate
Figure 2: ROUGE scores and 95% confidence in-
tervals for the MMR-based extractive summaries
produced from our acoustics-only approach (hori-
zontal lines), and from ASR-generated transcripts
having varying WER (points). The top, middle,
and bottom rows of subfigures correspond to sum-
maries whose lengths are fixed at 10%, 20%, and
30% the sizes of the source text, respectively. ? in
MMR takes 1, 0.7, and 0.4 in these rows, respec-
tively.
we attribute to fundamental differences between
these two methods. Table 1 presents the number
of different utterances correctly selected by the
acoustics-based and ASR-based methods across
three categories, namely those sentences that are
correctly selected by both methods, those ap-
pearing only in the acoustics-based summaries,
and those appearing only in the ASR-based sum-
maries. These are shown for summaries having
different proportional lengths relative to the source
documents and at different WERs. Again, correct-
ness here means that the utterance is also selected
when using a manual transcript, since that is our
defined topline.
A manual analysis of the corpus shows that
utterances correctly included in summaries by
555
Summ. Both ASR Aco.-
length only only
WER=12.6%
10% 85 37 8
20% 185 62 12
30% 297 87 20
WER=20.9%
10% 83 36 10
20% 178 65 19
30% 293 79 24
WER=29.2%
10% 77 34 16
20% 172 58 25
30% 286 64 31
WER=35.5%
10% 75 33 18
20% 164 54 33
30% 272 67 45
Table 1: Utterances correctly selected by both
the ASR-based models and acoustics-based ap-
proach, or by either of them, under different
WERs (12.6%, 20.9%, 29.2%, and 35.5%) and
summary lengths (10%, 20%, and 30% utterances
of the original documents)
the acoustics-based method often contain out-of-
vocabulary errors in the corresponding ASR tran-
scripts. For example, given the news topic of the
bombing of the U.S. destroyer ship Cole in Yemen,
the ASR-based method always mistook the word
Cole, which was not in the vocabulary, for cold,
khol, and called. Although named entities and
domain-specific terms are often highly relevant
to the documents in which they are referenced,
these types of words are often not included in
ASR vocabularies, due to their relative global rar-
ity. Importantly, an unsupervised acoustics-based
approach such as ours does not suffer from this
fundamental discord. At the very least, these find-
ings suggest that ASR-based summarization sys-
tems augmented with our type of approach might
be more robust against out-of-vocabulary errors.
It is, however, very encouraging that an acoustics-
based approach can perform to within a typical
WER range within non-broadcast-news domains,
although those domains can likewise be more
challenging for the acoustics-based approach. Fur-
ther experimentation is necessary. It is also of sci-
entific interest to be able to quantify this WER as
an acoustics-only baseline for further research on
ASR-based spoken document summarizers.
6 Conclusions and future work
In text summarization, statistics based on word
counts have traditionally served as the foundation
of state-of-the-art models. In this paper, the simi-
larity of utterances is estimated directly from re-
curring acoustic patterns in untranscribed audio
sequences. These relatedness scores are then in-
tegrated into a maximum marginal relevance lin-
ear model to estimate the salience and redundancy
of those utterance for extractive summarization.
Our empirical results show that the summarization
performance given acoustic information alone is
statistically indistinguishable from that of modern
ASR on broadcast news in cases where the WER
of the latter approaches 33%-37%. This is an en-
couraging result in cases where summarization is
required, but ASR is not available or speech recog-
nition performance is degraded. Additional anal-
ysis suggests that the acoustics-based approach
is useful in overcoming situations where out-of-
vocabulary error may be more prevalent, and we
suggest that a hybrid approach of traditional ASR
with acoustics-based pattern matching may be the
most desirable future direction of research.
One limitation of the current analysis is that
summaries are extracted only for collections of
spoken documents from among similar speakers.
Namely, none of the topics under analysis consists
of a mix of male and female speakers. We are cur-
rently investigating supervised methods to learn
joint probabilistic models relating the acoustics of
groups of speakers in order to normalize acoustic
similarity matrices (Toda et al, 2001). We sug-
gest that if a stochastic transfer function between
male and female voices can be estimated, then the
somewhat disparate acoustics of these groups of
speakers may be more easily compared.
References
R. Barzilay, K. McKeown, and M. Elhadad. 1999. In-
formation fusion in the context of multi-document
summarization. In Proc. of the 37th Association for
Computational Linguistics, pages 550?557.
J. G. Carbonell and J. Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st annual international ACM SIGIR con-
ference on research and development in information
retrieval, pages 335?336.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2004. From text summarisation to style-specific
556
summarisation for broadcast news. In Proceedings
of the 26th European Conference on Information Re-
trieval (ECIR-2004), pages 223?237.
S. Furui, T. Kikuichi, Y. Shinnaka, and C. Hori. 2003.
Speech-to-speech and speech to text summarization.
In First International workshop on Language Un-
derstanding and Agents for Real World Interaction.
M. Gajjar, R. Govindarajan, and T. V. Sreenivas. 2008.
Online unsupervised pattern discovery in speech us-
ing parallelization. In Proc. Interspeech, pages
2458?2461.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999.
Auto-summarization of audio-video presentations.
In Proceedings of the seventh ACM international
conference on Multimedia, pages 489?498.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000.
Comparing presentation summaries: Slides vs. read-
ing vs. listening. In Proceedings of ACM CHI, pages
177?184.
Y. Lin, T. Jiang, and Chao. K. 2002. Efficient al-
gorithms for locating the length-constrained heavi-
est segments with applications to biomolecular se-
quence analysis. J. Computer and System Science,
63(3):570?586.
C. Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of the
42st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Text Summarization
Branches Out Workshop, pages 74?81.
I Malioutov, A. Park, B. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. ACL, pages
504?511.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features
for speech summarization. In Proceedings of the
9th European Conference on Speech Communica-
tion and Technology (Eurospeech), pages 621?624.
K. Mckeown and D.R. Radev. 1995. Generating sum-
maries of multiple news articles. In Proc. of SIGIR,
pages 72?82.
C. Munteanu, R. Baecker, G Penn, E. Toms, and
E. James. 2006. Effect of speech recognition ac-
curacy rates on the usefulness and usability of we-
bcast archives. In Proceedings of SIGCHI, pages
493?502.
G. Murray, S. Renals, and J. Carletta. 2005.
Extractive summarization of meeting recordings.
In Proceedings of the 9th European Conference
on Speech Communication and Technology (Eu-
rospeech), pages 593?596.
A. Park and J. Glass. 2006. Unsupervised word ac-
quisition from speech using pattern discovery. Proc.
ICASSP, pages 409?412.
A. Park and J. Glass. 2008. Unsupervised pattern dis-
covery in speech. IEEE Trans. ASLP, 16(1):186?
197.
G. Penn and X. Zhu. 2008. A critical reassessment of
evaluation baselines for speech summarization. In
Proc. of the 46th Association for Computational Lin-
guistics, pages 407?478.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2007. Numerical recipes: The art of sci-
ence computing.
D. Radev and K. McKeown. 1998. Generating natural
language summaries from multiple on-line sources.
In Computational Linguistics, pages 469?500.
D. Radev, H. Jing, M. Stys, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40:919?938.
T. Toda, H. Saruwatari, and K. Shikano. 2001. Voice
conversion algorithm based on gaussian mixture
model with dynamic frequency warping of straight
spectrum. In Proc. ICASPP, pages 841?844.
S. Tucker and S. Whittaker. 2008. Temporal compres-
sion of speech: an evaluation. IEEE Transactions
on Audio, Speech and Language Processing, pages
790?796.
K. Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. the-
sis, Carnegie Mellon University.
J. Zhang, H. Chan, P. Fung, and L Cao. 2007. Compar-
ative study on speech summarization of broadcast
news and lecture speech. In Proc. of Interspeech,
pages 2781?2784.
X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proceedings of the 9th
International Conference on Spoken Language Pro-
cessing, pages 1531?1534.
557
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 60?68,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Correcting errors in speech recognition with articulatory dynamics
Frank Rudzicz
University of Toronto, Department of Computer Science
Toronto, Ontario, Canada
frank@cs.toronto.edu
Abstract
We introduce a novel mechanism for
incorporating articulatory dynamics into
speech recognition with the theory of task
dynamics. This system reranks sentence-
level hypotheses by the likelihoods of
their hypothetical articulatory realizations
which are derived from relationships
learned with aligned acoustic/articulatory
data. Experiments compare this with two
baseline systems, namely an acoustic hid-
den Markov model and a dynamic Bayes
network augmented with discretized rep-
resentations of the vocal tract. Our sys-
tem based on task dynamics reduces word-
error rates significantly by 10.2% relative
to the best baseline models.
1 Introduction
Although modern automatic speech recognition
(ASR) takes several cues from the biological per-
ception of speech, it rarely models its biological
production. The result is that speech is treated
as a surface acoustic phenomenon with lexical or
phonetic hidden dynamics but without any phys-
ical constraints in between. This omission leads
to some untenable assumptions. For example,
speech is often treated out of convenience as a se-
quence of discrete, non-overlapping packets, such
as phonemes, despite the fact that some major dif-
ficulties in ASR, such as co-articulation, are by
definition the result of concurrent physiological
phenomena (Hardcastle and Hewlett, 1999).
Many acoustic ambiguities can be resolved
with knowledge of the vocal tract?s configuration
(O?Shaughnessy, 2000). For example, the three
nasal sonorants, /m/, /n/, and /ng/, are acousti-
cally similar (i.e., they have large concentrations
of energy at the same frequencies) but uniquely
and reliably involve bilabial closure, tongue-tip
elevation, and tongue-dorsum elevation, respec-
tively. Having access to the articulatory goals of
the speaker would, in theory, make the identifica-
tion of linguistic intent almost trivial. Although
we don?t typically have access to the vocal tract
during speech recognition, its configuration can
be estimated reasonably well from acoustics alone
within adequate models or measurements of the
vocal tract (Richmond et al, 2003; Toda et al,
2008). Evidence that such inversion takes place
naturally in humans during speech perception sug-
gests that the discriminability of speech sounds de-
pends powerfully on their production (Liberman
and Mattingly, 1985; D?Ausilio et al, 2009).
This paper describes the use of explicit models
of physical speech production within recognition
systems. Initially, we augment traditional models
of ASR with probabilistic relationships between
acoustics and articulation learned from appropri-
ate data. This leads to the incorporation of a high-
level, goal-oriented, and control-based theory of
speech production within a novel ASR system.
2 Background and related work
The use of theoretical (phonological) features of
the vocal tract has provided some improvement
over traditional acoustic ASR systems in phoneme
recognition with neural networks (Kirchhoff,
1999; Roweis, 1999), but there has been very
little work in ASR informed by direct measure-
ments of the vocal tract. Recently, Markov et
al. (2006) have augmented hidden Markov models
with Bayes networks trained to describe articula-
tory constraints from a small amount of Japanese
vocal tract data, resulting in a small phoneme-
error reduction. This work has since been ex-
panded upon to inform ASR systems sensitive to
physiological speech disorders (Rudzicz, 2009).
Common among previous efforts is an interpre-
tation of speech as a sequence of short, instanta-
neous observations devoid of long-term dynamics.
60
2.1 Articulatory phonology
Articulatory phonology bridges the divide be-
tween the physical manifestation of speech and its
underlying lexical intentions. Within this disci-
pline, the theory of task dynamics is a combined
model of physical articulator motion and the plan-
ning of abstract vocal tract configurations (Saltz-
man, 1986). This theory introduces the notion that
all observed patterns of speech are the result of
overlapping gestures, which are abstracted goal-
oriented reconfigurations of the vocal tract, such
as bilabial closure or velar opening (Saltzman and
Munhall, 1989). Each gesture occurs within one
of the following tract variables (TVs): velar open-
ing (VEL), lip aperture (LA) and protrusion (LP),
tongue tip constriction location (TTCL) and de-
gree (TTCD) 1, tongue body constriction location
(TBCL) and degree (TBCD), lower tooth height
(LTH), and glottal vibration (GLO). For example,
the syllable pub consists of an onset (/p/), a nu-
cleus (/ah/), and a coda (/b/). Four gestural goals
are associated with the onset, namely the shutting
of GLO and of VEL, and the closure and release of
LA. Similarly, the nucleus of the syllable consists
of three goals, namely the relocation of TBCD and
TBCL, and the opening of GLO. The presence and
extent of these gestural goals are represented by
filled rectangles in figure 1. Inter-gestural timings
between these goals are specified relative to one
another according to human data as described by
Nam and Saltzman (2003).
TBCD closedopen
GLO openclosed
LA openclosed
100 200 300 400Time (ms)
Figure 1: Canonical example pub from Saltzman
and Munhall (1989).
The presence of these discrete goals influences
the vocal tract dynamically and continuously
as modelled by the following non-homogeneous
second-order linear differential equation:
Mz??+Bz?+K(z? z?) = 0. (1)
1Constriction locations generally refer to the front-back
dimension of the vocal tract and constriction degrees gener-
ally refer to the top-down dimension.
Here, z is a continuous vector representing the in-
stantaneous positions of the nine tract variables,
z? is the target (equilibrium) positions of those
variables, and vectors z? and z?? represent the first
and second derivatives of z with respect to time
(i.e., velocity and acceleration), respectively. The
matrices M, B, and K are syllable-specific coef-
ficients describing the inertia, damping, and stiff-
ness, respectively, of the virtual gestures. Gener-
ally, this theory assumes that the tract variables are
mutually independent, and that the system is criti-
cally damped (i.e., the tract variables do not oscil-
late around their equilibrium positions) (Nam and
Saltzman, 2003). The continuous state, z, of equa-
tion (1) is exemplified by black curves in figure 1.
2.2 Articulatory data
Tract variables provide the dimensions of an ab-
stract gestural space independent of the physical
characteristics of the speaker. In order to com-
plete our articulatory model, however, we require
physical data from which to infer these high-level
articulatory goals.
Electromagnetic articulography (EMA) is a
method to measure the motion of the vocal tract
during speech. In EMA, the speaker is placed
within a low-amplitude electromagnetic field pro-
duced within a cube of a known geometry. Tiny
sensors within this field induce small electric cur-
rents whose energy allows the inference of artic-
ulator positions and velocities to within 1 mm of
error (Yunusova et al, 2009). We derive data for
the following study from two EMA sources:
? The University of Edinburgh?s MOCHA
database, which provides phonetically-
balanced sentences repeated from TIMIT
(Zue et al, 1989) uttered by a male and a
female speaker (Wrench, 1999), and
? The University of Toronto?s TORGO
database, from which we select sentences
repeated from TIMIT from two females
and three males (Rudzicz et al, 2008).
(Cerebrally palsied speech, which is the
focus of this database, is not included here).
For the following study we use the eight 2D po-
sitions common to both databases, namely the up-
per lip (UL), lower lip (LL), upper incisor (UI),
lower incisor (LI), tongue tip (TT), tongue blade
(TB), and tongue dorsum (TD). Since these po-
sitions are recorded in 3D in TORGO, we project
61
these onto the midsagittal plane. (Additionally, the
MOCHA database provides velum (V) data on this
plane, and TORGO provides the left and right lip
corners (LL and RL) but these are excluded from
study except where noted).
All articulatory data is aligned with its associ-
ated acoustic data, which is transformed to Mel-
frequency cepstral coefficients (MFCCs). Since
the 2D EMA system in MOCHA and the 3D EMA
system in TORGO differ in their recording rates,
the length of each MFCC frame in each database
must differ in order to properly align acoustics
with articulation in time. Therefore, each MFCC
frame covers 16 ms in the TORGO database, and
32 ms in MOCHA. Phoneme boundaries are de-
termined automatically in the MOCHA database
by forced alignment, and by a speech-language
pathologist in the TORGO database.
We approximate the tract variable space from
the physical space of the articulators, in general,
through principal component analysis (PCA) on
the latter, and subsequent sigmoid normalization
on [0,1]. For example, the LTH tract variable is in-
ferred by calculating the first principal component
of the two-dimensional lower incisor (LI) motion
in the midsagittal plane, and by normalizing the
resulting univariate data through a scaled sigmoid.
The VEL variable is inferred similarly from velum
(V) EMA data. Tongue tip constriction location
and degree (TTCL and TTCD, respectively) are
inferred from the 1st and 2nd principal components
of tongue tip (TT) EMA data, with TBCL and
TBCD inferred similarly from tongue body (TB)
data. Finally, the glottis (GLO) is inferred by voic-
ing detection on acoustic energy below 150 Hz
(O?Shaughnessy, 2000), lip aperture (LA) is the
normalized Euclidean distance between the lips,
and lip protrusion (LP) is the normalized 2nd prin-
cipal component of the midpoint between the lips.
All PCA is performed without segmentation of the
data. The result is a low-dimensional set of contin-
uous curves describing goal-relevant articulatory
variables. Figure 2, for example, shows the degree
of the lip aperture (LA) over time for all instances
of the /b/ phoneme in the MOCHA database. The
relevant articulatory goal of lip closure is evident.
3 Baseline systems
We now turn to the task of speech recognition.
Traditional Bayesian learning is restricted to uni-
versal or immutable relationships, and is agnos-
0 50 100 150 2000
0.2
0.4
0.6
0.8
1
Time (ms)
nor
ma
lized
 LA
Figure 2: Lip aperture (LA) over time during all
MOCHA instances of /b/.
tic towards dynamic systems or time-varying rela-
tionships. Dynamic Bayes networks (DBNs) are
directed acyclic graphs that generalize the power-
ful stochastic mechanisms of Bayesian represen-
tation to temporal sequences. We are free to ex-
plicitly provide topological (i.e., dependency) re-
lationships between relevant variables in our mod-
els, which can include measurements of tract data.
We examine two baseline systems. The
first is the standard acoustic hidden Markov
model (HMM) augmented with a bigram language
model, as shown in figure 3(a). Here, Wt ?Wt+1
represents word transition probabilities, learned
by maximum likelihood estimation, and Pht ?
Pht+1 represents phoneme transition probabilities
whose order is explicitly specified by the relation-
ship Wt ? Pht . Likewise, each phoneme Ph con-
ditions the sub-phoneme state, Qt , whose transi-
tion probabilities Qt ? Qt+1 describe the dynam-
ics within phonemes. The variable Mt refers to
hidden Gaussian indices so that the likelihoods
of acoustic observations, Ot , are represented by a
mixture of 4, 8, 16, or 32 Gaussians for each state
and each phoneme. See Murphy (2002) for a fur-
ther description of this representation.
The second baseline model is the articulatory
dynamic Bayes network (DBN-A). This augments
the standard acoustic HMM by replacing hidden
indices, Mt , with discrete observations of the vo-
cal tract, Kt , as shown in figure 3(b). The pattern
of acoustics within each phoneme is dependent on
a relatively restricted set of possible articulatory
configurations (Roweis, 1999). To find these dis-
crete positions, we obtain k vectors that best de-
62
scribe the articulatory data according to k-means
clustering with the sum-of-squares error function.
During training, the DBN variable Kt is set ex-
plicitly to the index of the mean vector nearest to
the current frame of EMA data at time t. In this
way, the relationship Kt ? Ot allows us to learn
how discretized articulatory configurations affect
acoustics. The training of DBNs involves a spe-
cialized version of expectation-maximization, as
described in the literature (Murphy, 2002; Ghahra-
mani, 1998). During inference, variables Wt , Pht ,
and Kt become hidden and we marginalize over
their possible values when computing their likeli-
hoods. Bigrams are computed by maximum like-
lihood on lexical annotations in the training data.
T B
CB
T BDc
CBDc
l B
os B
l BDc
os BDc
e B e BDc
(a) HMM
TB
CB
TBDc
CBDc
l B
os B
l BDc
os BDc
e B e BDc
(b) DBN-A
Figure 3: Baseline systems: (a) acoustic hidden
Markov model and (b) articulatory dynamic Bayes
network. NodeWt represents the current word, Pht
is the current phoneme, Qt is that phoneme?s dy-
namic state, Ot is the acoustic observation, Mt is
the Gaussian mixture component, and Kt is the dis-
cretized articulatory configuration. Filled nodes
represent observed variables during training, al-
though only Ot is observed during recognition.
Square nodes are discrete variables while circular
nodes are continuous variables.
4 Switching Kalman filter
Our first experimental system attempts speech
recognition given only articulatory data. The true
state of the tract variables at time t?1 constitutes
a 9-dimensional vector, xt?1, of continuous val-
ues. Under the task dynamics model of section
2.1, the motions of these tract variables obey crit-
ically damped second-order oscillatory relation-
ships. We start with the simplifying assumption of
linear dynamics here with allowances for random
Gaussian process noise, vt , since articulatory be-
haviour is non-deterministic. Moreover, we know
that EMA recordings are subject to some error
(usually less than 1 mm (Yunusova et al, 2009)),
so the actual observation at time t, yt , will not in
general be the true position of the articulators. As-
suming that the relationship between yt and xt is
also linear, and that the measurement noise, wt ,
is also Gaussian, then the dynamical articulatory
system can be described by
xt = Dtxt?1 +vt
yt =Ctxt +wt .
(2)
Eqs. 2 form the basis of the Kalman filter
which allows us to use EMA measurements di-
rectly, rather than quantized abstractions thereof
as in the DBN-A model. Obviously, since artic-
ulatory dynamics vary significantly for different
goals, we replicate eq. (2) for each phoneme and
connect these continuous Kalman filters together
with discrete conditioning variables for phoneme
and word, resulting in the switching Kalman fil-
ter (SKF) model. Here, parameters Dt and vt are
implicit in the relationship xt ? xt+1, and param-
eters Ct and wt are implicit in xt ? yt . In this
model, observation yt is the instantaneous mea-
surements derived from EMA, and xt is their true
hidden states. These parameters are trained using
expectation-maximization, as described in the lit-
erature (Murphy, 1998; Deng et al, 2005).
5 Recognition with task dynamics
Our goal is to integrate task dynamics within an
ASR system for continuous sentences called TD-
ASR. Our approach is to re-rank an N-best list of
sentence hypotheses according to a weighted like-
lihood of their articulatory realizations. For ex-
ample, if a word sequence Wi : wi,1 wi,2 ... wi,m
has likelihoods LX(Wi) and L?(Wi) according to
purely acoustic and articulatory interpretations of
an utterance, respectively, then its overall score
would be
L(Wi) = ?LX(Wi)+(1??)L?(Wi) (3)
given a weighting parameter ? set manually, as in
section 6.2. Acoustic likelihoods LX(Wi) are ob-
tained from Viterbi paths through relevant HMMs
in the standard fashion.
5.1 The TADA component
In order to obtain articulatory likelihoods, L?(Wi),
for each word sequence, we first generate artic-
ulatory realizations of those sequences according
63
to task dynamics. To this end, we use compo-
nents from the open-source TADA system (Nam
and Goldstein, 2006), which is a complete imple-
mentation of task dynamics. From this toolbox,
we use the following components:
? A syllabic dictionary supplemented with
the International Speech Lexicon Dictionary
(Hasegawa-Johnson and Fleck, 2007). This
breaks word sequences Wi into syllable se-
quences Si consisting of onsets, nuclei, and
coda and covers all of MOCHA and TORGO.
? A syllable-to-gesture lookup table. Given
a syllabic sequence, Si, this table provides
the gestural goals necessary to produce those
syllables. For example, given the syllable
pub in figure 1, this table provides the tar-
gets for the GLO, VEL, TBCL, and TBCD
tract variables, and the parameters for the
second-order differential equation, eq. 1,
that achieves those goals. These parameters
have been empirically tuned by the authors
of TADA according to a generic, speaker-
independent representation of the vocal tract
(Saltzman and Munhall, 1989).
? A component that produces the continuous
tract variable paths that produce an utter-
ance. This component takes into account var-
ious physiological aspects of human speech
production, including intergestural and in-
terarticulator co-ordination and timing (Nam
and Saltzman, 2003; Goldstein and Fowler,
2003), and the neutral (?schwa?) forces of the
vocal tract (Saltzman and Munhall, 1989).
This component takes a sequence of gestu-
ral goals predicted by the segment-to-gesture
lookup table, and produces appropriate paths
for each tract variable.
The result of the TADA component is a set of
N 9-dimensional articulatory paths, TVi, neces-
sary to produce the associated word sequences, Wi
for i = 1..N. Since task dynamics is a prescrip-
tive model and fully deterministic, TVi sequences
are the canonical or default articulatory realiza-
tions of the associated sentences. These canonical
realizations are independent of our training data,
so we transform them in order to more closely re-
semble the observed articulatory behaviour in our
EMA data. Towards this end, we train a switch-
ing Kalman filter identical to that in section 4, ex-
cept the hidden state variable xt is replaced by the
observed instantaneous canonical TVs predicted
by TADA. In this way we are explicitly learning
a relationship between TADA?s task dynamics and
human data. Since the lengths of these sequences
are generally unequal, we align the articulatory be-
haviour predicted by TADA with training data from
MOCHA and TORGO using standard dynamic
time warping (Sakoe and Chiba, 1978). During
run-time, the articulatory sequence yt most likely
to have been produced by the human data given the
canonical sequence TVi is inferred by the Viterbi
algorithm through the SKF model with all other
variables hidden. The result is a set of articulatory
sequences, TV?i , for i = 1..N, that represent the
predictions of task dynamics that better resemble
our data.
5.2 Acoustic-articulatory inversion
In order to estimate the articulatory likelihood
of an utterance, we need to evaluate each trans-
formed articulatory sequence, TV?i , within proba-
bility distributions ranging over all tract variables.
These distributions can be inferred using acoustic-
articulatory inversion. There are a number of ap-
proaches to this task, including vector quantiza-
tion, and expectation-maximization with Gaussian
mixtures (Hogden and Valdez, 2001; Toda et al,
2008). These approaches accurately inferred the
xy position of articulators to within 0.41 mm and
2.73 mm. Here, we modify the approach taken
by Richmond et al (2003), who estimate proba-
bility functions over the 2D midsagittal positions
of 7 articulators, given acoustics, with a mixture-
density network (MDN). An MDN is essentially a
typical discriminative multi-layer neural network
whose output consists of the parameters to Gaus-
sian mixtures. Here, each Gaussian mixture de-
scribes a probability function over TV positions
given the acoustic frame at time t. For exam-
ple, figure 4 shows an intensity map of the likely
values for tongue-tip constriction degree (TTCD)
for each frame of acoustics, superimposed with
the ?true? trajectory of that TV. Our networks are
trained with acoustic and EMA-derived data as de-
scribed in section 2.2.
5.3 Recognition by reranking
During recognition of a test utterance, a standard
acoustic HMM produces word sequence hypothe-
ses, Wi, and associated likelihoods, L(Wi), for i =
1..N. The expected canonical motion of the tract
variables, TVi is then produced by task dynamics
64
Figure 4: Example probability density of tongue
tip constriction degree over time, inferred from
acoustics. The true trajectory is superimposed as a
black curve.
for each of these word sequences and transformed
by an SKF to better match speaker data, giving
TV?i . The likelihoods of these paths are then eval-
uated within probability distributions produced by
an MDN. The mechanism for producing the artic-
ulatory likelihood is shown in figure 5. The overall
likelihood, L(Wi) = ?LX(Wi)+ (1??)L?(Wi), is
then used to produce a final hypothesis list for the
given acoustic input.
6 Experiments
Experimental data is obtained from two sources,
as described in section 2.2. We procure 1200
sentences from Toronto?s TORGO database, and
896 from Edinburgh?s MOCHA. In total, there are
460 total unique sentence forms, 1092 total unique
word forms, and 11065 total words uttered. Ex-
cept where noted, all experiments randomly split
the data into 90% training and 10% testing sets for
5-cross validation. MOCHA and TORGO data are
never combined in a single training set due to dif-
fering EMA recording rates. In all cases, models
are database-dependent (i.e., all TORGO data is
conflated, as is all of MOCHA).
For each of our baseline systems, we calcu-
late the phoneme-error-rate (PER) and word-error-
rate (WER) after training. The phoneme-error-
rate is calculated according to the proportion of
frames of speech incorrectly assigned to the proper
phoneme. The word-error-rate is calculated as
the sum of insertion, deletion, and substitution er-
rors in the highest-ranked hypothesis divided by
the total number of words in the correct orthogra-
phy. The traditional HMM is compared by vary-
ing the number of Gaussians used in the modelling
System Parameters PER (%) WER (%)
HMM
|M|= 4 29.3 14.5
|M|= 8 27.0 13.9
|M|= 16 26.1 10.2
|M|= 32 25.6 9.7
DBN-A
|K|= 4 26.1 13.0
|K|= 8 25.2 11.3
|K|= 16 24.9 9.8
|K|= 32 24.8 9.4
Table 1: Phoneme- and Word-Error-Rate (PER
and WER) for different parameterizations of the
baseline systems.
No. of Gaussians
1 2 3 4
LTH ?0.28 ?0.18 ?0.15 ?0.11
LA ?0.36 ?0.32 ?0.30 ?0.29
LP ?0.46 ?0.44 ?0.43 ?0.43
GLO ?1.48 ?1.30 ?1.29 ?1.25
TTCD ?1.79 ?1.60 ?1.51 ?1.47
TTCL ?1.81 ?1.62 ?1.53 ?1.49
TBCD ?0.88 ?0.79 ?0.75 ?0.72
TDCL ?0.22 ?0.20 ?0.18 ?0.17
Table 2: Average log likelihood of true tract vari-
able positions in test data, under distributions pro-
duced by mixture density networks with varying
numbers of Gaussians.
of acoustic observations. Similarly, the DBN-A
model is compared by varying the number of dis-
crete quantizations of articulatory configurations,
as described in section 3. Results are obtained by
direct decoding. The average results across both
databases, between which there are no significant
differences, are shown in table 1. In all cases
the DBN-A model outperforms the HMM, which
highlights the benefit of explicitly conditioning
acoustic observations on articulatory causes.
6.1 Efficacy of TD-ASR components
In order to evaluate the whole system, we start by
evaluating its parts. First, we test how accurately
the mixture-density network (MDN) estimates the
position of the articulators given only information
from the acoustics available during recognition.
Table 2 shows the average log likelihood over each
tract variable across both databases. These re-
sults are consistent with the state-of-the-art (Toda
et al, 2008). In the following experiments, we use
MDNs that produce 4 Gaussians.
65
T T
BCDclosCl
BedBed
pnGpnG
L 1L 2OOOL NGA10lo234Do20l0l
?BnB?BnB ?? 1?? 2OOO?? N?? ??sC?????CoT???s?1?l
?dBGe?dBGe ??? 1?? ? 2OOO??? NpD??s0????CoT???s?1?l ???? i? L? 1L? 2OOOL? Nd0???0??lo
Figure 5: The TD-ASR mechanism for deriving articulatory likelihoods, L?(Wi), for each word sequence
Wi produced by standard acoustic techniques.
Manner Canonical Transformed
approximant 0.19 0.16
fricative 0.37 0.29
nasal* 0.24 0.18
retroflex 0.23 0.19
plosive 0.10 0.08
vowel 0.27 0.25
Table 3: Average difference between predicted
tract variables and observed data, on [0,1] scale.
(*) Nasals are evaluated only with MOCHA data,
since TORGO data lacks velum measurements.
We evaluate how closely transformations to the
canonical tract variables predicted by TADA match
the data. Namely, we input the known orthography
for each test utterance into TADA, obtain the pre-
dicted canonical tract variables TV, and transform
these according to our trained SKF. The resulting
predicted and transformed sequences are aligned
with our measurements derived from EMA with
dynamic time warping. Finally, we measure the
average difference between the observed data and
the predicted (canonical and transformed) tract
variables. Table 3 shows these differences accord-
ing to the phonological manner of articulation. In
all cases the transformed tract variable motion is
more accurate, and significantly so at the 95% con-
fidence level for nasal and retroflex phonemes, and
at 99% for fricatives. The practical utility of the
transformation component is evaluated in its effect
on recognition rates, as described below.
6.2 Recognition with TD-ASR
With the performance of the components of TD-
ASR better understood, we combine these and
study the resulting composite TD-ASR system.
0 0.2 0.4 0.6 0.8 18
8.5
9
9.5
10
?
WE
R (%
)
 
 TORGOMOCHA
Figure 6: Word-error-rate according to varying ?,
for both TORGO and MOCHA data.
Figure 6 shows the WER as a function of ? with
TD-ASR and N = 4 hypotheses per utterance. The
effect of ? is clearly non-monotonic, with articula-
tory information clearly proving useful. Although
systems whose rankings are weighted solely by the
articulatory component perform better than the ex-
clusively acoustic systems, the lists available to the
former are procured from standard acoustic ASR.
Interestingly, the gap between systems trained to
the two databases increases as ? approaches 1.0.
Although this gap is not significant, it may be the
result of increased inter-speaker articulatory varia-
tion in the TORGO database, which includes more
than twice as many speakers as MOCHA.
Figure 7 shows the WER obtained with TD-
ASR given varying-length N-best lists and ? =
0.7. TD-ASR accuracy at N = 4 is significantly
better than both TD-ASR at N = 2 and the base-
line approaches of table 1 at the 95% confidence
level. However, for N > 4 there is a noticeable
and systematic worsening of performance.
66
2 3 4 5 6 7 88.2
8.4
8.6
8.8
9
9.2
9.4
9.6
9.8
Length of N?best list
WE
R (%
)
 
 TORGOMOCHA
Figure 7: Word-error-rate according to vary-
ing lengths of N-best hypotheses used, for both
TORGO and MOCHA data.
The optimal parameterization of the TD-ASR
model results in an average word-error-rate of
8.43%, which represents a 10.3% relative error re-
duction over the best parameterization of our base-
line models. The SKF model of section 4 differs
from the HMM and DBN-A baseline models only
in its use of continuous (rather than discrete) hid-
den dynamics and in its articulatory observations.
However, its performance is far more variable, and
less conclusive. On the MOCHA database the
SKF model had an average of 9.54% WER with
a standard deviation of 0.73 over 5 trials, and an
average of 9.04% WER with a standard deviation
of 0.64 over 5 trials on the TORGO database. De-
spite the presupposed utility of direct articulatory
observations, the SKF system does not perform
significantly better than the best DBN-A model.
Finally, the experiments of tables 6 and 7 are
repeated with the canonical tract variables passed
untransformed to the probability maps generated
by the MDNs. Predictably, resulting articulatory
likelihoods L? are less representative and increas-
ing their contribution ? to the hypothesis rerank-
ing does not improve TD-ASR performance sig-
nificantly, and in some instances worsens it. Al-
though TADA is a useful prescriptive model of
generic articulation, its use must be tempered with
knowledge of inter-speaker variability.
7 Discussion and conclusions
The articulatory medium of speech rarely informs
modern speech recognition. We have demon-
strated that the use of direct articulatory knowl-
edge can substantially reduce phoneme and word
errors in speech recognition, especially if that
knowledge is motivated by high-level abstrac-
tions of vocal tract behaviour. Task dynamic the-
ory provides a coherent and biologically plausible
model of speech production with consequences for
phonology (Browman and Goldstein, 1986), neu-
rolinguistics (Guenther and Perkell, 2004), and the
evolution of speech and language (Goldstein et al,
2006). We have shown that it is also useful within
speech recognition.
We have overcome a conceptual impediment in
integrating task dynamics and ASR, which is the
former?s deterministic nature. This integration is
accomplished by stochastically transforming pre-
dicted articulatory dynamics and by calculating
the likelihoods of these dynamics according to
speaker data. However, there are several new av-
enues for exploration. For example, task dynamics
lends itself to more general applications of con-
trol theory, including automated self-correction,
rhythm, co-ordination, and segmentation (Fried-
land, 2005). Other high-level questions also re-
main, such as whether discrete gestures are the
correct biological and practical paradigm, whether
a purely continuous representation would be more
appropriate, and whether this approach general-
izes to other languages.
In general, our experiments have revealed very
little difference between the use of MOCHA and
TORGO EMA data. An ad hoc analysis of some
of the errors produced by the TD-ASR system
found no particular difference between how sys-
tems trained to each of these databases recognized
nasal phonemes, although only those trained with
MOCHA considered velum motion. Other errors
common to both sources of data include phoneme
insertion errors, normally vowels, which appear to
co-occur with some spurious motion of the tongue
between segments, especially for longer N-best
lists. Despite the relative slow motion of the ar-
ticulators relative to acoustics, there remains some
intermittent noise.
As more articulatory data becomes available
and as theories of speech production become more
refined, we expect that their combined value to
speech recognition will become indispensable.
Acknowledgments
This research is funded by the Natural Sciences
and Engineering Research Council and the Uni-
versity of Toronto.
67
References
Catherine P. Browman and Louis M. Goldstein. 1986. To-
wards an articulatory phonology. Phonology Yearbook,
3:219?252.
Alessandro D?Ausilio, Friedemann Pulvermuller, Paola
Salmas, Ilaria Bufalari, Chiara Begliomini, and Luciano
Fadiga. 2009. The motor somatotopy of speech percep-
tion. Current Biology, 19(5):381?385, February.
Jianping Deng, M. Bouchard, and Tet Yeap. 2005. Speech
Enhancement Using a Switching Kalman Filter with a Per-
ceptual Post-Filter. In Acoustics, Speech, and Signal Pro-
cessing, 2005. Proceedings. (ICASSP ?05). IEEE Interna-
tional Conference on, volume 1, pages 1121?1124, 18-23,.
Bernard Friedland. 2005. Control System Design: An Intro-
duction to State-Space Methods. Dover.
Zoubin Ghahramani. 1998. Learning dynamic Bayesian net-
works. In Adaptive Processing of Sequences and Data
Structures, pages 168?197. Springer-Verlag.
Louis M. Goldstein and Carol Fowler. 2003. Articulatory
phonology: a phonology for public language use. Phonet-
ics and Phonology in Language Comprehension and Pro-
duction: Differences and Similarities.
Louis Goldstein, Dani Byrd, and Elliot Saltzman. 2006. The
role of vocal tract gestural action units in understanding
the evolution of phonology. In M.A. Arib, editor, Action
to Language via the Mirror Neuron System, pages 215?
249. Cambridge University Press, Cambridge, UK.
Frank H. Guenther and Joseph S. Perkell. 2004. A neu-
ral model of speech production and its application to
studies of the role of auditory feedback in speech. In
Ben Maassen, Raymond Kent, Herman Peters, Pascal Van
Lieshout, and Wouter Hulstijn, editors, Speech Motor
Control in Normal and Disordered Speech, chapter 4,
pages 29?49. Oxford University Press, Oxford.
William J. Hardcastle and Nigel Hewlett, editors. 1999.
Coarticulation ? Theory, Data, and Techniques. Cam-
bridge University Press.
Mark Hasegawa-Johnson and Margaret Fleck. 2007. Inter-
national Speech Lexicon Project.
John Hogden and Patrick Valdez. 2001. A stochastic
articulatory-to-acoustic mapping as a basis for speech
recognition. In Proceedings of the 18th IEEE Instrumen-
tation and Measurement Technology Conference, 2001.
IMTC 2001, volume 2, pages 1105?1110 vol.2.
Katrin Kirchhoff. 1999. Robust Speech Recognition Us-
ing Articulatory Information. Ph.D. thesis, University of
Bielefeld, Germany, July.
Alvin M. Liberman and Ignatius G. Mattingly. 1985. The
motor theory of speech perception revised. Cognition,
21:1?36.
Konstantin Markov, Jianwu Dang, and Satoshi Nakamura.
2006. Integration of articulatory and spectrum features
based on the hybrid HMM/BN modeling framework.
Speech Communication, 48(2):161?175, February.
Kevin Patrick Murphy. 1998. Switching Kalman Filters.
Technical report.
Kevin Patrick Murphy. 2002. Dynamic Bayesian Networks:
Representation, Inference and Learning. Ph.D. thesis,
University of California at Berkeley.
Hosung Nam and Louis Goldstein. 2006. TADA (TAsk Dy-
namics Application) manual.
Hosung Nam and Elliot Saltzman. 2003. A competitive, cou-
pled oscillator model of syllable structure. In Proceedings
of the 15th International Congress of Phonetic Sciences
(ICPhS 2003), pages 2253?2256, Barcelona, Spain.
Douglas O?Shaughnessy. 2000. Speech Communications ?
Human and Machine. IEEE Press, New York, NY, USA.
Korin Richmond, Simon King, and Paul Taylor. 2003.
Modelling the uncertainty in recovering articulation from
acoustics. Computer Speech and Language, 17:153?172.
Sam T. Roweis. 1999. Data Driven Production Models for
Speech Processing. Ph.D. thesis, California Institute of
Technology, Pasadena, California.
Frank Rudzicz, Pascal van Lieshout, Graeme Hirst, Gerald
Penn, Fraser Shein, and Talya Wolff. 2008. Towards a
comparative database of dysarthric articulation. In Pro-
ceedings of the eighth International Seminar on Speech
Production (ISSP?08), Strasbourg France, December.
Frank Rudzicz. 2009. Applying discretized articulatory
knowledge to dysarthric speech. In Proceedings of
the 2009 IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP09), Taipei, Tai-
wan, April.
Hiroaki Sakoe and Seibi Chiba. 1978. Dynamic program-
ming algorithm optimization for spoken word recognition.
IEEE Transactions on Acoustics, Speech, and Signal Pro-
cessing, ASSP-26, February.
Elliot L. Saltzman and Kevin G. Munhall. 1989. A dynam-
ical approach to gestural patterning in speech production.
Ecological Psychology, 1(4):333?382.
Elliot M. Saltzman, 1986. Task dynamic co-ordination of the
speech articulators: a preliminary model, pages 129?144.
Springer-Verlag.
Tomoki Toda, Alan W. Black, and Keiichi Tokuda. 2008.
Statistical mapping between articulatory movements and
acoustic spectrum using a Gaussian mixture model.
Speech Communication, 50(3):215?227, March.
Alan Wrench. 1999. The MOCHA-TIMIT articulatory
database, November.
Yana Yunusova, Jordan R. Green, and Antje Mefferd. 2009.
Accuracy Assessment for AG500, Electromagnetic Artic-
ulograph. Journal of Speech, Language, and Hearing Re-
search, 52:547?555, April.
Victor Zue, Stephanie Seneff, and James Glass. 1989.
Speech Database Development: TIMIT and Beyond. In
Proceedings of ESCA Tutorial and Research Workshop on
Speech Input/Output Assessment and Speech Databases
(SIOA-1989), volume 2, pages 35?40, Noordwijkerhout,
The Netherlands.
68
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 944?953,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatic detection of deception in child-produced speech using syntactic
complexity features
Maria Yancheva
Division of Engineering Science,
University of Toronto
Toronto Ontario Canada
maria.yancheva@utoronto.ca
Frank Rudzicz
Toronto Rehabilitation Institute; and
Department of Computer Science,
University of Toronto
Toronto Ontario Canada
frank@cs.toronto.edu
Abstract
It is important that the testimony of chil-
dren be admissible in court, especially
given allegations of abuse. Unfortunately,
children can be misled by interrogators or
might offer false information, with dire
consequences. In this work, we evalu-
ate various parameterizations of five clas-
sifiers (including support vector machines,
neural networks, and random forests) in
deciphering truth from lies given tran-
scripts of interviews with 198 victims of
abuse between the ages of 4 and 7. These
evaluations are performed using a novel
set of syntactic features, including mea-
sures of complexity. Our results show
that sentence length, the mean number
of clauses per utterance, and the Stajner-
Mitkov measure of complexity are highly
informative syntactic features, that classi-
fication accuracy varies greatly by the age
of the speaker, and that accuracy up to
91.7% can be achieved by support vec-
tor machines given a sufficient amount of
data.
1 Introduction
The challenge of disambiguating between truth
and deception is critical in determining the ad-
missibility of court testimony. Unfortunately, the
testimony of maltreated children is often not ad-
mitted in court due to concerns about truthfulness
since children can be instructed to deny transgres-
sions or misled to elicit false accusations (Lyon
and Dorado, 2008). However, the child is often
the only witness of the transgression (Undeutsch,
2008); automatically determining truthfulness in
such situations is therefore a paramount goal so
that justice may be served effectively.
2 Related Work
Research in the detection of deception in adult
speech has included analyses of verbal and non-
verbal cues such as behavioral changes, facial ex-
pression, speech dysfluencies, and cognitive com-
plexity (DePaulo et al, 2003). Despite statistically
significant predictors of deception such as shorter
talking time, fewer semantic details, and less co-
herent statements, DePaulo et al (2003) found that
the median effect size is very small. Deception
without special motivation (e.g., everyday ?white
lies?) exhibited almost no discernible cues of de-
ception. However, analysis of moderating factors
showed that cues were significantly more numer-
ous and salient when lies were about transgres-
sions.
Literature on deception in children is relatively
limited. In one study, Lewis et al (1989) studied
3-year-olds and measured behavioral cues, such as
facial expression and nervous body movement, be-
fore and after the elicitation of a lie. Verbal re-
sponses consisted of yes/no answers. Results sug-
gested that 3-year-old children are capable of de-
ception, and that non-verbal behaviors during de-
ception include increases in ?positive? behaviors
(e.g., smiling). However, verbal cues of deception
were not analyzed. Crucially, Lewis et al (1989)
showed that humans are no more accurate in deci-
phering truth from deception in child speech than
in adult speech, being only about 50% accurate.
More recently, researchers have used linguis-
tic features to identify deception. Newman et al
(2003) inferred deception in transcribed, typed,
and handwritten text by identifying features of lin-
guistic style such as the use of personal pronouns
944
and exclusive words (e.g., but, except, without).
These features were obtained with the Linguistic
Inquiry and Word Count (LIWC) tool and used
in a logistic regression classifier which achieved,
on average, 61% accuracy on test data. Feature
analysis showed that deceptive stories were char-
acterized by fewer self-references, more negative
emotion words, and lower cognitive complexity,
compared to non-deceptive language.
Another recent stylometric experiment in auto-
matic identification of deception was performed
by Mihalcea and Strapparava (2009). The authors
used a dataset of truthful and deceptive typed re-
sponses produced by adult subjects on three dif-
ferent topics, collected through the Amazon Me-
chanical Turk service. Two classifiers, Na??ve
Bayes (NB) and a support vector machine (SVM),
were applied on the tokenized and stemmed state-
ments to obtain best classification accuracies of
70% (abortion topic, NB), 67.4% (death penalty
topic, NB), and 77% (friend description, SVM),
where the baseline was taken to be 50%. The
large variability of classifier performance based on
the topic of deception suggests that performance
is context-dependent. The authors note this as
well by demonstrating significantly lower results
of 59.8% for NB and 57.8% for SVM when cross-
topic classification is performed by training each
classifier on two topics and testing on the third.
The Mihalcea-Strapparava mturk dataset was
further used in a study by Feng et al (2012) which
employs lexicalized and unlexicalized production
rules to obtain deep syntactic features. The cross-
validation accuracy obtained on the three topics
was improved to 77% (abortion topic), 71.5%
(death penalty topic), and 85% (friend descrip-
tion). The results nevertheless varied with topic.
Another experiment using syntactic features for
identifying sentences containing uncertain or un-
reliable information was conducted by Zheng et al
(2010) on an adult-produced dataset of abstracts
and full articles from BioScope, and on paragraphs
from Wikipedia. The results demonstrated that us-
ing syntactic dependency features extracted with
the Stanford parser improved performance on the
biological dataset, while an ensemble classifier
combining a conditional random field (CRF) and
a MaxEnt classifier performed better than individ-
ual classifiers on the Wikipedia dataset.
A meta-analysis of features used in deception
detection was performed by Hauch et al (2012)
and revealed that verbal cues based on lexical cat-
egories extracted using the LIWC tool show sta-
tistically significant, though small, differences be-
tween truth- and lie-tellers. Vartapetiance and
Gillam (2012) surveyed existing cues to verbal de-
ception and demonstrated that features in LIWC
are not indicative of deception in online content,
recommending that the features used to identify
deception and the thresholds between deception
and truth be based on the specific data set.
In the speech community, analysis of deceptive
speech has combined various acoustic, prosodic,
and lexical features (Hirschberg et al, 2005). Gra-
ciarena et al (2006) combined two independent
systems ? an acoustic Gaussian mixture model
based on Mel cepstral features, and a prosodic
support vector machine based on features such as
pitch, energy, and duration ? and achieved an ac-
curacy of 64.4% on a test subset of the Columbia-
SRI-Colorado (CSC) corpus of deceptive and non-
deceptive speech (Hirschberg et al, 2005).
While previous studies have achieved some
promising results in detecting deception with lex-
ical, acoustic, and prosodic features, syntax re-
mains relatively unexplored compared to LIWC-
based features. Syntactic complexity as a cue
to deception is consistent with literature in social
psychology which suggests that emotion suppres-
sion (e.g., inhibition of guilt and fear) consumes
cognitive resources, which can influence the un-
derlying complexity of utterances (Richards and
Gross, 1999; Richards and Gross, 2000). Ad-
ditionally, the use of syntactic features is moti-
vated by their successful use on adult-produced
datasets for detecting deceptive or uncertain utter-
ances (Feng et al, 2012; Zheng et al, 2010), as
well as in other applications, such as the evaluation
of changes in text complexity (Stajner and Mitkov,
2012), the identification of personality in conver-
sation and text (Mairesse et al, 2007), and the de-
tection of dementia through syntactic changes in
writing (Le et al, 2011).
Past work has focused on identifying deceptive
speech produced by adults. The problem of deter-
mining validity of child testimony in high-stakes
child abuse court cases motivates the analysis of
child-produced deceptive language. Further, the
use of binary classification schemes in previous
work does not account for partial truths often en-
countered in real-life scenarios. Due to the rarity
of real deceptive data, studies typically use arti-
945
ficially produced deceptive language which falls
unambiguously in one of two classes: complete
truth or complete deception (Newman et al, 2003;
Mihalcea and Strapparava, 2009). Studies which
make use of real high-stakes courtroom data con-
taining partial truths, such as the Italian DECOUR
corpus analyzed by Fornaciari and Poesio (2012),
preprocess the dataset to eliminate any partially
truthful utterances. Since utterances of this kind
are common in real language, their elimination
from the dataset is not ideal.
The present study evaluates the viability of a
novel set of 17 syntactic features as markers of de-
ception in five classifiers. Moreover, to our knowl-
edge, it is the first application of automatic de-
ception detection to a real-life dataset of deceptive
speech produced by maltreated children. The data
is scored using a gradient of truthfulness, which
is used to represent completely true, partially true,
and completely false statements. Descriptions of
the data (section 3) and feature sets (section 4) pre-
cede experimental results (section 5) and the con-
cluding discussion (section 6).
3 Data
The data used in this study were obtained from
Lyon et al (2008), who conducted and transcribed
a truth-induction experiment involving maltreated
children awaiting court appearances in the Los
Angeles County Dependency Court. Subjects
were children between the ages of 4 and 7 (99 boys
and 99 girls) who were interviewed regarding an
unambiguous minor transgression involving play-
ing with a toy. To ensure an understanding of lying
and its negative consequences, all children passed
a preliminary oath-taking competency task, requir-
ing each child to correctly identify a truth-teller
and a lie-teller in an object labeling task, as well
as to identify which of the two would be the target
of negative consequences.
During data collection, a confederate first en-
gaged each child individually in one of four condi-
tions: a) play, b) play and coach, c) no play, and d)
no play and coach. In the two play conditions, the
confederate engaged the child in play with a toy
house (in the no play conditions, they did not); in
the two coach conditions, the confederate coached
the child to lie (i.e., to deny playing if they played
with the toy house, or to admit playing if they
did not). The confederate then left and the child
was interviewed by a second researcher who per-
formed a truth-induction manipulation consisting
of one of: a) control ? no manipulation, b) oath
? the interviewer reminded the child of the im-
portance of telling the truth and elicited a promise
of truth-telling, and c) reassurance ? the inter-
viewer reassured the child that telling the truth will
not lead to any negative consequences.
Each pre- and post-induction transcription may
contain explicit statements of up to seven features:
looking at toy-house, touching toy-house, playing
with toy-house, opening toy-house doors or win-
dows to uncover hidden toys, playing with these
hidden toys, spinning the toy-house, and putting
back or hiding a toy. All children in the play condi-
tion engaged in all seven actions, while children in
the no play condition engaged in none. An eighth
feature is the lack of explicit denial of touching or
playing with the toy house, which is considered
to be truthful in the play condition, and deceptive
in the no play condition (see the examples in the
appendix). A transcription is labeled as truth if
at least half of these features are truthful (53.2%
of all transcriptions) and lie otherwise (46.8% of
transcriptions). Other thresholds for this binary
discrimination are explored in section 5.4.
Each child?s verbal response was recorded
twice: at time T1 (prior to truth-induction), and
at time T2 (after truth-induction). Each child was
subject to one of the four confederate conditions
and one of the three induction conditions. The raw
data were pre-processed to remove subjects with
blank transcriptions, resulting in a total of 173 sub-
jects (87 boys and 86 girls) and 346 transcriptions.
4 Methods
Since the data consist of speech produced by 4- to
7-year-old children, the predictive features must
depend on the level of syntactic competence of
this age group. The ?continuity assumption? states
that children have a complete system of abstract
syntactic representation and have the same set of
abstract functional categories accessible to adults
(Pinker, 1984). An experimental study with 3-
to 8-year-old children showed that their syntac-
tic competence is comparable to that of adults;
specifically, children have a productive rule for
passive forms which allows them to generalize
to previously unheard predicates while following
adult-like constraints to avoid over-generalization
(Pinker et al, 1987). Recent experiments with
syntactic priming showed that children?s represen-
tations of abstract passive constructions are well-
developed as early as age 3 or 4, and young
946
children are generally able to form passive con-
structions with both action and non-action verbs
(Thatcher et al, 2007). These results suggest that
measures of syntactic complexity that are typically
used to evaluate adult language could be adapted
to child speech, provided that the children are at
least 3 or 4 years old.
Here, the complexity of speech is character-
ized by the length of utterances and by the fre-
quency of dependent and coordinate clauses, with
more complex speech consisting of longer utter-
ances and a higher number of subordinate clauses.
We segmented the transcriptions into sentences,
clauses and T-units, which are ?minimally ter-
minable units? consisting of a main clause and
its dependent clauses (Hunt, 1965; O?Donnell et
al., 1967)1. Deceptive communication generally
has shorter duration and is less detailed than non-
deceptive speech (DePaulo et al, 2003), so the
length of each type of segment was counted along
with frequency features over segments. Here, the
frequency of dependent and coordinate clauses per
constituent approximate clause-based measures of
complexity.
Our approach combines a set of features ob-
tained from a functional dependency grammar
(FDG) parser with another (non-overlapping) set
of features obtained from a phrase-based grammar
parser. We obtained FDG parses of the transcrip-
tions using Connexor?s Machinese Syntax parser
(Tapanainen and Ja?rvinen, 1997) and extracted the
following 5 features:
ARI Automated readability index. Measures
word and sentence difficulty, 4.71 cw +0.5ws ?
21.43, where c is the number of characters, w
is the number of words, and s is the number
of sentences (Smith and Senter, 1967).
ASL Average sentence length. The number of
words over the number of sentences.
COM Sentence complexity. The ratio of sen-
tences with ? 2 finite predicators to those
with ? 1 finite predicator (Stajner and
Mitkov, 2012).
PAS Passivity. The ratio of non-finite main
predicators in a passive construction (@?
1T-units include single clauses, two or more phrases in ap-
position, or clause fragments. Generally, coordinate clauses
are split into separate T-units, as are clauses interrupted by
discourse boundary markers.
FMAINV %VP) to the total number of fi-
nite (@+FMAINV %VA) and non-finite (@?
FMAINV %VA and @?FMAINV %VP)
main predicators, including active construc-
tions.
MCU Mean number of clauses per utterance.
Additionally, we searched for specific syntactic
patterns in phrase-based parses of the data. We
used the Stanford probabilistic natural language
parser (Klein and Manning, 2003) for construct-
ing these parse trees, the Stanford Tregex utility
(Levy and Andrew, 2006) for searching the con-
structed parse trees, and a tool provided by Lu
(2011) which extracts a set of 14 clause-based fea-
tures in relation to sentence, clause and T-unit con-
stituents.
4.1 Feature analysis
Analysis of variance (ANOVA) was performed on
the set of 17 features, shown in Table 1. A one-
factor ANOVA across the truth and lie groups
showed three significant feature variations: aver-
age sentence length (ASL), sentence complexity
(COM), and mean clauses per utterance (MCU).
Dependencies between some feature pairs that are
positively correlated are shown in Figure 1.
As expected, the number of clauses (MCU) is
dependent on sentence length (ASL) (r(344) =
.92, p < .001). Also, the number of T-units is de-
pendent on the number of clauses: CN/C is corre-
lated with CN/T (r(344) = .89, p < .001), CP/C
is correlated with CP/T (r(344) = .85, p < .001),
and DC/C is correlated with DC/T (r(344) = .92,
p < .001). Other features are completely un-
correlated. For example, the number of passive
constructions is independent of sentence length
(r(344) = ?.0020, p > .05), the number of com-
plex nominals per clause is independent of clause
length (r(344) = .076, p > .05), and the density
of dependent clauses is independent of the density
of coordinate phrases (r(344) = ?.027, p > .05).
5 Results
We evaluate five classifiers: logistic regres-
sion (LR), a multilayer perceptron (MLP), na??ve
Bayes (NB), a random forest (RF), and a support
vector machine (SVM). Here, na??ve Bayes, which
assumes conditional independence of the features,
and logistic regression, which has a linear deci-
sion boundary, are baselines. The MLP includes a
variable number of layers of hidden units, which
947
Figure 1: Independent and dependent feature pairs; data points are labeled as truth (blue) and lie (green).
Feature F1,344 d
Automated Readability Index (ARI) 0.187 0.047
Average Sentence Length (ASL) 3.870 0.213
Sentence Complexity (COM) 10.93 0.357
Passive Sentences (PAS) 1.468 0.131
Mean Clauses per Utterance (MCU) 6.703 0.280
Mean Length of T-Unit (MLT) 2.286 0.163
Mean Length of Clause (MLC) 0.044 -0.023
Verb Phrases per T-Unit (VP/T) 3.391 0.199
Clauses per T-Unit (C/T) 2.345 0.166
Dependent Clauses per Clause (DC/C) 1.207 0.119
Dependent Clauses per T-Unit (DC/T) 1.221 0.119
T-Units per Sentence (T/S) 3.692 0.208
Complex T-Unit Ratio (CT/T) 2.103 0.157
Coordinate Phrases per T-Unit (CP/T) 0.463 -0.074
Coordinate Phrases per Clause (CP/C) 0.618 -0.085
Complex Nominals per T-Unit (CN/T) 0.722 0.092
Complex Nominals per Clause (CN/C) 0.087 0.032
Table 1: One-factor ANOVA (F statistics and Co-
hen?s d-values, ? = 0.05) on all features across
truth and lie groups. Statistically significant re-
sults are in bold.
apply non-linear activation functions on a linear
combination of inputs. The SVM is a paramet-
ric binary classifier that provides highly non-linear
decision boundaries given particular kernels. The
random forest is an ensemble classifier that returns
the mode of the class predictions of several deci-
sion trees.
5.1 Binary classification across all data
The five classifiers were evaluated on the entire
pooled data set with 10-fold cross validation. Ta-
ble 2 lists the parameters varied for each classi-
fier, and Table 3 shows the cross-validation accu-
racy for the classifiers with the best parameter set-
tings. The na??ve Bayes classifier performs poorly,
as could be expected given the assumption of con-
ditional feature independence. The SVM classifier
performs best, with 59.5% cross-validation accu-
racy, which is a statistically significant improve-
ment over the baselines of LR (t(4) = 22.25, p <
.0001), and NB (t(4) = 16.19, p < .0001).
Parameter Values
LR R Ridge value 10?10 to 10?2
ML
P
L Learning rate 0.0003 to 0.3
M Momentum 0 to 0.5
H Number of hidden
layers
1 to 5
NB K Use kernelestimator
true, false
RF
I Number of trees 1 to 20
K Maximum depth unlimited, 1 to 10
SV
M
K Kernel Linear, RBF,
Polynomial
E Polynomial
Exponent
2 to 5
G RBF Gamma 0.001 to 0.1
C Complexity
constant
0.1 to 10
Table 2: Empirical parameter settings for each
classifier
5.2 Binary classification by age group
Significant variation in syntactic complexity is ex-
pected across ages. To account for such variation,
we segmented the dataset in four groups: 44 tran-
948
Accuracy Parameters
LR 0.5347 R = 10?10
MLP 0.5838 L = 0.003, M = 0.4
NB 0.5173 K = false
RF 0.5809 I = 10, K = 6
SVM 0.5954 Polynomial, E = 3, C = 1
Table 3: Cross-validation accuracy of binary clas-
sification performed on entire dataset of 346 tran-
scriptions.
scriptions of 4-year-olds, 120 of 5-year-olds, 94 of
6-year-olds, and 88 of 7-year-olds. By compari-
son, Vrij et al (2004) used data from only 35 chil-
dren in their study of 5- and 6-year-olds. Classi-
fication of truthfulness was performed separately
for each age, as shown in Table 4. In compar-
ison with classification accuracy on pooled data,
a paired t-test shows statistically significant im-
provement across all age groups using RF, t(3) =
10.37, p < .005.
Age (years)
4 5 6 7
LR 0.6136 0.5333 0.5957* 0.4886
MLP 0.6136? 0.5583 0.6170? 0.5909*
NB 0.6136* 0.5250 0.5426 0.5682
RF 0.6364? 0.6333* 0.6383? 0.6591?
SVM 0.6591 0.5583 0.6064 0.6250*
Table 4: Cross-validation accuracy of binary clas-
sification partitioned by age. The best classifier at
each age is shown in bold. The classifiers showing
statistically significant incremental improvement
are marked: *p < .05, ?p < .001 (paired t-test,
d.f. 4)
5.3 Binary classification by age group, on
verbose transcriptions
The length of speech, in number of words, varies
widely (min = 1, max = 167, ? = 36.83,
? = 28.34) as a result of the unregulated nature
of the interview interaction. To test the effect of
verbosity, we segment the data by child age and
select only the transcriptions with above-average
word counts (i.e., ? 37 words), resulting in four
groups: 12 transcriptions of 4-year-olds, 48 of 5-
year-olds, 39 of 6-year-olds, and 37 of 7-year-olds.
This mimics the scenario in which some mini-
mum threshold is placed on the length of a child?s
speech. In this verbose case, 63.3% of transcripts
are labeled truth across age groups (using the same
definition of truth as in section 3), with no sub-
stantial variation between ages; in the non-verbose
case, 53.2% are marked truth. Fisher?s exact test
on this contingency table reveals no significant dif-
ference between these distributions (p = 0.50).
Classification results are shown in Table 5. The
size of the training set for the youngest age cat-
egory is low compared to the other age groups,
which may reduce the reliability of the higher ac-
curacy achieved in that group. The other three age
groups show a growing trend, which is consistent
with expectations ? older children exhibit greater
syntactic complexity in speech, allowing greater
variability of feature values across truth and de-
ception. Here, both SVM and RF achieve 83.8%
cross-validation accuracy in identifying deception
in the speech of 7-year-old subjects.
4 5 6 7
LR 0.7500? 0.5417 0.6667? 0.7297?
MLP 0.8333? 0.6250? 0.6154 0.7838?
NB 0.6667? 0.4583 0.4103 0.7297*
RF 0.8333? 0.5625 0.7179? 0.8378?
SVM 0.9167* 0.6250? 0.6154* 0.8378?
Table 5: Cross-validation accuracy of binary clas-
sification performed on transcriptions with above
average word count (136 transcriptions), by age
group. Rows represent classifiers, columns repre-
sent ages. The best classifier for each age is in
bold. The classifiers showing statistically signifi-
cant incremental improvement are marked: *p <
.05, ?p < .001 (paired t-test, d.f. 4)
5.4 Threshold variation
To study the effect of the threshold between the
truth and lie classes, we vary the value of the
threshold, ? , from 1 to 8, requiring the admission
of at least ? truthful details (out of 8 possible de-
tails) in order to label a transcription as truth. The
effect of ? on classification accuracy over the en-
tire pooled dataset for each of the 5 classifiers is
shown in Figure 2. A one-factor ANOVA with
? as the independent variable with 8 levels, and
cross-validation accuracy as the dependent vari-
able, confirms that the effect of the threshold is sta-
tistically significant (F7,40 = 220.69, p < .0001)
with ? = 4 being the most conservative setting.
949
Figure 2: Effect of threshold and classifier choice
on cross-validation accuracy. Threshold ? = 0 is
not present, since all data would be labeled truth.
5.5 Linguistic Inquiry and Word Count
The Linguistic Inquiry and Word Count (LIWC)
tool for generating features based on word cate-
gory frequencies has been used in deception de-
tection with adults, specifically: first-person sin-
gular pronouns (FP), exclusive words (EW), nega-
tive emotion words (NW), and motion verbs (MV)
(Newman et al, 2003). We compare the perfor-
mance of classifiers trained with our 17 syntactic
features to those of classifiers trained with those
LIWC-based features on the same data. To evalu-
ate the four LIWC categories, we use the 86 words
of the Pennebaker model (Little and Skillicorn,
2008; Vartapetiance and Gillam, 2012). The per-
formance of the classifiers trained with LIWC fea-
tures is shown in Table 6.
The set of 17 syntactic features proposed here
result in significantly higher accuracies across
classifiers and experiments (? = 0.63, ? = 0.10)
than with the LIWC features used in previous
work (? = 0.58, ? = 0.09), as shown in Figure 3
(t(53) = ?0.0691, p < .0001).
6 Discussion and future work
This paper evaluates automatic estimation of truth-
fulness in the utterances of children using a novel
set of lexical-syntactic features across five types
of classifiers. While previous studies have favored
word category frequencies extracted with LIWC
(Newman et al, 2003; Little and Skillicorn, 2008;
Hauch et al, 2012; Vartapetiance and Gillam,
Figure 3: Effect of feature set choice on cross-
validation accuracy.
2012; Almela et al, 2012; Fornaciari and Poesio,
2012), our results suggest that the set of syntac-
tic features presented here perform significantly
better than the LIWC feature set on our data, and
across seven out of the eight experiments based on
age groups and verbosity of transcriptions.
Statistical analyses showed that the average sen-
tence length (ASL), the Stajner-Mitkov measure
of sentence complexity (COM), and the mean
number of clauses per utterance (MCU) are the
features most predictive of truth and deception
(see section 4.1). Further preliminary experi-
ments are exploring two methods of feature se-
lection, namely forward selection and minimum-
Redundancy-Maximum-Relevance (mRMR). In
forward selection, features are greedily added one-
at-a-time (given an initially empty feature set) un-
til the cross-validation error stops decreasing with
the addition of new features (Deng, 1998). This
results in a set of only two features: sentence
complexity (COM) and T-units per sentence (T/S).
Features are selected in mRMR by minimizing
redundancy (i.e., the average mutual information
between features) and maximizing the relevance
(i.e., the mutual information between the given
features and the class) (Peng et al, 2005). This
approach selects five features: verb phrases per T-
unit (VP/T), passive sentences (PAS), coordinate
phrases per clause (CP/C), sentence complexity
(COM), and complex nominals per clause (CN/C).
These results confirm the predictive strength of
sentence complexity. Further, preliminary classi-
950
Group Accuracy Best Classifier Parameters
Entire dataset 0.5578 RF I = 20, K = unlimited
4-yr-olds 0.5682 MLP L = 0.005, M = 0.3, H = 1
5-yr-olds 0.5583 RF I = 5, K = unlimited
6-yr-olds 0.5319 MLP L = 0.005, M = 0.3, H = 1
7-yr-olds 0.6591 RF I = 5, K = unlimited
4-yr-olds, verbose 0.8333 SVM PolyKernel, E = 4, C = 10
5-yr-olds, verbose 0.7083 SVM NormalizedPolyKernel, E = 1, C = 10
6-yr-olds, verbose 0.6154 MLP L = 0.09, M = 0.2, H = 1
7-yr-olds, verbose 0.7027 MLP L = 0.01, M = 0.5, H = 3
Table 6: Best 10-fold cross-validation accuracies achieved on various subsets of the data, using the
LIWC-based feature set.
fication results across all classifiers suggest that
accuracies are significantly higher given forward
selection (? = 0.58, ? = 0.02) relative to the
original feature set (? = 0.56, ? = 0.03); t(5) =
?2.28, p < .05 while the results given the mRMR
features are not significantly different.
Generalized cross-validation accuracy increases
significantly given partitioned age groups, which
suggests that the importance of features may be
moderated by age. A further incremental in-
crease is achieved by considering only transcrip-
tions above a minimum length. O?Donnell et
al. (1967) examined syntactic complexity in the
speech and writing of children aged 8 to 12, and
found that speech complexity increases with age.
This phenomenon appears to be manifested in the
current study by the extent to which classification
increases generally across the 5-, 6-, and 7-year-
old groups, as shown in Table 5. Future examina-
tion of the effect of age on feature saliency may
yield more appropriate age-dependent features.
While past research has used logistic regression
as a binary classifier (Newman et al, 2003), our
experiments show that the best-performing classi-
fiers allow for highly non-linear class boundaries;
SVM and RF models achieve between 62.5% and
91.7% accuracy across age groups ? a significant
improvement over the baselines of LR and NB,
as well as over previous results. Moreover, since
the performance of human judges in identifying
deception is not significantly better than chance
(Lewis et al, 1989; Newman et al, 2003), these
results show promise in the use of automatic de-
tection methods.
Partially truthful transcriptions were scored us-
ing a gradient of 0 to 8 truthful details, and a
threshold ? was used to perform binary classifica-
tion. Extreme values of ? lead to poor F-scores de-
spite high accuracy, since the class distribution of
transcriptions is very skewed towards either class.
Future work can explore the effect of threshold
variation given sufficient data with even class dis-
tributions for each threshold setting. When such
data is unavailable, experiments can make use of
the most conservative setting (? = 4, or an equiv-
alent mid-way setting) for analysis of real-life ut-
terances containing partial truths.
Future work should consider measures of con-
fidence for each classification, where possible, so
that more ambiguous classifications are not treated
on-par with more certain ones. For instance, con-
fidence can be approximated in MLPs by the en-
tropy across continuous-valued output nodes, and
in RFs by the number of component decision trees
that agree on a classification. Although acoustic
data were not provided with this data set (Lyon
and Dorado, 2008) (and, in practice, cannot be as-
sured), future work should also examine the dif-
ferences in the acoustics of children across truth
conditions.
Acknowledgments
The authors thank Kang Lee (Ontario Institute for
Studies in Education, University of Toronto) and
Angela Evans (Brock University) for sharing both
this data set and their insight.
951
Appendix
The following is an example of evasive deceptive
speech from a 6-year-old after no truth induction
(i.e., the control condition in which the interviewer
merely states that he needs to ask more questions):
... Yeah yeah ok, I?m a tell you. We
played that same game and I won and
he won. I?m going to be in trouble if I
tell you. It a secret. It?s a secret ?cuz
we?re friends. ...
Transcription excerpt labeled as truth by a
threshold of ? = 1: 7-year-old child?s response
(play, no coach condition), in which the child does
not explicitly deny playing with the toy house, and
admits to looking at it but does not confess to any
of the other six actions:
...I was playing, I was hiding the coin
and I was trying to find the house... try-
ing to see who was in there...
Transcription excerpt labeled as truth by a
threshold of ? = 4: 7-year-old child?s response
(play, no coach condition), in which the child does
not explicitly deny playing, and admits to three ac-
tions:
...me and him was playing with it... we
were just spinning it around and got the
toys out...
References
Angela Almela, Rafael Valencia-Garcia, and Pascual
Cantos. 2012. Seeing through deception: A compu-
tational approach to deceit detection in written com-
munication. Proceedings of the EACL 2012 Work-
shop on Computational Approaches to Deception
Detection, April 23-27, 2012, Avignon, France, 15-
22.
Ethem Alpaydin. 2010. Introduction to Machine
Learning. Cambridge, MA: MIT Press.
Kan Deng. 1998. OMEGA: On-line memory-based
general purpose system classifier. Doctoral thesis,
School of Computer Science, Carnegie Mellon Uni-
versity
Bella M. DePaulo, James J. Lindsay, Brian E. Mal-
one, Laura Muhlenbruck, Kelly Charlton, and Har-
ris Cooper. 2003. Cues to deception. Psychological
Bulletin, 129(1):74-118.
Song Feng, Ritwik Banerjee and Yejin Choi. 2012.
Syntactic stylometry for deception detection. Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, July 8-14, 2012,
Jeju, Republic of Korea, 171-175.
Tommaso Fornaciari and Massimo Poesio. 2012. On
the use of homogeneous sets of subjects in deceptive
language analysis. Proceedings of the EACL 2012
Workshop on Computational Approaches to Decep-
tion Detection, April 23-27, 2012, Avignon, France,
39-47.
Martin Graciarena, Elizabeth Shriberg, Andreas Stol-
cke, Frank Enos, Julia Hirschberg, Sachin Kajarekar.
2006. Combining prosodic lexical and cepstral sys-
tems for deceptive speech detection. Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pages I-
1033-I-1036.
Valerie Hauch, Jaume Masip, Iris Blandon-Gitlin, and
Siegfried L. Sporer. 2012. Linguistic cues to de-
ception assessed by computer programs: A meta-
analysis. Proceedings of the EACL Workshop on
Computational Approaches to Deception Detection,
pages 1-4.
Julia Hirschberg, Stefan Benus, Jason M. Brenier,
Frank Enos, Sarah Friedman, Sarah Gilman, Cynthia
Girand, Martin Graciarena, Andreas Kathol, Laura
Michaelis, Bryan Pellom, Elizabeth Shriberg, and
Andreas Stolcke. 2005. Distinguishing deceptive
from non-deceptive speech. Proceedings of Eu-
rospeech 2005, pages 1833-1836.
Kellogg W. Hunt. 1965. Grammatical structures writ-
ten at three grade levels. NCTE Research Report No.
3.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 423-430.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina
Jokel. 2011. Longitudinal detection of dementia
through lexical and syntactic changes in writing: a
case study of three British novelists. Literary and
Linguistic Computing, 26(4):435-461.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. 5th International Conference on
Language Resources and Evaluation.
Michael Lewis, Catherine Stanger, and Margaret W.
Sullivan. 1989. Deception in 3-year-olds. Devel-
opmental Psychology, 25(3):439-443.
A. Little and D. B. Skillicorn. 2008. Detecting de-
ception in testimony. Proceedings of IEEE Interna-
tional Conference of Intelligence and Security Infor-
matics (ISI 2008), June 17-20, 2008, Taipei, Taiwan,
13-18.
952
Xiaofei Lu. 2011. Automatic analysis of syntactic
complexity in second language writing. Interna-
tional Journal of Corpus Linguistics, 15(4):474-496.
Thomas D. Lyon and J. S. Dorado. 2008. Truth in-
duction in young maltreated children: the effects of
oath-taking and reassurance on true and false disclo-
sures. Child Abuse & Neglect, 32(7):738-748.
Thomas D. Lyon, Lindsay C. Malloy, Jodi A. Quas,
and Victoria A. Talwar. 2008. Coaching, truth in-
duction, and young maltreated children?s false al-
legations and false denials. Child Development,
79(4):914-929.
Franc?ois Mairesse, Marilyn A. Walker, Matthias R.
Mehl, and Roger K. Moore. 2007. Using linguis-
tic cues for the automatic recognition of personality
in conversation and text. Journal of Artificial Intel-
ligence Research, 30:457-500.
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: explorations in the automatic recognition
of deceptive language. Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, August 4,
2009, Suntec, Singapore, 309-312.
Matthew L. Newman, James W. Pennebaker, Diane S.
Berry, and Jane M. Richards. 2003. Lying words:
predicting deception from linguistic styles. PSPB,
29(5):665-675.
Roy C. O?Donnell, William J. Griffin, and Raymond
C. Norris. 1967. A transformational analysis of oral
and written grammatical structures in the language
of children in grades three, five, and seven. PSPB,
29(5):665-675.
Steven Pinker. 1984. Language learnability and lan-
guage development, Cambridge, MA: Harvard Uni-
versity Press.
Steven Pinker, David S. Lebeaux, and Loren Ann Frost.
1987. Productivity and constraints in the acquisition
of the passive. Cognition, 26:195-267.
Hanchuan Peng, Fuhui Long, and Chris Ding. 2005.
Feature selection based on mutual information: cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(8):1226-1238.
J. M. Richards and J. J. Gross. 1999. Composure at
any cost? The cognitive consequences of emotion
suppression. PSPB, 25(8):1033-1044.
J. M. Richards and J. J. Gross. 2000. Emotion regu-
lation and memory: the cognitive costs of keeping
one?s cool. Journal of Personality and Social Psy-
chology, 79:410-424.
E. A. Smith and R. J. Senter. 1967. Automated read-
ability index. Technical report, Defense Technical
Information Center. United States.
Sanja Stajner and Ruslan Mitkov. 2012. Diachronic
changes in text complexity in 20th century English
language: an NLP Approach. Proceedings of the In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 1577-1584.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. Proceedings of the
5th Conference on Applied Natural Language Pro-
cessing, pages 64-71.
Katherine Thatcher, Holly Branigan, Janet McLean,
and Antonella Sorace. 2007. Children?s early acqui-
sition of the passive: evidence from syntactic prim-
ing. Child Language Seminar, University of Read-
ing.
Udo Undeutsch. 2008. Courtroom evaluation of eye-
witness testimony. Applied Psychology, 33(1):51-
66.
Anna Vartapetiance and Lee Gillam. 2012. ?I don?t
know where he is not?: does deception research yet
offer a basis for deception detectives? Proceedings
of the EACL 2012 Workshop on Computational Ap-
proaches to Deception Detection, April 23-27, 2012,
Avignon, France, 5-14.
Aldert Vrij, Lucy Akehurst, Stavroula Soukara, and
Ray Bull. 2004. Detecting deceit via analyses
of verbal and nonverbal behavior in children and
adults. Human Communication Research, 30(1):8?
41
Yi Zheng, Qifeng Dai, Qiming Luo, and Enhong Chen.
2010. Hedge classification with syntactic depen-
dency features based on an ensemble classifier. Pro-
ceedings of the Fourteenth Conference on Compu-
tational Natural Language Learning, July 15-16,
2010, Uppsala, Sweden, 151-156.
953
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 80?88,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards a noisy-channel model of dysarthria in speech recognition
Frank Rudzicz
University of Toronto, Department of Computer Science
Toronto, Ontario, Canada
frank@cs.toronto.edu
Abstract
Modern automatic speech recognition is inef-
fective at understanding relatively unintelligi-
ble speech caused by neuro-motor disabilities
collectively called dysarthria. Since dysarthria
is primarily an articulatory phenomenon, we
are collecting a database of vocal tract mea-
surements during speech of individuals with
cerebral palsy. In this paper, we demonstrate
that articulatory knowledge can remove am-
biguities in the acoustics of dysarthric speak-
ers by reducing entropy relatively by 18.3%,
on average. Furthermore, we demonstrate
that dysarthric speech is more precisely por-
trayed as a noisy-channel distortion of an
abstract representation of articulatory goals,
rather than as a distortion of non-dysarthric
speech. We discuss what implications these
results have for our ongoing development of
speech systems for dysarthric speakers.
1 Introduction
Dysarthria is a set of congenital and traumatic
neuro-motor disorders that impair the physical pro-
duction of speech and affects approximately 0.8% of
individuals in North America (Hosom et al, 2003).
Causes of dysarthria include cerebral palsy (CP),
multiple sclerosis, Parkinson?s disease, and amy-
otrophic lateral sclerosis (ALS). These impairments
reduce or remove normal control of the primary vo-
cal articulators but do not affect the abstract produc-
tion of meaningful, syntactically correct language.
The neurological origins of dysarthria involve
damage to the cranial nerves that control the speech
articulators (Moore and Dalley, 2005). Spastic
dysarthria, for instance, is partially caused by le-
sions in the facial and hypoglossal nerves, which
control the jaw and tongue respectively (Duffy,
2005), resulting in slurred speech and a less differ-
entiable vowel space (Kent and Rosen, 2004). Sim-
ilarly, damage to the glossopharyngeal nerve can re-
duce control over vocal fold vibration (i.e., phona-
tion), resulting in guttural or grating raspiness. In-
adequate control of the soft palate caused by disrup-
tion of the vagus nerve may lead to a disproportion-
ate amount of air released through the nose during
speech (i.e., hypernasality).
Unfortunately, traditional automatic speech
recognition (ASR) is incompatible with dysarthric
speech, often rendering such software inaccessible
to those whose neuro-motor disabilities might make
other forms of interaction (e.g., keyboards, touch
screens) laborious. Traditional representations in
ASR such as hidden Markov models (HMMs)
trained for speaker independence that achieve
84.8% word-level accuracy for non-dysarthric
speakers might achieve less than 4.5% accuracy
given severely dysarthric speech on short sentences
(Rudzicz, 2007). Our research group is currently
developing new ASR models that incorporate em-
pirical knowledge of dysarthric articulation for use
in assistive applications (Rudzicz, 2009). Although
these models have increased accuracy, the disparity
is still high. Our aim is to understand why ASR
fails for dysarthric speakers by understanding the
acoustic and articulatory nature of their speech.
In this paper, we cast the speech-motor interface
within the mathematical framework of the noisy-
channel model. This is motivated by the charac-
80
terization of dysarthria as a distortion of parallel
biological pathways that corrupt motor signals be-
fore execution (Kent and Rosen, 2004; Freund et
al., 2005), as in the examples cited above. Within
this information-theoretic framework, we aim to in-
fer the nature of the motor signal distortions given
appropriate measurements of the vocal tract. That is,
we ask the following question: Is dysarthric speech
a distortion of typical speech, or are they both distor-
tions of some common underlying representation?
2 Dysarthric articulation data
Since the underlying articulatory dynamics of
dysarthric speech are intrinsically responsible for
complex acoustic irregularities, we are collecting
a database of dysarthric articulation. Time-aligned
movement and acoustic data are measured using
two systems. The first infers 3D positions of sur-
face facial markers given stereo video images. The
second uses electromagnetic articulography (EMA),
in which the speaker is placed within a cube that
produces a low-amplitude electromagnetic field, as
shown in figure 1. Tiny sensors within this field al-
low the inference of articulator positions and veloci-
ties to within 1 mm of error (Yunusova et al, 2009).
Figure 1: Electromagnetic articulograph system.
We have so far recorded one male speaker with
ALS, five male speakers with CP, four female
speakers with CP, and age- and gender-matched
controls. Measurement coils are placed as in
other studies (e.g., the University of Edinburgh?s
MOCHA database (Wrench, 1999) and the Uni-
versity of Wisconsin-Madison?s x-ray microbeam
database (Yunusova et al, 2008)). Specifically, we
are interested in the positions of the upper and lower
lip (UL and LL), left and right mouth corners (LM
and RM), lower incisor (LI), and tongue tip, blade,
and dorsum (TT, TB, and TD). Unfortunately, a few
of our male CP subjects had a severe gag reflex, and
we found it impossible to place more than one coil
on the tongue for these few individuals. Therefore,
of the tongue positions, only TT is used in this study.
All articulatory data are smoothed with third-order
median filtering in order to minimize measurement
?jitter?. Figure 2 shows the degree of lip aperture
(i.e., the distance between UL and LL) over time for
a control and a dysarthric speaker repeating the se-
quence /ah p iy/. Here, the dysarthric speech is no-
tably slower and has more excessive movement.
0 1 2 3 4 5 61
2
3
4
5
Time (s)
Lip a
pertu
re (cm
)
 
 ControlDysarthric
Figure 2: Lip aperture over time for four iterations of /ah
p iy/ given a dysarthric and control speaker.
Our dysarthric speech data include random repeti-
tions of phonetically balanced short sentences origi-
nally used in the TIMIT database (Zue et al, 1989),
as well as pairs of monosyllabic words identified
by Kent et al (1989) as having relevant articula-
tory contrasts (e.g., beat versus meat as a stop-
nasal contrast). All articulatory data are aligned
with associated acoustic data, which are transformed
to Mel-frequency cepstral coefficients (MFCCs).
Phoneme boundaries and pronunciation errors are
being transcribed by a speech-language pathologist
to the TIMIT phoneset. Table 1 shows pronuncia-
tion errors according to manner of articulation for
dysarthric speech. Plosives are mispronounced most
often, with substitution errors exclusively caused by
errant voicing (e.g. /d/ for /t/). By comparison, only
81
5% of corresponding plosives in total are mispro-
nounced in regular speech. Furthermore, the preva-
lence of deleted affricates in word-final positions, al-
most all of which are alveolar, does not occur in the
corresponding control data.
SUB (%) DEL (%)
i m f i m f
plosives 13.8 18.7 7.1 1.9 1.0 12.1
affricates 0.0 8.3 0.0 0.0 0.0 23.2
fricatives 8.5 3.1 5.3 22.0 5.5 13.2
nasals 0.0 0.0 1.5 0.0 0.0 1.5
glides 0.0 0.7 0.4 11.4 2.5 0.9
vowels 0.9 0.9 0.0 0.0 0.2 0.0
Table 1: Percentage of phoneme substitution (SUB) and
deletion (DEL) errors in word-initial (i), word-medial
(m), and word-final (f) positions across categories of
manner for dysarthric data.
Table 2 shows the relative durations of the five
most common vowels and sonorant consonants in
our database between dysarthric and control speech.
Here, dysarthric speakers are significantly slower
than their control counterparts at the 95% confidence
interval for /eh/ and at the 99.5% confidence interval
for all other phonemes.
Phoneme
duration (? (?2), in ms) Avg.
Dysarthric Control diff.
/ah/ 189.3 (19.2) 120.1 (4.0) 69.2
/ae/ 211.6 (16.4) 140.0 (4.4) 71.6
/eh/ 160.5 (7.4) 107.3 (2.6) 53.2
/iy/ 177.1 (86.7) 105.8 (93.1) 71.3
/er/ 220.5 (27.9) 148.6 (59.8) 71.9
/l/ 138.5 (8.0) 91.8 (2.4) 46.7
/m/ 173.5 (13.4) 94.7 (2.1) 78.8
/n/ 168.4 (14.4) 90.9 (2.3) 77.5
/r/ 138.8 (8.3) 95.3 (3.4) 43.5
/w/ 151.5 (12.0) 84.5 (1.3) 67.0
Table 2: Average lengths (and variances in parentheses)
in milliseconds for the five most common vowels and
sonorant consonants for dysarthric and control speakers.
The last column is the average difference in milliseconds
between dysarthric and control subjects.
Processing and annotation of further data from
additional dysarthric speakers is ongoing, including
measurements of all three tongue positions.
3 Entropy and the noisy-channel model
We wish to measure the degree of statistical disorder
in both acoustic and articulatory data for dysarthric
and non-dysarthric speakers, as well as the a posteri-
ori disorder of one type of data given the other. This
quantification will inform us as to the relative mer-
its of incorporating knowledge of articulatory be-
haviour into ASR systems for dysarthric speakers.
Entropy, H(X), is a measure of the degree of uncer-
tainty in a random variable X . When X is discrete,
this value is computed with the familiar
H(X) =?
n
?
i=1
p(xi) logb p(xi),
where b is the logarithm base, xi is a value of X ,
of which there are n possible, and p(xi) is its prob-
ability. When our observations are continuous, as
they are in our acoustic and articulatory database,
we must use differential entropy defined by
H(X) =?
?
X
f (X) log f (X)dX ,
where f (X) is the probability density function of X .
For a number of distributions f (X), the differential
entropy has known forms (Lazo and Rathie, 1978).
For example, if f (X) is a multivariate normal,
fX(x1, ...,xN) =
exp
(
?12(x??)
T??1(x??)
)
(2pi)N/2 |?|1/2
H(X) = 12 ln
(
(2pie)N |?|
)
,
(1)
where ? and ? are the mean and covariances of the
data. However, since we observe that both acous-
tic and articulatory data follow non-Gaussian dis-
tributions, we choose to represent these spaces by
mixtures of Gaussians. Huber et al (2008) have de-
veloped an accurate algorithm for estimating differ-
ential entropy of Gaussian mixtures based on itera-
tively merging Gaussians and the approximation
H?(X) =
L
?
i=1
?i
(
? log?i + 12 log((2pie)
N |?i|
)
,
where ?i is the weight of the ith(1? i? L) Gaussian
and ?i is that Gaussian?s covariance matrix. This
method is used to approximate entropies in the fol-
lowing study, with L = 32. Note that while differen-
tial entropies can be negative and not invariant under
82
change of variables, other properties of entropy are
retained (Huber et al, 2008), such as the chain rule
for conditional entropy
H(Y |X) = H(Y,X)?H(X),
which describes the uncertainty in Y given knowl-
edge of X , and the chain rule for mutual information
I(Y ;X) = H(X)+H(Y )?H(X ,Y ),
which describes the mutual dependence between X
and Y . Here, we quantize entropy with the nat,
which is the natural logarithmic unit, e (? 1.44 bits).
3.1 The noisy channel
The noisy-channel theorem states that information
passed through a channel with capacity C at a rate
R ? C can be reliably recovered with an arbitrarily
low probability of error given an appropriate coding.
Here, a message from a finite alphabet is encoded,
producing signal x ? X . That signal is then distorted
by a medium which transmits signal y ? Y accord-
ing to some distribution P(Y |X). Given that there is
some probability that the received signal, y, is cor-
rupted, the message produced by the decoder may
differ from the original (Shannon, 1949).
To what extent can we describe the effects of
dysarthria within an information-theoretic noisy
channel model? We pursue two competing hypothe-
ses within this general framework. The first hypoth-
esis models the assumption that dysarthric speech is
a distorted version of typical speech. Here, signal
X and Y represent the vocal characteristics of the
general and dysarthric populations, respectively, and
P(Y |X) models the distortion between them. The
second hypothesis models the assumption that both
dysarthric and typical speech are distorted versions
of some common abstraction. Here, Yd and Yc repre-
sent the vocal characteristics of dysarthric and con-
trol speakers, respectively, and X represents a com-
mon, underlying mechanism and that P(Yd |X) and
P(Yc |X) model distortions from that mechanism.
These two hypotheses are visualized in figure 3. In
each of these cases, signals can be acoustic, articu-
latory, or some combination thereof.
3.2 Common underlying abstractions
In order to test our hypothesis that both dysarthric
and control speakers share a common high-level ab-
P(Y | X) Dysarthric speechsignal, YTypical speechsignal, X
(a) Dysarthric speech as a distortion of control speechP(Y |X|? ) Dysarthric speechsignal, Y T?tract speechsignal, ? P(Y? |X|? ) ??tr? speechsignal, Y?
(b) Dysarthric and control speech as distortions of a common
abstraction
Figure 3: Sections of noisy channel models that mimic
the neuro-motor interface.
straction of the vocal tract that is in both cases dis-
torted during articulation, we incorporate the the-
ory of task dynamics (Saltzman and Munhall, 1989).
This theory represents the interface between the lex-
ical intentions and vocal tract realizations of speech
as a sequence of overlapping gestures, which are
continuous dynamical systems that describe goal-
oriented reconfigurations of the vocal tract, such as
bilabial closure during /m/. Figure 4 shows an ex-
ample of overlapping gestures for the word pub.
TBCD closedopen
GLO openclosed
LA openclosed
100 200 300 400Time (ms)
Figure 4: Canonical example pub from Saltzman
and Munhall (1989) representing overlapping goals for
tongue blade constriction degree (TBCD), lip aperture
(LA), and glottis (GLO). Boxes represent the present of
discretized goals, such as lip closure. Black curves repre-
sent the output of the TADA system.
The open-source TADA system (Nam and Gold-
stein, 2006) estimates the positions of various artic-
ulators during speech according to parameters that
have been carefully tuned by the authors of TADA
according to a generic, speaker-independent repre-
sentation of the vocal tract (Saltzman and Munhall,
1989). Given a word sequence and a syllable-to-
gesture dictionary, TADA produces the continuous
83
tract variable paths that are necessary to produce that
sequence. This takes into account various physio-
logical aspects of human speech production, such as
interarticulator co-ordination and timing (Nam and
Saltzman, 2003).
In this study, we use TADA to produce estimates
of a global, high-level representation of speech com-
mon to both dysarthric and non-dysarthric speakers
alike. Given a word sequence uttered by both types
of speaker, we produce five continuous curves pre-
scribed by that word sequence in order to match our
available EMA data. Those curves are lip aperture
and protrusion (LA and LP), tongue tip constriction
location and degree (TTCL and TTCD, representing
front-back and top-down positions of the tongue tip,
respectively), and lower incisor height (LIH). These
curves are then compared against actually observed
EMA data, as described below.
4 Experiments
First, in section 4.1, we ask whether the incorpo-
ration of articulatory data is theoretically useful in
reducing uncertainty in dysarthric speech. Second,
in section 4.2, we ask which of the two noisy chan-
nel models in figure 3 best describe the observed be-
haviour of dysarthric speech.
Data for this study are collected as described as in
section 2. Here, we use data from three dysarthric
speakers with cerebral palsy (males M01 and M04,
and female F03), as well as their age- and gender-
matched counterparts from the general population
(males MC01 and MC03, and female FC02). For
this study we restrict our analysis to 100 phrases ut-
tered in common by all six speakers.
4.1 Entropy
We measure the differential entropy of acoustics
(H(Ac)), of articulation (H(Ar)), and of acoustics
given knowledge of the vocal tract (H(Ac |Ar)) in
order to obtain theoretical estimates as to the utility
of articulatory data. Table 3 shows these quantities
across the six speakers in this study. As expected,
the acoustics of dysarthric speakers are much more
disordered than for non-dysarthric speakers. One
unexpected finding is that there is very little differ-
ence between speakers in terms of their entropy of
articulation. Although dysarthric speakers clearly
lack articulatory dexterity, this implies that they
nonetheless articulate with a level of consistency
similar to their non-dysarthric counterparts1. How-
ever, the equivocation H(Ac |Ar) is an order of mag-
nitude lower for non-dysarthric speakers. This im-
plies that there is very little ambiguity left in the
acoustics of non-dysarthric speakers if we have si-
multaneous knowledge of the vocal tract, but that
quite a bit of ambiguity remains for our dysarthric
speakers, despite significant reductions.
Speaker H(Ac) H(Ar) H(Ac |Ar)
Dys.
M01 66.37 17.16 50.30
M04 33.36 11.31 26.25
F03 42.28 19.33 39.47
Average 47.34 15.93 38.68
Ctrl.
MC01 24.40 21.49 1.14
MC03 18.63 18.34 3.93
FC02 16.12 15.97 3.11
Average 19.72 18.60 2.73
Table 3: Differential entropy, in nats, across dysarthric
and control speakers for acoustic ac and articulatory ar
data.
Table 4 shows the average mutual information be-
tween acoustics and articulation for each type of
speaker, given knowledge of the phonological man-
ner of articulation. In table 1 we noted a prevalence
of pronunciation errors among dysarthric speakers
for plosives, but table 4 shows no particularly low
congruity between acoustics and articulation for this
manner of phoneme. Those pronunciation errors
tended to be voicing errors, which would involve the
glottis, which is not measured in this study.
Table 4 appears to imply that there is little mu-
tual information between acoustics and articulation
in vowels across all speakers. However, this is al-
most certainly the result of our exclusion of tongue
blade and tongue dorsum measurements in order to
standardize across speakers who could not manage
these sensors. Indeed, the configuration of the en-
tire tongue is known to be useful in discriminat-
ing among the vowels (O?Shaughnessy, 2000). An
ad hoc analysis including all three tongue sensors
for speakers F03, MC01, MC03, and FC02 revealed
mutual information between acoustics and articula-
1This is borne out in the literature (Kent and Rosen, 2004).
84
Manner
I(Ac;Ar)
Dys. Ctrl.
plosives 10.92 16.47
affricates 8.71 9.23
fricatives 9.30 10.94
nasals 13.29 15.10
glides 11.92 12.68
vowels 6.76 7.15
Table 4: Mutual information I(Ac;Ar) of acoustics and
articulation for dysarthric and control subjects, across
phonological manners of articulation.
tion of 16.81 nats for F03 and 18.73 nats for the
control speakers, for vowels. This is compared with
mutual information of 11.82 nats for F03 and 13.88
nats for the control speakers across all other man-
ners. The trend seems to be that acoustics are better
predicted given more tongue measurements.
In order to better understand these results, we
compare the distributions of the vowels in acoustic
space across dysarthric and non-dysarthric speech.
Vowels in acoustic space are characterized by the
steady-state positions of the first two formants (F1
and F2) as determined automatically by applying the
pre-emphasized Burg algorithm (Press et al, 1992).
We fit Gaussians to the first two formants for each
of the vowels in our data, as exemplified in fig-
ure 5 and compute the entropy within these distri-
butions. Surprisingly, the entropies of these distri-
butions were relatively consistent across dysarthric
(34.6 nats) and non-dysarthric (33.3 nats) speech,
with some exceptions (e.g., iy). However, vowel
spaces overlap considerably more in the dysarthric
case signifying that, while speakers with CP can be
nearly as acoustically consistent as non-dysarthric
speakers, their targets in that space are not as dis-
cernible. Some research has shown larger variance
among dysarthric vowels relative to our findings
(Kain et al, 2007). This may partially be due to our
use of natural connected speech as data, rather than
restrictive consonant-vowel-consonant non-words.
4.2 Noisy channel
Our task is to determine whether dysarthric speech
is best represented as a distorted version of typi-
cal speech, or if both dysarthric and typical speech
ought to be viewed as distortions of a common ab-
300 400 500 600 700 800 900 1000 1100 1200 1300 14001000
1500
2000
2500 iy
ih eh
uw
aa
Dysarthric male
F1 (Hz)
F2 (Hz)
ah
300 400 500 600 700 800 900 1000 1100 1200 1300 14001000
1500
2000
2500 iy ih
eh
uw aa
Non?dysarthric male
F1 (Hz)
F2 (Hz) ah
Figure 5: Contours showing first standard deviation in
F1 versus F2 space for distributions of six representative
vowels in continuous speech for the dysarthric and non-
dysarthric male speakers.
stract representation. To explore this question, we
design a transformation system that produces the
most likely observation in one data space given its
counterpart in another and the statistical relationship
between the two spaces. This transformation in ef-
fect implements the noisy channel itself.
To accomplish this, we learn probability distri-
butions over our EMA data. First, we collect all
dysarthric data together and all non-dysarthric data
together. We then consider the acoustic (Ac) and
articulatory (Ar) subsets of these data. In each
case, we train Gaussian mixtures, each with 60 com-
ponents, over 90% of the data in both dysarthric
and non-dysarthric speech. Here, each of the 60
phonemes in the data is represented by one Gaussian
component, with the weight of that component de-
termined by the relative proportion of 10 ms frames
for that phoneme. Similarly, all training word se-
quences are passed to TADA, and we train a mixture
of Gaussians on its articulatory output.
Across all Gaussian mixtures, we end up with 5
Gaussians tuned to various aspects of each phoneme
p: its dysarthric acoustics and articulation (NAcp (Yd)
and NArp (Yd)), its control acoustics and articula-
tion (NAcp (Yd) and N
Ar
p (Yd)), and its prescribed ar-
ticulation from TADA (NArp (X)). Each Gaussian
NAp(B) is represented by its mean ?
(A,B)
p and its
85
covariance, ?(A,B)p . Furthermore, we compute the
cross-covariance matrix between Gaussians for a
given phoneme (e.g., ?(Ac,Yc)?(Ac,Yd)p is the cross-
covariance matrix of the acoustics of the control (Yc)
and dysarthric (Yd) speech for phoneme p). Given
these parameters, we estimate the most likely frame
in one domain given its counterpart in another. For
example, if we are given a frame of acoustics from
a control speaker, we can synthesize the most likely
frame of acoustics for a dysarthric speaker, given an
application of the noisy channel proposed by Hosom
et al (2003) used to transform dysarthric speech to
make it more intelligible. Namely, given a frame of
acoustics yc from a control speaker, we can estimate
the acoustics of a dysarthric speaker yd with:
fAc(yc) =E(yd |yc)
=
P
?
i=1
hi(yc)
[
?(Ac,Yd)i +
?(Ac,Yc)?(Ac,Yd)i ?
(
?(Ac,Yc)i
)?1
?
(
yc??(Ac,Yc)i
)]
,
(2)
where
hi(yc) =
?iN
(
yc;?(Ac,Yc)i ,?
(Ac,Yc)
i
)
?Pj=1 ? jN
(
yc;?(Ac,Yc)j ,?
(Ac,Yc)
j
) ,
where ?p is the proportion of the frames of phoneme
p in the data. Transforming between different types
and sources of data is accomplished merely by sub-
stituting in the appropriate Gaussians above.
We now measure how closely the transformed
data spaces match their true target spaces. In each
case, we transform test utterances (recorded, or syn-
thesized with TADA) according to functions learned
in training (i.e., we use the remaining 10% of the
data for each speaker type). These transformed
spaces are then compared against their target space
in our data. Table 5 shows the Gaussian mixture
phoneme-level Kullback-Leibler divergences given
various types of source and target data, weighted by
the relative proportions of the phonemes. Each pair
of N-dimensional Gaussians (Ni with mean ?i and
covariance ?i) for a given phone and data type is
KL divergence
(10?2 nats)
Type 1 Type 2 Acous. Artic.
Ctrl. Dys. 25.36 3.23
Ctrl. ? Dys. Dys. 17.78 2.11
TADA? Ctrl. Ctrl. N/A 1.69
TADA? Dys. Dys. N/A 1.84
Table 5: Average weighted phoneme-level Kullback-
Leibler divergences.
compared with
DKL(N0 ||N1) =
1
2
(
ln
(
|?1|
|?0|
)
+ trace(??11 ?0)
+(?1??0)T??11 (?1??0)?N
)
.
Our baseline shows that control and dysarthric
speakers differ far more in their acoustics than in
their articulation. When our control data (both
acoustic and articulatory) are transformed to match
the dysarthric data, the result is predictably more
similar to the latter than if the conversion had not
taken place. This corresponds to the noisy channel
model of figure 3(a), whereby dysarthric speech is
modelled as a distortion of non-dysarthric speech.
However, when we model dysarthric and control
speech as distortions of a common, abstract repre-
sentation (i.e., task dynamics) as in figure 3(b), the
resulting synthesized articulatory spaces are more
similar to their respective observed data than the
articulation predicted by the first noisy channel
model. Dysarthric articulation predicted by trans-
formations from task-dynamics space differ signifi-
cantly from those predicted by transformations from
control EMA data at the 95% confidence interval.
5 Discussion
This paper demonstrates a few acoustic and articu-
latory features in speakers with cerebral palsy. First,
these speakers are likely to mistakenly voice un-
voiced plosives, and to delete fricatives regardless of
their word position. We suggest that it might be pru-
dent to modify the vocabularies of ASR systems to
account for these expected mispronunciations. Sec-
ond, dysarthric speakers produce sonorants signifi-
cantly slower than their non-dysarthric counterparts.
86
This may present an increase in insertion errors in
ASR systems (Rosen and Yampolsky, 2000).
Although not quantified in this paper, we detect
that a lack of articulatory control can often lead
to observable acoustic consequences. For example,
our dysarthric data contain considerable involuntary
types of velopharyngeal or glottal noise (often as-
sociated with respiration), audible swallowing, and
stuttering. We intend to work towards methods of
explicitly identifying regions of non-speech noise in
our ASR systems for dysarthric speakers.
We have considered the amount of statistical dis-
order (i.e., entropy) in both acoustic and articula-
tory data in dysarthric and non-dysarthric speak-
ers. The use of articulatory knowledge reduces the
degree of this disorder significantly for dysarthric
speakers (18.3%, relatively), though far less than for
non-dysarthric speakers (86.2%, relatively). In real-
world applications we are not likely to have access to
measurements of the vocal tract; however, many ap-
proaches exist that estimate the configuration of the
vocal tract given only acoustic data (Richmond et al,
2003; Toda et al, 2008), often to an average error of
less than 1 mm. The generalizability of such work
to new speakers (particularly those with dysarthria)
without training is an open research question.
We have argued for noisy channel models of
the neuro-motor interface assuming that the path-
way of motor command to motor activity is a lin-
ear sequence of dynamics. The biological reality
is much more complicated. In particular, the path-
way of verbal motor commands includes several
sources of sensory feedback (Seikel et al, 2005) that
modulate control parameters during speech (Gracco,
1995). These senses include exteroceptive stimuli
(auditory and tactile), and interoceptive stimuli (par-
ticularly proprioception and its kinesthetic sense)
(Seikel et al, 2005), the disruption of which can lead
to a number of production changes. For instance,
Abbs et al (1976) showed that when conduction in
the mandibular branches of the trigeminal nerve is
blocked, the resulting speech has considerably more
pronunciation errors, although is generally intelligi-
ble. Barlow (1989) argues that the redundancy of
sensory messages provides the necessary input to the
motor planning stage, which relates abstract goals to
motor activity in the cerebellum. As we continue to
develop our articulatory ASR models for dysarthric
speakers, one potential avenue for future research in-
volves the incorporation of feedback from the cur-
rent state of the vocal tract to the motor planning
phase. This would be similar, in premise, to the
DIVA model (Guenther and Perkell, 2004).
In the past, we have shown that ASR systems that
adapt non-dysarthric acoustic models to dysarthric
data offer improved word-accuracy rates, but with
a clear upper bound approximately 75% below the
general population (Rudzicz, 2007). Incorporat-
ing articulatory knowledge into such adaptation im-
proved accuracy further, but with accuracy still
approximately 60% below the general population
(Rudzicz, 2009). In this paper, we have demon-
strated that dysarthric articulation can be more ac-
curately represented as a distortion of an underlying
model of abstract speech goals than as a distortion of
non-dysarthric articulation. These results will guide
our continued development of speech systems aug-
mented with articulatory knowledge, particularly the
incorporation of task dynamics.
Acknowledgments
This research is funded by Bell University Labs, the
Natural Sciences and Engineering Research Council
of Canada, and the University of Toronto.
References
James H. Abbs, John W. Folkins, and Murali Sivarajan.
1976. Motor Impairment following Blockade of the
Infraorbital Nerve: Implications for the Use of Anes-
thetization Techniques in Speech Research. Journal of
Speech and Hearing Research, 19(1):19?35.
H.B. Barlow. 1989. Unsupervised learning. Neural
Computation, 1(3):295?311.
Joseph R Duffy. 2005. Motor Speech Disorders:
Substrates, Differential Diagnosis, and Management.
Mosby Inc.
Hans-Joachim Freund, Marc Jeannerod, Mark Hallett,
and Ramo?n Leiguarda. 2005. Higher-order motor dis-
orders: From neuroanatomy and neurobiology to clin-
ical neurology. Oxford University Press.
Vincent L. Gracco. 1995. Central and peripheral compo-
nents in the control of speech movements. In Freder-
icka Bell-Berti and Lawrence J. Raphael, editors, In-
troducing Speech: Contempory Issues, for Katherine
Safford Harris, chapter 12, pages 417?431. American
Institute of Physics press.
87
Frank H. Guenther and Joseph S. Perkell. 2004. A neural
model of speech production and its application to stud-
ies of the role of auditory feedback in speech. In Ben
Maassen, Raymond Kent, Herman Peters, Pascal Van
Lieshout, and Wouter Hulstijn, editors, Speech Motor
Control in Normal and Disordered Speech, chapter 4,
pages 29?49. Oxford University Press, Oxford.
John-Paul Hosom, Alexander B. Kain, Taniya Mishra,
Jan P. H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2003. Intelligibility of modifications to
dysarthric speech. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP ?03), volume 1, pages 924?927,
April.
Marco F. Huber, Tim Bailey, Hugh Durrant-Whyte, and
Uwe D. Hanebeck. 2008. On entropy approximation
for Gaussian mixture random vectors. In Proceed-
ings of the 2008 IEEE International Conference on In
Multisensor Fusion and Integration for Intelligent Sys-
tems, pages 181?188, Seoul, South Korea.
Alexander B. Kain, John-Paul Hosom, Xiaochuan Niu,
Jan P.H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2007. Improving the intelligibil-
ity of dysarthric speech. Speech Communication,
49(9):743?759, September.
Ray D. Kent and Kristin Rosen. 2004. Motor control per-
spectives on motor speech disorders. In Ben Maassen,
Raymond Kent, Herman Peters, Pascal Van Lieshout,
and Wouter Hulstijn, editors, Speech Motor Control
in Normal and Disordered Speech, chapter 12, pages
285?311. Oxford University Press, Oxford.
Ray D. Kent, Gary Weismer, Jane F. Kent, and John C.
Rosenbek. 1989. Toward phonetic intelligibility test-
ing in dysarthria. Journal of Speech and Hearing Dis-
orders, 54:482?499.
Aida C. G. Verdugo Lazo and Pushpa N. Rathie. 1978.
On the entropy of continuous probability distributions.
IEEE Transactions on Information Theory, 23(1):120?
122, January.
Keith L. Moore and Arthur F. Dalley. 2005. Clinically
Oriented Anatomy, Fifth Edition. Lippincott, Williams
and Wilkins.
Hosung Nam and Louis Goldstein. 2006. TADA (TAsk
Dynamics Application) manual.
Hosung Nam and Elliot Saltzman. 2003. A compet-
itive, coupled oscillator model of syllable structure.
In Proceedings of the 15th International Congress of
Phonetic Sciences (ICPhS 2003), pages 2253?2256,
Barcelona, Spain.
Douglas O?Shaughnessy. 2000. Speech Communications
? Human and Machine. IEEE Press, New York, NY,
USA.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: the art of scientific computing. Cambridge Uni-
versity Press, second edition.
Korin Richmond, Simon King, and Paul Taylor. 2003.
Modelling the uncertainty in recovering articulation
from acoustics. Computer Speech and Language,
17:153?172.
Kristin Rosen and Sasha Yampolsky. 2000. Automatic
speech recognition and a review of its functioning with
dysarthric speech. Augmentative & Alternative Com-
munication, 16(1):48?60, Jan.
Frank Rudzicz. 2007. Comparing speaker-dependent
and speaker-adaptive acoustic models for recognizing
dysarthric speech. In Proceedings of the Ninth Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, Tempe, AZ, October.
Frank Rudzicz. 2009. Applying discretized articulatory
knowledge to dysarthric speech. In Proceedings of
the 2009 IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP09), Taipei,
Taiwan, April.
Elliot L. Saltzman and Kevin G. Munhall. 1989. A dy-
namical approach to gestural patterning in speech pro-
duction. Ecological Psychology, 1(4):333?382.
J. Anthony Seikel, Douglas W. King, and David G.
Drumright, editors. 2005. Anatomy & Physiology:
for Speech, Language, and Hearing. Thomson Del-
mar Learning, third edition.
Claude E. Shannon. 1949. A Mathematical Theory of
Communication. University of Illinois Press, Urbana,
IL.
Tomoki Toda, Alan W. Black, and Keiichi Tokuda.
2008. Statistical mapping between articulatory move-
ments and acoustic spectrum using a Gaussian mix-
ture model. Speech Communication, 50(3):215?227,
March.
Alan Wrench. 1999. The MOCHA-TIMIT articulatory
database, November.
Yana Yunusova, Gary Weismer, John R. Westbury, and
Mary J. Lindstrom. 2008. Articulatory movements
during vowels in speakers with dysarthria and healthy
controls. Journal of Speech, Language, and Hearing
Research, 51:596?611, June.
Yana Yunusova, Jordan R. Green, and Antje Mefferd.
2009. Accuracy Assessment for AG500, Electromag-
netic Articulograph. Journal of Speech, Language,
and Hearing Research, 52:547?555, April.
Victor Zue, Stephanie Seneff, and James Glass. 1989.
Speech Database Development: TIMIT and Beyond.
In Proceedings of ESCA Tutorial and Research Work-
shop on Speech Input/Output Assessment and Speech
Databases (SIOA-1989), volume 2, pages 35?40,
Noordwijkerhout, The Netherlands.
88
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 11?21,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Acoustic transformations to improve the intelligibility of dysarthric speech
Frank Rudzicz
University of Toronto, Department of Computer Science
6 King?s College Road
Toronto, Ontario, Canada
frank@cs.toronto.edu
Abstract
This paper describes modifications to acous-
tic speech signals produced by speakers with
dysarthria in order to make those utter-
ances more intelligible to typical listeners.
These modifications include the correction of
tempo, the adjustment of formant frequencies
in sonorants, the removal of aberrant voic-
ing, the deletion of phoneme insertion errors,
and the replacement of erroneously dropped
phonemes. Through simple evaluations of in-
telligibility with na??ve listeners, we show that
the correction of phoneme errors results in the
greatest increase in intelligibility and is there-
fore a desirable mechanism for the eventual
creation of augmentative application software
for individuals with dysarthria.
1 Introduction
Dysarthria is a set of neuromotor disorders that im-
pair the physical production of speech. These im-
pairments reduce the normal control of the primary
vocal articulators but do not affect the regular com-
prehension or production of meaningful, syntacti-
cally correct language. For example, damage to the
recurrent laryngeal nerve reduces control of vocal
fold vibration (i.e., phonation), which can result in
aberrant voicing. Inadequate control of soft palate
movement caused by disruption of the vagus cra-
nial nerve may lead to a disproportionate amount of
air being released through the nose during speech
(i.e., hypernasality). The lack of articulatory control
also leads to various involuntary non-speech sounds
including velopharyngeal or glottal noise (Rosen
and Yampolsky, 2000). More commonly, a lack
of tongue and lip dexterity often produces heavily
slurred speech and a more diffuse and less differen-
tiable vowel target space (Kent and Rosen, 2004).
The neurological damage that causes dysarthria
usually affects other physical activity as well which
can have a drastically adverse affect on mobility
and computer interaction. For instance, severely
dysarthric speakers are 150 to 300 times slower than
typical users in keyboard interaction (Hosom et al,
2003; Hux et al, 2000). However, since dysarthric
speech is often only 10 to 17 times slower than that
of typical speakers (Patel, 1998), speech is a viable
input modality for computer-assisted interaction.
Consider a dysarthric individual who must travel
into a city by public transportation. This might in-
volve purchasing tickets, asking for directions, or in-
dicating intentions to fellow passengers, all within
a noisy and crowded environment. A personal
portable communication device in this scenario (ei-
ther hand-held or attached to a wheelchair) would
transform relatively unintelligible speech spoken
into a microphone to make it more intelligible before
being played over a set of speakers. Such a system
could facilitate interaction and overcome difficult or
failed attempts at communication in daily life.
We propose a system that avoids drawbacks of
other voice-output communication aids that output
only synthetic speech. Before software for such a
device is designed, our goal is to establish and evalu-
ate a set of modifications to dysarthric speech to pro-
duce a more intelligible equivalent. Understanding
the utility of each of these techniques will be crucial
to effectively designing the proposed system.
11
2 Background and related work
Hawley et al (2007) described an experiment in
which 8 dysarthric individuals (with either cere-
bral palsy or multiple sclerosis) controlled non-
critical devices in their home (e.g., TV) with auto-
matic speech recognition. Command vocabularies
consisted of very simple phrases (e.g., ?TV chan-
nel up?, ?Radio volume down?) and feedback was
provided to the user either by visual displays or
by auditory cues. This speech-based environmen-
tal control was compared with a ?scanning? inter-
face in which a button is physically pressed to it-
eratively cycle through a list of alternative com-
mands, words, or phrases. While the speech inter-
face made more errors (between 90.8% and 100%
accuracy after training) than the scanning inter-
face (100% accuracy), the former was significantly
faster (7.7s vs 16.9s, on average). Participants com-
mented that speech was significantly less tiring than
the scanning interface, and just as subjectively ap-
pealing (Hawley et al, 2007). Similar results were
obtained in other comparisons of speech and scan-
ning interfaces (Havstam, Buchholz, and Hartelius,
2003), and command-and-control systems (Green et
al., 2003). Speech is a desirable method of expres-
sion for individuals with dysarthria. There are many
augmentative communication devices that employ
synthetic text-to-speech in which messages can be
written on a specialized keyboard or played back
from a repository of pre-recorded phrases (Messina
and Messina, 2007). This basic system architec-
ture can be modified to allow for the replacement
of textual input with spoken input. However, such
a scenario would involve some degree of automatic
speech recognition, which is still susceptible to fault
despite recent advances (Rudzicz, 2011). Moreover,
the type of synthetic speech output produced by such
systems often lacks a sufficient degree of individual
affectation or natural expression that one might ex-
pect in typical human speech (Kain et al, 2007). The
use of prosody to convey personal information such
as one?s emotional state is generally not supported
by such systems but is nevertheless a key part of a
general communicative ability.
Transforming one?s speech in a way that pre-
serves the natural prosody will similarly also pre-
serve extra-linguistic information such as emotions,
and is therefore a pertinent response to the limita-
tions of current technology. Kain et al (2007) pro-
posed the voice transformation system shown in fig-
ure 1 which produced output speech by concatenat-
ing together original unvoiced segments with syn-
thesized voiced segments that consisted of a super-
position of the original high-bandwidth signal with
synthesized low-bandwidth formants. These synthe-
sized formants were produced by modifications to
input energy, pitch generation, and formant mod-
ifications. Modifications to energy and formants
were performed by Gaussian mixture mapping, as
described below, in which learned relationships be-
tween dysarthric and target acoustics were used to
produce output closer to the target space. This pro-
cess was intended to be automated, but Kain et al
(2007) performed extensive hand-tuning and manu-
ally identified formants in the input. This will obvi-
ously be impossible in a real-time system, but these
processes can to some extent be automated. For ex-
ample, voicing boundaries can be identified by the
weighted combination of various acoustic features
(e.g., energy, zero-crossing rate) (Kida and Kawa-
hara, 2005; Hess, 2008), and formants can be iden-
tified by the Burg algorithm (Press et al, 1992) or
through simple linear predictive analysis with con-
tinuity constraints on the identified resonances be-
tween adjacent frames (O?Shaughnessy, 2008).
Spectral modifications traditionally involve fil-
tering or amplification methods such as spectral
subtraction or harmonic filtering (O?Shaughnessy,
2000), but these are not useful for dealing with more
serious mispronounciations (e.g., /t/ for /n/). Ho-
som et al (2003) showed that Gaussian mixture
mapping can be used to transform audio from one
set of spectral acoustic features to another. During
analysis, context-independent frames of speech are
analyzed for bark-scaled energy and their 24th order
cepstral coefficients.
For synthesis, a cepstral analysis approximates
the original spectrum, and a high-order linear pre-
dictive filter is applied to each frame, and excited
by impulses or white noise (for voiced and unvoiced
segments). Hosom et al (2003) showed that given
99% human accuracy in recognizing normal speech
data, this method of reconstruction gave 93% accu-
racy on the same data. They then trained a transfor-
mative model between dysarthric and regular speech
12
Audio recordings
Voicingdetector 2-bandfilter bank
Energyanalysis Formantanalysis
Energymodification F0generation Formantmodification
Formantsynthesis+
Overlapp-add
Input speech
voiced
unvoiced
highpass lowpass
energy
energy'
CV boundaries
formants
formants'F0''
voiced'
Figure 1: Voice transformation system proposed by Kain
et al (2007).
using aligned, phoneme-annotated, and orthograph-
ically identical sentences spoken by dysarthric and
regular speakers, and a Gaussian Mixture Model
(GMM) to model the probability distribution of the
dysarthric source spectral features x as the sum of
D normal distributions with mean vector ?, diago-
nal covariance matrix ?, and prior probability ?:
p(x) =
D?
d=1
?dN (x;?d,?d) . (1)
The GMM parameters were trained in an unsuper-
vised mode using the expectation-maximization al-
gorithm and 1, 2, 4, 8, and 16 mixture components,
with D = 4 apparently being optimal. A probabilis-
tic least-squares regression mapped the source fea-
tures x onto the target (regular speaker) features y,
producing the model Wd(x) + bd for each class, and
a simple spectral distortion is performed to produce
regularized versions of dysarthic speech y?:
y?(x) =
D?
d=1
hd(x) (Wd(x) + bd) (2)
for posterior probabilities hd(x). This model is in-
teresting in that it explicitly maps the acoustic differ-
ences for different features between disordered and
regular speech1. Reconstructing the dysarthric spec-
trum in this way to sound more ?typical? while leav-
ing pitch (F0), timing, and energy characteristics in-
tact resulted in a 59.4% relative error rate reduction
(68% to 87% accuracy) among a group of 18 naive
human listeners each of whom annotated a total of
206 dysarthric test words (Hosom et al, 2003).
3 The TORGOMorph transformations
TORGOMorph encapsulates of a number of trans-
formations of the acoustics uttered by speakers with
dysarthria. Each modification is implemented in re-
action to a particular effect of dysarthria on intelligi-
bility as determined by observations on the TORGO
database of dysarthric speech (Rudzicz, Namasi-
vayam, and Wolff, 2011). Currently, these modifica-
tions are uniformly preceded by noise reduction us-
ing spectral subtraction and either phonological or
phonemic annotations. This latter step is currently
necessary, since certain modifications require either
knowledge of the manner of articulation or the iden-
tities of the vowel segments, as explained below.
The purpose of this exercise is to determine which
modifications result in the most significant improve-
ments to intelligibility, so the correct annotation se-
quence is vital to avoid the introduction of an ad-
ditional dimension of error. Therefore, the annota-
tions used below are extracted directly from the pro-
fessional markup in the TORGO database. In prac-
tice, however, phonemic annotations determined au-
tomatically by speech recognition would be imper-
fect, which is why investigations of this type often
forgo that automation altogether (e.g., see Kain et
al. (2007)). Possible alternatives to full ASR are dis-
cussed in section 5.
In some cases, the dysarthric speech must be com-
pared or supplemented with another vocal source.
Here, we synthesize segments of speech using a
text-to-speech application developed by Black and
Lenzo (2004). This system is based on the Uni-
versity of Edinburgh?s Festival tool and synthesizes
phonemes using a standard method based on lin-
1This model can also be used to measure the difference be-
tween any two types of speech.
13
ear predictive coding with a pronunciation lexicon
and part-of-speech tagger that assists in the selection
of intonation parameters (Taylor, Black, and Caley,
1998). This system is invoked by providing the ex-
pected text uttered by the dysarthic speaker. In or-
der to properly combine this purely synthetic sig-
nal and the original waveforms we require identical
sampling rates, so we resample the former by a ra-
tional factor using a polyphase filter with low-pass
filtering to avoid aliasing (Hayes, 1999). Since the
discrete phoneme sequences themselves can differ,
we find an ideal alignment between the two by the
Levenshtein algorithm (Levenshtein, 1966), which
provides the total number of insertion, deletion, and
substitution errors.
The following sections detail the components of
TORGOMorph, which is outlined in figure 2. These
components allow for a cascade of one transfor-
mation followed by another, although we can also
perform these steps independently to isolate their
effects. In all cases, the spectrogram is derived
with the fast Fourier transform given 2048 bins on
the range of 0?5 kHz. Voicing boundaries are ex-
tracted in a unidimensional vector aligned with the
spectrogram using the method of Kida and Kawa-
hara (2005) which uses GMMs trained with zero-
crossing rate, amplitude, and the spectrum as in-
put parameters. A pitch (F0) contour is also ex-
tracted from the source by the method proposed by
Kawahara et al (2005), which uses a Viterbi-like po-
tential decoding of F0 traces described by cepstral
and temporal features. That work showed an error
rate of less than 0.14% in estimating F0 contours as
compared with simultaneously-recorded electroglot-
tograph data. These contours are not in general mod-
ified by the methods proposed below, since Kain et
al. (2007) showed that using original F0 results in
the highest intelligibility among alternative systems.
Over a few segments, however, these contours can
sometimes be decimated in time during the modi-
fication proposed in section 3.3 and in some cases
removed entirely (along with all other acoustics) in
the modification proposed in section 3.2.
3.1 High-pass filter on unvoiced consonants
The first acoustic modification is based on the ob-
servation that unvoiced consonants are improperly
voiced in up to 18.7% of plosives (e.g. /d/ for /t/)
Input acoustics 
Transformed acoustics 
Spectral subtraction 
High-pass filtering  of voiceless consonants (section 3.1) 
Splicing: correcting pronunciation errors (section 3.2) 
Morphing in time (section 3.3) Morphing in frequency (section 3.4) 
Figure 2: Outline of the TORGOMorph system. The
black path indicates the cascade to be used in practice.
Solid arrows indicate paths taken during evaluation.
and up to 8.5% of fricatives (e.g. /v/ for /f/) in
dysarthric speech in the TORGO database. Voiced
consonants are typically differentiated from their un-
voiced counterparts by the presence of the voice bar,
which is a concentration of energy below 150 Hz
indicative of vocal fold vibration that often persists
throughout the consonant or during the closure be-
fore a plosive (Stevens, 1998). Empirical analysis
of TORGO data suggests that for at least two male
dysarthric speakers this voice bar extends consider-
ably higher, up to 250 Hz.
In order to correct these mispronunciations, the
voice bar is filtered out of all acoustic sub-sequences
annotated as unvoiced consonants. For this task we
use a high-pass Butterworth filter, which is ?maxi-
mally flat? in the passband2 and monotonic in mag-
nitude in the frequency domain (Butterworth, 1930).
Here, this filter is computed on a normalized fre-
quency range respecting the Nyquist frequency, so
that if a waveform?s sampling rate is 16 kHz, the
normalized cutoff frequency for this component is
f?Norm = 250/(1.6? 10
4/2) = 3.125? 10?2. The
Butterworth filter is an all-pole transfer function be-
tween signals, and we use the 10th-order low-pass
2The passband is the frequency range in which the compo-
nent magnitudes in the original signal should not be changed.
14
Butterworth filter whose magnitude response is
|B(z; 10)|2 = |H(z; 10)|2 =
1
1 +
(
jz/jz?Norm
)2?10
(3)
where z is the complex frequency in polar coordi-
nates and z?Norm is the cutoff frequency in that do-
main (Hayes, 1999). This allows the transfer func-
tion
B(z; 10) = H(z; 10) =
1
1 + z10 +
?10
i=1 ciz
10?i
(4)
whose poles occur at known symmetric intervals
around the unit complex-domain circle (Butter-
worth, 1930). These poles are then transformed
by the Matlab function zp2ss, which produces the
state-space coefficients ?i and ?i that describe the
output signal resulting from applying the low-pass
Butterworth filter to the discrete signal x[n]. These
coefficients are further converted by
~a = z?Norm~?
?1
~b = ?z?Norm
(
~??1~?
) (5)
giving the high-pass Butterworth filter with the same
cutoff frequency of z?Norm. This continuous system
is converted to the discrete equivalent through the
impulse-invariant discretization method and is im-
plemented by the difference equation
y[n] =
10?
k=1
aky[n? k] +
10?
k=0
bkx[n? k]. (6)
As previously mentioned, this equation is applied to
each acoustic sub-sequence annotated as unvoiced
consonants, thereby smoothly removing the energy
below 250 Hz.
3.2 Splicing: correcting dropped and inserted
phoneme errors
The Levenshtein algorithm finds a best possible
alignment of the phoneme sequence in actually ut-
tered speech and the expected phoneme sequence,
given the known word sequence. Isolating phoneme
insertions and deletions are therefore a simple matter
of iteratively adjusting the source speech according
to that alignment. There are two cases where action
is required:
insertion error In this case a phoneme is present
where it ought not be. In the TORGO database,
these insertion errors tend to be repetitions of
phonemes occurring in the first syllable of a
word, according to the International Speech
Lexicon Dictionary (Hasegawa-Johnson and
Fleck, 2007). When an insertion error is iden-
tified the entire associated segment of the sig-
nal is simply removed. In the case that the as-
sociated segment is not surrounded by silence,
adjacent phonemes can be merged together
with time-domain pitch-synchronous overlap-
add (Moulines and Charpentier, 1990).
deletion error The vast majority of accidentally
deleted phonemes in the TORGO database are
fricatives, affricates, and plosives. Often, these
involve not properly pluralizing nouns (e.g.,
book instead of books). Given their high pre-
ponderance of error, these phonemes are the
only ones we insert into the dysarthric source
speech. Specifically, when the deletion of a
phoneme is recognized with the Levenshtein
algorithm, we simply extract the associated
segment from the aligned synthesized speech
and insert it into the appropriate spot in the
dysarthric speech. For all unvoiced fricatives,
affricates, and plosives no further action is re-
quired. When these phonemes are voiced, how-
ever, we first extract and remove the F0 curve
from the synthetic speech, linearly interpolate
the F0 curve from adjacent phonemes in the
source dysarthric speech, and resynthesize with
the synthetic spectrum and interpolated F0. If
interpolation is not possible (e.g., the synthetic
voiced phoneme is to be inserted beside an un-
voiced phoneme), we simply generate a flat F0
equal to the nearest natural F0 curve.
3.3 Morphing in time
Figure 3 exemplifies that vowels uttered by
dysarthric speakers are significantly slower than
those uttered by typical speakers. In fact, sonorants
can be twice as long in dysarthric speech, on aver-
age (Rudzicz, Namasivayam, and Wolff, 2011). In
this modification, phoneme sequences identified as
sonorant are simply contracted in time in order to be
equal in extent to the greater of half their original
15
(a) (b)
Figure 3: Repetitions of /iy p ah/ over 1.5s by (a) a male
speaker with athetoid CP, and (b) a female control in the
TORGO database. Dysarthric speech is notably slower
and more strained than regular speech.
length or the equivalent synthetic phoneme?s length.
In all cases this involved shortening the dysarthric
source sonorant.
Since we wish to contract the length of a signal
segment here without affecting its pitch or frequency
characteristics, we use a phase vocoder based on
digital short-time Fourier analysis (Portnoff, 1976).
Here, Hamming-windowed segments of the source
phoneme are analyzed with a z-transform giving
both frequency and phase estimates for up to 2048
frequency bands. During pitch-preserving time-
scaled warping, we specify the magnitude spectrum
directly from the input magnitude spectrum with
phase values chosen to ensure continuity (Sethares,
2007). Specifically, for the frequency band at fre-
quency F and frames j and k > j in the modified
spectrogram, the phase ? is predicted by
?(F )k = ?
(F )
j + 2piF (j ? k). (7)
In our case the discrete warping of the spectrogram
involves simple decimation by a constant factor. The
spectrogram is then converted into a time-domain
signal modified in tempo but not in pitch relative
to the original phoneme segment. This conversion
is accomplished simply through the inverse Fourier
transform.
3.4 Morphing in frequency
Formant trajectories inform the listener as to the
identities of vowels, but the vowel space of
dysarthric speakers tends to be constrained (Kain
et al, 2007). In order to improve a listener?s abil-
ity to differentiate between the vowels, this modifi-
cation component identifies formant trajectories in
the acoustics and modifies these according to the
known vowel identity of a segment. Here, formants
are identified with a 14th-order linear-predictive
coder with continuity constraints on the identi-
fied resonances between adjacent frames (Snell and
Milinazzo, 1993; O?Shaughnessy, 2008). Band-
widths are determined by the negative natural log-
arithm of the pole magnitude, as implemented in
the STRAIGHT analysis system (Banno et al, 2007;
Kawahara, 2006).
For each identified vowel in the dysarthric
speech3, formant candidates are identified at each
frame in time up to 5 kHz. Only those time frames
having at least 3 such candidates within 250 Hz of
expected values are considered. The expected values
of formants are derived from analyses performed by
Allen et al (1987). Given these subsets of candidate
time frames in the vowel, the one having the highest
spectral energy within the middle 50% of the length
of the vowel is established as the anchor position,
and the three formant candidates within the expected
ranges are established as the anchor frequencies for
formants F1 to F3. If more than one formant can-
didate falls within expected ranges, the one with the
lowest bandwidth becomes the anchor frequency.
Given identified anchor points and target
sonorant-specific frequencies and bandwidths,
there are several methods to modify the spectrum.
The most common may be to learn a statistical
conversion function based on Gaussian mixture
mapping, as described earlier, typically preceded by
alignment of sequences using dynamic time warping
(Stylianou, 2008). Here, we use the STRAIGHT
morphing implemented by Kawahara and Matsui
(2003), among others. The transformation of a
frame of speech xA for speaker A is performed with
a multivariate frequency-transformation function
TA? given known targets ? using
TA?(xA) =
? xA
0
exp
(
log
(
?TA?(?)
??
))
??
=
? xA
0
exp
(
(1? r) log
(
?TAA(?)
??
)
+ r log
(
?TA?(?)
??
))
??
=
? xA
0
(
?TA?(?)
??
)r
??,
(8)
3Accidentally inserted vowels are also included here, unless
previously removed by the splicing technique in section 3.2.
16
time (ms)
frequen
cy (Hz)
1300 1400 1500 1600 1700 1800 1900 20000
5001000
15002000
25003000
35004000
45005000
(a)
time (ms)
frequen
cy (Hz)
1300 1400 1500 1600 1700 1800 1900 20000
5001000
15002000
25003000
35004000
45005000
(b)
Figure 4: Spectrograms for (a) the dysarthric original and
(b) the frequency-modified renditions of the word fear.
Circles represent indicative formant locations.
where ? is the frame-based time dimension and
where 0 ? r ? 1 is an interpolative rate at which
to perform morphing (i.e., r = 1 implies complete
conversion of the parameters of speakerA to param-
eter set ? and r = 0 implies no conversion.) (Kawa-
hara et al, 2009). An example of the results of this
morphing technique is shown in figure 4 in which
the three identified formants are shifted to their ex-
pected frequencies.
This method tracks formants and warps the fre-
quency space automatically, whereas Kain et al
(2007) perform these functions manually. A future
implementation may use Kalman filters to reduce the
noise inherent in trajectory tracking. Such an ap-
proach has shown significant improvements in for-
mant tracking, especially for F1 (Yan et al, 2007).
4 Intelligibility experiments with
TORGOMorph
The intelligibility of both purely synthetic and mod-
ified speech signals can be measured objectively by
simply having a set of participants transcribe what
they hear from a selection of word, phrase, or sen-
tence prompts (Spiegel et al, 1990), although no sin-
gle standard has emerged as pre-eminent (Schroeter,
2008). Hustad (2006) suggested that orthographic
transcriptions provide a more accurate predictor of
intelligibility among dysarthric speakers than the
more subjective estimates used in clinical settings,
e.g., Enderby (1983). That study had 80 listeners
who transcribed audio (which is an atypically large
group for this task) and showed that intelligibility
increased from 61.9% given only acoustic stimuli to
66.75% given audiovisual stimuli on the transcrip-
tion task in normal speech. In the current work, we
modify only the acoustics of dysarthric speech; how-
ever future work might consider how to prompt lis-
teners in a more multimodal context.
In order to gauge the intelligibility of our mod-
ifications, we designed a simple experiment in
which human listeners attempt to identify words in
sentence-level utterances under a number of acoustic
scenarios. Sentences are either uttered by a speaker
with dysarthria, modified from their original source
acoustics, or manufactured by a text-to-speech syn-
thesizer. Each participant is seated at a personal
computer with a simple graphical user interface with
a button which plays or replays the audio (up to 5
times), a text box in which to write responses, and
a second button to submit those responses. Audio is
played over a pair of headphones. The participants
are told to only transcribe the words with which they
are reasonably confident and to ignore those that
they cannot discern. They are also informed that
the sentences are grammatically correct but not nec-
essarily semantically coherent, and that there is no
profanity. Each participant listens to 20 sentences
selected at random with the constraints that at least
two utterances are taken from each category of au-
dio, described below, and that at least five utter-
ances are also provided to another listener, in order
to evaluate inter-annotator agreement. Participants
are self-selected to have no extensive prior experi-
ence in speaking with individuals with dysarthria,
in order to reflect the general population. Although
dysarthric utterances are likely to be contextualized
within meaningful conversations in real-world situ-
ations, such pragmatic aspects of discourse are not
considered here in order to concentrate on acoustic
effects alone. No cues as to the topic or semantic
context of the sentences are given, as there is no
evidence that such aids to comprehension affect in-
telligibility (Hustad and Beukelman, 2002). In this
study we use sentence-level utterances uttered by
male speakers from the TORGO database.
Baseline performance is measured on the original
dysarthric speech. Two other systems are used for
reference:
Synthetic Word sequences are produced by the
Cepstral commercial text-to-speech system us-
ing the U.S. English voice ?David?. This sys-
17
tem is based on Festival in almost every respect,
including its use of linguistic pre-processing
(e.g., part-of-speech tagging) and rule-based
generation (Taylor, Black, and Caley, 1998).
This approach has the advantage that every as-
pect of the synthesized speech (e.g., the word
sequence) can be controlled although here, as
in practice, synthesized speech will not mimic
the user?s own acoustic patterns, and will of-
ten sound more ?mechanical? due to artificial
prosody (Black and Lenzo, 2007).
GMM This system uses the Gaussian mixture map-
ping type of modification suggested by Toda,
Black, and Tokuda (2005) and Kain et al
(2007). Here, we use the FestVox implementa-
tion of this algorithm, which includes pitch ex-
traction, some phonological knowledge (Toth
and Black, 2005), and a method for resynthe-
sis. Parameters for this model are trained by the
FestVox system using a standard expectation-
maximization approach with 24th-order cep-
stral coefficients and 4 Gaussian components.
The training set consists of all vowels uttered
by a male speaker in the TORGO database
and their synthetic realizations produced by the
method above.
Performance is evaluated on the three other acous-
tic transformations, namely those described in sec-
tions 3.2, 3.3, and 3.4 above. Tables 1 and 2 respec-
tively show the percentage of words and phonemes
correctly identified by each listener relative to the
expected word sequence under each acoustic con-
dition. In each case, annotator transcriptions were
aligned with the ?true? or expected sequences us-
ing the Levenshtein algorithm described in section
3. Plural forms of singular words, for example,
are considered incorrect in word alignment although
one obvious spelling mistake (i.e., ?skilfully?) is cor-
rected before evaluation. Words are split into com-
ponent phonemes according to the CMU dictionary,
with words having multiple pronunciations given the
first decomposition therein.
In these experiments there is not enough data from
which to make definitive claims of statistical signifi-
cance, but it is clear that the purely synthetic speech
has a far greater intelligibility than other approaches,
more than doubling the average accuracy of the
Orig. GMM Synth. Splice Time Freq.
L01 22.1 15.6 82.0 40.2 34.7 35.2
L02 27.8 12.2 75.5 44.9 39.4 33.8
L03 38.3 14.8 76.3 37.5 12.9 21.4
L04 24.7 10.8 72.1 32.6 22.2 18.4
Avg. 28.2 13.6 76.5 38.8 27.3 27.2
Table 1: Percentage of words correctly identified by each
listener (L0*) relative to the expected sequence. Sections
3.2, 3.3, and 3.4 discuss the ?Splice?, ?Time?, and ?Freq.?
techniques, respectively.
Orig. GMM Synth. Splice Time Freq.
L01 52.0 43.1 98.2 64.7 47.8 55.1
L02 57.8 38.2 92.9 68.9 50.6 53.3
L03 50.1 41.4 96.8 57.1 30.7 46.7
L04 51.6 33.8 88.7 51.9 43.2 45.0
Avg. 52.9 39.1 94.2 60.7 43.1 50.0
Table 2: Percentage of phonemes correctly identified by
each listener relative to the expected sequence. Sections
3.2, 3.3, and 3.4 discuss the ?Splice?, ?Time?, and ?Freq.?
techniques, respectively.
TORGOMorph modifications. The GMM transfor-
mation method proposed by Kain et al (2007) gave
poor performance, although our experiments are dis-
tinguished from theirs in that our formant traces are
detected automatically, rather than by hand. The rel-
ative success of the synthetic approach is not an ar-
gument against the type of modifications proposed
here and by Kain et al (2007), since our aim is to
avoid the use of impersonal and invariant utterances.
Indeed, future study in this area should incorporate
subjective measures of ?naturalness?. Further uses
of acoustic modifications not attainable by text-to-
speech synthesis are discussed in section 5.
In all cases, the splicing technique of removing
accidentally inserted phonemes and inserting miss-
ing ones gives the highest intelligibility relative to
all acoustic transformation methods. Although more
study is required, this result emphasizes the impor-
tance of lexically correct phoneme sequences. In the
word-recognition experiment, there are an average
of 5.2 substitution errors per sentence in the unmod-
ified dysarthric speech against 2.75 in the synthetic
speech. There are also 2.6 substitution errors on av-
erage per sentence for the speech modified in fre-
quency, but 3.1 deletion errors, on average, against
0.24 in synthetic speech. No correlation is found be-
18
tween the ?loudness? of the speech (determined by
the overall energy in the sonorants) and intelligibil-
ity results, although this might change with the ac-
quisition of more data. Neel (2009), for instance,
found that loud or amplified speech from individu-
als with Parkinson?s disease was more intelligible to
human listeners than quieter speech.
Our results are comparable in many respects to the
experiments of Kain et al (2007), although they only
looked at simple consonant-vowel-consonant stim-
uli. Their results showed an average of 92% correct
synthetic vowel recognition (compared with 94.2%
phoneme recognition in table 2) and 48% correct
dysarthric vowel recognition (compared with 52.9%
in table 2). Our results, however, show that modi-
fied timing and modified frequencies do not actually
benefit intelligibility in either the word or phoneme
cases. This disparity may in part be due to the fact
that our stimuli are much more complex (quicker
sentences do not necessarily improve intelligibility).
5 Discussion
This work represents an inaugural step towards
speech modification systems for human-human and
human-computer interaction. Tolba and Torgoman
(2009) claimed that significant improvements in au-
tomatic recognition of dysarthric speech are attain-
able by modifying formants F1 and F2 to be more
similar to expected values. In that study, formants
were identified using standard linear predictive cod-
ing techniques, although no information was pro-
vided as to how these formants were modified nor
how their targets were determined. However, they
claimed that modified dysarthric speech resulted
in ?recognition rates? (by which they presumably
meant word-accuracy) of 71.4% in the HTK speech
recognition system, as compared with 28% on the
unmodified dysarthric speech from 7 individuals.
The results in section 4 show that human listen-
ers are more likely to correctly identify utterances
in which phoneme insertion and deletion errors are
corrected than those in which formant frequencies
are adjusted. Therefore, one might hypothesize
that such pre-processing might provide even greater
gains than those reported by Tolba and Torgoman
(2009). Ongoing work ought to confirm or deny this
hypothesis.
A prototypical client-based application based on
our research for unrestricted speech transformation
of novel sentences is currently in development. Such
work will involve improving factors such as accu-
racy and accessibility for individuals whose neuro-
motor disabilities limit the use of modern speech
recognition, and for whom alternative interaction
modalities are insufficient. This application is being
developed under the assumption that it will be used
in a mobile device embeddable within a wheelchair.
If word-prediction is to be incorporated, the pre-
dicted continuations of uttered sentence fragments
can be synthesized without requiring acoustic input.
In practice, the modifications presented here will
have to be based on automatically-generated anno-
tations of the source audio. This is especially im-
portant to the ?splicing? module in which word-
identification is crucial. There are a number of tech-
niques that can be exercised in this area. Czyzewski,
Kaczmarek, and Kostek (2003) apply both a vari-
ety of neural networks and rough sets to the task
of classifying segments of speech according to the
presence of stop-gaps, vowel prolongations, and in-
correct syllable repetitions. In each case, input in-
cludes source waveforms and detected formant fre-
quencies. They found that stop-gaps and vowel pro-
longations could be detected with up to 97.2% ac-
curacy and that vowel repetitions could be detected
with up to 90% accuracy using the rough set method.
Accuracy was similar although slightly lower us-
ing traditional neural networks (Czyzewski, Kacz-
marek, and Kostek, 2003). These results appear
generally invariant even under frequency modifica-
tions to the source speech. Arbisi-Kelm (2010), for
example, suggest that disfluent repetitions can be
identified reliably through the use of pitch, dura-
tion, and pause detection (with precision up to 93%
(Nakatani, 1993)). If more traditional models of
speech recognition are to be deployed to identify
vowels, the probabilities that they generate across
hypothesized words might be used to weight the
manner in which acoustic transformations are made.
The use of one?s own voice to communicate is a
desirable goal, and continuations of this research are
therefore focused on the practical aspects of this re-
search towards usable and portable systems.
19
References
Allen, Jonathan, M. Sharon Hunnicutt, Dennis H. Klatt,
Robert C. Armstrong, and David B. Pisoni. 1987.
From text to speech: the MITalk system. Cambridge
University Press, New York, NY, USA.
Arbisi-Kelm, Timothy. 2010. Intonation structure and
disfluency detection in stuttering. Laboratory Phonol-
ogy 10, 4:405?432.
Banno, Hideki, Hiroaki Hata, Masanori Morise, Toru
Takahashi, Toshio Irino, and Hideki Kawahara. 2007.
Implementation of realtime STRAIGHT speech ma-
nipulation system: Report on its first implementation.
Acoustical Science and Technology, 28(3):140?146.
Black, Alan W. and Kevin A. Lenzo. 2004. Multilingual
text-to-speech synthesis. In 2004 IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP 2004).
Black, Alan W. and Kevin A. Lenzo. 2007. Building
synthetic voices. http://www.festvox.org/
festvox/bsv.ps.gz.
Butterworth, Stephen. 1930. On the theory of filter am-
plifiers. Experimental Wireless and the Wireless Engi-
neer, 7:536?541.
Czyzewski, Andrzej, Andrzej Kaczmarek, and Bozena
Kostek. 2003. Intelligent processing of stuttered
speech. Journal of Intelligent Information Systems,
21(2):143?171.
Enderby, Pamela M. 1983. Frenchay Dysarthria Assess-
ment. College Hill Press.
Green, Phil, James Carmichael, Athanassios Hatzis, Pam
Enderby, Mark Hawley, and Mark Parker. 2003. Au-
tomatic speech recognition with sparse training data
for dysarthric speakers. In Proceedings of Eurospeech
2003, pages 1189?1192, Geneva.
Hasegawa-Johnson, Mark and Margaret Fleck. 2007. In-
ternational Speech Lexicon Project. http://www.
isle.illinois.edu/dict/.
Havstam, Christina, Margret Buchholz, and Lena
Hartelius. 2003. Speech recognition and dysarthria: a
single subject study of two individuals with profound
impairment of speech and motor control. Logopedics
Phoniatrics Vocology, 28:81?90(10), August.
Hawley, Mark S., Pam Enderby, Phil Green, Stuart
Cunningham, Simon Brownsell, James Carmichael,
Mark Parker, Athanassios Hatzis, Peter O?Neill,
and Rebecca Palmer. 2007. A speech-controlled
environmental control system for people with se-
vere dysarthria. Medical Engineering & Physics,
29(5):586?593, June.
Hayes, Monson H. 1999. Digital Signal Processing.
Schaum?s Outlines. McGraw Hill.
Hess, Wolfgang J. 2008. Pitch and voicing determination
of speech with an extension toward music signal. In
Jacob Benesty, M. Mohan Sondhi, and Yiteng Huang,
editors, Speech Processing. Springer.
Hosom, John-Paul, Alexander B. Kain, Taniya Mishra,
Jan P. H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2003. Intelligibility of modifications to
dysarthric speech. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP ?03), volume 1, pages 924?927,
April.
Hustad, Katherine C. 2006. Estimating the intelligibil-
ity of speakers with dysarthria. Folia Phoniatrica et
Logopaedica, 58(3):217?228.
Hustad, Katherine C. and David R. Beukelman. 2002.
Listener comprehension of severely dysarthric speech:
Effects of linguistic cues and stimulus cohesion. Jour-
nal of Speech, Language, and Hearing Research,
45:545?558, June.
Hux, Karen, Joan Rankin-Erickson, Nancy Manasse, and
Elizabeth Lauritzen. 2000. Accuracy of three speech
recognition systems: Case study of dysarthric speech.
Augmentative and Alternative Communication (AAC),
16(3):186 ?196, January.
Kain, Alexander B., John-Paul Hosom, Xiaochuan Niu,
Jan P.H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2007. Improving the intelligibil-
ity of dysarthric speech. Speech Communication,
49(9):743?759, September.
Kawahara, H. and H. Matsui. 2003. Auditory mor-
phing based on an elastic perceptual distance metric
in an interference-free time-frequency representation.
In Acoustics, Speech, and Signal Processing, 2003.
Proceedings. (ICASSP ?03). 2003 IEEE International
Conference on, volume 1, pages I?256 ? I?259 vol.1,
April.
Kawahara, H., R. Nisimura, T. Irino, M. Morise, T. Taka-
hashi, and H. Banno. 2009. Temporally variable
multi-aspect auditory morphing enabling extrapolation
without objective and perceptual breakdown. In Pro-
ceedings of IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP 2009),
pages 3905?3908, April.
Kawahara, Hideki. 2006. STRAIGHT, exploitation of
the other aspect of VOCODER: Perceptually isomor-
phic decomposition of speech sounds. Acoustical Sci-
ence and Technology, 27(6):349?353.
Kawahara, Hideki, Alain de Cheveigne?, Hideki Banno,
Toru Takahashi, and Toshio Irino. 2005. Nearly
Defect-Free F0 Trajectory Extraction for Expressive
Speech Modifications Based on STRAIGHT. In Pro-
ceedings of INTERSPEECH 2005, pages 537?540,
September.
Kent, Ray D. and Kristin Rosen. 2004. Motor con-
trol perspectives on motor speech disorders. In Ben
20
Maassen, Raymond Kent, Herman Peters, Pascal Van
Lieshout, and Wouter Hulstijn, editors, Speech Mo-
tor Control in Normal and Disordered Speech. Oxford
University Press, Oxford, chapter 12, pages 285?311.
Kida, Yusuke and Tatsuya Kawahara. 2005. Voice
activity detection based on optimally weighted com-
bination of multiple features. In Proceedings of
INTERSPEECH-2005, pages 2621?2624.
Levenshtein, Vladimir I. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Cyber-
netics and Control Theory, 10(8):707?710.
Messina, James J. and Constance M. Messina.
2007. Description of AAC devices. http:
//www.coping.org/specialneeds/
assistech/aacdev.htm, April.
Moulines, Eric and Francis Charpentier. 1990. Pitch-
synchronous waveform processing techniques for text-
to-speech synthesis using diphones. Speech Commu-
nication, 9:453?467, December.
Nakatani, Christine. 1993. A speech-first model for re-
pair detection and correction. In Proceedings of the
31st Annual Meeting of the Association for Computa-
tional Linguistics, pages 46?53.
Neel, Amy T. 2009. Effects of loud and amplified speech
on sentence and word intelligibility in parkinson dis-
ease. Journal of Speech, Language, and Hearing Re-
search, 52:1021?1033, August.
O?Shaughnessy, Douglas. 2000. Speech Communica-
tions ? Human and Machine. IEEE Press, New York,
NY, USA.
O?Shaughnessy, Douglas. 2008. Formant estimation and
tracking. In Jacob Benesty, M. Mohan Sondhi, and
Yiteng Huang, editors, Speech Processing. Springer.
Patel, Rupal. 1998. Control of prosodic parameters by
an individual with severe dysarthria. Technical report,
University of Toronto, December.
Portnoff, Michael R. 1976. Implementation of the dig-
ital phase vocoder using the fast Fourier transform.
IEEE Transactions on Acoustics, Speech and Signal
Processing, 24(3):243?248.
Press, William H., Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: the art of scientific computing. Cambridge Uni-
versity Press, second edition.
Rosen, Kristin and Sasha Yampolsky. 2000. Automatic
speech recognition and a review of its functioning with
dysarthric speech. Augmentative & Alternative Com-
munication, 16(1):48?60, Jan.
Rudzicz, Frank. 2011. Production knowledge in the
recognition of dysarthric speech. Ph.D. thesis, Uni-
versity of Toronto, Department of Computer Science.
Rudzicz, Frank, Aravind Kumar Namasivayam, and
Talya Wolff. 2011. The TORGO database of acoustic
and articulatory speech from speakers with dysarthria.
Language Resources and Evaluation, (in press).
Schroeter, Juergen. 2008. Basic principles of speech
synthesis. In Jacob Benesty, M. Mohan Sondhi, and
Yiteng Huang, editors, Speech Processing. Springer.
Sethares, William Arthur. 2007. Rhythm and Trans-
forms. Springer.
Snell, Roy C. and Fausto Milinazzo. 1993. Formant Lo-
cation from LPC Analysis Data. IEEE Transactions
on Speech and Audio Processing, 1(2), April.
Spiegel, Murray F., Mary Jo Altom, Marian J. Macchi,
and Karen L. Wallace. 1990. Comprehensive assess-
ment of the telephone intelligibility of synthesized and
natural speech. Speech Communication, 9(4):279 ?
291.
Stevens, Kenneth N. 1998. Acoustic Phonetics. MIT
Press, Cambridge, Massachussetts.
Stylianou, Yannis. 2008. Voice transformation. In Ja-
cob Benesty, M. Mohan Sondhi, and Yiteng Huang,
editors, Speech Processing. Springer.
Taylor, Paul, Alan W. Black, and Richard Caley. 1998.
The architecture of the Festival speech synthesis sys-
tem. In Proceedings of the 3rd ESCA Workshop on
Speech Synthesis, pages 147?151, Jenolan Caves, Aus-
tralia.
Toda, Tomoki, Alan W. Black, and Keiichi Tokuda. 2005.
Spectral conversion based on maximum likelihood es-
timation considering global variance of converted pa-
rameter. In Proceedings of the 2005 International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP 2005), Philadelphia, Pennsylvania.
Tolba, Hesham and Ahmed S. El Torgoman. 2009. To-
wards the improvement of automatic recognition of
dysarthric speech. In International Conference on
Computer Science and Information Technology, pages
277?281, Los Alamitos, CA, USA. IEEE Computer
Society.
Toth, Arthur R. and Alan W. Black. 2005. Cross-speaker
articulatory position data for phonetic feature predic-
tion. In Proceedings of Interspeech 2005, Lisbon, Por-
tugal.
Yan, Qin, Saeed Vaseghi, Esfandiar Zavarehei, Ben Mil-
ner, Jonathan Darch, Paul White, and Ioannis An-
drianakis. 2007. Formant tracking linear predic-
tion model usng HMMs and Kalman filters for noisy
speech processing. Computer Speech and Language,
21:543?561.
21
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 47?55,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Communication strategies for a computerized caregiver for individuals with
Alzheimer?s disease
Frank Rudzicz1,2, ? and Rozanne Wilson1 and Alex Mihailidis2 and Elizabeth Rochon1
1 Department of Speech-Language Pathology,
2 Department of Occupational Science and Occupational Therapy
University of Toronto
Toronto Canada
Carol Leonard
School of Rehabilitation Sciences
University of Ottawa
Ottawa Canada
Abstract
Currently, health care costs associated with
aging at home can be prohibitive if individ-
uals require continual or periodic supervision
or assistance because of Alzheimer?s disease.
These costs, normally associated with human
caregivers, can be mitigated to some extent
given automated systems that mimic some of
their functions. In this paper, we present in-
augural work towards producing a generic au-
tomated system that assists individuals with
Alzheimer?s to complete daily tasks using ver-
bal communication. Here, we show how to
improve rates of correct speech recognition
by preprocessing acoustic noise and by mod-
ifying the vocabulary according to the task.
We conclude by outlining current directions of
research including specialized grammars and
automatic detection of confusion.
1 Introduction
In the United States, approximately $100 billion are
spent annually on the direct and indirect care of in-
dividuals with Alzheimer?s disease (AD), the major-
ity of which is attributed to long-term institutional
care (Ernst et al, 1997). As the population ages, the
incidence of AD will double or triple, with Medi-
care costs alone reaching $189 billion in the US by
2015 (Bharucha et al, 2009). Given the growing
need to support this population, there is an increas-
ing interest in the design and development of tech-
nologies that support this population at home and
extend ones quality of life and autonomy (Mihailidis
et al, 2008).
?Contact: frank@cs.toronto.edu
Alzheimer?s disease is a type of progres-
sive neuro-degenerative dementia characterized by
marked declines in mental acuity, specifically in
cognitive, social, and functional capacity. A decline
in memory (short- and long-term), executive capac-
ity, visual-spacial reasoning, and linguistic ability
are all typical effects of AD (Cummings, 2004).
These declines make the completion of activities of
daily living (e.g., finances, preparing a meal) diffi-
cult and more severe declines often necessitate care-
giver assistance. Caregivers who assist individuals
with AD at home are common, but are often the pre-
cursor to placement in a long-term care (LTC) facil-
ity (Gaugler et al, 2009).
We are building systems that automate, where
possible, some of the support activities that currently
require family or formal (i.e., employed) caregivers.
Specifically, we are designing an intelligent dialog
component that can engage in two-way speech com-
munication with an individual in order to help guide
that individual towards the completion of certain
daily household tasks, including washing ones hands
and brushing ones teeth. A typical installation setup
in a bathroom, shown in figure 1, consists of video
cameras that track a user?s hands and the area in and
around the sink, as well as microphones, speakers,
and a screen that can display prompting informa-
tion. Similar installations are being tested in other
household rooms as part of the COACH project (Mi-
hailidis et al, 2008), according to the task; this is
an example of ambient intelligence in which tech-
nology embedded in the environment is sensitive to
the activities of the user with it (Spanoudakis et al,
2010).
47
Our goal is to encode in software the kinds of
techniques used by caregivers to help their clients
achieve these activities; this includes automati-
cally identifying and recovering from breakdowns
in communication and flexibly adapting to the in-
dividual over time. Before such a system can be de-
ployed, the underlying models need to be adjusted
to the desired population and tasks. Similarly, the
speech output component would need to be pro-
grammed according to the vocabularies, grammars,
and dialog strategies used by caregivers. This paper
presents preliminary experiments towards dedicated
speech recognition for such a system. Evaluation
data were collected as part of a larger project exam-
ining the use of communication strategies by formal
caregivers while assisting residents with moderate to
severe AD during the completion of toothbrushing
(Wilson et al, 2012).
2 Background ? communication strategies
Automated communicative systems that are more
sensitive to the emotive and the mental states of their
users are often more successful than more neutral
conversational agents (Saini et al, 2005). In order to
be useful in practice, these communicative systems
need to mimic some of the techniques employed
by caregivers of individuals with AD. Often, these
caregivers are employed by local clinics or medical
institutions and are trained by those institutions in
ideal verbal communication strategies for use with
those having dementia (Hopper, 2001; Goldfarb and
Pietro, 2004). These include (Small et al, 2003) but
are not limited to:
1. Relatively slow rate of speech rate.
2. Verbatim repetition of misunderstood prompts.
3. Closed-ended questions (i.e., that elicit yes/no
responses).
4. Simple sentences with reduced syntactic com-
plexity.
5. Giving one question or one direction at a time.
6. Minimal use of pronouns.
These strategies, though often based on observa-
tional studies, are not necessarily based on quantita-
tive empirical research and may not be generalizable
across relevant populations. Indeed, Tomoeda et al
(1990) showed that rates of speech that are too slow
(a) Environmental setup
(b) On-screen prompting
Figure 1: Setup and on-screen prompting for COACH.
The environment includes numerous sensors including
microphones and video cameras as well as a screen upon
which prompts can be displayed. In this example, the
user is prompted to lather their hands after having applied
soap. Images are copyright Intelligent Assistive Technol-
ogy and Systems Lab).
may interfere with comprehension if they introduce
48
problems of short-term retention of working mem-
ory. Small, Andersen, and Kempler (1997) showed
that paraphrased repetition is just as effective as ver-
batim repetition (indeed, syntactic variation of com-
mon semantics may assist comprehension). Further-
more, Rochon, Waters, and Caplan (2000) suggested
that the syntactic complexity of utterances is not
necessarily the only predictor of comprehension in
individuals with AD; rather, correct comprehension
of the semantics of sentences is inversely related to
the increasing number of propositions used ? it is
preferable to have as few clauses or core ideas as
possible, i.e., one-at-a-time.
Although not the empirical subject of this pa-
per, we are studying methods of automating the
resolution of communication breakdown. Much of
this work is based on the Trouble Source-Repair
(TSR) model in which difficulties in speaking, hear-
ing, or understanding are identified and repairs are
initiated and carried out (Schegloff, Jefferson, and
Sacks, 1977). Difficulties can arise in a number
of dimensions including phonological (i.e., mispro-
nunciation), morphological/syntactic (e.g., incorrect
agreement among constituents), semantic (e.g., dis-
turbances related to lexical access, word retrieval,
or word use), and discourse (i.e., misunderstanding
of topic, shared knowledge, or cohesion) (Orange,
Lubinsky, and Higginbotham, 1996). The major-
ity of TSR sequences involve self-correction of a
speaker?s own error, e.g., by repetition, elaboration,
or reduction of a troublesome utterance (Schegloff,
Jefferson, and Sacks, 1977). Orange, Lubinsky,
and Higginbotham (1996) showed that while 18%
of non-AD dyad utterances involved TSR, whereas
23.6% of early-stage AD dyads and 33% of middle-
stage AD dyads involved TSR. Of these, individu-
als with middle-stage AD exhibited more discourse-
related difficulties including inattention, failure to
track propositions and thematic information, and
deficits in working memory. The most common
repair initiators and repairs given communication
breakdown involved frequent ?wh-questions and hy-
potheses (e.g., ?Do you mean??). Conversational
partners of individuals with middle-stage AD initi-
ated repair less frequently than conversational part-
ners of control subjects, possibly aware of their de-
teriorating ability, or to avoid possible further con-
fusion. An alternative although very closely related
paradigm for measuring communication breakdown
is Trouble Indicating Behavior (TIB) in which the
confused participant implicitly or explicitly requests
aid. In a study of 7 seniors with moderate/severe de-
mentia and 3 with mild/moderate dementia, Watson
(1999) showed that there was a significant difference
in TIB use (? < 0.005) between individuals with
AD and the general population. Individuals with
AD are most likely to exhibit dysfluency, lack of up-
take in the dialog, metalinguistic comments (e.g., ?I
can?t think of the word?), neutral requests for repeti-
tion, whereas the general population are most likely
to exhibit hypothesis formation to resolve ambiguity
(e.g., ?Oh, so you mean that you had a good time??)
or requests for more information.
2.1 The task of handwashing
Our current work is based on a study completed by
Wilson et al (2012) towards a systematic observa-
tional representation of communication behaviours
of formal caregivers assisting individuals with mod-
erate to severe AD during hand washing. In that
study, caregivers produced 1691 utterances, 78% of
which contained at least one communication strat-
egy. On average, 23.35 (? = 14.11) verbal strate-
gies and 7.81 (? = 5.13) non-verbal strategies were
used per session. The five most common communi-
cation strategies employed by caregivers are ranked
in table 1. The one proposition strategy refers to
using a single direction, request, or idea in the utter-
ance (e.g. ?turn the water on?). The closed-ended
question strategy refers to asking question with a
very limited, typically binary, response (e.g., ?can
you turn the taps on??) as opposed to questions elic-
iting a more elaborate response or the inclusion of
additional information. The encouraging comments
strategy refers to any verbal praise of the resident
(e.g., ?you are doing a good job?). The paraphrased
repetition strategy is the restatement of a misunder-
stood utterance using alternative syntactic or lexical
content (e.g., ?soap up your hands....please use soap
on your hands?). There was no significant difference
between the use of paraphrased and verbatim repe-
tition of misunderstood utterances. Caregivers also
reduced speech rate from an average baseline of 116
words per minute (s.d. 36.8) to an average of 36.5
words per minute (s.d. 19.8).
The least frequently used communication strate-
49
Number of occurrences % use of strategy Uses per session
Verbal strategy Overall Successful Overall Successful Mean SD
One proposition 619 441 35 36 8.6 6.7
Closed-ended question 215 148 12 12 3.0 3.0
Encouraging comments 180 148 10 12 2.9 2.5
Use of resident?s name 178 131 10 11 2.8 2.5
Paraphrased repetition 178 122 10 10 3.0 2.5
Table 1: Most frequent verbal communication strategies according to their number of occurrences in dyad communi-
cation. The % use of strategy is normalized across all strategies, most of which are not listed. These results are split
according to the total number of uses and the number of uses in successful resolution of a communication breakdown.
Mean (and standard deviation) of uses per session are given across caregivers. Adapted with permission from Wilson
et al (2012).
gies employed by experienced caregivers involved
asking questions that required verification of a res-
ident?s request or response (e.g., ?do you mean
that you are finished??), explanation of current ac-
tions (e.g., ?I am turning on the taps for you?), and
open-ended questions (e.g., ?how do you wash your
hands??).
The most common non-verbal strategies em-
ployed by experienced caregivers were guided touch
(193 times, 122 of which were successful) in which
the caregiver physically assists the resident in the
completion of a task, demonstrating action (113
times. 72 of which were successful) in which an
action is illustrated or mimicked by the caregiver,
handing an object to the resident (107 times, 85 of
which were successful), and pointing to an object
(105 times, 95 of which were successful) in which
the direction to an object is visually indicated by
the caregiver. Some of these strategies may be em-
ployed by the proposed system; for example, videos
demonstrating an action may be displayed on the
screen shown in figure 1(a), which may replace to
some extent the mimicry by the caregiver. A pos-
sible replication of the fourth most common non-
verbal strategy may be to highlight the required ob-
ject with a flashing light, a spotlight, or by display-
ing it on screen; these solutions require tangential
technologies that are beyond the scope of this cur-
rent study, however.
3 Data
Our experiments are based on data collected by Wil-
son et al (submitted) with individuals diagnosed
with moderate-to-severe AD who were recruited
from long-term care facilities (i.e., The Harold and
Grace Baker Centre and the Lakeside Long-Term
Care Centre) in Toronto. Participants had no pre-
vious history of stroke, depression, psychosis, alco-
holism, drug abuse, or physical aggression towards
caregivers. Updated measures of disease severity
were taken according to the Mini-Mental State Ex-
amination (Folstein, Folstein, and McHugh, 1975).
The average cognitive impairment among 7 individ-
uals classified as having severe AD (scores below
10/30) was 3.43 (? = 3.36) and among 6 individ-
uals classified as having moderate AD (scores be-
tween 10/30 and 19/30) was 15.8 (? = 4.07). The
average age of residents was 81.4 years with an aver-
age of 13.8 years of education and 3.1 years of resi-
dency at their respective LTC facility. Fifteen formal
caregivers participated in this study and were paired
with the residents (i.e., as dyads) during the comple-
tion of activities of daily living. All but one care-
giver were female and were comfortable with En-
glish. The average number of years of experience
working with AD patients was 12.87 (? = 9.61).
The toothbrushing task follows the protocol of the
handwashing task. In total, the data consists of 336
utterances by the residents and 2623 utterances by
their caregivers; this is manifested by residents utter-
ing 1012 words and caregivers uttering 12166 words
in total, using 747 unique terms. The toothbrushing
task consists of 9 subtasks, namely: 1) get brush and
paste, 2) put paste on brush, 3) turn on water, 4) wet
tooth brush, 5) brush teeth, 6) rinse mouth, 7) rinse
brush, 8) turn off water, 9) dry mouth.
These data were recorded as part of a large
project to study communication strategies of care-
givers rather than to study the acoustics of their
transactions with residents. As a result, the record-
50
ings were not of the highest acoustic quality; for
example, although the sampling rate and bit rate
were high (48 kHz and 384 kbps respectively), the
video camera used was placed relatively far from the
speakers, who generally faced away from the mi-
crophone towards the sink and running water. The
distribution of strategies employed by caregivers for
this task is the subject of ongoing work.
4 Experiments in speech recognition
Our first component of an automated caregiver
is the speech recognition subsystem. We test
two alternative systems, namely Carnegie Mellon?s
Sphinx framework and Microsoft?s Speech Plat-
form. Carnegie Mellon?s Sphinx framework (pock-
etsphinx, specifically) is an open-source speech
recognition system that uses traditional N -gram
language modeling, sub-phonetic acoustic hidden
Markov models (HMMs), Viterbi decoding and
lexical-tree structures (Lamere et al, 2003). Sphinx
includes tools to perform traditional Baum-Welch
estimation of acoustic models, but there were not
enough data for this purpose. The second ASR sys-
tem, Microsoft?s Speech Platform (version 11) is
less open but exposes the ability to vary the lexicon,
grammar, and semantics. Traditionally, Microsoft
has used continuous-density HMMs with 6000 tied
HMM states (senones), 20 Gaussians per state, and
Mel-cepstrum features (with delta and delta-delta).
Given the toothbrushing data described in section
3, two sets of experiments were devised to config-
ure these systems to the task. Specifically, we per-
form preprocessing of the acoustics to remove envi-
ronmental noise associated with toothbrushing and
adapt the lexica of the two systems, as described in
the following subsections.
4.1 Noise reduction
An emergent feature of the toothbrushing data is
very high levels of acoustic noise caused by the
running of water. In fact, the estimated signal-to-
noise ratio across utterances range from ?2.103 dB
to 7.63 dB, which is extremely low; for comparison
clean speech typically has an SNR of approximately
40 dB. Since the resident is likely to be situated close
to this source of the acoustic noise, it becomes im-
portant to isolate their speech in the incoming signal.
Speech enhancement involves the removal of
acoustic noise d(t) in a signal y(t), including am-
bient noise (e.g., running water, wind) and signal
degradation giving the clean ?source? signal x(t).
This involves an assumption that noise is strictly ad-
ditive, as in the formula:
y(t) = x(t) + d(t). (1)
Here, Yk, Xk, and Dk are the kth spectra of the
noisy observation y(t), source signal x(t), and un-
correlated noise signal d(t), respectively. Generally,
the spectral magnitude of a signal is more important
than its phase when assessing signal quality and per-
forming speech enhancement. Spectral subtraction
(SS), as the name suggests, subtracts an estimate of
the noisy spectrum from the measured signal (Boll,
1979; Martin, 2001), where the estimate of the noisy
signal is estimated from samples of the noise source
exclusively. That is, one has to learn estimates based
on pre-selected recordings of noise. We apply SS
speech enhancement given sample recordings of wa-
ter running. The second method of enhancement
we consider is the log-spectral amplitude estimator
(LSAE) which minimizes the mean squared error
(MMSE) of the log spectra given a model for the
source speech Xk = Ak exp(j?k), where Ak is the
spectral amplitude. The LSAE method is a modifi-
cation to the short-time spectral amplitude estima-
tor that attempts to find some estimate A?k that min-
imizes the distortion
E
[(
logAk ? log A?k
)2
]
, (2)
such that the log-spectral amplitude estimate is
A?k = exp (E [lnAk |Yk])
=
?k
1 + ?k
exp
(
1
2
? ?
vk
e?t
t
dt
)
Rk,
(3)
where ?k is the a priori SNR,Rk is the noisy spectral
amplitude, vk =
?k
1+?k
?k, and ?k is the a posteriori
SNR (Erkelens, Jensen, and Heusdens, 2007). Of-
ten this is based on a Gaussian model of noise, as
it is here (Ephraim and Malah, 1985). We enhance
our recordings by both the SS and LSAE methods.
Archetypal instances of typical, low, and (relatively)
high SNR waveform recordings and their enhanced
versions are shown in 4.1.
51
(a) Dyad1.1
(b) Dyad4.2
(c) Dyad11.1
Figure 2: Representative samples of toothbrushing data
audio. Figures show normalized amplitude over time for
signals cleaned by the LSAE method overlaid over the
larger-amplitude original signals.
We compare the effects of this enhanced audio
across two ASR systems. For the Sphinx system,
we use a continuous tristate HMM for each of the 40
phones from the CMU dictionary trained with audio
from the complete Wall Street Journal corpus and
the independent variable we changed was the num-
ber of Gaussians per state (n. ?). These parame-
ters are not exposed by the Microsoft speech system,
so we instead vary the minimum threshold of confi-
dence C ? [0..1] required to accept a word; in theory
lower values of C would result in more insertion er-
rors and higher values would result in more deletion
errors. For each system, we used a common dic-
tionary of 123, 611 unique words derived from the
Carnegie Mellon phonemic dictionary.
Table 2 shows the word error rate for each of
the two systems. Both the SS and LSAE methods
of speech enhancement result in significantly better
word error rates than with the original recordings at
the 99.9% level of confidence according to the one-
tailed paired t-test across both systems. The LSAE
method has significantly better word error rates than
the SS method at the 99% level of confidence with
this test. Although these high WERs are impractical
for a typical system, they are comparable to other re-
sults for speech recognition in very low-SNR envi-
ronments (Kim and Rose, 2003). Deng et al (2000),
for example, describe an ASR system trained with
clean speech that has a WER of 87.11% given addi-
tive white noise for a resulting 5 dB SNR signal for
a comparable vocabulary of 5000 words. An inter-
esting observation is that even at the low confidence
threshold of C = 0.2, the number of insertion er-
rors did not increase dramatically relative to for the
higher values in the Microsoft system; only 4.0% of
all word errors were insertion errors at C = 0.2, and
2.7% of all word errors at C = 0.8.
Given Levenshtein alignments between annotated
target (reference) and hypothesis word sequences,
we separate word errors across residents and across
caregivers. Specifically, table 3 shows the propor-
tion of deletion and substitution word errors (relative
to totals for each system separately) across residents
and caregivers. This analysis aims to uncover dif-
ferences in rates of recognition between those with
AD and the more general population. For exam-
ple, 12.6% of deletion errors made by Sphinx were
words spoken by residents. It is not possible to at-
52
Word error rate %
Parameters Original SS LSAE
Sphinx
n. ? = 4 98.13 75.31 70.61
n. ? = 8 98.13 74.95 69.66
n. ? = 16 97.82 75.09 69.78
n. ? = 32 97.13 74.88 67.22
Microsoft
C = 0.8 97.67 73.59 67.11
C = 0.6 97.44 72.57 67.08
C = 0.4 96.85 71.78 66.54
C = 0.2 94.30 71.36 64.32
Table 2: Word error rates for the Sphinx and Microsoft
ASR systems according to their respective adjusted pa-
rameters, i.e., number of Gaussians per HMM state (n. ?)
and minimum confidence threshold (C). Results are given
on original recordings and waveforms enhanced by spec-
tral subraction (SS) and MMSE with log-spectral ampli-
tude estimates (LSAE).
tribute word insertion errors to either the resident or
caregiver, in general. If we assume that errors should
be distributed across residents and caregivers in the
same proportion as their respective total number of
words uttered, then we can compute the Pearson ?2
statistic of significance. Given that 7.68% of all
words were uttered by residents, the observed num-
ber of substitutions was significantly different than
the expected value at the 99% level of confidence
for both the Sphinx and Microsoft systems, but the
number of deletions was not significantly different
even at the 95% level of confidence. In either case,
however, substantially more errors are made propor-
tionally by residents than we might expect; this may
in part be caused by their relatively soft speech.
Proportion of errors
Sphinx Microsoft
Res. Careg. Res. Careg.
deletion 13.9 86.1 12.6 87.4
substitution 23.2 76.8 18.4 81.6
Table 3: Proportion of deletion and substitution errors
made by both (Res)idents and (Careg)ivers. Proportions
are relative to totals within each system.
4.2 Task-specific vocabulary
We limit the common vocabulary used in each
speech recognizer in order to be more specific to the
task. Specifically, we begin with the 747 words ut-
tered in the data as our most restricted vocabulary.
Then, we expand this vocabulary according to two
methods. The first method adds words that are se-
mantically similar to those already present. This
is performed by taking the most common sense for
each noun, verb, adjective, and adverb, then adding
each entry in the respective synonym sets accord-
ing to WordNet 3.0 (Miller, 1995). This results in
a vocabulary of 2890 words. At this point, we it-
eratively add increments of words at intervals of
10, 000 (up to 120, 000) by selecting random words
in the vocabulary and adding synonym sets for all
senses as well as antonyms, hypernyms, hyponyms,
meronyms, and holonyms. The result is a vocabu-
lary whose semantic domain becomes increasingly
generic. The second approach to adjusting the vo-
cabulary size is to add phonemic foils to more re-
stricted vocabularies. Specifically, as before, we be-
gin with the restricted 747 words observed in the
data but then add increments of new words that
are phonemically similar to existing words. This
is done exhaustively by selecting a random word
and searching for minimal phonemic misalignments
(i.e., edit distance) among out-of-vocabulary words
in the Carnegie Mellon phonemic dictionary. This
approach of adding decoy words is an attempt to
model increasing generalization of the systems. Ev-
ery vocabulary is translated into the format expected
by each recognizer so that each test involves a com-
mon set of words.
Word error rates are measured for each vocabu-
lary size across each ASR system and the manner in
which those vocabularies were constructed (seman-
tic or phonemic expansion). The results are shown
in figure 4.2 and are based on acoustics enhanced
by the LSAE method. Somewhat surprisingly, the
method used to alter the vocabulary did appear to
have a very large effect. Indeed, the WER across
the semantic and phonemic methods were correlated
at ? >= 0.99 across both ASR systems; there was
no significant difference between traces (within sys-
tem) even at the 60% level of confidence using the
two-tailed heteroscedastic t-test.
5 Ongoing work
This work represents the first phase of development
towards a complete communicative artificial care-
giver for the home. Here, we are focusing on the
53
102 103 104 105 10635
40
45
50
55
60
65
70
Vocabulary size
Wo
rd E
rror
 Ra
te (%
)
 
 
Sphinx ? Phonemic
Microsoft ? PhonemicSphinx ? Semantic
Microsoft ? Semantic
Figure 3: Word error rate versus size of vocabulary (log
scale) for each of the Sphinx and Microsoft ASR systems
according to whether the vocabularies were expanded by
semantic or phonemic similarity.
speech recognition component and have shown re-
ductions in error of up to 72% (Sphinx ASR with
n.? = 4) and 63.1% (Sphinx ASR), relative to base-
line rates of error. While significant, baseline er-
rors were so severe that other techniques will need
to be explored. We are now collecting additional
data by fixing the Microsoft Kinect sensor in the
environment, facing the resident; this is the default
configuration and may overcome some of the ob-
stacles present in our data. Specifically, the beam-
forming capabilities in the Kinect (generalizable to
other multi-microphone arrays) can isolate speech
events from ambient environmental noise (Balan and
Rosca, 2002). We are also collecting speech data for
a separate study in which individuals with AD are
placed before directional microphones and complete
tasks related to the perception of emotion.
As tasks can be broken down into non-linear (par-
tially ordered) sets of subtasks (e.g., replacing the
toothbrush is a subtask of toothbrushing), we are
specifying grammars ?by hand? specific to those sub-
tasks. Only some subset of all subtasks are possible
at any given time; e.g., one can only place tooth-
paste on the brush once both items have been re-
trieved. The possibility of these subtasks depend on
the state of the world which can only be estimated
through imperfect techniques ? typically computer
vision. Given the uncertainty of the state of the
world, we are integrating subtask-specific grammars
into a partially-observable Markov decision process
(POMDP). These grammars include the semantic
state variables of the world and break each task
down into a graph-structure of interdependent ac-
tions. Each ?action? is associated with its own gram-
mar subset of words and phrases that are likely to
be uttered during its performance, as well as a set
of prompts to be spoken by the system to aid the
user. Along these lines, we we will attempt to gen-
eralize the approach taken in section 4.2 to gener-
ate specific sub-vocabularies automatically for each
subtask. The relative weighting of words will be
modeled based on ongoing data collection.
Acknowledgments
This research was partially funded by Mitacs and
an operating grant from the Canadian Institutes of
Health Research and the American Alzheimer As-
sociation (ETAC program). The authors acknowl-
edge and thank the administrative staff, caregivers,
and residents at the Harold and Grace Baker Centre
and the Lakeside Long-Term Care Centre.
References
Balan, Radu and Justinian Rosca. 2002. Microphone
Array Speech Enhancement by Bayesian Estimation
of Spectral Amplitude and Phase. In Proceedings of
IEEE Sensor Array and Multichannel Signal Process-
ing Workshop.
Bharucha, Ashok J., Vivek Anand, Jodi Forlizzi,
Mary Amanda Dew, Charles F. Reynolds III, Scott
Stevens, and Howard Wactlar. 2009. Intelligent assis-
tive technology applications to dementia care: Current
capabilities, limitations, and future challenges. Amer-
ican Journal of Geriatric Psychiatry, 17(2):88?104,
February.
Boll, S.F. 1979. Suppression of acoustic noise in speech
using spectral subtraction. IEEE Transactions on
Acoustics, Speech, and Signal Processing, 27(2):113?
120, April.
Cummings, Jeffrey L. 2004. Alzheimer?s disease. New
England Journal of Medicine, 351(1):56?67.
Deng, Li, Alex Acero, M. Plumpe, and Xuedong Huang.
2000. Large-vocabulary speech recognition under ad-
verse acoustic environments. In Proceedings of the In-
ternational Conference on Spoken Language Process-
ing, October.
54
Ephraim, Y. and D. Malah. 1985. Speech enhancement
using a minimum mean-square error log-spectral am-
plitude estimator. Acoustics, Speech and Signal Pro-
cessing, IEEE Transactions on, 33(2):443 ? 445, apr.
Erkelens, Jan, Jesper Jensen, and Richard Heusdens.
2007. A data-driven approach to optimizing spectral
speech enhancement methods for various error crite-
ria. Speech Communication, 49:530?541.
Ernst, Richard L., Joel W. Hay, Catharine Fenn, Jared
Tinklenberg, and Jerome A. Yesavage. 1997. Cog-
nitive function and the costs of alzheimer disease ?
an exploratory study. Archives of Neurology, 54:687?
693.
Folstein, Marshal F., Susan E. Folstein, and Paul R.
McHugh. 1975. Mini-mental state: A practical
method for grading the cognitive state of patients
for the clinician. Journal of Psychiatric Research,
12(3):189?198, November.
Gaugler, J. E., F. Yu, K. Krichbaum, and J.F. Wyman.
2009. Predictors of nursing home admission for per-
sons with dementia. Medical Care, 47(2):191?198.
Goldfarb, R. and M.J.S. Pietro. 2004. Support sys-
tems: Older adults with neurogenic communication
disorders. Journal of Ambulatory Care Management,
27(4):356?365.
Hopper, T. 2001. Indirect interventions to facilitate com-
munication in Alzheimers disease. Seminars in Speech
and Language, 22(4):305?315.
Kim, Hong Kook and Richard C. Rose. 2003. Cepstrum-
Domain Acoustic Feature Compensation Based on De-
composition of Speech and Noise for ASR in Noisy
Environments. IEEE Transactions on Speech and Au-
dio Processing, 11(5), September.
Lamere, Paul, Philip Kwok, Evandro Gouvea, Bhiksha
Raj, Rita Singh, William Walker, M. Warmuth, and
Peter Wolf. 2003. The CMU Sphinx-4 speech recog-
nition system. In IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP
2003), Hong Kong, April.
Martin, Rainer. 2001. Noise power spectral density es-
timation based on optimal smoothing and minimum
statistics. IEEE Transactions of Speech and Audio
Processing, 9(5):504?512, July.
Mihailidis, Alex, Jennifer N Boger, Tammy Craig, and
Jesse Hoey. 2008. The COACH prompting system to
assist older adults with dementia through handwash-
ing: An efficacy study. BMC Geriatrics, 8(28).
Miller, George A. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39?
41.
Orange, J.B., Rosemary B. Lubinsky, and D. Jeffery Hig-
ginbotham. 1996. Conversational repair by individu-
als with dementia of the alzheimer?s type. Journal of
Speech and Hearing Research, 39:881?895, August.
Rochon, Elizabeth, Gloria S. Waters, and David Caplan.
2000. The Relationship Between Measures of Work-
ing Memory and Sentence Comprehension in Patients
With Alzheimer?s Disease. Journal of Speech, Lan-
guage, and Hearing Research, 43:395?413.
Saini, Privender, Boris de Ruyter, Panos Markopoulos,
and Albert van Breemen. 2005. Benefits of social in-
telligence in home dialogue systems. In Proceedings
of INTERACT 2005, pages 510?521.
Schegloff, Emanuel A., Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. 1977,
53(2):361?382.
Small, Jeff A., Elaine S. Andersen, and Daniel Kempler.
1997. Effects of working memory capacity on under-
standing rate-altered speech. Aging, Neuropsychology,
and Cognition, 4(2):126?139.
Small, Jeff A., Gloria Gutman, Saskia Makela, and
Beth Hillhouse. 2003. Effectiveness of communi-
cation strategies used by caregivers of persons with
alzheimer?s disease during activities of daily living.
Journal of Speech, Language, and Hearing Research,
46(2):353?367.
Spanoudakis, Nikolaos, Boris Grabner, Christina Kot-
siopoulou, Olga Lymperopoulou, Verena Moser-
Siegmeth, Stylianos Pantelopoulos, Paraskevi Sakka,
and Pavlos Moraitis. 2010. A novel architecture and
process for ambient assisted living - the hera approach.
In Proceedings of the 10th IEEE International Confer-
ence on Information Technology and Applications in
Biomedicine (ITAB), pages 1?4.
Tomoeda, Cheryl K., Kathryn A. Bayles, Daniel R.
Boone, Alfred W. Kaszniak, and Thomas J. Slauson.
1990. Speech rate and syntactic complexity effects
on the auditory comprehension of alzheimer patients.
Journal of Communication Disorders, 23(2):151 ?
161.
Watson, Caroline M. 1999. An analysis of trou-
ble and repair in the natural conversations of people
with dementia of the Alzheimer?s type. Aphasiology,
13(3):195 ? 218.
Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,
and Carol Le?onard. 2012. Examining success of com-
munication strategies used by formal caregivers assist-
ing individuals with alzheimer?s disease during an ac-
tivity of daily living. Journal of Speech, Language,
and Hearing Research, 55:328?341.
Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,
and Carol Le?onard. submitted. Quantitative analy-
sis of formal caregivers? use of communication strate-
gies while assisting individuals with moderate and se-
vere alzheimer?s disease during oral care. Journal of
Speech, Language, and Hearing Research.
55
Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 20?28,
Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational Linguistics
Speech recognition in Alzheimer?s disease with personal assistive robots
Frank Rudzicz1,2,? and Rosalie Wang1 and Momotaz Begum3 and Alex Mihailidis2,1
1 Toronto Rehabilitation Institute, Toronto ON; 2 University of Toronto, Toronto ON;
3 University of Massachussetts Lowell
?frank@cs.toronto.edu
Abstract
To help individuals with Alzheimer?s dis-
ease live at home for longer, we are de-
veloping a mobile robotic platform, called
ED, intended to be used as a personal care-
giver to help with the performance of ac-
tivities of daily living. In a series of ex-
periments, we study speech-based inter-
actions between each of 10 older adults
with Alzheimers disease and ED as the
former makes tea in a simulated home en-
vironment. Analysis reveals that speech
recognition remains a challenge for this
recording environment, with word-level
accuracies between 5.8% and 19.2% dur-
ing household tasks with individuals with
Alzheimer?s disease. This work provides a
baseline assessment for the types of tech-
nical and communicative challenges that
will need to be overcome in human-robot
interaction for this population.
1 Introduction
Alzheimer?s disease (AD) is a progressive neu-
rodegenerative disorder primarily impairing mem-
ory, followed by declines in language, ability to
carry out motor tasks, object recognition, and ex-
ecutive functioning (American Psychiatric Asso-
ciation, 2000; Gauthier et al., 1997). An accu-
rate measure of functional decline comes from
performance in activities of daily living (ADLs),
such as shopping, finances, housework, and self-
care tasks. The deterioration in language com-
prehension and/or production resulting from spe-
cific brain damage, also known as aphasia, is a
common feature of AD and other related con-
ditions. Language changes observed clinically
in older adults with dementia include increasing
word-finding difficulties, loss of ability to verbally
express information in detail, increasing use of
generic references (e.g., ?it?), and progressing dif-
ficulties understanding information presented ver-
bally (American Psychiatric Association, 2000).
Many nations are facing healthcare crises in the
lack of capacity to support rapidly aging popula-
tions nor the chronic conditions associated with
aging, including dementia. The current healthcare
model of removing older adults from their homes
and placing them into long-term care facilities
is neither financially sustainable in this scenario
(Bharucha et al., 2009), nor is it desirable. Our
team has been developing ?smart home? systems
at the Toronto Rehabilitation Institute (TRI, part
of the University Health Network) to help older
adults ?age-in-place? by providing different types
of support, such as step-by-step prompts for daily
tasks (Mihailidis et al., 2008), responses to emer-
gency situations (Lee and Mihaildis, 2005), and
means to communicate with family and friends.
These systems are being evaluated within a com-
pletely functional re-creation of a one-bedroom
apartment located within The TRI hospital, called
HomeLab. These smart home technologies use
advanced sensing techniques and machine learn-
ing to autonomously react to their users, but they
are fixed and embedded into the environment, e.g.,
as cameras in the ceiling. Fixing the location of
these technologies carries a tradeoff between util-
ity and feasibility ? installing multiple hardware
units at all locations where assistance could be re-
quired (e.g., bathroom, kitchen, and bedroom) can
be expensive and cumbersome, but installing too
few units will present gaps where a user?s activ-
ity will not be detected. Alternatively, integrat-
ing personal mobile robots with smart homes can
overcome some of these tradeoffs. Moreover, as-
sistance provided via a physically embodied robot
is often more acceptable than that provided by an
embedded system (Klemmer et al., 2006).
With these potential advantages in mind, we
conducted a ?Wizard-of-Oz? study to explore the20
feasibility and usability of a mobile assistive robot
that uses the step-by-step prompting approaches
for daily activities originally applied to our smart
home research (Mihailidis et al., 2008). We con-
ducted the study with older adults with mild or
moderate AD and the tasks of hand washing and
tea making. Our preliminary data analysis showed
that the participants reacted well to the robot itself
and the prompts that it provided, suggesting the
feasibility of using personal robots for this appli-
cation (Begum et al., 2013). One important iden-
tified issue is the need for an automatic speech
recognition system to detect and understand ut-
terances specifically from older adults with AD.
The development of such a system will enable
the assistive robot to better understand the be-
haviours and needs of these users for effective in-
teractions and will further enhance environmental-
based smart home systems.
This paper presents an analysis of the speech
data collected from our participants with AD when
interacting with the robot. In a series of exper-
iments, we measure the performance of modern
speech recognition with this population and with
their younger caregivers with and without signal
preprocessing. This work will serve as the basis
for further studies by identifying some of the de-
velopment needs of a speech-based interface for
robotic caregivers for older adults with AD.
2 Related Work
Research in smart home systems, assistive robots,
and integrated robot/smart home systems for older
adults with cognitive impairments has often fo-
cused on assistance with activities of daily living
(i.e., reminders to do specific activities according
to a schedule or prompts to perform activity steps),
cognitive and social stimulation and emergency
response systems. Archipel (Serna et al., 2007)
recognizes the user?s intended plan and provides
prompts, e.g. with cooking tasks. Autominder,
(Pollack, 2006), provides context-appropriate re-
minders for activity schedules, and the COACH
(Cognitive Orthosis for Assisting with aCtivities
in the Home) system prompts for the task of hand-
washing (Mihailidis et al., 2008) and tea-making
(Olivier et al., 2009). Mynatt et al. (2004) have
been developing technologies to support aging-in-
place such as the Cooks Collage, which uses a se-
ries of photos to remind the user what the last step
completed was if the user is interrupted during a
cooking task. These interventions tend to be em-
bedded in existing environments (e.g., around the
sink area).
More recent innovations have examined in-
tegrated robot-smart home systems where sys-
tems are embedded into existing environments that
communicate with mobile assistive robots (e.g.,
CompanionAble, (Mouad et al., 2010); Mobiserv
Kompai, (Lucet, 2012); and ROBADOM (Tapus
and Chetouani, 2010)). Many of these projects
are targeted towards older adults with cognitive
impairment, and not specifically those with sig-
nificant cognitive impairment. One of these sys-
tems, CompanionAble, with a fully autonomous
assistive robot, has recently been tested in a simu-
lated home environment for two days each with
four older adults with dementia (AD or Pick?s
disease/frontal lobe dementia) and two with mild
cognitive impairment. The system provides assis-
tance with various activities, including appoint-
ment reminders for activities input by users or
caregivers, video calls, and cognitive exercises.
Participants reported an overall acceptance of the
system and several upgrades were reported, in-
cluding a speech recognition system that had to be
deactivated by the second day due to poor perfor-
mance.
One critical component for the successful use of
these technological interventions is the usability of
the communication interface for the targeted users,
in this case older adults with Alzheimer?s disease.
As in communication between two people, com-
munication between the older adult and the robot
may include natural, freeform speech (as opposed
to simple spoken keyword interaction) and non-
verbal cues (e.g., hand gestures, head pose, eye
gaze, facial feature cues), although speech tends to
be far more effective (Green et al., 2008; Goodrich
and Schultz, 2007). Previous research indicates
that automated communication systems are more
effective if they take into account the affective
and mental states of the user (Saini et al., 2005).
Indeed, speech appears to be the most powerful
mode of communication for an assistive robot to
communicate with its users (Tapus and Chetouani,
2010; Lucet, 2012).
2.1 Language use in dementia and
Alzheimer?s disease
In order to design a speech interface for individ-
uals with dementia, and AD in particular, it is21
important to understand how their speech differs
from that of the general population. This then can
be integrated into future automatic speech recog-
nition systems. Guinn and Habash (2012) showed,
through an analysis of conversational dialogs, that
repetition, incomplete words, and paraphrasing
were significant indicators of Alzheimer?s dis-
ease relative but several expected measures such
as filler phrases, syllables per minute, and pro-
noun rate were not. Indeed, pauses, fillers, for-
mulaic speech, restarts, and speech disfluencies
are all hallmarks of speech in individuals with
Alzheimer?s (Davis and Maclagan, 2009; Snover
et al., 2004). Effects of Alzheimer?s disease on
syntax remains controversial, with some evidence
that deficits in syntax or of agrammatism could be
due to memory deficits in the disease (Reilly et al.,
2011).
Other studies has applied similar analyses to
related clinical groups. Pakhomov et al. (2010)
identified several different features from the au-
dio and corresponding transcripts of 38 patients
with frontotemporal lobar degeneration (FTLD).
They found that pause-to-word ratio and pronoun-
to-noun ratios were especially discriminative of
FTLD variants and that length, hesitancy, and
agramatism correspond to the phenomenology of
FTLD. Roark et al. (2011) tested the ability of an
automated classifier to distinguish patients with
mild cognitive impairment from healthy controls
that include acoustic features such as pause fre-
quency and duration.
2.2 Human-robot interaction
Receiving assistance from an entity with a physi-
cal body (such as a robot) is often psychologically
more acceptable than receiving assistance from an
entity without a physical body (such as an em-
bedded system) (Klemmer et al., 2006). Physical
embodiment also opens up the possibility of hav-
ing more meaningful interaction between the older
adult and the robot, as discussed in Section 5.
Social collaboration between humans and
robots often depends on communication in which
each participant?s intention and goals are clear
(Freedy et al., 2007; Bauer et al., 2008; Green
et al., 2008). It is important that the human
participant is able to construct a useable ?men-
tal model? of the robot through bidirectional com-
munication (Burke and Murphy, 1999) which can
include both natural speech and non-verbal cues
(e.g., hand gestures, gaze, facial cues), although
speech tends to be far more effective (Green et al.,
2008; Goodrich and Schultz, 2007).
Automated communicative systems that are
more sensitive to the emotive and the mental states
of their users are often more successful than more
neutral conversational agents (Saini et al., 2005).
In order to be useful in practice, these commu-
nicative systems need to mimic some of the tech-
niques employed by caregivers of individuals with
AD. Often, these caregivers are employed by lo-
cal clinics or medical institutions and are trained
by those institutions in ideal verbal communica-
tion strategies for use with those having demen-
tia (Hopper, 2001; Goldfarb and Pietro, 2004).
These include (Wilson et al., 2012) but are not
limited to relatively slow rate of speech, verba-
tim repetition of misunderstood prompts, closed-
ended (e.g., ?yes/no?) questions, and reduced syn-
tactic complexity. However, Tomoeda et al. (1990)
showed that rates of speech that are too slow
may interfere with comprehension if they intro-
duce problems of short-term retention of working
memory. Small et al. (1997) showed that para-
phrased repetition is just as effective as verbatim
repetition (indeed, syntactic variation of common
semantics may assist comprehension). Further-
more, Rochon et al. (2000) suggested that the syn-
tactic complexity of utterances is not necessarily
the only predictor of comprehension in individuals
with AD; rather, correct comprehension of the se-
mantics of sentences is inversely related to the in-
creasing number of propositions used ? it is prefer-
able to have as few clauses or core ideas as possi-
ble, i.e., one-at-a-time.
3 Data collection
The data in this paper come from a study to
examine the feasibility and usability of a per-
sonal assistive robot to assist older adults with
AD in the completion of daily activities (Begum
et al., 2013). Ten older adults diagnosed with
AD, aged ? 55, and their caregivers were re-
cruited from a local memory clinic in Toronto,
Canada. Ethics approval was received from the
Toronto Rehabilitation Institute and the Univer-
sity of Toronto. Inclusion criteria included fluency
in English, normal hearing, and difficulty com-
pleting common sequences of steps, according to
their caregivers. Caregivers had to be a family
or privately-hired caregiver who provides regular22
care (e.g., 7 hours/week) to the older adult partici-
pant. Following informed consent, the older adult
participants were screened using the Mini Mental
State Exam (MMSE) (Folstein et al., 2001) to as-
certain their general level of cognitive impairment.
Table 1 summarizes relevant demographics.
Sex Age (years) MMSE (/30)
OA1 F 76 9
OA2 M 86 24
OA3 M 88 25
OA4 F 77 25
OA5 F 59 18
OA6 M 63 23
OA7 F 77 25
OA8 F 83 19
OA9 F 84 25
OA10 M 85 15
Table 1: Demographics of older adults (OA).
(a)
(b)
Figure 1: ED and two participants with AD during
the tea-making task in the kitchen of HomeLab at
TRI.
3.1 ED, the personal caregiver robot
The robot was built on an iRobot base (operat-
ing speed: 28 cm/second) and both its internal
construction and external enclosure were designed
and built at TRI. It is 102 cm in height and has
separate body and head components; the latter is
primarily a LCD monitor that shows audiovisual
prompts or displays a simple ?smiley face? other-
wise, as shown in Figure 2. The robot has two
speakers embedded in its ?chest?, two video cam-
eras (one in the head and one near the floor, for
navigation), and a microphone. For this study,
the built-in microphones were not used in favor of
environmental Kinect microphones, discussed be-
low. This was done to account for situations when
the robot and human participant were not in the
same room simultaneously.
The robot was tele-operated throughout the
task. The tele-operator continuously monitored
the task progress and the overall affective state
of the participants in a video stream sent by the
robot and triggered social conversation, asked
task-related questions, and delivered prompts to
guide the participants towards successful comple-
tion of the tea-making task (Fig. 1).
Figure 2: The prototype robotic caregiver, ED.
The robot used the Cepstral commercial text-to-
speech (TTS) system using the U.S. English voice
?David? and its default parameters. This system
is based on the Festival text-to-speech platform in
many respects, including its use of linguistic pre-
processing (e.g., part-of-speech tagging) and cer-
tain heuristics (e.g., letter-to-sound rules). Spo-
ken prompts consisted of simple sentences, some-
times accompanied by short video demonstrations
designed to be easy to follow by people with a cog-
nitive impairment.
For efficient prompting, the tea-making task
was broken down into different steps or sub-task.
Audio or audio-video prompts corresponding to23
each of these sub-tasks were recorded prior to
data collection. The human-robot interaction pro-
ceeded according to the following script when col-
laborating with the participants:
1. Allow the participant to initiate steps in each
sub-task, if they wish.
2. If a participant asks for directions, deliver the
appropriate prompt.
3. If a participant requests to perform the sub-
task in their own manner, agree if this does
not involve skipping an essential step.
4. If a participant asks about the location of an
item specific to the task, provide a full-body
gesture by physically orienting the robot to-
wards the sought item.
5. During water boiling, ask the participant to
put sugar or milk or tea bag in the cup. Time
permitting, engage in a social conversation,
e.g., about the weather.
6. When no prerecorded prompt sufficiently an-
swers a participant question, respond with the
correct answer (or ?I don?t know?) through
the TTS engine.
3.2 Study set-up and procedures
Consent included recording video, audio, and
depth images with the Microsoft Kinect sensor in
HomeLab for all interviews and interactions with
ED. Following informed consent, older adults and
their caregivers were interviewed to acquire back-
ground information regarding their daily activi-
ties, the set-up of their home environment, and the
types of assistance that the caregiver typically pro-
vided for the older adult.
Participants were asked to observe ED mov-
ing in HomeLab and older adult participants were
asked to have a brief conversation with ED to
become oriented with the robot?s movement and
speech characteristics. The older adults were
then asked to complete the hand-washing and tea-
making tasks in the bathroom and kitchen, respec-
tively, with ED guiding them to the locations and
providing specific step-by-step prompts, as neces-
sary. The tele-operator observed the progress of
the task, and delivered the pre-recorded prompts
corresponding to the task step to guide the older
adult to complete each task. The TTS system
was used to respond to task-related questions and
to engage in social conversation. The caregivers
were asked to observe the two tasks and to in-
tervene only if necessary (e.g., if the older adult
showed signs of distress or discomfort). The
older adult and caregiver participants were then
interviewed separately to gain their feedback on
the feasibility of using such a robot for assis-
tance with daily activities and usability of the sys-
tem. Each study session lasted approximately 2.5
hours including consent, introduction to the robot,
tea-making interaction with the robot, and post-
interaction interviews. The average duration for
the tea-making task alone was 12 minutes.
4 Experiments and analysis
Automatic speech recognition given these data is
complicated by several factors, including a pre-
ponderance of utterances in which human care-
givers speak concurrently with the participants, as
well as inordinately challenging levels of noise.
The estimated signal-to-noise ratio (SNR) across
utterances range from?3.42 dB to 8.14 dB, which
is extremely low compared to typical SNR of 40
dB in clean speech. One cause of this low SNR
is that microphones are placed in the environment,
rather than on the robot (so the distance to the mi-
crophone is variable, but relatively large) and that
the participant often has their back turned to the
microphone, as shown in figure 1.
As in previous work (Rudzicz et al., 2012),
we enhance speech signals with the log-spectral
amplitude estimator (LSAE) which minimizes the
mean squared error of the log spectra given a
model for the source speech Xk = Ake(j?k),
where Ak is the spectral amplitude. The LSAE
method is a modification of the short-time spectral
amplitude estimator that finds an estimate of the
spectral amplitude, A?k, that minimizes the distor-
tion
E
[(
logAk ? log A?k
)2]
, (1)
such that the log-spectral amplitude estimate is
A?k = exp (E [lnAk |Yk])
= ?k1 + ?k
exp
(1
2
? ?
vk
e?t
t dt
)
Rk,
(2)
where ?k is the a priori SNR,Rk is the noisy spec-
tral amplitude, vk = ?k1+?k ?k, and ?k is the a pos-teriori SNR (Erkelens et al., 2007). Often this is
based on a Gaussian model of noise, as it is here
(Ephraim and Malah, 1985).24
As mentioned, there are many utterances in
which human caregivers speak concurrently with
the participants. This is partially confounded by
the fact that utterances by individuals with AD
tend to be shorter, so more of their utterance is lost,
proportionally. Examples of this type where the
caregiver?s voice is louder than the participant?s
voice are discarded, amounting to about 10% of
all utterances. In the following analyses, func-
tion words (i.e., prepositions, subordinating con-
junctions, and determiners) are removed from con-
sideration, although interjections are kept. Proper
names are also omitted.
We use the HTK (Young et al., 2006) toolchain,
which provides an implementation of a semi-
continuous hidden Markov model (HMM) that al-
lows state-tying and represents output densities by
mixtures of Gaussians. Features consisted of the
first 13 Mel-frequency cepstral coefficients, their
first (?) and second (??) derivatives, and the log
energy component, for 42 dimensions. Our own
data were z-scaled regardless of whether LSAE
noise reduction was applied.
Two language models (LMs) are used, both tri-
gram models derived from the English Gigaword
corpus, which contains 1200 word tokens (Graff
and Cieri, 2003). The first LM uses the first 5000
most frequent words and the second uses the first
64,000 most frequent words of that corpus. Five
acoustic models (AMs) are used with 1, 2, 4, 8,
and 16 Gaussians per output density respectively.
These are trained with approximately 211 hours
of spoken transcripts of the Wall Street Journal
(WSJ) from over one hundred non-pathological
speakers (Vertanen, 2006).
Table 2 shows, for the small- and large-
vocabulary LMs, the word-level accuracies of the
baseline HTK ASR system, as determined by
the inverse of the Levenshtein edit distance, for
two scenarios (sit-down interviews vs. during
the task), with and without LSAE noise reduc-
tion, for speech from individuals with AD and
for their caregivers. These values are computed
over all complexities of acoustic model and are
consistent with other tasks of this type (i.e., with
the challenges associated with the population and
recording set up), with this type of relatively un-
constrained ASR (Rudzicz et al., 2012). Apply-
ing LSAE results in a significant increase in ac-
curacy for both the small-vocabulary (right-tailed
homoscedastic t(58) = 3.9, p < 0.005, CI =
[6.19,?]) and large-vocabulary (right-tailed ho-
moscedastic t(58) = 2.4, p < 0.01, CI =
[2.58,?]) tasks. For the participants with AD,
ASR accuracy is significantly higher in inter-
views (paired t(39) = 8.7, p < 0.0001, CI =
[13.8,?]), which is expected due in large part
to the closer proximity of the microphone. Sur-
prisingly, ASR accuracy on participants with ASR
was not significantly different than on caregivers
(two-tailed heteroscedastic t(78) = ?0.32, p =
0.75, CI = [?5.54, 4.0]).
Figure 3 shows the mean ASR accuracy, with
standard error (?/?n), for each of the small-
vocabulary and large-vocabulary ASR systems.
The exponential function b0 + b1 exp(b2x) is fit
to these data for each set, where bi are coef-
ficients that are iteratively adjustable via mean
squared error. For the small-vocabulary data,
R2 = 0.277 and F8 = 3.06, p = 0.12 ver-
sus the constant model. For the large-vocabulary
data, R2 = 0.445 and F8 = 2.81, p = 0.13
versus the constant model. Clearly, there is an
increasing trend in ASR accuracy with MMSE
scores, however an n-way ANOVA on ASR ac-
curacy scores reveals that this increase is not sig-
nificant (F1 = 47.07, p = 0.164). Furthermore,
neither the age (F1 = 1.39, p = 0.247) nor the sex
(F1 = 0.98, p = 0.33) of the participant had a sig-
nificant effect on ASR accuracy. An additional n-
way ANOVA reveals no strong interaction effects
between age, sex, and MMSE.
8 10 12 14 16 18 20 22 24 2610
15
20
25
30
35
MMSE score
AS
R a
ccu
racy
 (%)
 
 Small vocab
Large vocab
Figure 3: MMSE score versus mean ASR accu-
racy (with std. error bars) and fits of exponential
regression for each of the small-vocabulary and
large-vocabulary ASR systems.
25
Scenario Noise reduction AD caregiver
Small vocabulary
Interview None 25.1 (? = 9.9) 28.8 (? = 6.0)LSAE 40.9 (? = 5.6) 40.2 (? = 5.3)
In task None 13.7 (? = 3.7) -LSAE 19.2 (? = 9.8) -
Large vocabulary
Interview None 23.7 (? = 12.9) 27.0 (? = 10.0)LSAE 38.2 (? = 6.3) 35.1 (? = 11.2)
In task None 5.8 (? = 3.7) -LSAE 14.3 (? = 12.8) -
Table 2: ASR accuracy (means, and std. dev.) across speakers, scenario (interviews vs. during the task),
and presence of noise reduction for the small and large language models.
5 Discussion
This study examined low-level aspects of speech
recognition among older adults with Alzheimer?s
disease interacting with a robot in a simulated
home environment. The best word-level accura-
cies of 40.9% (? = 5.6) and 39.2% (? = 6.3)
achievable with noise reduction and in a quiet in-
terview setting are comparable with the state-of-
the-art in unrestricted large-vocabulary text entry.
These results form the basis for ongoing work in
ASR and interaction design for this domain. The
trigram language model used in this work encap-
sulates the statistics of a large amount of speech
from the general population ? it is a speaker-
independent model derived from a combination
of English news agencies that is not necessarily
representative of the type of language used in the
home, or by our target population. The acoustic
models were also derived from newswire data read
by younger adults in quiet environments. We are
currently training and adapting language models
tuned specifically to older adults with Alzheimer?s
disease using data from the Carolina Conversa-
tions database (Pope and Davis, 2011) and the De-
mentiaBank database (Boller and Becker, 1983).
Additionally, to function realistically, a lot of
ambient and background noise will need to be
overcome. We are currently looking into deploy-
ing a sensor network in the HomeLab that will in-
clude microphone arrays. Another method of im-
proving rates of correct word recognition is to aug-
ment the process from redundant information from
a concurrent sensory stream, i.e., in multimodal
interaction (Rudzicz, 2006). Combining gesture
and eye gaze with speech, for example, can be
used to disambiguate speech-only signals.
Although a focus of this paper, verbal infor-
mation is not the only modality in which human-
robot interaction can take place. Indeed, Wil-
son et al. (2012) showed that experienced human
caregivers employed various non-verbal and semi-
verbal strategies to assist older adults with demen-
tia about 1/3 as often as verbal strategies (see sec-
tion 2.2). These non-verbal and semi-verbal strate-
gies included eye contact, sitting face-to-face, us-
ing hand gestures, a calm tone of voice, instru-
mental touch, exaggerated facial expressions, and
moving slowly. Multi-modal communication can
be extremely important for individuals with de-
mentia, who may require redundant channels for
disambiguating communication problems, espe-
cially if they have a language impairment or a sig-
nificant hearing impairment.
It is vital that our current technological ap-
proaches to caring for the elderly in their homes
progresses quickly, given the demographic shift
in many nations worldwide. This paper provides
a baseline assessment for the types of technical
and communicative challenges that will need to be
overcome in the near future to provide caregiving
assistance to a growing number of older adults.
6 Acknowledgements
The authors would like to thank Rajibul Huq and
Colin Harry, who designed and built the robot,
Jennifer Boger and Goldie Nejat for their assis-
tance in designing the study, and Sharon Cohen
for her consultations during the study.
References
American Psychiatric Association. 2000. Delirium,
dementia, and amnestic and other cognitive disor-
ders. In Diagnostic and Statistical Manual of Men-
tal Disorders, Text Revision (DSM-IV-TR), chap-
ter 2. American Psychiatric Association, Arlington,
VA, fourth edition.26
A. Bauer, D. Wollherr, and M. Buss. 2008. Human-
robot collaboration: A survey. International Journal
of Humanoid Robotics, 5:47?66.
Momotaz Begum, Rosalie Wang, Rajibul Huq, and
Alex Mihailidis. 2013. Performance of daily ac-
tivities by older adults with dementia: The role of
an assistive robot. In Proceedings of the IEEE In-
ternational Conference on Rehabilitation Robotics,
Washington USA, June.
Ashok J. Bharucha, Vivek Anand, Jodi Forlizzi,
Mary Amanda Dew, Charles F. Reynolds III, Scott
Stevens, and Howard Wactlar. 2009. Intelligent
assistive technology applications to dementia care:
Current capabilities, limitations, and future chal-
lenges. American Journal of Geriatric Psychiatry,
17(2):88?104, February.
Franc?ois Boller and James Becker. 1983. Dementia-
Bank database.
J.L. Burke and R.R. Murphy. 1999. Situation
awareness, team communication, and task perfor-
mance in robot-assisted technical search: Bujold
goes to bridgeport. CMPSCI Tech. Rep. CRASAR-
TR2004-23, University of South Florida.
B. Davis and M. Maclagan. 2009. Examining
pauses in Alzheimer?s discourse. American jour-
nal of Alzheimer?s Disease and other dementias,
24(2):141?154.
Y. Ephraim and D. Malah. 1985. Speech enhancement
using a minimum mean-square error log-spectral
amplitude estimator. Acoustics, Speech and Signal
Processing, IEEE Transactions on, 33(2):443 ? 445,
apr.
Jan Erkelens, Jesper Jensen, and Richard Heusdens.
2007. A data-driven approach to optimizing spec-
tral speech enhancement methods for various error
criteria. Speech Communication, 49:530?541.
M. F. Folstein, S. E. Folstein, T. White, and M. A.
Messer. 2001. Mini-Mental State Examination
user?s guide. Odessa (FL): Psychological Assess-
ment Resources.
A. Freedy, E. de Visser, G. Weltman, and N. Coeyman.
2007. Measurement of trust in human-robot collab-
oration. In Proceedings of International Conference
on Collaborative Technologies and Systems, pages
17 ?24.
Serge Gauthier, Michel Panisset, Josephine Nalban-
toglu, and Judes Poirier. 1997. Alzheimer?s dis-
ease: current knowledge, management and research.
Canadian Medical Association Journal, 157:1047?
1052.
R. Goldfarb and M.J.S. Pietro. 2004. Support systems:
Older adults with neurogenic communication dis-
orders. Journal of Ambulatory Care Management,
27(4):356?365.
M. A. Goodrich and A. C. Schultz. 2007. Human-
robot interaction: A survey. Foundations and Trends
in Human-Computer Interaction, 1:203?275.
David Graff and Christopher Cieri. 2003. English gi-
gaword. Linguistic Data Consortium.
S. A. Green, M. Billinghurst, X. Chen, and J. G. Chase.
2008. Human-robot collaboration: A literature re-
view and augmented reality approach in design. In-
ternational Journal Advanced Robotic Systems, 5:1?
18.
Curry Guinn and Anthony Habash. 2012. Technical
Report FS-12-01, Association for the Advancement
of Artificial Intelligence.
T Hopper. 2001. Indirect interventions to facilitate
communication in Alzheimers disease. Seminars in
Speech and Language, 22(4):305?315.
S. Klemmer, B. Hartmann, and L. Takayama. 2006.
How bodies matter: five themes for interaction de-
sign. In Proceedings of the conference on Designing
Interactive systems, pages 140?149.
Tracy Lee and Alex Mihaildis. 2005. An intelligent
emergency response system: Preliminary develop-
ment and testing of automated fall detection. Jour-
nal of Telemedicine and Telecare, 11:194?198.
Eric Lucet. 2012. Social Mobiserv Kompai Robot to
Assist People. In euRobotics workshop on Robots in
Healthcare and Welfare.
Alex Mihailidis, Jennifer N Boger, Tammy Craig, and
Jesse Hoey. 2008. The COACH prompting system
to assist older adults with dementia through hand-
washing: An efficacy study. BMC Geriatrics, 8(28).
Mehdi Mouad, Lounis Adouane, Pierre Schmitt,
Djamel Khadraoui, Benjamin Ga?teau, and Philippe
Martinet. 2010. Multi-agents based system to coor-
dinate mobile teamworking robots. In Proceedings
of the 4th Companion Robotics Institute, Brussels.
Elizabeth D. Mynatt, Anne-Sophie Melenhorst,
Arthur D. Fisk, and Wendy A. Rogers. 2004. Aware
technologies for aging in place: Understanding user
needs and attitudes. IEEE Pervasive Computing,
3:36?41.
Patrick Olivier, Andrew Monk, Guangyou Xu, and
Jesse Hoey. 2009. Ambient kitchen: Designing
situation services using a high fidelity prototyping
environment. In Proceedings of the ACM 2nd Inter-
national Conference on Pervasive Technologies Re-
lated to Assistive Environments, Corfu Greece.
S. V. Pakhomov, G. E. Smith, D. Chacon, Y. Feliciano,
N. Graff-Radford, R. Caselli, and D. S. Knopman.
2010. Computerized analysis of speech and lan-
guage to identify psycholinguistic correlates of fron-
totemporal lobar degeneration. Cognitive and Be-
havioral Neurology, 23:165?177.27
M. E. Pollack. 2006. Autominder: A case study of as-
sistive technology for elders with cognitive impair-
ment. Generations, 30:67?69.
Charlene Pope and Boyd H. Davis. 2011. Finding
a balance: The Carolinas Conversation Collection.
Corpus Linguistics and Linguistic Theory, 7(1).
J. Reilly, J. Troche, and M. Grossman. 2011. Lan-
guage processing in dementia. In A. E. Budson and
N. W. Kowall, editors, The Handbook of Alzheimer?s
Disease and Other Dementias. Wiley-Blackwell.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffery Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Au-
dio, Speech, and Language Processing, 19(7):2081?
2090.
Elizabeth Rochon, Gloria S. Waters, and David Ca-
plan. 2000. The Relationship Between Measures
of Working Memory and Sentence Comprehension
in Patients With Alzheimer?s Disease. Journal of
Speech, Language, and Hearing Research, 43:395?
413.
Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Eliz-
abeth Rochon, and Carol Leonard. 2012. Commu-
nication strategies for a computerized caregiver for
individuals with alzheimer?s disease. In Proceed-
ings of the Third Workshop on Speech and Language
Processing for Assistive Technologies (SLPAT2012)
at the 13th Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL 2012), Montreal Canada, June.
Frank Rudzicz. 2006. Clavius: Bi-directional parsing
for generic multimodal interaction. In Proceedings
of the joint meeting of the International Conference
on Computational Linguistics and the Association
for Computational Linguistics, Sydney Australia.
Privender Saini, Boris de Ruyter, Panos Markopoulos,
and Albert van Breemen. 2005. Benefits of social
intelligence in home dialogue systems. In Proceed-
ings of INTERACT 2005, pages 510?521.
A. Serna, H. Pigot, and V. Rialle. 2007. Modeling the
progression of alzheimer?s disease for cognitive as-
sistance in smart homes. User Modelling and User-
Adapted Interaction, 17:415?438.
Jeff A. Small, Elaine S. Andersen, and Daniel Kem-
pler. 1997. Effects of working memory capacity
on understanding rate-altered speech. Aging, Neu-
ropsychology, and Cognition, 4(2):126?139.
M. Snover, B. Dorr, and R. Schwartz. 2004. A
lexically-driven algorithm for disfluency detection.
In ?Proceedings of HLT-NAACL 2004: Short Papers,
pages 157?160.
Adriana Tapus and Mohamed Chetouani. 2010.
ROBADOM: the impact of a domestic robot on the
psychological and cognitive state of the elderly with
mild cognitive impairment. In Proceedings of the
International Symposium on Quality of Life Technol-
ogy Intelligent Systems for Better Living, Las Vegas
USA, June.
Cheryl K. Tomoeda, Kathryn A. Bayles, Daniel R.
Boone, Alfred W. Kaszniak, and Thomas J. Slau-
son. 1990. Speech rate and syntactic complexity
effects on the auditory comprehension of alzheimer
patients. Journal of Communication Disorders,
23(2):151 ? 161.
Keith Vertanen. 2006. Baseline WSJ acoustic models
for HTK and Sphinx: Training recipes and recogni-
tion experiments. Technical report, Cavendish Lab-
oratory, University of Cambridge.
Rozanne Wilson, Elizabeth Rochon, Alex Mihailidis,
and Carol Leonard. 2012. Examining success of
communication strategies used by formal caregivers
assisting individuals with alzheimer?s disease during
an activity of daily living. Journal of Speech, Lan-
guage, and Hearing Research, 55:328?341, April.
Steve Young, Gunnar Evermann, Mark Gales, Thomas
Hain, Dan Kershaw, Xunying (Andrew) Liu, Gareth
Moore, Julian Odell, Dave Ollason and Dan Povey,
Valtcho Valtchev, and Phil Woodland. 2006. The
HTK Book (version 3.4).
28
