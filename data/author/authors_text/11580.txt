Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 95?99,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2009
Jinhua Du, Yifan He, Sergio Penkale, Andy Way
Centre for Next Generation Localisation
Dublin City University
Dublin 9, Ireland
{jdu,yhe,spenkale,away}@computing.dcu.ie
Abstract
In this paper, we describe the machine
translation system in the evaluation cam-
paign of the Fourth Workshop on Statisti-
cal Machine Translation at EACL 2009.
We describe the modular design of our
multi-engine MT system with particular
focus on the components used in this par-
ticipation.
We participated in the translation task
for the following translation directions:
French?English and English?French, in
which we employed our multi-engine ar-
chitecture to translate. We also partic-
ipated in the system combination task
which was carried out by the MBR de-
coder and Confusion Network decoder.
We report results on the provided devel-
opment and test sets.
1 Introduction
In this paper, we present a multi-engine MT
system developed at DCU, MATREX (Machine
Translation using Examples). This system exploits
EBMT, SMT and system combination techniques
to build a cascaded translation framework.
We participated in both the French?English and
English-French News tasks. In these two tasks,
we employ three individual MT system which are
1) Baseline: phrase-based system (PB); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chun-
ker (Gough and Way, 2004). 3) HPB: a typical
hierarchical phrase-based system (Chiang, 2005).
Meanwhile, we also use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final result.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypothe-
sis as the alignment reference for the Confusion
Network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search and generate the translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide results on the development and
test sets. Section 4 is our conclusion.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits aspects
of both the EBMT and SMT paradigms.
This architecture includes three individual sys-
tems which are phrase-based, example-based and
hierarchical phrase-based.
The combination structure is the MBR decoder
and CN decoder, which is based on the word-level
combination strategy.
In the final stage, we use a new rescoring mod-
ule to process the N -best list generated by the
combination module. See Figure 1 as a detailed
illustration.
2.2 Example-Based Machine Translation
EBMT obtains resources using the Marker Hy-
pothesis (Green, 1979), a psycholinguistic con-
straint which posits that all languages are marked
for surface syntax by a specific closed set of lex-
emes or morphemes which signify context. Given
a set of closed-class words we segment each sen-
tence into chunks, creating a chunk at each new
occurrence of a marker word, with the restriction
that each segment must contain at least one non-
marker word (Gough and Way, 2004).
95
Mutiple 1-best
MBR Decoder
CN/MERT
System 
Combination
HPB Baseline EBMT
Dev/MERT
Decoding
Rescore/MERT
Rescore/MERT
TestSet
Recaser
Rescore
Mutiple 1-best
MBR Decoder
CN Decoder
Rescore
Recaser
Figure 1: System Framework
We then align these segments using an edit-
distance-style algorithm, in which the insertion
and deletion probabilities depend on word-to-
word translation probabilities and word-to-word
cognates (Stroppa and Way, 2006).
We extracted phrases of at most 7 words on
each side. We then merged these phrases with the
phrases extracted by the baseline system adding
word alignment information, and used this system
seeded with this additional information.
2.3 Hierarchical Machine Translation
HPB translation system is a re-implementation of
the hierarchical phrase translation model which is
based on PSCFG (Chiang, 2005). We generate re-
cursively PSCFG rules from the initial rules as
N ? f1 . . . fm/e1 . . . en
where N is a rule which is initial or includes non-
terminals.
M ? fi . . . fj/eu . . . ev
where 1 ? i ? j ? m and 1 ? u ? v ? n, at
which point a new rule can be obtained, named,
N ? f i?11 Xkfmj+1/eu?11 Xkenv+1
where k is an index for the nonterminal X . The
number of nonterminals permitted in a rule is no
more than two.
When extracting hierarchical rules,we set some
limitations that initial rules are of no more than
7 words in length and other rules should have
no more than 5 terminals and nonterminals, and
we disallow rules with adjacent source-side and
target-side nonterminals.
The decoder is an enhanced CYK-style chart
parser that maximizes the derivation probability
and spans up to 12 source words. A 4-gram lan-
guage model generated by SRI Language Model-
ing toolkit (SRILM) (Stolcke, 2002) is used in the
cube-pruning process. The search space is pruned
with a chart cell size limit of 50.
2.4 System Combination
For multiple system combination, we implement
an MBR-CN framework as shown in Figure 1. In-
stead of using a single system output as the skele-
ton, we employ a minimum Bayes-risk decoder
to select the best single system output from the
merged N -best list by minimizing the BLEU (Pa-
pineni et al, 2002) loss.
The confusion network is built by the output of
MBR as the backbone which determines the word
order of the combination. The other hypotheses
are aligned against the backbone based on the TER
metric. NULL words are allowed in the alignment.
Each arc in the CN represents an alternative word
at that position in the sentence and the number of
votes for each word is counted when constructing
the network. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
Also, we use MERT (Och, 2003) to tune the
weights of confusion network.
2.5 Rescore
Rescore is a very important part in post-processing
which can select a better hypothesis from the N -
best list. We add some new global features in
rescore model. The features we used are as fol-
lows:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram POS language model (Ratna-
parkhi, 1996; Schmid, 1994);
96
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT algorithm.
3 Experimental Setup
The following section describes the system and
experimental setup for the French-English and
English-French translation tasks.
3.1 Statistics of Data
Parallel Corpus
We used Europarl and Giga data for this evalua-
tion. The statistics of parallel data are shown in
Table 1.
Corpra Sen Token-En Token-Fr Len
Europarl 1.46M 39,240,672 42,252,067 80
Giga 2M 48,648,104 57,869,002 65
Table 1: Statistics of Parallel Data
In this table, Sen indicates the number of sentence
pairs; Len denotes the maximum sentence length
of each corpus. This year the translation task is
only evaluated on News Domain. Experimental re-
sults showed that giga data is more correlated than
Europarl and the BLEU score is significantly im-
proved(See Table 4).
Monolingual Corpus
In this evaluation, we trained a small 4-gram lan-
guage model using data in Table 1 and a large 4-
gram language model using data in Table 2. We
configured these two LMs for Baseline and EBMT
systems while HPB only used the large one.
Language Sen Token Source
English 9,966,838 240,849,221 E/N/NC
French 9,966,838 260,520,313 E/N/NC
Table 2: Statistics of Monolingual Data
In the above table, E/N/NC refers to Eu-
roparl/News/New Commentary corpus.
3.2 Pre-Processing
We preprocessed both Europarl and Giga Release
1 corpus. For the Europarl corpus, we removed
the reserved characters in GIZA++ and tokenized
and lowercased the corpus with tools provided by
WMT09. The Giga corpus was too large for our
resource, so we performed sentence selection be-
fore cleaning, in the following steps.
? We split the Giga corpus into even segments,
each segment consisting of 20 lines.
? We trained an SVM classifier on English side
with positive examples from the monolin-
gual news data and negative examples from
noisy sentences (numbers, meaningless word
combinations, and random segments) from
the Giga corpus. We used ?-ly? and ?-ing?
to approximate adverbs and present partici-
ples and did not use other POS-induced fea-
tures, as in (Ferizis and Bailey, 2006). We
added these features to remove noise: aver-
age length of sentences, frequency of capital-
ized characters, frequency of numerical char-
acters and short word penalty (equals to 1
when average length of words < 4, and 0
otherwise). We used the classifier to remove
20% segments of lowest scores.
? We selected 1, 600 words having the highest
mutual information scores with monolingual
training data against the Giga corpus.
? We selected 100, 000 segments where these
words occurred most frequently. However
the sentence was dropped if the length ratio
between English and French was larger than
1.5 or less than 0.67.
3.3 System Configuration
The two language models were done using the
SRILM employing linear interpolation and modi-
fied K-N discounting (Chen and Goodman, 1996).
The configuration for the three systems is listed
in Table 3.
System P-Table Length LM Features
Baseline-E 55.9M 7 2 15
Baseline-G 58.4M 7 2 15
EBMT 59.4M 7 2 15
HPB 122M 5 1 8
Table 3: Statistics of MT Systems
In this table, E indicates the Europarl corpus
97
which is used for all three systems, and G stands
for the Giga corpus which is only used for the
Baseline system. We can see from Table 3 that
the size of the HPB phrase-table is more than 2
times as large as the other phrase tables. How to
filter and process such a huge hierarchical table is
a challenging problem.
We tuned our systems on the development set
devset2009-a and devset2009-b, and performed
the crossover experiment by these two devsets.
3.4 Experimental Results
The system output is evaluated with respect to
BLEU score. In Table 4, we used devset2009-b
to tune the various parameters in our three single
systems and devset2009-a for testing. In terms of
the Europarl data, we can see that the three sys-
tems we used achieved similar performance on the
test set for both translation directions, with the
Baseline-E system yielding slightly better results
than the other two.
System Fr-En En-Fr
Baseline-E 22.24 22.68
Baseline-G 24.90 ?1
EBMT 22.04 22.12
HPB 21.69 21.12
MBR 25.11 22.68
CN 25.24 22.76
Rescore 25.40 22.97
Table 4: Experimental Results on Devset2009-a
We then used the translations of the devset2009-
a produced by each system to tune the parame-
ters of our system combination module. From Ta-
ble 4, we can see that using MBR and confusion
network decoding leads to a slight improvement
over the strongest single system, i.e. the baseline
Phrase-Based SMT system. Rescoring the N -best
lists yielded an increase of 0.5 (2.0 relative) ab-
solute BLEU points over the baseline for French?
English Translation and 0.29 (1.28 relative) abso-
lute BLEU points for English?French Translation.
Table 5 is the results on 2009 Test Data. The
scores with a slash in the last two rows are low-
ercased and cased respectively. From the table we
1Not much time to do the experiments on English-French
direction. EBMT and HPB just used the Europarl corpus.
2The official automatic result is scored on 2525 sentences
out of the whole 3007 sentences in test set. The other 502
sentences are used as the development set for combination
evaluation task.
System Fr-En En-Fr
Baseline-E 25.64 24.47
Baseline-G 26.75 ?
EBMT 25.67 24.43
HPB 25.20 24.19
Combination 27.20/25.14 25.26/22.28
Official-Auto2 26.86/24.93 23.78/22.14
Table 5: Summary of Results on 2009 Test Data
can see that combination yielded 0.45 and 0.79 ab-
solute BLEU points over the best single system for
Fr-En and En-Fr direction respectively. However,
1.93 (7.2 relative) and 1.64 (6.58 relative) BLEU
points are dropped between cased and lowercased
results of both directions. Accordingly, training an
effective recasing model is very important for our
future work.
4 Conclusion
This paper presents our machine translation sys-
tem in WMT2009 shared task campaign. We de-
veloped a multi-engine framework which com-
bined the output results of the three MT sys-
tems and generated a new N -best list after CN
decoding. Then by using some global features
the rescoring model generated the final translation
output. The experimental result proved that the
combination module and rescoring module are ef-
fective in our framework.
We also applied simple yet effective methods
of genre and topical classification to remove noise
and out-of-domain sentences in the Giga corpus,
from which we built better translation models than
from Europarl.
In future work, we will refine our system frame-
work to investigate its effect on the tasks pre-
sented here, and we will develop more powerful
post-processing tools such as recaser to reduce the
BLEU loss.
Acknowledgments
This work is supported by Science Foundation Ireland (Grant
No. 07/CE/I1142). Thanks also to the reviewers for their
insightful comments and suggestions.
References
Chen, S. F. and Goodman, J. (1996). An Empirical Study of
Smoothing Techniques for Language Modeling. In Pro-
ceedings of the Thirty-Fourth Annual Meeting of the As-
sociation for Computational Linguistics, pages 310?318,
San Francisco, CA.
Chiang, D. (2005). A Hierarchical Phrase-Based Model for
Statistical Machine Translation. In Proceedings of the
98
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 263?270, Ann Arbor,
MI.
Ferizis, G. and Bailey, P. (2006). Towards practical genre
classification of web documents. In Proceedings of the
15th international conference on World Wide Web (WWW
?06), pages 1013?1014, New York, USA.
Fiscus, J. G. (1997). A post-processing system to yield re-
duced word error rates: Recognizer output voting error
reduction (ROVER). In Proceedings 1997 IEEE Work-
shop on Automatic Speech Recognition and Understand-
ing (ASRU), pages 347?352, Santa Barbara, CA.
Gough, N. and Way, A. (2004). Robust Large-Scale EBMT
with Marker-Based Segmentation. In Proceedings of
the 10th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-04),
pages 95?104, Baltimore, MD.
Green, T. (1979). The Necessity of Syntax Markers. Two
experiments with artificial languages. Journal of Verbal
Learning and Behavior, 18:481?496.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the Joint Meeting of the Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2004), pages 169?176, Boston, MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Finding con-
sensus in speech recognition: Word error minimization
and other applications of confusion networks. Computer
Speech and Language, 14(4):373?400.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-02),
pages 311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy Model for
Part-Of-Speech Tagging. In Proceedings of the Empiri-
cal Methods in Natural Language Processing Conference
(EMNLP), pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S., Schwartz, R., Ayan,
N. F., and Dorr, B. J. (2007). Combining outputs from
multiple machine translation systems. In Proceedings
of the Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2007), pages 228?235, Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44?49, Manchester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate with
targeted human annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Translation in the
Americas (AMTA 2006), pages 223?231, Cambridge, MA.
Stolcke, A. (2002). SRILM - An Extensible Language Mod-
eling Toolkit. In Proceedings of the International Confer-
ence Spoken Language Processing, pages 901?904, Den-
ver, CO.
Stroppa, N. and Way, A. (2006). MaTrEx: the DCU machine
translation system for IWSLT 2006. In Proceedings of the
International Workshop on Spoken Language Translation,
pages 31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
Joint Meeting of the Human Language Technology Con-
ference and the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL 2006),
pages 72?77, New York, USA.
99
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 286?294,
Beijing, August 2010
A Discriminative Latent Variable-Based ?DE? Classifier
for Chinese?English SMT
Jinhua Du and Andy Way
CNGL, School of Computing
Dublin City University
{jdu, away}@computing.dcu.ie
Abstract
Syntactic reordering on the source-side
is an effective way of handling word or-
der differences. The { (DE) construc-
tion is a flexible and ubiquitous syntac-
tic structure in Chinese which is a ma-
jor source of error in translation quality.
In this paper, we propose a new classi-
fier model ? discriminative latent vari-
able model (DPLVM) ? to classify the
DE construction to improve the accuracy
of the classification and hence the transla-
tion quality. We also propose a new fea-
ture which can automatically learn the re-
ordering rules to a certain extent. The ex-
perimental results show that the MT sys-
tems using the data reordered by our pro-
posed model outperform the baseline sys-
tems by 6.42% and 3.08% relative points
in terms of the BLEU score on PB-SMT
and hierarchical phrase-based MT respec-
tively. In addition, we analyse the impact
of DE annotation on word alignment and
on the SMT phrase table.
1 Introduction
Syntactic structure-based reordering has been
shown to be significantly helpful for handling
word order issues in phrase-based machine trans-
lation (PB-SMT) (Xia and McCord, 2004; Collins
et al, 2005; Wang et al, 2007; Li et al, 2007;
Elming, 2008; Chang et al, 2009). It is well-
known that in MT, it is difficult to translate be-
tween Chinese?English because of the different
word orders (cf. the different orderings of head
nouns and relative clauses). Wang et al (2007)
pointed out that Chinese differs from English in
several important respects, such as relative clauses
appearing before the noun being modified, prepo-
sitional phrases often appearing before the head
they modify, etc. Chang et al (2009) argued
that many of the structural differences are re-
lated to the ubiquitous Chinese structural parti-
cle phrase { (DE) construction, used for a wide
range of noun modification constructions (both
single word and clausal) and other uses. They
pointed out that DE is a major source of word
order error when a Chinese sentence is translated
into English due to the different ways that the DE
construction can be translated.
In this paper, we focus on improving the clas-
sification accuracy of DE constructions in Chi-
nese as well as investigating its impact on trans-
lation quality. From the grammatical perspective,
the {(DE) in Chinese represents the meaning of
?noun modification? which generally is shown in
the form of a Noun phrase (NP) [A DE B]. A in-
cludes all the words in the NP before DE and B
contains all the words in the NP after DE. Wang
et al (2007) first introduced a reordering of the
DE construction based on a set of rules which
were generated manually and achieved significant
improvements in translation quality. Chang et
al. (2009) extended this work by classifying DE
into 5 finer-grained categories using a log-linear
classifier with rich features in order to achieve
higher accuracy both in reordering and in lexical
choice. Their experiments showed that a higher
286
accuracy of the DE classification improved the ac-
curacy of reordering component, and further indi-
rectly improved the translation quality in terms of
BLEU (Papineni et al, 2002) scores.
We regard the DE classification as a labeling
task, and hence propose a new model to label the
DE construction using a discriminative latent vari-
able algorithm (DPLVM) (Morency et al, 2007;
Sun and Tsujii, 2009), which uses latent vari-
ables to carry additional information that may not
be expressed by those original labels and capture
more complicated dependencies between DE and
its corresponding features. We also propose a new
feature defined as ?tree-pattern? which can auto-
matically learn the reordering rules rather than us-
ing manually generated ones.
The remainder of this paper is organised as fol-
lows. In section 2, we introduce the types of
word order errors caused by the DE construc-
tion. Section 3 describes the closely related work
on DE construction. In section 4, we detail our
proposed DPLVM algorithm and its adaptation to
our task. We also describe the feature templates
as well as the proposed new feature used in our
model. In section 5, the classification experiments
are conducted to compare the proposed classifica-
tion model with a log-linear model. Section 6 re-
ports comparative experiments conducted on the
NIST 2008 data set using two sets of reordered
and non-reordered data. Meanwhile, in section 7,
an analysis on how the syntactic DE reordering
affects word alignment and phrase table is given.
Section 8 concludes and gives avenues for future
work.
2 The Problem of Chinese DE
Construction Translation
Although syntactic reordering is an effective
way of significantly improving translation quality,
word order is still a major error source between
Chinese and English translation. Take examples
in Figure 1 as an illustration. The errors of three
translation results in Figure 1 are from different
MT systems, and many errors relate to incorrect
reordering for the{ (DE) structure.
These three translations are from different Hi-
ero systems. Although Hiero has an inherent re-
ordering capability, none of them correctly re-
Source: h?(local) ?(a) ?XProceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 420?429,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Facilitating Translation Using Source Language Paraphrase Lattices
Jinhua Du, Jie Jiang, Andy Way
CNGL, School of Computing
Dublin City University, Dublin, Ireland
{jdu, jjiang, away}@computing.dcu.ie
Abstract
For resource-limited language pairs, coverage
of the test set by the parallel corpus is an
important factor that affects translation qual-
ity in two respects: 1) out of vocabulary
words; 2) the same information in an input
sentence can be expressed in different ways,
while current phrase-based SMT systems can-
not automatically select an alternative way
to transfer the same information. Therefore,
given limited data, in order to facilitate trans-
lation from the input side, this paper pro-
poses a novel method to reduce the transla-
tion difficulty using source-side lattice-based
paraphrases. We utilise the original phrases
from the input sentence and the correspond-
ing paraphrases to build a lattice with esti-
mated weights for each edge to improve trans-
lation quality. Compared to the baseline sys-
tem, our method achieves relative improve-
ments of 7.07%, 6.78% and 3.63% in terms
of BLEU score on small, medium and large-
scale English-to-Chinese translation tasks re-
spectively. The results show that the proposed
method is effective not only for resource-
limited language pairs, but also for resource-
sufficient pairs to some extent.
1 Introduction
In recent years, statistical MT systems have been
easy to develop due to the rapid explosion in data
availability, especially parallel data. However, in
reality there are still many language pairs which
lack parallel data, such as Urdu?English, Chinese?
Italian, where large amounts of speakers exist for
both languages; of course, the problem is far worse
for pairs such as Catalan?Irish. For such resource-
limited language pairs, sparse amounts of parallel
data would cause the word alignment to be inac-
curate, which would in turn lead to an inaccurate
phrase alignment, and bad translations would re-
sult. Callison-Burch et al (2006) argue that lim-
ited amounts of parallel training data can lead to the
problem of low coverage in that many phrases en-
countered at run-time are not observed in the train-
ing data and so their translations will not be learned.
Thus, in recent years, research on addressing the
problem of unknown words or phrases has become
more and more evident for resource-limited lan-
guage pairs.
Callison-Burch et al (2006) proposed a novel
method which substitutes a paraphrase for an un-
known source word or phrase in the input sentence,
and then proceeds to use the translation of that para-
phrase in the production of the target-language re-
sult. Their experiments showed that by translating
paraphrases a marked improvement was achieved in
coverage and translation quality, especially in the
case of unknown words which previously had been
left untranslated. However, on a large-scale data set,
they did not achieve improvements in terms of auto-
matic evaluation.
Nakov (2008) proposed another way to use para-
phrases in SMT. He generates nearly-equivalent syn-
tactic paraphrases of the source-side training sen-
tences, then pairs each paraphrased sentence with
the target translation associated with the original
sentence in the training data. Essentially, this
method generates new training data using para-
phrases to train a new model and obtain more useful
420
phrase pairs. However, he reported that this method
results in bad system performance. By contrast,
real improvements can be achieved by merging the
phrase tables of the paraphrase model and the orig-
inal model, giving priority to the latter. Schroeder
et al (2009) presented the use of word lattices
for multi-source translation, in which the multiple
source input texts are compiled into a compact lat-
tice, over which a single decoding pass is then per-
formed. This lattice-based method achieved positive
results across all data conditions.
In this paper, we propose a novel method us-
ing paraphrases to facilitate translation, especially
for resource-limited languages. Our method does
not distinguish unknown words in the input sen-
tence, but uses paraphrases of all possible words
and phrases in the source input sentence to build a
source-side lattice to provide a diverse and flexible
list of source-side candidates to the SMT decoder
so that it can search for a best path and deliver the
translation with the highest probability. In this case,
we neither need to change the phrase table, nor add
new features in the log-linear model, nor add new
sentences in the training data.
The remainder of this paper is organised as fol-
lows. In Section 2, we define the ?translation diffi-
culty? from the perspective of the source side, and
then examine how well the test set is covered by
the phrase table and the parallel training data . Sec-
tion 3 describes our paraphrase lattice method and
discusses how to set the weights for the edges in the
lattice network. In Section 4, we report comparative
experiments conducted on small, medium and large-
scale English-to-Chinese data sets. In Section 5,
we analyse the influence of our paraphrase lattice
method. Section 6 concludes and gives avenues for
future work.
2 What Makes Translation Difficult?
2.1 Translation Difficulty
We use the term ?translation difficulty? to explain
how difficult it is to translate the source-side sen-
tence in three respects:
? The OOV rates of the source sentences in the
test set (Callison-Burch et al, 2006).
? Translatability of a known phrase in the input
sentence. Some particular grammatical struc-
tures on the source side cannot be directly
translated into the corresponding structures on
the target side. Nakov (2008) presents an ex-
ample showing how hard it is to translate an En-
glish construction into Spanish. Assume that an
English-to-Spanish SMT system has an entry
in its phrase table for ?inequality of income?,
but not for ?income inequality?. He argues that
the latter phrase is hard to translate into Span-
ish where noun compounds are rare: the correct
translation in this case requires a suitable Span-
ish preposition and a reordering, which are hard
for the system to realize properly in the target
language (Nakov, 2008).
? Consistency between the reference and the
target-side sentence in the training corpus.
Nakov (2008) points out that if the target-side
sentence in the parallel corpus is inconsistent
with the reference of the test set, then in some
cases, a test sentence might contain pieces that
are equivalent, but syntactically different from
the phrases learned in training, which might re-
sult in practice in a missed opportunity for a
high-quality translation. In this case, if we use
paraphrases for these pieces of text, then we
might improve the opportunity for the transla-
tion to approach the reference, especially in the
case where only one reference is available.
2.2 Coverage
As to the first aspect ? coverage ? we argue that
the coverage rate of the new words or unknown
words are more and more becoming a ?bottleneck?
for resource-limited languages. Furthermore, cur-
rent SMT systems, either phrase-based (Koehn et al,
2003; Chiang, 2005) or syntax-based (Zollmann and
Venugopal, 2006), use phrases as the fundamental
translation unit, so how much the phrase table and
training data can cover the test set is an important
factor which influences the translation quality. Ta-
ble 1 shows the statistics of the coverage of the test
set on English-to-Chinese FBIS data, where we can
see that the coverage of unigrams is very high, es-
pecially when the data is increased to the medium
size (200K), where unigram coverage is greater than
90%. Based on the observations of the unknown un-
421
20K Cov.(%) 200K Cov.(%)
PL Tset PT Corpus in PT in Corpus PT Corpus in PT in Corpus
1 5,369 3,785 4,704 70.5 87.61 4,941 5,230 92.03 97.41
2 24,564 8,631 15,109 35.14 61.51 16,803 21,071 68.40 85.78
3 37,402 4,538 12,091 12.13 32.33 12,922 22,531 34.55 60.24
4 41,792 1,703 6,150 4.07 14.72 5,974 14,698 14.29 35.17
5 43,008 626 2,933 1.46 6.82 2,579 8,425 5.99 19.59
6 43,054 259 1,459 0.6 3.39 1,192 4,856 2.77 11.28
7 42,601 119 821 0.28 1.93 581 2,936 1.36 6.89
8 41,865 51 505 0.12 1.21 319 1,890 0.76 4.51
9 40,984 34 341 0.08 0.83 233 1,294 0.57 3.16
10 40,002 22 241 0.05 0.6 135 923 0.34 2.31
Table 1: The coverage of the test set by the phrase table and the parallel corpus based on different amount of the
training data. ?PL? indicates the Phrase Length N , where {1 <= N <= 10}; ?20K? and ?200K? represent the sizes
of the parallel data for model training and phrase extraction; ?Cov.? indicates the coverage rate; ?Tset? represents the
number of unique phrases with the length N in the Test Set; ?PT? represents the number of phrases of the Test Set
occur in the Phrase Table; ?Corpus? indicates the number of phrases of the Test Set appearing in the parallel corpus;
?in PT? indicates the coverage of the phrases in the Test Set by the phrase table and correspondingly ?in Corpus?
represents the coverage of the phrases in the Test Set by the Parallel Corpus.
igrams, we found that most are named entities (NEs)
such as person name, location name, etc. From the
bigram phrases, the coverage rates begin to signifi-
cantly decline. It can also be seen that phrases con-
taining more than 5 words rarely appear either in the
phrase table or in the parallel corpus, which indi-
cates that data sparseness is severe for long phrases.
Even if the size of the corpus is significantly in-
creased (e.g. from 20K to 200K), the coverage of
long phrases is still quite low.
With respect to these three aspects of the transla-
tion difficulty, especially for data-limited language
pairs, we propose a more effective method to make
use of the paraphrases to facilitate translation pro-
cess.
3 Paraphrase Lattice for Input Sentences
In this Section, we propose a novel method to em-
ploy paraphrases to reduce the translation difficulty
and in so doing increase the translation quality.
3.1 Motivation
Our idea to build a paraphrase lattice for SMT is in-
spired by the following points:
? Handling unknown words is a challenging issue
for SMT, and using paraphrases is an effective
way to facilitate this problem (Callison-Burch
et al, 2006);
? The method of paraphrase substitution does not
show any significant improvement, especially
on a large-scale data set in terms of BLEU (Pa-
pineni et al, 2002) scores (Callison-Burch et
al., 2006);
? Building a paraphrase lattice might provide
more translation options to the decoder so that
it can flexibly search for the best path.
The major contributions of our method are:
? We consider all N -gram phrases rather than
only unknown phrases in the test set, where
{1 <= N <= 10};
? We utilise lattices rather than simple substitu-
tion to facilitate the translation process;
? We propose an empirical weight estimation
method to set weights for edges in the word lat-
tice, which is detailed in Section 3.4.
3.2 Paraphrase Acquisition
Paraphrases are alternative ways to express the same
or similar meaning given a certain original word,
phrase or segment. The paraphrases used in our
method are generated from the parallel corpora
based on the algorithm in (Bannard and Callison-
Burch, 2005), in which paraphrases are identified
422
by pivoting through phrases in another language.
In this algorithm, the foreign language translations
of an English phrase are identified, all occurrences
of those foreign phrases are found, and all English
phrases that they translate as are treated as potential
paraphrases of the original English phrase (Callison-
Burch et al, 2006). A paraphrase has a probability
p(e2|e1) which is defined as in (2):
p(e2|e1) =
?
f
p(f |e1)p(e2|f) (1)
where the probability p(f |e1) is the probability that
the original English phrase e1 translates as a particu-
lar phrase f in the other language, and p(e2|f) is the
probability that the candidate paraphrase e2 trans-
lates as the foreign language phrase.
p(e2|f) and p(f |e1) are defined as the transla-
tion probabilities which can be calculated straight-
forwardly using maximum likelihood estimation by
counting how often the phrases e and f are aligned
in the parallel corpus as in (2) and (3):
p(e2|f) ?
count(e2, f)
?
e2 count(e2, f)
(2)
p(f |e1) ?
count(f, e1)
?
f count(f, e1)
(3)
3.3 Construction of Paraphrase Lattice
To present paraphrase options to the PB-SMT de-
coder, lattices with paraphrase options are con-
structed to enrich the source-language sentences.
The construction process takes advantage of the cor-
respondence between detected paraphrases and po-
sitions of the original words in the input sentence,
then creates extra edges in the lattices to allow the
decoder to consider paths involving the paraphrase
words.
An toy example is illustrated in Figure 1: given
a sequence of words {w1, . . . , wN} as the input,
two phrases ? = {?1, . . . , ?p} and ? = {?1, . . . , ?q}
are detected as paraphrases for S1 = {wx, . . . , wy}
(1 ? x ? y ? N ) and S2 = {wm, . . . , wn}
(1 ? m ? n ? N ) respectively. The following
steps are taken to transform them into word lattices:
1. Transform the original source sentence into
word lattices. N + 1 nodes (?k, 0 ? k ? N )
... ...wx wm... wy
...
 1
...
Source side 
sentence
Generated 
lattice
!1 !2 ? !p
 1  2 ?  q
Paraphrase A
Paraphrase B
!1
!2 ... !p
 2
 q
... ... wn
...wx wm wy wn... ... ...
Figure 1: An example of lattice-based paraphrases for an
input sentence
are created, and N edges (referred to as ?ORG-
E? edges) labeled with wi (1 ? i ? N ) are
generated to connect them sequentially.
2. Generate extra nodes and edges for each of the
paraphrases. Taking ? as an example, firstly,
p ? 1 nodes are created, and then p edges
(referred as ?NEW-E? edges) labeled with ?j
(1 ? j ? p) are generated to connect node
?x?1, p? 1 nodes and ?y?1.
Via step 2, word lattices are generated by adding
new nodes and edges coming from paraphrases.
Note that to build word lattices, paraphrases with
multi-words are broken into word sequences, and
each of the words produces one extra edge in the
word lattices as shown in the bottom part in Figure 1.
Figure 2 shows an example of constructing the
word lattice for an input sentence which is from the
test set used in our experiments.1 The top part in
Figure 2 represents nodes (double-line circles) and
edges (solid lines) that are constructed by the orig-
inal words from the input sentence, while the bot-
tom part in Figure 2 indicates the final word lattice
with the addition of new nodes (single-line circles)
and new edges (dashed lines) which come from the
paraphrases. We can see that the paraphrase lattice
increases the diversity of the source phrases so that it
can provide more flexible translation options during
the decoding process.
1Figure 2 contains paths that are duplicates except for the
weights. We plan to handle this in future work.
423
Figure 2: An example of how to build a paraphrase lattice for an input sentence
3.4 Weight Estimation
Estimating and normalising the weight for each edge
in the word lattice is a challenging issue when the
edges come from different sources. In this section,
we propose an empirical method to set the weights
for the edges by distinguishing the original (?ORG-
E?) and new (?NEW-E?) edges in the lattices. The
aim is to utilize the original sentences as the ref-
erences to weight the edges from paraphrases, so
that decoding paths going through ?ORG-E? edges
will tend to have higher scores than those which use
?NEW-E? ones. The assumption behind this is that
the paraphrases are alternatives for the original sen-
tences, so decoding paths going though them ought
to be penalised.
Therefore, for all the ?ORG-E? edges, their
weights in the lattice are set to 1.0 as the reference.
Thus, in the log-linear model, decoding paths going
though these edges are not penalised because they
do not come from the paraphrases.
By contrast, ?NEW-E? are divided into two
groups for the calculation of weights:
? For ?NEW-E? edges which are outgoing edges
of the lattice nodes that come from the original
sentences, the probabilities p(es|ei)2 of their
2es indicates the source phrase S, ei represents one of the
corresponding paraphrases are utilised to pro-
duce empirical weights. Supposing that a set of
paraphrases X = {x1, . . . , xk} start at node A
which comes from the original sentence, so that
X are sorted descendingly based on the proba-
bilities p(es|ei), their corresponding edges for
node A are G = {g1, . . . , gk}, then the weights
are calculated as in (4):
w(ei) =
1
k + i
(1 <= i <= k) (4)
where k is a predefined parameter to trade off
between decoding speed and the number of
potential paraphrases being considered. Thus,
once a decoding path goes though one of these
edges, it will be penalised according to its para-
phrase probabilities.
? For all other ?NEW-E? edges, their weights
are set to 1.0, because the paraphrase penalty
has been counted in their preceding ?NEW-E?
edges.
Figure 2 illustrates the weight estimation results.
Nodes coming from the original sentences are drawn
in double-line circles (e.g. nodes 0 to 7), while
paraphrases of S.
424
nodes created from paraphrases are shown in single-
line circles (e.g. nodes 8 to 10). ?ORG-E? edges are
drawn in solid lines and ?NEW-E? edges are shown
using dashed lines. As specified previously, ?ORG-
E? edges are all weighted by 1.0 (e.g. edge labeled
?the? from node 0 to 1). By contrast, ?NEW-E?
edges in the first group are weighted by equation
(4) (e.g. edges in dashed lines start from node 0
to node 2 and 8), while others in the second group
are weighted by 1.0 (e.g. edge labeled ?training?
from node 8 to 2). Note that penalties of the paths
going through paraphrases are counted by equation
(4), which is represented by the weights of ?NEW-
E? edges in the first group. For example, starting
from node 2, paths going to node 9 and 10 are pe-
nalised because lattice weights are also considered
in the log-linear model. However, other edges do
not imply penalties since their weights are set to 1.0.
The reason to set al weights for the ?ORG-E?
edges to a uniform weight (e.g. 1.0) instead of a
lower empirical weight is to avoid excessive penal-
ties for the original words. For example, in Fig-
ure 2, the original edge from node 3 to 4 (con-
tinue) has a weight of 1.0, so the paths going though
the original edges from node 2 to 4 (will continue)
have a higher lattice score (1.0 ? 1.0 = 1.0) than
the paths going through the edges of paraphrases
(e.g. will resume (score: 0.125 ? 1.0 = 0.125) and
will go (score: 0.11 ? 1.0 = 0.11)), or any other
mixed paths that goes through original edges and
paraphrase edges, such as will continuous (score:
1.0 ? 0.125 = 0.125). The point is that we should
have more trust when translating the original words,
but if we penalise (set weights < 1.0) the ?ORG-
E? edges whenever there is a paraphrase for them,
then when considering the context of the lattice,
paraphrases will be favoured systematically. That is
why we just penalise the ?NEW-E? edges in the first
group and set other weights to 1.0.
As to unknown words in the input sentence, even
if we give them a prioritised weight, they would
be severely penalised in the decoding process. So
we do not need to distinguish unknown words when
building and weighting the paraphrase lattice.
4 Experiments
4.1 System and Data Preparation
For our experiments, we use Moses (Koehn et al,
2007) as the baseline system which can support
lattice decoding. We also realise a paraphrase
substitution-based system (Para-Sub)3 based on the
method in (Callison-Burch, 2006) to compare with
the baseline system and our proposed paraphrase
lattice-based (Lattice) system.
The alignment is carried out by GIZA++ (Och
and Ney, 2003) and then we symmetrized the word
alignment using the grow-diag-final heuristic. The
maximum phrase length is 10 words. Parameter tun-
ing is performed using Minimum Error Rate Train-
ing (Och, 2003).
The experiments are conducted on English-to-
Chinese translation. In order to fully compare our
proposed method with the baseline and the ?Para-
Sub? system, we perform the experiments on three
different sizes of training data: 20K, 200K and 2.1
million pairs of sentences. The former two sizes of
data are derived from FBIS,4 and the latter size of
data consists of part of HK parallel corpus,5 ISI par-
allel data,6 other news data and parallel dictionar-
ies from LDC. All the language models are 5-gram
which are trained on the monolingual part of parallel
data.
The development set (devset) and the test set for
experiments using 20K and 200K data sets are ran-
domly extracted from the FBIS data. Each set in-
cludes 1,200 sentences and each source sentence
has one reference. For the 2.1 million data set, we
use a different devset and test set in order to verify
whether our proposed method can work on a lan-
guage pair with sufficient resources. The devset is
the NIST 2005 Chinese-English current set which
has only one reference for each source sentence and
the test set is the NIST 2003 English-to-Chinese
current set which contains four references for each
source sentence. All results are reported in BLEU
and TER (Snover et al, 2006) scores.
3We use ?Para-Sub? to represent their system in the rest of
this paper.
4This is a multilingual paragraph-aligned corpus with LDC
resource number LDC2003E14.
5LDC number: LDC2004T08.
6LDC number: LDC2007T09.
425
20K 200K
SYS BLEU CI 95% pair-CI 95% TER BLEU CI 95% pair-CI 95% TER
Baseline 14.42 [-0.81, +0.74] ? 75.30 23.60 [-1.03, +0.97] ? 63.56
Para-Sub 14.78 [-0.78, +0.82] [+0.13, +0.60] 73.75 23.41 [-1.04, +1.00] [-0.46, +0.09] 63.84
Lattice 15.44 [-0.85, +0.84] [+0.74, +1.30] 73.06 25.20 [-1.11, +1.15] [+1.19, +2.01] 62.37
Table 2: Comparison between the baseline, ?Para-Sub? and our ?Lattice? (paraphrase lattice) method.
The paraphrase data set used in our lattice-based
and the ?Para-Sub? systems is same which is de-
rived from the ?Paraphrase Phrase Table?7 of TER-
Plus (Snover et al, 2009). The parameter k in equa-
tion 4 is set to 7.
4.2 Paraphrase Filtering
The more edges there are in a lattice, the more
complicated the decoding is in the search process.
Therefore, in order to reduce the complexity of the
lattice and increase decoding speed, we must fil-
ter out some potential noise in the paraphrase table.
Two measures are taken to optimise the paraphrases
when building a paraphrase lattice:
? Firstly, we filter out all the paraphrases whose
probability is less than 0.01;
? Secondly, given a source-side input sentence,
we retrieve all possible paraphrases and their
probabilities for source-side phrases which ap-
pear in the paraphrase table. Then we remove
the paraphrases which are not occurred in the
?phrase table? of the SMT system. This mea-
sure intends to avoid adding new ?unknown
words? to the source-side sentence. After
this measure, we can acquire the final para-
phrases which can be denoted as a quadru-
ple < SEN ID,Span, Para, Prob >, where
?SEN ID? indicates the ID of the input sen-
tence, ?Span? represents the span of the source-
side phrase in the original input sentence,
?Para? indicates the paraphrase of the source-
side phrase, and ?Prob? represents the probabil-
ity between the source-side phrase and its para-
phrase, which is used to set the weight of the
edge in the lattice. The quadruple is used to
construct the weighted lattice.
7http://www.umiacs.umd.edu/?snover/terp/
downloads/terp-pt.v1.tgz.
4.3 Experimental Results
The experimental results conducted on small and
medium-sized data sets are shown in Table 2. The
95% confidence intervals (CI) for BLEU scores are
independently computed on each of three systems,
while the ?pair-CI 95%? are computed relative to
the baseline system only for ?Para-Sub? and ?Lat-
tice? systems. All the significance tests use boot-
strap and paired-bootstrap resampling normal ap-
proximation methods (Zhang and Vogel, 2004).8
Improvements are considered to be significant if the
left boundary of the confidence interval is larger
than zero in terms of the ?pair-CI 95%?. It can
be seen that 1) our ?Lattice? system outperforms
the baseline by 1.02 and 1.6 absolute (7.07% and
6.78% relative) BLEU points in terms of the 20K
and 200K data sets respectively, and our system also
decreases the TER scores by 2.24 and 1.19 (2.97%
and 1.87% relative) points than the baseline system.
In terms of the ?pair-CI 95%?, the left boundaries
for 20K and 200K data are respectively ?+0.74? and
?+1.19?, which indicate that the ?Lattice? system is
significantly better than the baseline system on these
two data sets. 2) The ?Para-Sub? system performs
slightly better (0.36 absolute BLEU points) than the
baseline system on the 20K data set, but slightly
worse (0.19 absolute BLEU points) than the baseline
on the 200K data set, which indicates that the para-
phrase substitution method used in (Callison-Burch
et al, 2006) does not work on resource-sufficient
data sets. In terms of the ?pair-CI 95%?, the left
boundary for 20K data is ?+0.13?, which indicates
that it is significantly better than the baseline sys-
tem, while the left boundary is ?-0.46? for 200K
data, which indicates that the ?Para-Sub? system
is significantly worse than the baseline system. 3)
comparing the ?Lattice? system with the ?Para-Sub?
8http://projectile.sv.cmu.edu/research/
public/tools/bootStrap/tutorial.htm.
426
SYS BLEU CI 95% pair-CI 95% NIST TER
Baseline 14.04 [-0.73, +0.40] ? 6.50 74.88
Para-Sub 14.13 [-0.56, +0.56] [-0.18, +0.40] 6.52 74.43
Lattice 14.55 [-0.75, +0.32] [+0.15,+0.83] 6.55 73.28
Table 3: Comparison between the baseline and our paraphrase lattice method on a large-scale data set.
system, the ?pair-CI 95%? for 20K and 200K data
are respectively [+0.41, +0.92] and [+1.40, +2.17],
which indicates that the ?Lattice? system is signif-
icantly better than the ?Para-Sub? system on these
two data sets as well. 4) In terms of the two metrics,
our proposed method achieves the best performance,
which shows that our method is effective and consis-
tent on different sizes of data.
In order to verify our method on large-scale
data, we also perform experiments on 2.1 million
sentence-pairs of English-to-Chinese data as de-
scribed in Section 4.1. The results are shown in Ta-
ble 3. From Table 3, it can be seen that the ?Lattice?
system achieves an improvement of 0.51 absolute
(3.63% relative) BLEU points and a decrease of 1.6
absolute (2.14% relative) TER points compared to
the baseline. In terms of the ?pair-CI 95%?, the left
boundary for the ?Lattice? system is ?+0.15? which
indicates that it is significantly better than the base-
line system in terms of BLEU. Interestingly, in our
experiment, the ?Para-Sub? system also outperforms
the baseline on those three automatic metrics. How-
ever, in terms of the ?pair-CI 95%?, the left bound-
ary for the ?Para-Sub? system is ?-0.18? which indi-
cates that it is not significantly better than the base-
line system in terms of BLEU. The results also show
that our proposed method is effective and consistent
even on a large-scale data set.
It also can be seen that the improvement on 2.1
million sentence-pairs is less than that of the 20K
and 200K data sets. That is, as the size of the train-
ing data increases, the problems of data sparseness
decrease, so that the coverage of the test set by the
parallel corpus will correspondingly increase. In this
case, the role of paraphrases in decoding becomes a
little weaker. On the other hand, it might become a
kind of noise to interfere with the exact translation
of the original source-side phrases when decoding.
Therefore, our proposed method may be more ap-
propriate for language pairs with limited resources.
5 Analysis
5.1 Coverage of Paraphrase Test Set
The coverage rate of the test set by the phrase ta-
ble is an important factor that could influence the
translation result, so in this section we examine the
characteristics of the updated test set that adds in the
paraphrases. We take the 200K data set to examine
the coverage issue. Table 4 is an illustration to com-
pare the new coverage and the old coverage (without
paraphrases) on medium sized training data.
PL Tset PT New Cov.(%) Old Cov.(%)
1 9,264 8,994 97.09 92.03
2 32,805 25,796 78.63 68.40
3 39,918 15,708 39.35 34.55
4 42,247 6,479 15.34 14.29
5 43,088 2,670 6.20 5.99
6 43,066 1,204 2.80 2.77
7 42,602 582 1.37 1.36
8 41,865 319 0.76 0.76
9 40,984 233 0.57 0.57
10 40,002 135 0.34 0.34
Table 4: The coverage of the paraphrase-added test set by
the phrase table on medium size of the training data.
From Table 4, we can see that the coverage of un-
igrams, bigrams, trigrams and 4-grams goes up by
about 5%, 10%, 5% and 1%, while from 5-grams
there is only a slight or no increase in coverage.
These results show that 1) most of the paraphrases
that are added in are lower-order n-grams; 2) the
paraphrases can increase the coverage of the input
by handling the unknown words to some extent.
However, we observed that most untranslated
words in the ?Para-Sub? and ?Lattice? systems are
still NEs, which shows that in our paraphrase table,
there are few paraphrases for the NEs. Therefore,
to further improve the translation quality using para-
phrases, we also need to acquire the paraphrases for
NEs to increase the coverage of unknown words.
427
Source:     whether or the albanian rebels can be genuinely disarmed completely is the main challenge to nato .
Ref:           ??    ??    ??    ?    ??    ??    ?    ??    ?    ??    ??    ?    ??    ??    ?
Baseline:   ??    ?    ??    ??    ?    ?    ??    disarmed    ??    ?    ??    ?    ??   ??   ?
Para-Sub:  ??    ??    ??    ??    ??    ??    ??    ??    ?    ??    ?    ??    ??    ?
Lattice:      ??   ??  ??   ??    ??    ??    ??  ? ?? ??  ?    ??    ?    ??    ??   ? 
Figure 3: An example from three systems to compare the processing of OOVs
5.2 Analysis on Translation Results
In this section, we give an example to show the ef-
fectiveness of using paraphrase lattices to deal with
unknown words. The example is evaluated accord-
ing to both automatic evaluation and human evalua-
tion at sentence level.
See Figure 3 as an illustration of how the
paraphrase-based systems process unknown words.
According to the word alignments between the
source-side sentence and the reference, the word
?disarmed? is translated into two Chinese words ??
?? and ????. These two Chinese words are dis-
continuous in the reference, so it is difficult for the
PB-SMT system to correctly translate the single En-
glish word into a discontinuous Chinese phrase. In
fact in this example, ?disarmed? is an unknownword
and it is kept untranslated in the result of the base-
line system. In the ?Para-Sub? system, it is trans-
lated into ?`? based on a paraphrase pair PP1 =
?disarmed ? disarmament ? 0.087? and its transla-
tion pair T1 = ?disarmament ? `?. The number
?0.087? is the probability p1 that indicates to what
extent these two words are paraphrases. It can be
seen that although ?`? is quite different from the
meaning of ?disarmed?, it is understandable for hu-
man in some sense. In the ?Lattice? system, the
word ?disarmed? is translated into three Chinese
words ?: / ??? based on a paraphrase pair
PP2 = ?disarmed ? demilitarized ? 0.099? and its
translation pair T2 = ?demilitarized ? : / ?
??. The probability p2 is slightly greater than p1.
We argue that the reason that the ?Lattice? system
selects PP2 and T2 rather than PP1 and T1 is be-
cause of the weight estimation in the lattice. That
is, PP2 is more prioritised, while PP1 is more pe-
nalised based on equation (4).
From the viewpoint of human evaluation, the
paraphrase pair PP2 is more appropriate than PP1,
and the translation T2 is more similar to the origi-
nal meaning than T1. The sentence-level automatic
evaluation scores for this example in terms of BLEU
and TER metrics are shown in Table 5.
SYS BLEU TER
Baseline 20.33 66.67
Para-Sub 21.78 53.33
Lattice 23.51 53.33
Table 5: Comparison on sentence-level scores in terms of
BLEU and TER metrics.
The BLEU score of the ?Lattice? system is much
higher than the baseline, and the TER score is quite
a bit lower than the baseline. Therefore, from the
viewpoint of automatic evaluation, the translation
from the ?Lattice? system is also better than those
from the baseline and ?Para-Sub? systems.
6 Conclusions and Future Work
In this paper, we proposed a novel method using
paraphrase lattices to facilitate the translation pro-
cess in SMT. Given an input sentence, our method
firstly discovers all possible paraphrases from a
paraphrase database for N -grams (1 <= N <= 10)
in the test set, and then filters out the paraphrases
which do not appear in the phrase table in order to
avoid adding new unknown words on the input side.
We then use the original words and the paraphrases
to build a word lattice, and set the weights to priori-
tise the original edges and penalise the paraphrase
edges. Finally, we import the lattice into the de-
coder to perform lattice decoding. The experiments
are conducted on English-to-Chinese translation us-
ing the FBIS data set with small and medium-sized
amounts of data, and on a large-scale corpus of 2.1
428
million sentence pairs. We also performed compar-
ative experiments for the baseline, the ?Para-Sub?
system and our paraphrase lattice-based system. The
experimental results show that our proposed system
significantly outperforms the baseline and the ?Para-
Sub? system, and the effectiveness is consistent on
the small, medium and large-scale data sets.
As for future work, firstly we plan to propose a
pruning algorithm for the duplicate paths in the lat-
tice, which will track the edge generation with re-
spect to the path span, and thus eliminate duplicate
paths. Secondly, we plan to experiment with another
feature function in the log-linear model to discount
words derived from paraphrases, and use MERT to
assign an appropriate weight to this feature function.
Acknowledgments
Many thanks to the reviewers for their insight-
ful comments and suggestions. This work is sup-
ported by Science Foundation Ireland (Grant No.
07/CE/I1142) as part of the Centre for Next Genera-
tion Localisation (www.cngl.ie) at Dublin City Uni-
versity.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In 43rd An-
nual meeting of the Association for Computational
Linguistics, Ann Arbor, MI, pages 597?604.
Chris Callison-Burch, Philipp Koehn and Miles Osborne.
2006. Improved statistical machine translation using
paraphrases. In Proceedings of HLT-NAACL 2006:
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL,
NY, USA, pages 17?24.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In 43rd Annual
meeting of the Association for Computational Linguis-
tics, Ann Arbor, MI, pages 263?270.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003: conference combining Hu-
man Language Technology conference series and the
North American Chapter of the Association for Com-
putational Linguistics conference series, Edmonton,
Canada, pages 48?54.
Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Wade Shen, C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and
Evan Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007: demo
and poster sessions, Prague, Czech Republic, pages
177?180.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of ACL-08:HLT. Third Work-
shop on Statistical Machine Translation, Columbus,
Ohio, USA, pages 147?150.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In 41st Annual meeting
of the Association for Computational Linguistics, Sap-
poro, Japan, pages 160?167.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: aMethod for Automatic Eval-
uation of Machine Translation. In 40th Annual meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA, pages 311?318.
Josh Schroeder, Trevor Cohn and Philipp Koehn. 2009.
Word Lattices for Multi-source Translation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL, Athens, Greece, pages 719?727.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, Cam-
bridge, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J.Dorr and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, Athens,
Greece, pages 259?268.
Ying Zhang and Stephan Vogel. 2004. Measuring Con-
fidence Intervals for the Machine Translation Evalua-
tion Metrics. In Proceedings of the 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation (TMI), pages 85?94.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of HLT-NAACL 2006: Proceedings of
the Workshop on Statistical Machine Translation, New
York, pages 138?141.
429
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143?148,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2010
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava,
Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, Andy Way
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
{ spenkale, rhaque, sdandapat, pbanerjee, asrivastava, jdu, ppecina, snaskar, mforcada, away }@computing.dcu.ie
Abstract
This paper describes the DCU machine
translation system in the evaluation cam-
paign of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics
in ACL-2010. We describe the modular
design of our multi-engine machine trans-
lation (MT) system with particular focus
on the components used in this partici-
pation. We participated in the English?
Spanish and English?Czech translation
tasks, in which we employed our multi-
engine architecture to translate. We also
participated in the system combination
task which was carried out by the MBR
decoder and confusion network decoder.
1 Introduction
In this paper, we present the DCU multi-engine
MT system MATREX (Machine Translation using
Examples). This system exploits example-based
MT, statistical MT (SMT), and system combina-
tion techniques.
We participated in the English?Spanish (en?
es) and English?Czech (en?cs) translation
tasks. For these two tasks, we employ several
individual MT systems: 1) Baseline: phrase-
based SMT (Koehn et al, 2007); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004); 3) Factored translation
model (Koehn and Hoang, 2007); 4) Source-side
context-informed (SSCI) systems (Stroppa et al,
2007); 5) the moses-chart (a Moses imple-
mentation of the hierarchical phrase-based (HPB)
approach of Chiang (2007)) and 6) Apertium (For-
cada et al, 2009) rule-based machine translation
(RBMT). Finally, we use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final translation.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypoth-
esis as the alignment reference for the confusion
network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search for the best translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide evaluation results on the test set.
Section 4 concludes the paper.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits as-
pects of both the EBMT and SMT paradigms.
The architecture includes various individual sys-
tems: phrase-based, example-based, hierarchical
phrase-based and tree-based MT.
The combination structure uses the MBR and
CN decoders, and is based on a word-level com-
bination strategy (Du et al, 2009). In the final
stage, we use a new rescoring module to process
the N -best list generated by the combination mod-
ule. Figure 1 illustrates the architecture.
2.2 Example-Based Machine Translation
The EBMT system uses a language-specific, re-
duced set of closed-class marker morphemes or
lexemes (Gough and Way, 2004) to define a way
to segment sentences into chunks, which are then
aligned using an edit-distance-style algorithm, in
which edit costs depend on word-to-word transla-143
Figure 1: System Framework.
tion probabilities and the amount of word-to-word
cognates (Stroppa and Way, 2006).
Once these phrase pairs were obtained they
were merged with the phrase pairs extracted by
the baseline system adding word alignment infor-
mation.
2.3 Apertium RBMT
Apertium1 is a free/open-source platform for
RBMT. The current version of the en?es system
in Apertium was used for the system combination
task (section 2.7), and its morphological analysers
and part-of-speech taggers were used to build a
factored Moses model.
2.4 Factored Translation Model
We also used a factored model for the en?es
translation task. Factored models (Koehn and
Hoang, 2007) facilitate the translation by break-
ing it down into several factors which are further
combined using a log-linear model (Och and Ney,
2002).
We used three factors in our factored translation
model, which are used in two different decoding
paths: a surface form (SF) to SF translation factor,
a lemma to lemma translation factor, and a part-of-
speech (PoS) to PoS translation factor.
Finally, we used two decoding paths based on
1http://www.apertium.org
the above three translation factors: an SF to SF
decoding path and a path which maps lemma to
lemma, PoS to PoS, and an SF generated using
the TL lemma and PoS. The lemmas and PoS for
en and es were obtained using Apertium (sec-
tion 2.3).
2.5 Source-Side Context-informed PB-SMT
One natural way to express a context-informed
feature (h?MBL) is to view it as the conditional
probability of the target phrases (e?k) given the
source phrase (f?k) and its source-side context in-
formation (CI):
h?MBL = logP (e?k|f?k,CI(f?k)) (1)
We use a memory-based machine learning
(MBL) classifier (TRIBL:2 Daelemans and
van den Bosch (2005)) that is able to estimate
P (e?k|f?k,CI(f?k)) by similarity-based reasoning
over memorized nearest-neighbour examples of
source?target phrase translations. In equation (1),
SSCI may include any feature (lexical, syntactic,
etc.), which can provide useful information to
disambiguate a given source phrase. In addition
to using local words and PoS-tags as features,
as in (Stroppa et al, 2007), we incorporate
grammatical dependency relations (Haque et al,
2009a) and supertags (Haque et al, 2009b) as
syntactic source context features in the log-linear
PB-SMT model.
In addition to the above feature, we derived a
simple binary feature h?best, defined in (2):
h?best =
{
1 if e?k maximizes P (e?k|f?k,CI(f?k))
0 otherwise
(2)
We performed experiments by integrating these
two features, h?MBL and h?best, directly into the
log-linear framework of Moses.
2.6 Hierarchical PB-SMT model
For the en?cs translation task, we built
a weighted synchronous context-free grammar
model (Chiang, 2007) of translation that uses
the bilingual phrase pairs of PB-SMT as a start-
ing point to learn hierarchical rules. We used
the open-source Tree-Based translation system
moses-chart3 to perform this experiment.
2An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl
3http://www.statmt.org/moses/?n=Moses.SyntaxTutorial144
2.7 System Combination
For multiple system combination, we used an
MBR-CN framework (Du et al, 2009, 2010) as
shown in Figure 1. Due to the varying word or-
der in the MT hypotheses, it is essential to define
the backbone which determines the general word
order of the CN. Instead of using a single system
output as the skeleton, we employ an MBR de-
coder to select the best single system output Er
from the merged N -best list by minimizing the
BLEU (Papineni et al, 2002) loss, as in (3):
r = argmin
i
Ns?
j=1
(1? BLEU(Ej , Ei)) (3)
where Ns indicates the number of translations in
the merged N -best list, and {Ei}Nsi=1 are the trans-
lations themselves. In our task, we only merge the
1-best output of each individual system.
The CN is built by aligning other hypotheses
against the backbone, based on the TER metric.
Null words are allowed in the alignment. Ei-
ther votes or different confidence measures are as-
signed to each word in the network. Each arc in
the CN represents an alternative word at that po-
sition in the sentence and the number of votes for
each word is counted when constructing the net-
work. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
We use MERT (Och, 2003) to tune the weights
of the CN.
2.8 Rescoring
Rescoring is a very important part in post-
processing which can select a better hypothesis
from the N -best list. We augmented our previ-
ous rescoring model (Du et al, 2009) with more
large-scale data. The features we used include:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram PoS language model (Schmid,
1994; Ratnaparkhi, 1996);
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT.
3 Experimental Setup
This section describes our experimental setup for
the en?cs and en?es translation tasks.
3.1 Data
Bilingual data: In the experiments we used data
sets provided by the workshop organizers. For the
en?cs translation table extraction we employed
both parallel corpora (News-Commentary10 and
CzEng 0.9), and for the en?es experiments, we
used the Europarl(Koehn, 2005), News Commen-
tary and United Nations parallel data. We used a
maximum sentence length of 80 for en?es and
40 for en?cs. Detailed statistics are shown in Ta-
ble 1.
Corpus Langs. Sent. Source
tokens
Target
tokens
Europarl en?es 1.6M 43M 45M
News-comm en?es 97k 2.4M 2.7M
UN en?es 5.9M 160M 190M
News-Comm en?cs 85k 1.8M 1.6M
CzEng en?cs 7.8M 80M 69M
Table 1: Statistics of en?cs and en?es parallel data.
Monolingual data: For language modeling pur-
poses, in addition to the target parts of the bilin-
gual data, we used the monolingual News corpus
for cs; and the Gigaword corpus for es. For both
languages, we used the SRILM toolkit (Stolcke,
2002) to train a 5-gram language model using all
monolingual data provided. However, for en?es
we used the IRSTLM toolkit (Federico and Cet-
tolo, 2007) to train a 5-gram language model using
the es Gigaword corpus. Both language models
use modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Statistics for the monolingual
corpora are given in Table 2.
Corpus Language Sentences Tokens
E/N/NC/UN es 9,6M 290M
Gigaword es 40M 1,2G
News cs 13M 210M
Table 2: Statistics of Monolingual Data. E/N/NC/UN
refers to Europarl/News/News Commentary/United Nations
corpora.
For all the systems except Apertium, we first
lowercase and tokenize all the monolingual and
bilingual data using the tools provided by the
WMT10 organizers. After translation, system
combination output is detokenised and true-cased.145
3.2 English?Czech (en?cs) Experiments
The CzEng corpus (Bojar and Z?abokrtsky?, 2009)
is a collection of parallel texts from sources of dif-
ferent quality and as such it contains some noise.
As the first step, we discarded those sentence pairs
having more than 10% of non-Latin characters.
The CzEng corpus is quite large (8M sen-
tence pairs). Although we were able to build
a vanilla SMT system on all parallel data avail-
able (News-Commentary + CzEng), we also at-
tempted to build additional systems using News-
Commentary data (which we considered in-
domain) and various in-domain subsets of CzEng
hoping to achieve better results on domain-
specific data.
For our first system, we selected 128,218 sen-
tence pairs from CzEng labeled as news. For the
other two systems, we selected subsets of 2M and
4M sentence pairs identified as most similar to
the development sets (as a sample of in-domain
data) based on cosine similarity of their represen-
tation in a TF-IDF weighted vector space model
(cf. Byrne et al (2003)). We also applied the
pseudo-relevavance-feedback technique for query
expansion (Manning et al, 2008) to select another
subset with 2M sentence pairs.
We used the output of 15 systems for sys-
tem combination for the en?cs translation task.
Among these, 5 systems were built using Moses
and varying the size of the training data (DCU-
All, DCU-Ex2M, DCU-4M, DCU-2M and DCU-
News); 9 context-informed PB-SMT systems
(DCU-SSCI-*) using (combinations of) various
context features (word, PoS, supertags and depen-
dency relations) trained only on the News Com-
mentary data (marked with ? in Table 4); and one
system using the moses-chart decoder, also
trained on the news commentary data.
3.3 English?Spanish (en?es) Experiments
Three baseline systems using Moses were built,
where we varied the amount of training data used:
? epn: This system uses all of the Europarl and
News-Commentary parallel data.
? UN-half: This system uses the data suplied
to ?epn?, plus an additional 2.1M sentences
pairs randomly selected from the United Na-
tions corpus.
? all: This system uses all of the available par-
allel data.
For en?es we also obtained output from the
factored model (trained only on the news com-
mentary corpus) and the Apertium RBMT sys-
tem. We also derived phrase alignments using the
MaTrEx EBMT system (Stroppa and Way, 2006),
and added those phrase translations in the Moses
phrase table. The systems marked with ? use a
language model built using the Spanish Gigaword
corpus, in addition to the one built using the pro-
vided monolingual data. These 6 sets of system
outputs are then used for system combination.
3.4 Experimental Results
The evaluation results for en?es and en?cs ex-
periments are shown in Table 3 and Table 4 re-
spectively. The output of the systems marked ?
were submitted in the shared tasks.
System BLEU NIST METEOR TER
DCU-half ?? 29.77% 7.68 59.86% 59.55%
DCU-all ?? 29.63% 7.66 59.82% 59.74%
DCU-epn ?? 29.45% 7.66 59.71% 59.64%
DCU-ebmt ?? 29.38% 7.62 59.59% 60.11%
DCU-factor 22.58% 6.56 54.94% 67.65%
DCU-apertium 19.22% 6.37 49.68% 67.68%
DCU-system-
combination ? 30.42% 7.78 60.56% 58.71%
Table 3: en?es experimental results.
System BLEU NIST METEOR TER
DCU-All 10.91% 4.60 39.18% 81.76%
DCU-Ex2M 10.63% 4.56 39.12% 81.96%
DCU-4M 10.61% 4.56 39.26% 82.04%
DCU-2M 10.48% 4.58 39.35% 81.56%
DCU-Chart 9.34% 4.25 37.04% 83.87%
DCU-News 8.64% 4.16 36.27% 84.96%
DCU-SSCI-ccg? 8.26% 4.02 34.76% 85.58%
DCU-SSCI-
supertag-pair? 8.11% 3.95 34.93% 86.63%
DCU-SSCI-
ccg-ltag? 8.09% 3.96 34.90% 86.62%
DCU-SSCI-PR? 8.06% 4.00 34.89% 85.99%
DCU-SSCI-base? 8.05% 3.97 34.61% 86.02%
DCU-SSCI-PRIR? 8.03% 3.99 34.81% 85.98%
DCU-SSCI-ltag? 8.00% 3.95 34.57% 86.41%
DCU-SSCI-PoS? 7.91% 3.94 34.57% 86.51%
DCU-SSCI-word? 7.57% 3.88 34.16% 87.14%
DCU-system-
combination ? 13.22% 4.98 40.39% 78.59%
Table 4: en?cs experimental results.
4 Conclusion
This paper presents the Dublin City University
MT system in WMT2010 shared task campaign.
This was DCU?s first attempt to translate from en
to es and cs in any shared task. We developed a
multi-engine framework which combined the out-
puts of several individual MT systems and gener-
ated a new N -best list after CN decoding. Then by146
using some global features, the rescoring model
generated the final translation output. The experi-
mental results demonstrated that the combination
module and rescoring module are effective in our
framework for both language pairs, and produce
statistically significant improvements as measured
by bootstrap resampling methods (Koehn, 2004)
on BLEU over the single best system.
Acknowledgements: This work is supported
by Science Foundation Ireland (Grant No.
07/CE/I1142) and by PANACEA, a 7th Frame-
work Research Programme of the European
Union, contract number 7FP-ITC-248064. M.L.
Forcada?s sabbatical stay at Dublin City Univer-
sity is supported by Science Foundation Ireland
through ETS Walton Award 07/W.1/I1802 and by
the Universitat d?Alacant (Spain).
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics,
92:63?83.
Byrne, W., Khudanpur, S., Kim, W., Kumar, S.,
Pecina, P., Virga, P., Xu, P., and Yarowsky, D.
(2003). The Johns Hopkins University 2003
Chinese?English machine translation system.
In Proceedings of MT Summit IX, pages 447?
450, New Orleans, LA.
Chen, S. F. and Goodman, J. (1996). An Empir-
ical Study of Smoothing Techniques for Lan-
guage Modeling. In Proc. 34th Ann. Meeting of
the Association for Computational Linguistics,
pages 310?318, San Francisco, CA.
Chiang, D. (2007). Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daelemans, W. and van den Bosch, A. (2005).
Memory-Based Language Processing (Studies
in Natural Language Processing). Cambridge
University Press, New York, NY.
Du, J., He, Y., Penkale, S., and Way, A. (2009).
MaTrEx: The DCU MT System for WMT2009.
In Proc. 3rd Workshop on Statistical Machine
Translation, EACL 2009, pages 95?99, Athens,
Greece.
Du, J., Pecina, P., and Way, A. (2010). An
Augmented Three-Pass System Combination
Framework: DCU Combination System for
WMT 2010. In Proc. ACL 2010 Joint Workshop
in Statistical Machine Translation and Metrics
Matr, Uppsala, Greece.
Federico, M. and Cettolo, M. (2007). Efficient
Handling of N-gram Language Models for Sta-
tistical Machine Translation. In Proceedings
of the Second Workshop on Statistical Machine
Translation, pages 88?95, Prague, Czech Re-
public.
Fiscus, J. G. (1997). A post-processing sys-
tem to yield reduced word error rates: Recog-
nizer output voting error reduction (ROVER).
In Proceedings 1997 IEEE Workshop on Auto-
matic Speech Recognition and Understanding
(ASRU), pages 347?352, Santa Barbara, CA.
Forcada, M. L., Tyers, F. M., and Ram??rez-
Sa?nchez, G. (2009). The free/open-source ma-
chine translation platform Apertium: Five years
on. In Proceedings of the First International
Workshop on Free/Open-Source Rule-Based
Machine Translation FreeRBMT?09, pages 3?
10.
Gough, N. and Way, A. (2004). Robust Large-
Scale EBMT with Marker-Based Segmenta-
tion. In Proceedings of the 10th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-04), pages
95?104, Baltimore, MD.
Haque, R., Naskar, S. K., Bosch, A. v. d., and
Way, A. (2009a). Dependency relations as
source context in phrase-based smt. In Proc.
23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 170?179,
Hong Kong, China.
Haque, R., Naskar, S. K., Ma, Y., and Way, A.
(2009b). Using supertags as source language
context in SMT. In EAMT-2009: Proceed-
ings of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation,
pages 234?241, Barcelona, Spain.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Proceedings
of EMNLP, volume 4, pages 388?395.
Koehn, P. (2005). Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit X, pages 79?86, Phuket,
Thailand.
Koehn, P. and Hoang, H. (2007). Factored Trans-
lation Models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural147
Language Learning (EMNLP-CoNLL), pages
868?876, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague,
Czech Republic.
Kumar, S. and Byrne, W. (2004). Minimum
Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Meet-
ing of the Human Language Technology Con-
ference and the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL 2004), pages 169?176, Boston,
MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Find-
ing consensus in speech recognition: Word er-
ror minimization and other applications of con-
fusion networks. Computer Speech and Lan-
guage, 14(4):373?400.
Manning, C. D., Raghavan, P., and Schu?tze, H.
(2008). Introduction to Information Retrieval.
Cambridge University Press.
Och, F. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan.
Och, F. and Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In Proceedings of ACL,
volume 2, pages 295?302.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages
311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy
Model for Part-Of-Speech Tagging. In Pro-
ceedings of the Empirical Methods in Natural
Language Processing Conference (EMNLP),
pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S.,
Schwartz, R., Ayan, N. F., and Dorr, B. J.
(2007). Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chap-
ter of the Association for Computational Lin-
guistics (HLT-NAACL 2007), pages 228?235,
Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pages 44?49, Manch-
ester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Amer-
icas (AMTA 2006), pages 223?231, Cambridge,
MA.
Stolcke, A. (2002). SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of
the International Conference Spoken Language
Processing, pages 901?904, Denver, CO.
Stroppa, N., van den Bosch, A., and Way, A.
(2007). Exploiting Source Similarity for SMT
using Context-Informed Features. In Proceed-
ings of the 11th International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-07), pages 231?240,
Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the
DCU machine translation system for IWSLT
2006. In Proceedings of the International Work-
shop on Spoken Language Translation, pages
31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Poste-
rior Probabilities for Statistical Machine Trans-
lation. In Proceedings of the Joint Meeting of
the Human Language Technology Conference
and the North American Chapter of the As-
sociation for Computational Linguistics (HLT-
NAACL 2006), pages 72?77, New York, NY.
148
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 290?295,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
An Augmented Three-Pass System Combination Framework:
DCU Combination System for WMT 2010
Jinhua Du, Pavel Pecina, Andy Way
CNGL, School of Computing
Dublin City University
Dublin 9, Ireland
{jdu,ppecina,away}@computing.dcu.ie
Abstract
This paper describes the augmented three-
pass system combination framework of
the Dublin City University (DCU) MT
group for the WMT 2010 system combi-
nation task. The basic three-pass frame-
work includes building individual confu-
sion networks (CNs), a super network, and
a modified Minimum Bayes-risk (mCon-
MBR) decoder. The augmented parts for
WMT2010 tasks include 1) a rescoring
component which is used to re-rank the
N -best lists generated from the individual
CNs and the super network, 2) a new hy-
pothesis alignment metric ? TERp ? that
is used to carry out English-targeted hy-
pothesis alignment, and 3) more differ-
ent backbone-based CNs which are em-
ployed to increase the diversity of the
mConMBR decoding phase. We took
part in the combination tasks of English-
to-Czech and French-to-English. Exper-
imental results show that our proposed
combination framework achieved 2.17 ab-
solute points (13.36 relative points) and
1.52 absolute points (5.37 relative points)
in terms of BLEU score on English-to-
Czech and French-to-English tasks re-
spectively than the best single system. We
also achieved better performance on hu-
man evaluation.
1 Introduction
In several recent years, system combination has
become not only a research focus, but also a pop-
ular evaluation task due to its help in improving
machine translation quality. Generally, most com-
bination approaches are based on a confusion net-
work (CN) which can effectively re-shuffle the
translation hypotheses and generate a new target
sentence. A CN is essentially a directed acyclic
graph built from a set of translation hypotheses
against a reference or ?backbone?. Each arc be-
tween two nodes in the CN denotes a word or to-
ken, possibly a null item, with an associated pos-
terior probability.
Typically, the dominant CN is constructed at the
word level by a state-of-the-art framework: firstly,
a minimum Bayes-risk (MBR) decoder (Kumar
and Byrne, 2004) is utilised to choose the back-
bone from a merged set of hypotheses, and then
the remaining hypotheses are aligned against the
backbone by a specific alignment approach. Cur-
rently, most research in system combination has
focused on hypothesis alignment due to its signif-
icant influence on combination quality.
A multiple CN or ?super-network? framework
was firstly proposed in Rosti et al (2007) who
used each of all individual system results as the
backbone to build CNs based on the same align-
ment metric, TER (Snover et al, 2006). A consen-
sus network MBR (ConMBR) approach was pre-
sented in (Sim et al, 2007), where MBR decod-
ing is employed to select the best hypothesis with
the minimum cost from the original single system
outputs compared to the consensus output.
Du and Way (2009) proposed a combination
strategy that employs MBR, super network, and
a modified ConMBR (mConMBR) approach to
construct a three-pass system combination frame-
work which can effectively combine different hy-
pothesis alignment results and easily be extended
to more alignment metrics. Firstly, a number of
individual CNs are built based on different back-
bones and different kinds of alignment metrics.
Each network generates a 1-best output. Secondly,
a super network is constructed combining all the
individual networks, and a consensus is generated
based on a weighted search model. In the third290
pass, all the 1-best hypotheses coming from sin-
gle MT systems, individual networks, and the su-
per network are combined to select the final result
using the mConMBR decoder.
In the system combination task of WMT 2010,
we adopted an augmented framework by extend-
ing the strategy in (Du and Way, 2009). In addi-
tion to the basic three-pass architecture, we aug-
ment our combination system as follows:
? We add a rescoring component in Pass 1 and
Pass 2.
? We introduce the TERp (Snover et al, 2009)
alignment metric for the English-targeted
combination.
? We employ different backbones and hypothe-
sis alignment metrics to increase the diversity
of candidates for our mConMBR decoding.
The remainder of this paper is organised as fol-
lows. In Section 2, we introduce the three hy-
pothesis alignment methods used in our frame-
work. Section 3 details the steps for building our
augmented three-pass combination framework. In
Section 4, a rescoring model with rich features
is described. Then, Sections 5 and 6 respec-
tively report the experimental settings and exper-
imental results on English-to-Czech and French-
to-English combination tasks. Section 7 gives our
conclusions.
2 Hypothesis Alignment Methods
Hypothesis alignment plays a vital role in the CN,
as the backbone sentence determines the skeleton
and the word order of the consensus output.
In the combination evaluation task, we inte-
grated TER (Snover et al, 2006), HMM (Ma-
tusov et al, 2006) and TERp (Snover et al,
2009) into our augmented three-pass combination
framework. In this section, we briefly describe
these three methods.
2.1 TER
The TER (Translation Edit Rate) metric measures
the ratio of the number of edit operations between
the hypothesis E? and the reference Eb to the total
number of words in Eb. Here the backbone Eb is
assumed to be the reference. The allowable edits
include insertions (Ins), deletions (Del), substitu-
tions (Sub), and phrase shifts (Shft). The TER of
E? compared to Eb is computed as in (1):
TER(E?, Eb) = Ins + Del + Sub + ShftNb ? 100% (1)
where Nb is the total number of words in Eb. The
difference between TER and Levenshtein edit dis-
tance (or WER) is the sequence shift operation al-
lowing phrasal shifts in the output to be captured.
The phrase shift edit is carried out by a greedy
algorithm and restricted by three constraints: 1)
The shifted words must exactly match the refer-
ence words in the destination position. 2) The
word sequence of the hypothesis in the original
position and the corresponding reference words
must not exactly match. 3) The word sequence
of the reference that corresponds to the desti-
nation position must be misaligned before the
shift (Snover et al, 2006).
2.2 HMM
The hypothesis alignment model based on HMM
(Hidden Markov Model) considers the align-
ment between the backbone and the hypoth-
esis as a hidden variable in the conditional
probability Pr(E?|Eb). Given the backbone
Eb = {e1, . . . , eI} and the hypothesis E? =
{e?1, . . . , e?J}, which are both in the same lan-
guage, the probability Pr(E?|Eb) is defined as in
(2):
Pr(E?|Eb) =
?
A
Pr(E?, A|Eb) (2)
where the alignemnt A ? {(j, i) : 1 ? j ?
J ; 1 ? i ? I}, i and j represent the word po-
sition in Eb and E? respectively. Hence, the align-
ment issue is to seek the optimum alignment A?
such that:
A? = argmax
A
P (A|eI1, e?J1 ) (3)
For the HMM-based model, equation (2) can be
represented as in (4):
Pr(E?|Eb) =
?
aJj
J?
j=1
[p(aj |aj?1, I) ? p(e?j |eaj )] (4)
where p(aj |aj?1, I) is the alignment probability
and p(e?j |ei) is the translation probability.
2.3 TER-Plus
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms (Snover
et al, 2009). In addition, it uses probabilistic
phrasal substitutions to align phrases in the hy-
pothesis and reference. In contrast to the use of291
the constant edit cost for all operations such as
shifts, insertion, deleting or substituting in TER,
all edit costs in TERp are optimized to maximize
correlation with human judgments.
TERp uses all the edit operations of TER ?
matches, insertions, deletions, substitutions, and
shifts ? as well as three new edit operations:
stem matches, synonym matches, and phrase sub-
stitutions (Snover et al, 2009). TERp employs
the Porter stemming algorithm (Porter, 1980) and
WordNet (Fellbaum, 1998) to perform the ?stem
match? and ?synonym match? respectively. Se-
quences of words in the reference are considered
to be paraphrases of a sequence of words in the
hypothesis if that phrase pair occurs in the TERp
phrase table (Snover et al, 2009).
In our experiments, TERp was used for the
French-English system combination task, and we
used the default configuration of optimised edit
costs.
3 Augmented Three-Pass Combination
Framework
The construction of the augmented three-pass
combination framework is shown in Figure 1.
Hypotheses Set
BLEU TER TERp
MBR
BLEU TER TERp
Top M Single
HMM TER TERp
Alignment
Individual CNs
Nbest 
Re-ranking Super CN Networks
mConMBR
Pass 1
Pass 2
Pass 3
N Single MT 
Systems
Figure 1: Three-Pass Combination Framework
In Figure 1, the dashed boxes labeled ?TERp?
indicate that the TERp alignment is only appli-
cable for English-targeted hypothesis alignment.
The lines with arrows pointing to ?mConMBR?
represent adding outputs into the mConMBR de-
coding component. ?Top M Single? indicates that
the 1-best results from the best M individual MT
systems are also used as backbones to build in-
dividual CNs under different alignment metrics.
The three dashed boxes represent Pass 1, Pass 2
and Pass 3 respectively. The steps can be sum-
marised as follows:
Pass 1: Specific Metric-based Single Networks
1. Merge all the 1-best hypotheses from single
MT systems into a new N -best set Ns.
2. Utilise the standard MBR decoder to se-
lect one from the Ns as the backbone given
some specific loss function such as TER,
BLEU (Papineni et al, 2002) and TERp; Ad-
ditionally, in order to increase the diversity
of candidates used for Pass 2 and Pass 3, we
also use the 1-best hypotheses from the top
M single MT systems as the backbone. Add
the backbones generated by MBR into Ns.
3. Perform the word alignment between the dif-
ferent backbones and the other hypotheses
via the TER, HMM, TERp (only for English)
metrics.
4. Carry out word reordering based on word
alignment (TER and TERp have completed
the reordering in the process of scoring) and
build individual CNs (Rosti et al, 2007);
5. Decode the single networks and export the 1-
best outputs and the N -best lists separately.
Add these 1-best outputs into Ns.
Pass 2: Super-Network
1. Connect the single networks using a start
node and an end node to form a super-
network based on multiple hypothesis align-
ment and different backbones. In this evalu-
ation, we set uniform weights for these dif-
ferent individual networks when building the
super network(Du and Way, 2009).
2. Decode the super network and generate a
consensus output as well as the N -best list.
Add the 1-best result into Ns.
3. Rescore the N -best lists from all individual
networks and super network and add the new
1-best results into Ns.
Pass 3: mConMBR
1. Rename the set Ns as a new set Ncon;
2. Use mConMBR decoding to search for the
best final result from Ncon. In this step, we
set a uniform distribution between the candi-
dates in Ncon.292
4 Rescoring Model
We adapted our previous rescoring model (Du
et al, 2009) to larger-scale data. The features we
used are as follows:
? Direct and inverse IBM model;
? 4-gram and 5-gram target language model;
? 3, 4, and 5-gram Part-of-Speech (POS) lan-
guage model (Schmid, 1994; Ratnaparkhi,
1996);
? Sentence-length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
best list (Zens and Ney, 2006);
? Minimum Bayes Risk cost. This process is
similar to the calculation of the MBR decod-
ing in which we take the current hypothesis
in the N -best list as the ?backbone?, and then
calculate and sum up all the Bayes risk cost
between the backbone and each of the rest of
the N -best list using BLEU metric as the loss
function;
? Length ratio between source and target sen-
tence.
The weights are optimized via the MERT algo-
rithm (Och, 2003).
5 Experimental Settings
We participated in the English?Czech and
French?English system combination tasks.
In our system combination framework, we use
a large-scale monolingual data to train language
models and carry out POS-tagging.
5.1 English-Czech
Training Data
The statistics of the data used for language models
training are shown in Table 1.
Monolingual Number of
Corpus tokens (Cz) sentences
News-Comm 2,214,757 84,706
CzEng 81,161,278 8,027,391
News 205,600,053 13,042,040
Total 288,976,088 21,154,137
Table 1: Statistics of data in the En?Cz task
All the data are provided by the workshop
organisers. 1 In Table 1, ?News-Comm? indi-
cates the data set of News-Commentary v1.0 and
1http://www.statmt.org/wmt10/translation-task.html
?CzEng? is the Czech?English corpus v0.9 (Bo-
jar and Z?abokrtsky?, 2009). ?News? is the Czech
monolingual News corpus.
As to our CN and rescoring components,
we use ?News-Comm+CzEng? to train a
4-gram language model and use ?News-
Comm+CzEng+News? to train a 5-gram
language model. Additionally, we per-
form POS tagging (Hajic?, 2004) for ?News-
Comm+CzEng+News? data, and train 3-gram,
4-gram, and 5-gram POS-tag language models.
Devset and Testset
The devset includes 455 sentences and the testset
contains 2,034 sentences. Both data sets are pro-
vided by the workshop organizers. Each source
sentence has only one reference. There are 11 MT
systems in the En-Cz track and we use all of them
in our combination experiments.
5.2 French-English
Training Data
The statistics of the data used for language models
training and POS tagging are shown in Table 2.
Monolingual Number of
Corpus tokens (En) sentences
News-Comm 2,973,711 125,879
Europarl 50,738,215 1,843,035
News 1,131,527,255 48,648,160
Total 1,184,234,384 50,617,074
Table 2: Statistics of data in the Fr?En task
?News? is the English monolingual News
corpus. We use ?News-Comm+Europarl? to
train a 4-gram language model and use ?News-
Comm+Europarl+News? to train a 5-gram lan-
guage model. We also perform POS tagging (Rat-
naparkhi, 1996) for all available data, and train
3-gram, 4-gram and, 5-gram POS-tag language
models.
Devset and Testset
We also use all the 1-best results to carry out sys-
tem combination. There are 14 MT systems in the
Fr-En track and we use all of them in our combi-
nation experiments.
6 Experimental Results
In this section, all the results are reported on de-
vsets in terms of BLEU and NIST scores.
6.1 English?Czech
In this task, we only used one hypothesis align-
ment method ? TER ? to carry out hypothesis293
alignment. However, in order to increase diversity
for our 3-pass framework, in addition to using the
output from MBR decoding as the backbone, we
also separately selected the top 4 individual sys-
tems (SYS1, SYS4, SYS6, and SYS11 in our sys-
tem set) in terms of BLEU scores on the devset as
the backbones so that we can build multiple indi-
vidual CNs for the super network. All the results
are shown in Table 3.
SYS BLEU4 NIST
Worst 9.09 3.83
Best 17.28 4.99
SYS1 15.11 4.76
SYS4 12.67 4.40
SYS6 17.28 4.99
SYS11 15.75 4.81
CN-SYS1 17.36 5.12
CN-SYS4 16.94 5.10
CN-SYS6 17.91 5.13
CN-SYS11 17.45 5.09
CN-MBR 18.29 5.15
SuperCN 18.44 5.17
mConMBR-BAS 18.60 5.18
mConMBR-New 18.84 5.11
Table 3: Automatic evaluation of the combination
results on the En-Cz devset.
?Worst? indicates the 1-best hypothesis from
the worst single system, the ?Best? is the 1-best
hypothesis from the best single system (SYS11)).
?CN-SYSX? denotes that we use SYSX (X =
1, 4, 6, 11 and MBR) as the backbone to build an
individual CN. ?mConMBR-BAS? stands for the
original three-pass combination framework with-
out rescoring component, while ?mConMBR-
New? indicates the proposed augmented combina-
tion framework. It can be seen from Table 3 that 1)
in all individual CNs, the CN-MBR achieved the
best performance; 2) SuperCN and mConMBR-
New improved by 1.16 (6.71% relative) and 1.56
(9.03% relative) absolute BLEU points compared
to the best single MT system. 3) our new
three-pass combination framework achieved the
improvement of 0.24 absolute (1.29% relative)
BLEU points than the original framework.
The final results on the test set are shown in Ta-
ble 4.
SYS BLEU4 human eval.(%win)
Best 16.24 70.38
mConMBR-BAS 17.91 -
mConMBR-New 18.41 2 75.17
Table 4: Evaluation of the combination results on
the En-Cz testset.
It can be seen that our ?mConMBR-New?
framework performs better than the best single
system and our original framework ?mConMBR-
BAS? in terms of automatic BLEU scores and hu-
man evaluation for the English-to-Czech task. In
this task campaign, we achieved top 1 in terms of
the human evaluation.
6.2 French?English
We used three hypothesis alignment methods ?
TER, TERp and HMM ? to carry out word align-
ment between the backbone and the rest of the
hypotheses. Apart from the backbone generated
from MBR, we separately select the top 5 individ-
ual systems (SYS1, SYS10, SYS11, SYS12, and
SYS13 in our system set) respectively as the back-
bones using HMM, TER and TERp to carry out
hypothesis alignment so that we can build more
individual CNs for the super network to increase
the diversity of candidates for mConMBR. The re-
sults are shown in Table 5.3
SYS BLEU4(%) NIST
Worst 15.04 4.97
Best 28.88 6.71
CN-SYS1-TER 29.56 6.78
CN-SYS1-HMM 29.60 6.84
CN-SYS1-TERp 29.77 6.83
CN-MBR-TER 30.16 6.91
CN-MBR-HMM 30.19 6.92
CN-MBR-TERp 30.27 6.92
SuperCN 30.58 6.90
mConMBR-BAS 30.74 7.01
mConMBR-New 31.02 6.96
Table 5: Automatic evaluation of the combination
results on the Fr-En devset.
?CN-MBR-X? represents the different possi-
ble hypothesis alignment methods (X = {TER,
HMM, TERp}) which are used to build indi-
vidual CNs using the output from MBR de-
coding as the backbone. We can see that the
SuperCN and mConMBR-New respectively im-
proved by 1.7 absolute (5.89% relative) and 2.88
absolute (9.97% relative) BLEU points compared
to the best single system. Furthermore, our aug-
mented framework ?mConMBR-New? achieved
the improvement of 0.28 absolute (0.91% relative)
BLEU points than the original three-pass frame-
work as well.
2This score was measured in-house on the refer-
ence provided by the organizer using metric mteval-v13
(ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl).
3In this Table, we take SYS1 as an example to show the
results using a single MT system as the backbone under the
three alignment metrics.294
The final results on the test set are shown in Ta-
ble 6.
SYS BLEU4 human eval.(%win)
Best 28.30 66.84
mConMBR-BAS 29.21 -
mConMBR-New 29.82 2 72.15
Table 6: Evaluation of the combination results on
Fr-En test set.
It can be seen that our ?mConMBR-New?
framework performs the best than the best single
system and our original framework ?mConMBR-
BAS? in terms of automatic BLEU scores and hu-
man evaluation for the French?English task.
7 Conclusions and Future Work
We proposed an augmented three-pass mul-
tiple system combination framework for the
WMT2010 system combination shared task. The
augmented parts include 1) a rescoring model to
select the potential 1-best result from the indi-
vidual CNs and super network to increase the di-
versity for ?mConMBR? decoding; 2) a new hy-
pothesis alignment metric ?TERp? for English-
targeted alignment; 3) 1-best results from the top
M individual systems employed to build CNs
to augment the ?mConMBR? decoding. We
took part in the English-to-Czech and French-to-
English tasks. Experimental results reported on
test set of these two tasks showed that our aug-
mented framework performed better than the best
single system in terms of BLEU scores and hu-
man evaluation. Furthermore, the proposed aug-
mented framework achieved better results than our
basic three-pass combination framework (Du and
Way, 2009) as well in terms of automatic evalua-
tion scores. In the released preliminary results, we
achieved top 1 and top 3 for the English-to-Czech
and French-to-English tasks respectively in terms
of human evaluation.
As for future work, firstly we plan to do further
experiments using automatic weight-tuning algo-
rithm to tune our framework. Secondly, we plan
to examine how the differences between the hy-
pothesis alignment metrics impact on the accuracy
of the super network. We also intend to integrate
more alignment metrics to the networks and verify
on the other language pairs.
Acknowledgments
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University
and has been partially funded by PANACEA, a 7th Frame-
work Research Programme of the European Union (contract
number: 7FP-ITC-248064) as well as partially supported by
the project GA405/09/0278 of the Grant Agency of the Czech
Republic. Thanks also to the reviewers for their insightful
comments.
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9: Large Par-
allel Treebank with Rich Annotation. Prague Bulletin of
Mathematical Linguistics, 92.
Du, J., He, Y., Penkale, S., and Way, A. (2009). MaTrEx:
The DCU MT System for WMT2009. In Proceedings of
the EACL-WMT 2009, pages 95?99, Athens, Greece.
Du, J. and Way, A. (2009). A Three-pass System Com-
bination Framework by Combining Multiple Hypothesis
Alignment Methods. In Proceedings of the International
Conference on Asian Language Processing (IALP), pages
172?176, Singapore.
Fellbaum, C., editor (1998). WordNet: an electronic lexical
database. MIT Press.
Hajic?, J. (2004). Disambiguation of Rich Inflection (Compu-
tational Morphology of Czech), volume 1. Charles Uni-
versity Press, Prague.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL 2004, pages 169?176, Boston,
MA.
Matusov, E., Ueffing, N., and Ney, H. (2006). Computing
consensus translation from multiple machine translation
systems using enhanced hypotheses alignment. In Pro-
ceedings of EACL?06, pages 33?40.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the ACL-02, pages 311?
318, Philadelphia, PA.
Porter, M. F. (1980). An algorithm for suffix stripping, pro-
gram.
Ratnaparkhi, A. (1996). A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
EMNLP?96, pages 133?142, Philadelphia, PA.
Rosti, A., Matsoukas, S., and Schwartz, R. (2007). Improved
Word-Level System Combination for Machine Transla-
tion. In Proceedings of ACL?07, pages 312?319.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44?49, Manchester, UK.
Sim, K., Byrne, W., Gales, M., Sahbi, H., and Woodland, P.
(2007). Consensus network decoding for statistical ma-
chine translation system combination. In Proceedings of
the ICASSP?07, pages 105?108.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In Proceedings of the
AMTA?06), pages 223?231, Cambridge, MA.
Snover, M., Madnani, N., J.Dorr, B., and Schwartz, R.
(2009). Fluency, adequacy, or HTER? Exploring different
human judgments with a tunable MT metric. In Proceed-
ings of the WMT?09, pages 259?268, Athens, Greece.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
HLT-NAACL?06), pages 72?77, New York, USA.
295
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349?353,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The DCU Dependency-Based Metric in WMT-MetricsMATR 2010
Yifan He Jinhua Du Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
Dublin 9, Ireland
{yhe,jdu,away,josef}@computing.dcu.ie
Abstract
We describe DCU?s LFG dependency-
based metric submitted to the shared eval-
uation task of WMT-MetricsMATR 2010.
The metric is built on the LFG F-structure-
based approach presented in (Owczarzak
et al, 2007). We explore the following
improvements on the original metric: 1)
we replace the in-house LFG parser with
an open source dependency parser that
directly parses strings into LFG depen-
dencies; 2) we add a stemming module
and unigram paraphrases to strengthen the
aligner; 3) we introduce a chunk penalty
following the practice of METEOR to re-
ward continuous matches; and 4) we intro-
duce and tune parameters to maximize the
correlation with human judgement. Exper-
iments show that these enhancements im-
prove the dependency-based metric?s cor-
relation with human judgement.
1 Introduction
String-based automatic evaluation metrics such as
BLEU (Papineni et al, 2002) have led directly
to quality improvements in machine translation
(MT). These metrics provide an alternative to ex-
pensive human evaluations, and enable tuning of
MT systems based on automatic evaluation results.
However, there is widespread recognition in
the MT community that string-based metrics are
not discriminative enough to reflect the translation
quality of today?s MT systems, many of which
have gone beyond pure string-based approaches
(cf. (Callison-Burch et al, 2006)).
With that in mind, a number of researchers have
come up with metrics which incorporate more so-
phisticated and linguistically motivated resources.
Examples include METEOR (Banerjee and Lavie,
2005; Lavie and Denkowski, 2009) and TERP
(Snover et al, 2010), both of which now uti-
lize stemming, WordNet and paraphrase informa-
tion. Experimental and evaluation campaign re-
sults have shown that these metrics can obtain bet-
ter correlation with human judgements than met-
rics that only use surface-level information.
Given that many of today?s MT systems incor-
porate some kind of syntactic information, it was
perhaps natural to use syntax in automatic MT
evaluation as well. This direction was first ex-
plored by (Liu and Gildea, 2005), who used syn-
tactic structure and dependency information to go
beyond the surface level matching.
Owczarzak et al (2007) extended this line of
research with the use of a term-based encoding of
Lexical Functional Grammar (LFG:(Kaplan and
Bresnan, 1982)) labelled dependency graphs into
unordered sets of dependency triples, and calculat-
ing precision, recall, and F-score on the triple sets
corresponding to the translation and reference sen-
tences. With the addition of partial matching and
n-best parses, Owczarzak et al (2007)?s method
considerably outperforms Liu and Gildea?s (2005)
w.r.t. correlation with human judgement.
The EDPM metric (Kahn et al, 2010) im-
proves this line of research by using arc labels
derived from a Probabilistic Context-Free Gram-
mar (PCFG) parse to replace the LFG labels,
showing that a PCFG parser is sufficient for pre-
processing, compared to a dependency parser in
(Liu and Gildea, 2005) and (Owczarzak et al,
2007). EDPM also incorporates more information
sources: e.g. the parser confidence, the Porter
stemmer, WordNet synonyms and paraphrases.
Besides the metrics that rely solely on the de-
pendency structures, information from the depen-
dency parser is a component of some other metrics
that use more diverse resources, such as the textual
entailment-based metric of (Pado et al, 2009).
In this paper we extend the work of (Owczarzak
et al, 2007) in a different manner: we use an
349
adapted version of the Malt parser (Nivre et al,
2006) to produce 1-best LFG dependencies and
allow triple matches where the dependency la-
bels are different. We incorporate stemming, syn-
onym and paraphrase information as in (Kahn et
al., 2010), and at the same time introduce a chunk
penalty in the spirit of METEOR to penalize dis-
continuous matches. We sort the matches accord-
ing to the match level and the dependency type,
and weight the matches to maximize correlation
with human judgement.
The remainder of the paper is organized as fol-
lows. Section 2 reviews the dependency-based
metric. Sections 3, 4, 5 and 6 introduce our im-
provements on this metric. We report experimen-
tal results in Section 7 and conclude in Section 8.
2 The Dependency-Based Metric
In this section, we briefly review the metric pre-
sented in (Owczarzak et al, 2007).
2.1 C-Structure and F-Structure in LFG
In Lexical Functional Grammar (Kaplan and Bres-
nan, 1982), a sentence is represented as both a hi-
erarchical c-(onstituent) structure which captures
the phrasal organization of a sentence, and a f-
(unctional) structure which captures the functional
relations between different parts of the sentence.
Our metric currently only relies on the f-structure,
which is encoded as labeled dependencies in our
metric.
2.2 MT Evaluation as Dependency Triple
Matching
The basic method of (Owczarzak et al, 2007) can
be illustrated by the example in Table 1.
The metric in (Owczarzak et al, 2007) performs
triple matching over the Hyp- and Ref-Triples and
calculates the metric score using the F-score of
matching precision and recall. Let m be the num-
ber of matches, h be the number of triples in the
hypothesis and e be the number of triples in the
reference. Then we have the matching precision
P = m/h and recall R = m/e. The score of the
hypothesis in (Owczarzak et al, 2007) is the F-
score based on the precision and recall of match-
ing as in (1):
Fscore = 2PRP +R (1)
Table 1: Sample Hypothesis and Reference
Hypothesis
rice will be held talks in egypt next week
Hyp-Triples
adjunct(will, rice)
xcomp(will, be)
adjunct(talks, held)
xcomp(be, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
adjunct(talks, week)
Reference
rice to hold talks in egypt next week
Ref-Triples
obl(rice, to)
obj(hold, to)
adjunct(week, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
obj(hold, week)
2.3 Details of the Matching Strategy
(Owczarzak et al, 2007) uses several techniques
to facilitate triple matching. First of all, consider-
ing that the MT-generated hypotheses have vari-
able quality and are sometimes ungrammatical,
the metric will search the 50-best parses of both
the hypothesis and reference and use the pair that
has the highest F-score to compensate for parser
noise.
Secondly, the metric performs complete or par-
tial matching according to the dependency labels,
so the metric will find more matches on depen-
dency structures that are presumably more infor-
mative.
More specifically, for all except the LFG
Predicate-Only labeled triples of the form
dep(head, modifier), the method does not
allow a match if the dependency labels (deps)
are different, thus enforcing a complete match.
For the Predicate-Only dependencies, par-
tial matching is allowed: i.e. two triples are con-
sidered identical even if only the head or the
modifier are the same.
Finally, the metric also uses linguistic resources
for better coverage. Besides using WordNet syn-
onyms, the method also uses the lemmatized out-
put of the LFG parser, which is equivalent to using
350
an English lemmatizer.
If we do not consider these additional lin-
guistic resources, the metric would find the fol-
lowing matches in the example in Table 1:
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next), as these three
triples appear both in the reference and in the hy-
pothesis.
2.4 Points for Improvement
We see several points for improvement from Table
1 and the analysis above.
? More linguistic resources: we can use more
linguistic resources than WordNet in pursuit
of better coverage.
? Using the 1-best parse instead of 50-best
parses: the parsing model we currently use
does not produce k-best parses and using only
the 1-best parse significantly improves the
speed of triple matching. We allow ?soft?
triple matches to capture the triple matches
which we might otherwise miss using the 1-
best parse.
? Rewarding continuous matches: it
would be more desirable to reflect
the fact that the 3 matching triples
adjunct(talks, in), obj(in,
egypt) and adjunct(week, next)
are continuous in Table 1.
We introduce our improvements to the metric
in response to these observations in the following
sections.
3 Producing and Matching LFG
Dependency Triples
3.1 The LFG Parser
The metric described in (Owczarzak et al, 2007)
uses the DCU LFG parser (Cahill et al, 2004)
to produce LFG dependency triples. The parser
uses a Penn treebank-trained parser to produce
c-structures (constituency trees) and an LFG f-
structure annotation algorithm on the c-structure
to obtain f-structures. In (Owczarzak et al, 2007),
triple matching on f-structures produced by this
paradigm correlates well with human judgement,
but this paradigm is not adequate for the WMT-
MetricsMatr evaluation in two respects: 1) the in-
house LFG annotation algorithm is not publicly
available and 2) the speed of this paradigm is not
satisfactory.
We instead use the Malt Parser1 (Nivre et al,
2006) with a parsing model trained on LFG de-
pendencies to produce the f-structure triples. Our
collaborators2 first apply the LFG annotation algo-
rithm to the Penn Treebank training data to obtain
f-structures, and then the f-structures are converted
into dependency trees in CoNLL format to train
the parsing model. We use the liblinear (Fan et
al., 2008) classification module to for fast parsing
speed.
3.2 Hard and Soft Dependency Matching
Currently our parser produces only the 1-best
outputs. Compared to the 50-best parses in
(Owczarzak et al, 2007), the 1-best parse limits
the number of triple matches that can be found. To
compensate for this, we allow triple matches that
have the same Head and Modifier to consti-
tute a match, even if their dependency labels are
different. Therefore for triples Dep1(Head1,
Mod1) and Dep2(Head2, Mod2), we allow
three types of match: a complete match if
the two triples are identical, a partial match if
Dep1=Dep2 and Head1=Head2, and a soft
match if Head1=Head2 and Mod1=Mod2.
4 Capturing Variations in Language
In (Owczarzak et al, 2007), lexical variations at
the word-level are captured by WordNet. We
use a Porter stemmer and a unigram paraphrase
database to allow more lexical variations.
With these two resources combined, there are
four stages of word level matching in our sys-
tem: exact match, stem match, WordNet match and
unigram paraphrase match. The stemming mod-
ule uses Porter?s stemmer implementation3 and the
WordNet module uses the JAWS WordNet inter-
face.4 Our metric only considers unigram para-
phrases, which are extracted from the paraphrase
database in TERP5 using the script in the ME-
TEOR6 metric.
1http://maltparser.org/index.html
2O?zlem C?etinog?lu and Jennifer Foster at the National
Centre for Language Technology, Dublin City University
3http://tartarus.org/?martin/
PorterStemmer/
4http://lyle.smu.edu/?tspell/jaws/
index.html
5http://www.umiacs.umd.edu/?snover/
terp/
6http://www.cs.cmu.edu/?alavie/METEOR/
351
5 Adding Chunk Penalty to the
Dependency-Based Metric
The metric described in (Owczarzak et al, 2007)
does not explicitly consider word order and flu-
ency. METEOR, on the other hand, utilizes this in-
formation through a chunk penalty. We introduce
a chunk penalty to our dependency-based metric
following METEOR?s string-based approach.
Given a reference r = wr1...wrn, we denote
wri as ?covered? if it is the head or modifier of
a matched triple. We only consider the wris that
appear as head or modifier in the reference
triples. After this notation, we follow METEOR?s
approach by counting the number of chunks in
the reference string, where a chunk wrj ...wrk is
a sequence of adjacent covered words in the refer-
ence. Using the hypothesis and reference in Ta-
ble 1 as an example, the three matched triples
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next) will cover a con-
tinuous word sequence in the reference (under-
lined), constituting one single chunk:
rice to hold talks (in) egypt next week
Based on this observation, we introduce a simi-
lar chunk penalty Pen as in METEOR in our met-
ric, as in 2:
Pen = ? ? ( #chunks#matches )
? (2)
where ? and ? are free parameters, which we tune
in Section 6.2. We add this penalty to the depen-
dency based metric (cf. Eq. (1)), as in Eq. (3).
score = (1? Pen) ? Fscore (3)
6 Parameter Tuning
6.1 Parameters of the Metric
In our metric, dependency triple matches can be
categorized according to many criteria. We as-
sume that some matches are more critical than
others and encode the importance of matches by
weighting them differently. The final match will
be the sum of weighted matches, as in (4):
m =
?
?tmt (4)
where ?t and mt are the weight and number of
match category t. We categorize a triple match ac-
cording to three perspectives: 1) the level of match
L={complete, partial}; 2) the linguistic resource
used in matching R={exact, stem, WordNet, para-
phrase}; and 3) the type of dependency D. To
avoid too large a number of parameters, we only
allow a set of frequent dependency types, along
with the type other, which represents all the other
types and the type soft for soft matches. We have
D={app, subj, obj, poss, adjunct, topicrel, other,
soft}.
Therefore for each triple match m, we can have
the type of the match t ? L?R?D.
6.2 Tuning
In sum, we have the following parameters to tune
in our metric: precision weight ?, chunk penalty
parameters ?, ?, and the match type weights
?1...?n. We perform Powell?s line search (Press et
al., 2007) on the sufficient statistics of our metric
to find the set of parameters that maximizes Pear-
son?s ? on the segment level. We perform the op-
timization on the MT06 portion of the NIST Met-
ricsMATR 2010 development set with 2-fold cross
validation.
7 Experiments
We experiment with four settings of the metric:
HARD, SOFT, SOFTALL and WEIGHTED in or-
der to validate our enhancements. The first two
settings compare the effect of allowing/not al-
lowing soft matches, but only uses WordNet as
in (Owczarzak et al, 2007). The third setting ap-
plies our additional linguistic features and the final
setting tunes parameter weights for higher correla-
tion with human judgement.
We report Pearson?s r, Spearman?s ? and
Kendall?s ? on segment and system levels on the
NIST MetricsMATR 2010 development set using
Snover?s scoring tool.7
Table 2: Correlation on the Segment Level
r ? ?
HARD 0.557 0.586 0.176
SOFT 0.600 0.634 0.213
SOFTALL 0.633 0.662 0.235
WEIGHTED 0.673 0.709 0.277
Table 2 shows that allowing soft triple matches
and using more linguistic features all lead
to higher correlation with human judgement.
Though the parameters might somehow overfit on
7http://www.umiacs.umd.edu/?snover/
terp/scoring/
352
the data set even if we apply cross validation, this
certainly confirms the necessity of weighing de-
pendency matches according to their types.
Table 3: Correlation on the System Level
r ? ?
HARD 0.948 0.905 0.786
SOFT 0.964 0.905 0.786
SOFTALL 0.975 0.976 0.929
WEIGHTED 0.989 1.000 1.000
When considering the system-level correlation
in Table 3, the trend is very similar to that of the
segment level. The improvements we introduce all
lead to improvements in correlation with human
judgement.
8 Conclusions and Future Work
In this paper we describe DCU?s dependency-
based MT evaluation metric submitted to WMT-
MetricsMATR 2010. Building upon the LFG-
based metric described in (Owczarzak et al,
2007), we use a publicly available parser instead
of an in-house parser to produce dependency la-
bels, so that the metric can run on a third party
machine. We improve the metric by allowing more
lexical variations and weighting dependency triple
matches depending on their importance according
to correlation with human judgement.
For future work, we hope to apply this method
to languages other than English, and performmore
refinement on dependency type labels and linguis-
tic resources.
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank O?zlem C?etinog?lu and Jennifer Foster for providing
us with the LFG parsing model for the Malt Parser, as well as
the anonymous reviewers for their insightful comments.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, pages
65?72, Ann Arbor, MI.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance depen-
dency resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings of the
42nd Meeting of the Association for Computational Lin-
guistics (ACL-2004), pages 319?326, Barcelona, Spain.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluation the role of bleu in machine trans-
lation research. In Proceedings of 11th Conference of the
European Chapter of the Association for Computational
Linguistics, pages 249?256, Trento, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2010. Expected dependency pair match: predicting trans-
lation quality with expected syntactic structure. Machine
Translation.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammatical
representation. The mental representation of grammatical
relations, pages 173?281.
Alon Lavie andMichael J. Denkowski. 2009. he meteor met-
ric for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization,
pages 25?32, Ann Arbor, MI.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In In The fifth international conference on Lan-
guage Resources and Evaluation (LREC-2006), pages
2216?2219, Genoa, Italy.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation eval-
uation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 104?111, Prague, Czech
Republic.
Sebastian Pado, Michel Galley, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Robust machine translation
evaluation with entailment features. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 297?305,
Suntec, Singapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics
(ACL-2002), pages 311?318, Philadelphia, PA.
William H. Press, Saul A. Teukolsky, William T. Vetterling,
and Brian P. Flannery. 2007. Numerical Recipes 3rd Edi-
tion: The Art of Scientific Computing. Cambridge Univer-
sity Press, New York, NY.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2010. Ter-plus: paraphrase, semantic, and
alignment enhancements to translation edit rate. Machine
Translation.
353
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19?27,
COLING 2010, Beijing, August 2010.
Source-side Syntactic Reordering Patterns with Functional Words for
Improved Phrase-based SMT
Jie Jiang, Jinhua Du, Andy Way
CNGL, School of Computing, Dublin City University 
{jjiang,jdu,away}@computing.dcu.ie
Abstract
Inspired by previous source-side syntactic
reordering methods for SMT, this paper
focuses on using automatically learned
syntactic reordering patterns with func-
tional words which indicate structural re-
orderings between the source and target
language. This approach takes advan-
tage of phrase alignments and source-side
parse trees for pattern extraction, and then
filters out those patterns without func-
tional words. Word lattices transformed
by the generated patterns are fed into PB-
SMT systems to incorporate potential re-
orderings from the inputs. Experiments
are carried out on a medium-sized cor-
pus for a Chinese?English SMT task. The
proposed method outperforms the base-
line system by 1.38% relative on a ran-
domly selected testset and 10.45% rela-
tive on the NIST 2008 testset in terms
of BLEU score. Furthermore, a system
with just 61.88% of the patterns filtered
by functional words obtains a comparable
performance with the unfiltered one on the
randomly selected testset, and achieves
1.74% relative improvements on the NIST
2008 testset.
1 Introduction
Previous work has shown that the problem of
structural differences between language pairs in
SMT can be alleviated by source-side syntactic
reordering. Taking account for the integration
with SMT systems, these methods can be divided
into two different kinds of approaches (Elming,
2008): the deterministic reordering and the non-
deterministic reordering approach.
To carry out the deterministic approach, syntac-
tic reordering is performed uniformly on the train-
ing, devset and testset before being fed into the
SMT systems, so that only the reordered source
sentences are dealt with while building during
the SMT system. In this case, most work is fo-
cused on methods to extract and to apply syntac-
tic reordering patterns which come from manually
created rules (Collins et al, 2005; Wang et al,
2007a), or via an automatic extraction process tak-
ing advantage of parse trees (Collins et al, 2005;
Habash, 2007). Because reordered source sen-
tence cannot be undone by the SMT decoders (Al-
Onaizan et al, 2006), which implies a systematic
error for this approach, classifiers (Chang et al,
2009b; Du & Way, 2010) are utilized to obtain
high-performance reordering for some specialized
syntactic structures (e.g. DE construction in Chi-
nese).
On the other hand, the non-deterministic ap-
proach leaves the decisions to the decoders to
choose appropriate source-side reorderings. This
is more flexible because both the original and
reordered source sentences are presented in the
inputs. Word lattices generated from syntactic
structures for N-gram-based SMT is presented
in (Crego et al, 2007). In (Zhang et al, 2007a;
Zhang et al, 2007b), chunks and POS tags are
used to extract reordering rules, while the gener-
ated word lattices are weighted by language mod-
els and reordering models. Rules created from a
syntactic parser are also utilized to form weighted
n-best lists which are fed into the decoder (Li et
al., 2007). Furthermore, (Elming, 2008; Elm-
19
ing, 2009) uses syntactic rules to score the output
word order, both on English?Danish and English?
Arabic tasks. Syntactic reordering information is
also considered as an extra feature to improve PB-
SMT in (Chang et al, 2009b) for the Chinese?
English task. These results confirmed the effec-
tiveness of syntactic reorderings.
However, for the particular case of Chinese
source inputs, although the DE construction has
been addressed for both PBSMT and HPBSMT
systems in (Chang et al, 2009b; Du & Way,
2010), as indicated by (Wang et al, 2007a), there
are still lots of unexamined structures that im-
ply source-side reordering, especially in the non-
deterministic approach. As specified in (Xue,
2005), these include the bei-construction, ba-
construction, three kinds of de-construction (in-
cluding DE construction) and general preposition
constructions. Such structures are referred with
functional words in this paper, and all the con-
structions can be identified by their correspond-
ing tags in the Penn Chinese TreeBank. It is in-
teresting to investigate these functional words for
the syntactic reordering task since most of them
tend to produce structural reordering between the
source and target sentences.
Another related work is to filter the bilingual
phrase pairs with closed-class words (Sa?nchez-
Mart??nez, 2009). By taking account of the word
alignments and word types, the filtering process
reduces the phrase tables by up to a third, but still
provide a system with competitive performance
compared to the baseline. Similarly, our idea is to
use special type of words for the filtering purpose
on the syntactic reordering patterns.
In this paper, our objective is to exploit
these functional words for source-side syntac-
tic reordering of Chinese?English SMT in the
non-deterministic approach. Our assumption is
that syntactic reordering patterns with functional
words are the most effective ones, and others can
be pruned for both speed and performance.
To validate this assumption, three systems are
compared in this paper: a baseline PBSMT sys-
tem, a syntactic reordering system with all pat-
terns extracted from a corpus, and a syntactic re-
ordering system with patterns filtered with func-
tional words. To accomplish this, firstly the lat-
tice scoring approach (Jiang et al, 2010) is uti-
lized to discover non-monotonic phrase align-
ments, and then syntactic reordering patterns are
extracted from source-side parse trees. After that,
functional word tags specified in (Xue, 2005) are
adopted to perform pattern filtering. Finally, both
the unfiltered pattern set and the filtered one are
used to transform inputs into word lattices to
present potential reorderings for improving PB-
SMT system. A comparison between the three
systems is carried out to examine the performance
of syntactic reordering as well as the usefulness of
functional words for pattern filtering.
The rest of this paper is organized as follows:
in section 2 we describe the extraction process of
syntactic reordering patterns, including the lattice
scoring approach and the extraction procedures.
Then section 3 presents the filtering process used
to obtain patterns with functional words. After
that, section 4 shows the generation of word lat-
tices with patterns, and experimental setup and re-
sults included related discussion are presented in
section 5. Finally, we give our conclusion and av-
enues for future work in section 6.
2 Syntactic reordering patterns
extraction
Instead of top-down approaches such as (Wang
et al, 2007a; Chang et al, 2009a), we use a
bottom-up approach similar to (Xia et al, 2004;
Crego et al, 2007) to extract syntactic reordering
patterns from non-monotonic phrase alignments
and source-side parse trees. The following steps
are carried out to extract syntactic reordering pat-
terns: 1) the lattice scoring approach proposed
in (Jiang et al, 2010) is used to obtain phrase
alignments from the training corpus; 2) reorder-
ing regions from the non-monotonic phrase align-
ments are used to identify minimum treelets for
pattern extraction; and 3) the treelets are trans-
formed into syntactic reordering patterns which
are then weighted by their occurrences in the
training corpus. Details of each of these steps are
presented in the rest of this section.
2.1 Lattice scoring for phrase alignments
The lattice scoring approach is proposed in (Jiang
et al, 2010) for the SMT data cleaning task.
20
To clean the training corpus, word alignments
are used to obtain approximate decoding results,
which are then used to calculate BLEU (Papineni
et al, 2002) scores to filter out low-scoring sen-
tences pairs. The following steps are taken in
the lattice scoring approach: 1) train an initial
PBSMT model; 2) collect anchor pairs contain-
ing source and target phrase positions from word
alignments generated in the training phase; 3)
build source-side lattices from the anchor pairs
and the translation model; 4) search on the source-
side lattices to obtain approximate decoding re-
sults; 5) calculate BLEU scores for the purpose of
data cleaning.
Note that the source-side lattices in step 3 come
from anchor pairs, so each edge in the lattices con-
tain both the source and target phrase positions.
Thus the outputs of step 4 contain phrase align-
ments on the training corpus. These phrase align-
ments are used to identify non-monotonic areas
for the extraction of reordering patterns.
2.2 Reordering patterns
Non-monotonic regions of the phrase alignments
are examined as potential source-side reorderings.
By taking a bottom-up approach, the reordering
regions are identified and mapped to minimum
treelets on the source parse trees. After that, syn-
tactic reordering patterns are transformed from
these minimum treelets.
In this paper, reordering regions A and B indi-
cating swapping operations on the source side are
only considered as potential source-side reorder-
ings. Thus, given reordering regions AB, this im-
plies (1):
AB ? BA (1)
on the source-side word sequences. Referring to
the phrase alignment extraction in the last section,
each non-monotonic phrase alignment produces
one reordering region. Furthermore, for each re-
ordering region identified, all of its sub-areas in-
dicating non-monotonic alignments are also at-
tempted to produce more reordering regions.
To represent the reordering region using syn-
tactic structure, given the extracted reordering re-
gions AB, the following steps are taken to map
them onto the source-side parse trees, and to gen-
erate corresponding patterns:
1. Generate a parse tree for each of the source
sentences. The Berkeley parser (Petrov,
2006) is used in this paper. To obtain sim-
pler tree structures, right-binarization is per-
formed on the parse trees, while tags gener-
ated from binarization are not distinguished
from the original ones (e.g. @V P and V P
are the same).
2. Map reordering regions AB onto the parse
trees. Denote NA as the set of leaf nodes in
region A and NB for region B. The mapping
is carried out on the parse tree to find a mini-
mum treelet T , which satisfies the following
two criteria: 1) there must exist a path from
each node in NA ? NB to the root node of
T ; 2) each leaf node of T can only be the
ancestor of nodes in NA or NB (or none of
them).
3. Traverse T in pre-order to obtain syntactic
reordering pattern P . Label all the leaf nodes
of T with A or B as reorder options, which
indicate that the descendants of nodes with
label A are supposed to be swapped with
those with label B.
Instead of using subtrees, we use treelets to
refer the located parse tree substructures, since
treelets do not necessarily go down to leaf nodes.
Since phrase alignments cannot always be per-
fectly matched with parse trees, we also expand
AB to the right and/or the left side with a limited
number of words to find a minimum treelet. In
this situation, a minimum number of ancestors of
expanded tree nodes are kept in T but they are as-
signed the same labels as those from which they
have been expanded. In this case, the expanded
tree nodes are considered as the context nodes of
syntactic reordering patterns.
Figure 1 illustrates the extraction process. Note
the symbol @ indicates the right-binarization sym-
bols (e.g. @V P in the figure). In the figure, tree
T (surrounded by dashed lines) is the minimum
treelet mapped from the reordering region AB.
Leaf node NP is labeled by A, V P is labeled by
B, and the context node P is also labeled by A.
Leaf nodes labeled A or B are collected into node
sequences LA or LB to indicate the reordering op-
21
A B
T
Figure 1: Reordering pattern extraction
erations. Thus the syntactic reordering pattern P
is obtained from T as in (2):
P = {V P (PP (P NP ) V P )|O = {LA, LB}}
(2)
where the first part of P is the V P with its tree
structure, and the second part O indicates the re-
ordering scheme, which implies that source words
corresponding with descendants of LA are sup-
posed to be swapped with those of LB .
2.3 Pattern weights estimation
We use preo to represent the chance of reordering
when a treelet is located by a pattern on the parse
tree. It is estimated by the number of reorderings
for each of the occurrences of the pattern as in (3):
preo(P ) =
count{reorderings of P}
count{observation of P} (3)
By contrast, one syntactic pattern P usually con-
tains several reordering schemes (specified in for-
mula (2)), each of them weighted as in (4):
w(O,P ) = count{reorderings of O in P}count{reorderings of P}
(4)
Generally, a syntactic reordering pattern is ex-
pressed as in (5):
P = {tree | preo | O1, w1, ? ? ? , On, wn} (5)
where tree is the tree structures of the pattern,
preo is the reordering probability, Oi and wi are
the reordering schemes and weights (1 ? i ? n).
3 Patterns with functional words
Some of the patterns extracted may not benefit
the final system since the extraction process is
controlled by phrase alignments rather than syn-
tactic knowledge. Inspired by the study of DE
constructions (Chang et al, 2009a; Du & Way,
2010), we assume that syntactic reorderings are
indicated by functional words for the Chinese?
English task. To incorporate the knowledge of
functional words into the extracted patterns, in-
stead of directly specifying the syntactic struc-
ture from the linguistic aspects, we use functional
word tags to filter the extracted patterns. In this
case, we assume that all patterns containing func-
tional words tend to produce meaningful syntactic
reorderings. Thus the filtered patterns carry the re-
ordering information from the phrase alignments
as well as the linguistic knowledge. Thus the
noise produced in phrase alignments and the size
of pattern set can be reduced, so that the speed and
the performance of the system can be improved.
The functional word tags used in this paper are
shown in Table 1, which come from (Xue, 2005).
We choose them as functional words because nor-
mally they imply word reorders between Chinese
and English sentence pairs.
Tag Description
BA ba-construction
DEC de (1st kind) in a relative-clause
DEG associative de (1st kind)
DER de (2nd kind) in V-de const. & V-de-R
DEV de (3rd kind) before VP
LB bei in long bei-construction
P preposition excluding bei and ba
SB bei in short bei-construction
Table 1: Syntactic reordering tags for functional
words
Note that there are three kinds of de-
constructions, but only the first kind is the DE
construction in (Chang et al, 2009a; Du & Way,
2010). After the filtering process, both the unfil-
tered pattern set and the filtered one are used to
build different syntactic reordering PBSMT sys-
tems for comparison purpose.
22
4 Word lattice construction
Both the devset and testset are transformed into
word lattices by the extracted patterns to incor-
porate potential reorderings. Figure 2 illustrates
this process: treelet T ? is matched with a pat-
tern, then its leaf nodes {a1, ? ? ? am} ? LA (span-
ning {w1, ? ? ? , wp}) are swapped with leaf nodes
{b1, ? ? ? , bn} ? LB (spanning {v1, ? ? ? , vq}) on
the generated paths in the word lattice.
T?
a1
am b1
bn
... ...
... ...
w1 w2 ... wp v1 v2 vq...
w1 w2 ... wp v1 v2 vq...
w2...
wpv1
v2 ...
... ...
vq w1
Sub parse tree 
matched with 
a pattern
Source side 
sentence
Generated 
lattice
Figure 2: Incorporating potential reorderings into
lattices
We sort the matched patterns by preo in formula
(5), and only apply a pre-defined number of re-
orderings for each sentence. For each lattice node,
if we denote E0 as the edge from the original sen-
tence, while patterns {P1, ? ? ? , Pi, ? ? ? , Pk} are ap-
plied to this node, then E0 is weighted as in (6):
w(E0) = ? +
k?
i=1
{(1? ?)k ? {1? preo(Pi)}}
(6)
where preo(Pi) is the pattern weight in formula
(3), and ? is the base probability to avoid E0 be-
ing equal to zero. Suppose {Es, ? ? ? , Es+r?1} are
generated by r reordering schemes of Pi, then Ej
is weighted as in (7):
w(Ej) =
(1 ? ?)
k ?preo(Pi)?
ws?j+1(Pi)?r
t=1 wt(Pi)
(7)
where wt(Pi) is the reordering scheme in formula
(5), and s <= j < s + r. Reordering patterns
with the same root lattice node share equal proba-
bilities in formula (6) and (7).
5 Experiments and results
We conducted our experiments on a medium-sized
corpus FBIS (a multilingual paragraph-aligned
corpus with LDC resource number LDC2003E14)
for the Chinese?English SMT task. The Cham-
pollion aligner (Ma, 2006) is utilized to perform
sentence alignment. A total number of 256,911
sentence pairs are obtained, while 2,000 pairs for
devset and 2,000 pairs for testset are randomly se-
lected, which we call FBIS set. The rest of the
data is used as the training corpus.
The baseline system is Moses (Koehn et
al., 2007), and GIZA++1 is used to perform
word alignment. Minimum error rate training
(MERT) (Och, 2003) is carried out for tuning. A
5-gram language model built via SRILM2 is used
for all the experiments in this paper.
Experiments results are reported on two differ-
ent sets: the FBIS set and the NIST set. For the
NIST set, the NIST 2005 testset (1,082 sentences)
is used as the devset, and the NIST 2008 test-
set (1,357 sentences) is used as the testset. The
FBIS set contains only one reference translation
for both devset and testset, while NIST set has
four references.
5.1 Pattern extraction and filtering with
functional words
The lattice scoring approach is carried out with
the same baseline system as specified above to
produce the phrase alignments. The initial PB-
SMT system in the lattice scoring approach is
tuned with the FBIS devset to obtain the weights.
As specified in section 2.1, phrase alignments are
generated in the step 4 of the lattice scoring ap-
proach.
From the generated phrase alignments and
source-side parse trees of the training corpus,
we obtain 48,285 syntactic reordering patterns
(57,861 reordering schemes) with an average
number of 11.02 non-terminals. For computa-
tional efficiency, any patterns with number of non-
terminal less than 3 and more than 9 are pruned.
This procedure leaves 18,169 syntactic reordering
patterns (22,850 reordering schemes) with a aver-
1http://fjoch.com/GIZA++.html
2http://www.speech.sri.com/projects/srilm/
23
age number of 7.6 non-terminals. This pattern set
is used to built the syntactic reordering PBSMT
system without pattern filtering, which here after
we call the ?unfiltered system?.
Using the tags specified in Table 1, the ex-
tracted syntactic reordering patterns without func-
tional words are filtered out, while only 6,926 syn-
tactic reordering patterns (with 9,572 reordering
schemes) are retained. Thus the pattern set are
reduced by 61.88%, and over half of them are
pruned by the functional word tags. The filtered
pattern set is used to build the syntactic reorder-
ing PBSMT system with pattern filtering, which
we refer as the ?filtered system?.
Type Tag Patterns Percent
ba-const. BA 222 3.20%
bei-const. LB 97 2.79%SB 96
de-const. (1st) DEC 1662 60.11%DEG 2501
de-const. (2nd) DER 52 0.75%
de-const. (3rd) DEV 178 2.57%
preposition P 2591 37.41%
excl. ba & bei
Table 2: Statistics on the number of patterns for
each type of functional word
Statistics on the patterns with respect to func-
tional word types are shown in Table 2. The num-
ber of patterns for each functional word in the fil-
tered pattern set are illustrated, and percentages of
functional word types are also reported. Note that
some patterns contain more than one kind of func-
tional word, so that the percentages of functional
word types do not sum to one.
As demonstrated in Table 2, the first kind of de-
construction takes up 60.11% of the filtered pat-
tern set, and is the main type of patterns used in
our experiment. This indicates that more than half
of the patterns are closely related to the DE con-
struction examined in (Chang et al, 2009b; Du
& Way, 2010). However, the general preposi-
tion construction (excluding bei and ba) accounts
for 37.41% of the filtered patterns, which implies
that it is also a major source of syntactic reorder-
ing. By contrast, other constructions have much
smaller amount of percentages, so have a minor
impact on our experiments.
5.2 Word lattice construction
As specified in section 4, for both unfiltered and
the filtered systems, both the devset and testset
are converted into word lattices with the unfiltered
and filtered syntactic reordering patterns respec-
tively. To avoid a dramatic increase in size of the
lattices, the following constraints are applied: for
each source sentence, the maximum number of re-
ordering schemes is 30, and the maximum span of
a pattern is 30.
For the lattice construction, the base probabil-
ity in (6) and (7) is set to 0.05. The two syntac-
tic reordering PBSMT systems also incorporate
the built-in reordering models (distance-based and
lexical reordering) of Moses, and their weights in
the log-linear model are tuned with respect to the
devsets.
The effects of the pattern filtering by functional
words are also reported in Table 3. For both the
FBIS and NIST sets, the average number of nodes
in word lattices are illustrated before and after pat-
tern filtering. From the table, it is clear that the
pattern filtering procedure dramatically reduces
the input size for the PBSMT system. The reduc-
tion is up to 37.99% for the NIST testset.
Data set Unfiltered Filtered Reduced
FBIS dev 183.13 131.38 28.26%
FBIS test 183.68 136.56 25.65%
NIST dev 175.78 115.89 34.07%
NIST test 149.13 92.48 37.99%
Table 3: Comparison of the average number of
nodes in word lattices
5.3 Results on FBIS set
Three systems are compared on the FBIS set:
the baseline PBSMT system, and the syntactic
reordering systems with and without pattern fil-
tering. Since the built-in reordering models of
Moses are enabled, several values of the distor-
tion limit (DL) parameter are chosen to validate
consistency. The evaluation results on the FBIS
set are shown in Table 4.
As shown in Table 4, the syntactic reordering
systems with and without pattern filtering outper-
24
System DL BLEU NIST METE
Baseline
0 22.32 6.45 52.51
6 23.67 6.63 54.07
10 24.52 6.66 54.04
12 24.57 6.69 54.31
Unfiltered
0 23.92 6.60 54.30
6 24.57 6.68 54.64
10 24.98 6.71 54.67
12 24.84 6.69 54.65
Filtered
0 23.71 6.60 54.11
6 24.65 6.68 54.61
10 24.87 6.71 54.84
12 24.91 6.7 54.51
Table 4: Results on FBIS testset (DL = distortion
limit, METE=METEOR)
form the baseline system for each of the distortion
limit parameters in terms of the BLEU, NIST and
METEOR scores (scores in bold face). By con-
trast, the filtered systems has a comparable perfor-
mance with the unfiltered system: for some of the
distortion limits, the filtered systems even outper-
forms the unfiltered system (scores in bold face,
e.g. BLEU and NIST for DL=12, METEOR for
DL=10).
The best performance of the baseline system
is obtained with distortion limit 12 (underlined);
the best performance of the unfiltered system is
achieved with distortion limit 10 (underlined);
while for the filtered system, the best BLEU score
is accomplished with distortion limit 12 (under-
lined), and the best NIST and METEOR scores
are shown with distortion limit 10 (underlined).
Thus the unfiltered system outperforms the base-
line by 0.41 (1.67% relative) BLEU points, 0.02
(0.30% relative) NIST points and 0.36 (0.66%
relative) METEOR points. By contrast, the fil-
tered system outperforms the baseline by 0.34
(1.38% relative) BLEU points, 0.02 (0.30% rel-
ative) NIST points and 0.53 (0.98% relative) ME-
TEOR points.
Compared with the unfiltered system, pattern
filtering with functional words degrades perfor-
mance by 0.07 (0.28% relative) in term of BLEU,
but improves the system by 0.17 (0.31% rela-
tive) in term of METEOR, while the two systems
achieved the same best NIST score.
These results indicates that the filtered system
has a comparable performance with the unfiltered
one on the FBIS set, while both of them outper-
form the baseline.
5.4 Results on NIST set
The evaluation results on the NIST set are illus-
trated in Table 5.
System DL BLEU NIST METE
Baseline
0 14.43 5.75 45.03
6 15.61 5.88 45.75
10 15.73 5.78 45.27
12 15.89 6.16 45.88
Unfiltered
0 16.77 6.54 47.16
6 17.25 6.67 47.65
10 17.15 6.64 47.78
12 16.88 6.56 47.17
Filtered
0 16.79 6.64 47.67
6 17.55 6.71 48.06
10 17.51 6.72 48.15
12 17.37 6.72 48.08
Table 5: Results on NIST testset (DL = distortion
limit, METE=METEOR)
From Table 5, the unfiltered system outper-
forms the baseline system for each of the distor-
tion limits in terms of the BLEU, NIST and ME-
TEOR scores (scores in bold face). By contrast,
the filtered system also outperform the unfiltered
system for each of the distortion limits in terms of
the three evaluation methods (scores in bold face).
The best performance of the baseline system
is obtained with distortion limit 12 (underlined),
while the best performance of the unfiltered sys-
tem is obtained with distortion limit 6 for BLEU
and NIST, and 10 for METEOR (underlined). For
the filtered system, the best BLEU score is shown
with distortion limit 6, and the best NIST and ME-
TEOR scores are accomplished with distortion
limit 10 (underlined). Thus the unfiltered system
outperforms the baseline by 1.36 (8.56% relative)
BLEU points, 0.51 (8.28% relative) NIST points
and 1.90 (4.14% relative) METEOR points. By
contrast, the filtered system outperforms the base-
line by 1.66 (10.45% relative) BLEU points, 0.56
(9.52% relative) NIST points and 2.27 (4.95% rel-
ative) METEOR points.
25
Compared with the unfiltered system, patterns
with functional words boost the performance by
0.30 (1.74% relative) in term of BLEU, 0.05
(0.75% relative) in term of NIST, and 0.37 (0.77%
relative) in term of METEOR.
These results demonstrate that the pattern filter-
ing improves the syntactic reordering system on
the NIST set, while both of them significantly out-
perform the baseline.
5.5 Discussion
Experiments in the previous sections demonstrate
that: 1) the two syntactic reordering systems im-
prove the PBSMT system by providing potential
reorderings obtained from phrase alignments and
parse trees; 2) patterns with functional words play
a major role in the syntactic reordering process,
and filtering the patterns with functional words
maintains or even improves the system perfor-
mance for Chinese?English SMT task. Further-
more, as shown in the previous section, pattern
filtering prunes the whole pattern set by 61.88%
and also reduces the sizes of word lattices by up
to 37.99%, thus the whole syntactic reordering
procedure for the original inputs as well as the
tuning/decoding steps are sped up dramatically,
which make the proposed methods more useful in
the real world, especially for online SMT systems.
From the statistics on the filtered pattern set
in Table 2, we also argue that the first kind
of de-construction and general preposition (ex-
cluding bei and ba) are the main sources of
Chinese?English syntactic reordering. Previous
work (Chang et al, 2009b; Du & Way, 2010)
showed the advantages of dealing with the DE
construction. In our experiments too, even though
all the patterns are automatically extracted from
phrase alignments, these two constructions still
dominate the filtered pattern set. This result con-
firms the effectiveness of previous work on DE
construction, and also highlights the importance
of the general preposition construction in this task.
6 Conclusion and future work
Syntactic reordering patterns with functional
words are examined in this paper. The aim is to
exploit these functional words within the syntactic
reordering patterns extracted from phrase align-
ments and parse trees. Three systems are com-
pared: a baseline PBSMT system, a syntactic re-
ordering system with all patterns extracted from a
corpus and a syntactic reordering system with pat-
terns filtered with functional words. Evaluation
results on a medium-sized corpus showed that the
two syntactic reordering systems consistently out-
perform the baseline system. The pattern filtering
with functional words prunes 61.88% of patterns,
but still maintains a comparable performance with
the unfiltered one on the randomly select testset,
and even obtains 1.74% relative improvement on
the NIST 2008 testset.
In future work, the structures of patterns con-
taining functional words will be investigated to
obtain fine-grained analysis on such words in this
task. Furthermore, experiments on larger corpora
as well as on other language pairs will also be car-
ried out to validation our method.
Acknowledgements
This research is supported by Science Foundation
Ireland (Grant 07/CE/I1142) as part of the Centre
for Next Generation Localisation (www.cngl.ie) at
Dublin City University. Thanks to Yanjun Ma for
the sentence-aligned FBIS corpus.
References
Yaser Al-Onaizan and Kishore Papineni 2006. Dis-
tortion models for statistical machine translation.
Coling-ACL 2006: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pages 529-536, Sydney,
Australia.
Pi-Chuan Chang, Dan Jurafsky, and Christopher
D.Manning 2009a. Disambiguating DE for
Chinese?English machine translation. Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 215-223, Athens, Greece.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009b. Discriminative
reordering with Chinese grammatical features. Pro-
ceedings of SSST-3: Third Workshop on Syntax and
Structure in Statistical Translation, pages 51-59,
Boulder, CO.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. ACL-2005: 43rd Annual meeting of the
26
Association for Computational Linguistics, pages
531-540, University of Michigan, Ann Arbor, MI.
Josep M. Crego, and Jose? B. Marin?o. 2007. Syntax-
enhanced N-gram-based SMT. MT Summit XI,
pages 111-118, Copenhagen, Denmark.
Jinhua Du and Andy Way. 2010. The Impact of
Source-Side Syntactic Reordering on Hierarchical
Phrase-based SMT. EAMT 2010: 14th Annual Con-
ference of the European Association for Machine
Translation, Saint-Raphae?l, France.
Jakob Elming. 2008. Syntactic reordering integrated
with phrase-based SMT. Coling 2008: 22nd In-
ternational Conference on Computational Linguis-
tics, Proceedings of the conference, pages 209-216,
Manchester, UK.
Jakob Elming, and Nizar Habash. 2009. Syntac-
tic reordering for English-Arabic phrase-based ma-
chine translation. Proceedings of the EACL 2009
Workhop on Computational Approaches to Semitic
Languages, pages 69-77, Athens, Greece.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. MT Summit XI, pages
215-222, Copenhagen, Denmark.
Jie Jiang, Andy Way, Julie Carson-Berndsen. 2010.
Lattice Score-Based Data Cleaning For Phrase-
Based Statistical Machine Translation. EAMT
2010: 14th Annual Conference of the European As-
sociation for Machine Translation, Saint-Raphae?l,
France.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. ACL 2007: proceedings of demo and
poster sessions, pp. 177-180, Prague, Czech Repub-
lic.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. ACL 2007: proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 720-727, Prague, Czech
Republic.
Xiaoyi Ma. 2006. Champollion: A Robust Paral-
lel Text Sentence Aligner. LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation, pp.489-492, Genova, Italy.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. ACL-2003:
41st Annual meeting of the Association for Compu-
tational Linguistics, pp. 160-167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method For Automatic
Evaluation of Machine Translation. ACL-2002:
40th Annual meeting of the Association for Compu-
tational Linguistics, pp.311-318, Philadelphia, PA.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. Coling-ACL 2006:
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 433-440, Sydney, Australia.
Felipe Sa?nchez-Mart??nez and Andy Way. 2009.
Marker-based filtering of bilingual phrase pairs for
SMT. EAMT-2009: Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation, pages 144-151, Barcelona,
Spain.
Chao Wang, Michael Collins, and Philipp Koehn.
2007a. Chinese syntactic reordering for statistical
machine translation. EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
737-745, Prague, Czech Republic.
Fei Xia, and Michael McCord 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. Coling 2004: 20th International
Conference on Computational Linguistics, pages
508-514, University of Geneva, Switzerland.
Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2), pages 207-238.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
Proceedings of the 47th Annual Meeting of the ACL
and the 4th IJCNLP, pages 333-341, Suntec, Singa-
pore.
Yuqi Zhang, Richard Zens, and Hermann Ney 2007a.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statisti-
cal machine translation. SSST, NAACL-HLT-2007
AMTA Workshop on Syntax and Structure in Statis-
tical Translation, pages 1-8, Rochester, NY.
Yuqi Zhang, Richard Zens, and Hermann Ney 2007b.
Improved chunk-level reordering for statistical ma-
chine translation. IWSLT 2007: International Work-
shop on Spoken Language Translation, pages 21-28,
Trento, Italy.
27
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31?40,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Incorporating Source-Language Paraphrases into Phrase-Based SMT with
Confusion Networks
Jie Jiang,? Jinhua Du,? and Andy Way?
?CNGL, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland
{jjiang, away}@computing.dcu.ie
?School of Automation and Information Engineering,
Xi?an University of Technology, Xi?an, Shaanxi, China
jhdu@xaut.edu.cn
Abstract
To increase the model coverage, source-
language paraphrases have been utilized to
boost SMT system performance. Previous
work showed that word lattices constructed
from paraphrases are able to reduce out-of-
vocabulary words and to express inputs in
different ways for better translation quality.
However, such a word-lattice-based method
suffers from two problems: 1) path dupli-
cations in word lattices decrease the capac-
ities for potential paraphrases; 2) lattice de-
coding in SMT dramatically increases the
search space and results in poor time effi-
ciency. Therefore, in this paper, we adopt
word confusion networks as the input struc-
ture to carry source-language paraphrase in-
formation. Similar to previous work, we use
word lattices to build word confusion net-
works for merging of duplicated paths and
faster decoding. Experiments are carried out
on small-, medium- and large-scale English?
Chinese translation tasks, and we show that
compared with the word-lattice-based method,
the decoding time on three tasks is reduced
significantly (up to 79%) while comparable
translation quality is obtained on the large-
scale task.
1 Introduction
With the rapid development of large-scale parallel
corpus, research on data-driven SMT has made good
progress to the real world applications. Currently,
for a typical automatic translation task, the SMT
system searches and exactly matches the input sen-
tences with the phrases or rules in the models. Obvi-
ously, if the following two conditions could be sat-
isfied, namely:
? the words in the parallel corpus are highly
aligned so that the phrase alignment can be per-
formed well;
? the coverage of the input sentence by the paral-
lel corpus is high;
then the ?exact phrase match? translation method
could bring a good translation.
However, for some language pairs, it is not easy
to obtain a huge amount of parallel data, so it is not
that easy to satisfy these two conditions. To allevi-
ate this problem, paraphrase-enriched SMT systems
have been proposed to show the effectiveness of in-
corporating paraphrase information. In terms of the
position at which paraphrases are incorporated in the
MT-pipeline, previous work can be organized into
three different categories:
? Translation model augmentation with para-
phrases (Callison-Burch et al, 2006; Marton et
al., 2009). Here the focus is on the translation
of unknown source words or phrases in the in-
put sentences by enriching the translation table
with paraphrases.
? Training corpus augmentation with para-
phrases (Bond et al, 2008; Nakov, 2008a;
Nakov, 2008b). Paraphrases are incorporated
into the MT systems by expanding the training
data.
? Word-lattice-based method with para-
phrases (Du et al, 2010; Onishi et al,
31
2010). Instead of augmenting the transla-
tion table, source-language paraphrases are
constructed to enrich the inputs to the SMT
system. Another directly related work is to
use word lattices to deal with multi-source
translation (Schroeder et al, 2009), in which
paraphrases are actually generated from the
alignments of difference source sentences.
Comparing these three methods, the word-lattice-
based method has the least overheads because:
? The translation model augmentation method
has to re-run the whole MT pipeline once
the inputs are changed, while the word-lattice-
based method only need to transform the new
input sentences into word lattices.
? The training corpus augmentation method re-
quires corpus-scale expansion, which drasti-
cally increases the computational complexity
on large corpora, while the word-lattice-based
method only deals with the development set
and test set.
In (Du et al, 2010; Onishi et al, 2010), it is also
observed that the word-lattice-based method per-
formed better than the translation model augmen-
tation method on different scales and two different
language pairs in several translation tasks. Thus
they concluded that the word-lattice-based method
is preferable for this task.
However, there are still some drawbacks for the
word-lattice-based method:
? In the lattice construction processing, dupli-
cated paths are created and fed into SMT de-
coders. This decreases the paraphrase capacity
in the word lattices. Note that we use the phrase
?paraphrase capacity? to represent the amount
of paraphrases that are actually built into the
word lattices. As presented in (Du et al, 2010),
only a limited number of paraphrases are al-
lowed to be used while others are pruned during
the construction process, so duplicate paths ac-
tually decrease the number of paraphrases that
contribute to the translation quality.
? The lattice decoding in SMT decoder have
a very high computational complexity which
makes the system less feasible in real time ap-
plication.
Therefore, in this paper, we use confusion net-
works (CNs) instead of word lattices to carry para-
phrase information in the inputs for SMT decoders.
CNs are constructed from the aforementioned word
lattices, while duplicate paths are merged to increase
paraphrase capacity (e.g. by admitting more non-
duplicate paraphrases without increasing the input
size). Furthermore, much less computational com-
plexity is required to perform CN decoding instead
of lattice decoding in the SMT decoder. We car-
ried out experiments on small-, medium- and large-
scale English?Chinese translation tasks to compare
against a baseline PBSMT system, the translation
model augmentation of (Callison-Burch et al, 2006)
method and the word-lattice-based method of (Du et
al., 2010) to show the effectiveness of our novel ap-
proach.
The motivation of this work is to use CN as
the compromise between speed and quality, which
comes from previous studies in speech recog-
nition and speech translation: in (Hakkani-Tu?r
et al, 2005), word lattices are transformed into
CNs to obtain compact representations of multiple
aligned ASR hypotheses in speech understanding;
in (Bertoldi et al, 2008), CNs are also adopted
instead of word lattices as the source-side inputs
for speech translation systems. The main contribu-
tion of this paper is to show that this compromise
also works for SMT systems incorporating source-
language paraphrases in the inputs.
Regarding the use of paraphrases SMT system,
there are still other two categories of work that are
related to this paper:
? Using paraphrases to improve system optimiza-
tion (Madnani et al, 2007). With an English?
English MT system, this work utilises para-
phrases to reduce the number of manually
translated references that are needed in the
parameter tuning process of SMT, while pre-
served a similar translation quality.
? Using paraphrases to smooth translation mod-
els (Kuhn et al, 2010; Max, 2010). Either
cluster-based or example-based methods are
32
proposed to obtain better estimation on phrase
translation probabilities with paraphrases.
The rest of this paper is organized as follows:
In section 2, we present an overview of the word-
lattice-based method and its drawbacks. Section 3
proposes the CN-based method, including the build-
ing process and its application on paraphrases in
SMT. Section 4 presents the experiments and results
of the proposed method as well as discussions. Con-
clusions and future work are then given in Section
5.
2 Word-lattice-based method
Compared with translation model augmentation
with paraphrases (Callison-Burch et al, 2006),
word-lattice-based paraphrasing for PBSMT is in-
troduced in (Du et al, 2010). A brief overview of
this method is given in this section.
2.1 Lattice construction from paraphrases
The first step of the word-lattice-based method is to
generate paraphrases from parallel corpus. The al-
gorithm in (Bannard and Callison-Burch, 2005) is
used for this purpose by pivoting through phrases
in the source- and the target- languages: for each
source phrase, all occurrences of its target phrases
are found, and all the corresponding source phrases
of these target phrases are considered as the potential
paraphrases of the original source phrase (Callison-
Burch et al, 2006). A paraphrase probability
p(e2|e1) is defined to reflect the similarities between
two phrases, as in (1):
p(e2|e1) =
?
f
p(f |e1)p(e2|f) (1)
where the probability p(f |e1) is the probability that
the original source phrase e1 translates as a partic-
ular phrase f on the target side, and p(e2|f) is the
probability that the candidate paraphrase e2 trans-
lates as the source phrase. Here p(e2|f) and p(f |e1)
are defined as the translation probabilities estimated
using maximum likelihood by counting the observa-
tions of alignments between phrases e and f in the
w
x
w
y
...
q
1
q
2
...
q
m
...
w
x+1
w
y
...
w
x+1
w
x-1
w
y+1
q
1
q
2 
? q
m
......
w
x
...
w
y+1
w
x-1
Figure 1: Construct word lattices from paraphrases.
parallel corpus, as in (2) and (3):
p(e2|f) ?
count(e2, f)
?
e2 count(e2, f)
(2)
p(f |e1) ?
count(f, e1)
?
f count(f, e1)
(3)
The second step is to transform input sentences
in the development and test sets into word lattices
with paraphrases extracted in the first step. As il-
lustrated in Figure 1, given a sequence of words
{w1, . . . , wN} as the input, for each of the para-
phrase pairs found in the source sentence (e.g. pi =
{q1, . . . , qm} for {wx, . . . , wy}), add in extra nodes
and edges to make sure those phrases coming from
paraphrases share the same start nodes and end
nodes with that of the original ones. Subsequently
the following empirical methods are used to assign
weights on paraphrases edges:
? Edges originating from the input sentences are
assigned weight 1.
? The first edges for each of the paraphrases are
calculated as in (4):
w(e1pi) =
1
k + i
(1 <= i <= k) (4)
where 1 stands for the first edge of paraphrase
pi, and i is the probability rank of pi among
those paraphrases sharing with a same start
node, while k is a predefined constant as a
trade-off parameter for efficiency and perfor-
mance, which is related to the paraphrase ca-
pacity.
? The rest of the edges corresponding to the para-
phrases are assigned weight 1.
33
The last step is to modify the MT pipeline to tune
and evaluate the SMT system with word lattice in-
puts, as is described in (Du et al, 2010; Onishi et
al., 2010).
For further discussion, a real example of the gen-
erated word lattice is illustrated in Figure 2. In
the word lattice, double-line circled nodes and solid
lined edges come from originated from the origi-
nal sentence, while others are generated from para-
phrases. Word, weight and ranking of each edge are
displayed in the figure. By adopting such an input
structure, the diversity of the input sentences is in-
creased to provide more flexible translation options
during the decoding process, which has been shown
to improve translation performance (Du et al, 2010).
2.2 Path duplication and decoding efficiency
As can be seen in Figure 2, the construction pro-
cess in the previous steps tends to generate duplicate
paths in the word lattices. For example, there are two
paths from node 6 to node 11 with the same words
?secretary of state? but different edge probabilities
(the path via node 27 and 28 has the probability
1/12, while the path via node 26 and 9 has the prob-
ability 1/99). This is because the aforementioned
straightforward construction process does not track
path duplications from different spans on the source
side. Since the number of admitted paraphrases is
restricted by parameter k in formula (4), the path
duplication will decrease the paraphrase capacity to
a certain extend.
Moreover, state of the art PBSMT decoders (e.g.
Moses (Koehn et al, 2007)) have a much higher
computational complexity for lattice structures than
for sentences. Thus even though only the test sen-
tences need to be transformed into word lattices, de-
coding time is still too slow for real-time applica-
tions.
Motivated by transforming ASR word-graphs into
CNs (Bertoldi et al, 2008), we adopt CN as the
trade-off between efficiency and quality. We aim to
merge duplicate paths in the word lattices to increase
paraphrase capacity, and to speed up the decoding
process via CN decoding. Details of the proposed
method are presented in the following section.
3 Confusion-network-based method
CNs are weighted direct graphs where each path
from the start node to the end node goes through
all the other nodes. Each edge is labelled with a
word and a probability (or weight). Although it is
commonly required to normalize the probability of
edges between two consecutive nodes to sum up to
one, from the point of view of the decoder, this is
not a strict constraint as long as any score is pro-
vided (similar to the weights on the word lattices in
the last section, and we prefer to call it ?weight? in
this case).
The benefits of using CNs are:
1. the ability to represent the original word lattice
with a highly compact structure;
2. all hypotheses in the word lattice are totally or-
dered, so that the decoding algorithm is mostly
retained except for the collection of translation
options and the handeling of  edges (Bertoldi
et al, 2008), which requires much less compu-
tational resources than the lattice decoding.
The rest of this section details the construction pro-
cess of the CNs and the application in paraphrase-
enriched SMT.
3.1 Confusion Network building
We build our CN from the aforementioned word lat-
tices. Previous studies provide several methods to
do this. (Mangu et al, 2000) propose a method to
cluster lattice words on the similarity of pronuncia-
tions and frequency of occurrence, and then to create
CNs using cluster orders. Although this method has
a computational complexity of O(n3), the SRILM
toolkit (Stolcke, 2002) provides a modified algo-
rithm which runs much faster than the original ver-
sion. In (Hakkani-Tu?r et al, 2005), a pivot algorithm
is proposed to form CNs by normalizing the topol-
ogy of the input lattices.
In this paper, we use the modified method
of (Mangu et al, 2000) provided by the SRILM
toolkit to convert word lattices into CNs. Moreover,
we aim to obtain CNs with the following guidelines:
? Cluster the lattice words only by topological
orders and edge weights without considering
word similarity. The objective is to reduce the
34
Figure 2: An example of a real paraphrase lattice. Note that it is a subsection of the whole word lattice that is too big
to fit into this page, and edge weights have been evenly distributed for CN conversion as specified by formula (5).
impact of path duplications in the building pro-
cess, since duplicate words will bias the impor-
tance of paths.
? Assign edge weights by the ranking of para-
phrase probabilities, rather than by poste-
rior probabilities from the modified method
of (Mangu et al, 2000). This is similar to that
given in formula (4). The reason for this is to
reduce the impact of path duplications on the
calculation of weights.
Thus, we modified the construction process as fol-
lows:
1. For each of the input word lattices, replace
word texts with unique identifiers (to make the
lattice alignment uncorrelated to the word sim-
ilarity, since in this case, all words in the lattice
are different from each other).
2. Evenly distribute edge weights for each of the
lattices by modifying formula (4) as in (5):
w(ejpi) =
1
Mi
?
(k + i)
(1 <= i <= k) (5)
where 1 <= j <= Mi, given e
j
pi is the j
th
edge of paraphrase pi, and Mi is the number of
words in pi. This is to avoid large weights on
the paraphrase edges for lattice alignments.
3. Transform the weighted word lattices into CNs
with the SRILM toolkit, and the paraphrase
ranking information is carried on the edges.
4. Replace the word texts in step 1, and then for
each column of the CN, merge edges with same
words by keeping those with the highest rank-
ing (a smaller number indicates a higher rank-
ing, and edges from the original sentences will
always have the highest ranking). Note that to
assign ranking for each  edge which does not
appear in the word lattice, we use the ranking of
non-original edges (in the same column) which
have the closest posterior probability to it. (As-
sign ranking 1 if failed to find a such edge).
5. Reassign the edge weights: 1) edges from orig-
inal sentences are assigned with weight 1; 2)
edges from paraphrases are assigned with an
35
empirical method as in (6):
w(ecnpi ) =
1
k + i
(1 <= i <= k) (6)
where ecnpi are edges corresponding with para-
phrase pi, and i is the probability rank of pi in
formula (4), while k is also defined in formula
(4).
A real example of a constructed CN is depicted
in Figure 3, which is correspondent with the word
lattice in Figure 2. Unlike the word lattices, all the
nodes in the CN are generated from the original sen-
tence, while solid lined edges come from the orig-
inal sentence, and dotted lined edges correspond to
paraphrases.
As in shown in the Figures, duplicate paths in the
word lattices have been merged into CN edges by
step 4. For example, the two occurrences of ?sec-
retary of state? in the word lattices (one path from
node 6 to 11 via 27 and 28, and one path from node
6 to 11 via 26 and 9 in the word lattice) are merged
to keep the highest-ranked path in the CN (note
there is one  edge between node 9 and 10 to ac-
complish the merging operation). Furthermore, each
edge in the CN is assigned a weight by formula (6).
This weight assignment procedure penalizes paths
from paraphrases according to the paraphrase prob-
abilities, in a similar manner to the aforementioned
word-lattice-based method.
3.2 Modified MT pipeline
By transforming word lattices into CNs, dupli-
cate paths are merged. Furthermore the new fea-
tures on the edges are introduced by formula (6),
which is then tuned on the development set using
MERT (Och, 2003) in the log-linear model (Och and
Ney, 2002). Since the SMT decoders are able to
perform CN decoding (Bertoldi et al, 2008) in an
efficient multi-stack decoding way, decoding time is
drastically reduced compared to lattice decoding.
The training steps are then modified as fol-
lows: 1) Extract phrase table, reordering table, and
build target-side language models from parallel and
monolingual corpora respectively for the PBSMT
model; 2) Transform source sentences in the devel-
opment set into word lattices, and then transform
them into CNs using the method proposed in Sec-
tion 3.1; 3) Tune the PBSMT model on the CNs via
the development set. Note that the overhead of the
evaluation steps are: transform each test set sentence
into a word lattice, and also transform them into a
CN, then feed them into the SMT decoder to obtain
decoding results.
4 Experiments
4.1 Experimental setup
Experiments were carried out on three English?
Chinese translation tasks. The training corpora com-
prise 20K, 200K and 2.1 million sentence pairs,
where the former two corpora are derived from FBIS
corpus1 which is sentence-aligned by Champollion
aligner (Ma, 2006), the latter corpus comes from
HK parallel corpus,2 ISI parallel corpus,3 other news
data and parallel dictionaries from LDC.
The development set and the test set for the 20K
and 200K corpora are randomly selected from the
FBIS corpus, each of which contains 1,200 sen-
tences, with one reference. For the 2.1 million cor-
pus, the NIST 2005 Chinese?English current set
(1,082 sentences) with one reference is used as the
development set, and NIST 2003 English?Chinese
current set (1,859 sentences) with four references is
used as the test set.
Three baseline systems are built for comparison:
Moses PBSMT baseline system (Koehn et al, 2007),
a realization of the translation model augmentation
system described in (Callison-Burch et al, 2006)
(named ?Para-Sub? hereafter), and the word-lattice
based system proposed in (Du et al, 2010).
Word alignments on the parallel corpus are per-
formed using GIZA++ (Och and Ney, 2003) with
the ?grow-diag-final? refinement. Maximum phrase
length is set to 10 words and the parameters in the
log-linear model are tuned by MERT (Och, 2003).
All the language models are 5-gram built with the
SRILM toolkit (Stolcke, 2002) on the monolingual
part of the parallel corpora.
4.2 Paraphrase acquisition
The paraphrases data for all paraphrase-enriched
system is derived from the ?Paraphrase Phrase Ta-
1Paragraph-aligned corpus with LDC number
LDC2003E14.
2LDC number: LDC2004T08.
3LDC number: LDC2007T09.
36
Figure 3: An example of a real CN converted from a paraphrase lattice. Note that it is a subsection of the whole CN
that is converted from the word lattice in Figure 2.
ble?4 of TER-Plus (Snover et al, 2009). Further-
more, the following two steps are taken to filter out
noise paraphrases as described in (Du et al, 2010):
1. Filter out paraphrases with probabilities lower
than 0.01.
2. Filter out paraphrases which are not observed
in the phrase table. This objective is to guar-
antee that no extra out-of-vocabulary words are
introduced into the paraphrase systems.
The filtered paraphrase table is then used to generate
word lattices and CNs.
4.3 Experimental results
The results are reported in BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) scores.
Table 1 compares the performance of four sys-
tems on three translation tasks. As can be observed
from the Table, for 20K and 200K corpora, the
word-lattice-based system accomplished the best re-
sults. For the 20K corpus, the CN outperformed
the baseline PBSMT by 0.31 absolute (2.15% rel-
ative) BLEU points and 1.5 absolute (1.99% rela-
tive) TER points. For the 200K corpus, it still out-
performed the ?Para-Sub? by 0.06 absolute (0.26%
relative) BLEU points and 0.15 absolute (0.23% rel-
ative) TER points. Note that for the 2.1M corpus,
although CN underperformed the best word lattice
by an insignificant amount (0.06 absolute, 0.41%
4http://www.umiacs.umd.edu/?snover/terp/
downloads/terp-pt.v1.tgz
relative) in terms of BLEU points, it has the best
performance in terms of TER points (0.22 abso-
lute, 0.3% relative than word lattice). Furthermore,
the CN outperformed ?Para-Sub? by 0.36 absolute
(2.55% relative) BLEU points and 1.37 absolute
(1.84% relative) TER points, and also beat the base-
line PBSMT system by 0.45 absolute (3.21% rela-
tive) BLEU points and 1.82 absolute (2.43% rela-
tive) TER points. The paired 95% confidence in-
terval of significant test (Zhang and Vogel, 2004)
between the ?Lattice? and ?CN? system is [-0.19,
+0.38], which also suggests that the two system has
a comparable performance in terms of BLEU.
In Table 2, decoding time on test sets is re-
ported to compare the computational efficiency of
the baseline PBSMT, word-lattice-based and CN-
based methods. Note that word lattice construc-
tion time and CN building time (including word lat-
tice construction and conversion from word lattices
into CNs with the SRILM toolkit (Stolcke, 2002))
are counted in the decoding time and illustrated in
the table within parentheses respectively. Although
both word-lattice-based and CN-based methods re-
quire longer decoding times than the baseline PB-
SMT system, it is observed that compared with the
word lattices, CNs reduced the decoding time signif-
icantly on three tasks, namely 52.06% for the 20K
model, 75.75% for the 200K model and 78.88% for
the 2.1M model. It is also worth noting that the
?Para-Sub? system has a similar decoding time with
baseline PBSMT since only the translation table is
modified.
37
20K 200K 2.1M
System BLEU TER BLEU TER BLEU TER
Baseline PBSMT 14.42 75.30 23.60 63.65 14.04 74.88
Para-Sub 14.78 73.75 23.41 63.84 14.13 74.43
Word-lattice-based 15.44 73.06 25.20 62.37 14.55 73.28
CN-based 14.73 73.8 23.47 63.69 14.49 73.06
Table 1: Comparison on PBSMT, ?Para-Sub?, word-lattice and CN-based methods.
System
FBIS testset (1,200 inputs) NIST testset (1,859 inputs)
20K model 200K model 2.1M model
Baseline 21 min 41 min 37 min
Lattice 102 min (+ 15 sec) 398 min (+ 20 sec) 559 min (+ 21 sec)
CN 48 min (+ 61 sec) 95 min (+ 96 sec) 116 min (+ 129 sec)
Table 2: Decoding time comparison of PBSMT, word-lattice (?Lattice?) and CN-based (?CN?) methods.
4.4 Discussion
From the performance and decoding time reported in
the last section, it is obvious that on large scale cor-
pora, the CN-based method significantly reduced the
computational complexity while preserved the sys-
tem performance of the best lattice-based method.
Thus it makes the paraphrase-enriched SMT system
more applicable to real-world applications. On the
other hand, for small- and medium-scale data, CNs
can be used as a compromise between speed and
quality, since decoding time is much less than word
lattices, and compared with the ?Para-Sub? system,
the only overhead is the transforming of the input
sentences.
It is also interesting that the relative performance
of the CNs increases gradually with the size of the
training corpus, which indicates that it is more suit-
able for models built from large scale data. Consid-
ering the decoding time, it is preferable to use CNs
instead of word lattices for such translation tasks.
However, for the small- and medium-scale data, the
CN system is not competitive even compared with
the baseline. In this case it suggests that, on these
two tasks, the coverage issue is not solved by in-
corporating paraphrases with the CN structure. It
might because of the ambiguity that introduced by
CNs harms the decoder to choose the appropriate
source words from paraphrases. On the other hand,
this ambiguity could be decreased with translation
models trained on a large corpus, which provides
enough observations for the decoders to favour para-
phrases.
5 Conclusion and future work
In this paper, CNs are used instead of word lattices
to incorporate paraphrases into SMT. Transforma-
tion from word lattices into CNs is used to merge
path duplications, and decoding time is drastically
reduced with CN decoding. Experiments are carried
out on small-, medium- and large-scale English?
Chinese translation tasks and confirm that compared
with word lattices, it is much more computationally
efficient to use CNs, while no loss of performance is
observed on the large-scale task.
In the future, we plan to apply more features
such as source-side language models and phrase
length (Onishi et al, 2010) on the CNs to obtain bet-
ter system performance. Furthermore, we will carry
out this work on other language pairs to show the ef-
fectiveness of paraphrases in SMT systems. We will
also investigate the reason for its lower performance
on the small- and medium-scale corpora, as well as
the impact of the paraphrase filtering procedure on
translation quality.
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. Thanks to the reviewers
for their invaluable comments and suggestions.
38
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In 43rd An-
nual meeting of the Association for Computational
Linguistics, Ann Arbor, MI, pages 597?604.
Nicola Bertoldi, Richard Zens, Marcello Federico, and
Wade Shen 2008. Efficient Speech Translation
Through Confusion Network Decoding. In IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 16(8), pages 1696?1705.
Francis Bond, Eric Nichols, Darren Scott Appling and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), Hawaii, pages 150?
157.
Chris Callison-Burch, Philipp Koehn and Miles Osborne.
2006. Improved Statistical Machine Translation Using
Paraphrases. In Proceedings of the Human Language
Technology conference - North American chapter of
the Association for Computational Linguistics (HLT-
NAACL), NY, pages 17?24.
Jinhua Du, Jie Jiang and Andy Way. 2010. Facilitating
Translation Using Source Language Paraphrase Lat-
tices. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Cambridge, MA, pages 420?429.
Dilek Hakkani-Tu?r, Fre?de?ric Be?chet, Giuseppe Riccardi
and Gokhan Tur. 2005. Beyond ASR 1-best: Using
word confusion networks in spoken language under-
standing. In Computer Speech and Language (2005):
20(4), pages 495?514.
Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Wade Shen, C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and
Evan Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007: demo
and poster sessions, Prague, Czech Republic, pages
177?180.
Roland Kuhn, Boxing Chen, George Foster and Evan
Stratford. 2010. Phrase Clustering for Smoothing TM
Probabilities - or, How to Extract Paraphrases from
Phrase Tables. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), Beijing, China, pages 608?616.
Xiaoyi Ma. 2006. Champollion: A Robust Parallel Text
Sentence Aligner. LREC 2006: Fifth International
Conference on Language Resources and Evaluation,
Genova, Italy, pages 489?492.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik and Bon-
nie J. Dorr. 2007. Using Paraphrases for Parameter
Tuning in Statistical Machine Translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, Prague, Czech Republic, pages 120?127.
Lidia Mangu, Eric Brill and Andreas Stolcke. 2000.
Finding Consensus in Speech Recognition: Word Er-
ror Minimization and Other Applications of Confusion
Networks. In Computer Speech and Language 14 (4),
pages 373?400.
Yuval Marton, Chris Callison-Burch and Philip Resnik.
2009. Improved Statistical Machine Translation Us-
ing Monolingually-Derived Paraphrases. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), Singapore, pages
381?390.
Aure?lien Max. 2010. Example-Based Paraphrasing
for Improved Phrase-Based Statistical Machine Trans-
lation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
Cambridge, MA, pages 656?666.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases In Pro-
ceedings of the European Conference on Artificial In-
telligence (ECAI), Patras, Greece, pages 338?342.
Preslav Nakov. 2008b. Improving English-Spanish sta-
tistical machine translation: experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of ACL-08:HLT. Third Work-
shop on Statistical Machine Translation, Columbus,
Ohio, USA, pages 147?150.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA, pages 295?302.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1), pages 19?51.
Takashi Onishi, Masao Utiyama and Eiichiro, Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of the ACL 2010 Confer-
ence Short Papers, Uppsala, Sweden, pages 1?5.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method For Automatic
Evaluation of Machine Translation. ACL-2002: 40th
Annual meeting of the Association for Computational
Linguistics, pp.311-318, Philadelphia, PA.
Josh Schroeder, Trevor Cohn and Philipp Koehn. 2009.
Word Lattices for Multi-Source Translation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), Athens, Greece,
pages 719?727.
39
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, Cam-
bridge, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J.Dorr and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, Athens,
Greece, pages 259?268.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP),
Denver, Colorado, pages 901?904.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI). pages 85?94.
40
