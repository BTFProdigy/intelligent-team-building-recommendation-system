Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042?1051,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Multiple Dependency Corpora
for Inducing Wide-coverage Japanese CCG Resources
Sumire Uematsu?
uematsu@cks.u-tokyo.ac.jp
Takuya Matsuzaki?
takuya-matsuzaki@nii.ac.jp
Hiroki Hanaoka?
hanaoka@nii.ac.jp
Yusuke Miyao?
yusuke@nii.ac.jp
Hideki Mima?
mima@t-adm.t.u-tokyo.ac.jp
?The University of Tokyo
Hongo 7-3-1, Bunkyo, Tokyo, Japan
?National Institute of Infomatics
Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan
Abstract
This paper describes a method of in-
ducing wide-coverage CCG resources for
Japanese. While deep parsers with corpus-
induced grammars have been emerging
for some languages, those for Japanese
have not been widely studied, mainly be-
cause most Japanese syntactic resources
are dependency-based. Our method first
integrates multiple dependency-based cor-
pora into phrase structure trees and then
converts the trees into CCG derivations.
The method is empirically evaluated in
terms of the coverage of the obtained lexi-
con and the accuracy of parsing.
1 Introduction
Syntactic parsing for Japanese has been domi-
nated by a dependency-based pipeline in which
chunk-based dependency parsing is applied and
then semantic role labeling is performed on the de-
pendencies (Sasano and Kurohashi, 2011; Kawa-
hara and Kurohashi, 2011; Kudo and Matsumoto,
2002; Iida and Poesio, 2011; Hayashibe et al,
2011). This dominance is mainly because chunk-
based dependency analysis looks most appropriate
for Japanese syntax due to its morphosyntactic ty-
pology, which includes agglutination and scram-
bling (Bekki, 2010). However, it is also true that
this type of analysis has prevented us from deeper
syntactic analysis such as deep parsing (Clark and
Curran, 2007) and logical inference (Bos et al,
2004; Bos, 2007), both of which have been sur-
passing shallow parsing-based approaches in lan-
guages like English.
In this paper, we present our work on induc-
ing wide-coverage Japanese resources based on
combinatory categorial grammar (CCG) (Steed-
man, 2001). Our work is basically an extension of
a seminal work on CCGbank (Hockenmaier and
Steedman, 2007), in which the phrase structure
trees of the Penn Treebank (PTB) (Marcus et al,
1993) are converted into CCG derivations and a
wide-coverage CCG lexicon is then extracted from
these derivations. As CCGbank has enabled a va-
riety of outstanding works on wide-coverage deep
parsing for English, our resources are expected to
significantly contribute to Japanese deep parsing.
The application of the CCGbank method to
Japanese is not trivial, as resources like PTB are
not available in Japanese. The widely used re-
sources for parsing research are the Kyoto corpus
(Kawahara et al, 2002) and the NAIST text corpus
(Iida et al, 2007), both of which are based on the
dependency structures of chunks. Moreover, the
relation between chunk-based dependency struc-
tures and CCG derivations is not obvious.
In this work, we propose a method to integrate
multiple dependency-based corpora into phrase
structure trees augmented with predicate argument
relations. We can then convert the phrase structure
trees into CCG derivations. In the following, we
describe the details of the integration method as
well as Japanese-specific issues in the conversion
into CCG derivations. The method is empirically
evaluated in terms of the quality of the corpus con-
version, the coverage of the obtained lexicon, and
the accuracy of parsing with the obtained gram-
mar. Additionally, we discuss problems that re-
main in Japanese resources from the viewpoint of
developing CCG derivations.
There are three primary contributions of this pa-
per: 1) we show the first comprehensive results for
Japanese CCG parsing, 2) we present a methodol-
ogy for integrating multiple dependency-based re-
1042
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 1: A CCG derivation.
X/Y : f Y : a ? X : fa (>)
Y : a X\Y : a ? X : fa (<)
X/Y : f Y/Z : g ? X/Z : ?x.f(gx) (> B)
Y\Z : g X\Y : f ? X\Z : ?x.f(gx) (< B)
Figure 2: Combinatory rules (used in the current
implementation).
sources to induce CCG derivations, and 3) we in-
vestigate the possibility of further improving CCG
analysis by additional resources.
2 Background
2.1 Combinatory Categorial Grammar
CCG is a syntactic theory widely accepted in the
NLP field. A grammar based on CCG theory con-
sists of categories, which represent syntactic cat-
egories of words and phrases, and combinatory
rules, which are rules to combine the categories.
Categories are either ground categories like S and
NP or complex categories in the form of X/Y or
X\Y , where X and Y are the categories. Cate-
gory X/Y intuitively means that it becomes cat-
egory X when it is combined with another cat-
egory Y to its right, and X\Y means it takes a
category Y to its left. Categories are combined
by applying combinatory rules (Fig. 2) to form
categories for larger phrases. Figure 1 shows a
CCG analysis of a simple English sentence, which
is called a derivation. The verb give is assigned
category S\NP/NP/NP , which indicates that it
takes two NPs to its right, one NP to its left, and fi-
nally becomes S. Starting from lexical categories
assigned to words, we can obtain categories for
phrases by applying the rules recursively.
An important property of CCG is a clear inter-
face between syntax and semantics. As shown in
Fig. 1, each category is associated with a lambda
term of semantic representations, and each com-
binatory rule is associated with rules for semantic
composition. Since these rules are universal, we
can obtain different semantic representations by
switching the semantic representations of lexical
categories. This means that we can plug in a vari-
Sentence S Verb S\$ (e.g. S\NPga)
Noun phrase NP Post particle NPga|o|ni|to\NP
Auxiliary verb S\S
Table 1: Typical categories for Japanese syntax.
Cat. Feature Value Interpretation
NP case ga nominal
o accusative
ni dative
to comitative, complementizer, etc.
nc none
S form stem stem
base base
neg imperfect or negative
cont continuative
vo s causative
Table 2: Features for Japanese syntax (those used
in the examples in this paper).
ety of semantic theories with CCG-based syntactic
parsing (Bos et al, 2004).
2.2 CCG-based syntactic theory for Japanese
Bekki (2010) proposed a comprehensive theory
for Japanese syntax based on CCG. While the the-
ory is based on Steedman (2001), it provides con-
crete explanations for a variety of constructions of
Japanese, such as agglutination, scrambling, long-
distance dependencies, etc. (Fig. 3).
The ground categories in his theory are S, NP,
and CONJ (for conjunctions). Table 1 presents
typical lexical categories. While most of them
are obvious from the theory of CCG, categories
for auxiliary verbs require an explanation. In
Japanese, auxiliary verbs are extensively used to
express various semantic information, such as
tense and modality. They agglutinate to the main
verb in a sequential order. This is explained in
Bekki?s theory by the category S\S combined with
a main verb via the function composition rule
(<B). Syntactic features are assigned to categories
NP and S (Table 2). The feature case represents a
syntactic case of a noun phrase. The feature form
denotes an inflection form, and is necessary for
constraining the grammaticality of agglutination.
Our implementation of the grammar basically
follows Bekki (2010)?s theory. However, as a first
step in implementing a wide-coverage Japanese
parser, we focused on the frequent syntactic con-
structions that are necessary for computing pred-
icate argument relations, including agglutination,
inflection, scrambling, case alternation, etc. Other
details of the theory are largely simplified (Fig. 3),
1043
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 3: A simplified CCG analysis of the sentence ?The ambassador participated in the negotiation.?.
S ? NP/NP (RelExt)
S\NP1 ? NP1/ P1 (RelIn)
S ? S1/S1 (Con)
S\$1\NP1 ? (S1\$1\NP1)/(S1\$1\NP1) (ConCoord)
Figure 4: Type changing rules. The upper two are
for relative clauses and the others for continuous
clauses.
coordination and semantic representation in par-
ticular. The current implementation recognizes
coordinated verbs in continuous clauses (e.g., ??
???????????/he played the pia o and
sang?), but the treatment of other types of coor-
dination is largely simplified. For semantic repre-
sentation, we define predicate argument structures
(PASs) rather than the theory?s formal representa-
tion based on dynamic logic. Sophisticating our
semantic representation is left for future work.
For parsing efficiency, we modified the treat-
ment of some constructions so that empty el-
ements are excluded from the implementation.
First, we define type changing rules to produce
relative and continuous clauses (shown in Fig. 4).
The rules produce almost the same results as the
theory?s treatment, but without using empty ele-
ments (pro, etc.). We also used lexical rules to
treat pro-drop and scrambling. For the sentence in
Fig. 3, the deletion of the nominal phrase (??
?), the dative phrase (???), or both results in
valid sentences, and shuffling the two phrases does
so as well. Lexical entries with the scrambled or
dropped arguments are produced by lexical rules
in our implementation.
2.3 Linguistic resources for Japanese parsing
As described in Sec. 1, dependency-based analysis
has been accepted for Japanese syntax. Research
on Japanese parsing also relies on dependency-
based corpora. Among them, we used the follow-
ing resources in this work.
Kyoto corpus A news text corpus annotated
with morphological information, chunk bound-
Kyoto Corpus 
Chunk 
?? ? government NOM ?? ? ambassador ACC ?? ? negotiation DAT ?? ? ? ? participation do cause PAST 
NAIST Corpus 
Dep. 
Causer ARG-ga ARG-ni 
Figure 5: The Kyoto and NAIST annotations for
?The government had the ambassador participate
in the negotiation.?. Accusatives are labeled as
ARG-ga in causative (see Sec. 3.2).
aries, and dependency relations among chunks
(Fig. 5). The dependencies are classified into four
types: Para (coordination), A (apposition), I (ar-
gument cluster), and Dep (default). Most of the
dependencies are annotated as Dep.
NAIST text corpus A corpus annotated with
anaphora and coreference relations. The same set
as the Kyoto corpus is annotated.1 The corpus
only focuses on three cases: ?ga? (subject), ?o?
(direct object), and ?ni? (indirect object) (Fig. 5).
Japanese particle corpus (JP) (Hanaoka et al,
2010) A corpus annotated with distinct gram-
matical functions of the Japanese particle (postpo-
sition) ?to?. In Japanese, ?to? has many functions,
including a complementizer (similar to ?that?), a
subordinate conjunction (similar to ?then?), a co-
ordination conjunction (similar to ?and?), and a
case marker (similar to ?with?).
2.4 Related work
Research on Japanese deep parsing is fairly lim-
ited. Formal theories of Japanese syntax were
presented by Gunji (1987) based on Head-driven
Phrase Structure Grammar (HPSG) (Sag et al,
2003) and by Komagata (1999) based on CCG, al-
though their implementations in real-world pars-
ing have not been very successful. JACY (Siegel
1In fact, the NAIST text corpus includes additional texts,
but in this work we only use the news text section.
1044
and Bender, 2002) is a large-scale Japanese gram-
mar based on HPSG, but its semantics is tightly
embedded in the grammar and it is not as easy
to systematically switch them as it is in CCG.
Yoshida (2005) proposed methods for extracting
a wide-coverage lexicon based on HPSG from a
phrase structure treebank of Japanese. We largely
extended their work by exploiting the standard
chunk-based Japanese corpora and demonstrated
the first results for Japanese deep parsing with
grammar induced from large corpora.
Corpus-based acquisition of wide-coverage
CCG resources has enjoyed great success for En-
glish (Hockenmaier and Steedman, 2007). In
that method, PTB was converted into CCG-based
derivations from which a wide-coverage CCG lex-
icon was extracted. CCGbank has been used for
the development of wide-coverage CCG parsers
(Clark and Curran, 2007). The same methodology
has been applied to German (Hockenmaier, 2006),
Italian (Bos et al, 2009), and Turkish (C?ak?c?,
2005). Their treebanks are annotated with depen-
dencies of words, the conversion of which into
phrase structures is not a big concern. A notable
contribution of the present work is a method for in-
ducing CCG grammars from chunk-based depen-
dency structures, which is not obvious, as we dis-
cuss later in this paper.
CCG parsing provides not only predicate argu-
ment relations but also CCG derivations, which
can be used for various semantic processing tasks
(Bos et al, 2004; Bos, 2007). Our work consti-
tutes a starting point for such deep linguistic pro-
cessing for languages like Japanese.
3 Corpus integration and conversion
For wide-coverage CCG parsing, we need a)
a wide-coverage CCG lexicon, b) combinatory
rules, c) training data for parse disambiguation,
and d) a parser (e.g., a CKY parser). Since d) is
grammar- and language-independent, all we have
to develop for a new language is a)?c).
As we have adopted the method of CCGbank,
which relies on a source treebank to be converted
into CCG derivations, a critical issue to address is
the absence of a Japanese counterpart to PTB. We
only have chunk-based dependency corpora, and
their relationship to CCG analysis is not clear.
Our solution is to first integrate multiple
dependency-based resources and convert them
into a phrase structure treebank that is independent
ProperNoun 
????? Yeltsin 
NP 
ProperNoun 
??? Russia 
Noun 
???president 
PostP 
? DAT 
PP 
NP 
Aux 
??? not 
VP 
Verb 
?? forgive 
VerbSuffix 
? PASSIVE 
VP Aux 
? PAST 
VP 
?to Russian president Yeltsin? ?(one) was not forgiven? 
Figure 6: Internal structures of a nominal chunk
(left) and a verbal chunk (right).
of CCG analysis (Step 1). Next, we translate the
treebank into CCG derivations (Step 2). The idea
of Step 2 is similar to what has been done with
the English CCGbank, but obviously we have to
address language-specific issues.
3.1 Dependencies to phrase structure trees
We first integrate and convert available Japanese
corpora?namely, the Kyoto corpus, the NAIST
text corpus, and the JP corpus ?into a phrase
structure treebank, which is similar in spirit to
PTB. Our approach is to convert the depen-
dency structures of the Kyoto corpus into phrase
structures and then augment them with syntac-
tic/semantic roles from the other two corpora.
The conversion involves two steps: 1) recogniz-
ing the chunk-internal structures, and (2) convert-
ing inter-chunk dependencies into phrase struc-
tures. For 1), we don?t have any explicit infor-
mation in the Kyoto corpus although, in princi-
ple, each chunk has internal structures (Vadas and
Curran, 2007; Yamada et al, 2010). The lack of
a chunk-internal structure makes the dependency-
to-constituency conversion more complex than a
similar procedure by Bos et al (2009) that con-
verts an Italian dependency treebank into con-
stituency trees since their dependency trees are an-
notated down to the level of each word. For the
current implementation, we abandon the idea of
identifying exact structures and instead basically
rely on the following generic rules (Fig. 6):
Nominal chunks Compound nouns are first
formed as a right-branching phrase and
post-positions are then attached to it.
Verbal chunks Verbal chunks are analyzed as
left-branching structures.
The rules amount to assume that all but the last
word in a compound noun modify the head noun
(i.e., the last word) and that a verbal chunk is typ-
ically in a form V A1 . . . An, where V is a verb
1045
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP 
PP 
Noun 
?? process  
PostPcm  
? ACC  PP 
NP 
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP PP 
Noun 
?? process  
PostPcm  
? ACC  
Para Dep 
?$ proFess froP EirtK to deatK? 
Figure 7: From inter-chunk dependencies to a tree.
(or other predicative word) and Ais are auxiliaries
(see Fig. 6). We chose the left-branching structure
as default for verbal chunks because the semantic
scopes of the auxiliaries are generally in that or-
der (i.e., A1 has the narrowest scope). For both
cases, phrase symbols are percolated upward from
the right-most daughters of the branches (except
for a few cases like punctuation) because in almost
all cases the syntactic head of a Japanese phrase is
the right-most element.
In practice, we have found several patterns of
exceptions for the above rules. We implemented
exceptional patterns as a small CFG and deter-
mined the chunk-internal structures by determin-
istic parsing with the generic rules and the CFG.
For example, two of the rules we came up with are
rule A: Number ? PrefixOfNumber Number
rule B: ClassifierPhrase ? Number Classifier
in the precedence: rule A > B > generic rules.
Using the above, we bracket a compound noun
? ? ? ??
approximately thousand people death
PrefixOfNumber Number Classifier CommonNoun
?death of approximately one thousand people?
as in
(((? ?) ?) ??)
(((approximately thousand) people) death)
We can improve chunk-internal structures to some
extent by refining the CFG rules. A complete solu-
tion like the manual annotation by Vadas and Cur-
ran (2007) is left for future work.
The conversion of inter-chunk dependencies
into phrase structures may sound trivial, but it is
not necessarily easy when combined with chunk-
internal structures. The problem is to which node
in the internal structure of the head the dependent
dep modifier-type precedence
Para ??/PostPcm ??/PostPcm, */(Verb|Aux), ...
Dep */PostPcm */(Verb|Aux), */Noun, ...
Dep */PostPadnom */Noun, */(Verb|Aux), ...
Table 3: Rules to determine adjoin position.
PP 
Noun 
? dog 
PostP 
? DAT 
VP 
NP 
Adj  
?? 
white  
NP 
VP 
Noun 
? 
cat  
Verb 
?? say  
Aux 
? PAST 
VP PP 
Verb 
?? go!  
PostP 
? 
CMP  
ARG - to 
ARG - ni 
ARG - ga 
ARG - ga 
ARG - ga 
ARG - ga ARG - ni 
ARG - CLS  
NAIST 
JP 
Figure 8: Overlay of pred-arg structure annotation
(?The white cat who said ?Go!? to the dog.?).
tree is adjoined (Fig. 7). In the case shown in the
figure, three chunks are in the dependency relation
indicated by arrows on the top. The dotted arrows
show the nodes to which the subtrees are adjoined.
Without any human-created resources, we can-
not always determine the adjoin positions cor-
rectly. Therefore, as a compromise, we wound up
implementing approximate heuristic rules to deter-
mine the adjoin positions. Table 3 shows examples
of such rules. A rule specifies a precedence of the
possible adjoin nodes as an ordered list of patterns
on the lexical head of the subtree under an ad-
join position. The precedence is defined for each
combination of the type of the dependent phrase,
which is determined by its lexical head, and the
dependency type in the Kyoto corpus.
To select the adjoin position for the left-most
subtree in Fig. 7, for instance, we look up the
rule table using the dependency type, ?Para?, and
the lexical head of the modifier subtree, ? ??
/PostPcm?, as the key, and find the precedence ??
?/PostPcm, */(Verb|Aux), ...?. We thus select the
PP-node on the middle subtree indicated by the
dotted arrow because its lexical head (the right-
most word), ? ??/PostPcm?, matches the first
pattern in the precedence list. In general, we seek
for an adjoin node for each pattern p in the prece-
dence list, until we find a first match.
The semantic annotation given in the NAIST
corpus and the JP corpus is overlaid on the phrase
structure trees with slight modifications (Fig. 8).
1046
PP 
Noun 
?? negotiation 
PostPcm  
? DAT VP Noun 
?? participation  
Verb 
? do 
VerbSuffix 
? CAUSE  
Aux 
? PAST 
VP 
VP 
S 
NPni 
NP 
?? negotiation 
T1 
? DAT T4 
T5 
?? participation  
S?S 
? do 
S?S 
? CAUSE  
S?S 
? PAST 
T3 
T2 
S ? 
? 
?  or   ?B   
?  or   ?B   
?  or   ?B   
NPni 
NPnc 
?? negotiation 
NPni?NPnc 
? DAT 
Svo_s?NPni 
Svo_s?NPni 
?? participation  
Svo_s?Svo_s 
? do 
Scont?Svo_s 
? CAUSE  
Sbase?Scont 
? PAST 
Scont?NPni 
Sbase?NPni 
Sbase 
Step 2 - 1  
Step 2 - 2, 2 - 3  
Figure 9: A phrase structure into a CCG deriva-
tion.
In the figure, the annotation given in the two cor-
pora is shown inside the dotted box at the bottom.
We converted the predicate-argument annotations
given as labeled word-to-word dependencies into
the relations between the predicate words and their
argument phrases. The results are thus similar to
the annotation style of PropBank (Palmer et al,
2005). In the NAIST corpus, each pred-arg re-
lation is labeled with the argument-type (ga/o/ni)
and a flag indicating that the relation is medi-
ated by either a syntactic dependency or a zero
anaphora. For a relation of a predicate wp and its
argument wa in the NAIST corpus, the boundary
of the argument phrase is determined as follows:
1. If wa precedes wp and the relation is medi-
ated by a syntactic dep., select the maximum
PP that is formed by attaching one or more
postpositions to the NP headed by wa.
2. If wp precedes wa or the relation is mediated
by a zero anaphora, select the maximum NP
headed by wa that does not include wp.
In the figure, ??/dog?/DAT? is marked as the ni-
argument of the predicate ???/say? (Case 1), and
???/white ?/cat? is marked as its ga-argument
(Case 2). Case 1 is for the most basic construction,
where an argument PP precedes its predicate. Case
VP 
??    ? 
friend- DAT 
PP VP 
?? 
meet - BASE  
NPni ? 
VP 
10 ?    ? 
10 o?clock - TIME  
PP VP 
?? 
meet - BASE  
T/T ? 
X  
S 
??    ? 
friend- DAT 
NPni S?NPni 
?? 
meet - BASE  
S 
10 ?    ? 
10 o?clock - TIME  
S?S S 
?? 
meet - BASE  
?(to) Peet at ten? 
?(to) Peet a friend? 
Figure 10: An argument post particle phrase (PP)
(upper) and an adjunct PP (lower).
2 covers the relative clause construction, where a
relative clause precedes the head NP, the modifi-
cation of a noun by an adjective, and the relations
mediated by zero anaphora.
The JP corpus provides only the function label
to each particle ?to? in the text. We determined
the argument phrases marked by the ?to? particles
labeled as (nominal or clausal) argument-markers
in a similar way to Case 1 above and identified the
predicate words as the lexical heads of the phrases
to which the PPto phrases attach.
3.2 Phrase structures to CCG derivations
This step consists of three procedures (Fig. 9):
1. Add constraints on categories and features
to tree nodes as far as possible and assign a
combinatory rule to each branching.
2. Apply combinatory rules to all branching and
obtain CCG derivations.
3. Add feature constraints to terminal nodes.
3.2.1 Local constraint on derivations
According to the phrase structures, the first proce-
dure in Step 2 imposes restrictions on the resulting
CCG derivations. To describe the restrictions, we
focus on some of the notable constructions and il-
lustrate the restrictions for each of them.
Phrases headed by case marker particles A
phrase of this type must be either an argument
(Fig. 10, upper) or a modifier (Fig. 10, lower) of a
predicative. Distinction between the two is made
based on the pred-arg annotation of the predica-
tive. If a phrase is found to be an argument, 1) cat-
egory NP is assigned to the corresponding node,
2) the case feature of the category is given accord-
ing to the particle (in the case of Fig. 10 (upper),
1047
VP 
Verb 
?? 
Speak - NEG  
Aux 
??? 
not- CONT  
Aux 
? 
PAST- BASE  
VP 
S cont ?S  
S ba s e ?S  
?did not speak? 
? or ?B  
? or ?B  S cont ?NPg a  
Sneg ?NPg a  
?? 
Speak - NEG  
S cont ?Sneg  
??? 
not- CONT  
S b ase ?S cont  
? 
PAST- BASE  
S b ase ?NPg a  
Figure 11: An auxiliary verb and its conversion.
VP 
Verb 
?? inquire - NEG  
VerbSuffix 
??? cause - BASE  
??    ? her - DAT 
PP 
VP 
ARG - ga  
?(to) Kave Ker inTuire? 
? 
S?NPni[1 ] 
S?S 
??? cause - BASE  
??    ? her - DAT 
NPni[1 ] 
S 
S?NPni[1 ] 
?? inquire - NEG  
ga         [1]  
NPni[1 ] 
ga: [1 ] 
Figure 12: A causative construction.
ni for dative), and 3) the combinatory rule that
combines the particle phrase and the predicative
phrase is assigned backward function application
rule (<). Otherwise, a category T/T is assigned to
the corresponding modifier node and the rule will
be forward function application (>).
Auxiliary verbs As described in Sec. 2.2, an
auxiliary verb is always given the category S\S
and is combined with a verbal phrase via < or <B
(Fig. 11). Furthermore, we assign the form feature
value of the returning category S according to the
inflection form of the auxiliary. In the case shown
in the figure, Sbase\S is assigned for ??/PAST-
BASE? and Scont\S for ????/not-CONT?. As
a result of this restriction, we can obtain condi-
tions for every auxiliary agglutination because the
two form values in S\S are both restricted after
applying combinatory rules (Sec. 3.2.2).
Case alternations In addition to the argu-
ment/adjunct distinction illustrated above, a pro-
cess is needed for argument phrases of predicates
involving case alternation. Such predicates are
either causative (see Fig. 12) or passive verbs
and can be detected by voice annotation from the
NAIST corpus. For an argument of that type of
verb, its deep case (ga for Fig. 12) must be used
to construct the semantic representation, namely
the PAS. As well as assigning the shallow case
value (ni in Fig. 12) to the argument?s category
NP, as usual, we assign a restriction to the PAS
S?NPo[1] 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
NP 
? 
book 
NP 
NP[1]?NP[1] 
VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
Noun 
? 
book 
NP 
S?NP[1] 
NP[1]?NP[1] 
Noun 
? 
store 
NP 
?   ? 
book-ACC 
PP VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
VP X 
S 
NP?NP 
NP 
? 
store 
NP 
NP?NP 
?    ? 
book-ACC 
NPo 
S 
S?NPo 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
?a store wKere (,) EougKt tKe EooN? 
?a EooN wKiFK (,) EougKt? 
Figure 13: A relative clause with/without argu-
ment extraction (upper/lower, respectively).
of the verb so that the semantic argument corre-
sponding to the deep case is co-indexed with the
argument NP. These restrictions are then utilized
for PAS construction in Sec. 3.2.3.
Relative clauses A relative clause can be de-
tected as a subtree that has a VP as its left child
and an NP as its right child, as shown in Fig. 13.
The conversion of the subtree consists of 1) in-
serting a node on the top of the left VP (see the
right-hand side of Fig. 13), and 2) assigning the
appropriate unary rule to make the new node. The
difference between candidate rules RelExt and Re-
lIn (see Fig. 4) is whether the right-hand NP is
an obligatory argument of the VP or not, which
can be determined by the pred-arg annotation on
the predicate in the VP. In the upper example in
Fig. 13, RelIn is assigned because the right NP
?book? is annotated as an accusative argument of
the predicate ?buy?. In contrast, RelExt is as-
signed in the lower side in the figure because the
right NP ?store? is not annotated as an argument.
Continuous clauses A continuous clause can be
detected as a subtree with a VP of continuous form
as its left child and a VP as its right child. Its
conversion is similar to that of a relative clause,
and only differs in that the candidate rules are Con
and ConCoord. ConCoord generates a continu-
ous clause that shares arguments with the main
clause while Con produces one without shared ar-
guments. Rule assignment is done by comparing
the pred-arg annotations of the two phrases.
1048
Training Develop. Test
#Sentences 24,283 4,833 9,284
#Chunks 234,685 47,571 89,874
#Words 664,898 136,585 255,624
Table 4: Statistics of input linguistic resources.
3.2.2 Inverse application of rules
The second procedure in Step 2 begins with as-
signing a category S to the root node. A combi-
natory rule assigned to each branching is then ?in-
versely? applied so that the constraint assigned to
the parent transfers to the children.
3.2.3 Constraints on terminal nodes
The final process consists of a) imposing restric-
tions on the terminal category in order to instan-
tiate all the feature values, and b) constructing a
PAS for each verbal terminal. An example of pro-
cess a) includes setting the form features in the
verb category, such as S\NPni, according to the
inflection form of the verb. As for b), arguments
in a PAS are given according to the category and
the partial restriction. For instance, if a category
S\NPni is obtained for ???/inquire? (Fig. 12),
the PAS for ?inquire? is unary because the cate-
gory has one argument category (NPni), and the
category is co-indexed with the semantic argument
ga in the PAS due to the partial restriction depicted
in Sec. 3.2.1. As a result, a lexical entry is ob-
tained as?? ` S\NPni[1]: inquire([1]).
3.3 Lexical entries
Finally, lexical rules are applied to each of the ob-
tained lexical entries in order to reduce them to
the canonical form. Since words in the corpus (es-
pecially verbs) often involve pro-drop and scram-
bling, there are a lot of obtained entries that have
slightly varied categories yet share a PAS. We as-
sume that an obtained entry is a variation of the
canonical one and register the canonical entries in
the lexicon. We treat only subject deletion for pro-
drop because there is not sufficient information to
judge the deletion of other arguments. Scrambling
is simply treated as permutation of arguments.
4 Evaluation
We used the following for the implementation of
our resources: Kyoto corpus ver. 4.02, NAIST text
2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?
Kyoto\%20University\%20Text\%20Corpus
Training Develop. Test
St.1 St.2 St.1 St.2 St.1 St.2
Sent. 24,283 24,116 4,833 4,803 9,284 9,245
Converted 24,116 22,820 4,803 4,559 9,245 8,769
Con. rate 99.3 94.6 99.4 94.9 99.6 94.9
Table 5: Statistics of corpus conversion.
Sentential Coverage
Covered Uncovered Cov. (%)
Devel. 3,920 639 85.99
Test 7,610 1,159 86.78
Lexical Coverage
Word Known Unknown
combi. cat. word
Devel. 127,144 126,383 682 79 0
Test 238,083 236,651 1,242 145 0
Table 6: Sentential and lexical coverage.
corpus ver. 1.53, and JP corpus ver. 1.04. The
integrated corpus is divided into training, devel-
opment, and final test sets following the standard
data split in previous works on Japanese depen-
dency parsing (Kudo and Matsumoto, 2002). The
details of these resources are shown in Table 4.
4.1 Corpus conversion and lexicon extraction
Table 5 shows the number of successful conver-
sions performed by our method. In total, we ob-
tained 22,820 CCG derivations from 24,283 sen-
tences (in the training set), resulting in the to-
tal conversion rate of 93.98%. The table shows
we lost more sentences in Step 2 than in Step 1.
This is natural because Step 2 imposed more re-
strictions on resulting structures and therefore de-
tected more discrepancies including compounding
errors. Our conversion rate is about 5.5 points
lower than the English counterpart (Hockenmaier
and Steedman, 2007). Manual investigation of the
sampled derivations would be beneficial for the
conversion improvement.
For the lexicon extraction from the CCGbank,
we obtained 699 types of lexical categories from
616,305 word tokens. After lexical reduction, the
number of categories decreased to 454, which in
turn may produce 5,342 categories by lexical ex-
pansion. The average number of categories for a
word type was 11.68 as a result.
4.2 Evaluation of coverage
Following the evaluation criteria in (Hockenmaier
and Steedman, 2007), we measured the coverage
3http://cl.naist.jp/nldata/corpus/
4https://alaginrc.nict.go.jp/resources/tocorpus/
tocorpusabstract.html
1049
of the grammar on unseen texts. First, we obtained
CCG derivations for evaluation sets by applying
our conversion method and then used these deriva-
tions as gold standard. Lexical coverage indicates
the number of words to which the grammar assigns
a gold standard category. Sentential coverage indi-
cates the number of sentences in which all words
are assigned gold standard categories 5.
Table 6 shows the evaluation results. Lexical
coverage was 99.40% with rare word treatment,
which is in the same level as the case of the En-
glish CCG parser C&C (Clark and Curran, 2007).
We also measured coverage in a ?weak? sense,
which means the number of sentences that are
given at least one analysis (not necessarily cor-
rect) by the obtained grammar. This number was
99.12 % and 99.06 % for the development and the
test set, respectively, which is sufficiently high for
wide-coverage parsing of real-world texts.
4.3 Evaluation of parsing accuracy
Finally, we evaluated the parsing accuracy. We
employed the parser and the supertagger of
(Miyao and Tsujii, 2008), specifically, its gen-
eralized modules for lexicalized grammars. We
trained log-linear models in the same way as
(Clark and Curran, 2007) using the training set as
training data. Feature sets were simply borrowed
from an English parser; no tuning was performed.
Following conventions in research on Japanese de-
pendency parsing, gold morphological analysis re-
sults were input to a parser. Following C&C, the
evaluation measure was precision and recall over
dependencies, where a dependency is defined as a
4-tuple: a head of a functor, a functor category, an
argument slot, and a head of an argument.
Table 7 shows the parsing accuracy on the de-
velopment and the test sets. The supertagging ac-
curacy is presented in the upper table. While our
coverage was almost the same as C&C, the perfor-
mance of our supertagger and parser was lower.
To improve the performance, tuning disambigua-
tion models for Japanese is a possible approach.
Comparing the parser?s performance with previ-
ous works on Japanese dependency parsing is dif-
ficult as our figures are not directly comparable
to theirs. Sassano and Kurohashi (2009) reported
the accuracy of their parser as 88.48 and 95.09
5Since a gold derivation can logically be obtained if gold
categories are assigned to all words in a sentence, sentential
coverage means that the obtained lexicon has the ability to
produce exactly correct derivations for those sentences.
Supertagging accuracy
Lex. Cov. Cat. Acc.
Devel. 99.40 90.86
Test 99.40 90.69
C&C 99.63 94.32
Overall performance
LP LR LF UP UR UF
Devel. 82.55 82.73 82.64 90.02 90.22 90.12
Test 82.40 82.59 82.50 89.95 90.15 90.05
C&C 88.34 86.96 87.64 93.74 92.28 93.00
Table 7: Parsing accuracy. LP, LR and LF refer to
labeled precision, recall, and F-score respectively.
UP, UR, and UF are for unlabeled.
in unlabeled chunk-based and word-based F1 re-
spectively. While our score of 90.05 in unlabeled
category dependency seems to be lower than their
word-based score, this is reasonable because our
category dependency includes more difficult prob-
lems, such as whether a subject PP is shared by
coordinated verbs. Thus, our parser is expected to
be capable of real-world Japanese text analysis as
well as dependency parsers.
5 Conclusion
In this paper, we proposed a method to induce
wide-coverage Japanese resources based on CCG
that will lead to deeper syntactic analysis for
Japanese and presented empirical evaluation in
terms of the quality of the obtained lexicon and
the parsing accuracy. Although our work is basi-
cally in line with CCGbank, the application of the
method to Japanese is not trivial due to the fact that
the relationship between chunk-based dependency
structures and CCG derivations is not obvious.
Our method integrates multiple dependency-
based resources to convert them into an integrated
phrase structure treebank. The obtained treebank
is then transformed into CCG derivations. The
empirical evaluation in Sec. 4 shows that our cor-
pus conversion successfully converts 94 % of the
corpus sentences and the coverage of the lexicon
is 99.4 %, which is sufficiently high for analyz-
ing real-world texts. A comparison of the parsing
accuracy with previous works on Japanese depen-
dency parsing and English CCG parsing indicates
that our parser can analyze real-world Japanese
texts fairly well and that there is room for improve-
ment in disambiguation models.
1050
References
Daisuke Bekki. 2010. Formal Theory of Japanese Syn-
tax. Kuroshio Shuppan. (In Japanese).
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING 2004, pages
1240?1246.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cate-
gorial grammar treebank for Italian. In Proceedings
of the Eighth International Workshop on Treebanks
and Linguistic Theories (TLT8), pages 27?38.
Johan Bos. 2007. Recognising textual entailment and
computational semantics. In Proceedings of Seventh
International Workshop on Computational Seman-
tics IWCS-7, page 1.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of ACL Stu-
dent Research Workshop, pages 73?78.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Takao Gunji. 1987. Japanese Phrase Structure Gram-
mar: A Unification-based Approach. D. Reidel.
Hiroki Hanaoka, Hideki Mima, and Jun?ichi Tsujii.
2010. A Japanese particle corpus built by example-
based annotation. In Proceedings of LREC 2010.
Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP 2011, pages 201?209.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of the Joint Conference of COLING/ACL
2006.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of ACL-HLT 2011, pages 804?813.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text
corpus with predicate-argument and coreference re-
lations. In Proceedings of Linguistic Annotation
Workshop, pages 132?139.
Daisuke Kawahara and Sadao Kurohashi. 2011. Gen-
erative modeling of coordination by factoring paral-
lelism and selectional preferences. In Proceedings
of IJCNLP 2011.
Daisuke Kawahara, Sadao Kurohashi, and Koiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 495?498. (In Japanese).
Nobo Komagata. 1999. Information Structure in Texts:
A Computational Analysis of Contextual Appropri-
ateness in English and Japanese. Ph.D. thesis, Uni-
versity of Pennsylvania.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analyisis using cascaded chunking. In
Proceedings of CoNLL 2002.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction, 2nd
Edition. CSLI Publications.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP 2011.
Manabu Sassano and Sadao Kurohashi. 2009. A uni-
fied single scan algorithm for Japanese base phrase
chunking and dependency parsing. In Proceedings
of ACL-IJCNLP 2009.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL 2007, pages 240?247.
Emiko Yamada, Eiji Aramaki, Takeshi Imai, and
Kazuhiko Ohe. 2010. Internal structure of a disease
name and its application for ICD coding. Studies
in health technology and informatics, 160(2):1010?
1014.
Kazuhiro Yoshida. 2005. Corpus-oriented develop-
ment of Japanese HPSG parsers. In Proceedings of
the ACL Student Research Workshop.
1051
Proceedings of the Fifth Law Workshop (LAW V), pages 56?64,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Collaborative Annotation between Human Annotators and a Statistical
Parser
Shun?ya Iwasawa Hiroki Hanaoka Takuya Matsuzaki
University of Tokyo
Tokyo, Japan
{iwasawa,hkhana,matuzaki}@is.s.u-tokyo.ac.jp
Yusuke Miyao
National Institute of Informatics
Tokyo, Japan
yusuke@nii.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
Beijing, P.R.China
jtsujii@microsoft.com
Abstract
We describe a new interactive annotation
scheme between a human annotator who
carries out simplified annotations on CFG
trees, and a statistical parser that converts
the human annotations automatically into a
richly annotated HPSG treebank. In order
to check the proposed scheme?s effectiveness,
we performed automatic pseudo-annotations
that emulate the system?s idealized behavior
and measured the performance of the parser
trained on those annotations. In addition,
we implemented a prototype system and con-
ducted manual annotation experiments on a
small test set.
1 Introduction
On the basis of the success of the research on the
corpus-based development in NLP, the demand for
a variety of corpora has increased, for use as both a
training resource and an evaluation data-set. How-
ever, the development of a richly annotated cor-
pus such as an HPSG treebank is not an easy task,
since the traditional two-step annotation, in which
a parser first generates the candidates and then an
annotator checks each candidate, needs intensive ef-
forts even for well-trained annotators (Marcus et al,
1994; Kurohashi and Nagao, 1998). Among many
NLP problems, adapting a parser for out-domain
texts, which is usually referred to as domain adap-
tation problem, is one of the most remarkable prob-
lems. The main cause of this problem is the lack
of corpora in that domain. Because it is difficult to
prepare a sufficient corpus for each domain without
reducing the annotation cost, research on annotation
methodologies has been intensively studied.
There has been a number of research projects
to efficiently develop richly annotated corpora with
the help of parsers, one of which is called a
discriminant-based treebanking (Carter, 1997). In
discriminant-based treebanking, the annotation pro-
cess consists of two steps: a parser first generates
the parse trees, which are annotation candidates,
and then a human annotator selects the most plau-
sible one. One of the most important characteristics
of this methodology is to use easily-understandable
questions called discriminants for picking up the fi-
nal annotation results. Human annotators can per-
form annotations simply by answering those ques-
tions without closely examining the whole tree. Al-
though this approach has been successful in break-
ing down the difficult annotations into a set of easy
questions, specific knowledge about the grammar,
especially in the case of a deep grammar, is still re-
quired for an annotator. This would be the bottle-
neck to reduce the cost of annotator training and can
restrict the size of annotations.
Interactive predictive parsing (Sa?nchez-Sa?ez et
al., 2009; Sa?nchez-Sa?ez et al, 2010) is another ap-
proach of annotations, which focuses on CFG trees.
In this system, an annotator revises the currently
proposed CFG tree until he or she gets the correct
tree by using a simple graphical user interface. Al-
though our target product is a more richly anno-
tated treebanks, the interface of CFG can be useful
to develop deep annotations such as HPSG features
by cooperating with a statistical deep parser. Since
CFG is easier to understand than HPSG, it can re-
56
duce the cost of annotator training; non-experts can
perform annotations without decent training. As a
result, crowd-sourcing or similar approach can be
adopted and the annotation process would be accel-
erated.
Before conducting manual annotation, we sim-
ulated the annotation procedure for validating our
system. In order to check whether the CFG-based
annotations can lead to sufficiently accurate HPSG
annotations, several HPSG treebanks were created
with various qualities of CFG and evaluated by their
HPSG qualities.
We further conducted manual annotation experi-
ments by two human annotators to evaluate the ef-
ficiency of the annotation system and the accuracy
of the resulting annotations. The causes of annota-
tion errors were analyzed and future direction of the
further development is discussed.
2 Statistical Deep Parser
2.1 HPSG
Head-Driven Phrase Structure Grammar (HPSG)
is one of the lexicalized grammatical formalisms,
which consists of lexical entries and a collection of
schemata. The lexical entries represent the syntac-
tic and semantic characteristics of words, and the
schemata are the rules that construct larger phrases
from smaller phrases. Figure 1 shows the mecha-
nism of the bottom-up HPSG parsing for the sen-
tence ?Dogs run.? First, a lexical entry is as-
signed to each word, and then, the lexical signs
for ?Dogs? and ?run? are combined by Subject-
Head schema. In this way, lexical signs and phrasal
signs are combined until the whole sentence be-
comes one sign. Compared to Context Free Gram-
mar (CFG), since each sign of HPSG has rich infor-
mation about the phrase, such as subcategorization
frame or predicate-argument structure, a corpus an-
notated in an HPSG manner is more difficult to build
than CFG corpus. In our system, we aim at building
HPSG treebanks with low-cost in which even non-
experts can perform annotations.
2.2 HPSG Deep Parser
The Enju parser (Ninomiya et al, 2007) is a statis-
tical deep parser based on the HPSG formalism. It
produces an analysis of a sentence that includes the
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Dogs
2
64
HEAD verb
SUBJ < noun >
COMPS <>
3
75
Drung
?
2
64
HEAD verb
SUBJ <>
COMPS <>
3
75
Subject
1
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Headj
2
664
HEAD verb
SUBJ < 1 >
COMPS <>
3
775
Figure 1: Example of HPSG parsing for ?Dogs run.?
syntactic structure (i.e., parse tree) and the semantic
structure represented as a set of predicate-argument
dependencies. The grammar design is based on
the standard HPSG analysis of English (Pollard and
Sag, 1994). The parser finds a best parse tree
scored by a maxent disambiguation model using a
CKY-style algorithm and beam search. We used
a toolkit distributed with the Enju parser for ex-
tracting a HPSG lexicon from a PTB-style treebank.
The toolkit initially converts the PTB-style treebank
into an HPSG treebank and then extracts the lexi-
con from it. The HPSG treebank converted from the
test section is also used as the gold standard in the
evaluation.
2.3 Evaluation Metrics
In the experiments shown below, we evaluate the ac-
curacy of an annotation result (i.e., an HPSG deriva-
tion on a sentence) by evaluating the accuracy of
the semantic description produced by the deriva-
tion, as well as a more traditional metrics such
as labeled bracketing accuracy of the tree struc-
ture. Specifically, we used labeled and unlabeled
precision/recall/F-score of the predicate-argument
dependencies and the labeled brackets compared
against a gold-standard annotation obtained by using
the Enju?s treebank conversion tool. A predicate-
argument dependency is represented as a tuple of
?wp, wa, r?, where wp is the predicate word, wa
is the argument word, and r is the label of the
predicate-argument relation, such as verb-ARG1
(semantic subject of a verb) and prep-MOD (modi-
57
fiee of a prepositional phrase). As for the bracketing
accuracies, the label of a bracket is obtained by pro-
jecting the sign corresponding to the phrase into a
simple phrasal labels such as S, NP, and VP.
3 Proposed Annotation System
In our system, a human annotator and a statistical
deep parser cooperate to build a treebank. Our sys-
tem uses CFG as user interface and bridges a gap be-
tween CFG and HPSG with a statistical CKY parser.
Following the idea of the discriminant-based tree-
banking model, the parser first generates candidate
trees and then an annotator selects the correct tree in
the form of a packed forest. For selecting the correct
tree, the annotator only edits a CFG tree projected
from an HPSG tree through pre-defined set of oper-
ations, to eventually give the constraints onto HPSG
trees. This is why annotators can annotate HPSG
trees without HPSG knowledge. The current system
is implemented based on the following client-server
model.
3.1 Client: Annotator Interface
The client-side is an annotator?s interface imple-
mented with Ajax technique, on which annotator?s
revision is carried out through Web-Browser. When
the client-side receives the data of the current best
tree from the server-side, it shows an annotator the
CFG representation of the tree. Then, an annotator
adds revisions to the CFG tree using the same GUI,
until the current best tree has the CFG structure that
exactly matches the annotators? interpretation of the
sentence. Finally, the client-side sends the annota-
tor?s revision as a CGI query to the server. Based
on interactive predicative parsing system, two kinds
of operations are implemented in our system: ?span
modification? and ?label substitution?, here abbrevi-
ated as ?S? and ?L? operations:
?S? operation modify span(left, right)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified span, by sequentially clicking the leaf
nodes at the left and right boundaries.
?L? operation modify label(pos, label)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified label, by inputting a label and clicking the
node position.
In addition to ?S? and ?L? operations, one more
operation, ?tree fixation?, abbreviated ?F?, is imple-
mented for making annotation more efficient. Our
system computes the best tree under the current con-
straints, which are specified by the ?S? and ?L? op-
erations that the annotator has given so far. It means
other parts of the tree that are not constrained may
change after a new operation by the annotator. This
change may lead to a structure that the annotator
does not want. To avoid such unexpected changes,
an annotator can specify a subtree which he or she
does not want to change by ?tree fixation? operation:
?F? operation fix tree(pos = i)
An annotator can specify a subtree as correct
and not to be changed. The specified subtree
does not change and always appears in the best
tree.
3.2 Server: Parsing Constraints
In our annotation system, the server-side carries out
the conversion of annotator?s constraints into HPSG
grammatical constraints on CKY chart and the re-
computation of the current best tree under the con-
straints added so far. The server-side works in the
following two steps. The first step is the conversion
of the annotator?s revision into a collection of dead
edges or dead cells; a dead edge means the edge
must not be a part of the correct tree, and a dead cell
means all edges in the cell are dead. As mentioned
in the background section, Enju creates a CKY chart
during the parsing where all the terminal and non-
terminal nodes are stored with the information of its
sign and links to daughter edges. In our annotation
system, to change the best tree according to the an-
notator?s revision, we determine whether each edge
in the chart is either alive or dead. The server-side
re-constructs the best tree under the constraints that
all the edges used in the tree are alive. The sec-
ond step is the computation of the best tree by re-
constructing the tree from the chart, under the con-
straint that the best tree contains only the alive edges
as its subconstituents. Re-construction includes the
following recursive process:
1. Start from the root edge.
58
2. Choose the link which has the highest probabil-
ity among the links and whose daughter edges
are all alive.
3. If there is such a link, recursively carry out the
process for the daughter edge.
4. If all the links from the edge are dead, go back
to the previous edge.
Note that our system parses a sentence only once,
the first time, instead of re-parsing the sentence after
each revision. Now, we are going to list the revision
operations again and explain how the operations are
interpreted as the constraints in the CKY chart. In
the description below, label(x) means the CFG-
symbol that corresponds to edge x. Note that there
is in principle an infinite variety of possible HPSG
signs. The label function maps this multitude of
signs onto a small set of simple CFG nonterminal
symbols.
?S? operation span(left = i, right = j)
When the revision type is ?S? and the left and
right boundary of the specified span is i and j
in the CGI query, we add the cells which satisfy
the following formula to the list of dead edges.
Suppose the sentence length is L, then the set
of new dead cells is defined as:
{cell(a, b) | 0 ? a < i,i ? b < j }
? {cell(c, d) | i+ 1 ? c ? j,j + 1 ? d ? n },
where the first set means the inhibition of the
edges that span across the left boundary of the
specified span. The second set means a similar
conditions for the right span.
?L? operation fix label(position = i, label = l)
When the revision type is ?L?, the node posi-
tion is i and the label is l in the CGI query, we
determine the set of new dead edges and dead
cells as follows:
1. let cell(a, b) = the cell including i
2. mark those cells that are generated by
span(a, b) as defined above to be dead,
and
3. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= l
?F? operation fix tree(position = i)
(a) prob = 0.4 (b) prob = 0.3 (c) prob = 0.2
NP
NX
NP
Time
NX
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
Time
VP
VP
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
NP
Time
NX
flies
VP
VX
like
NP
DP
an
NX
arrow
Figure 2: Three parse tree candidates of ?Time flies like
an arrow.?
When the revision type is ?F? and the target
node position is i in the CGI query, we carry
out the following process to determine the new
dead edges and cells:
1. for each edge e in the subtree rooted at
node i,
2. let cell(a, b) = the cell including e
3. mark those cells that are generated by
span(a, b) as defined above to be dead
4. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= label(e)
The above procedure adds the constraints so
that the correct tree includes a subtree that has
the same CFG-tree representation as the sub-
tree rooted at i in the current tree.
Finally we show how the best tree for the sentence
?Time flies like an arrow.? changes with the anno-
tator?s operations. Let us assume that the chart in-
cludes the three trees shown (in the CFG representa-
tion) in (Figure 2), and that there are no dead edges.
Let us further assume that the probability of each
tree is as shown in the figure and hence the current
best tree is (a). If the annotator wants to select (b)
as the best tree, s/he can apply ?L? operation on the
root node. The operation makes some of the edges
dead, which include the root edge of tree (a) (see
Figure 3). Accordingly, the best tree is now selected
from (b), (c), etc., and tree (b) will be selected as the
next best tree.
4 Validation of CFG-based Annotation
Because our system does not present HPSG anno-
tations to the annotators, there is a risk that HPSG
annotations are wrong even when their projections
to CFG trees are completely correct. Our expecta-
59
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
fix label
(root,S)
?
Figure 3: Chart constraints by ?L? operation. Solid lines
represent the link of the current best tree and dashed lines
represent the second best one. Dotted lines stand for an
unavailable link due to the death of the source edge.
tion is that the stochastic model of the HPSG parser
properly resolves the remaining ambiguities in the
HPSG annotation within the constraints given by a
part of the CFG trees. In order to check the validity
of this expectation and to measure to what extent the
CFG-based annotations can achieve correct HPSG
annotations, we performed a pseudo-annotation ex-
periment.
In this experiment, we used bracketed sentences
in the Brown Corpus (Kuc?era and Francis, 1967),
and a court transcript portion of the Manually An-
notated Sub-Corpus (MASC) (Ide et al, 2010). We
automatically created HPSG annotations that mimic
the annotation results by an ideal annotator in the
following four steps. First, HPSG treebanks for
these sentences are created by the treebank conver-
sion program distributed with the Enju parser. This
program converts a syntactic tree annotated by Penn
Treebank style into an HPSG tree. Since this pro-
gram cannot convert the sentences that are not cov-
ered by the basic design of the grammar, we used
only those that are successfully converted by the
program throughout the experiments and considered
this converted treebank as the gold-standard tree-
bank for evaluation. Second, the same sentences are
parsed by the Enju parser and the results are com-
pared with the gold-standard treebank. Then, CFG-
level differences between the Enju parser?s outputs
and the gold-standard trees are translated into oper-
ation sequences of the annotation system. For ex-
ample, ?L? operation of NX ? VP at the root node
is obtained in the case of Figure 4. Finally, those
operation sequences are executed on the annotation
system and HPSG annotations are produced.
total size ave. s. l. convertible
Brown 24,243 18.94 22,214
MASC 1,656 14.81 1,353
Table 1: Corpus and experimental data information (s. l.
means ?sentence length.?)
(a) NX
NX PP
PX NP
(b) VP
VP PP
PX NP
Figure 4: CFG representation of parser output (a) and
gold-standard tree (b)
4.1 Relationship between CFG and HPSG
Correctness
We evaluated the automatically produced annota-
tions in terms of three measures: the labeled brack-
eting accuracies of their projections to CFG trees,
the accuracy of the HPSG lexical entry assignments
to the words, and the accuracy of the semantic de-
pendencies extracted from the annotations. The
CFG-labeled bracketing accuracies are defined in
the same way as the traditional PARSEVAL mea-
sures. The HPSG lexical assignment accuracy is
the ratio of words to which the correct HPSG lex-
ical entry is assigned, and the semantic dependency
accuracy is defined as explained in Section 2.3. In
this experiment, we cut off sentences longer than 40
words for time reasons. We split the Brown Cor-
pus into three parts: training, development test and
evaluation, and evaluated the automatic annotation
results only for the training portion.
We created three sets of automatic annotations as
follows:
Baseline No operation; default parsing results are
considered as the annotation results.
S-full Only ?S? operations are used; the tree struc-
tures of the resulting annotations should thus be
identical to the gold-standard annotations.
SL-full ?S? and ?L? operations are used; the la-
beled tree structures of the resulting anno-
tations should thus be identical to the gold-
standard annotations.
Before showing the evaluation results, splitting of
the data should be described here. Our system as-
sumes that the correct tree is included in the parser?s
60
CKY chart; however, because of the beam-search
limitation and the incomplete grammar coverage, it
does not always hold true. In this paper, such sit-
uations are called ?out-chart?. Conversely, the sit-
uations in which the parser does include the cor-
rect tree in the CKY chart are ?in-chart?. The re-
sults of ?in-chart? are here considered to be the re-
sults in the ideal situation of the perfect parser. In
our experimental setting, the training portion of the
Brown Corpus has 10,576 ?in-chart? and 7,208 ?out-
chart? sentences, while the MASC portion has 864
?in-chart? and 489 ?out-chart? sentences (Table 2).
Under ?out-chart? situations, we applied the opera-
tions greedily for calculating S-full and SL-full; that
is, all operations are sequentially applied and an op-
eration is skipped when there are no HPSG trees in
the CKY chart after applying that operation.
Table 3 shows the results of our three measures:
the CFG tree bracketing accuracy, the accuracy of
HPSG lexical entry assignment and that of the se-
mantic dependency. In both of S-full and SL-full,
the improvement from the baseline is significant.
Especially, SL-full for ?in-chart? data has almost
complete agreement with the gold-standard HPSG
annotations. The detailed figures are shown in Ta-
ble 4. Therefore, we can therefore conclude that
high quality CFG annotations lead to high quality
HPSG annotations when the are combined with a
good statistical HPSG parser.
4.2 Domain Adaptation
We evaluated the parser accuracy adapted with the
automatically created treebank on the Brown Cor-
pus. In this experiment, we used the adaptation al-
gorithm by (Hara et al, 2007), with the same hyper-
parameters used there. Table 5 shows the result of
the adapted parser. Each line of this table stands for
the parser adapted with different data. ?Gold? is the
result adapted on the gold-standard annotations, and
?Gold (only covered)? is that adapted on the gold
data which is covered by the original Enju HPSG
grammar that was extracted from the WSJ portion
of the Penn Treebank. ?SL-full? is the result adapted
on our automatically created data. ?Baseline? is the
result by the original Enju parser, which is trained
only on the WSJ-PTB and whose grammar was ex-
tracted from the WSJ-PTB. The table shows SL-full
slightly improves the baseline results, which indi-
#operations
S L F Avg. Time
Brown A. 1 122 1 0 1.19 43.32A. 2 91 4 1 0.94 41.77
MASC A. 1 275 2 5 2.76 33.33A. 2 52 2 0 0.51 35.13
Table 6: The number of operations and annotation time
by human annotators. ?Annotator? is abbreviated as A.
Avg. is the average number of operations per sentence
and Time is annotation time per sentence [sec.].
cates our annotation system can be useful for do-
main adaptation. Because we used mixed data of
?in-chart? and ?out-chart? in this experiment, there
still is much room for improvement by increasing
the ratio of the ?in-chart? sentences using a larger
beam-width.
5 Interactive Annotation on a
Prototype-system
We developed an initial version of the annotation
system described in Section 3, and annotated 200
sentences in total on the system. Half of the sen-
tences were taken from the Brown corpus and the
other half were taken from a court-debate section of
the MASC corpus. All of the sentences were an-
notated twice by two annotators. Both of the anno-
tators has background in computer science and lin-
guistics.
Table 6 shows the statistics of the annotation pro-
cedures. This table indicates that human annotators
strongly prefer ?S? operation to others, and that the
manual annotation on the prototype system is at least
comparable to the recent discriminant-based annota-
tion system by (Zhang and Kordoni, 2010), although
the comparison is not strict because of the difference
of the text.
Table 7 shows the automatic evaluation results.
We can see that the interactive annotation gave slight
improvements in all accuracy metrics. The improve-
ments were however not as much as we desired.
By classifying the remaining errors in the anno-
tation results, we identified several classes of major
errors:
1. Truly ambiguous structures, which require the
context or world-knowledge to correctly re-
solve them.
61
in out in+out
Brown (train.) 10,576 / 10,394 7,190 / 6,464 17,766 / 16,858
MASC 864 / 857 489 / 449 1,353 / 1,306
Table 2: The number of ?in-chart? and ?out-chart? sentences (total / 1-40 length)
in out in+out
Brown
SL-full 100.00 / 99.31 / 99.60 88.67 / 83.95 / 82.00 94.91 / 92.21 / 92.24
S-full 98.46 / 96.64 / 96.83 89.60 / 82.02 / 81.20 94.48 / 89.88 / 90.29
Baseline 92.39 / 92.69 / 90.54 82.10 / 78.38 / 73.80 87.78 / 86.07 / 83.54
MASC
SL-full 100.00 / 99.13 / 99.30 85.91 / 80.75 / 78.80 93.38 / 90.55 / 91.02
S-full 98.71 / 96.88 / 96.73 86.95 / 79.14 / 77.43 93.18 / 88.60 / 88.93
Baseline 93.98 / 93.51 / 91.56 80.00 / 75.89 / 72.22 87.43 / 85.30 / 83.75
Table 3: Evaluation of the automatic annotation sets. Each cell has the score of CFG F1 / Lex. Acc. / Dep. F1.
CFG tree accuracy
Brown MASC
A. 1 90.55 / 90.83 / 90.69 90.62 / 90.80 / 90.71
A. 2 91.01 / 91.09 / 91.05 91.01 / 91.09 / 91.05
Enju 89.70 / 89.74 / 89.72 90.02 / 90.20 / 90.11
PAS dependency accuracy
Brown MASC
A. 1 87.48 / 87.55 / 87.52 86.02 / 86.02 / 86.02
A. 2 88.42 / 88.27 / 88.34 85.28 / 91.01 / 85.32
Enju 87.12 / 86.91 / 87.01 84.81 / 84.26 / 84.53
Table 7: Automatic evaluation of the annotation results
(LP / LR / F1)
CFG tree accuracy
in-chart out-chart
A. 1 94.52 / 94.65 / 94.58 83.95 / 84.44 / 84.19
A. 2 95.07 / 95.14 / 95.10 84.22 / 84.32 / 84.27
Enju 94.44 / 94.37 / 94.40 81.81 / 82.00 / 81.90
PAS dependency accuracy
in-chart out-chart
A. 1 92.85 / 92.85 / 92.85 77.47 / 77.65 / 77.56
A. 2 93.34 / 93.34 / 93.34 79.17 / 78.80 / 78.98
Enju 92.73 / 92.73 / 92.73 76.57 / 76.04 / 76.30
Table 8: Automatic evaluation of the annotation results
(LP/LR/F1); in-chart sentences (left-column) and out-
chart sentences (right column) both from Brown
2. Purely grammar-dependent analyses, which re-
quire in-depth knowledge of the specific HPSG
grammar behind the simplified CFG-tree repre-
sentation given to the annotators.
3. Discrepancy between human intuition and the
convention in the HPSG grammar introduced
by the automatic conversion.
4. Apparently wrong analysis left untouched due
to the limitation of the annotation system.
We suspect some of the errors of type 1 have been
caused by the experimental setting of the annotation;
we gave the test sentences randomly drawn from
the corpus in a randomized order. This would have
made it difficult for the annotators to interpret the
sentences correctly. We thus expect this kind of er-
rors would be reduced by doing the annotation on a
larger chunk of text.
The second type of the errors are due to the fact
that the annotators are not familiar with the details
of the Enju English HPSG grammar. For example,
one of the annotators systematically chose a struc-
ture like (NP (NP a cat) (PP on the mat)). This struc-
ture is however always analysed as (NP a (NP? cat
(PP on the mat))) by the Enju grammar. The style of
the analysis implemented in the grammar thus some-
times conflicts with the annotators? intuition and it
introduces errors in the annotation results.
Our intention behind the design of the annotation
system was to make the annotation system more ac-
cessible to non-experts and reduce the cost of the
annotation. To reduce the type 2 errors, rather than
the training of the annotators for a specific gram-
mar, we plan to introduce another representation
system in which the grammar-specific conventions
become invisible to the annotators. For example, the
above-shown difference in the bracketing structures
of a determiner-noun-PP sequence can be hidden by
showing the noun phrase as a ternary branch on the
three children: (NP a cat (PP on the mat)).
The third type of the errors are mainly due to the
rather arbitrary choice of the HPSG analysis intro-
duced through the semi-automatic treebank conver-
sion used to extract the HPSG grammar. For in-
stance, the Penn Treebank annotates a structure in-
cluding an adverb that intervenes an auxiliary verb
62
Lex-Acc Dep-LP Dep-LR Dep-UP Dep-UR Dep-F1 Dep-EM
Brown 99.26 99.61 99.59 99.69 99.67 99.60 95.80
MASC 99.13 99.26 99.33 99.42 99.49 99.30 95.68
Table 4: HPSG agreement of SL-full for ?in-chart? data (EM means ?Exact Match.?)
LP LR UP UR F1 EM
Gold 85.62 85.41 89.70 69.47 85.51 45.07
Gold (only covered) 84.32 84.01 88.72 88.40 84.17 42.52
SL-full 83.27 82.88 87.93 87.52 83.08 40.19
Baseline 82.64 82.20 87.50 87.03 82.42 37.63
Table 5: Domain Adaptation Results
and a following verb as in (VP is (ADVP already)
installed). The attachment direction of the adverb is
thus left unspecified. Such structures are however
indistinguishably transformed to a binary structure
like (VP (VP? is already) installed) in the course of
the conversion to HPSG analysis since there is no
way to choose the proper direction only with the
information given in the source corpus. This de-
sign could be considered as a best-effort, systematic
choice under the insufficient information, but it con-
flicts with the annotators? intuition in some cases.
We found in the annotation results that the anno-
tators have left apparently wrong analyses on some
sentences, either those remaining from the initial
output proposed by the parser or a wrong structure
appeared after some operations by the annotators
(error type 4). Such errors are mainly due to the
fact that for some sentences a correct analysis cannot
be found in the parser?s CKY chart. This can hap-
pen either when the correct analysis is not covered
by the HPSG grammar, or the correct analysis has
been pruned by the beam-search mechanism in the
parser. To correct a wrong analysis from the insuffi-
cient grammar coverage, an expansion of the gram-
mar is necessary, either in the form of the expan-
sion of the lexicon, or an introduction of a new lex-
ical type. For the other errors from the beam-search
limitation, there is a chance to get a correct analysis
from the parser by enlarging the beam size as nec-
essary. The introduction of a new lexical type def-
initely requires a deep knowledge on the grammar
and thus out of the scope of our annotation frame-
work. The other cases can in principle be handled in
the current framework, e.g., by a dynamic expansion
of the lexicon (i.e., an introduction of a new associ-
ation between a word and known lexical type), and
by a dynamic tuning of the beam size.
To see the significance of the last type of the er-
ror, we re-evaluated the annotation results on the
Brown sentences after classifying them into: (1)
those for which the correct analyses were included
in the parser?s chart (in-chart, 65 sentences) and (2)
those for which the correct analyses were not in the
chart (out-chart, 35 sentences), either because of the
pruning effect or the insufficient grammar coverage.
The results shown in Table 8 clearly show that there
is a large difference in the accuracy of the annota-
tion results between these two cases. Actually, on
the in-chart sentences, the parser has returned the
correct analysis as the initial solution for over 50%
of the sentences, and the annotators saved it without
any operations. Thus, we believe it is quite effective
to add the above-mentioned functionalities to reduce
this type of errors.
6 Conclusion and Future Work
We proposed a new annotation framework for deep
grammars by using statistical parsers. From the the-
oretical point of view, we can achieve significantly
high quality HPSG annotations only by CFG annota-
tions, and the products can be useful for the domain
adaptation task. On the other hand, preliminary ex-
periments of a manual annotation show some diffi-
culties about CFG annotations for non-experts, es-
pecially grammar-specific ones. We hence need to
develop some bridging functions reducing such dif-
ficulties. One possible strategy is to introduce an-
other representation such as flat CFG than binary
CFG. While we adopted CFG interface in our first
prototype system, our scheme can be applied to an-
other interface such as dependency as long as there
exist some relatedness over syntax or semantics.
63
References
David Carter. 1997. The treebanker: a tool for super-
vised training of parsed corpora. In Workshop On
Computational Environments For Grammar Develop-
ment And Linguistic Engineering, pages 9?15.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of the 10th International Con-
ference on Parsing Technologies, pages 11?22, Prague,
Czech Republic.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the peo-
ple. In Proceedings of the ACL 2010 Conference Short
Papers, pages 68?73, Uppsala, Sweden, July.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a japanese parsed corpus while improving the parsing
system. In Proceedings of the NLPRS, pages 719?724.
Henry Kuc?era and W. Nelson Francis. 1967. Compu-
tational Analysis of Present Day American English.
Brown University Press, June.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on Human Language
Technology, pages 114?119.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of the 10th International Confer-
ence on Parsing Technologies, pages 60?68.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez, and Jose?-
Miguel Bened??. 2009. Interactive predictive parsing.
In Proceedings of the 11th International Conference
on Parsing Technologies, pages 222?225.
Ricardo Sa?nchez-Sa?ez, Luis A. Leiva, Joan-Andreu
Sa?nchez, and Jose?-Miguel Bened??. 2010. Interactive
predictive parsing using a web-based architecture. In
Proceedings of the NAACL HLT 2010 Demonstration
Session, pages 37?40.
Yi Zhang and Valia Kordoni. 2010. Discriminant rank-
ing for efficient treebanking. In Coling 2010: Posters,
pages 1453?1461, Beijing, China, August. Coling
2010 Organizing Committee.
64
