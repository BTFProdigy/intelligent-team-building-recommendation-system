Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 1?9,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Sense-based Interpretation of Logical Metonymy Using a Statistical
Method
Ekaterina Shutova
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
Ekaterina.Shutova@cl.cam.ac.uk
Abstract
The use of figurative language is ubiqui-
tous in natural language texts and it is a
serious bottleneck in automatic text un-
derstanding. We address the problem of
interpretation of logical metonymy, using
a statistical method. Our approach origi-
nates from that of Lapata and Lascarides
(2003), which generates a list of non-
disambiguated interpretations with their
likelihood derived from a corpus. We pro-
pose a novel sense-based representation
of the interpretation of logical metonymy
and a more thorough evaluation method
than that of Lapata and Lascarides (2003).
By carrying out a human experiment we
prove that such a representation is intu-
itive to human subjects. We derive a rank-
ing scheme for verb senses using an unan-
notated corpus, WordNet sense numbering
and glosses. We also provide an account
of the requirements that different aspec-
tual verbs impose onto the interpretation
of logical metonymy. We tested our sys-
tem on verb-object metonymic phrases. It
identifies and ranks metonymic interpreta-
tions with the mean average precision of
0.83 as compared to the gold standard.
1 Introduction
Metonymy is defined as the use of a word or a
phrase to stand for a related concept which is not
explicitly mentioned. Here are some examples of
metonymic phrases:
(1) The pen is mightier than the sword.
(2) He played Bach.
(3) He drank his glass. (Fass, 1991)
(4) He enjoyed the book. (Lapata and Lascarides,
2003)
(5) After three martinis John was feeling well.
(Godard and Jayez, 1993)
The metonymic adage in (1) is a classical ex-
ample. Here the pen stands for the press and the
sword for military power. In the following exam-
ple Bach is used to refer to the composer?s music
and in (3) the glass stands for its content, i.e. the
actual drink (beverage).
The sentences (4) and (5) represent a varia-
tion of this phenomenon called logical metonymy.
Here both the book and three martinis have even-
tive interpretations, i.e. the noun phrases stand
for the events of reading the book and drinking
three martinis respectively. Such behaviour is
triggered by the type requirements the verb (or
the preposition) places onto its argument. This
is known in linguistics as a phenomenon of type
coercion. Many existing approaches to logical
metonymy explain systematic syntactic ambiguity
of metonymic verbs (such as enjoy) or preposi-
tions (such as after) by means of type coercion
(Pustejovsky, 1991; Pustejovsky, 1995; Briscoe
et al, 1990; Verspoor, 1997; Godard and Jayez,
1993).
Logical metonymy occurs in natural language
texts relatively frequently. Therefore, its auto-
matic interpretation would significantly facilitate
the task of many NLP applications that require
semantic processing (e.g., machine translation,
information extraction, question answering and
many others). Utiyama et al (2000) followed by
Lapata and Lascarides (2003) used text corpora to
automatically derive interpretations of metonymic
phrases.
1
Utiyama et al (2000) used a statistical model
for the interpretation of general metonymies for
Japanese. Given a verb-object metonymic phrase,
such as read Shakespeare, they searched for en-
tities the object could stand for, such as plays of
Shakespeare. They considered all the nouns co-
occurring with the object noun and the Japanese
equivalent of the preposition of. Utiyama and his
colleagues tested their approach on 75 metonymic
phrases taken from the literature and reported a
precision of 70.6%, whereby an interpretation was
considered correct if it made sense in some imag-
inary context.
Lapata and Lascarides (2003) extend Utiyama?s
approach to interpretation of logical metonymies
containing aspectual verbs (e.g. begin the book)
and polysemous adjectives (e.g. good meal vs.
good cook). Their method generates a list of in-
terpretations with their likelihood derived from a
corpus.
Lapata and Lascarides define an interpretation
of logical metonymy as a verb string, which is am-
biguous with respect to word sense. Some of these
strings indeed correspond to paraphrases that a hu-
man would give for the metonymic phrase. But
they are not meaningful as such for automatic pro-
cessing, since their senses still need to be disam-
biguated in order to obtain the actual meaning. For
example, compare the grab sense of take vs. its
film sense for the metonymic phrase finish video.
It is obvious that only the latter sense is a correct
interpretation.
We extend the experiment of Lapata and Las-
carides by disambiguating the interpretations with
respect to WordNet (Fellbaum, 1998) synsets (for
verb-object metonymic phrases). We propose a
novel ranking scheme for the synsets using a
non-disambiguated corpus, address the issue of
sense frequency distribution and utilize informa-
tion from WordNet glosses to refine the ranking.
We conduct and experiment to show that our
representation of a metonymic interpretation as a
synset is intuitive to human subjects. In the dis-
cussion section we provide an overview of the
constraints on logical metonymy pointed out in
linguistics literature, as well as proposing some
additional constraints (e.g. on the type of the
metonymic verb, on the type of the reconstructed
event, etc.)
Metonymic Phrase Interpretations Log-probability
finish video film -19.65
edit -20.37
shoot -20.40
view -21.19
play -21.29
stack -21.75
make -21.95
programme -22.08
pack -22.12
use -22.23
watch -22.36
produce -22.37
Table 1: Interpretations of Lapata and Lascarides
(2003) for finish video
2 Lapata and Lascarides? Method
The intuition behind the approach of Lapata and
Lascarides is similar to that of Pustejovsky (1991;
1995), namely that there is an event not explic-
itly mentioned, but implied by the metonymic
phrase (begin to read the book, or the meal that
tastes good vs. the cook that cooks well). They
used the British National Corpus (BNC)(Burnard,
2007) parsed by the Cass parser (Abney, 1996) to
extract events (verbs) co-occurring with both the
metonymic verb (or adjective) and the noun inde-
pendently and ranked them in terms of their like-
lihood according to the data. The likelihood of a
particular interpretation is calculated using the fol-
lowing formula:
P (e, v, o) =
f(v, e) ? f(o, e)
N ? f(e)
, (1)
where e stands for the eventive interpretation of
the metonymic phrase, v for the metonymic verb
and o for its noun complement. f(e), f(v, e)
and f(o, e) are the respective corpus frequencies.
N =
?
i
f(e
i
) is the total number of verbs in the
corpus. The list of interpretations Lapata and Las-
carides (2003) report for the phrase finish video is
shown in Table 1.
Lapata and Lascarides compiled their test set by
selecting 12 verbs that allow logical metonymy
1
from the lexical semantics literature and combin-
ing each of them with 5 nouns. This yields 60
phrases, which were then manually filtered, ex-
cluding 2 phrases as non-metonymic.
They compared their results to paraphrase
judgements elicited from humans. The subjects
were presented with three interpretations for each
1
attempt, begin, enjoy, finish, expect, postpone, prefer, re-
sist, start, survive, try, want
2
metonymic phrase (from high, medium and low
probability ranges) and were asked to associate a
number with each of them reflecting how good
they found the interpretation. They report a cor-
relation of 0.64, whereby the inter-subject agree-
ment was 0.74. It should be noted, however, that
such an evaluation scheme is not very informa-
tive as Lapata and Lascarides calculate correlation
only on 3 data points for each phrase out of many
more yielded by the model. It fails to take into
account the quality of the list of top interpreta-
tions, although the latter is deemed to be the aim of
such applications. In comparison the fact that La-
pata and Lascarides initially select the interpreta-
tions from high, medium or low probability ranges
makes the task significantly easier.
3 Alternative Interpretation of Logical
Metonymy
The approach of Lapata and Lascarides (2003)
produces a list of non-disambiguated verbs, essen-
tially just strings, representing possible interpreta-
tions of a metonymic phrase. We propose an alter-
native representation of metonymy interpretation
consisting of a list of senses that map to WordNet
synsets. However, the sense-based representation
builds on the list of non-disambiguated interpreta-
tions similar to the one of Lapata and Lascarides.
Our method consists of the following steps:
? Step 1 Use the method of Lapata and Las-
carides (2003) to obtain a set of candidate in-
terpretations (strings) from a non-annotated
corpus. We expect our reimplementation of
the method to extract data more accurately,
since we use a more robust parser (RASP
(Briscoe et al, 2006)), take into account more
syntactic structures (coordination, passive),
and extract our data from a newer version of
the BNC.
? Step 2 Map strings to WordNet synsets. We
noticed that good interpretations in the lists
yielded by Step 1 tend to form coherent se-
mantic classes (e.g. take, shoot [a video] vs.
view, watch [a video]). We search the list
for verbs, whose senses are in hyponymy and
synonymy relations with each other accord-
ing to WordNet and store these senses.
? Step 3 Rank the senses, adopting Zipfian
sense frequency distribution and using the
initial string likelihood as well as the infor-
mation from WordNet glosses.
Sense disambiguation is essentially performed
in both Step 2 and Step 3. One of the challenges
of our task is that we use a non-disambiguated cor-
pus while ranking particular senses. This is due to
the fact that there is no word sense disambiguated
corpus available, which would be large enough to
reliably extract statistics for metonymic interpre-
tations.
4 Extracting Ambiguous Interpretations
4.1 Parameter Estimation
We used the method developed by Lapata and
Lascarides (2003) to create the initial list of non-
disambiguated interpretations. The parameters of
the model were estimated from the British Na-
tional Corpus (BNC) (Burnard, 2007) that was
parsed using the RASP parser of Briscoe et al
(2006). We used the grammatical relations (GRs)
output of RASP for BNC created by Andersen et
al. (2008). In particular, we extracted all direct
and indirect object relations for the nouns from
the metonymic phrases, i.e. all the verbs that take
the head noun in the compliment as an object (di-
rect or indirect), in order to obtain the counts for
f(o, e). Relations expressed in the passive voice
and with the use of coordination were also ex-
tracted. The verb-object pairs attested in the cor-
pus only once were discarded, as well as the verb
be, since it does not add any semantic informa-
tion to the metonymic interpretation. In the case
of indirect object relations, the verb was consid-
ered to constitute an interpretation together with
the preposition, e.g. for the metonymic phrase en-
joy the city the correct interpretation is live in as
opposed to live.
As the next step we need to identify all possible
verb phrase (VP) complements to the metonymic
verb (both progressive and infinitive), which rep-
resent f(v, e). This was done by searching for
xcomp relations in the GRs output of RASP, in
which our metonymic verb participates in any of
its inflected forms. Infinitival and progressive
complement counts were summed up to obtain the
final frequency f(v, e).
After the frequencies f(v, e) and f(o, e) were
obtained, possible interpretations were ranked ac-
cording to the model of Lapata and Lascarides
(2003). The top interpretations for the metonymic
3
finish video enjoy book
Interpretations Log-prob Interpretations Log-prob
view -19.68 read -15.68
watch -19.84 write -17.47
shoot -20.58 work on -18.58
edit -20.60 look at -19.09
film on -20.69 read in -19.10
film -20.87 write in -19.73
view on -20.93 browse -19.74
make -21.26 get -19.90
edit of -21.29 re-read -19.97
play -21.31 talk about -20.02
direct -21.72 see -20.03
sort -21.73 publish -20.06
look at -22.23 read through -20.10
record on -22.38 recount in -20.13
Table 2: Possible Interpretations of Metonymies
Ranked by our System
phrases enjoy book and finish video together with
their log-probabilities are shown in Table 2.
4.2 Comparison with the Results of Lapata
and Lascarides
We compared the output of our reimplementation
of Lapata and Lascarides? algorithm with their re-
sults, which we obtained from the authors. The
major difference between the two systems is that
we extracted our data from the BNC parsed by
RASP, as opposed to the Cass chunk parser (Ab-
ney, 1996) utilized by Lapata and Lascarides. Our
system finds approximately twice as many in-
terpretations as theirs and covers 80% of their
lists (our system does not find some of the low-
probability range verbs of Lapata and Lascarides).
We compared the rankings of the two implemen-
tations in terms of Pearson correlation coefficient
and obtained the average correlation of 0.83 (over
all metonymic phrases).
We also evaluated the performance of our sys-
tem against the judgements elicited from humans
in the framework of the experiment of Lapata and
Lascarides (2003) (for a detailed description of
the human evaluation setup see (Lapata and Las-
carides, 2003), pages 12-18). The Pearson corre-
lation coefficient between the ranking of our sys-
tem and the human ranking equals to 0.62 (the in-
tersubject agreement on this task is 0.74). This
is slightly lower than the number achieved by La-
pata and Lascarides (0.64). Such a difference is
probably due to the fact that our system does not
find some of the low-probability range verbs that
Lapata and Lascarides included in their test set,
and thus those interpretations get assigned a prob-
ability of 0. We conducted a one-tailed t-test to
determine if our counts were significantly differ-
ent from those of Lapata and Lascarides. The dif-
ference is statistically insignificant (t=3.6; df=180;
p<.0005), and the output of the system is deemed
acceptable to be used for further experiments.
5 Mapping Interpretations to WordNet
Senses
The interpretations at this stage are just strings
representing collectively all senses of the verb.
What we aim for is the list of verb senses that are
correct interpretations for the metonymic phrase.
We assume the WordNet synset representation of
a sense.
It has been recognized (Pustejovsky, 1991;
Pustejovsky, 1995; Godard and Jayez, 1993) and
verified by us empirically that correct interpreta-
tions tend to form semantic classes, and therefore,
correct interpretations should be related to each
other by semantic relations, such as synonymy or
hyponymy. In order to select the right senses of
the verbs in the context of the metonymic phrase
we did the following.
? We searched the WordNet database for the
senses of the verbs that are in synonymy, hy-
pernymy and hyponymy relations.
? We stored the corresponding synsets in a new
list of interpretations. If one synset was a hy-
pernym (or hyponym) of the other, then both
synsets were stored.
For example, for the metonymic phrase finish
video the interpretations watch, view and see
are synonymous, therefore a synset contain-
ing (watch(3) view(3) see(7)) was
stored. This means that sense 3 of watch, sense
3 of view and sense 7 of see would be correct
interpretations of the metonymic expression.
The obtained number of synsets ranges from 14
(try shampoo) to 1216 (want money) for the whole
dataset of Lapata and Lascarides (2003).
6 Ranking the Senses
A problem that arises with the lists of synsets ob-
tained is that they contain different senses of the
same verb. However, very few verbs have such a
range of meanings that their two different senses
could represent two distinct metonymic interpre-
tations (e.g., in case of take interpretation of finish
video shoot sense and look at, consider sense are
4
both acceptable interpretations, the second obvi-
ously being dispreferred). In the vast majority of
cases the occurrence of the same verb in different
synsets means that the list still needs filtering.
In order to do this we rank the synsets accord-
ing to their likelihood of being a metonymic inter-
pretation. The sense ranking is largely based on
the probabilities of the verb strings derived by the
model of Lapata and Lascarides (2003).
6.1 Zipfian Sense Frequency Distribution
The probability of each string from our initial list
represents the sum of probabilities of all senses of
this verb. Hence this probability mass needs to be
distributed over senses first. The sense frequency
distribution for most words tends to be closer to
Zipfian, rather than uniform or any other distribu-
tion (Preiss, 2006). This is an approximation that
we rely on, as it has been shown to realistically
describe the majority of words.
This means that the first senses will be favoured
over the others, and the frequency of each sense
will be inversely proportional to its rank in the list
of senses (i.e. sense number, since word senses are
ordered in WordNet by frequency).
P
v,k
= P
v
?
1
k
(2)
where k is the sense number and P
v
is the likeli-
hood of the verb string being an interpretation ac-
cording to the corpus data, i.e.
P
v
=
N
v
?
k=1
P
v,k
(3)
where N
v
is the total number of senses for the verb
in question.
The problem that arises with (2) is that the in-
verse sense numbers (1/k) do not add up to 1. In
order to circumvent this, the Zipfian distribution
is commonly normalised by the Nth generalised
harmonic number. Assuming the same notation
P
v,k
= P
v
?
1/k
?
N
v
n=1
1/n
(4)
Once we have obtained the sense probabilities
P
v,k
, we can calculate the likelihood of the whole
synset
P
s
=
I
s
?
i=1
P
v
i
,k
(5)
where v
i
is a verb in the synset s and I
s
is the
total number of verbs in the synset s. The verbs
suggested by WordNet, but not attested in the
corpus in the required environment, are assigned
the probability of 0. Some output synsets for
the metonymic phrase finish video and their log-
probabilities are demonstrated in Table 3.
In our experiment we compare the performance
of the system assuming a Zipfian distribution of
senses against the baseline using a uniform distri-
bution. We expect the former to yield better re-
sults.
6.2 Gloss Processing
The model in the previous section penalizes
synsets that are incorrect interpretations. How-
ever, it can not discriminate well between the ones
consisting of a single verb. By default it favours
the sense with a smaller sense number in Word-
Net. This poses a problem for the examples such
as direct for the phrase finish video: our list con-
tains several senses of it, as shown in Table 4, and
their ranking is not satisfactory. The only correct
interpretation in this case, sense 3, is assigned a
lower likelihood than the senses 1 and 2.
The most relevant synset can be found by us-
ing the information from WordNet glosses (the
verbal descriptions of concepts, often with ex-
amples). We searched for the glosses contain-
ing terms related to the noun in the metonymic
phrase, here video. Such related terms would
be its direct synonyms, hyponyms, hypernyms,
meronyms or holonyms according to WordNet.
We assigned more weight to the synsets whose
gloss contained related terms. In our example
the synset (direct-v-3), which is the correct
metonymic interpretation, contained the term film
in its gloss and was therefore selected. Its likeli-
hood was multiplied by the factor of 10.
It should be noted, however, that the glosses do
not always contain the related terms; the expecta-
tion is that they will be useful in the majority of
cases, not in all of them.
7 Evaluation
7.1 The Gold Standard
We selected the most frequent metonymic verbs
for our experiments: begin, enjoy, finish, try, start.
We randomly selected 10 metonymic phrases con-
taining these verbs. We split them into the devel-
opment set (5 phrases) and the test set (5 phrases)
5
Synset and its Gloss Log-prob
( watch-v-1 ) - look attentively; ?watch a basketball game? -4.56
( view-v-2 consider-v-8 look-at-v-2 ) - look at carefully; study mentally; ?view a problem? -4.66
( watch-v-3 view-v-3 see-v-7 catch-v-15 take-in-v-6 ) - see or watch; ?view a show on television?; ?This program
will be seen all over the world?; ?view an exhibition?; ?Catch a show on Broadway?; ?see a movie? -4.68
( film-v-1 shoot-v-4 take-v-16 ) - make a film or photograph of something; ?take a scene?; ?shoot a movie? -4.91
( edit-v-1 redact-v-2 ) - prepare for publication or presentation by correcting, revising, or adapting; ?Edit a
book on lexical semantics?; ?she edited the letters of the politician so as to omit the most personal passages? -5.11
( film-v-2 ) - record in film; ?The coronation was filmed? -5.74
( screen-v-3 screen-out-v-1 sieve-v-1 sort-v-1 ) - examine in order to test suitability; ?screen these samples?;
?screen the job applicants? -5.91
( edit-v-3 cut-v-10 edit-out-v-1 ) - cut and assemble the components of; ?edit film?; ?cut recording tape? -6.20
Table 3: Metonymy Interpretations as Synsets (for finish video)
Synset and its Gloss Log-prob
( direct-v-1 ) - command with authority; ?He directed the children to do their homework? -6.65
( target-v-1 aim-v-5 place-v-7 direct-v-2 point-v-11 ) - intend (something) to move towards a certain goal;
?He aimed his fists towards his opponent?s face?; ?criticism directed at her superior?; ?direct your anger
towards others, not towards yourself? -7.35
( direct-v-3 ) - guide the actors in (plays and films) -7.75
( direct-v-4 ) - be in charge of -8.04
Table 4: Different Senses of direct (for finish video)
Development Set Test Set
enjoy book enjoy story
finish video finish project
start experiment try vegetable
finish novel begin theory
enjoy concert start letter
Table 5: Metonymic Phrases in Development and
Test Sets
given in the table 5.
The gold standards were created for the top 30
synsets of each metonymic phrase after ranking.
This threshold was set experimentally: the recall
of correct interpretations among the top 30 synsets
is 0.75 (average over metonymic phrases from the
development set). This threshold allows to filter
out a large number of incorrect interpretations.
The interpretations that are plausible in some
imaginary context are marked as correct in the
gold standard.
7.2 Evaluation Measure
We evaluated the performance of the system
against the gold standard. The objective was to
find out if the synsets were distributed in such a
way that the plausible interpretations appear at the
top of the list and the incorrect ones at the bottom.
The evaluation was done in terms of mean average
precision (MAP) at top 30 synsets.
MAP =
1
M
M
?
j=1
1
N
j
N
j
?
i=1
P
ji
, (6)
where M is the number of metonymic phrases,
N
j
is the number of correct interpretations for the
metonymic phrase, P
ji
is the precision at each cor-
rect interpretation (the number of correct interpre-
tations among the top i ranks). First, the aver-
age precision was computed for each metonymic
phrase independently. Then the mean values were
calculated for the development and the test sets.
The reasoning behind computing MAP instead
of precision at a fixed number of synsets (e.g.
top 30) is that the number of correct interpreta-
tions varies dramatically for different metonymic
phrases. MAP essentially evaluates how many
good interpretations appear at the top of the list,
which takes this variation into account.
7.3 Results
We compared the ranking obtained by applying
Zipfian sense frequency distribution against that
obtained by distributing probability mass over
senses uniformly (baseline). We also considered
the rankings before and after gloss processing.
The results are shown in Table 6. These results
demonstrate the positive contribution of both Zip-
fian distribution and gloss processing to the rank-
ing.
7.4 Human Experiment
We conducted an experiment with humans in order
to prove that this task is intuitive to people, i.e.
they agree on the task.
We had 8 volunteer subjects altogether. All of
6
Dataset Verb Probability Gloss MAP
Mass Distribution Processing
Development set Uniform No 0.51
Development set Zipfian No 0.65
Development set Zipfian Yes 0.73
Test set Zipfian Yes 0.83
Table 6: Evaluation of the Model Ranking
Group 1 Group 2
finish video finish project
start experiment begin theory
enjoy concert start letter
Table 7: Metonymic Phrases for Groups 1 and 2
them were native speakers of English and non-
linguists. We divided them into 2 groups: 4 and 4.
Subjects in each group annotated three metonymic
phrases as shown in Table 7. They received writ-
ten guidelines, which were the only source of in-
formation on the experiment.
For each metonymic phrase they were presented
with a list of 30 possible interpretations produced
by the system. For each synset in the list they had
to decide whether it was a plausible interpretation
of the metonymic phrase in an imaginary context.
We evaluated interannotator agreement in terms
of Fleiss? kappa (Fleiss, 1971) and f-measure com-
puted pairwise and then averaged across the an-
notators. The agreement in group 1 was 0.76
(f-measure) and 0.56 (kappa); in group 2 0.68
(f-measure) and 0.51 (kappa). This yielded the
average agreement of 0.72 (f-measure) and 0.53
(kappa).
8 Linguistic Perspective on Logical
Metonymy
There has been debate in linguistics literature as
whether it is the noun or the verb in the metonymic
phrase that determines the interpretation. Some of
the accounts along with our own analysis are pre-
sented below.
8.1 The Effect of the Noun Complement
The interpretation of logical metonymy is often
explained by the lexical defaults associated with
the noun complement in the metonymic phrase.
Pustejovsky (1991) models these lexical defaults
in the form of the qualia structure of the noun. The
qualia structure of a noun specifies the following
aspects of its meaning:
? CONSTITUTIVE Role (the relation between
an object and its constituents)
? FORMAL Role (that which distinguishes the
object within a larger domain)
? TELIC Role (purpose and function of the ob-
ject)
? AGENTIVE Role (how the object came into
being)
For the problem of logical metonymy the telic and
agentive roles are of particular interest. For ex-
ample, the noun book would have read specified
as its telic role and write as its agentive role in
its qualia structure. Following Pustejovsky (1991;
1995) and others, we take this information from
the noun qualia to represent the default interpre-
tations of metonymic constructions. Nevertheless,
multiple telic and agentive roles can exist and be
valid interpretations, which is supported by the ev-
idence derived from the corpus (Verspoor, 1997).
Such lexical defaults operate with a lack of
pragmatic information. In some cases, however,
lexical defaults can be overridden by context.
Consider the following example taken from Las-
carides and Copestake (1995).
(6) My goat eats anything. He really enjoyed
your book.
Here it is clear that the goat enjoyed eating the
book and not reading the book, which is enforced
by the context. Thus, incorporating the context of
the metonymic phrase into the model would be an-
other interesting extension of our experiment.
8.2 The Effect of the Metonymic Verb
By analysing phrases from the dataset of Lap-
ata and Lascarides (2003) we found that different
metonymic verbs have different effect on the inter-
pretation of logical metonymy. In this section we
provide some criteria based on which one could
classify metonymic verbs:
? Control vs. raising. Consider the phrase ex-
pect poetry taken from the dataset of Lap-
ata and Lascarides. Expect is a typical ob-
ject raising verb and, therefore, the most ob-
vious interpretation of this phrase would be
expect someone to learn/recite poetry, rather
than expect to hear poetry or expect to learn
poetry, as suggested by the model of Lapata
7
and Lascarides. Their model does not take
into account raising syntactic frame and as
such its interpretation of raising metonymic
phrases will be based on the wrong kind
of corpus evidence. Our expectation, how-
ever, is that control verbs tend to form logical
metonymies more frequently. By analyzing
the lists of control and raising verbs compiled
by Boguraev and Briscoe (1987) we found
evidence supporting this claim. Only 20% of
raising verbs can form metonymic construc-
tions (e.g. expect, allow, command, request,
require etc.), while others can not (e.g. ap-
pear, seem, consider etc.). Due to both this
and the fact that we build on the approach of
Lapata and Lascarides (2003), we gave pref-
erence to control verbs to develop and test our
system.
? Activity vs. result. Some metonymic verbs
require the reconstructed event to be an ac-
tivity (e.g. begin writing the book), while oth-
ers require a result (e.g. attempt to reach the
peak). This distinction potentially allows to
rule out some incorrect interpretations, e.g. a
resultative find for enjoy book, as enjoy re-
quires an event of the type activity. Automat-
ing this would be an interesting route for ex-
tension of our experiment.
? Telic vs. agentive vs. other events. An-
other interesting observation we made cap-
tures the constraints that the metonymic verb
imposes on the reconstructed event in terms
of its function. While some metonymic verbs
require rather telic events (e.g., enjoy, want,
try), others have strong preference for agen-
tive (e.g., start). However, for some cate-
gories of verbs it is hard to define a partic-
ular type of the event they require (e.g., at-
tempt the peak should be interpreted as at-
tempt to reach the peak, which is neither telic
nor agentive).
9 Conclusions and Future Work
We presented a system producing disambiguated
interpretations of logical metonymy with respect
to word sense. Such representation is novel and
it is intuitive to humans, as demonstrated by the
human experiment. We also proposed a novel
scheme for estimating the likelihood of a WordNet
synset as a unit from a non-disambiguated corpus.
The obtained results demonstrate the effectiveness
of our approach to deriving metonymic interpreta-
tions.
Along with this we provided criteria for dis-
criminating between different metonymic verbs
with respect to their effect on the interpretation
of logical metonymy. Our empirical analysis has
shown that control verbs tend to form logical
metonymy more frequently than raising verbs, as
well as that the former comply with the model of
Lapata and Lascarides (2003), whereas the latter
form logical metonymies based on a different syn-
tactic frame. Incorporating such linguistic knowl-
edge into the model would be an interesting exten-
sion of this experiment.
One of the motivations of the proposed sense-
based representation is the fact that the interpreta-
tions of metonymic phrases tend to form coher-
ent semantic classes (Pustejovsky, 1991; Puste-
jovsky, 1995; Godard and Jayez, 1993). The au-
tomatic discovery of such classes would require
word sense disambiguation as an initial step. This
is due to the fact that it is verb senses that form the
classes rather than verb strings. Comparing the in-
terpretations obtained for the phrase finish video,
one can clearly distinguish between the meaning
pertaining to the creation of the video, e.g., film,
shoot, take, and those denoting using the video,
e.g., watch, view, see. Discovering such classes
using the existing verb clustering techniques is our
next experiment.
Using sense-based interpretations of logical
metonymy as opposed to ambiguous verbs could
benefit other NLP applications that rely on disam-
biguated text (e.g. for the tasks of information re-
trieval (Voorhees, 1998) and question answering
(Pasca and Harabagiu, 2001)).
Acknowledgements
I would like to thank Simone Teufel and Anna Ko-
rhonen for their valuable feedback on this project
and my anonymous reviewers whose comments
helped to improve the paper. I am also very grate-
ful to Cambridge Overseas Trust who made this
research possible by funding my studies.
8
References
S. Abney. 1996. Partial parsing via finite-state cas-
cades. In J. Carroll, editor, Workshop on Robust
Parsing, pages 8?15, Prague.
O. E. Andersen, J. Nioche, E. Briscoe, and J. Car-
roll. 2008. The BNC parsed with RASP4UIMA.
In Proceedings of the Sixth International Language
Resources and Evaluation Conference (LREC?08),
Marrakech, Morocco.
B. Boguraev and E. Briscoe. 1987. Large lexicons
for natural language processing: utilising the gram-
mar coding system of the Longman Dictionary of
Contemporary English. Computational Linguistics,
13(4):219?240.
E. Briscoe, A. Copestake, and B. Boguraev. 1990.
Enjoy the paper: lexical semantics via lexicology.
In Proceedings of the 13th International Conference
on Computational Linguistics (COLING-90), pages
42?47, Helsinki.
E. Briscoe, J. Carroll, and R. Watson. 2006. The sec-
ond release of the rasp system. In Proceedings of the
COLING/ACL on Interactive presentation sessions,
pages 77?80.
L. Burnard. 2007. Reference Guide for the British Na-
tional Corpus (XML Edition).
D. Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
D. Godard and J. Jayez. 1993. Towards a proper treat-
ment of coercion phenomena. In Sixth Conference
of the European Chapter of the ACL, pages 168?177,
Utrecht.
M. Lapata and A. Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Lin-
guistics, 29(2):261?315.
A. Lascarides and A. Copestake. 1995. The prag-
matics of word meaning. In Journal of Linguistics,
pages 387?414.
M. Pasca and S. Harabagiu. 2001. The informative
role of WordNet in open-domain question answer-
ing. In Proceedings of NAACL-01 Workshop on
WordNet and Other Lexical Resources, pages 138?
143, Pittsburgh, PA.
J. Preiss. 2006. Probabilistic word sense disambigua-
tion analysis and techniques for combining knowl-
edge sources. Technical report, Computer Labora-
tory, University of Cambridge.
J. Pustejovsky. 1991. The generative lexicon. Compu-
tational Linguistics, 17(4).
J. Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, MA.
M. Utiyama, M. Masaki, and I. Hitoshi. 2000. A sta-
tistical approach to the processing of metonymy. In
Proceedings of the 18th International Conference on
Computational Linguistics, Saarbrucken, Germany.
C. M. Verspoor. 1997. Conventionality-governed log-
ical metonymy. In Proceedings of the Second In-
ternational Workshop on Computational Semantics,
pages 300?312, Tilburg.
E. M. Voorhees. 1998. Using WordNet for text re-
trieval. In C. Fellbaum, editor, WordNet: An Elec-
tornic Lexical Database, pages 285?303. MIT Press.
9
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1002?1010,
Beijing, August 2010
Metaphor Identification Using Verb and Noun Clustering
Ekaterina Shutova, Lin Sun and Anna Korhonen
Computer Laboratory, University of Cambridge
es407,ls418,alk23@cam.ac.uk
Abstract
We present a novel approach to auto-
matic metaphor identification in unre-
stricted text. Starting from a small seed set
of manually annotated metaphorical ex-
pressions, the system is capable of har-
vesting a large number of metaphors of
similar syntactic structure from a corpus.
Our method is distinguished from previ-
ous work in that it does not employ any
hand-crafted knowledge, other than the
initial seed set, but, in contrast, captures
metaphoricity by means of verb and noun
clustering. Being the first to employ un-
supervised methods for metaphor identifi-
cation, our system operates with the pre-
cision of 0.79.
1 Introduction
Besides enriching our thought and communica-
tion with novel imagery, the phenomenon of
metaphor also plays a crucial structural role in our
use of language. Metaphors arise when one con-
cept is viewed in terms of the properties of the
other. Below are some examples of metaphor.
(1) How can I kill a process? (Martin, 1988)
(2) Inflation has eaten up all my savings. (Lakoff
and Johnson, 1980)
(3) He shot down all of my arguments. (Lakoff
and Johnson, 1980)
(4) And then my heart with pleasure fills,
And dances with the daffodils.1
In metaphorical expressions seemingly unrelated
features of one concept are associated with an-
other concept. In the computer science metaphor
1?I wandered lonely as a cloud?, William Wordsworth,
1804.
in (1) the computational process is viewed as
something alive and, therefore, its forced termi-
nation is associated with the act of killing. Lakoff
and Johnson (1980) explain metaphor as a system-
atic association, or a mapping, between two con-
cepts or conceptual domains: the source and the
target. The metaphor in (3) exemplifies a map-
ping of a concept of argument to that of war. The
argument, which is the target concept, is viewed
in terms of a battle (or a war), the source concept.
The existence of such a link allows us to talk about
arguments using the war terminology, thus giving
rise to a number of metaphors.
Characteristic to all areas of human activity
(from poetic to ordinary to scientific) and, thus,
to all types of discourse, metaphor becomes an
important problem for natural language process-
ing (NLP). In order to estimate the frequency of
the phenomenon, Shutova and Teufel (2010) con-
ducted a corpus study on a subset of the British
National Corpus (BNC) (Burnard, 2007) repre-
senting various genres. They manually anno-
tated metaphorical expressions in this data and
found that 241 out of 761 sentences contained a
metaphor, whereby in 164 phrases metaphoricity
was introduced by a verb. Due to such a high fre-
quency of their use, a system capable of recog-
nizing and interpreting metaphorical expressions
in unrestricted text would become an invaluable
component of any semantics-oriented NLP appli-
cation.
Automatic processing of metaphor can be
clearly divided into two subtasks: metaphor
identification (distinguishing between literal and
metaphorical language in text) and metaphor
interpretation (identifying the intended literal
meaning of a metaphorical expression). Both of
them have been repeatedly attempted in NLP.
To date the most influential account of
metaphor identification is that of Wilks (1978).
1002
According to Wilks, metaphors represent a viola-
tion of selectional restrictions in a given context.
Consider the following example.
(5) My car drinks gasoline. (Wilks, 1978)
The verb drink normally takes an animate subject
and a liquid object. Therefore, drink taking a car
as a subject is an anomaly, which may as well in-
dicate metaphorical use of drink.
This approach was automated by Fass (1991)
in his met* system. However, Fass himself in-
dicated a problem with the method: it detects
any kind of non-literalness or anomaly in lan-
guage (metaphors, metonymies and others), i.e.,
it overgenerates with respect to metaphor. The
techniques met* uses to differentiate between
those are mainly based on hand-coded knowledge,
which implies a number of limitations. In a sim-
ilar manner manually created knowledge in the
form of WordNet (Fellbaum, 1998) is employed
by the system of Krishnakumaran and Zhu (2007),
which essentially differentiates between highly
lexicalized metaphors included in WordNet, and
novel metaphorical senses.
Alternative approaches (Gedigan et al, 2006)
search for metaphors of a specific domain defined
a priori (e.g. MOTION metaphors) in a specific
type of discourse (e.g. Wall Street Journal). In
contrast, the scope of our experiments is the whole
of the British National Corpus (BNC) (Burnard,
2007) and the domain of the expressions we iden-
tify is unrestricted. However, our technique is also
distinguished from the systems of Fass (1991) and
Krishnakumaran and Zhu (2007) in that it does
not rely on any hand-crafted knowledge, but rather
captures metaphoricity in an unsupervised way by
means of verb and noun clustering.
The motivation behind the use of clustering
methods for metaphor identification task lies in
the nature of metaphorical reasoning based on as-
sociation. Compare, for example, the target con-
cepts of marriage and political regime. Having
quite distinct meanings, both of them are cogni-
tively mapped to the source domain of mecha-
nism, which shows itself in the following exam-
ples:
(6) Our relationship is not really working.
(7) Diana and Charles did not succeed in mend-
ing their marriage.
(8) The wheels of Stalin?s regime were well oiled
and already turning.
We expect that such relatedness of distinct tar-
get concepts should manifest itself in the exam-
ples of language use, i.e. target concepts that are
associated with the same source concept should
appear in similar lexico-syntactic environments.
Thus, clustering concepts using grammatical rela-
tions (GRs) and lexical features would allow us to
capture their relatedness by association and har-
vest a large number of metaphorical expressions
beyond our seed set. For example, the sentence
in (6) being part of the seed set should enable the
system to identify metaphors in both (7) and (8).
In summary, our system (1) starts from a seed
set of metaphorical expressions exemplifying a
range of source?target domain mappings; (2) per-
forms unsupervised noun clustering in order to
harvest various target concepts associated with the
same source domain; (3) by means of unsuper-
vised verb clustering creates a source domain verb
lexicon; (4) searches the BNC for metaphorical
expressions describing the target domain concepts
using the verbs from the source domain lexicon.
We tested our system starting with a collection
of metaphorical expressions representing verb-
subject and verb-object constructions, where the
verb is used metaphorically. We evaluated the pre-
cision of metaphor identification with the help of
human judges. In addition to this we compared
our system to a baseline built upon WordNet,
whereby we demonstrated that our method goes
far beyond synonymy and captures metaphors not
directly related to any of those seen in the seed set.
2 Experimental Data
2.1 Seed Phrases
We used the dataset of Shutova (2010) as a seed
set. Shutova (2010) annotated metaphorical ex-
pressions in a subset of the BNC sampling vari-
ous genres: literature, newspaper/journal articles,
essays on politics, international relations and his-
tory, radio broadcast (transcribed speech). The
dataset consists of 62 phrases that are single-word
1003
metaphors representing verb-subject and verb-
object relations, where a verb is used metaphor-
ically. The seed phrases include e.g. stir ex-
citement, reflect enthusiasm, accelerate change,
grasp theory, cast doubt, suppress memory, throw
remark (verb - direct object constructions) and
campaign surged, factor shaped [..], tension
mounted, ideology embraces, changes operated,
approach focuses, example illustrates (subject -
verb constructions).
2.2 Corpus
The search space for metaphor identification was
the British National Corpus (BNC) that was
parsed using the RASP parser of Briscoe et al
(2006). We used the grammatical relations out-
put of RASP for BNC created by Andersen et al
(2008). The system searched the corpus for the
source and target domain vocabulary within a par-
ticular grammatical relation (verb-object or verb-
subject).
3 Method
Starting from a small seed set of metaphorical ex-
pressions, the system implicitly captures the as-
sociations that underly their production and com-
prehension. It generalizes over these associations
by means of unsupervised verb and noun clus-
tering. The obtained clusters then represent po-
tential source and target concepts between which
metaphorical associations hold. The knowledge
of such associations is then used to annotate
metaphoricity in a large corpus.
3.1 Clustering Motivation
Abstract concepts that are associated with the
same source domain are often related to each
other on an intuitive and rather structural level,
but their meanings, however, are not necessarily
synonymous or even semantically close. The re-
sults of previous research on corpus-based lexi-
cal semantics suggest that the linguistic environ-
ment in which a lexical item occurs can shed light
on its meaning. A number of works have shown
that it is possible to automatically induce seman-
tic word classes from corpus data via clustering of
contextual cues (Pereira et al, 1993; Lin, 1998;
Schulte im Walde, 2006). The consensus is that
the lexical items exposing similar behavior in a
large body of text most likely have the same mean-
ing. However, the concepts of marriage and po-
litical regime, that are also observed in similar
lexico-syntactic environments, albeit having quite
distinct meanings are likewise assigned by such
methods to the same cluster. In contrast to con-
crete concepts, such as tea, water, coffee, beer,
drink, liquid, that are clustered together due to
meaning similarity, abstract concepts tend to be
clustered together by association with the same
source domain. It is the presence of this associ-
ation that explains the fact that they share com-
mon contexts. We exploit this idea for identifi-
cation of new target domains associated with the
same source domain. We then use unsupervised
verb clustering to collect source domain vocab-
ulary, which in turn allows us to harvest a large
number of new metaphorical expressions.
3.2 Verb and Noun Clustering
Since Levin (1993) published her classification,
there have been a number of attempts to automati-
cally classify verbs into semantic classes using su-
pervised and unsupervised approaches (Lin, 1998;
Brew and Schulte im Walde, 2002; Korhonen et
al., 2003; Schulte im Walde, 2006; Joanis et al,
2008; Sun and Korhonen, 2009). Similar methods
were also applied to acquisition of noun classes
from corpus data (Rooth et al, 1999; Pantel and
Lin, 2002; Bergsma et al, 2008).
We adopt a recent verb clustering approach of
Sun and Korhonen (2009), who used rich syntac-
tic and semantic features extracted using a shallow
parser and a clustering method suitable for the re-
sulting high dimensional feature space. When Sun
and Korhonen evaluated their approach on 204
verbs from 17 Levin classes, they obtained 80.4
F-measure (which is high in particular for an un-
supervised approach). We apply this approach to a
much larger set of 1610 verbs: all the verb forms
appearing in VerbNet (Kipper et al, 2006) with
the exception of highly infrequent ones. In addi-
tion, we adapt the approach to noun clustering.
3.2.1 Feature Extraction
Our verb dataset is a subset of VerbNet com-
piled as follows. For all the verbs in VerbNet we
1004
extracted their occurrences (up to 10,000) from
the raw corpus data collected originally by Korho-
nen et al (2006) for construction of VALEX lexi-
con. Only the verbs found in this data more than
150 times were included in the experiment.
For verb clustering, we adopted the best per-
forming features of Sun and Korhonen (2009):
automatically acquired verb subcategorization
frames (SCFs) parameterized by their selectional
preferences (SPs). We obtained these features us-
ing the SCF acquisition system of Preiss et al
(2007). The system tags and parses corpus data
using the RASP parser and extracts SCFs from the
resulting GRs using a rule-based classifier which
identifies 168 SCF types for English verbs. It pro-
duces a lexical entry for each verb and SCF com-
bination occurring in corpus data. We obtained
SPs by clustering argument heads appearing in the
subject and object slots of verbs in the resulting
lexicon.
Our noun dataset consists of 2000 most fre-
quent nouns in the BNC. Following previous
works on semantic noun classification (Pantel and
Lin, 2002; Bergsma et al, 2008), we used GRs as
features for noun clustering. We employed all the
argument heads and verb lemmas appearing in the
subject, direct object and indirect object relations
in the RASP-parsed BNC.
The feature vectors were first constructed from
the corpus counts, and subsequently normalized
by the sum of the feature values before applying
clustering.
3.2.2 Clustering Algorithm
We use spectral clustering (SPEC) for both
verbs and nouns. This technique has proved to be
effective in previous verb clustering works (Brew
and Schulte im Walde, 2002; Sun and Korhonen,
2009) and in related NLP tasks involving high di-
mensional data (Chen et al, 2006). We use the
MNCut algorithm for SPEC which has a wide ap-
plicability and a clear probabilistic interpretation
(Meila and Shi, 2001).
The task is to group a given set of words W =
{wn}Nn=1 into a disjoint partition of K classes.
SPEC takes a similarity matrix as input. We
construct it using the Jensen-Shannon divergence
(JSD) as a measure. The JSD between two feature
vectors w and w? is djsd(w, w?) = 12D(w||m) +1
2D(w?||m) where D is the Kullback-Leibler di-vergence, and m is the average of the w and w?.
The similarity matrix S is constructed where
Sij = exp(?djsd(w, w?)). In SPEC, the simi-
larities Sij are viewed as weights on the edges
ij of a graph G over W . The similarity matrix
S is thus the adjacency matrix for G. The de-
gree of a vertex i is di = ?Nj=1 Sij . A cut be-
tween two partitions A and A? is defined to be
Cut(A, A?) =?m?A,n?A? Smn.
The similarity matrix S is then transformed into
a stochastic matrix P .
P = D?1S (1)
The degree matrix D is a diagonal matrix where
Dii = di.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constants2 with respect to a partition I? and their
eigenvalues are not zero, then I? minimizes the
multiway normalized cut (MNCut):
MNCut(I) = K ??Kk=1 Cut(Ik,Ik)Cut(Ik,I)
Pmn can be interpreted as the transition probabil-
ity between the vertexes m, n. The criterion can
thus be expressed as MNCut(I) = ?Kk=1(1 ?
P (Ik ? Ik|Ik)) (Meila, 2001), which is the sum
of transition probabilities across different clusters.
This criterion finds the partition where random
walks are most likely to happen within the same
cluster. In practice, the leading eigenvectors of
P are not piecewise constants. However, we can
extract the partition by finding the approximately
equal elements in the eigenvectors using a cluster-
ing algorithm, such as K-Means.
Since SPEC has elements of randomness, we ran
the algorithm multiple times and the partition that
minimizes the distortion (the distances to cluster
centroid) is reported. Some of the clusters ob-
tained as a result of applying the algorithm to our
noun and verb datasets are demonstrated in Fig-
ures 1 and 2 respectively. The noun clusters rep-
resent target concepts that we expect to be asso-
ciated with the same source concept (some sug-
gested source concepts are given in Figure 1, al-
though the system only captures those implicitly).
2An eigenvector v is piecewise constant with respect to I
if v(i) = v(j)?i, j ? Ik and k ? 1, 2...K
1005
Source: MECHANISM
Target Cluster: consensus relation tradition partnership
resistance foundation alliance friendship contact reserve
unity link peace bond myth identity hierarchy relation-
ship connection balance marriage democracy defense
faith empire distinction coalition regime division
Source: STORY; JOURNEY
Target Cluster: politics practice trading reading occupa-
tion profession sport pursuit affair career thinking life
Source: LOCATION; CONTAINER
Target Cluster: lifetime quarter period century succes-
sion stage generation decade phase interval future
Source: LIVING BEING; END
Target Cluster: defeat fall death tragedy loss collapse de-
cline disaster destruction fate
Figure 1: Clustered target concepts
Source Cluster: sparkle glow widen flash flare gleam
darken narrow flicker shine blaze bulge
Source Cluster: gulp drain stir empty pour sip spill swal-
low drink pollute seep flow drip purify ooze pump bub-
ble splash ripple simmer boil tread
Source Cluster: polish clean scrape scrub soak
Source Cluster: kick hurl push fling throw pull drag haul
Source Cluster: rise fall shrink drop double fluctuate
dwindle decline plunge decrease soar tumble surge spiral
boom
Figure 2: Clustered verbs (source domains)
The verb clusters contain coherent lists of source
domain vocabulary.
3.3 Selectional Preference Strength Filter
Following Wilks (1978), we take metaphor to rep-
resent a violation of selectional restrictions. How-
ever, not all verbs have an equally strong capacity
to constrain their arguments, e.g. remember, ac-
cept, choose etc. are weak in that respect. We
suggest that for this reason not all the verbs would
be equally prone to metaphoricity, but only the
ones exhibiting strong selectional preferences. We
test this hypothesis experimentally and expect that
placing this criterion would enable us to filter out
a number of candidate expressions, that are less
likely to be used metaphorically.
We automatically acquired selectional pref-
erence distributions for Verb-Subject and
Verb-Object relations from the BNC parsed
by RASP. We first clustered 2000 most frequent
nouns in the BNC into 200 clusters using SPEC
as described in the previous section. The ob-
tained clusters formed our selectional preference
classes. We adopted the selectional preference
measure proposed by Resnik (1993) and success-
fully applied to a number of tasks in NLP includ-
ing word sense disambiguation (Resnik, 1997).
Resnik models selectional preference of a verb in
probabilistic terms as the difference between the
posterior distribution of noun classes in a partic-
ular relation with the verb and their prior distri-
bution in that syntactic position regardless of the
identity of the predicate. He quantifies this dif-
ference using the relative entropy (or Kullback-
Leibler distance), defining the selectional prefer-
ence strength (SPS) as follows.
SR(v) = D(P (c|v)||P (c)) =
?
c
P (c|v) log P (c|v)P (c) ,
(2)
where P (c) is the prior probability of the noun
class, P (c|v) is the posterior probability of the
noun class given the verb and R is the gram-
matical relation in question. SPS measures how
strongly the predicate constrains its arguments.
We use this measure to filter out the verbs with
weak selectional preferences. The optimal SPS
threshold was set experimentally on a small held-
out dataset and approximates to 1.32. We ex-
cluded expressions containing the verbs with pref-
erence strength below this threshold from the set
of candidate metaphors.
4 Evaluation and Discussion
In order to prove that our metaphor identification
method generalizes well over the seed set and goes
far beyond synonymy, we compared its output to
that of a baseline taking WordNet synsets to repre-
sent source and target domains. We evaluated the
quality of metaphor tagging in terms of precision
with the help of human judges.
4.1 Comparison against WordNet Baseline
The baseline system was implemented using syn-
onymy information from WordNet to expand on
the seed set. Assuming all the synonyms of the
verbs and nouns in seed expressions to represent
the source and target vocabularies respectively,
the system searches for phrases composed of lex-
ical items belonging to those vocabularies. For
example, given a seed expression stir excitement,
the baseline finds phrases such as arouse fervour,
1006
stimulate agitation, stir turmoil etc. However, it is
not able to generalize over the concepts to broad
semantic classes, e.g. it does not find other feel-
ings such as rage, fear, anger, pleasure etc., which
is necessary to fully characterize the target do-
main. The same deficiency of the baseline system
manifests itself in the source domain vocabulary:
the system has only the knowledge of direct syn-
onyms of stir, as opposed to other verbs charac-
teristic to the domain of liquids, e.g. pour, flow,
boil etc., successfully identified by means of clus-
tering.
To compare the coverage achieved by unsuper-
vised clustering to that of the baseline in quanti-
tative terms, we estimated the number of Word-
Net synsets, i.d. different word senses, in the
metaphorical expressions captured by the two sys-
tems. We found that the baseline system covers
only 13% of the data identified using clustering
and does not go beyond the concepts present in
the seed set. In contrast, most metaphors tagged
by our method are novel and represent a con-
siderably wider range of meanings, e.g. given
the seed metaphors stir excitement, throw remark,
cast doubt the system identifies previously unseen
expressions swallow anger, hurl comment, spark
enthusiasm etc. as metaphorical.
4.2 Comparison with Human Judgements
In order to access the quality of metaphor identifi-
cation by both systems we used the help of human
annotators. The annotators were presented with
a set of randomly sampled sentences containing
metaphorical expressions as annotated by the sys-
tem and by the baseline. They were asked to mark
the tagged expressions that were metaphorical in
their judgement as correct.
The annotators were encouraged to rely on their
own intuition of metaphor. However, we also pro-
vided some guidance in the form of the following
definition of metaphor3:
1. For each verb establish its meaning in con-
text and try to imagine a more basic meaning
of this verb on other contexts. Basic mean-
ings normally are: (1) more concrete; (2) re-
3taken from the annotation procedure of Shutova and
Teufel (2010) that is in turn partly based on the work of Prag-
glejaz Group (2007).
CKM 391 Time and time again he would stare at the
ground, hand on hip, if he thought he had received a bad
call, and then swallow his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when
he found the comms system down, but Tammuz was
nearly hysterical by this stage.
AMA 349 We will halt the reduction in NHS services
for long-term care and community health services which
support elderly and disabled patients at home.
ADK 634 Catch their interest and spark their enthu-
siasm so that they begin to see the product?s potential.
K2W 1771 The committee heard today that gangs regu-
larly hurled abusive comments at local people, making
an unacceptable level of noise and leaving litter behind
them.
Figure 3: Sentences tagged by the system
(metaphors in bold)
lated to bodily action; (3) more precise (as
opposed to vague); (4) historically older.
2. If you can establish the basic meaning that
is distinct from the meaning of the verb in
this context, the verb is likely to be used
metaphorically.
We had 5 volunteer annotators who were all na-
tive speakers of English and had no or sparse lin-
guistic knowledge. Their agreement on the task
was 0.63 in terms of ? (Siegel and Castellan,
1988), whereby the main source of disagreement
was the presence of highly lexicalized metaphors,
e.g. verbs such as adopt, convey, decline etc.
We then evaluated the system performance against
their judgements in terms of precision. Precision
measures the proportion of metaphorical expres-
sions that were tagged correctly among the ones
that were tagged. We considered the expressions
tagged as metaphorical by at least three annota-
tors to be correct. As a result our system identi-
fies metaphor with the precision of 0.79, whereas
the baseline only attains 0.44. Some examples of
sentences annotated by the system are shown in
Figure 3.
Such a striking discrepancy between the per-
formance levels of the clustering approach and
the baseline can be explained by the fact that a
large number of metaphorical senses are included
in WordNet. This means that in WordNet synsets
source domain verbs are mixed with more abstract
terms. For example, the metaphorical sense of
shape in shape opinion is part of the synset (de-
1007
termine, shape, mold, influence, regulate). This
results in the baseline system tagging literal ex-
pressions as metaphorical, erroneously assuming
that the verbs from the synset belong to the source
domain.
The main source of confusion in the output of
our clustering method was the conventionality of
some metaphorical expressions, e.g. hold views,
adopt traditions, tackle a problem. The system
is capable of tracing metaphorical etymology of
conventional phrases, but their senses are highly
lexicalized. This lexicalization is reflected in the
data and affects clustering in that conventional
metaphors are sometimes clustered together with
literally used terms, e.g. tackle a problem and re-
solve a problem, which may suggest that the lat-
ter are metaphorical. It should be noted, however,
that such errors are rare.
Since there is no large metaphor-annotated cor-
pus available, it was impossible for us to reli-
ably evaluate the recall of the system. How-
ever, the system identified a total number of 4456
metaphorical expressions in the BNC starting with
a seed set of only 62, which is a promising result.
5 Related Work
One of the first attempts to identify and inter-
pret metaphorical expressions in text automati-
cally is the approach of Fass (1991). Fass devel-
oped a system called met*, capable of discrimi-
nating between literalness, metonymy, metaphor
and anomaly. It does this in three stages. First,
literalness is distinguished from non-literalness
using selectional preference violation as an in-
dicator. In the case that non-literalness is de-
tected, the respective phrase is tested for be-
ing a metonymic relation using hand-coded pat-
terns (such as CONTAINER-for-CONTENT). If
the system fails to recognize metonymy, it pro-
ceeds to search the knowledge base for a rele-
vant analogy in order to discriminate metaphor-
ical relations from anomalous ones. E.g., the
sentence in (5) would be represented in this
framework as (car,drink,gasoline), which does
not satisfy the preference (animal,drink,liquid),
as car is not a hyponym of animal. met*
then searches its knowledge base for a triple
containing a hypernym of both the actual ar-
gument and the desired argument and finds
(thing,use,energy source), which represents the
metaphorical interpretation.
Birke and Sarkar (2006) present a sen-
tence clustering approach for non-literal lan-
guage recognition implemented in the TroFi sys-
tem (Trope Finder). This idea originates from
a similarity-based word sense disambiguation
method developed by Karov and Edelman (1998).
The method employs a set of seed sentences,
where the senses are annotated, computes simi-
larity between the sentence containing the word
to be disambiguated and all of the seed sentences
and selects the sense corresponding to the anno-
tation in the most similar seed sentences. Birke
and Sarkar (2006) adapt this algorithm to perform
a two-way classification: literal vs. non-literal,
and they do not clearly define the kinds of tropes
they aim to discover. They attain a performance
of 53.8% in terms of f-score.
The method of Gedigan et al (2006) discrimi-
nates between literal and metaphorical use. They
trained a maximum entropy classifier for this pur-
pose. They obtained their data by extracting the
lexical items whose frames are related to MO-
TION and CURE from FrameNet (Fillmore et al,
2003). Then they searched the PropBank Wall
Street Journal corpus (Kingsbury and Palmer,
2002) for sentences containing such lexical items
and annotated them with respect to metaphoric-
ity. They used PropBank annotation (arguments
and their semantic types) as features to train the
classifier and report an accuracy of 95.12%. This
result is, however, only a little higher than the per-
formance of the naive baseline assigning majority
class to all instances (92.90%). These numbers
can be explained by the fact that 92.00% of the
verbs of MOTION and CURE in the Wall Street
Journal corpus are used metaphorically, thus mak-
ing the dataset unbalanced with respect to the tar-
get categories and the task notably easier.
Both Birke and Sarkar (2006) and Gedigan et
al. (2006) focus only on metaphors expressed by
a verb. As opposed to that the approach of Kr-
ishnakumaran and Zhu (2007) deals with verbs,
nouns and adjectives as parts of speech. They
use hyponymy relation in WordNet and word bi-
gram counts to predict metaphors at the sentence
1008
level. Given an IS-A metaphor (e.g. The world is
a stage4) they verify if the two nouns involved are
in hyponymy relation in WordNet, and if this is
not the case then this sentence is tagged as con-
taining a metaphor. Along with this they con-
sider expressions containing a verb or an adjec-
tive used metaphorically (e.g. He planted good
ideas in their minds or He has a fertile imagi-
nation). Hereby they calculate bigram probabil-
ities of verb-noun and adjective-noun pairs (in-
cluding the hyponyms/hypernyms of the noun in
question). If the combination is not observed in
the data with sufficient frequency, the system tags
the sentence containing it as metaphorical. This
idea is a modification of the selectional prefer-
ence view of Wilks. However, by using bigram
counts over verb-noun pairs as opposed to verb-
object relations extracted from parsed text Kr-
ishnakumaran and Zhu (2007) loose a great deal
of information. The authors evaluated their sys-
tem on a set of example sentences compiled from
the Master Metaphor List (Lakoff et al, 1991),
whereby highly conventionalized metaphors (they
call them dead metaphors) are taken to be neg-
ative examples. Thus, they do not deal with lit-
eral examples as such: essentially, the distinc-
tion they are making is between the senses in-
cluded in WordNet, even if they are conventional
metaphors, and those not included in WordNet.
6 Conclusions and Future Directions
We presented a novel approach to metaphor iden-
tification in unrestricted text using unsupervised
methods. Starting from a limited set of metaphor-
ical seeds, the system is capable of capturing the
regularities behind their production and annotat-
ing a much greater number and wider range of
previously unseen metaphors in the BNC.
Our system is the first of its kind and it is capa-
ble of identifying metaphorical expressions with a
high precision (0.79). By comparing its coverage
to that of a WordNet baseline, we proved that our
method goes far beyond synonymy and general-
izes well over the source and target domains. Al-
though at this stage we tested our system on verb-
subject and verb-object metaphors only, we are
4William Shakespeare
convinced that the described identification tech-
niques can be similarly applied to a wider range
of syntactic constructions. Extending the system
to deal with more parts of speech and types of
phrases is part of our future work.
One possible limitation of our approach is that
it is seed-dependent, which makes the recall of the
system questionable. Thus, another important fu-
ture research avenue is the creation of a more di-
verse seed set. We expect that a set of expres-
sions representative of the whole variety of com-
mon metaphorical mappings, already described in
linguistics literature, would enable the system to
attain a very broad coverage of the corpus. Mas-
ter Metaphor List (Lakoff et al, 1991) and other
existing metaphor resources could be a sensible
starting point on a route to such a dataset.
Acknowledgments
We are very grateful to our anonymous reviewers
for their useful feedback on this work and the vol-
unteer annotators for their interest, time and help.
This research is funded by generosity of Cam-
bridge Overseas Trust (Katia Shutova), Dorothy
Hodgkin Postgraduate Award (Lin Sun) and the
Royal Society, UK (Anna Korhonen).
References
Andersen, O. E., J. Nioche, E. Briscoe, and J. Carroll.
2008. The BNC parsed with RASP4UIMA. In Pro-
ceedings of LREC 2008, Marrakech, Morocco.
Bergsma, S., D. Lin, and R. Goebel. 2008. Discrimi-
native learning of selectional preference from unla-
beled text. In Proceedings of the EMNLP.
Birke, J. and A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonlit-
eral language. In In Proceedings of EACL-06, pages
329?336.
Brew, C. and S. Schulte im Walde. 2002. Spectral
clustering for German verbs. In Proceedings of
EMNLP.
Briscoe, E., J. Carroll, and R. Watson. 2006. The sec-
ond release of the rasp system. In Proceedings of
the COLING/ACL on Interactive presentation ses-
sions, pages 77?80.
Burnard, L. 2007. Reference Guide for the British
National Corpus (XML Edition).
1009
Chen, J., D. Ji, C. Lim Tan, and Z. Niu. 2006. Un- Lin, D. 1998. Automatic retrieval and clustering of
supervised relation disambiguation using spectral similar words. In Proceedings of the 17th inter-
clustering. In Proceedings of COLING/ACL. national conference on Computational linguistics,
pages 768?774.Fass, D. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa- Martin, J. H. 1988. Representing regularities in the
tional Linguistics, 17(1):49?90. metaphoric lexicon. In Proceedings of the 12th con-
ference on Computational linguistics, pages 396?Fellbaum, C., editor. 1998. WordNet: An Electronic 401.Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition. Meila, M. and J. Shi. 2001. A random walks view of
spectral segmentation. In AISTATS.Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International Meila, M. 2001. The multicut lemma. Technical re-
Journal of Lexicography, 16(3):235?250. port, University of Washington.
Gedigan, M., J. Bryant, S. Narayanan, and B. Ciric. Pantel, P. and D. Lin. 2002. Discovering word
2006. Catching metaphors. In In Proceedings of the senses from text. In Proceedings of the eighth ACM
3rd Workshop on Scalable Natural Language Un- SIGKDD international conference on Knowledge
derstanding, pages 41?48, New York. discovery and data mining, pages 613?619. ACM.
Joanis, E., S. Stevenson, and D. James. 2008. A gen- Pereira, F., N. Tishby, and L. Lee. 1993. Distribu-
eral feature space for automatic verb classification. tional clustering of English words. In Proceedings
Natural Language Engineering, 14(3):337?367. of ACL-93, pages 183?190, Morristown, NJ, USA.
Karov, Y. and S. Edelman. 1998. Similarity-based Pragglejaz Group. 2007. MIP: A method for iden-
word sense disambiguation. Computational Lin- tifying metaphorically used words in discourse.
guistics, 24(1):41?59. Metaphor and Symbol, 22:1?39.
Kingsbury, P. and M. Palmer. 2002. From TreeBank Preiss, J., T. Briscoe, and A. Korhonen. 2007. A sys-
to PropBank. In Proceedings of LREC-2002, Gran tem for large-scale acquisition of verbal, nominal
Canaria, Canary Islands, Spain. and adjectival subcategorization frames from cor-
pora. In Proceedings of ACL-2007, volume 45, pageKipper, K., A. Korhonen, N. Ryant, and M. Palmer. 912.2006. Extensive classifications of English verbs.
In Proceedings of the 12th EURALEX International Resnik, P. 1993. Selection and Information: A Class-
Congress. based Approach to Lexical Relationships. Ph.D.
thesis, Philadelphia, PA, USA.Korhonen, A., Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame dis- Resnik, P. 1997. Selectional preference and sense dis-
tributions semantically. In Proceedings of ACL ambiguation. In ACL SIGLEX Workshop on Tag-
2003, Sapporo,Japan. ging Text with Lexical Semantics, Washington, D.C.
Korhonen, A., Y. Krymolowski, and T. Briscoe. 2006. Rooth, M., S. Riezler, D. Prescher, G. Carroll, and
A large subcategorization lexicon for natural lan- F. Beil. 1999. Inducing a semantically annotated
guage processing applications. In Proceedings of lexicon via EM-based clustering. In Proceedings of
LREC 2006. ACL 99, pages 104?111.
Krishnakumaran, S. and X. Zhu. 2007. Hunting elu- Schulte im Walde, S. 2006. Experiments on the au-
sive metaphors using lexical resources. In Proceed- tomatic induction of German semantic verb classes.
ings of the Workshop on Computational Approaches Computational Linguistics, 32(2):159?194.
to Figurative Language, pages 13?20, Rochester,
NY. Shutova, E. and S. Teufel. 2010. Metaphor corpusannotated for source - target domain mappings. In
Lakoff, G. and M. Johnson. 1980. Metaphors We Live Proceedings of LREC 2010, Malta.
By. University of Chicago Press, Chicago. Shutova, E. 2010. Automatic metaphor interpretation
Lakoff, G., J. Espenson, and A. Schwartz. 1991. The as a paraphrasing task. In Proceedings of NAACL
master metaphor list. Technical report, University 2010, Los Angeles, USA.
of California at Berkeley. Siegel, S. and N. J. Castellan. 1988. Nonparametric
Levin, B. 1993. English Verb Classes and Alterna- statistics for the behavioral sciences. McGraw-Hill
tions. University of Chicago Press, Chicago. Book Company, New York, USA.
1010
Statistical Metaphor Processing
Ekaterina Shutova?
University of Cambridge
Simone Teufel?
University of Cambridge
Anna Korhonen?
University of Cambridge
Metaphor is highly frequent in language, which makes its computational processing indis-
pensable for real-world NLP applications addressing semantic tasks. Previous approaches to
metaphor modeling rely on task-specific hand-coded knowledge and operate on a limited domain
or a subset of phenomena. We present the first integrated open-domain statistical model of
metaphor processing in unrestricted text. Our method first identifies metaphorical expressions
in running text and then paraphrases them with their literal paraphrases. Such a text-to-text
model of metaphor interpretation is compatible with other NLP applications that can benefit
from metaphor resolution. Our approach is minimally supervised, relies on the state-of-the-art
parsing and lexical acquisition technologies (distributional clustering and selectional preference
induction), and operates with a high accuracy.
1. Introduction
Our production and comprehension of language is a multi-layered computational
process. Humans carry out high-level semantic tasks effortlessly by subconsciously
using a vast inventory of complex linguistic devices, while simultaneously integrating
their background knowledge, to reason about reality. An ideal computational model
of language understanding would also be capable of performing such high-level se-
mantic tasks. With the rapid advances in statistical natural language processing (NLP)
and computational lexical semantics, increasingly complex semantic tasks can now
be addressed. Tasks that have received much attention so far include, for example,
word sense disambiguation (WSD), supervised and unsupervised lexical classification,
selectional preference induction, and semantic role labeling. In this article, we take a
step further and show that state-of-the-art statistical NLP and computational lexical
semantic techniques can be used to successfully model complexmeaning transfers, such
as metaphor.
? Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge CB3 0FD, UK.
E-mail: {Ekaterina.Shutova, Simone.Teufel, Anna.Korhonen}@cl.cam.ac.uk.
Submission received: 28 July 2011; revised submission received: 21 April 2012; accepted for publication:
31 May 2012.
doi:10.1162/COLI a 00124
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 2
Metaphors arise when one concept is viewed in terms of the properties of another.
Humans often use metaphor to describe abstract concepts through reference to more
concrete or physical experiences. Some examples of metaphor include the following.
(1) How can I kill a process? (Martin 1988)
(2) Hillary brushed aside the accusations.
(3) I investedmyself fully in this research.
(4) And then my heart with pleasure fills,
And danceswith the daffodils.
(?I wandered lonely as a cloud,? William Wordsworth, 1804)
Metaphorical expressions may take a great variety of forms, ranging from conventional
metaphors, which we produce and comprehend every day, for example, those in Exam-
ples (1)?(3), to poetic and novel ones, such as Example (4). In metaphorical expressions,
seemingly unrelated features of one concept are attributed to another concept. In Ex-
ample (1), a computational process is viewed as something alive and, therefore, its forced
termination is associated with the act of killing. In Example (2) Hillary is not literally
cleaning the space by sweeping accusations. Instead, the accusations lose their validity
in that situation, in other words Hillary rejects them. The verbs brush aside and reject both
entail the resulting disappearance of their object, which is the shared salient property
that makes it possible for this analogy to be lexically expressed as a metaphor.
Characteristic of all areas of human activity (from poetic to ordinary to scientific)
and thus of all types of discourse, metaphor becomes an important problem for NLP.
As Shutova and Teufel (2010) have shown in an empirical study, the use of conventional
metaphor is ubiquitous in natural language text (according to their data, on average
every third sentence in general-domain text contains a metaphorical expression). This
makes metaphor processing essential for automatic text understanding. For example,
an NLP application which is unaware that a ?leaked report? is a ?disclosed report?
and not, for example, a ?wet report,? would fail further semantic processing of the
piece of discourse in which this phrase appears. A system capable of recognizing and
interpreting metaphorical expressions in unrestricted text would become an invaluable
component of any real-world NLP application that needs to access semantics (e.g., infor-
mation retrieval [IR], machine translation [MT], question answering [QA], information
extraction [IE], and opinion mining).
So far, these applications have not used any metaphor processing techniques and
thus often fail to interpret metaphorical data correctly. Consider an example from MT.
Figure 1 shows metaphor translation from English into Russian by a state-of-the-art
statistical MT system (Google Translate1). For both sentences the MT system produces
literal translations of metaphorical terms in English, rather than their literal interpreta-
tions. This results in otherwise grammatical sentences being semantically infelicitous,
poorly formed, and barely understandable to a native speaker of Russian. The meaning
of stir in Figure 1 (1) and spill in Figure 1 (2) would normally be realized in Russian only
via their literal interpretation in the given context (provoke and tell), as shown under
CORRECT TRANSLATION in Figure 1. A metaphor processing component could help to
1 http://translate.google.com/.
302
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 1
Examples of metaphor translation.
avoid such errors. We conducted a pilot study of the importance of metaphor for MT,
by running an English-to-Russian MT system (Google Translate) on the sentences from
the data set of Shutova (2010) containing single-word verb metaphors. We found that
27 out of 62 sentences (44%) were translated incorrectly due to metaphoricity. Due to
the high frequency of metaphor in text according to corpus studies, such a high level of
error becomes important for MT.
Examples where metaphor understanding is crucial can also be found in opinion
mining, that is, detection of the speaker?s attitude to what is said and to the topic.
Consider the following sentences.
(5) a. Government loosened its strangle-hold on business. (Narayanan 1999)
b. Government deregulated business. (Narayanan 1999)
Both sentences describe the same fact. The use of the metaphor loosened strangle-hold in
Example (5a) suggests that the speaker opposes government control of economy, how-
ever, whereas Example (5b) does not imply this. One can infer the speaker?s negative
attitude via the presence of a negativeword strangle-hold. Ametaphor processing system
would establish the correct meaning of Example (5a) and thus discover the actual fact
towards which the speaker has a negative attitude.
Because metaphor understanding requires resolving non-literal meanings via ana-
logical comparisons, the development of a complete and computationally practical
account of this phenomenon is a challenging and complex task. Despite the impor-
tance of metaphor for NLP systems dealing with semantic interpretation, its automatic
processing has received little attention in contemporary NLP, and is far from being a
solved problem. The majority of computational approaches to metaphor still exploit
ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They
often rely on task-specific hand-coded knowledge (Martin 1990; Fass 1991; Narayanan
1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al 2007)
and reduce the task to reasoning about a limited domain or a subset of phenomena
(Gedigian et al 2006; Krishnakumaran and Zhu 2007). So far there has been no robust
statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein
andManning 2003; Briscoe, Carroll, andWatson 2006; Clark and Curran 2007), however,
as well as recent work on computational lexical semantics (Schulte im Walde 2006;
303
Computational Linguistics Volume 39, Number 2
Mitchell and Lapata 2008; Davidov, Reichart, and Rappoport 2009; Erk and McCarthy
2009; Sun and Korhonen 2009; Abend and Rappoport 2010; O? Se?aghdha 2010) open up
many avenues for the creation of such a system. This is the niche the presented work is
intending to fill.
1.1 What Is Metaphor?
Metaphor has traditionally been viewed as an artistic device that lends vividness and
distinction to an author?s style. This view was first challenged by Lakoff and Johnson
(1980), who claimed that it is a productive phenomenon that operates at the level of
mental processes. According to Lakoff and Johnson, metaphor is thus not merely a
property of language (i.e., a linguistic phenomenon), but rather a property of thought
(i.e., a cognitive phenomenon). This view was subsequently adopted and extended
by a multitude of approaches (Grady 1997; Narayanan 1997; Fauconnier and Turner
2002; Feldman 2006; Pinker 2007) and the term conceptual metaphor was coined to
describe it.
The view postulates that metaphor is not limited to similarity-based meaning ex-
tensions of individual words, but rather involves reconceptualization of a whole area
of experience in terms of another. Thus metaphor always involves two concepts or
conceptual domains: the target (also called the topic or tenor in the linguistics literature)
and the source (also called the vehicle). Consider Examples (6) and (7).
(6) He shot down all of my arguments. (Lakoff and Johnson 1980)
(7) He attacked every weak point in my argument. (Lakoff and Johnson 1980)
According to Lakoff and Johnson, a mapping of the concept of argument to that of war is
used in both Examples (6) and (7). The argument, which is the target concept, is viewed
in terms of a battle (or awar), the source concept. The existence of such a link allows us to
talk about arguments using war terminology, thus giving rise to a number of metaphors.
Conceptual metaphor, or source?target domain mapping, is thus a generalization over
a set of individual metaphorical expressions that covers multiple cases in which ways
of reasoning about the source domain systematically correspond to ways of reasoning
about the target.
Conceptual metaphor manifests itself in natural language in the form of linguistic
metaphor (or metaphorical expressions) in a variety of ways. The most common types
of linguistic metaphor are lexical metaphor (i.e., metaphor at the level of a single
word sense, as in the Examples (1)?(4)), multi-word metaphorical expressions (e.g.,
?whether we go on pilgrimagewith Raleigh or put out to seawith Tennyson?), or extended
metaphor, that spans over longer discourse fragments.
Lexical metaphor is by far the most frequent type. In the presence of a certain
conceptual metaphor individual words can be used in entirely novel contexts, which
results in the formation of new meanings. Consider the following example.
(8) How can we build a ?Knowledge economy? if research is handcuffed? (Barque
and Chaumartin 2009)
In this sentence the physical verb handcuff is used with an abstract object research
and its meaning adapts accordingly. Metaphor is a productive phenomenon (i.e., its
304
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
novel examples continue to emerge in language). A large number of metaphorical
expressions, however, become conventionalized (e.g., ?I cannot grasp his way of think-
ing?). Although metaphorical in nature, their meanings are deeply entrenched in every-
day use, and are thus cognitively treated as literal terms. Both novel and conventional
metaphors are important for text processing, hence our work is concerned with both
types. Fixed non-compositional idiomatic expressions (e.g., kick the bucket, rock the boat,
put a damper on), however, are left aside, because the mechanisms of their formation are
no longer productive in modern language and, as such, they are of little interest for the
design of a generalizable computational model of metaphor.
Extended metaphor refers to the use of metaphor at the discourse level. A famous
example of extended metaphor can be found in William Shakespeare?s play As You Like
It, where he first compares the world to a stage and then in the following discourse
describes its inhabitants as players. Extended metaphor often appears in literature
in the form of an allegory or a parable, whereby a whole story from one domain is
metaphorically transferred onto another in order to highlight certain attributes of the
subject or teach a moral lesson.
1.2 Computational Modeling of Metaphor
In this article we focus on lexical metaphor and the computational modeling thereof.
From an NLP viewpoint, not all metaphorical expressions are equally important. A
metaphorical expression is interesting for computational modeling if its metaphorical
sense is significantly distinct from its original literal sense and cannot be interpreted
directly (e.g., by existing word sense disambiguation techniques using a predefined
sense inventory). The identification of highly conventionalized metaphors (e.g., the
verb impress, whose meaning originally stems from printing) are not of interest for NLP
tasks, because their metaphorical senses have long been dominant in language and their
original literal senses may no longer be used. A number of conventionalizedmetaphors,
however, require explicit interpretation in order to be understood by computer (e.g.,
?cast doubt,? ?polish the thesis,? ?catch a disease?), as do all novel metaphors. Thus
we are concerned with both novel and conventional metaphors, but only consider the
cases whereby the literal and metaphorical senses of the word are in clear opposition in
common use in contemporary language.
Automatic processing of metaphor can be divided into two subtasks: metaphor
identification, or recognition (distinguishing between literal and metaphorical language
in text); and metaphor interpretation (identifying the intended literal meaning of a
metaphorical expression). An ideal metaphor processing system should address both
of these tasks and provide useful information to support semantic interpretation in
real-world NLP applications. In order to be directly applicable to other NLP systems,
it should satisfy the following criteria:
 Provide a representation of metaphor interpretation that can be easily
integrated with other NLP systems: This criterion places constraints
on how the metaphor processing task should be defined. The most
universally applicable metaphor interpretation would be in the text-to-text
form. This means that a metaphor processing system would take raw text
as input and provide a more literal text as output, in which metaphors
are interpreted.
305
Computational Linguistics Volume 39, Number 2
 Operate on unrestricted running text: In order to be useful for real-world
NLP the system needs to be capable of processing real-world data. Rather
than only dealing with individual carefully selected clear-cut examples,
the system should be fully implemented and tested on free naturally
occurring text.
 Be open-domain: The system needs to cover all domains, genres, and
topics. Thus it should not rely on any domain-specific information or focus
on individual types of instances (e.g., a hand-chosen limited set of
source-target domain mappings).
 Be unsupervised or minimally supervised: To be easily adaptable to new
domains, the system needs to be unsupervised or minimally supervised.
This means it should not use any task-specific (i.e., metaphor-specific)
hand-coded knowledge. The only acceptable exception might be a
multi-purpose general-domain lexicon that is already in existence and
does not need to be created in a costly manner, although it would be an
advantage if no such resource is required.
 Cover all syntactic constructions: To be robust, the system needs to be
able to deal with metaphors represented by all word classes and syntactic
constructions.
In this article, we address both the metaphor identification and interpretation
tasks, resulting in the first integrated domain-independent corpus-based computational
model of metaphor. The method is designed with the listed criteria in mind. It takes
unrestricted text as input and produces textual output. Metaphor identification and
interpretation modules, based on the algorithms of Shutova, Sun, and Korhonen (2010)
and Shutova (2010), are first evaluated independently, and then combined and evalu-
ated together as an integrated system. All components of the method are in principle
applicable to all part-of-speech classes and syntactic constructions. In the current ex-
periments, however, we tested the system only on single-word metaphors expressed by
a verb. Verbs are frequent in language and central to conceptual metaphor. Cameron
(2003) conducted a corpus study of the use of metaphor in educational discourse for all
parts of speech. She found that verbs account for around 50% of the data, the rest being
shared by nouns, adjectives, adverbs, copula constructions, and multi-word metaphors.
This suggests that verb metaphors provide a reliable testbed for both linguistic and
computational experiments. Restricting the scope to verbs is a methodological step
aimed at testing the main principles of the proposed approach in a well-defined setting.
We would, however, expect the presented methods to scale to other parts of speech
and to a wide range of syntactic constructions, because they rely on techniques from
computational lexical semantics that have been shown to be effective in modeling not
only verb meanings, but also those of nouns and adjectives.
As opposed to previous approaches that modeled metaphorical reasoning starting
from a hand-crafted description and applying it to explain the data, we aim to design
a statistical model that captures regular patterns of metaphoricity in a large corpus and
thus generalizes to unseen examples. Compared to labor-intensive manual efforts, this
approach is more robust and, being nearly unsupervised, cost-effective. In contrast to
previous statistical approaches, which addressed metaphors of a specific topic or did
not consider linguistic metaphor at all (e.g., Mason 2004), the proposed method covers
all metaphors in principle, can be applied to unrestricted text, and can be adapted to
different domains and genres.
306
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Our first experiment is concerned with the identification of metaphorical expres-
sions in unrestricted text. Starting from a small set of metaphorical expressions, the
system learns the analogies involved in their production in aminimally supervised way.
It generalizes over the exemplified analogies by means of verb and noun clustering
(i.e., the identification of groups of similar concepts). This generalization allows it to
recognize previously unseen metaphorical expressions in text. Consider the following
examples:
(9) All of this stirred an uncontrollable excitement in her.
(10) Time and time again he would stare at the ground, hand on hip, and then
swallow his anger and play tennis.
Having once seen the metaphor ?stir excitement? in Example (9) the metaphor identifi-
cation system successfully concludes that ?swallow anger? in Example (10) is also used
metaphorically.
The identified metaphors then need to be interpreted. Ideally, a metaphor interpre-
tation task should be aimed at producing a representation of metaphor understanding
that can be directly embedded into other NLP applications that could benefit frommeta-
phor resolution. We define metaphor interpretation as a paraphrasing task and build
a system that discovers literal meanings of metaphorical expressions in text and pro-
duces their literal paraphrases. For example, for metaphors in Examples (11a) and (12a)
the system produces the paraphrases in Examples (11b) and (12b), respectively.
(11) a. All of this stirred an uncontrollable excitement in her.
b. All of this provoked an uncontrollable excitement in her.
(12) a. a carelessly leaked report
b. a carelessly disclosed report
The paraphrases for metaphorical expressions are acquired in a data-driven manner
from a large corpus. Literal paraphrases are then identified using a selectional prefer-
ence model.
This article first surveys the relevant theoretical and computational work on meta-
phor, then describes the design of the identification and paraphrasingmodules and their
independent evaluation, and concludes with the evaluation of the integrated text-to-text
metaphor processing system. The evaluations were carried out with the aid of human
subjects. In the case of identification, the subjects were asked to judge whether a system-
annotated phrase is a metaphor. In case of paraphrasing, they had to decide whether
the system-produced paraphrase for the metaphorical expression is correct and literal
in the given context. In addition, we created a metaphor paraphrasing gold standard
by asking human subjects (not previously exposed to system output) to produce their
own literal paraphrases for metaphorical verbs. The system paraphrasing was then also
evaluated against this gold standard.
2. Theoretical and Computational Background
2.1 Metaphor and Polysemy
Theorists of metaphor distinguish between two kinds of metaphorical language: novel
(or poetic) metaphors (i.e., those that are imaginative), and conventionalized metaphors
307
Computational Linguistics Volume 39, Number 2
(i.e., those that are used as a part of an ordinary discourse). According to Nunberg
(1987), all metaphors emerge as novel, but over time they become part of general usage
and their rhetorical effect vanishes, resulting in conventionalized metaphors. Following
Orwell (1946), Nunberg calls such metaphors ?dead? and claims that they are not
psychologically distinct from literally used terms. The scheme described by Nunberg
demonstrates how metaphorical associations capture patterns governing polysemy,
namely, the capacity of aword to havemultiplemeanings. Over time some of the aspects
of the target domain are added to the meaning of a term in the source domain, resulting
in a (metaphorical) sense extension of this term. Copestake and Briscoe (1995) discuss
sense extension mainly based on metonymic examples and model the phenomenon
using lexical rules encoding metonymic patterns. They also suggest that similar mecha-
nisms can be used to account for metaphorical processes. According to Copestake and
Briscoe, the conceptual mappings encoded in the sense extension rules would define
the limits to the possible shifts in meaning.
General-domain lexical resources often include information about metaphorical
word senses, although unsystematically and without any accompanying semantic an-
notation. For example, WordNet2 (Fellbaum 1998) contains the comprehension sense
of grasp, defined as ?get the meaning of something,? and the reading sense of skim,
defined as ?read superficially.? A great deal of metaphorical senses are absent from
the current version of WordNet, however. A number of researchers have advocated the
necessity of systematic inclusion and mark-up of metaphorical senses in such general-
domain lexical resources (Alonge and Castelli 2003; Lo?nneker and Eilts 2004) and claim
that this would be beneficial for the computational modeling of metaphor. Metaphor
processing systems could then either use this knowledge or be evaluated against it.
Lo?nneker (2004) mapped the senses from EuroWordNet3 to the Hamburg Metaphor
Database (Lo?nneker 2004; Reining and Lo?nneker-Rodman 2007) containing examples
of metaphorical expressions in German and French. Currently no explicit information
about metaphor is integrated into WordNet for English, however.
Although consistent inclusion in WordNet is in principle possible for conventional
metaphorical senses, it is not viable for novel contextual sense alternations. Because
metaphor is a productive phenomenon, all possible cases of contextual meaning alter-
nations it results in cannot be described via simple sense enumeration (Pustejovsky
1995). Computational metaphor processing therefore cannot be approached using the
standard word sense disambiguation paradigm, whereby the contextual use of a word
is classified according to an existing sense inventory. The metaphor interpretation task
is inherently more complex and requires generation of new and often uncommon
meanings of the metaphorical term based on the context.
2.2 Theoretical Views on Metaphor
The following views on metaphor are prominent in linguistics and philosophy: the
comparison view (e.g., the Structure-Mapping Theory of Gentner [1983]), the interaction
view (Black 1962; Hesse 1966), the selectional restrictions violation view (Wilks 1975,
1978), and conceptual metaphor theory (CMT) (Lakoff and Johnson 1980). All of these
2 http://wordnet.princeton.edu/.
3 EuroWordNet is a multilingual database containing WordNets for several European languages (Dutch,
Italian, Spanish, German, French, Czech, and Estonian). The WordNets are structured in the same way
as the Princeton WordNet for English. URL: http://www.illc.uva.nl/EuroWordNet/.
308
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
approaches share the idea of an interconceptual mapping that underlies the production
of metaphorical expressions. Gentner?s Structure-Mapping Theory postulates that the
ground for metaphor lies in similar properties and relations shared by the two con-
cepts (the target and the source). Tourangeau and Sternberg (1982), however, criticize
this view by noting that ?everything has some feature or category that it shares with
everything else, but we cannot combine just any two things in metaphor? (Tourangeau
and Sternberg 1982, page 226). The interaction view focuses on the surprise and novelty
that metaphor introduces. Its proponents claim that the source concept (or domain) rep-
resents a template for seeing the target concept in an entirely new way. The conceptual
metaphor theory of Lakoff and Johnson (1980) takes this idea much further by stating
that metaphor operates at the level of thought rather than at the level of language, and
that it is based on a set of cognitive mappings between source and target domains. Thus
Lakoff and Johnson put the emphasis on the structural aspect of metaphor, rather than
its decorative function in language that dominated the preceding theories. The selec-
tional restrictions violation view of Wilks (1978) concerns manifestation of metaphor in
language. Wilks suggests that metaphor represents a violation of combinatory norms
in the linguistic context and that metaphorical expressions can be detected via such
violation.
2.2.1 Conceptual Metaphor Theory. Examples (6) and (7) provided a good illustration of
CMT. Lakoff and Johnson explain them via the conceptual metaphor ARGUMENT IS
WAR, which is systematically reflected in language in a variety of expressions.
(13) Your claims are indefensible. (Lakoff and Johnson 1980)
(14) I demolished his argument. (Lakoff and Johnson 1980)
(15) I?ve never won an argument with him. (Lakoff and Johnson 1980)
(16) You disagree? Okay, shoot! (Lakoff and Johnson 1980)
According to CMT, we conceptualize and structure arguments in terms of battle, which
systematically influences the way we talk about arguments within our culture. In
other words, the conceptual structure behind battle (i.e., that one can shoot, demolish,
devise a strategy, win, and so on), is metaphorically transferred onto the domain of
argument.
Manifestations of conceptual metaphor are ubiquitous in language and communi-
cation. Here are a few other examples of common metaphorical mappings.
 TIME IS MONEY (e.g., ?That flat tire costme an hour?)
 IDEAS ARE PHYSICAL OBJECTS (e.g., ?I cannot grasp his way of
thinking?)
 LINGUISTIC EXPRESSIONS ARE CONTAINERS (e.g., ?I would not
be able to put all my feelings intowords?)
 EMOTIONS ARE VEHICLES (e.g., ?[...] she was transportedwith
pleasure?)
 FEELINGS ARE LIQUIDS (e.g., ?[...] all of this stirred an unfathomable
excitement in her?)
309
Computational Linguistics Volume 39, Number 2
 LIFE IS A JOURNEY (e.g., ?He arrived at the end of his life with very little
emotional baggage?)
CMT produced a significant resonance in the fields of philosophy, linguistics, cogni-
tive science, and artificial intelligence, including NLP. It inspired novel research (Martin
1990, 1994; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman andNarayanan 2004;
Mason 2004; Martin 2006; Agerri et al 2007), but was also criticized for the lack of
consistency and empirical verification (Murphy 1996; Shalizi 2003; Pinker 2007). The
sole evidence with which Lakoff and Johnson (1980) supported their theory was a set of
carefully selected examples. Such examples, albeit clearly illustrating the main tenets of
the theory, are not representative. They cannot possibly capture the whole spectrum
of metaphorical expressions, and thus do not provide evidence that the theory can
adequately explain the majority of metaphors in real-world texts. Aiming to verify the
latter, Shutova and Teufel (2010) conducted a corpus-based analysis of conceptual meta-
phor in the data from the British National Corpus (BNC) (Burnard 2007). In their study
three independent participants annotated both linguistic metaphors and the underlying
source?target domain mappings. Their results show that although the annotators reach
some overall agreement on the annotation of interconceptual mappings, they experi-
enced a number of difficulties, one of which was the problem of finding the right level
of abstraction for the source and target domain categories. The difficulties in category
assignment for conceptual metaphor suggest that it is hard to consistently assign explicit
labels to source and target domains, even though the interconceptual associations exist
in some sense and are intuitive to humans.
2.2.2 Selectional Restrictions Violation View. Lakoff and Johnson do not discuss howmeta-
phors can be recognized in linguistic data. To date, the most influential account of this
issue is that ofWilks (1975, 1978). According toWilks, metaphors represent a violation of
selectional restrictions (or preferences) in a given context. Selectional restrictions are the
semantic constraints that a predicate places onto its arguments. Consider the following
example.
(17) a. My aunt always drinks her tea on the terrace.
b. My car drinks gasoline. (Wilks 1978)
The verb drink normally requires a grammatical subject of type ANIMATE and a gram-
matical object of type LIQUID, as in Example (17a). Therefore, drink taking a car as a
subject in (17b) is an anomaly, which, according to Wilks, indicates a metaphorical use
of drink.
Although Wilks?s idea inspired a number of computational experiments on meta-
phor recognition (Fass and Wilks 1983; Fass 1991; Krishnakumaran and Zhu 2007), it
is important to note that in practice this approach has a number of limitations. Firstly,
there are other kinds of non-literalness or anomaly in language that cause a violation of
semantic norm, such as metonymies. Thus the method would overgenerate. Secondly,
there are kinds of metaphor that do not represent a violation of selectional restrictions
(i.e., the approach may also undergenerate). This would happen, for example, when
highly conventionalized metaphorical word senses are more frequent than the original
literal senses. Due to their frequency, selectional preference distributions of such words
in real-world data would be skewed towards the metaphorical senses (e.g., capturemay
select for ideas rather than captives according to the data). As a result, no selectional
preferences violation can be detected in the use of such verbs. Another case where the
310
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
method does not apply is copula constructions, such as ?All the world?s a stage.? And
finally, the method does not take into account the fact that interpretation (of metaphor
as well as other linguistic phenomena) is always context-dependent. For example,
the phrase ?All men are animals? uttered by a biology professor or a feminist would
have entirely different interpretations, the latter clearly metaphorical, but without any
violation of selectional restrictions.
2.3 Computational Approaches to Metaphor
2.3.1 Automatic Metaphor Recognition. One of the first attempts to automatically identify
and interpret metaphorical expressions in text is the approach of Fass (1991). It origi-
nates in the idea of Wilks (1978) and utilizes hand-coded knowledge. Fass developed a
system called met*, which is capable of discriminating between literalness, metonymy,
metaphor, and anomaly. It does this in three stages. First, literalness is distinguished
from non-literalness using selectional preference violation as an indicator. In the case
that non-literalness is detected, the respective phrase is tested for being metonymic
using hand-coded patterns (such as CONTAINER-FOR-CONTENT). If the system fails
to recognize metonymy, it proceeds to search the knowledge base for a relevant analogy
in order to discriminate metaphorical relations from anomalous ones. For example, the
sentence in Example (17b) would be represented in this framework as (car,drink,gasoline),
which does not satisfy the preference (animal,drink,liquid), as car is not a hyponym of
animal. met* then searches its knowledge base for a triple containing a hypernym of
both the actual argument and the desired argument and finds (thing,use,energy source),
which represents the metaphorical interpretation.
Goatly (1997) identifies a set of linguistic cues, namely, lexical patterns indicating
the presence of a metaphorical expression in running text, such as metaphorically speak-
ing, utterly, completely, so to speak, and literally. This approach, however, is likely to find
only a small proportion ofmetaphorical expressions, as the vast majority of them appear
without any signaling context. We conducted a corpus study in order to investigate the
effectiveness of linguistic cues asmetaphor indicators. For each cue suggested by Goatly
(1997), we randomly sampled 50 sentences from the BNC containing it and manually
annotated them for metaphoricity. The results are presented in Table 1. The average
precision (i.e., the proportion of identified expressions that were metaphorical) of the
linguistic cue method according to these data is 0.40, which suggests that the set of
metaphors that this method generates contains a great deal of noise. Thus the cues are
unlikely to be sufficient for metaphor extraction on their own, but together with some
additional filters, they could contribute to a more complex system.
The work of Peters and Peters (2000) concentrates on detecting figurative language
in lexical resources. They mine WordNet (Fellbaum 1998) for examples of systematic
Table 1
Corpus statistics for linguistic cues.
Cue BNC frequency Sample size Metaphors Precision
?metaphorically speaking? 7 7 5 0.71
?literally? 1,936 50 13 0.26
?figurative? 125 50 9 0.18
?utterly? 1,251 50 16 0.32
?completely? 8,339 50 13 0.26
?so to speak? 353 49 35 0.71
311
Computational Linguistics Volume 39, Number 2
polysemy, which allows them to capture metonymic and metaphorical relations. Their
system searches for nodes that are relatively high in the WordNet hierarchy (i.e., are
relatively general) and that share a set of commonword forms among their descendants.
Peters and Peters found that such nodes often happen to be in a metonymic (e.g.,
publisher ? publication) or a metaphorical (e.g., theory ? supporting structure) relation.
The CorMet system (Mason 2004) is the first attempt at discovering source?
target domain mappings automatically. It does this by finding systematic variations in
domain-specific selectional preferences, which are inferred from texts on the Web. For
example, Mason collects texts from the LAB domain and the FINANCE domain, in both
of which pourwould be a characteristic verb. In the LAB domain pour has a strong selec-
tional preference for objects of type liquid, whereas in the FINANCE domain it selects for
money. From this Mason?s system infers the domain mapping FINANCE ? LAB and the
concept mappingMONEY IS LIQUID. He compares the output of his system against the
Master Metaphor List (MML; Lakoff, Espenson, and Schwartz 1991) and reports a per-
formance of 77% in terms of accuracy (i.e., proportion of correctly induced mappings).
Birke and Sarkar (2006) present a sentence clustering approach for non-literal lan-
guage recognition, implemented in the TroFi system (Trope Finder). The idea behind
their system originates from a similarity-based word sense disambiguation method
developed by Karov and Edelman (1998). The latter uses a set of seed sentences anno-
tated with respect to word sense. The system computes similarity between the sentence
containing the word to be disambiguated and all of the seed sentences and selects the
sense corresponding to the annotation in the most similar seed sentences. Birke and
Sarkar adapt this algorithm to perform a two-way classification (literal vs. non-literal),
not aiming to distinguish between specific kinds of tropes. An example for the verb
pour in their database is shown in Figure 2. They attain a performance of 0.54 in terms
of F-measure (van Rijsbergen 1979).
Themethod of Gedigian et al (2006) discriminates between literal andmetaphorical
use. The authors trained a maximum entropy classifier for this purpose. They col-
lected their data using FrameNet (Fillmore, Johnson, and Petruck 2003) and PropBank
(Kingsbury and Palmer 2002) annotations. FrameNet is a lexical resource for English
containing information on words? semantic and syntactic combinatory possibilities, or
valencies, in each of their senses. PropBank is a corpus annotated with verbal propo-
sitions and their arguments. Gedigian et al (2006) extracted the lexical items whose
frames are related to MOTION and CURE from FrameNet, then searched the PropBank
Wall Street Journal corpus (Kingsbury and Palmer 2002) for sentences containing such
lexical items and annotated them with respect to metaphoricity. For example, the verb
run in the sentence ?Texas Air has run into difficulty? was annotated as metaphorical,
and in ?I was doing the laundry and nearly broke my neck running upstairs to see?
as literal. Gedigian et al used PropBank annotation (arguments and their semantic
pour
*nonliteral cluster*
wsj04:7878 N As manufacturers get bigger, they are likely to pour more money into the battle for shelf
space, raising the ante for new players.
wsj25:3283 N Salsa and rap music pour out of the windows.
wsj06:300 U Investors hungering for safety and high yields are pouring record sums into single-
premium, interest-earning annuities.
*literal cluster*
wsj59:3286 L Custom demands that cognac be poured from a freshly opened bottle.
Figure 2
An example of the data of Birke and Sarkar (2006).
312
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
types) as features to train the classifier, and report an accuracy of 95.12%. This result is,
however, only 2.22 percentage points higher than the performance of the naive baseline
assigning majority class to all instances (92.90%). Such high performance of their system
can be explained by the fact that 92.90% of the verbs ofMOTION and CURE in their data
are used metaphorically, thus making the data set unbalanced with respect to target
categories and making the task easier.
Both Birke and Sarkar (2006) and Gedigian et al (2006) focus only on metaphors
expressed by a verb. The approach of Krishnakumaran and Zhu (2007) additionally
covers metaphors expressed by nouns and adjectives. Krishnakumaran and Zhu use
hyponymy relation in WordNet and word bigram counts to predict metaphors at a
sentence level. Given a metaphor in copula constructions, or an IS-A metaphor (e.g., the
famous quote by William Shakespeare ?All the world?s a stage?) they verify if the two
nouns involved are in hyponymy relation inWordNet, otherwise this sentence is tagged
as containing a metaphor. They also treat expressions containing a verb or an adjective
used metaphorically (e.g., ?He planted good ideas in their minds? or ?He has a fertile
imagination?). For those cases, they calculate bigram probabilities of verb?noun and
adjective?noun pairs (including the hyponyms/hypernyms of the noun in question). If
the combination is not observed in the data with sufficient frequency, the system tags
the sentence as metaphorical. This idea is a modification of the selectional preference
view of Wilks, although applied at the bigram level. Alternatively, one could extract
verb?object relations from parsed text. Compared to the latter, Krishnakumaran and
Zhu (2007) lose a great deal of information. The authors evaluated their system on a
set of example sentences compiled from the Master Metaphor List, whereby highly con-
ventionalized metaphors are taken to be negative examples. Thus they do not deal with
literal examples as such. Essentially, the distinction Krishnakumaran and Zhu are mak-
ing is between the senses included inWordNet, even if they are conventional metaphors
(e.g., ?capture an idea?), and those not included in WordNet (e.g., ?planted good ideas?).
2.3.2 Automatic Metaphor Interpretation. One of the first computational accounts of meta-
phor interpretation is that of Martin (1990). In his metaphor interpretation, denotation
and acquisition system (MIDAS), Martin models the hierarchical organization of con-
ventional metaphors. The main assumption underlying this approach is that more spe-
cific conventional metaphors (e.g., COMPUTATIONAL PROCESS viewed as a LIVING
BEING in ?How can I kill a process??) descend from more general ones (e.g., PROCESS
[general, as a sequence of events] is a LIVINGBEING). Given an example of ametaphor-
ical expression, MIDAS searches its database for a corresponding conceptual metaphor
that would explain the anomaly. If it does not find any, it abstracts from the example to
more general concepts and repeats the search. If a suitable general metaphor is found,
it creates a new mapping for its descendant, a more specific metaphor, based on this
example. This is also how novel conceptual metaphors are acquired by the system.
The metaphors are then organized into a resource called MetaBank (Martin 1994). The
knowledge is represented in MetaBank in the form of metaphor maps (Martin 1988)
containing detailed information about source-target concept mappings and empirically
derived examples. MIDAS has been integrated with Unix Consultant, a system that
answers users? questions about Unix. The system first tries to find a literal answer to the
question. If it is not able to, it calls MIDAS, which detects metaphorical expressions via
selectional preference violation and searches its database for a metaphor explaining the
anomaly in the question.
Another cohort of approaches aims to perform inference about entities and events
in the source and target domains for the purpose of metaphor interpretation. These
313
Computational Linguistics Volume 39, Number 2
include the KARMA system (Narayanan 1997, 1999; Feldman and Narayanan 2004)
and the ATT-Meta project (Barnden and Lee 2002; Agerri et al 2007). Within both
systems the authors developed a metaphor-based reasoning framework in accordance
with CMT. The reasoning process relies on manually coded knowledge about the world
and operates mainly in the source domain. The results are then projected onto the target
domain using the conceptual mapping representation. The ATT-Meta project concerns
metaphorical and metonymic description of mental states; and reasoning about mental
states is performed using first order logic. Their system, however, does not take natural
language sentences as input, but hand-coded logical expressions that are representa-
tions of small discourse fragments. KARMA in turn deals with a broad range of abstract
actions and events and takes parsed text as input.
Veale and Hao (2008) derive a ?fluid knowledge representation for metaphor inter-
pretation and generation? called Talking Points. Talking Points is a set of characteristics
of concepts belonging to source and target domains and related facts about the world
which are acquired automatically from WordNet and from the Web. Talking Points are
then organized in Slipnet, a framework that allows for a number of insertions, deletions,
and substitutions in definitions of such characteristics in order to establish a connection
between the target and the source concepts. This work builds on the idea of slippage in
knowledge representation for understanding analogies in abstract domains (Hofstadter
and Mitchell 1994; Hofstadter 1995). The following is an example demonstrating how
slippage operates to explain the metaphorMake-up is a Western burqa.
Make-up =>
? typically worn by women
? expected to be worn by women
?must be worn by women
?must be worn by Muslim women
Burqa <=
By doing insertions and substitutions, the system arrives from the definition ?typi-
cally worn by women? to that of ?must be worn byMuslimwomen.? Thus it establishes
a link between the concepts of make-up and burqa. Veale and Hao, however, did not
evaluate to what extent their system is able to interpret metaphorical expressions in
real-world text.
The next sections of the paper are devoted to our own experiments on metaphor
identification and interpretation.
3. Metaphor Identification Method and Experiments
The first task for metaphor processing within NLP is its identification in text. As dis-
cussed earlier, previous approaches to this problem either utilize hand-coded knowl-
edge (Fass 1991; Krishnakumaran and Zhu 2007) or reduce the task to searching for
metaphors of a specific domain defined a priori (e.g., MOTIONmetaphors) in a specific
type of discourse (e.g., the Wall Street Journal [Gedigian et al 2006]). In contrast, the
search space in our experiments is the entire BNC and the domain of the expressions
identified is unrestricted. In addition, the developed technique does not rely on any
hand-crafted lexical or world knowledge, but rather captures metaphoricity by means
of verb and noun clustering in a data-driven manner.
Themotivation behind the use of clusteringmethods for themetaphor identification
task lies in CMT. The patterns of conceptual metaphor (e.g., FEELINGS ARE LIQUIDS)
314
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
always operate on semantic classes, that is, groups of related concepts, defined by
Lakoff and Johnson as conceptual domains (FEELINGS include love, anger, hatred, etc.;
LIQUIDS include water, tea, petrol, beer, etc.). Thus modeling metaphorical mechanisms
in accordance with CMT would involve capturing such semantic classes automatically.
Previous research on corpus-based lexical semantics has shown that it is possible to
automatically induce semantic word classes from corpus data via clustering of contex-
tual cues (Pereira, Tishby, and Lee 1993; Lin 1998; Schulte im Walde 2006). The current
consensus is that the lexical items showing similar behavior in a large body of text most
likely have related meanings.
The second reason for the use of unsupervised and weakly supervised methods
is suggested by the results of corpus-based studies of conceptual metaphor. The anal-
ysis of conceptual mappings in unrestricted text, conducted by Shutova and Teufel
(2010), although confirming some aspects of CMT, uncovered a number of fundamental
difficulties. One of these is the choice of the level of abstraction and granularity of
categories (i.e., labels for source and target domains). This suggests that it is hard
to define a comprehensive inventory of labels for source and target domains. Thus a
computational model of metaphorical associations should not rely on explicit domain
labels. Unsupervised methods allow us to recover patterns in data without assigning
any explicit labels to concepts, and thus to model interconceptual mappings implicitly.
The method behind our metaphor identification system relies on distributional
clustering. Noun clustering, specifically, is central to the approach. It is traditionally
assumed that noun clusters produced using distributional clustering contain concepts
that are similar to each other. This is true only in part, however. There exist two types
of concepts: concrete, those concepts denoting physical entities or physical experiences
(e.g., chair, apple, house, rain) and abstract, those concepts that do not physically exist
at any particular time or place, but rather exist as a type of thing or as an idea (e.g.,
justice, love, democracy). It is the abstract concepts that tend to be described metaphori-
cally, rather than concrete concepts. Humans use metaphor attempting to gain a better
understanding of an abstract concept by comparing it to their physical experiences. As
a result, abstract concepts expose different distributional behavior in a corpus. This
in turn affects the application of clustering techniques and the obtained clusters for
concrete and abstract concepts would be structured differently. Consider the example in
Figure 3. The figure shows a cluster containing concrete concepts (on the right) that are
various kinds of mechanisms; a cluster containing verbs co-occurring with mechanisms
in the corpus (at the bottom); and a cluster containing abstract concepts (on the left)
that tend to co-occur with these verbs. Such abstract concepts, albeit having quite
distinct meanings (e.g., marriage and democracy), are observed in similar lexico-syntactic
environments. This is due to the fact that they are systematically used metaphorically
with the verbs from the domain of MECHANISM. Hence, they are automatically
assigned to the same cluster. The following examples illustrate this phenomenon in
textual data.
(18) Our relationship is not really working.
(19) Diana and Charles did not succeed in mending their marriage.
(20) The wheels of Stalin?s regime were well oiled and already turning.
Such a structure of the abstract clusters can be explained by the fact that relationships,
marriages, collaborations, and political systems are all cognitively mapped to the same
315
Computational Linguistics Volume 39, Number 2
Figure 3
Cluster of target concepts associated with MECHANISM.
source domain of MECHANISM. In contrast to concrete concepts, such as tea, water,
coffee, beer, drink, liquid, that are clustered together when they have similar meanings,
abstract concepts tend to be clustered together if they are associated with the same
source domain.We define this phenomenon as clustering by association and it becomes
central to the system design. The expectation is that clustering by association would
allow the harvesting of new target domains that are associated with the same source
domain, and thus identify new metaphors.
The metaphor identification system starts from a small set of seed metaphorical
expressions, that is, annotatedmetaphors (such as those in Examples (18) or (19)), which
serve as training data. Note that seed annotation only concerns linguistic metaphors;
metaphorical mappings are not annotated. The system then (1) creates source domains
describing these examples by means of verb clustering (such as the verb cluster in
Figure 3); (2) identifies new target domains associated with the same source domain by
means of noun clustering (see, e.g., ABSTRACT cluster in Figure 3), and (3) establishes a
link between the source and the target clusters based on the seed examples.
Thus the system captures metaphorical associations implicitly. It generalizes over
the associated domains by means of verb and noun clustering. The obtained clusters
then represent source and target concepts between which metaphorical associations
hold. The knowledge of such associations is then used to identify new metaphorical
expressions in a large corpus.
In addition to this, we build a selectional preference?based metaphor filter. This
idea stems from the view of Wilks (1978), but is, however, a modification of it. The
filter assumes that the verbs exhibiting weak selectional preferences, namely, verbs co-
occurring with any argument class in linguistic data (remember, influence, etc.) generally
have no or only weak potential for being a metaphor. It has been previously shown
that it is possible to quantify verb selectional preferences on the basis of corpus data,
using, for example, a measure defined by Resnik (1993). Once the candidate metaphors
are identified in the corpus using clustering methods, those displaying weak selectional
preferences can be filtered out.
Figures 4 and 5 depict the metaphor identification pipeline: first, the identifica-
tion of metaphorical associations and then that of metaphorical expressions in text. In
316
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 4
Learning metaphorical associations by means of verb and noun clustering and using the seed set.
summary, the system (1) starts from a seed set of metaphorical expressions exemplifying
a range of source?target domain mappings; (2) performs noun clustering in order to
harvest various target concepts associated with the same source domain; (3) creates a
source domain verb lexicon by means of verb clustering; (4) searches the corpus for
metaphorical expressions describing the target domain concepts using the verbs from
the source domain lexicon; and (5) filters out the candidates exposing weak selectional
preference strength as non-metaphorical.
Figure 5
Identification of new metaphorical expressions in text.
317
Computational Linguistics Volume 39, Number 2
3.1 Experimental Data
The identification system takes a list of seed phrases as input. Seed phrases contain
manually annotated linguistic metaphors. The system generalizes from these linguistic
metaphors to the respective conceptual metaphors by means of clustering. This gen-
eralization is then used to harvest a large number of new metaphorical expressions in
unseen text. Thus the data needed for the identification experiment consist of a seed set,
data sets of verbs and nouns that are subsequently clustered, and an evaluation corpus.
3.1.1 Metaphor Corpus and Seed Phrases. The data to test the identification module were
extracted from the metaphor corpus created by Shutova and Teufel (2010). Their corpus
is a subset of the BNC (Burnard 2007) and, as such, it provides a suitable platform
for testing the metaphor processing system on real-world general-domain expressions
in contemporary English. Our data set consists of verb?subject and verb?direct object
metaphorical expressions. In order to avoid extra noise, we enforced some additional
selection criteria. All phrases were included unless they fell in one of the following
categories:
 Phrases where the subject or object referent is unknown (e.g., containing
pronouns such as in ?in which they [changes] operated?) or represented
by a named entity (e.g., ?Then Hillary leapt into the conversation?).
These cases were excluded from the data set because their processing
would involve the use of additional modules for coreference resolution
and named entity recognition, which in turn may introduce additional
errors into the system.
 Phrases whose metaphorical meaning is realized solely in passive
constructions (e.g., ?sociologists have been inclined to [..]?). These cases
were excluded because for many such examples it was hard for humans
to produce a literal paraphrase realized in the form of the same syntactic
construction. Thus their paraphrasing was deemed to be an unfairly
hard task for the system.
 Multiword metaphors (e.g., ?whether we go on pilgrimage with Raleigh or
put out to seawith Tennyson?). The current system is designed to identify
and paraphrase single-word, lexical metaphors. In the future the system
needs to be modified to process multiword metaphorical expressions;
this is, however, outside the scope of the current experiments.
The resulting data set consists of 62 phrases that are different single-word metaphors
representing verb?subject and verb?direct object relations, where a verb is used meta-
phorically. The phrases include, for instance, ?stir excitement,? ?reflect enthusiasm,?
?grasp theory,? ?cast doubt,? ?suppressmemory,? ?throw remark? (verb?direct object con-
structions); and ?campaign surged,? ?factor shaped [...],? ?tension mounted,? ?ideology
embraces,? ?example illustrates? (subject?verb constructions). This data set was used as
a seed set in the identification experiments. The phrases in the data set were manually
annotated for grammatical relations.
3.1.2 Verb and Noun Data Sets. The noun data set used for clustering consists of the
2,000 most frequent nouns in the BNC. The 2,000 most frequent nouns cover most
common target categories and their linguistic realizations. BNC represents a suitable
318
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
source for such nouns because the corpus is balanced with respect to genre, style,
and theme.
The verb data set is a subset of VerbNet (Kipper et al 2006). VerbNet is the largest
resource for general-domain verbs organized into semantic classes as proposed by Levin
(1993). The data set includes all the verbs in VerbNet with the exception of highly
infrequent ones. The frequency of the verbs was estimated from the data collected by
Korhonen, Krymolowski, and Briscoe (2006) for the construction of the VALEX lexicon,
which to date is one of the largest automatically created verb resources. The verbs from
VerbNet that appear less than 150 times in this data were excluded. The resulting data
set consists of 1,610 general-domain verbs.
3.1.3 Evaluation Corpus. The evaluation data for metaphor identification was the BNC
parsed by the RASP parser (Briscoe, Carroll, and Watson 2006). We used the gram-
matical relation (GR) output of RASP for the BNC created by Andersen et al (2008).
The system searched the corpus for the source and target domain vocabulary within a
particular grammatical relation (verb?direct object or verb?subject).
3.2 Method
The main components of the method include (1) distributional clustering of verbs
and nouns, (2) search through the parsed corpus, and (3) selectional preference-based
filtering.
3.2.1 Verb and Noun Clustering Method. The metaphor identification system relies on
the clustering method of Sun and Korhonen (2009). They use a rich set of syntactic
and semantic features (GRs, verb subcategorization frames [SCFs], and selectional
preferences) and spectral clustering, a method particularly suitable for the resulting
high dimensional feature space. This algorithm has proved to be effective in previous
verb clustering experiments (Brew and Schulte im Walde 2002) and in other NLP tasks
involving high dimensional data (Chen et al 2006).
Spectral clustering partitions objects relying on their similarity matrix. Given a set
of data points, the similarity matrix records similarities between all pairs of points. The
system of Sun and Korhonen (2009) constructs similarity matrices using the Jensen-
Shannon divergence as a measure. Jensen-Shannon divergence between two feature
vectors wi and wj is defined as follows:
JSD(wi,wj) =
1
2
D(wi||m)+ 12D(wj||m) (1)
where D is the Kullback-Leibler divergence, and m is the average of the wi and wj.
Spectral clustering can be viewed in abstract terms as the partitioning of a graph
G over a set of words W. The weights on the edges of G are the similarities Sij. The
similarity matrix S thus represents the adjacency matrix for G. The clustering problem
is then defined as identifying the optimal partition, or cut, of the graph into clusters,
such that the intra-cluster weights are high and the inter-cluster weights are low. The
system of Sun and Korhonen (2009) uses the MNCut algorithm of Meila and Shi (2001)
for this purpose.
Sun and Korhonen (2009) evaluated their clustering approach on 204 verbs from
17 Levin classes and obtained an F-measure of 80.4, which is the state-of-the-art
319
Computational Linguistics Volume 39, Number 2
performance level. The metaphor identification system uses the method of Sun and
Korhonen to cluster both verbs and nouns (separately), however, significantly extending
its coverage to unrestricted general-domain data and applying the method to a con-
siderably larger data set of 1,610 verbs.
3.2.2 Feature Extraction and Clustering Experiments. For verb clustering, the best perform-
ing features from Sun and Korhonen (2009) were adopted. These include automatically
acquired verb SCFs parameterized by their selectional preferences. These features were
obtained using the SCF acquisition system of Preiss, Briscoe, and Korhonen (2007). The
system tags and parses corpus data using the RASP parser (Briscoe, Carroll, andWatson
2006) and extracts SCFs from the produced grammatical relations using a rule-based
classifier which identifies 168 SCF types for English verbs. It produces a lexical entry
for each verb and SCF combination occurring in corpus data. The selectional preference
classes were obtained by clustering nominal arguments appearing in the subject and
object slots of verbs in the resulting lexicon.
Following previous works on semantic noun classification (Pantel and Lin 2002;
Bergsma, Lin, and Goebel 2008), grammatical relations were used as features for noun
clustering. More specifically, the frequencies of nouns and verb lemmas appearing in
the subject, direct object, and indirect object relations in the RASP-parsed BNC were
included in the feature vectors. For example, the feature vector for bananawould contain
the following entries: {eat-dobj n1, fry-dobj n2, sell-dobj n3,..., eat with-iobj ni,
look at-iobj ni+1,..., rot-subj nk, grow-subj nk+1,...}.
We experimented with different clustering granularities, subjectively examined the
obtained clusters, and determined that the number of clusters set to 200 is the most
suitable setting for both nouns and verbs in our task. This was done by means of qual-
itative analysis of the clusters as representations of source and target domains?that is,
by judging how complete and homogeneous the verb clusters were as lists of potential
source domain vocabulary and howmany new target domains associated with the same
source domain were found correctly in the noun clusters. This analysis was performed
on a randomly selected set of 10 clusters taken from different granularity settings and
none of the seed expressions were used for it. Examples of such clusters are shown in
Figures 6 (nouns) and 7 (verbs), respectively. The noun clusters represent target concepts
associated with the same source concept (some suggested source concepts are given in
Figure 6, although the system only captures those implicitly). The verb clusters contain
lists of source domain vocabulary.
3.2.3 Corpus Search.Once the clusters have been obtained, the system proceeds to search
the corpus for source and target domain terms within verb?object (both direct and
indirect) and verb?subject relations. For each seed expression, a cluster is retrieved for
the verb to form the source concept, and a cluster is retrieved for the noun to form a list
of target concepts. The retrieved verb and noun clusters are then linked, and such links
represent metaphorical associations. The system then classifies grammatical relations in
the corpus as metaphorical if the lexical items in the grammatical relation appear in the
linked source (verb) and target (noun) clusters. This search is performed on the BNC
parsed by RASP. Consider the following example sentence extracted from the BNC (the
BNC text ID is given in brackets, followed by the hypothetical conceptual metaphor):
(21) Few would deny that in the nineteenth century change was greatly accelerated.
(ACA) ? CHANGE IS MOTION
320
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Source: MECHANISM
Target Cluster: consensus relation tradition partnership resistance foundation alliance friendship con-
tact reserve unity link peace bond myth identity hierarchy relationship connection balance marriage
democracy defense faith empire distinction coalition regime division
Source: PHYSICAL OBJECT; LIVING BEING; STRUCTURE
Target Cluster: view conception theory concept ideal belief doctrine logic hypothesis interpretation
proposition thesis assumption idea argument ideology conclusion principle notion philosophy
Source: STORY; JOURNEY
Target Cluster: politics practice trading reading occupation profession sport pursuit affair career think-
ing life
Source: LIQUID
Target Cluster: disappointment rage concern desire hostility excitement anxiety passion doubt panic
delight anger fear curiosity shock terror surprise pride happiness pain enthusiasm alarm hope memory
love satisfaction sympathy spirit frustration impulse instinct warmth beauty ambition thought guilt
emotion sensation horror feeling laughter suspicion pleasure
Source: LIVING BEING; END
Target Cluster: defeat fall death tragedy loss collapse decline disaster destruction fate
Figure 6
Clustered nouns (the associated source domain labels are suggested by the authors for clarity;
the system does not assign any labels, but models source and target domains implicitly).
Source Cluster: sparkle glow widen flash flare gleam darken narrow flicker shine blaze bulge
Source Cluster: gulp drain stir empty pour sip spill swallow drink pollute seep flow drip purify ooze
pump bubble splash ripple simmer boil tread
Source Cluster: polish clean scrape scrub soak
Source Cluster: kick hurl push fling throw pull drag haul
Source Cluster: rise fall shrink drop double fluctuate dwindle decline plunge decrease soar tumble
surge spiral boom
Source Cluster: initiate inhibit aid halt trace track speed obstruct impede accelerate slow stimulate
hinder block
Source Cluster: work escape fight head ride fly arrive travel come run go slip move
Figure 7
Clustered verbs.
The relevant GRs identified by the parser are presented in Figure 8. The relation between
the verb accelerate and its semantic object change in Example (21) is expressed in the
passive voice and is, therefore, tagged by RASP as an ncsubj GR. Because this GR con-
tains terminology from associated source (MOTION) and target (CHANGE) domains,
it is marked as metaphorical and so is the term accelerate, which belongs to the source
domain of MOTION.
3.2.4 Selectional Preference Strength Filter. In the previous step a set of candidate verb
metaphors and the associated grammatical relations were extracted from the BNC.
These now need to be filtered based on selectional preference strength. To do this, we
Figure 8
Grammatical relations output for metaphorical expressions.
321
Computational Linguistics Volume 39, Number 2
automatically acquire selectional preference distributions for verb?subject and verb?
direct object relations from the RASP-parsed BNC. The noun clusters obtained using
Sun and Korhonen?s method as described earlier form the selectional preference classes.
To quantify selectional preferences, we adopt the selectional preference strength (SPS)
measure of Resnik (1993). Resnik models selectional preferences of a verb in proba-
bilistic terms as the difference between the posterior distribution of noun classes in a
particular relation with the verb and their prior distribution in that syntactic position
irrespective of the identity of the verb. He quantifies this difference using the Kullback-
Leibler divergence and defines selectional preference strength as follows:
SR(v) = D(P(c|v)||P(c)) =
?
c
P(c|v) log
P(c|v)
P(c) (2)
where P(c) is the prior probability of the noun class, P(c|v) is the posterior probability
of the noun class given the verb, and R is the grammatical relation in question. In order
to quantify how well a particular argument class fits the verb, Resnik defines another
measure called selectional association:
AR(v, c) =
1
SR(v)
P(c|v) log
P(c|v)
P(c)
(3)
which stands for the contribution of a particular argument class to the overall selectional
preference strength of a verb.
The probabilities P(c|v) and P(c) were estimated from the corpus data as follows:
P(c|v) =
f (v, c)
?
k f (v, ck)
(4)
P(c) =
f (c)
?
k f (ck)
(5)
where f (v, c) is the number of times the predicate v co-occurs with the argument class c
in the relation R, and f (c) is the number of times the argument class occurs in the relation
R regardless of the identity of the predicate.
Thus for each verb, its SPS can be calculated for specific grammatical relations.
This measure was used to filter out the verbs with weak selectional preferences. The
expectation is that such verbs are unlikely to be used metaphorically. The optimal
selectional preference strength threshold was set experimentally for both verb?subject
and verb?object relations on a small held-out data set (via qualitative analysis of the
data). It approximates to 1.32. The system excludes expressions containing the verbs
with preference strength below this threshold from the set of candidate metaphors.
Examples of verbs with weak and strong direct object SPs are shown in Tables 2 and
3, respectively. Given the SPS threshold of 1.32, the filter discards 31% of candidate
expressions initially identified in the corpus.
3.3 Evaluation
In order to show that the described metaphor identification method generalizes well
over the seed set and that it operates beyond synonymy, its output was compared to
322
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 2
Verbs with weak direct object SPs.
SPS Verb
1.3175 undo
1.3160 bud
1.3143 deplore
1.3138 seal
1.3131 slide
1.3126 omit
1.3118 reject
1.3097 augment
1.3094 frustrate
1.3087 restrict
1.3082 employ
1.3081 highlight
1.3081 correspond
1.3056 dab
1.3053 assist
1.3043 neglect
...
Table 3
Verbs with strong direct object SPs.
SPS Verb SPS Verb
...
3.0810 aggravate 2.9434 coop
3.0692 dispose 2.9326 hobble
3.0536 rim 2.9285 paper
3.0504 deteriorate 2.9212 sip
3.0372 mourn ...
3.0365 tread 1.7889 schedule
3.0348 cadge 1.7867 cheat
3.0254 intersperse 1.7860 update
3.0225 activate 1.7840 belt
3.0085 predominate 1.7835 roar
3.0033 lope 1.7824 intensify
2.9957 bone 1.7811 read
2.9955 pummel 1.7805 unnerve
2.9868 disapprove 1.7776 arrive
2.9838 hoover 1.7775 publish
2.9824 beam 1.7775 reason
2.9807 amble 1.7774 bond
2.9760 diversify 1.7770 issue
2.9759 mantle 1.7760 verify
2.9730 pulverize 1.7734 vomit
2.9604 skim 1.7728 impose
2.9539 slam 1.7726 phone
2.9523 archive 1.7723 purify
2.9504 grease ...
323
Computational Linguistics Volume 39, Number 2
that of a baseline using WordNet. In the baseline system, WordNet synsets represent
source and target domains. The quality of metaphor identification for both the system
and the baseline was evaluated in terms of precision with the aid of human judges.
To compare the coverage of the system to that of the baseline in quantitative terms we
assessed how broadly they expand on the seed set. To do this, we estimated the number
of word senses captured by the two systems and the proportion of identified metaphors
that are not synonymous with any of those seen in the seed set, according to WordNet.
This type of evaluation assesses how well clustering methods are suited to identify new
metaphors not directly related to those in the seed set.
3.3.1 Comparison with WordNet Baseline. The baseline system was implemented using
synonymy information from WordNet to expand on the seed set. Source and target
domain vocabularies were thus represented as sets of synonyms of verbs and nouns in
seed expressions. The baseline system then searched the corpus for phrases composed
of lexical items belonging to those vocabularies. For example, given a seed expression
?stir excitement,? the baseline finds phrases such as ?arouse fervour, stimulate agitation,
stir turmoil,? and so forth. It is not able to generalize over the concepts to broad
semantic classes, however?for example, it does not find other FEELINGS such as
rage, fear, anger, pleasure. This, however, is necessary to fully characterize the target
domain. Similarly, in the source domain, the system only has access to direct synonyms
of stir, rather than to other verbs characteristic of the domain of LIQUIDS (pour, flow,
boil, etc.).
To compare the coverage achieved by the system using clustering to that of the
baseline in quantitative terms, we estimated the number of WordNet synsets, that
is, different word senses, in the metaphorical expressions captured by the two sys-
tems. We found that the baseline system covers only 13% of the data identified using
clustering. This is due to the fact that it does not reach beyond the concepts present
in the seed set. In contrast, most metaphors tagged by the clustering method (87%)
are non-synonymous to those in the seed set and some of them are novel. Together,
these metaphors represent a considerably wider range of meanings. Given the seed
metaphors ?stir excitement, throw remark, cast doubt,? the system identifies previously
unseen expressions ?swallow anger, hurl comment, spark enthusiasm,? and so on, as
metaphorical. Tables 4 and 5 show examples of how the system and the baseline expand
on the seed set, respectively. Full sentences containing metaphors annotated by the
system are shown in Figure 9. Twenty-one percent of the expressions identified by the
system do not have their correspondingmetaphorical senses included inWordNet, such
as ?spark enthusiasm?; the remaining 79% are, however, more common conventional
metaphors. Starting with a seed set of only 62 examples, the system expands signif-
icantly on the seed set and identifies a total of 4,456 metaphorical expressions in the
BNC. This suggests that the method has the potential to attain a broad coverage of the
corpus given a large and representative seed set.
3.3.2 Evaluation Against Human Judgments. In order to assess the quality of metaphor
identification by both systems, their output was assessed by human judgments. For
this purpose, we randomly sampled sentences containing metaphorical expressions as
annotated by the system and by the baseline and asked human annotators to decide
whether these were metaphorical or not.
Participants Five volunteers participated in the experiment. They were all native
speakers of English and had no formal training in linguistics.
324
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 4
Examples of seed set expansion by the system.
Seed phrase Harvested metaphors BNC frequency
reflect concern (V-O): reflect concern 78
reflect interest 74
reflect commitment 26
reflect preference 22
reflect wish 17
reflect determination 12
reflect intention 8
reflect willingness 4
reflect sympathy 3
reflect loyalty 2
disclose interest 10
disclose intention 3
disclose concern 2
disclose sympathy 1
disclose commitment 1
disguise interest 6
disguise intention 3
disguise determination 2
obscure interest 1
obscure determination 1
cast doubt (V-O): cast doubt 197
cast fear 3
cast suspicion 2
catch feeling 3
catch suspicion 2
catch enthusiasm 1
catch emotion 1
spark fear 10
spark enthusiasm 3
spark passion 1
spark feeling 1
campaign surged (S-V): campaign surged 1
charity boomed 1
effort decreased 1
expedition doubled 1
effort doubled 1
campaign shrank 1
campaign soared 1
drive spiraled 1
Materials The subjects were presented with a set of 78 randomly sampled sentences
annotated by the two systems. Fifty percent of the data set were the sentences annotated
by the identification system and the remaining 50%were annotated by the baseline; and
the sentences were randomized. The annotation was done electronically in Microsoft
Word. An example of annotated sentences is given in Figure 10.
Task and guidelines The subjects were asked to mark which of the expressions were
metaphorical in their judgment. The participants were encouraged to rely on their
own intuition of what a metaphor is in the annotation process. Additional guidance,
325
Computational Linguistics Volume 39, Number 2
Table 5
Examples of seed set expansion by the baseline.
Seed phrase Harvested metaphors BNC frequency
reflect concern (V-O): reflect concern 78
ponder business 1
ponder headache 1
reflect business 4
reflect care 2
reflect fear 19
reflect worry 3
cast doubt (V-O): cast doubt 197
cast question 11
couch question 1
drop question 2
frame question 21
purge doubt 2
put doubt 12
put question 151
range question 1
roll question 1
shed doubt 2
stray question 1
throw doubt 35
throw question 17
throw uncertainty 1
campaign surged (S-V): campaign surged 1
campaign soared 1
however, in the form of the following definition of metaphor (Pragglejaz Group 2007)
was also provided:
1. For each verb establish its meaning in context and try to imagine a more
basic meaning of this verb in other contexts. Basic meanings normally are:
(1) more concrete; (2) related to bodily action; (3) more precise (as opposed
to vague); (4) historically older.
2. If you can establish a basic meaning that is distinct from the meaning of
the verb in this context, the verb is likely to be used metaphorically.
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then swallow his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AMA 349Wewill halt the reduction in NHS services for long-term care and community health services
which support elderly and disabled patients at home.
ADK 634 Catch their interest and spark their enthusiasm so that they begin to see the product?s
potential.
K2W 1771 The committee heard today that gangs regularly hurled abusive comments at local people,
making an unacceptable level of noise and leaving litter behind them.
Figure 9
Sentences tagged by the system (metaphors in bold).
326
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 10
Evaluation of metaphor identification.
Interannotator agreement Reliability was measured at ? = 0.63 (n = 2,N = 78, k = 5).
The data suggest that the main source of disagreement between the annotators was the
presence of conventional metaphors (e.g., verbs such as adopt, convey, decline).
Results The system performance was then evaluated against the elicited judgments in
terms of precision. The system output was compared to the gold standard constructed
by merging the judgments, whereby the expressions tagged as metaphorical by at least
three annotators were considered to be correct. This resulted in P = 0.79, with the
baseline attaining P = 0.44. In addition, the system tagging was compared to that of
each annotator pairwise, yielding an average P = 0.74 for the system and P = 0.41 for
the baseline.
In order to compare system performance to the human ceiling, pairwise agreement
was additionally calculated in terms of precision between the majority gold standard
and each judge. This corresponds to an average of P = 0.94.
To show that the system performance is significantly different from that of the base-
line, we annotated additional 150 instances identified by both systems for correctness
and conducted a one-tailed t-test for independent samples. The difference is statistically
significant with t = 4.11 (df = 148, p < 0.0005).
3.4 Discussion
We have shown that the method leads to a considerable expansion on the seed set and
operates with high precision?namely, it produces high quality annotations, and iden-
tifies fully novel metaphorical expressions relying only on the knowledge of source?
target domain mappings that it learns automatically. By comparing its coverage to that
of a WordNet baseline, we showed that the method reaches beyond synonymy and
generalizes well over the source and target domains.
The observed discrepancy in precision between the clustering approach and the
baseline can be explained by the fact that a large number of metaphorical senses are
included in WordNet. This means that in WordNet synsets source domain verbs appear
together with more abstract terms. For instance, the metaphorical sense of shape in
the phrase ?shape opinion? is part of the synset ?(determine, shape, mold, influence,
regulate).? This results in the low precision of the baseline system, because it tags literal
expressions (e.g., influence opinion) as metaphorical, assuming that all verbs from the
synset belong to the source domain.
327
Computational Linguistics Volume 39, Number 2
To perform a more comprehensive error analysis, we examined a larger subset of
the metaphorical expressions identified by the system (200 sentences, equally covering
verb?subject and verb?object constructions). System precision against the additional
judgments by one of the authors was measured at 76% (48 instances were tagged
incorrectly according to the judgments). The classification of system errors by type is
presented in Table 6. Precision errors in the output of the system were also concentrated
around the problem of conventionality of some metaphorical verbs, such as those in
?hold views, adopt traditions, tackle a problem.? This conventionality is reflected in the
data in that such verbs are frequently used in their ?metaphorical? contexts. As a result,
they are clustered together with literally used terms. For instance, the verb tackle is
found in a cluster with solve, resolve, handle, confront, face, and so forth. This results in
the system tagging ?resolve a problem? as metaphorical if it has previously seen ?tackle
a problem.?
A number of system errors affecting its precision are also due to cases of general
polysemy and homonymy of both verbs and nouns. For example, the noun passage
can mean both ?the act of passing from one state or place to the next? and ?a section
of text; particularly a section of medium length,? as defined in WordNet. Sun and
Korhonen?s (2009) method performs hard clustering, that is, it does not distinguish
between different word senses. Hence the noun passage occurred in only one cluster,
containing concepts like thought, word, sentence, expression, reference, address, description,
and so on. This cluster models the ?textual? meaning of passage. As a result of sense
ambiguity within the cluster, given the seed phrase ?she blocked the thought,? the system
tags such expressions as ?block passage,? ?impede passage,? ?obstruct passage,? and
?speed passage? as metaphorical.
The errors that may cause low recall of the system are of a different nature. Whereas
noun clustering considerably expands the seed set by identifying new associated tar-
get concepts (e.g., given the seed metaphor ?sell soul? it identifies ?sell skin? and
?launch pulse? as metaphorical), the verb clusters sometimes miss a certain proportion
of source domain vocabulary. For instance, given the seed metaphor ?example illus-
trates,? the system identifies the following expressions: ?history illustrates,? ?episode
illustrates,? ?tale illustrates,? ?combination illustrates,? ?event illustrates,? and so forth. It
does not, however, capture obvious verb-based expansions, such as ?episode portrays,?
present in the BNC. This is one of the problems that could lead to a lower recall
of the system.
Nevertheless, in many cases the system benefits not only from dissimilar concepts
within the noun clusters used to detect new target domains, but also from dissim-
ilar concepts in the verb clusters. Verb clusters produced automatically relying on
Table 6
Common system errors by type.
Source of error Subject?Verb Verb?Object Totals
Metaphor conventionality 7 14 21
General polysemy 9 6 15
Verb clustering 4 5 9
Noun clustering 2 1 3
SP filter 0 0 0
Totals 22 26 48
328
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
contextual features may contain lexical items with distinct, or even opposite meanings
(e.g., throw and catch, take off and land). They tend to belong to the same semantic
domain, however (e.g., verbs of dealing with LIQUIDS, verbs describing a FIGHT) It is
the diversity of verb meanings within the domain cluster that allows the generalization
from a limited number of seed expressions to a broader spectrum of previously unseen
and novel metaphors, non-synonymous with those in the seed set.
The fact that the approach is seed-dependent is one of its possible limitations,
affecting the coverage of the system. Wide coverage is essential for the practical use
of the system. At this stage, however, it was impossible for us to reliably measure the
recall of the system, because there is no large corpus annotated for metaphor available.
In addition, because the current system was only tested with very few seeds (again,
due to the lack of metaphor-annotated data), we expect the current overall recall of the
system to be relatively low. In order to obtain a full coverage of the corpus, a large and
representative seed set is necessary. Although it is hard to capture the whole variety of
metaphorical language in a limited set of examples, it is possible to compile a seed set
representative of all common source?target domain mappings. The learning capabilities
of the system can then be used to expand on those to the whole range of conventional
and novel metaphorical mappings and expressions. In addition, because the precision
of the system was measured on the data set produced by expanding individual seed
expressions, we would expect the expansion of other, new seed expressions to yield a
comparable quality of annotations. Incorporating new seed expressions is thus likely to
result in increasing recall without a significant loss in precision.
The current system harvests a large and relatively clean set of metaphorical
expressions from the corpus. These annotations could provide a new platform for the
development and testing of other metaphor systems.
4. Metaphor Interpretation Method and Experiments
As is the case in metaphor identification, the majority of existing approaches to meta-
phor interpretation also rely on task-specific hand-coded knowledge (Martin 1990; Fass
1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004;
Agerri et al 2007) and produce interpretations in a non-textual format (Veale and Hao
2008). The ultimate objective of automatic metaphor processing, however, is a type
of interpretation that can be directly embedded into other systems to enhance their
performance. We thus define metaphor interpretation as a paraphrasing task and build
a system that automatically derives literal paraphrases for metaphorical expressions in
unrestricted text. Our method is also distinguished from previous work in that it does
not rely on any hand-crafted knowledge aboutmetaphor, but in contrast is corpus-based
and uses automatically induced selectional preferences.
The metaphor paraphrasing task can be divided into two subtasks: (1) generating
paraphrases, that is, other ways of expressing the same meaning in a given context,
and (2) discriminating between literal and metaphorical paraphrases. Consequently,
the proposed approach is theoretically grounded in two ideas underlying each of these
subtasks:
 The meaning of a word in context emerges through interaction with the
meaning of the words surrounding it. This assumption is widely accepted
in lexical semantics theory (Pustejovsky 1995; Hanks and Pustejovsky
2005) and has been exploited for lexical acquisition (Schulte im Walde
2006; Sun and Korhonen 2009). It suggests that the context itself imposes
329
Computational Linguistics Volume 39, Number 2
certain semantic restrictions on the words which can occur within it.
Given a large amount of linguistic data, it is possible to model these
semantic restrictions in probabilistic terms (Lapata 2001). This can be
done by deriving a ranking scheme for possible paraphrases that fit or
do not fit in a specific context based on word co-occurrence evidence.
This is how initial paraphrases are generated within the metaphor
paraphrasing module.
 Literalness can be detected via strong selectional preference. This idea
is a mirror-image of the selectional preference violation view of Wilks
(1978), who suggested that a violation of selectional preferences indicates
a metaphor. The key information that selectional preferences provide is
whether there is an association between the predicate and its potential
argument and how strong that association is. A literal paraphrase
would normally come from the target domain (e.g., ?understand the
explanation?) and be strongly associated with the target concept, whereas
a metaphorical paraphrase would belong to the source domain (e.g.,
?grasp the explanation?) and be associated with the concepts from this
source domain more strongly than with the target concept. Hence we
use a selectional preference model to measure the semantic fit of the
generated paraphrases into the given context as opposed to all other
contexts. The highest semantic fit then indicates the most literal
paraphrase.
Thus the context-based probabilistic model is used for paraphrase generation and
the selectional preference model for literalness detection. The key difference between
the two models is that the former favors the paraphrases that co-occur with the words
in the context more frequently than other paraphrases do, and the latter favors the
paraphrases that co-occur with the words from the context more frequently than with
any other lexical items in the corpus. This is the main intuition behind our approach.
The system thus incorporates the following components:
 a context-based probabilistic model that acquires paraphrases for
metaphorical expressions from a large corpus;
 a WordNet similarity component that filters out the irrelevant
paraphrases based on their similarity to the metaphorical term (similarity
is defined as sharing a common hypernym within three levels in the
WordNet hierarchy);
 a selectional preference model that discriminates literal paraphrases from
the metaphorical ones. It re-ranks the paraphrases, de-emphasizing the
metaphorical ones and emphasizing the literal ones.
In addition, the system disambiguates the sense of the paraphrases using the
WordNet inventory of senses. The context-based model together with the WordNet
filter constitute a metaphor paraphrasing baseline. By comparing the final system to
this baseline, we demonstrate that simple context-based substitution, even supplied by
extensive knowledge contained in lexical resources, is not sufficient for metaphor inter-
pretation and that a selectional preference model is needed to establish the literalness of
the paraphrases.
330
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
This section first provides an overview of paraphrasing and lexical substitution
and relates these tasks to the problem of metaphor interpretation. It then describes
the experimental data used to develop and test the paraphrasing system and the
method itself, and finally, concludes with the system evaluation and the presentation
of results.
4.1 Paraphrasing and Lexical Substitution
Paraphrasing can be viewed as a text-to-text generation problem, whereby a new piece
of text is produced conveying the same meaning as the original text. Paraphrasing can
be carried out at multiple levels (sentence-, phrase-, and word-levels), and may involve
both syntactic and lexical transformations. Paraphrasing by replacing individual words
in a sentence is known as lexical substitution (McCarthy 2002). Because, in this article,
we address the phenomenon of metaphor at a single-word level, our task is close in
nature to lexical substitution. The task of lexical substitution originates from word
sense disambiguation (WSD). The key difference between the two is that whereas WSD
makes use of a predefined sense-inventory to characterize the meaning of a word in
context, lexical substitution is aimed at automatic induction of meanings. Thus the goal
of lexical substitution is to generate the set of semantically valid substitutes for the
word. Consider the following sentences from Preiss, Coonce, and Baker (2009).
(22) His parents felt that he was a bright boy.
(23) Our sun is a bright star.
Bright in Example (22) can be replaced by the word intelligent. The same replacement
in the context of Example (23) will not produce an appropriate sentence. A lexical
substitution system needs to (1) find a set of candidate synonyms for the word and
(2) select the candidate that matches the context of the word best.
Both sentence- or phrase-level paraphrasing and lexical substitution find a wide
range of applications in NLP. These include summarization (Knight and Marcu 2000;
Zhou et al 2006), information extraction (Shinyama and Sekine 2003), machine trans-
lation (Kurohashi 2001; Callison-Burch, Koehn, and Osborne 2006), text simplification
(Carroll et al 1999), question answering (McKeown 1979; Lin and Pantel 2001) and
textual entailment (Sekine et al 2007). Consequently, there has been a plethora of
NLP approaches to paraphrasing (McKeown 1979; Meteer and Shaked 1988; Dras 1999;
Barzilay and McKeown 2001; Lin and Pantel 2001; Barzilay and Lee 2003; Bolshakov
and Gelbukh 2004; Quirk, Brockett, and Dolan 2004; Kauchak and Barzilay 2006; Zhao
et al 2009; Kok and Brockett 2010) and lexical substitution (McCarthy and Navigli 2007,
2009; Erk and Pado? 2009; Preiss, Coonce, and Baker 2009; Toral 2009; McCarthy, Keller,
and Navigli 2010).
Among paraphrasing methods one can distinguish (1) rule-based approaches,
which rely on a set of hand-crafted (McKeown 1979; Zong, Zhang, and Yamamoto 2001)
or automatically learned (Lin and Pantel 2001; Barzilay and Lee 2003; Zhao et al 2008)
paraphrasing patterns; (2) thesaurus-based approaches, which generate paraphrases
by substituting words in the sentence by their synonyms (Bolshakov and Gelbukh
2004; Kauchak and Barzilay 2006); (3) natural language generation?based approaches
(Kozlowski, McCoy, and Vijay-Shanker 2003; Power and Scott 2005), which transform
a sentence into its semantic representation and generate a new sentence from it; and
(4) SMT-based methods (Quirk, Brockett, and Dolan 2004), operating as monolingual
331
Computational Linguistics Volume 39, Number 2
MT. A number of approaches to lexical substitution rely on manually constructed
thesauri to find sets of candidate synonyms (McCarthy and Navigli 2007), whereas
others address the task in a fully unsupervised fashion. In order to derive and rank
candidate substitutes, the latter systems make use of distributional similarity measures
(Pucci et al 2009; McCarthy, Keller, and Navigli 2010), vector space models of word
meaning (De Cao and Basili 2009; Erk and Pado? 2009) or statistical learning techniques,
such as hidden Markov models and n-grams (Preiss, Coonce, and Baker 2009).
The metaphor interpretation task is different from the WSD task, because it is
impossible to predefine a set of senses of metaphorical words, in particular for novel
metaphors. Instead, the correct substitute for the metaphorical term needs to be gen-
erated in a data-driven manner, as for lexical substitution. The metaphor paraphrasing
task, however, also differs from lexical substitution in the following two ways. Firstly,
a suitable substitute needs to be used literally in the target context, or at least more
conventionally than the original word. Secondly, by definition, the substitution is not
required to be a synonym of the metaphorical word. Moreover, for our task this is not
even desired, because there is the danger that synonymous paraphrasing may result
in another metaphorical expression, rather than the literal interpretation of the original
one. Metaphor paraphrasing therefore presents an additional challenge in comparison
to lexical substitution, namely, that of discriminating between literal and metaphorical
substitutes. This second, harder, and not previously addressed task is the main focus
of the work presented in this section. The remainder of the section is devoted to the
description of the metaphor paraphrasing experiment.
4.2 Experimental Data
The paraphrasing system is first tested individually on a set of metaphorical expres-
sions extracted from a manually annotated metaphor corpus of Shutova and Teufel
(2010). This is the same data set as the one used for seeding the identification module
(see Section 3.1.1 for description). Because the paraphrasing evaluation described in
this section is conducted independently from the identification experiment, and no
part of the paraphrasing system relies on the output of the identification system and
vice versa, the use of the same data set does not give any unfair advantage to the
systems. In the later experiment (Section 5) when the identification and paraphrasing
system are evaluated jointly, again the same seed set will be used for identification;
paraphrasing, however, will be performed on the output of the identification system
(i.e., the new identified metaphors) and both the identified metaphors and their para-
phrases will be evaluated by human judges not used in the previous and the current
experiments.
4.3 Method
The system takes phrases containing annotated single-word metaphors as input; where
a verb is used metaphorically, its context is used literally. It generates a list of possible
paraphrases of the verb that can occur in the same context and ranks them according
to their likelihood, as derived from the corpus. It then identifies shared features of the
paraphrases and themetaphorical verb using theWordNet hierarchy and removes unre-
lated concepts. It then identifies the literal paraphrases among the remaining candidates
based on the verb?s automatically induced selectional preferences and the properties of
the context.
332
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
4.3.1 Context-based Paraphrase Ranking Model. Terms replacing the metaphorical verb v
will be called its interpretations i. We model the likelihood L of a particular paraphrase
as a joint probability of the following events: the interpretation i co-occurring with the
other lexical items from its context w1, ...,wN in syntactic relations r1, ..., rN, respectively.
Li = P(i, (w1, r1), (w2, r2), ..., (wN, rN )) (6)
where w1, ...,wN and r1, ..., rN represent the fixed context of the term used metaphori-
cally in the sentence. In the system output, the context w1, ...,wN will be preserved, and
the verb v will be replaced by the interpretation i.
We assume statistical independence between the relations of the terms in a phrase.
For instance, for a verb that stands in a relation with both a subject and an object, the
verb?subject and verb?direct object relations are considered to be independent events
within the model. The likelihood of an interpretation is then calculated as follows:
P(i, (w1, r1), (w2, r2), ..., (wN, rN )) = P(i) ? P((w1, r1)|i) ? ... ? P((wN, rN )|i) (7)
The probabilities can be calculated using maximum likelihood estimation
P(i) =
f (i)
?
k f (ik)
(8)
P(wn, rn|i) =
f (wn, rn, i)
f (i)
(9)
where f (i) is the frequency of the interpretation irrespective of its arguments,
?
k f (ik) is
the number of times its part of speech class is attested in the corpus, and f (wn, rn, i) is
the number of times the interpretation co-occurs with context word wn in relation rn. By
performing appropriate substitutions into Equation (7) one obtains
P(i, (w1, r1), (w2, r2), ..., (wN, rN )) =
f (i)
?
k f (ik)
?
f (w1, r1, i)
f (i)
? ... ?
f (wN, rN, i)
f (i)
=
?N
n=1 f (wn, rn, i)
( f (i))N?1 ?
?
k f (ik)
(10)
This model is then used to rank the possible replacements of the term used meta-
phorically in the fixed context according to the data. The parameters of the model were
estimated from the RASP-parsed BNC using the grammatical relations output created
by Andersen et al (2008).
4.3.2 WordNet Filter. The context-based model described in Section 4.3.1 overgenerates
and hence there is a need to further narrow down the results. It is acknowledged in the
linguistics community that metaphor is, to a great extent, based on similarity between
the concepts involved (Gentner et al 2001). We exploit this fact to refine paraphrasing.
After obtaining the initial list of possible substitutes for the metaphorical term, the
system filters out the terms whose meanings do not share any common properties
with that of the metaphorical term. Consider the computer science metaphor ?kill a
process,? which stands for ?terminate a process.? The basic sense of kill implies an end
333
Computational Linguistics Volume 39, Number 2
Table 7
The list of paraphrases with the initial ranking (correct paraphrases are underlined).
Log-likelihood Replacement
Verb?DirectObject
hold back truth:
?13.09 contain
?14.15 conceal
?14.62 suppress
?15.13 hold
?16.23 keep
?16.24 defend
stir excitement:
?14.28 create
?14.84 provoke
?15.53 make
?15.53 elicit
?15.53 arouse
?16.23 stimulate
?16.23 raise
?16.23 excite
?16.23 conjure
leak report:
?11.78 reveal
?12.59 issue
?13.18 disclose
?13.28 emerge
?14.84 expose
?16.23 discover
Subject?Verb
campaign surge:
?13.01 run
?15.53 improve
?16.23 soar
?16.23 lift
or termination of life. Thus termination is the shared element of the metaphorical verb
and its literal interpretation.
Such an overlap of properties can be identified using the hyponymy relations in the
WordNet taxonomy. Within the initial list of paraphrases, the system selects the terms
that are hypernyms of the metaphorical term, or share a common hypernym with it. To
maximize the accuracy, we restrict the hypernym search to a depth of three levels in the
taxomomy. Table 7 shows the filtered lists of paraphrases for some of the test phrases,
together with their log-likelihood. Selecting the highest ranked paraphrase from this list
as a literal interpretation will serve as a baseline.
4.3.3 Re-ranking Based on Selectional Preferences. The lists which were generated contain
some irrelevant paraphrases (e.g., ?contain the truth? for ?hold back the truth?) and
some paraphrases where the substitute itself is metaphorically used (e.g., ?suppress the
334
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
truth?). As the task is to identify the literal interpretation, however, the system should
remove these.
One way of dealing with both problems simultaneously is to use selectional prefer-
ences of the verbs. Verbs used metaphorically are likely to demonstrate semantic pref-
erence for the source domain, e.g., suppress would select for MOVEMENTS (political)
rather than IDEAS, or TRUTH (the target domain), whereas the ones used literally for
the target domain (e.g., conceal) would select for TRUTH. Selecting the verbs whose
preferences the noun in the metaphorical expression matches best should allow filtering
out non-literalness, as well as unrelated terms.
We automatically acquired selectional preference distributions of the verbs in the
paraphrase lists (for verb?subject and verb?direct object relations) from the RASP-
parsed BNC. As in the identification experiment, we derived selectional preference
classes by clustering the 2,000 most frequent nouns in the BNC into 200 clusters us-
ing Sun and Korhonen?s (2009) algorithm. In order to quantify how well a particular
argument class fits the verb, we adopted the selectional association measure proposed
by Resnik (1993), identical to the one we used within the selectional preference-based
filter for metaphor identification, as described in Section 3.2.4. To remind the reader,
selectional association is defined as follows:
AR(v, c) =
1
SR(v)
P(c|v) log
P(c|v)
P(c)
(11)
where P(c) is the prior probability of the noun class, P(c|v) is the posterior probability
of the noun class given the verb, and SR is the overall selectional preference strength of
the verb in the grammatical relation R.
We use selectional association as a measure of semantic fitness (i.e., literalness) of
the paraphrases. The paraphrases are re-ranked based on their selectional association
with the noun in the context. Those paraphrases that are not well suited or used meta-
phorically are dispreferred within this ranking. The new ranking is shown in Table 8.
The expectation is that the paraphrase in the first rank (i.e., the verb with which the
noun in the context has the highest association) represents a literal interpretation.
4.4 Evaluation and Discussion
As in the case of identification, the paraphrasing system was tested on verb?subject and
verb?direct object metaphorical expressions. These were extracted from the manually
annotated metaphor corpus of Shutova and Teufel (2010), as described in Section 3.1.1.
We compared the output of the final selectional-preference based system to that of the
WordNet filter acting as a baseline. We evaluated the quality of paraphrasing with the
help of human judges in two different experimental settings. The first setting involved
direct judgments of system output by humans. In the second setting, the subjects did
not have access to system output and had to provide their own literal paraphrases for
the metaphorical expressions in the data set. The system was then evaluated against
human judgments in Setting 1 and a paraphrasing gold standard created by merging
annotations in Setting 2.
4.4.1 Setting 1: Direct Judgment of System Output. The subjects were presented with a
set of sentences containing metaphorical expressions and the top-ranked paraphrases
produced by the system and by the baseline, randomized. They were asked to mark as
335
Computational Linguistics Volume 39, Number 2
Table 8
Paraphrases re-ranked by SP model (correct paraphrases are underlined).
Association Replacement
Verb?DirectObject
hold back truth:
0.1161 conceal
0.0214 keep
0.0070 suppress
0.0022 contain
0.0018 defend
0.0006 hold
stir excitement:
0.0696 provoke
0.0245 elicit
0.0194 arouse
0.0061 conjure
0.0028 create
0.0001 stimulate
? 0 raise
? 0 make
? 0 excite
leak report:
0.1492 disclose
0.1463 discover
0.0674 reveal
0.0597 issue
? 0 emerge
? 0 expose
Subject?Verb
campaign surge:
0.0086 improve
0.0009 run
? 0 soar
? 0 lift
correct the paraphrases that have the same meaning as the term used metaphorically if
they are used literally in the given context.
Subjects Seven volunteers participated in the experiment. They were all native
speakers of English (one bilingual) and had little or no linguistics expertise.
Interannotator agreement The reliability was measured at ? = 0.62 (n = 2,
N = 95, k = 7).
System evaluation against judgments We then evaluated the system performance
against the subjects? judgments in terms of Precision at Rank 1, P(1). Precision at Rank
(1) measures the proportion of correct literal interpretations among the paraphrases
in rank 1. The results are shown in Table 9. The system identifies literal paraphrases
with a P(1) = 0.81 and the baseline with a P(1) = 0.55. We then conducted a one-tailed
Sign test (Siegel and Castellan 1988) that showed that this difference in performance is
statistically significant (N = 15, x = 1, p < 0.001).
336
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 9
System and baseline P(1) and MAP.
Relation System P(1) Baseline P(1) System MAP Baseline MAP
Verb?DirectObject 0.79 0.52 0.60 0.54
Verb?Subject 0.83 0.57 0.66 0.57
Average 0.81 0.55 0.62 0.56
4.4.2 Setting 2: Creation of a Paraphrasing Gold Standard. The subjects were presented with
a set of sentences containing metaphorical expressions and asked to write down all suit-
able literal paraphrases for the highlighted metaphorical verbs that they could think of.
Subjects Five volunteer subjects who were different from the ones used in the pre-
vious setting participated in this experiment. They were all native speakers of English
and some of them had a linguistics background (postgraduate-level degree in English).
Gold Standard The elicited paraphrases combined together can be interpreted
as a gold standard. For instance, the gold standard for the phrase ?brushed aside the
accusations? consists of the verbs rejected, ignored, disregarded, dismissed, overlooked, and
discarded.
System evaluation by gold standard comparison The system output was com-
pared against the gold standard using mean average precision (MAP) as a measure.
MAP is defined as follows:
MAP = 1
M
M
?
j=1
1
Nj
Nj
?
i=1
Pji (12)
where M is the number of metaphorical expressions, Nj is the number of correct para-
phrases for the metaphorical expression j, Pji is the precision at each correct paraphrase
(the number of correct paraphrases among the top i ranks). First, average precision
is estimated for individual metaphorical expressions, and then the mean is computed
across the data set. This measure allows one to assess ranking quality beyond rank 1,
as well as the recall of the system. As compared with the gold standard, MAP of the
paraphrasing system is 0.62 and that of the baseline is 0.56, as shown in Table 9.
4.4.3 Discussion. Given that the metaphor paraphrasing task is open-ended, any gold
standard elicited on the basis of it cannot be exhaustive. Some of the correct paraphrases
may not occur to subjects during the experiment. As an example, for the phrase ?stir
excitement? most subjects suggested only one paraphrase ?create excitement,? which is
found in rank 3, suggesting an average precision of 0.33 for this phrase. The top ranks of
the system output are occupied by provoke and stimulate, however, which are intuitively
correct, more precise paraphrases, despite none of the subjects having thought of them.
Such examples contribute to the fact that the system?s MAP is significantly lower than
its precision at rank 1, because a number of correct paraphrases proposed by the system
are not included in the gold standard.
The selectional preference-based re-ranking yields a considerable improvement in
precision at rank 1 (26%) over the baseline. This component is also responsible for some
errors of the system, however. One of the potential limitations of selectional preference-
based approaches to metaphor paraphrasing is the presence of verbs exhibiting weak
337
Computational Linguistics Volume 39, Number 2
selectional preferences. This means that these verbs are not strongly associated with
any of their argument classes. As noted in Section 3, such verbs tend to be used
literally, and are therefore suitable paraphrases. Our selectional preference model de-
emphasizes them, however, and, as a result, they are not selected as literal paraphrases
despite matching the context. This type of error is exemplified by the phrase ?mend
marriage.? For this phrase, the system ranking overruns the correct top suggestion
of the baseline, ?improve marriage,? and outputs ?repair marriage? as the most likely
literal interpretation, although it is in fact a metaphorical use. This is likely to be due to
the fact that improve exposes a moderate selectional preference strength.
Table 10 provides frequencies of the common errors of the system by type. The
most common type of error is triggered by the conventionality of certain metaphorical
verbs. Because they frequently co-occur with the target noun class in the corpus, they
receive a high association score with that noun class. This results in a high ranking
of conventional metaphorical paraphrases. Examples of top-ranked metaphorical para-
phrases include ?confront a question? for ?tackle a question,? ?repairmarriage? for ?mend
marriage,? ?example pictures? for ?example illustrates.?
These errors concern non-literalness of the produced paraphrases. A less frequently
occurring error was paraphrasing with a verb that has a different meaning. One such
example was the metaphorical expression ?tensionmounted,? for which the system pro-
duced a paraphrase ?tension lifted,? which has the opposite meaning. This error is likely
to have been triggered by the WordNet filter, whereby one of the senses of lift would
have a common hypernym with the metaphorical verb mount. This results in lift not
being discarded by the filter, and subsequently ranked top due to the conventionality of
the expression ?tension lifted.?
Another important issue that the paraphrase analysis brought to the foreground
is the influence of wider context on metaphorical interpretation. The current system
processes only the information contained within the GR of interest, discarding the
rest of the context. For some cases, however, this is not sufficient and the analysis
of a wider context is necessary. For instance, given the phrase ?scientists focus? the
system produces a paraphrase ?scientists think,? rather than the more likely paraphrase
?scientists study.? Such ambiguity of focus could potentially be resolved by taking its
wider context into account. The context-based paraphrase ranking model described in
Section 4.3.1 allows for the incorporation of multiple relations of the metaphorical verb
in the sentence.
Although the paraphrasing system uses hand-coded lexical knowledge from
WordNet, it is important to note that metaphor paraphrasing is not restricted to
metaphorical senses included in WordNet. Even if a metaphorical sense is absent from
WordNet, the system can still identify its correct literal paraphrase relying on the
Table 10
Common system errors by type.
Source of error Subject?Verb Verb?Object Totals
Metaphor conventionality 0 5 5
General polysemy/WordNet filter 1 1 2
SP re-ranking 0 1 1
Lack of context 1 1 2
Totals 2 8 10
338
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
hyponymy relation and similarity between concepts, as described in Section 4.3.2. For
example, the metaphorical sense of handcuff in ?research is handcuffed? is not included
in Wordnet, although the system correctly identifies its paraphrase confine (?research is
confined?).
5. Evaluation of Integrated System
Up to now, the identification and the paraphrasing systemswere evaluated individually
as modules. To determine to which extent the presented systems are applicable within
NLP, we then ran the two systems together in a pipeline and evaluated the accuracy of
the resulting text-to-text metaphor processing. First, the metaphor identification system
was applied to naturally occurring text taken from the BNC and then the metaphorical
expressions identified in those texts were paraphrased by the paraphrasing system.
Some of the expressions identified and paraphrased by the integrated system are shown
in Figure 11. The system output was compared against human judgments in two
phases. In phase 1, a small sample of sentences containing metaphors identified and
paraphrased by the system was judged by multiple judges. In phase 2, a larger sample
of phrases was judged by only one judge (one of the authors of this article). Agreement
of the judgments of the latter with the other judges was measured on the data from
phase 1.
Because our goal was to evaluate both the accuracy of the integrated system and
its usability by other NLP tasks, we assessed its performance in a two-fold fashion.
Instances where metaphors were both correctly identified and paraphrased by the
system were considered strictly correct, as they show that the system fully achieved
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then swallow his anger and play tennis.
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then suppress his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AD9 3205 He tried to hide the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AMA 349Wewill halt the reduction in NHS services for long-term care and community health services
which support elderly and disabled patients at home.
AMA 349 We will prevent the reduction in NHS services for long-term care and community health
services which support elderly and disabled patients at home.
J7F 77 An economist would frame this question in terms of a cost-benefit analysis: the maximization of
returns for the minimum amount of effort injected.
J7F 77 An economist would phrase this question in terms of a cost-benefit analysis: the maximization
of returns for the minimum amount of effort injected.
EEC 1362 In it, Younger stressed the need for additional alternatives to custodial sentences, which had
been implicit in the decision to ask the Council to undertake the enquiry.
EEC 1362 In it, Younger stressed the need for additional alternatives to custodial sentences, which had
been implicit in the decision to ask the Council to initiate the enquiry.
A1F 24 Moreover, Mr Kinnock brushed aside the suggestion that he needed a big idea or unique selling
point to challenge the appeal of Thatcherism.
A1F 24 Moreover, Mr Kinnock dismissed the suggestion that he needed a big idea or unique selling
point to challenge the appeal of Thatcherism.
Figure 11
Metaphors identified (first sentences) and paraphrased (second sentences) by the system.
339
Computational Linguistics Volume 39, Number 2
its goals. Instances where the paraphrasing retained the meaning and resulted in a
literal paraphrase (including the cases where the identification module tagged a literal
expression as a metaphor) were considered correct lenient. The intuition behind this
evaluation setting is that correct paraphrasing of literal expressions by other literal
expressions, albeit not demonstrating the positive contribution of metaphor processing,
does not lead to any errors in system output and thus does not hamper the overall
usability of the integrated system.
5.1 Phase 1: Small Sample, Multiple Judges
Three volunteer subjects participated in the experiment. They were all native speakers
of English and had no formal training in linguistics.
Materials and task Subjects were presented with a set of sentences containing
metaphorical expressions identified by the system and their paraphrases, as shown
in Figure 12. There were 35 such sentences in the sample. They were asked to do the
following:
1. Compare the sentences, decide whether the highlighted expressions have
the same meaning, and record this in the box provided;
2. Decide whether the verbs in both sentences are used metaphorically or
literally and tick the respective boxes.
For the second task, the same definition of metaphor as in the identification evaluation
(cf. Section 3.3.2) was provided for guidance.
Interannotator agreement The reliability of annotations was evaluated indepen-
dently for judgments on similarity of paraphrases and their literalness. The inter-
annotator agreement on the task of distinguishing metaphoricity from literalness was
measured at ? = 0.53 (n = 2,N = 70, k = 3). On the paraphrase (i.e., meaning retention)
task, reliability was measured at ? = 0.63 (n = 2,N = 35, k = 3).
System performance We then evaluated the integrated system performance
against the subjects? judgments in terms of accuracy (both strictly correct and correct
Figure 12
Evaluation of metaphor identification and paraphrasing.
340
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 11
Integrated system performance.
Tagging case Acceptability Percentage
Correct paraphrase: metaphorical? literal ? 53.8
Correct paraphrase: literal? literal ? 13.5
Correct paraphrase: literal?metaphorical ? 0.5
Correct paraphrase: metaphorical?metaphorical ? 10.7
Incorrect paraphrase ? 21.5
lenient). Strictly correct accuracy in this task measures the proportion of metaphors
both identified and paraphrased correctly in the given set of sentences. Correct lenient
accuracy, which demonstrates applicability of the system, is represented by the overall
proportion of paraphrases that retained their meaning and resulted in a literal para-
phrase (i.e., including literal paraphrasing of literal expressions in original sentences).
Human judgments were merged into a majority gold standard, which consists of those
instances that were considered correct (i.e., identified metaphor correctly paraphrased
by the system) by at least two judges. Compared to this majority gold standard, the
integrated system operates with a strictly correct accuracy of 0.66 and correct lenient
accuracy of 0.71. The average human agreement with the majority gold standard in
terms of accuracy is 0.80 on the literalness judgments and 0.89 on the meaning retention
judgments.
5.2 Phase 2: Larger Sample, One Judge
The systemwas also evaluated on a larger sample of automatically annotatedmetaphor-
ical expressions (600 sentences) using one person?s judgments produced following
the procedure from phase 1. We measured how far these judgments agree with the
judges used in phase 1. The agreement on meaning retention was measured at ? = 0.59
(n = 2,N = 35, k = 4) and that on the literalness of paraphrases at ? = 0.54 (n = 2,N =
70, k = 4).
On this larger data set, the system achieved an accuracy of 0.54 (strictly correct) and
0.67 (correct lenient). The proportions of different tagging cases are shown in Table 11.
The table also shows the acceptability of tagging cases. Acceptability indicates whether
or not this type of system paraphrasing would cause an error when hypothetically
integrated with an external NLP application. Cases where the system produces correct
literal paraphrases for metaphorical expressions identified in the text would benefit
another NLP application, whereas cases where literal expressions are correctly para-
phrased by other literal expressions are considered neutral. Both such cases are deemed
acceptable, because they increase or preserve literalness of the text. All other tagging
cases introduce errors, thus they are marked as unacceptable. Examples of different
tagging cases are shown in Table 12.
The accuracy of metaphor-to-literal paraphrasing (0.54) indicates the level of in-
formative contribution of the system, and the overall accuracy of correct paraphrasing
resulting in a literal expression (0.67) represents the level of its acceptability within NLP.
5.3 Discussion and Error Analysis
The results of integrated system evaluation suggest that the system is capable of pro-
viding useful information about metaphor for an external text processing application
341
Computational Linguistics Volume 39, Number 2
Table 12
Examples of different tagging cases.
Tagging case Examples
Correct paraphrase: met? lit throw an idea? express an idea
Correct paraphrase: lit? lit adopt a recommendation? accept a recommendation
Correct paraphrase: lit?met arouse memory? awakenmemory
Correct paraphrase: met?met work killed him?work exhausted him
with a reasonable accuracy (0.67). It may, however, also introduce errors in the text by
incorrect paraphrasing, as well as by producing metaphorical paraphrases. If the latter
errors are rare (0.5%), the errors of the former type are sufficiently frequent (21.5%) to
make the metaphor system less desirable for use in NLP. It is therefore important to
address such errors.
Table 13 shows the contribution of the individual system components to the overall
error. The identification system tags 28% of all instances incorrectly (170). This yields a
component performance of 72%. This result is slightly lower than that obtained in its
individual evaluation in a setting with multiple judges (79%). This can be explained
by the fact that the integrated system was evaluated by one judge only, rather than
using a majority gold standard. When compared with the judgments of each annotator
pairwise the system precision was measured at 74% (cf. Section 3.3.2). Some of the
literal instances tagged as ametaphor by the identification component are then correctly
paraphrased with a literal expression by the paraphrasing component. Such cases do
not change the meaning of the text, and hence are considered acceptable. The resulting
contribution of the identification component to the overall error of the integrated system
is thus 15%.
As Table 13 shows, the paraphrasing component failed in 32% of all cases
(196 instances out of 600 were paraphrased incorrectly). As mentioned previously, this
error can be further split into paraphrasing without meaning retention (21.5%) and
metaphorical paraphrasing (11%). Both of these error types are unacceptable and lead
to lower performance of the integrated system. This error rate is also higher than that
of the paraphrasing system when evaluated individually on a manually created data
set (19%). The reasons for incorrect paraphrasing by the integrated system are manifold
Table 13
System errors by component. Three categories are cases where the identification model
incorrectly tagged a literal expression as metaphoric (false negatives from this module
were not measured). The remaining two categories are for paraphrase errors on correctly
identified metaphors.
Type of error Identification Paraphrasing
Correct paraphrase: lit? lit 81 0
Correct paraphrase: lit?met 3 3
Correct paraphrase: met?met ? 64
Incorrect paraphrase for literal 86 86
Incorrect paraphrase for metaphor ? 43
Totals 170 196
342
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
and concern both themetaphor identification and paraphrasing components. One of the
central problems stems from the initial tagging of literal expressions as metaphorical by
the identification system. The paraphrasing system is not designed with literal-to-literal
paraphrasing in mind. When it receives literal expressions which have been incorrectly
identified as input, it searches for a more literal paraphrase for them. Not all literally
used words have suitable substitutes in the given context, however. For instance,
the literal expression ?approve conclusion? is incorrectly paraphrased as ?evaluate
conclusion.?
Similar errors occur when metaphorical expressions do not have any single-word
literal paraphrases, for example, ?country functions according to...?. This is, however,
a more fundamental problem for metaphor paraphrasing as a task. In such cases, the
system, nonetheless, attempts to produce a substitute with approximately the same
meaning, which often leads to either metaphorical or incorrect paraphrasing. For in-
stance, ?country functions? is paraphrased by ?country runs,? with suggestions with
lower rank being ?country works? and ?country operates.?
Some errors that occur at the paraphrasing level are also due to the general
word sense ambiguity of certain verbs or nouns. Consider the following paraphras-
ing example, where Example (24a) shows an automatically identified metaphor and
Example (24b) its system-derived paraphrase:
(24) a. B71 852 Craig Packer and Anne Pusey of the University of Chicago have
continued to follow the life and loves of these Tanzanian lions.
b. B71 852 Craig Packer and Anne Pusey of the University of Chicago have
continued to succeed the life and loves of these Tanzanian lions.
This error results from the fact that the verb succeed has a high selectional preference for
life in one of its senses (?attain success or reach a desired goal?) and is similar to follow
in WordNet in another of its senses (?be the successor [of]?). The system merges these
two senses in one, resulting in an incorrect paraphrase.
One automatically identified example exhibited interaction of metaphor with
metonymy at the interpretation level. In the phrase ?break word,? the verb break is used
metaphorically (although conventionally) and the noun word is a metonym standing
for promise. This affected paraphrasing in that the system searched for verbs denoting
actions that could be done with words, rather than promises, and suggested the para-
phrase ?interrupt word(s).? This paraphrase is interpretable in the context of a person
giving a speech, but not in the context of a person giving a promise. This was the only
case of metonymy in the analyzed data, however.
Another issue that the evaluation on a larger data set revealed is the limitations of
the WordNet filter used in the paraphrasing system. Despite being a wide-coverage
general-domain database, WordNet does not include information about all possible
relations that exist between particular word senses. This means that some of the correct
paraphrases suggested by the context-based model get discarded by the WordNet filter
due to missing information in WordNet. For instance, the system produces no para-
phrase for the metaphors ?hurl comment,? ?spark enthusiasm,? and ?magnify thought?
that it correctly identified. This problemmotivates the exploration of possibleWordNet-
free solutions for similarity detection in the metaphor paraphrasing task. The system
could either rely entirely on such a solution, or back off to it in cases when theWordNet-
based system fails.
Table 14 provides a summary of system errors by type. The most common errors
are caused by metaphor conventionality resulting in metaphorical paraphrasing (e.g.,
343
Computational Linguistics Volume 39, Number 2
Table 14
Errors of the paraphrasing component by type.
Source of error Met?Met Lit?Met Incorr. for Lit Incorr. for Met Total
No literal paraphrase exists 11 0 5 2 18
Metaphor conventionality 53 3 0 0 56
General polysemy 0 0 13 10 23
WordNet filter 0 0 21 21 42
SP re-ranking 0 0 41 7 48
Lack of context 0 0 6 2 8
Interaction with metonymy 0 0 0 1 1
Totals 64 3 86 43 196
?swallow anger? suppress anger,? ?work killed him? work exhausted him?), followed
by the WordNet filter? and general polysemy?related errors (e.g. ?follow lives ?
succeed lives?), resulting in incorrect paraphrasing or the system not producing any
paraphrase at all. Metaphor paraphrasing by another conventional metaphor instead
of a literal expression is undesirable, although it may still be useful if the paraphrases
are more lexicalized than the original expression. The word sense ambiguity? and
WordNet-based errors are more problematic, however, and need to be addressed in the
future. SP re-ranking is responsible for the majority of incorrect paraphrasing of literal
expressions. This may be due to the fact that the model is ignorant of the meaning
retention aspect, but rather favors the paraphrases that are used literally (albeit
incorrectly) in the given context. This shows that when building an integrated system,
it is necessary to adapt the metaphor paraphrasing module to be able to also handle
literal expressions, because the identification module is likely to produce at least some
of them.
5.4 Comparison to the CorMet System
It is hard to directly compare the performance of the presented system to the other
recent approaches to metaphor, because all of these approaches assume different task
definitions, and hence use data sets and evaluation techniques of their own. Among the
data-driven methods, however, the closest in nature to ours is Mason?s (2004) CorMet
system. Mason?s system does not perform metaphor interpretation or identification of
metaphorical expressions in text, but rather focuses on the detection of metaphorical
links between distant domains. Our system also involves such detection. Whereas
Mason relies on domain-specific selectional preferences for this purpose, however, our
system uses information about verb subcategorization, as well as general selectional
preferences, to perform distributional clustering of verbs and nouns and then link the
clusters based on metaphorical seeds. Another fundamental difference is that whereas
CorMet assigns explicit domain labels, our system models source and target domains
implicitly. In the evaluation of the CorMet system, the acquired metaphorical mappings
are compared to those in the manually created Master Metaphor List demonstrating the
accuracy of 77%. In our system, on the contrary, metaphor acquisition is evaluated via
extraction of naturally occurring metaphorical expressions, achieving a performance of
79% in terms of precision. In order to compare the new mapping acquisition ability
344
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
of our system to that of CorMet, however, we performed an additional analysis of the
mappings hypothesized by our noun clusters in relation to those in the MML. It was not
possible to compare the new mappings discovered by our system to the MML directly
as was done in Mason?s experiments, because in our approach source domains are
represented by clusters of their characteristic verbs. The analysis of the noun clusters
with respect to expansion of the the seed mappings taken from the MML, however,
allowed us to evaluate the mapping acquisition by our system in terms of both precision
and recall. The goal was to confirm our hypothesis that abstract concepts get clustered
together if they are associated with the same source domain and to evaluate the quality
of the newly acquired mappings.
To do this, we randomly selected 10 target domain categories described in the
MML and manually extracted all corresponding mappings (42 mappings in total).
The categories included SOCIETY, IDEA, LIFE, OPPORTUNITY, CHANGE, LOVE,
DIFFICULTY, CREATION, RELATIONSHIP, and COMPETITION. For the concept of
OPPORTUNITY, for example, three mappings were present in the MML: OPPORTU-
NITIES ARE PHYSICAL OBJECTS, OPPORTUNITIES ARE MOVING ENTITIES, and
OPPORTUNITIES ARE OPEN PATHS, whereas for the concept of COMPETITION the
list describes only two mappings: COMPETITION IS A RACE, COMPETITION IS A
WAR.
We then extracted the system-produced clusters containing the selected 10 target
concepts. Examples of the mappings and the corresponding clusters are shown in
Figure 13. Our goal was to verify whether other concepts in the cluster containing the
target concept are associated with the source domains given in the mappings. Each
member of these clusters was analyzed for possible association with the respective
source domains. For each concept in a cluster, we verified that it is associated with
the respective source domain by finding a corresponding metaphorical expression and
annotating the concepts accordingly. The degree of association of the members of the
clusters with a given source domain was evaluated in terms of precision on the set
of hypothesized mappings. The precision of the cluster?s association with the source
Conceptual mapping: RELATIONSHIP IS A MECHANISM (VEHICLE)
Cluster: consensus relation tradition partnership resistance foundation alliance friendship con-
tact reserve unity link peace bond myth identity hierarchy relationship connection balance
marriage democracy defense faith empire distinction coalition regime division
Conceptual mapping: LIFE IS A JOURNEY
Cluster: politics practice trading reading occupation profession sport pursuit affair career
thinking life
Conceptual mapping: SOCIETY is a (HUMAN) BODY
Cluster: class population nation state country family generation trade profession household
kingdom business industry economy market enterprise world community institution society
sector
Conceptual mapping: DIFFICULTY is DIFFICULTY IN MOVING (OBSTACLE); PHYSICAL
HARDNESS
Cluster: threat crisis risk problem poverty obstacle dilemma challenge prospect danger dis-
crimination barrier difficulty shortage
Conceptual mapping: OPPORTUNITY is a PHYSICAL OBJECT; MOVING ENTITY; OPEN
PATH
Cluster: incentive attraction scope remedy chance choice solution option perspective range
possibility contrast opportunity selection alternative focus
Figure 13
Noun clusters.
345
Computational Linguistics Volume 39, Number 2
concept was calculated as a proportion of the associated concepts in it. Based on these
results we computed the average precision (AP) as follows:
AP = 1
M
M
?
j=1
#associated concepts in cluster cj
|cj|
(13)
whereM is the number of hypothesizedmappings and cj is the cluster of target concepts
corresponding to mapping j.
The annotation was carried out by one of the authors and its average precision
is 0.82. This confirms the hypothesis of clustering by association and shows that our
method favorably compares to Mason?s system. This is only an approximate compari-
son, however. Direct comparison of metaphor acquisition by the two systems was not
possible, as they produce the output in different formats and, as mentioned earlier, our
system models conceptual mappings implicitly, both within the noun clusters, as well
as by linking them to the verb clusters.
We then additionally evaluated the recall of mapping acquisition by our system
against the MML. For each selected MML mapping, we manually extracted all alter-
native target concepts associated with the source domain in the mapping from the
MML. For example, in case of LIFE IS A JOURNEY we identified all target concepts
associated with JOURNEY according to the MML and extracted them. These included
LIFE, CAREER, LOVE, and CHANGE. We then verified whether the relevant system-
produced noun clusters contained these concepts. The recall was then calculated as a
proportion of the concepts in this list within one cluster. For example, the concepts LIFE
and CAREER are found in the same cluster, but not LOVE and CHANGE. The overall
recall of mapping acquisition was measured at 0.50.
These results show that the system is able to discover a large number of metaphor-
ical connections in the data with high precision. Although the evaluation against the
Master Metaphor List is subjective, it suggests that the use of statistical data-driven
methods in general, and distributional clustering in particular, is a promising direction
for computational modeling of metaphor.
6. Conclusion and Future Directions
The 1980s and 1990s provided us with a wealth of ideas on the structure and mecha-
nisms of metaphor. The computational approaches formulated back then are still highly
influential, although their use of task-specific hand-coded knowledge is becoming in-
creasingly less popular. The last decade witnessed a significant technological leap in
natural language computation, whereby manually crafted rules gradually gave way
to more robust corpus-based statistical methods. This is also the case for metaphor
research. In this article, we presented the first integrated statistical system for metaphor
processing in unrestricted text. Our method is distinguished from previous work in that
it does not rely on anymetaphor-specific hand-coded knowledge (besides the seed set in
the identification experiments), operates on open-domain text, and produces interpreta-
tions in textual format. The system, consisting of independent metaphor identification
and paraphrasing modules, operates with a high precision (0.79 for identification, 0.81
for paraphrasing, and 0.67 as an integrated system). Although the system has been
tested only on verb?subject and verb?object metaphors at this stage, the described iden-
tification and paraphrasing methods should be similarly applicable to a wider range
of syntactic constructions. This expectation rests on the fact that both distributional
346
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
clustering and selectional preference induction techniques have been shown to model
the meanings of a range of word classes (Hatzivassiloglou and McKeown 1993; Boleda
Torrent and Alonso i Alemany 2003; Brockmann and Lapata 2003; Zapirain, Agirre,
and Ma`rquez 2009). Extending the system to deal with metaphors represented by other
word classes and constructions as well as multi-word metaphors is part of future work.
Such an extension of the identification system would require the creation of a
seed set exemplifying more syntactic constructions and the corpus search over fur-
ther grammatical relations (e.g., verb?prepositional phrase [PP] complement relations:
?Hillary leapt in the conversation;? adjectival modifier?noun relations ?slippery mind,
deep unease, heavy loss;? noun?PP complement relations: ?a fraction of self-control, a
foot of a mountain;? verb?VP complement relations: ?aching to begin the day;? and
copula constructions: ?Death is the sorry end of the human story, not a mysterious
prelude to a new one?). Besides noun and verb clustering, it would also be necessary
to perform clustering of adjectives and adverbs. Clusters of verbs, adjectives, adverbs,
and concrete nouns would then represent source domains within the model. The data
study of Shutova and Teufel (2010) suggested that it is sometimes difficult to choose the
optimal level of abstraction of domain categories that would generalize well over the
data. Although the system does not explicitly assign any domain labels, its domain
representation is still restricted by the fixed level of generality of source concepts,
defined by the chosen cluster granularity. To relax this constraint, one could attempt
to automatically optimize cluster granularity to fit the data more accurately and to
ensure that the generated clusters explain themetaphorical expressions in the data more
comprehensively. A hierarchical clustering algorithm, such as that of Yu, Yu, and Tresp
(2006) or Sun andKorhonen (2011), could be used for this purpose. Besides this, it would
be desirable to be able to generalize metaphorical associations learned from one type
of syntactic construction across all syntactic constructions, without providing explicit
seed examples for the latter. For instance, given the seed phrase ?stir excitement,?
representing the conceptual mapping FEELINGS ARE LIQUIDS, the system should be
able to discover not only that phrases such as ?swallow anger? are metaphorical, but
that phrases such as ?ocean of happiness? are as well.
The extension of the paraphrasing system to other syntactic constructions would
involve the extraction of further grammatical relations from the corpus, such as those
listed herein, and their incorporation into the context-based paraphrase selectionmodel.
Extending both the identification system and the paraphrasing system would require
the application of the selectional preference model to other word classes. Although
Resnik?s selectional association measure has been used to model selectional preferences
of verbs for their nominal arguments, it is in principle a generalizable measure of word
association. Information-theoretic word association measures (e.g., mutual information
[Church and Hanks 1990]) have been continuously successfully applied to a range of
syntactic constructions in a number of NLP tasks (Hoang, Kim, and Kan 2009; Baldwin
and Kim 2010). This suggests that applying a distributional association measure, such
as the one proposed by Resnik, to other part-of-speech classes should still result in a
realistic model of semantic fitness, which in our terms corresponds to a measure of
?literalness? of the paraphrases.
In addition, the selectional preference model can be improved by using an SP
acquisition algorithm that can handle word sense ambiguity (e.g., Rooth et al 1999;
O? Se?aghdha 2010; Reisinger and Mooney 2010). The current approach relies on SP
classes produced by hard clustering and fails to accurately model word senses of gener-
ally polysemous words. This resulted in a number of errors in metaphor paraphrasing
and it therefore needs to be addressed in the future.
347
Computational Linguistics Volume 39, Number 2
The current version of the metaphor paraphrasing system still relies on some hand-
coded knowledge in the form of WordNet. WordNet has been criticized for a lack of
consistency, high granularity of senses, and negligence with respect to some important
semantic relations (Lenat, Miller, and Yokoi 1995). In addition, WordNet is a general-
domain resource, which is less suitable if one wanted to apply the system to domain-
specific data. For all of these reasons it would be preferable to develop a WordNet-free
fully automated approach to metaphor resolution. Vector space models of word mean-
ing (Erk 2009; Rudolph and Giesbrecht 2010; Van de Cruys, Poibeau, and Korhonen
2011) might provide a solution, as they have proved efficient in general paraphrasing
and lexical substitution settings (Erk and Pado? 2009). The feature similarity component
of the paraphrasing system that is currently based on WordNet could be replaced by
such a model.
Another crucial problem that needs to be addressed is the coverage of the iden-
tification system. To enable high usability of the system it is necessary to perform
high-recall processing. One way to improve the coverage is the creation of a larger,
more diverse seed set. Although it is hardly possible to describe the whole variety of
metaphorical language, it is possible to compile a set representative of (1) all most com-
mon source?target domain mappings and (2) all types of syntactic constructions that
exhibit metaphoricity. The existing metaphor resources, primarily the Master Metaphor
List (Lakoff, Espenson, and Schwartz 1991), and examples from the linguistic literature
about metaphor, could be a sensible starting point on a route to such a data set. Having
a diverse seed set should enable the identification system to attain a broad coverage of
the corpus.
The proposed text-to-text representation of metaphor processing is directly trans-
ferable to other NLP tasks and applications that could benefit from the inclusion of
a metaphor processing component. Overall, our results suggest that the system can
provide useful and accurate information about metaphor to other NLP tasks relying
on lexical semantics. In order to prove its usefulness for external applications, however,
an extrinsic task-based evaluation is outstanding. In the future, we intend to integrate
metaphor processing with NLP applications, exemplified by MT and opinion mining,
in order to demonstrate the contribution of this pervasive yet rarely addressed phe-
nomenon to natural language semantics.
Acknowledgments
We would like to thank the volunteer
annotators for their help in the evaluations,
as well as the Cambridge Overseas Trust
(UK), EU FP-7 PANACEA project, and the
Royal Society (UK), who funded our work.
References
Abend, Omri and Ari Rappoport. 2010.
Fully unsupervised core-adjunct argument
classification. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 226?236,
Uppsala.
Agerri, Rodrigo, John Barnden, Mark Lee,
and Alan Wallington. 2007. Metaphor,
inference and domain-independent
mappings. In Proceedings of RANLP-2007,
pages 17?23, Borovets.
Alonge, Antonietta and Margherita Castelli.
2003. Encoding information on metaphoric
expressions in WordNet-like resources.
In Proceedings of the ACL 2003 Workshop
on Lexicon and Figurative Language,
pages 10?17, Sapporo.
Andersen, Oistein, Julien Nioche, Ted
Briscoe, and John Carroll. 2008. The
BNC parsed with RASP4UIMA. In
Proceedings of LREC 2008, pages 865?869,
Marrakech.
Baldwin, Timothy and Su Nam Kim. 2010.
Multiword expressions. In N. Indurkhya
and F. J. Damerau, editors, Handbook of
Natural Language Processing, Second Edition.
CRC Press, Taylor and Francis Group,
Boca Raton, FL, pages 267?292.
348
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Barnden, John and Mark Lee. 2002. An
artificial intelligence approach to
metaphor understanding. Theoria et
Historia Scientiarum, 6(1):399?412.
Barque, Lucie and Franc?ois-Re?gis
Chaumartin. 2009. LDV Forum, 24(2):5?18.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: an unsupervised
approach using multiple-sequence
alignment. In Proceedings of the 2003
Conference of the North American Chapter of
the Association for Computational Linguistics
on Human Language Technology - Volume 1,
NAACL ?03, pages 16?23, Edmonton.
Barzilay, Regina and Kathryn McKeown.
2001. Extracting paraphrases from a
parallel corpus. In Proceedings of the
39th Annual Meeting on Association for
Computational Linguistics, ACL ?01,
pages 50?57, Toulouse.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
EMNLP ?08, pages 59?68, Honolulu, HI.
Birke, Julia and Anoop Sarkar. 2006.
A clustering approach for the nearly
unsupervised recognition of nonliteral
language. In Proceedings of EACL-06,
pages 329?336, Trento.
Black, Max. 1962.Models and Metaphors.
Cornell University Press, Ithaca, NY.
Boleda Torrent, Gemma and Laura Alonso i
Alemany. 2003. Clustering adjectives for
class acquisition. In Proceedings of the Tenth
Conference of the European Chapter of the
Association for Computational Linguistics -
Volume 2, EACL ?03, pages 9?16, Budapest.
Bolshakov, Igor and Alexander Gelbukh.
2004. Synonymous paraphrasing using
Wordnet and Internet. In Proceedings of the
9th International Conference on Applications
of Natural Language to Information Systems,
NLDB 2004, pages 312?323, Alicante.
Brew, Chris and Sabine Schulte im Walde.
2002. Spectral clustering for German verbs.
In Proceedings of EMNLP, pages 117?124,
Philadelphia, PA.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release of
the RASP system. In Proceedings of the
COLING/ACL on Interactive Presentation
Sessions, pages 77?80, Sydney.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the Tenth
Conference of the European Chapter of the
Association for Computational Linguistics -
Volume 1, EACL ?03, pages 27?34,
Budapest.
Burnard, Lou. 2007. Reference Guide for the
British National Corpus (XML Edition).
Available at http://www.natcorp.
ox.ac.uk/docs/URG.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of NAACL, HLT-NAACL ?06,
pages 17?24, New York, NY.
Cameron, Lynne. 2003.Metaphor in
Educational Discourse. Continuum, London.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and
John Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings
of the 9th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL), pages 269?270, Bergen.
Chen, Jinxiu, Donghong Ji, Chew Lim Tan,
and Zhengyu Niu. 2006. Unsupervised
relation disambiguation using spectral
clustering. In Proceedings of the
COLING/ACL, pages 89?96, Sydney.
Church, Kenneth and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Clark, Stephen and James Curran. 2007.
Wide-coverage efficient statistical
parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493?552.
Copestake, Ann and Ted Briscoe. 1995.
Semi-productive polysemy and sense
extension. Journal of Semantics, 12:15?67.
Davidov, Dmitry, Roi Reichart, and Ari
Rappoport. 2009. Superior and efficient
fully unsupervised pattern-based concept
acquisition using an unsupervised parser.
In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning,
CoNLL ?09, pages 48?56, Boulder, CO.
De Cao, Diego and Roberto Basili.
2009. Combining distributional and
paradigmatic information in a lexical
substitution task. In Proceedings of
EVALITA Workshop, 11th Congress of
Italian Association for Artificial Intelligence,
Reggie Emilia.
Dras, Mark. 1999. Tree Adjoining Grammar
and the Reluctant Paraphrasing of Text. Ph.D.
thesis, Macquarie University, Australia.
Erk, Katrin. 2009. Representing words as
regions in vector space. In Proceedings of
the Thirteenth Conference on Computational
Natural Language Learning, pages 57?65,
Boulder, CO.
349
Computational Linguistics Volume 39, Number 2
Erk, Katrin and Diana McCarthy. 2009.
Graded word sense assignment. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 440?449, Edinburgh.
Erk, Katrin and Sebastian Pado?. 2009.
Paraphrase assessment in structured
vector space: exploring parameters and
datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language
Semantics, pages 57?65, Athens.
Fass, Dan. 1991. met*: A method for
discriminating metonymy and metaphor
by computer. Computational Linguistics,
17(1):49?90.
Fass, Dan and Yorick Wilks. 1983. Preference
semantics, ill-formedness, and metaphor.
Computational Linguistics, 9(3-4):178?187.
Fauconnier, Gilles and Mark Turner. 2002.
The Way We Think: Conceptual Blending and
the Mind?s Hidden Complexities. Basic Books,
New York, NY.
Feldman, Jerome. 2006. From Molecule to
Metaphor: A Neural Theory of Language.
The MIT Press, Cambridge, MA.
Feldman, Jerome and Srini Narayanan.
2004. Embodied meaning in a neural
theory of language. Brain and Language,
89(2):385?392.
Fellbaum, Christiane, editor. 1998.
WordNet: An Electronic Lexical Database
(ISBN: 0-262-06197-X). MIT Press,
Cambridge, MA.
Fillmore, Charles, Christopher Johnson,
and Miriam Petruck. 2003. Background
to FrameNet. International Journal of
Lexicography, 16(3):235?250.
Gedigian, Matt, John Bryant, Srini
Narayanan, and Branimir Ciric. 2006.
Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48,
New York, NY.
Gentner, Dedre. 1983. Structure mapping:
A theoretical framework for analogy.
Cognitive Science, 7:155?170.
Gentner, Dedre, Brian Bowdle, Phillip Wolff,
and Consuelo Boronat. 2001. Metaphor is
like analogy. In D. Gentner, K. J. Holyoak,
and B. N. Kokinov, editors, The Analogical
Mind: Perspectives from Cognitive Science.
MIT Press, Cambridge, MA,
pages 199?253.
Goatly, Andrew. 1997. The Language of
Metaphors. Routledge, London.
Grady, Joe. 1997. Foundations of Meaning:
Primary Metaphors and Primary Scenes.
Ph.D. thesis, University of California
at Berkeley.
Hanks, Patrick and James Pustejovsky. 2005.
A pattern dictionary for natural language
processing. Revue Franc?aise de linguistique
applique?e, 10(2):63?82.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1993. Towards the automatic
identification of adjectival scales:
Clustering adjectives according to
meaning. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, ACL ?93, pages 172?182,
Columbus, OH.
Hesse, Mary. 1966.Models and Analogies in
Science. Notre Dame University Press,
Notre Dame, IN.
Hoang, Hung Huu, Su Nam Kim, and
Min-Yen Kan. 2009. A re-examination of
lexical association measures. In Proceedings
of the Workshop on Multiword Expressions,
pages 31?39, Singapore.
Hofstadter, Douglas. 1995. Fluid Concepts and
Creative Analogies: Computer Models of the
Fundamental Mechanisms of Thought.
HarperCollins Publishers, London.
Hofstadter, Douglas and Melanie Mitchell.
1994. The Copycat Project: A model of
mental fluidity and analogy-making. In
K. J. Holyoak and J. A. Barnden, editors,
Advances in Connectionist and Neural
Computation Theory. Ablex, New York, NY.
Karov, Yael and Shimon Edelman.
1998. Similarity-based word sense
disambiguation. Computational
Linguistics, 24(1):41?59.
Kauchak, David and Regina Barzilay. 2006.
Paraphrasing for automatic evaluation. In
Proceedings of the Main Conference on Human
Language Technology, Conference of the North
American Chapter of the Association of
Computational Linguistics, HLT-NAACL
?06, pages 455?462, New York, NY.
Kingsbury, Paul and Martha Palmer. 2002.
From TreeBank to PropBank. In Proceedings
of LREC-2002, pages 1989?1993,
Gran Canaria, Canary Islands.
Kipper, Karin, Anna Korhonen, Neville
Ryant, and Martha Palmer. 2006.
Extensive classifications of English verbs.
In Proceedings of the 12th EURALEX
International Congress, pages 1?15, Torino.
Klein, Dan and Christopher Manning.
2003. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics, pages 423?430, Sapporo.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?step one:
Sentence compression. In Proceedings of the
Seventeenth National Conference on Artificial
350
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Intelligence and Twelfth Conference on
Innovative Applications of Artificial
Intelligence, pages 703?710, Austin, TX.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good time.
In Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, HLT ?10, pages 145?153,
Los Angeles, CA.
Korhonen, Anna, Yuval Krymolowski,
and Ted Briscoe. 2006. A large
subcategorization lexicon for natural
language processing applications.
In Proceedings of LREC 2006,
pages 1015?1020, Genoa.
Kozlowski, Raymond, Kathleen F.
McCoy, and K. Vijay-Shanker. 2003.
Generation of single-sentence
paraphrases from predicate/argument
structure using lexico-grammatical
resources. In Proceedings of the Second
International Workshop on Paraphrasing -
Volume 16, PARAPHRASE ?03,
pages 1?8, Sapporo.
Krishnakumaran, Saisuresh and Xiaojin Zhu.
2007. Hunting elusive metaphors using
lexical resources. In Proceedings of the
Workshop on Computational Approaches to
Figurative Language, pages 13?20,
Rochester, NY.
Kurohashi, Sadao. 2001. SENSEVAL-2
Japanese translation task. In Proceedings of
the SENSEVAL-2 Workshop, pages 37?44,
Toulouse.
Lakoff, George, Jane Espenson, and Alan
Schwartz. 1991. The master metaphor list.
Technical report, University of California
at Berkeley.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Lapata, Mirella. 2001. The Acquisition
and Modeling of Lexical Knowledge: A
Corpus-Based Investigation of Systematic
Polysemy. Ph.D. thesis, University of
Edinburgh.
Lenat, Doug, George Miller, and Toshio
Yokoi. 1995. CYC, WordNet, and EDR:
Critiques and responses. Commun.
ACM, 38(11):45?48.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago, IL.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics, pages 768?774,
Montreal.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7:343?360.
Lo?nneker, Birte. 2004. Lexical databases as
resources for linguistic creativity: Focus on
metaphor. In Proceedings of the LREC 2004
Workshop on Language Resources for
Linguistic Creativity, pages 9?16, Lisbon.
Lo?nneker, Birte and Carina Eilts. 2004. A
current resource and future perspectives
for enriching Wordnets with metaphor
information. In Proceedings of the Second
International WordNet Conference?GWC
2004, pages 157?162, Brno.
Martin, James. 1988. Representing
regularities in the metaphoric lexicon.
In Proceedings of the 12th Conference on
Computational Linguistics, pages 396?401,
Budapest.
Martin, James. 1990. A Computational Model of
Metaphor Interpretation. Academic Press
Professional, Inc., San Diego, CA.
Martin, James. 1994. Metabank: A
knowledge-base of metaphoric language
conventions. Computational Intelligence,
10:134?149.
Martin, James. 2006. A corpus-based
analysis of context effects on metaphor
comprehension. In A. Stefanowitsch and
S. T. Gries, editors, Corpus-Based Approaches
to Metaphor and Metonymy. Mouton de
Gruyter, Berlin, pages 214?236.
Mason, Zachary. 2004. Cormet: A
computational, corpus-based conventional
metaphor extraction system. Computational
Linguistics, 30(1):23?44.
McCarthy, Diana. 2002. Lexical
substitution as a task for WSD evaluation.
In Proceedings of the ACL-02 Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions - Volume 8, WSD ?02,
pages 109?115, Philadelphia, PA.
McCarthy, Diana, Bill Keller, and
Roberto Navigli. 2010. Getting synonym
candidates from raw data in the English
lexical substitution task. In Proceedings of
the 14th EURALEX International Congress,
Leeuwarden.
McCarthy, Diana and Roberto Navigli.
2007. Semeval-2007 task 10: English
lexical substitution task. In Proceedings
of the 4th Workshop on Semantic
Evaluations (SemEval-2007), pages 48?53,
Prague.
McCarthy, Diana and Roberto Navigli. 2009.
The English lexical substitution task.
Language Resources and Evaluation,
43(2):139?159.
351
Computational Linguistics Volume 39, Number 2
McKeown, Kathleen. 1979. Paraphrasing
using given and new information in a
question-answer system. In Proceedings
of the 17th Annual Meeting of the Association
for Computational Linguistics, ACL ?79,
pages 67?72, La Jolla, CA.
Meila, Marina and Jianbo Shi. 2001.
A random walks view of spectral
segmentation. In AISTATS, Key West, FL.
Meteer, Marie and Varda Shaked. 1988.
Strategies for effective paraphrasing.
In Proceedings of the 12th Conference on
Computational Linguistics - Volume 2,
COLING ?88, pages 431?436, Budapest.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL,
pages 236?244, Columbus, OH.
Murphy, Gregory. 1996. On metaphoric
representation. Cognition, 60:173?204.
Narayanan, Srini. 1997. Knowledge-based
Action Representations for Metaphor and
Aspect (KARMA). Ph.D. thesis, University
of California at Berkeley.
Narayanan, Srini. 1999. Moving right along:
A computational model of metaphoric
reasoning about events. In Proceedings of
AAAI 99, pages 121?128, Orlando, FL.
Nunberg, Geoffrey. 1987. Poetic and prosaic
metaphors. In Proceedings of the 1987
Workshop on Theoretical Issues in Natural
Language Processing, pages 198?201,
Stroudsburg, PA.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 435?444, Uppsala.
Orwell, George. 1946. Politics and the
English Language. Horizon, 13(76):252?265.
Pantel, Patrick and Dekang Lin. 2002.
Discovering word senses from text. In
Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 613?619,
Edmonton.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of ACL-93,
pages 183?190, Morristown, NJ.
Peters, Wim and Ivonne Peters. 2000.
Lexicalised systematic polysemy in
Wordnet. In Proceedings of LREC 2000,
Athens.
Pinker, Stephen. 2007. The Stuff of Thought:
Language as a Window into Human Nature.
Viking Adult, New York, NY.
Power, Richard and Donia Scott. 2005.
Automatic generation of large-scale
paraphrases. In Proceedings of IWP,
pages 73?79.
Pragglejaz Group. 2007. MIP: A method
for identifying metaphorically used
words in discourse.Metaphor and Symbol,
22:1?39.
Preiss, Judita, Ted Briscoe, and Anna
Korhonen. 2007. A system for large-scale
acquisition of verbal, nominal and
adjectival subcategorization frames from
corpora. In Proceedings of ACL-2007,
volume 45, page 912, Prague.
Preiss, Judita, Andrew Coonce, and
Brittany Baker. 2009. HMMs, GRs, and
n-grams as lexical substitution techniques:
are they portable to other languages?
In Proceedings of the Workshop on Natural
Language Processing Methods and Corpora
in Translation, Lexicography, and Language
Learning, MCTLLL ?09, pages 21?27,
Borovets.
Pucci, Dario, Marco Baroni, Franco Cutugno,
and Alessandro Lenci. 2009. Unsupervised
lexical substitution with a word space
model. In Proceedings of the EVALITA
Workshop, 11th Congress of Italian
Association for Artificial Intelligence,
Reggie Emilia.
Pustejovsky, James. 1995. The Generative
Lexicon.MIT Press, Cambridge, MA.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation.
In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing, pages 142?149, Barcelona.
Reining, Astrid and Birte Lo?nneker-Rodman.
2007. Corpus-driven metaphor harvesting.
In Proceedings of the HLT/NAACL-07
Workshop on Computational Approaches
to Figurative Language, pages 5?12,
Rochester, NY.
Reisinger, Joseph and Raymond Mooney.
2010. A mixture model with sharing for
lexical semantics. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10,
pages 1173?1182, Cambridge, MA.
Resnik, Philip. 1993. Selection and
Information: A Class-based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings
of ACL 99, pages 104?111, Maryland.
Rudolph, Sebastian and Eugenie Giesbrecht.
2010. Compositional matrix-space
352
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
models of language. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 907?916,
Uppsala.
Schulte im Walde, Sabine. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Sekine, Satoshi, Kentaro Inui, Ido Dagan,
Bill Dolan, Danilo Giampiccolo, and
Bernardo Magnini, editors. 2007.
Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
Prague.
Shalizi, Cosma. 2003. Analogy and metaphor.
Available at http://masi.cscs.lsa.
umich.edu/?crshalizi/notabene.
Shinyama, Yusuke and Satoshi Sekine. 2003.
Paraphrase acquisition for information
extraction. In Proceedings of the Second
International Workshop on Paraphrasing -
Volume 16, PARAPHRASE ?03,
pages 65?71, Sapporo.
Shutova, Ekaterina. 2010. Automatic
metaphor interpretation as a paraphrasing
task. In Proceedings of NAACL 2010,
pages 1029?1037, Los Angeles, CA.
Shutova, Ekaterina, Lin Sun, and Anna
Korhonen. 2010. Metaphor identification
using verb and noun clustering.
In Proceedings of COLING 2010,
pages 1,002?1,010, Beijing.
Shutova, Ekaterina and Simone Teufel.
2010. Metaphor corpus annotated for
source?target domain mappings.
In Proceedings of LREC 2010,
pages 3,255?3,261, Malta.
Siegel, Sidney and N. John Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill Book Company,
New York, NY.
Sun, Lin and Anna Korhonen. 2009.
Improving verb clustering with
automatically acquired selectional
preferences. In Proceedings of
EMNLP 2009, pages 638?647, Singapore.
Sun, Lin and Anna Korhonen. 2011.
Hierarchical verb clustering using graph
factorization. In Proceedings of EMNLP,
pages 1,023?1,033, Edinburgh.
Toral, Antonio. 2009. The lexical substitution
task at EVALITA 2009. In Proceedings of
EVALITA Workshop, 11th Congress of Italian
Association for Artificial Intelligence,
Regio Emilia.
Tourangeau, Roger and Robert Sternberg.
1982. Understanding and appreciating
metaphors. Cognition, 11:203?244.
Van de Cruys, Tim, Thierry Poibeau, and
Anna Korhonen. 2011. Latent vector
weighting for word meaning in
context. In Proceedings of EMNLP,
pages 1,012?1,022, Edinburgh.
van Rijsbergen, Keith. 1979. Information
Retrieval, 2nd edition. Butterworths,
London.
Veale, Tony and Yanfen Hao. 2008.
A fluid knowledge representation for
understanding and generating creative
metaphors. In Proceedings of COLING 2008,
pages 945?952, Manchester.
Wilks, Yorick. 1975. A preferential
pattern-seeking semantics for natural
language inference. Artificial Intelligence,
6:53?74.
Wilks, Yorick. 1978. Making preferences more
active. Artificial Intelligence, 11(3):197?223.
Yu, K., S. Yu, and V. Tresp. 2006. Soft
clustering on graphs. NIPS,
pages 1553?1561, Vancouver.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: selectional preferences for
semantic role classification. In Proceedings
of the ACL-IJCNLP 2009 Conference Short
Papers, pages 73?76, Singapore.
Zhao, S., H. Wang, T. Liu, and S. Li. 2008.
Pivot approach for extracting paraphrase
patterns from bilingual corpora.
In Proceedings of ACL-08:HLT,
pages 780?788, Columbus, OH.
Zhao, Shiqi, Xiang Lan, Ting Liu, and
Sheng Li. 2009. Application-driven
statistical paraphrase generation. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP:
Volume 2, ACL ?09, pages 834?842, Suntec.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Munteanu, and Eduard H. Hovy. 2006.
PARAEVAL: Using paraphrases to
evaluate summaries automatically.
In Proceedings of HLT-NAACL,
pages 447?454, New York, NY.
Zong, Chengqing, Yujie Zhang, and
Kazuhide Yamamoto. 2001. Approach to
spoken Chinese paraphrasing based on
feature extraction. In Proceedings of NLPRS,
pages 551?556, Tokyo.
353

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1029?1037,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Metaphor Interpretation as a Paraphrasing Task
Ekaterina Shutova
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
Ekaterina.Shutova@cl.cam.ac.uk
Abstract
We present a novel approach to metaphor in-
terpretation and a system that produces lit-
eral paraphrases for metaphorical expressions.
Such a representation is directly transferable
to other applications that can benefit from a
metaphor processing component. Our method
is distinguished from the previous work in that
it does not rely on any hand-crafted knowl-
edge about metaphor, but in contrast employs
automatically induced selectional preferences.
Being the first of its kind, our system is capa-
ble of paraphrasing metaphorical expressions
with a high accuracy (0.81).
1 Introduction
Metaphors arise when one concept is viewed in
terms of the properties of the other. In other words
it is based on similarity between the concepts. Sim-
ilarity is a kind of association implying the presence
of characteristics in common. Here are some exam-
ples of metaphor.
(1) News travels fast. (Lakoff and Johnson, 1980)
(2) How can I kill a process? (Martin, 1988)
(3) And then my heart with pleasure fills,
And dances with the daffodils.1
In metaphorical expressions seemingly unrelated
features of one concept are associated with another
1taken from the verse ?I wandered lonely as a cloud? written
by William Wordsworth in 1804.
concept. In the example (2) the computational pro-
cess is viewed as something alive and, therefore,
its forced termination is associated with the act of
killing.
Metaphorical expressions represent a great vari-
ety, ranging from conventional metaphors, which we
reproduce and comprehend every day, e.g. those in
(1) and (2), to poetic and largely novel ones, such
as (3). The use of metaphor is ubiquitous in natural
language text and it is a serious bottleneck in auto-
matic text understanding. In order to estimate the
frequency of the phenomenon, we conducted a cor-
pus study on a subset of the British National Corpus
(BNC) (Burnard, 2007) representing various genres.
We manually annotated metaphorical expressions in
this data and found that 241 out of 761 sentences
contained a metaphor or (rarely) an idiom. Due to
such a high frequency of their use, a system capa-
ble of interpreting metaphorical expressions in unre-
stricted text would become an invaluable component
of any semantics-oriented NLP application.
Automatic processing of metaphor can be clearly
divided into two subtasks: metaphor recognition
(distinguishing between literal and metaphorical
language in text) and metaphor interpretation (iden-
tifying the intended literal meaning of a metaphori-
cal expression). Both of them have been repeatedly
addressed in NLP.
To date the most influential account of metaphor
recognition has been that of Wilks (1978). Accord-
ing to Wilks, metaphors represent a violation of se-
lectional restrictions in a given context. Consider the
following example.
(4) My car drinks gasoline. (Wilks, 1978)
1029
The verb drink normally takes an animate subject
and a liquid object. Therefore, drink taking a car as
a subject is an anomaly, which may as well indicate
metaphorical use of drink.
Most approaches to metaphor interpretation rely
on task-specific hand-coded knowledge (Fass, 1991;
Martin, 1990; Narayanan, 1997; Narayanan, 1999;
Feldman and Narayanan, 2004; Barnden and Lee,
2002; Agerri et al, 2007) and produce interpreta-
tions in a non-textual format. However, the ultimate
objective of automatic metaphor processing is a type
of interpretation that can be directly embedded into
other systems to enhance their performance. Thus,
we define metaphor interpretation as a paraphrasing
task and build a system that automatically derives
literal paraphrases for metaphorical expressions in
unrestricted text.
In summary, our system (1) produces a list of
all possible paraphrases for a metaphorical expres-
sion (induced automatically from a large corpus);
(2) ranks the paraphrases according to their likeli-
hood derived from the corpus; (3) discriminates be-
tween literal and figurative paraphrases by detect-
ing selectional preference violation and outputs the
literal ones; and (4) disambiguates the sense of the
paraphrases using WordNet (Fellbaum, 1998) inven-
tory of senses.
We tested our system on a collection of metaphor-
ical expressions representing verb-subject and verb-
object constructions, where the verb is used
metaphorically. To compile this dataset we manually
annotated such phrases in a subset of the BNC using
the metaphor identification procedure (MIP) (Prag-
glejaz Group, 2007). We then evaluated the quality
of paraphrasing with the help of human annotators
and created a gold standard for this task.
2 Experimental Data
Since we focus on single-word metaphors expressed
by a verb, our annotation task can be viewed as
verb classification according to whether the verbs
are used metaphorically or literally. However, some
verbs have weak or no potential of being a metaphor
and, thus, our study is not concerned with them. We
excluded the following verb classes: (1) auxiliary
verbs; (2) modal verbs; (3) aspectual verbs (e.g. be-
gin, start, finish); (4) light verbs (e.g. take, give, put,
get, make).
2.1 The Corpus
Our corpus is a subset of the BNC. We sampled
texts representing various genres: literature, news-
paper/journal articles, essays on politics, interna-
tional relations and history, radio broadcast (tran-
scribed speech). The corpus contains 761 sentences
and 13642 words.
2.2 Annotation Scheme
The annotation scheme we use is based on the
principles of the metaphor identification procedure
(MIP) developed by Pragglejaz Group (2007). We
adopt their definition of basic sense of a word and
their approach to distinguishing basic senses from
the metaphorical ones. MIP involves metaphor an-
notation at the word level as opposed to identifying
metaphorical relations (between words) or source?
target domain mappings (between concepts or do-
mains). Such annotation can be viewed as a form
of word sense disambiguation with an emphasis on
metaphoricity.
In order to discriminate between the verbs used
metaphorically and literally we use the following
procedure as part of our guidelines:
1. For each verb establish its meaning in context
and try to imagine a more basic meaning of this
verb on other contexts. As defined in the frame-
work of MIP (Pragglejaz Group, 2007) basic
meanings normally are: (1) more concrete; (2)
related to bodily action; (3) more precise (as
opposed to vague); (4) historically older.
2. If you can establish the basic meaning that is
distinct from the meaning of the verb in this
context, the verb is likely to be used metaphor-
ically.
Consider the following example sentence:
(5) If he asked her to post a letter or buy some razor
blades from the chemist, she was transported
with pleasure.
In this sentence one needs to annotate four verbs that
are underlined. The first 3 verbs are used in their ba-
sic sense, i.e. literally (ask in the context of ?a per-
son asking another person a question or a favour?;
1030
post in the context of ?a person posting/sending a
letter?; buy in the sense of ?making a purchase?).
Thus, they are tagged as literal. The verb trans-
port, however, in its basic sense is used in the con-
text of ?goods being transported/carried by a vehi-
cle?. The context in this sentence involves ?a per-
son being transported by a feeling?, which contrasts
the basic sense in that the agent of transporting is
an EMOTION as opposed to a VEHICLE. Thus, we
can infer that the use of transport in this sentence is
metaphorical.
2.3 Annotation Reliability
We tested reliability of this annotation scheme using
multiple annotators on a subset of the corpus. The
rest of the annotation was done by a single annota-
tor.
Annotators We had three independent volunteer an-
notators, who were all native speakers of English
and had some linguistics background.
Material and Task All of them received the same
text taken from the BNC containing 142 verbs to
annotate. They were asked to classify verbs as
metaphorical or literal.
Guidelines and Training The annotators received
written guidelines (2 pages) and were asked to do a
small annotation exercise (2 sentences containing 8
verbs in total). The goal of the exercise was to en-
sure they were at ease with the annotation format.
Interannotator Agreement We evaluate reliability
of our annotation scheme by assessing interannota-
tor agreement in terms of ? (Siegel and Castellan,
1988). The classification was performed with the
agreement of 0.64 (?), which is considered reliable.
The main source of disagreement was the high con-
ventionality of some expressions, i.e. cases where
the metaphorical etymology could be clearly traced,
but the senses are highly lexicalized.
2.4 Phrase Selection
Only the phrases that were tagged as metaphorical
by all of the annotators were included in the test set.
Here are some examples of such phrases: memo-
ries were slipping away; hold the truth back; stirred
an unfathomable excitement; factors shape results;
mending their marriage; brushed aside the accusa-
tions etc. In order to avoid extra noise we placed
some additional criteria to select the test phrases:
(1) exclude phrases where subject or object referent
is unknown, e.g. containing pronouns such as in in
which they [changes] operated; (2) exclude phrases
whose metaphorical meaning is realised solely in
passive constructions (e.g. sociologists have been
inclined to [..]); (3) exclude phrases where the sub-
ject or object of interest are represented by a named
entity (e.g. Then Hillary leapt into the conversa-
tion); (4) exclude multiword metaphors (e.g. go on
pilgrimage with Raleigh or put out to sea with Ten-
nyson). The resulting test set contains 62 metaphor-
ical expressions.
3 The Method
The system takes phrases containing annotated
single-word metaphors (where a verb is used
metaphorically, its context is used literally) as in-
put. It generates a list of possible paraphrases that
can occur in the same context and ranks them ac-
cording to their likelihood derived from the cor-
pus. Subsequently it identifies shared features of the
paraphrases and the metaphorical verb using Word-
Net hierarchy of concepts and removes the unrelated
concepts. Among the related paraphrases it then
identifies the literal ones given the context relying on
the automatically induced selectional preferences.
3.1 The Model for Paraphrase Ranking
We model the likelihood of a particular paraphrase
as a joint probability of the following events: the
interpretation (another term to replace the one used
metaphorically) i co-occurring with the other lexi-
cal items from its context w1, ..., wN in the relations
r1, ..., rN respectively.
Li = P (i, (w1, r1), (w2, r2), ..., (wN , rN )), (1)
where w1, ..., wN and r1, ..., rN represent the fixed
context of the term used metaphorically in the sen-
tence. This context will be kept as part of the para-
phrase, and the term used metaphorically will be re-
placed.
We take each relation of the term in a phrase to be
independent from the other relations of this term in
this phrase. E.g. for a verb in the presence of both
the subject and the object the Verb-Subject and
Verb-Object relations would be considered to
be independent events within the model. This yields
1031
the following approximation:
P (i, (w1, r1), (w2, r2), ..., (wN , rN )) =
P (i) ? P ((w1, r1)|i) ? ... ? P ((wN , rN )|i).
(2)
We can calculate the probabilities using maximum
likelihood estimation
P (i) =
f(i)
?
k f(ik)
, (3)
P (wn, rn|i) =
f(wn, rn, i)
f(i)
, (4)
where f(i) is the frequency of the interpretation on
its own,
?
k f(ik) is the number of times this part
of speech is attested in the corpus and f(wn, rn, i)
- the frequency of the co-occurrence of the interpre-
tation with the context word wn in the relation rn.
By performing appropriate substitutions into (2) we
obtain
P (i, (w1, r1), (w2, r2), ..., (wN , rN )) =
f(i)
?
k f(ik)
?
f(w1, r1, i)
f(i)
? ... ?
f(wN , rN , i)
f(i)
=
?N
n=1 f(wn, rn, i)
(f(i))N?1 ?
?
k f(ik)
(5)
This model is then used to rank the possible re-
placements of the term used metaphorically in the
fixed context according to the data.
3.2 Parameter Estimation
The parameters of the model were estimated from
the British National Corpus that was parsed using
the RASP parser of Briscoe et al (2006). We used
the grammatical relations (GRs) output of RASP
for BNC created by Andersen et al (2008). The
same output of RASP was used to identify the GRs
in the metaphorical expressions themselves, as the
metaphor corpus from which they were extracted
is a subset of the BNC. To obtain the counts for
f(wn, rn, i) we extracted all the terms appearing in
the corpus in the relation rn with wn for each lexical
item - relation pair. The initial list of replacements
for the metaphorical term was constructed by taking
an overlap of the lists of terms for each lexical item
- relation pair.
3.3 Identifying Shared Meanings in WordNet
It should be noted that the context-based model
described in 3.1 overgenerates and hence there is
a need to further narrow the search space. It
is acknowledged in the linguistics community that
metaphor is to a great extent based on similarity be-
tween the concepts involved. We exploit this fact to
refine paraphrasing. After obtaining the initial list
of possible substitutes for the metaphorical term, we
filter out the terms whose meaning does not share
any common features with that of the metaphorical
term. Consider a Computer Science metaphor kill a
process, which stands for terminate a process. The
basic sense of kill implies an end or termination of
life. Thus, termination is the shared element of the
metaphorical verb and its literal interpretation.
Such overlap of features can be identified using
the hyponymy relations in the WordNet taxonomy.
Within the initial list of paraphrases we select the
terms that are a hypernym of the metaphorical term
or share a common hypernym with it2. To maxi-
mize the accuracy we restrict the hypernym search
to three level distance in the taxomomy. The filtered
lists of metaphorical verb replacements for some of
the phrases from our dataset together with their log-
likelihood are demonstrated in Table 1. Selecting
the highest ranked paraphrase from this list as a lit-
eral interpretation will serve as a baseline.
3.4 Filtering Based on Selectional Preferences
The obtained lists contain some irrelevant para-
phrases (e.g. contain the truth for hold back the
truth) and some paraphrases where the substitute is
used metaphorically again (e.g. suppress the truth).
However, the task is to identify the literal interpreta-
tion, therefore, these need to be removed.
One way of dealing with both problems at once
is to take into account selectional preferences of the
verbs in our list. The verbs used metaphorically are
likely to demonstrate strong semantic preference for
the source domain, e.g. suppress would select for
movements (political) rather than ideas, or truth, (the
target domain), whereas the ones used literally (e.g.,
2We excluded the expressions containing a term whose
metaphorical sense is included in WordNet from the test set,
to ensure that the system does not rely on this extra hand-coded
knowledge about metaphor.
1032
Log-likelihood Replacement
Verb-DirectObject
hold back truth:
-13.09 contain
-14.15 conceal
-14.62 suppress
-15.13 hold
-16.23 keep
-16.24 defend
stir excitement:
-14.28 create
-14.84 provoke
-15.53 make
-15.53 elicit
-15.53 arouse
-16.23 stimulate
-16.23 raise
-16.23 excite
-16.23 conjure
Subject-Verb
report leak:
-11.78 reveal
-12.59 issue
-13.18 disclose
-13.28 emerge
-14.84 expose
-16.23 discover
Table 1: The list of paraphrases with the initial ranking
conceal) would select for truth. This would poten-
tially allow us to filter out non-literalness, as well as
unrelated verbs, by selecting the verbs that the noun
in the metaphorical expression matches best.
We automatically acquired selectional preference
distributions of the verbs in the paraphrase lists
(for Verb-Subject and Verb-Object rela-
tions) from the BNC parsed by RASP. We first clus-
tered 2000 most frequent nouns in the BNC into 200
clusters using the algorithm of Sun and Korhonen
(2009). The obtained clusters formed our selectional
preference classes. We adopted the association mea-
sure proposed by Resnik (1993) and successfully ap-
plied to a number of tasks in NLP including word
sense disambiguation (Resnik, 1997). Resnik mod-
els selectional preference of a verb in probabilistic
terms as the difference between the posterior distri-
bution of noun classes in a particular relation with
the verb and their prior distribution in that syntac-
tic position regardless of the identity of the predi-
cate. He quantifies this difference using the relative
entropy (or Kullback-Leibler distance), defining the
Association Replacement
Verb-DirectObject
hold back truth:
0.1161 conceal
0.0214 keep
0.0070 suppress
0.0022 contain
0.0018 defend
0.0006 hold
stir excitement:
0.0696 provoke
0.0245 elicit
0.0194 arouse
0.0061 conjure
0.0028 create
0.0001 stimulate
? 0 raise
? 0 make
? 0 excite
Subject-Verb
report leak:
0.1492 disclose
0.1463 discover
0.0674 reveal
0.0597 issue
? 0 emerge
? 0 expose
Table 2: The list of paraphrases reranked using selec-
tional preferences
selectional preference strength as follows.
SR(v) = D(P (c|v)||P (c)) =
?
c
P (c|v) log
P (c|v)
P (c)
,
(6)
where P (c) is the prior probability of the noun class,
P (c|v) is the posterior probability of the noun class
given the verb and R is the grammatical relation in
question. Selectional preference strength measures
how strongly the predicate constrains its arguments.
In order to quantify how well a particular argument
class fits the verb, Resnik defines another measure
called selectional association:
AR(v, c) =
1
SR(v)
P (c|v) log
P (c|v)
P (c)
. (7)
We use this measure to rerank the paraphrases and
filter out those not well suited or used metaphor-
ically. The new ranking is demonstrated in Table
2. The expectation is that the paraphrase in the first
rank (i.e. the verb with which the noun in question
1033
has the highest association) represents the literal in-
terpretation.
3.5 Sense Disambiguation
Another feature of our system is that having identi-
fied literal interpretations, it is capable to perform
their word sense disambiguation (WSD). Disam-
biguated metaphorical interpretations are potentially
a useful source of information for NLP applications
dealing with word senses.
We adopt WordNet representation of a sense.
Disambiguation is performed by selecting WordNet
nodes containing those verbs that share a common
hypernym with the metaphorical verb. The list of
disambiguated interpretations for a random selection
of phrases from our dataset is demonstrated in Table
3. However, we did not evaluate the WSD of the
paraphrases at this stage.
4 Evaluation and Discussion
We evaluated the paraphrases with the help of hu-
man annotators in two different experimental set-
tings.
Setting 1: the annotators were presented with a set
of sentences containing metaphorical expressions
and their rank 1 paraphrases produced by the system
and by the baseline. They were asked to mark the
ones that have the same meaning as the term used
metaphorically and are used literally in the context
of the paraphrase expression as correct.
We had 7 volunteer annotators who were all na-
tive speakers of English (one bilingual) and had no
or sparse linguistic expertise. Their agreement on
the task was 0.62 (?), whereby the main source
of disagreement was the presence of highly lexi-
calised metaphorical paraphrases. We then evalu-
ated the system performance against their judgments
in terms of accuracy. Accuracy measures the pro-
portion of correct literal interpretations among the
paraphrases in rank 1. The results are demonstrated
in Table 4, the final systems identifies literal para-
phrases with the accuracy of 0.81.
Setting 2: the annotators were presented with a set
of sentences containing metaphorical expressions
and asked to write down all suitable literal para-
phrases for the highlighted metaphorical verbs. We
had 5 volunteer subjects for this experiment (note
that these were people not employed in the previ-
ous setting); they were all native speakers of En-
glish and had some linguistics background. We then
compiled a gold standard by incorporating all of the
annotations. E.g. the gold standard for the phrase
brushed aside the accusations contains the verbs re-
jected, ignored, disregarded, dismissed, overlooked,
discarded.
We compared the system output against the gold
standard using mean reciprocal rank (MRR) as a
measure. MRR is traditionally used to evaluate the
performance of Question-Answering systems. We
adapted this measure in order to be able to assess
ranking quality beyond rank 1 and the recall of our
system. An individual metaphorical expression re-
ceives a score equal to the reciprocal of the rank at
which the first correct literal interpretation (accord-
ing to the human gold standard) is found among the
top five paraphrases, or 0 if none of the five para-
phrases contains a correct interpretation. Once the
individual reciprocal ranks of metaphorical expres-
sions are estimated their mean is computed across
the dataset. The MRR of our system equals 0.63
and that of the baseline is 0.55. However, it should
be noted that given that our task is open-ended, it
is hard to construct a comprehensive gold standard.
For example, for the phrase stir excitement most an-
notators suggested only one paraphrase create ex-
citement, which is found in rank 3. However, the top
ranks of the system output are occupied by provoke
and stimulate, which are more precise paraphrases,
although they have not occurred to the annotators.
Such examples result in the system?s MRR being
significantly lower than its accuracy at rank 1.
The obtained results are promising, the selec-
tional preference-based reranking yields a consider-
able improvement in accuracy (26%) over the base-
line. However, for one of the phrases in the dataset,
mend marriage, the new ranking overruns the cor-
rect top suggestion of the baseline, improve mar-
riage, and outputs repair marriage as the most likely
literal interpretation. This is due to both the conven-
tionality of some metaphorical senses (in this case
repair) and to the fact that some verbs, e.g. improve,
expose a moderate selectional preference strength,
i.e. they are equally associated with a large number
of classes. This demonstrates potential drawbacks of
the selectional preference-based solutions. Another
1034
Met. Expression Top Int. Its WordNet Sense
Verb-DirectObject
stir excitement provoke (arouse-1 elicit-1 enkindle-2 kindle-3 evoke-1 fire-7 raise-10 provoke-1) - call forth
(emotions, feelings, and responses): ?arouse pity?; ?raise a smile?; ?evoke sympathy?
inherit state acquire (get-1 acquire-1) - come into the possession of something concrete or abstract: ?She got
a lot of paintings from her uncle?; ?They acquired a new pet?
reflect concern manifest (attest-1 certify-1 manifest-1 demonstrate-3 evidence-1) - provide evidence for; stand
as proof of; show by one?s behavior, attitude, or external attributes: ?The buildings in
Rome manifest a high level of architectural sophistication?; ?This decision demonstrates
his sense of fairness?
brush aside accusation reject (reject-1) - refuse to accept or acknowledge: ?we reject the idea of starting a war?; ?The
journal rejected the student?s paper?
Verb-Subject
campaign surged improve (better-3 improve-2 ameliorate-2 meliorate-2) - to make better: ?The editor improved
the manuscript with his changes?
report leaked disclose (unwrap-2 disclose-1 let on-1 bring out-9 reveal-2 discover-6 expose-2 divulge-1
break-15 give away-2 let out-2) - make known to the public information that was pre-
viously known only to a few people or that was meant to be kept a secret: ?The auction
house would not disclose the price at which the van Gogh had sold?; ?The actress won?t
reveal how old she is?
tension mounted lift (rise-1 lift-4 arise-5 move up-2 go up-1 come up-6 uprise-6) - move upward: ?The fog
lifted?; ?The smoke arose from the forest fire?; ?The mist uprose from the meadows?
Table 3: Disambiguated paraphrases produced by the system
Relation Baseline System
Verb-DirectObject 0.52 0.79
Verb-Subject 0.57 0.83
Average 0.55 0.81
Table 4: Accuracy with the evaluation setting 1
controvertial example was the metaphorical expres-
sion tension mounted, for which the system pro-
duced a paraphrase tension lifted with the opposite
meaning. This error is likely to have been triggered
by the feature similarity component, whereby one of
the senses of lift would stem from the same node in
WordNet as the metaphorical sense of mount.
5 Related Work
According to Conceptual Metaphor Theory (Lakoff
and Johnson, 1980) metaphor can be viewed as an
analogy between two distinct domains - the target
and the source. Consider the following example:
(6) He shot down all of my arguments. (Lakoff and
Johnson, 1980)
A mapping of a concept of argument (target) to
that of war (source) is employed here. The idea of
such interconceptual mappings has been exploited in
some NLP systems.
One of the first attempts to identify and inter-
pret metaphorical expressions in text automatically
is the approach of Fass (1991). It originates in
the work of Wilks (1978) and utilizes hand-coded
knowledge. Fass (1991) developed a system called
met*, capable of discriminating between literal-
ness, metonymy, metaphor and anomaly. It does
this in three stages. First, literalness is distin-
guished from non-literalness using selectional pref-
erence violation as an indicator. In the case that non-
literalness is detected, the respective phrase is tested
for being a metonymic relation using hand-coded
patterns (such as CONTAINER-for-CONTENT). If
the system fails to recognize metonymy, it pro-
ceeds to search the knowledge base for a relevant
analogy in order to discriminate metaphorical re-
lations from anomalous ones. E.g., the sentence
in (4) would be represented in this framework as
(car,drink,gasoline), which does not satisfy the pref-
erence (animal,drink,liquid), as car is not a hy-
ponym of animal. met* then searches its knowl-
edge base for a triple containing a hypernym of
both the actual argument and the desired argument
and finds (thing,use,energy source), which repre-
sents the metaphorical interpretation.
Almost simultaneously with the work of Fass
(1991), Martin (1990) presents a Metaphor Inter-
1035
pretation, Denotation and Acquisition System (MI-
DAS). The idea behind this work is that the more
specific conventional metaphors descend from the
general ones. Given an example of a metaphor-
ical expression, MIDAS searches its database for
a corresponding metaphor that would explain the
anomaly. If it does not find any, it abstracts from
the example to more general concepts and repeats
the search. If it finds a suitable general metaphor, it
creates a mapping for its descendant, a more specific
metaphor, based on this example. This is also how
novel metaphors are acquired. MIDAS has been in-
tegrated with the Unix Consultant (UC), the system
that answers users questions about Unix.
Another cohort of approaches relies on perform-
ing inferences about entities and events in the source
and target domains for metaphor interpretation.
These include the KARMA system (Narayanan,
1997; Narayanan, 1999; Feldman and Narayanan,
2004) and the ATT-Meta project (Barnden and Lee,
2002; Agerri et al, 2007). Within both systems
the authors developed a metaphor-based reasoning
framework in accordance with the theory of concep-
tual metaphor. The reasoning process relies on man-
ually coded knowledge about the world and operates
mainly in the source domain. The results are then
projected onto the target domain using the concep-
tual mapping representation. The ATT-Meta project
concerns metaphorical and metonymic description
of mental states and reasoning about mental states
using first order logic. Their system, however, does
not take natural language sentences as input, but
logical expressions that are representations of small
discourse fragments. KARMA in turn deals with a
broad range of abstract actions and events and takes
parsed text as input.
Veale and Hao (2008) derive a ?fluid knowledge
representation for metaphor interpretation and gen-
eration?, called Talking Points. Talking Points are a
set of characteristics of concepts belonging to source
and target domains and related facts about the world
which the authors acquire automatically from Word-
Net and from the web. Talking Points are then orga-
nized in Slipnet, a framework that allows for a num-
ber of insertions, deletions and substitutions in def-
initions of such characteristics in order to establish
a connection between the target and the source con-
cepts. This work builds on the idea of slippage in
knowledge representation for understanding analo-
gies in abstract domains (Hofstadter and Mitchell,
1994; Hofstadter, 1995). Consider the metaphor
Make-up is a Western burqa:
Make-up =>
? typically worn by women
? expected to be worn by women
? must be worn by women
? must be worn by Muslim women
Burqa <=
By doing insertions and substitutions the system ar-
rives from the definition typically worn by women to
that of must be worn by Muslim women, and thus es-
tablish a link between the concepts of make-up and
burqa. Veale and Hao (2008), however, did not eval-
uate to which extent their method is useful to inter-
pret metaphorical expressions occurring in text.
6 Conclusions
We presented a novel approach to metaphor interpre-
tation and a system that produces literal paraphrases
for metaphorical expressions. Such a representation
is directly transferable to other applications that can
benefit from a metaphor processing component. Our
method is distinguished from the previous work in
that it does not rely on any hand-crafted knowledge,
other than WordNet, but in contrast employs auto-
matically induced selectional preferences.
Our system is the first of its kind and it is capa-
ble of paraphrasing metaphorical expressions with a
high accuracy (0.81). Although we reported results
on a test set consisting of verb-subject and verb-
object metaphors only, we are convinced that the
described interpretation techniques can be similarly
applied to other parts of speech and a wider range
of syntactic constructions. Extending the system to
deal with more types of phrases is part of our future
work.
Acknowledgments
I am very grateful to Anna Korhonen, Simone
Teufel, Ann Copestake and the reviewers for their
helpful feedback on this work, Lin Sun for sharing
his noun clustering data and the volunteer annota-
tors. My studies and, thus, this research are funded
by generosity of Cambridge Overseas Trust.
1036
References
R. Agerri, J.A. Barnden, M.G. Lee, and A.M.Wallington.
2007. Metaphor, inference and domain-independent
mappings. In Proceedings of International Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP-2007), pages 17?23, Borovets, Bul-
garia.
O. E. Andersen, J. Nioche, E. Briscoe, and J. Carroll.
2008. The BNC parsed with RASP4UIMA. In
Proceedings of the Sixth International Language Re-
sources and Evaluation Conference (LREC?08), Mar-
rakech, Morocco.
J.A. Barnden and M.G. Lee. 2002. An artificial intelli-
gence approach to metaphor understanding. Theoria
et Historia Scientiarum, 6(1):399?412.
E. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the rasp system. In Proceedings of the COL-
ING/ACL on Interactive presentation sessions, pages
77?80.
L. Burnard. 2007. Reference Guide for the
British National Corpus (XML Edition).
URL=http://www.natcorp.ox.ac.uk/XMLedition/URG/.
D. Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
J. Feldman and S. Narayanan. 2004. Embodied meaning
in a neural theory of language. Brain and Language,
89(2):385?392.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database (ISBN: 0-262-06197-X). MIT Press, first
edition.
D. Hofstadter and M. Mitchell. 1994. The Copy-
cat Project: A model of mental fluidity and analogy-
making. In K.J. Holyoak and J. A. Barnden, editors,
Advances in Connectionist and Neural Computation
Theory, Ablex, New Jersey.
D. Hofstadter. 1995. Fluid Concepts and Creative
Analogies: Computer Models of the Fundamental
Mechanisms of Thought. HarperCollins Publishers.
G. Lakoff and M. Johnson. 1980. Metaphors We Live By.
University of Chicago Press, Chicago.
J. H. Martin. 1988. Representing regularities in the
metaphoric lexicon. In Proceedings of the 12th con-
ference on Computational linguistics, pages 396?401.
J. H. Martin. 1990. A Computational Model of Metaphor
Interpretation. Academic Press Professional, Inc., San
Diego, CA, USA.
S. Narayanan. 1997. Knowledge-based action represen-
tations for metaphor and aspect (KARMA). Technical
report, PhD thesis, University of California at Berke-
ley.
S. Narayanan. 1999. Moving right along: A computa-
tional model of metaphoric reasoning about events. In
In Proceedings of the National Conference on Artifi-
cial Intelligence (AAAI 99), pages 121?128, Orlando,
Florida.
Pragglejaz Group (P. Crisp, R. Gibbs, A. Cienki, G.
Low, G. Steen, L. Cameron, E. Semino, J. Grady, A.
Deignan and Z. Kovecses). 2007. MIP: A method for
identifying metaphorically used words in discourse.
Metaphor and Symbol, 22:1?39.
P. Resnik. 1993. Selection and Information: A Class-
based Approach to Lexical Relationships. Ph.D. the-
sis, Philadelphia, PA, USA.
P. Resnik. 1997. Selectional preference and sense disam-
biguation. In ACL SIGLEX Workshop on Tagging Text
with Lexical Semantics, Washington, D.C.
S. Siegel and N. J. Castellan. 1988. Nonparametric
statistics for the behavioral sciences. McGraw-Hill
Book Company, New York, USA.
L. Sun and A. Korhonen. 2009. Improving verb clus-
tering with automatically acquired selectional prefer-
ences. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 638?647, Singapore, August.
T. Veale and Y. Hao. 2008. A fluid knowledge rep-
resentation for understanding and generating creative
metaphors. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 945?952, Manchester, UK.
Y. Wilks. 1978. Making preferences more active. Artifi-
cial Intelligence, 11(3):197?223.
1037
Proceedings of NAACL-HLT 2013, pages 978?988,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Unsupervised Metaphor Identification Using Hierarchical Graph
Factorization Clustering
Ekaterina Shutova
International Computer Science Institute and
Institute for Cognitive and Brain Sciences
University of California, Berkeley
katia@icsi.berkeley.edu
Lin Sun
Computer Laboratory
University of Cambridge
lin.sun@cl.cam.ac.uk
Abstract
We present a novel approach to automatic
metaphor identification, that discovers both
metaphorical associations and metaphorical
expressions in unrestricted text. Our sys-
tem first performs hierarchical graph factor-
ization clustering (HGFC) of nouns and then
searches the resulting graph for metaphorical
connections between concepts. It then makes
use of the salient features of the metaphori-
cally connected clusters to identify the actual
metaphorical expressions. In contrast to pre-
vious work, our method is fully unsupervised.
Despite this fact, it operates with an encour-
aging precision (0.69) and recall (0.61). Our
approach is also the first one in NLP to exploit
the cognitive findings on the differences in or-
ganisation of abstract and concrete concepts in
the human brain.
1 Introduction
Metaphor has traditionally been viewed as a form of
linguistic creativity, that gives our expression more
vividness, distinction and artistism. While this is
true on the surface, the mechanisms of metaphor
have a much deeper origin in our reasoning. To-
day metaphor is widely understood as a cognitive
phenomenon operating at the level of mental pro-
cesses, whereby one concept or domain is system-
atically viewed in terms of the properties of another
(Lakoff and Johnson, 1980). Consider the examples
(1) ?He shot down all of my arguments? and (2) ?He
attacked every weak point in my argument?. They
demonstrate a metaphorical mapping of the concept
of argument to that of war. The argument, which is
the target concept, is viewed in terms of a battle (or
a war), the source concept. The existence of such a
link allows us to systematically describe arguments
using the war terminology, thus leading to a num-
ber of metaphorical expressions. Lakoff and John-
son call such generalisations a source?target domain
mapping, or conceptual metaphor.
The ubiquity of metaphor in language has been
established in a number of corpus studies (Cameron,
2003; Martin, 2006; Steen et al, 2010; Shutova
and Teufel, 2010) and the role it plays in human
reasoning has been confirmed in psychological ex-
periments (Thibodeau and Boroditsky, 2011). This
makes metaphor an important research area for com-
putational and cognitive linguistics, and its auto-
matic processing indispensable for any semantics-
oriented NLP application. The problem of metaphor
modeling is gaining interest within NLP, with a
growing number of approaches exploiting statisti-
cal techniques (Mason, 2004; Gedigian et al, 2006;
Shutova, 2010; Shutova et al, 2010; Turney et al,
2011; Shutova et al, 2012). Compared to more
traditional approaches based on hand-coded knowl-
edge (Fass, 1991; Martin, 1990; Narayanan, 1997;
Narayanan, 1999; Feldman and Narayanan, 2004;
Barnden and Lee, 2002; Agerri et al, 2007), these
more recent methods tend to have a wider cover-
age, as well as be more efficient, accurate and ro-
bust. However, even the statistical metaphor pro-
cessing approaches so far often focused on a lim-
ited domain or a subset of phenomena (Gedigian
et al, 2006; Krishnakumaran and Zhu, 2007), and
only addressed one of the metaphor processing sub-
tasks: identification of metaphorical mappings (Ma-
son, 2004) or identification of metaphorical expres-
sions (Shutova et al, 2010; Turney et al, 2011). In
this paper, we present the first computational method
978
that identifies the generalisations that govern the
production of metaphorical expressions, i.e. con-
ceptual metaphors, and then uses these generalisa-
tions to identify metaphorical expressions in unre-
stricted text. As opposed to previous works on sta-
tistical metaphor processing that were supervised or
semi-supervised, and thus required training data, our
method is fully unsupervised. It relies on building a
hierarchical graph of concepts connected by their as-
sociation strength (using hierarchical clustering) and
then searching for metaphorical links in this graph.
Shutova et al (2010) introduced the hypothesis
of ?clustering by association? and claimed that in
the course of distributional noun clustering, abstract
concepts tend to cluster together if they are associ-
ated with the same source domain, while concrete
concepts cluster by meaning similarity. We share
this intuition, but take this idea a significant step
further. Our approach is theoretically grounded in
the cognitive science findings suggesting that ab-
stract and concrete concepts are organised differ-
ently in the human brain (Crutch and Warrington,
2005; Binder et al, 2005; Wiemer-Hastings and
Xu, 2005; Huang et al, 2010; Crutch and Warring-
ton, 2010; Adorni and Proverbio, 2012). Accord-
ing to Crutch and Warrington (2005), these differ-
ences emerge from their general patterns of relation
with other concepts. However, most NLP systems
to date treat abstract and concrete concepts as iden-
tical. In contrast, we incorporate this distinction
into our model by creating a network (or a graph)
of concepts, and automatically learning the differ-
ent patterns of association of abstract and concrete
concepts with other concepts. We expect that, while
concrete concepts would tend to naturally organise
into a tree-like structure (with more specific terms
descending from the more general terms), abstract
concepts would exhibit a more complex pattern of
associations. Consider the example in Figure 1. The
figure schematically shows a small portion of the
graph describing the concepts of mechanism (con-
crete), political system and relationship (abstract)
at two levels of generality. One can see from this
graph that if concrete concepts, such as bike or en-
gine tend to be connected to only one concept at the
higher level in the hierarchy (mechanism), abstract
concepts may have multiple higher-level associates:
the literal ones and the metaphorical ones. For ex-
ample, the abstract concept of democracy is liter-
ally associated with a more general concept of po-
litical system, as well as metaphorically associated
with the concept of mechanism. Such multiple as-
sociations are due to the fact that political systems
are metaphorically viewed as mechanisms, they can
function, break, they can be oiled etc. We often dis-
cuss them using mechanism terminology, and thus a
corpus-based distributional learning approach would
learn that they share features with political systems
(from their literal uses), as well as with mechanisms
(from their metaphorical uses, as shown next to the
respective graph edges in the figure). Our system
discovers such association patterns within the graph
and uses them to identify metaphorical connections
between the concepts.
To the best of our knowledge, our method is the
first one to use a hierarchical clustering model for
the metaphor processing task. The original graph of
concepts is built using hierarchical graph factoriza-
tion clustering (HGFC) (Yu et al, 2006) of nouns,
yielding a network of clusters with different levels
of generality. The weights on the edges of the graph
indicate association between the clusters (concepts).
HGFC has not been previously employed for noun
clustering in NLP, but showed successful results in
the verb clustering task (Sun and Korhonen, 2011).
In summary, our system (1) builds a graph of con-
cepts using HGFC, (2) traverses it to find metaphor-
ical associations between clusters using weights on
the edges of the graph, (3) generates lists of salient
features for the metaphorically connected clusters
and (4) searches the British National Corpus (BNC)
(Burnard, 2007) for metaphorical expressions de-
scribing the target domain concepts using the verbs
from the set of salient features. We evaluated the
performance of the system with the aid of human
judges in precision- and recall-oriented settings. In
addition, we compared its performance to that of two
baselines, an unsupervised baseline using agglom-
erative clustering (AGG) and a supervised baseline
built upon WordNet (Fellbaum, 1998) (WN).
2 Method
2.1 Dataset and Feature Extraction
Our noun dataset used for clustering contains 2000
most frequent nouns in the BNC (Burnard, 2007).
979
Figure 1: Organisation of the hierarchical graph of concepts
Following previous semantic noun classification ex-
periments (Pantel and Lin, 2002; Bergsma et al,
2008), we use the grammatical relations (GRs)
as features for clustering. We extracted the fea-
tures from the Gigaword corpus (Graff et al,
2003), which was first parsed using the RASP
parser (Briscoe et al, 2006). The verb lemmas
in VERB?SUBJECT, VERB?DIRECT OBJECT and
VERB?INDIRECT OBJECT relations with the nouns
in the dataset were then extracted from the GR out-
put of the parser. The feature values were the relative
frequencies of the features.
2.2 Hierarchical Graph Factorization
Clustering
The most widely used method for hierarchical word
clustering is AGG (Schulte im Walde and Brew,
2001; Stevenson and Joanis, 2003; Ferrer, 2004;
Devereux and Costello, 2005). The method treats
each word as a singleton cluster and then succes-
sively merges two closest clusters until all the clus-
ters have been merged into one. The cluster simi-
larity is measured using linkage criteria (e.g. Ward
(1963) measures the decrease in variance for the
clusters being merged). As opposed to this, HGFC
derives probabilistic bipartite graphs from the sim-
ilarity matrix (Yu et al, 2006). Since we require a
graph of concepts, our task is rather different from
standard hierarchical word clustering that produces
a tree of concepts. In a tree, each word can only
have a unique parent cluster at each level. Our con-
cept graph does not have this constraint: at any level
a word can be associated with an arbitrary number
of parent clusters. Therefore, not only HGFC out-
performed agglomerative clustering methods in hi-
erarchical clustering tasks (Yu et al, 2006; Sun and
Korhonen, 2011), but its hierarchical graph output
is also a more suitable representation of the concept
graph. In addition, HGFC can detect the number of
levels and the number of clusters on each level of
the hierarchical graph automatically. This is essen-
tial for our task as these settings are difficult to pre-
define for a general-purpose concept graph.
Given a set of nouns, V = {vn}Nn=1, the similar-
ity matrix W for HGFC is constructed using Jensen-
Shannon Divergence. W can be encoded by an undi-
rected graph G (Figure 2(a)), where the nouns are
mapped to vertices and Wij is the edge weight be-
tween vertices i and j. The graph G and the clus-
ter structure can be represented by a bipartite graph
K(V,U). V are the vertices on G. U = {up}mp=1
represent the hidden m clusters. For example, look-
ing at Figure 2(b), V on G can be grouped into three
clusters u1, u2 and u3. The matrix B denotes the
n?m adjacency matrix, with bip being the connec-
tion weight between the vertex vi and the cluster up.
Thus, B represents the connections between clus-
ters at an upper and lower level of clustering. A
flat clustering algorithm can be induced by assign-
ing a lower level node to the parent node that has the
largest connection weight. The number of clusters
at any level can be determined by only counting the
number of non-empty nodes (namely the nodes that
have at least one lower level node associated).
The bipartite graph K also induces a similarity
(W ?) between vi and vj : w?ij =
?m
p=1
bipbjp
?p
=
(B??1BT )ij where ? = diag(?1, ..., ?m). There-
fore, B can be found by minimizing the divergence
distance (?) between the similarity matrices W and
W ?:
980
v1
v6
v2
v4
v3
v5
v7 v8v9
(a)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
(b)
u3
u1
u2
v1
v6
v2
v4
v3
v5
v7 v8v9
(c)
u1 u2
u3
(d)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
q1
q2
(e)
Figure 2: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters
on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1
min
H,?
?(W,H?HT ), s.t.
n?
i=1
hip = 1 (1)
H = B??1; ?(X,Y ) =
?
ij
(xij log
xij
yij
? xij + yij)
Yu et al (2006) showed that this cost function is
non-increasing under the update rule:
h?ip ? hip
?
j
wij
(H?HT )ij
?phjp s.t.
?
i
h?ip = 1 (2)
??p ? ?p
?
j
wij
(H?HT )ij
hiphjp s.t.
?
p
??p =
?
ij
wij (3)
The cost function can thus be optimized by updating
h and ? alternately.
The similarity between clusters p(up, uq) can be
induced from B, as follows:
p(up, uq) = p(up)p(up|uq) = (B
TD?1B)pq (4)
D = diag(d1, ..., dn) where di =
m?
p=0
bip
We can then construct a new graph G1 (Figure
2(d)) with the clusters U as vertices, and the clus-
ter similarity p as the connection weight. The clus-
tering algorithm can now be applied again (Figure
2(e)). This process can go on iteratively, leading to
a hierarchical graph.
The number of levels (L) and the number of
clusters (ml) are detected automatically, using the
method of Sun and Korhonen (2011). Clustering
starts with an initial setting of number of clusters
(m0) for the first level. In our experiment, we set the
value of m0 to 800. For the subsequent levels, ml
is set to the number of non-empty clusters (bipartite
graph nodes) on the parent level. The matrices B
and ? are initialized randomly. We found that the
actual initialization values have little impact on the
final result. The rows in B are normalized after the
initialization so the values in each row add up to one.
For a word vi, the probability of assigning it to clus-
ter x(l)p ? Xl at level l is given by:
p(x(l)p |vi) =
?
Xl?1
...
?
x(1)?X1
p(x(l)p |x
(l?1))...p(x(1)|vi)
= (D(?1)1 B1D
?1
2 B2...D
?1
l Bl)ip (5)
Due to the random walk property of the graph, ml
is non-increasing for higher levels (Sun and Korho-
nen, 2011). The algorithm can thus terminate when
all nouns are assigned to one cluster. We run 1000
iterations of updates of h and ? (equation 2 and 3)
for each two adjacent levels.
The resulting graph is composed of a set of bipar-
tite graphs defined by Bl, Bl?1, ..., B1. A bipartite
graph has a similar structure as in Figure 1. For a
given noun, we can rank the clusters at any level ac-
cording to the soft assignment probability (eq. 5).
The clusters that have no member noun were hidden
from the ranking since they do not explicitly repre-
sent any concept. However, these clusters are still
part of the organisation of conceptual space within
the model and they contribute to the probability for
the clusters on upper levels (eq. 5). We call the view
of the hierarchical graph where these empty clusters
981
are hidden an explicit graph. The whole algorithm
can be summarized as follows:
Require: N nouns V , initial number of clusters m1
Compute the similarity matrix W0 from V
Build the graph G0 from W0, l? 1
while ml > 1 do
FactorizeGl?1 to obtain bipartite graph Kl with the
adjacency matrix Bl (eq. 1, 2 and 3)
Build a graph Gl with similarity matrix Wl =
BTl D
?1
l Bl according to equation 4
l? l + 1 ; ml ? No. non-empty clusters (eq. 5)
end while
return Bl, Bl?1...B1
2.3 Identification of Metaphorical Associations
Once we obtained the explicit graph of concepts, we
can now identify metaphorical associations based on
the weights connecting the clusters at different levels
(eq. 5). Taking a single noun (e.g. fire) as input, the
system computes the probability of its cluster mem-
bership for each cluster at each level, using these
weights. We expect the cluster membership prob-
abilities to indicate the level of association of the
input noun with the clusters. The system can then
rank the clusters at each level based on these prob-
abilities. We chose level 3 as the optimal level of
generality for our experiments, based on our qualita-
tive analysis of the graph. The system selects 6 top-
ranked clusters from this level (we expect an average
source concept to have no more than 5 typical tar-
get associates) and excludes the literal cluster con-
taining the input concept (e.g. ?fire flame blaze?).
The remaining clusters represent the target concepts
associated with the input source concept. Example
output for the input concepts of fire and disease is
shown in Figure 3. One can see from the Figure
that each of the noun-to-cluster mappings represents
a new conceptual metaphor, e.g. EMOTION is FIRE,
VIOLENCE is FIRE, CRIME is a DISEASE etc. These
mappings are exemplified in language by a number
of metaphorical expressions (e.g. ?His anger will
burn him?, ?violence flared again?, ?it?s time they
found a cure for corruption?).
2.4 Identification of Salient Features and
Metaphorical Expressions
After extracting the source?target domain mappings,
we now move on to the identification of the cor-
SOURCE: fire
TARGET 1: sense hatred emotion passion enthusiasm
sentiment hope interest feeling resentment optimism
hostility excitement anger
TARGET 2: coup violence fight resistance clash rebel-
lion battle drive fighting riot revolt war confrontation
volcano row revolution struggle
TARGET 3: alien immigrant
TARGET 4: prisoner hostage inmate
SOURCE: disease
TARGET 1: fraud outbreak offense connection leak
count crime violation abuse conspiracy corruption ter-
rorism suicide
TARGET 2: opponent critic rival
TARGET 3: execution destruction signing
TARGET 4: refusal absence fact failure lack delay
Figure 3: Discovered metaphorical associations
rage-ncsubj engulf -ncsubj erupt-ncsubj burn-ncsubj
light-dobj consume-ncsubj flare-ncsubj sweep-ncsubj
spark-dobj battle-dobj gut-idobj smolder-ncsubj ig-
nite-dobj destroy-idobj spread-ncsubj damage-idobj
light-ncsubj ravage-ncsubj crackle-ncsubj open-dobj
fuel-dobj spray-idobj roar-ncsubj perish-idobj destroy-
ncsubj wound-idobj start-dobj ignite-ncsubj injure-
idobj fight-dobj rock-ncsubj retaliate-idobj devastate-
idobj blaze-ncsubj ravage-idobj rip-ncsubj burn-idobj
spark-ncsubj warm-idobj suppress-dobj rekindle-dobj
Figure 4: Salient features for fire and the violence cluster
responding metaphorical expressions. The system
does this by harvesting the salient features that lead
to the input noun being strongly associated with the
extracted clusters. The salient features are selected
by ranking the features according to the joint prob-
ability of the feature (f ) occurring both with the in-
put noun (w) and the cluster (c). Under a simplified
independence assumption, p(w, c|f) = p(w|f) ?
p(c|f). p(w|f) and p(c|f) are calculated as the ra-
tio of the frequency of the feature f to the total
frequency of the input noun and the cluster respec-
tively. The features ranked higher are expected to
represent the source domain vocabulary that can be
used to metaphorically describe the target concepts.
We selected the top 50 features from the ranked list.
Example features (verbs and their grammatical rela-
tions) extracted for the source domain noun fire and
the violence cluster are shown in Figure 4.
We then refined the lists of features by means of
selectional preference (SP) filtering. We use SPs to
982
FEELING IS FIRE
hope lit (Subj), anger blazed (Subj), optimism raged
(Subj), enthusiasm engulfed them (Subj), hatred flared
(Subj), passion flared (Subj), interest lit (Subj), fuel re-
sentment (Dobj), anger crackled (Subj), feelings roared
(Subj), hostility blazed (Subj), light with hope (Iobj)
CRIME IS A DISEASE
cure crime (Dobj), abuse transmitted (Subj), eradicate
terrorism (Dobj), suffer from corruption (Iobj), diag-
nose abuse (Dobj), combat fraud (Dobj), cope with
crime (Iobj), cure abuse (Dobj), eradicate corruption
Figure 5: Identified metaphorical expressions for the
mappings FEELING IS FIRE and CRIME IS A DISEASE
quantify how well the extracted features describe the
source domain (e.g. fire). We extracted nominal ar-
gument distributions of the verbs in our feature lists
for VERB?SUBJECT, VERB?DIRECT OBJECT and
VERB?INDIRECT OBJECT relations. We used the al-
gorithm of Sun and Korhonen (2009) to create SP
classes and the measure of Resnik (1993) to quantify
how well a particular argument class fits the verb.
Resnik measures selectional preference strength
SR(v) of a predicate as a Kullback-Leibler distance
between two distributions: the prior probability of
the noun class P (c) and the posterior probability
of the noun class given the verb P (c|v). SR(v) =
D(P (c|v)||P (c)) =
?
c P (c|v) log
P (c|v)
P (c) . In order
to quantify how well a particular argument class fits
the verb, Resnik defines selectional association as
AR(v, c) = 1SR(v)P (c|v) log
P (c|v)
P (c) . We rank the
nominal arguments of the verbs in our feature lists
using their selectional association with the verb, and
then only retain the features whose top 5 arguments
contain the source concept. For example, the verb
start, that is a common feature for both fire and the
violence cluster (e.g. ?start a war?, ?start a fire?),
would be filtered out in this way, whereas the verbs
flare or blaze would be retained as descriptive source
domain vocabulary.
We then search the RASP-parsed BNC for gram-
matical relations, in which the nouns from the target
domain cluster appear with the verbs from the source
domain vocabulary (e.g. ?war blazed? (subj), ?to
fuel violence? (dobj) for the mapping VIOLENCE is
FIRE). The system thus annotates metaphorical ex-
pressions in text, as well as the corresponding con-
ceptual metaphors, as shown in Figure 5.
3 Evaluation and Discussion
3.1 Baselines
AGG: the agglomerative clustering baseline is
constructed using SciPy implementation (Oliphant,
2007) of Ward?s linkage method (Ward, 1963). The
output tree is cut according to the number of lev-
els and the number of clusters of the explicit graph
detected by HGFC. The resulting tree is converted
into a graph by adding connections from each clus-
ter to all the clusters one level above. The connec-
tion weight (the cluster distance) is measured us-
ing Jensen-Shannon Divergence between the cluster
centroids. This graph is used in place of the HGFC
graph in the metaphor identification experiments.
WN: in the WN baseline, the WordNet hierarchy is
used as the underlying graph of concepts, to which
the metaphor extraction method is applied. Given
a source concept, the system extracts all its sense-
1 hypernyms two levels above and subsequently all
of their sister terms. The hypernyms themselves are
considered to represent the literal sense of the source
noun and are, therefore, removed. The sister terms
are kept as potential target domains.
3.2 Evaluation of Metaphorical Associations
To create our dataset, we extracted 10 common
source concepts that map to multiple targets from
the Master Metaphor List (Lakoff et al, 1991) and
linguistic analyses of metaphor (Lakoff and John-
son, 1980; Shutova and Teufel, 2010). These
included FIRE, CHILD, SPEED, WAR, DISEASE,
BREAKDOWN, CONSTRUCTION, VEHICLE, SYS-
TEM, BUSINESS. Each of the three systems identi-
fied 50 source?target domain mappings for the given
source domains, resulting in a set of 150 conceptual
metaphors (each representing a number of submap-
pings since all the target concepts are clusters or
synsets). These were then evaluated against human
judgements in two different experimental settings.
Setting 1: The judges were presented with a set
of conceptual metaphors identified by the three sys-
tems, randomized. They were asked to annotate the
mappings they considered valid. In all our experi-
ments, the judges were encouraged to rely on their
own intuition of metaphor, but they also reviewed
the metaphor annotation guidelines of Shutova and
Teufel (2010). Two independent judges, both na-
983
tive speakers of English, participated in this exper-
iment. Their agreement on the task was ? = 0.60
(n = 2, N = 150, k = 2) (Siegel and Castel-
lan, 1988). The main differences in the annotators?
judgements stem from the fact that some metaphor-
ical associations are less obvious and common than
others, and thus need more context (or imaginative
effort) to establish. Such examples, where the judges
disagreed included metaphorical mappings such as
INTENSITY is SPEED, GOAL is a CHILD, COLLEC-
TION is a SYSTEM, ILLNESS is a BREAKDOWN.
The system performance was then evaluated
against these judgements in terms of precision (P ),
i.e. the proportion of the valid metaphorical map-
pings among those identified. We calculated sys-
tem precision (in all experiments) as an average over
both annotations. HGFC operates with a precision of
P = 0.69, whereas the baselines attain P = 0.36
(AGG) and P = 0.29 (WN). The precision of an-
notator judgements against each other (the human
ceiling) is P = 0.80, suggesting that this is a chal-
lenging task.
Setting 2: To measure recall, R, of the systems we
asked two annotators (both native speakers with a
background in metaphor, different from Setting 1)
to write down up to 5 target concepts they strongly
associated with each of the 10 source concepts.
Their annotations were then aggregated into a sin-
gle metaphor association gold standard, consisting
of 63 mappings in total. The recall of the systems
was measured against this gold standard, resulting in
HGFC R = 0.61, AGG R = 0.11 and WN R = 0.03.
As expected, HGFC outperforms both AGG and
WN baselines in both settings. AGG has been pre-
viously shown to be less accurate than HGFC in the
verb clustering task (Sun and Korhonen, 2011). Our
analysis of the noun clusters indicated that HGFC
tends to produce more pure and complete clusters
than AGG. Another important reason AGG fails is
that it by definition organises all concepts into tree
and optimises its solution locally, taking into ac-
count a small number of clusters at a time. How-
ever, being able to discover connections between
more distant domains and optimising globally over
all concepts is crucial for metaphor identification.
This makes AGG less suitable for the task, as demon-
strated by our results. However, AGG identified a
number of interesting mappings missed by HGFC,
e.g. CAREER IS A CHILD, LANGUAGE IS A SYS-
TEM, CORRUPTION IS A VEHICLE, EMPIRE IS A
CONSTRUCTION, as well as a number of mappings
in common with HGFC, e.g. DEBATE IS A WAR, DE-
STRUCTION IS A DISEASE. The WN system also
identified a few interesting metaphorical mappings
(e.g. COGNITION IS FIRE, EDUCATION IS CON-
STRUCTION), but its output is largely dominated by
the concepts similar to the source noun and contains
some unrelated concepts. The comparison of HGFC
to WN shows that HGFC identifies meaningful prop-
erties and relations of abstract concepts that can not
be captured in a tree-like classification (even an ac-
curate, manually created one). The latter is more ap-
propriate for concrete concepts, and a more flexible
representation is needed to model abstract concepts.
The fact that both baselines identified some valid
metaphorical associations, relying on less suitable
conceptual graphs, suggests that our way of travers-
ing the graph is a viable approach in principle.
HGFC identifies valid metaphorical associations
for a range of source concepts. On of them (CRIME
IS A VIRUS) happened to have been already vali-
dated in psychological experiments (Thibodeau and
Boroditsky, 2011). The most frequent type of error
of HGFC is the presence of target clusters similar or
closely related to the source noun (e.g. the parent
cluster for child). The clusters from the same do-
main can, however, be filtered out if their nouns fre-
quently occur in the same documents with the source
noun (in a large corpus), i.e. by topical similarity.
The latter is less likely for the metaphorically con-
nected nouns. We intend to implement this improve-
ment in the future version of the system.
3.3 Evaluation of Metaphorical Expressions
For each of the identified conceptual metaphors, the
three systems extracted a number of metaphorical
expressions from the corpus (average of 430 for
HGFC, 148 for AGG, and 855 for WN). The ex-
pressions were also evaluated against human judge-
ments. The judges were presented with a set of ran-
domly sampled sentences containing metaphorical
expressions as annotated by the system and by the
baselines (200 each), randomized. They were asked
to mark the tagged expressions that were metaphor-
ical in their judgement as correct. Their agreement
on the task was ? = 0.56 (n = 2, N = 600, k = 2),
984
HLJ 26 [..] ?effective action? was needed to eradicate
terrorism, drug-trafficking and corruption.
EG0 275 In the 1930s the words ?means test? was a
curse, fuelling the resistance against it both among the
unemployed and some of its administrators.
CRX 1054 [..] if the rehabilitative approach were
demonstrably successful in curing crime.
HL3 1206 [..] he would strive to accelerate progress
towards the economic integration of the Caribbean.
HXJ 121 [..] it is likely that some industries will flour-
ish in certain countries as the market widens.
Figure 6: Metaphors tagged by the system (in bold)
whereby the main source of disagreement was the
presence of lexicalized metaphors, e.g. verbs such
as impose, decline etc. The system performance
against these annotations is P = 0.65 (HGFC), P =
0.47 (AGG) and P = 0.12 (WN). The human ceiling
for this task was measured at P = 0.79. Figure 6
shows example sentences annotated by HGFC. The
performance of our unsupervised approach is close
to the previous supervised systems of Mason (2004)
(accuracy of 0.73) and Shutova et al (2010) (preci-
sion of 0.79), however, the results are not directly
comparable due to different experimental settings.
The system errors in this task stem from multiple
word senses of the salient features or the source and
target sharing some physical properties (e.g. one can
?die from crime? and ?die from a disease?). Some
identified expressions invoke a chain of mappings
(e.g. ABUSE IS A DISEASE, DISEASE IS AN ENEMY
for ?combat abuse?), however, such chains are not
yet incorporated into the system. The performance
of AGG is higher than in the mappings identification
task, since it outputs only few expressions for the
incorrect mappings. In contrast, WN tagged a large
number of literal expressions due to the incorrect
prior identification of the underlying associations.
Since there is no large metaphor-annotated corpus
available, it was impossible for us to reliably evalu-
ate the recall of metaphorical expressions. However,
we estimated it as a recall of salient features. We
manually compiled sets of typical features for the
10 source domains, and measured their recall among
the top 50 HGFC features at R = 0.70. However, in
practice the coverage in this task would directly de-
pend on that of the metaphorical associations.
4 Related Work
One of the first attempts to identify and interpret
metaphorical expressions in text is the met* sys-
tem of Fass (1991), that utilizes hand-coded knowl-
edge and detects non-literalness via selectional pref-
erence violation. In case of a violation, the re-
spective phrase is first tested for being metonymic
using hand-coded patterns (e.g. CONTAINER-FOR-
CONTENT). If this fails, the system searches the
knowledge base for a relevant analogy in order to
discriminate metaphorical relations from anomalous
ones. The system of Krishnakumaran and Zhu
(2007) uses WordNet (the hyponymy relation) and
word bigram counts to predict verbal, nominal and
adjectival metaphors at the sentence level. The au-
thors discriminate between conventional metaphors
(included in WordNet) and novel metaphors. Birke
and Sarkar (2006) present a sentence clustering ap-
proach that employs a set of seed sentences an-
notated for literalness and computes similarity be-
tween the new input sentence and all of the seed sen-
tences. The system then tags the sentence as literal
or metaphorical according to the annotation in the
most similar seeds, attaining an f-score of 53.8%.
The first system to discover source?target domain
mappings automatically is CorMet (Mason, 2004).
It does this by searching for systematic variations
in domain-specific verb selectional preferences. For
example, pour is a characteristic verb in both LAB
and FINANCE domains. In the LAB domain it has
a strong preference for liquids and in the FINANCE
domain for money. From this the system infers the
domain mapping FINANCE ? LAB and the concept
mapping money ? liquid. Gedigian et al (2006)
trained a maximum entropy classifier to discrimi-
nate between literal and metaphorical use. They
annotated the sentences from PropBank (Kingsbury
and Palmer, 2002) containing the verbs of MOTION
and CURE for metaphoricity. They used PropBank
annotation (arguments and their semantic types) as
features for classification and report an accuracy
of 95.12% (however, against a majority baseline of
92.90%). The metaphor identification system of
Shutova et al (2010) starts from a small seed set
of metaphorical expressions, learns the analogies in-
volved in their production and extends the set of
analogies by means of verb and noun clustering. As
985
a result, the system can recognize new metaphorical
expressions in unrestricted text (e.g. from the seed
?stir excitement? it infers that ?swallow anger? is
also a metaphor), achieving a precision of 79%.
Turney et al (2011) classify verbs and adjectives
as literal or metaphorical based on their level of con-
creteness or abstractness in relation to a noun they
appear with. They learn concreteness rankings for
words automatically (starting from a set of exam-
ples) and then search for expressions where a con-
crete adjective or verb is used with an abstract noun
(e.g. ?dark humour? is tagged as a metaphor and
?dark hair? is not). They report an accuracy of 73%.
5 Conclusions and Future Directions
Previous research on metaphor addressed a num-
ber of different aspects of the phenomenon, and has
shown that these aspects can be successfully mod-
eled using statistical techniques. However, the meth-
ods often focused on a limited domain and needed
manually-labeled training data. This made them dif-
ficult to apply in a real-world setting with the goal of
improving semantic interpretation in NLP at large.
Our method takes a step towards this direction. It is
fully unsupervised, and thus more robust, and can
perform accurate metaphor identification in unre-
stricted text. It identifies metaphor with a precision
of 69% and a recall of 61%, which is a very encour-
aging result for an unsupervised method. We be-
lieve that this work has important implications for
computational and cognitive modeling of metaphor,
but is also applicable to a range of other seman-
tic tasks within NLP. Integrating different represen-
tations of abstract and concrete concepts into NLP
systems may improve their performance, as well as
make the models more cognitively plausible.
One of our key future research objectives is to in-
vestigate the use and adaptation of the created con-
ceptual graph to perform metaphor interpretation. In
addition, we plan to extend this work to cover nom-
inal and adjectival metaphors, by harvesting salient
nominal and adjectival features.
Acknowledgments
This work was funded by the MetaNet project (grant
number W911NF-12-C-0022) and the Dorothy
Hodgkin Postgraduate Award.
References
Roberta Adorni and Alice Mado Proverbio. 2012. The
neural manifestation of the word concreteness effect:
An electrical neuroimaging study. Neuropsychologia,
50(5):880 ? 891.
Rodrigo Agerri, John Barnden, Mark Lee, and Alan
Wallington. 2007. Metaphor, inference and domain-
independent mappings. In Proceedings of RANLP-
2007, pages 17?23, Borovets, Bulgaria.
John Barnden and Mark Lee. 2002. An artificial intel-
ligence approach to metaphor understanding. Theoria
et Historia Scientiarum, 6(1):399?412.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 59?68, Honolulu, Hawaii.
Jeffrey R. Binder, Chris F. Westbury, Kristen A. McKier-
nan, Edward T. Possing, and David A. Medler. 2005.
Distinct brain systems for processing concrete and ab-
stract concepts. Journal of Cognitive Neuroscience,
17(6):905?917.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In In Proceedings of EACL-06, pages
329?336.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions.
Lou Burnard. 2007. Reference Guide for the British Na-
tional Corpus (XML Edition).
Lynne Cameron. 2003. Metaphor in Educational Dis-
course. Continuum, London.
Sebastian J. Crutch and Elizabeth K. Warrington.
2005. Abstract and concrete concepts have struc-
turally different representational frameworks. Brain,
128(3):615?627.
Sebastian J Crutch and Elizabeth K Warrington. 2010.
The differential dependence of abstract and concrete
words upon associative and similarity-based informa-
tion: Complementary semantic interference and facil-
itation effects. Cognitive Neuropsychology, 27(1):46?
71.
Barry Devereux and Fintan Costello. 2005. Propane
stoves and gas lamps: How the concept hierarchy in-
fluences the interpretation of noun-noun compounds.
In Proceedings of the Twenty-Seventh Annual Confer-
ence of the Cognitive Science Society.
Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
986
Jerome Feldman and Srini Narayanan. 2004. Embodied
meaning in a neural theory of language. Brain and
Language, 89(2):385?392.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition.
Eva E. Ferrer. 2004. Towards a semantic classification of
spanish verbs based on subcategorisation information.
In Proceedings of the ACL 2004 workshop on Student
research, page 13. Association for Computational Lin-
guistics.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In In Proceed-
ings of the 3rd Workshop on Scalable Natural Lan-
guage Understanding, pages 41?48, New York.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2003. English gigaword. Linguistic Data Consortium,
Philadelphia.
Hsu-Wen Huang, Chia-Lin Lee, and Kara D. Federmeier.
2010. Imagine that! erps provide evidence for distinct
hemispheric contributions to the processing of con-
crete and abstract concepts. NeuroImage, 49(1):1116
? 1123.
Paul Kingsbury and Martha Palmer. 2002. From
TreeBank to PropBank. In Proceedings of LREC-
2002, pages 1989?1993, Gran Canaria, Canary Is-
lands, Spain.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, NY.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press, Chicago.
George Lakoff, Jane Espenson, and Alan Schwartz.
1991. The master metaphor list. Technical report,
University of California at Berkeley.
James Martin. 1990. A Computational Model of
Metaphor Interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
James Martin. 2006. A corpus-based analysis of con-
text effects on metaphor comprehension. In A. Ste-
fanowitsch and S. T. Gries, editors, Corpus-Based Ap-
proaches to Metaphor and Metonymy, Berlin. Mouton
de Gruyter.
Zachary Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Srini Narayanan. 1997. Knowledge-based Action Repre-
sentations for Metaphor and Aspect (KARMA). Tech-
nical report, PhD thesis, University of California at
Berkeley.
Srini Narayanan. 1999. Moving right along: A compu-
tational model of metaphoric reasoning about events.
In Proceedings of AAAI 99), pages 121?128, Orlando,
Florida.
Travis E. Oliphant. 2007. Python for scientific comput-
ing. Computing in Science and Engineering, 9:10?20.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 613?619. ACM.
Philip Resnik. 1993. Selection and Information: A
Class-based Approach to Lexical Relationships. Ph.D.
thesis, Philadelphia, PA, USA.
Sabine Schulte im Walde and Chris Brew. 2001. Induc-
ing German semantic verb classes from purely syntac-
tic subcategorisation information. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 223?230, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Ekaterina Shutova and Simone Teufel. 2010. Metaphor
corpus annotated for source - target domain map-
pings. In Proceedings of LREC 2010, pages 3255?
3261, Malta.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of Coling 2010, pages 1002?1010,
Beijing, China.
Ekaterina Shutova, Simone Teufel, and Anna Korhonen.
2012. Statistical Metaphor Processing. Computa-
tional Linguistics, 39(2).
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings of
NAACL 2010, pages 1029?1037, Los Angeles, USA.
Sidney Siegel and N. John Castellan. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill Book Company, New York, USA.
Gerard J. Steen, Aletta G. Dorst, J. Berenike Herrmann,
Anna A. Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identifica-
tion: From MIP to MIPVU. John Benjamins, Ams-
terdam/Philadelphia.
Suzanne Stevenson and Eric Joanis. 2003. Semi-
supervised verb class discovery using noisy features.
In Proceedings of HLT-NAACL 2003, pages 71?78.
Lin Sun and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired selectional
preferences. In Proceedings of EMNLP 2009, pages
638?647, Singapore, August.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP, pages 1023?1033, Edinburgh, UK.
987
Paul H. Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in rea-
soning. PLoS ONE, 6(2):e16782, 02.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 680?690, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joe H. Ward. 1963. Hierarchical grouping to optimize an
objective function. Journal of the American statistical
association, 58(301):236?244.
Katja Wiemer-Hastings and Xu Xu. 2005. Content Dif-
ferences for Abstract and Concrete Concepts. Cogni-
tive Science, 29(5):719?736.
Kai Yu, Shipeng Yu, and Volker Tresp. 2006. Soft
clustering on graphs. Advances in Neural Information
Processing Systems, 18.
988
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 688?697,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Models of Metaphor in NLP
Ekaterina Shutova
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
Ekaterina.Shutova@cl.cam.ac.uk
Abstract
Automatic processing of metaphor can
be clearly divided into two subtasks:
metaphor recognition (distinguishing be-
tween literal and metaphorical language in
a text) and metaphor interpretation (iden-
tifying the intended literal meaning of a
metaphorical expression). Both of them
have been repeatedly addressed in NLP.
This paper is the first comprehensive and
systematic review of the existing compu-
tational models of metaphor, the issues of
metaphor annotation in corpora and the
available resources.
1 Introduction
Our production and comprehension of language
is a multi-layered computational process. Hu-
mans carry out high-level semantic tasks effort-
lessly by subconsciously employing a vast inven-
tory of complex linguistic devices, while simulta-
neously integrating their background knowledge,
to reason about reality. An ideal model of lan-
guage understanding would also be capable of per-
forming such high-level semantic tasks.
However, a great deal of NLP research to date
focuses on processing lower-level linguistic infor-
mation, such as e.g. part-of-speech tagging, dis-
covering syntactic structure of a sentence (pars-
ing), coreference resolution, named entity recog-
nition and many others. Another cohort of re-
searchers set the goal of improving application-
based statistical inference (e.g. for recognizing
textual entailment or automatic summarization).
In contrast, there have been fewer attempts to
bring the state-of-the-art NLP technologies to-
gether to model the way humans use language to
frame high-level reasoning processes, such as for
example, creative thought.
The majority of computational approaches to
figurative language still exploit the ideas articu-
lated three decades ago (Wilks, 1978; Lakoff and
Johnson, 1980; Fass, 1991) and often rely on task-
specific hand-coded knowledge. However, recent
work on lexical semantics and lexical acquisition
techniques opens many new avenues for creation
of fully automated models for recognition and in-
terpretation of figurative language. In this pa-
per I will focus on the phenomenon of metaphor
and describe the most prominent computational
approaches to metaphor, as well the issues of re-
source creation and metaphor annotation.
Metaphors arise when one concept is viewed
in terms of the properties of the other. In other
words it is based on similarity between the con-
cepts. Similarity is a kind of association implying
the presence of characteristics in common. Here
are some examples of metaphor.
(1) Hillary brushed aside the accusations.
(2) How can I kill a process? (Martin, 1988)
(3) I invested myself fully in this relationship.
(4) And then my heart with pleasure fills,
And dances with the daffodils.1
In metaphorical expressions seemingly unrelated
features of one concept are associated with an-
other concept. In the example (2) the computa-
tional process is viewed as something alive and,
therefore, its forced termination is associated with
the act of killing.
Metaphorical expressions represent a great vari-
ety, ranging from conventional metaphors, which
we reproduce and comprehend every day, e.g.
those in (2) and (3), to poetic and largely novel
ones, such as (4). The use of metaphor is ubiq-
uitous in natural language text and it is a seri-
ous bottleneck in automatic text understanding.
1?I wandered lonely as a cloud?, William Wordsworth,
1804.
688
In order to estimate the frequency of the phe-
nomenon, Shutova (2010) conducted a corpus
study on a subset of the British National Corpus
(BNC) (Burnard, 2007) representing various gen-
res. They manually annotated metaphorical ex-
pressions in this data and found that 241 out of
761 sentences contained a metaphor. Due to such
a high frequency of their use, a system capable of
recognizing and interpreting metaphorical expres-
sions in unrestricted text would become an invalu-
able component of any semantics-oriented NLP
application.
Automatic processing of metaphor can be
clearly divided into two subtasks: metaphor
recognition (distinguishing between literal and
metaphorical language in text) and metaphor in-
terpretation (identifying the intended literal mean-
ing of a metaphorical expression). Both of them
have been repeatedly addressed in NLP.
2 Theoretical Background
Four different views on metaphor have been
broadly discussed in linguistics and philosophy:
the comparison view (Gentner, 1983), the inter-
action view (Black, 1962), (Hesse, 1966), the se-
lectional restrictions violation view (Wilks, 1975;
Wilks, 1978) and the conceptual metaphor view
(Lakoff and Johnson, 1980)2. All of these ap-
proaches share the idea of an interconceptual map-
ping that underlies the production of metaphorical
expressions. In other words, metaphor always in-
volves two concepts or conceptual domains: the
target (also called topic or tenor in the linguistics
literature) and the source (or vehicle). Consider
the examples in (5) and (6).
(5) He shot down all of my arguments. (Lakoff
and Johnson, 1980)
(6) He attacked every weak point in my argu-
ment. (Lakoff and Johnson, 1980)
According to Lakoff and Johnson (1980), a
mapping of a concept of argument to that of war
is employed here. The argument, which is the tar-
get concept, is viewed in terms of a battle (or a
war), the source concept. The existence of such
a link allows us to talk about arguments using the
war terminology, thus giving rise to a number of
metaphors.
2A detailed overview and criticism of these four views can
be found in (Tourangeau and Sternberg, 1982).
However, Lakoff and Johnson do not discuss
how metaphors can be recognized in the linguis-
tic data, which is the primary task in the auto-
matic processing of metaphor. Although humans
are highly capable of producing and comprehend-
ing metaphorical expressions, the task of distin-
guishing between literal and non-literal meanings
and, therefore, identifying metaphor in text ap-
pears to be challenging. This is due to the vari-
ation in its use and external form, as well as a
not clear-cut semantic distinction. Gibbs (1984)
suggests that literal and figurative meanings are
situated at the ends of a single continuum, along
which metaphoricity and idiomaticity are spread.
This makes demarcation of metaphorical and lit-
eral language fuzzy.
So far, the most influential account of metaphor
recognition is that of Wilks (1978). According to
Wilks, metaphors represent a violation of selec-
tional restrictions in a given context. Selectional
restrictions are the semantic constraints that a verb
places onto its arguments. Consider the following
example.
(7) My car drinks gasoline. (Wilks, 1978)
The verb drink normally takes an animate subject
and a liquid object. Therefore, drink taking a car
as a subject is an anomaly, which may in turn in-
dicate the metaphorical use of drink.
3 Automatic Metaphor Recognition
One of the first attempts to identify and inter-
pret metaphorical expressions in text automati-
cally is the approach of Fass (1991). It originates
in the work of Wilks (1978) and utilizes hand-
coded knowledge. Fass (1991) developed a system
called met*, capable of discriminating between
literalness, metonymy, metaphor and anomaly.
It does this in three stages. First, literalness
is distinguished from non-literalness using selec-
tional preference violation as an indicator. In the
case that non-literalness is detected, the respective
phrase is tested for being a metonymic relation us-
ing hand-coded patterns (such as CONTAINER-
for-CONTENT). If the system fails to recognize
metonymy, it proceeds to search the knowledge
base for a relevant analogy in order to discriminate
metaphorical relations from anomalous ones. E.g.,
the sentence in (7) would be represented in this
framework as (car,drink,gasoline), which does not
satisfy the preference (animal,drink,liquid), as car
689
is not a hyponym of animal. met* then searches its
knowledge base for a triple containing a hypernym
of both the actual argument and the desired argu-
ment and finds (thing,use,energy source), which
represents the metaphorical interpretation.
However, Fass himself indicated a problem with
the selectional preference violation approach ap-
plied to metaphor recognition. The approach de-
tects any kind of non-literalness or anomaly in
language (metaphors, metonymies and others),
and not only metaphors, i.e., it overgenerates.
The methods met* uses to differentiate between
those are mainly based on hand-coded knowledge,
which implies a number of limitations.
Another problem with this approach arises from
the high conventionality of metaphor in language.
This means that some metaphorical senses are
very common. As a result the system would ex-
tract selectional preference distributions skewed
towards such conventional metaphorical senses of
the verb or one of its arguments. Therefore, al-
though some expressions may be fully metaphor-
ical in nature, no selectional preference violation
can be detected in their use. Another counterar-
gument is bound to the fact that interpretation is
always context dependent, e.g. the phrase all men
are animals can be used metaphorically, however,
without any violation of selectional restrictions.
Goatly (1997) addresses the phenomenon of
metaphor by identifying a set of linguistic cues
indicating it. He gives examples of lexical pat-
terns indicating the presence of a metaphorical ex-
pression, such as metaphorically speaking, utterly,
completely, so to speak and, surprisingly, liter-
ally. Such cues would probably not be enough for
metaphor extraction on their own, but could con-
tribute to a more complex system.
The work of Peters and Peters (2000) concen-
trates on detecting figurative language in lexical
resources. They mine WordNet (Fellbaum, 1998)
for the examples of systematic polysemy, which
allows to capture metonymic and metaphorical re-
lations. The authors search for nodes that are rel-
atively high up in the WordNet hierarchy and that
share a set of common word forms among their de-
scendants. Peters and Peters found that such nodes
often happen to be in metonymic (e.g. publica-
tion ? publisher) or metaphorical (e.g. supporting
structure ? theory) relation.
The CorMet system discussed in (Mason, 2004)
is the first attempt to discover source-target do-
main mappings automatically. This is done by
?finding systematic variations in domain-specific
selectional preferences, which are inferred from
large, dynamically mined Internet corpora?. For
example, Mason collects texts from the LAB do-
main and the FINANCE domain, in both of which
pour would be a characteristic verb. In the LAB
domain pour has a strong selectional preference
for objects of type liquid, whereas in the FI-
NANCE domain it selects for money. From this
Mason?s system infers the domain mapping FI-
NANCE ? LAB and the concept mapping money
? liquid. He compares the output of his system
against the Master Metaphor List (Lakoff et al,
1991) containing hand-crafted metaphorical map-
pings between concepts. Mason reports an accu-
racy of 77%, although it should be noted that as
any evaluation that is done by hand it contains an
element of subjectivity.
Birke and Sarkar (2006) present a sentence clus-
tering approach for non-literal language recog-
nition implemented in the TroFi system (Trope
Finder). This idea originates from a similarity-
based word sense disambiguation method devel-
oped by Karov and Edelman (1998). The method
employs a set of seed sentences, where the senses
are annotated; computes similarity between the
sentence containing the word to be disambiguated
and all of the seed sentences and selects the sense
corresponding to the annotation in the most simi-
lar seed sentences. Birke and Sarkar (2006) adapt
this algorithm to perform a two-way classification:
literal vs. non-literal, and they do not clearly de-
fine the kinds of tropes they aim to discover. They
attain a performance of 53.8% in terms of f-score.
The method of Gedigan et al (2006) discrimi-
nates between literal and metaphorical use. They
trained a maximum entropy classifier for this pur-
pose. They obtained their data by extracting the
lexical items whose frames are related to MO-
TION and CURE from FrameNet (Fillmore et
al., 2003). Then they searched the PropBank
Wall Street Journal corpus (Kingsbury and Palmer,
2002) for sentences containing such lexical items
and annotated them with respect to metaphoric-
ity. They used PropBank annotation (arguments
and their semantic types) as features to train the
classifier and report an accuracy of 95.12%. This
result is, however, only a little higher than the per-
formance of the naive baseline assigning major-
ity class to all instances (92.90%). These numbers
690
can be explained by the fact that 92.00% of the
verbs of MOTION and CURE in the Wall Street
Journal corpus are used metaphorically, thus mak-
ing the dataset unbalanced with respect to the tar-
get categories and the task notably easier.
Both Birke and Sarkar (2006) and Gedigan et
al. (2006) focus only on metaphors expressed by
a verb. As opposed to that the approach of Kr-
ishnakumaran and Zhu (2007) deals with verbs,
nouns and adjectives as parts of speech. They
use hyponymy relation in WordNet and word bi-
gram counts to predict metaphors at a sentence
level. Given an IS-A metaphor (e.g. The world
is a stage3) they verify if the two nouns involved
are in hyponymy relation in WordNet, and if
they are not then this sentence is tagged as con-
taining a metaphor. Along with this they con-
sider expressions containing a verb or an adjec-
tive used metaphorically (e.g. He planted good
ideas in their minds or He has a fertile imagi-
nation). Hereby they calculate bigram probabil-
ities of verb-noun and adjective-noun pairs (in-
cluding the hyponyms/hypernyms of the noun in
question). If the combination is not observed in
the data with sufficient frequency, the system tags
the sentence containing it as metaphorical. This
idea is a modification of the selectional prefer-
ence view of Wilks. However, by using bigram
counts over verb-noun pairs Krishnakumaran and
Zhu (2007) loose a great deal of information com-
pared to a system extracting verb-object relations
from parsed text. The authors evaluated their sys-
tem on a set of example sentences compiled from
the Master Metaphor List (Lakoff et al, 1991),
whereby highly conventionalized metaphors (they
call them dead metaphors) are taken to be negative
examples. Thus they do not deal with literal exam-
ples as such: essentially, the distinction they are
making is between the senses included in Word-
Net, even if they are conventional metaphors, and
those not included in WordNet.
4 Automatic Metaphor Interpretation
Almost simultaneously with the work of Fass
(1991), Martin (1990) presents a Metaphor In-
terpretation, Denotation and Acquisition System
(MIDAS). In this work Martin captures hierarchi-
cal organisation of conventional metaphors. The
idea behind this is that the more specific conven-
tional metaphors descend from the general ones.
3William Shakespeare
Given an example of a metaphorical expression,
MIDAS searches its database for a corresponding
metaphor that would explain the anomaly. If it
does not find any, it abstracts from the example to
more general concepts and repeats the search. If it
finds a suitable general metaphor, it creates a map-
ping for its descendant, a more specific metaphor,
based on this example. This is also how novel
metaphors are acquired. MIDAS has been inte-
grated with the Unix Consultant (UC), the sys-
tem that answers users questions about Unix. The
UC first tries to find a literal answer to the ques-
tion. If it is not able to, it calls MIDAS which
detects metaphorical expressions via selectional
preference violation and searches its database for a
metaphor explaining the anomaly in the question.
Another cohort of approaches relies on per-
forming inferences about entities and events in
the source and target domains for metaphor in-
terpretation. These include the KARMA sys-
tem (Narayanan, 1997; Narayanan, 1999; Feld-
man and Narayanan, 2004) and the ATT-Meta
project (Barnden and Lee, 2002; Agerri et al,
2007). Within both systems the authors developed
a metaphor-based reasoning framework in accor-
dance with the theory of conceptual metaphor.
The reasoning process relies on manually coded
knowledge about the world and operates mainly in
the source domain. The results are then projected
onto the target domain using the conceptual map-
ping representation. The ATT-Meta project con-
cerns metaphorical and metonymic description of
mental states and reasoning about mental states
using first order logic. Their system, however,
does not take natural language sentences as input,
but logical expressions that are representations of
small discourse fragments. KARMA in turn deals
with a broad range of abstract actions and events
and takes parsed text as input.
Veale and Hao (2008) derive a ?fluid knowl-
edge representation for metaphor interpretation
and generation?, called Talking Points. Talk-
ing Points are a set of characteristics of concepts
belonging to source and target domains and re-
lated facts about the world which the authors ac-
quire automatically from WordNet and from the
web. Talking Points are then organized in Slip-
net, a framework that allows for a number of
insertions, deletions and substitutions in defini-
tions of such characteristics in order to establish
a connection between the target and the source
691
concepts. This work builds on the idea of slip-
page in knowledge representation for understand-
ing analogies in abstract domains (Hofstadter and
Mitchell, 1994; Hofstadter, 1995). Below is an
example demonstrating how slippage operates to
explain the metaphor Make-up is a Western burqa.
Make-up =>
? typically worn by women
? expected to be worn by women
? must be worn by women
? must be worn by Muslim women
Burqa <=
By doing insertions and substitutions the sys-
tem arrives from the definition typically worn by
women to that of must be worn by Muslim women,
and thus establishes a link between the concepts
of make-up and burqa. Veale and Hao (2008),
however, did not evaluate to which extent their
knowledge base of Talking Points and the asso-
ciated reasoning framework are useful to interpret
metaphorical expressions occurring in text.
Shutova (2010) defines metaphor interpretation
as a paraphrasing task and presents a method for
deriving literal paraphrases for metaphorical ex-
pressions from the BNC. For example, for the
metaphors in ?All of this stirred an unfathomable
excitement in her? or ?a carelessly leaked report?
their system produces interpretations ?All of this
provoked an unfathomable excitement in her? and
?a carelessly disclosed report? respectively. They
first apply a probabilistic model to rank all pos-
sible paraphrases for the metaphorical expression
given the context; and then use automatically in-
duced selectional preferences to discriminate be-
tween figurative and literal paraphrases. The se-
lectional preference distribution is defined in terms
of selectional association measure introduced by
Resnik (1993) over the noun classes automatically
produced by Sun and Korhonen (2009). Shutova
(2010) tested their system only on metaphors ex-
pressed by a verb and report a paraphrasing accu-
racy of 0.81.
5 Metaphor Resources
Metaphor is a knowledge-hungry phenomenon.
Hence there is a need for either an exten-
sive manually-created knowledge-base or a robust
knowledge acquisition system for interpretation of
metaphorical expressions. The latter being a hard
task, a great deal of metaphor research resorted to
the first option. Although hand-coded knowledge
proved useful for metaphor interpretation (Fass,
1991; Martin, 1990), it should be noted that the
systems utilizing it have a very limited coverage.
One of the first attempts to create a multi-
purpose knowledge base of source?target domain
mappings is the Master Metaphor List (Lakoff et
al., 1991). It includes a classification of metaphor-
ical mappings (mainly those related to mind, feel-
ings and emotions) with the corresponding exam-
ples of language use. This resource has been criti-
cized for the lack of clear structuring principles of
the mapping ontology (Lo?nneker-Rodman, 2008).
The taxonomical levels are often confused, and the
same classes are referred to by different class la-
bels. This fact and the chosen data representation
in the Master Metaphor List make it not suitable
for computational use. However, both the idea of
the list and its actual mappings ontology inspired
the creation of other metaphor resources.
The most prominent of them are MetaBank
(Martin, 1994) and the Mental Metaphor Data-
bank4 created in the framework of the ATT-meta
project (Barnden and Lee, 2002; Agerri et al,
2007). The MetaBank is a knowledge-base of En-
glish metaphorical conventions, represented in the
form of metaphor maps (Martin, 1988) contain-
ing detailed information about source-target con-
cept mappings backed by empirical evidence. The
ATT-meta project databank contains a large num-
ber of examples of metaphors of mind classified
by source?target domain mappings taken from the
Master Metaphor List.
Along with this it is worth mentioning metaphor
resources in languages other than English. There
has been a wealth of research on metaphor
in Spanish, Chinese, Russian, German, French
and Italian. The Hamburg Metaphor Database
(Lo?nneker, 2004; Reining and Lo?nneker-Rodman,
2007) contains examples of metaphorical expres-
sions in German and French, which are mapped
to senses from EuroWordNet5 and annotated with
source?target domain mappings taken from the
Master Metaphor List.
Alonge and Castelli (2003) discuss how
metaphors can be represented in ItalWordNet for
4http://www.cs.bham.ac.uk/?jab/ATT-Meta/Databank/
5EuroWordNet is a multilingual database with wordnets
for several European languages (Dutch, Italian, Spanish, Ger-
man, French, Czech and Estonian). The wordnets are struc-
tured in the same way as the Princeton WordNet for English.
URL: http://www.illc.uva.nl/EuroWordNet/
692
Italian and motivate this by linguistic evidence.
Encoding metaphorical information in general-
domain lexical resources for English, e.g. Word-
Net (Lo?nneker and Eilts, 2004), would undoubt-
edly provide a new platform for experiments and
enable researchers to directly compare their re-
sults.
6 Metaphor Annotation in Corpora
To reflect two distinct aspects of the phenomenon,
metaphor annotation can be split into two stages:
identifying metaphorical senses in text (akin word
sense disambiguation) and annotating source ? tar-
get domain mappings underlying the production of
metaphorical expressions. Traditional approaches
to metaphor annotation include manual search
for lexical items used metaphorically (Pragglejaz
Group, 2007), for source and target domain vocab-
ulary (Deignan, 2006; Koivisto-Alanko and Tis-
sari, 2006; Martin, 2006) or for linguistic mark-
ers of metaphor (Goatly, 1997). Although there
is a consensus in the research community that
the phenomenon of metaphor is not restricted to
similarity-based extensions of meanings of iso-
lated words, but rather involves reconceptualiza-
tion of a whole area of experience in terms of an-
other, there still has been surprisingly little inter-
est in annotation of cross-domain mappings. How-
ever, a corpus annotated for conceptual mappings
could provide a new starting point for both linguis-
tic and cognitive experiments.
6.1 Metaphor and Polysemy
The theorists of metaphor distinguish between two
kinds of metaphorical language: novel (or poetic)
metaphors, that surprise our imagination, and con-
ventionalized metaphors, that become a part of an
ordinary discourse. ?Metaphors begin their lives
as novel poetic creations with marked rhetorical
effects, whose comprehension requires a special
imaginative leap. As time goes by, they become
a part of general usage, their comprehension be-
comes more automatic, and their rhetorical effect
is dulled? (Nunberg, 1987). Following Orwell
(1946) Nunberg calls such metaphors ?dead? and
claims that they are not psychologically distinct
from literally-used terms.
This scheme demonstrates how metaphorical
associations capture some generalisations govern-
ing polysemy: over time some of the aspects of
the target domain are added to the meaning of a
term in a source domain, resulting in a (metaphor-
ical) sense extension of this term. Copestake
and Briscoe (1995) discuss sense extension mainly
based on metonymic examples and model the phe-
nomenon using lexical rules encoding metonymic
patterns. Along with this they suggest that similar
mechanisms can be used to account for metaphoric
processes, and the conceptual mappings encoded
in the sense extension rules would define the lim-
its to the possible shifts in meaning.
However, it is often unclear if a metaphorical
instance is a case of broadening of the sense in
context due to general vagueness in language, or it
manifests a formation of a new distinct metaphor-
ical sense. Consider the following examples.
(8) a. As soon as I entered the room I noticed
the difference.
b. How can I enter Emacs?
(9) a. My tea is cold.
b. He is such a cold person.
Enter in (8a) is defined as ?to go or come into
a place, building, room, etc.; to pass within the
boundaries of a country, region, portion of space,
medium, etc.?6 In (8b) this sense stretches to
describe dealing with software, whereby COM-
PUTER PROGRAMS are viewed as PHYSICAL
SPACES. However, this extended sense of enter
does not appear to be sufficiently distinct or con-
ventional to be included into the dictionary, al-
though this could happen over time.
The sentence (9a) exemplifies the basic sense
of cold ? ?of a temperature sensibly lower than
that of the living human body?, whereas cold in
(9b) should be interpreted metaphorically as ?void
of ardour, warmth, or intensity of feeling; lacking
enthusiasm, heartiness, or zeal; indifferent, apa-
thetic?. These two senses are clearly linked via
the metaphoric mapping between EMOTIONAL
STATES and TEMPERATURES.
A number of metaphorical senses are included
in WordNet, however without any accompanying
semantic annotation.
6.2 Metaphor Identification
6.2.1 Pragglejaz Procedure
Pragglejaz Group (2007) proposes a metaphor
identification procedure (MIP) within the frame-
6Sense definitions are taken from the Oxford English Dic-
tionary.
693
work of the Metaphor in Discourse project (Steen,
2007). The procedure involves metaphor annota-
tion at the word level as opposed to identifying
metaphorical relations (between words) or source?
target domain mappings (between concepts or do-
mains). In order to discriminate between the verbs
used metaphorically and literally the annotators
are asked to follow the guidelines:
1. For each verb establish its meaning in context
and try to imagine a more basic meaning of
this verb on other contexts. Basic meanings
normally are: (1) more concrete; (2) related
to bodily action; (3) more precise (as opposed
to vague); (4) historically older.
2. If you can establish the basic meaning that
is distinct from the meaning of the verb in
this context, the verb is likely to be used
metaphorically.
Such annotation can be viewed as a form of
word sense disambiguation with an emphasis on
metaphoricity.
6.2.2 Source ? Target Domain Vocabulary
Another popular method that has been used to ex-
tract metaphors is searching for sentences contain-
ing lexical items from the source domain, the tar-
get domain, or both (Stefanowitsch, 2006). This
method requires exhaustive lists of source and tar-
get domain vocabulary.
Martin (2006) conducted a corpus study in
order to confirm that metaphorical expressions
occur in text in contexts containing such lex-
ical items. He performed his analysis on the
data from the Wall Street Journal (WSJ) cor-
pus and focused on four conceptual metaphors
that occur with considerable regularity in the
corpus. These include NUMERICAL VALUE
AS LOCATION, COMMERCIAL ACTIVITY
AS CONTAINER, COMMERCIAL ACTIVITY
AS PATH FOLLOWING and COMMERCIAL
ACTIVITY AS WAR. Martin manually compiled
the lists of terms characteristic for each domain
by examining sampled metaphors of these types
and then augmented them through the use of
thesaurus. He then searched the WSJ for sen-
tences containing vocabulary from these lists
and checked whether they contain metaphors of
the above types. The goal of this study was to
evaluate predictive ability of contexts containing
vocabulary from (1) source domain and (2) target
domain, as well as (3) estimating the likelihood
of a metaphorical expression following another
metaphorical expression described by the same
mapping. He obtained the most positive results for
metaphors of the type NUMERICAL-VALUE-
AS-LOCATION (P (Metaphor|Source) =
0.069, P (Metaphor|Target) = 0.677,
P (Metaphor|Metaphor) = 0.703).
6.3 Annotating Source and Target Domains
Wallington et al (2003) carried out a metaphor an-
notation experiment in the framework of the ATT-
Meta project. They employed two teams of an-
notators. Team A was asked to annotate ?inter-
esting stretches?, whereby a phrase was consid-
ered interesting if (1) its significance in the doc-
ument was non-physical, (2) it could have a phys-
ical significance in another context with a similar
syntactic frame, (3) this physical significance was
related to the abstract one. Team B had to anno-
tate phrases according to their own intuitive defi-
nition of metaphor. Besides metaphorical expres-
sions Wallington et al (2003) attempted to anno-
tate the involved source ? target domain mappings.
The annotators were given a set of mappings from
the Master Metaphor List and were asked to assign
the most suitable ones to the examples. However,
the authors do not report the level of interannota-
tor agreement nor the coverage of the mappings in
the Master Metaphor List on their data.
Shutova and Teufel (2010) adopt a different ap-
proach to the annotation of source ? target do-
main mappings. They do not rely on prede-
fined mappings, but instead derive independent
sets of most common source and target categories.
They propose a two stage procedure, whereby the
metaphorical expressions are first identified using
MIP, and then the source domain (where the ba-
sic sense comes from) and the target domain (the
given context) are selected from the lists of cate-
gories. Shutova and Teufel (2010) report interan-
notator agreement of 0.61 (?).
7 Conclusion and Future Directions
The eighties and nineties provided us with a
wealth of ideas on the structure and mechanisms
of the phenomenon of metaphor. The approaches
formulated back then are still highly influential,
although their use of hand-coded knowledge is
becoming increasingly less convincing. The last
decade witnessed a high technological leap in
694
natural language computation, whereby manually
crafted rules gradually give way to more robust
corpus-based statistical methods. This is also the
case for metaphor research. The latest develop-
ments in the lexical acquisition technology will
in the near future enable fully automated corpus-
based processing of metaphor.
However, there is still a clear need in a uni-
fied metaphor annotation procedure and creation
of a large publicly available metaphor corpus.
Given such a resource the computational work on
metaphor is likely to proceed along the following
lines: (1) automatic acquisition of an extensive set
of valid metaphorical associations from linguis-
tic data via statistical pattern matching; (2) using
the knowledge of these associations for metaphor
recognition in the unseen unrestricted text and, fi-
nally, (3) interpretation of the identified metaphor-
ical expressions by deriving the closest literal
paraphrase (a representation that can be directly
embedded in other NLP applications to enhance
their performance).
Besides making our thoughts more vivid and
filling our communication with richer imagery,
metaphors also play an important structural role
in our cognition. Thus, one of the long term goals
of metaphor research in NLP and AI would be to
build a computational intelligence model account-
ing for the way metaphors organize our conceptual
system, in terms of which we think and act.
Acknowledgments
I would like to thank Anna Korhonen and my re-
viewers for their most helpful feedback on this pa-
per. The support of Cambridge Overseas Trust,
who fully funds my studies, is gratefully acknowl-
edged.
References
R. Agerri, J.A. Barnden, M.G. Lee, and A.M. Walling-
ton. 2007. Metaphor, inference and domain-
independent mappings. In Proceedings of RANLP-
2007, pages 17?23, Borovets, Bulgaria.
A. Alonge and M. Castelli. 2003. Encoding informa-
tion on metaphoric expressions in WordNet-like re-
sources. In Proceedings of the ACL 2003 Workshop
on Lexicon and Figurative Language, pages 10?17.
J.A. Barnden and M.G. Lee. 2002. An artificial intelli-
gence approach to metaphor understanding. Theoria
et Historia Scientiarum, 6(1):399?412.
J. Birke and A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonlit-
eral language. In In Proceedings of EACL-06, pages
329?336.
M. Black. 1962. Models and Metaphors. Cornell Uni-
versity Press.
L. Burnard. 2007. Reference Guide for the British Na-
tional Corpus (XML Edition).
A. Copestake and T. Briscoe. 1995. Semi-productive
polysemy and sense extension. Journal of Seman-
tics, 12:15?67.
A. Deignan. 2006. The grammar of linguistic
metaphors. In A. Stefanowitsch and S. T. Gries,
editors, Corpus-Based Approaches to Metaphor and
Metonymy, Berlin. Mouton de Gruyter.
D. Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
J. Feldman and S. Narayanan. 2004. Embodied mean-
ing in a neural theory of language. Brain and Lan-
guage, 89(2):385?392.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition.
C. J. Fillmore, C. R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International
Journal of Lexicography, 16(3):235?250.
M. Gedigan, J. Bryant, S. Narayanan, and B. Ciric.
2006. Catching metaphors. In In Proceedings of the
3rd Workshop on Scalable Natural Language Un-
derstanding, pages 41?48, New York.
D. Gentner. 1983. Structure mapping: A theoretical
framework for analogy. Cognitive Science, 7:155?
170.
R. Gibbs. 1984. Literal meaning and psychological
theory. Cognitive Science, 8:275?304.
A. Goatly. 1997. The Language of Metaphors. Rout-
ledge, London.
M. Hesse. 1966. Models and Analogies in Science.
Notre Dame University Press.
D. Hofstadter and M. Mitchell. 1994. The Copycat
Project: A model of mental fluidity and analogy-
making. In K.J. Holyoak and J. A. Barnden, editors,
Advances in Connectionist and Neural Computation
Theory, Ablex, New Jersey.
D. Hofstadter. 1995. Fluid Concepts and Creative
Analogies: Computer Models of the Fundamental
Mechanisms of Thought. HarperCollins Publishers.
Y. Karov and S. Edelman. 1998. Similarity-based
word sense disambiguation. Computational Lin-
guistics, 24(1):41?59.
695
P. Kingsbury and M. Palmer. 2002. From TreeBank
to PropBank. In Proceedings of LREC-2002, Gran
Canaria, Canary Islands, Spain.
P. Koivisto-Alanko and H. Tissari. 2006. Sense
and sensibility: Rational thought versus emotion
in metaphorical language. In A. Stefanowitsch
and S. T. Gries, editors, Corpus-Based Approaches
to Metaphor and Metonymy, Berlin. Mouton de
Gruyter.
S. Krishnakumaran and X. Zhu. 2007. Hunting elusive
metaphors using lexical resources. In Proceedings
of the Workshop on Computational Approaches to
Figurative Language, pages 13?20, Rochester, NY.
G. Lakoff and M. Johnson. 1980. Metaphors We Live
By. University of Chicago Press, Chicago.
G. Lakoff, J. Espenson, and A. Schwartz. 1991. The
master metaphor list. Technical report, University
of California at Berkeley.
B. Lo?nneker and C. Eilts. 2004. A Current Re-
source and Future Perspectives for Enriching Word-
Nets with Metaphor Information. In Proceedings
of the Second International WordNet Conference?
GWC 2004, pages 157?162, Brno, Czech Republic.
B. Lo?nneker-Rodman. 2008. The hamburg metaphor
database project: issues in resource creation. Lan-
guage Resources and Evaluation, 42(3):293?318.
B. Lo?nneker. 2004. Lexical databases as resources
for linguistic creativity: Focus on metaphor. In Pro-
ceedings of the LREC 2004 Workshop on Language
Resources for Linguistic Creativity, pages 9?16, Lis-
bon, Portugal.
J. H. Martin. 1988. Representing regularities in the
metaphoric lexicon. In Proceedings of the 12th con-
ference on Computational linguistics, pages 396?
401.
J. H. Martin. 1990. A Computational Model of
Metaphor Interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
J. H. Martin. 1994. Metabank: A knowledge-base of
metaphoric language conventions. Computational
Intelligence, 10:134?149.
J. H. Martin. 2006. A corpus-based analysis of con-
text effects on metaphor comprehension. In A. Ste-
fanowitsch and S. T. Gries, editors, Corpus-Based
Approaches to Metaphor and Metonymy, Berlin.
Mouton de Gruyter.
Z. J. Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
S. Narayanan. 1997. Knowledge-based action repre-
sentations for metaphor and aspect (karma. Tech-
nical report, PhD thesis, University of California at
Berkeley.
S. Narayanan. 1999. Moving right along: A computa-
tional model of metaphoric reasoning about events.
In Proceedings of AAAI 99), pages 121?128, Or-
lando, Florida.
G. Nunberg. 1987. Poetic and prosaic metaphors. In
Proceedings of the 1987 workshop on Theoretical
issues in natural language processing, pages 198?
201.
G. Orwell. 1946. Politics and the english language.
Horizon.
W. Peters and I. Peters. 2000. Lexicalised system-
atic polysemy in wordnet. In Proceedings of LREC
2000, Athens.
Pragglejaz Group. 2007. MIP: A method for iden-
tifying metaphorically used words in discourse.
Metaphor and Symbol, 22:1?39.
A. Reining and B. Lo?nneker-Rodman. 2007. Corpus-
driven metaphor harvesting. In Proceedings of
the HLT/NAACL-07 Workshop on Computational
Approaches to Figurative Language, pages 5?12,
Rochester, New York.
P. Resnik. 1993. Selection and Information: A Class-
based Approach to Lexical Relationships. Ph.D. the-
sis, Philadelphia, PA, USA.
E. Shutova and S. Teufel. 2010. Metaphor corpus an-
notated for source - target domain mappings. In Pro-
ceedings of LREC 2010, Malta.
E. Shutova. 2010. Automatic metaphor interpretation
as a paraphrasing task. In Proceedings of NAACL
2010, Los Angeles, USA.
G. J. Steen. 2007. Finding metaphor in discourse:
Pragglejaz and beyond. Cultura, Lenguaje y Rep-
resentacion / Culture, Language and Representation
(CLR), Revista de Estudios Culturales de la Univer-
sitat Jaume I, 5:9?26.
A. Stefanowitsch. 2006. Corpus-based approaches
to metaphor and metonymy. In A. Stefanowitsch
and S. T. Gries, editors, Corpus-Based Approaches
to Metaphor and Metonymy, Berlin. Mouton de
Gruyter.
L. Sun and A. Korhonen. 2009. Improving verb clus-
tering with automatically acquired selectional pref-
erences. In Proceedings of EMNLP 2009, pages
638?647, Singapore, August.
R. Tourangeau and R. Sternberg. 1982. Understand-
ing and appreciating metaphors. Cognition, 11:203?
244.
T. Veale and Y. Hao. 2008. A fluid knowledge repre-
sentation for understanding and generating creative
metaphors. In Proceedings of COLING 2008, pages
945?952, Manchester, UK.
696
A.M.Wallington, J. A. Barnden, P. Buchlovsky, L. Fel-
lows, and S. R. Glasbey. 2003. Metaphor annota-
tion: A systematic study. Technical report, School
of Computer Science, The University of Birming-
ham.
Y. Wilks. 1975. A preferential pattern-seeking seman-
tics for natural language inference. Artificial Intelli-
gence, 6:53?74.
Y. Wilks. 1978. Making preferences more active. Ar-
tificial Intelligence, 11(3):197?223.
697
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 276?285, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Metaphor Identification as Interpretation
Ekaterina Shutova
International Computer Science Institute and
Institute for Cognitive and Brain Sciences
University of California, Berkeley
katia@berkeley.edu
Abstract
Automatic metaphor identification and inter-
pretation in text have been traditionally con-
sidered as two separate tasks in natural lan-
guage processing (NLP) and addressed in-
dividually within computational frameworks.
However, cognitive evidence suggests that hu-
mans are likely to perform these two tasks si-
multaneously, as part of a holistic metaphor
comprehension process. We present a novel
method that performs metaphor identification
through its interpretation, being the first one
in NLP to combine the two tasks in one
step. It outperforms the previous approaches
to metaphor identification both in terms of ac-
curacy and coverage, as well as providing an
interpretation for each identified expression.
1 Introduction
Metaphor undoubtedly gives our expression more
vividness, distinction and artistry, however, it is also
an important linguistic tool that has long become
part of our every-day language. Metaphors arise
when one concept or domain is viewed in terms
of the properties of another (Lakoff and Johnson,
1980). Consider the examples in (1) and (2).
(1) My car drinks gasoline. (Wilks, 1978)
(2) This policy is strangling business.
The car in (1) and business in (2) are viewed as
living beings and thus they can drink or be stran-
gled respectively. The mapping between the car
(the target concept) and living being (the source
concept) is systematic and results in a number of
metaphorical expressions (e.g. ?This oil gives your
car a second life?, ?this car has is very temperamen-
tal? etc.) Lakoff and Johnson call such generalisa-
tions a source?target domain mapping, or concep-
tual metaphor.
The ubiquity of metaphor in language has been
established in a number of corpus studies (Cameron,
2003; Martin, 2006; Steen et al, 2010; Shutova
and Teufel, 2010) and the role it plays in human
reasoning has been confirmed in psychological ex-
periments (Thibodeau and Boroditsky, 2011). This
makes its automatic processing an important prob-
lem for NLP and its numerous applications (such
as machine translation, information extraction, opin-
ion mining and many others). For example, the
use of the metaphorical verb strangle in (2) reflects
the speaker?s negative opinion regarding the gov-
ernment?s tight business regulations, which would
be an important fact for an opinion mining system
to discover (Narayanan, 1999). Other experiments
(Agerri, 2008) have investigated and confirmed the
role of metaphor interpretation for textual entailment
resolution (RTE).
The problem of metaphor modeling is rapidly
gaining interest within NLP, with a growing number
of approaches exploiting statistical techniques (Ma-
son, 2004; Gedigian et al, 2006; Shutova, 2010;
Shutova et al, 2010; Turney et al, 2011; Shutova
et al, 2012a). Compared to more traditional ap-
proaches based on hand-coded knowledge (Fass,
1991; Martin, 1990; Narayanan, 1997; Narayanan,
1999; Feldman and Narayanan, 2004; Barnden and
Lee, 2002; Agerri et al, 2007), these more recent
methods tend to have a wider coverage, as well as be
more efficient, accurate and robust. However, even
the statistical metaphor processing approaches so far
often focused on a limited domain or a subset of
276
phenomena (Gedigian et al, 2006; Krishnakumaran
and Zhu, 2007), and required training data (Shutova
et al, 2010; Turney et al, 2011), often resulting in
a limited coverage. The metaphor processing task
itself has been most commonly addressed in NLP
as two individual subtasks: metaphor identification
and metaphor interpretation, with the systems focus-
ing only on one of them at a time, or at best comb-
ing the two in a pipeline (Shutova et al, 2012a).
Metaphor identification systems annotate metaphor-
ical language in text, and metaphor interpretation
systems discover literal meanings of the previously
annotated expressions. However, cognitive evidence
suggests that humans are likely to perform identifi-
cation and interpretation simultaneously, as part of
a holistic metaphor comprehension process (Coul-
son, 2008; Utsumi, 2011; Gibbs and Colston, 2012).
In this paper, we also take this stance and present the
first computational method that identifies metaphori-
cal expressions in unrestricted text by means of their
interpretation. Following Shutova (2010), we define
metaphor interpretation as a task of finding a literal
paraphrase for a metaphorically used word and in-
troduce the concept of symmetric reverse paraphras-
ing as a criterion for metaphor identification. The
main assumption behind our method is that the lit-
eral paraphrases of literally-used words should yield
the original phrase when paraphrased in reverse. For
example, when the expression ?clean the house? is
paraphrased as ?tidy the house?, the reverse para-
phrasing of tidy would generate clean. Our expec-
tation is that such a symmetry in paraphrasing is
indicative of literal use. The metaphorically-used
words are unlikely to exhibit this symmetry prop-
erty when paraphrased in reverse. For example, the
literal paraphrasing of the verb stir in ?stir excite-
ment? would yield ?provoke excitement?, but the
reverse paraphrasing of provoke would not retrieve
stir, indicating the non-literal use of stir.
We experimentally verify this hypothesis in a set-
ting involving single-word metaphors expressed by
a verb in verb-subject and verb-direct object rela-
tions. We apply the selectional preference-based
metaphor paraphrasing method of Shutova (2010) to
retrieve literal paraphrases of all input verbs and ex-
tend the method to perform metaphor identification.
In summary, our system (1) determines the likeli-
hood of a verb being metaphorical based on its selec-
tional preference strength (Resnik, 1993); (2) identi-
fies a set of literal paraphrases for verbs that may be
used metaphorically using the algorithm of Shutova
(2010); (3) performs reverse paraphrasing of each
of the identified paraphrases, aiming to retrieve the
original expression; and (4) if the original expres-
sion is retrieved then the verb is tagged as literal,
otherwise it is tagged as metaphorical.
We evaluated the performance of the system using
the manually annotated metaphor corpus of Shutova
and Teufel (2010) in precision- and recall-oriented
settings. In addition, we compared its performance
to that of a baseline using selectional preference vi-
olation as an indicator of metaphor, as well as to
two previous metaphor identification approaches of
Shutova et al (2010) and Turney et al (2011).
2 Related Work
One of the first attempts to identify and interpret
metaphorical expressions in text is the met* sys-
tem of Fass (1991), that utilizes hand-coded knowl-
edge and detects non-literalness via selectional pref-
erence violation. In case of a violation, the re-
spective phrase is first tested for being metonymic
using hand-coded patterns (e.g. CONTAINER-FOR-
CONTENT). If this fails, the system searches the
knowledge base for a relevant analogy in order to
discriminate metaphorical relations from anomalous
ones. The system of Krishnakumaran and Zhu
(2007) uses WordNet (the hyponymy relation) and
word bigram counts to predict verbal, nominal and
adjectival metaphors at the sentence level. The au-
thors discriminate between conventional metaphors
(included in WordNet) and novel metaphors. Birke
and Sarkar (2006) present a sentence clustering ap-
proach that employs a set of seed sentences an-
notated for literalness and computes similarity be-
tween the new input sentence and all of the seed sen-
tences. The system then tags the sentence as literal
or metaphorical according to the annotation in the
most similar seeds, attaining an f-score of 53.8%.
The first system to discover source?target domain
mappings automatically is CorMet (Mason, 2004).
It does this by searching for systematic variations
in domain-specific verb selectional preferences. For
example, pour is a characteristic verb in both LAB
and FINANCE domains. In the LAB domain it has
277
a strong preference for liquids and in the FINANCE
domain for money. From this the system infers the
domain mapping FINANCE ? LAB and the concept
mapping money ? liquid. Gedigian et al (2006)
trained a maximum entropy classifier to discrimi-
nate between literal and metaphorical use. They
annotated the sentences from PropBank (Kingsbury
and Palmer, 2002) containing the verbs of MOTION
and CURE for metaphoricity. They used PropBank
annotation (arguments and their semantic types) as
features for classification and report an accuracy
of 95.12% (however, against a majority baseline of
92.90%). The metaphor identification system of
Shutova et al (2010) starts from a small seed set
of metaphorical expressions, learns the analogies in-
volved in their production and extends the set of
analogies by means of verb and noun clustering. As
a result, the system can recognize new metaphorical
expressions in unrestricted text (e.g. from the seed
?stir excitement? it infers that ?swallow anger? is
also a metaphor), achieving a precision of 79%.
Turney et al (2011) classify verbs and adjectives
as literal or metaphorical based on their level of con-
creteness or abstractness in relation to a noun they
appear with. They learn concreteness rankings for
words automatically (starting from a set of exam-
ples) and then search for expressions where a con-
crete adjective or verb is used with an abstract noun
(e.g. ?dark humour? is tagged as a metaphor and
?dark hair? is not). They report an accuracy of 73%.
3 Method
3.1 Selectional Preference Strength Filtering
One of the early influential ideas in the field of com-
putational metaphor processing is that metaphor rep-
resents a violation of selectional preferences (SP)
of a word in a given context (Wilks, 1975; Wilks,
1978). However, applied directly as an identifica-
tion criterion, violation of SPs is also indicative of
many other linguistic phenomena (e.g. metonymy),
and not only metaphor, which is problematic. We
modify this view and apply it to measure the poten-
tial of a word to be used metaphorically based on its
selectional preference strength (SPS). The main in-
tuition behind SPS filtering is that not all verbs have
an equal potential of being a metaphor. For example,
verbs such as choose, remember, describe or like do
not have a strong preference for their direct objects
and are equally likely to appear with many argument
classes. If metaphor represents a violation of SPs,
then the verbs with weak SPS are unlikely to be used
metaphorically in any context. For every verb in the
input text, the filter determines their likelihood of
being a metaphor based on their SPS and discards
the weak ones. The SPS filter is context-free, and
the reverse paraphrasing method is then applied in
the next steps to determine if the remaining verbs
are indeed used metaphorically in the given context.
We automatically acquired selectional preference
distributions for verb-subject and verb-direct object
relations from the British National Corpus (BNC)
(Burnard, 2007) that was parsed using the RASP
parser (Briscoe et al, 2006; Andersen et al, 2008).
We applied the noun clustering method of Sun and
Korhonen (2009) to 2000 most frequent nouns in
the BNC to obtain 200 common selectional prefer-
ence classes. To quantify selectional preferences, we
adopted the SPS measure of Resnik (1993). Resnik
defines SPS of a verb as the difference between the
posterior distribution of noun classes in a particular
relation with the verb and their prior distribution in
that syntactic position (regardless of the verb). He
quantifies this difference using the Kullback-Leibler
divergence:
SR(v) = D(P (c|v)||P (c)) =
?
c
P (c|v) log
P (c|v)
P (c)
,
(1)
where P (c) is the prior probability of the noun class,
P (c|v) is the posterior probability of the noun class
given the verb and R is the grammatical relation.
We calculated SPS for verb-subject and verb-
direct object grammatical relations. The optimal se-
lectional preference strength thresholds were set ex-
perimentally on a small heldout dataset at 0.30 for
verb-subject and 0.70 for verb-direct object relations
(via qualitative analysis of the data). The system ex-
cludes expressions containing the verbs with prefer-
ence strength below these thresholds from the set of
candidate metaphors. Examples of verbs with weak
direct object SPs include e.g. imagine, avoid, con-
tain, dislike, make, admire, separate, remember and
the strong SPs are exhibited by e.g. sip, hobble, roar,
hoover, slam, skim, drink etc.
278
3.2 Literal Paraphrasing
The verbs that can be used metaphorically ac-
cording to the SPS filter are then paraphrased us-
ing the context-based literal paraphrasing method
of Shutova (2010). While Shutova only used
the method to paraphrase manually annotated
metaphors, we extend and apply the method to para-
phrasing of literally used terms and metaphor identi-
fication, eliminating the need for manual annotation
of metaphorical expressions.
The system takes verbs and their context in the
form of subject and direct-object relations as input.
It generates a list of possible paraphrases of the verb
that can occur in the same context and ranks them
according to their likelihood, as derived from the
corpus. It then identifies shared features of the para-
phrases and the verb using the WordNet (Fellbaum,
1998) hierarchy and removes unrelated concepts. It
then identifies literal paraphrases among the remain-
ing candidates based on the verb?s automatically in-
duced selectional preferences and the properties of
the context.
3.2.1 Context-based Paraphrase Ranking
Following Shutova (2010), we compute the like-
lihood L of a particular paraphrase of the verb
v as a joint probability of the paraphrase i co-
occurring with the other lexical items from its con-
text w1, ..., wN in syntactic relations r1, ..., rN .
Li = P (i, (w1, r1), (w2, r2), ..., (wN , rN )). (2)
Assuming statistical independence between the rela-
tions of the terms in a phrase, we obtain:
P (i, (w1, r1), (w2, r2), ..., (wN , rN )) =
P (i) ? P ((w1, r1)|i) ? ... ? P ((wN , rN )|i).
(3)
The probabilities can be calculated using maxi-
mum likelihood estimation as P (i) = f(i)?
k f(ik)
and P (wn, rn|i) =
f(wn,rn,i)
f(i) , where f(i) is the
frequency of the interpretation irrespective of its
arguments,
?
k f(ik) is the number of times its
part of speech class is attested in the corpus and
f(wn, rn, i) is the number of times the interpreta-
tion co-occurs with context word wn in relation rn.
By performing appropriate substitutions into (3), we
obtain:
P (i, (w1, r1), (w2, r2), ..., (wN , rN )) =
f(i)
?
k f(ik)
?
f(w1, r1, i)
f(i)
? ... ?
f(wN , rN , i)
f(i)
=
?N
n=1 f(wn, rn, i)
(f(i))N?1 ?
?
k f(ik)
.
(4)
This model is then used to rank the candidate sub-
stitutes of the verb v in the fixed context according
to the data. The parameters of the model were esti-
mated from the RASP-parsed BNC using the gram-
matical relations output created by Andersen et al
(2008). The goal of this model is to emphasize the
paraphrases that match the context of the verb in the
sentence best.
3.2.2 WordNet Filter
After obtaining the initial list of possible substi-
tutes for the verb v, the system filters out the terms
whose meanings do not share any common proper-
ties with that of the verb. This overlap of properties
is identified using the hyponymy relation in Word-
Net. Within the initial list of paraphrases, the sys-
tem selects the terms that are hypernyms of the verb
v, or share a common hypernym with it. Follow-
ing Shutova, we restrict the hypernym search to a
depth of three levels in the taxonomy. Table 1 shows
the filtered lists of paraphrases for the expressions
?stir excitement? and ?campaign surged?. The goal
of the filter is to discard unrelated paraphrases and
thus ensure the meaning retention during paraphras-
ing. Note, however, that we define meaning reten-
tion broadly, as sharing a set of similar basic prop-
erties. Such a broad definition distinguishes our sys-
tem from other WordNet-based approaches to lexi-
cal substitution (McCarthy and Navigli, 2007) and
allows for a transition from metaphorical to literal
language, while preserving the original meaning.
3.2.3 SP-based Re-ranking
The lists of paraphrases which were generated as
described above contain some irrelevant paraphrases
(e.g. ?campaign lifted? for ?campaign surged?) and
some metaphorically-used paraphrases (e.g. ?cam-
paign soared?). However, our aim is to identify lit-
eral paraphrases among the candidates. Shutova?s
method uses selectional preferences of the candi-
279
Log-likelihood Paraphrase
Verb-DirectObject
stir excitement:
-14.28 create
-14.84 provoke
-15.53 make
-15.53 elicit
-15.53 arouse
-16.23 stimulate
-16.23 raise
-16.23 excite
-16.23 conjure
Subject-Verb
campaign surge:
-13.01 run
-15.53 improve
-16.23 soar
-16.23 lift
Table 1: The list of paraphrases with the initial ranking
dates for this purpose. Candidates used metaphor-
ically are likely to demonstrate semantic preference
for the source domain, e.g. soar would select for
birds or flying devices as its subject rather than cam-
paigns (the target domain), whereas the ones used
literally would have a higher preference for the tar-
get domain. This is yet another modification of
Wilks? SP violation view of metaphor. Shutova
(2010) has previously shown that selecting the para-
phrases whose preferences the noun in the context
matches best allows to filter out non-literalness, as
well as unrelated terms.
As in case of the SPS filter, we automatically
acquired selectional preference distributions of the
verbs in the paraphrase lists (for verb-subject and
verb-direct object relations) from the RASP-parsed
BNC. In order to quantify how well a particular ar-
gument class fits the verb, we adopted the selectional
association measure proposed by Resnik (1993). Se-
lectional association is defined as follows:
AR(v, c) =
1
SR(v)
P (c|v) log
P (c|v)
P (c)
, (5)
where P (c) is the prior probability of the noun class,
P (c|v) is the posterior probability of the noun class
given the verb and SR is the overall selectional pref-
erence strength of the verb in the grammatical rela-
tion R.
We use selectional association as a measure of
semantic fitness of the paraphrases into the con-
Association Paraphrase
Verb-DirectObject
stir excitement:
0.0696 provoke
0.0245 elicit
0.0194 arouse
0.0061 conjure
0.0028 create
0.0001 stimulate
? 0 raise
? 0 make
? 0 excite
Subject-Verb
campaign surge:
0.0086 improve
0.0009 run
? 0 soar
? 0 lift
Table 2: The list of paraphrases re-ranked using SPs
text, which stands for their literalness. The para-
phrases are re-ranked based on their selectional as-
sociation with the noun in the context. The incor-
rect or metaphorical paraphrases are de-emphasized
within this ranking. The new ranking is shown in
Table 2. While the model in 3.2.1 selected the can-
didate paraphrases that match the context better than
all other candidates, the SP model emphasizes the
paraphrases that match this particular context better
than any other context they may appear in. Shutova?s
experiments have shown that the paraphrase in rank
1 (i.e. the verb with which the noun in the context
has the highest selectional association) represents a
literal interpretation in 81% of all cases. Such a level
of accuracy makes Shutova?s method state-of-the-art
in metaphor paraphrasing. We now apply it to the
task of metaphor identification.
3.3 Reverse Paraphrasing
At the heart of our approach to metaphor iden-
tification is the concept of reverse paraphrasing.
The main intuition behind it is that when literally-
used words are paraphrased with their literal substi-
tutes, the reverse literal paraphrasing of that substi-
tute should yield the original expression as one of
the candidates. This is, however, not the case for
metaphor, since its literal paraphrase would yield
another literal expression via literal paraphrasing.
We ran the above paraphrasing method on every
verb in the input text and then again on the top
280
Original expression Lit. paraphrase Reverse paraphrase
Verb-DirectObject
stir excitement provoke: elicit, arouse,
cause, create,
stimulate, raise,
make
elicit: provoke, arouse,
see, derive, create,
raise, make
buy a dress
get: change, find, buy,
purchase, take, hit,
alter, ...
purchase: get, buy
Subject-Verb
campaign surge improve: change, turn
run: succeed, direct,
continue, lead, last,
win, extend, ...
prisoner escape flee: escape, run
get: drive, go, turn,
transfer, arrive,
bring, come, ...
Table 3: The list of top two literal paraphrases and their
reverse paraphrases, as identified by the system
two paraphrases it produces. If this process resulted
in retrieving the original expression then the latter
was tagged as literal, otherwise it was tagged as
metaphorical. Some examples of reverse paraphras-
ing results are given in Table 3. One can see from
the table that when the metaphorical verb stir in ?stir
excitement? is paraphrased as the literal ?provoke?,
the subsequent paraphrasing of ?provoke? does not
produce ?stir?. In contrast, when the literal expres-
sion ?buy a dress? is paraphrased as ?purchase?, the
reverse paraphrasing generates ?buy? as one of the
candidates, indicating the literalness of the original
expression. The same is true for the metaphorical
surge in ?campaign surged? and the literal escape in
?the prisoner escaped?.
4 Evaluation and Discussion
4.1 Baseline
The baseline system is the implementation of the se-
lectional preference violation view of Wilks (1978)
using automatically induced SPs. Such a choice of a
baseline allows us to compare our own modifications
of the SP violation view to the original approach of
Wilks in a computational setting, as well as evaluate
the latter on real-world data. Another motivation be-
hind this choice is that the symmetry of reverse para-
phrasing can be seen as a kind of ?normality? test, in
a similar way as the satisfied selectional preferences
are in Wilk?s approach. However, we believe that
the SP-based reverse paraphrasing method captures
significantly more information than SP violations do
and thus compare the performance of the two meth-
ods in an experimental setting.
The baseline SP classes were created as described
above and the preferences were quantified using se-
lectional association as a measure. The baseline sys-
tem then classified the instances where selectional
association of the verb and the noun in the phrase
were below a certain threshold, as metaphorical.
We determined the optimal threshold by qualitative
analysis of the selectional preference distributions of
50 verbs of different frequency and SPS (through the
analysis of literally and metaphorically-used argu-
ments). The threshold was averaged over individual
verbs? thresholds and equals 0.07 for direct object
relations, and 0.09 for subject relations.
4.2 Evaluation Corpus
We evaluated the system and the baseline against the
corpus of Shutova and Teufel (2010), that was man-
ually annotated for metaphorical expressions. The
corpus is a 14,000-word subset of the BNC, with
the texts selected to retain the original balance of
genre in the BNC itself. The corpus contains ex-
tracts from fiction, newspaper text, radio broadcast
(transcribed speech), essays and journal articles on
politics, social science and literature. Shutova and
Teufel (2010) identified 241 metaphorical expres-
sions in the corpus, out of which 164 were verbal
metaphors.
We parsed the corpus using the RASP parser and
extracted subject and direct object relations from its
output. Among the direct object relations there were
310 literal phrases and 79 metaphorical ones; and
among the subject relations 206 were literal and 67
metaphorical. This constitutes a dataset of 662 rela-
tions for the systems to classify.
4.3 Results and Discussion
The system and baseline performance was evaluated
against the corpus in terms of precision and recall.
Precision, P , measures the proportion of metaphor-
ical expressions that were tagged correctly among
281
Relation Bsln P System P Bsln R System R
Verb-DObj 0.20 0.69 0.52 0.63
Verb-Subj 0.13 0.66 0.59 0.70
Average 0.17 0.68 0.55 0.66
Table 4: Baseline and system performance by relation
the ones that were tagged by the system. Recall,
R, measures the proportion of metaphorical expres-
sions that were identified out of all metaphorical ex-
pressions in the gold standard corpus. The system
P = 0.68 and R = 0.66, whereas the baseline only
attains P = 0.17 and R = 0.55. System perfor-
mance by relation is shown in Table 4. The hu-
man ceiling for this task, according to the annotation
experiments of Shutova and Teufel (2010) approxi-
mates to P = 0.80. Figure 1 shows example sen-
tences with metaphors identified and paraphrased by
the system. Table 5 provides a breakdown of the an-
notated instances into true / false positives and true
/ false negatives. As one can see from the table, the
systems can accurately annotate both metaphorical
and literal expressions, providing a balance between
precision and recall.
The system outperforms the baseline for both
verb-subject and verb-direct object constructions.
Its performance is also close to the previous
metaphor identification systems of Turney et al
(2011) (accuracy of 0.73) and Shutova et al (2010)
(precision of 0.79), however, the results are not di-
rectly comparable due to different experimental set-
tings. Our method has a strong advantage over the
system of Shutova et al (2010) in terms of cover-
age: the latter system heavily relied on manually an-
notated seed metaphors which limited its applicabil-
ity in unrestricted text to the set of topics covered by
the seeds. As opposed to this, our method is domain-
independent and can be applied to any data. Shutova
et al (2010) have not measured the recall of their
system, however indicated its possible coverage lim-
itations.
In addition, our system produces paraphrases for
the identified metaphorical expressions. Since the
identification is directly dependent on the quality
of literal paraphrasing, the majority of the inter-
pretations the system provided for the identified
metaphors appear to be correct. However, we found
a few instances where, despite the correct initial
paraphrasing, the system was not able to identify
FYT Gorbachev inherited a Soviet state which was, in
a celebrated Stalinist formulation, national in form but
socialist in content.
Paraphrase: Gorbachev received a Soviet state which
was, in a celebrated Stalinist formulation, national in
form but socialist in content.
CEK The Clinton campaign surged again and he easily
won the Democratic nomination.
Paraphrase: The Clinton campaign improved again and
he easily won the Democratic nomination.
CEK Their views reflect a lack of enthusiasm among
the British people at large for John Major ?s idea of Eu-
ropean unity.
Paraphrase: Their views show a lack of enthusiasm
among the British people at large for John Major ?s idea
of European unity.
J85 [..] the reasons for this superiority are never spelled
out.
Paraphrase [..] the reasons for this superiority are never
specified.
J85 Anyone who has introduced speech act theory to
students will know that these technical terms are not at
all easy to grasp.
Paraphrase: Anyone who has introduced speech act the-
ory to students will know that these technical terms are
not at all easy to understand.
G0N The man?s voice cut in .
Paraphrase: The man?s voice interrupted.
Figure 1: Metaphors tagged by the system (in bold) and
their paraphrases
the metaphor, usually in case of highly convention-
alized metaphorical expressions. Overall, the most
frequent system errors fall into the following cate-
gories:
Errors due to incorrect parsing: The system failed
to discover some of the metaphorical expressions in
the corpus since their grammatical relations were
missed by the parser. In addition, some of the in-
stances were misclassified, e.g. ?pounds paid to
[...]? or ?change was greatly accelerated? were la-
beled as subject relations. Overall, the parser missed
9 metaphorical expressions.
Errors due to incorrect paraphrasing: The most
common type of error that leads to false positives is
the incorrect paraphrasing (resulting in a change of
meaning). This makes it nearly impossible for the
system to retrieve the original term. There were also
282
Positives Negatives Total
True 99 464 563
False 47 52 99
Total 146 516
Table 5: System tagging statistics
cases where the system could not generate any para-
phrase (usually for literal expressions, e.g. ?play an
anthem?).
Errors due to metaphorical paraphrasing: Some
of the system errors are due to metaphorical para-
phrasing. For example, the metaphorical expression
?mend marriage? was paraphrased as ?repair mar-
riage?, which is also used metaphorically. And re-
pair in return generated mend, when paraphrased in
reverse. Errors of this type have been mainly trig-
gered by the WordNet filter, and the fact that some
metaphorical senses are included in WordNet.
Errors due to metaphor conventionality: a num-
ber of conventional metaphors were missed by the
system, since the original verb was retrieved due to
its conventionality. Such examples include ?impose
a decision?, ?put the issue forward?, ?lead a life?.
Such cases suggest that the system is better suited to
identify more creative, novel metaphors.
Cases of metonymy: a few cases of gen-
eral metonymy were annotated by the system as
metaphorical, e.g. ?shout support?, which stands for
?shout the words of support?, and ?humiliate a mo-
ment?, that is likely to mean ?humiliate the event of
the moment?. However, there were only 4 errors of
this type in the data.
Baseline Errors: The output of the baseline exhib-
ited two main types of error. The first stemmed from
the conventionality of many metaphorical expres-
sions, which resulted in their literal annotation. Con-
ventionality leads to high selectional association for
verbs with their metaphorical arguments, e.g. em-
brace has {view, ideology, conception etc.} class as
its top ranked direct object argument with the selec-
tional association of 0.18. The second type of error
was the system selecting many language anomalies
that violate selectional preferences and tagging these
as metaphors. This resulted in a high number of false
positives.
5 Conclusions and Future Directions
Previous research on metaphor addressed a num-
ber of its aspects using both symbolic and statisti-
cal techniques. While some of this work met with
success with respect to precision in metaphor an-
notation, the methods often focused on a limited
domain and needed manually-labeled training data.
Their dependence on manually annotated training
data made the systems hard to scale. As a result,
many of these systems are not directly applicable to
aid real-world NLP due to their limited coverage. In
contrast, our method does not require any manually-
labeled data, which makes it more robust and appli-
cable to a wide range of genres. It is also the first
one to perform accurate metaphor identification and
interpretation in one step, as opposed to the previ-
ous systems focusing on one part of the task only.
It identifies metaphor with a precision of 68% and
a recall of 66%, which is a very encouraging result.
We believe that this work has important implications
for computational modeling of metaphor, and is rel-
evant to a range of other semantic tasks within NLP.
Although we have so far tested our system on
verb-subject and verb-object metaphors only, we be-
lieve that the described identification and paraphras-
ing techniques can be similarly applied to a wider
range of syntactic constructions. Extending the sys-
tem to deal with more parts of speech and types of
phrases (e.g. nominal and adjectival metaphors) is
part of our future work.
Another promising future research avenue is inte-
grating the techniques with unsupervised paraphras-
ing and lexical substitution methods, using e.g. dis-
tributional similarity measures (Pucci et al, 2009;
McCarthy et al, 2010) or vector space models of
word meaning (Erk and Pado?, 2008; Erk and Pado?,
2009; De Cao and Basili, 2009; Shutova et al,
2012b). These methods could fully or partly replace
the WordNet filter in the detection of similar basic
features of the concepts, or add useful information
to it. Fully replacing the WordNet filter by an un-
supervised method would make the system more ro-
bust and more easily portable across domains and
genres. This may also eliminate some of the system
errors that arise from the inconsistent sense annota-
tion and the inclusion of some metaphorical senses
in WordNet.
283
Acknowledgments
This work was supported by the ICSI MetaNet
project (grant number W911NF-12-C-0022). Many
thanks to Srini Narayanan, Eve Sweetser and Jerry
Feldman for their advice and feedback.
References
Rodrigo Agerri, John Barnden, Mark Lee, and Alan
Wallington. 2007. Metaphor, inference and domain-
independent mappings. In Proceedings of RANLP-
2007, pages 17?23, Borovets, Bulgaria.
Rodrigo Agerri. 2008. Metaphor in textual entailment.
In Proceedings of COLING 2008, pages 3?6, Manch-
ester, UK.
Oistein Andersen, Julien Nioche, Ted Briscoe, and John
Carroll. 2008. The BNC parsed with RASP4UIMA.
In Proceedings of LREC 2008, pages 865?869, Mar-
rakech, Morocco.
John Barnden and Mark Lee. 2002. An artificial intel-
ligence approach to metaphor understanding. Theoria
et Historia Scientiarum, 6(1):399?412.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In In Proceedings of EACL-06, pages
329?336.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions, pages 77?80.
Lou Burnard. 2007. Reference Guide for the British Na-
tional Corpus (XML Edition).
Lynne Cameron. 2003. Metaphor in Educational Dis-
course. Continuum, London.
Seana Coulson. 2008. Metaphor comprehension and the
brain. In R.W. Gibbs, editor, Metaphor and Thought,
Cambridge. Cambridge University Press.
Diego De Cao and Roberto Basili. 2009. Combining dis-
tributional and paradigmatic information in a lexical
substitution task. In Proceedings of EVALITA work-
shop, 11th Congress of Italian Association for Artifi-
cial Intelligence.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Waikiki, Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase as-
sessment in structured vector space: exploring param-
eters and datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language Semantics,
pages 57?65. Association for Computational Linguis-
tics.
Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Jerome Feldman and Srini Narayanan. 2004. Embodied
meaning in a neural theory of language. Brain and
Language, 89(2):385?392.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In In Proceed-
ings of the 3rd Workshop on Scalable Natural Lan-
guage Understanding, pages 41?48, New York.
Raymond W. Gibbs and Herbert L. Colston. 2012. In-
terpreting Figurative Meaning. Cambridge University
Press.
Paul Kingsbury and Martha Palmer. 2002. From
TreeBank to PropBank. In Proceedings of LREC-
2002, pages 1989?1993, Gran Canaria, Canary Is-
lands, Spain.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, NY.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press, Chicago.
James Martin. 1990. A Computational Model of
Metaphor Interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
James Martin. 2006. A corpus-based analysis of con-
text effects on metaphor comprehension. In A. Ste-
fanowitsch and S. T. Gries, editors, Corpus-Based Ap-
proaches to Metaphor and Metonymy, Berlin. Mouton
de Gruyter.
Zachary Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th workshop on Semantic Evaluations
(SemEval-2007), pages 48?53.
Diana McCarthy, Bill Keller, and Roberto Navigli. 2010.
Getting synonym candidates from raw data in the en-
glish lexical substitution task. In Proceedings of the
14th EURALEX International Congress, Leeuwarden,
The Netherlands.
Srini Narayanan. 1997. Knowledge-based Action Repre-
sentations for Metaphor and Aspect (KARMA). Tech-
nical report, PhD thesis, University of California at
Berkeley.
284
Srini Narayanan. 1999. Moving right along: A compu-
tational model of metaphoric reasoning about events.
In Proceedings of AAAI 99), pages 121?128, Orlando,
Florida.
Dario Pucci, Marco Baroni, Franco Cutugno, and
Alessandro Lenci. 2009. Unsupervised lexical sub-
stitution with a word space model. In Proceedings of
EVALITA workshop, 11th Congress of Italian Associ-
ation for Artificial Intelligence.
Philip Resnik. 1993. Selection and Information: A
Class-based Approach to Lexical Relationships. Ph.D.
thesis, Philadelphia, PA, USA.
Ekaterina Shutova and Simone Teufel. 2010. Metaphor
corpus annotated for source - target domain map-
pings. In Proceedings of LREC 2010, pages 3255?
3261, Malta.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of Coling 2010, pages 1002?1010,
Beijing, China.
Ekaterina Shutova, Simone Teufel, and Anna Korhonen.
2012a. Statistical Metaphor Processing. Computa-
tional Linguistics, 39(2).
Ekaterina Shutova, Tim Van de Cruys, and Anna Korho-
nen. 2012b. Unsupervised metaphor paraphrasing us-
ing a vector space model. In Proceedings of COLING
2012, Mumbai, India.
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings of
NAACL 2010, pages 1029?1037, Los Angeles, USA.
Gerard J. Steen, Aletta G. Dorst, J. Berenike Herrmann,
Anna A. Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identifica-
tion: From MIP to MIPVU. John Benjamins, Ams-
terdam/Philadelphia.
Lin Sun and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired selectional
preferences. In Proceedings of EMNLP 2009, pages
638?647, Singapore, August.
Paul H. Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in rea-
soning. PLoS ONE, 6(2):e16782, 02.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 680?690, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Akira Utsumi. 2011. Computational exploration of
metaphor comprehension processes using a semantic
space model. Cognitive Science, 35(2):251?296.
Yorick Wilks. 1975. A preferential pattern-seeking se-
mantics for natural language inference. Artificial In-
telligence, 6:53?74.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
285
