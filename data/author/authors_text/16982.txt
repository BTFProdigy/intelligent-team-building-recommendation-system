First Joint Conference on Lexical and Computational Semantics (*SEM), pages 631?634,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Three Approaches for Semantic Textual Similarity
Maya Carrillo, Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{cmaya, darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we describe the three approaches
we submitted to the Semantic Textual Similar-
ity task of SemEval 2012. The first approach
considers to calculate the semantic similar-
ity by using the Jaccard coefficient with term
expansion using synonyms. The second ap-
proach uses the semantic similarity reported
by Mihalcea in (Mihalcea et al, 2006). The
third approach employs Random Indexing and
Bag of Concepts based on context vectors. We
consider that the first and third approaches ob-
tained a comparable performance, meanwhile
the second approach got a very poor behav-
ior. The best ALL result was obtained with
the third approach, with a Pearson correlation
equal to 0.663.
1 Introduction
Finding the semantic similarity between two sen-
tences is very important in applications of natural
language processing such as information retrieval
and related areas. The problem is complex due to the
small number of terms involved in sentences which
are tipically less than 10 or 15. Additionally, it is re-
quired to ?understand? the meaning of the sentences
in order to determine the ?semantic? similarity of
texts, which is quite different of finding the lexical
similarity.
There exist different works at literature dealing
with semantic similarity, but the problem is far to
be solved because of the aforementioned issues.
In (Mihalcea et al, 2006), for instance, it is pre-
sented a method for measuring the semantic simi-
larity of texts, using corpus-based and knowledge-
based measures of similarity. The approaches pre-
sented in (Shrestha, 2011) are based on the Vector
Space Model, with the aim to capture the contex-
tual behavior, senses and correlation, of terms. The
performance of the method is better than the base-
line method that uses vector based cosine similarity
measure.
In this paper, we present three different ap-
proaches for the Textual Semantic Similarity task of
Semeval 2012 (Agirre et al, 2012). The task is de-
scribed as follows: Given two sentences s1 and s2,
the aim is to compute how similar s1 and s2 are,
returning a similarity score, and an optional confi-
dence score. The approaches should provide values
between 0 and 5 for each pair of sentences. These
values roughly correspond to the following consid-
erations, even when the system should output real
values:
5: The two sentences are completely equivalent,
as they mean the same thing.
4: The two sentences are mostly equivalent, but
some unimportant details differ.
3: The two sentences are roughly equivalent, but
some important information differs/missing.
2: The two sentences are not equivalent, but share
some details.
1: The two sentences are not equivalent, but are
on the same topic.
0: The two sentences are on different topics.
631
The description of the runs submitted to the com-
petition follows.
2 Experimentation setup
The three runs submitted to the competition use
completely different mechanisms to find the degree
of semantic similarity between two sentences. The
approaches are described as follows:
2.1 Approach BUAP-RUN-1: Term expansion
with synonyms
Let s1 = w1,1w1,2...w1,|s1| and s2 =
w2,1w2,2...w2,|s2| be two sentences. The synonyms
of a given word wi,k, expressed as synonyms(wi,k),
are obtained from online dictionaries by extracting
the synonyms of wi,k. A better matching between
the terms contained in the text fragments and the
terms at the dictionary are obtained by stemming all
the terms (using the Porter stemmer).
In order to determine the semantic similarity be-
tween any pair of terms of the two sentences (w1,i
and w2,j) we use Eq. (1).
sim(w1,i, w2,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w2,j) ||
w1,i ? synonyms(w2,j) ||
w2,j ? synonyms(w1,i)
0 otherwise
(1)
The similarity between sentences s1 and s2 is cal-
culated as shown in Eq. (2).
similarity(s1, s2) =
5 ? ?ni=1
?n
j=1 sim(w1i, w2j)
|s1 ? s2|
(2)
2.2 Approach BUAP-RUN-2
In this approach, the similarity of s1 and s2 is calcu-
lated as shown in Eq. (3) (Mihalcea et al, 2006).
similarity(s1, s2) = 12 (
?
w?{s1}
(maxSim(w,s2)?idf(w))
?
w?{s1}
idf(w)
+
?
w?{s2}
(maxSim(w,s1)?idf(w))
?
w?{s2}
idf(w) )
(3)
where idf(w) is the inverse document frequency of
the word w, and maxSim(w, s2) is the maximum
lexical similarity between the word w in sentence s2
and all the words in sentence s2 calculated by means
of the Eq. (4) reported by (Wu and Palmer, 1994).
The sentence terms are assumed to be concepts, LCS
is the depth of the least common subsumer, and the
equation is calculated using the NLTK libraries1.
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(4)
2.3 Approach BUAP-RUN-3: Random
Indexing and Bag of Concepts
The vector space model (VSM) for document rep-
resentation supporting search is probably the most
well-known IR model. The VSM assumes that term
vectors are pair-wise orthogonal. This assumption
is very restrictive because words are not indepen-
dent. There have been various attempts to build
representations for documents that are semantically
richer than only vectors based on the frequency of
terms occurrence. One example is Latent Seman-
tic Indexing (LSI), a method of word co-occurrence
analysis to compute semantic vectors (context vec-
tors) for words. LSI applies singular-value decom-
position (SVD) to the term-document matrix in or-
der to construct context vectors. As a result the di-
mension of the produced vector space will be signif-
icantly smaller; consequently the vectors that repre-
sent terms cannot be orthogonal. However, dimen-
sion reduction techniques such as SVD are expen-
sive in terms of memory and processing time. Per-
forming the SVD takes time O (nmz), where n is
the vocabulary size, m is the number of documents,
and z is the number of nonzero elements per column
in the words-by-documents matrix. As an alterna-
tive, there is a vector space methodology called Ran-
dom Indexing (RI) (Sahlgren, 2005), which presents
an efficient, scalable, and incremental method for
building context vectors. Its computational com-
plexity is O (nr) where n is as previously described
and r is the vector dimension. Particularly, we apply
RI to capture the inherent semantic structure using
Bag of Concepts representation (BoC) as proposed
by Sahlgren and Co?ster (Sahlgren and Co?ster, 2004),
where the meaning of a term is considered as the
sum of contexts in which it occurs.
1http://www.nltk.org/
632
2.3.1 Random Indexing
Random Indexing (RI) is a vector space method-
ology that accumulates context vectors for words
based on co-occurrence data. The technique can be
described as:
? First a unique random representation known as
index vector is assigned to each context (docu-
ment). Index vectors are binary vectors with a
small number of non-zero elements, which are
either +1 or -1, with equal amounts of both.
For example, if the index vectors have twenty
non-zero elements in a 1024-dimensional vec-
tor space, they have ten +1s and ten -1s. Index
vectors serve as indices or labels for documents
? Index vectors are used to produce context vec-
tors by scanning through the text and every
time a target word occurs in a context, the in-
dex vector of the context is added to the con-
text vector of the target word. Thus, at each
encounters of the target word t with a context c
the context vector of t is updated as follows: ct
+ = ic where ct is the context vector of t and ic
is the index vector of c. In this way, the context
vector of a word keeps track of the contexts in
which it occurred.
RI methodology is similar to latent semantic in-
dexing (LSI) (Deerwester et al, 1990). However,
to reduce the co-occurrence matrix no dimension re-
duction technique such as SVD is needed, since the
dimensionality d of the random index vectors is pre-
established as a parameter (implicit dimension re-
duction). Consequently d does not change once it
has been set; as a result, the dimensionality of con-
text vectors will never change with the addition of
new data.
2.3.2 Bag of Concepts
Bag of Concepts (BoC) is a recent representa-
tion scheme proposed by Sahlgren and Co?ster in
(Sahlgren and Co?ster, 2004), which is based on the
perception that the meaning of a document can be
considered as the union of the meanings of its terms.
This is accomplished by generating term context
vectors from each term within the document, and
generating a document vector as the weighted sum
of the term context vectors contained within that
document. Therefore, we use RI to represent the
meaning of a word as the sum of contexts (entire
documents) in which it occurs. Illustrating this tech-
nique, suppose you have two documents: D1: A man
with a hard hat is dancing, and D2: A man wearing
a hard hat is dancing. Let us suppose that they have
index vectors ID1 and ID2, respectively: the context
vector for hat will be the ID1 + ID2, because this
word appears in both documents. Once the context
vectors have been built by RI, they are used to repre-
sent the document as BoC. For instance, supposing
CV1, CV2, CV3, . . . and CV8, are the context vec-
tors of each word in D1, then document D1 will be
represented as the weighted sum of these eight con-
text vectors.
2.3.3 Implementation
The sentences of each file were processed to gen-
erate the BoC representations of them. BoC rep-
resentations were generated by first stemming all
words in the sentences. We then used random index-
ing to produce context vectors for each word in the
files (i.e. STS.input.MSRpar, STS.input.MSRvid,
etc.), each file was considered a different corpus and
documents were the sentences in them. The dimen-
sion of the context vectors was fixed at 2048, de-
termined by experimentation using the training set.
These context vectors were then tf ? idf -weighted,
according to the corpus, and added up for each sen-
tence, to produce BoC representations. Therefore
the similarity values were calculated by the cosine
function. Finally cosine values were multiplied by 5
to produce values between 0 and 5.
3 Experimental results
In Table 1 we show the results obtained by the
three approaches submitted to the competition. The
columns of Table 1 stand for:
? ALL: Pearson correlation with the gold stan-
dard for the five datasets, and corresponding
rank.
? ALLnrm: Pearson correlation after the system
outputs for each dataset are fitted to the gold
standard using least squares, and corresponding
rank.
633
Run ALL Rank ALL
nrm
Rank
Nrm
Mean Rank
Mean
MSR
par
MSR
vid
SMT
eur
On -
WN
SMT-
news
BUAP-
RUN-1
0.4997 63 0.7568 62 0.4892 57 0.4037 0.6532 0.4521 0.605 0.4537
BUAP-
RUN-2
-0.026 89 0.5933 89 0.0669 89 0.1109 0.0057 0.0348 0.1788 0.1964
BUAP-
RUN-3
0.663 25 0.7474 64 0.488 59 0.4018 0.6378 0.4758 0.5691 0.4057
Table 1: Results of approaches of BUAP in Task 6.
? Mean: Weighted mean across the 5 datasets,
where the weight depends on the number of
pairs in the dataset.
Followed by Pearson for individual datasets.
At this moment, we are not aware of the reasons
because the second approach obtained a very poor
performance. The way in which the idf(w) is calcu-
lated could be one of the reasons, because the corpus
used is relatively small and also from a different do-
main. With respect to the other two approaches, we
consider that they (first and third) obtained a com-
parable performance, even when the third approach
obtained the best ALL result with a Pearson correla-
tion equal to 0.663.
4 Discussion and conclusion
We have presented three different approaches for
tackling the problem of Semantic Textual Similarity.
The use of term expansion by synonyms performed
well in general and obtained a comparable behavior
than the third approach which used random index-
ing and bag of concepts. It is interesting to observe
that these two approaches performed similar when
the two term expansion mechanism are totally dif-
ferent. As further, it is important to analyze the poor
behavior of the second approach. We would like also
to introduce semantic relationships other than syn-
onyms in the process of term expansion.
Acknowledgments
This project has been partially supported by
projects CONACYT #106625, #VIAD-ING11-II,
PROMEP/103.5/11/4481 and VIEP #PIAD-ING11-
II.
References
E. Agirre, D. Cer, M. Diab, and B. Dolan. 2012.
SemEval-2012 Task 6: Semantic Textual Similarity.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In proceedings of AAAI?06,
pages 775?780.
Magnus Sahlgren and Rickard Co?ster. 2004. Using bag-
of-concepts to improve the performance of support
vector machines in text categorization. In Proceedings
of the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
M. Sahlgren. 2005. An Introduction to Random Index-
ing. Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering, TKE 2005.
Prajol Shrestha. 2011. Corpus-based methods for short
text similarity. In TALN 2011, Montpellier, France.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In 32nd. Annual Meeting of the
Association for Computational Linguistics, pages 133
?138, New Mexico State University, Las Cruces, New
Mexico.
634
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 706?709,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Lexical and Semantic Similarity for Cross-lingual Textual
Entailment
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we present a report of the two di-
fferent runs submitted to the task 8 of Semeval
2012 for the evaluation of Cross-lingual Tex-
tual Entailment in the framework of Content
Synchronization. Both approaches are based
on textual similarity, and the entailment judg-
ment (bidirectional, forward, backward or no
entailment) is given based on a set of decision
rules. The first approach uses textual simi-
larity on the translated and original versions
of the texts, whereas the second approach ex-
pands the terms by means of synonyms. The
evaluation of both approaches show a similar
behavior which is still close to the average and
median.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of the Textual Entail-
ment task (Dagan and Glickman, 2004). Given a text
(T ) and an hypothesis (H) in different languages,
the CLTE task consists of determining if the mea-
ning of H can be inferred from the meaning of T .
In this paper we present a report of the obtained
results after submitting two different runs for the
Task 8 of Semeval 2012, named ?Cross-lingual Tex-
tual Entailment for Content Synchronization? (Negri
et al, 2012). In this task, the Cross-Lingual Tex-
tual Entailment addresses textual entailment recog-
nition under a new dimension (cross-linguality), and
within a new challenging application scenario (con-
tent synchronization). The task 8 of Semeval 2012
may be formally defined as follows:
Given a pair of topically related text fragments
(T1 and T2) in different languages, the task consists
of automatically annotating it with one of the follo-
wing entailment judgments:
? Bidirectional (T1 ? T2 & T1 ? T2): the two
fragments entail each other (semantic equiva-
lence)
? Forward (T1 ? T2 & T1 ! ? T2): unidirec-
tional entailment from T1 to T2
? Backward (T1 ! ? T2 & T1 ? T2): unidirec-
tional entailment from T2 to T1
? No Entailment (T1 ! ? T2 & T1 ! ? T2): there
is no entailment between T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs. Cross-lingual datasets are avai-
lable for the following language combinations:
? Spanish/English (SPA-ENG)
? German/English (DEU-ENG)
? Italian/English (ITA-ENG)
? French/English (FRA-ENG)
The remaining of this paper is structured as fo-
llows: Section 2 describes the two different approa-
ches presented in the competition. The obtained re-
sults are shown and dicussed in Section 3. Finally,
the findings of this work are given in Section 4.
706
2 Experimental setup
For this experiment we have considered to tackle the
CLTE task by means of textual similarity and textual
length. In particular, the textual similarity is used to
determine whether some kind of entailment exists or
not. We have established the threshold of 0.5 for the
similarity function as evidence of textual entailment.
Since the two sentences to be evaluated are written
in two different languages, we have translated each
sentence to the other language, so that, we have two
sentences in English, and two sentences in the origi-
nal language (Spanish, German, Italian and French).
We have used the Google translate for this purpose
1
.
The corpora used in the experiments comes from
a cross-lingual Textual Entailment dataset presented
in (Negri et al, 2011), and provided by the task orga-
nizers. We have employed the training dataset only
for adjust some parameters of the system, but the
approach is knowledge-based and, therefore, it does
not need a training corpus. Both, the training and
test corpus contain 500 sentences for each language.
The textual length is used to determine the entail-
ment judgment (bidirectional, forward, backward,
no entailment). We have basically, assumed that the
length of a text may give some evidence of the type
of entailment. The decision rules used for determi-
ning the entailment judgment are described in Sec-
tion 2.3.
In this competition we have submitted two diffe-
rent runs which differ with respect to the type of tex-
tual similarity used (lexical vs semantic). The first
one, calculates the similarity using only the trans-
lated version of the original sentences, whereas the
second approach uses text expansion by means of
synonyms and, thereafter, it calculates the similarity
between the pair of sentences.
Let T1 be the sentence in the original language,
T2 the T1 topically related text fragment (written in
English). Let T3 be the English translation of T1,
and T4 the translation of T2 to the original language
(Spanish, German, Italian and French). The formal
description of these two approaches are given as fo-
llows.
1http://translate.google.com.mx/
2.1 Approach 1: Lexical similarity
The evidence of textual entailment between T1 and
T2 is calculated using two formulae of lexical si-
milarity. Firstly, we determine the similarity bet-
ween the two texts written in the source language
(SimS). Additionally, we calculate the lexical simi-
larity between the two sentences written in the target
language (SimT ), in this case English.
Given the limited text length of the text fragments,
we have used the Jaccard coefficient as similarity
measure. Eq. (1) shows the lexical similarity for the
two texts written in the original language, whereas,
Eq. (2) presents the Jaccard coefficient for the texts
written in English.
simS = simJaccard(T1, T4) =
|T1 ? T4|
|T1 ? T4|
(1)
simT = simJaccard(T2, T3) =
|T2 ? T3|
|T2 ? T3|
(2)
2.2 Approach 2: Semantic similarity
In this case we calculate the semantic similarity bet-
ween the two texts written in the original language
(simS), and the semantic similarity between the two
text fragments written in English (simT ). The se-
mantic level of similarity is given by considering
the synonyms of each term for each sentence (in
the original and target language). For this purpose,
we have employed five dictionaries containing syno-
nyms for the five different languages considered in
the competition (English, Spanish, German, Italian,
and French)2. In Table 1 we show the number of
terms, so as the number of synonyms in average by
term considered for each language.
Let T1 = w1,1w1,2...w1,|T1|, T2 =
w2,1w2,2...w2,|T2| be the source and target
sentences, and let T3 = w3,1w3,2...w3,|T3|,
T4 = w4,1w4,2...w4,|T4| be translated version of the
original source and target sentences, respectively.
The synonyms of a given word wi,k, expressed as
synset(wi,k), are obtained from the aforementioned
dictionaries by extracting the synonyms of wi,k. In
order to obtain a better matching between the terms
contained in the text fragments and the terms in the
2http://extensions.services.openoffice.org/en/dictionaries
707
Table 1: Dictionaries of synonyms used for term expan-
sion
Language Terms synonyms per term
(average)
English 2,764 60
Spanish 9,887 45
German 21,958 115
Italian 25,724 56
French 36,207 93
dictionary, we have stemmed all the terms using the
Porter stemmer.
In order to determine the semantic similarity bet-
ween two terms of sentences written in the source
language (w1,i and w4,j) we use Eq. (3). The se-
mantic similariy between two terms of the English
sentences are calculated as shown in Eq. (4).
sim(w1,i, w4,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w4,j) ||
w1,i ? synset(w4,j) ||
w4,j ? synset(w1,i)
0 otherwise
(3)
sim(w2,i, w3,j) =
?
?
?
?
?
?
?
?
?
1 if (w2,i == w3,j) ||
w2,i ? synset(w3,j) ||
w3,j ? synset(w2,i)
0 otherwise
(4)
Both equations consider the existence of semantic
similarity when the two words are identical, or when
the some of the two words appear in the synonym set
of the other word.
The semantic similarity of the complete text frag-
ments T1 and T4 (simS) is calculated as shown in
Eq. (5). Whereas, the semantic similarity of the
complete text fragments T2 and T3 (simT ) is cal-
culated as shown in Eq. (6).
simS(T1, T4) =
?|T1|
i=1
?|T4|
j=1 sim(w1,i,w4,j)
|T1?T4|
(5)
simT (T2, T3) =
?|T2|
i=1
?|T3|
j=1 sim(w2,i,w3,j)
|T2?T3|
(6)
2.3 Decision rules
Both approches used the same decision rules in or-
der to determine the entailment judgment for a given
pair of text fragments (T1 and T2). The following al-
gorithm shows the decision rules used.
Algorithm 1.
If |T2| < |T3| then
If (simT > 0.5 and simS > 0.5)
then forward
ElseIf |T2| > |T3| then
If (simT > 0.5 and simS > 0.5)
then backward
ElseIf (|T1| == |T4| and |T2| == |T3|) then
If (simT > 0.5 and simS > 0.5)
then bidirectional
Else no entailment
As mentioned above, the rules employed the le-
xical or semantic textual similarity, and the textual
length for determining the textual entailment.
3 Results
In Table 2 we show the overall results obtained by
the two approaches submitted to the competition.
We also show the highest, lowest, average and me-
dian overall results obtained in the competition.
SPA-
ENG
ITA-
ENG
FRA-
ENG
DEU-
ENG
Highest 0.632 0.566 0.57 0.558
Average 0.407 0.362 0.366 0.357
Median 0.346 0.336 0.336 0.336
Lowest 0.266 0.278 0.278 0.262
BUAP run1 0.35 0.336 0.334 0.33
BUAP run2 0.366 0.344 0.342 0.268
Table 2: Overall statistics obtained in the Task 8 of Se-
meval 2012
The runs submitted perform similar, but the se-
mantic approach obtained a slightly better perfor-
mance. The two results are above the median but
below the average. We consider that better results
may be obtained if the two features used (textual si-
milarity and textual length) were introduced into a
supervised classifier, so that, the decision rules were
approximated on the basis of a training dataset, ins-
tead of the empirical setting done in this work. Fu-
ture experiments will be carried out in this direction.
708
4 Discussion and conclusion
Two different approaches for the Cross-lingual Tex-
tual Entailment for Content Synchronization task of
Semeval 2012 are reported in this paper. We used
two features for determining the textual entailment
judgment between two texts T1 and T2 (written in
two different languages). The first approach pro-
posed used lexical similarity, meanwhile the second
used semantic similarity by means of term expan-
sion with synonyms.
Even if the performance of both approaches is
above the median and slighly below the average,
we consider that we may easily improve this perfor-
mance by using syntactic features of the text frag-
ments. Additionally, we are planning to integrate
some supervised techniques based on decision rules
which may be trained in a supervised dataset. Future
experiments will be executed in this direction.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, #VIAD-ING11-II and VIEP
#PIAD-ING11-II.
References
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Learning Methods for Text Un-
derstanding and Mining, January.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 321?
324, Los Angeles, California, June. Association for
Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
709
