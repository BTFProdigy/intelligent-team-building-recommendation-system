A Tool for Automated Revision of Grammars for NLP Systems 
Nanda Kambhatla nd Wlodek Zadrozny 
IBM T.J. Watson Research Center 
30 Saw Mill River Road, 
Hawthorne, NY, 10532 
{nanda, wlodz} @us.ibm.com 
Abstract 
We present an algorithm and a tool for 
automatically revising grammars for natural 
language processing (NLP) systems to 
disallow specifically identified sentences or 
sets of sentences. We also outline an 
approach for automatically revising attribute 
value grammars using counter-examples. 
Developing rammars for NLP systems that 
are both general enough to accept most 
sentences about a domain, but constrained 
enough to disallow other sentences i very 
tedious. Our approach of revising grammars 
automatically using counter-examples 
greatly simplifies the development and 
revision of tightly constrained grammars. 
We have successfully used our tool to 
constrain over-generalizing rammars of 
speech understanding systems and obtained 
higher ecognition accuracy. 
1 Introduction 
Natural language processing systems often 
constrain the set of "utterances" from a user 
(spoken, typed in, etc.) to narrow down the 
possible syntactic and semantic resolutions of 
the utterance and reduce the number of 
misrecognitions and/or misunderstandings by 
the system. Such constraints on the allowed 
syntax and the inferred semantics are often 
expressed in the form of a "grammar "l, a set of 
Throughout his document, by using the word 
"grammar", we refer to a Context-Free Grammar that 
consists of a finite set of non-terminals, a finite set of 
terminals, a unique non-terminal called the start 
symbol, and a set of production rules of the form A-> 
a, where A is a non-terminal nd a is a string of 
terminal or non-terminal symbols. The 'language' 
rules specifying the set of allowed utterances 
and possibly also specifying the semantics 
associated with these utterances. For instance, 
grammars are commonly used in speech 
understanding systems to specify both the set of 
allowed sentences and to specify "tags" to 
extract semantic entities (e.g. the "amount" of 
money). 
Constraining the number of sentences accepted 
by a grammar is essential for reducing 
misinterpretations of user queries by an NLP 
system. For instance, for speech understanding 
systems, if the grammar accepts a large number 
of sentences, then the likelihood of recognizing 
uttered sentences as random, irrelevant, or 
undesirable sentences is increased. For 
transaction processing systems, misrecognized 
words can lead to unintended transactions being 
processed. An effective constraining rammar 
can reduce transactional errors by limiting the 
number of sentence l vel errors. The problem of 
over-generalization f speech grammars and 
related issues is well discussed by Seneff (1992). 
Thus, speech grammars must often balance the 
conflicting requirements of 
? accepting a wide variety of sentences to 
increase flexibility, and 
? accepting a small number of sentences 
to increase system accuracy and 
robustness. 
Developing tight grammars which trade-off 
these conflicting constraints is a tedious and 
accepted by a grammar is the set of all terminal 
strings that can be generated from the start symbol by 
successive application of the production rules. The 
grammar may optionally have semantic interpretation 
rules associated with each production rule (e.g. see 
(Allen 95)). 
210 
difficult process. Typically, grammars 
overgeneralize and accept too many sentences 
that are irrelevant or undesirable for a given 
application. We call such sentences "counter- 
examples". The problem is usually handled by 
revising the grammar manually to disallow such 
counter-examples. For instance, the sentence 
"give me my last eighteen transactions" may 
need to be excluded from a grammar for a 
speech understanding system, since the words 
"eighteen" and "ATM" are easily confused by 
the speech recogniser. However, "five" and 
"ten" should remain as possible modifiers of 
"transactions". Counter-examples can also be 
sets of sentences that need to be excluded from a 
grammar (specified by allowing the inclusion of 
non-terminals in counter-examples). For 
example, for a banking application that 
disallows money transfers to online accounts, we 
might wish to exclude the set of sentences 
"transfer <AMOUNT> dollars to my online 
account" from the grammar, where 
<AMOUNT> is a non-terminal in the grammar 
that maps to all possible ways of specifying 
amounts. 
In this paper, we are proposing techniques for 
automatically revising grammars using counter- 
examples. The grammar developer identifies 
counter-examples from among sentences (or sets 
of sentences) mis-recognized by the speech 
recognizer or from sentences randomly 
generated by a sentence generator using the 
original grammar. The grammar reviser modifies 
F igure  I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Grammar  Rev iser  
Perser L counter.- 
r examples  
parse  ~ 
t ree  
Grammar  L in i t ia l  
Modi f ie r  I~ grammar  
rev ised  grammar  
the original grammar to invalidate the counter- 
examples. The revised grammar can be fed back 
to the grammar reviser and whole process can be 
iterated several times until the resulting 
grammar is deemed satisfactory. 
In the next sections, we first describe our 
algorithm for revising grammars to disallow 
counter-examples. Wealso discuss algorithms to 
make the revised grammar compact using 
minimum description length (MDL) based 
grammar compaction techniques and extensions 
to our basic algorithm to handle grammars with 
recursion. We then present some results of 
applying our grammar reviser tool to constrain 
speech grammars of speech understanding 
systems. Finally, we present an approach for 
revising attribute value grammars using our 
technique and present our conclusions. 
2 Automated Grammar Revision by rule 
modification 
In this section, we describe an algorithm (see 
Figure 1) for revising grammars that directly 
modifies the rules of the grammar to disallow 
counter-examples. For each counter-example 2, 
we generate the parse tree (representation f all 
the grammar rules needed to generate the 
sentence or set of sentences) and the grammar 
modifier modifies the production rules of the 
grammar to invalidate the counter-example. This 
process is repeated for each counter-example 
using the revised grammar from the previous 
iteration for generating the parse tree for the 
current counter-example. If a counter-example 
generates multiple parse trees, the above 
algorithm is repeated for each parse tree in turn. 
2.1 Grammar modification algorithm 
We present the grammar modification algorithm 
below. For, we assume that the parse-tree(s) of
the counter-example contain no recursion (i.e. 
the same production rule does not occur twice in 
any of the parse trees). In section 2.4, we present 
an approach for using the algorithm even when 
the parse-trees contain recursion. Thus, the 
algorithm is applicable for any context-free 
grammar. The grammar modification algorithm 
a Note that a counter-example can be a sentence such 
as "move to operator" or a set of sentences such as 
"transfer <AMOUNT> to online account". The latter 
is specified using non-terminals interspersed with 
words. 
~11 
for modifying the rules of a grammar to disallow 
a counter-example c (identified by a grammar 
developer) using a parse-tree for e proceeds as 
follows : 
1. For each non-terminal <N> in the parse 
tree, except he <<START>> symbol, 
a. Add a rule to define a new non- 
terminal <N'> such that <N'> 
generates all phrases that <N> 
generates except for the phrase 
in the counter-example that <N> 
generates. 
b. Add a rule to define a new non- 
terminal <No> such that <No> 
generates only the phrase(s) in 
the counter-example that <N> 
generates. 
2. Modify the rule that contains the 
<<START>> symbol in the parse tree, 
such that the <<START>> symbol no 
longer generates the given counter- 
example. 
Figure 2 
(a) Original grammar: 
<<START>> : :=  <V> <N> <PP> t 
<V> <PP> . 
<PP> : := " to  m <N> . 
<V> : := "move"  I " t rans fer"  . 
<N> : := "check ing"  I "sav ings"  I 
"money" \[ "operator"  . 
(b) Parse Tree for "move to operator" 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
<%'> : == "move"  . ! 
I <PP> : := " to"  <N> . 1 
' <N> : := "operator" .  I 
............................................................................................................................... i 
We illustrate the algorithm with an example. 
Figure 2(a) shows a simple grammar. Suppose 
the sentence "move to operator" is a counter- 
example for an application. Figure 2(b) shows 
the parse-tree for "move to operator". Since the 
parse tree contains the rule: <V> ::= "move", 
new rules are added to define non-terminals 
<V'> and <Vo>, where <V'> does not generate 
"move" and <Vo> generates only "move". 
Similarly, since the parse tree contains the rule: 
<N>::= "operator", the new rules: <N'>::= 
"checking" I "savings" I "money"; and <No>::= 
"operator", are added. For the non-terminal 
<PP>, the new rules: <PP'>::= "to" <N'>; and 
<PPo>::= "to" <No>, are added. Note that since 
<No> only generates the phrase "operator" 
which is part of the counter-example, <PPo> 
only generates the phrase "to operator" which is 
part of the counter-example. Also, <PP'> 
generates all phrases that <PP> generates except 
for the phrase "to operator". Finally, the rule: 
<<START>>::= <V> <PP> is modified using 
the newly created non-terminals <V'>, <Vo>, 
<PP'> and <PPo> such that the only sentences 
which are accepted by the grammar and begin 
with the phrase "move" do not end with the 
phrase "to operator", and also, the only 
sentences which are accepted by the grammar 
and end with the phrase "to operator" do not 
begin with the phrase "move". Figure 3 shows 
the final modified grammar that accepts all the 
sentences that the grammar in Figure 2(a) 
accepts except for the sentence "move to 
Figure 3 
<<START>> : := <V> <N> <PP> I 
<V'> <PPo> \] 
<Vo> <PP '> l 
<V '> <PP '> . 
<PP> : :=  " tO"  <N> . 
<PP '> : :=  " tO"  <N'> . 
<PPo> : :=  " tO"  <NO> . 
<V> : := "move"  \[ " t rans fer"  . 
<V'> :: = " t rans fer "  . 
<Vo> : :=  "move"  . 
<N> : := "check ing"  I " sav ings"  \[ 
"money"  I "operator"  . 
<N'> : :=  "check ing"  I " sav ings"  l 
? money"  . 
<No> : :=  "operator "  . 
operator". In Figure 3, all the grammar rules that 
are new or modified are shown in bold and 
italics. 
The above algorithm for grammar modification 
has a time complexity of O(m*2 k) rule creation 
(or modification) steps for removing a counter- 
example, where m is the number of production 
rules in the parse tree of the counter-example 
and k is the largest number of non-terminals on 
the right hand side of any of these production 
212 
rules. Since grammars used for real applications 
rarely have more than a handful of non-terminals 
on the right hand side of production rules, this 
complexity is quite manageable. 
2.2 Automated grammar compaction using 
MDL based grammar induction 
As seen in the example described above, the size 
of the grammar (number of production rules) can 
increase greatly by applying our algorithm 
successively for a number of counter-examples. 
However, we can remedy this by applying 
grammar induction algorithms based on 
minimum description length (MDL) (e.g. 
Grunwald (1996) and Zadrozny (1997)) to 
combine rules and create a compact grammar 
that accepts the same language. 
The MDL principle (Rissanen (1982)) selects 
that description (theory) of data, which 
minimizes the sum of the length, in bits, of the 
description of the theory, and the length, in bits, 
of data when encoded using the theory. In our 
case, the data is the set of possible word 
combinations and the theory is the grammar that 
specifies it. We are primarily interested in using 
the MDL principle to obtain (select) a compact 
grammar (the theory) from among a set of 
equivalent grammars. Since the set of possible 
word combinations (data) is the same for all 
grammars in consideration, we focus on the 
description length of the grammars itself, which 
we approximate by using a set of heuristics 
described in step 1 below. 
We use the following modified version of 
Zadrozny's (1997) algorithm to generate a more 
compact grammar from the revised grammar 
using the MDL principle: 
1. Compute the description length of the 
grammar, i.e. the total number of 
symbols needed to specify the grammar, 
where each non-terminal, "::=", and "1" 
are counted as one symbol. 
2. Modify the current grammar by 
concatenating all possible pairs of non- 
terminals, and compute the description 
length of each such resultant grammar. 
For concatenating <NI> and <N2>, 
introduce the rule <N3>::= <NI> <N2>, 
search all other rules for consecutive 
occurrences of <NI> and <N2>, and 
replace such occurrences with <N3>. 
Note that this change results in an 
equivalent grammar (that accepts the 
same set of sentences as the original 
grammar). 
3. Modify the current grammar by merging 
all possible pairs of non-terminals, and 
compute the description length of each 
such resultant grammar. For merging 
<N4> and <N5>, introduce the rule: 
<N6>::= <N4> \[ <N5>, search for pairs 
of rules which differ only in one 
position such that for one of the rules, 
<N4> occurs in that position and the 
other rule, the <N5> occurs in the same 
position. Replace the pair of rules with a 
new rule that is exactly the same as 
either of the pairs of rules, except for the 
use of <N6> instead of <N3> or <N4>. 
Note that this change results in an 
equivalent grammar (that accepts the 
same set of sentences as the original 
grammar). 
4. Compute a table of description lengths 
of the grammars obtained by 
concatenating or merging all possible 
pairs of non- terminals of the initial 
grammar, as described above. Select he 
pair of non-terminals (if any) together 
with the action (concatenate or merge) 
that results in the least description 
length and execute the corresponding 
action. 
5. Iterate steps 2, 3, and 4 until the 
description length does not decrease. 
No further modification is performed if
the base description length of the 
grammar is lower than that resulting 
from merging or concatenating any pair 
of non- terminals. 
In variations of this algorithm, the selection of 
the pairs of non-terminals to concatenate or 
merge, can be based on; the syntactic ategories 
of the corresponding terminals, the semantic 
categories of the corresponding terminals, and 
the frequency of occurrence of the non- 
terminals. 
213
Using the algorithm described above in 
conjunction with the algorithm in section 2.1, we 
can obtain a compact grammar that is guaranteed 
to disallow the counter-examples. 
2.3 Results for grammar revision for speech 
understanding systems 
We have built a graphical tool for revising 
grammars for NLP systems based on the 
algorithm described in sections 2.1 and 2.2 
above. The tool takes as input an existing 
grammar and can randomly generate sentences 
accepted by the grammar including non-terminal 
strings and strings containing terminals and non- 
terminals (e.g. both "move to operator" and 
"transfer <AMOUNT> to online account" would 
be generated if they were accepted by the 
grammar). A grammar developer (a human) 
interacts with the tool and either inputs counter- 
examples selected from speech recognition error 
logs or selects counter-examples ike the ones 
listed above. The grammar developer can then 
revise the grammar to disallow the counter- 
examples by pressing a button and then reduce 
the size of the resulting grammar using the 
algorithm in section 2.2 by pressing another 
button to obtain a compact grammar that does 
not accept any of the identified counter- 
examples. Typically, the grammar developer 
repeats the above cycle several times to obtain a 
tightly constrained grammar. 
We have successfully used the tool described 
above to greatly constrain overgeneralizing 
grammars for speech understanding systems that 
we built for telephony banking, stock trading 
and directory assistance (Zadrozny et al 1998). 
The speech recognition grammars for these 
systems accepted around fifty million sentences 
each. We successfully used the reviser tool to 
constrain these grammars by eliminating 
thousands of sentences and obtained around 20- 
30% improvement in sentence recognition 
accuracy. We conducted two user studies of our 
telephony banking system at different stages of 
development. The user studies were conducted 
eight months apart. During these eight months, 
we used a multi-pronged strategy of constraining 
grammars using the grammar revision 
algorithms described in this paper, improving 
the pronunciation models of some words and 
redesigning the prompts of the system to enable 
fast and easy error recovery by users. The 
combination of all these techniques resulted in 
improving the 'successful transaction in first 
try '3 from 43% to 71?/0, an improvement of65%. 
The average number of wrong tries (turns of 
conversation) to get a successful answer was 
reduced from 2.1 to 0.5 tries. We did not 
conduct experiments o isolate the contribution 
of each factor towards this improvement in 
system performance. 
It is important o note here that we would 
probably have obtained this improvement in 
recognition accuracy even with a manual 
revision of the grammars. However, the main 
advantage in using our tool is the tremendous 
simplification of the whole process of revision 
for a grammar developer who now selects 
counter-examples with an interactive tool 
instead of manually revising the grammars. 
2.4 Handling recursion in grammars 
We now describe an extension of the algorithm 
in section 2.1 that can modify grammars with 
recursion to disallow a finite set of counter- 
examples. The example grammars shown above 
are regular grammars (i.e. equivalent finite state 
automatons exist). For regular grammars (and 
only for regular grammars), an alternative 
approach for eliminating counter-examples 
using standard automata theory is" 
? Compute the finite state automaton 
(FSA) G corresponding to the original 
grammar. 
? Compute the FSA C corresponding to
the set of counter-examples. 
? Compute C', the complement of C with 
respect to the given alphabet. 
? Compute G', the intersection of G and 
C'. The FSA G' is equivalent to a revised 
grammar which disallows the counter- 
examples. 
3 We measured the number of times the user's 
transactional intent (e.g. checking balance, last five 
transactions etc.) was recognized and acted upon 
correctly by the system in the first try, even when the 
actual utterance may not have been recognized 
correctly word for word. 
914. 214
The time complexity of the algorithm is O(n*m), 
where n and m are the number of states in the 
finite state automatons G and C respectively. 
This is comparable to the quadratic time 
complexity of our grammar revision algorithm 
presented in Section 3.1. 
However, the above algorithm for eliminating 
counter-examples only works for regular 
grammars. This is because context-free 
grammars are not closed under complementation 
and intersection. However we can use our 
algorithm for grammar modification (section 
2.1) to handle any context-free grammar as 
follows: 
1) As before, generate parse tree p for 
counter-example c for an initial 
grammar G. 
2) If p contains a recursion (two or 
more repetitions of any production 
rule in the same parse tree), rewrite 
the initial grammar G as the 
equivalent grammar G', where the 
recursion is "unrolled" sufficiently 
many times (at least one more time 
than the number of repetitions of the 
recursive production rule in the 
parse tree). We explain the unrolling 
of recursion in greater detail below. 
If p does not contain any recursion, 
go to step 4. 
3) Generate parse tree p' for the 
counter-example c for the rewritten 
grammar G'. Note that p' will no 
longer contain a recursive 
application of any production rules, 
though G' itself will still have 
recursion. 
4) Use the algorithm described in 
section 2.1 to modify the grammar 
G' to eliminate the counter-example 
c using the parse tree p'. 
We illustrate the above algorithm with an 
example. Figure 4(a) shows a context free 
grammar which accepts all strings of the form 
a"b", for any n greater than 0. Note that this is 
not a regular language. Suppose we wish to 
eliminate the counter-example aaabbb from the 
initial grammar. The parse treep for the counter- 
example aaabbb is shown in Figure 4(b). The 
grammar in 4(a) can be rewritten as the 
equivalent grammar 4(c), where the recursion of 
(S->aSb) is unrolled three times. The parse tree 
p '  for the counter-example aaabbb with respect 
to grammar in 4(c) is shown in Figure 4(d). Note 
that p '  does not contain any recursion, though 
the rewritten grammar does. We revised the 
FIGURE 4 
(a) ORIG INAL  GRAMMAR G 
<S> : := "a"  <S> "b" \[ "a n "b" . 
(b) PARSE TREE p 
<S> : := "a  n <S> "b" . 
<S> : := "a"  <S> "b" . 
<S> : := "a  n rib" . 
(c) REWRITTEN GRAMMAR G'  
<S> : := "a" <$1> "b" l "a" "b" . 
<S l> : := "a" <$2> "b" I "a" "b"  . 
<$2> : := "a"  <$3> "b" I "a" "b" . 
<$3> : := "a"  <$3> "b"  \[ "a"  "b"  . 
(d) PARSE TREE p'  
<S> : := "a"  <S l> "b" . 
<$1> : := "a" <$2> "b"  . 
<$2> : :=  "a"  "b" . 
~) REVISED GRAMMAR Gr  
<S> : := "a"  <S l> "b" \[ "a" "b" . 
<S I> : :=  "a"  <$2> "b" I "a"  "b"  . 
<82> : := "a" <$3> "b" . 
<$3> : :=  "a"  <$3> "b" \[ "a" "b"  . 
grammar in 4(c) to eliminate the counter- 
example aaabbb using the parse tree in Figure 
4(d). The revised grammar is shown in Figure 
4(e). Note that here we are assuming that a 
mechanism exists for rewriting the rules of a 
grammar with recursion to unroll the recursion 
(if it exists) a finite number of times. Such an 
unrolling is readily accomplished by introducing 
a set of new non-terminars, one for each iteration 
of unrolling as shown in Figure 4(c). 
3 Automated revision of attribute-value 
grammars 
In this section, we delineate an approach for 
automatically modifying attribute value 
grammars using counter-examples. We first 
convert an attribute value grammar into an 
equivalent non-attributed grammar by creating 
new non-terminals and encoding the attributes in 
the names of the new non-terminals (see 
Manaster Ramer and Zadrozny (1990) and 
Pollard and Sag (1994)). 
For example, suppose the grammar in Figure 
2(a) is an attribute value grammar with an 
~l f i  215
Figure 5 
<<START>> : := <V> <N> <PP> \[ 
<V> <pp> . 
<PP> : : :  " tO"  <N> . 
<V> : :=  "move"  \[ " t rans fer "  . 
<N> : := <N_account_check ing> \[ 
<N_accountsavings> \[ 
<N_accountunspec i f ied> 
<N_ .account_check ing> : :=  "check ing"  . 
<N_account_savings> : :=  "sav ings" .  
<N_account_unspecified> ::= "money" I 
"operator "  . 
attribute 'account', which encodes information 
about the type of account specified, e.g. 
'account' might have the values, SAVINGS, 
CHECKING and UNSPECIFIED. Figure 5 
shows an equivalent non-attributed grammar, 
where the value of the attribute 'account' has 
been encoded in the names of the non-terminals. 
Note that such an encoding can potentially 
create a very large number of non-terminals. 
Also, the specific coding used needs to be such 
<<START>> : := <V> <N> <PP> \[ <V '> <PPO> 
Fiaure 6 . -vo> .cp_p,> I <v ,> <pp,> . 
<PP> : := " to  n <N> . 
<PP '> :.'= "tO" <N'> ? 
<PPo> : :=  "tO" <NO> ? 
<V> : := "move"  ~ " t rans fer "  . 
<Vr> :~= " t rans fer "  . 
<Vo> :~= .~ve  # . 
<N> :~= <Naccount_check ing> I 
<N_account_sav ings> 1 
<N__account_unspec i f ied> . 
<N'> : : :  <N_account_check ing> \] 
<N_account_savings> \] 
<N '_acco un t _unspec  i f i ed> 
<No> : : :  <No_account_umspec i f ied> . 
<N_accountcheck ing> : := "check ing"  . 
<N_account  sav ings> : := "sav ings" .  
<Naccountunspec i f ied> : := "money"  I 
"operator "  . 
<N'_account__unspec i f ied> : := "money"  . 
<No account_unspec i f ied> : := "operator "  . 
that the attributes can be easily recovered from 
the non-terminal names later on. 
We can now use our modification algorithms 
(Section 2.1 and 2.2) to eliminate counter- 
examples from the non-attributed grammar. For 
instance, suppose we wish to eliminate 'move to 
operator' from the attributed grammar based on 
Figure 2(a), as discussed above. We apply our 
algorithm (Section 2.1) to the grammar in Figure 
5 and obtain the grammar shown in Figure 6. 
Note that we name any new non-terminals 
created uring the grammar modification i such 
a way as to leave the encoding of the attribute 
values in the non-terminal names intact. 
After applying the grammar revision algorithm, 
we can extract the attribute values from the 
encoding in the non-terminal names. For 
instance, in the example outlined above, we 
might systematically check for suffixes of a 
certain type and recover the attributes and their 
values. Also, as described earlier, we can use the 
algorithm described in section 2.2 to make the 
resulting rammar compact again by using MDL 
based grammar induction algorithms. 
4 Conclusions 
We have presented a set of algorithms and an 
interactive tool for automatically revising 
grammars of NLP systems to disallow identified 
counter-examples (sentences or sets of sentences 
accepted by the current grammar but deemed to 
be irrelevant for a given application). We have 
successfully used the tool to constrain 
overgeneralizing grammars of speech 
understanding systems and obtained 20-30% 
higher recognition accuracy. However, we 
believe the primary benefit of using our tool is 
the tremendously reduced effort for the grammar 
developer. Our technique relieves the grammar 
developer f om the burden of going through the 
tedious and time consuming task of revising 
grammars by manually modifying production 
rules one at a time. Instead, the grammar 
developer simply identifies counter-examples to 
an interactive tool that revises the grammar to 
invalidate the identified sentences. 
We also discussed an MDL based algorithm for 
grammar compaction to reduce the size of the 
revised grammar. Thus, using a combination of 
the algorithms presented in this paper, one can 
obtain a compact grammar that is guaranteed to
disallow the counter-examples. 
Although our discussion here was focussed on 
speech understanding applications, the 
algorithms and the tool described here are 
applicable for any domain where grammars are 
used. We are currently implementing an 
extension of the grammar modifier to handle 
attribute-value grammars. We outlined an 
216 
approach for automated modification of 
attribute-value grammars in Section 3. 
We conclude that algorithms for automatically 
constraining grammars based on counter- 
examples can be highly effective in reducing the 
burden on grammar developers to develop 
constrained, domain specific grammars. 
Moreover, these algorithms can be used in any 
applications, which deal with grammars. 
Acknowledgements 
We thank all of our colleagues in the 
conversation machines group at IBM T.J. 
Watson Research Center for several helpful 
comments and suggestions through the course of 
this work. 
References 
Zadrozny W., Wolf C., Kambhatla N., and Ye Y. 
(1998). Conversation machines for transaction 
processing. In proceedings of AAAI'98/IAAI'98, 
AAAI Press/MIT Press, pp 1160-1166. 
Allen J. (1995). Natural Language Understanding. 
The Benjamin/Cummings Publishing Company, 
Redwood City, CA 94065. 
Gnmwald P. (1996). A minimum description length 
approach to grammar inference. In S. Wemter et 
al., editors, Symbolic, Connectionist and Statistical 
Approach to Learning for Natural Language 
Processing, Springer, Berlin, p 203-216. 
Manaster-Ramer and Zadrozny W. (1990), 
Expressive Power of Grammatical Formalisms, 
Proceedings of Coling-90. Universitas 
Helsingiensis. Helsinki, Finland", pp. 195-200. 
Pollard, C. and Sag I A. (1994). Head-Driven Phrase 
Structure Grammar. The U. of Chicago Press. 
Rissanen J. (1982). A universal prior for integers and 
estimation by minimum description length. Annals 
of Statistics, 11:416-431. 
Seneff S. (1992). TINA: A natural anguage system 
for spoken language applications, Computational 
Linguistics, 18:p61-86. 
Zadrozny W. (1997). Minimum description length 
and compositionality. Proceedings of Second 
International Workshop for Computational 
Semantics, Tilburg. Recently re-published as a 
book chapter in: H.Bunt and R.Muskens (eds.) 
Computing Meaning. Kluwer Academic 
Publishers, Dordrecht/Boston, 1999. 
~17 217
A Conversational Interface for Online Shopping
Joyce Chai, Veronika Horvath, Nanda Kambhatla, Nicolas Nicolov & Margo Stys-Budzikowska
Conversational Dialog Systems
IBM T. J. Watson Research Center
30 Saw Mill River Rd, Hawthorne, NY 10532, USA
{jchai, veronika, nanda, nicolas, sm1}@us.ibm.com
ABSTRACT
We present a deployed, conversational dialog system that assists
users in finding computers based on their usage patterns and
constraints on specifications. We discuss findings from a market
survey and two user studies. We compared our system to a directed
dialog system and a menu driven navigation system. We found that
the conversational interface reduced the average number of clicks by
63% and the average interaction time by 33% over a menu driven
search system. The focus of our continuing work includes
developing a dynamic, adaptive dialog management strategy,
robustly handling user input and improving the user interface.
1. INTRODUCTION
Conversational interfaces allow users to interact with automated
systems using speech or typed in text via "conversational dialog".
For the purposes of this paper, a conversational dialog consists of a
sequence of interactions between a user and a system. The user
input is interpreted in the context of previous user inputs in the
current session and from previous sessions.
Conversational interfaces offer greater flexibility to users than
menu-driven (i.e., directed-dialog) interfaces, where users navigate
menus that have a rigid structure [5,4]. Conversational interfaces
permit users to ask queries directly in their own words. Thus, users
do not have to understand the terminology used by system designers
to label hyperlinks on a website or internalize the hierarchical
menus of a telephone system [3] or websites.
Recently, conversational interfaces for executing simple transactions
and for finding information are proliferating [7,6]. In this paper, we
present a conversational dialog system, Natural Language Assistant
(or NLA), that helps users shop for notebook computers and discuss
the results of user studies that we conducted with this system.
2. NATURAL LANGUAGE ASSISTANT
NLA assists users in finding notebooks that satisfy their needs by
engaging them in a dialog. At each turn of the dialog, NLA provides
incremental feedback about its understanding of the user's
constraints and shows products that match these constraints. By
encouraging iterative refinement of the user's query, the system
finds more user constraints and, ultimately, recommends a product
that best matches the user's criteria.
The system consists of three major modules (cf. Figure 1):
Presentation Manager, Dialog Manager, and Action Manager. The
Presentation Manager interprets user input and generates system
responses. It embodies the user interface and contains a shallow
semantic parser and a response generator. The semantic parser
identifies concepts (e.g., MULTIMEDIA) and constraints on
product attributes (e.g., hard disk size more than 20GB) from the
textual user input. The concepts mediate the mapping between user
input and available products through product specifications. They
implement the business logic.
The Dialog Manager uses the current requirements and formulates
action plans for the Action Manager to perform back-end operations
(e.g., database access1). The Dialog Manager constructs a response
to the user based on the results from the Action Manager and the
discourse history and sends the system response to the Presentation
Manager that displays it to the user. The system prompts for features
relevant in the current context. In our mixed initiative dialog
system, the user can always answer the specific question put to
him/her or provide any constraints.
The system has been recently deployed on an external website.
Figure 2 shows the start of a dialog.2
1 See [1] for a survey of natural language interfaces to databases.
2 We are demonstrating the system at HLT?2001 [2].
ManagerPresentation
Manager
telephone
PDA
web
Conversational
Dialog Manager
USER
APIs
speech,
text,..
NLP
Services
History
Action
Manager
Application
Action
Templates
etc...
APIs
Discourse
History
Figure 1. Architecture of the NLA conversational system.
3. USER STUDIES
We conducted a preliminary market survey and two user studies
described in subsections 3.1 and 3.2 respectively.
3.1 Market Survey
For understanding specific user needs and user vocabulary, we
conducted a user survey. Users were given three sets of questions.
The first set, in turn, contained three questions: "What kind of
notebook computer are you looking for?", "What features are
important to you?", and "What do you plan to use this notebook
computer for?". By applying statistical n-gram models and a
shallow noun phrase grammar to the user responses, we extracted
keywords and phrases expressing user's needs and interests. In the
second set of questions, users were asked to rank 10 randomly
selected terms from 90 notebook related terms in order of
familiarity to them. The third set of questions asked for
demographical information about users such as their gender, years
of experience with notebook computers, native language, etc. We
computed correlations between vocabulary/terms and user
demographic information. Over a 30-day period, we received 705
survey responses. From these responses, we learned 195 keywords
and phrases that were included in NLA.
3.2 Usability Testing
3.2.1 Experimental Setup
We conducted two user studies to evaluate usability of the system,
focusing on: dialog flow, ease of use, system responses, and user
vocabulary. The first user study focused on the functionality of
NLA and the second user study compared the functionality of
NLA with that of a directed dialog system and a menu driven
navigation system.
The moderators interviewed 52 users in the user studies: 18 and
34 in the two studies, respectively. All participants were
consumers or small business users with "beginner" or
"intermediate" computer skills. Each participant was asked to find
laptops for a variety of scenarios using three different systems (the
NLA, a directed dialog system and a menu driven navigation
system). Participants were asked to rate each system for each task
on a 1 to 10 scale (10 ? easiest) with respect to the ease of
navigation, clarity of terminology and their confidence in the
system responses. The test subjects were also asked whether the
system had found relevant products and were prompted to share
their impressions as to how well the system understood them and
responded to their requests.
Figure 2. The start of the dialog.
3.2.2 Results
In both studies, participants were very receptive to using natural
language dialog-based search. The users clearly preferred dialog-
based searches to non-dialog based searches3 (79% to 21% users).
Furthermore, they liked the narrowing down of a product list
based on identified constraints as the interaction proceeded. In the
first user study, comparing NLA with a menu driven system, we
found that using NLA reduced the average number of clicks by
63% and the average interaction time by 33%.
In the second user study, we compared NLA with a directed
dialog system and a menu driven search system for finding
computers. One goal of the comparative study was to find out if
there were any statistical differences in confidence, terminology
and navigation ratings across the three systems and whether they
were correlated with different categories of users. The ANOVA
analysis reveals statistical differences in terminology ratings
among the three systems for the category of beginner users only.
There were no statistical differences found in the other ratings of
navigation and confidence across the three sites for different
categories of users. Sandler's A test confirmed that the
terminology rating was significantly different for the categories of
consumers, small business owners, beginners and intermediates.
These comparative results suggest that asking questions relative to
the right level of end user experience is crucial. Asking users
questions about their lifestyle and how they were going to use a
computer accounted for a slight preference of the directed dialog
system over the NLA that uses questions presented on the basis of
understanding features and functions of computer terms.
3.2.3 Lessons from the user studies
Both user studies revealed several dimensions along which NLA
can be improved. The first user study highlighted a definite need
for system acknowledgement and feedback. The users wanted to
know whether the system had understood them. User comments
also revealed that a comparison of features across the whole pool
of products was important for them.
The focus of the second study, incorporating 34 subjects, was to
compare systems of similar functionality and to draw conclusions
about the functionality of NLA. Both the ANOVA and the
Sandler's test point out that terminology was a statistically
significant factor differentiating among the systems. We believe
that using terminology that is not overly technical would
contribute to the success of the dialog search. While the questions
asked by NLA were based on features and functionality of
notebook computers, the users preferred describing usage patterns
and life style issues rather than technical details of computers.
We also found that users' confidence in NLA decreased when the
system responses were inconsistent i.e., were not relevant to their
input. Lack of consistent visual focus on the dialog box was also a
serious drawback since it forced users to scroll in search of the
dialog box on each interaction page.
3 We define a dialog-based search as one comprising of a
sequence of interactions with a system where the system keeps
track of contextual (discourse) information.
3.2.4 Future work
Based on the results of the user studies, we are currently focused
on: developing a dynamic and adaptive dialog management
strategy, improving the robustness of the natural language
processing (NLP), and improving the user interface. Some of
issues mentioned here have been implemented in the next version
of NLA.
We are currently re-designing the questions that NLA asks users
to be simpler, and to focus on usage patterns rather than technical
features. We are also implementing a new dialog management
strategy in NLA that is more adaptive to the user's input, and
implements a mapping from high-level usage patterns to
constraints on low-level technical features.
We are integrating a statistical parser with NLA to more robustly
handle varied user input. The statistical parser should enable NLA
to scale to multiple languages and multiple domains in a more
robust and reliable fashion. We are aiming at an architecture that
separates the NLP processing from the business logic that will
make maintenance of the system easier.4
Improvements to the GUI include better acknowledgement and
feedback mechanisms as well as graphical UI issues. We now
reiterate the user's last query at the beginning of each interaction
page and also convey to the user an explanation of features
incrementally accumulated in the course of the interaction. We
have designed a more uniform, more compact and consistent UI.
In the welcome page, we have abandoned a three-step initiation
(typed input, experience level and preferences for major
specifications) keeping the emphasis on the dialog box. The user
preferences contributed to creating confusion as to the main
means of interaction (many users just clicked on the radial buttons
and did not use the full dialog functionality). We now infer the
technical specifications based the user's stated needs and usage
patterns. Our UI now has a no scrolling policy and we allow for
larger matching set of products to be visualized over a number of
pages.
4. DISCUSSION
In this paper, we have presented a conversational dialog system
for helping users shop for notebook computers. User studies
comparing our conversational dialog system with a menu driven
system have found that the conversational interface reduced the
average number of clicks by 63% and the average interaction time
by 33%. Based on our findings, it appears that for conversational
systems like ours, the sophistication of dialog management and
the actual human computer interface are more important than the
complexity of the natural language processing technique used.
This is especially true for web-based systems where user queries
are often brief and shallow linguistic processing seems to be
adequate. For web-based systems, integrating the conversational
interface with other interfaces (like menu-driven and search-
driven interfaces) for providing a complete and consistent user
experience assumes greater importance.
4 Many systems' fate has been decided not because they cannot
handle complex linguistic constructions but because of the
difficulties in porting such systems out of the research
environments.
The user studies we conducted have highlighted several directions
for further improvements for our system. We plan to modify our
interface to integrate different styles of interaction (e.g., menus,
search, browsing, etc.). We also intend to dynamically classify
each user as belonging to one or more categories of computer
shoppers (e.g., gamers, student users, home business users, etc.)
based on all the user interactions so far. We can then tailor the
whole interface to the perceived category including but not
limited to the actual questions asked, the technical knowledge
assumed by the system and the whole style of interaction.
Another area of potential improvement for the NLA is its inability
to handle any meta-level queries about itself or any deeper
questions about its domain (e.g., NLA currently can not properly
handle the queries, "How can I add memory to this model?" or
"What is DVD?"). Our long-term goal is to integrate different
sources of back-end information (databases, text documents, etc.)
and present users with an integrated, consistent conversational
interface to it.
We believe that conversational interfaces offer the ultimate kind
of personalization. Personalization can be defined as the process
of presenting each user of an automated system with an interface
uniquely tailored to his/her preference of content and style of
interaction. Thus, mixed initiative conversational interfaces are
highly personalized since they allow users to interact with systems
using the words they want, to fetch the content they want in the
style they want. Users can converse with such systems by phrasing
their initial queries at a right level of comfort to them (e.g., "I am
looking for a gift for my wife" or "I am looking for a fast
computer with DVD under 1500 dollars").
5. CONCLUSIONS
Based on our results, we conclude that conversational natural
language dialog interfaces offer powerful personalized alternatives
to traditional menu-driven or search-based interfaces to websites.
For such systems, it is especially important to present users with a
consistent interface integrating different styles of interaction and
to have robust dialog management strategies. The system feedback
and the follow up questions should strike a delicate balance
between exposing the system limitations to users, and making
users aware of the flexibility of the system. In current work we are
focusing on developing dynamic, adaptive dialog management,
robust multi-lingual NLP and improving the user interface.
6. REFERENCES
[1] Androutsopoulos, Ion, and Ritchie, Graeme. Natural
Language Interfaces to Databases ? An Introduction, Natural
Language Engineering 1.1:29-81, 1995.
[2] Budzikowska, M., Chai, J., Govindappa, S., Horvath, V.,
Kambhatla, N., Nicolov, N., and Zadrozny, W.
Conversational Sales Assistant for Online Shopping,
Demonstration at Human Language Technologies
Conference (HLT'2001), San Diego, Calif., 2001.
[3] Carpenter, Bob, and Chu-Carroll, J. Natural Language Call
Routing: A Robust, Self-organizing Approach, Proceedings
of the 5th Int. Conf. on Spoken Language Processing. 1998
[4] Chai, J., Lin, J., Zadrozny, W., Ye, Y., Budzikowska, M.,
Horvath, V., Kambhatla, N., and Wolf, C. Comparative
Evaluation of a Natural Language Dialog Based System and
a Menu-Driven System for Information Access: A Case
Study, Proceedings of RIAO 2000, Paris.
[5] Saito, M., and Ohmura, K. A Cognitive Model for Searching
for Ill-defined Targets on the Web - The Relationship
between Search Strategies and User Satisfaction, 21st Int.
Conference on Research and Development in Information
Retrieval, Australia, 1998.
[6] Walker, M., Fromer, J., and Narayanan, S. Learning Optimal
Dialogue Strategies: A Case Study of a Spoken Dialogue
Agent for Email, 36th Annual Meeting of the ACL, Montreal,
Canada, 1998.
[7] Zadrozny, W., Wolf, C., Kambhatla, N. & Ye, Y.
Conversation Machines for Transaction Processing,
Proceedings of AAAI / IAAI - 1998, Madison, Wisconsin,
U.S.A. 1998.
Conversational Sales Assistant for Online Shopping
Margo Budzikowska, Joyce Chai, Sunil Govindappa, Veronika Horvath, Nanda Kambhatla,
Nicolas Nicolov & Wlodek Zadrozny
Conversational Machines Group
IBM T. J. Watson Research Center
30 Saw Mill River Rd, Hawthorne, NY 10532, U.S.A.
{sm1, jchai, govindap, veronika, nanda, nicolas, wlodz}@us.ibm.com
ABSTRACT
Websites of businesses should accommodate both customer
needs and business requirements. Traditional menu-driven
navigation and key word search do not allow users to describe
their intentions precisely. We have developed a conversational
interface to online shopping that provides convenient,
personalized access to information using natural language
dialog. User studies show significantly reduced length of
interactions in terms of time and number of clicks in finding
products. The core dialog engine is easily adaptable to other
domains.
1. INTRODUCTION
Natural language dialog has been used in many areas, such as
for call-center/routing application (Carpenter & Chu-Carroll
1998), email routing (Walker, Fromer & Narayanan 1998),
information retrieval and database access (Androutsopoulos &
Ritchie 1995), and for telephony banking (Zadrozny et al 1998).
In this demonstration, we present a natural language dialog
interface to online shopping. Our user studies show natural
language dialog to be a very effective means for negotiating
user's requests and intentions in this domain.
2. SYSTEM ARCHITECTURE
In our system, a presentation manager captures queries from
users, employs a parser to transform the user's query into a
logical form, and sends the logical form to a dialog manager.
The presentation manager is also responsible for obtaining the
system's response from the dialog manager and presenting it to
the user using template-based generation. The dialog manager
formulates action plans for an action manager to perform back-
end tasks such as database access, business transactions, etc. The
dialog manager applies information state-based dialog strategies
to formulate responses depending on the current state, discourse
history and the action results from the action manager.
The Data Management Subsystem maintains a ?concept?
repository with common sense ?concepts? and a phrasal lexicon
that lists possible ways for referring to the concepts. Business
Rules map concepts to business specifications by defining
concepts using a propositional logic formula of constraints over
product specifications. Thus, the Business Rules reflect business
goals and decisions. The Extended Database combines product
specifications and precompiled evaluations of the concept
definitions for each product to provide a representation that
guides the natural language dialog. We are investigating
automated tools for helping developers and maintainers extract
relevant concepts and terms on the basis of user descriptions and
queries about products.
3. EVALUATION
We conducted several user studies to evaluate the usability of
NLA (Chai et al 2000). In one study, seventeen test subjects
preferred the dialog-driven navigation of NLA two to one over
menu-driven navigation. Moreover, with NLA, the average
number of clicks was reduced by 63.2% and the average time
was reduced by 33.3%. Analysis of the user queries (average
length = 5.31 words long; standard deviation = 2.62; 85% of
inputs are noun phrases) revealed the brevity and relative
linguistic simplicity of user input. Hence, shallow parsing
techniques were adequate for processing user input. In general,
sophisticated dialog management appears to be more important
than the ability to handle complex natural language sentences.
The user studies also highlighted the need to combine multiple
modalities and styles of interaction.
4. REFERENCES
[1] Androutsopoulos, Ion & Ritchie, Graeme. Natural
Language Interfaces to Databases ? An Introduction,
Natural Language Engineering 1.1:29-81, 1995.
[2] Carpenter, Bob & Chu-Carroll, Jeniffer. Natural Language
Call Routing: A Robust, Self-organizing Approach,
Proceedings of the 5th International Conference on Spoken
Language Processing, 1998.
[3] Chai, J., Lin, J., Zadrozny, W., Ye, Y., Budzikowska, M.,
Horvath, V., Kambhatla, N. & Wolf, C. Comparative
Evaluation of a Natural Language Dialog Based System
and a Menu-Driven System for Information Access: A Case
Study, Proceedings of RIAO 2000, Paris, 2000.
[4] Saito, M. & Ohmura, K. A Cognitive Model for Searching
for Ill-defined Targets on the Web ? The Relationship
between Search Strategies and User Satisfaction. 21st Int.
Conf. on Research and Development in Information
Retrieval, Australia, 1998.
[5] Walker, M., Fromer, J. & Narayanan, S. Learning Optimal
Dialogue Strategies: A Case Study of a Spoken Dialogue
Agent for Email, 36th Annual Meeting of the ACL,
Montreal, Canada, 1998.
[6] Zadrozny, W., Wolf, C., Kambhatla, N. & Ye, Y.
Conversation Machines for Transaction Processing,
Proceedings of AAAI / IAAI - 1998, Madison, Wisconsin,
U.S.A., 1998.
HTML
Application
Server
Client
HTTP
Server
HTML
Servlet
Web Server
Network
(HTTP)
Presentation
Manager
Dialog
Manager
Action
Manager
Quick
Parser
Response
Generator Vector Space Engine
Product
Database
Business Rules
Concepts
Data Management
(Off line)
User Interface
Concept
Interpreter
Explanation
ModelPresentationStrategies
Dialog
Strategies
Action
Strategies
input
output
Communication
Acts
Communication
Acts
Action Specs
Online
Interaction Discourse
Analyzer
Extended
PD
Database
Query
Discourse
History
ActionResults
State
Interpreter



A Statistical Model for Multilingual Entity Detection and Tracking
R. Florian, H. Hassan   , A. Ittycheriah, H. Jing
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos
I.B.M. T.J. Watson Research Center
Yorktown Heights, NY 10598
{raduf,abei,hjing,nanda,xiaoluo, nicolas,roukos}@us.ibm.com

hanyh@eg.ibm.com
Abstract
Entity detection and tracking is a relatively new
addition to the repertoire of natural language
tasks. In this paper, we present a statistical
language-independent framework for identify-
ing and tracking named, nominal and pronom-
inal references to entities within unrestricted
text documents, and chaining them into clusters
corresponding to each logical entity present in
the text. Both the mention detection model
and the novel entity tracking model can use
arbitrary feature types, being able to integrate
a wide array of lexical, syntactic and seman-
tic features. In addition, the mention detec-
tion model crucially uses feature streams de-
rived from different named entity classifiers.
The proposed framework is evaluated with sev-
eral experiments run in Arabic, Chinese and
English texts; a system based on the approach
described here and submitted to the latest Au-
tomatic Content Extraction (ACE) evaluation
achieved top-tier results in all three evaluation
languages.
1 Introduction
Detecting entities, whether named, nominal or pronom-
inal, in unrestricted text is a crucial step toward under-
standing the text, as it identifies the important concep-
tual objects in a discourse. It is also a necessary step for
identifying the relations present in the text and populating
a knowledge database. This task has applications in in-
formation extraction and summarization, information re-
trieval (one can get al hits for Washington/person and not
the ones for Washington/state or Washington/city), data
mining and question answering.
The Entity Detection and Tracking task (EDT hence-
forth) has close ties to the named entity recognition
(NER) and coreference resolution tasks, which have been
the focus of attention of much investigation in the recent
past (Bikel et al, 1997; Borthwick et al, 1998; Mikheev
et al, 1999; Miller et al, 1998; Aberdeen et al, 1995;
Ng and Cardie, 2002; Soon et al, 2001), and have been
at the center of several evaluations: MUC-6, MUC-7,
CoNLL?02 and CoNLL?03 shared tasks. Usually, in com-
putational linguistic literature, a named entity represents
an instance of a name, either a location, a person, an or-
ganization, and the NER task consists of identifying each
individual occurrence of such an entity. We will instead
adopt the nomenclature of the Automatic Content Extrac-
tion program1 (NIST, 2003a): we will call the instances
of textual references to objects or abstractions mentions,
which can be either named (e.g. John Mayor), nominal
(e.g. the president) or pronominal (e.g. she, it). An entity
consists of all the mentions (of any level) which refer to
one conceptual entity. For instance, in the sentence
President John Smith said he has no comments.
there are two mentions: John Smith and he (in the order
of appearance, their levels are named and pronominal),
but one entity, formed by the set {John Smith, he}.
In this paper, we present a general statistical frame-
work for entity detection and tracking in unrestricted text.
The framework is not language specific, as proved by ap-
plying it to three radically different languages: Arabic,
Chinese and English. We separate the EDT task into a
mention detection part ? the task of finding all mentions
in the text ? and an entity tracking part ? the task of com-
bining the detected mentions into groups of references to
the same object.
The work presented here is motivated by the ACE eval-
uation framework, which has the more general goal of
building multilingual systems which detect not only enti-
ties, but also relations among them and, more recently,
events in which they participate. The EDT task is ar-
guably harder than traditional named entity recognition,
because of the additional complexity involved in extract-
ing non-named mentions (nominals and pronouns) and
the requirement of grouping mentions into entities.
We present and evaluate empirically statistical mod-
els for both mention detection and entity tracking prob-
lems. For mention detection we use approaches based on
Maximum Entropy (MaxEnt henceforth) (Berger et al,
1996) and Robust Risk Minimization (RRM henceforth)
1For a description of the ACE program see
http://www.nist.gov/speech/tests/ace/.
(Zhang et al, 2002). The task is transformed into a se-
quence classification problem. We investigate a wide ar-
ray of lexical, syntactic and semantic features to perform
the mention detection and classification task including,
for all three languages, features based on pre-existing sta-
tistical semantic taggers, even though these taggers have
been trained on different corpora and use different seman-
tic categories. Moreover, the presented approach implic-
itly learns the correlation between these different seman-
tic types and the desired output types.
We propose a novel MaxEnt-based model for predict-
ing whether a mention should or should not be linked to
an existing entity, and show how this model can be used
to build entity chains. The effectiveness of the approach
is tested by applying it on data from the above mentioned
languages ? Arabic, Chinese, English.
The framework presented in this paper is language-
universal ? the classification method does not make any
assumption about the type of input. Most of the fea-
ture types are shared across the languages, but there are a
small number of useful feature types which are language-
specific, especially for the mention detection task.
The paper is organized as follows: Section 2 describes
the algorithms and feature types used for mention detec-
tion. Section 3 presents our approach to entity tracking.
Section 4 describes the experimental framework and the
systems? results for Arabic, Chinese and English on the
data from the latest ACE evaluation (September 2003), an
investigation of the effect of using different feature types,
as well as a discussion of the results.
2 Mention Detection
The mention detection system identifies the named, nom-
inal and pronominal mentions introduced in the previous
section. Similarly to classical NLP tasks such as base
noun phrase chunking (Ramshaw and Marcus, 1994), text
chunking (Ramshaw and Marcus, 1995) or named entity
recognition (Tjong Kim Sang, 2002), we formulate the
mention detection problem as a classification problem,
by assigning to each token in the text a label, indicating
whether it starts a specific mention, is inside a specific
mention, or is outside any mentions.
2.1 The Statistical Classifiers
Good performance in many natural language process-
ing tasks, such as part-of-speech tagging, shallow pars-
ing and named entity recognition, has been shown to de-
pend heavily on integrating many sources of information
(Zhang et al, 2002; Jing et al, 2003; Ittycheriah et al,
2003). Given the stated focus of integrating many feature
types, we are interested in algorithms that can easily in-
tegrate and make effective use of diverse input types. We
selected two methods which satisfy these criteria: a linear
classifier ? the Robust Risk Minimization classifier ? and
a log-linear classifier ? the Maximum Entropy classifier.
Both methods can integrate arbitrary types of informa-
tion and make a classification decision by aggregating all
information available for a given classification.
Before formally describing the methods2, we introduce
some notations: let
 	



be the set of pre-
dicted classes,  be the example space and 

be the feature space. Each example tRuEcasIng
Lucian Vlad Lita ?
Carnegie Mellon
llita@cs.cmu.edu
Abe Ittycheriah
IBM T.J. Watson
abei@us.ibm.com
Salim Roukos
IBM T.J. Watson
roukos@us.ibm.com
Nanda Kambhatla
IBM T.J. Watson
nanda@us.ibm.com
Abstract
Truecasing is the process of restoring
case information to badly-cased or non-
cased text. This paper explores truecas-
ing issues and proposes a statistical, lan-
guage modeling based truecaser which
achieves an accuracy of ?98% on news
articles. Task based evaluation shows a
26% F-measure improvement in named
entity recognition when using truecasing.
In the context of automatic content ex-
traction, mention detection on automatic
speech recognition text is also improved
by a factor of 8. Truecasing also en-
hances machine translation output legibil-
ity and yields a BLEU score improvement
of 80.2%. This paper argues for the use of
truecasing as a valuable component in text
processing applications.
1 Introduction
While it is true that large, high quality text corpora
are becoming a reality, it is also true that the digital
world is flooded with enormous collections of low
quality natural language text. Transcripts from var-
ious audio sources, automatic speech recognition,
optical character recognition, online messaging and
gaming, email, and the web are just a few exam-
ples of raw text sources with content often produced
in a hurry, containing misspellings, insertions, dele-
tions, grammatical errors, neologisms, jargon terms
? Work done at IBM TJ Watson Research Center
etc. We want to enhance the quality of such sources
in order to produce better rule-based systems and
sharper statistical models.
This paper focuses on truecasing, which is the
process of restoring case information to raw text.
Besides text rEaDaBILiTY, truecasing enhances the
quality of case-carrying data, brings into the pic-
ture new corpora originally considered too noisy for
various NLP tasks, and performs case normalization
across styles, sources, and genres.
Consider the following mildly ambiguous sen-
tence ?us rep. james pond showed up riding an it
and going to a now meeting?. The case-carrying al-
ternative ?US Rep. James Pond showed up riding an
IT and going to a NOW meeting? is arguably better
fit to be subjected to further processing.
Broadcast news transcripts contain casing errors
which reduce the performance of tasks such as
named entity tagging. Automatic speech recognition
produces non-cased text. Headlines, teasers, section
headers - which carry high information content - are
not properly cased for tasks such as question answer-
ing. Truecasing is an essential step in transforming
these types of data into cleaner sources to be used by
NLP applications.
?the president? and ?the President? are two viable
surface forms that correctly convey the same infor-
mation in the same context. Such discrepancies are
usually due to differences in news source, authors,
and stylistic choices. Truecasing can be used as a
normalization tool across corpora in order to pro-
duce consistent, context sensitive, case information;
it consistently reduces expressions to their statistical
canonical form.
In this paper, we attempt to show the benefits of
truecasing in general as a valuable building block
for NLP applications rather than promoting a spe-
cific implementation. We explore several truecasing
issues and propose a statistical, language modeling
based truecaser, showing its performance on news
articles. Then, we present a straight forward appli-
cation of truecasing on machine translation output.
Finally, we demonstrate the considerable benefits of
truecasing through task based evaluations on named
entity tagging and automatic content extraction.
1.1 Related Work
Truecasing can be viewed in a lexical ambiguity res-
olution framework (Yarowsky, 1994) as discriminat-
ing among several versions of a word, which hap-
pen to have different surface forms (casings). Word-
sense disambiguation is a broad scope problem that
has been tackled with fairly good results generally
due to the fact that context is a very good pre-
dictor when choosing the sense of a word. (Gale
et al, 1994) mention good results on limited case
restoration experiments on toy problems with 100
words. They also observe that real world problems
generally exhibit around 90% case restoration accu-
racy. (Mikheev, 1999) also approaches casing dis-
ambiguation but models only instances when capi-
talization is expected: first word in a sentence, after
a period, and after quotes. (Chieu and Ng, 2002)
attempted to extract named entities from non-cased
text by using a weaker classifier but without focus-
ing on regular text or case restoration.
Accents can be viewed as additional surface forms
or alternate word casings. From this perspective, ei-
ther accent identification can be extended to truecas-
ing or truecasing can be extended to incorporate ac-
cent restoration. (Yarowsky, 1994) reports good re-
sults with statistical methods for Spanish and French
accent restoration.
Truecasing is also a specialized method for
spelling correction by relaxing the notion of casing
to spelling variations. There is a vast literature on
spelling correction (Jones and Martin, 1997; Gold-
ing and Roth, 1996) using both linguistic and statis-
tical approaches. Also, (Brill and Moore, 2000) ap-
ply a noisy channel model, based on generic string
to string edits, to spelling correction.
2 Approach
In this paper we take a statistical approach to true-
casing. First we present the baseline: a simple,
straight forward unigram model which performs rea-
sonably well in most cases. Then, we propose a bet-
ter, more flexible statistical truecaser based on lan-
guage modeling.
From a truecasing perspective we observe four
general classes of words: all lowercase (LC), first
letter uppercase (UC), all letters uppercase (CA), and
mixed case word MC). The MC class could be fur-
ther refined into meaningful subclasses but for the
purpose of this paper it is sufficient to correctly iden-
tify specific true MC forms for each MC instance.
We are interested in correctly assigning case la-
bels to words (tokens) in natural language text. This
represents the ability to discriminate between class
labels for the same lexical item, taking into account
the surrounding words. We are interested in casing
word combinations observed during training as well
as new phrases. The model requires the ability to
generalize in order to recognize that even though the
possibly misspelled token ?lenon? has never been
seen before, words in the same context usually take
the UC form.
2.1 Baseline: The Unigram Model
The goal of this paper is to show the benefits of true-
casing in general. The unigram baseline (presented
below) is introduced in order to put task based eval-
uations in perspective and not to be used as a straw-
man baseline.
The vast majority of vocabulary items have only
one surface form. Hence, it is only natural to adopt
the unigram model as a baseline for truecasing. In
most situations, the unigram model is a simple and
efficient model for surface form restoration. This
method associates with each surface form a score
based on the frequency of occurrence. The decoding
is very simple: the true case of a token is predicted
by the most likely case of that token.
The unigram model?s upper bound on truecasing
performance is given by the percentage of tokens
that occur during decoding under their most frequent
case. Approximately 12% of the vocabulary items
have been observed under more than one surface
form. Hence it is inevitable for the unigram model
to fail on tokens such as ?new?. Due to the over-
whelming frequency of its LC form, ?new? will take
this particular form regardless of what token follows
it. For both ?information? and ?york? as subsequent
words, ?new? will be labeled as LC. For the latter
case, ?new? occurs under one of its less frequent sur-
face forms.
2.2 Truecaser
The truecasing strategy that we are proposing seeks
to capture local context and bootstrap it across a
sentence. The case of a token will depend on the
most likely meaning of the sentence - where local
meaning is approximated by n-grams observed dur-
ing training. However, the local context of a few
words alone is not enough for case disambiguation.
Our proposed method employs sentence level con-
text as well.
We capture local context through a trigram lan-
guage model, but the case label is decided at a sen-
tence level. A reasonable improvement over the un-
igram model would have been to decide the word
casing given the previous two lexical items and their
corresponding case content. However, this greedy
approach still disregards global cues. Our goal is
to maximize the probability of a larger text segment
(i.e. a sentence) occurring under a certain surface
form. Towards this goal, we first build a language
model that can provide local context statistics.
2.2.1 Building a Language Model
Language modeling provides features for a label-
ing scheme. These features are based on the prob-
ability of a lexical item and a case content condi-
tioned on the history of previous two lexical items
and their corresponding case content:
Pmodel(w3|w2, w1) = ?trigramP (w3|w2, w1)
+ ?bigramP (w3|w2)
+ ?unigramP (w3)
+ ?uniformP0 (1)
where trigram, bigram, unigram, and uniform prob-
abilities are scaled by individual ?is which are
learned by observing training examples. wi repre-
sents a word with a case tag treated as a unit for
probability estimation.
2.2.2 Sentence Level Decoding
Using the language model probabilities we de-
code the case information at a sentence level. We
construct a trellis (figure 1) which incorporates all
the sentence surface forms as well as the features
computed during training. A node in this trellis con-
sists of a lexical item, a position in the sentence, a
possible casing, as well as a history of the previous
two lexical items and their corresponding case con-
tent. Hence, for each token, all surface forms will
appear as nodes carrying additional context infor-
mation. In the trellis, thicker arrows indicate higher
transition probabilities.
Figure 1: Given individual histories, the decodings
delay and DeLay, are most probable - perhaps in the
context of ?time delay? and respectively ?Senator
Tom DeLay?
The trellis can be viewed as a Hidden Markov
Model (HMM) computing the state sequence
which best explains the observations. The states
(q1, q2, ? ? ? , qn) of the HMM are combinations of
case and context information, the transition proba-
bilities are the language model (?) based features,
and the observations (O1O2 ? ? ?Ot) are lexical items.
During decoding, the Viterbi algorithm (Rabiner,
1989) is used to compute the highest probability
state sequence (q?? at sentence level) that yields the
desired case information:
q?? = argmaxqi1qi2???qitP (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?)
(2)
where P (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?) is the proba-
bility of a given sequence conditioned on the obser-
vation sequence and the model parameters. A more
sophisticated approach could be envisioned, where
either the observations or the states are more expres-
sive. These alternate design choices are not explored
in this paper.
Testing speed depends on the width and length of
the trellis and the overall decoding complexity is:
Cdecoding = O(SMH+1) where S is the sentence
size, M is the number of surface forms we are will-
ing to consider for each word, and H is the history
size (H = 3 in the trigram case).
2.3 Unknown Words
In order for truecasing to be generalizable it must
deal with unknown words ? words not seen during
training. For large training sets, an extreme assump-
tion is that most words and corresponding casings
possible in a language have been observed during
training. Hence, most new tokens seen during de-
coding are going to be either proper nouns or mis-
spellings. The simplest strategy is to consider all
unknown words as being of the UC form (i.e. peo-
ple?s names, places, organizations).
Another approach is to replace the less frequent
vocabulary items with case-carrying special tokens.
During training, the word mispeling is replaced with
by UNKNOWN LC and the word Lenon with UN-
KNOWN UC. This transformation is based on the
observation that similar types of infrequent words
will occur during decoding. This transformation cre-
ates the precedent of unknown words of a particular
format being observed in a certain context. When a
truly unknown word will be seen in the same con-
text, the most appropriate casing will be applied.
This was the method used in our experiments. A
similar method is to apply the case-carrying special
token transformation only to a small random sam-
ple of all tokens, thus capturing context regardless
of frequency of occurrence.
2.4 Mixed Casing
A reasonable truecasing strategy is to focus on to-
ken classification into three categories: LC, UC, and
CA. In most text corpora mixed case tokens such as
McCartney, CoOl, and TheBeatles occur with mod-
erate frequency. Some NLP tasks might prefer map-
ping MC tokens starting with an uppercase letter into
the UC surface form. This technique will reduce the
feature space and allow for sharper models. How-
ever, the decoding process can be generalized to in-
clude mixed cases in order to find a closer fit to the
true sentence. In a clean version of the AQUAINT
(ARDA) news stories corpus, ? 90% of the tokens
occurred under the most frequent surface form (fig-
ure 2).
Figure 2: News domain casing distribution
The expensive brute force approach will consider
all possible casings of a word. Even with the full
casing space covered, some mixed cases will not be
seen during training and the language model prob-
abilities for n-grams containing certain words will
back off to an unknown word strategy. A more fea-
sible method is to account only for the mixed case
items observed during training, relying on a large
enough training corpus. A variable beam decod-
ing will assign non-zero probabilities to all known
casings of each word. An n-best approximation is
somewhat faster and easier to implement and is the
approach employed in our experiments. During the
sentence-level decoding only the n-most-frequent
mixed casings seen during training are considered.
If the true capitalization is not among these n-best
versions, the decoding is not correct. Additional lex-
ical and morphological features might be needed if
identifying MC instances is critical.
2.5 First Word in the Sentence
The first word in a sentence is generally under the
UC form. This sentence-begin indicator is some-
times ambiguous even when paired with sentence-
end indicators such as the period. While sentence
splitting is not within the scope of this paper, we
want to emphasize the fact that many NLP tasks
would benefit from knowing the true case of the first
word in the sentence, thus avoiding having to learn
the fact that beginning of sentences are artificially
important. Since it is uneventful to convert the first
letter of a sentence to uppercase, a more interest-
ing problem from a truecasing perspective is to learn
how to predict the correct case of the first word in a
sentence (i.e. not always UC).
If the language model is built on clean sentences
accounting for sentence boundaries, the decoding
will most likely uppercase the first letter of any sen-
tence. On the other hand, if the language model
is trained on clean sentences disregarding sentence
boundaries, the model will be less accurate since dif-
ferent casings will be presented for the same context
and artificial n-grams will be seen when transition-
ing between sentences. One way to obtain the de-
sired effect is to discard the first n tokens in the train-
ing sentences in order to escape the sentence-begin
effect. The language model is then built on smoother
context. A similar effect can be obtained by initial-
izing the decoding with n-gram state probabilities so
that the boundary information is masked.
3 Evaluation
Both the unigram model and the language model
based truecaser were trained on the AQUAINT
(ARDA) and TREC (NIST) corpora, each consist-
ing of 500M token news stories from various news
agencies. The truecaser was built using IBM?s
ViaVoiceTMlanguage modeling tools. These tools
implement trigram language models using deleted
interpolation for backing off if the trigram is not
found in the training data. The resulting model?s
perplexity is 108.
Since there is no absolute truth when truecasing a
sentence, the experiments need to be built with some
reference in mind. Our assumption is that profes-
sionally written news articles are very close to an
intangible absolute truth in terms of casing. Fur-
thermore, we ignore the impact of diverging stylistic
forms, assuming the differences are minor.
Based on the above assumptions we judge the
truecasing methods on four different test sets. The
first test set (APR) consists of the August 25,
2002 ? top 20 news stories from Associated Press
and Reuters excluding titles, headlines, and sec-
tion headers which together form the second test set
(APR+). The third test set (ACE) consists of ear-
?Randomly chosen test date
Figure 3: LM truecaser vs. unigram baseline.
lier news stories from AP and New York Times be-
longing to the ACE dataset. The last test set (MT)
includes a set of machine translation references (i.e.
human translations) of news articles from the Xin-
hua agency. The sizes of the data sets are as follows:
APR - 12k tokens, ACE - 90k tokens, and MT - 63k
tokens. For both truecasing methods, we computed
the agreement with the original news story consid-
ered to be the ground truth.
3.1 Results
The language model based truecaser consistently
displayed a significant error reduction in case
restoration over the unigram model (figure 3). On
current news stories, the truecaser agreement with
the original articles is ? 98%.
Titles and headlines usually have a higher con-
centration of named entities than normal text. This
also means that they need a more complex model to
assign case information more accurately. The LM
based truecaser performs better in this environment
while the unigram model misses named entity com-
ponents which happen to have a less frequent surface
form.
3.2 Qualitative Analysis
The original reference articles are assumed to have
the absolute true form. However, differences from
these original articles and the truecased articles are
not always casing errors. The truecaser tends to
modify the first word in a quotation if it is not
proper name: ?There has been? becomes ?there has
been?. It also makes changes which could be con-
sidered a correction of the original article: ?Xinhua
BLEU Breakdown
System BLEU 1gr Precision 2gr Precision 3gr Precision 4gr Precision
all lowercase 0.1306 0.6016 0.2294 0.1040 0.0528
rule based 0.1466 0.6176 0.2479 0.1169 0.0627
1gr truecasing 0.2206 0.6948 0.3328 0.1722 0.0988
1gr truecasing+ 0.2261 0.6963 0.3372 0.1734 0.0997
lm truecasing 0.2596 0.7102 0.3635 0.2066 0.1303
lm truecasing+ 0.2642 0.7107 0.3667 0.2066 0.1302
Table 1: BLEU score for several truecasing strategies. (truecasing+ methods additionally employ the ?first
sentence letter uppercased? rule adjustment).
Baseline With Truecasing
Class Recall Precision F Recall Precision F
ENAMEX 48.46 36.04 41.34 59.02 52.65 55.66 (+34.64%)
NUMEX 64.61 72.02 68.11 70.37 79.51 74.66 (+9.62%)
TIMEX 47.68 52.26 49.87 61.98 75.99 68.27 (+36.90%)
Overall 52.50 44.84 48.37 62.01 60.42 61.20 (+26.52%)
Table 2: Named Entity Recognition performance with truecasing and without (baseline).
news agency? becomes ?Xinhua News Agency? and
?northern alliance? is truecased as ?Northern Al-
liance?. In more ambiguous cases both the original
version and the truecased fragment represent differ-
ent stylistic forms: ?prime minister Hekmatyar? be-
comes ?Prime Minister Hekmatyar?.
There are also cases where the truecaser described
in this paper makes errors. New movie names are
sometimes miss-cased: ?my big fat greek wedding?
or ?signs?. In conducive contexts, person names
are correctly cased: ?DeLay said in?. However, in
ambiguous, adverse contexts they are considered to
be common nouns: ?pond? or ?to delay that?. Un-
seen organization names which make perfectly nor-
mal phrases are erroneously cased as well: ?interna-
tional security assistance force?.
3.3 Application: Machine Translation
Post-Processing
We have applied truecasing as a post-processing step
to a state of the art machine translation system in or-
der to improve readability. For translation between
Chinese and English, or Japanese and English, there
is no transfer of case information. In these situations
the translation output has no case information and it
is beneficial to apply truecasing as a post-processing
step. This makes the output more legible and the
system performance increases if case information is
required.
We have applied truecasing to Chinese-to-English
translation output. The data source consists of news
stories (2500 sentences) from the Xinhua News
Agency. The news stories are first translated, then
subjected to truecasing. The translation output is
evaluated with BLEU (Papineni et al, 2001), which
is a robust, language independent automatic ma-
chine translation evaluation method. BLEU scores
are highly correlated to human judges scores, pro-
viding a way to perform frequent and accurate au-
tomated evaluations. BLEU uses a modified n-gram
precision metric and a weighting scheme that places
more emphasis on longer n-grams.
In table 1, both truecasing methods are applied to
machine translation output with and without upper-
casing the first letter in each sentence. The truecas-
ing methods are compared against the all letters low-
ercased version of the articles as well as against an
existing rule-based system which is aware of a lim-
ited number of entity casings such as dates, cities,
and countries. The LM based truecaser is very ef-
fective in increasing the readability of articles and
captures an important aspect that the BLEU score is
sensitive to. Truecasig the translation output yields
Baseline With Truecasing
Source Recall Precision F Recall Precision F
BNEWS ASR 23 3 5 56 39 46 (+820.00%)
BNEWS HUMAN 77 66 71 77 68 72 (+1.41%)
XINHUA 76 71 73 79 72 75 (+2.74%)
Table 3: Results of ACE mention detection with and without truecasing.
an improvement ? of 80.2% in BLEU score over the
existing rule base system.
3.4 Task Based Evaluation
Case restoration and normalization can be employed
for more complex tasks. We have successfully lever-
aged truecasing in improving named entity recogni-
tion and automatic content extraction.
3.4.1 Named Entity Tagging
In order to evaluate the effect of truecasing on ex-
tracting named entity labels, we tested an existing
named entity system on a test set that has signif-
icant case mismatch to the training of the system.
The base system is an HMM based tagger, similar
to (Bikel et al, 1997). The system has 31 semantic
categories which are extensions on the MUC cate-
gories. The tagger creates a lattice of decisions cor-
responding to tokenized words in the input stream.
When tagging a word wi in a sentence of words
w0...wN , two possibilities. If a tag begins:
p(tN1 |wN1 )i = p(ti|ti?1, wi?1)p?(wi|ti, wi?1)
If a tag continues:
p(tN1 |wN1 )i = p(wi|ti, wi?1)
The ? indicates that the distribution is formed from
words that are the first words of entities. The p? dis-
tribution predicts the probability of seeing that word
given the tag and the previous word instead of the
tag and previous tag. Each word has a set of fea-
tures, some of which indicate the casing and embed-
ded punctuation. These models have several levels
of back-off when the exact trigram has not been seen
in training. A trellis spanning the 31 futures is built
for each word in a sentence and the best path is de-
rived using the Viterbi algorithm.
?Truecasing improves legibility, not the translation itself
The performance of the system shown in table 2
indicate an overall 26.52% F-measure improvement
when using truecasing. The alternative to truecas-
ing text is to destroy case information in the train-
ing material 	 SNORIFY procedure in (Bikel et al,
1997). Case is an important feature in detecting
most named entities but particularly so for the title
of a work, an organization, or an ambiguous word
with two frequent cases. Truecasing the sentence is
essential in detecting that ?To Kill a Mockingbird? is
the name of a book, especially if the quotation marks
are left off.
3.4.2 Automatic Content Extraction
Automatic Content Extraction (ACE) is task fo-
cusing on the extraction of mentions of entities and
relations between them from textual data. The tex-
tual documents are from newswire, broadcast news
with text derived from automatic speech recognition
(ASR), and newspaper with text derived from optical
character recognition (OCR) sources. The mention
detection task (ace, 2001) comprises the extraction
of named (e.g. ?Mr. Isaac Asimov?), nominal (e.g.
?the complete author?), and pronominal (e.g. ?him?)
mentions of Persons, Organizations, Locations, Fa-
cilities, and Geo-Political Entities.
The automatically transcribed (using ASR) broad-
cast news documents and the translated Xinhua
News Agency (XINHUA) documents in the ACE
corpus do not contain any case information, while
human transcribed broadcast news documents con-
tain casing errors (e.g. ?George bush?). This prob-
lem occurs especially when the data source is noisy
or the articles are poorly written.
For all documents from broadcast news (human
transcribed and automatically transcribed) and XIN-
HUA sources, we extracted mentions before and af-
ter applying truecasing. The ASR transcribed broad-
cast news data comprised 86 documents containing
a total of 15,535 words, the human transcribed ver-
sion contained 15,131 words. There were only two
XINHUA documents in the ACE test set containing
a total of 601 words. None of this data or any ACE
data was used for training the truecasing models.
Table 3 shows the result of running our ACE par-
ticipating maximum entropy mention detection sys-
tem on the raw text, as well as on truecased text. For
ASR transcribed documents, we obtained an eight
fold improvement in mention detection from 5% F-
measure to 46% F-measure. The low baseline score
is mostly due to the fact that our system has been
trained on newswire stories available from previous
ACE evaluations, while the latest test data included
ASR output. It is very likely that the improvement
due to truecasing will be more modest for the next
ACE evaluation when our system will be trained on
ASR output as well.
4 Possible Improvements & Future Work
Although the statistical model we have considered
performs very well, further improvements must go
beyond language modeling, enhancing how expres-
sive the model is. Additional features are needed
during decoding to capture context outside of the
current lexical item, medium range context, as well
as discontinuous context. Another potentially help-
ful feature to consider would provide a distribu-
tion over similar lexical items, perhaps using an
edit/phonetic distance.
Truecasing can be extended to cover a more gen-
eral notion surface form to include accents. De-
pending on the context, words might take different
surface forms. Since punctuation is a notion exten-
sion to surface form, shallow punctuation restora-
tion (e.g. word followed by comma) can also be ad-
dressed through truecasing.
5 Conclusions
We have discussed truecasing, the process of restor-
ing case information to badly-cased or non-cased
text, and we have proposed a statistical, language
modeling based truecaser which has an agreement
of ?98% with professionally written news articles.
Although its most direct impact is improving legibil-
ity, truecasing is useful in case normalization across
styles, genres, and sources. Truecasing is a valu-
able component in further natural language process-
ing. Task based evaluation shows a 26% F-measure
improvement in named entity recognition when us-
ing truecasing. In the context of automatic content
extraction, mention detection on automatic speech
recognition text is improved by a factor of 8. True-
casing also enhances machine translation output leg-
ibility and yields a BLEU score improvement of
80.2% over the original system.
References
2001. Entity detection and tracking. ACE Pilot Study
Task Definition.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: A high-performance learning name
finder. pages 194?201.
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. ACL.
H.L. Chieu and H.T. Ng. 2002. Teaching a weaker clas-
sifier: Named entity recognition on upper case text.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1994. Discrimination decisions for
100,000-dimensional spaces. Current Issues in Com-
putational Linguistics, pages 429?450.
Andrew R. Golding and Dan Roth. 1996. Applying win-
now to context-sensitive spelling correction. ICML.
M. P. Jones and J. H. Martin. 1997. Contextual spelling
correction using latent semantic analysis. ANLP.
A. Mikheev. 1999. A knowledge-free method for capi-
talized word disambiguation.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. IBM Research Re-
port.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Read-
ings in Speech Recognition, pages 267?295.
David Yarowsky. 1994. Decision lists for ambiguity res-
olution: Application to accent restoration in spanish
and french. ACL, pages 88?95.
A Mention-Synchronous Coreference Resolution Algorithm Based on the
Bell Tree
Xiaoqiang Luo and Abe Ittycheriah
Hongyan Jing and Nanda Kambhatla and Salim Roukos
1101 Kitchawan Road
Yorktown Heights, NY 10598, U.S.A.
{xiaoluo,abei,hjing,nanda,roukos}@us.ibm.com
Abstract
This paper proposes a new approach for
coreference resolution which uses the Bell
tree to represent the search space and casts
the coreference resolution problem as finding
the best path from the root of the Bell tree to
the leaf nodes. A Maximum Entropy model
is used to rank these paths. The coreference
performance on the 2002 and 2003 Auto-
matic Content Extraction (ACE) data will be
reported. We also train a coreference system
using the MUC6 data and competitive results
are obtained.
1 Introduction
In this paper, we will adopt the terminologies used in
the Automatic Content Extraction (ACE) task (NIST,
2003). Coreference resolution in this context is defined
as partitioning mentions into entities. A mention is an
instance of reference to an object, and the collection
of mentions referring to the same object in a document
form an entity. For example, in the following sentence,
mentions are underlined:
?The American Medical Association voted
yesterday to install the heir apparent as its
president-elect, rejecting a strong, upstart
challenge by a District doctor who argued
that the nation?s largest physicians? group
needs stronger ethics and new leadership.?
?American Medical Association?, ?its? and ?group?
belong to the same entity as they refer to the same ob-
ject.
Early work of anaphora resolution focuses on find-
ing antecedents of pronouns (Hobbs, 1976; Ge et al,
1998; Mitkov, 1998), while recent advances (Soon et
al., 2001; Yang et al, 2003; Ng and Cardie, 2002; Itty-
cheriah et al, 2003) employ statistical machine learn-
ing methods and try to resolve reference among all
kinds of noun phrases (NP), be it a name, nominal, or
pronominal phrase ? which is the scope of this paper
as well. One common strategy shared by (Soon et al,
2001; Ng and Cardie, 2002; Ittycheriah et al, 2003) is
that a statistical model is trained to measure how likely
a pair of mentions corefer; then a greedy procedure is
followed to group mentions into entities. While this ap-
proach has yielded encouraging results, the way men-
tions are linked is arguably suboptimal in that an instant
decision is made when considering whether two men-
tions are linked or not.
In this paper, we propose to use the Bell tree to rep-
resent the process of forming entities from mentions.
The Bell tree represents the search space of the coref-
erence resolution problem ? each leaf node corresponds
to a possible coreference outcome. We choose to model
the process from mentions to entities represented in the
Bell tree, and the problem of coreference resolution is
cast as finding the ?best? path from the root node to
leaves. A binary maximum entropy model is trained to
compute the linking probability between a partial entity
and a mention.
The rest of the paper is organized as follows. In
Section 2, we present how the Bell tree can be used
to represent the process of creating entities from men-
tions and the search space. We use a maximum en-
tropy model to rank paths in the Bell tree, which is dis-
cussed in Section 3. After presenting the search strat-
egy in Section 4, we show the experimental results on
the ACE 2002 and 2003 data, and the Message Under-
standing Conference (MUC) (MUC, 1995) data in Sec-
tion 5. We compare our approach with some recent
work in Section 6.
2 Bell Tree: From Mention to Entity
Let us consider traversing mentions in a document from
beginning (left) to end (right). The process of form-
ing entities from mentions can be represented by a tree
structure. The root node is the initial state of the pro-
cess, which consists of a partial entity containing the
first mention of a document. The second mention is
[1][2] 3*
[1][2][3]
[1] [23]
[13][2]
[123]
[12][3]
[1] 2* 3
[1]
[12]
[1]
[2]
(c1)
(c5)
(b1)
(c2)
(c3)
(c4)
(a) [12] 3*
(b2)
Figure 1: Bell tree representation for three mentions:
numbers in [] denote a partial entity. In-focus entities
are marked on the solid arrows, and active mentions
are marked by *. Solid arrows signify that a mention
is linked with an in-focus partial entity while dashed
arrows indicate starting of a new entity.
added in the next step by either linking to the exist-
ing entity, or starting a new entity. A second layer
of nodes are created to represent the two possible out-
comes. Subsequent mentions are added to the tree in
the same manner. The process is mention-synchronous
in that each layer of tree nodes are created by adding
one mention at a time. Since the number of tree leaves
is the number of possible coreference outcomes and it
equals the Bell Number (Bell, 1934), the tree is called
the Bell tree. The Bell Number
 
is the num-
ber of ways of partitioning

distinguishable objects
(i.e., mentions) into non-empty disjoint subsets (i.e.,
entities). The Bell Number has a ?closed? formula
 
	


and it increases rapidly as

in-
creases:
 Combining Lexical, Syntactic, and Semantic Features with
Maximum Entropy Models for Extracting Relations
Nanda Kambhatla
IBM T. J. Watson Research Center
1101 Kitchawan Road Route 134
Yorktown Heights, NY 10598
nanda@us.ibm.com
Abstract
Extracting semantic relationships between entities
is challenging because of a paucity of annotated
data and the errors induced by entity detection mod-
ules. We employ Maximum Entropy models to
combine diverse lexical, syntactic and semantic fea-
tures derived from the text. Our system obtained
competitive results in the Automatic Content Ex-
traction (ACE) evaluation. Here we present our gen-
eral approach and describe our ACE results.
1 Introduction
Extraction of semantic relationships between en-
tities can be very useful for applications such as
biography extraction and question answering, e.g.
to answer queries such as ?Where is the Taj Ma-
hal??. Several prior approaches to relation extrac-
tion have focused on using syntactic parse trees.
For the Template Relations task of MUC-7, BBN
researchers (Miller et al, 2000) augmented syn-
tactic parse trees with semantic information corre-
sponding to entities and relations and built genera-
tive models for the augmented trees. More recently,
(Zelenko et al, 2003) have proposed extracting rela-
tions by computing kernel functions between parse
trees and (Culotta and Sorensen, 2004) have ex-
tended this work to estimate kernel functions be-
tween augmented dependency trees.
We build Maximum Entropy models for extract-
ing relations that combine diverse lexical, syntactic
and semantic features. Our results indicate that us-
ing a variety of information sources can result in
improved recall and overall F measure. Our ap-
proach can easily scale to include more features
from a multitude of sources?e.g. WordNet, gazat-
teers, output of other semantic taggers etc.?that can
be brought to bear on this task. In this paper, we
present our general approach, describe the features
we currently use and show the results of our partic-
ipation in the ACE evaluation.
Automatic Content Extraction (ACE, 2004) is an
evaluation conducted by NIST to measure Entity
Detection and Tracking (EDT) and relation detec-
tion and characterization (RDC). The EDT task en-
tails the detection of mentions of entities and chain-
ing them together by identifying their coreference.
In ACE vocabulary, entities are objects, mentions
are references to them, and relations are explic-
itly or implicitly stated relationships among enti-
ties. Entities can be of five types: persons, organiza-
tions, locations, facilities, and geo-political entities
(geographically defined regions that define a politi-
cal boundary, e.g. countries, cities, etc.). Mentions
have levels: they can be names, nominal expressions
or pronouns.
The RDC task detects implicit and explicit rela-
tions1 between entities identified by the EDT task.
Here is an example:
The American Medical Association
voted yesterday to install the heir ap-
parent as its president-elect, rejecting a
strong, upstart challenge by a District
doctor who argued that the nation?s
largest physicians? group needs stronger
ethics and new leadership.
In electing Thomas R. Reardon, an
Oregon general practitioner who had
been the chairman of its board, ...
In this fragment, all the underlined phrases are men-
tions referring to the American Medical Associa-
tion, or to Thomas R. Reardon or the board (an or-
ganization) of the American Medical Association.
Moreover, there is an explicit management rela-
tion between chairman and board, which are ref-
erences to Thomas R. Reardon and the board of the
American Medical Association respectively. Rela-
tion extraction is hard, since successful extraction
implies correctly detecting both the argument men-
tions, correctly chaining these mentions to their re-
1Explict relations occur in text with explicit evidence sug-
gesting the relationship. Implicit relations need not have ex-
plicit supporting evidence in text, though they should be evi-
dent from a reading of the document.
Type Subtype Count
AT based-In 496
located 2879
residence 395
NEAR relative-location 288
PART other 6
part-Of 1178
subsidiary 366
ROLE affiliate-partner 219
citizen-Of 450
client 159
founder 37
general-staff 1507
management 1559
member 1404
other 174
owner 274
SOCIAL associate 119
grandparent 10
other-personal 108
other-professional 415
other-relative 86
parent 149
sibling 23
spouse 89
Table 1: The list of relation types and subtypes used
in the ACE 2003 evaluation.
spective entities, and correctly determining the type
of relation that holds between them.
This paper focuses on the relation extraction
component of our ACE system. The reader is re-
ferred to (Florian et al, 2004; Ittycheriah et al,
2003; Luo et al, 2004) for more details of our men-
tion detection and mention chaining modules. In the
next section, we describe our extraction system. We
present results in section 3, and we conclude after
making some general observations in section 4.
2 Maximum Entropy models for
extracting relations
We built Maximum Entropy models for predicting
the type of relation (if any) between every pair of
mentions within each sentence. We only model
explicit relations, because of poor inter-annotator
agreement in the annotation of implicit relations.
Table 1 lists the types and subtypes of relations
for the ACE RDC task, along with their frequency
of occurence in the ACE training data2. Note that
only 6 of these 24 relation types are symmetric:
2The reader is referred to (Strassel et al, 2003) or LDC?s
web site for more details of the data.
?relative-location?, ?associate?, ?other-relative?,
?other-professional?, ?sibling?, and ?spouse?. We
only model the relation subtypes, after making them
unique by concatenating the type where appropri-
ate (e.g. ?OTHER? became ?OTHER-PART? and
?OTHER-ROLE?). We explicitly model the argu-
ment order of mentions. Thus, when comparing
mentions   and   , we distinguish between the case
where    -citizen-Of-   and    -citizen-Of-   . We
thus model the extraction as a classification problem
with 49 classes, two for each relation subtype and a
?NONE? class for the case where the two mentions
are not related.
For each pair of mentions, we compute several
feature streams shown below. All the syntactic fea-
tures are derived from the syntactic parse tree and
the dependency tree that we compute using a statis-
tical parser trained on the PennTree Bank using the
Maximum Entropy framework (Ratnaparkhi, 1999).
The feature streams are:
Words The words of both the mentions and all the
words in between.
Entity Type The entity type (one of PERSON,
ORGANIZATION, LOCATION, FACILITY,
Geo-Political Entity or GPE) of both the men-
tions.
Mention Level The mention level (one of NAME,
NOMINAL, PRONOUN) of both the men-
tions.
Overlap The number of words (if any) separating
the two mentions, the number of other men-
tions in between, flags indicating whether the
two mentions are in the same noun phrase, verb
phrase or prepositional phrase.
Dependency The words and part-of-speech and
chunk labels of the words on which the men-
tions are dependent in the dependency tree de-
rived from the syntactic parse tree.
Parse Tree The path of non-terminals (removing
duplicates) connecting the two mentions in the
parse tree, and the path annotated with head
words.
Here is an example. For the sentence fragment,
been the chairman of its board ...
the corresponding syntactic parse tree is shown in
Figure 1 and the dependency tree is shown in Figure
2. For the pair of mentions chairman and board,
the feature streams are shown below.
Words 	
  ,   , 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473?480,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Factorizing Complex Models: A Case Study in Mention
Detection
Radu Florian, Hongyan Jing, Nanda Kambhatla and Imed Zitouni
IBM TJ Watson Research Center
Yorktown Heights, NY 10598
{raduf,hjing,nanda,izitouni}@us.ibm.com
Abstract
As natural language understanding re-
search advances towards deeper knowledge
modeling, the tasks become more and more
complex: we are interested in more nu-
anced word characteristics, more linguistic
properties, deeper semantic and syntactic
features. One such example, explored in
this article, is the mention detection and
recognition task in the Automatic Content
Extraction project, with the goal of iden-
tifying named, nominal or pronominal ref-
erences to real-world entities?mentions?
and labeling them with three types of in-
formation: entity type, entity subtype and
mention type. In this article, we investi-
gate three methods of assigning these re-
lated tags and compare them on several
data sets. A system based on the methods
presented in this article participated and
ranked very competitively in the ACE?04
evaluation.
1 Introduction
Information extraction is a crucial step toward un-
derstanding and processing natural language data,
its goal being to identify and categorize impor-
tant information conveyed in a discourse. Exam-
ples of information extraction tasks are identifi-
cation of the actors and the objects in written
text, the detection and classification of the rela-
tions among them, and the events they participate
in. These tasks have applications in, among other
fields, summarization, information retrieval, data
mining, question answering, and language under-
standing.
One of the basic tasks of information extraction
is the mention detection task. This task is very
similar to named entity recognition (NER), as the
objects of interest represent very similar concepts.
The main difference is that the latter will identify,
however, only named references, while mention de-
tection seeks named, nominal and pronominal ref-
erences. In this paper, we will call the identified
references mentions ? using the ACE (NIST, 2003)
nomenclature ? to differentiate them from entities
which are the real-world objects (the actual person,
location, etc) to which the mentions are referring
to1.
Historically, the goal of the NER task was to find
named references to entities and quantity refer-
ences ? time, money (MUC-6, 1995; MUC-7, 1997).
In recent years, Automatic Content Extraction
evaluation (NIST, 2003; NIST, 2004) expanded the
task to also identify nominal and pronominal refer-
ences, and to group the mentions into sets referring
to the same entity, making the task more compli-
cated, as it requires a co-reference module. The set
of identified properties has also been extended to
include the mention type of a reference (whether it
is named, nominal or pronominal), its subtype (a
more specific type dependent on the main entity
type), and its genericity (whether the entity points
to a specific entity, or a generic one2), besides the
customary main entity type. To our knowledge,
little research has been done in the natural lan-
guage processing context or otherwise on investi-
gating the specific problem of how such multiple la-
bels are best assigned. This article compares three
methods for such an assignment.
The simplest model which can be considered for
the task is to create an atomic tag by ?gluing? to-
gether the sub-task labels and considering the new
label atomic. This method transforms the prob-
lem into a regular sequence classification task, sim-
ilar to part-of-speech tagging, text chunking, and
named entity recognition tasks. We call this model
the all-in-one model. The immediate drawback
of this model is that it creates a large classifica-
tion space (the cross-product of the sub-task clas-
sification spaces) and that, during decoding, par-
tially similar classifications will compete instead of
cooperate - more details are presented in Section
3.1. Despite (or maybe due to) its relative sim-
plicity, this model obtained good results in several
instances in the past, for POS tagging in morpho-
logically rich languages (Hajic and Hladka?, 1998)
1In a pragmatic sense, entities are sets of mentions
which co-refer.
2This last attribute, genericity, depends only loosely
on local context. As such, it should be assigned while
examining all mentions in an entity, and for this reason
is beyond the scope of this article.
473
and mention detection (Jing et al, 2003; Florian
et al, 2004).
At the opposite end of classification methodol-
ogy space, one can use a cascade model, which per-
forms the sub-tasks sequentially in a predefined or-
der. Under such a model, described in Section 3.3,
the user will build separate models for each sub-
task. For instance, it could first identify the men-
tion boundaries, then assign the entity type, sub-
type, and mention level information. Such a model
has the immediate advantage of having smaller
classification spaces, with the drawback that it re-
quires a specific model invocation path.
In between the two extremes, one can use a joint
model, which models the classification space in the
same way as the all-in-one model, but where the
classifications are not atomic. This system incor-
porates information about sub-model parts, such
as whether the current word starts an entity (of
any type), or whether the word is part of a nomi-
nal mention.
The paper presents a novel contrastive analysis
of these three models, comparing them on several
datasets in three languages selected from the ACE
2003 and 2004 evaluations. The methods described
here are independent of the underlying classifiers,
and can be used with any sequence classifiers. All
experiments in this article use our in-house imple-
mentation of a maximum entropy classifier (Flo-
rian et al, 2004), which we selected because of its
flexibility of integrating arbitrary types of features.
While we agree that the particular choice of classi-
fier will undoubtedly introduce some classifier bias,
we want to point out that the described procedures
have more to do with the organization of the search
space, and will have an impact, one way or another,
on most sequence classifiers, including conditional
random field classifiers.3
The paper is organized as follows: Section 2 de-
scribes the multi-task classification problem and
prior work, Section 3.3 presents and contrasts the
three meta-classification models. Section 4 outlines
the experimental setup and the obtained results,
and Section 5 concludes the paper.
2 Multi-Task Classification
Many tasks in Natural Language Processing in-
volve labeling a word or sequence of words with
a specific property; classic examples are part-of-
speech tagging, text chunking, word sense disam-
biguation and sentiment classification. Most of the
time, the word labels are atomic labels, containing
a very specific piece of information (e.g. the word
3While not wishing to delve too deep into the issue
of label bias, we would also like to point out (as it
was done, for instance, in (Klein, 2003)) that the label
bias of MEMM classifiers can be significantly reduced
by allowing them to examine the right context of the
classification point - as we have done with our model.
is noun plural, or starts a noun phrase, etc). There
are cases, though, where the labels consist of sev-
eral related, but not entirely correlated, properties;
examples include mention detection?the task we
are interested in?, syntactic parsing with func-
tional tag assignment (besides identifying the syn-
tactic parse, also label the constituent nodes with
their functional category, as defined in the Penn
Treebank (Marcus et al, 1993)), and, to a lesser
extent, part-of-speech tagging in highly inflected
languages.4
The particular type of mention detection that we
are examining in this paper follows the ACE gen-
eral definition: each mention in the text (a refer-
ence to a real-world entity) is assigned three types
of information:5
? An entity type, describing the type of the en-
tity it points to (e.g. person, location, organi-
zation, etc)
? An entity subtype, further detailing the type
(e.g. organizations can be commercial, gov-
ernmental and non-profit, while locations can
be a nation, population center, or an interna-
tional region)
? A mention type, specifying the way the en-
tity is realized ? a mention can be named
(e.g. John Smith), nominal (e.g. professor),
or pronominal (e.g. she).
Such a problem ? where the classification consists
of several subtasks or attributes ? presents addi-
tional challenges, when compared to a standard
sequence classification task. Specifically, there are
inter-dependencies between the subtasks that need
to be modeled explicitly; predicting the tags inde-
pendently of each other will likely result in incon-
sistent classifications. For instance, in our running
example of mention detection, the subtype task is
dependent on the entity type; one could not have a
person with the subtype non-profit. On the other
hand, the mention type is relatively independent of
the entity type and/or subtype: each entity type
could be realized under any mention type and vice-
versa.
The multi-task classification problem has been
subject to investigation in the past. Caruana
et al (1997) analyzed the multi-task learning
4The goal there is to also identify word properties
such as gender, number, and case (for nouns), mood
and tense (for verbs), etc, besides the main POS tag.
The task is slightly different, though, as these proper-
ties tend to have a stronger dependency on the lexical
form of the classified word.
5There is a fourth assigned type ? a flag specifying
whether a mention is specific (i.e. it refers at a clear
entity), generic (refers to a generic type, e.g. ?the sci-
entists believe ..?), unspecified (cannot be determined
from the text), or negative (e.g. ?no person would do
this?). The classification of this type is beyond the
goal of this paper.
474
(MTL) paradigm, where individual related tasks
are trained together by sharing a common rep-
resentation of knowledge, and demonstrated that
this strategy yields better results than one-task-at-
a-time learning strategy. The authors used a back-
propagation neural network, and the paradigm was
tested on several machine learning tasks. It also
contains an excellent discussion on how and why
the MTL paradigm is superior to single-task learn-
ing. Florian and Ngai (2001) used the same multi-
task learning strategy with a transformation-based
learner to show that usually disjointly handled
tasks perform slightly better under a joint model;
the experiments there were run on POS tagging
and text chunking, Chinese word segmentation and
POS tagging. Sutton et al (2004) investigated
the multitask classification problem and used a dy-
namic conditional random fields method, a gener-
alization of linear-chain conditional random fields,
which can be viewed as a probabilistic generaliza-
tion of cascaded, weighted finite-state transducers.
The subtasks were represented in a single graphi-
cal model that explicitly modeled the sub-task de-
pendence and the uncertainty between them. The
system, evaluated on POS tagging and base-noun
phrase segmentation, improved on the sequential
learning strategy.
In a similar spirit to the approach presented in
this article, Florian (2002) considers the task of
named entity recognition as a two-step process:
the first is the identification of mention boundaries
and the second is the classification of the identified
chunks, therefore considering a label for each word
being formed from two sub-labels: one that spec-
ifies the position of the current word relative in a
mention (outside any mentions, starts a mention, is
inside a mention) and a label specifying the men-
tion type . Experiments on the CoNLL?02 data
show that the two-process model yields consider-
ably higher performance.
Hacioglu et al (2005) explore the same task, in-
vestigating the performance of the AIO and the
cascade model, and find that the two models have
similar performance, with the AIO model having a
slight advantage. We expand their study by adding
the hybrid joint model to the mix, and further in-
vestigate different scenarios, showing that the cas-
cade model leads to superior performance most of
the time, with a few ties, and show that the cas-
cade model is especially beneficial in cases where
partially-labeled data (only some of the component
labels are given) is available. It turns out though,
(Hacioglu, 2005) that the cascade model in (Ha-
cioglu et al, 2005) did not change to a ?mention
view? sequence classification6 (as we did in Section
3.3) in the tasks following the entity detection, to
allow the system to use longer range features.
6As opposed to a ?word view?.
3 Classification Models
This section presents the three multi-task classifi-
cation models, which we will experimentally con-
trast in Section 4. We are interested in performing
sequence classification (e.g. assigning a label to
each word in a sentence, otherwise known as tag-
ging). Let X denote the space of sequence elements
(words) and Y denote the space of classifications
(labels), both of them being finite spaces. Our goal
is to build a classifier
h : X+ ? Y+
which has the property that |h (x?)| = |x?| ,?x? ? X+
(i.e. the size of the input sequence is preserved).
This classifier will select the a posteriori most likely
label sequence y? = argmaxy?? p
(y??|x?); in our case
p (y?|x?) is computed through the standard Markov
assumption:
p (y1,m| x?) =
?
i
p (yi|x?, yi?n+1,i?1) (1)
where yi,j denotes the sequence of labels yi..yj .
Furthermore, we will assume that each label y
is composed of a number of sub-labels y =(y1y2 . . . yk)7; in other words, we will assume the
factorization of the label space into k subspaces
Y = Y1 ? Y2 ? . . .? Yk.
The classifier we used in the experimental sec-
tion is a maximum entropy classifier (similar to
(McCallum et al, 2000))?which can integrate sev-
eral sources of information in a rigorous manner.
It is our empirical observation that, from a perfor-
mance point of view, being able to use a diverse
and abundant feature set is more important than
classifier choice, and the maximum entropy frame-
work provides such a utility.
3.1 The All-In-One Model
As the simplest model among those presented here,
the all-in-one model ignores the natural factoriza-
tion of the output space and considers all labels as
atomic, and then performs regular sequence clas-
sification. One way to look at this process is the
following: the classification space Y = Y1 ? Y2 ?
. . . ? Yk is first mapped onto a same-dimensional
space Z through a one-to-one mapping o : Y ? Z;
then the features of the system are defined on the
space X+ ?Z, instead of X+ ? Y.
While having the advantage of being simple, it
suffers from some theoretical disadvantages:
? The classification space can be very large, be-
ing the product of the dimensions of sub-task
spaces. In the case of the 2004 ACE data
there are 7 entity types, 4 mention types and
many subtypes; the observed number of actual
7We can assume, without any loss of generality, that
all labels have the same number of sub-labels.
475
All-In-One Model Joint Model
B-PER
B-LOC
B-ORG B-
B-MISC
Table 1: Features predicting start of an entity in
the all-in-one and joint models
sub-label combinations on the training data is
401. Since the dynamic programing (Viterbi)
search?s runtime dependency on the classifica-
tion space is O (|Z|n) (n is the Markov depen-
dency size), using larger spaces will negatively
impact the decoding run time.8
? The probabilities p (zi|x?, zi?n,i?1) require
large data sets to be computed properly. If
the training data is limited, the probabilities
might be poorly estimated.
? The model is not friendly to partial evaluation
or weighted sub-task evaluation: different, but
partially similar, labels will compete against
each other (because the system will return a
probability distribution over the classification
space), sometimes resulting in wrong partial
classification.9
? The model cannot directly use data that is
only partially labeled (i.e. not all sub-labels
are specified).
Despite the above disadvantages, this model has
performed well in practice: Hajic and Hladka?
(1998) applied it successfully to find POS se-
quences for Czech and Florian et al (2004) re-
ports good results on the 2003 ACE task. Most
systems that participated in the CoNLL 2002 and
2003 shared tasks on named entity recognition
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003) applied this model, as they
modeled the identification of mention boundaries
and the assignment of mention type at the same
time.
3.2 The Joint Model
The joint model differs from the all-in-one model
in the fact that the labels are no longer atomic: the
features of the system can inspect the constituent
sub-labels. This change helps alleviate the data
8From a practical point of view, it might not be very
important, as the search is pruned in most cases to only
a few hypotheses (beam-search); in our case, pruning
the beam only introduced an insignificant model search
error (0.1 F-measure).
9To exemplify, consider that the system outputs the
following classifications and probabilities: O (0.2), B-
PER-NAM (0.15), B-PER-NOM (0.15); even the latter
2 suggest that the word is the start of a person mention,
the O label will win because the two labels competed
against each other.
Detect Boundaries   & Entity Types
Assemble full tag
Detect Entity Subtype Detect Mention Type
Figure 1: Cascade flow example for mention detec-
tion.
sparsity encountered by the previous model by al-
lowing sub-label modeling. The joint model the-
oretically compares favorably with the all-in-one
model:
? The probabilities p (yi|x?, yi?n,i?1) =
p
((y1i , . . . , yki
) |x?,
(
yji?n,i?1
)
j=1,k
)
might
require less training data to be properly
estimated, as different sub-labels can be
modeled separately.
? The joint model can use features that predict
just one or a subset of the sub-labels. Ta-
ble 1 presents the set of basic features that
predict the start of a mention for the CoNLL
shared tasks for the two models. While the
joint model can encode the start of a mention
in one feature, the all-in-one model needs to
use four features, resulting in fewer counts per
feature and, therefore, yielding less reliably es-
timated features (or, conversely, it needs more
data for the same estimation confidence).
? The model can predict some of the sub-tags
ahead of the others (i.e. create a dependency
structure on the sub-labels). The model used
in the experimental section predicts the sub-
labels by using only sub-labels for the previous
words, though.
? It is possible, though computationally expen-
sive, for the model to use additional data
that is only partially labeled, with the model
change presented later in Section 3.4.
3.3 The Cascade Model
For some tasks, there might already exist a natural
hierarchy among the sub-labels: some sub-labels
could benefit from knowing the value of other,
primitive, sub-labels. For example,
? For mention detection, identifying the men-
tion boundaries can be considered as a primi-
tive task. Then, knowing the mention bound-
aries, one can assign an entity type, subtype,
and mention type to each mention.
? In the case of parsing with functional tags, one
can perform syntactic parsing, then assign the
functional tags to the internal constituents.
476
Words Since Donna Karan International went public in 1996 ...
Labels O B-ORG I-ORG I-ORG O O O O ...
Figure 2: Sequence tagging for mention detection: the case for a cascade model.
? For POS tagging, one can detect the main
POS first, then detect the other specific prop-
erties, making use of the fact that one knows
the main tag.
The cascade model is essentially a factorization
of individual classifiers for the sub-tasks; in this
framework, we will assume that there is a more
or less natural dependency structure among sub-
tasks, and that models for each of the subtasks
will be built and applied in the order defined by
the dependency structure. For example, as shown
in Figure 1, one can detect mention boundaries and
entity type (at the same time), then detect mention
type and subtype in ?parallel? (i.e. no dependency
exists between these last 2 sub-tags).
A very important advantage of the cascade
model is apparent in classification cases where
identifying chunks is involved (as is the case with
mention detection), similar to advantages that
rescoring hypotheses models have: in the second
stage, the chunk classification stage, it can switch
to a mention view, where the classification units
are entire mentions and words outside of mentions.
This allows the system to make use of aggregate
features over the mention words (e.g. all the words
are capitalized), and to also effectively use a larger
Markov window (instead of 2-3 words, it will use 2-
3 chunks/words around the word of interest). Fig-
ure 2 contains an example of such a case: the cas-
cade model will have to predict the type of the
entire phrase Donna Karan International, in the
context ?Since <chunk> went public in ..?, which
will give it a better opportunity to classify it as an
organization. In contrast, because the joint model
and AIO have a word view of the sentence, will lack
the benefit of examining the larger region, and will
not have access at features that involve partial fu-
ture classifications (such as the fact that another
mention of a particular type follows).
Compared with the other two models, this clas-
sification method has the following advantages:
? The classification spaces for each subtask are
considerably smaller; this fact enables the cre-
ation of better estimated models
? The problem of partially-agreeing competing
labels is completely eliminated
? One can easily use different/additional data to
train any of the sub-task models.
3.4 Adding Partially Labeled Data
Annotated data can be sometimes expensive to
come by, especially if the label set is complex. But
not all sub-tasks were created equal: some of them
might be easier to predict than others and, there-
fore, require less data to train effectively in a cas-
cade setup. Additionally, in realistic situations,
some sub-tasks might be considered to have more
informational content than others, and have prece-
dence in evaluation. In such a scenario, one might
decide to invest resources in annotating additional
data only for the particularly interesting sub-task,
which could reduce this effort significantly.
To test this hypothesis, we annotated additional
data with the entity type only. The cascade model
can incorporate this data easily: it just adds it
to the training data for the entity type classifier
model. While it is not immediately apparent how
to incorporate this new data into the all-in-one and
joint models, in order to maintain fairness in com-
paring the models, we modified the procedures to
allow for the inclusion. Let T denote the original
training data, and T ? denote the additional train-
ing data.
For the all-in-one model, the additional training
data cannot be incorporated directly; this is an in-
herent deficiency of the AIO model. To facilitate a
fair comparison, we will incorporate it in an indi-
rect way: we train a classifier C on the additional
training data T ?, which we then use to classify the
original training data T . Then we train the all-
in-one classifier on the original training data T ,
adding the features defined on the output of ap-
plying the classifier C on T .
The situation is better for the joint model: the
new training data T ? can be incorporated directly
into the training data T .10 The maximum entropy
model estimates the model parameters by maxi-
mizing the data log-likelihood
L =
?
(x,y)
p? (x, y) log q? (y|x)
where p? (x, y) is the observed probability dis-
tribution of the pair (x, y) and q? (y|x) =
1
Z
?
j exp (?j ? fj (x, y)) is the conditional ME
probability distribution as computed by the model.
In the case where some of the data is partially an-
notated, the log-likelihood becomes
L =
?
(x,y)?T ?T ?
p? (x, y) log q? (y|x)
10The solution we present here is particular for
MEMM models (though similar solutions may exist for
other models as well). We also assume the reader is fa-
miliar with the normal MaxEnt training procedure; we
present here only the differences to the standard algo-
rithm. See (Manning and Schu?tze, 1999) for a good
description.
477
=
?
(x,y)?T
p? (x, y) log q? (y|x)
+
?
(x,y)?T ?
p? (x, y) log q? (y|x) (2)
The only technical problem that we are faced with
here is that we cannot directly estimate the ob-
served probability p? (x, y) for examples in T ?, since
they are only partially labeled. Borrowing the
idea from the expectation-maximization algorithm
(Dempster et al, 1977), we can replace this proba-
bility by the re-normalized system proposed prob-
ability: for (x, yx) ? T ?, we define
q? (x, y) = p? (x) ? (y ? yx) q? (y|x)?
y??yx q? (y?|x)? ?? ?
=q??(y|x)
where yx is the subset of labels from Y which are
consistent with the partial classification of x in T ?.
? (y ? yx) is 1 if and only if y is consistent with
the partial classification yx.11 The log-likelihood
computation in Equation (2) becomes
L =
?
(x,y)?T
p? (x, y) log q? (y|x)
+
?
(x,y)?T ?
q? (x, y) log q? (y|x)
To further simplify the evaluation, the quantities
q? (x, y) are recomputed every few steps, and are
considered constant as far as finding the optimum
? values is concerned (the partial derivative com-
putations and numerical updates otherwise become
quite complicated, and the solution is no longer
unique). Given this new evaluation function, the
training algorithm will proceed exactly the same
way as in the normal case where all the data is
fully labeled.
4 Experiments
All the experiments in this section are run on the
ACE 2003 and 2004 data sets, in all the three
languages covered: Arabic, Chinese, and English.
Since the evaluation test set is not publicly avail-
able, we have split the publicly available data into
a 80%/20% data split. To facilitate future compar-
isons with work presented here, and to simulate a
realistic scenario, the splits are created based on
article dates: the test data is selected as the last
20% of the data in chronological order. This way,
the documents in the training and test data sets
do not overlap in time, and the ones in the test
data are posterior to the ones in the training data.
Table 2 presents the number of documents in the
training/test datasets for the three languages.
11For instance, the full label B-PER is consistent
with the partial label B, but not with O or I.
Language Training Test
Arabic 511 178
Chinese 480 166
English 2003 658 139
English 2004 337 114
Table 2: Datasets size (number of documents)
Each word in the training data is labeled with
one of the following properties:12
? if it is not part of any entity, it?s labeled as O
? if it is part of an entity, it contains a tag spec-
ifying whether it starts a mention (B -) or is
inside a mention (I -). It is also labeled with
the entity type of the mention (seven possible
types: person, organization, location, facility,
geo-political entity, weapon, and vehicle), the
mention type (named, nominal, pronominal,
or premodifier13), and the entity subtype (de-
pends on the main entity type).
The underlying classifier used to run the experi-
ments in this article is a maximum entropy model
with a Gaussian prior (Chen and Rosenfeld, 1999),
making use of a large range of features, includ-
ing lexical (words and morphs in a 3-word win-
dow, prefixes and suffixes of length up to 4, Word-
Net (Miller, 1995) for English), syntactic (POS
tags, text chunks), gazetteers, and the output of
other information extraction models. These fea-
tures were described in (Florian et al, 2004), and
are not discussed here. All three methods (AIO,
joint, and cascade) instantiate classifiers based on
the same feature types whenever possible. In terms
of language-specific processing, the Arabic system
uses as input morphological segments, while the
Chinese system is a character-based model (the in-
put elements x ? X are characters), but it has
access to word segments as features.
Performance in the ACE task is officially eval-
uated using a special-purpose measure, the ACE
value metric (NIST, 2003; NIST, 2004). This
metric assigns a score based on the similarity be-
tween the system?s output and the gold-standard
at both mention and entity level, and assigns dif-
ferent weights to different entity types (e.g. the
person entity weights considerably more than a fa-
cility entity, at least in the 2003 and 2004 evalu-
ations). Since this article focuses on the mention
detection task, we decided to use the more intu-
itive (unweighted) F-measure: the harmonic mean
of precision and recall.
12The mention encoding is the IOB2 encoding pre-
sented in (Tjong Kim Sang and Veenstra, 1999) and
introduced by (Ramshaw and Marcus, 1994) for the
task of base noun phrase chunking.
13This is a special class, used for mentions that mod-
ify other labeled mentions; e.g. French in ?French
wine?. This tag is specific only to ACE?04.
478
For the cascade model, the sub-task flow is pre-
sented in Figure 1. In the first step, we identify
the mention boundaries together with their entity
type (e.g. person, organization, etc). In prelimi-
nary experiments, we tried to ?cascade? this task.
The performance was similar on both strategies;
the separated model would yield higher recall at
the expense of precision, while the combined model
would have higher precision, but lower recall. We
decided to use in the system with higher precision.
Once the mentions are identified and classified with
the entity type property, the data is passed, in par-
allel, to the mention type detector and the subtype
detector.
For English and Arabic, we spent three person-
weeks to annotate additional data labeled with
only the entity type information: 550k words for
English and 200k words for Arabic. As mentioned
earlier, adding this data to the cascade model is a
trivial task: the data just gets added to the train-
ing data, and the model is retrained. For the AIO
model, we have build another mention classifier on
the additional training data, and labeled the orig-
inal ACE training data with it. It is important
to note here that the ACE training data (called
T in Section 3.4) is consistent with the additional
training data T ?: the annotation guidelines for T ?
are the same as for the original ACE data, but we
only labeled entity type information. The result-
ing classifications are then used as features in the
final AIO classifier. The joint model uses the addi-
tional partially-labeled data in the way described
in Section 3.4; the probabilities q? (x, y) are updated
every 5 iterations.
Table 3 presents the results: overall, the cascade
model performs significantly better than the all-
in-one model in four out the six tested cases - the
numbers presented in bold reflect that the differ-
ence in performance to the AIO model is statisti-
cally significant.14 The joint model, while manag-
ing to recover some ground, falls in between the
AIO and the cascade models.
When additional partially-labeled data was
available, the cascade and joint models receive a
statistically significant boost in performance, while
the all-in-one model?s performance barely changes.
This fact can be explained by the fact that the en-
tity type-only model is in itself errorful; measuring
the performance of the model on the training data
yields a performance of 82 F-measure;15 therefore
the AIO model will only access partially-correct
14To assert the statistical significance of the results,
we ran a paired Wilcoxon test over the series obtained
by computing F-measure on each document in the test
set. The results are significant at a level of at least
0.009.
15Since the additional training data is consistent in
the labeling of the entity type, such a comparison is in-
deed possible. The above mentioned score is on entity
types only.
Language Data+ A-I-O Joint Cascade
Arabic?04 no 59.2 59.1 59.7
yes 59.4 60.0 60.7
English?04 no 72.1 72.3 73.7
yes 72.5 74.1 75.2
Chinese?04 no 71.2 71.7 71.7
English ?03 no 79.5 79.5 79.7
Table 3: Experimental results: F-measure on the
full label
Language Data+ A-I-O Joint Cascade
Arabic?04 no 66.3 66.5 67.5
yes 66.4 67.9 68.9
English?04 no 77.9 78.1 79.2
yes 78.3 80.5 82.6
Chinese?04 no 75.4 76.1 76.8
English ?03 no 80.4 80.4 81.1
Table 4: F-measure results on entity type only
data, and is unable to make effective use of it.
In contrast, the training data for the entity type
in the cascade model effectively triples, and this
change is reflected positively in the 1.5 increase in
F-measure.
Not all properties are equally valuable: the en-
tity type is arguably more interesting than the
other properties. If we restrict ourselves to eval-
uating the entity type output only (by projecting
the output label to the entity type only), the differ-
ence in performance between the all-in-one model
and cascade is even more pronounced, as shown in
Table 4. The cascade model outperforms here both
the all-in-one and joint models in all cases except
English?03, where the difference is not statistically
significant.
As far as run-time speed is concerned, the AIO
and cascade models behave similarly: our imple-
mentation tags approximately 500 tokens per sec-
ond (averaged over the three languages, on a Pen-
tium 3, 1.2Ghz, 2Gb of memory). Since a MaxEnt
implementation is mostly dependent on the num-
ber of features that fire on average on a example,
and not on the total number of features, the joint
model runs twice as slow: the average number of
features firing on a particular example is consider-
ably higher. On average, the joint system can tag
approximately 240 words per second. The train
time is also considerably longer; it takes 15 times as
long to train the joint model as it takes to train the
all-in-one model (60 mins/iteration compared to
4 mins/iteration); the cascade model trains faster
than the AIO model.
One last important fact that is worth mention-
ing is that a system based on the cascade model
participated in the ACE?04 competition, yielding
very competitive results in all three languages.
479
5 Conclusion
As natural language processing becomes more so-
phisticated and powerful, we start focus our at-
tention on more and more properties associated
with the objects we are seeking, as they allow for
a deeper and more complex representation of the
real world. With this focus comes the question of
how this goal should be accomplished ? either de-
tect all properties at once, one at a time through
a pipeline, or a hybrid model. This paper presents
three methods through which multi-label sequence
classification can be achieved, and evaluates and
contrasts them on the Automatic Content Extrac-
tion task. On the ACE mention detection task,
the cascade model which predicts first the mention
boundaries and entity types, followed by mention
type and entity subtype outperforms the simple all-
in-one model in most cases, and the joint model in
a few cases.
Among the proposed models, the cascade ap-
proach has the definite advantage that it can easily
and productively incorporate additional partially-
labeled data. We also presented a novel modifica-
tion of the joint system training that allows for the
direct incorporation of additional data, which in-
creased the system performance significantly. The
all-in-one model can only incorporate additional
data in an indirect way, resulting in little to no
overall improvement.
Finally, the performance obtained by the cas-
cade model is very competitive: when paired with a
coreference module, it ranked very well in the ?En-
tity Detection and Tracking? task in the ACE?04
evaluation.
References
R. Caruana, L. Pratt, and S. Thrun. 1997. Multitask
learning. Machine Learning, 28:41.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, Computer Sci-
ence Department, Carnegie Mellon University.
A. P. Dempster, N. M. Laird, , and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal statistical Soci-
ety, 39(1):1?38.
R. Florian and G. Ngai. 2001. Multidimensional
transformation-based learning. In Proceedings of
CoNLL?01, pages 1?8.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N Nicolov, and S Roukos.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proceedings of the Human
Language Technology Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004, pages 1?8.
R. Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceedings
of CoNLL-2002, pages 175?178.
Kadri Hacioglu, Benjamin Douglas, and Ying Chen.
2005. Detection of entity mentions occuring in en-
glish and chinese text. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 379?386, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Kadri Hacioglu. 2005. Private communication.
J. Hajic and Hladka?. 1998. Tagging inflective lan-
guages: Prediction of morphological categories for a
rich, structured tagset. In Proceedings of the 36th
Annual Meeting of the ACL and the 17th ICCL,
pages 483?490, Montre?al, Canada.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. It-
tycheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Proceed-
ings of EMNLP?03, pages 200?207.
Dan Klein. 2003. Maxent models, conditional estima-
tion, and optimization, without the magic. Tutorial
presented at NAACL-03 and ACL-03.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19:313?330.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models
for information extraction and segmentation. In Pro-
ceedings of ICML-2000.
G. A. Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38(11).
MUC-6. 1995. The sixth mes-
sage understanding conference.
www.cs.nyu.edu/cs/faculty/grishman/muc6.html.
MUC-7. 1997. The seventh mes-
sage understanding conference.
www.itl.nist.gov/iad/894.02/related projects/
muc/proceedings/muc 7 toc.html.
NIST. 2003. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
NIST. 2004. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
L. Ramshaw and M. Marcus. 1994. Exploring the sta-
tistical derivation of transformational rule sequences
for part-of-speech tagging. In Proceedings of the
ACL Workshop on Combining Symbolic and Statis-
tical Approaches to Language, pages 128?135.
C. Sutton, K. Rohanimanesh, and A. McCallum.
2004. Dynamic conditional random fields: Factor-
ized probabilistic models for labeling and segment-
ing sequence data. In In Proceedings of the Twenty-
First International Conference on Machine Learning
(ICML-2004).
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmonton,
Canada.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL?99.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named en-
tity recognition. In Proceedings of CoNLL-2002,
pages 155?158.
480
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 460?466,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Minority Vote: At-Least-N Voting
Improves Recall for Extracting Relations
Nanda Kambhatla
IBM T.J. Watson Research Center
1101 Kitchawan Road Rt 134
Yorktown, NY 10598
nanda@us.ibm.com
Abstract
Several NLP tasks are characterized by
asymmetric data where one class label
NONE, signifying the absence of any
structure (named entity, coreference, re-
lation, etc.) dominates all other classes.
Classifiers built on such data typically
have a higher precision and a lower re-
call and tend to overproduce the NONE
class. We present a novel scheme for vot-
ing among a committee of classifiers that
can significantly boost the recall in such
situations. We demonstrate results show-
ing up to a 16% relative improvement in
ACE value for the 2004 ACE relation ex-
traction task for English, Arabic and Chi-
nese.
1 Introduction
Statistical classifiers are widely used for diverse
NLP applications such as part of speech tagging
(Ratnaparkhi, 1999), chunking (Zhang et al, 2002),
semantic parsing (Magerman, 1993), named entity
extraction (Borthwick, 1999; Bikel et al, 1997; Flo-
rian et al, 2004), coreference resolution (Soon et al,
2001), relation extraction (Kambhatla, 2004), etc. A
number of these applications are characterized by a
dominance of a NONE class in the training exam-
ples. For example, for coreference resolution, classi-
fiers might classify whether a given pair of mentions
are references to the same entity or not. In this case,
we typically have a lot more examples of mention
pairs that are not coreferential (i.e. the NONE class)
than otherwise. Similarly, if a classifier is predicting
the presence/absence of a semantic relation between
two mentions, there are typically far more examples
signifying an absence of a relation.
Classifiers built with asymmetric data dominated
by one class (a NONE class donating absence of a
relation or coreference or a named entity etc.) can
overgenerate the NONE class. This often results in a
unbalanced classifier where precision is higher than
recall.
In this paper, we present a novel approach for
improving the recall of such classifiers by using a
new voting scheme from a committee of classifiers.
There are a plethora of algorithms for combining
classifiers (e.g. see (Xu et al, 1992)). A widely
used approach is a majority voting scheme, where
each classifier in the committee gets a vote and the
class with the largest number of votes ?wins? (i.e. the
corresponding class is output as the prediction of the
committee).
We are interested in improving overall recall and
reduce the overproduction of the class NONE. Our
scheme predicts the class label C obtaining the sec-
ond highest number of votes when NONE gets the
highest number of votes, provided C gets at least
N votes. Thus, we predict a label other than NONE
when there is some evidence of the presense of the
structure we are looking for (relations, coreference,
named entities, etc.) even in the absense of a clear
majority.
This paper is organized as follows. In section 2,
we give an overview of the various schemes for com-
bining classifiers. In section 3, we present our vot-
460
ing algorithm. In section 4, we describe the ACE
relation extraction task. In section 5, we present em-
pirical results for relation extraction and we discuss
our results and conclude in section 6.
2 Combining Classifiers
Numerous methods for combining classifiers have
been proposed and utlized to improve the perfor-
mance of different NLP tasks such as part of speech
tagging (Brill and Wu, 1998), identifying base noun
phrases (Tjong Kim Sang et al, 2000), named en-
tity extraction (Florian et al, 2003), etc. Ho et al
(1994) investigated different approaches for rerank-
ing the outputs of a committee of classifiers and also
explored union and intersection methods for reduc-
ing the set of predicted categories. Florian et al
(2002) give a broad overview of methods for com-
bining classifiers and present empirical results for
word sense disambiguation.
Xu et al(1992) and Florian et al(2002) consider
three approaches for combining classifiers. In the
first approach, individual classifiers output posterior
probabilities that are merged (e.g. by taking an av-
erage) to arrive at a composite posterior probability
of each class. In the second scheme, each classifier
outputs a ranked list of classes instead of a proba-
bility distribution and the different ranked lists are
merged to arrive at a final ranking. Methods us-
ing the third approach, often called voting methods,
treat each classifier as a black box that outputs only
the top ranked class and combines these to arrive at
the final decision (class). The choice of approach
and the specific method of combination may be con-
strained by the specific classification algorithms in
use.
In this paper, we focus on voting methods, since
for small data sets, it is hard to reliably estimate
probability distributions or even a complete order-
ing of classes especially when the number of classes
is large.
A widely used voting method for combining clas-
sifiers is a Majority Vote scheme (e.g. (Brill and
Wu, 1998; Tjong Kim Sang et al, 2000)). Each
classifier gets to vote for its top ranked class and
the class with the highest number of votes ?wins?.
Henderson et al(1999) use a Majority Vote scheme
where different parsers vote on constituents? mem-
bership in a hypothesized parse. Halteren et al
(1998) compare a number of voting methods includ-
ing a Majority Vote scheme with other combination
methods for part of speech tagging.
In this paper, we induce multiple classifiers by us-
ing bagging (Breiman, 1996). Following Breiman?s
approach, we obtain multiple classifiers by first
making bootstrap replicates of the training data and
training different classifiers on each of the replicates.
The bootstrap replicates are induced by repeatedly
sampling with replacement training events from the
original training data to arrive at replicate data sets
of the same size as the training data set. Breiman
(1996) uses a Majority Vote scheme for combining
the output of the classifiers. In the next section, we
will describe the different voting schemes we ex-
plored in our work.
3 At-Least-N Voting
We are specifically interested in NLP tasks char-
acterized by asymmetric data where, typically, we
have far more occurances of a NONE class that sig-
inifies the absense of structure (e.g. a named en-
tity, or a coreference relation or a semantic relation).
Classifiers trained on such data sets can overgener-
ate the NONE class, and thus have a higher preci-
sion and lower recall in discovering the underlying
structure (i.e. the named entities or coreference links
etc.). With such tasks, the benefits yielded by a Ma-
jority Vote is limited, since, because of the asym-
metry in the data, a majority of the classifiers might
predict NONE most of the time.
We propose alternative voting schemes, dubbed
At-Least-N Voting, to deal with the overproduction
of NONE. Given a committee of classifiers (obtained
by bagging or some other mechanism), the classi-
fiers first cast their vote. If the majority vote is for a
class C other than NONE, we simply output C as the
prediction. If the majority vote is for NONE, we out-
put the class label obtaining the second highest num-
ber of votes, provided it has at least N votes. Thus,
we choose to defer to the minority vote of classifiers
which agree on finding some structure even when
the majority of classifiers vote for NONE. We expect
this voting scheme to increase recall at the expense
of precision.
At-Least-N Voting induces a spectrum of combi-
461
nation methods ranging from a Majority Vote (when
N is more than half of the total number of classifiers)
to a scheme, where the evidence of any structure by
even one classifier is believed (At-Least-1 Voting).
The exact choice of N is an empirical one and de-
pends on the amount of asymmetry in the data and
the imbalance between precision and recall in the
classifiers.
4 The ACE Relation Extraction Task
Automatic Content Extraction (ACE) is an annual
evaluation conducted by NIST (NIST, 2004) on in-
formation extraction, focusing on extraction of en-
tities, events, and relations. The Entity Detection
and Recognition task entails detection of mentions
of entities and grouping together the mentions that
are references to the same entity. In ACE terminol-
ogy, mentions are references in text (or audio, chats,
...) to real world entities. Similarly relation men-
tions are references in text to semantic relations be-
tween entity mentions and relations group together
all relation mentions that identify the same semantic
relation between the same entities.
In the frament of text:
John?s son, Jim went for a walk. Jim liked
his father.
all the underlined words are mentions referring to
two entities, John, and Jim. Morover, John and
Jim have a family relation evidenced as two relation
mentions ?John?s son? between the entity mentions
?John? and ?son? and ?his father? between the entity
mentions ?his? and ?father?.
In the relation extraction task, systems must pre-
dict the presence of a predetermined set of binary
relations among mentions of entities, label the rela-
tion, and identify the two arguments. In the 2004
ACE evaluation, systems were evaluated on their ef-
ficacy in correctly identifying relations among both
system output entities and with ?true? entities (i.e. as
annotated by human annotators as opposed to sys-
tem output). In this paper, we present results for ex-
tracting relations between ?true? entities.
Table 1 shows the set of relation types, subtypes,
and their frequency counts in the training data for the
2004 ACE evaluation. For training classifiers, the
great paucity of positive training events (where rela-
tions exist) compared to the negative events (where
Type Subtype Count
ART user-or-owner 140
(agent artifact) inventor/manufacturer 3
other 6
EMP-ORG employ-executive 420
employ-staff 416
employ-undetermined 62
member-of-group 126
partner 11
subsidiary 213
other 37
GPE-AFF citizen-or-resident 173
(GPE affiliation) based-in 225
other 63
DISCOURSE -none- 122
PHYSICAL located 516
near 81
part-whole 333
PER-SOC business 119
(personal/social) family 115
other 28
OTHER-AFF ethnic 28
(PER/ORG ideology 26
affiliation) other 27
Table 1: The set of types and subtypes of relations
used in the 2004 ACE evaluation.
relations do not exist) suggest that schemes for im-
proving recall might benefit this task.
5 Experimental Results
In this section, we present results of experiments
comparing three different methods of combining
classifiers for ACE relation extraction:
? At-Least-N for different values of N,
? Majority Voting, and
? a simple algorithm, called summing, where we
add the posterior scores for each class from all
the classifiers and select the class with the max-
imum summed score.
Since the official ACE evaluation set is not pub-
licly available, to facilitate comparison with our re-
sults and for internal testing of our algorithms, for
each language (English, Arabic, and Chinese), we
462
En Ar Ch
Training Set (documents) 227 511 480
Training Set (rel-mentions) 3290 4126 4347
Test Set (documents) 114 178 166
Test Set (rel-mentions) 1381 1894 1774
Table 2: The Division of LDC annotated data into
training and development test sets.
divided the ACE 2004 training data provided by
LDC in a roughly 75%:25% ratio into a training set
and a test set. Table 2 summarizes the number of
documents and the number of relation mentions in
each data set. The test sets were deliberately chosen
to be the most recent 25% of documents in chrono-
logical order, since entities and relations in news
tend to repeat and random shuffles can greatly re-
duce the out-of-vocabulary problem.
5.1 Maximum Entropy Classifiers
We used bagging (Breiman, 1996) to create replicate
training sets of the same size as the original training
set by repeatedly sampling with replacement from
the training set. We created 25 replicate training sets
(bags) for each language (Arabic, Chinese, English)
and trained separate maximum entropy classifiers on
each bag. We then applied At-Least-N (N = 1,2,5),
Majority Vote, and Summing algorithms with the
trained classifiers and measured the resulting perfor-
mance on our development set.
For each bag, we built maximum entropy models
to predict the presence of relation mentions and the
type and subtype of relations, when their presence
is predicted. Our models operate on every pair of
mentions in a document that are not references to
the same entity, to extract relation mentions. Since
there are 23 unique type-subtype pairs in Table 1,
our classifiers have 47 classes: two classes for each
pair corresponding to the two argument orderings
(e.g. ?John?s son? vs. ?his father?) and a NONE
class signifying no relation.
Similar to our earlier work (Kambhatla, 2004),
we used a combination of lexical, syntactic, and se-
mantic features including all the words in between
the two mentions, the entity types and subtypes of
the two mentions, the number of words in between
the two mentions, features derived from the small-
est parse fragment connecting the two mentions, etc.
These features were held constant throughout these
experiments.
5.2 Results
We report the F-measure, precision and recall for
extracting relation mentions for all three languages.
We also report ACE value1, the official metric used
by NIST that assigns 0% value to a system that pro-
duces no output and a 100% value to a system that
extracts all relations without generating any false
alarms. Note that the ACE value counts each rela-
tion only once even if it is expressed in text many
times as different relation mentions. The reader is
referred to the NIST web site (NIST, 2004) for more
details on the ACE value computation.
Figures 1(a), 1(b), and 1(c) show the F-measure,
precision, and recall respectively for the English test
set obtained by different classifier combination tech-
niques as we vary the number of bags. Figures 2(a),
2(b), and 2(c) show similar curves for Chinese, and
Figures 3(a), 3(b), and 3(c) show similar curves for
Arabic. All these figures show the performance of a
single classifier as a straight line.
From the plots, it is clear that our hope of increas-
ing recall by combining classifiers is realized for all
three languages. As expected, the recall rises fastest
for At-Least-N when N is small, i.e when small mi-
nority opinion or even a single dissenting opinion is
being trusted. Of course, the rise in recall is at the
expense of a loss of precision. Overall, At-Least-N
for intermediate ranges of N (N=5 for English and
Chinese and N=2 for Arabic) performs best where
the moderate loss in precision is more than offset by
a rise in recall.
Both the Majority Vote method and the Summing
method succeed in avoiding a sharp loss of preci-
sion. However, they fail to increase the recall signif-
icantly either.
Table 3 summarizes the best results (F-measure)
for each classifier combination method for all three
languages compared with the result for a single clas-
sifier. At their best operating points, all three combi-
nation methods handily outperform the single clas-
sifier. At-Least-N seems to have a slight edge over
the other two methods, but the difference is small.
1Here we use the ACE value metric used for the ACE 2004
evaluation
463
 43
 44
 45
 46
 47
 48
 49
 50
 0  5  10  15  20  25
F
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(a) F-measure
 46
 48
 50
 52
 54
 56
 58
 60
 62
 64
 66
 68
 0  5  10  15  20  25
Pr
ec
is
io
n
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(b) Precision
 34
 36
 38
 40
 42
 44
 46
 0  5  10  15  20  25
R
ec
al
l
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(c) Recall
Figure 1: Comparing F-measure, precision, and recall of different voting schemes for English relation
extraction.
 61
 62
 63
 64
 65
 66
 67
 0  5  10  15  20  25
F
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(a) F-measure
 56
 58
 60
 62
 64
 66
 68
 70
 72
 74
 76
 0  5  10  15  20  25
Pr
ec
is
io
n
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(b) Precision
 52
 54
 56
 58
 60
 62
 64
 66
 68
 70
 0  5  10  15  20  25
R
ec
al
l
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(c) Recall
Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relation
extraction.
 25
 26
 27
 28
 29
 30
 31
 0  5  10  15  20  25
F
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(a) F-measure
 28
 30
 32
 34
 36
 38
 40
 42
 44
 0  5  10  15  20  25
Pr
ec
is
io
n
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(b) Precision
 18
 20
 22
 24
 26
 28
 30
 0  5  10  15  20  25
R
ec
al
l
Number of Bags
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
(c) Recall
Figure 3: Comparing F-measure, precision, and recall of different voting schemes for Arabic relation ex-
traction.
464
English Arabic Chinese
Single 46.87 27.47 63.75
At-Least-N 49.52 30.41 66.79
Majority Vote 49.24 29.02 66.21
Summing 48.66 29.02 66.40
Table 3: Comparing the best F-measure obtained by
At-Least-N Voting with Majority Voting, Summing
and the single best classifier.
English Arabic Chinese
Single 59.6 37.3 69.6
At-Least-N 63.9 43.5 71.0
Table 4: Comparing the ACE Value obtained by At-
Least-N Voting with the single best classifier for the
operating points used in Table 3.
Table 4 shows the ACE value obtained by our
best performing classifier combination method (At-
Least-N at the operating points in Table 3) compared
with a single classifier. Note that while the improve-
ment for Chinese is slight, for Arabic performance
improves by over 16% relative and for English, the
improvement is over 7% relative over the single clas-
sifier2. Since the ACE value collapses relation men-
tions referring to the same relation, finding new re-
lations (i.e. recall) is more important. This might
explain the relatively larger difference in ACE value
between the single classifier performance and At-
Least-N.
The rules of the ACE evaluation prohibit us from
presenting a detailed comparison of our relation ex-
traction system with the other participants. How-
ever, our relation extraction system (using the At-
Least-N classifier combination scheme as described
here) performed very competitively in 2004 ACE
evaluation both in the system output relation ex-
traction task (RDR) and the relation extraction task
where the ?true? mentions and entities are given.
Due to time limitations, we did not try At-Least-N
with N > 5. From the plots, there is a potential for
getting greater gains by experimenting with a larger
2Note that ACE value metric used in the ACE 2004 eval-
uation weights entitites differently based on their type. Thus,
relations with PERSON-NAME arguments end up contribut-
ing a lot more the overall score than relations with FACILITY-
PRONOUN arguments.
number of bags and with a larger N.
6 Discussion
Several NLP problems exhibit a dominance of a
NONE class that typically signifies a lack of struc-
ture like a named entity, coreference, etc. Especially
when coupled with small training sets, this results in
classifiers with unbalanced precision and recall. We
have argued that a classifier voting scheme that is fo-
cused on improving recall can help increase overall
performance in such situations.
We have presented a class of voting methods,
dubbed At-Least-N that defer to the opinion of a mi-
nority of classifiers (consisting of N members) even
when the majority predicts NONE. This can boost
recall at the expense of precision. However, by vary-
ing N and the number of classifiers, we can pick an
operating point that improves the overall F-measure.
We have presented results for ACE relation ex-
traction for three languages comparing At-Least-N
with Majority Vote and Summing methods for com-
bining classifiers. All three classifier combination
methods significantly outperform a single classifier.
Also, At-Least-N consistently gave us the best per-
formance across different languages.
We used bagging to induce multiple classifiers for
our task. Because of the random bootstrap sam-
pling, different replicate training sets might tilt to-
wards one class or another. Thus, if we have many
classifiers trained on the replicate training sets, some
of them are likely to be better at predicting certain
classes than others. In future, we plan to experi-
ment with other methods for collecting a committee
of classifiers.
References
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of ANLP-97, pages 194?201.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York
University.
L. Breiman. 1996. Bagging predictors. In Machine
Learning, volume 24, page 123.
E. Brill and J. Wu. 1998. Classifier combination
for improved lexical disambiguation. Proceedings of
COLING-ACL?98, pages 191?195, August.
465
Radu Florian and David Yarowsky. 2002. Modeling con-
sensus: Classifier combination for word sense disam-
biguation. In Proceedings of EMNLP?02, pages 25?
32.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In Proceedings of CoNNL?03, pages 168?171.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics:
HLT-NAACL 2004, pages 1?8.
J. Henderson and E. Brill. 1999. Exploiting diversity in
natural language processing: Combining parsers. In
Proceedings on EMNLP99, pages 187?194.
T. K. Ho, J. J. Hull, and S. N. Srihari. 1994. Deci-
sion combination in multiple classifier systems. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 16(1):66?75, January.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In The Proceedings of
42st Annual Meeting of the Association for Computa-
tional Linguistics, pages 178?181, Barcelona, Spain,
July. Association for Computational Linguistics.
D. Magerman. 1993. Parsing as statistical pattern recog-
nition.
NIST. 2004. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151?178.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean,
R. Koeling, Y. Krymolowsky, V. Punyakanok, and
D. Roth. 2000. Applying system combination to base
noun phrase identification. In Proceedings of COL-
ING 2000, pages 857?863.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Im-
proving data driven wordclass tagging by system com-
bination. In Proceedings of COLING-ACL?98, pages
491?497.
L. Xu, A. Krzyzak, and C. Suen. 1992. Methods of
combining multiple classifiers and their applications
to handwriting recognition. IEEE Trans. on Systems,
Man. Cybernet, 22(3):418?435.
T. Zhang, F. Damerau, and D. E. Johnson. 2002. Text
chunking based on a generalization of Winnow. Jour-
nal of Machine Learning Research, 2:615?637.
466
A Flexible Framework for Developing  
Mixed-Initiative Dialog Systems 
 
Judith HOCHBERG, Nanda KAMBHATLA, Salim ROUKOS 
IBM T.J. Watson Research Center 
Yorktown Heights, NY 10598, USA 
{judyhoch, nanda, roukos}@us.ibm.com 
 
Abstract  
We present a new framework for rapid 
development of mixed-initiative dialog 
systems. Using this framework, a developer 
can author sophisticated dialog systems for 
multiple channels of interaction by 
specifying an interaction modality, a rich 
task hierarchy and task parameters, and 
domain-specific modules.  The framework 
includes a dialog history that tracks input, 
output, and results. We present the 
framework and preliminary results in two 
application domains. 
1 Introduction 
Developing a mixed-initiative dialog system is a 
complex task.  The developer must model the 
user?s goals, the ?results? (domain objects) 
retrieved, and the state of the dialog, and 
generate the system response at each turn of the 
dialog. In mixed-initiative systems, as opposed 
to directed dialog systems, users can influence 
the dialog flow, and are not restricted to 
answering system questions in a prescribed 
format (e.g. Walker 1990, Chu-Carroll 2000). 
Compounding these challenges, dialog 
applications have evolved from simple look-up 
tasks to complex transactional systems like 
telephony banking and stock trading (Zadrozny 
et al 1998), and air travel information systems. 
These systems increasingly cater to multiple 
channels of user interaction (telephone, PDA, 
web, etc.), each with its own set of modalities. 
To simplify the development of such systems, 
researchers have created frameworks that 
embody core dialog functionalities. 
In MIT?s framework, a developer creates a 
dialog system by specifying a dialog control 
table comprising actions and their triggering 
events. The developer has great freedom in 
designing this table, but must specify basic 
actions such as prompting for missing 
information.  As a result, these tables can 
become quite complex ? the travel system 
control table contains over 200 ordered rules.  
MIT has applied this framework to both weather 
and travel (Zue et al 2000, Seneff and Polifroni 
2000). 
In IBM?s form-based dialog manager, or 
FDM (Papineni et al 1998), a developer defines 
a set of forms that correspond to separate tasks 
in the application, such as finding a flight leg.  
The forms have powerful built-in capabilities, 
including mechanisms that trigger various types 
of prompts, and allow the user to specify 
inheritance and other relationships between 
tasks. Just as in the MIT framework, domain-
specific modules perform database queries and 
other backend processes; the forms call 
additional developer-defined modules that affect 
the dialog state and flow. FDM has supported 
dialog systems for air travel (Papineni et al 
1999, Axelrod 2000) and financial services 
(IBM 2001, IBM 2002). The University of 
Colorado framework also has a form-based 
architecture (Pellom et al 2001), while CMU 
and Bell Labs? frameworks allow the 
specification of deep task hierarchies (Wei and 
Rudnicky 2000, Potamianos et al 2000). 
Our goal is to design a framework that is 
both powerful, embodying much dialog 
functionality, and flexible, accommodating a 
variety of dialog domains, modalities, and styles.  
Our new framework goes beyond FDM in 
building more core functionality into its task 
model, yet provides a variety of software tools, 
such as API calls and overwritable functions, for 
customizing tasks.  The framework allows 
developers to specify a wide range of 
relationships among tasks, and provides a focus 
model that respects these relationships.  To 
support the task framework we introduce a 
       Philadelphia, July 2002, pp. 60-63.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
dialog history component that remembers input, 
output, and cumulative task results.  Section 2 of 
this paper describes the framework, and section 
3 some applications.  In section 4 we discuss 
future plans and implications. 
2 The HOT framework 
Our framework? s moniker is HOT, which stands 
for its three components: dialog History, domain 
Objects, and Task hierarchy.  It is implemented 
as a Java library. In this section, we describe the 
HOT framework. We assume the existence of an 
application specific natural language parser that 
brackets and labels chunks of text corresponding 
to domain specific attributes, and a natural 
language generation module for generating 
prompts from abstract specifications. 
2.1 Task hierarchy 
A task defines a unit of work for a dialog 
system. The HOT framework enables the 
specification of tasks that are organized as a 
hierarchy (e.g. Fig. 1).  The terminal tasks in the 
hierarchy  (UserID, Fund, and Shares) derive 
canonical values of domain attributes (such as 
fund symbol) from parsed portions of user input. 
The RootTask specifies methods for managing 
the dialog, e.g. for disambiguating among 
different sub-tasks in case of ambiguous user 
input. All other tasks perform scripted actions 
using the output produced by other non-terminal 
or terminal tasks: generate a user profile, a share 
transaction, or a price quote. 
The task hierarchy constitutes a plan for the 
dialog.  It remains to be seen whether it can also 
be used for planning domains in which task 
input can come either from a user or from an 
external process such as an environmental 
monitor, as in [Allen at al. 2001]. 
The framework allows developers to easily 
specify five different relationships among tasks 
in a hierarchy.  Many of these will be 
exemplified in Section 3. 
1. Subtasking: UserID is a subtask of Login 
because Login needs the user? s ID to log the 
user in. 
2. Ordering: Login precedes all other tasks, but 
Buy, Sell, and Price are unordered. 
3. Cardinality: Login is executed only once per 
session, and UserId, Fund, and Shares are 
executed only once per parent task. 
However, Buy, Sell, and Price can be 
executed multiple times. 
4. Inheritance: Buy and Sell can potentially 
inherit a fund name from Price and vice 
versa. 
5. Subdialog: The user can carry out certain 
subdialogs, such as a Price query within a 
Buy task. 
2.2 Focus model 
At each turn of the dialog, we automatically 
score the user? s input to infer the task that the 
user wants to work on.  Only a non-terminal task 
can receive focus.  As in FDM, scoring is 
primarily based on the number of matches 
between attributes in the parsed user input, 
different task attributes, and the last system 
prompt.  The developer can specify the 
appropriate system behavior if the inferred user 
focus conflicts with task relationships, e.g. if a 
user wants to Buy but has not yet Logged in.  In 
the absence of such conflicts, the framework 
triggers execution of the inferred task.  If the 
task completes without ending a turn, the focus 
model returns focus to a previously started task 
if possible, or else defaults to the developer? s 
preference for what to do next. 
2.3 Task functionality 
Within RootTask, a developer can specify the 
modalities of interaction and the specific 
backends used, create an initial task layout, and 
set some dialog parameters. Developers must 
specify how they want RootTask to respond to 
various focus situations.  For example, if no 
tasks are eligible for focus, this may represent an 
error condition in one application, but the 
expected end of a dialog in another application. 
For all other tasks, task functionality can be 
divided into operations that happen before and 
after the task calls its backend process 
Root
UserID Fund Name
Login
Number 
of Shares
Buy PriceSell
Fund 
Name
Number 
of Shares
Fund 
Name
 
Figure 1: A task hierarchy for a simple 
mutual fund application. 
(accessing a database, the Internet, or other 
information channel) to create a result.  Pre-
backend functionality involves assessing, and 
possibly confirming with the user, the 
parameters to be sent to the backend.  Post-
backend functionality acts on different backend 
outcomes: for example, informing the user of a 
result, confirming a result, or requesting further 
constraints.  Because the framework already 
defines these functionalites, the developer? s role 
is to define the backend and its result, and to 
choose the pre-defined functionalities that apply. 
As tasks execute, they post communicative 
intentions ? dialog acts (e.g., ?Inform?, 
?Confirm?) and the domain objects they concern 
(e.g., flights) ? to the dialog history.  A separate 
NLG module generates the text of the system 
response based on these communicative 
intentions and the specific modalities in use. 
2.4 Dialog History 
The dialog history provides developers with an 
organized way to store system data regardless of 
the application domain. We store the user input 
(attribute-value pairs), the system response 
(communicative intentions), and the cumulative 
results for each dialog turn.  The developer can 
additionally store the user input at various stages 
of processing. Results can be generic objects 
(e.g., a String representing a fund name) or 
complex, domain-specific objects.  The results 
are tagged with a core set of status indicators 
that developers can add to. The dialog history 
also serves as a task stack, since the current 
result from each task indicates the task? s status. 
The dialog history is reminiscent of BBN? s 
state manager (Stallard 2000), but the latter also 
includes many dialog management 
responsibilities that we reserve to tasks. 
3 Applications 
We are currently implementing the HOT 
framework described above. In this section, we 
describe two sample applications built using this 
framework as implemented so far. 
3.1 Mutual Funds 
We built a mutual funds application, using the  
framework to create a task hierarchy and to add 
task relationships such as ordering and 
inheritance, as described in section 2.1.  We 
customized the tasks by writing backend 
methods and specifying confirmation strategies, 
the domain attributes of interest to each task, and 
domain-specific result objects (UserProfile and 
ShareTransaction).  Figure 2 shows an actual 
dialog with this application. 
The dialog flows through a combination of 
developer specifications and the HOT 
framework, with the framework doing most of 
the work.  In turn 1, only Login is specified as 
active, so the framework generates an initial 
prompt for it.  In turn 2, the framework scores 
the input and executes both UserID and Signin.  
It informs the user of Login? s success (as 
specified), then issues a general prompt because 
Login enabled all other tasks when it finished 
(as specified).  In turn 3, the framework issues 
an error message because of the conflict between 
Login? s cardinality of 1 and the user? s input.  In 
turn 4, the fund name lookup finds an 
ambiguous result; the framework looks to the 
developer to specify what type of prompt to 
issue in response.  In turn 5, the framework 
correctly interprets the user? s selection, and 
requests fund name confirmation (as specified).  
In turn 6, Sell runs its Number of Shares 
subtask.  In turn 7, the user performs an 
authorized Price subdialog from within Sell.  
Price inherits the fund name from Sell, executes 
the task, and returns focus to Sell.  In turn 8, the 
framework requests confirmation prior to 
 User System 
1  Welcome to HOT-DM. 
User ID? 
2 1677 Greetings Judy Hochberg! 
How may I help you? 
3 I want to sign 
in again. 
Sorry, you can only sign in once. 
How may I help you? 
4 I want to sell 
Watson fund 
Watson Yorktown fund or 
Watson Hawthorne fund? 
5 the first one Is Watson Yorktown the fund 
you want to sell? 
6 yes How many shares? 
7 can you tell 
me what the 
price is 
The price of Watson Yorktown 
is $100.2 per share 
How many shares? 
8 I want to sell 
30 shares 
Do you want to sell 30 shares of 
Watson Yorktown fund? 
9 yes i do Transaction #1: sold 30 shares of 
Watson Yorktown fund at 
$100.2 per share on 2/18/2002 
How may I help you? 
Figure 2: A dialog with a mutual funds application 
built using the HOT framework. 
executing the sale (as specified).  In turn 9, note 
that the Sell backend accesses the dialog history 
to determine the correct transaction number. 
3.2 Travel 
We have developed a travel information dialog 
system using the HOT framework. For this 
application, we reused several components of 
the IBM DARPA Communicator system. The 
system accepts either standard text input or text 
from instant messaging from a mobile platform.  
Figure 3 shows a screen shot of a dialog on a 
? Palm?  (PDA) interface to the travel system. 
4 Discussion 
We have presented a new framework for 
developing mixed-initiative dialog systems. This 
framework, dubbed HOT, enables developers to 
rapidly develop dialog systems by specifying 
tasks, their relationships, and relevant domain 
objects. We are currently implementing this 
framework as a toolkit and have developed two 
sample applications in two different modalities. 
The new framework departs from other 
frameworks in the range of functionality that it 
covers.  Its task model triggers not only 
informational prompts and confirmations, but 
also customizable responses to task problems of 
different sorts, such as underspecification.  The 
task relationships modeled are likewise quite 
rich, including subdialog and inheritance.  
Finally, the dialog history provides a generic 
specification of output semantics, a way to track 
task status, and uniform access to dialog results 
of varying complexity.  Our future goal is 
continue to build functionality, especially in 
NLG, without sacrificing flexibility. 
References  
J. Allen, G Ferguson, and Amanda Stent (2001) An 
architecture for more realistic conversational 
systems.  Proc. Intelligent User Interfaces. 
S. Axelrod, (2000) Natural Language Generation in 
the IBM Flight Information System.  Proc. ANLP-
NAACL Workshop on Conversational Systems. 
J. Chu-Carroll (2000) MIMIC: An Adaptive Mixed 
Initiative Spoken Dialogue System for Information 
Queries.  Proc ANLP. 
IBM (2001) http://www-3.ibm.com/software/speech/ 
news/20010609trp.html 
IBM (2002) http://www-3.ibm.com/software/speech/ 
enterprise/dcenter/demo_2.html 
K. Papineni, S. Roukos, and T. Ward (1999) Free-
Flow Dialog Management Using Forms.  Proc. 
Eurospeech, pp. 1411-1414. 
B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. 
Zhang, X. Yu, and S. Pradhan (2001) University of 
Colorado Dialog Systems for Travel and 
Navigation.  Proc. HLT. 
A. Potamianos, E. Ammicht, and H-K. Kuo (2000) 
Dialogue Management in the Bell Labs 
Communicator System.  Proc. ICSLP. 
S. Seneff and J. Polifroni (2000) Dialogue 
Management in the Mercury Flight Reservation 
System.  Proc. Satellite Dialogue Workshop, 
ANLP-NAACL. 
D. Stallard (2000) Talk?N?Travel: A Conversational 
System for Air Travel Planning.  Proc ANLP. 
M. Walker (1990)  Mixed Initiative in Dialogue: An 
Investigation into Discourse Segmentation. Proc. 
ACL90, pp. 70-78. 
X. Wei and A. Rudnicky (2000) Task-based dialog 
management using an agenda.  Proc 
ANLP/NAACL Workshop on Conversational 
Systems, pp. 42-47. 
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. 
Hazen, and L. Hetherington (2000) JUPITER: A 
Telephone-Based conversational Interface for 
Weather Information.  IEEE Trans. Speech and 
Audio Proc., 20/Y, pp. 100-112.  
W. Zadrozny, C. Wolf, N. Kambhatla, and Y. Ye,  
1998. Conversation Machines for Transaction 
Processing. PROC AAAI/IAAI, pp. 1160-1166. Figure 3: A dialog in a ? Palm?  interface to 
an air travel dialog system. 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1040?1047,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Extracting Social Networks and Biographical Facts From Conversational
Speech Transcripts
Hongyan Jing
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
hjing@us.ibm.com
Nanda Kambhatla
IBM India Research Lab
EGL, Domlur Ring Road
Bangalore - 560071, India
kambhatla@in.ibm.com
Salim Roukos
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
roukos@us.ibm.com
Abstract
We present a general framework for
automatically extracting social networks
and biographical facts from conversational
speech. Our approach relies on fusing
the output produced by multiple informa-
tion extraction modules, including entity
recognition and detection, relation detec-
tion, and event detection modules. We
describe the specific features and algo-
rithmic refinements effective for conver-
sational speech. These cumulatively in-
crease the performance of social network
extraction from 0.06 to 0.30 for the devel-
opment set, and from 0.06 to 0.28 for the
test set, as measured by f-measure on the
ties within a network. The same frame-
work can be applied to other genres of text
? we have built an automatic biography
generation system for general domain text
using the same approach.
1 Introduction
A social network represents social relationships
between individuals or organizations. It consists
of nodes and ties. Nodes are individual actors
within the networks, generally a person or an or-
ganization. Ties are the relationships between the
nodes. Social network analysis has become a key
technique in many disciplines, including modern
sociology and information science.
In this paper, we present our system for au-
tomatically extracting social networks and bio-
graphical facts from conversational speech tran-
scripts by integrating the output of different IE
modules. The IE modules are the building blocks;
the fusing module depicts the ways of assembling
these building blocks. The final output depends on
which fundamental IE modules are used and how
their results are integrated.
The contributions of this work are two fold. We
propose a general framework for extracting social
networks and biographies from text that applies to
conversational speech as well as other genres, in-
cluding general newswire stories. Secondly, we
present specific methods that proved effective for
us for improving the performance of IE systems on
conversational speech transcripts. These improve-
ments include feature engineering and algorithmic
revisions that led to a nearly five-fold performance
increase for both development and test sets.
In the next section, we present our framework
for extracting social networks and other biograph-
ical facts from text. In Section 3, we discuss the
refinements we made to our IE modules in order
to reliably extract information from conversational
speech transcripts. In Section 4, we describe the
experiments, evaluation metrics, and the results of
social network and biography extraction. In Sec-
tion 5, we show the results of applying the frame-
work to other genres of text. Finally, we discuss
related work and conclude with lessons learned
and future work.
2 The General Framework
For extraction of social networks and biographi-
cal facts, our approach relies on three standard IE
modules ? entity detection and recognition, rela-
tion detection, and event detection ? and a fusion
module that integrates the output from the three IE
systems.
2.1 Entity, Relation, and Event Detection
We use the term entity to refer to a person, an or-
ganization, or other real world entities, as adopted
1040
in the Automatic Content Extraction (ACE) Work-
shops (ACE, 2005). A mention is a reference to
a real world entity. It can be named (e.g. ?John
Lennon?), nominal (e.g. ?mother?), or pronomi-
nal (e.g. ?she?).
Entity detection is generally accomplished in
two steps: first, a mention detection module iden-
tifies all the mentions of interest; second, a co-
reference module merges mentions that refer to the
same entity into a single co-reference chain.
A relation detection system identifies (typi-
cally) binary relationships between pairs of men-
tions. For instance, for the sentence ?I?m in New
York?, the following relation exists: locatedAt (I,
New York).
An event detection system identifies events of
interest and the arguments of the event. For ex-
ample, from the sentence ?John married Eva in
1940?, the system should identify the marriage
event, the people who got married and the time
of the event.
The latest ACE evaluations involve all of the
above tasks. However, as shown in the next sec-
tion, our focus is quite different from ACE ?
we are particularly interested in improving perfor-
mance for conversational speech and building on
top of ACE tasks to produce social networks and
biographies.
2.2 Fusion Module
The fusion module merges the output from IE
modules to extract social networks and biographi-
cal facts. For example, if a relation detection sys-
tem has identified the relation motherOf (mother,
my) from the input sentence ?my mother is a
cook?, and if an entity recognition module has
generated entities referenced by the mentions {my,
Josh, me, I, I, ......} and {mother, she, her, her,
Rosa......}, then by replacing my and mother with
the named mentions within the same co-reference
chains, the fusion module produces the follow-
ing nodes and ties in a social network: motherOf
(Rosa, Josh).
We generate the nodes of social networks by se-
lecting all the PERSON entities produced by the
entity recognition system. Typically, we only in-
clude entities that contain at least one named men-
tion. To identify ties between nodes, we retrieve
all relations that indicate social relationships be-
tween a pair of nodes in the network.
We extract biographical profiles by selecting the
events (extracted by the event extraction module)
and corresponding relations (extracted by the rela-
tion extraction module) that involve a given indi-
vidual as an argument. When multiple documents
are used, then we employ a cross-document co-
reference system.
3 Improving Performance for
Conversational Speech Transcripts
Extracting information from conversational
speech transcripts is uniquely challenging. In this
section, we describe the data collection used in
our experiments, and explain specific techniques
we used to improve IE performance on this data.
3.1 Conversational Speech Collection
We use a corpus of videotaped, digitized oral in-
terviews with Holocaust survivors in our experi-
ments. This data was collected by the USC Shoah
Foundation Institute (formerly known as the Vi-
sual History Foundation), and has been used in
many research activities under the Multilingual
Access to Large Spoken Archives (MALACH)
project (Gustman et al, 2002; Oard et al, 2004).
The collection contains oral interviews in 32 lan-
guages from 52,000 survivors, liberators, rescuers
and witnesses of the Holocaust.
This data is very challenging. Besides the usual
characteristics of conversational speech, such as
speaker turns and speech repairs, the interview
transcripts contain a large percentage of ungram-
matical, incoherent, or even incomprehensible
clauses (a sample interview segment is shown in
Figure 1). In addition, each interview covers many
people and places over a long time period, which
makes it even more difficult to extract social net-
works and biographical facts.
speaker2 in on that ninth of Novem-
ber nineteen hundred thirty eight I was
with my parents at home we heard
not through the we heard even through
the windows the crashing of glass the
crashing of and and they are our can?t
Figure 1: Sample interview segment.
3.2 The Importance of Co-reference
Resolution
Our initial attempts at social network extraction
for the above data set resulted in a very poor score
1041
of 0.06 f-measure for finding the relations within
a network (as shown in Table 3 as baseline perfor-
mance).
An error analysis indicated poor co-reference
resolution to be the chief culprit for the low per-
formance. For instance, suppose we have two
clauses: ?his mother?s name is Mary? and ?his
brother Mark went to the army?. Further sup-
pose that ?his? in the first clause refers to a
person named ?John? and ?his? in the second
clause refers to a person named ?Tim?. If the
co-reference system works perfectly, the system
should find a social network involving four peo-
ple: {John, Tim, Mary, Mark}, and the ties: moth-
erOf (Mary, John), and brotherOf (Mark, Tim).
However, if the co-reference system mistakenly
links ?John? to ?his? in the second clause and links
?Tim? to ?his? in the first clause, then we will still
have a network with four people, but the ties will
be: motherOf (Mary, Tim), and brotherOf (Mark,
John), which are completely wrong. This example
shows that co-reference errors involving mentions
that are relation arguments can lead to very bad
performance in social network extraction.
Our existing co-reference module is a state-of-
the-art system that produces very competitive re-
sults compared to other existing systems (Luo et
al., 2004). It traverses the document from left to
right and uses a mention-synchronous approach to
decide whether a mention should be merged with
an existing entity or start a new entity.
However, our existing system has shortcomings
for this data: the system lacks features for han-
dling conversational speech, and the system of-
ten makes mistakes in pronoun resolution. Re-
solving pronominal references is very important
for extracting social networks from conversational
speech, as illustrated in the previous example.
3.3 Improving Co-reference for
Conversational Speech
We developed a new co-reference resolution sys-
tem for conversational speech transcripts. Simi-
lar to many previous works on co-reference (Ng,
2005), we cast the problem as a classification task
and solve it in two steps: (1) train a classifier to
determine whether two mentions are co-referent or
not, and (2) use a clustering algorithm to partition
the mentions into clusters, based on the pairwise
predictions.
We added many features to our model specifi-
cally designed for conversational speech, and sig-
nificantly improved the agglomerative clustering
used for co-reference, including integrating rela-
tions as constraints, and designing better cluster
linkage methods and clustering stopping criteria.
3.3.1 Adding Features for Conversational
Speech
We added many features to our model specifi-
cally designed for conversational speech:
Speaker role identification. In manual tran-
scripts, the speaker turns are given and each
speaker is labeled differently (e.g. ?speaker1?,
?speaker2?), but the identity of the speaker is not
given. An interview typically involves 2 or more
speakers and it is useful to identify the roles of
each speaker (e.g. interviewer, interviewee, etc.).
For instance, ?you? spoken by the interviewer is
likely to be linked with ?I? spoken by the inter-
viewee, but ?you? spoken by the third person in
the interview is more likely to be referring to the
interviewer than to the interviewee.
We developed a program to identify the speaker
roles. The program classifies the speakers into
three categories: interviewer, interviewee, and
others. The algorithm relies on three indicators
? number of turns by each speaker, difference in
number of words spoken by each speaker, and the
ratio of first-person pronouns such as ?I?, ?me?,
and ?we? vs. second-person pronouns such as
?you? and ?your?. This speaker role identifica-
tion program works very well when we checked
the results on the development and test set ? the
interviewers and survivors in all the documents in
the development set were correctly identified.
Speaker turns. Using the results from the
speaker role identification program, we enrich cer-
tain features with speaker turn information. For
example, without this information, the system can-
not distinguish ?I? spoken by an interviewer from
?I? spoken by an interviewee.
Spelling features for speech transcripts. We
add additional spelling features so that mentions
such as ?Cyla C Y L A Lewin? and ?Cyla Lewin?
are considered as exact matches. Names with
spelled-out letters occur frequently in our data col-
lection.
Name Patterns. We add some features that
capture frequent syntactic structures that speakers
use to express names, such as ?her name is Irene?,
?my cousin Mark?, and ?interviewer Ellen?.
Pronoun features. To improve the perfor-
1042
mance on pronouns, we add features such as the
speaker turns of the pronouns, whether the two
pronouns agree in person and number, whether
there exist other mentions between them, etc.
Other miscellaneous features. We also in-
clude other features such as gender, token dis-
tance, sentence distance, and mention distance.
We trained a maximum-entropy classifier using
these features. For each pair of mentions, the clas-
sifier outputs the probability that the two mentions
are co-referent.
We also modified existing features to make
them more applicable to conversational speech.
For instance, we added pronoun-distance features
taking into account the presence of other pronom-
inal references in between (if so, the types of the
pronouns), other mentions in between, etc.
3.3.2 Improving Agglomerative Clustering
We use an agglomerative clustering approach
for partitioning mentions into entities. This is a
bottom-up approach which joins the closest pair
of clusters (i.e., entities) first. Initially, each men-
tion is placed into its own cluster. If we have N
mentions to cluster, we start with N clusters.
The intuition behind choosing the agglomera-
tive method is to merge the most confident pairs
first, and use the properties of existing clusters to
constrain future clustering. This seems to be espe-
cially important for our data collection, since con-
versational speech tends to have a lot of repetitions
or local structures that indicate co-reference. In
such cases, it is beneficial to merge these closely
related mentions first.
Cluster linkage method. In agglomerative
clustering, each cycle merges two clusters into a
single cluster, thus reducing the number of clus-
ters by one. We need to decide upon a method of
measuring the distance between two clusters.
At each cycle, the two mentions with the high-
est co-referent probability are linked first. This re-
sults in the merging of the two clusters that contain
these two mentions.
We improve upon this method by imposingmin-
imal distance criteria between clusters. Two clus-
ters C
1
and C
2
can be combined only if the dis-
tance between all the mentions from C
1
and all
the mentions from C
2
is above the minimal dis-
tance threshold. For instance, suppose C
1
=
{he, father}, and C
2
= {he, brother}, and ?he?
from C
1
and ?he? from C
2
has the highest linkage
probability. The standard single linkage method
will combine these two clusters, despite the fact
that ?father? and ?brother? are very unlikely to
be linked. Imposing minimal distance criteria
can solve this problem and prevent the linkage of
clusters which contain very dissimilar mentions.
In practice, we used multiple minimal distance
thresholds, such as minimal distance between two
named mentions and minimal distance between
two nominal mentions.
We chose not to use complete or average link-
age methods. In our data collection, the narrations
contain a lot of pronouns and the focus tends to
be very local. Whereas the similarity model may
be reasonably good at predicting the distance be-
tween two pronouns that are close to each other, it
is not good at predicting the distance between pro-
nouns that are furthur apart. Therefore, it seems
more reasonable to use single linkage method with
modifications than complete or average linkage
methods.
Using relations to constrain clustering. An-
other novelty of our co-reference system is the
use of relations for constraining co-reference. The
idea is that two clusters should not be merged if
such merging will introduce contradictory rela-
tions. For instance, if we know that person entity
A is the mother of person entity B, and person en-
tity C is the sister of B, then A and C should not
be linked since the resulting entity will be both the
mother and the sister of B.
We construct co-existent relation sets from the
training data. For any two pairs of entities, we col-
lect all the types of relations that exist between
them. These types of relations are labeled as
co-existent. For instance, ?motherOf? and ?par-
entOf? can co-exist, but ?motherOf? and ?sis-
terOf? cannot. By using these relation constraints,
the system refrains from generating contradictory
relations in social networks.
Speed improvement. Suppose the number of
mentions is N , the time complexity of simple link-
age method is O(N2). With the minimal dis-
tance criteria, the complexity is O(N3). However,
N can be dramatically reduced for conversational
transcripts by first linking all the first-person pro-
nouns by each speaker.
4 Experiments
In this section, we describe the experimental setup
and present sample outputs and evaluation results.
1043
Train Dev Test
Words 198k 73k 255k
Mentions 43k 16k 56k
Relations 7K 3k 8k
Table 2: Experimental Data Sets.
4.1 Data Annotation
The data used in our experiments consist of partial
or complete English interviews of Holocaust sur-
vivors. The input to our system is transcripts of
interviews.
We manually annotated manual transcripts with
entities, relations, and event categories, specifi-
cally designed for this task and the results of care-
ful data analysis. The annotation was performed
by a single annotator over a few months. The an-
notation categories for entities, events, and rela-
tions are shown in Table 1. Please note that the
event and relation definitions are slightly different
than the definitions in ACE.
4.2 Training and Test Sets
We divided the data into training, development,
and test data sets. Table 2 shows the size of each
data set. The training set includes transcripts of
partial interviews. The development set consists
of 5 complete interviews, and the test set con-
sists of 15 complete interviews. The reason that
the training set contains only partial interviews is
due to the high cost of transcription and annota-
tion. Since those partial interviews had already
been transcribed for speech recognition purpose,
we decided to reuse them in our annotation. In ad-
dition, we transcribed and annotated 20 complete
interviews (each interview is about 2 hours) for
building the development and test sets, in order
to give a more accurate assessment of extraction
performance.
4.3 Implementation
We developed the initial entity detection, rela-
tion detection, and event detection systems using
the same techniques as our submission systems to
ACE (Florian et al, 2004). Our submission sys-
tems use statistical approaches, and have ranked
in the top tier in ACE evaluations. We easily built
the models for our application by retraining exist-
ing systems with our training set.
The entity detection task is accomplished in two
steps: mention detection and co-reference resolu-
tion. The mention detection is formulated as a la-
Figure 2: Social network extracted by the system.
beling problem, and a maximum-entropy classifier
is trained to identify all the mentions.
Similarly, relation detection is also cast as a
classification problem ? for each pair of men-
tions, the system decides which type of relation
exists between them. It uses a maximum-entropy
classifier and various lexical, contextual, and syn-
tactic features for such predications.
Event detection is accomplished in two steps:
first, identifying the event anchor words using an
approach similar to mention detection; then, iden-
tifying event arguments using an approach similar
to relation detection.
The co-reference resolution system for conver-
sational speech and the fusion module were devel-
oped anew.
4.4 The Output
The system aims to extract the following types of
information:
? The social network of the survivor.
? Important biographical facts about each per-
son in the social network.
? Track the movements of the survivor and
other individuals in the social network.
Figure 2 shows a sample social network ex-
tracted by the system (only partial of the network
is shown). Figure 3 shows sample biographical
facts and movement summaries extracted by the
system. In general, we focus more on higher pre-
cision than recall.
4.5 Evaluation
In this paper, we focus only on the evaluation
of social network extraction. We first describe
the metrics for social network evaluation and then
present the results of the system.
1044
Entity (12) Event (8) Relation (34)
Social Rels (12) Event Args (8) Bio Facts (14)
AGE CUSTODY aidgiverOf affectedBy bornAt
COUNTRY DEATH auntOf agentOf bornOn
DATE HIDING cousinOf participantIn citizenOf
DATEREF LIBERATION fatherOf timeOf diedAt
DURATION MARRIAGE friendOf travelArranger diedOn
GHETTOORCAMP MIGRATION grandparentOf travelFrom employeeOf
OCCUPATION SURVIVAL motherOf travelPerson hasProperty
ORGANIZATION VIOLENCE otherRelativeOf travelTo locatedAt
OTHERLOC parentOf managerOf
PEOPLE siblingOf memberOf
PERSON spouseOf near
SALUTATION uncleOf partOf
partOfMany
resideIn
Table 1: Annotation Categories for Entities, Events, and Relations.
Sidonia Lax:
date of birth: June the eighth nineteen twenty
seven
Movements:
Moved To: Auschwitz
Moved To: United States
... ...
Figure 3: Biographical facts and movement sum-
maries extracted by the system.
To compare two social networks, we first need
to match the nodes and ties between the networks.
Two nodes (i.e., entities) are matched if they have
the same canonical name. Two ties (i.e., edges or
relations) are matched if these three criteria are
met: they contain the same type of relations, the
arguments of the relation are the same, and the or-
der of the arguments are the same if the relation is
unsymmetrical.
We define the the following measurements for
social network evaluation: the precision for nodes
(or ties) is the ratio of common nodes (or ties) in
the two networks to the total number of nodes (or
ties) in the system output, the recall for nodes (or
ties) is the ratio of common nodes (or ties) in the
two networks to the total number of nodes/ties in
the reference output, and the f-measure for nodes
(or ties) is the harmonic mean of precision and re-
call for nodes (or ties). The f-measure for ties in-
dicates the overall performance of social network
extraction.
F-mea Dev Test
Baseline New Baseline New
Nodes 0.59 0.64 0.62 0.66
Ties 0.06 0.30 0.06 0.28
Table 3: Performance of social network extraction.
Table 3 shows the results of social network ex-
traction. The new co-reference approach improves
the performance for f-measure on ties by five-fold
on development set and by nearly five-fold for test
set.
We also tested the system using automatic tran-
scripts by our speech recognition system. Not sur-
prisingly, the result is much worse: the nodes f-
measure is 0.11 for the test set, and the system
did not find any relations. A few factors are ac-
countable for this low performance: (1) Speech
recognition is very challenging for this data set,
since the testimonies contained elderly, emotional,
accented speech. Given that the speech recogni-
tion system fails to recognize most of the person
names, extraction of social networks is difficult.
(2) The extraction systems perform worse on au-
tomatic transcripts, due to the quality of the auto-
matic transcript, and the discrepancy between the
training and test data. (3) Our measurements are
very strict, and no partial credit is given to partially
correct entities or relations.
We decided not to present the evaluation results
of the individual components since the perfor-
mance of individual components are not at all in-
dicative of the overall performance. For instance,
a single pronoun co-reference error might slighlty
1045
change the co-reference score, but can introduce a
serious error in the social network, as shown in the
example in Section 3.2.
5 Biography Generation from General
Domain Text
We have applied the same framework to biogra-
phy generation from general news articles. This
general system also contains three fundamental IE
systems and a fusion module, similar to the work
presented in the paper. The difference is that the IE
systems are trained on general news text using dif-
ferent categories of entities, relations, and events.
A sample biography output extracted from
TDT5 English documents is shown in Figure 4.
The numbers in brackets indicate the corpus count
of the facts.
Saddam Hussein:
Basic Information:
citizenship: Iraq [203]
occupation: president [4412], leader [1792],
dictator [664],...
relative: odai [89], qusay [65], uday [65],...
Life Events:
places been to: bagdad [403], iraq [270],
palaces [149]...
Organizations associated with: manager of
baath party [1000], ...
Custody Events: Saddam was arrested [52],
Communication Events: Saddam said [3587]
... ...
Figure 4: Sample biography output.
6 Related Work
While there has been previous work on extracting
social networks from emails and the web (Culotta
et al, 2004), we believe this is the first paper to
present a full-fledged system for extracting social
networks from conversational speech transcripts.
Similarly, most of the work on co-reference res-
olution has not focused on conversational speech.
(Ji et al, 2005) uses semantic relations to refine
co-reference decisions, but in a approach different
from ours.
7 Conclusions and Future Work
We have described a novel approach for extracting
social networks, biographical facts, and movement
summaries from transcripts of oral interviews with
Holocaust survivors. We have improved the per-
formance of social network extraction five-fold,
compared to a baseline system that already uses
state-of-the-art technology. In particular, we im-
proved the performance of co-reference resolution
for conversational speech, by feature engineering
and improving the clustering algorithm.
Although our application data consists of con-
versational speech transcripts in this paper, the
same extraction approach can be applied to
general-domain text as well. Extracting general,
rich social networks is very important in many ap-
plications, since it provides the knowledge of who
is connected to whom and how they are connected.
There are many interesting issues involved in
biography generation from a large data collection,
such as how to resolve contradictions. The counts
from the corpus certainly help to filter out false
information which would otherwise be difficult to
filter. But better technology at detecting and re-
solving contradictions will definitely be beneficial.
Acknowledgment
We would like to thank Martin Franz and Bhuvana
Ramabhadran for their help during this project.
This project is funded by NSF under the Infor-
mation Technology Research (ITR) program, NSF
IIS Award No. 0122466. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the NSF.
References
2005. Automatic content extraction.
http://www.nist.gov/speech/tests/ace/.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and con-
tact information from email and the web. In CEAS,
Mountain View, CA.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In Proceedings of. HLT-NAACL 2004.
Samuel Gustman, Dagobert Soergeland Douglas Oard,
William Byrne, Michael Picheny, Bhuvana Ramab-
hadran, and Douglas Greenberg. 2002. Support-
ing access to large digital oral history archives. In
Proceedings of the Joint Conference on Digital Li-
braries, pages 18?27.
1046
Heng Ji, DavidWestbrook, and Ralph Grishman. 2005.
Using semantic relations to refine coreference deci-
sions. In Proceedings of HLT/EMNLP?05, Vancou-
ver, B.C., Canada.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL2004), pages 135?142, Barcelona,
Spain.
Vincent Ng. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proceedings of ACL?04.
D. Oard, D. Soergel, D. Doermann, X. Huang, G.C.
Murray, J. Wang, B. Ramabhadran, M. Franz,
S. Gustman, J. Mayfield, L. Kharevych, and
S. Strassel. 2004. Building an information re-
trieval test collection for spontaneous conversational
speech. In Proceedings of SIGIR?04, Sheffield, U.K.
1047
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1119?1127,
Beijing, August 2010
Syntax Based Reordering with Automatically Derived Rules for
Improved Statistical Machine Translation
Karthik Visweswariah
IBM Research
v-karthik@in.ibm.com
Jiri Navratil
IBM Research
jiri@us.ibm.com
Jeffrey Sorensen
Google, Inc.
sorenj@google.com
Vijil Chenthamarakshan
IBM Research
vijil.e.c@in.ibm.com
Nanda Kambhatla
IBM Research
kambhatla@in.ibm.com
Abstract
Syntax based reordering has been shown
to be an effective way of handling word
order differences between source and
target languages in Statistical Machine
Translation (SMT) systems. We present
a simple, automatic method to learn rules
that reorder source sentences to more
closely match the target language word or-
der using only a source side parse tree and
automatically generated alignments. The
resulting rules are applied to source lan-
guage inputs as a pre-processing step and
demonstrate significant improvements in
SMT systems across a variety of lan-
guages pairs including English to Hindi,
English to Spanish and English to French
as measured on a variety of internal test
sets as well as a public test set.
1 Introduction
Different languages arrange words in different or-
ders, whether due to grammatical constraints or
other conventions. Dealing with these word order
permutations is one of the fundamental challenges
of machine translation. Given an exceptionally
large training corpus, a phrase-based system can
learn these reordering on a case by case basis.
But, if our systems are to generalize to phrases not
seen in the training data, they must explicitly cap-
ture and model these reorderings. However, per-
mutations are difficult to model and impractical to
search.
Presently, approaches that handle reorderings
typically model word and phrase movements via
a distortion model and rely on the target language
model to produce words in the right order. Early
distortion models simply penalized longer jumps
more than shorter jumps (Koehn et al, 2003)
independent of the source or target phrases
in question. Other models (Tillman, 2004),
(Al-Onaizan and Papineni, 2006) generalize this
to include lexical dependencies on the source.
Another approach is to incorporate features,
based on the target syntax, during modeling and
decoding, and this is shown to be effective for var-
ious language pairs (Yamada and Knight, 2001),
(Zollmann and Venugopal, 2006). Hierarchical
phrase-based decoding (Chiang, 2005) also al-
lows for long range reordering without explic-
itly modeling syntax. While these approaches
have been shown to improve machine translation
performance (Zollmann et al, 2008) they usually
combine chart parsing with the decoding process,
and are significantly more computationally inten-
sive than phrase-based systems.
A third approach, one that has proved to be
useful for phrase-based SMT systems, is to re-
order each source-side sentence using a set of
rules applied to a parse tree of the source sen-
tence. The goal of these rules is to make the
word order of the source sentence more sim-
ilar to the expected target sentence word or-
der. With this approach, the reordering rules
are applied before training and testing with an
SMT system. The efficacy of these methods has
been shown on various language pairs including:
French to English (Xia and McCord, 2004), Ger-
man to English (Collins et al, 2005), English to
1119
Chinese, (Wang et al, 2007) and Hindi to English
(Ramanathan et al, 2008).
In this paper, we propose a simple model for re-
ordering conditioned on the source side parse tree.
The model is learned using a parallel corpus of
source-target sentence pairs, machine generated
word alignments, and source side parses. We ap-
ply the reordering model to both training and test
data, for four different language pairs: English
? Spanish, English ? French, English ? Hindi,
and English ? German. We show improvements
in machine translation performance for all of the
language pairs we consider except for English ?
German. We use this negative result to propose
extensions to our reordering model. We note that
the syntax based reordering we propose can be
combined with other approaches to handling re-
ordering and does not have to be followed by an
assumption of monotonicity. In fact, our phrase-
based model, trained upon reordered data, retains
its reordering models and search, but we expect
that these facilities are employed much more spar-
ingly with reordered inputs.
2 Related work
There is a significant quantity of work in syntax
based reordering employed to improve machine
translation systems. We summarize our contribu-
tions to be:
? Learning the reordering rules based on train-
ing data (without relying on linguistic knowl-
edge of the language pair)
? Requiring only source side parse trees
? Experimental results showing the efficacy for
multiple language pairs
? Using a lexicalized distortion model for our
baseline decoder
There have been several studies that have
demonstrated improvements with syntax
based reordering based upon hand-written
rules. There have also been studies inves-
tigating the sources of these improvements
(Zwarts and Dras, 2007). Hand-written rules
depend upon expert knowledge of the linguis-
tic properties of the particular language pair.
Initial efforts (Niessen and Ney, 2001) were
made at improving German-English translation
by handling two phenomena: question inver-
sion and detachable verb prefixes in German.
In (Collins et al, 2005), (Wang et al, 2007),
(Ramanathan et al, 2008), (Badr et al, 2009)
rules are developed for translation from Ger-
man to English, Chinese to English, English
to Hindi, and English to Arabic respectively.
(Xu et al, 2009) develop reordering rules based
upon a linguistic analysis of English and Korean
sentences and then apply those rules to trans-
lation from English into Korean and four other
languages: Japanese, Hindi, Urdu and Turkish.
Unlike this body of work, we automatically learn
the rules from the training data and show efficacy
on multiple language pairs.
There have been some studies that try to learn
rules from the data. (Habash, 2007) learns re-
ordering rules based on a dependency parse and
they report a negative result for Arabic to En-
glish translation. (Zhang et al, 2007) learn re-
ordering rules on chunks and part of speech
tags, but the rules they learn are not hierarchi-
cal and would require large amounts of training
data to learn rules for long sentences. Addition-
ally, we only keep a single best reordering (in-
stead of a lattice with possible reorderings) which
makes the decoding significantly more efficient.
(Xia and McCord, 2004) uses source and target
side parse trees to automatically learn rules to re-
order French sentences to match English order.
The requirement to have both source and target
side parse trees makes this method inapplicable
to any language that does not have adequate tree
bank resources. In addition, this work reports re-
sults using monotone decoding, since their exper-
iments using non-monotone decoding without a
distortion model were actually worse.
3 Reordering issues in specific languages
In this section we discuss the reordering issues
typical of translating between English and Hindi,
French, Spanish and German which are the four
language pairs we experiment on in this paper.
3.1 Spanish and French
Typical word ordering patterns common to these
two European languages relate to noun phrases in-
cluding groups of nouns and adjectives. In con-
1120
trast to English, French and Spanish adjectives
and adjunct nouns follow the main noun, i.e. we
typically observe a reversal of word order in noun
phrases, e.g., ?A beautiful red car? translates
into French as ?Une voiture rouge beau?, and as
?Un coche rojo bonito? into Spanish. Phrase-
based MT systems are capable of capturing these
patterns provided they occur with sufficient fre-
quency for each example in the training data. For
rare noun phrases, however, the MT may pro-
duce erroneous word order that can lead to seri-
ous distortions in the meaning. Particularly dif-
ficult are nominal phrases from specialized do-
mains that involve challenging terminology, for
example: ?group reference attribute? and ?valida-
tion checking code?. In both instances, the base-
line MT system generated translations with an in-
correct word order and, consequently, possibly a
different meaning. We will return to these two ex-
amples in Section 5.1 to compare the output of a
MT system with and without reordering.
3.2 German
Unlike French and Spanish, German poses a con-
siderably different challenge with respect to word
ordering. The most frequent reordering in German
relates to verbs, particularly verb groups consist-
ing of auxiliary and main verbs, as well as verbs
in relative clauses. Moreover, reordering patterns
between German and English tend to span large
portions of the sentence. We included German in
our investigations to determine whether our auto-
mated rule extraction procedure can capture such
long distance patterns.
3.3 Hindi
Hindi word order is significantly different than
English word order; the typical order followed
is Subject Object Verb (although Object Subject
Verb order can be used if nouns are followed by
appropriate case markers). This is in contrast to
English which has a Subject Verb Object order.
This can result in words that are close in English
moving arbitrarily far apart in Hindi depending on
the length of the noun phrase representing the ob-
ject and the length of the verb phrase. These long
range reorderings are generally hard for a phrase
based system to capture. Another way Hindi and
English differ is that prepositions in English be-
come postpositions in Hindi and appear after the
noun phrase. Again, this reordering can lead to
long distance movements of words. We include
Hindi in our investigation since it has significantly
different structure as compared to English.
4 Learning reordering rules
In this section we describe how we learn rules that
transform source parse trees so the leaf word order
is more like the target language. We restrict our-
selves to reorderings that can be obtained by per-
muting child nodes at various interior nodes in a
parse tree. With many reordering phenomena dis-
cussed in Section 3 this is a fairly strong assump-
tion about pairs of languages, and there are exam-
ples in English?Hindi where such an assumption
will not allow us to generate the right reordering.
As an example consider the English sentence ?I
do not want to play?. The sentence has a parse:
S
NP
PRP
I
VP
VBP
do
RB
not
VP
VB
want
S
VP
TO
to
VP
VB
play
The correct word order of the translation in Hindi
is ?I to play not want? In this case, the word not
breaks up the verb phrase want to play and hence
the right Hindi word order cannot be obtained by
the reordering allowed by our model. We found
such examples to be rare in English?Hindi, and
we impose this restriction for the simplicity of the
model. Experimental results on several languages
show benefits of reordering in spite of this simpli-
fying assumption.
Consider a source sentence s and its corre-
sponding constituency parse tree S1. We set up
the problem in a probabilistic framework, i.e. we
would like to build a probabilistic model P (T |S)
that assigns probabilities to trees such that the
1In this paper we work with constituency parse trees. Ini-
tial experiments, applying similar techniques to dependency
parse trees did not yield improvements.
1121
word order in trees T which are assigned higher
probability match the order of words in the target
language. A parse tree, S is a set of nodes. Inte-
rior nodes have an ordered list of children. Leaf
nodes in the tree are the words in the sentence
s, and interior nodes are labeled by the linguis-
tic constituent that they represent. Each word has
a parent node (with only one child) labeled by the
part-of-speech tag of the word.
Our model assigns non-zero probabilities to
trees that can be obtained by permuting the child
nodes at various interior nodes of the tree S. We
assume that children of a node are ordered inde-
pendently of all other nodes in the tree. Thus
P (T |S) =
?
n?I(S)
P (?(cn)|S, n, cn),
where I(S) is the set of interior nodes in the tree
S, cn is the list of children of node n and ? is a
permutation. We further assume that the reorder-
ing at a particular node is dependent only on the
labels of its children:
P (T |S) =
?
n?I(S)
P (?(cn)|cn).
We parameterize our model using a log-linear
model:
P (?(cn)|cn) =
1
Z(cn)
exp(?T f(?, cn)). (1)
We choose the simplest possible set of feature
functions: for each observed sequence of non-
terminals we have one boolean feature per per-
mutation of the sequence of non-terminals, with
the feature firing iff that particular sequence is ob-
served. Assuming, we have a training corpus C of
(T, S) tree pairs, we could optimize the parame-
ters of our model to maximize :
?
S?C P (T |S).
With the simple choice of feature functions de-
scribed above, this amounts to:
P (?(cn)|cn) =
count(?(cn))
count(cn)
,
where count(cn) is the number of times the se-
quences of nodes cn is observed in the training
data and count(?(cn)) is the number of times
that cn in S is permuted to ?(cn) in T . In Sec-
tion 6, we show considering more general fea-
ture functions and relaxing some of the indepen-
dence might yield improvements on certain lan-
guage pairs.
For each source sentence s with parse S we find
the tree T that makes the given alignment for that
sentence pair most monotone. For each node n in
the source tree S let Dn be the set of words that
are descendants of n. Let us denote by tpos(n) the
average position of words in the target sentence
that are aligned to words in Dn. Then
tpos(n) = 1|Dn|
?
w?Dn
a(w),
where a(w) is the index of the word on the target
side that w is aligned with. If a word w is not
aligned to any target word, we leave it out from
the mean position calculation above. If a word w
is aligned to many words we let a(w) be the mean
position of the words that w is aligned to. For each
node n in the tree we transform the tree by sorting
the list of children of n according to tpos. The
pairs of parse trees that we obtain (S, T ) in this
manner form our training corpus to estimate our
parameters.
In using our model, we once again go for the
simplest choice, we simply reorder the source side
sentences by choosing arg maxT P (T |S) both in
training and in testing; this amounts to reordering
each interior node based on the most frequent re-
ordering of the constituents seen in training. To
reduce the effect of noise in training alignments
we apply the reordering, only if we have seen the
constituent sequence often enough in our training
data (a count threshold parameter) and if the most
frequent reordering is sufficiently more frequent
than the next most frequent reordering (a signifi-
cance threshold).
5 Experiments
5.1 Results for French, Spanish, and German
In each language, the rule extraction was
performed using approximately 1.2M sen-
tence pairs aligned using a maxent aligner
(Ittycheriah and Roukos, 2005) trained using a
variety of domains (Europarl, computer manuals)
1122
and a maximum entropy parser for English
(Ratnaparkhi, 1999). With a significance thresh-
old of 1.2, we obtain about 1000 rules in the
eventual reordering process.
Phrase-based systems were trained for each lan-
guage pair using 11M sentence pairs spanning a
variety of publicly available (e.g. Europarl, UN
speeches) and internal corpora (IT technical and
news domains). The system phrase blocks were
extracted based on a union of HMM and max-
ent alignments with corpus-selective count prun-
ing. The lexicalized distortion model was used
as described in (Al-Onaizan and Papineni, 2006)
with a window width of up to 5 and a maximum
number of skipped (not covered) words during de-
coding of 2. The distortion model assigns a prob-
ability to a particular word to be observed with
a specific jump. The decoder uses a 5-gram in-
terpolated language model spanning the various
domains mentioned above. The baseline system
without reordering and a system with reordering
was trained and evaluated in contrastive experi-
ments. The evaluation was performed utilizing the
following (single-reference) test sets:
? News: 541 sentences from the news domain.
? TechA: 600 sentences from a computer-
related technical domain, this has been used
as a dev set.
? TechB: 1038 sentences from a similar do-
main as TechA used as a blind test.
? Dev09: 1026 sentences defined as the news-
dev2009b development set of the Workshop
on Statistical Machine Translation 2009 2.
This set provides a reference measurement
using a public data set. Previously published
results on this set can be found, for example,
in (Popovic et al, 2009).
In order to assess changes in word ordering pat-
terns prior to and after an application of the re-
ordering, we created histograms of word jumps
in the alignments obtained in the baseline as well
as in the reordered system. Given a source word
si at index i and the target word tj it is aligned
to at index j, a jump of 1 would correspond to
si+1 aligning to target word tj+1, while an align-
ment to tj?1 corresponds to a jump of -1, etc. A
2http://statmt.org/wmt09/
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1
?0.5
0
0.5
1
1.5
2
x 105
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?ES)
?8 ?6 ?4 ?2 0 2 4 6 8 10
?5000
0
5000
10000
15000
20000
Distance to next position
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?FR)
Figure 1: Difference-histogram of word order
distortions for English?Spanish (upper), and
English?French (lower).
histogram over the jump values gives us a sum-
mary of word order distortion. If all of the jumps
were one, then there is no reordering between the
two languages. To gain insight into changes in-
troduced by our reordering we look at differences
of the two histograms i.e., counts after reordering
minus counts before reordering. We would hope
that after reordering most of the jumps are small
and concentrated around one. Figure 1 shows
such difference-histograms for the language pairs
English?Spanish and English?French, respec-
tively, on a sample of about 15k sentence pairs
held out of the system training data. Here, a pos-
itive difference value indicates an increased num-
ber after reordering. In both cases a consistent
trend toward monotonicity is observed, i.e more
jumps of size one and two, and fewer large jumps.
This confirms the intended reordering effect and
indicates that the reordering rules extracted gen-
eralize well.
Table 1 shows the resulting uncased BLEU
scores for English-Spanish and English-French.
In both cases the reordering has a consistent
positive effect on the BLEU scores across test sets.
In examining the sources of improvement, we no-
ticed that word order in several noun phrases that
1123
System News TechA TechB Dev09
Baseline 0.3849 0.3371 0.3483 0.2244
Sp
an
ish
Reordered 0.4031 0.3582 0.3605 0.2320
Baseline 0.5140 0.2971 0.3035 0.2014
Fr
en
ch
Reordered 0.5242 0.3152 0.3154 0.2092
Baseline 0.2580 0.1582 0.1697 0.1281
G
er
m
an
Reordered 0.2544 0.1606 0.1682 0.1271
Baseline 20.0
H
in
di
Reordered 21.7
Table 1: Uncased BLEU scores for phrase-based
machine translation.
were not common in the training data were fixed
by use of the reordering rules.
Table 1 shows the BLEU scores for the
English?German language pair, for which a
mixed result is observed. The difference-
histogram for English?German, shown in Figure
2, differs from those of the other languages with
several increases in jumps of large magnitude, in-
dicating failure of the extracted rules to general-
ize.
The failure of our simple method to gain con-
sistent improvements comparable to Spanish and
French, along with our preliminary finding that a
relatively few manually crafted reordering rules
(we describe these in Section 6.4) tend to outper-
form our method, leads us to believe that a more
refined approach is needed in this case and will be
subject of further discussion below.
5.2 Results for Hindi
Our Hindi-English experiments were run with
an internal parallel corpus of roughly 250k sen-
tence pairs (5.5M words) consisting of various
domains (including news). To learn reordering
rules we used HMM alignments and a maxent
parser (Ratnaparkhi, 1999), with a count thresh-
old of 100, and a significance threshold of 1.7
(these settings gave us roughly 200 rules). We also
experimented with other values of these thresh-
olds and found that the performance of our sys-
tems were not very sensitive to these thresholds.
We trained Direct Translation Model 2 (DTM)
?10 ?5 0 5 10
?600
?400
?200
0
200
400
600
800
1000
1200
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?DE)
Figure 2: Difference-histogram of word order dis-
tortions for English?German.
systems (Ittycheriah and Roukos, 2007) with and
without source reordering and evaluated on a test
set of 357 sentences from the News domain.
We note that the DTM baseline includes features
(functions of target words and jump size) that al-
low it to model lexicalized reordering phenomena.
The reordering window size was set to +/- 8 words
for the baseline and system with reordered in-
puts. Table 1 shows the uncased BLEU scores for
English-Hindi, showing a gain from using the re-
ordering rules. For the reordered case, the HMM
alignments are rederived, but the accuracy of these
were no better than those of the unreordered in-
put and experiments showed that the gains in per-
formance were not due to the effect on the align-
ments.
Figure 3 shows difference-histograms for the
language pair English?Hindi, on a sample of
about 10k sentence pairs held out of the system
training data. The histogram indicates that our
reordering rules generalize and that the reordered
English is far more monotonic with respect to the
Hindi.
6 Analysis of errors and future
directions
In this section, we analyze some of the sources of
errors in reordering rules learned via our model, to
better understand directions for further improve-
ment.
1124
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1.5
?1
?0.5
0
0.5
1
1.5
x 104
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?HI)
Figure 3: Difference-histogram of word order dis-
tortions for English?Hindi.
6.1 Model weakness
In our initial experiments, we noticed that for the
most frequent reordering rules in English?Hindi
(e.g that IN NP or NP PP flips in Hindi) the prob-
ability of a reordering was roughly 65%. This
was concerning since it meant that on 35% of the
data we would be making wrong reordering deci-
sions by choosing the most likely reordering. To
get a better feel for whether we needed a stronger
model (e.g by lexicalization or by looking at larger
context in the tree rather than just the children),
we analyzed some of the cases in our training data
where (IN,NP), (NP, PP) pairs were left unaltered
in Hindi. In doing that analysis, we noticed exam-
ples involving negatives that our model does not
currently handle. The first issue was mentioned
in Section 4, where the assumption that we can
achieve the right word order by reordering con-
stituent phrases, is incorrect. The second issue
is illustrated by the following sentences: I have
some/no books, which have similar parse struc-
tures, the only difference being the determiner
some vs the determiner no. In Hindi, the order
of the fragments some books and the fragment
no books are different (in the first case the words
stay in order, in the second the flip). Handling
this example would need our model to be lexical-
ized. These issue of negatives requiring special
handling also came up in our analysis of German
(Section 6.4). Other than the negatives (which re-
quire a lexicalized model), the major reason for
the lack of sharpness of the reordering rule proba-
bility was alignment errors and parser issues. We
Aligner
Number of
Sentences fMeasure BLEU score
HMM 250k 62.4 21.7
MaxEnt 250k 76.6 21.4
Manual 5k - 21.3
Table 2: Using different alignments
look at these topics next.
6.2 Alignment accuracy
Since we rely on automatically generated align-
ments to learn the rules, low accuracy of
the alignments could impact the quality of
the rules learned. This is especially a con-
cern for English?Hindi since the quality of
HMM alignments are fairly low. To quan-
tify this effect, we learn reordering rules us-
ing three sets of alignments: HMM alignments,
alignments from a supervised MaxEnt aligner
(Ittycheriah and Roukos, 2005), and hand align-
ments. Table 2 summarizes our results using
aligners with differing alignment qualities for our
English?Hindi task and shows that quality of
alignments in learning the rules is not the driving
factor in affecting rule quality.
6.3 Parser accuracy
Accuracy of the parser in the source language is
a key requirement for our reordering method, be-
cause we choose the single best reordering based
on the most likely parse of the source sentence.
This would especially be an issue in translat-
ing from languages other than English, where the
parser would not be of quality comparable to the
English parser.
In examining some of the errors in reordering
we did observe a fair fraction attributable to
issues in parsing, as seen in the example sentence:
The rich of this country , corner almost 90% of
the wealth .
The second half of the sentence is parsed by the
Berkeley parser (Petrov et al, 2006) as:
FRAG
NP-SBJ
NN
corner
ADVP
RB
almost
NP-SBJ
NP
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
1125
and by IBM?s maximum entropy
parser parser (Ratnaparkhi, 1999) as:
VP
VB
corner
NP
NP
QP
RB
almost
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
With the first parse, we get the right Hindi order
for the second part of the sentence which is: the
wealth of almost 90% corner . To investigate the
effect of choice of parser we compared using the
Berkeley parser and the IBM parser for reorder-
ing, and we found the BLEU score essentially
unchanged: 21.6 for the Berkeley parser and
21.7 for the IBM parser. A potential source of
improvements might be to use alternative parses
(via different parsers or n-best parses) to generate
n-best reorderings both in training and at test.
6.4 Remarks on German reordering
Despite a common heritage, German word order is
distinct from English, particularly regarding verb
placement. This difference can be dramatic, if an
auxiliary (e.g. modal) verb is used in conjunction
with a full verb, or the sentence contains a subor-
dinate clause. In addition to our experiments with
automatically learned rules, a small set of hand-
crafted reordering rules was created and evalu-
ated. Our preliminary results indicate that the lat-
ter rules tend to outperform the automatically de-
rived ones by 0.5-1.0 BLEU points on average.
These rules are summarized as follows:
1. In a VP immediately following an NP, move
the negation particle to main verb.
2. Move a verb group away from a modal verb;
to the end the of a VP. Negation also moves
along with verb.
3. Move verb group to end of an embed-
ded/relative clause.
4. In a VP following a subject, move negation
to the end of VP (handling residual cases)
The above hand written rules show several weak-
nesses of our automatically learned rules for re-
ordering. Since our model is not lexicalized, nega-
tions are not handled properly as they are tagged
RB (along with other adverbs). Another limitation
apparent from the first rule above (the movement
of verbs in a verb phrase depends on the previous
phrase being a noun phrase) is that the automatic
reordering rule for a node?s children depends only
on the children of that node and not a larger con-
text. For instance, a full verb following a modal
verb is typically parsed as a VP child node of the
modal VP node, hence the automatic rule, as cur-
rently considered, will not take the modal verb
(being a sibling of the full-verb VP node) into ac-
count. We are currently investigating extensions
of the automatic rule extraction alorithm to ad-
dress these shortcomings.
6.5 Future directions
Based on our analysis of the errors and on the
hand designed German rules we would like to ex-
tend our model with more general feature func-
tions in Equation 1 by allowing features: that
are dependent on the constituent words (or head-
words), that examine a large context than just a
nodes children (see the first German rule above)
and that fire for all permutations when the con-
stituent X is moved to the end (or start). This
would allow us to generalize more easily to learn
rules of the type ?move X to the end of the
phrase?. Another direction that we feel should be
explored, is the use of multiple parses to obtain
multiple reorderings and combine these at a later
stage.
7 Conclusions
In this paper we presented a simple method to
automatically derive rules for reordering source
sentences to make it look more like target
language sentences. Experiments (on inter-
nal and public test sets) indicate performance
gains for English?French, English?Spanish,
and English?Hindi. For English?German we
did not see improvements with automatically
learned rules while a few hand designed rules did
give improvements, which motivated a few direc-
tions to explore.
1126
References
[Al-Onaizan and Papineni2006] Al-Onaizan, Yaser and
Kishore Papineni. 2006. Distortion models for sta-
tistical machine translation. In Proceedings of ACL.
[Badr et al2009] Badr, Ibrahim, Rabih Zbib, and
James Glass. 2009. Syntactic phrase reordering for
english-to-arabic statistical machine translation. In
Proceedings of EACL.
[Chiang2005] Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine transla-
tion. In Proceedings of ACL.
[Collins et al2005] Collins, Michael, Philipp Koehn,
and Ivona Kucerova. 2005. Clause restructuring
for statistical machine translation. In Proceedings
of ACL.
[Habash2007] Habash, Nizar. 2007. Syntactic prepro-
cessing for statistical machine translation. In MT
Summit.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In Proceedings of HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Proceedings of HLT-NAACL, pages 57?64.
[Koehn et al2003] Koehn, Philipp, Franz Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of HLT-NAACL.
[Niessen and Ney2001] Niessen, Sonja and Hermann
Ney. 2001. Morpho-syntactic analysis for reorder-
ing in statistical machine translation. In Proc. MT
Summit VIII.
[Petrov et al2006] Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning accu-
rate, compact, and interpretable tree annotation. In
COLING-ACL.
[Popovic et al2009] Popovic, Maja, David Vilar,
Daniel Stein, Evgeny Matusov, and Hermann Ney.
2009. The RWTH machine translation system for
WMT 2009. In Proceedings of WMT 2009.
[Ramanathan et al2008] Ramanathan, A., P. Bhat-
tacharyya, J. Hegde, R. M. Shah, and M. Sasikumar.
2008. Simple syntactic and morphological process-
ing can help english-hindi statistical machine trans-
lation. In Proceedings of International Joint Con-
ference on Natural Language Processing.
[Ratnaparkhi1999] Ratnaparkhi, Adwait. 1999. Learn-
ing to parse natural language with maximum en-
tropy models. Machine Learning, 34(1-3).
[Tillman2004] Tillman, Christoph. 2004. A unigram
orientation model for statistical machine translation.
In Proceedings of HLT-NAACL.
[Wang et al2007] Wang, Chao, Michael Collins, and
Philipp Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In Proceedings
of EMNLP-CoNLL.
[Xia and McCord2004] Xia, Fei and Michael McCord.
2004. Improving a statistical mt system with auto-
matically learned rewrite patterns. In Proceedings
of Coling.
[Xu et al2009] Xu, Peng, Jaeho Kang, Michael Ring-
gaard, and Franz Och. 2009. Using a dependency
parser to improve SMT for Subject-Object-Verb lan-
guages. In Proceedings of NAACL-HLT.
[Yamada and Knight2001] Yamada, Kenji and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of ACL.
[Zhang et al2007] Zhang, Yuqi, Richard Zens, and
Hermann Ney. 2007. Chunk-level reordering
of source language sentences with automatically
learned rules for statistical machine translation. In
NAACL-HLT AMTA Workshop on Syntax and Struc-
ture in Statistical Translation.
[Zollmann and Venugopal2006] Zollmann, Andreas
and Ashish Venugopal. 2006. Syntax augmented
machine translation via chart parsing. In Pro-
ceedings on the Workshop on Statistical Machine
Translation.
[Zollmann et al2008] Zollmann, Andreas, Ashish
Venugopal, Franz Och, and Jay Ponte. 2008. A
systematic comparison of phrase-based, hierar-
chical and syntax-augmented statistical MT. In
Proceedings of COLING.
[Zwarts and Dras2007] Zwarts, Simon and Mark Dras.
2007. Syntax-based word reordering in phrase-
based statistical machine translation: why does it
work? In Proc. MT Summit.
1127
Coling 2010: Poster Volume, pages 1283?1291,
Beijing, August 2010
Urdu and Hindi: Translation and sharing of linguistic resources
Karthik Visweswariah, Vijil Chenthamarakshan, Nandakishore Kambhatla
IBM Research India
{v-karthik,vijil.e.c,kambhatla}@in.ibm.com
Abstract
Hindi and Urdu share a common phonol-
ogy, morphology and grammar but are
written in different scripts. In addition,
the vocabularies have also diverged signif-
icantly especially in the written form. In
this paper we show that we can get rea-
sonable quality translations (we estimated
the Translation Error rate at 18%) between
the two languages even in absence of a
parallel corpus. Linguistic resources such
as treebanks, part of speech tagged data
and parallel corpora with English are lim-
ited for both these languages. We use the
translation system to share linguistic re-
sources between the two languages. We
demonstrate improvements on three tasks
and show: statistical machine translation
from Urdu to English is improved (0.8
in BLEU score) by using a Hindi-English
parallel corpus, Hindi part of speech tag-
ging is improved (upto 6% absolute) by
using an Urdu part of speech corpus and
a Hindi-English word aligner is improved
by using a manually word aligned Urdu-
English corpus (upto 9% absolute in F-
Measure).
1 Introduction
Hindi and Urdu are official languages of India
and Urdu is also the national language of Pak-
istan. Hindi is spoken by around 853 million peo-
ple and Urdu by around 164 million people (Malik
et al, 2008). Although native speakers of Hindi
can comprehend most of spoken Urdu and vice
versa, these languages have diverged a bit since
independence of India and Pakistan ? with Hindi
deriving a lot of words from Sanskrit and Urdu
from Persian. One clear difference between Hindi
and Urdu is the script: Hindi is written in a left-
to-right Devanagari script while Urdu is written
in Nastaliq calligraphy style of the right-to-left
Perso-Arabic script. Hence, despite the similari-
ties, it is impossible for an Urdu speaker to read
Hindi text and vice versa. The first problem we
address is the translation between Hindi and Urdu
in the absence of a Hindi-Urdu parallel corpus.
Though these languages together are spoken by
around a billion people they are not very rich in
linguistic resources. A treebank for Hindi is still
under development1 and part of speech taggers for
Hindi and Urdu are trained on very small amounts
of data. For translation between Hindi/Urdu and
English there are no large corpora, the available
corpora are an order of magnitude smaller than
those available for European languages or Arabic-
English. Given the lack of linguistic resources
in each of the languages and the similarities be-
tween these languages, we explore whether each
language can benefit from resources available in
the other language.
1.1 Urdu-Hindi script conversion/translation
Sharing resources between Hindi and Urdu re-
quires us to be able to convert from one written
form to the other. Given that the languages share a
good fraction of their spoken vocabularies, the ob-
vious approach to convert between the two scripts
would be to transliterate between them. While this
approach has recently been attempted (Malik et
al., 2009), (Malik et al, 2008) there are two main
problems with this approach.
Challenges in Hindi-Urdu transliteration:
Urdu uses diacritical marks that were taken from
the Arabic script which serve various purposes.
Urdu has short and long vowels. Short vowels
are indicated by placing a diacritic with the con-
1https://verbs.colorado.edu/hindi
wiki/index.php/Hindi Treebank Data
1283
Figure 1: An Urdu sentence transliterated and
translated to Hindi
sonant that precedes it in the syllable. The diacrit-
ical marks are also used for gemination (doubling
of a consonant), which in Hindi is handled using a
conjunct form where the consonant is essentially
repeated twice. Yet another function of diacritical
marks is to mark the absence of a vowel follow-
ing a base consonant. Though diacritical marks
are critical for correct pronunciation and some-
times even for disambiguation of certain words,
they are sparingly used in written material in-
tended for native speakers of the language. Miss-
ing diacritical marks create substantial difficulties
for transliteration systems. Another difficulty is
created by the fact that Urdu words cannot have
a short vowel at the end of a word, whereas the
corresponding Hindi word can sometimes have a
short vowel. This cannot be resolved deterministi-
cally and results ambiguity in transliteration from
Urdu to Hindi. A third issue is the presence of
certain sounds (and their corresponding letters)
that have no equivalent in Urdu. These letters
are approximated in Urdu with phonetic equiva-
lents. Transliteration from Urdu to Hindi suffers
in the presence of words with these letters. Re-
cent work on Urdu-Hindi transliteration (Malik et
al., 2009) report transliteration word error rates
of 16.4% and 23.1% for Urdu sentences with and
without diacritical marks respectively. This prob-
lem is illustrated in Figure 1. The figure shows an
Urdu sentence that is transliterated to Hindi using
the Hindi Urdu Machine Transliteration (HUMT)
system 2 and translated using our Statistical Ma-
chine Translation System. The words which are
in red are transliteration errors (mainly because of
missing diacritical marks).
Difference in Word Frequency Distribu-
tions: Even if we could transliterate perfectly be-
tween Urdu and Hindi it might not be desirable to
2http://www.puran.info/HUMT/HUMT.aspx
do so from the point of view of human understand-
ing or for machine consumption. This is because
word frequencies of shared words would be dif-
ferent in Hindi and Urdu. At the extreme, there
are several Urdu words that a fluent Hindi speaker
would not understand and vice versa. More com-
monly, native speakers of Hindi and Urdu would
use different words to refer to the same concept,
even though both these words are technically cor-
rect in either of these languages. In initial experi-
ments to quantify this issue on our corpus, which
is mainly from the news domain, we estimated
that around 28% of the word tokens in Urdu would
not be natural in Hindi. This estimate assumes
perfect transliteration, and we estimated the total
error rate including transliteration at around 55%
for the publicly available HUMT system. In Fig-
ure 1, the words that have been underlined have
been replaced using a different word by our SMT
system, even though the original word might be
technically correct. Our preliminary experiments
exploring this issue convinced us that to be able
to convert from Urdu into natural Hindi (and vice
versa) we would need to go beyond transliteration
to translation to deal with the divergence of the
vocabularies in the written forms of the two lan-
guages.
Importance of Context We would like to point
out that in addition to word for word fidelity,
there are more subtle issues in translating from
Urdu-Hindi. One issue is that words in Hindi are
drawn from different source languages, and with
word to word translations, we might end up with
phrases that are unnatural. For example, consider
different ways of writing the English phrase Na-
tional and News in Hindi. The word National
in Hindi could possibly be written as rashtriya,
kaumi or national which have origins in Sanskrit,
Persian/Arabic and English respectively. Simi-
larly the word News could be written as samachar,
khabaren or news (once again with origins in San-
skrit, Persian/Arabic and English). The natural
ways for writing the phrase national news are:
rashtriya samachar, kaumi khabaren or national
news, any of the other six combinations would be
quite rare.
Another issue is that corresponding words in
Hindi and Urdu might have different genders. An
1284
example from (Sinha, 2009) are the words vajah
(Urdu, feminine) and karan (Hindi, masculine),
which would mean that the phrase because of him
would be written as us ke karan in Hindi and as us
ki vajah se in Urdu. We note that the ke in Hindi
and ki in Urdu are different because of the differ-
ence in genders of the word following them. This
suggests we would need to go beyond word for
word translation and would need to use a higher
order n-gram language model to translate with fi-
delity between Hindi and English.
We have established the need for going beyond
transliteration, but a key challenge is to achieve
good translation accuracy in the absence of a
Hindi-Urdu parallel corpus. In Section 3 we de-
scribe a multi-pronged approach to translate be-
tween Hindi and Urdu in the absence of a parallel
corpus that exploits the similarities between the
languages.
1.2 Applications: sharing linguistic resources
We next outline the three tasks for which we con-
sider sharing resources between Hindi and Urdu
which serve as a test of the efficacy of our sys-
tems.
Statistical machine translation
In recent years, there is a lot of interest in Statis-
tical Machine Translation (SMT) Systems (Brown
et al, 1993). Modern SMT systems (Koehn et al,
2003; Ittycheriah and Roukos, 2007) learn trans-
lation models based on large amounts of paral-
lel data. The quality of an SMT system is de-
pendent on the amount of parallel data on which
the system is trained. Unfortunately, for the pairs
Urdu-English and Hindi-English, parallel data are
not available in large quantities, thereby limiting
the quality of these SMT systems. In this pa-
per we show that we can improve the accuracy of
an Urdu?English SMT system by using a Hindi-
English parallel corpus.
Part of Speech tagging
Part of Speech (POS) tagging involves marking
the part of speech of a word based on its defini-
tion and surrounding context in a sentence. Se-
quential modeling techniques like HiddenMarkov
Models (Rabiner, 1990) and Conditional Random
Fields (Lafferty et al, 2001) are commonly used
to build Part of Speech taggers. These models are
typically trained using a manually tagged part of
speech corpus. Manual tagging of data requires
lot of human effort and hence large corpora are not
readily available for many languages. We improve
a Hindi POS tagger by using a manually tagged
Urdu POS corpus.
Supervised bitext alignment
Machine generated word alignments between
pairs of languages have many applications: build-
ing statistical machine translation systems, build-
ing dictionaries, projection of syntactic informa-
tion to resource poor languages (Yarowsky and
Ngai, 2001). Most of the early work on generat-
ing word alignments has been unsupervised, e.g.
IBM Models 1-5 (Brown et al, 1993), recent im-
provements on the IBM Models (Moore, 2004),
and the HMM algorithm described in (Vogel et al,
1996). Recently, significant improvements in per-
formance of aligners have been achieved by the
use of human annotated word alignments (Itty-
cheriah and Roukos, 2007; Lacoste-Julien et al,
2006). We describe a method to transfer man-
ual word alignments from Urdu-English to Hindi-
English to improve Hindi-English word align-
ments.
1.3 Contributions
Our main contributions are summarized below:
We present a hybrid technique to translate be-
tween Hindi and Urdu in the absence of a Hindi-
Urdu parallel corpus that significantly improves
upon past efforts to convert between Hindi and
Urdu via transliteration. We validate the efficacy
of the translation systems we present, by using it
to share linguistic resources between Hindi and
Urdu for three important tasks:
1. We improve a part of speech tagger for Hindi
using an Urdu part of speech corpus.
2. We use manual Urdu-English word align-
ments to improve the task of Hindi-English
bitext alignments.
3. We use a Hindi-English parallel corpus to
improve translation from Urdu to English.
1285
2 Related work
Converting between the scripts of Hindi and Urdu
is non-trivial and has been a recent focus (Ma-
lik et al, 2008; Malik et al, 2009). (Malik et
al., 2008) uses hand designed rules encoded us-
ing finite state transducers to transliterate between
Hindi and Urdu. As reported in (Malik et al,
2009) these hand designed rules achieve accu-
racies of only about 50% in the absence of di-
acritical marks. (Malik et al, 2009) improves
Urdu?Urdu transliteration performance to 79%
by post processing the output of the transducer
with a statistical language model. In contrast to
(Malik et al, 2009) we use a statistical model
for character transliteration. As discussed in Sec-
tion 1.1, due to the divergence of vocabularies
in written Hindi and Urdu, transliteration is not
sufficient to convert from written Urdu to written
Hindi. We also use a more flexible model that
allows for more natural translations by allowing
Urdu words to translate into Hindi words that do
not sound the same.
(Sinha, 2009) builds an English-Urdu machine
translation system using an English-Hindi ma-
chine translation system and a Hindi-Urdu word
mapping table, suitably adjusted for part of speech
and gender. Their system is not statistical, and
is largely based on manual creation of a large
database of Hindi-Urdu correspondences. Addi-
tionally, as mentioned in the conclusion, their sys-
tem cannot be used for direct translation from
Hindi to Urdu, since a grammatical analysis of
the English provides information necessary for the
Hindi to Urdu mapping. In contrast to this work,
our techniques are largely statistical, require min-
imal manual effort and can directly translate be-
tween Hindi and Urdu without the associated En-
glish.
3 Approach to translating between Hindi
and Urdu
As discussed in Section 1, transliteration between
Hindi and Urdu is not a straightforward task and
current efforts result in fairly high error rates. We
would like to combine the approaches of translit-
eration and translation since our goal is to use the
translation for sharing linguistic resources rather
than for direct consumption.
We use a fairly standard phrase based transla-
tion system to translate between Hindi and Urdu.
The key challenge that we overcome is being able
to develop such a system with acceptable accu-
racy in the absence of Hindi-Urdu resources (we
have neither a parallel corpus nor a dictionary with
sufficient coverage). In spite of the absence of re-
sources, translation between this language pair is
made feasible by the fact that word order is largely
maintained and translation can be done maintain-
ing a word to word correspondence. There are
some exceptions to the monotonicity in the two
languages. Consider the English phrase Govern-
ment of Sindh which in Urdu would be hukumat
e sindh in the same word order as in English,
while in Hindi it would be sindhi sarkar with the
word order flipped (with respect to English and
Urdu). This example also shows that sometimes
we do not have a word for word translation be-
tween Hindi and Urdu, the word sindhi in Hindi
corresponding to the Urdu words e sindh. In spite
of these exceptions, Hindi-Urdu translation can
largely be done with the monotonicity assumption
and with the assumption of word to word corre-
spondences. Thus the central issue in translating
between Hindi and Urdu is the creation of a word
to word conditional probability table. We explain
our technique assuming we are translating from
Urdu to Hindi. We take a hybrid approach to cre-
ating this table, using three different approaches.
The first approach is the pivot language ap-
proach (Wu and Wang, 2007), with English as a
pivot language. We get probabilities of a Urdu
word u being generated by a Hindi word h, con-
sidering intermediate English phrases e as:
Pp(u|h) =
?
e
P (u|e)P (e|h)
The translation probabilities P (u|e) and P (e|h)
are obtained using an Urdu-English and an
English-Hindi parallel corpus respectively.
This approach works reasonably well, but suf-
fers from a couple of drawbacks. There are sev-
eral common Hindi and Urdu words for which the
translation is unsatisfactory. This is because the
alignments for these words are not precise, they
often do not align to any English word, or align to
1286
an English words in combination with other Hindi
words. A common example of this is with verbs,
consider for example the English sentence
He works
which would translate into Hindi/Urdu as:
vah kaam karta hai
with word alignments He? vah, works? kaam
karta hai . Automatic aligners often make mis-
takes on these multi-word alignments, and this
create problems for words like karta and hai
which often do not have direct equivalents in En-
glish. To deal with this issue we manually build a
small phrase table for the most frequent Hindi and
Urdu words by a consulting an online Hindi-Urdu-
English dictionary (Platts, 1884). We also man-
ually handle the frequent examples we observed
of cases where we need to handle differences in
tokenization between Hindi and Urdu (e.g keliye
written as one word in Urdu and as ke liye in
Hindi).
The other issue with the pivot language ap-
proach is that for word pairs which are rare in
one of the languages,?e P (u|e)P (e|h) can eas-
ily work out to zero. This is exacerbated by align-
ment errors for rarer words. Thus, to strengthen
our phrase table especially for infrequent words,
we use a transliteration approach to build a phrase
table. Note that for rare words like names of peo-
ple and places, the words in Hindi and Urdu are
transliterations of each other.
In light of the issues in transliterating between
Hindi and Urdu (Malik et al, 2008; Malik et
al., 2009) we take a statistical approach (Abdul-
Jaleel and Larkey, 2003) to building a translitera-
tion based phrase table.
We assume a generative model for producing
Urdu words from Hindi words based on a charac-
ter transliteration probability table Pc. The prob-
ability Pt(u|h) of generating a Urdu word u from
a Hindi word h is given by:
Pt(u|h) =
?
a
?
i
Pc(ui|ha(i))P (ai|ai?1),
where a represents the alignment between the
Hindi and Urdu characters, a(i) is the the index
of the Hindi character that the ith Urdu charac-
ter is aligned to, Pc(uc|hc) is the probability of
an Urdu character uc being generated by a Hindi
character hc and P (ai|ai?1) represents a distor-
tion probability. Since transliteration is mono-
tonic and we want to encourage small jumps we
set: P (ai|ai?1) = c?(ai?ai?1) for ai > ai?1 and
0 otherwise. To obtain Pc we use the EM algo-
rithm and we can reuse standard machinery that
is used to obtain HMM word alignments in Statis-
tical Machine Translation (with the constraint of
Monotone alignments). To calculate a translitera-
tion based phrase table, for each Hindi word h we
search over a large vocabulary of Urdu words and
retain words u for which Pt(u|h) is sufficiently
high as possible transliterations of h. We set the
probabilities in the transliteration based phrase ta-
ble to be proportional to Pt(u|h). Finding this ta-
ble requires calculating Pt(u|h) for every pair of
words in the Urdu and Hindi vocabulary, we use
the Forward-Backward algorithm for efficiency
and parallelize the calculations over several ma-
chines.
The only remaining issue is how we get train-
ing data to train our transliteration model. To ob-
tain such training data we use a table of consonant
character conversions between Hindi and Urdu as
given in (Malik et al, 2008). We look for words in
our pivot language based translation table, where
there are at least three consonants and at least 50%
of the consonants are shared. We observed that
this yields pairs of words that are transliterations
of one another with high precision. These word
pairs are used as training data to build our charac-
ter transliteration model Pc.
Final word translation table is obtained by com-
bining our three approaches as follows: If the
word is present in our dictionary, we use the trans-
lation given in the dictionary and exclude all oth-
ers, if not we linearly interpolate between the
probability table we get based on using English
as a pivot language and probability table we get
based on transliteration.
4 Experimental results
In this section we report on experiments to eval-
uate the quality of our translation method de-
scribed in Section 3 and report on the application
of Hindi?Urdu translation to the sharing of lin-
guistic resources between the two languages.
1287
Algorithm 1 Create Urdu-Hindi Phrase Table
for all u such that u is very frequent Urdu word
do
h? Hindi word for u from dictionary
Pd(u|h)? 1
end for
U ? Urdu vocabulary
H ? Hindi vocabulary vocabulary
for all u ? U , h ? H do
Pp(u|h) ?
?
e P (u|e)P (e|h) {Create an
Urdu-Hindi translation table using English as
the pivot}
end for
for all u ? U , h ? H such that Pp(u|h) > ?
and ConsonantOverlap(u, h) > ? do
Add (u, h) to training set T
end for
Pc ?
argmax
Q
?
(u,h)?T
?
a
?
i
Q(ui|hai))P (ai|ai?1)
{Maximize using EM}
for all u ? U , h ? H do
Pt(u|h) ? c
?
a
?
i
Pc(ui|ha(i))P (ai|ai?1)
{Use Forward-Backward Algorithm}
end for
for all u ? U , h ? H do
if Pd(u|h)? 1 then
Pfinal(u|h)? 1
else
Pfinal(u|h)? ?pPp(u|h) + ?tPt(u|h)
end if
end for
4.1 Evaluation of Hindi-Urdu translation
We built a Hindi-Urdu transliteration system as
explained in Section 3. For building a pivot
language based translation table we used 70k
sentences from the NIST MT-08 corpus train-
ing corpus for Urdu-English. For Hindi-English
we used an internal corpus of 230k sentences.
We built our statistical transliteration model on
roughly 3k word pairs that we obtained as de-
scribed in Section 3. For Urdu?Hindi translation,
we used a five gram language model built from
a crawl of archives from Hindi news web sites
(the corpus size was about 60 million words). For
Hindi?Urdu translation we use the MT-08 Urdu
corpus (about 1.5 million words) to build a trigram
LM.
We evaluated the translation system in translat-
ing from Urdu to Hindi. We asked an annotator to
evaluate 100 sentences ( 2700 words), by marking
an error on a word if it was a wrong translation or
unnatural in Hindi. We compared our translation
system against the Hindi Urdu Machine Translit-
eration (HUMT) system3. We found an error rate
of 18% for our system as against 46% for the
HUMT system.
4.2 Word alignments
In this section we describe experiments at im-
proving a Hindi-English word aligner using hand
alignments for an Urdu-English corpus. For the
Urdu-English corpus we use a manually word
aligned corpus of roughly 10k sentences, while
for the Hindi-English corpus we had roughly 3k
sentences out of which we set aside 300 sentences
( 5300 words) for a test set. In addition to these
(relatively) small supervised corpora we also use
a sentence parallel Hindi-English corpus (without
manual word alignments) of roughly 250k sen-
tences.
For word alignments we use the Maximum
Entropy aligner described in (Ittycheriah and
Roukos, 2005) that is trained using hand aligned
training data. We first translate the Urdu sentences
in the Urdu-English word aligned corpus to Hindi,
and then transfer the alignments by simply replac-
ing the alignment links to a Urdu word by links
to the corresponding decoded Hindi word. The
above procedure covers bulk of the cases since
Urdu-Hindi translation is largely a word to word
translation. The special case of a phrase of multi-
ple Urdu words decoded to multiple Hindi words
is handled as follows: we align each of the words
in the Hindi phrase to the union of the sets of
English words that each word in the Urdu phrase
aligns to. Once we convert the Urdu-English man-
ual alignments to an additional corpus we build
two Hindi-English alignment models, one on the
original corpus, the other on the (Urdu?Hindi)-
English corpus. The MaxEnt aligner (Ittycheriah
and Roukos, 2005) models the probability of a
3http://www.puran.info/HUMT/HUMT.aspx
1288
nTrain Hindi data + Urdu
5 60.8 69.8
50 64.1 70.5
800 71.4 73.0
2800 75.1 75.7
Table 1: Word alignment F-Measure as a func-
tion of the number of manually aligned Hindi-
English sentences used for training. The third col-
umn shows improvements obtained by adding 10k
Urdu-English word alignments sentences.
particular set of links in the alignment L given the
source sentence S and the target sentence T as:
P (L|S, T ) = ?Mi=1 p(li|tM1 , sK1 , li?11 ). Let us de-
note by Ph and Pu the alignment models trained
on the Hindi-English and the (Urdu?Hindi)-
English corpora respectively. We combine these
models log-linearly to obtain our final model for
alignment:
P (L|S, T ) = P?h (L|S, T )P 1??u (L|S, T ).
To find the most likely alignment we use the same
algorithm as in (Ittycheriah and Roukos, 2005)
since the structure of the model is unchanged.
We report on the performance (Table 1) of a
baseline Hindi-English word aligner built with
varying amounts of Hindi-English manually word
aligned training data compared against an aligner
that combines in a model trained on the 10k
(Urdu?Hindi)-English sentences. We observe
large gains with small amounts of labelled Hindi-
English alignment data, and even when we have
2800 sentences of Hindi-English data we see a
gain in performance adding in the Urdu data.
We note that the MaxEnt aligner we use (Itty-
cheriah and Roukos, 2005) defaults to (roughly)
doing an HMM alignment using a word trans-
lation matrix obtained via unsupervised training.
Thus the aligners reported on in Table 1 use a
large amount of unsupervised data in addition to
the small amounts of labelled data mentioned in
the Table.
4.3 POS tagging
Unlike English for which there is an abundance
of POS training data for Hindi and Urdu data is
quite limited. For our experiments, we use the
num. words f(wi, ti), g(ti?1, ti) + h(tui , ti)
5k 76.5 82.5
10k 81.7 84.7
20k 84.5 86.7
47k 90.6 91.0
Table 2: POS tagging accuracy as a function of
the amount of Hindi POS tagged data used to
build the model. The third column indicates the
use of the Urdu data via a feature type.
CRULP corpus (Hussain, 2008) for Urdu and a
corpus from IITB (Dalal et al, 2007) for Hindi.
The CRULP POS corpus has 150k words and
uses a tagset of size 46 to tag the corpus. The
IITB corpus has 50k words and uses a tagset of
size 26. We set a side a test set of size 5k words
from the IITB corpus. For part of speech tagging
we use CRFs (Lafferty et al, 2001) with two types
of features, f(ti, wi) and g(ti, ti?1). With the
small amounts of training data we have, adding
additional feature templates degraded the perfor-
mance.
In our POS tagging experiments we consider
using the Urdu corpus to help POS tagging in
Hindi. We first translate all of the CRULP Urdu
data to Hindi. We cannot simply add in this data
to the training data because of differences in the
tagsets used in the data sets for the two languages.
In order to make use of the additional Urdu POS
tagged data (translated to Hindi), we build a sep-
arate POS tagger on this data, and use predictions
from this model as a feature in training the Hindi
POS tagger. We use these predictions via a fea-
ture template h(ti, tui ) where tui denotes the tag
assigned to the ith word by the POS tagger built
from the CRULP Urdu data set translated into
Hindi.
We present results in Table 2 with varying
amounts of Hindi data used for training, in each
case we present results with and without use of
the Urdu resources. We see a small gain even
when we use all of the available Hindi training
data and as expected we see larger gains when
smaller amounts of Hindi data are used.
We analyzed the type of errors and the er-
ror reduction when using the Urdu data for the
case where we used only 5k words of Hindi data.
1289
We find that the two frequent error types that
were greatly reduced were noun being tagged
as main verb (reduction of 65% relative) and
main verb tagged as auxiliary verb (reduction of
71%). Reduction in confusion between nouns and
main verbs is expected since these are open word
classes that can most benefit from additional data.
This also causes the reduction in errors of tag-
ging main verbs as auxiliary verbs, since in Hindi,
verbs are multi word groups with a main verb fol-
lowed by one or more auxiliary verbs. Reduction
of error rate in most of the other error types were
close to the overall error rate reduction.
4.4 Sharing parallel corpora for machine
translation
We experimented with using our internal Hindi-
English parallel corpus ( 230k) sentences to obtain
better translation for Urdu-English. The Urdu-
English corpus we use is the NIST MT-08 training
data set ( 70k sentences). We use the Direct Trans-
lation Model 2 (DTM) described in (Ittycheriah
and Roukos, 2007) for all our translation experi-
ments.
We build our baseline Urdu?English system
using the NIST MT-08 training data. In training
our DTM model we use HMM alignments, align-
ments with the MaxEnt aligner, and hand align-
ments for 10k sentences (the hand alignments
were used to train the MaxEnt aligner).
We translated the Hindi in our Hindi-English
corpus to Urdu, creating an additional Urdu-
English corpus. We then use a MaxEnt aligner
to align the Urdu-English words in this corpus.
Since we expect this corpus to be relatively noisy
due to incorrect translation from Urdu to Hindi we
do not include this corpus while generating HMM
alignments. We add the synthetic Urdu-English
data with MaxEnt alignments to our baseline data
and train a DTM model. Results comparing to the
baseline are given Table 3, which shows an im-
provement of 0.8 in BLEU score over the baseline
system by using data from the Hindi-English cor-
pus.
This improvement is not due to unknown
words being covered (the vocabulary covered is
the same). Also note that in the bridge language
approach we cannot get alernative translations
Corpus MT08 Eval
Urdu 23.1
+Hindi 23.9
Table 3: Improvement in Urdu-English machine
translation using Hindi-English data .
for single words that were not already present in
the Urdu-English phrase table. Thus, we believe
that the improvement is due to longer phrases
being seen more often in training. An example
improved translation is shown below:
Ref: just as long as its there they feel safe
Baseline: as long as this they just think there are safe
Improved: just as long as they are there they feel safe
5 Conclusions
In this paper, we showed that we can translate be-
tween Hindi and English without a parallel corpus
and improve upon previous efforts at transliterat-
ing between the two languages. We also showed
that Hindi-Urdu translation can be useful to the
sharing of linguistic resources between the two
languages. We believe this approach to sharing
linguistic resources will be of immense value es-
pecially with resources like treebanks which re-
quire a large effort to develop.
Acknowledgments
We thank Salim Roukos and Abe Ittycheriah for
discussions that helped guide our efforts.
References
[AbdulJaleel and Larkey2003] AbdulJaleel, Nasreen
and Leah S. Larkey. 2003. Statistical transliteration
for english-arabic cross language information
retrieval. In CIKM.
[Brown et al1993] Brown, Peter F., Vincent J.Della
Pietra, Stephen A. Della Pietra, and Robert. L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19:263?311.
[Dalal et al2007] Dalal, Aniket, Kumara Nagaraj, Uma
Sawant, Sandeep Shelke, and Pushpak Bhat-
tacharyya. 2007. Building feature rich pos tagger
for morphologically rich languages. In Proceed-
ings of the Fifth International Conference on Nat-
ural Language Processing, Hyderabad, India, Jan-
uary.
1290
[Hussain2008] Hussain, Sarmad. 2008. Resources for
urdu language processing. In Proceedings of the 6th
workshop on Asian Language Resources.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Sidner, Candace L., Tanja Schultz, Matthew
Stone, and ChengXiang Zhai, editors, HLT-NAACL,
pages 57?64. The Association for Computational
Linguistics.
[Koehn et al2003] Koehn, Philipp, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
[Lacoste-Julien et al2006] Lacoste-Julien, Simon,
Benjamin Taskar, Dan Klein, and Michael I. Jordan.
2006. Word alignment via quadratic assignment. In
HLT-NAACL.
[Lafferty et al2001] Lafferty, J., A. McCallum, , and
F. Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning.
[Malik et al2008] Malik, M. G. Abbas, Christian
Boitet, and Pushpak Bhattacharyya. 2008. Hindi
urdu machine transliteration using finite-state trans-
ducers. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 537?544, Manchester, UK, August.
Coling 2008 Organizing Committee.
[Malik et al2009] Malik, Abbas, Laurent Besacier,
Christian Boitet, and Pushpak Bhattacharyya. 2009.
A hybrid model for urdu hindi transliteration. In
Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration (NEWS 2009), pages
177?185, Suntec, Singapore, August. Association
for Computational Linguistics.
[Moore2004] Moore, Robert C. 2004. Improving
ibm word alignment model 1. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
518?525, Barcelona, Spain, July.
[Platts1884] Platts, John T. 1884. A dictionary of
Urdu, classical Hindi and English. W. H. Allen and
Co.
[Rabiner1990] Rabiner, Lawrence R. 1990. A tutorial
on hidden markov models and selected applications
in speech recognition. pages 267?296.
[Sinha2009] Sinha, R. Mahesh K. 2009. Developing
english-urdu machine translation via hindi. In Third
Workshop on Computational Approaches to Arabic-
Script-based Languages.
[Vogel et al1996] Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of
the 16th conference on Computational linguistics,
pages 836?841, Morristown, NJ, USA. Association
for Computational Linguistics.
[Wu and Wang2007] Wu, Hua and Haifeng Wang.
2007. Pivot language approach for phrase-based
statistical machine translation. In ACL.
[Yarowsky and Ngai2001] Yarowsky, David and Grace
Ngai. 2001. Inducing multilingual pos taggers and
np bracketers via robust projection across aligned
corpora. In NAACL.
1291
