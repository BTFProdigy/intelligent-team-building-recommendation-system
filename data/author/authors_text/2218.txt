Text Genre Detection Using Common Word Frequencies 
E. STAMATATOS, N. FAKOTAKIS, and G. KOKKINAKIS 
Dept. of Electrical and Computer Engineering 
University of Patras 
Patras, Greece, 26500 
stamatatos@wcl.ee.upatras.gr 
Abstract 
In this paper we present a method for 
detecting the text genre quickly and easily 
following an approach originally proposed 
in authorship attribution studies which uses 
as style markers the frequencies of 
occurrence of the most frequent words in a 
training corpus (Burrows, 1992). In contrast 
to this approach we use the frequencies of 
occurrence of the most frequent words of the 
entire written language. Using as testing 
ground a part of the Wall Street Journal 
corpus, we show that the most frequent 
words of the British National Corpus, 
representing the most frequent words of the 
written English language, are more reliable 
discriminators oftext genre in comparison to 
the most frequent words of the training 
corpus. Moreover, the fi'equencies of 
occurrence of the most common punctuation 
marks play an important role in terms of 
accurate text categorization aswell as when 
dealing with training data of limited size. 
Introduction 
The development of text databases via the 
Internet has given impetus to research in 
computational linguistics towards the automatic 
handling of this information. In particular, the 
enormous amount of texts coming from 
heterogeneous sources revealed the need for 
robust ext classification tools which are able to 
be easily ported to other domains and natural 
languages and be employed with minimal 
computational cost. 
Apart from the propositional content &the text, 
stylistic aspects can also be used as 
classificatory means. Biber studied the stylistic 
differences between written and spoken 
language (Biber, 1988) as well as the variation 
of registers in a cross-linguistic omparison 
(Biber, 1995) and presented a model for 
interpreting the functions of various linguistic 
features. Unfortunately, his model can not be 
easily realized using existing natural anguage 
processing tools. On the other hand, some 
computational models for detecting 
automatically the text genre have recently been 
available (Karlgren and Cutting, 1994; Kessler 
et al, 1997). Kessler gives an excellent 
summarization f the potential applications of a 
text genre detector. In particular, part-of-speech 
tagging, parsing accuracy and word-sense 
disambiguation could be considerably enhanced 
by taking genre into account since certain 
grammatical constructions or word senses are 
closely related to specific genres. Moreover, in 
information retrieval the search results could be 
sorted according to the genre as well. 
Towards the automatic detection of text genre, 
various types of style markers (i.e., countable 
linguistic features) have been proposed so far. 
Karlgren and Cutting (1994) use a combination 
of structural markers (e.g., noun count), lexical 
markers (e.g., "it" count), and token-level 
markers (e.g., words per sentence average, 
type/token ratio, etc.). Kessler et al (1997) 
avoid structural markers since they require 
tagged or parsed text and replace them with 
character-level markers (e.g., punctuation mark 
counts) and derivative markers, i.e., ratios and 
variation measures derived from measures of 
lexical and character-level markers. 
Furthermore, some interesting stylometric 
approaches have been followed in authorship 
attribution studies. Specifically, various 
functions that attempt to represent the 
vocabulary richness have been proposed 
(Honore 1979; Sichel, 1975). The combination 
808 
of the best vocabulary richness functions in a 
lnultivariate model can then be used tbr 
capturing the characteristics of a stylistic 
category (llohnes, 1992). However, recent 
studies have shown that the m~tjority of these 
liinctions depend heavily on text-length 
(Tweedie and Baaycn, 1998). Additionally, 
Stamatatos el al. (1999) attempted to take 
advantage ot' ah'eady existing text processing 
tools by proposing the analysis-level markers 
taking into account the methodology of the 
particular tool that has been used to analyze the 
text. This approach requires the availability era  
robust text processing tool and the time and/or 
conaputational cost for the calculation of the 
style markers is proportional to the 
corresponding cost of the analysis of the text by 
this tool. 
Last but not least, a stylometric approach 
proposed by Burrows (1987; 1992) uses as style 
markers the li'equeneies of occurrence of the 
most frequent words (typically the 50 most 
fi'equent words) as regards a training corpus. 
1"his method requires mininml computational 
cost and has achieved remarkable results for a 
wide variety of authors. Moreover, it is domain 
and language independent since it does not 
require the mauual selection of the words that 
best distinguish the categories (i.e., fnnction 
words), ltowever, in order to achieve better 
results Burrows took into account some 
additional restrictions, namely: 
? Expansion of the contracted forms. For 
exalnple, "l'na" counts as "1" and "am". 
? Separation of common homographic tbrms. 
For exalnple, the word "to" has the infinitive 
and the prepositional form. 
? Exception of proper names fi'om the list of 
the most frequent words. 
? Text-sampling so that only the narrative 
parts of the text contribute to the 
compilation of the list of the most frequent 
words. Note that a 'narrative' part is simply 
defined as 'non-dialogue'. 
From a computational point of view, these 
restrictions (except he first one) complicate the 
procedure of extracting the 1nest frequent words 
of the training corpus. Thus, the second 
restriction requires a part-of-speech tagger, the 
third has to be performed via a named-entity 
recognizer, and the last requires the development 
of a robust text sampling tool able to detect he 
narrative parts of any text. 
In this paper we present a variation of tiffs 
approach. Instead of extracting the most fi'equent 
word list of the training corpus, we use as style 
markers the fi'equencies of occurrence of the 
most fi'equent words of the entire written 
language. For English, the most frequent words 
of the written language component of the Brilisq7 
National Co,7ms are considered. We show that 
our approach performs better than the Burrows' 
original method without aking into account any 
of the above restrictions. Moreover, we show 
that the frequencies of occurrence of the most 
fi'equent punctuation marks contain very useful 
stylistic information that can enhance the 
performance ofan automatic text genre detector. 
The paper is organized as follows. The next 
section describes both the corpora nsed in tiffs 
study and the procedure of extracting the most 
li'equent word lists. Section 2 includes the text 
genre detection experiments while section 3 
contains experiments dealing with the role of 
punctuation marks. Finally, in the last section 
some conclusions are drawn and future work 
directions are given. 
1 Testing Ground 
1.1 Corpora 
As regards the English language, in the previous 
work on text genre detection (Karlgren and 
Cutting, 1994; Kessler et al, 1997) the Brown 
corpus was used as testing ground. It comprises 
approximately 500 samples divided into 15 
categories (e.g., press editorial, press reportage, 
learned, etc.) that can be considered as genres. 
However, this corpus was not built exclusively 
tbr text genre detection purposes. Therefore, the 
texts inchlded in the same category are not 
always stylistically homogeneous. Kessler el al., 
(1997) underlined this fact and attempted to 
avoid the problem by eliminating texts that did 
not fall nneqnivocally into one of their 
categories. Moreover, some of the categories of 
the Brown corpus are either too general (e.g., 
general fiction) or unlikely to be considered in 
the fi'anaework of a practical application (e.g., 
belles lettres, religion, etc.). Taking all these into 
809 
I. the 
2. of 
3. and 
4. a 
5. in 
6. to 
7. is 
8. was 
9. it 
10. for 
1 I. with 
12. he 
13. be 
14. on 
15. i 
16. that 
17. by 
18. at 
19. you 
20. 's 
21. are 
22. not 
23. his 
24. this 
25. from 
26. but 
27. had 
28. which 
29. she 
30. they 
31. or 
32. an 
33. were 
34. we 
35. their 
36. been 
37. has 
38. have 
39. will 
40. would 
41. her 
42. n't 
43. there 
44. can 
45. all 
46. as 
47. if 
48. who 
49. what 
50. said 
Table 1: The 50 most frequent words of the BNC. 
account we decided to use the Wall &reet 
Journal (WSJ) corpus as testing ground for our 
approach. The texts comprising this corpus 
cover the majority of the press genres. Although 
there is no manual categorization of the WSJ 
documents according to their genre, there are 
headlines that sometimes help in predicting the 
corresponding text genre. The selection of the 
texts included in the presented corpus was 
performed automatically by reading the headline 
tag (<HL>) of each doculnent. A typical 
headline tag ofa WSJ document is as follows: 
<HL> Market ing  & Media: 
@ RJR Nabisco Hires 
@ Adv iser  to Study 
@ Sale of ESPN Stake 
@ By Michael  J. McCarthy  
@ Staff  Reporter  of The Wal l  Street 
Journal  </HL> 
Thus, we constructed a genre-corpus of four 
categories, namely: Editorials, Letters to the 
Editor, Reportage, and Spot news taking 
documents from the WSJ corpus of the year 
1989. The documents containing the string 
"REVIEW & OUTLOOK (Ed i to r ia l )  :"in 
their headline tag were classified as editorials 
while the documents containing the string 
"Let te rs  to the  Ed i to r :  '" were 
considered as letters to the editor. The 
documents containing either the string 
"What ' s  News  -" or "Who 's  News: "  
were considered as spot news. Finally, all the 
documents containing one of the following 
strings in their headline: 
" In ternat iona l  : ", "Market ing  & 
Med ia : " ,  "Po l i t i cs  & Po l i cy : " ,  or 
"Wor ld  Markets  : " without inchlding a line 
starting with the string "@ By " were 
considered as reportage. The latter assures that 
no signed article is considered as reportage. For 
example, the document of the above example 
was not included ill any of the tour genre 
categories. 
1.2 Most  F requent  Words  
In order to extract he most fi'equent words of 
the acquired corpora we used equally-sized text 
samples (approximately 640k) from each 
category providing a genre-corpus of 2560k for 
the four categories. The genre-corpus was 
divided into 160 text samples of 16k (i.e., 
approximately 2,000 words) each, including 40 
text samples from each genre. Hall' of the text 
samples from each category were used as 
training corpus and the rest as test corpus. 
For the extraction of the most frequent words of 
the entire English language we used the British 
National Corpus (BNC). This corpus consists of 
100M tokens covering both written and spoken 
language. 
In this study we used the unlemmatized word 
frequency list of the written language 
component of the BNC I which comprises 
roughly 89.7M tokens. Since the homographic 
forms are separated, we added the frequencies of 
the words with more than one forms (e.g. "to") 
in order to attain a representative ordered list of 
the most frequent words of the entire written 
language. The resulted ordered word list of the 
50 most frequent words is given in table 1. 
The comparison of the most frequent word list 
of the genre-corpus with the one acquired by the 
BNC is given in figure 1. The common words 
(i.e., those included in both lists) constitute 
I Available at: http://www.itri.brighton.ac.uk/ 
-Adam.Kilgarriff/bnc-rcadmc.html 
810 
apt)l'oximately 75% of tile most frequent words 
of BNC. 
. . . . . . .  BNC - -  Ge,ne-corpus 
E 
E 
O 
100 
80 
60 
40 
20 
0 
1~?.? 
e l "  
. . . . .  " ~  . . . . . . . . . . . . .  i . . . . . . . . . . . . . . . . .  q . . . . . . . . . . . . . . . .  7 . . . . . . . . .  
0 20 40 60 80 100 
Most frequent woriR of BNC 
Figure 1: Comparison of the most fi'equent 
word lists. 
2 Text  Genre  l )etect ion 
In order to detect automatically tile text genre 
we used discriminant analy.vis, a well-known 
classification technique of lnultivariate statistics 
that has been used in previous work in text genre 
detection (Biber, 1993; Karlgrcn and Cutting, 
1994). This methodology takes some 
multivariate vectors precategorized into 
naturally occurring groups (i.e., training data) 
and extracts a set of discriminant./imctions that 
distinguish the groups. The mathenlatical 
objective of discHminant analysis is to weight 
and linearly combine tile discriminating 
variables (i.e., style markers) in some way so 
that the groups are tbrced to be as statistically 
distinct as possible (Eisenbeis & Avery, 1972). 
Then, discrinfinant analysis can be used lbr 
predicting tile group membership of previously 
unseen cases (i.e., test data). 
in the present case, tile multivariate vectors are 
tile fi'equencies of occurrence of tile most 
fi'equent words of the BNC lbr each text sample 
and the naturally occurring groups are the four 
text genres. We applied discriminant analysis to 
tile training genre-corpus using 5 to 75 most 
fiequent words of BNC with a step of 5 words. 
Tile classification models were, then, cross- 
validated by applying them to the corresponding 
test corpus. The same procedure was followed 
using as style markers tile fi'equencies of 
occurrence of the most fi'equent words of the 
training corpus (according to the original 
method of l~urrows). Comparative results in 
terms of classification error rate are given in 
figure 2. As can been seen, tile best perlbrnlance 
achieved by our approach is 2.5% error rate 
(2/80) based oil the 30 most frequent words of 
the BNC while the best performance of the 
Burrows' approach is 6.25% error rate (5180) 
based on the 55 most fi'equent words of the 
training corpus. 
- -  words taken from BNC 
. . . . . . .  words taken li'om training corpus 
40 
35 
30 
25 
20 
15 
10 
5 
0 
x 
0 20 40 60 80 
Most frequent ~r~R 
Figure 2: Comparative classification restllts for 
text genre detection. 
It is worih noting that tile performance of the 
classification model is not improved using more 
words beyond a certain threshold (in our 
approach 30 words). This is due to tlle training 
data overfitting. Figure 3 shows the training 
corpus in tile space of the first two discriminant 
functions based on the 10, 30, and 70 most 
frequent words of tile BNC. 
It is obvious that 10 words are not enough tbr 
the sufficient discrimination of the genre 
categories. Oil tile other hand, using tile 70 most 
frequent words the discriminant functions are 
biased to the training data. 
Furthermore, the genres Edilorial and Letters lo 
lhe editor could be grouped into a higher level 
genre since they share common stylistic features. 
Similarly, tile genres Reportage and Spol news 
could be grouped as well. Note that tile 
presented model managed to capture this 
811 
? editorial 
10 most fi'cquent words 
A letter 0 reportage + spot news 
30 nms t ti'equent words 70 most ti'equenl words 
S+ ~ 0 & AA 
6 ~ Y 
++,+> {+, = ? ++++ 
"~ I+4, + ,,, "" +' +m " 0++ 
..= + o+,++++r+ +, , .+ + . ,~  ,~+ 
-4 i o IP~ .- t, ~x 
l i "= A 
-6 I -2 
-- discriminant I (75%)  --> - discriminant 1 (68%)  --> 
3 
t,+.l 
o -  
0 E "r. 
I 
6 I 
4p 
+.+. !
I I  "+~ 
2 
i I  
I ~ 0 
-3 -2 -1 0 
--discrhnlnant 1 (71%)  --> 
Figure 3: The training corpus in the space of the two first discriminant functions 
for different word lists. The numbers inside parentheses indicate the 
percentage ofvariation explained by the corresponding function. 
information since in all the cases shown in 
figure 3 the first discriminant function (axis x), 
which accounts for the greatest part of the total 
variation, distinguishes between these high level 
genres. Then, the second discriminant function 
(axis y) attempts to discriminate each of the high 
level genres into genres of more specific level. 
3 Punctuat ion  Mark  F requenc ies  
In addition to the most fi'equent words, the 
punctuation marks play an important role for 
discriminating reliably text genres. In fact, there 
are cases where the frequency of occurrence of a 
certain punctuation mark could be used alone for 
predicting a certain text genre. For example, an 
interview is usually characterized by an 
uncommonly high frequency of question marks. 
In order to enhance the performance of the 
pmlt~mtasnifidetiowenm~mllm viQto account the 
frequencies of occurrence of the eight most 
frequent punctuation marks, namely: period, 
comma, colon, semicolon, quotes, parenthesis, 
question mark and hyphen. Thus, we applied 
discriminant analysis to the training genre- 
corpus taking into account he frequencies of 
occurrence of the above punctuation marks plus 
5 to 75 most frequent words of BNC with a step 
of 5 words. Th 
cross-validated by applying them to the 
corresponding test corpus. 
. . . . . . .  word frequencies only 
- -  word and punctuation mark fi'equencies 
35 
3O 
25 
2 20 
l ,m 
g +5 
~- 10 
5 
\ 
\ 
0 20 40 60 80 
Most frequent words 
F igure  4: Classification results taking into 
account the punctuation marks. 
The results are given in figure 4 together with 
the performance of the model using word 
frequencies only (from figure 2) for purposes of 
comparison. The error rate is now considerably 
lower and very reliable classification accuracy 
results (>97%) can be achieved based on a 
relatively small set of style markers (i.e., the 
frequencies of occurrence of the eight 
812 
punctuation marks t)lus 15 to 35 most fiequent 
words). 
. . . . . . .  word fi'cquencies only 
- -word  and pullctuation mark fi'equencies 
30 
~. 25 
20 
,- 15 
t~ 
10 
\ 
0 . . . . . . . . . .  l . . . . . . . . .  r i i 
g 10 12 14 16 18 20 22 
Training thin (in icxt samples Iler genre) 
Figure 5: l~2rror rate vs. training data size. 
The role of the punctuation marks in achieving 
reliable classification restilts can be further 
illustrated by examining the relation between 
classification accuracy and training data size. 
Towards this end, we applied discriminant 
analysis to different raining corpora consisting 
of 10 to 20 text samples from each genre taking 
into account he frequencies of occurrence of the 
30 most flequent words of the BNC. This 
procedure was followed once again taking into 
account he eight additional style markers of the 
punctuation marks (i.e., totally 38 style 
markers). The comparative results are given in 
figure 5. As can been seen, the perlbrmance of 
the model taking into account only word 
frequencies is affected dramatically by the 
decrease of the training data. On the other hand, 
the performance of the model taking into 
account both word and punctuation mark 
frequencies remains satisfactory (i.e., error rate 
< 7%) using 13 to 20 text san\]ples from each 
genre. 
Conc lus ion  
In this paper we presented a methodology for 
detecting automatically the text genre of 
unrestricted text. We followed the main idea of a 
stylometric approach originally proposed for 
attributing authorship, which uses as style 
markers the fi'equencies of occurrence of the 
most fi'equent words of a certain training corpus. 
I n  order to improve the accuracy of this model 
various additional restrictions have been 
proposed (see the introduction), which in general 
complicate the computational processing ot' the 
texts. 
Instead of taking into account such restrictions, 
we considered the fi'equencies of occurrence of 
the most fi'equent words of the entire written 
language. It has been shown that they are more 
reliable stylistic discriminators as regards the 
combination of classification accuracy and the 
number of the required common words that have 
to be taken into account. Note that when dealing 
with multivariate models, the reduction of the 
required parameters i a vein crucial factor for 
attaining reliable results and mininfizing the 
computational cost. 
As testing ground in this study we used a part of 
the WSJ corpus classified into four low-level 
genres that can be grouped into two higher-level 
genres. The automated classification model 
based on discriminant analysis applied to the 
frequencies of occurrence of the most frequent 
words of the BNC, that represent he most 
frequent words of the entire written English 
language, managed to capture the stylistic 
homogeneity in both levels. 
Moreover, it has been shown that the 
fi'equencies of occurrence of the most fi'equent 
punctuation marks can considerably enhance the 
peM'ormance of the proposed model and increase 
the reliability of the classification results 
especially when training data of limited size are 
available. 
The proposed approach lneets the current rends 
in natural anguage processing since: 
? it is able to deal with unrestricted text, 
? it requires minimal computational cost, 
? it is not based on specifc characteristics of a 
certain domain/language. 
On the other hand, any of the additional 
restrictions mentioned in the introduction, and 
especially the separation of the common 
homographic forlns, can still be considered. The 
combination of this approach with style markers 
dealing with syntactic annotation seems to be a 
better solution for a general-purpose automated 
text genre detector. Another useful direction is 
813 
the development of a text-sampling tool able to 
detect different genres within the same 
document. 
References 
Biber, D. (1993). Using Register-Diversified Corpora 
for General Language Studies. Computational 
Linguistics, 19(2), pp. 2 \[ 9-242. 
Biber, D. (1988). Variation Across Speech and 
Writing. Cambridge University Press. 
Biber, D. (I 995). Dimensions of Register Variation: 
A Cross-linguistic Coml)arison. Cambridge 
University l'ress. 
Burrows, J. (1987). Word-patterns and Story-shapes: 
The Statistical Analysis of Narrative Style. Litera W 
and Linguistic Computing, 2(2), pp. 61-70. 
Burrows, J. (1992). Not Unless You Ask Nicely: The 
Inte,'pretativc Nexus Between Analysis and 
Infornaation. Literaly and Linguistic Computing, 
7(2), pp. 91-109. 
Eisenbeis, R., and R. Avery (1972). Discriminant 
Analysis and Classification Procedures: Theo W 
and Applications. Lexington, Mass.: D.C. Health 
and Co. 
Hohncs, D. (1992). A Stylometric Analysis of 
Mormon Scripture and Related Texts. Journal of 
the Royal Statistical SocieO~ Series A, 155(1), pp. 
91-120. 
Honore, A. (1979). Some Simple Measures of 
Richness of Vocabulary. Association Jbr Literaw 
and Linguistic Computing Bulletin, 7(2), pp. 172- 
177. 
Karlgren, J., and D. Cutting (1994). Recognizing text 
Genres with Simple Metrics Using Discriminant 
Analysis. In Proc. of lhe 15 '1' International 
Conference on Computational Linguistics 
(COLING '94). 
Kessler, B., G. Nunberg, and H. Schutze (1997). 
Automatic Detection of Text Genre. In Proc. of 
35 a' Annual Meeting oJ" the Association for 
Computational Linguistics (ACL/EACL'97), pp. 
32-38. 
Sichel, H. (1975). On a Distribution Law for Word 
Frequencies. Journal oJ" the American Statistical 
Associaton, 70, pp. 542-547. 
Stamatatos, E., N. Fakotakis, and G. Kokkinakis 
(1999). Automatic Anthorship Attribution. In 
Proc. of the 9" Confi o\[" the European Chapter oJ" 
the Association Jbr Computational Linguistics 
(EACL '99), pp. 158-164. 
Twcedie, F. and Baayen, R. (1998). How Variable 
may a Constant be? Measures of Lexical Richness 
in Perspective. Computers and the Humanities, 
32(5), pp.323-352. 
814 
Automatic Extraction of Semantic Relations from Specialized Corpora 
Ar i s tomenis  Thanopou los ,  N ikos  Fakotak is  and  George  Kokk inak is  
Wire Communications Laboratory 
Electrical & Computer Engineering Dept., University of  Patras 
26500 Rion, Patras, Greece 
aristom@wcl, ee. upatras.gr 
Abstract 
In this paper we address the problem of 
discovering word semantic similarities via 
statistical processing of text corpora. We 
propose a knowledge-poor method that 
exploits the sentencial context of words for 
extracting similarity relations between them 
as well as semantic in nature word clusters. 
The approach aims at full portability across 
domains and languages and therefore is 
based on minimal resources. 
1 Motivation 
Providing digital computers with the capability to 
acquire conceptual relations between lexical items 
by processing real-life text corpora is not only an 
exciting research activity but also a significant 
task in the framework of many NLP systems. 
Specifically: 
1. State-of-the-art Language Modeling techniques 
(McMahon and Smith., 1996) require lexical 
intbrmation about word classes. 
2. Thesauri creation in a (semi-) automatic manner 
in any domain and language with minimal 
dependence on specialized tools and resources 
is very important. Most thematic domains 
today in most of the languages lack semantic 
resources. Adopting a knowledge-poor corpus- 
based method not only much less labor is 
necessary in construction of conceptual 
structures but also domain-dependent semantic 
relations are obtained. New resources can be 
readily created in new domains or existing 
thesauri can be enlarged or refined by re- 
training on larger corpora as soon as they 
become available. 
3. Many currently implemented, both spoken and 
written, NLP systems operate in a specific 
domain and usually utilize a constrained 
vocabulary related directly to their task 
domain. Therefore semantic domain-dependent 
knowledge can be acquired directly from 
relevant corpora. 
4. Autonomous computational intelligence should 
rely mainly on processing of tree flow 
electronic texts for acquiring new semantic and 
world knowledge. 
The present approach aims at corpus-based 
automatic extraction of domain-dependent 
semantic similarity relations between lexical items 
and the formation of corresponding semantic 
clusters. For this purpose, the usage of readily 
available domain-specific text corpora is 
imperative. The guideline of our approach was the 
adaptation to the special characteristics of this 
type of corpora (specialization, restricted size) 
without imposing the need for other domain- 
dependent resources and obtaining portability 
across languages. 
2 Related work 
Three main approaches have been proposed for 
the automatic extraction of lexical semantics 
knowledge: syntax-based, n-gram-based and 
window-based. Syntax-based methods (referred 
also as knowledge-rich in contrast to the others -
knowledge-poor methods) (Pereira and Thishby, 
1992; Grefenstette, 1993; Li and Abe, 1997) 
represent the words under consideration asvectors 
containing statistic values of their syntactic 
properties in relation to a given set of words (e.g. 
statistics of object syntax relations referring to a 
set of verbs) and cluster the considered words 
according to similarity of the corresponding 
vectors. Methods that use bigrams (Brown et al, 
1992) or trigrams (Martin et al, 1998) cluster 
words considering as a word's context he one or 
two immediately adjacent words and employ as 
clustering criteria the minimal loss of average 
836 
nmtual information and the perplexity 
improvement respectively. Such methods are 
oriented to language modeling and aim primarily 
at rough but fast clustering of large vocabularies. 
Brown et al (1992) also proposed a window 
method introducing the concept of "semantic 
stickiness" of two words as the relatively frequent 
close occurrence between them (less than 500 
words distance). Although this is an efficient and 
entirely knowledge-poor method tbr extracting 
both semantic relations and clusters, the extracted 
relations are not restricted to semantic similarity 
but extend on thematic roles. Moreover its 
applicability to small and specialized corpora is 
uncertain. 
3 A knowledge-poor approach 
In order to achieve portability we approach the 
issue from a knowledge-poor perspective. Syntax- 
based methods employ partial parsers which 
require highly language-dependent resources 
(morphological/grammatical analysis), and/or 
properly tagged training corpus in order to detect 
syntactic relations between sentence constituents. 
On the other hand, n-gram methods operate on 
large corpora and, in order to reduce 
computational resources, consider as context 
words only the immediately adjacent ones. 
Medium-distance word context is not exploited. 
Since large corpora are available only for few 
domains we aimed at developing a method for 
processing small or medium sized corpora 
exploiting the most of contextual information, that 
is, the tifll sentential context of words. Our 
approach was driven by the observation that in 
domain-constrained corpora, unlike fiction or 
general journalese, the vocabulary is limited, the 
syntactic structures are not complex and that 
medium-distance lexical patterns are frequently 
used to express imilar facts. 
Specifically we have developed two different 
algorithms in respect o the context consideration 
they employ: Word-based and Pattern-based. The 
former acquires word-based contextual data 
(extended up to sentence boundaries), according to 
the distributional similarity of which, word 
similarity relations are extracted. The latter detects 
common patterns throughout he corpus that 
indicate possible word similarities. For example, 
consider the sentence fragments: 
"...while the S&I' index inched up 0.3%." 
"The DAX #Mex inched up O. 70 point to close..." 
Although their syntactic structures are different, 
the common contextual pattern (appearing beyond 
immediately adjacent words) indicates a possible 
similarity between the tokens 'S&P' and 'DAX'. 
Word pairs that persistently appear such context 
similarity throughout the corpus (frequently 
observed in technical texts) are confidently 
indicated as semantically similar. Our method 
captures such context similarity and extracts a 
proportionate measure about semantic similarity 
between lexical items. 
Most approaches (Brown et al, 1992; Li & Abe, 
1997) inherently extract semantic knowledge in 
the abstracted form of semantic clusters. Our 
method produces emantic similarity relations as 
an intermediate (and information-richer) 
semantics representation formalism, from which 
cluster hierarchies can be generated. Of great 
importance is that soft clustering methods can also 
be applied to this set of relations and cluster 
polysemous words to more than one classes. 
Stock lnarket-financial news and Modem Greek, 
were used as domain and language test case 
respectively, ltowever demonstrative xamples 
taken from the WSJ corpus have been used 
throughout the paper as well. 
4 Context Similarity Estimation 
The main idea supporting context-based word 
clustering is that two words that can substitute one 
another in several different contexts always 
providing meaningful word sequences are 
probably semantically similar. Present n-gram 
based methods utilize this assumption considering 
as a context of a focus word only the one or two 
immediately adjacent parameter words. 
In the present work, we consider as word context 
the whole sentence in which the examined word 
appears, excluding only the semantically empty 
(i.e. functional) words such as articles, 
conjunctions, particles, auxiliaries. Adopting this 
word context notion we proceed to tile ibllowing 
analysis: 
Let us consider a text corpus Tc with vocabulary 
Vc and Vs _c Vc the set of words that are of interest 
in extracting semantic similarity relations between 
them. Vx comprises the non-functional words of 
837 
Vc appearing in Tc with a frequency higher than a 
threshold (set to 20 in the presented experiments) 
in order to acquire sufficient data for every focus 
word. Let Vr_~ Vc be the set of words that will be 
used as context parameters. Ideally, any word 
appearing at least twice in the corpus could be 
used as context parameter. However we specified 
this word frequency threshold to 10 in order to 
diminish computational time. Consider a sentence 
of Tc: 
Sm= WI,W2,..-,Wj-I,Wj,Wj+I,...,Wk 
We define as sentential context o fw~ in S,,, the set 
of the pairs of the sentence words which are 
members of Vp, accompanied by their 
corresponding distance from wj: 
Cs.,(wi)= {( i - j ,w~), i=l . .k,( i  ~j),  Vw~ e VI, } 
Equation (1): Sentential context of wi in S,,, 
More formally, Cs,.(wi) can be represented as a 
binary-valued matrix defined over the set 
~t =Sxo)  where 8={ -l,l,-2,2,...,-Lm,Lm}, Lm 
being the maximum word distance we regard that 
carries useful contextual information (for full 
sentence distance Lm=Lma~-I where L,,,x the 
maximum sentence length in Tc), and o~ the 
ordered set Vs: 
Cs,,, (wi) = {ci,,, (d, w)}, where: 
de&wE~ 
cj,,.(d,w)=J'l, w=wj ,w i cco, d=i - j  (2) 
0, otherwise 
Summing over all corpus sentences we obtain the 
contextual data matrices for every wj c V s : 
(w/) : {cj (d, w)} = C,,, (w j) (s) 
de6, we0) m 
The word semantic similarity estimation has been 
reduced to matrices similarity estimation. The 
obtained contextual matrices are compared using a 
weighted Tanimoto measure (Chamiak, 1993) and 
a word similarity measure S,.(wl,wj) is obtained: 
ZE\ [h (d) "  ci(d,w)" cj(d,w)l 
d w 
Sm(wi,wj) = ZEmax{c i  (d,w), cj(d,w)} (4) 
d w 
The weight function h(d) defines the desired 
influence that the distance between words should 
have to context similarity estimation. In this 
experiment we set: h(d)=l/ld I. In order to reduce 
computational time the denominator was set to 
ZEc i (d ,w)+ZZc j (d ,w) ,  a modification 
d w d w 
that has minimal effect on the final result. 
Experimental results of this method (Word-based 
Context Similarity Es t imat ion -  WCSE), are 
shown in Table 1. Note that, since the Cfc,(wj) 
matrix is sparse, (l) was used as data storing 
formula instead of (2), in order to diminish 
computational cost. 
The previously described algorithm is handling all 
contextual data in a uniform way. However, study 
of the results showed that preference should be 
given to hits derived from many different similar 
contexts instead of few ones appearing many 
tilnes. This would clearly give better esults since 
the latter case may be due to often-used 
stereotyped expressions or repeated ~3cts. In order 
to achieve this we modified (4) to: 
ZZ\ [h (d) "  log2 \[ci (d,w) ? cj (d,w)~ 
d w 
d w d w 
Indeed the experimental results of this variation 
(Variant WCSE - VWCSE) show a significant 
improvement (see Table 1 ). 
5 Dynamic pattern detection for 
context similarity estimation 
In the previously described method the notion of 
word context is based on independent intra- 
sentential word co-occurrences. Itowever 
similarity of contextual patterns is much more 
reliable word similarity criterion than word-based 
context similarity. That is, if the sentential 
contexts Cs,,,(w~) and Cs,(w/) have at least two 
common elements, we count this as a much more 
confident hit regarding the wi and w.i similarity. A 
measure expressing the weight of the common 
pattern is obtained. Since the patterns under 
detection vary across languages and domains we 
need a method that extracts them dynamically, 
regardless of the text genre, domain or language. 
For this purpose we propose an algorithm that 
performs a sentence-by-sentence comparison 
along the corpus. This comparison is based on the 
838 
cross-correlation concept as it is used in digital 
signal processing. A sentence can be considered as 
a digital signal where every semantic token 
corresponds to a signal sample. In order to detect 
words with common contexts every sentence is 
checked on matching every other one partially (i.e. 
matching the semantic ategory of one or more 
tokens) on every possible relative position 
between the two sentences. Wherever colnmon 
patterns of semantic tokens are lbund the 
neighboring respective tokens on the two 
sentences are stored as candidate semantic 
relatives. 
During this process contextual data are not 
maintained in memory; instead the detection of a 
common pattern in both sentences results to the 
storage of several hits (i.e. candidate silnilar word 
pairs) or to the increase of their corresponding 
similarity measure according to the pattern 
similarity ot'their contexts. 
Let Sm and S, be two sentences that undergo the 
cross-correlation procedure. If 8~={dx, x=l..xl, 
xl>l }, is the set of word distances that satisfy the 
equality: ci,,, , (d, ,  Wy) = c./.,, (d,-, Wy ) = 1, then the 
pair (wi,wi) is stored as a hit accompanied by the 
l'ollowing context similarity measure: 
.v 1 .v 1 
Keeping only the first term we obtain the same 
result as in the WCSE method with weight 
function h(d) =l/\]d\[. The second term augments the 
score in proportion to the cohesion and the size o1' 
the: detected pattern depending on the position of 
wi (or, equivalently, wi). I)ividing (6) by the total 
length of S,,, and S. (i.e. 1~.,,, =L,c?I.,, ) we obtain a 
normalized measure of the cross-correlation f the 
two sentences: 
1,,,,,, (lv,, w.i ) 
F ",,,,,, (u,,, ~,j) : (7) 
L 
I I I l l  
The total similarity measure is obtained from: 
(8) 
m n~n 
applied throughout the corpus. 
In order to reduce search thne and required 
memory during the whole process a pruning 
mechanism is applied at regular time intervals to 
eliminate word pairs with a relatively very low 
semantic similarity score. 
Dividing (8) by the product of the word 
probabilities P(wi)-P(wi) we obtain the normalized 
similarity measure FN(Wl,Wj). 
in order to constrain the degradation of our results 
due to sparse data regarding less frequent words, 
we multiply (8) by Pc, a data sufficiency measure 
function of P(wi) and P(wi) , obtaining Fu, a more 
reliable measure, ltere we employed: 
. r ,  J' < 
1' c(P, 1')= -1~ ' 1~. ./ (9) 
C 1 , otherwise 
where we used PTI,=30/ITcl, ITcl being the size of 
the corpus. 
Finally, sorting the resulting pairs by Fu and 
keeping the N-best scoring pairs, we obtain the 
preponderant semantically related candidates. 
6 Preprocessing 
In order to apply the above described algorithms 
some preprocessing is necessalT: 
1. A trainable sentence splitter and a rule-based 
chunker are applied. Sentence boundaries confine 
the scope of context while phrase boundaries 
determine the nmximunl extent of semantic tokens 
(see below). 
2. The next step of the preprocessing is what we 
call "xemantic lokenizalion". We try to reduce 
context parameters and simultaneously to 
incerease the volulne of contextual data either by 
reducing the volume of both the focus and 
parameter word set or by discaring or merging 
lexical items resulting in reduction of the distance 
between semantic tokens. Words or word 
sequences are thus classified in common semantic 
categories employing syntactical, morphological 
and coltocational intbrmation: 
a.Functionals (auxiliaries, determiners) are 
discarded since they do not modit) semantically 
their head words. Words of indeterminable 
semantic content (pronouns, low fi'equency 
words) are treated as empty tokens. 
b.Known domain-independent lexical patterns 
incorporating arithmetic and temporal 
839 
expressions (e.g. dates, numbers, amounts, etc.) 
are regarded as a single semantic token and 
tagged accordingly. Their information content is 
indifferent o semantic kmowledge acquisition; 
therefore we preserve only class information. 
c.Frequently appearing lexical patterns which 
represent single semantic entities in the specific 
domain are treated as a single (albeit composite) 
"semantic token". Their detection is based on 
the following algorithm (cf. Smadja, 1993): 
1. Extract "significant bigrams" confined inside 
noun phrases i.e. immediately adjacent words 
that contain a relatively high amount of 
nmtual information: 
I ......... ; (w. ,w2)= log 2 P(w 'w2) (10) 
P(w t )P(w 2 ) 
2. Combine significant bigrams together to 
obtain "significant n-grams" found in the 
corpus and confined inside noun phrases as 
well. Discard subsumed m-grams (re<n) only 
if they do not occur indepentently in the 
corpus. 
3. Tag throughout the corpus the significant n- 
grams as single semantic tokens, starting 
from the higher-order ones. 
Semantic entities that are lexically represented 
as sticky word chains may be either standard - 
in the framework of the information extraction 
task - named entities, such as "Latin America" 
(location), "Russian president Boris Yeltsin" 
(person), "Tpdne~a Mct~c6ovk%-Qpdmlg" 
("Bank of Macedonia and Thrace"; 
organization) or representations of donmin- 
specific typical events ("Gt6~qm 1 tm'coztKo6 
Keq)c0~cdon" = rise of equity capital), abstract 
concepts ("Dow Jones industrials"), etc. To 
ensure that the detected "sticky" phrases actually 
represent semantic entities, human inspection is 
necessary for discarding the spurious ones, since 
repeated word sequences that do not constitute 
always single semantic entities often appear in 
specialized texts. 
From the above it is apparent that we use the term 
"semantic token" to refer to a recognized semantic 
pattern (e.g. <date>), a rigid word chain (e.g. 
"Dow Jones industrials') or a single content 
word. The context similarity estimation algorithms 
were run using vocabularies of focus and 
parameter words derived from the extracted set of 
semantic tokens. 
7 Incorporating heuristics 
From the study of the erroneously extracted 
semantic relations certain systematic errors were 
detected. For example, adjectives, adverbs or 
adjunctive nouns that occur interpolating in 
otherwise similar contexts lead to the extraction of 
spurious pairs. Consider ~br example the phrases: 
"11 c~6~qml "nlg ztgl\]g "Cllg J~v~ixrll~" (= the increase 
of the benzine price) and "11 c~6~qcn 1 zqg "cqa\]g 
~d0~31mlg zqg \[~cv~iwl?' (=the increase of the 
di,sposal benzine price). Every algorithm based on 
word adjacency data outputs as erroneous hits the 
pairs {benzine-disposal} nd { increase-disposal}. 
A rule that was applied to deal with this problem 
is: 
If wiGS m and wjGSn have s imi lar  
contexts,  count the pair (wi,w j) as 
a hit on ly  if wi~wj+3 and wj~wi+1. 
Such contextual rules can be applied only using 
the cross-correlation method for the context 
similarity estimation (either pattern-based or 
word-based). 
8 Word Clustering 
Although the obtained semantically related N-best 
pair list constitutes already a thesaurus-like and 
information-rich form of semantic knowledge 
representation, many NLP applications (e.g. 
language modeling) require word clusters instead 
of word relations. However, since a word 
similarity measure has been extracted, the 
formation of clusters is a rather trivial problem, 
although more complex for "soft clustering" (i.e. a 
word can be classified in more than one classes). 
In order to construct word classes we applied the 
unsupervised agglomerative hard clustering 
algorithn3 shown in Figure 1 over the set of 
senmntic relations. Each distinct lexical item is 
initially assigned to a cluster and then clusters are 
merged into larger ones according to the average 
linkage measure. Merging of clusters tops when 
the distance between the more proximate clusters 
exceeds a threshold proportional to the average 
distance between words. Tracking the successive 
merges we obtain sub-cluster hierarchies, uch as 
the one shown in l<igure 2. 
840 
Repeat until mJn(AvgDis tance  (C / ,C  I ) ) > k . . . .  
for every  c lus ter  C:~ 
for  every  c lus ter  Ca@CT 
ca lcu la te  AvgDJs tance(C ; ,C \ [  ) : : -  
merge C',, Arg Inin (AvgDis tall c'e(C'.i,(\]l)) 
C,/ 
1 Z distance(w i, w j) 
Ivsl 2 v,,,,.,,,,~,, 
1 . Z distance(w ,wj) 
Figure 1 : Unsupervised Itard Clustering Algorithm 
9 Experimental Results 
The reported experinaents have been carried out on 
a 220.000 words corpus, comprised o1' financial 
news of 1998, which was constructed in the 
framework of a currently carried out R&D project 
for Information Extraction from raw text ~. 
The methods and their variations described in 
sections 4 and 5 for obtaining lexical senmntic 
relations were tested and their accuracy per 
nurnber of best hits was measured by hunmn 
inspection. The VWCSE method was tested using 
only the previous and next word as context 
parameters (N&P method), to sketch a method 
baseline lbr the particular corpus. Using a 
Morphological Analyzer & Part-of Speech tagger 
to restrict semantic relations only between words 
of the same Part-of-Speech (PoS) we obtain 
apparently higher accuracy, though we loose some 
interesting verb - noun pairs referring to the same 
action or condition, e.g. (mOiOqKc (=increased) 
and dvogo.q (=increment). The results indicate that 
the norlnalization factors indeed iinprove the 
accuracy of the methods and that context 
similarity detection based on dynamic pattern- 
matching yields significantly more reliable results 
than the word-based method. This demonstrates 
the importance of the cross-correlation algorithm, 
which is the only suitable tbr pattern-based 
context similarity detection. 
Regarding the clustering procedure, a set of 1300 
words was clustered to 84 hierarchically 
structured clusters. Considering an interested 
cluster formed (Figure 2) we note that from the 18 
lexical entities (words or rigid phrases) that 
constitute the cluster all but two refer to money 
1 Project "M1TOS" of the Greek General Sec|'etariat lbr 
Reseach and Technology 
investment o1' profit. Froln the vocabulary subject 
to clustering 4 words belonging to the same class 
were not detected; therefore accuracy and recall 
for the specific cluster were found at 88.9% and 
80% respectively. 
Although comparision with other knowledge-poor 
methods would be very useful it was not realized, 
nminly because our method produces semantic 
relations while other methods produce semantic 
clusters and our clustering process is not yet 
elaborated enough to yield quality results. 
Lexical ltem English TransL 
Kox'~o)d0)v outlays/g 
capilals 
_ w~ot6tl m l\] ui d i t ) , /~___  
cxcx'66ocl~ investments 
cn~:x'~ocn 1 investment 
*xpoTpd\[qtct'ro ~ program/g 
~kLmv &ltmo\[oo state stocks/g 
oltoLd,/(0v income bonds/g 
c.wdwo)v_yptqt\[tat\[ow time notes/g 
~q\[tt?~ losses 
K{;p& 1 profits 
KfvmO~:oclc; deposils 
~q~u(ov losses/g 
a(0Lqcmt~ purchases 
co680w incomes/g 
~c~o&t incomes 
m)vcO, Lfty?~ dealings 
*o6\[t~ctoq contract 
Figure 2: A derived sample hierarchial cluster of 
lexieal entities ('/g' denotes genitive case) 
841 
METHOD 
N&P 
WCSE 
VWCSE 
CCPM 
CCPM-N 
CCPM-N-F 
CCPM-N-F-Pc 
PoS & 
VWCSE 
CCPM 
CCPM-N 
CCPM-N-F 
CCPM-N-F-Pc 
Precision (%) per number of 
best hits 
100 1200 1300 1400 
64 61 57.7 54.75 
72 65.5 61.7 57 
81 7O 66 62.5 
74 59 54 50.5 
89 81.5 70.3 63 
90 80.5 72.7 67.25 
93 82 71.3 66.75 
86 80 75 67.5 
86 77.5 68 59.5 
93 88 79 74 
95 88.5 83 77 
97 89 82.7 77 
N&P: Context = next and previous word 
WCSE: every word into the sentence is
taken as context parameter venly - Eq.(4) 
VWCSE: contextual similarity variance 
is favored - Eq.(5) 
CCPM: Dynamic Pattern-Matching 
based on Cross-Correlation - Eq. (6)&(8) 
CCPM-N: normalized by L ..... (7)&(8) 
CCPM-N-F: normalized by P(wi)' P(wj) 
CCPM-N-F-Pc: normalized by Pc, Eq.(9) 
Table I: Comparative Results and Explanation Memo 
10 Conclusion 
Initiating from the conception of word similarity 
estimation in terms of context similarity we have 
proposed an approach with several variations for 
extracting semantic similarity relations 
betweenlexical entities by processing word 
adjacency data obtained from small or medium 
sized corpora. The described cross-correlation 
procedure, offers the possibility to dynamically 
detect pattern context similarities offering strong 
evidence for semantic similarity. The presented 
algorithm featureslanguage and domain 
portability and the ability to classify keywords 
irrespective of their grammatical characteristics. 
The implementation of the soft clustering 
algorithm, the test of the method to a different 
domain and language and the quantified 
comparison with other kamwledge-poor methods 
are quite interesting matters belonging to future 
work. 
References 
Brown P.F., DellaPietra V.J., DeSouza P.V., Lai 
J.C., Mercer R.L.: Class-Based n-gram Models 
of Natural Language. Computational 
Linguistics, 18(4): pp. 467-479, 1992. 
Chamiak E.: Statistical Language Learning. The 
M1T Press, 1993. 
Grefenstette, G.: SEXTANT." Extracting 
Semantics fromRaw Text: Implementation 
Details. The Journal of Knowledge Engineering, 
1993. 
Li H., Abe N.: Clustering Words with the MDL 
Principle. Journal of Natural Language 
Processing v.4, n.2, 1997. 
Martin S., Liermann J., Ney i1.: Algorithms for 
bigram and trigram word clustering. Speech 
Communication 24, pp. 19-37, 1998. 
McMahon J.G., Smith F.J.: Improving Statistical 
Language Model Pelformance with 
Automatically Generated Word Ilierarchies. 
Computational Linguistics, 22(2) 1996. 
Pereira F., Tishby N.: Distributional SimilariO~, 
Phrase Transitions and Itierarchical Clustering. 
In Working Notes, Fall Symposium Series. 
AAAI pp.108-112, 1992. 
Smadja F.: Retrieving Collocations from text: 
Xtract. Computational Linguistics, 19(1): pp. 
143-177, 1993. 
842 
Learning Greek Verb Complements: Addressing the Class Imbalance 
Katia Kermanidis, Manolis Maragoudakis, Nikos Fakotakis, George Kokkinakis 
Wire Communications Laboratory 
University of Patras, Rio, 26500, Greece 
{kerman,mmarag,fakotaki,gkokkin}@wcl.ee.upatras.gr
 
Abstract 
Imbalanced training sets, where one class is 
heavily underrepresented compared to the oth-
ers, have a bad effect on the classification of 
rare class instances. We apply One-sided Sam-
pling for the first time to a lexical acquisition 
task (learning verb complements from Modern 
Greek corpora) to remove redundant and mis-
leading training examples of verb non-
dependents and thereby balance our training 
set. We experiment with well-known learning 
algorithms to classify new examples. Perform-
ance improves up to 22% in recall and 15% in 
precision after balancing the dataset1. 
1 Introduction  
Among the dependents of a verb, arguments are 
key participants in the event described by the verb, 
while adjuncts comprise secondary information 
concerning the ?setting? of the event (its context, 
location etc.).  
In previous work in automatic complement-
adjunct distinction, Buchholz (1998) uses memory-
based learning on the part-of-speech tagged and 
phrase structured part of the Wall Street Journal 
with a generalization accuracy of 91.6% and she 
includes verb subcategorization information in her 
data. Merlo and Leybold (2001) use decision trees 
to distinguish prepositional arguments from prepo-
sitional modifiers. They incorporate semantic verb 
class, preposition and noun cluster information and 
reach an accuracy of 86.5% with a training set of 
3692 and a test set of 400 instances. Aldezabal et 
al. (2002) work on Basque. They apply mutual in-
formation and Fisher?s Exact Test to verb-case 
pairs (a case is any type of argument) which were 
obtained from a partially parsed newspaper corpus 
of 1.3 million words. Evaluation was performed by 
human tagging of the dependents of ten test verbs 
inside (55% f-measure) and outside (95% f-
measure) the context of the sentence. Many re-
searchers have attempted to distinguish comple-
ments from adjuncts as a prerequisite for 
identifying verb subcategorization frames: Sarkar 
                                                          
1
 This work was supported by the EU Project INSPIRE (IST-
2001-32746). 
and Zeman (2000) use a treebank and iteratively 
reduce the size of the candidate frame to filter out 
adjuncts. Briscoe and Carroll (1997) and Korhonen 
et al (2000) use a grammar and a sophisticated 
parsing tool for argument-adjunct distinction.  
In this paper we address the issue of comple-
ment-adjunct distinction in Modern Greek (MG) 
texts using well-known machine learning tech-
niques (instance based learning, Na?ve Bayes, and 
decision trees) and minimal resources. We make 
use of input that is automatically annotated only up 
to the phrase level, where the verb dependents are 
not identified. Therefore, a significant dispropor-
tion between the number of complements and non-
complements (adjuncts and non-dependents) arises 
among the candidates (complements being signifi-
cantly fewer). This disproportion causes a signifi-
cant drop in the minority (or positive) class (i.e. 
complements) prediction accuracy. Henceforth by 
adjuncts we will mean non-complements. The 
problem of class imbalance has been dealt with in 
previous work in different ways: oversampling of 
the minority class until it consists of as many ex-
amples as the majority (or negative) class (Japko-
wicz 2000), undersampling of the majority class 
(either random or focused), their combination 
(Ling and Li 1998), the implementation of cost-
sensitive classifiers (Domingos 1999), and the 
ROC convex hull method (Provost and Fawcett 
2001).  
In general, undersampling the majority class 
leads to better classifier performances than 
oversampling the minority class (Chawla et al 
2002). Therefore, we apply One-sided Sampling 
and Tomek links (Tomek 1976) to our training data 
to obtain a more balanced subset of the initial 
training set by pruning out noisy and redundant 
instances of the majority class. This approach has 
been used in the past in several domains such as 
image processing (Kubat and Matwin 1997), 
medicine (Laurikkala 2001), text categorization 
(Lewis and Gale 1994), and we apply it here for 
the first time to lexical acquisition. 
A novel variation in detecting Tomek links in 
this work is the metric used for calculating the dis-
tance between instance vectors. Features in our 
task take exclusively nominal values. We therefore 
experiment with the value difference metric (Stan-
fill and Waltz 1986) besides the broadly used 
Euclidean distance. The former is more suitable for 
this type of features, a claim supported by Stanfill 
and Waltz and also by our experimental results. 
2 Modern Greek  
Concerning morphology, MG is highly inflec-
tional. The part-of-speech (pos), the grammatical 
case, and the verb voice are key morphological 
features for complement detection. 
Concerning sentence structure, MG is a ?semi-
free? word-order language. The arguments of a 
verb do not have fixed positions with respect to the 
verb and are therefore determined primarily by 
their morphology rather than their position. 
Certain semantic verb attributes are also very 
significant: the verb?s copularity, its mode, and 
whether it is (im)personal. A verb is copular when 
it assigns a quality to its subject. Mode is the prop-
erty that determines the semantic relation between 
the verb and its subject (whether the latter affects 
or is affected by the verb action. Although all of 
these features are normally context-dependent, 
there are verbs with apriori known values for them. 
This apriori information is taken into account in 
our final dataset, as context-dependent semantic 
information could not be provided automatically, 
and we tried to keep manual intervention to a 
minimum. 
In MG, verbs can take zero, one or two com-
plements. A complement may be a noun phrase in 
the accusative or the genitive case, a prepositional 
phrase or a secondary clause (Klairis and Babini-
otis 1999). Often the complements appear within 
the verb phrase itself in the form of weak personal 
pronouns. Copular verbs only can take as an argu-
ment a noun or adjective in the nominative (predi-
cative). Each of the above features is important but 
not definitive on its own for complement detection. 
When combined, however, and including context 
information of the candidate complement, many 
cases of ambiguity are correctly resolved. The big-
gest sources of ambiguity are the accusative noun 
phrase, which is very often adverbial denoting usu-
ally time, and the prepositional phrase introduced 
by ?? (to), also often adverbial, denoting usually 
place. 
3 Data Collection  
The corpora used in our experiments were:  
1. The ILSP/ELEFTHEROTYPIA (Hatzigeor-
giu et al 2000) and ESPRIT 860 (Partners of 
ESPRIT-291/860 1986) Corpora (a total of 
300,000 words). Both these corpora are balanced 
and manually annotated with complete morpho-
logical information. The former also provides ad-
verb type information (temporal, of manner etc.). 
Further (phrase structure) information is obtained 
automatically. 
2. The DELOS Corpus (Kermanidis et al 2002) 
is a collection of economic domain texts of ap-
proximately five million words and of varying 
genre. It has been automatically annotated from the 
ground up. Morphological tagging on DELOS was 
performed by the analyzer of Sgarbas et al (2000). 
Accuracy in pos tagging reaches 98%. Case and 
voice tagging reach 94% and 84% accuracy re-
spectively. Further (phrase structure) information 
is again obtained automatically. DELOS also con-
tains subject-verb-object information limited to 
nominal and prepositional objects and detected 
automatically by a shallow parser that reaches 70% 
precision and recall. 
All the corpora have been phrase-analyzed by 
the chunker described in detail in Stamatatos et al 
(2000). Noun (NP), verb (VP), prepositional (PP), 
adverbial phrases (ADP) and conjunctions (CON) 
are detected via multi-pass parsing. Precision and 
recall reach 94.5% and 89.5% respectively. 
Phrases are non-overlapping. Concerning phrase 
structure, complements (except for weak personal 
pronouns) are not included in the verb phrase, 
nominal modifiers in the genitive case are included 
within the noun phrase they modify, coordinated 
simple noun and adverbial phrases are grouped 
into one phrase. 
The next step is empirical headword identifica-
tion. NP headwords are determined based on the 
pos and case of the phrase constituents. For VPs, 
the headword is the main verb or the conjunction if 
they are introduced by one. For PPs it is the prepo-
sition introducing them. 
3.1  Data Formation  
To take into account the freedom of the language 
structure, context information of every verb in the 
corpus focuses on the two phrases preceding and 
the three phrases following it. Only one out of 200 
complements in the corpus appears outside this 
window. Each of these phrases is in turn the focus 
phrase (the candidate complement or adjunct) and 
an instance of twenty nine features (28 features 
plus the class label) is formed for every focus 
phrase (fp). So a maximum of five instances per 
verb occurrence are formed. Forming of these in-
stances from a corpus sentence is shown in Figure 
1. 
The first five features are the verb lemma 
(VERB), its mode (F1), whether it is (im)personal 
(F2), its copularity (F3), and its voice (F4). Two 
features encode the presence of a personal pronoun 
in the accusative (F5) or genitive (F6) within the 
VP. For every fp (fps are in bold), apart from the 
seven features described above, a context window 
of three phrases preceding the fp and three phrases 
following it is taken into account. Each of these six 
phrases (as well as the fp itself) is encoded into a 
set of three features (a total of twenty one fea-
tures). These triples appear next in each instance, 
from the leftmost (-3) to the rightmost phrase (+3). 
For each feature triple, the first feature is the type 
of the phrase. The second is the pos of the head-
word for NPs and ADPs. The third feature for NPs 
is the case of the headword. For ADPs it is the type 
of the adverb, if available. If VPs are introduced by 
a conjunction, the second feature is its type (coor-
dinating/subordinating) and the third is the con-
junction itself. Otherwise the second feature is the 
verb?s pos and the third empty. For PPs, the second 
feature is empty and the third is the preposition.  
 
VP[*?????] NP1[???? *?????] NP2[? *???????] CON[???] VP[*????????] PP[*??? ???.] 
 
(VP[Is] NP1[good boy] NP2[the Labros] CON[and] VP[believes] PP[in God.]) (Labros is a good boy and believes in God.) 
 
 
VERB   F1 F2 F3 F4 F5 F6 FP   -3    -2    -1   +1      +2      +3    LABEL 
?????,   O, P, C,  P, F, F, NP,N,n,  -,-,-,  -,-,-,  VP,V,-,  NP,N,n,  VP,V,-,  PP,-,??,  C 
?????,   O, P, C,  P, F, F, NP,N,n,  -,-,-,  VP,V,-, NP,N,n,  VP,V,-,  PP,-,??,  -,-,-,   A 
???????, E, P, NC, A, F, F, NP,N,n,  -,-,-,  -,-,-,  VP,V,-,  NP,N,n,  VP,V,-,  PP,-,??,  A 
???????, E, P, NC, A, F, F, NP,N,n,  -,-,-,  VP,V,-, NP,N,n,  VP,V,-,  PP,-,??,  -,-,-,   A 
???????, E, P, NC, A, F, F, PP,-,??,  NP,N,n, NP,N,n, VP,V,-,  -,-,-,   -,-,-,   -,-,-,   C 
Figure 1: A sentence is transformed into the 5 labeled instances shown. Words starting with the asterisk (*) are head-
words. 
 
The first instance is for the verb ????? and the 
candidate complement/adjunct is the fp NP1. In the 
second instance, for the same verb, the candidate 
complement/adjunct is the fp NP2. There are only 
two instances for this verb because 1. there are no 
phrases preceding it, and 2. the third phrase follow-
ing it (consisting only of the coordinating conjunc-
tion) has not much to contribute and is disregarded 
altogether forcing us to consider the next phrase in 
the sentence. As the next phrase is a verb phrase 
that is not introduced by a subordinating conjunc-
tion (and therefore cannot be a dependent of the 
verb ?????), it is also disregarded and no further 
phrases are tested. In the same way, for the verb 
??????? we have an instance with fp the NP1, an 
instance with fp the NP2 and one with PP as the fp. 
We experimented with various window sizes re-
garding the context of the fp, i.e. [fp], [-1, fp], [-2, 
fp], [-2, +1], [-3, +3].  
The formatting described in the previous section 
was applied to the ILSP and ESPRIT corpora and 
to part (approximately 500,000 words) of the 
DELOS corpus. For the first two corpora, the class 
of each fp for every created instance was hand-
labeled by two linguists by looking up the verb in 
its context, based on the detailed descriptions for 
complements and adjuncts by Klairis and Babini-
otis (1999). For DELOS, which already contained 
automatically detected verb-object information to 
an extent, existing erroneous complement informa-
tion was manually corrected, while clausal com-
plements were manually detected. The dataset 
consisted of 63,000 instances. The imbalance ratio 
is 1:6.3 (one complement instance for every 6.3 
adjunct instances). 
4 Addressing the Imbalance  
From the ratio given above, the complement class 
is underrepresented compared to the adjunct class 
in the data. As the number of examples of the ma-
jority class increases, the more likely it becomes 
for the nearest neighbor of a complement to be an 
adjunct. Therefore, complements are prone to mis-
classifications. We address this problem with One-
sided Sampling, i.e. pruning out redundant adjunct 
(negative) examples while keeping all the com-
plement (positive) examples. Instances of the ma-
jority class can be categorized into four groups 
(Figure 2): Noisy are instances that appear within a 
cluster of examples of the opposite class, border-
line are instances close to the boundary region be-
tween two classes, redundant are instances that can 
be already described by other examples of the 
same class and safe are instances crucial for deter-
mining the class. Instances belonging to one of the 
three first groups need to be eliminated as they do 
not contribute to class prediction. 
Noisy and borderline examples can be detected 
using Tomek links: Two examples, x and y, of op-
posite classes have a distance of ?(x,y). This pair of 
instances constitutes a Tomek link if no other ex-
ample exists at a smaller distance to x or y than 
?(x,y). 
Redundant instances may be removed by creat-
ing a consistent subset of the initial training set. A 
subset C of training set T is consistent with T, if, 
when using the nearest neighbor (1-NN) algorithm, 
it correctly classifies all the instances in T. To this 
end we start with a subset C consisting of all com-
plement examples and one adjunct example. We 
train a learner with C and try to classify the rest of 
the instances of the initial training set. All misclas-
sified instances are added to C, which is the final 
reduced dataset.  
The exact process of the proposed algorithm is: 
 
1. Let T be the original training set, where the 
size of the negative examples outnumbers that 
of the positive examples. 
2. Construct a dataset C, containing all positive 
instances plus one randomly selected negative 
instance. 
3. Classify T with 1-NN using the training ex-
amples of C and move all misclassified items to 
C. C is consistent with T, only smaller. 
4. Remove all negative examples participating 
in Tomek links. The resulting set Topt is used 
for classification instead of T.4.1 Distance func-
tions  
4.1 Distance functions 
The distance functions used to determine the in-
stances participating in Tomek links are described 
in this section. 
 The most commonly used distance function is 
the Euclidean distance. One drawback of the 
Euclidean distance is that it is not very flexible 
regarding nominal attributes. The value difference 
metric (VDM) is more appropriate for this type of 
attributes, as it considers two nominal values to be 
closer if they have more similar classifications, i.e. 
more similar correlations with the output class. The 
VDM of two values ax and ay of a nominal attribute 
A in two vectors x and y is estimated as: 
, ,, ,
, ,
( , ) yx
x y
A a cA a c
A x y
c C A a A a
NN
vdm a a
N N?
= ??  
,A aN is the number of times value a of attribute A 
was found in the training set,
, ,A a cN is the number 
of times value a co-occurred with output class c 
and C is the set of class labels. 
4.2 The reduced dataset  
We used the above distance metrics to detect ex-
amples that are safe to remove, and then applied 
the methodology of the previous section to our 
data. Figure 3 depicts the reduction in the number 
of negative instances for both metrics and every fp 
context window. The more phrases are considered 
(the higher the vector dimension), the noisier the 
instances, and the more redundant examples are 
removed. For small windows, the positive effect of 
VDM is clear (more redundant examples are de-
tected and removed). As the window size in-
creases, the Euclidean distance becomes smoother 
(depending on more features) and leads to the re-
moval of as many examples as VDM. 
 
 
 
Figure 2: The four groups of negative instances. 
 
0,00%
2,00%
4,00%
6,00%
8,00%
10,00%
12,00%
14,00%
[0] [-1,0] [-2,0] [-2,1] [-3,3]
Euclidean
VDM
 
Figure 3: Reduction (%) in the number of negative in-
stances after applying One-sided Sampling. 
 
It is interesting to observe the type of instances 
which are removed from the initial dataset after 
balancing. Redundant instances are usually those 
with as fp headword a punctuation mark, a symbol 
etc. Such fps could never constitute a complement 
and appear in the dataset due to errors in the auto-
matic nature of pre-processing. Borderline in-
stances are usually formed by fps that have a 
syntactically ambiguous headword like a noun in 
the accusative case, an adjective in the nominative 
case if the verb is copular, certain prepositional 
phrases. The following negative instance of the 
initial dataset (with window [fp]) shows the differ-
ence between the two distances. 
 
???????????,  E,  P,  NC,  A,  F,  F,  PP,-,??,  ? 
 
This instance appears only as negative through-
out the whole dataset. If the verb ??????????? (to 
replace) were omitted, the remaining instance ap-
pears several times in the data as positive with a 
variety of other verbs. The Euclidean distance be-
tween these instances is small, while the VDM is 
greater, because the verb is a feature with a high 
correlation to the output class. So the above in-
stance is removed with the Euclidean distance as 
being borderline, while it remains untouched with 
VDM. 
5 Classifying new instances  
For classification we experimented with a set of 
algorithms that have been broadly used in several 
domains and their performance is well-known: in-
stance-based learning (IB1), decision trees (an im-
plementation of C4.5 with reduced error pruning) 
and Na?ve Bayes were used to classify new, unseen 
instances as complements or adjuncts. Unlike pre-
vious approaches that test their methodology on 
only a few new verb examples, we performed 10-
fold cross validation on all our data: the dataset 
(whether initial or reduced) was divided into ten 
sets of equal size, making sure that the proportion 
of the examples of the two classes remained the 
same. For guiding the C4.5 pruning process, one of 
the ten subsets was used as the held-out validation 
set. 
6 Experimental results  
Unlike previous approaches that evaluate their 
methodology using the accuracy metric, we evalu-
ated classification using precision and recall met-
rics for every class. a and d are the correctly 
identified adjuncts and complements respectively, 
b are the adjuncts which have been misclassified as 
complements and c are the misclassified comple-
ments.  
A
apr =
a+c
, A
a
re =
a+b
, C
dpr =
b+d
, C
d
re =
c+d
 
The f-measure for each class combines the pre-
vious two metrics into one: 
2 precision recallf-measure=
precision+recall
? ?
 
Table 1 shows the results for each classification 
algorithm and various window sizes using the ini-
tial dataset before any attempt is made to reduce its 
size. The drop in performance of the minority class 
compared to the majority class is obvious. The 
scores corresponding to the best f-measure for the 
complement class are indicated in bold. 
By explicitly storing and taking into account 
every training example, IB1 presents a drop in per-
formance as the window size increases due to 
sparse data. The performance of C4.5 remains rela-
tively stable, regardless of the size of the instance 
vector. Na?ve Bayes leads to a significant number 
of adjunct instances being labeled as complements. 
This is attributed to the fact that the Na?ve Bayes 
learner does not take into account conditional de-
pendencies among features. Given that an instance 
is a complement, for example, if the fp is an adjec-
tive in the nominative case, there is a very high 
probability in reality that the verb is copular. This 
dependence is not captured by the Na?ve Bayes 
learner.  
 
  
[0] [-1,0] [-2,0] [-2,1] [-3,3] 
PrA 91.3 92.5 92.4 92.6 92.9 
ReA 86.4 83.2 82.1 83.1 82.6 
PrC 45.5 43.4 41.8 43.4 43.1 
Na?ve 
Bayes 
ReA 57.8 65.6 65.7 66.1 67.8 
PrA 91.5 91.4 91.3 91.3 91.5 
ReA 94.9 95.1 95.2 95.1 95.2 
PrC 68.0 68.5 68.7 68.2 68.9 
C4.5 
ReC 54.9 54.4 53.9 53.7 54.7 
PrA 91.7 92.2 91.6 90.0 87.7 
ReA 93.7 93.8 92.8 91.6 90.0 
PrC 63.8 65.4 60.6 52.8 40.5 
IB1 
ReC 56.9 59.8 56.5 47.9 35.1 
Table 1: Results for each algorithm and various fp con-
text window sizes using the initial dataset. 
 
Tables 2 and 3 show the classification results af-
ter balancing the dataset using the Euclidean dis-
tance and VDM respectively. The increase in f-
measure after reducing the dataset is very interest-
ing to observe and depends on the size of the fp 
context window. 
When taking into account the fp only, the high-
est increase is over 8% in complement class f-
measure with the Euclidean distance.  
When regarding the context surrounding the fp, 
the positive impact of balancing the dataset is even 
stronger. As the fp window size increases, Na?ve 
Bayes performs better, reaching an f-measure of 
over 60% with [-3,+3] (as opposed to 53.4% prior 
to balancing). Recall with C4.5 increases by 14% 
in context [-3,+3] after balancing. Instance-based 
learning, as mentioned earlier is not helped by a lot 
of context information and reaches its highest 
score when considering only one phrase preceding 
the fp. The increase in complement class precision 
with IB1 exceeds 12% with VDM. This is the ex-
periment which achieved the highest f-measure 
(73.7%). Regarding larger context windows and 
IB1, the removal of the noisy and redundant exam-
ples seems to compensate for the noise introduced 
by the increased number of features in the vector. 
Increase in recall reaches 22%. As a general re-
mark, instance-based learning performs best when 
the context surrounding the candidate complement 
is very restricted (at most one phrase preceding the 
fp), while Bayesian learning improves its perform-
ance as the window increases. 
In most of the experiments VDM leads to better 
results than the Euclidean distance because it is 
more appropriate for nominal features, especially 
when the instance vector is small. When larger 
windows are considered, the two metrics have the 
same effect. Minor occasional differences (~0.1%) 
mirrored in the results are attributed to the 10-fold 
experimentation. 
 
 
  
[0] [-1,0] [-2,0] [-2,1] [-3,3] 
PrA 91.1 92.4 92.8 93.0 93.0 
ReA 87.4 83.2 82.6 84.6 85.1 
PrC 49.0 45.7 46.7 50.3 51.8 
Na?ve 
Bayes 
ReA 58.4 67.4 70.5 70.9 71.3 
PrA 92.3 92.0 91.7 93.2 92.9 
ReA 95.1 95.2 95.6 94.6 94.9 
PrC 72.4 72.4 74.8 73.6 73.3 
C4.5 
ReC 61.7 60.4 60.1 68.5 68.8 
PrA 93.0 93.8 93.1 92.1 90.2 
ReA 94.7 95.5 94.6 93.0 90.5 
PrC 71.7 76.5 73.0 66.7 55.3 
IB1 
ReC 65.4 69.7 67.5 68.6 56.7 
Table 2: Results for the reduced dataset and the Euclid-
ean distance. 
 
 
  [0] [-1,0] [-2,0] [-2,1] [-3,3] 
PrA 91.0 92.5 92.8 93.0 93.2 
ReA 87.3 83.1 82.6 84.6 85.4 
PrC 49.0 46.5 46.7 50.3 51.6 
Na?ve 
Bayes 
ReA 58.6 68.6 70.5 70.9 71.3 
PrA 92.0 92.6 91.7 93.2 93.0 
ReA 95.0 95.2 95.6 94.6 94.8 
PrC 71.5 74.3 74.8 73.6 73.2 
C4.5 
ReC 60.1 64.6 60.1 68.5 68.9 
PrA 92.7 93.8 93.1 93.6 90.2 
ReA 94.4 95.6 94.6 93.0 90.5 
PrC 70.4 77.5 73.0 66.7 55.3 
IB1 
ReC 64.5 70.3 67.5 68.6 56.7 
Table 3: Results for the reduced set and VDM. 
 
Apart from the positive impact of One-sided 
Sampling on predicting positive examples, the ta-
bles show its positive (or at least non-negative) 
impact on predicting negative instances. Non-
complement accuracy either increases or remains 
the same after balancing. 
Concerning the resolution of the ambiguities 
discussed in section 2, three classified examples of 
the verb ???? (to exercise) with context environ-
ment [-1,fp] follow. The first class label is the true 
and the second is the predicted class. Example (a) 
has been classified correctly with and without One-
sided Sampling. Examples (b) and (c) are the same 
instance classified without (b) and with (c) One-
sided Sampling. Example (b) is erroneously tagged 
as an adjunct due to class imbalance. The phrase 
preceding the fp helps resolve the ambiguity in (a) 
and (c): usually a punctuation mark before the fp 
(indicated by the triple NP,F,-)  separates syntacti-
cally the fp from the verb and the fp is unlikely to 
be a complement. 
 
a. ????, E, P, NC, A, F, F, PP,-,??, NP,F,-,  A A 
b. ????, E, P, NC, A, F, F, PP,-,??, NP,N,a, C A 
c. ????, E, P, NC, A, F, F, PP,-,??, NP,N,a, C C 
7 Conclusion  
In this paper we describe the positive effect of 
One-sided Sampling of an imbalanced dataset for 
the first time on the linguistic task of automatically 
learning verb complements from Greek text cor-
pora. Unlike traditional One-sided Sampling, we 
employ the VDM metric and show that it is more 
appropriate for nominal features. We experiment 
with various learning algorithms to classify new 
examples and reach a precision and a recall value 
of 77.5% and 70.3% respectively, having used only 
a chunker for preprocessing. 
References  
I. Aldezabal, M. Aranzabe, A. Atutxa, K. Gojenola and 
K. Sarasola. 2002. Learning argument/adjunct dis-
tinction for Basque. SIGLEX Workshop of the ACL, 
pages 42-50. Philadelphia. 
T. Briscoe and J. Carroll. 1997. Automatic extraction of 
subcategorization from corpora. Proceedings of 
ANLP 1997, pages 356-363. Washington D.C. 
S. Buchholz. 1998. Distinguishing complements from 
adjuncts using memory-based learning. Proceedings 
of the Workshop on Automated Acquisition of Syntax 
and Parsing, ESSLLI-98, pages 41-48. Saarbruecken, 
Germany. 
N. Chawla, K. Bowyer, L. Hall and W.P. Kegelmeyer. 
2002. SMOTE: Synthetic minority over-sampling 
technique. Journal of Artificial Intelligence Research 
16:321-357. Morgan Kaufmann. 
P. Domingos. 1999. Metacost: A general method for 
making classifiers cost-sensitive. Proceedings of the 
International Conference on Knowledge Discovery 
and Data Mining, pages 155-164. San Diego, CA. 
N. Hatzigeorgiu et al 2000. Design and Implementation 
of the online ILSP Greek Corpus. Proceedings of 
LREC 2000, pages 1737-1742. Greece. 
N. Japkowicz. 2000.  The class imbalance problem: 
significance and strategies. Proceedings of the Inter-
national Conference on Artificial Intelligence. Las 
Vegas, Nevada. 
K. Kermanidis, N. Fakotakis and G. Kokkinakis. 2002. 
DELOS: An automatically tagged economic corpus 
for Modern Greek. Proceedings of LREC 2002, pages 
93-100. Las Palmas de Gran Canaria. 
C. Klairis and G. Babiniotis. 1999. Grammar of Modern 
Greek. II. The Verb. (in Greek). Athens: Greek Let-
ters Publications. 
A. Korhonen, G. Gorrell and D. McCarthy. 2000. Statis-
tical filtering and subcategorization frame acquisi-
tion. Proceedings of the Joint SIGDAT EMNLP 
Conference, pages 199-205. Hong Kong. 
M. Kubat and S. Matwin. 1997. Addressing the curse of 
imbalanced training sets. Proceedings of ICML 97, 
pages 179- 186. 
J. Laurikkala. 2001. Improving identification of difficult 
small classes by balancing class distribution. Pro-
ceedings of the Conference on Artificial Intelligence 
in Medicine in Europe, pages 63-66. Portugal. 
D. Lewis and W. Gale. 1994. Training text classifiers by 
uncertainty sampling. Proceedings of the Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 3-12. 
Dublin. 
C. Ling and C. Li. 1998. Data mining for direct market-
ing problems and solutions. Proceedings of KDD 98 
Conference. New York, NY. 
P. Merlo and M. Leybold. 2001. Automatic distinction 
of arguments and modifiers: the case of prepositional 
phrases. Proceedings of the Workshop on Computa-
tional Language Learning, Toulouse, France.  
Partners of ESPRIT-291/860. 1986. Unification of the 
word classes of the ESPRIT Project 860. Internal Re-
port BU-WKL-0376. 
F. Provost and T. Fawcett. 2001. Robust classification 
for imprecise environments. Machine Learning 
42(3): 203-231. 
A. Sarkar and D. Zeman. 2000. Automatic extraction of 
subcategorization frames for Czech. Proceedings of 
COLING 2000, pages 691-697. Saarbruecken, Ger-
many. 
K. Sgarbas, N. Fakotakis and G. Kokkinakis. 2000. A 
straightforward approach to morphological analysis 
and synthesis. Proceedings of COMLEX 2000, pages 
31-34. Kato Achaia, Greece. 
E. Stamatatos, N. Fakotakis and G. Kokkinakis. 2000. A 
practical chunker for unrestricted text. Proceedings 
of NLP 2000, pages 139-150. Patras, Greece. 
C. Stanfill and D. Waltz. 1986. Toward memory-based 
reasoning. Communications of the ACM 29:1213-
1228.  
I. Tomek. 1976. Two modifications of CNN. IEEE 
Transactions on Systems, Man and Communications, 
SMC-6:769-772. 
 
Automatic Text Categorization 
in Terms of Genre and Author 
Efs ta th ios  S tamatatos*  
University of Patras 
George  Kokk inak is  * 
University of Patras 
N ikos  Fakotak is  t
University of Patras 
The two main factors that characterize a text are its content and its style, and both can be used 
as a means of categorization. In this paper we present an approach to text categorization i
terms of genre and author for Modern Greek. In contrast o previous tylometric approaches, 
we attempt to take full advantage of existing natural language processing (NLP) tools. To this 
end, we propose aset of style markers including analysis-level measures that represent the way in 
which the input text has been analyzed and capture useful stylistic information without additional 
cost. We present aset of small-scale but reasonable experiments in text genre detection, author 
identification, and author verification tasks and show that the proposed method performs better 
than the most popular distributional lexical measures, i.e.,functions of vocabulary richness and 
frequencies of occurrence of the most frequent words. All the presented experiments are based on 
unrestricted text downloaded from the World Wide Web without any manual text preprocessing 
or text sampling. Various performance issues regarding the training set size and the significance of 
the proposed style markers are discussed. Our system can be used in any application that requires 
fast and easily adaptable text categorization i terms of stylistically homogeneous categories. 
Moreover, the procedure of defining analysis-level markers can be followed in order to extract 
useful stylistic information using existing text processing tools. 
1. Introduction 
The rapid expansion of the World Wide Web (WWW) in recent years has resulted 
in the creation of large volumes of text in electronic form. NLP applications uch as 
information retrieval and information extraction have been developed to treat this 
information automatically. Since the Internet is a very heterogeneous domain, these 
applications usually involve text categorization tasks with the following desiderata: 
? minimal computational cost, 
? ability to handle real-world (or unrestricted) text, and 
? either ease of adaptation to a certain domain or application or generality 
in order to cover a wide range of domains or applications. 
* University of Patras, Department ofElectrical & Computer Engineering, 26500 Patras, Greece. E-mail: 
stamatatos@wcl.ee.upatras.gr. 
t University of Patras, Department ofElectrical & Computer Engineering, 26500 Patras, Greece. E-mail: 
fakotaki@wcl.ee.upatras.gr. 
University of Patras, Department ofElectrical & Computer Engineering, 26500 Patras, Greece. E-mail: 
gkokkin@wcl.ee.upatras.gr. 
? 2001 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 4 
The two main factors that characterize a text are its content and its style, both of 
which can be used for categorization purposes. Nevertheless, the literature on compu- 
tational stylistics is very limited in comparison to the work dealing with the proposi- 
tional content of the text. This is due to the lack of a formal definition of style as well 
as to the inability of current NLP systems to incorporate stylistic theories that require 
complicated information. In contrast o traditional stylistics based on formal linguistic 
theories, the use of statistical methods in style processing has proved to be a reliable 
approach (Biber 1995). According to the stylostatisticians, a given style is defined as a 
set of measurable patterns, called style markers. We adopt this definition in this study. 
Typical classificatory tasks in computational stylistics are the following: 
? Text genre detection concerns the identification of the kind (or functional 
style) of the text (Karlgren and Cutting 1994; Michos et al 1996; Kessler, 
Nunberg, and Schi.itze 1997). 
? Authorship attribution concerns the identification of the author of the text 
(Holmes and Forsyth 1995; Baayen, Van Halteren, and Tweedie 1996; 
Tweedie, Singh, and Holmes 1996). 
These tasks have so far been considered completely separate problems. A typical text 
categorization system utilizing stylistic analysis (i.e., either text genre or authorship 
identification) is usually based on the following modules: 
. 
2. 
Extraction of style markers: A set of quantifiable measures are defined 
and a text-processing tool is usually developed, to automatically count 
them. 
Classification procedure: A disambiguation method (e.g., statistical, 
connectionist, etc.) is applied to classify the text in question into a 
predefined category (i.e., a text genre or an author). 
The most important computational pproaches to text genre detection have fo- 
cused on the use of simple measures that can be easily detected and reliably counted 
by a computational tool (Kessler, Nunberg, and Sch~itze 1997). To this end, various sets 
of style markers have been proposed (Karlgren and Cutting 1994), all of which are, in 
essence, subsets of the set used by Biber (1995), who ranked registers along seven di- 
mensions by applying factor analysis to a set of lexical and syntactic style markers that 
had been manually counted. In general, the current text genre detection approaches 
try to avoid using existing text processing tools rather than taking advantage of them. 
Authorship attribution studies have focused on the establishment of the authorship 
of anonymous or doubtful iterary texts, such as the Federalist Papers, 12 of which 
are of disputed authorship (Mosteller and Wallace 1984; Holmes and Forsyth 1995). 
Typical methodologies deal with a limited number of candidate authors using long 
text samples of several thousand words. Almost all the approaches to this task are 
based mainly on distributional lexical style markers. In a review paper of authorship 
attribution studies, Holmes (1994) claims: "yet, to date, no stylometrist has managed 
to establish a methodology which is better able to capture the style of a text than that 
based on lexical items" (p. 87). 
To the best of our knowledge, there is still no computational system that can 
distinguish the texts of a randomly chosen group of authors without requiring human 
assistance in the selection of both the most appropriate set of style markers and the 
most accurate disambiguation procedure. 
472 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
In this paper we describe an approach to text categorization i terms of genre and 
author based on a new stylometric method that utilizes already existing NLP tools. In 
addition to the style markers relevant o the actual output of the NLP tool (i.e., the 
analyzed text), we introduce analysis-level style markers, which represent the way in 
which the text has been analyzed by that tool. Such measures contain useful stylistic 
information and are easily available without additional computational cost. 
To illustrate, we apply the proposed technique to text categorization tasks for Mod- 
ern Greek corpora using an already existing sentence and chunk boundaries detector 
(SCBD) in unrestricted Modern Greek text (Stamatatos, Fakotakis, and Kokkinakis 
2000). We present a set of small-scale but reasonable xperiments in text genre detec- 
tion, author identification, and author verification tasks and show that the performance 
of the proposed method is better in comparison with the most popular distributional 
lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence 
of the most frequent words. Our approach is trainable and can be easily adapted to 
any set of stylistically homogeneous categories. 
We begin by discussing work relevant o text genre detection and authorship attri- 
bution focusing on the various types of style markers employed (Section 2). Next, we 
describe the proposed solution for extracting style markers using already existing NLP 
tools (Section 3) and apply our method to Modern Greek (Section 4), briefly describing 
the SCBD and proposing our set of style markers. The techniques used for automatic 
categorization of the stylistic vectors are discussed in Section 5. Section 6 deals with 
the application of our approach to text genre detection, and Section 7, with authorship 
attribution, for both author identification and author verification. In Sections 8 and 
9, we discuss important performance issues of the proposed methodology and the 
conclusions that can be drawn from this study. 
2. Current Trends in Stylometry 
The main feature that characterizes both text genre detection and authorship attri- 
bution studies is the selection of the most appropriate measures, namely, those that 
reflect the style of the writing. Various sets have been proposed in the literature. In 
this section, we classify the most popular of the proposed style markers, taking into 
account he information required for their calculation rather than the task they have 
been applied to. 
2.1 Token-Level Measures 
The simplest approach considers the sample text as a set of tokens grouped in sen- 
tences. Typical measures of this category are word count, sentence count, character per 
word count, and punctuation marks count. Such features have been widely used in 
both text genre detection and authorship attribution research since they can be easily 
detected and computed. It is worth noting that the first pioneering works in author- 
ship attribution, when no powerful computational systems were available, were based 
exclusively on these measures. For example, Morton (1965) used sentence length for 
testing the authorship of Greek prose, Brinegar (1963) adopted word length measures, 
and Brainerd's (1974) approach was based on distribution of syllables per word. Al- 
though such measures seemed to work in specific cases, they became subject o heavy 
criticism for their lack of generality (Smith 1983, 1985). 
2.2 Syntactic Annotation 
The use of measures related to syntactic annotation of the text is very common in text 
genre detection. Such measures provide very useful information for the exploration 
473 
Computational Linguistics Volume 26, Number 4 
of the characteristics of style (Biber 1995). Typical paradigms are passive count, nom- 
inalization count, and counts of the frequency of various syntactic ategories (e.g., 
part-of-speech tags). Recently, syntactic information has also been applied to author- 
ship attribution. Specifically, Baayen, Van Halteren, and Tweedie (1996) used frequen- 
cies of occurrence of rewrite rules as they appear in a syntactically annotated corpus 
and proved that they perform better than word frequencies. Their calculation requires 
tagged or parsed text, however. Current NLP tools are not able to provide accurate 
calculation results for many of the previously proposed style markers. In the study of 
register variation conducted by Biber (1995), a subset of the measures (i.e., the simplest 
ones) was calculated by computational tools and the remaining were counted manu- 
ally. Additionally, the automatically acquired measures were counterchecked manually. 
Many researchers, therefore, try to avoid the use of features related to syntactic an- 
notation in order to avoid such problems (Kessler, Nunberg, and Sch~itze 1997). As 
a result, the recent advances in computational linguistics have not notably affected 
research in computational stylistics. 
2.3 Vocabulary Richness 
Various measures have been proposed for capturing the richness or the diversity of 
the vocabulary of a text and they have been applied mainly to authorship attribution 
studies. The most typical measure of this category is the type-token ratio V/N, where 
V is the size of the vocabulary of the sample text, and N is the number of tokens of 
the sample text. Similar features are the hapax legomena (i.e., words occurring once 
in the sample text) and the dislegomena (i.e., words occurring twice in the sample 
text). Since text length dramatically affects these features, many researchers have pro- 
posed functions of these features that they claim are text length independent (Honor6 
1979; Yule 1944; Sichel 1975). Additionally, instead of using a single measure, some 
researchers have used a set of such vocabulary richness functions in combination with 
multivariate statistical techniques to achieve better results in authorship attribution 
(Holmes 1992). In general, these measures are not computationally expensive. How- 
ever, according to results of recent studies, the majority of the vocabulary richness 
functions are highly text length dependent and quite unstable for texts shorter than 
1,000 words (Tweedie and Baayen 1998). 
2.4 Common Word Frequencies 
Instead of using vocabulary distribution measures, ome researchers have counted the 
frequency of occurrence of individual words in the sample text. Such counts are a reli- 
able discriminating factor (Karlgreen and Cutting 1994; Kessler, Nunberg, and Schi~tze 
1997) and have been applied to many works in text genre detection. Their calculation is
simple, but nontrivial effort is required for the selection of the most appropriate words 
for a given problem. Morever, the words that best distinguish a given group of authors 
cannot be applied to a different group of authors with the same success (Holmes and 
Forsyth 1995). Oakman (1980) notes: "The lesson seems clear not only for function 
words but for authorship word studies in general: particular words may work for 
specific cases uch as 'The Federalist Papers' but cannot be counted on for other anal- 
yses" (p. 28). Furthermore, the results of such studies are highly language dependent. 
Michos et al (1996) introduce the idea of grouping certain words in categories, uch as 
idiomatic expressions, cientific terminology, formal words, and so on. Although this 
solution is language independent, i  requires the construction of a complicated com- 
putational mechanism for the automated detection of the categories in the sample text. 
Alternatively, the use of sets of common high-frequency words (typically 30 or 
50 words) has been applied mainly to authorship attribution studies (Burrows 1987). 
474 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
NCPtoo, A ,ys,s  
I i measures J 
L. measures j 
Figure 1 
The proposed method. 
The application of a principal components analysis on the frequencies of occurrence 
of the most frequent words achieved remarkable results in plotting the texts in the 
space of the first two principal components, for a wide variety of authors (Burrows 
1992). This approach is language independent and computationally inexpensive. Vari- 
ous additional restrictions to this basic method have been proposed (e.g., separation of 
common homographic forms, removal of proper names from the most frequent word 
list, etc.), aimed at improving its performance. For a fully automated system, such 
restrictions require robust and accurate NLP tools. 
3. The Proposed Method 
Our method attempts to exploit already existing NLP tools for the extraction of stylistic 
information. To this end, we use two types of measures, as can be seen in Figure 1: 
? measures relevant o the actual output of the NLP tool (i.e., usually 
tagged or parsed text), and 
? measures relevant o the particular methodology by which the NLP tool 
analyzes the text (analysis-level measures). 
Thus, the set of style markers is adapted to a specific, already existing NLP tool, 
taking into account its particular properties. Analysis-level measures capture useful 
stylistic information without additional cost. The NLP tool is not considered a black 
box. Therefore, full access to its source code is required in order to define and measure 
analysis-level style markers. Moreover, tool-specific knowledge, rather than language- 
specific knowledge, is required for the definition of such measures. In other words, 
researchers using this approach can define analysis-level measures based on their deep 
understanding of a particular NLP tool even if they are not familiar with the natural 
language to which the methodology is to be applied. 
To illustrate the proposed method, we apply it to Modern Greek using the SCBD, 
an existing NLP tool able to detect sentence and chunk boundaries in unrestricted text, 
as described in the next section. In addition to a set of easily computable f atures (i.e., 
token-level and syntax-level measures) provided by the actual output of the SCBD, 
475 
Computational Linguistics Volume 26, Number 4 
we use a set of analysis-level features, i.e., measures that represent the way in which 
the input text has been analyzed by the SCBD. 
The particular analysis-level style markers can be calculated only when this specific 
computational tool is utilized. However, the SCBD is a general-purpose tool and was 
not designed for providing stylistic information exclusively. Thus, any NLP tool (e.g., 
part-of-speech taggers, parsers, etc.) can provide similar measures. The appropriate 
analysis-level style markers have to be defined according to the methodology used by 
the tool in order to analyze the text. For example, some similar measures have been 
used in stylistic experiments in information retrieval on the basis of a robust parser 
built for information retrieval purposes (Strzalkowski 1994). This parser produces trees 
to represent the structure of the sentences that compose the text. However, it is set 
to "skip" or surrender attempts to parse clauses after reaching a time-out hreshold. 
When the parser skips, it notes that in the parse tree. The measures proposed by 
Karlgren (1999) as indicators of clausal complexity are the average parse tree depth 
and the number of parser skips per sentence, which in essence are analysis-level style 
markers. 
4. Style Markers for Modem Greek 
As mentioned above, the subset of style markers used for Modern Greek depends on 
the text analysis by the specific NLP tool, the SCBD. Thus, before describing the set 
of style markers we used, we briefly present he main features of the SCBD. 
4.1 Description of the SCBD 
The SCBD is a text-processing tool able to deal with unrestricted Modern Greek text. 
No manual preprocessing is required. It performs the following procedures: 
Sentence boundary detection: The following punctuation marks are 
considered potential sentence boundaries: period, exclamation point, 
question mark, and ellipsis. A set of automatically acquired 
disambiguation rules (Stamatatos, Fakotakis, and Kokkinakis 1999) is 
applied to every potential sentence boundary in order to locate the 
actual sentence boundaries. These rules utilize neither lexicons with 
specialized information or abbreviation lists. 
Chunk boundary detection: Intrasentential phrase detection is achieved 
through multiple-pass parsing making use of an approximately 
450-keyword lexicon (i.e., closed-class words such as articles and 
prepositions) and a 300-suffix lexicon containing the most common 
suffixes of Modern Greek words. Initially, using the suffix lexicon, a set 
of morphological descriptions i assigned to any word of the sentence 
not included in the keyword lexicon. If the suffix of a word does not 
match any of the entries of the suffix lexicon, then no morphological 
description is assigned to this word. It is marked as a special word and 
is not ignored in subsequent analysis. Then, each parsing pass (five 
passes are performed) analyzes a part of the sentence, based on the 
results of the previous passes, and the remaining part is kept for the 
subsequent passes. In general, the first passes try to detect simple cases 
that are easily recognizable, while the last passes deal with more 
complicated ones. Cases that are not covered by the disambiguation 
rules remain unanalyzed. The detected chunks are noun phrases (NPs), 
476 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
Unrestricted Text 
i, 
Sentence Boundary I:! 
Detection II 
. . . . . . . . . . . . . . .  
Assignment of ~ t_.._._.__._._.l 
Morphological Descriptions H 
~_ ~=~ ..... ~ . . . . . . . . . . . .  ~,X~K~yv~r~ts I 
I Multiple-Pass Parsing ~ \[-_.._.______.J 
Parsed Text 
Figure 2 
The SCBD structure. 
prepositional phrases (PPs), verb phrases (VPs), and adverbial phrases 
(APs). In addition, two chunks are usually connected by a sequence of 
conjunctions (CONs). 
The SCBD is able to cope rapidly with any piece of text, even ill-formed text, and 
its performance is comparable to more sophisticated systems that require more com- 
plicated resources. Figure 2 gives an overview of the SCBD. An example of its output 
for a sample text, together with a rough English translation (included in parentheses), 
is given below (note that special words, those that do not match with any of the stored 
suffixes, are marked with an asterisk): 
VP\[&eu 0gAco uoz pg{oo (I don't want to pour)\] NP\[A&& (oil)\] PP\[crrr/9~wr~& (in the 
fire)\] CON\[of&kale (but)\] VP\[rr~?re4a; (I believe)\] CON\[drL (that)\] NP\[r/ 
err~fldpvu~rr/(the encumbrance)\] PP\[o-rou rrpo~rcoko7Lcr#6 (of the budget)\] PP\[ozrr6 
rov? flov&evrg? (by the deputies)\] VP\[&u #rcopeg uce rrpoe#erpeirc~L (can not be 
measured)\] #6uo (merely) PP\[#e rc~ 5*&?.*6px. rcou c~uc,Spo#~n&u (with the 5 
bil. Dr. of the retroactive salaries)\] troy (that) NP\[rc~po~u re&evrcdc~ (they took 
lately)\] VP\[rrponc~&cburc~? (causing)\] NP\[rr/(Sva~op&~ r~\]g ~oLu~? 7v,&#r/~ (the 
discontent of the public opinion)\]. 
It is worth noting that we did not modify the structure of the SCBD in order to 
calculate style markers, aside from adding simple functions for their measurement. 
4.2 Stylometric Levels 
Our aim during the definition of the set of style markers was to take full advantage 
of the analysis of the text by the SCBD. To this end, we included measures relevant to 
the actual output of this tool as well as measures relevant to the methodology used by 
the SCBD to analyze the text. Specifically, the proposed set of style markers comprises 
three levels: 
? Token Level: The sample text is considered as a set of tokens grouped in 
sentences. This level is based on the output of the sentence boundary 
477 
Computational Linguistics Volume 26, Number 4 
detector: 
Code 
M01 
M02 
M03 
Description 
detected sentences/words 1 
punctuation marks/words 
detected sentences/potential sentence boundaries 
Phrase Level: The sample text is considered as a set of phrases (i.e., 
chunks). This level is based on the output of the chunk boundary 
detector: 
Code 
M04 
M05 
M06 
M07 
M08 
M09 
M10 
Ml l  
M12 
M13 
Description 
detected NPs/total detected chunks 
detected VPs/total detected chunks 
detected APs/total detected chunks 
detected PPs/total detected chunks 
detected CONs/total detected chunks 
words included in NPs/detected NPs 
words included in VPs/detected VPs 
words included in APs/detected APs 
words included in PPs/detected PPs 
words included in CONs/detected CONs 
Analysis Level: Measures that represent the way in which the sample 
text has been analyzed by the particular methodology of the SCBD are 
included here: 
Code 
M14 
M15 
M16 
M17 
M18 
M19 
M20 
M21 
M22 
Description 
detected keywords/words 
special words/words 
assigned morphological descriptions/words 
chunks' morphological descriptions/total detected chunks 
words remaining unanalyzed after pass 1/words 
words remaining unanalyzed after pass 2/words 
words remaining unanalyzed after pass 3/words 
words remaining unanalyzed after pass 4/words 
words remaining unanalyzed after pass 5/words 
It is clear that the analysis level contains extremely useful stylistic information. For 
example, M14 and M15 are valuable markers that indicate of the percentage of high- 
frequency words and the percentage of unusual words included in the sample text, 
respectively. M16 is a useful indicator of the morphological ambiguity of the words 
and M17 indicates the degree to which this ambiguity has been resolved. Moreover, 
markers M18 to M22 indicate the syntactic omplexity of the text. Since the first parsing 
passes analyze the most common cases, it is easy to understand that a large part of 
a syntactically complicated text would not be analyzed by them (e.g., high values for 
M18, M19, and M20 in conjunction with low values for M21 and M22). Similarly, a 
syntactically simple text would be characterized by low values for M18, M19, and M20. 
1 We consider words as word tokens. 
478 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
Note that all the proposed style markers are produced as ratios of two relative 
measures in order for them to be stable over the text length. However, they are not 
standardized. 
5. Text Categorization 
The methodology described in the previous ection provides a vector of 22 variables 
for each text. For automatically classifying this vector into one group (either genre or 
author) various techniques are available, which stem from multivariate statistics (e.g., 
discriminant analysis), neural networks, and machine learning (e.g., decision trees). 
Recently, Yang (1999) studied the performance of several classifiers on text categoriza- 
tion tasks and concluded that all the tested methods perform comparably when the 
training set comprises over 300 instances per category. On the other hand, when the 
number of positive training instances per category is small (less than 10) a regression- 
like method called linear least-squares fit and k-nearest neighbors outperform neural 
networks and naive Bayes classifiers (Yang and Liu 1999). 
In the present paper we used two well-known techniques of multivariate statistics: 
multiple regression and discriminant analysis. The response of these techniques i very 
fast since they are based on the calculation of simple linear functions. Moreover, their 
training procedures do not require excessive time or computational cost. Thus, they 
can be easily incorporated into a real-time application. 
5.1 Multiple Regression 
Multiple regression predicts values of a group of response (dependent) variables from 
a collection of predictor (independent) variable values (Edwards 1979). The response 
is expressed as a linear combination of the predictor variables, namely: 
yi = bo + z~bli + z2b2i  + ? " q- zrbri q- ei 
where yi is the response for the ith category (i.e., text genre), Zl, z2,... , Zr are the pre- 
dictor variables (i.e., in our case r = 22), b0, bli, b2i . . . . .  bri, are the unknown coefficients 
calculated uring the training procedure, and ei is the random error. An indication of 
the goodness of fit of the model is provided by the coefficient of determination, R 2, 
defined as follows: 
n 
R2 _ j= l  
y/ 
E (yj - 9) 
j= l  
where n is the total amount of the training data (texts), 9 is the mean response, and 
finally, ~j and yj are the estimated response and the training response value, respec- 
tively. R 2 equals 1 if the fitted equation passes through all the data points, and, at the 
other extreme, equals 0. 
Moreover, multiple regression can also be used for the estimation of the signifi- 
cance of the independent variables. In particular, the amount by which R 2 is reduced if 
a certain independent variable is deleted from the regression equation (in other words, 
the contribution of the independent variable to R 2) is represented by the squared semi- 
partial correlation sri 2 (Tabachnick and Fidell 1996): 
sr~ = t~ (1 - a 2) 
df ~?s 
479 
Computational Linguistics Volume 26, Number 4 
where ti is the value of the t statistic for the ith variable and dffr?~ are the residual 
degrees of freedom. Thus, the contribution of an independent variable to R 2 can be 
expressed as a function of the absolute value of t. The absolute t value of the jth 
estimated regression coefficient bj is calculated as follows: 
bj 
tbj = S--~ 
where Sb i is the standard error. The greater the t value, the more important the con- 
tribution of the independent variable (i.e., style marker) to the response value. 
5.2 Discriminant Analysis 
The mathematical objective of discriminant analysis is to weight and linearly com- 
bine the discriminating variables in some way so that the groups are forced to be as 
statistically distinct as possible (Eisenbeis and Avery 1972). The optimal discriminant 
function, therefore, is assumed to be a linear function of the variables and is deter- 
mined by maximizing the between-group variance while minimizing the within-group 
variance using a training sample. 
Discriminant analysis can be used for predicting the group membership of pre- 
viously unseen cases (i.e., test data) based on Mahalonobis distance (i.e., a measure 
of distance between two points in the space defined by multiple correlated variables). 
Initially, for each group, the location of the centroids, i.e., the points that represent 
the means for all variables in the multivariate space defined by the independent vari- 
ables, is determined. Then, for each case the Mahalanobis distances from each of the 
group centroids are computed and the case is classified into the closest group. The 
Mahalanobis distance d of a vector x from a mean vector mx is given by the formula: 
d 2 = (x  - mx)'C~-l(x - mx)  
where Cx is the covariance matrix of x. Using this classification method we can also 
derive the probability that a case belongs to a particular group (i.e., posterior proba- 
bilities), which is roughly proportional to the Mahalanobis distance from that group 
centroid. Discriminant analysis has been employed by researchers in automatic text 
genre detection (Biber 1993b; Karlgren and Cutting 1994) since it offers a simple and 
robust solution despite the fact that it presupposes normal distributions of the dis- 
criminating variables. 
6. Text Genre Detection 
6.1 Genre-based Corpus 
Since no Modern Greek corpus covering a wide range of text genres was available, 
we decided to compose one from scratch. The corpus used in experiments in Michos 
et al (1996) includes a limited number of carefully selected and manually edited texts 
divided into generic categories (e.g., journalistic, scientific, etc.). In general, the use 
of already existing corpora not built for text genre detection (e.g., the Brown corpus) 
raises several problems ince such categories may not be stylistically homogeneous 
(Kessler, Nunberg, and Schiitze 1997). The corpus used in our study contains texts 
that meet the following criteria: 
? Real-world text: The texts have to be already in electronic form and thus 
may be ill-formed. 
480 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
Table 1 
The genre-based corpus. 
Code Text Genre Texts Words Source 
(Average) 
G01 Press editorial 25 729 
G02 Press reportage 25 902 
G03 Academic prose 25 2,120 
G04 Official documents 25 1,059 
G05 Literature 25 1,508 
G06 Recipes 25 109 
G07 Curricula vitae 25 333 
G08 Interviews 25 2,625 
G09 Planned speeches 25 2,569 
G10 Broadcast news, scripted 25 137 
Newspaper TO BHMA 
Newspaper TO BHMA 
Journal of ARCHIVES OF 
HELLENIC PATHOLOGY 
High Court decisions, 
Ministerial decisions 
Various pages 
Magazine NETLIFE 
Various pages 
Newspaper TO BHMA 
Ministry of defense 
Radio station FLASH 9.61 
? Raw text: Neither manually inserted tags nor other manual 
text-preprocessing restrictions are set. 
? Whole text: Neither text length limitations nor other manual 
text-sampling restrictions are set. In other words, a text has to be 
available as it appears in its source. 
We constructed a corpus by downloading texts from various WWW sites edited 
in Modern Greek, trying to cover as many genres as possible. This corpus is shown 
in Table 1. Although the complete set of text genres may differ significantly among 
two languages (Biber 1995), they usually overlap to a great extent, especially for Indo- 
European languages. The set we propose, therefore, can be compared to the ones used 
in similar studies of English (Karlgren and Cutting 1994; Biber 1995). Additionally, 
no manual preprocessing was performed aside from removing unnecessary headings 
irrelevant to the text itself. 
It must also be pointed out that the last three text genres (i.e., G08, G09, and 
G10) refer to spoken language that has been transcribed either before (i.e., planned 
speeches, broadcast news) or after (i.e., interviews) it has been uttered. On the other 
hand, G01 to G07 refer to written language. 
The genre-based corpus was divided into a training part and a test part of equal 
size. Ten texts per genre were included in the training corpus and ten texts per genre 
were included in the test corpus. The remaining five texts per genre were used only 
in the experiments described in Section 7. 
6.2 Setting the Baseline 
To evaluate the proposed approach, we decided to apply two previous tylometric ap- 
proaches that are based on distributional lexical measures to the same testing round: 
(i) a multivariate model of functions of vocabulary richness (Holmes 1992) and (ii) the 
frequencies ofoccurrence of the most frequent words (Burrows 1992). These two meth- 
ods were selected since they are language independent and computationally inexpen- 
sive. 
To measure the richness of the vocabulary, we used a set of five functions, namely, 
K proposed by Yule (1944), R proposed by Honor6 (1979), W proposed by Brunet 
(1978), S proposed by Sichel (1975), and D proposed by Simpson (1949), which are 
481 
Computational Linguistics Volume 26, Number 4 
defined as follows: 
10 4 (Ei~__i i 2Wi -  N)  
K = 
N 2 
(1001ogN) R - 
(1 - (~-))  
W = N v-~ 
V2 
S - 
V 
V i ( i -  1) 
D = Z.-, 'NTlqL-i'~ 
i=1 " " 
where Vi is the number of words used exactly i times (see Section 2.3 for the defini- 
tion of V and N) and o~ is a parameter usually fixed at 0.17. The same set of func- 
tions has been used by Baayen and his colleagues for similar purposes (Baayen, Van 
Halteren, and Tweedie 1996). For every text, these functions are calculated and a vec- 
tor of five parameters i produced. These vectors can then be classified to the most 
likely genre by applying one of the classification techniques discussed in the previ- 
ous section. Hereafter, this approach will be called VR (which stands for vocabulary 
richness). 
The second method, which is lexically based, uses the frequencies of occurrence of 
the most frequent words of the training corpus as style markers. Typically, sets of 30 
or 50 most frequent words are used (Baayen, Van Halteren, and Tweedie 1996; Holmes 
and Forsyth 1995). For comparison purposes, we employed two sets of common words 
based on 30 and 50 most frequent words of the training corpus, respectively. Thus, 
for each text a vector of 30 (or 50) parameters indicating the frequencies of the most 
frequent words of the training corpus (normalized by the text length) are calculated. 
As above, these vectors can then be classified to the most likely genre. These two 
approaches will be called CWF-30 and CWF-50 for common word frequencies and the 
number of the high-frequency words. 
6.3 Results 
The entire corpus described in the previous ection was analyzed by the SCBD, which 
automatically provided avector of 22 parameters for each text. The vectors of the train- 
ing corpus were used in order to extract he classification model using both multiple 
regression and discriminant analysis. These classification models were then applied to 
the vectors of the test corpus for cross-validating their performance on unseen cases. 
The same training and test procedure was performed for the VR approach and for the 
CWF-30 and CWF-50 methods. 
Comparative results in terms of identification error (i.e., erroneously classified 
texts/total texts) are given in Figure 3. In general, discriminant analysis seems to be 
better able to distinguish the texts of the test corpus. The performance of the VR 
approach is quite poor. This is due to the limited text length of the majority of the 
texts of the genre-based corpus (Tweedie and Baayen 1998). Moreover, our approach 
is more accurate than the CWF-30 and the CWF-50. The identification error rate of 
our approach using both multiple regression and discriminant analysis is given in 
Table 2. Although the average rror rate is equal for the two methodologies, there are 
significant differences in the disambiguation accuracy of certain text genres (see G01 
and G05). In general, the error rate is more normally distributed using discriminant 
analysis. Moreover, approximately 60% of the identification errors using multiple re- 
482 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
Our approach 
VR 
CWF-50 
CWF-30 
\[\] Discriminant analysis ? Multiple regression 
0.44 
? . . . . .  ,,,, i i  i , i  l 0 .41  
0.22 
. .  I 0.2\] 
0.22 
1 0.22 
0 0.1 0.2 0.3 0.4 
Ident i f icat ion er ror  
Figure 3 
Comparative r sults for text genre detection. 
0.5 
Table 2 
The text genre detection results. 
Identification Error 
Code Multiple Regression Discriminant Analysis 
G01 0.7 0.4 
G02 0.2 0.1 
G03 0.0 0.0 
G04 0.1 0.2 
G05 0.1 0.4 
G06 0.0 0.0 
G07 0.4 0.4 
G08 0.1 0.0 
G09 0.2 0.2 
G10 0.0 0.1 
Average 0.18 0.18 
gression were caused by G01 and G07, while 65% of the identification errors using 
discriminant analysis were caused by G01, G05, and G07. On the other hand, G04, 
G06, G08, and G10 are stylistically homogeneous to a great extent in both cases. 
The complete identification results of our method using discriminant analysis are 
presented in a confusion matrix in Table 3. Each row represents a text genre being 
tested and the columns represent he classification results of the test texts of that 
particular genre. The main misclassifications are as follows: 
? press editorial ~ press reportage. Notice that the texts were taken from the 
same newspaper, which is published on a weekly basis. In many cases, 
therefore, the reportage documents review a whole week and present 
some comments by the author. 
? curricula vitae --~ official documents. Both are usually characterized by an 
abstract style. 
? literature ~ interviews and planned speeches. These two text genres of the 
spoken language usually involve narration. 
483 
Computational Linguistics Volume 26, Number 4 
Table 3 
Confusion matrix for text genre detection using discriminant analysis. 
Actual Classification 
G01 G02 G03 G04 G05 G06 G07 G08 G09 G10 
Total Texts 
G01 6 3 0 0 0 0 0 0 1 
G02 0 9 0 0 0 0 0 0 0 
G03 0 0 10 0 0 0 0 0 0 
G04 0 1 0 8 0 0 0 0 0 
G05 0 0 0 0 6 0 0 2 2 
G06 0 0 0 0 0 10 0 0 0 
G07 0 0 0 3 0 0 6 0 0 
G08 0 0 0 0 0 0 0 10 0 
G09 1 0 0 0 0 0 0 1 8 
G10 0 0 0 1 0 0 0 0 0 
0 10 
1 10 
0 10 
1 10 
0 10 
0 10 
1 10 
0 10 
0 10 
9 10 
40 
35 
30 
25 
20 
15 
10 
5 
0 
t.?3 
I 
\[\] Correct ? Error 
I I A 
Text  length  ( in words)  
Figure 4 
Text length related to accuracy for the text genre detection experiment. 
Note that spoken language text genres (i.e., G08-G10) have a lower identification error 
rate, on average (0.10), than written language text genres (0.21) as calculated by either 
multiple regression or discriminant analysis. 
The classification accuracy of our method related to the text length for the text 
genre experiment using multiple regression is presented in Figure 4. Due to the stylistic 
homogeneity of recipes and broadcast news, the accuracy of texts shorter than 500 words 
(see Table 1) is relatively high. In addition, texts over 1,500 words seem to be classified 
more reliably. Note that according to Biber (1990, 1993a) a text length of 1,000 words 
is adequate for representing the distributions of many core linguistic features of a 
stylistic category. 
484 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
Table 4 
The structure of the Modern Greek weekly newspaper TO BHMA. 
Section Title (Translation) Description 
Code 
A TO BHMA (the tribune) 
B 
C 
D 
E 
I 
S 
Z 
T 
NEEX' E I IOXEE (new ages) 
TOAAAOBHMA (the other tribune) 
ANA FiTY~H (development) 
H &PAXMH ZAZ (your money) 
EI&IKH EKZ~O ZH (special issue) 
BIBAIA (books) 
TEXNE22KAIKAAAITEXNEZ~ (arts and artists) 
TA~I,~IA (travels) 
Editorials, diaries, 
reportage, politics, 
international ffairs, 
sport reviews 
Cultural supplement 
Review magazine 
Business, finance 
Personal finance 
Issue of the week 
Book review supplement 
Art review supplement 
Travels upplement 
7. Authorship Attribution 
7.1 Author-based Corpus 
In authorship attribution experiments we chose to deal with texts taken from news- 
papers, since a wide variety of authors frequently publish their writings in the press, 
making the collection of a considerable number of texts for several authors easier. In 
particular, the corpus used for this study comprises texts downloaded from the WWW 
site of the Modern Greek weekly newspaper TO BHMA,  (the tribune).2 The structure of 
this newspaper is shown in Table 4. We performed experiments based on two groups 
of authors, namely: 
. 
2. 
Group A: Ten randomly selected authors whose writings are frequently 
found in section A. This section comprises texts written mainly by 
journalists on a variety of current affairs. Moreover, a certain author may 
sign texts from different text genres (e.g., editorial, reportage, tc.). Note 
that in many cases such writings are highly edited to conform to a 
predefined style, thus washing out specific characteristics of the authors, 
which complicates the task of attributing authorship. 
Group B: Ten randomly selected authors whose writings are frequently 
found in section B. This supplement comprises essays on science, 
culture, history, and so on, in other words, writings in which the 
idiosyncratic style of the author is not overshadowed by functional 
objectives. In general, the texts included in the B section are written by 
scholars, rather than journalists. 
Analytical information on the author-based corpus is in Table 5. All the downloaded 
texts were taken from issues published uring 1998 in order to minimize the potential 
change of the personal style of an author over time. The last column of this table refers 
to the thematic area of the majority of the writings of each author. This information 
was not taken into account during the construction of the corpus. The author-based 
2 The Web address i : http://tovima.dolnet.gr 
485 
Computational Linguistics Volume 26, Number 4 
Table 5 
The author-based corpus. 
Group Code Author Name Texts Words Thematic Area 
(Average) 
A 
A01 N. Nikolaou 20 797 Economy 
A02 N. Marakis 20 871 International ffairs 
A03 D. Psychogios 20 535 Politics 
A04 G. Bitros 20 689 Politics, society 
A05 D. Nikolakopoulos 20 1,162 Politics, society 
A06 T. Lianos 20 696 Society 
A07 K. Chalbatzakis 20 1,061 Technology 
A08 G. Lakopoulos 20 1,248 Politics 
A09 R. Someritis 20 721 Politics, society 
A10 D. Mitropoulos 20 888 International ffairs 
B01 D. Maronitis 20 589 Culture, society 
B02 M. Ploritis 20 1,147 Culture, history 
B03 K. Tsoukalas 20 1,516 International ffairs 
B04 C. Kiosse 20 1,741 Archaeology 
B05 S. Alachiotis 20 958 Biology 
B06 G. Babiniotis 20 1,273 Linguistics 
B07 T. Tasios 20 1,049 Technology, society 
B08 G. Dertilis 20 916 History, society 
B09 A. Liakos 20 1,291 History, society 
B10 G. Vokos 20 1,002 Philosophy 
corpus was divided into a training part and a test part of equal size (i.e., 10 texts per 
author for training and 10 texts per author for test). 
7.2 Author Identification 
As for the text genre detection experiment, the entire corpus was first analyzed auto- 
matically by the SCBD. We then used the stylistic vectors of the training corpus to train 
the classification model for each group separately, based on multiple regression and 
discriminant analysis. We cross-validated the acquired models by applying them to 
the test corpus of the corresponding group. The same procedure was followed based 
on the VR, CWF-30, and CWF-50 approaches. Comparative results in terms of the 
identification error rate for groups A and B are given in Figures 5 and 6, respectively. 
As in the case of text genre detection, the VR method achieved far lower accuracy 
results than the others. The performance of the CWF-30 and CWF-50 is significantly 
better in group B than in group A. In both groups, our approach achieved the best 
performance. 
The identification error rates of our approach using both multiple regression and 
discriminant analysis are presented in Table 6. For group A, there are significant dif- 
ferences in the accuracy of the two techniques. However, three authors (A01, A03, 
and A06) are responsible for approximately 50% of the average rror rate, probably 
because the average text length of these authors is relatively short, i.e., shorter than 
800 words (see Table 5). 
On the other hand, the two techniques give similar disambiguation results for 
group B. A considerable percentage of the average rror rate is caused by the authors 
B01, B05, and B08 (i.e., 65% for multiple regression, 55% for discriminant analysis). 
These authors also have a relatively short average text length, i.e., shorter than 1,000 
words. 
486 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
[] Discriminant analysis ? Multiple regression 
Our approach 
VR 
CWF-50 
CWF-30 
0.34 
: i l i , ,  I I ) ,28  
. . . . . . . .  L . . . .  = .... . . . . . . .  l l J l l~$  
I i I I i i 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 
Ident i f icat ion error  
Figure 5 
Comparative r sults for authorship identification in group A. 
Our approach 
VR 
CWF-50 
CWF-30 
[] Discriminant analysis ? Multiple regression 
0.31  
0 .3  
0 .35  
10.35 
i i i J 
0 0.1 0.2 0.3 0.4 0.5 
Ident i f icat ion error  
Figure 6 
Comparative r sults for authorship identification i  group B. 
0.6 
Table 6 
The author identification results for both group A and group B. 
Identification Error 
Code Multiple Discriminant Code 
Regression Analysis 
Identification Error 
Multiple Discriminant 
Regression Analysis 
A01 0.5 0.4 B01 
A02 0.3 0.2 B02 
A03 0.6 0.5 B03 
A04 0.2 0.1 B04 
A05 0.3 0.3 B05 
A06 0.7 0.5 B06 
A07 0.3 0.3 B07 
A08 0.1 0.1 B08 
A09 0.2 0.3 B09 
A10 0.2 0.1 B10 
Average 0.34 0.28 Average 
0.7 0.6 
0.0 0.0 
0.2 0.4 
0.1 0.1 
0.7 0.4 
0.3 0.3 
0.0 0.1 
0.6 0.6 
0.1 0.1 
0.4 0.4 
0.31 0.30 
487 
Computational Linguistics Volume 26, Number 4 
\[\] Correct ? Error 
120 
100 
80 
60 ~D 
40 
20 
Figure 7 
o I ~ A 
t?3 O hr~ 
Text length (in words) 
Text length related to accuracy for the author identification experiments. 
It seems, therefore, that text length is a crucial factor in identifying the stylistic 
features that characterize a certain author. Classification accuracy for both groups us- 
ing multiple regression related to text length is presented in more detail in Figure 7. 
Approximately 80% (i.e., 53 out of 65) of the total erroneously classified texts are 
shorter than 1,000 words. Moreover, the accuracy results for the two groups are com- 
parable. In fact, the best results have been achieved under discriminant analysis for 
group A. This fact verifies that the proposed set of style markers is capable of captur- 
ing the underlying stylistic features that characterize the author of a text even when 
dealing with texts taken from various text genres. Note that CWF-30 and CWF-50 
failed to achieve comparable performance for groups A and B. 
7.3 Author Verification 
Instead of trying to select he most likely author of a given text from among a given 
group of authors (i.e., the author identification problem), many applications require 
the confirmation (or rejection) of the hypothesis that a given person is the author of the 
text (i.e., the author verification problem). In such cases, the classification procedure 
is less complicated since there are only two possible answers: yes, i.e., the author in 
question is indeed the person who wrote the text, or no, i.e., the text was not written 
by this person. 
Implementing an automatic author verification system requires: 
The development of a response function for a given author. For a given 
text, this function must provide a response value based on the vector of 
the style markers of the text. 
The definition of a threshold value. Any text whose response value is 
greater than that of the threshold is accepted as written by the author in 
question. Otherwise, it is rejected. 
488 
Stamatatos, Fakotakis, and Kokkhlakis Text Categorization 
- -  FR . . . . . . .  FA . . . .  Mean 
1 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0 - -  " . . . . . . . . . . .  ~ " "  " i  . . . . .  ?" . . . .  J P I I I i \[ 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
xR  
Figure 8 
FR, FA, and mean error for group A related to threshold values expressed as subdivisions of R. 
Additionally, for measuring the accuracy of the author verification method for 
a given author, False Rejection (FR) and False Acceptance (FA) can be used. These 
measures are commonly used in the area of speaker verification in speech processing 
(Fakotakis, Tsopanoglou, and Kokkinakis 1993) and are defined as follows: 
FR = rejected texts of the author~total texts of the author 
FA = accepted texts of other authors~total texts of other authors 
In our study, we used the response functions taken from the application of multiple 
regression to group A and group B, as described in the previous ection. The selection 
of a threshold value, on the other hand, is highly dependent on the application. Some 
applications require either minimal FR or minimal FA, while others require minimal 
mean error, i.e., (FR + FA)/2. 
We chose to express the threshold value as a function of the multiple correlation 
coefficient R = +v~ of the regression functions (see Section 5.1) since it measures 
the degree to which the regression function fits the training data. It equals 1 if the 
fitted equation passes through all the data points and at the other extreme, equals 0, 
as already mentioned for R 2. Figures 8 and 9 depict he variation of the average FR, 
FA, and the mean error values for the test corpus of group A and group B, respectively, 
using various subdivisions of R as threshold. Notice that the evaluation shown used 
texts within the same group of authors for testing (i.e., closed-set evaluation). Low 
threshold values correspond to minimal FR, while high threshold values correspond 
to minimal FA. The minimal mean error corresponds tothreshold values between 0.4R 
and 0.5R for both groups. The FR and FA values for group A and group B using 0.5R 
as threshold are given in Table 7. The greatest part of the total FR in both groups 
accounts for the authors characterized by short text length (i.e., group A: A01, A03, 
and A06, group B: B01, B05, and B08) as in the case of author identification. On the 
other hand, FA seems to be highly relevant o the threshold value. The smaller the 
threshold value, the greater the false acceptance. 
489 
Computational Linguistics Volume 26, Number 4 
- -  FR . . . . . . .  FA . . . .  Mean 
1 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0 ~ t ~ ~ , k . . . .  ? . . . . .  ? . . . .  
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
xR 
Figure 9 
FR, FA, and mean error for group B related to threshold values expressed as subdivisions of R. 
Table 7 
The author verification results for both groups (threshold=R/2). 
Code R/2 FR FA Code RI2 FR FA 
A01 0.33 0.5 0.033 B01 0.32 0.3 0.022 
A02 0.33 0.3 0.011 B02 0.42 0.0 0.044 
A03 0.36 0.6 0.044 B03 0.33 0.0 0.155 
A04 0.36 0.2 0.111 B04 0.33 0.1 0.089 
A05 0.35 0.3 0.067 B05 0.28 0.6 0.144 
A06 0.35 0.7 0.044 B06 0.36 0.2 0.011 
A07 0.34 0.2 0.044 B07 0.38 0.0 0.022 
A08 0.31 0.1 0.111 B08 0.30 0.6 0.100 
A09 0.35 0.2 0.055 B09 0.36 0.0 0.055 
A10 0.35 0.1 0.089 B10 0.40 0.4 0.033 
Average 0.35 0.32 0.061 Average 0.35 0.22 0.068 
8. Performance Issues 
8.1 Training Set Size 
Our study makes use of 10 training texts from each category (i.e., either a text genre or 
an author) in order to extract he appropriate coefficients. This assessment meets the 
criteria of a system that requires easy adaptation of the text categorization methodol-  
ogy to a certain domain. Biber (1990, 1993a) claims that it is possible to represent the 
distributions of many core linguistic features of a stylistic category based on relatively 
few texts from each category (as few as 10 texts), but we were interested in exploring 
the way in which the identification error rate is affected by increasing the training 
data. To this end, we performed experiments on text genre detection using multiple 
regression based on variable training data. Specifically, we varied the training corpus, 
including 7 to 15 texts for each genre, but used the same test corpus of 10 texts for 
all of the experiments. The same procedure was followed for the lexically based ap- 
proaches VR, CWF-30, and CWF-50. Comparat ive results of the average identification 
error rate related to the training set size are shown in Figure 10. 
490 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
. . . .  VR - - - CWF-30 
. . . . .  CWF-50 Our approach 
t _  
t _  
.2 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0 
6 
Q 
7 8 9 10 11 12 13 14 15 16 
Training set (in texts per genre) 
Figure 10 
The identification error rate of the text genre detection experiment related to the training set 
size. 
The performance of VR is not significantly affected by increasing the training set 
size. On the other hand, the identification error rate of CWF-30, CWF-50, and that of 
our approach is generally reduced by increasing the number of texts used for training. 
The performance of CWF-30 is more stable as compared to CWF-50 but is lower than 
that of our set of style markers. 
The best results are achieved by our approach using 14 training texts per genre 
(i.e., only 15 out of 100 texts misclassified). However, the identification error rate does 
not continuously decrease from 11 to 15 training texts; the identification error rate 
using 12 as well as 15 training texts for each category is greater than the rate attained 
by using 10 texts. Thus, it is clear that satisfactory accuracy can be achieved with only 
10 training texts. 
8.2 Significance of Style Markers 
The proposed set of style markers is divided into three levels--token level, syntax level, 
and analysis level. It would be useful to calculate the contribution of each marker, and 
consequently of each level, to the classification procedure. To this end, we used the 
absolute t values of the linear regression functions that indicate the contribution of 
each independent variable to the response value (see Section 5.1). 
The average absolute t values of the 22 style markers, taking into account he 
regression functions for both text genre and author identification experiments, are 
presented in Table 8. In both cases, the most important stylometric level is the token 
level, while the syntax level contributes the least to the final response. On the other 
hand, M02, M12, and M15 are the most important style markers for text genre detection 
(i.e., average t > 1.50) while the token-level measures, M01, M02, and M03, are the 
most valuable measures for authorship attribution (for the specific groups of authors). 
8.3 Defective Computational Analysis 
The set of style markers is provided by the SCBD, an existing computational tool. To 
explore the degree to which the accuracy results are dependent on the accuracy of 
491 
Computational Linguistics Volume 26, Number 4 
Table 8 
Absolute t values (average) for the regression functions of both text genre detection and 
authorship attribution. 
Absolute t Values (Average) 
Stylometric Style Marker Text Genre Authorship 
Level Detection Attribution 
M01 1.06 1.80 
Token level M02 2.52 1.85 
M03 1.43 1.98 
Level average 1.67 1.88 
M04 0.57 0.76 
M05 0.58 0.77 
M06 0.56 0.77 
M07 0.57 0.75 
M08 0.57 0.76 
Syntax level M09 0.77 0.98 
M10 0.93 0.85 
Mll 0.59 0.90 
M12 1.72 1.07 
M13 0.67 0.97 
Level average 0.75 0.86 
M14 1.03 1.30 
M15 2.11 1.05 
M16 1.45 0.79 
M17 1.08 1.42 
Analysis level M18 0.72 1.06 
M19 1.14 0.84 
M20 1.00 0.86 
M21 0.81 0.90 
M22 0.65 0.84 
Level average 1.11 1.01 
the SCBD, we created an artificial defect in the output of the SCBD by corrupting the 
sentence and chunk boundary detection procedures. In particular: 
in the sentence boundary detection procedure, only periods were 
considered to denote a potential sentence boundary, and 
the fifth parsing pass was excluded from the chunk boundary detection 
procedure. 
These changes significantly decreased the accuracy of the output of the SCBD. 
We performed the text genre experiment again using multiple regression based on 
the defective data. The average identification error rate was increased approximately 
25% (i.e., new identification error = 0.23). As expected, the accuracy of the text cate- 
gorization methodology strongly depends on the accuracy of the SCBD. Note that the 
contribution of the stylometric levels to the final response has also changed. Table 9 
shows average absolute t values for both the regular and the defective computational 
analysis. Although the token-level measures are still the most important contributors 
to the response, the disproportion between them and both the analysis-level and the 
syntax-level measures has considerably decreased. 
492 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
Table 9 
Absolute values of t (average) of the stylometric levels for both regular and defective analysis. 
Absolute t (average) Stylometric 
Level Regular Analysis Defective Analysis 
Token level 1.67 1.55 
Syntax level 0.75 0.97 
Analysis level 1.11 1.29 
9. Conclusions 
In this paper we presented an approach to text categorization i terms of stylisti- 
cally homogeneous categories, either text genres or authors. The results of apply- 
ing this methodology to text genre detection and author identification and verifica- 
tion experiments are strongly encouraging; this methodology outperforms existing 
lexically based methods. Since the stylistic differences are clearer among text gen- 
res, the results achieved in text genre detection are considerably better than those 
of the authorship attribution tasks. However, in both cases, a limited number of 
text genres or authors are responsible for the greatest part of the identification er- 
ror rate. 
As seen in Figures 4 and 7, text length plays an important role, especially in 
the case of author identification. A lower boundary of 1,000 words for each text 
seems reasonable for assuring improved performance. Nevertheless, when dealing 
with real-world text, it is not always possible to reach this lower bound. The cor- 
pora used in all the experiments presented here consist of real-world texts down- 
loaded from the Internet without any manual text preprocessing or text sampling 
limitations. The majority of these texts have an average text length shorter than 1,000 
words. 
Our experiments have shown that our method can be applied to a randomly 
selected group of stylistically homogeneous categories without any manual adapta- 
tion restrictions. A training corpus consisting of 10 texts per category is adequate for 
achieving relatively high classification accuracy. We attempted to take advantage of 
existing NLP tools by using analysis-level style markers that provide useful stylistic 
information without any additional cost. In essence, such measures represent the way 
in which the text has been analyzed by the computational tool. We proved that these 
measures are more important to the final response than measures related to the actual 
output of the tool on the syntactic level (see Table 8). 
Much work remains to be done on the stylistic interpretation f the acquired results 
and the automatic extraction of stylistic onclusions related to both the text itself and its 
author. Such stylistic onclusions could explain the differences and similarities among 
various genres or authors on a formal basis. Moreover, the definition of a basic text 
length unit would open the way to the exploration of the variation of style within a 
single text. This procedure could assist in the detection of certain sections of the input 
text where the useful information is more likely to be found. We believe that such 
tasks can be performed using a set of style markers imilar to the one we proposed. 
Finally, the combination of our approach with lexically based methods, such as CWF- 
30, can result in a very reliable text categorization system in terms of stylistically 
homogeneous categories. 
493 
Computational Linguistics Volume 26, Number 4 
Acknowledgment 
We would like to thank the anonymous CL 
reviewers for their valuable and insightful 
comments. Their suggestions have greatly 
improved an earlier draft of this paper. 
References 
Baayen, Harald, Hans Van Halteren, and 
Fiona Tweedie. 1996. Outside the cave of 
shadows: Using syntactic annotation to 
enhance authorship attribution. Literary 
and Linguistic Computing, 11(3):121-131. 
Biber, Douglas. 1990. Methodological issues 
regarding corpus-based analyses of 
linguistic variations. Literary and Linguistic 
Computing, 5:257-269. 
Biber, Douglas. 1993a. Representativeness in 
corpus design. Literary and Linguistic 
Computing, 8:1-15. 
Biber, Douglas. 1993b. Using 
register-diversified corpora for general 
language studies. Computational 
Linguistics, 19(2):219-242. 
Biber, Douglas. 1995. Dimensions of Register 
Variation: A Cross-Linguistic Comparison. 
Cambridge University Press. 
Brainerd, Barron. 1974. Weighting Evidence in 
Language and Literature: A Statistical 
Approach. University of Toronto Press. 
Brinegar, Claude S. 1963. Mark Twain and 
the Quintus Curtius Snodgrass letters: A 
statistical test of authorship. Journal of the 
American Statistical Association, 58:85-96. 
Brunet, Ettienne. 1978. Vocabulaire de Jean 
Giraudoux: Structure t Evolution. Slatkine. 
Burrows, John F. 1987. Word-patterns and 
story-shapes: The statistical analysis of 
narrative style. Literary and Linguistic 
Computing, 2(2):61-70. 
Burrows, John F. 1992. Not unless you ask 
nicely: The interpretative n xus between 
analysis and information. Literary and 
Linguistic Computing, 7(2):91-109. 
Edwards, Allen F. 1979. Multiple Regression 
and the Analysis of Variance and Covariance. 
W. H. Freeman, San Francisco, CA. 
Eisenbeis, Robert A., and Robert B. Avery. 
1972. Discriminant Analysis and 
Classification Procedures: Theory and 
Applications. D.C. Health and Co., 
Lexington, MA. 
Fakotakis, Nikos, Anastasios Tsopanoglou, 
and George Kokkinakis. 1993. A 
text-independent speaker ecognition 
system based on vowel spotting. Speech 
Communication, 12:57-68. 
Holmes, David I. 1992. A stylometric 
analysis of Mormon scripture and related 
texts. Journal of the Royal Statistical Society, 
Series A, 155(1):91-120. 
Holmes, David I. 1994. Authorship 
attribution. Computers and the Humanities, 
28:87-106. 
Holmes, David I., and Richard S. Forsyth. 
1995. The Federalist revisited: New 
directions in authorship attribution. 
Literary and Linguistic Computing, 
10(2):111-127. 
HonorG Antony. 1979. Some simple 
measures of richness of vocabulary. 
Association for Literary and Linguistic 
Computing Bulletin, 7(2):172-177. 
Karlgren, Jussi. 1999. Stylistic experiments 
in information retrieval. In T. 
Strzalkowski, editor, Natural Language 
Information Retrieval. Kluwer Academic 
Publishers, pages 147-166. 
Karlgren, Jussi and Douglass Cutting. 1994. 
Recognizing text genres with simple 
metrics using discriminant analysis. In 
Proceedings ofthe 15th International 
Conference on Computational Linguistics 
(COLING '94), pages 1,071-1,075. 
Kessler, Brett, Geoffrey Nunberg, and 
Hinrich Schiitze. 1997. Automatic 
detection of text genre. In Proceedings of
35th Annual Meeting, pages 32-38. 
Association for Computational 
Linguistics. 
Michos, Stefanos, Efstathios Stamatatos, 
Nikos Fakotakis, and George Kokkinakis. 
1996. An empirical text categorizing 
computational model based on stylistic 
aspects. In Proceedings ofthe 8th Conference 
on Tools with Artificial Intelligence 
(ICTAI'96), pages 71-77. 
Morton, Andrew Q. 1965. The authorship of 
Greek prose. Journal of the Royal Statistical 
Society, Series A, 128:169-233. 
Mosteller, Fredrick and David Wallace. 1984. 
Applied Bayesian and Classical Inference: The 
Case of the Federalist Papers. 
Addison-Wesley, Reading, MA. 
Oakman, Robert L. 1980. Computer Methods 
for Literary Research. University of South 
Carolina Press, Columbia. 
Sichel, Herbert S. 1975. On a distribution 
law for word frequencies. Journal of the 
American Statistical Association, 70:542-547. 
Simpson, Edward H. 1949. Measurement of
diversity. Nature, 163:688. 
Smith, M. W. A. 1983. Recent experience 
and new developments of methods for 
the determination of authorship. 
Association for Literary and Linguistic 
Computing Bulletin, 11:73-82. 
Smith, M. W. A. 1985. An investigation of 
494 
Stamatatos, Fakotakis, and Kokkinakis Text Categorization 
Morton's method to distinguish 
Elizabethan playwrights. Computers and 
the Humanities, 19(1):3-21. 
Stamatatos, Efstathios, Nikos Fakotakis, and 
George Kokkinakis. 1999. Automatic 
extraction of rules for sentence boundary 
disambiguation. In Proceedings ofthe 
Workshop in Machine Learning in Human 
Language Technology, Advance Course on 
Arti~'cial Intelligence (ACAI'99), 
pages 88-92. 
Stamatatos, Efstathios, Nikos Fakotakis, and 
George Kokkinakis. 2000. A practical 
chunker for unrestricted text. In 
Proceedings ofthe 2nd International 
Conference on Natural Language Processing, 
pages 139-150. 
Strzalkowski, Tomek. 1994. Robust text 
processing in automated information 
retrieval. In Proceedings ofthe 4th Conference 
On Applied Natural Language Processing 
(ANLP'94), pages 168-173. 
Tabachnick, Barbara G. and Linda S. Fidell. 
1996. Using Multivariate Statistics. Third 
edition. HarperCollins College Publishers. 
Tweedie, Fiona and Harald Baayen. 1998. 
How variable may a constant be? 
Measures of lexical richness in 
perspective. Computers and the Humanities, 
32(5):323-352. 
Tweedie, Fiona, Sameer Singh, and David I. 
Holmes. 1996. Neural network 
applications in stylometry: The Federalist 
Papers. Computers and the Humanities, 
30(1):1-10. 
Yang, Yiming. 1999. An evaluation of 
statistical approaches to text 
categorization. I formation Retrieval 
Journal, 1(1):69-90. 
Yang, Yiming and Xin Liu. 1999. A 
re-examination f text categorization 
methods. In Proceedings ofthe ACM SIGIR 
Conference on Research and Development in 
Information Retrieval (SIGIR'99), 
pages 42--49. 
Yule, George U. 1944. The Statistical Study of 
Literary Vocabulary. Cambridge University 
Press. 
495 

,QFUHPHQWDO&RQVWUXFWLRQRI&RPSDFW$F\FOLF1)$V
.\ULDNRV16JDUEDV1LNRV')DNRWDNLV*HRUJH..RNNLQDNLV
:LUH&RPPXQLFDWLRQV/DERUDWRU\
(OHFWULFDODQG&RPSXWHU(QJLQHHULQJ'HSDUWPHQW
8QLYHUVLW\RI3DWUDV*55LR*UHHFH
^VJDUEDVIDNRWDNLJNRNNLQ`#ZFOHHXSDWUDVJU


$EVWUDFW
7KLV SDSHU SUHVHQWV DQG DQDO\]HV DQ
LQFUHPHQWDO DOJRULWKP IRU WKH
FRQVWUXFWLRQ RI $F\FOLF 1RQ
GHWHUPLQLVWLF )LQLWHVWDWH $XWRPDWD
1)$$XWRPDWDRIWKLVW\SHDUHTXLWH
XVHIXO LQ FRPSXWDWLRQDO OLQJXLVWLFV
HVSHFLDOO\ IRU VWRULQJ OH[LFRQV 7KH
SURSRVHG DOJRULWKP SURGXFHV FRPSDFW
1)$V LH 1)$V WKDW GR QRW FRQWDLQ
HTXLYDOHQW VWDWHV8QOLNH'HWHUPLQLVWLF
)LQLWHVWDWH $XWRPDWD ')$ WKLV
SURSHUW\ LV QRW VXIILFLHQW WR HQVXUH
PLQLPDOLW\EXWVWLOOWKHUHVXOWLQJ1)$V
DUH FRQVLGHUDEO\ VPDOOHU WKDQ WKH
PLQLPDO')$VIRUWKHVDPHODQJXDJHV
 ,QWURGXFWLRQ
$F\FOLF)LQLWH6WDWH$XWRPDWD)6$SURYLGHD
YHU\ HIILFLHQW GDWD VWUXFWXUH IRU OH[LFRQ
UHSUHVHQWDWLRQ DQG IDVW VWULQJ PDWFKLQJ ZLWK D
JUHDWYDULHW\RIDSSOLFDWLRQV LQ OH[LFRQEXLOGLQJ
'DFLXNHWDOPRUSKRORJLFDOSURFHVVLQJ
6JDUEDV HW DO E DQG VSHHFK SURFHVVLQJ
/DFRXWXUHDQG'H0RUL7KH\FRQVWLWXWH
YHU\ FRPSDFW UHSUHVHQWDWLRQV RI OH[LFRQV VLQFH
FRPPRQ ZRUG SUHIL[HV DQG VXIIL[HV DUH
UHSUHVHQWHG E\ WKH VDPH WUDQVLWLRQV 7KLV
UHSUHVHQWDWLRQ DOVR IDFLOLWDWHV FRQWHQW
DGGUHVVDEOHSDWWHUQPDWFKLQJ

 6RPH DXWKRUV HJ 3HUULQ  $RH HW DO 
6JDUEDV HW DO  XVH WKH WHUP '$:* 'LUHFWHG
$F\FOLF :RUG *UDSK ZKHQ UHIHUULQJ WR DF\FOLF )6$V
+RZHYHURWKHUVHJ&URFKHPRUHDQG9HULQ