Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 279?289, Dublin, Ireland, August 23-29 2014.
Chinese Word Ordering Errors Detection and Correction 
for Non-Native Chinese Language Learners 
 
 
Shuk-Man Cheng, Chi-Hsin Yu, Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University, Taipei, Taiwan 
{smcheng,jsyu}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw 
 
  
 
Abstract 
Word Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence 
level for non-native Chinese language learners. Learners taking Chinese as a foreign language 
often place character(s) in the wrong places in sentences, and that results in wrong word(s) or 
ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences. 
That makes WOEs detection and correction more challenging. In this paper, we propose 
methods to detect and correct WOEs in Chinese sentences. Conditional random fields (CRFs) 
based WOEs detection models identify the sentence segments containing WOEs. Segment 
point-wise mutual information (PMI), inter-segment PMI difference, language model, tag of 
the previous segment, and CRF bigram template are explored. Words in the segments contain-
ing WOEs are reordered to generate candidates that may have correct word orderings.  Rank-
ing SVM based models rank the candidates and suggests the most proper corrections. Train-
ing and testing sets are selected from HSK dynamic composition corpus created by Beijing 
Language and Culture University. Besides the HSK WOE dataset, Google Chinese Web 5-
gram corpus is used to learn features for WOEs detection and correction. The best model 
achieves an accuracy of 0.834 for detecting WOEs in sentence segments. On the average, the 
correct word orderings are ranked 4.8 among 184.48 candidates. 
1 Introduction 
Detection and correction of grammatical errors are practical for many applications such as document 
editing and language learning. Non-native language learners usually encounter problems in learning a 
new foreign language and are prone to generate ungrammatical sentences. Sentences with various 
types of errors are written by language learners of different backgrounds. In the HSK corpus, which 
contains compositions of students from different countries who study Chinese in Beijing Language 
and Culture University (http://nlp.blcu.edu.cn/online-systems/hsk-language-lib-indexing-system.html), 
there are 35,884 errors at sentence level. The top 10 error types and their occurrences are listed below: 
Word Ordering Errors (WOE) (8,515), Missing Component (Adverb) (3,244), Missing Component 
(Predicate) (3,018), Grammatical Error (?Is ? DE?) (2,629), Missing Component (Subject) (2,405), 
Missing Component (Head Noun) (2364), Grammatical Error (?Is? sentence) (1,427), Redundant 
Component (Predicate) (1,130), Uncompleted Sentence (1,052), and Redundant Component (Adverb) 
(1,051). WOEs are the most frequent type of errors (Yu and Chen, 2012). 
The types of WOEs in Chinese are different from those in English. A Chinese character has its own 
meaning in text, while individual characters are meaningless in English. Learners taking Chinese as a 
foreign language often place character(s) in the wrong places in sentences, and that results in wrong 
word(s) or ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences.  
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
279
Word segmentation is fundamental in Chinese language processing (Huang and Zhao, 2007). WOEs 
may result in wrong segmentation. That may make WOEs detection and correction more challenging. 
This paper aims at identifying the positions of WOEs in the text written by non-native Chinese lan-
guage learners, and proposes candidates to correct the errors. It is organized as follows. Section 2 sur-
veys the related work. Section 3 gives an overview of the study. Section 4 introduces the dataset used 
for training and testing. Sections 5 and 6 propose models to detect and correct Chinese WOEs, respec-
tively. Section 7 concludes this study and propose some future work. 
2 Related Work 
There are only a few researches on the topic of detection and correction of WOEs in Chinese language 
until now. We survey the related work from the four aspects: (1) grammatical errors made by non-
native Chinese learners, (2) word ordering errors in Chinese language, (3) computer processing of 
grammatical errors in Chinese language, and (4) grammatical error correction in other languages.  
Leacock et al. (2014) give thorough surveys in automated grammatical error detection for language 
learners. Error types, available corpora, evaluation methods, and approaches for different types of er-
rors are specified. Several shared tasks on grammatical error correction in English have been orga-
nized in recent years, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012) 
and CoNLL 2013 (Ng et al., 2013). Different types of grammatical errors are focused: (1) HOO 2011: 
article and preposition errors, (2) HOO 2012: determiner and preposition errors, and (3) CoNLL 2013: 
article or determiner errors, preposition errors, noun number errors, verb form errors, and subject-verb 
agreement errors. In Chinese, spelling check evaluation was held at SIGHAN Bake-off 2013 (Wu et 
al., 2013). However, none of the above evaluations deals with word ordering errors. 
Wang (2011) focuses on the Chinese teaching for native English-speaking students. He shows the 
most frequent grammatical errors made by foreigners are missing components, word orderings and 
sentence structures. One major learning problem of foreign learners is the influence of negative trans-
fer of mother tongue. Lin (2011) studies the biased errors of word order in Chinese written by foreign 
students in the HSK corpus. Sun (2011) compares the word orderings between English and Chinese to 
figure out the differences in sentence structures. Yu and Chen (2012) propose classifiers to detect sen-
tences containing WOEs, but they do not deal with where WOEs are and how to correct them. 
Wagner et al. (2007) deal with common grammatical errors in English. They consider frequencies 
of POS n-grams and the outputs of parsers as features. Gamon et al. (2009) identify and correct errors 
made by non-native English writers. They first detect article and preposition errors, and then apply 
different techniques to correct each type of errors. Huang et al. (2010) propose a correction rule ex-
traction model trained from 310,956 sets of erroneous and corrected pairwise sentences. Some studies 
related to word orderings are specific to the topic of pre-processing or post-processing of statistical 
machine translation, such as Galley and Manning (2008), Setiawan et al. (2009), and DeNero and 
Uszkoreit (2011). 
The major contributions of this paper cover the following aspects: (1) application aspect: detecting 
and correcting a common type of Chinese written errors of foreign learners with HSK corpus; (2) lan-
guage aspect: considering the effects of words and segments in Chinese sentences; and (3) resource 
aspect: exploring the feasibility of using a Chinese web n-gram corpus in WOE detection/correction. 
3 Overview of a Chinese Word Ordering Detection and Correction System 
Figure 1 sketches an overview of our Chinese WOE detection and correction system. It is composed of 
three major parts, including dataset preparation, WOE detection, and WOE correction. At first, a cor-
pus is prepared. Sentences containing WOEs are selected from the corpus and corrected by two Chi-
nese native speakers. This corpus will be used for training and testing. Then, a sentence is segmented 
into a sequence of words, and chunked into several segments based on punctuation marks. Regarding 
words and segments as fundamental units reduce the number of reordering and limit the reordering 
scope. The segments containing WOEs are identified by using CRF-based models. Finally, the candi-
dates are generated by reordering and ranked by Ranking SVM-based models. To examine the per-
formance of WOE correction, two datasets, Cans and Csys, consisting of error segments labelled by hu-
man and detected by our system, respectively, are employed. 
 
280
 Figure 1: Overview of word ordering error detection and correction. 
 
The example shown below demonstrates the major steps.  This sentence is composed of three seg-
ments.  The second segment contains a WOE, i.e., ????????? (Graduated college this 
summer). The correct sentence should be  ????????? (Graduated from college this sum-
mer).  
(1) Reduce the number of reordering units in a sentence by using word segmentation. 
? ? ??? ? ?? ?? ?? ? ?? ? ?? ? ?? ?
( I  / am /Wang Daan/  , /this         /summer  /graduated/le  /college   /,     /now       /look for/job    /.) 
(2) Chunk a sentence into segments by punctuation marks. 
? ? ??? ? ?? ?? ?? ? ?? ? ?? ? ?? ?
(3) Detect the possible segments containing WOEs in a sentence by CRF-based methods. 
? ? ??? ? ?? ?? ?? ? ?? ? ?? ? ?? ?
(4) Reorder words in an erroneous segment and generate candidates. 
? ? ??? ? ?? ?? ?? ?? ? ? ?? ? ?? ?
  ? 
? ? ??? ? ?? ?? ?? ?? ? ? ?? ? ?? ?
(5) Rank candidates and suggest correct word ordering by Ranking SVM-based methods. 
? ? ??? ? ?? ?? ?? ?? ? ? ?? ? ?? ?
? 
281
4 A Word Ordering Errors (WOEs) Corpus 
HSK dynamic composition corpus created by Beijing Language and Culture University is adopted.  It 
contains the Chinese composition articles written by non-native Chinese learners.  There are 11,569 
articles and 4.24 million characters in 29 composition topics.  Composition articles are scanned into 
text and annotated with tags of error types ranging from character level, word level, sentence level, to 
discourse level.  There are 35,884 errors at sentence level, and WOEs are the most frequent type at this 
level.  Total 8,515 sentences are annotated with WOEs.  We filter out sentences with multiple error 
types and remove duplicate sentences. Total 1,150 error sentences with WOEs remain for this study. 
Two Chinese native speakers are asked to correct the 1,150 sentences.  Only reordering operation is 
allowed during correction.  A dataset composed of 1,150 sets of original sentence S and its two correc-
tions A1 and A2 is formed for training and testing in the experiments.  A1 may be different from A2.  
The following shows an example.  Without context, either A1 or A2 is acceptable. 
S:   ???????????????? 
      (She we encouraged to study music and foreign languages.) 
A1: ???????????????? 
      (We encouraged her to study music and foreign languages.) 
A2: ???????????????? 
      (She encouraged us to study music and foreign languages.) 
In some cases, A1 and/or A2 may be equal to S.  That is, the annotators may think S is correct.  That 
may happen when context is not available.  Finally, 327 of 1,150 sets contain different corrections.  
Both A1 and A2 are equal to S in 27 sets. Total 47 sentences corrected by one annotator are the same 
as the original sentences, and total 65 sentences corrected by another annotator are the same as the 
original sentences.  This corpus is available at http://nlg.csie.ntu.edu.tw/nlpresource/woe_corpus/. 
Figure 2 shows the Damerau Levenshtein distance between the original sentences S and the correc-
tions A1 and A2.  It counts the minimum number of operations needed to transform a source string 
into a target one.  Here the operation is the transposition of two adjacent characters.  Total 823 sets of 
A1 and A2 have a distance of 0.  It means 71.5% of sentences have the same corrections by the two 
Chinese native speakers.  The distances between S and A1 are similar to those between S and A2. To-
tal 850 sets of original sentences and the corrections have a distance below 10 characters and 1,014 
sets of sentences have a distance below 20.  We can also observe that the number of sentences with 
even distances is larger than that of sentences with odd distances because most of the Chinese words 
are composed of two characters. 
 
 Figure 2: Transposition distance among the original sentences and two corrections. 
282
5 Detection of Word Ordering Errors 
This section first defines the fundamental units for error detection, then introduces the error detection 
models along with their features, and finally presents and discusses the experimental results. 
5.1 Fundamental Units for Reordering 
Permutation is an intuitive way to find out the correct orderings, but its cost is very high. Unrestrictive 
permutation will generate too many candidates to be acceptable in computation time.  What units to be 
reordered in what range under what condition has be considered. Chinese is different from English in 
that characters are the smallest meaningful units, and there are no clear word boundaries.  Computa-
tion cost and segmentation performance is a trade-off to select character or word as a reordering unit. 
On the one hand, using words as the reordering units will reduce the number of candidates generated. 
On the other hand, word segmentation results will affect the performance of WOE detection and cor-
rection.  The following two examples show that reordering the words cannot generate the correct an-
swers. In these two examples, a word in the original sentence (S) is segmented into two words in the 
correct sentence (A). These words are underlined. Because a word is regarded as a unit for reordering, 
the correct sentence cannot be generated by word reordering only in these two cases. 
S:  ? / ?? / ??? / ?? /? 
      (He / teach to / students / English / .) 
A: ? / ? / ??? / ? / ?? / ? 
      (He / for / students / teach / English / .) 
S:  ?? / ? / ?? / ? / ?? / ? / ??? 
      (Recently / I / start to / learn / China / ?s / cooking cuisine.) 
A: ?? / ? / ?? / ? / ? / ?? / ? / ?? 
      (Recently / I / start to / learn / cooking / China / ?s /cuisine.) 
Total 76 sets of sentences belong to such cases. They occupy 6% of the experimental dataset. Consid-
ering the benefits of words, we still adopt words as reordering units in the following experiments.   
To prevent reordering all the words in the original sentences, we further divide a sentence into seg-
ments based on comma, caesura mark, semi-colon, colon, exclamation mark, question mark, and full 
stop. Sentence segments containing WOEs will be detected and words will be reordered within the 
segments to generate the candidates for correction.  In our dataset, there are only 31 sets of sentences 
(i.e., 2.7%) with WOEs across segments.  The following shows two examples. The underlined words 
are moved to other segments. 
S: ??????????????????? 
   (In fact, when I am still working, I am not honest.) 
A: ??????????????????? 
   (In fact, when I am working, I am still not honest.) 
S: ??????????????????? 
    (Therefore we have absolute guide work experience, we do not need retraining.) 
A: ??????????????????? 
    (We have absolute guide work experience, therefore we do not need retraining.) 
In summary, the upper bound of the correction performance would be 91.3%.  That is, 6%+2.7% of 
sentences cannot be resolved. 
5.2 Word Ordering Errors Detection Models 
Conditional random fields (CRFs) (Lafferty, 2001) are used to implement the WOE detection in sen-
tence segments. Segments with WOEs are labelled with answer tags before training. The original sen-
tence S written by non-native Chinese learner is compared with the annotated correct sentence A. 
Characters are compared from the start and the end of sentences, respectively. The positions are 
marked ERRstart and ERRend once the characters are different. All words within ERRstart and ERRend are 
marked ERRrange. The longest common subsequence (LCS) within ERRrange of S and ERRrange of A are 
excluded from ERRrange and the remaining words are marked ERRwords.  Figure 3 shows an example.  
We use BIO encoding (Ramshaw and Marcus, 1995) to label segments with WOEs. Segments contain-
283
ing words in ERRwords are defined to be segments with WOEs. The leftmost segment with WOEs is 
tagged B, and the following segment with WOEs are tagged I. Those segments without WOEs are 
tagged O. 
 
 Figure 3: An example for ERRrange and ERRwords. 
 
Table 1 lists the distribution of B, I and O segments. Recall that two Chinese native speakers are 
asked to correct the 1,150 sentences, thus we have two sets of B-I-O tagging.   
 
Tagging? B Tag I Tag O Tag Total 
Statistics? #Segments Percentage #Segments Percentage #Segments Percentage Segments
Annotator 1 1111 40.6% 53 1.9% 1572 57.5% 2736 
Annotator 2 1097 40.1% 59 2.2% 1580 57.7% 2736 
Table 1: Distribution of B, I, and O segments. 
 
Five features are proposed as follows for CRF training. Google Chinese Web 5-gram corpus (Liu, 
Yang and Lin, 2010) is adopted to get the frequencies of Chinese words for fPMI, fDiff and fLM. 
(1) Segment Pointwise Mutual Information (fPMI) 
PMI(Segi) defined below measures the coherence of a segment Segi by calculating PMI of all 
word bigrams in Segi. To avoid the bias from different lengths, the sum of PMI of all word bi-
grams is divided by n-1 for normalization, where n denotes the segment length. The segment 
PMI values are partitioned into intervals by equal frequency discretization. Feature fPMI of the 
segment Segi reflects the label of the interval to which PMI(Segi) belongs. 
 
(2) Inter-segment PMI Difference (fDiff) 
Feature fDiff captures the PMI difference between two segments Segj-1 and Segj. It aims to meas-
ure the coherence between segments. The feature setting is also based on equal frequency dis-
cretization.  
(3) Language Model (fLM) 
Feature fLM uses bigram language model to measure the log probability of the words in a seg-
ment defined below. Labels of interval are also determined by equal frequency discretization. 
 
(4) Tag of the previous segment (fTag) 
Feature fTag reflects the tag B, I or O of the previous segment. 
(5) CRF bigram template (fB) 
Feature fB is a bigram template given by SGD-CRF tool1.  Bigram template combines the tags of 
the previous segment and current segment, and generates T*T*N feature functions, where T is 
number of tags and N is number of strings expanded with a macro. 
                                                 
1 http://leon.bottou.org/projects/sgd 
284
5.3 Results and Discussion 
WOE detection models will annotate the segments of a sentence with labels B, I or O. These labels 
will determine which segments may contain WOEs. In the experiments, we use 5-fold cross-validation 
to evaluate the proposed models. Performance for detecting WOEs is measured at the segment and the 
sentence levels, respectively. The metrics at the segment level are defined as follows.  Here set nota-
tion is adopted. The symbol |S| denotes the number of elements in the set S which is derived by the 
logical formula after vertical bar. TAGpred(SEG) and TAGans(SEG) mean the labels of segment SEG 
tagged by WOE detection model and human, respectively.  The symbol m denotes total number of 
segments in the test set. 
 
 
 
 
  
The metrics at the sentence level are defined as follows: 
	
 
 
Accuracy and F1-score measure whether the models can find out segments with WOEs. Correcta-
ble Rate of sentences measures whether it is possible that the candidates of the correct word order can 
be generated by the WOE correction models. If a segment without WOEs is misjudged to be erroneous, 
the word order still has a chance to be kept by the WOE correction models. However, if a segment 
with WOEs is misjudged to be correct, words in the misjudged segment will not be reordered in the 
correction part because the error correction module is not triggered.  A sentence is said to be ?correct-
able? if no segments in it are misjudged as ?correct?. The ratio of the ?correctable? sentences is con-
sidered as a metric at the sentence level.  
Table 2 shows the performance of WOE detection. Five models are compared. We regard tagging 
all the segments with the labels B and O respectively as two baselines. Clearly, the recall at the seg-
ment level and the correctable rate at the sentence level are 1 by the all-tag-B baseline.  However, its 
accuracy at the segment and the sentence levels are low. The all-tag-O baseline has better accuracy at 
the segment level than the all-tag-B baseline, but has very bad F1-score, i.e., 0. The proposed models 
are much better than the two baselines. Among the feature combinations, fPMI fDiff fTag fB show the best 
performance. The accuracy at the segment level is 0.834, and the correctable rate is 0.883.  The best 
detection result will be sent for further correction.  
 
Model Segment Sentence Accuracy Recall Precision F1-Score Accuracy Correctable Rate
Baseline (all tag B) 0.404 1.000 0.424 0.595 0.271 1.000 
Baseline (all tag O) 0.576 0.000 0.000 0.000 0.074 0.074 
fPMI fLM fTag fB 0.830  0.781 0.802  0.791 0.787  0.862  
fPMI fDiff fTag fB 0.834  0.795 0.805  0.800  0.788  0.883  
fPMI fDiff fLM fTag fB 0.831  0.769 0.823  0.795  0.777  0.850  
Table 2: Performance of word ordering error detection 
285
6 Correction of Word Ordering Errors 
This section deals with generating and ranking candidates to correct WOEs. Two datasets, Cans and Csys, 
are explored in the experiments. We evaluate the optimal performance of the WOE correction models 
with the Cans dataset, and evaluate WOE detection and correction together with the Csys dataset. 
6.1 Candidate Generation 
Instead of direct permutation, we consider three strategies shown as follows to correct the error sen-
tences. The complexity of generating candidates by permutation is O(n!). The complexity of using 
these three strategies decreases to O(n2). 
(1) Reorder single unit (Rsingle) 
Rsingle strategy reorders only one reordering unit (i.e., a word) to n-1 positions within a 
segment containing n words. Total (n-1)2 candidates can be generated by this strategy. The 
following shows an example. 
S:  ?? / ?? / ? 
     (Today / school / go to) 
A: ?? / ? / ?? 
 (Today / go to / school) 
(2) Reorder bi-word (Rbi-word) 
Rbi-word is similar to Rsingle, but two reordering units are merged into a new word before re-
ordering. Because n-1 bi-words can be generated in a segment and n-2 positions are avail-
able for each merged bi-word, (n-1)(n-2) candidates are generated by Rbi-word. The follow-
ing shows an example. 
  S:  ?/?/??/??/?/?/?? 
                      (before / already / one / company / employ / me / work) 
A:  ??/??/?/?/?/?/?? 
          (one / company / before / already / employ / me / work) 
(3) Reorder tri-word (Rtri-word) 
Rtri-word works similarly to Rbi-word, but three reordering units are merged before reordering. 
Total (n-2)(n-3) candidates are generated by Rtri-word. The following shows an example. 
S: ?/??/??/?/??/?/?/?/??? 
           (I / need / working / (de) / experience / in / your / (de) / company.) 
A: ?/??/?/?/?/??/??/?/??? 
          (I / need / in / your / (de) / company / working / (de) / experience.) 
Table 3 shows the recall rate of each candidate generation strategy.  With the Cans dataset, correct 
word ordering can be generated for 85.8% of the original sentences by fusing Rsingle, Rbi-word and Rtri-word. 
The candidates generated by using the Csys dataset cover 69.7% of the correct word orderings. The dif-
ference would probably be due to the error propagation of word ordering error detection specified in 
Section 5.3. Furthermore, 6% of correct word orderings are unable to be generated by using the reor-
dering units due to the word segmentation issue as mentioned in Section 5.1. We can also find that 
72.3% of sentences with WOEs can be corrected by the Rsingle strategy using the Cans dataset. It means 
most of the WOEs made by non-native Chinese learners can be corrected by moving only one word. 
 
Strategy\Dataset Cans Csys 
Rsingle  0.723 0.577 
Rbi-word 0.365 0.308 
Rtri-word 0.239 0.217 
Rsingle ? Rbi-word ? Rtri-word 0.858 0.697 
Table 3: Recall of candidate generation strategies 
6.2 Candidate Ranking 
We use Ranking SVM (Joachims, 2002) for candidates ranking. Because WOEs may produce abnor-
mal POS sequence, POS bigrams and POS trigrams are considered as features for Ranking SVM. We 
286
use a k-tuple feature vector for each candidate sentence, where k is the number of features. In each di-
mension, binary weight is assigned: 1 if the feature exists in a candidate, and 0 otherwise. Score for 
each candidate is assigned by a binary classifier: 1 if the candidate is the same as either of the annotat-
ed corrections, and 0 otherwise. 
6.3 Results and Discussion 
Mean Reciprocal Rank (MRR) defined below is used for performance evaluation. The reciprocal rank 
is the multiplicative inverse of the rank of the first correct answer. MRR is the mean of reciprocal rank 
for all sentences S, value from 0 to 1. The larger MRR means the correct answer more closes to the top 
ranking. 
 
Percentage of answers having rank 1 is another metric. Five-fold cross-validation is used for training 
and testing. In the Cans and Csys datasets, 182.03 and 184.48 candidates are proposed by the approach 
of fusing the results of Rsingle, Rbi-word, and Rtri-word on the average.  Experimental results are listed in 
Table 4. The proposed candidate ranking method achieves an MRR of 0.270 in the Cans dataset. It 
means the correct candidates are ranked 3.7 on the average. In contrast, the MRR by using the Csys da-
taset is 0.208. It means the correct candidates are ranked 4.8 on the average when error detection and 
correction are performed in pipelining. 
 
Metric\Dataset Cans Csys 
MRR 0.270 0.208 
% of rank 1 0.195 0.144 
Table 4: Performance of candidate ranking 
 
There are some major types of errors shown as follows in WOE correction. 
(1) Word ordering errors across segments 
Section 5.1 mentions there are 31 sets of sentences (i.e., 2.7%) with WOEs across segments.  
Our algorithm cannot capture such kinds of sentences. 
(2) Propagation errors from candidate generation 
Table 3 shows the recall of word ordering error detection using the Cans dataset is 0.858. Be-
sides, 6% of sentences mentioned in Section 5.1 cannot be reordered to correct word ordering 
due to word segmentation issue. 
(3) Limitation of our models 
In the fused n-gram models, only one n-gram can be moved. It reduces the number of candi-
dates to be generated, but some types of reorderings are missed.  An example is shown as fol-
lows.  The 2-gram?? / ? (was born in) and the unigram? (on) have to be exchanged. 
S?? / ?? / ? / 1968? 10? 25? / ? / ???? 
           (I / was born / in / 25 October 1968 / on / Vienna.) 
A?? / ? / 1968? 10? 25? / ?? / ? / ???? 
         (I / on / 25 October 1968 / was born / in / Vienna.) 
7 Conclusion 
In this paper, we consider words as the reordering units in WOE detection and correction. Sentences 
are chunked into segments based on punctuation marks and the CRF technique is used to detect seg-
ments that possibly contain WOEs. The best error detection model achieves an accuracy of 0.834. 
Three reordering strategies are further proposed to generate candidates with correct word ordering and 
reduce the numerous number of candidates generated by permutation. If the segments containing 
WOEs are known, 85.8% of correct sentences can be generated by our approach. Finally, Ranking 
SVM orders the generated candidates based on POS bigrams and POS trigrams features, and achieves 
an MRR of 0.270 when all erroneous segments are given and an MRR of 0.208 when both detection 
and correction modules are considered. 
287
Using words as the reordering unit reduces the cost to generate numerous candidates, but 6% of sen-
tences are unable to reorder due to the word segmentation issue. How to balance the trade-off has to be 
investigated further. In the candidate ranking, selection of proper weights for POS bigram and trigram 
features may improve the ranking performance. Since the corpus of WOEs in Chinese is still in a lim-
ited size, expanding the related corpus for further research is also indispensable. 
Acknowledgements 
This research was partially supported by National Taiwan University and Ministry of Science and 
Technology, Taiwan under grants 103R890858, 101-2221-E-002-195-MY3 and 102-2221-E-002-103-
MY3.  We are also very thankful to the anonymous reviewers for their helpful comments to revise this 
paper. 
References 
Robert Dale, Ilya Anisimoff and George Narroway. 2012. HOO 2012: A Report on the Preposition and Deter-
miner Error Correction Shared Task. In Proceedings of The 7th Workshop on the Innovative Use of NLP for 
Building Educational Applications, pages 54?62, Montre?al, Canada. 
Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. In Proceedings of 
the 13th European Workshop on Natural Language Generation (ENLG), pages 242?249, Nancy, France. 
John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Reordering. In 
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193?203, 
Edinburgh, Scotland, UK. 
Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Mod-
el. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 
848?856, Honolulu. 
Michael Gamon, Claudia Leacock, Chris Brockett, William B. Dolan, Jianfeng Gao, Dmitriy Belenko, and Alex-
andre Klementiev. 2009. Using Statistical Techniques and Web Search to Correct ESL Errors. CALICO Jour-
nal, 26(3):491?511. 
An-Ta Huang, Tsung-Ting Kuo, Ying-Chun Lai, and Shou-De Lin. 2010. Discovering Correction Rules for Auto 
Editing. Computational Linguistics and Chinese Language Processing, 15(3-4):219-236. 
Chang-ning Huang and Hai Zhao. 2007. Chinese Word Segmentation: A Decade Review. Journal of Chinese 
Information Processing, 21(3):8-19. 
Thorsten Joachims. 2002. Optimizing Search Engines using Clickthrough Data. In Proceedings of the Eighth 
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 133-142, Edmon-
ton, Alberta, Canada. 
John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labelling Sequence Data. In Proceedings of the 18th International Conference on 
Machine Learning (ICML 2001), pages 282-289, San Francisco, CA, USA. 
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2014. Automated Grammatical Error 
Detection for Language Learners. 2nd Edition. Morgan and Claypool Publishers. 
Jia-Na Lin. 2011. Analysis on the Biased Errors of Word Order in Written Expression of Foreign Students. Mas-
ter Thesis. Soochow University. 
Fang Liu, Meng Yang, Dekang Lin. 2010. Chinese Web 5-gram Version 1. Linguistic Data Consortium, Phila-
delphia. http://catalog.ldc.upenn.edu/LDC2010T06. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-2013 
Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computa-
tional Natural Language Learning: Shared Task, pages 1?12, Sofia, Bulgaria. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text Chunking Using Transformation-based Learning. In 
Proceedings of Third Workshop on Very Large Corpora. Pages 82-94. 
Hendra Setiawan, Min-Yen Kan, Haizhou Li, and Philip Resnik. 2009. Topological Ordering of Function Words 
in Hierarchical Phrase-based Translation. In Proceedings of the 47th Annual Meeting of the ACL and the 4th 
IJCNLP of the AFNLP, pages 324?332, Suntec, Singapore. 
288
Li-Li Sun. 2011. Comparison of Chinese and English Word Ordering and Suggestion of Chinese Teaching for 
Foreign Learners. Master Thesis. Heilongjiang University. 
Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2007. A Comparative Evaluation of Deep and Shal-
low Approaches to the Automatic Detection of Common Grammatical Errors. In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-
guage Learning, pages 112?121, Prague, Czech Republic. 
Zhuo Wang. 2011. A Study on the Teaching of Unique Syntactic Pattern in Modern Chinese for Native English-
Speaking Students. Master Thesis.  Northeast Normal University. 
Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee. 2013. Chinese Spelling Check Evaluation at SIGHAN Bake-
off 2013. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), 
pages 35?42, Nagoya, Japan. 
Chi-Hsin Yu and Hsin-Hsi Chen. 2012. Detecting Word Ordering Errors in Chinese Sentences for Learning Chi-
nese as a Foreign Language. In Proceedings of the 24th International Conference on Computational Linguis-
tics, pages 3003-3018, Mumbai, India. 
289
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 70?78,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Analyses of the Association between Discourse Relation and Sentiment Polarity with a Chinese Human-Annotated Corpus 
 Hen-Hsen Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {hhhuang, jsyu, twchang, cklin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw     Abstract 
Discourse relation may entail sentiment in-formation. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annota-tion, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks. 1 Introduction A discourse relation indicates how two argu-ments (i.e., elementary discourse units) cohere to each other. Various discourse relations were de-fined according to different taxonomy (Carlson and Marcu, 2001; Carlson et al, 2002; Prasad et al, 2008). In the work of the Penn Discourse Treebank 2.0 annotation, Prasad et al (2008) labeled four grammatical classes of connectives in English, including subordinating conjunctions, coordinating conjunctions, adverbial connectives, and implicit connectives. Besides, the sense of each connective was also tagged. They defined three levels of sense hierarchy for the connec-tives. The four classes on the top level are Tem-poral, Contingency, Comparison, and Expansion.  There are explicit and implicit uses of dis-course relations. An explicit discourse relation indicates the arguments are connected with an overt discourse marker (i.e., connective). A con-nective joins two discourse units such as phrases, clauses, or sentences together. For example, the word however is a common connective that indi-cates a Comparison relation between two argu-ments. The sense of a discourse marker denotes how its two arguments cohere. In other words, a 
discourse marker presents the relation of its two arguments. In other cases, discourse marker is absent from an implicit relation. However, readers can still infer the relation from its argument pair. To re-solve implicit discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al, 2009; Zhou et al, 2010).   Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sen-timent transition of its two arguments. Veridi-cality, the second dimension of a discourse marker, specifies whether both the two argu-ments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third di-mension of a discourse marker.  Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interac-tion between the sentiment polarity and the dis-course structure in Chinese. Consider (S1), which consists of three clauses and forms a nest-ed discourse structure shown in Figure 1.  (S1) ??????????????????????????????? (Although the management office tried to make the Yang-mingshan area a more natural environment as the long-term garden of Taipei)??????????????? (but due to the two-day weekend and the improved economic conditions)??????????????????? (the is-sues of tourist parking, garbage, and other indi-rect effects become more serious)?  The second and the third clauses form a Con-tingency relation with a sentiment polarity transi-tion from Positive to Negative. Furthermore, 
70
 Figure 1: Discourse structure and sentiment po-larities of (S1).  these two clauses also constitute one of the ar-guments of a Positive-Negative Comparison rela-tion. As the PDTB 2.0 annotation manual sug-gests (Prasad, et al, 2007), a Comparison rela-tion is established to emphasize the differences between two arguments. Therefore, it is expected that the two arguments of a Comparison relation are relatively likely to have the opposing polarity states (i.e., Positive-Negative or Negative-Positive). On the other hand, the two arguments of an Expansion relation are relatively likely to belong to the same polarity states (e.g., Positive-Positive or Neutral-Neutral).  Discourse relation recognition (Hernault et al, 2010; Soricut and Marcu, 2003) and sentiment analysis (Pang and Lee, 2008) have attracted much attention recently. Due to the limitation of the resources, the research on Chinese discourse relation analysis is relatively rare. In our previ-ous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Dis-course Resources (http://nlg.csie.ntu.edu.tw/ntu-discourse/), for inter-sentential and intra-sentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). How-ever, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an annotation scheme to construct a Chinese discourse corpus with rich information including sentiment polarities, but the corpus is still under construction due to its complexity. Zhou and Xue (2012) did PDTB-style Chinese discourse corpus annotation, but the corpus is also not available yet. In this paper, we annotate a moderate-sized Chinese corpus with the information of discourse relations and sentiment polarities. Total 7,638 sentences are sampled from the ClueWeb09. We review the results of annotation and analyze some language phenomena found in the corpus.  The rest of this paper is organized as follows. In Section 2, we introduce the ClueWeb corpus 
and a dictionary of Chinese discourse markers. In Section 3, the criteria to sample instances and the annotation scheme are shown. We analyze the language phenomena found in the annotated data and discuss the correlation between discourse relations and sentiment polarities in Section 4. Finally, we conclude the remarks in Section 5.  2 Linguistic Resources The PDTB is a popular dataset used in the Eng-lish discourse research. In contrast, no Chinese discourse corpus is publicly available at present. To construct a Chinese discourse corpus, we sample instances from a huge Chinese corpus (Yu et al, 2012). This corpus was developed based on the ClueWeb09 dataset, where Chinese material is the second largest. It contains a total of 9,598,430,559 POS-tagged sentences in 172,298,866 documents.   In this paper, only the explicit discourse rela-tions are concerned. A dictionary of discourse markers is consulted to extract the instances of explicit discourse relations from the ClueWeb. This Chinese discourse marker dictionary is de-veloped based on Cheng and Tian (1989), Cheng (2006) and Lu (2007). Table 1 shows an over-view of the discourse marker dictionary. It con-tains 808 words and word pairs mapped into the PDTB four top-level classes (Cheng and Tian, 1989; Wolf and Gibson, 2005). Besides the types of discourse relations, we further classify the markers into three groups of scopes shown in the second column, including Single word, Intra-sentential, and Inter-sentential, according to their grammatical usages. The Single word group con-tains those individual words used as discourse markers. The Intra-sentential group contains pairs of words that occur inside the same senten-ce and denote a discourse relation. Here, a Chi-nese sentence is defined as a sequence of succes-sive words that is ended by a period, a question mark, or an exclamation mark. The clauses of a sentence are delimited by commas. The Inter-sentential discourse markers are similar to the Intra-sentential ones, but the two words of a pair individually appear in different sentences. Some discourse markers can be used as both Inter-sentential and Intra-sentential. In this work, the Inter-sentential only discourse markers are ex-cluded because we only concern the discourse relation occurring within a sentence. The third column lists the number of discourse markers for each scope under each PDTB class, and the fourth column gives some examples. 
71
PDTB Class Scope # Markers Examples 
Expansion 
Single word 177 ?? (besides), ?? (or), ?? (not only), ?? (such as) Intra-sentential 106 ??????? (on the one hand ... on the other hand), ????? (not ... but), ???? (not only ... also) Inter-sentential 26 ????? (first ... second), ???? (or ... perhaps), ????? (not only ... not only) 
Temporal 
Single word 41 ?? (then) Intra-sentential 80 ????? (first ... finally) Inter-sentential 30 ????? (first ... now) 
Comparison 
Single word 34 ?? (even if) Intra-sentential 38 ???? (although ... but) Inter-sentential 15 ????? (in spite of ... in fact) 
Contingency 
Single word 67 ?? (because), ? (if), ?? (suppose), ?? (in order to avoid) Intra-sentential 180 ??? (because ... then), ??? (if then), ??? (any ... can) Inter-sentential 14 ????? (since ... then), ????? (at least ... otherwise) Table 1: Overview of a Chinese discourse marker dictionary. 3 Annotation Based on the Chinese part of the ClueWeb09 (Yu et al, 2012), we sample a moderate-sized data with some criteria and annotate them with the information of discourse relations and sentiment polarities. 3.1 Sampling a reliable dataset Discourse relations may be explicit or implicit, and a sentence may contain more than one dis-course marker. Multiple discourse relations oc-curring in a sentence will make the annotation more complex. In this work, we focus on the cor-relation between discourse relations and senti-ment polarity. To get a reliable dataset for analy-sis, we sample sentences based on the following three criteria. 1.  A sentence should contain only two clauses. 2. A sentence should contain exact one dis-course marker shown in the Chinese discourse marker dictionary. We match the discourse marker on the word level. For the Single word markers, the marker can appear in either of the clauses. For the pairwise markers, the first word should appear in the first clause, and the second word should appear in the second one. 3. The lengths of both clauses in a sentence are no more than 20 Chinese characters.  As shown in Figure 1, the sentiment polarity determination is more challenging when more than one discourse relation is involved in a sen-tence. In order to facilitate the analysis, we focus on those sentences that contain exact one dis-
course marker. The limitation of clause length is also applied to avoid the noise from implicit dis-course relation. Based on a preliminary statistics, we find that most clauses in the Chinese part of the ClueWeb (Yu et al, 2012) are no longer than 20 Chinese characters shown in Figure 2. 
 Figure 2: Length distribution in the ClueWeb. 3.2 Annotation scheme Using the criteria described in Section 3.1, total 7,638 instances are randomly selected from the ClueWeb, and 87 native speakers annotate these instances. Each instance is shown to three anno-tators. The annotator labels the polarities of the first clause, the second clause, and the whole in-stance with Negative, Neutral, and Positive. In addition, the discourse relation between the two clauses is also labeled with Temporal, Contin-gency, Comparison, and Expansion. For each target sentence, the annotation is based on the information from the sentence only. The sen-tences are not given to annotators. Finally, the majority of each label is taken. For example, the 
72
polarity p1 of the first clause in the instance (S2) is labeled as Positive, the polarity p2 of the sec-ond clause is labeled as Negative, the resulting polarity pw of the whole sentence is also labeled as Negative, and the discourse relation between the two clauses is labeled as Comparison.    (S2) ???????????????????? (Although French brand cars share more than half of the domestic market share)?????????? (but the market share con-tinued to shrink)? The inter-agreements of p1, p2, pw, and dis-course relation among annotators are 0.49, 0.50, 0.47, and 0.41 in Fleiss? Kappa values, respec-tively (all are moderate agreement). The result-ing corpus is publicly available on the website of NTU Chinese Discourse Resources1.  4 Results and Discussion To investigate the corpus annotated with dis-course relation and sentiment polarity, we firstly give an overview of results with respect to these two types of linguistic phenomena. And then, the most frequent discourse markers for each class of discourse relations are discussed. Finally, we reorganize the results to several aspects and dis-cuss the association between discourse relations and sentiment polarities.  4.1 Overview of the annotated corpus The distribution of the discourse relations versus the polarities of whole sentence (pw) is shown in Table 2. Compared to the distributions of dis-course relations in the Penn Discourse Treebank (Prasad et al, 2008) shown in Table 3, the ex-plicit Chinese discourse corpus is more similar to the whole English corpus. The instances of Ex-pansion form the largest set among four dis-course relation classes. In Chinese, the instances of Expansion are even more. Temporal is the most infrequent relation which has close fre-quencies in both corpora. The different charac-teristic is the frequency of Comparison relation. In our Chinese corpus, the frequency of Compar-ison relation is about half of that in the PDTB.  In Table 2, the symbol ? is used to highlight the relatively major polarity of each relation. The symbol ? is marked when the polarity is the ma-jority (i.e., with a frequency greater than 50%). Near half (49.11%) of the instances belong to Neutral. Neutral statements are major in Tem-                                                1 http://nlg.csie.ntu.edu.tw/ntu-discourse/  
poral and Expansion classes. On the other hand, Comparison is the relation which is most in-volved in expressing sentiment, negative senti-ment in particular. Contingency is second to Comparison in expressing sentiment. The distribution of the discourse relations ver-sus (p1, p2), the sentiment polarity transitions be-tween two clauses, is shown in Table 4. Neutral-Neutral is the most frequent polarity transition in all relations. More than half of the Temporal in-stances are Neutral-Neutral. The reason may be that the Temporal relations are usually used in the sentences that describe the objective facts of the past, present, or the future. In such sentences, the sentiments are relatively rare. On the other hand, the sentences of Comparison and Contin-gency occur more in the critical and analytical scenarios. Although the most frequent transition of Com-parison is also Neutral-Neutral (23.14%), the other three types of transitions, Positive-Negative, Neutral-Negative, and Negative-Positive, have close frequencies of 22.71%, 16.90%, and 15.72%, respectively. Moreover, Negative polar-ity is involved in all these three transitions in one of their clauses. The relations between p1, p2, and pw are also interesting. Table 5 shows the top 10 most fre-quent correlations of the polarities (p1, p2, pw) of the first clause, the second clause, and the whole sentence. On the one hand, it is not surprising that most instances belong to (Neutral, Neutral, Neutral). On the other hand, it is worthy of not-ing that p2 and pw are identical in the top eight types of combinations in Table 5. In other words, the resulting sentiment polarity of a two-clause sentence is mostly consistent with the polarity of   Relation # % Neu  (%) Pos  (%) Neg  (%) Temporal 849 11.12 ?60.66 22.38 16.96 Contingency 1,598 20.92 ?44.74 26.97 28.29 Comparison 929 12.16 33.37 27.88 ?38.75 Expansion 4,262 55.80 ?51.88 31.75 16.38 Overall 7,638 100.00 ?49.11 29.24 21.65 Table 2: Distribution of discourse relations vs. polarities of whole sentences.  Relation Only Explicit Cases Total # % # % Temporal 3,612 18.88 4,650 12.71 Contingency 3,581 18.72 8,042 21.98 Comparison 5,516 28.83 8,394 22.94 Expansion 6,424 33.58 15,506 42.38 Overall 19,133 100.00 36,592 100.00 Table 3: Distribution of discourse relations in the Penn Discourse TreeBank 2.0. 
73
PDTB Class # Distribution of each type of sentiment polarity transition (p1, p2) (%) Neu Neu Pos Neu Neg Neu Neu Pos Pos Pos Neg Pos Neu Neg Pos Neg Neg Neg Temporal 849 ?57.01 1.53 2.12 16.37 3.53 2.36 12.72 1.06 3.30 Contingency 1,598 ?35.42 3.69 5.88 13.70 10.45 2.32 11.64 1.81 15.08 Comparison 929 ?23.14 2.69 2.48 8.61 3.12 15.72 16.90 22.71 4.63 Expansion 4,262 ?48.33 2.86 1.92 14.24 16.19 0.59 7.86 0.63 7.37 Overall 7,638 ?43.53 2.87 2.84 13.68 11.99 2.99 10.29 3.61 8.20 Table 4: Distribution of discourse relations vs. types of sentiment transitions.  p1 p2 pw Occurrences Neutral Neutral Neutral 3,268 Neutral Positive Positive 945 Positive Positive Positive 908 Neutral Negative Negative 706 Negative Negative Negative 614 Positive Negative Negative 204 Negative Positive Positive 199 Negative Neutral Neutral 125 Positive Neutral Positive 121 Neutral Positive Neutral 99 Table 5: Most frequent (p1, p2, pw) combinations.   p1 = pw p1 ? pw Total p2 = pw 62.71% 29.79% 92.50% p2 ? pw 5.51% 1.99% 7.50% Total 68.22% 31.78% 100.00% Table 6: Correlations between (p1,pw) and (p2,pw).  the second clause. Table 6 shows the correlations of sentiment polarities between clauses and the whole sentence. Total 92.50% of instances be-long to the case (p2 = pw), where the polarity of the second clause is identical to the polarity of the whole sentence. In Chinese writing, putting the important part of a sentence at the end of the sentence is very common.  4.2 Frequent discourse markers The top discourse markers in our Chinese corpus are shown in Table 7. For each PDTB class, the five most frequent discourse markers are listed. In each row of the table, its number of occur-rences and the distribution of its nine sentiment polarity transitions are given. Note that there are three polarities, i.e., positive, neutral, and nega-tive. The relatively major sentiment polarity tran-sition of each discourser maker is labeled with the symbol ?. The symbol ? is marked when the sentiment polarity is the majority, i.e., its ratio is greater than 50%. Some discourse markers are the top markers in more than one discourse relation such as ? (also) and ? (still). In the discourse marker dictionary, the word ?  (also) is defined as a discourse 
marker of the Expansion relation. However, this word is frequent in the instances of all the four relations. In different relations, the distributions of the sentiment transitions of this word differ. In other words, the word ?  (also), which is a common word in Chinese, is not only used as a discourse marker for emphasizing the Expansion relation, but also has various senses in other us-ages.  For instance, the word ? in (S3) is a dis-course marker to denote an Expansion relation, but it is a particle in (S4). In fact, (S4) is an in-stance of the implicit Contingency relation. We ignore all of instances of the word ? (also) in the following analysis since it is an outlier. (S3) ??????????? (This is an af-firmation of our work)??????????????(and also our encouragement and mo-tivation)? (S4) ??????? (The mind cannot be open to forward progress)???????? (the world becomes narrow)? The word ? (still) is another ambiguous dis-course marker. Besides the Expansion relation defined in the dictionary, it is sometimes used to denote the Temporal relation, especially in the negation context, e.g., ?? (not yet). The two frequent discourse markers of the Contingency relation, ?? (due to) and ?? (because) share the similar sense, and their dis-tributions of sentiment polarity transitions are more consistent than the other markers of the Contingency relation.  The most frequent discourse marker of the Comparison class is ? (but). The other two dis-course markers ? (but) and ?? (but) share the similar sense, however, their polarity distribu-tions differ significantly. Compared to the more general marker ?  (but), the second frequent marker ? (but) is bolder and more critical. (S5) is an example of the marker ? (but). As shown in our data, the marker ? (but) is likely to high-light the negative sentences. 
74
PDTB  Class Discourse Markers # Distribution of each type of sentiment polarity transition (%) Neu Neu Pos Neu Neg Neu Neu Pos Pos Pos Neg Pos Neu Neg Pos Neg Neg Neg Temporal ?? (and then) in Arg1 69 ?50.72 1.45 2.90 15.94 5.80 2.90 8.70 4.35 7.25 ? (also) in Arg2 50 ?44.00 2.00 2.00 18.00 6.00 0.00 20.00 0.00 8.00 ? (again) in Arg2 49 ?71.43 0.00 0.00 12.24 2.04 0.00 10.20 4.08 0.00 ? (still) in Arg2 46 ?58.70 0.00 0.00 10.87 8.70 0.00 17.39 0.00 4.35 ? (again) in Arg2 38 ?78.95 2.63 0.00 10.53 0.00 0.00 2.63 0.00 5.26 Contingency ?? (if) in Arg1 190 ?42.63 4.21 11.58 14.21 3.68 3.16 10.53 1.05 8.95 ?? (due to) in Arg1 82 ?31.71 2.44 2.44 4.88 18.29 3.66 13.41 1.22 21.95 ? (also) in Arg2 77 20.78 0.00 1.30 20.78 19.48 0.00 11.69 2.60 ?23.38 ?? (because ) in Arg1 70 ?28.57 4.29 7.14 7.14 10.00 2.86 18.57 4.29 17.14 ?? (in order to) in Arg1 62 ?50.00 14.52 1.61 6.45 9.68 1.61 8.06 6.45 1.61 Comparison ? (but) in Arg2 176 21.59 4.55 2.84 4.55 3.41 16.48 15.91 ?28.98 1.70 ? (but) in Arg2 85 11.76 0.00 2.35 4.71 1.18 10.59 22.35 ?42.35 4.71 ? (however) in Arg2 77 ?46.75 5.19 0.00 5.19 1.30 3.90 10.39 22.08 5.19 ? (also) in Arg2 44 ?31.82 0.00 2.27 6.82 15.91 13.64 18.18 2.27 9.09 ?? (but) in Arg2 44 15.91 4.55 0.00 0.00 2.27 25.00 11.36 ?40.91 0.00 Expansion ? (also) in Arg2 603 ?43.62 1.66 1.49 15.26 19.07 1.00 7.79 0.33 9.78 ? (still) in Arg2 231 ?50.65 2.60 0.87 11.26 14.72 0.87 9.96 0.43 8.66 ? (say) in Arg1 206 ?48.54 2.43 0.49 18.45 9.22 0.00 16.50 0.49 3.88 ? (and) in Arg2 191 ?54.45 3.14 0.52 10.47 25.65 0.00 4.19 0.00 1.57 ? (also) in Arg1 159 ?37.11 7.55 3.14 11.95 25.16 0.63 3.77 0.63 10.06 Table 7. Five most frequent discourse makers of each PDTB class in our corpus.   (S5) ???????????  (The new type of crime is so startling)????????????(but had never been disclosed before solved)? The other discourser marker ?? (but) is an emphasized version of the marker ? (but) so that it is more likely used in the stronger polarity transitions such as Positive-Negative and Nega-tive-Positive. In addition, the sense of the marker ? (however) is also similar to the sense of ? (but), but it is more frequent to be used in the neutral situations. These linguistic phenomena show that the synonyms may have different sen-timent usages in the real world. 4.3 Association between discourse relation and sentiment polarity To analyze the data at a higher level, we reor-ganize the sentiment transitions into several tran-sition categories from four aspects. The details are shown in Table 8. The first aspect is Polarity Tendency, which classifies the transitions into three categories, including Positive-Tendency, Neutral, and Negative-Tendency. This aspect reflects the overall polarity of both arguments. The Negative-Positive transition is considered as Positive-Tendency because the emphasis of a Chinese sentence is usually placed in the last clause. Similarly, the Positive-Negative transition is considered as Negative-Tendency. The second aspect is Polarity Change, which indicates if the polarities of both arguments are opposite. Only Negative-Positive and Positive-Negative are re-garded as Opposite. All the rest transitions are 
treated as NonOpposite. The third aspect is Di-rection, which captures the movement from the first clause to the second one. To-Positive stands for the transitions in which the polarity of the second clause is more positive than that of the first clause. On the other hand, To-Negative stands for the transitions in which the polarity of the second clause is less positive than that of the first clause. Equal stands for the cases in which the polarities of both clauses are identical. The last aspect is Negativity, which regards the polar-ity of an argument as binary values, i.e., Negative and NonNegative. In this way, we re-classify the nine-way sentiment polarity transitions into four transitions. In other words, both the polarity states Neutral and Positive are merged into one state NonNegative in this aspect. Such a binary scheme is also used in some related work, in which the negative polarity is distinguished and the rest are considered Positive (Kim and Hovy, 2004; Devitt and Ahmad, 2007). For each type of each aspect, five discourse markers that occur more than 10 times in the dataset and have the highest ratio of the corresponding type are listed in the fifth column of Table 8 as significant dis-course markers.  We analyze the annotations according to the four aspects, and the results are shown in Table 9. The chi-squared test is used to test the dependen-cy between the PDTB classes of discourse mark-ers and each aspect of sentiment transitions. The results show that no matter whether the senti-ment polarity transitions are categorized into Po-larity Tendency, Polarity Change, Direction, or Negativity, the classes of discourse relations are 
75
significantly dependent on the sentiment polari-ties of the arguments at p=0.001. In the aspect of Polarity Tendency, the ratios of Neutral in the Temporal and Expansion rela-tions are 57.01% and 48.33%, respectively, which are definitely higher than those of Contin-gency and Comparison relations. In other words, the two arguments of Contingency and Compari-son relations are less likely to be neutral. The ratio of Negative-Tendency of the Comparison relation is 46.72%. It confirms the Comparison relation is likely to be involved in negative statements. As shown in Table 8, three of the five significant discourse markers of Negative-Tendency are the synonyms of ? (but), which are discourse markers of  the Comparison rela-tion. The other two markers, ?? (otherwise) and ? (because), are discourse markers of the Contingency relation. Like the word otherwise in English, ?? (otherwise) is used for introducing what bad scenario will happen if something is not done. The marker ? (because) is not only a significant discourse marker of the category Negative-Tendency, but also a significant marker 
of Negative-Negative from the aspect of Negativ-ity. From the real data, we find this marker is often used in bad cause-and-effect statements. (S6) is an example. The usage of the other dis-course marker ??  (because), which is a syno-nyms of ? (because), is more general.  (S6) ????????? (Because the tow-el is without sunlight for a long time)?????????? (it is easy to breed bacteria and fungi)? The ratio of Opposite of Comparison relation from the aspect of Polarity Change is 38.43%. Although it is not as high as expected, it is the highest among the four PDTB classes and much higher than those of three other classes. Com-pared to the other classes, Comparison is most likely to have a pair of opposite arguments. Four of the five significant discourse markers of Opposite in Table 8 are the synonyms of ? (but). Expansion relation has the highest ratio of NonOpposite. This matches our expectation that the Expansion relation is used to concatenate several events which have similar properties   Aspect  Transition Category Sentiment polarity transitions Explanation Significant Discourse Markers Polarity Tendency Positive-Tendency Pos-Neu, Neu-Pos, Pos-Pos, Neg-Pos The two arguments present an overall positive polarity. ?? ...?  (not only... also), ?? (finally) , ?...? (now that... ), ?? ...?  (as long as... ), ?? (recently) Neutral Neu-Neu Both arguments are neutral.  ?? (and then), ?? (hence) , ?? (at the end), ? (so) , ?? (as well as) Negative-Tendency Pos-Neg, Neg-Neu, Neu-Neg, Neg-Neg The two arguments present an overall negative polarity. ??  (otherwise), ?  (but), ?? (but), ?? (but), ? (because) Polarity Change Opposite Neg-Pos, Pos-Neg The polarities of both arguments are opposite. ?? (but), ??...? (although...) ,  ? (but), ? (but), ?? (but)  NonOpposite Neu-Neu, Pos-Neu, Neg-Neu, Neu-Pos, Pos-Pos, Neu-Neg, Neg-Neg The polarities of both arguments are not opposite. ? (or), ? (as), ?? (moreover), ??...? (if ... may), ?? (say) Direction To-Positive Neg-Neu, Neg-Pos, Neu-Pos The second argument is less negative than the first one. ? ?  (finally), ? ? ... ? (although...) , ?? (recently), ?? ...?  (as long as...) , ?? (seem...) Equal Neg-Neg, Neu-Neu, Pos-Pos Both arguments are the same polarity value. ??...? (Not only... even), ?? (at the end), ?? (in addition), ? (so),  ?...? (now that...) To-Negative Pos-Neu, Pos-Neg, Neu-Neg The second argument is less positive than the first one. ? (but), ?? (but), ?? (but), ?? (otherwise), ??...? (even if...) Negativity NonNegative- NonNegative Neu-Neu, Neu-Pos, Pos-Neu, Pos-Pos Both arguments are not negative.  ??  (as well as), ??  (in the future),  ?? (in order to), ?? (in addition), ?? (and then) NonNegative- Negative Neu-Neg, Pos-Neg The first argument is not negative while the second argument is negative. ?  (but), ??  (otherwise), ?? (but), ??...?  (even if...), ?? (but) Negative-NonNegative Neg-Neu, Neg-Pos The first argument is negative while the second argument is not negative. ??...? (although...), ?? (but), ?? (but), ?? (finally), ? (but) Negative-Negative Neg-Neg Both arguments are negative. ?? (even), ? (but), ? (because), ??...? (if... may), ?? (but) 
76
Table 8: Aspects of sentiment transition. PDTB Class # Polarity Tendency (%) Polarity Change (%) Direction(%) Negativity (%) Pos Tend Neutral Neg Tend Oppo Non Oppo To Pos Eq. To Neg NonNeg-NonNeg NonNeg-Neg Neg-NonNeg Neg-Neg Tem 849 23.79 57.01 19.20 3.42 96.58 20.85 63.84 15.31 78.45 13.78 4.48 3.30 Con 1,598 30.16 35.42 34.42 4.13 95.87 21.90 60.95 17.15 63.27 13.45 8.20 15.08 Com 929 30.14 23.14 46.72 38.43 61.57 26.80 30.89 42.30 37.57 39.61 18.19 4.63 Exp 4,262 33.88 48.33 17.79 1.22 98.78 16.75 71.89 11.36 81.63 8.49 2.51 7.37 Table 9: Statistics of sentiment transition for each PDTB class over the corpus annotated by human.  from certain perspective. The ratio of To-Negative of Comparison rela-tion from the aspect of Direction in Table 9 is 42.30%, which is significantly higher than the ratios of To-Negative of the other classes. This also confirms the Comparison relation is likely to be used to express critical opinions. Further-more, the ratio of Equal of Comparison relations is much lower than those of other classes. This result shows the Comparison relation is more involved in sentiment polarity transitions. The Negativity aspect in Table 9 also shows the NonNegative-Negative is more likely to hap-pen than the Negative-NonNegative in all rela-tions. This statistics reflects a particular phenom-enon ?good words ahead? in Chinese.  That is, speakers tend to express a negative opinion after kind words. The sentiment polarity flips in the instances of the two categories Negative-NonNegative and NonNegative-Negative. However, the significant discourse markers of the two categories are very different. In spite of the general marker ?? (but), the discourse markers ? (but), ?? (oth-erwise), ??...? (even if...), and ?? (but) are often used in NonNegative-Negative, which usu-ally results a negative remark. On the other hand, the discourse markers??...? (although...), ?? (but), ?? (finally), and ? (but) are often used in Negative-NonNegative, which usually results a positive remark. For example, the dis-course marker ?? (finally), which is a dis-course marker of the Temporal relation, is usu-ally used when an event successfully accom-plished after twists and turns such as (S7). (S7) ???????????????	 (Domestic mobile phone giant Ningbo Bird after many tribulations)??????????? (finally successfully fought in the automotive industry)? 
5 Conclusion To investigate the discourse relation and the sen-timent polarity of Chinese discourse markers, we construct a moderate-sized corpus based on the Chinese part of ClueWeb09. In this paper, our annotation scheme and the analysis of the anno-tation results are shown. Total 7,638 instances are annotated by native speakers. The discourse relation distribution of the annotated data is comparable to the distribution of the well-known English discourse corpus PDTB 2.0. Through the data analysis, we validate certain human intui-tions in Chinese language. Near half of instances are in neutral sentiment while the Comparison relation is most likely to be involved in negative sentiment. Furthermore, the high sentiment de-pendency between the last clause and the whole sentence is validated in the data. The data shows the significant association be-tween the discourse relation and the sentiment polarity. The arguments of a Comparison rela-tion or a Contingency relation are more likely to be involved in expressing sentiment. Moreover, the Comparison relation often occurs in the sen-tences with sentiment polarity transitions, and frequently occurs in the instances with the nega-tive sentiment. On the other hand, the arguments of the Temporal and the Expansion relations are relatively objective. The behavior of word choice between synonyms is also observed in the data. Each synonym of a sense may have its own us-age in expressing sentiment. This paper points out the ambiguities of the discourse markers in Chinese.  That is, a marker may suggest more than one discourse relation. Besides, words may have both the functions of discourse connectives and non-discourse ones in their surface forms. These two issues make the interpretation of Chinese discourse markers more challenging. Determination of their correct uses and disambiguation of their discourse functions will be investigated in the future.  
77
Acknowledgments 
This research was partially supported by Excel-lent Research Projects of National Taiwan Uni-versity under contract 102R890858 and 2012 Google Research Award. References Lynn Carlson and Daniel Marcu. 2001. Discourse Tagging Reference Manual.   http://www.isi.edu/~marcu/discourse/tagging-ref-manual.pdf Lynn Carlson, Daniel Marcu, and Mary Ellen Oku-rowski. 2002. RST Discourse Treebank. Linguistic Data Consortium, Philadelphia.  Shou-Yi Cheng. 2006. Corpus-Based Coherence Re-lation Tagging in Chinese Discourse. Master?s Thesis, National Chiao Tung University, Hsinchu, Taiwan. Xianghui Cheng and Xiaolin Tian. 1989. Xian dai Han yu (????), San lian shu dian (????), Hong Kong. Ann Devitt and Khurshid Ahmad. 2007. Sentiment polarity identification in financial news: a cohe-sion-based approach. In Proceedings of the 45th Annual Meeting of the Association of Computa-tional Linguistics (ACL 2007), pages 984-991, Pra-gue, Czech Republic. Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka. 2010. HILDA: A Discourse Parser Using Support Vector Machine Classifica-tion. Dialogue and Discourse, 1(3): 1-33. Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese discourse relation recognition. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011). pages 1442-1446, Chiang Mai, Thailand. Hen-Hsen Huang and Hsin-Hsi Chen. 2012a. Contin-gency and comparison relation labeling and struc-ture prediction in Chinese sentences. In Proceed-ings of the 13th Annual Meeting of the Special In-terest Group on Discourse and Dialogue (SIGDI-AL 2012), pages 261-269, Seoul, South Korea. Hen-Hsen Huang and Hsin-Hsi Chen. 2012b. An An-notation System for Development of Chinese Dis-course Corpus. In Proceedings of the 24th Interna-tional Conference on Computational Linguistics (COLING 2012): Demonstration Papers, pages 223-230, Mumbai, India. Ben Hutchinson. 2004. Acquiring the meaning of dis-course markers. In Proceedings of the 42nd Annual Meeting of the Association for Computational Lin-guistics (ACL 2004), pages 684-691, Barcelona, Spain. 
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), pages 1367-1373, Ge-neva, Switzerland. Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 343-351. Shuxiang Lu. 2007. Eight Hundred Words of The Contemporary Chinese (Xian dai Han yu Ba bai Ci), China Social Sciences Press. Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in In-formation Retrieval, 2(1-2): 1-135. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2007. The Penn Discourse Tree-bank 2.0 Annotation Manual. The PDTB Research Group. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Tree-Bank 2.0. In Proceedings of the 6th Language Re-sources and Evaluation Conference (LREC 2008), pages 2961-2968, Marrakech, Morocco. Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical in-formation. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Lin-guistics (HLT/NAACL 2003), pages 149-156, Ed-monton, Canada. Florian Wolf and Edward Gibson. 2005. Representing Discourse Coherence: A Corpus-Based Analysis. Computational Linguistics, 31(2): 249-287. Chi-Hsin Yu, Yi-jie Tang and Hsin-Hsi Chen. 2012. Development of a web-scale Chinese word N-gram corpus with parts of speech information. In Pro-ceedings the 8th International Conference on Lan-guage Resources and Evaluation (LREC 2012), pages 320-324, Istanbul, Turkey. Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connectives for implicit discourse relation recogni-tion. In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING 2010): Posters, pages 1507-1514. Yuping Zhou and Nianwen Xue. 2012. PDTB-style discourse annotation of Chinese text. In Proceed-ings the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 69-77, Jeju, South Korea. 
78
