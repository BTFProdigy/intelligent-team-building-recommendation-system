Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 57?64, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Redundancy-based Correction of Automatically Extracted Facts
Roman Yangarber and Lauri Jokipii
Department of Computer Science
University of Helsinki, Finland
first.last@cs.helsinki.fi
Abstract
The accuracy of event extraction is lim-
ited by a number of complicating factors,
with errors compounded at all sages in-
side the Information Extraction pipeline.
In this paper, we present methods for re-
covering automatically from errors com-
mitted in the pipeline processing. Recov-
ery is achieved via post-processing facts
aggregated over a large collection of doc-
uments, and suggesting corrections based
on evidence external to the document. A
further improvement is derived from prop-
agating multiple, locally non-best slot fills
through the pipeline. Evaluation shows
that the global analysis is over 10 times
more likely to suggest valid corrections to
the local-only analysis than it is to suggest
erroneous ones. This yields a substantial
overall gain, with no supervised training.
1 Introduction
Information Extraction (IE) is a technology for find-
ing facts in plain text, and coding them in a logical
representation, such as, e.g., a relational database.
IE is typically viewed and implemented as a se-
quence of stages?a ?pipeline?:
1. Layout, tokenization, lexical analysis
2. Name recognition and classification
3. Shallow (commonly,) syntactic parsing
4. Resolution of co-reference among entities
5. Pattern-based event matching and role mapping
6. Normalization and output generation
While accuracy at the lowest levels can reach high
90?s, as the stages advance, complexity increases
and performance degrades considerably.
The problem of IE as a whole, as well each of
the listed subproblems, has been studied intensively
for well over a decade, in many flavors and varieties.
Key observations about much state-of-the-art IE are:
a. IE is typically performed by a pipeline process;
b. Only one hypothesis is propagated through the
pipeline for each fact?the ?best guess? the
system can make for each slot fill;
c. IE is performed in a document-by-document
fashion, applying a priori knowledge locally to
each document.
The a priori knowledge may be encoded in a set of
rules, an automatically trained model, or a hybrid
thereof. Information extracted from documents?
which may be termed a posteriori knowledge?
is usually not reused across document boundaries,
because the extracted facts are imprecise, and are
therefore not a reliable basis for future extraction.
Furthermore, locally non-best slot fills are not
propagated through the pipeline, and are conse-
quently not available downstream, nor for any global
analysis.
In most systems, these stages are performed in se-
quence. The locally-best slot fills are passed from
57
the ?lower-? to the ?higher-level? modules, with-
out feedback. Improvements are usually sought
(e.g., the ACE research programme, (ACE, 2004))
by boosting performance at the lower levels, to reap
benefits in the subsequent stages, where fewer errors
are propagated.
The point of departure for this paper is: the
IE process is noisy and imprecise at the single-
document level; this has been the case for some time,
and though there is much active research in the area,
the situation is not likely to change radically in the
immediate future?rather, we can expect slow, in-
cremental improvements over some years.
In our experiments, we approach the performance
problem from the opposite end: start with the ex-
tracted results and see if the totality of a posteri-
ori knowledge about the domain?knowledge gen-
erated by the same noisy process we are trying to
improve?can help recover from errors that stem
from locally insufficient a priori knowledge.
The aim of the research presented in this paper
is to improve performance by aggregating related
facts, which were extracted from a large document
collection, and to examine to what extent the cor-
rectly extracted facts can help correct those that were
extracted erroneously.
The rest of the paper is organized as follows. Sec-
tion 2 contains a brief review of relevant prior work.
Section 3 presents the experimental setup: the text
corpus, the IE process, the extracted facts, and what
aspects of the the extracted facts we try to improve
in this paper. Section 4 presents the methods for im-
proving the quality of the data using global analysis,
starting with a naive, baseline method, and proceed-
ing with several extensions. Each method is then
evaluated, and the results are examined in section 5.
In section 6, we present further extensions currently
under research, followed by the conclusion.
2 Prior Work
As we stated in the introduction, typical IE sys-
tems consist of modules arranged in a cascade, or
a pipeline. The modules themselves are be based
on heuristic rules or automatically trained, there is
an abundance of approaches in both camps (and ev-
erywhere in between,) to each of the pipeline stages
listed in the introduction.
It is our view that to improve performance we
ought to depart from the traditional linear, pipeline-
style design. This view is shared by others in the
research community; the potential benefits have pre-
viously been recognized in several contexts.
In (Nahm and Mooney, 2000a; Nahm and
Mooney, 2000b), it was shown that learning rules
from a fact base, extracted from a corpus of job post-
ings for computer programmers, improves future ex-
traction, even though the originally extracted facts
themselves are far from error-free. The idea is to
mine the data base for association rules, and then to
integrate these rules into the extraction process.
The baseline system is obtained by supervised
learning from a few hundred manually annotated ex-
amples. Then the IE system is applied to succes-
sively larger sets of unlabeled examples, and associ-
ation rules are mined from the extracted facts. The
resulting combined system (trained model plus as-
sociation rules) showed an improvement in perfor-
mance on a test set, which correlated with the size
of the unlabeled corpus.
In work on improving (Chinese) named entity tag-
ging, (Ji and Grishman, 2004; Ji and Grishman,
2005), show benefits to this component from in-
tegrating decisions made in later stages, viz. co-
reference, and relation extraction.1
Tighter coupling and integration between IE and
KDD components for mutual benefit is advocated by
(McCallum and Jensen, 2003), which present mod-
els based on CRFs and supervised training.
This work is related in spirit to the work pre-
sented in this paper, in its focus on leveraging cross-
document information that information?though it
is inherently noisy?to improve local decisions. We
expect that the approach could be quite powerful
when these ideas are used in combination, and our
experiments seem to confirm this expectation.
3 Experimental Setup
In this section we describe the text corpus, the un-
derlying IE process, the form of the extracted facts,
and the specific problem under study?i.e., which
aspects of these facts we first try to improve.
1Performance on English named entity tasks reaches mid to
high 90?s in many domains.
58
3.1 Corpus
We conducted experiments with redundancy-based
auto-correction over a large database of facts ex-
tracted from the texts in ProMED-Mail, a mailing
list which carries reports about outbreaks of infec-
tious epidemics around the world and the efforts
to contain them. This domain has been explored
earlier; see, e.g., (Grishman et al, 2003) for an
overview.
Our underlying IE system is described in (Yan-
garber et al, 2005). The system is a hybrid
automatically- and manually-built pattern base for
finding facts, an HMM-based name tagger, auto-
matically compiled and manually verified domain-
specific ontology, based in part on MeSH, (MeS,
2004), and a rule-based co-reference module, that
uses the ontology.
The database is live on-line, and is continuously
updated with new incoming reports; it can be ac-
cessed at doremi.cs.helsinki.fi/plus/.
Text reports have been collected by ProMED-
Mail for over 10 years. The quality of reporting (and
editing) has been rising over time, which is easy to
observe in the text data. The distribution of the data,
aggregated by month is shown in Figure 1, where
one can see a steady increase in volume over time.2
3.2 Extracted Facts
We now describe the makeup of the data extracted
from text by the IE process, with basic terminology.
Each document in the corpus, contains a single re-
port, which may contain one or more stories. Story
breaks are indicated by layout features, and are ex-
tracted by heuristic rules, tuned for this domain and
corpus. When processing a multi-story report, the
IE system treats each story as a separate document;
no information is shared among stories, except that
the text of the main headline of a multi-story report
is available to each story. 3
Since outbreaks may be described in complex
ways, it is not obvious how to represent a single fact
in this context. To break down this problem, we use
the notion of an incident. Each story may contain
2This is beneficial to the IE process, which operates better
with formulaic, well-edited text.
3The format of the documents in the archive can be exam-
ined by browsing the source site www.promedmail.org.
 0
 200
 400
 600
 800
 1000
1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005
 
Co
un
t 
 Date 
 
  Records   (46,317)
  Stories   (30,015)
  Documents (22,560)
Figure 1: Distribution of data in ProMED-Mail
multiple outbreak-related incidents/facts.4
We analyze an outbreak as a series of incidents.
The incidents may give ?redundant? information
about an outbreak, e.g., by covering overlapping
time intervals or geographic areas. For example, a
report may first state the number of cases within the
last month, and then give the total for the entire year.
We treat each of these statements as a separate inci-
dent; the containment relations among them are be-
yond the scope of our current goals.5
Thus each incident corresponds to a partial de-
scription of an outbreak, over a period of time and
geographic area. This makes it easy to represent
each incident/fact as a separate row in the table.
The key fields of the incident table are:
  Disease Name
  Location
  Date (start and end)
Where possible, the system also extracts informa-
tion about the victims affected in the incident?their
count, severity (affected or dead), and a descriptor
(people, animals, etc.). The system also extracts
bookkeeping information about each incident: loca-
tions of mentions of the key fields in the text, etc.
The system?s performance is currently at 71.16 F-
measure: 67% recall, 74% precision. This score is
obtained by a MUC scorer (Douthat, 1998) on a 50-
document test corpus, which was manually tagged
with correct incidents with these slots. We have
4In this paper, we use the terms fact, incident, and event
interchangeably.
5This problem is addressed in, e.g., (Huttunen et al, 2002).
59
no blind-test corpus at present, but prior experience
suggests that we ought to expect about a 10% reduc-
tion in F-measure on unseen data; this is approxi-
mately borne out by our informal evaluations.
Further, the system attempts to ?normalize? the
key fields. An alias for a disease name (e.g., ?bird
flu?) is mapped to a canonical name (?avian in-
fluenza.?)6 Date expressions are normalized to a
standard format yyyy.mm.dd?yyyy.mm.dd.7
Note that the system may not be able to normalize
some entities, which then remain un-normalized.
Such normalization is clearly helpful for search-
ing, but it is not only a user-interface issue. Normal-
izing reduces sparseness of data; and since our intent
is to aggregate related facts across a large fact base,
excessive variation in the database fields would re-
duce the effectiveness of the proposed methods.
3.3 Experimental Focus: Location
Normalization
A more complex problem arises out of the need to
normalize location names. For each record, we nor-
malize the location field?which may be a name of
a small village or a larger area?by relating it to the
name of the containing country; we also decided to
map locations in the United States to the name of the
containing state, (rather than the name of the coun-
try, ?USA?).8 This mapping will be henceforth re-
ferred to as ?location?state,? for short. The ideas
presented in the introduction are explored in the re-
mainder of this paper in the context of correcting the
location?state mapping.
Section 6 will touch upon our current work on ex-
tending the methodology to slots other than state.
(Please see Section 5 for further justification of this
choice for our initial experiments.)
To make the experiments interesting and fair, we
kept the size of the gazetteer small. The a priori geo-
graphic knowledge base contains names of countries
of the world (270), with aliases for several of them; a
list of capitals and other selected major cities (300);
a list of states in the USA and acronyms (50); major
6This is done by means of a set of scenario-specific patterns
and a dictionary of about 2500 disease names with aliases.
7Some date intervals may not have a starting date, e.g., if the
text states ?As of last Tuesday, the victim count is N...?
8This decision was made because otherwise records with
state = USA strongly skew the data, and complicate learning.
US cities (100); names of the (sub)continents (10),
and oceans. In our current implementation, conti-
nents are treated semantically as ?states? as well.9
The IE system operates in a local, document-by-
document fashion. Upon encountering a location
name that is not in its dictionaries, the system has
two ways to map it to the state name. One way is
by matching patterns over the immediate local con-
text, (?Milan, Italy?). Failing that, it tries to find
the corresponding state by positing an ?underspeci-
fied? state name (as if referred to by a kind of spe-
cial ?pronoun?) and mapping the location name to
that. The reference resolution module then finds the
most likely antecedent entity, of the semantic type
?state/country,? where likelihood is determined by
its proximity to the mention of the location name.
Note that the IE system outputs only a single, best
hypothesis for the state fill for each record.
3.4 The Data
The database currently contains about  
	 in-
dividual facts/incidents, extracted from  sto-
ries, from 

reports (cf. Fig. 1). Each incident
has a location and a state filler. We say a location
name is ?ambiguous? if it appears in the location slot
of at least two records, which have different names
in the state slot. The number of distinct ?ambigu-
ous? location names is



.
Note, this terminology is a bit sloppy: the fillers
to which we refer as ?ambiguous location names?,
may not be valid location names at all; they may
simply be errors in the IE process. E.g., at the name
classification stage, a disease name (especially if not
in the disease dictionary) may be misclassified, and
used as a filler for the location slot.
We further group together the location fills by
stripping lower-case words that are not part of the
proper name, from the front and the end of the fill.
E.g., we group together ?southern Mumbai? and
?the Mumbai area,? as referring to the same name.
After grouping and trimming insignificant words,
the number of distinct names appearing in location
fills is

, which covers a total of

records,
or
 
 of all extracted facts. As an estimate of
the potential for erroneous mapping from locations
to states, this is quite high, about  in 	 records.10
9By the same token, both Connecticut and USA are ?states.?
10Of course, it can be higher as well, if the IE system con-
60
4 Experiments and Results
We now present the methods of correcting possible
errors in the location?state relation. A method  
tries to suggest a new value for the state fill for every
incident I that contains an ambiguous location fill:
	
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 22?23,
Vancouver, October 2005.
Extracting Information about Outbreaks of Infectious Epidemics
Roman Yangarber Lauri Jokipii
Department of Computer Science
University of Helsinki, Finland
first.last@cs.helsinki.fi
Antti Rauramo
Index, Oy
Helsinki, Finland
Silja Huttunen
Department of Linguistics
University of Helsinki, Finland
Abstract
This work demonstrates the ProMED-
PLUS Epidemiological Fact Base. The
facts are automatically extracted from
plain-text reports about outbreaks of in-
fectious epidemics around the world. The
system collects new reports, extracts new
facts, and updates the database, in real
time. The extracted database is available
on-line through a Web server.
1 Introduction
Information Extraction (IE) is a technology for find-
ing facts in plain text, and coding them in a logical
representation, such as a relational database.
Much published work on IE reports on ?closed?
experiments; systems are built and evaluated based
on carefully annotated corpora, at most a few hun-
dred documents.1 The goal of the work presented
here is to explore the IE process in the large: the
system integrates a number of off-line and on-line
components around the core IE engine, and serves
as a base for research on a wide range of problems.
The system is applied to a large dynamic collec-
tion of documents in the epidemiological domain,
containing tens of thousands of documents. The
topic is outbreaks of infectious epidemics, affecting
humans, animals and plants. To our knowledge, this
is the first large-scale IE database in the epidemio-
logical domain publicly accessible on-line.2
1Cf., e.g., the MUC and ACE IE evaluation programmes.
2On-line IE databases do exist, e.g., CiteSeer, but none that
extract multi-argument events from plain natural-language text.
2 System Description
The architecture of the ProMED-PLUS system3 is
shown in Fig. 1. The core IE Engine (center) is im-
plemented as a sequence, or ?pipeline,? of stages:
  Layout analysis, tokenisation, lexical analysis;
  Name recognition and classification;
  Shallow syntactic analysis;
  Resolution of co-reference among entities;
  Pattern-based event matching and role mapping;
  Normalisation and output generation
The database (DB) contains facts extracted from
ProMED-Mail, a mailing list about epidemic out-
breaks.4
The IE engine is based in part on earlier work,
(Grishman et al, 2003). Novel components use ma-
chine learning at several stages to enhance the per-
formance of the system and the quality of the ex-
tracted data: acquisition of domain knowledge for
populating the knowledge bases (left side in Fig. 1),
and automatic post-validation of extracted facts for
detecting and reducing errors (upper right). Novel
features include the notion of confidence,5 and ag-
gregation of separate facts into outbreaks across
multiple reports, based on confidence.
Operating in the large is essential, because the
learning components in the system rely on the
availability of large amounts of data. Knowledge
3PLUS: Pattern-based Learning and Understanding System.
4ProMED, www.promedmail.org, is the Program for Mon-
itoring Emerging Diseases, of the International Society for In-
fectious Diseases. It is one of the most comprehensive sources
of reports about the spread of infectious epidemics around the
world, collected for over 10 years.
5Confidence for individual fields of extracted facts, and for
entire facts, is based on document-local and global information.
22
IE engine
Customization 
environment
Lexicon
Ontology
Patterns
Inference rules
Unsupervised 
learning
Extracted facts
Candidate knowledge
DB server
User query Response
publisher user
Data collection
Web server
customizer
Noise reduction/
Data correction/ 
Cross-validation
Other corpora
Text documents
Knowledge bases:
Figure 1: System architecture of ProMED-PLUS
acquisition, (Yangarber et al, 2002; Yangarber,
2003) requires a large corpus of domain-specific and
general-topic texts. On the other hand, automatic
error reduction requires a critical mass of extracted
facts. Tighter integration between IE and KDD com-
ponents, for mutual benefit, is advocated in recent
related research, e.g., (Nahm and Mooney, 2000;
McCallum and Jensen, 2003). In this system we
have demonstrated that redundancy in the extracted
data (despite the noise) can be leveraged to improve
quality, by analyzing global trends and correcting
erroneous fills which are due to local mis-analysis,
(Yangarber and Jokipii, 2005). For this kind of ap-
proach to work, it is necessary to aggregate over a
large body of extracted records.
The interface to the DB is accessible on-line
at doremi.cs.helsinki.fi/plus/ (lower-right
of Fig. 1). It allows the user to view, select and sort
the extracted outbreaks, as well as the individual in-
cidents that make up the aggregated outbreaks. All
facts in the database are linked back to the original
reports from which they were extracted. The dis-
tribution of the outbreaks may also be plotted and
queried through the Geographic Map view.
References
R. Grishman, S. Huttunen, and R. Yangarber. 2003. In-
formation extraction for enhanced access to disease
outbreak reports. J. of Biomed. Informatics, 35(4).
A. McCallum and D. Jensen. 2003. A note on the uni-
fication of information extraction and data mining us-
ing conditional-probability, relational models. In IJ-
CAI?03 Workshop on Learning Statistical Models from
Relational Data.
U. Y. Nahm and R. Mooney. 2000. A mutually beneficial
integration of data mining and information extraction.
In AAAI-2000, Austin, TX.
R. Yangarber and L. Jokipii. 2005. Redundancy-based
correction of automatically extracted facts. In Proc.
HLT-EMNLP 2005, Vancouver, Canada.
R. Yangarber, W. Lin, and R. Grishman. 2002. Un-
supervised learning of generalized names. In Proc.
COLING-2002, Taipei, Taiwan.
R. Yangarber. 2003. Counter-training in discovery of se-
mantic patterns. In Proc. ACL-2003, Sapporo, Japan.
23
