Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558?566,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Evaluating the Inferential Utility of Lexical-Semantic Resources
Shachar Mirkin, Ido Dagan, Eyal Shnarch
Computer Science Department, Bar-Ilan University
Ramat-Gan 52900, Israel
{mirkins,dagan,shey}@cs.biu.ac.il
Abstract
Lexical-semantic resources are used ex-
tensively for applied semantic inference,
yet a clear quantitative picture of their
current utility and limitations is largely
missing. We propose system- and
application-independent evaluation and
analysis methodologies for resources? per-
formance, and systematically apply them
to seven prominent resources. Our find-
ings identify the currently limited recall of
available resources, and indicate the po-
tential to improve performance by exam-
ining non-standard relation types and by
distilling the output of distributional meth-
ods. Further, our results stress the need
to include auxiliary information regarding
the lexical and logical contexts in which
a lexical inference is valid, as well as its
prior validity likelihood.
1 Introduction
Lexical information plays a major role in seman-
tic inference, as the meaning of one term is of-
ten inferred form another. Lexical-semantic re-
sources, which provide the needed knowledge for
lexical inference, are commonly utilized by ap-
plied inference systems (Giampiccolo et al, 2007)
and applications such as Information Retrieval and
Question Answering (Shah and Croft, 2004; Pasca
and Harabagiu, 2001). Beyond WordNet (Fell-
baum, 1998), a wide range of resources has been
developed and utilized, including extensions to
WordNet (Moldovan and Rus, 2001; Snow et al,
2006) and resources based on automatic distri-
butional similarity methods (Lin, 1998; Pantel
and Lin, 2002). Recently, Wikipedia is emerg-
ing as a source for extracting semantic relation-
ships (Suchanek et al, 2007; Kazama and Tori-
sawa, 2007).
As of today, only a partial comparative picture
is available regarding the actual utility and limi-
tations of available resources for lexical-semantic
inference. Works that do provide quantitative
information regarding resources utility have fo-
cused on few particular resources (Kouylekov and
Magnini, 2006; Roth and Sammons, 2007) and
evaluated their impact on a specific system. Most
often, works which utilized lexical resources do
not provide information about their isolated con-
tribution; rather, they only report overall per-
formance for systems in which lexical resources
serve as components.
Our paper provides a step towards clarify-
ing this picture. We propose a system- and
application-independent evaluation methodology
that isolates resources? performance, and sys-
tematically apply it to seven prominent lexical-
semantic resources. The evaluation and analysis
methodology is specified within the Textual En-
tailment framework, which has become popular in
recent years for modeling practical semantic infer-
ence in a generic manner (Dagan and Glickman,
2004). To that end, we assume certain definitions
that extend the textual entailment paradigm to the
lexical level.
The findings of our work provide useful insights
and suggested directions for two research com-
munities: developers of applied inference systems
and researchers addressing lexical acquisition and
resource construction. Beyond the quantitative
mapping of resources? performance, our analysis
points at issues concerning their effective utiliza-
tion and major characteristics. Even more impor-
tantly, the results highlight current gaps in exist-
ing resources and point at directions towards fill-
ing them. We show that the coverage of most
resources is quite limited, where a substantial
part of recall is attributable to semantic relations
that are typically not available to inference sys-
tems. Notably, distributional acquisition methods
558
are shown to provide many useful relationships
which are missing from other resources, but these
are embedded amongst many irrelevant ones. Ad-
ditionally, the results highlight the need to rep-
resent and inference over various aspects of con-
textual information, which affect the applicability
of lexical inferences. We suggest that these gaps
should be addressed by future research.
2 Sub-sentential Textual Entailment
Textual entailment captures the relation between a
text t and a textual statement (termed hypothesis)
h, such that a person reading t would infer that h
is most likely correct (Dagan et al, 2005).
The entailment relation has been defined insofar
in terms of truth values, assuming that h is a com-
plete sentence (proposition). However, there are
major aspects of inference that apply to the sub-
sentential level. First, in certain applications, the
target hypotheses are often sub-sentential. For ex-
ample, search queries in IR, which play the hy-
pothesis role from an entailment perspective, typ-
ically consist of a single term, like drug legaliza-
tion. Such sub-sentential hypotheses are not re-
garded naturally in terms of truth values and there-
fore do not fit well within the scope of the textual
entailment definition. Second, many entailment
models apply a compositional process, through
which they try to infer each sub-part of the hy-
pothesis from some parts of the text (Giampiccolo
et al, 2007).
Although inferences over sub-sentential ele-
ments are being applied in practice, so far there
are no standard definitions for entailment at sub-
sentential levels. To that end, and as a prerequisite
of our evaluation methodology and our analysis,
we first establish two relevant definitions for sub-
sentential entailment relations: (a) entailment of a
sub-sentential hypothesis by a text, and (b) entail-
ment of one lexical element by another.
2.1 Entailment of Sub-sentential Hypotheses
We first seek a definition that would capture the
entailment relationship between a text and a sub-
sentential hypothesis. A similar goal was ad-
dressed in (Glickman et al, 2006), who defined
the notion of lexical reference to model the fact
that in order to entail a hypothesis, the text has
to entail each non-compositional lexical element
within it. We suggest that a slight adaptation of
their definition is suitable to capture the notion of
entailment for any sub-sentential hypotheses, in-
cluding compositional ones:
Definition 1 A sub-sentential hypothesis h is en-
tailed by a text t if there is an explicit or implied
reference in t to a possible meaning of h.
For example, the sentence ?crude steel output
is likely to fall in 2000? entails the sub-sentential
hypotheses production, steel production and steel
output decrease.
Glickman et al, achieving good inter-annotator
agreement, empirically found that almost all non-
compositional terms in an entailed sentential hy-
pothesis are indeed referenced in the entailing text.
This finding suggests that the above definition is
consistent with the original definition of textual
entailment for sentential hypotheses and can thus
model compositional entailment inferences.
We use this definition in our annotation method-
ology described in Section 3.
2.2 Entailment between Lexical Elements
In the majority of cases, the reference to an
?atomic? (non-compositional) lexical element e in
h stems from a particular lexical element e? in t,
as in the example above where the word output
implies the meaning of production.
To identify this relationship, an entailment sys-
tem needs a knowledge resource that would spec-
ify that the meaning of e? implies the meaning of
e, at least in some contexts. We thus suggest the
following definition to capture this relationship be-
tween e? and e:
Definition 2 A lexical element e? entails another
lexical element e, denoted e??e, if there exist
some natural (non-anecdotal) texts containing e?
which entail e, such that the reference to the mean-
ing of e can be implied solely from the meaning of
e? in the text.
(Entailment of e by a text follows Definition 1).
We refer to this relation in this paper as lexical
entailment1, and call e? ? e a lexical entailment
rule. e? is referred to as the rule?s left hand side
(LHS) and e as its right hand side (RHS).
Currently there are no knowledge resources de-
signed specifically for lexical entailment model-
ing. Hence, the types of relationships they cap-
ture do not fully coincide with entailment infer-
ence needs. Thus, the definition suggests a spec-
ification for the rules that should be provided by
1Section 6 discusses other definitions of lexical entailment
559
a lexical entailment resource, following an oper-
ative rationale: a rule e? ? e should be included
in an entailment knowledge resource if it would be
needed, as part of a compositional process, to infer
the meaning of e from some natural texts. Based
on this definition, we perform an analysis of the re-
lationships included in lexical-semantic resources,
as described in Section 5.
A rule need not apply in all contexts, as long
as it is appropriate for some texts. Two contex-
tual aspects affect rule applicability. First is the
?lexical context? specifying the meanings of the
text?s words. A rules is applicable in a certain con-
text only when the intended sense of its LHS term
matches the sense of that term in the text. For ex-
ample, the application of the rule lay ? produce is
valid only in contexts where the producer is poul-
try and the products are eggs. This is a well known
issue observed, for instance, by Voorhees (1994).
A second contextual factor requiring validation
is the ?logical context?. The logical context de-
termines the monotonicity of the LHS and is in-
duced by logical operators such as negation and
(explicit or implicit) quantifiers. For example, the
rule mammal ? whale may not be valid in most
cases, but is applicable in universally quantified
texts like ?mammals are warm-blooded?. This is-
sue has been rarely addressed in applied inference
systems (de Marneffe et al, 2006). The above
mentioned rules both comply with Definition 2
and should therefore be included in a lexical en-
tailment resource.
3 Evaluating Entailment Resources
Our evaluation goal is to assess the utility of
lexical-semantic resources as sources for entail-
ment rules. An inference system applies a rule by
inferring the rule?s RHS from texts that match its
LHS. Thus, the utility of a resource depends on the
performance of its rule applications rather than on
the proportion of correct rules it contains. A rule,
whether correct or incorrect, has insignificant ef-
fect on the resource?s utility if it rarely matches
texts in real application settings. Additionally,
correct rules might produce incorrect applications
when applied in inappropriate contexts. There-
fore, we use an instance-based evaluation method-
ology, which simulates rule applications by col-
lecting texts that contain rules? LHS and manually
assessing the correctness of their applications.
Systems typically handle lexical context either
implicitly or explicitly. Implicit context valida-
tion occurs when the different terms of a compos-
ite hypothesis disambiguate each other. For exam-
ple, the rule waterside ? bank is unlikely to be
applied when trying to infer the hypothesis bank
loans, since texts that match waterside are unlikely
to contain also the meaning of loan. Explicit meth-
ods, such as word-sense disambiguation or sense
matching, validate each rule application according
to the broader context in the text. Few systems
also address logical context validation by handling
quantifiers and negation. As we aim for a system-
independent comparison of resources, and explicit
approaches are not standardized yet within infer-
ence systems, our evaluation uses only implicit
context validation.
3.1 Evaluation Methodology
Figure 1: Evaluation methodology flow chart
The input for our evaluation methodology is a
lexical-semantic resource R, which contains lex-
ical entailment rules. We evaluate R?s utility by
testing how useful it is for inferring a sample of
test hypotheses H from a corpus. Each hypothesis
in H contains more than one lexical element in or-
der to provide implicit context validation for rule
applications, e.g. h: water pollution.
We next describe the steps of our evaluation
methodology, as illustrated in Figure 1. We refer
to the examples in the figure when needed:
1) Fetch rules: For each h ? H and each
lexical element e ? h (e.g. water), we fetch all
rules e? ? e in R that might be applied to entail e
(e.g. lake ? water).
2) Generate intermediate hypotheses h?:
For each rule r: e? ? e, we generate an intermedi-
ate hypothesis h? by replacing e in h with e? (e.g.
560
h?1: lake pollution). From a text t entailing h
?, h
can be further entailed by the single application of
r. We thus simulate the process by which an en-
tailment system would infer h from t using r.
3) Retrieve matching texts: For each h? we
retrieve from a corpus all texts that contain the
lemmatized words of h? (not necessarily as a sin-
gle phrase). These texts may entail h?. We dis-
card texts that also match h since entailing h from
them might not require the application of any rule
from the evaluated resource. In our example, the
retrieved texts contain lake and pollution but do
not contain water.
4) Annotation: A sample of the retrieved texts
is presented to human annotators. The annotators
are asked to answer the following two questions
for each text, simulating the typical inference pro-
cess of an entailment system:
a) Does t entail h?? If t does not entail h?
then the text would not provide a useful example
for the application of r. For instance, t1 (in Fig-
ure 1) does not entail h?1 and thus we cannot de-
duce h from it by applying the rule r. Such texts
are discarded from further evaluation.
b) Does t entail h? If t is annotated as en-
tailing h?, an entailment system would then infer
h from h? by applying r. If h is not entailed from
t even though h? is, the rule application is consid-
ered invalid. For instance, t2 does not entail h even
though it entails h?2. Indeed, the application of r2:
*soil ? water 2, from which h?2 was constructed,
yields incorrect inference. If the answer is ?yes?,
as in the case of t3, the application of r for t is
considered valid.
The above process yields a sample of annotated
rule applications for each test hypothesis, from
which we can measure resources performance, as
described in Section 5.
4 Experimental Setting
4.1 Dataset and Annotation
Current available state-of-the-art lexical-semantic
resources mainly deal with nouns. Therefore, we
used nominal hypotheses for our experiment3.
We chose TREC 1-8 (excluding 4) as our test
corpus and randomly sampled 25 ad-hoc queries
of two-word compounds as our hypotheses. We
did not use longer hypotheses to ensure that
2The asterisk marks an incorrect rule.
3We suggest that the definitions and methodologies can be
applied for other parts of speech as well.
enough texts containing the intermediate hypothe-
ses are found in the corpus. For annotation sim-
plicity, we retrieved single sentences as our texts.
For each rule applied for an hypothesis h, we
sampled 10 sentences from the sentences retrieved
for that rule. As a baseline, we also sampled 10
sentences for each original hypothesis h in which
both words of h are found. In total, 1550 unique
sentences were sampled and annotated by two an-
notators.
To assess the validity of our evaluation method-
ology, the annotators first judged a sample of 220
sentences. The Kappa scores for inter-annotator
agreement were 0.74 and 0.64 for judging h? and
h, respectively. These figures correspond to sub-
stantial agreement (Landis and Koch, 1997) and
are comparable with related semantic annotations
(Szpektor et al, 2007; Bhagat et al, 2007).
4.2 Lexical-Semantic Resources
We evaluated the following resources:
WordNet (WNd): There is no clear agreement
regarding which set of WordNet relations is use-
ful for entailment inference. We therefore took a
conservative approach using only synonymy and
hyponymy rules, which typically comply with the
lexical entailment relation and are commonly used
by textual entailment systems, e.g. (Herrera et al,
2005; Bos and Markert, 2006). Given a term e,
we created a rule e? ? e for each e? amongst the
synonyms or direct hyponyms for all senses of e
in WordNet 3.0.
Snow (Snow30k): Snow et al (2006) pre-
sented a probabilistic model for taxonomy induc-
tion which considers as features paths in parse
trees between related taxonomy nodes. They show
that the best performing taxonomy was the one
adding 30,000 hyponyms to WordNet. We created
an entailment rule for each new hyponym added to
WordNet by their algorithm4.
LCC?s extended WordNet (XWN?): In
(Moldovan and Rus, 2001) WordNet glosses were
transformed into logical form axioms. From this
representation we created a rule e? ? e for each e?
in the gloss which was tagged as referring to the
same entity as e.
CBC: A knowledgebase of labeled clusters gen-
erated by the statistical clustering and labeling al-
gorithms in (Pantel and Lin, 2002; Pantel and
4Available at http://ai.stanford.edu/? rion/swn
561
Ravichandran, 2004)5. Given a cluster label e, an
entailment rule e? ? e is created for each member
e? of the cluster.
Lin Dependency Similarity (Lin-dep): A
distributional word similarity resource based on
syntactic-dependency features (Lin, 1998). Given
a term e and its list of similar terms, we construct
for each e? in the list the rule e? ? e. This resource
was previously used in textual entailment engines,
e.g. (Roth and Sammons, 2007).
Lin Proximity Similarity (Lin-prox): A
knowledgebase of terms with their cooccurrence-
based distributionally similar terms. Rules are cre-
ated from this resource as from the previous one6.
Wikipedia first sentence (WikiFS): Kazama
and Torisawa (2007) used Wikipedia as an exter-
nal knowledge to improve Named Entity Recog-
nition. Using the first step of their algorithm, we
extracted from the first sentence of each page a
noun that appears in a is-a pattern referring to the
title. For each such pair we constructed a rule title
? noun (e.g. Michelle Pfeiffer ? actress).
The above resources represent various meth-
ods for detecting semantic relatedness between
words: Manually and semi-automatically con-
structed (WNd and XWN?, respectively), automat-
ically constructed based on a lexical-syntactic pat-
tern (WikiFS), distributional methods (Lin-dep and
Lin-prox) and combinations of pattern-based and
distributional methods (CBC and Snow30k).
5 Results and Analysis
The results and analysis described in this section
reveal new aspects concerning the utility of re-
sources for lexical entailment, and experimentally
quantify several intuitively-accepted notions re-
garding these resources and the lexical entailment
relation. Overall, our findings highlight where ef-
forts in developing future resources and inference
systems should be invested.
5.1 Resources Performance
Each resource was evaluated using two measures -
Precision and Recall-share, macro averaged over
all hypotheses. The results achieved for each re-
source are summarized in Table 1.
5Kindly provided to us by Patrick Pantel.
6Lin?s resources were downloaded from:
http://www.cs.ualberta.ca/? lindek/demos.htm
Resource Precision (%) Recall-share (%)
Snow30k 56 8
WNd 55 24
XWN? 51 9
WikiFS 45 7
CBC 33 9
Lin-dep 28 45
Lin-prox 24 36
Table 1: Lexical resources performance
5.1.1 Precision
The Precision of a resource R is the percentage of
valid rule applications for the resource. It is esti-
mated by the percentage of texts entailing h from
those that entail h?: countR(entailing h=yes)countR(entailing h?=yes) .
Not surprisingly, resources such as WNd, XWN?
or WikiFS achieved relatively high precision
scores, due to their accurate construction meth-
ods. In contrast, Lin?s distributional resources are
not designed to include lexical entailment relation-
ships. They provide pairs of contextually simi-
lar words, of which many have non-entailing rela-
tionships, such as co-hyponyms7 (e.g. *doctor ?
journalist) or topically-related words, such as *ra-
diotherapy ? outpatient. Hence their relatively
low precision.
One visible outcome is the large gap between
the perceived high accuracy of resources con-
structed by accurate methods, most notably WNd,
and their performance in practice. This finding
emphasizes the need for instance-based evalua-
tions, which capture the ?real? contribution of a
resource. To better understand the reasons for
this gap we further assessed the three factors
that contribute to incorrect applications: incorrect
rules, lexical context and logical context (see Sec-
tion 2.2). This analysis is presented in Table 2.
From Table 2 we see that the gap for accurate
resources is mainly caused by applications of cor-
rect rules in inappropriate contexts. More inter-
estingly, the information in the table allows us to
asses the lexical ?context-sensitivity? of resources.
When considering only the COR-LEX rules to re-
calculate resources precision, we find that Lin-dep
achieves precision of 71% ( 15%15%+6% ), while WN
d
yields only 56% ( 55%55%+44% ). This result indicates
that correct Lin-dep rules are less sensitive to lexi-
cal context, meaning that their prior likelihoods to
7a.k.a. sister terms or coordinate terms
562
(%)
Invalid Rule Applications Valid Rule Applications
INCOR COR-LOG COR-LEX Total INCOR COR-LOG COR-LEX Total (P)
WNd 1 0 44 45 0 0 55 55
WikiFS 13 0 42 55 3 0 42 45
XWN? 19 0 30 49 0 0 51 51
Snow30k 23 0 21 44 0 0 56 56
CBC 51 12 4 67 14 0 19 33
Lin-prox 59 4 13 76 8 3 13 24
Lin-dep 61 5 6 72 9 4 15 28
Table 2: The distribution of invalid and valid rule applications by rule types: incorrect rules (INCOR), correct rules requiring
?logical context? validation (COR-LOG), and correct rules requiring ?lexical context? matching (COR-LEX). The numbers of each
resource?s valid applications add up to the resource?s precision.
be correct are higher. This is explained by the fact
that Lin-dep?s rules are calculated across multiple
contexts and therefore capture the more frequent
usages of words. WordNet, on the other hand, in-
cludes many anecdotal rules whose application is
rare, and thus is very sensitive to context. Simi-
larly, WikiFS turns out to be very context-sensitive.
This resource contains many rules for polysemous
proper nouns that are scarce in their proper noun
sense, e.g. Captive ? computer game. Snow30k,
when applied with the same calculation, reaches
73%, which explains how it achieved a compara-
ble result to WNd, even though it contains many
incorrect rules in comparison to WNd.
5.1.2 Recall
Absolute recall cannot be measured since the total
number of texts in the corpus that entail each hy-
pothesis is unknown. Instead, we measure recall-
share, the contribution of each resource to recall
relative to matching only the words of the origi-
nal hypothesis without any rules. We denote by
yield(h) the number of texts that match h directly
and are annotated as entailing h. This figure is es-
timated by the number of sampled texts annotated
as entailing h multiplied by the sampling propor-
tion. In the same fashion, for each resource R,
we estimate the number of texts entailing h ob-
tained through entailment rules of the resource R,
denoted yieldR(h). Recall-share of R for h is the
proportion of the yield obtained by the resource?s
rules relative to the overall yield with and without
the rules from R: yieldR(h)yield(h)+yieldR(h) .
From Table 1 we see that along with their rela-
tively low precision, Lin?s resources? recall greatly
surpasses that of any other resource, including
WordNet8. The rest of the resources are even infe-
8A preliminary experiment we conducted showed that re-
rior to WNd in that respect, indicating their limited
utility for inference systems.
As expected, synonyms and hyponyms in Word-
Net contributed a noticeable portion to recall in all
resources. Additional correct rules correspond to
hyponyms and synonyms missing from WordNet,
many of them proper names and some slang ex-
pressions. These rules were mainly provided by
WikiFS and Snow30k, significantly supplementing
WordNet, whose HasInstance relation is quite par-
tial. However, there are other interesting types of
entailment relations contributing to recall. These
are discussed in Sections 5.2 and 5.3. Examples
for various rule types are found in Table 3.
5.1.3 Valid Applications of Incorrect Rules
We observed that many entailing sentences were
retrieved by inherently incorrect rules in the distri-
butional resources. Analysis of these rules reveals
they were matched in entailing texts when the LHS
has noticeable statistical correlation with another
term in the text that does entail the RHS. For ex-
ample, for the hypothesis wildlife extinction, the
rule *species ? extinction yielded valid applica-
tions in contexts about threatened or endangered
species. Has the resource included a rule between
the entailing term in the text and the RHS, the
entailing text would have been matched without
needing the incorrect rule.
These correlations accounted for nearly a third
of Lin resources? recall. Nonetheless, in princi-
ple, we suggest that such rules, which do not con-
form with Definition 2, should not be included in a
lexical entailment resource, since they also cause
invalid rule applications, while the entailing texts
they retrieve will hopefully be matched by addi-
call does not dramatically improve when using the entire hy-
ponymy subtree from WordNet.
563
Type Correct Rules
HYPO Shevardnadze ? official Snow30k
ANT efficacy ? ineffectiveness Lin-dep
HOLO government ? official Lin-prox
HYPER arms ? gun Lin-prox
? childbirth ? motherhood Lin-dep
? mortgage ? bank Lin-prox
? Captive ? computer WikiFS
? negligence ? failure CBC
? beatification ? pope XWN?
Type Incorrect Rules
CO-HYP alcohol ? cigarette CBC
? radiotherapy ? outpatient Lin-dep
? teen-ager ? gun Snow30k
? basic ? paper WikiFS
? species ? extinction Lin-prox
Table 3: Examples of lexical resources rules by types.
HYPO: hyponymy, HYPER: hypernymy (class entailment of
its members), HOLO: holonymy, ANT: antonymy, CO-HYP: co-
hyponymy. The non-categorized relations do not correspond
to any WordNet relation.
tional correct rules in a more comprehensive re-
source.
5.2 Non-standard Entailment Relations
An important finding of our analysis is that some
less standard entailment relationships have a con-
siderable impact on recall (see Table 3). These
rules, which comply with Definition 2 but do
not conform to any WordNet relation type, were
mainly contributed by Lin?s distributional re-
sources and to a smaller degree are also included
in XWN?. In Lin-dep, for example, they accounted
for approximately a third of the recall.
Among the finer grained relations we identi-
fied in this set are topical entailment (e.g. IBM
as the company entailing the topic computers),
consequential relationships (pregnancy?mother-
hood) and an entailment of inherent arguments by
a predicate, or of essential participants by a sce-
nario description, e.g. beatification ? pope. A
comprehensive typology of these relationships re-
quires further investigation, as well as the identi-
fication and development of additional resources
from which they can be extracted.
As opposed to hyponymy and synonymy rules,
these rules are typically non-substitutable, i.e. the
RHS of the rule is unlikely to have the exact same
role in the text as the LHS. Many inference sys-
tems perform rule-based transformations, substi-
tuting the LHS by the RHS. This finding suggests
that different methods may be required to utilize
such rules for inference.
5.3 Logical Context
WordNet relations other than synonyms and hy-
ponyms, including antonyms, holonyms and hy-
pernyms (see Table 3), contributed a noticeable
share of valid rule applications for some resources.
Following common practice, these relations are
missing by construction from the other resources.
As shown in Table 2 (COR-LOG columns), such
relations accounted for a seventh of Lin-dep?s
valid rule applications, as much as was the con-
tribution of hyponyms and synonyms to this re-
source?s recall. Yet, using these rules resulted with
more erroneous applications than correct ones. As
discussed in Section 2.2, the rules induced by
these relations do conform with our lexical entail-
ment definition. However, a valid application of
these rules requires certain logical conditions to
occur, which is not the common case. We thus
suggest that such rules are included in lexical en-
tailment resources, as long as they are marked
properly by their types, allowing inference sys-
tems to utilize them only when appropriate mech-
anisms for handling logical context are in place.
5.4 Rules Priors
In Section 5.1.1 we observed that some resources
are highly sensitive to context. Hence, when con-
sidering the validity of a rule?s application, two
factors should be regarded: the actual context in
which the rule is to be applied, as well as the rule?s
prior likelihood to be valid in an arbitrary con-
text. Somewhat indicative, yet mostly indirect, in-
formation about rules? priors is contained in some
resources. This includes sense ranks in WordNet,
SemCor statistics (Miller et al, 1993), and similar-
ity scores and rankings in Lin?s resources. Infer-
ence systems often incorporated this information,
typically as top-k or threshold-based filters (Pan-
tel and Lin, 2003; Roth and Sammons, 2007). By
empirically assessing the effect of several such fil-
ters in our setting, we found that this type of data
is indeed informative in the sense that precision
increases as the threshold rises. Yet, no specific
filters were found to improve results in terms of
F1 score (where recall is measured relatively to
the yield of the unfiltered resource) due to a sig-
nificant drop in relative recall. For example, Lin-
564
prox loses more than 40% of its recall when only
the top-50 rules for each hypothesis are exploited,
and using only the first sense of WNd costs the re-
source over 60% of its recall. We thus suggest a
better strategy might be to combine the prior in-
formation with context matching scores in order
to obtain overall likelihood scores for rule appli-
cations, as in (Szpektor et al, 2008). Furthermore,
resources should include explicit information re-
garding the prior likelihoods of of their rules.
5.5 Operative Conclusions
Our findings highlight the currently limited re-
call of available resources for lexical inference.
The higher recall of Lin?s resources indicates
that many more entailment relationships can be
acquired, particularly when considering distribu-
tional evidence. Yet, available distributional ac-
quisition methods are not geared for lexical entail-
ment. This suggests the need to develop acqui-
sition methods for dedicated and more extensive
knowledge resources that would subsume the cor-
rect rules found by current distributional methods.
Furthermore, substantially better recall may be ob-
tained by acquiring non-standard lexical entail-
ment relationships, as discussed in Section 5.2, for
which a comprehensive typology is still needed.
At the same time, transformation-based inference
systems would need to handle these kinds of rules,
which are usually non-substitutable. Our results
also quantify and stress earlier findings regarding
the severe degradation in precision when rules are
applied in inappropriate contexts. This highlights
the need for resources to provide explicit informa-
tion about the suitable lexical and logical contexts
in which an entailment rule is applicable. In par-
allel, methods should be developed to utilize such
contextual information within inference systems.
Additional auxiliary information needed in lexical
resources is the prior likelihood for a given rule to
be correct in an arbitrary context.
6 Related Work
Several prior works defined lexical entailment.
WordNet?s lexical entailment is a relationship be-
tween verbs only, defined for propositions (Fell-
baum, 1998). Geffet and Dagan (2004) defined
substitutable lexical entailment as a relation be-
tween substitutable terms. We find this definition
too restrictive as non-substitutable rules may also
be useful for entailment inference. Examples are
breastfeeding ? baby and hospital ? medical.
Hence, Definition 2 is more broadly applicable for
defining the desired contents of lexical entailment
resources. We empirically observed that the rules
satisfying their definition are a proper subset of
the rules covered by our definition. Dagan and
Glickman (2004) referred to entailment at the sub-
sentential level by assigning truth values to sub-
propositional text fragments through their existen-
tial meaning. We find this criterion too permissive.
For instance, the existence of country implies the
existence of its flag. Yet, the meaning of flag is
typically not implied by country.
Previous works assessing rule application via
human annotation include (Pantel et al, 2007;
Szpektor et al, 2007), which evaluate acquisition
methods for lexical-syntactic rules. They posed an
additional question to the annotators asking them
to filter out invalid contexts. In our methodology
implicit context matching for the full hypothesis
was applied instead. Other related instance-based
evaluations (Giuliano and Gliozzo, 2007; Connor
and Roth, 2007) performed lexical substitutions,
but did not handle the non-substitutable cases.
7 Conclusions
This paper provides several methodological and
empirical contributions. We presented a novel
evaluation methodology for the utility of lexical-
semantic resources for semantic inference. To that
end we proposed definitions for entailment at sub-
sentential levels, addressing a gap in the textual
entailment framework. Our evaluation and analy-
sis provide a first quantitative comparative assess-
ment of the isolated utility of a range of prominent
potential resources for entailment rules. We have
shown various factors affecting rule applicability
and resources performance, while providing oper-
ative suggestions to address them in future infer-
ence systems and resources.
Acknowledgments
The authors would like to thank Naomi Frankel
and Iddo Greental for their excellent annotation
work, as well as Roy Bar-Haim and Idan Szpektor
for helpful discussion and advice. This work was
partially supported by the Negev Consortium of
the Israeli Ministry of Industry, Trade and Labor,
the PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1095/05.
565
References
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
J. Bos and K. Markert. 2006. When logical infer-
ence helps determining textual entailment (and when
it doesn?t). In Proceedings of the Second PASCAL
RTE Challenge.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan and Oren Glickman. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Joaquin Quinonero Candela, Ido Da-
gan, Bernardo Magnini, and Florence d?Alche? Buc,
editors, MLCW, Lecture Notes in Computer Science.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D. Manning. 2006. Learning to distinguish
valid textual entailments. In Proceedings of the Sec-
ond PASCAL RTE Challenge.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature vector
quality and distributional similarity. In Proceedings
of COLING.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proceedings of EMNLP-CoNLL.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo.
2005. Textual entailment recognition based on de-
pendency analysis and wordnet. In Proceedings of
the First PASCAL RTE Challenge.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
Milen Kouylekov and Bernardo Magnini. 2006. Build-
ing a large-scale repository of textual entailment
rules. In Proceedings of LREC.
J. R. Landis and G. G. Koch. 1997. The measurements
of observer agreement for categorical data. In Bio-
metrics, pages 33:159?174.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of HLT.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of ACM
SIGKDD.
Patrick Pantel and Dekang Lin. 2003. Automatically
discovering word senses. In Proceedings of NAACL.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT-NAACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of HLT.
Marius Pasca and Sanda M. Harabagiu. 2001. The in-
formative role of wordnet in open-domain question
answering. In Proceedings of NAACL Workshop on
WordNet and Other Lexical Resources.
Dan Roth and Mark Sammons. 2007. Semantic and
logical inference model for textual entailment. In
Proceedings of ACL-WTEP Workshop.
Chirag Shah and Bruce W. Croft. 2004. Evaluating
high accuracy retrieval techniques. In Proceedings
of SIGIR.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
566
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 791?799,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Source-Language Entailment Modeling for Translating Unknown Terms
Shachar Mirkin?, Lucia Specia?, Nicola Cancedda?, Ido Dagan?, Marc Dymetman?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Xerox Research Centre Europe
{mirkins,dagan,szpekti}@cs.biu.ac.il
{lucia.specia,nicola.cancedda,marc.dymetman}@xrce.xerox.com
Abstract
This paper addresses the task of handling
unknown terms in SMT. We propose us-
ing source-language monolingual models
and resources to paraphrase the source text
prior to translation. We further present a
conceptual extension to prior work by al-
lowing translations of entailed texts rather
than paraphrases only. A method for
performing this process efficiently is pre-
sented and applied to some 2500 sentences
with unknown terms. Our experiments
show that the proposed approach substan-
tially increases the number of properly
translated texts.
1 Introduction
Machine Translation systems frequently encounter
terms they are not able to translate due to some
missing knowledge. For instance, a Statistical Ma-
chine Translation (SMT) system translating the
sentence ?Cisco filed a lawsuit against Apple for
patent violation? may lack words like filed and
lawsuit in its phrase table. The problem is espe-
cially severe for languages for which parallel cor-
pora are scarce, or in the common scenario when
the SMT system is used to translate texts of a do-
main different from the one it was trained on.
A previously suggested solution (Callison-
Burch et al, 2006) is to learn paraphrases of
source terms from multilingual (parallel) corpora,
and expand the phrase table with these para-
phrases1. Such solutions could potentially yield a
paraphrased sentence like ?Cisco sued Apple for
patent violation?, although their dependence on
bilingual resources limits their utility.
In this paper we propose an approach that con-
sists in directly replacing unknown source terms,
1As common in the literature, we use the term para-
phrases to refer to texts of equivalent meaning, of any length
from single words (synonyms) up to complete sentences.
using source-language resources and models in or-
der to achieve two goals.
The first goal is coverage increase. The avail-
ability of bilingual corpora, from which para-
phrases can be learnt, is in many cases limited.
On the other hand, monolingual resources and
methods for extracting paraphrases from monolin-
gual corpora are more readily available. These
include manually constructed resources, such as
WordNet (Fellbaum, 1998), and automatic meth-
ods for paraphrases acquisition, such as DIRT (Lin
and Pantel, 2001). However, such resources have
not been applied yet to the problem of substitut-
ing unknown terms in SMT. We suggest that by
using such monolingual resources we could pro-
vide paraphrases for a larger number of texts with
unknown terms, thus increasing the overall cover-
age of the SMT system, i.e. the number of texts it
properly translates.
Even with larger paraphrase resources, we may
encounter texts in which not all unknown terms are
successfully handled through paraphrasing, which
often results in poor translations (see Section 2.1).
To further increase coverage, we therefore pro-
pose to generate and translate texts that convey a
somewhat more general meaning than the original
source text. For example, using such approach,
the following text could be generated: ?Cisco ac-
cused Apple of patent violation?. Although less in-
formative than the original, a translation for such
texts may be useful. Such non-symmetric relation-
ships (as between filed a lawsuit and accused) are
difficult to learn from parallel corpora and there-
fore monolingual resources are more appropriate
for this purpose.
The second goal we wish to accomplish by
employing source-language resources is to rank
the alternative generated texts. This goal can be
achieved by using context-models on the source
language prior to translation. This has two advan-
tages. First, the ranking allows us to prune some
791
candidates before supplying them to the transla-
tion engine, thus improving translation efficiency.
Second, the ranking may be combined with target
language information in order to choose the best
translation, thus improving translation quality.
We position the problem of generating alterna-
tive texts for translation within the Textual Entail-
ment (TE) framework (Giampiccolo et al, 2007).
TE provides a generic way for handling language
variability, identifying when the meaning of one
text is entailed by the other (i.e. the meaning of
the entailed text can be inferred from the mean-
ing of the entailing one). When the meanings of
two texts are equivalent (paraphrase), entailment
is mutual. Typically, a more general version of
a certain text is entailed by it. Hence, through TE
we can formalize the generation of both equivalent
and more general texts for the source text. When
possible, a paraphrase is used. Otherwise, an alter-
native text whose meaning is entailed by the orig-
inal source is generated and translated.
We assess our approach by applying an SMT
system to a text domain that is different from the
one used to train the system. We use WordNet
as a source language resource for entailment rela-
tionships and several common statistical context-
models for selecting the best generated texts to be
sent to translation. We show that the use of source
language resources, and in particular the extension
to non-symmetric textual entailment relationships,
is useful for substantially increasing the amount of
texts that are properly translated. This increase is
observed relative to both using paraphrases pro-
duced by the same resource (WordNet) and us-
ing paraphrases produced from multilingual paral-
lel corpora. We demonstrate that by using simple
context-models on the source, efficiency can be
improved, while translation quality is maintained.
We believe that with the use of more sophisticated
context-models further quality improvement can
be achieved.
2 Background
2.1 Unknown Terms
A very common problem faced by machine trans-
lation systems is the need to translate terms (words
or multi-word expressions) that are not found in
the system?s lexicon or phrase table. The reasons
for such unknown terms in SMT systems include
scarcity of training material and the application
of the system to text domains that differ from the
ones used for training.
In SMT, when unknown terms are found in the
source text, the systems usually omit or copy them
literally into the target. Though copying the source
words can be of some help to the reader if the
unknown word has a cognate in the target lan-
guage, this will not happen in the most general
scenario where, for instance, languages use dif-
ferent scripts. In addition, the presence of a sin-
gle unknown term often affects the translation of
wider portions of text, inducing errors in both lex-
ical selection and ordering. This phenomenon is
demonstrated in the following sentences, where
the translation of the English sentence (1) is ac-
ceptable only when the unknown word (in bold) is
replaced with a translatable paraphrase (3):
1. ?. . . , despite bearing the heavy burden of the
unemployed 10% or more of the labor force.?
2. ?. . . , malgre? la lourde charge de compte le
10% ou plus de cho?meurs labor la force .?
3. ?. . . , malgre? la lourde charge des cho?meurs
de 10% ou plus de la force du travail.?
Several approaches have been proposed to deal
with unknown terms in SMT systems, rather than
omitting or copying the terms. For example, (Eck
et al, 2008) replace the unknown terms in the
source text by their definition in a monolingual
dictionary, which can be useful for gisting. To
translate across languages with different alpha-
bets approaches such as (Knight and Graehl, 1997;
Habash, 2008) use transliteration techniques to
tackle proper nouns and technical terms. For trans-
lation from highly inflected languages, certain ap-
proaches rely on some form of lexical approx-
imation or morphological analysis (Koehn and
Knight, 2003; Yang and Kirchhoff, 2006; Langlais
and Patry, 2007; Arora et al, 2008). Although
these strategies yield gain in coverage and transla-
tion quality, they only account for unknown terms
that should be transliterated or are variations of
known ones.
2.2 Paraphrasing in MT
A recent strategy to broadly deal with the prob-
lem of unknown terms is to paraphrase the source
text with terms whose translation is known to
the system, using paraphrases learnt from multi-
lingual corpora, typically involving at least one
?pivot? language different from the target lan-
guage of immediate interest (Callison-Burch et
792
al., 2006; Cohn and Lapata, 2007; Zhao et al,
2008; Callison-Burch, 2008; Guzma?n and Gar-
rido, 2008). The procedure to extract paraphrases
in these approaches is similar to standard phrase
extraction in SMT systems, and therefore a large
amount of additional parallel corpus is required.
Moreover, as discussed in Section 5, when un-
known texts are not from the same domain as the
SMT training corpus, it is likely that paraphrases
found through such methods will yield misleading
translations.
Bond et al (2008) use grammars to paraphrase
the whole source sentence, covering aspects like
word order and minor lexical variations (tenses
etc.), but not content words. The paraphrases are
added to the source side of the corpus and the cor-
responding target sentences are duplicated. This,
however, may yield distorted probability estimates
in the phrase table, since these were not computed
from parallel data.
The main use of monolingual paraphrases in
MT to date has been for evaluation. For exam-
ple, (Kauchak and Barzilay, 2006) paraphrase ref-
erences to make them closer to the system transla-
tion in order to obtain more reliable results when
using automatic evaluation metrics like BLEU
(Papineni et al, 2002).
2.3 Textual Entailment and Entailment Rules
Textual Entailment (TE) has recently become a
prominent paradigm for modeling semantic infer-
ence, capturing the needs of a broad range of
text understanding applications (Giampiccolo et
al., 2007). Yet, its application to SMT has been so
far limited to MT evaluation (Pado et al, 2009).
TE defines a directional relation between two
texts, where the meaning of the entailed text (hy-
pothesis, h) can be inferred from the meaning of
the entailing text, t. Under this paradigm, para-
phrases are a special case of the entailment rela-
tion, when the relation is symmetric (the texts en-
tail each other). Otherwise, we say that one text
directionally entails the other.
A common practice for proving (or generating)
h from t is to apply entailment rules to t. An
entailment rule, denoted LHS ? RHS, specifies
an entailment relation between two text fragments
(the Left- and Right- Hand Sides), possibly with
variables (e.g. build X in Y ? X is completed
in Y ). A paraphrasing rule is denoted with ?.
When a rule is applied to a text, a new text is in-
ferred, where the matched LHS is replaced with the
RHS. For example, the rule skyscraper? building
is applied to ?The world?s tallest skyscraper was
completed in Taiwan? to infer ?The world?s tallest
building was completed in Taiwan?. In this work,
we employ lexical entailment rules, i.e. rules with-
out variables. Various resources for lexical rules
are available, and the prominent one is WordNet
(Fellbaum, 1998), which has been used in virtu-
ally all TE systems (Giampiccolo et al, 2007).
Typically, a rule application is valid only under
specific contexts. For example, mouse ? rodent
should not be applied to ?Use the mouse to mark
your answers?. Context-models can be exploited
to validate the application of a rule to a text. In
such models, an explicit Word Sense Disambigua-
tion (WSD) is not necessarily required; rather, an
implicit sense-match is sought after (Dagan et al,
2006). Within the scope of our paper, rule ap-
plication is handled similarly to Lexical Substitu-
tion (McCarthy and Navigli, 2007), considering
the contextual relationship between the text and
the rule. However, in general, entailment rule ap-
plication addresses other aspects of context match-
ing as well (Szpektor et al, 2008).
3 Textual Entailment for Statistical
Machine Translation
Previous solutions for handling unknown terms in
a source text s augment the SMT system?s phrase
table based on multilingual corpora. This allows
indirectly paraphrasing s, when the SMT system
chooses to use a paraphrase included in the table
and produces a translation with the corresponding
target phrase for the unknown term.
We propose using monolingual paraphrasing
methods and resources for this task to obtain a
more extensive set of rules for paraphrasing the
source. These rules are then applied to s directly
to produce alternative versions of the source text
prior to the translation step. Moreover, further
coverage increase can be achieved by employing
directional entailment rules, when paraphrasing is
not possible, to generate more general texts for
translation.
Our approach, based on the textual entailment
framework, considers the newly generated texts as
entailed from the original one. Monolingual se-
mantic resources such as WordNet can provide en-
tailment rules required for both these symmetric
and asymmetric entailment relations.
793
Input: A text t with one or more unknown terms;
a monolingual resource of entailment rules;
k - maximal number of source alternatives to produce
Output: A translation of either (in order of preference):
a paraphrase of t OR a text entailed by t OR t itself
1. For each unknown term - fetch entailment rules:
(a) Fetch rules for paraphrasing; disregard rules
whose RHS is not in the phrase table
(b) If the set of rules is empty: fetch directional en-
tailment rules; disregard rules whose RHS is not
in the phrase table
2. Apply a context-model to compute a score for each rule
application
3. Compute total source score for each entailed text as a
combination of individual rule scores
4. Generate and translate the top-k entailed texts
5. If k > 1
(a) Apply target model to score the translation
(b) Compute final source-target score
6. Pick highest scoring translation
Figure 1: Scheme for handling unknown terms by using
monolingual resources through textual entailment
Through the process of applying entailment
rules to the source text, multiple alternatives of
entailed texts are generated. To rank the candi-
date texts we employ monolingual context-models
to provide scores for rule applications over the
source sentence. This can be used to (a) directly
select the text with the highest score, which can
then be translated, or (b) to select a subset of top
candidates to be translated, which will then be
ranked using the target language information as
well. This pruning reduces the load of the SMT
system, and allows for potential improvements in
translation quality by considering both source- and
target-language information.
The general scheme through which we achieve
these goals, which can be implemented using dif-
ferent context-models and scoring techniques, is
detailed in Figure 1. Details of our concrete im-
plementation are given in Section 4.
Preliminary analysis confirmed (as expected)
that readers prefer translations of paraphrases,
when available, over translations of directional en-
tailments. This consideration is therefore taken
into account in the proposed method.
The input is a text unit to be translated, such as a
sentence or paragraph, with one or more unknown
terms. For each unknown term we first fetch a
list of candidate rules for paraphrasing (e.g. syn-
onyms), where the unknown term is the LHS. For
example, if our unknown term is dodge, a possi-
ble candidate might be dodge ? circumvent. We
inflect the RHS to keep the original morphologi-
cal information of the unknown term and filter out
rules where the inflected RHS does not appear in
the phrase table (step 1a in Figure 1).
When no applicable rules for paraphrasing are
available (1b), we fetch directional entailment
rules (e.g. hypernymy rules such as dodge ?
avoid), and filter them in the same way as for para-
phrasing rules. To each set of rules for a given un-
known term we add the ?identity-rule?, to allow
leaving the unknown term unchanged, the correct
choice in cases of proper names, for example.
Next, we apply a context-model to compute an
applicability score of each rule to the source text
(step 2). An entailed text?s total score is the com-
bination (e.g. product, see Section 4) of the scores
of the rules used to produce it (3). A set of the
top-k entailed texts is then generated and sent for
translation (4).
If more than one alternative is produced by the
source model (and k > 1), a target model is ap-
plied on the selected set of translated texts (5a).
The combined source-target model score is a com-
bination of the scores of the source and target
models (5b). The final translation is selected to be
the one that yields the highest combined source-
target score (6). Note that setting k = 1 is equiva-
lent to using the source-language model alone.
Our algorithm validates the application of the
entailment rules at two stages ? before and af-
ter translation, through context-models applied at
each end. As the experiments will show in Sec-
tion 4, a large number of possible combinations of
entailment rules is a common scenario, and there-
fore using the source context models to reduce this
number plays an important role.
4 Experimental Setting
To assess our approach, we conducted a series of
experiments; in each experiment we applied the
scheme described in 3, changing only the mod-
els being used for scoring the generated and trans-
lated texts. The setting of these experiments is de-
scribed in what follows.
SMT data To produce sentences for our experi-
ments, we use Matrax (Simard et al, 2005), a stan-
dard phrase-based SMT system, with the excep-
tion that it allows gaps in phrases. We use approxi-
mately 1M sentence pairs from the English-French
794
Europarl corpus for training, and then translate a
test set of 5,859 English sentences from the News
corpus into French. Both resources are taken
from the shared translation task in WMT-2008
(Callison-Burch et al, 2008). Hence, we compare
our method in a setting where the training and test
data are from different domains, a common sce-
nario in the practical use of MT systems.
Of the 5,859 translated sentences, 2,494 contain
unknown terms (considering only sequences with
alphabetic symbols), summing up to 4,255 occur-
rences of unknown terms. 39% of the 2,494 sen-
tences contain more than a single unknown term.
Entailment resource We use WordNet 3.0 as
a resource for entailment rules. Paraphrases are
generated using synonyms. Directionally entailed
texts are created using hypernyms, which typically
conform with entailment. We do not rely on sense
information in WordNet. Hence, any other seman-
tic resource for entailment rules can be utilized.
Each sentence is tagged using the OpenNLP
POS tagger2. Entailment rules are applied for un-
known terms tagged as nouns, verbs, adjectives
and adverbs. The use of relations from WordNet
results in 1,071 sentences with applicable rules
(with phrase table entries) for the unknown terms
when using synonyms, and 1,643 when using both
synonyms and hypernyms, accounting for 43%
and 66% of the test sentences, respectively.
The number of alternative sentences generated
for each source text varies from 1 to 960 when
paraphrasing rules were applied, and reaches very
large numbers, up to 89,700 at the ?worst case?,
when all TE rules are employed, an average of 456
alternatives per sentence.
Scoring source texts We test our proposed
method using several context-models shown to
perform reasonably well in previous work:
? FREQ: The first model we use is a context-
independent baseline. A common useful
heuristic to pick an entailment rule is to se-
lect the candidate with the highest frequency
in the corpus (Mccarthy et al, 2004). In this
model, a rule?s score is the normalized num-
ber of occurrences of its RHS in the training
corpus, ignoring the context of the LHS.
? LSA: Latent Semantic Analysis (Deerwester
et al, 1990) is a well-known method for rep-
2http://opennlp.sourceforge.net
resenting the contextual usage of words based
on corpus statistics. We represented each
term by a normalized vector of the top 100
SVD dimensions, as described in (Gliozzo,
2005). This model measures the similarity
between the sentence words and the RHS in
the LSA space.
? NB: We implemented the unsupervised
Na??ve Bayes model described in (Glickman
et al, 2006) to estimate the probability that
the unknown term entails the RHS in the
given context. The estimation is based on
corpus co-occurrence statistics of the context
words with the RHS.
? LMS: This model generates the Language
Model probability of the RHS in the source.
We use 3-grams probabilities as produced by
the SRILM toolkit (Stolcke, 2002).
Finally, as a simple baseline, we generated a ran-
dom score for each rule application, RAND.
The score of each rule application by any of
the above models is normalized to the range (0,1].
To combine individual rule applications in a given
sentence, we use the product of their scores. The
monolingual data used for the models above is the
source side of the training parallel corpus.
Target-language scores On the target side we
used either a standard 3-gram language-model, de-
noted LMT, or the score assigned by the com-
plete SMT log-linear model, which includes the
language model as one of its components (SMT).
A pair of a source:target models comprises a
complete model for selecting the best translated
sentence, where the overall score is the product of
the scores of the two models.
We also applied several combinations of source
models, such as LSA combined with LMS, to take
advantage of their complementary strengths. Ad-
ditionally, we assessed our method with source-
only models, by setting the number of sentences to
be selected by the source model to one (k = 1).
5 Results
5.1 Manual Evaluation
To evaluate the translations produced using the
various source and target models and the different
rule-sets, we rely mostly on manual assessment,
since automatic MT evaluation metrics like BLEU
do not capture well the type of semantic variations
795
Model
Precision (%) Coverage (%)
PARAPH. TE PARAPH. TE
1 ?:SMT 75.8 73.1 32.5 48.1
2 NB:SMT 75.2 71.5 32.3 47.1
3 LSA:SMT 74.9 72.4 32.1 47.7
4 NB:? 74.7 71.1 32.1 46.8
5 LMS:LMT 73.8 70.2 31.7 46.3
6 FREQ:? 72.5 68.0 31.2 44.8
7 RAND 57.2 63.4 24.6 41.8
Table 1: Translation acceptance when using only para-
phrases and when using all entailment rules. ?:? indicates
which model is applied to the source (left side) and which to
the target language (right side).
generated in our experiments, particularly at the
sentence level.
In the manual evaluation, two native speakers
of the target language judged whether each trans-
lation preserves the meaning of its reference sen-
tence, marking it as acceptable or unacceptable.
From the sentences for which rules were applica-
ble, we randomly selected a sample of sentences
for each annotator, allowing for some overlap-
ping for agreement analysis. In total, the transla-
tions of 1,014 unique source sentences were man-
ually annotated, of which 453 were produced us-
ing only hypernyms (no paraphrases were appli-
cable). When a sentence was annotated by both
annotators, one annotation was picked randomly.
Inter-annotator agreement was measured by the
percentage of sentences the annotators agreed on,
as well as via the Kappa measure (Cohen, 1960).
For different models, the agreement rate varied
from 67% to 78% (72% overall), and the Kappa
value ranged from 0.34 to 0.55, which is compa-
rable to figures reported for other standard SMT
evaluation metrics (Callison-Burch et al, 2008).
Translation with TE For each model m, we
measured Precisionm, the percentage of accept-
able translations out of all sampled translations.
Precisionm was measured both when using only
paraphrases (PARAPH.) and when using all entail-
ment rules (TE). We also measured Coveragem,
the percentage of sentences with acceptable trans-
lations, Am, out of all sentences (2,494). As
our annotators evaluated only a sample of sen-
tences, Am is estimated as the model?s total num-
ber of sentences with applicable rules, Sm, mul-
tiplied by the model?s Precision (Sm was 1,071
for paraphrases and 1,643 for entailment rules):
Coveragem = Sm?Precisionm2,494 .
Table 1 presents the results of several source-
target combinations when using only paraphrases
and when also using directional entailment rules.
When all rules are used, a substantial improve-
ment in coverage is consistently obtained across
all models, reaching a relative increase of 50%
over paraphrases only, while just a slight decrease
in precision is observed (see Section 5.3 for some
error analysis). This confirms our hypothesis that
directional entailment rules can be very useful for
replacing unknown terms.
For the combination of source-target models,
the value of k is set depending on which rule-set
is used. Preliminary analysis showed that k = 5
is sufficient when only paraphrases are used and
k = 20 when directional entailment rules are also
considered.
We measured statistical significance between
different models for precision of the TE re-
sults according to the Wilcoxon signed ranks test
(Wilcoxon, 1945). Models 1-6 in Table 1 are sig-
nificantly better than the RAND baseline (p <
0.03), and models 1-3 are significantly better than
model 6 (p < 0.05). The difference between
?:SMT and NB:SMT or LSA:SMT is not statisti-
cally significant.
The results in Table 1 therefore suggest that
taking a source model into account preserves the
quality of translation. Furthermore, the quality is
maintained even when source models? selections
are restricted to a rather small top-k ranks, at a
lower computational cost (for the models combin-
ing source and target, like NB:SMT or LSA:SMT).
This is particularly relevant for on-demand MT
systems, where time is an issue. For such systems,
using this source-language based pruning method-
ology will yield significant performance gains as
compared to target-only models.
We also evaluated the baseline strategy where
unknown terms are omitted from the translation,
resulting in 25% precision. Leaving unknown
words untranslated also yielded very poor transla-
tion quality in an analysis performed on a similar
dataset.
Comparison to related work We compared our
algorithm with an implementation of the algo-
rithm proposed by (Callison-Burch et al, 2006)
(see Section 2.2), henceforth CB, using the Span-
ish side of Europarl as the pivot language.
Out of the tested 2,494 sentences with unknown
terms, CB found paraphrases for 706 sentences
(28.3%), while with any of our models, including
796
Model Precision (%) Coverage (%) Better (%)
NB:SMT (TE) 85.3 56.2 72.7
CB 85.3 24.2 12.7
Table 2: Comparison between our top model and the
method by Callison-Burch et al (2006), showing the per-
centage of times translations were considered acceptable, the
model?s coverage and the percentage of times each model
scored better than the other (in the 14% remaining cases, both
models produced unacceptable translations).
NB:SMT , our algorithm found applicable entail-
ment rules for 1,643 sentences (66%).
The quality of the CB translations was manually
assessed for a sample of 150 sentences. Table 2
presents the precision and coverage on this sample
for both CB and NB:SMT , as well as the number
of times each model?s translation was preferred by
the annotators. While both models achieve equally
high precision scores on this sample, the NB:SMT
model?s translations were undoubtedly preferred
by the annotators, with a considerably higher cov-
erage.
With the CB method, given that many of the
phrases added to the phrase table are noisy, the
global quality of the sentences seem to have been
affected, explaining why the judges preferred the
NB:SMT translations. One reason for the lower
coverage of CB is the fact that paraphrases were
acquired from a corpus whose domain is differ-
ent from that of the test sentences. The entail-
ment rules in our models are not limited to para-
phrases and are derived from WordNet, which has
broader applicability. Hence, utilizing monolin-
gual resources has proven beneficial for the task.
5.2 Automatic MT Evaluation
Although automatic MT evaluation metrics are
less appropriate for capturing the variations gen-
erated by our method, to ensure that there was no
degradation in the system-level scores according
to such metrics we also measured the models? per-
formance using BLEU and METEOR (Agarwal
and Lavie, 2007). The version of METEOR we
used on the target language (French) considers the
stems of the words, instead of surface forms only,
but does not make use of WordNet synonyms.
We evaluated the performance of the top mod-
els of Table 1, as well as of a baseline SMT sys-
tem that left unknown terms untranslated, on the
sample of 1,014 manually annotated sentences. As
shown in Table 3, all models resulted in improve-
ment with respect to the original sentences (base-
Model BLEU (TE) METEOR (TE)
?:SMT 15.50 0.1325
NB:SMT 15.37 0.1316
LSA:SMT 15.51 0.1318
NB:? 15.37 0.1311
CB 15.33 0.1299
Baseline SMT 15.29 0.1294
Table 3: Performance of the best models according to auto-
matic MT evaluation metrics at the corpus level. The baseline
refers to translation of the text without applying any entail-
ment rules.
line). The difference in METEOR scores is statis-
tically significant (p < 0.05) for the three top mod-
els against the baseline. The generally low scores
may be attributed to the fact that training and test
sentences are from different domains.
5.3 Discussion
The use of entailed texts produced using our ap-
proach clearly improves the quality of translations,
as compared to leaving unknown terms untrans-
lated or omitting them altogether. While it is clear
that textual entailment is useful for increasing cov-
erage in translation, further research is required to
identify the amount of information loss incurred
when non-symmetric entailment relations are be-
ing used, and thus to identify the cases where such
relations are detrimental to translation.
Consider, for example, the sentence: ?Conven-
tional military models are geared to decapitate
something that, in this case, has no head.?. In this
sentence, the unknown term was replaced by kill,
which results in missing the point originally con-
veyed in the text. Accordingly, the produced trans-
lation does not preserve the meaning of the source,
and was considered unacceptable: ?Les mode`les
militaires visent a` faire quelque chose que, dans
ce cas, n?est pas responsable.?.
In other cases, the selected hypernyms were too
generic words, such as entity or attribute, which
also fail to preserve the sentence?s meaning. On
the other hand, when the unknown term was a
very specific word, hypernyms played an impor-
tant role. For example, ?Bulgaria is the most
sought-after east European real estate target, with
its low-cost ski chalets and oceanfront homes?.
Here, chalets are replaced by houses or units (de-
pending on the model), providing a translation that
would be acceptable by most readers.
Other incorrect translations occurred when the
unknown term was part of a phrase, for exam-
ple, troughs replaced with depressions in peaks
797
and troughs, a problem that also strongly affects
paraphrasing. In another case, movement was the
hypernym chosen to replace labor in labor move-
ment, yielding an awkward text for translation.
Many of the cases which involved ambiguity
were resolved by the applied context-models, and
can be further addressed, together with the above
mentioned problems, with better source-language
context models.
We suggest that other types of entailment rules
could be useful for the task beyond the straight-
forward generalization using hypernyms, which
was demonstrated in this work. This includes
other types of lexical entailment relations, such as
holonymy (e.g. Singapore ? Southeast Asia) as
well as lexical syntactic rules (X cure Y ? treat
Y with X). Even syntactic rules, such as clause re-
moval, can be recruited for the task: ?Obama, the
44th president, declared Monday . . . ?? ?Obama
declared Monday . . . ?. When the system is un-
able to translate a term found in the embedded
clause, the translation of the less informative sen-
tence may still be acceptable by readers.
6 Conclusions and Future Work
In this paper we propose a new entailment-based
approach for addressing the problem of unknown
terms in machine translation. Applying this ap-
proach with lexical entailment rules from Word-
Net, we show that using monolingual resources
and textual entailment relationships allows sub-
stantially increasing the quality of translations
produced by an SMT system. Our experiments
also show that it is possible to perform the process
efficiently by relying on source language context-
models as a filter prior to translation. This pipeline
maintains translation quality, as assessed by both
human annotators and standard automatic mea-
sures.
For future work we suggest generating entailed
texts with a more extensive set of rules, in particu-
lar lexical-syntactic ones. Combining rules from
monolingual and bilingual resources seems ap-
pealing as well. Developing better context-models
to be applied on the source is expected to further
improve our method?s performance. Specifically,
we suggest taking into account the prior likelihood
that a rule is correct as part of the model score.
Finally, some researchers have advocated re-
cently the use of shared structures such as parse
forests (Mi and Huang, 2008) or word lattices
(Dyer et al, 2008) in order to allow a compact rep-
resentation of alternative inputs to an SMT system.
This is an approach that we intend to explore in
future work, as a way to efficiently handle the dif-
ferent source language alternatives generated by
entailment rules. However, since most current MT
systems do not accept such type of inputs, we con-
sider the results on pruning by source-side context
models as broadly relevant.
Acknowledgments
This work was supported in part by the ICT Pro-
gramme of the European Community, under the
PASCAL 2 Network of Excellence, ICT-216886
and The Israel Science Foundation (grant No.
1112/08). We wish to thank Roy Bar-Haim and
the anonymous reviewers of this paper for their
useful feedback. This publication only reflects the
authors? views.
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of WMT-08.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of Unknown Words in Phrase-
Based Statistical Machine Translation for Lan-
guages of Rich Morphology. In Proceedings of
SLTU.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of IWSLT.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proceedings of WMT.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proceedings of ACL.
798
Ido Dagan, Oren Glickman, Alfio Massimiliano
Gliozzo, Efrat Marmorshtein, and Carlo Strappar-
ava. 2006. Direct Word Sense Matching for Lexical
Substitution. In Proceedings of ACL.
Scott Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R.A. Harshman. 1990. Indexing by La-
tent Semantic Analysis. Journal of the American So-
ciety for Information Science, 41.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Trans-
lation. In Proceedings of ACL-HLT.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2008.
Communicating Unknown Words in Machine Trans-
lation. In Proceedings of LREC.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of ACL-WTEP Workshop.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy
Bengio, and Walter Daelemans. 2006. Investigat-
ing Lexical Substitution Scoring for Subtitle Gener-
ation. In Proceedings of CoNLL.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis, Univer-
sity of Trento.
Francisco Guzma?n and Leonardo Garrido. 2008.
Translation Paraphrases in Phrase-Based Machine
Translation. In Proceedings of CICLing.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-HLT.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating Unknown Words by Analogical Learning. In
Proceedings of EMNLP-CoNLL.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
SIGKDD.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Substitu-
tion Task. In Proceedings of SemEval.
Diana Mccarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding Predominant Word Senses
in Untagged Text. In Proceedings of ACL.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of
EMNLP.
Sebastian Pado, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2009. Textual Entail-
ment Features for Machine Translation Evaluation.
In Proceedings of WMT.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL.
M. Simard, N. Cancedda, B. Cavestro, M. Dymet-
man, E. Gaussier, C. Goutte, and K. Yamada. 2005.
Translating with Non-contiguous Phrases. In Pro-
ceedings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual Preferences. In Pro-
ceedings of ACL-HLT.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80?83.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-Based
Backoff Models for Machine Translation of Highly
Inflected Languages. In Proceedings of EACL.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-HLT.
799
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 579?586,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integrating Pattern-based and Distributional Similarity Methods for 
Lexical Entailment Acquisition 
 
                   Shachar Mirkin     Ido Dagan         Maayan Geffet 
School of Computer Science and Engineering 
The Hebrew University, Jerusalem, Israel, 
91904 
mirkin@cs.huji.ac.il  
 
Department of Computer Science 
Bar-Ilan University, Ramat Gan, Israel,  
52900 
{dagan,zitima}@cs.biu.ac.il 
 
Abstract 
This paper addresses the problem of acquir-
ing lexical semantic relationships, applied to 
the lexical entailment relation. Our main con-
tribution is a novel conceptual integration 
between the two distinct acquisition para-
digms for lexical relations ? the pattern-
based and the distributional similarity ap-
proaches. The integrated method exploits 
mutual complementary information of the 
two approaches to obtain candidate relations 
and informative characterizing features. 
Then, a small size training set is used to con-
struct a more accurate supervised classifier, 
showing significant increase in both recall 
and precision over the original approaches. 
1 Introduction 
Learning lexical semantic relationships is a fun-
damental task needed for most text understand-
ing applications. Several types of lexical 
semantic relations were proposed as a goal for 
automatic acquisition. These include lexical on-
tological relations such as synonymy, hyponymy 
and meronymy, aiming to automate the construc-
tion of WordNet-style relations. Another com-
mon target is learning general distributional 
similarity between words, following Harris' Dis-
tributional Hypothesis (Harris, 1968). Recently, 
an applied notion of entailment between lexical 
items was proposed as capturing major inference 
needs which cut across multiple semantic rela-
tionship types (see Section 2 for further back-
ground).  
The literature suggests two major approaches 
for learning lexical semantic relations: distribu-
tional similarity and pattern-based. The first ap-
proach recognizes that two words (or two multi-
word terms) are semantically similar based on 
distributional similarity of the different contexts 
in which the two words occur. The distributional 
method identifies a somewhat loose notion of 
semantic similarity, such as between company 
and government, which does not ensure that the 
meaning of one word can be substituted by the 
other. The second approach is based on identify-
ing joint occurrences of the two words within 
particular patterns, which typically indicate di-
rectly concrete semantic relationships. The pat-
tern-based approach tends to yield more accurate 
hyponymy and (some) meronymy relations, but 
is less suited to acquire synonyms which only 
rarely co-occur within short patterns in texts. It 
should be noted that the pattern-based approach 
is commonly applied also for information and 
knowledge extraction to acquire factual instances 
of concrete meaning relationships (e.g. born in, 
located at) rather than generic lexical semantic 
relationships in the language. 
While the two acquisition approaches are 
largely complementary, there have been just few 
attempts to combine them, usually by pipeline 
architecture. In this paper we propose a method-
ology for integrating distributional similarity 
with the pattern-based approach. In particular, 
we focus on learning the lexical entailment rela-
tionship between common nouns and noun 
phrases (to be distinguished from learning rela-
tionships for proper nouns, which usually falls 
within the knowledge acquisition paradigm).  
The underlying idea is to first identify candi-
date relationships by both the distributional ap-
proach, which is applied exhaustively to a local 
corpus, and the pattern-based approach, applied 
to the web. Next, each candidate is represented 
by a unified set of distributional and pattern-
based features. Finally, using a small training set 
we devise a supervised (SVM) model that classi-
fies new candidate relations as correct or incor-
rect. 
To implement the integrated approach we de-
veloped state of the art pattern-based acquisition 
579
methods and utilized a distributional similarity 
method that was previously shown to provide 
superior performance for lexical entailment ac-
quisition. Our empirical results show that the 
integrated method significantly outperforms each 
approach in isolation, as well as the na?ve com-
bination of their outputs. Overall, our method 
reveals complementary types of information that 
can be obtained from the two approaches. 
 
2 Background 
2.1 Distributional Similarity and 
Lexical Entailment 
The general idea behind distributional similarity 
is that words which occur within similar contexts 
are semantically similar (Harris, 1968). In a 
computational framework, words are represented 
by feature vectors, where features are context 
words weighted by a function of their statistical 
association with the target word. The degree of 
similarity between two target words is then de-
termined by a vector comparison function. 
Amongst the many proposals for distributional 
similarity measures, (Lin, 1998) is maybe the 
most widely used one, while (Weeds et al, 2004) 
provides a typical example for recent research. 
Distributional similarity measures are typically 
computed through exhaustive processing of a 
corpus, and are therefore applicable to corpora of 
bounded size. 
It was noted recently by Geffet and Dagan 
(2004, 2005) that distributional similarity cap-
tures a quite loose notion of semantic similarity, 
as exemplified by the pair country ? party (iden-
tified by Lin's similarity measure). Consequently, 
they proposed a definition for the lexical entail-
ment relation, which conforms to the general 
framework of applied textual entailment (Dagan 
et al, 2005). Generally speaking, a word w lexi-
cally entails another word v if w can substitute v 
in some contexts while implying v's original 
meaning. It was suggested that lexical entailment 
captures major application needs in modeling 
lexical variability, generalized over several types 
of known ontological relationships. For example, 
in Question Answering (QA), the word company 
in a question can be substituted in the text by 
firm (synonym), automaker (hyponym) or sub-
sidiary (meronym), all of which entail company. 
Typically, hyponyms entail their hypernyms and 
synonyms entail each other, while entailment 
holds for meronymy only in certain cases. 
In this paper we investigate automatic acquisi-
tion of the lexical entailment relation. For the 
distributional similarity component we employ 
the similarity scheme of (Geffet and Dagan, 
2004), which was shown to yield improved pre-
dictions of (non-directional) lexical entailment 
pairs. This scheme utilizes the symmetric simi-
larity measure of (Lin, 1998) to induce improved 
feature weights via bootstrapping. These weights 
identify the most characteristic features of each 
word, yielding cleaner feature vector representa-
tions and better similarity assessments. 
2.2 Pattern-based Approaches 
Hearst (1992) pioneered the use of lexical-
syntactic patterns for automatic extraction of 
lexical semantic relationships. She acquired hy-
ponymy relations based on a small predefined set 
of highly indicative patterns, such as ?X, . . . , Y 
and/or other Z?, and ?Z such as X, . . . and/or Y?, 
where X and Y are extracted as hyponyms of Z. 
Similar techniques were further applied to pre-
dict hyponymy and meronymy relationships us-
ing lexical or lexico-syntactic patterns (Berland 
and Charniak, 1999; Sundblad, 2002), and web 
page structure was exploited to extract hy-
ponymy relationships by Shinzato and Torisawa 
(2004). Chklovski and Pantel (2004) used pat-
terns to extract a set of relations between verbs, 
such as similarity, strength and antonymy. Syno-
nyms, on the other hand, are rarely found in such 
patterns. In addition to their use for learning lexi-
cal semantic relations, patterns were commonly 
used to learn instances of concrete semantic rela-
tions for Information Extraction (IE) and QA, as 
in (Riloff and Shepherd, 1997; Ravichandran and 
Hovy, 2002; Yangarber et al, 2000).  
Patterns identify rather specific and informa-
tive structures within particular co-occurrences 
of the related words. Consequently, they are rela-
tively reliable and tend to be more accurate than 
distributional evidence. On the other hand, they 
are susceptive to data sparseness in a limited size 
corpus. To obtain sufficient coverage, recent 
works such as (Chklovski and Pantel, 2004) ap-
plied pattern-based approaches to the web. These 
methods form search engine queries that match 
likely pattern instances, which may be verified 
by post-processing the retrieved texts. 
Another extension of the approach was auto-
matic enrichment of the pattern set through boot-
strapping. Initially, some instances of the sought 
580
relation are found based on a set of manually 
defined patterns.  Then, additional co-
occurrences of the related terms are retrieved, 
from which new patterns are extracted (Riloff 
and Jones, 1999; Pantel et al, 2004). Eventually, 
the list of effective patterns found for ontological 
relations has pretty much converged in the litera-
ture. Amongst these, Table 1 lists the patterns 
that were utilized in our work. 
Finally, the selection of candidate pairs for a 
target relation was usually based on some func-
tion over the statistics of matched patterns. To 
perform more systematic selection Etzioni et al 
(2004) applied a supervised Machine Learning 
algorithm (Na?ve Bayes), using pattern statistics 
as features. Their work was done within the IE 
framework, aiming to extract semantic relation 
instances for proper nouns, which occur quite 
frequently in indicative patterns. In our work we 
incorporate and extend the supervised learning 
step for the more difficult task of acquiring gen-
eral language relationships between common 
nouns. 
2.3 Combined Approaches 
It can be noticed that the pattern-based and dis-
tributional approaches have certain complemen-
tary properties. The pattern-based method tends 
to be more precise, and also indicates the direc-
tion of the relationship between the candidate 
terms. The distributional similarity approach is 
more exhaustive and suitable to detect symmetric 
synonymy relations. Few recent attempts on re-
lated (though different) tasks were made to clas-
sify (Lin et al, 2003) and label (Pantel and 
Ravichandran, 2004) distributional similarity 
output using lexical-syntactic patterns, in a pipe-
line architecture. We aim to achieve tighter inte-
gration of the two approaches, as described next. 
 
3 An Integrated Approach for Lexi-
cal Entailment Acquisition 
This section describes our integrated approach 
for acquiring lexical entailment relationships, 
applied to common nouns. The algorithm re-
ceives as input a target term and aims to acquire 
a set of terms that either entail or are entailed by 
it. We denote a pair consisting of the input target 
term and an acquired entailing/entailed term as 
entailment pair. Entailment pairs are directional, 
as in bank  company. 
Our approach applies a supervised learning 
scheme, using SVM, to classify candidate en-
tailment pairs as correct or incorrect. The SVM 
training phase is applied to a small constant 
number of training pairs, yielding a classification 
model that is then used to classify new test en-
tailment pairs. The designated training set is also 
used to tune some additional parameters of the 
method. Overall, the method consists of the fol-
lowing main components:  
1: Acquiring candidate entailment pairs for 
the input term by pattern-based and distribu-
tional similarity methods (Section 3.2); 
2: Constructing a feature set for all candidates 
based on pattern-based and distributional in-
formation (Section 3.3); 
3: Applying SVM training and classification 
to the candidate pairs (Section 3.4).  
The first two components, of acquiring candidate 
pairs and collecting features for them, utilize a 
generic module for pattern-based extraction from 
the web, which is described first in Section 3.1.    
3.1 Pattern-based Extraction Mod-
ule 
The general pattern-based extraction module re-
ceives as input a set of lexical-syntactic patterns 
(as in Table 1) and either a target term or a can-
didate pair of terms. It then searches the web for 
occurrences of the patterns with the input term(s). 
A small set of effective queries is created for 
each pattern-terms combination, aiming to re-
trieve as much relevant data with as few queries 
as possible. 
Each pattern has two variable slots to be in-
stantiated by candidate terms for the sought rela-
tion. Accordingly, the extraction module can be 
1 NP1 such as NP2 
2 Such NP1 as NP2 
3 NP1 or other NP2 
4 NP1 and other NP2 
5 NP1 ADV known as NP2 
6 NP1 especially NP2 
7 NP1 like NP2 
8 NP1 including NP2 
9 NP1-sg is (a OR an) NP2-sg 
10 NP1-sg (a OR an) NP2-sg 
11 NP1-pl are NP2-pl 
Table 1: The patterns we used for entailment ac-
quisition based on (Hearst, 1992) and (Pantel et al, 
2004). Capitalized terms indicate variables. pl and 
sg stand for plural and singular forms. 
 
581
used in two modes: (a) receiving a single target 
term as input and searching for instantiations of 
the other variable to identify candidate related 
terms (as in Section 3.2); (b) receiving a candi-
date pair of terms for the relation and searching 
pattern instances with both terms, in order to 
validate and collect information about the rela-
tionship between the terms (as in Section 3.3). 
Google proximity search1 provides a useful tool 
for these purposes, as it allows using a wildcard 
which might match either an un-instantiated term 
or optional words such as modifiers.  For exam-
ple, the query "such ** as *** (war OR wars)" is 
one of the queries created for the input pattern 
such NP1 as NP2 and the input target term war, 
allowing new terms to match the first pattern 
variable. For the candidate entailment pair war 
? struggle, the first variable is instantiated as 
well. The corresponding query would be: "such * 
(struggle OR struggles) as *** (war OR wars)?. 
This technique allows matching terms that are 
sub-parts of more complex noun phrases as well 
as multi-word terms. 
The automatically constructed queries, cover-
ing the possible combinations of multiple wild-
cards, are submitted to Google2 and a specified 
number of snippets is downloaded, while avoid-
ing duplicates. The snippets are passed through a 
word splitter and a sentence segmenter3, while 
filtering individual sentences that do not contain 
all search terms. Next, the sentences are proc-
essed with the OpenNLP4  POS tagger and NP 
chunker. Finally, pattern-specific regular expres-
sions over the chunked sentences are applied to 
verify that the instantiated pattern indeed occurs 
in the sentence, and to identify variable instantia-
tions.  
On average, this method extracted more than 
3300 relationship instances for every 1MB of 
downloaded text, almost third of them contained 
multi-word terms. 
3.2 Candidate Acquisition 
Given an input target term we first employ pat-
tern-based extraction to acquire entailment pair 
candidates and then augment the candidate set 
with pairs obtained through distributional simi-
larity. 
                                                          
1
 Previously used by (Chklovski and Pantel, 2004). 
2
 http://www.google.com/apis/ 
3 Available from the University of Illinois at Urbana-
Champaign, http://l2r.cs.uiuc.edu/~cogcomp/tools.php 
4
 www.opennlp.sourceforge.net/ 
3.2.1 Pattern-based Candidates 
At the candidate acquisition phase pattern in-
stances are searched with one input target term, 
looking for instantiations of the other pattern 
variable to become the candidate related term 
(the first querying mode described in Section 
3.1). We construct two types of queries, in which 
the target term is either the first or second vari-
able in the pattern, which corresponds to finding 
either entailing or entailed terms that instantiate 
the other variable.  
In the candidate acquisition phase we utilized 
patterns 1-8 in Table 1, which we empirically 
found as most suitable for identifying directional 
lexical entailment pairs. Patterns 9-11 are not 
used at this stage as they produce too much noise 
when searched with only one instantiated vari-
able. About 35 queries are created for each target 
term in each entailment direction for each of the 
8 patterns. For every query, the first n snippets 
are downloaded (we used n=50). The 
downloaded snippets are processed as described 
in Section 3.1, and candidate related terms are 
extracted, yielding candidate entailment pairs 
with the input target term.  
Quite often the entailment relation holds be-
tween multi-word noun-phrases rather than 
merely between their heads. For example, trade 
center lexically entails shopping complex, while 
center does not necessarily entail complex. On 
the other hand, many complex multi-word noun 
phrases are too rare to make a statistically based 
decision about their relation with other terms. 
Hence, we apply the following two criteria to 
balance these constraints:  
1. For the entailing term we extract only the 
complete noun-chunk which instantiate the 
pattern. For example: we extract housing 
project ? complex, but do not extract pro-
ject as entailing complex since the head noun 
alone is often too general to entail the other 
term. 
2. For the entailed term we extract both the 
complete noun-phrase and its head in order 
to create two separate candidate entailment 
pairs with the entailing term, which will be 
judged eventually according to their overall 
statistics. 
As it turns out, a large portion of the extracted 
pairs constitute trivial hyponymy relations, 
where one term is a modified version of the other, 
like low interest loan ? loan. These pairs were 
removed, along with numerous pairs including 
proper nouns, following the goal of learning en-
582
tailment relationships for distinct common 
nouns.  
Finally, we filter out the candidate pairs whose 
frequency in the extracted patterns is less than a 
threshold, which was set empirically to 3. Using 
a lower threshold yielded poor precision, while a 
threshold of 4 decreased recall substantially with 
just little effect on precision. 
3.2.2 Distributional Similarity 
Candidates 
As mentioned in Section 2, we employ the distri-
butional similarity measure of (Geffet and Da-
gan, 2004) (denoted here GD04 for brevity), 
which was found effective for extracting non-
directional lexical entailment pairs.  Using local 
corpus statistics, this algorithm produces for each 
target noun a scored list of up to a few hundred 
words with positive distributional similarity 
scores. 
Next we need to determine an optimal thresh-
old for the similarity score, considering words 
above it as likely entailment candidates. To tune 
such a threshold we followed the original meth-
odology used to evaluate GD04. First, the top-k 
(k=40) similarities of each training term are 
manually annotated by the lexical entailment cri-
terion (see Section 4.1). Then, the similarity 
value which yields the maximal micro-averaged 
F1 score is selected as threshold, suggesting an 
optimal recall-precision tradeoff. The selected 
threshold is then used to filter the candidate simi-
larity lists of the test words.   
Finally, we remove all entailment pairs that al-
ready appear in the candidate set of the pattern-
based approach, in either direction (recall that the 
distributional candidates are non-directional). 
Each of the remaining candidates generates two 
directional pairs which are added to the unified 
candidate set of the two approaches. 
3.3 Feature Construction 
Next, each candidate is represented by a set of 
features, suitable for supervised classification. To 
this end we developed a novel feature set based 
on both pattern-based and distributional data. 
 
To obtain pattern statistics for each pair, the 
second mode of the pattern-based extraction 
module is applied (see Section 3.1). As in this 
case, both variables in the pattern are instantiated 
by the terms of the pair, we could use all eleven 
patterns in Table 1, creating a total of about 55 
queries per pair and downloading m=20 snippets 
for each query. The downloaded snippets are 
processed as described in Section 3.1 to identify 
pattern matches and obtain relevant statistics for 
feature scores.  
Following is the list of feature types computed 
for each candidate pair. The feature set was de-
signed specifically for the task of extracting the 
complementary information of the two methods. 
Conditional Pattern Probability: This type of 
feature is created for each of the 11 individual 
patterns. The feature value is the estimated con-
ditional probability of having the pattern 
matched in a sentence given that the pair of terms 
does appear in the sentence (calculated as the 
fraction of pattern matches for the pair amongst 
all unique sentences that contain the pair). This 
feature yields normalized scores for pattern 
matches regardless of the number of snippets 
retrieved for the given pair. This normalization is 
important in order to bring to equal grounds can-
didate pairs identified through either the pattern-
based or distributional approaches, since the lat-
ter tend to occur less frequently in patterns. 
Aggregated Conditional Pattern Probability: 
This single feature is the conditional probability 
that any of the patterns match in a retrieved sen-
tence, given that the two terms appear in it. It is 
calculated like the previous feature, with counts 
aggregated over all patterns, and aims to capture 
overall appearance of the pair in patterns, regard-
less of the specific pattern. 
Conditional List-Pattern Probability: This fea-
ture was designed to eliminate the typical non-
entailing cases of co-hyponyms (words sharing 
the same hypernym), which nevertheless tend to 
co-occur in entailment patterns. We therefore 
also check for pairs' occurrences in lists, using 
appropriate list patterns, expecting that correct 
entailment pairs would not co-occur in lists. The 
probability estimate, calculated like the previous 
one, is expected to be a negative feature for the 
learning model. 
Relation Direction Ratio: The value of this fea-
ture is the ratio between the overall number of 
pattern matches for the pair and the number of 
pattern matches for the reversed pair (a pair cre-
ated with the same terms in the opposite entail-
ment direction). We found that this feature 
strongly correlates with entailment likelihood. 
Interestingly, it does not deteriorate performance 
for synonymous pairs. 
Distributional Similarity Score: The GD04 simi-
larity score of the pair was used as a feature. We 
583
also attempted adding Lin's (1998) similarity 
scores but they appeared to be redundant. 
Intersection Feature: A binary feature indicating 
candidate pairs acquired by both methods, which 
was found to indicate higher entailment likeli-
hood. 
    In summary, the above feature types utilize 
mutually complementary pattern-based and dis-
tributional information. Using cross validation 
over the training set we verified that each feature 
makes marginal contribution to performance 
when added on top of the remaining features.  
3.4 Training and Classification 
In order to systematically integrate different fea-
ture types we used the state-of-the-art supervised 
classifier SVMlight (Joachims, 1999) for entail-
ment pair classification. Using 10-fold cross-
validation over the training set we obtained the 
SVM configuration that yields an optimal micro-
averaged F1 score. Through this optimization we 
chose the RBF kernel function and obtained op-
timal values for the J, C and the RBF's Gamma 
parameters. The candidate test pairs classified as 
correct entailments constitute the output of our 
integrated method. 
 
4 Empirical Results 
4.1 Data Set and Annotation 
We utilized the experimental data set from Geffet 
and Dagan (2004). The dataset includes the simi-
larity lists calculated by GD04 for a sample of 30 
target (common) nouns, computed from an 18 
million word subset of the Reuters corpus5. We 
randomly picked a small set of 10 terms for train-
ing, leaving the remaining 20 terms for testing. 
Then, the set of entailment pair candidates for all 
nouns was created by applying the filtering 
method of Section 3.2.2 to the distributional 
similarity lists, and by extracting pattern-based 
                                                          
5
 Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19. 
candidates from the web as described in Section 
3.2.1. 
Gold standard annotations for entailment pairs 
were created by three judges. The judges were 
guided to annotate as ?Correct? the pairs con-
forming to the lexical entailment definition, 
which was reflected in two operational tests: i) 
Word meaning entailment: whether the meaning 
of the first (entailing) term implies the meaning 
of the second (entailed) term under some com-
mon sense of the two terms; and ii) Substitutabil-
ity: whether the first term can substitute the 
second term in some natural contexts, such that 
the meaning of the modified context entails the 
meaning of the original one. The obtained Kappa 
values (varying between 0.7 and 0.8) correspond 
to substantial agreement on the task. 
4.2 Results 
The numbers of candidate entailment pairs col-
lected for the test terms are shown in Table 2. 
These figures highlight the markedly comple-
mentary yield of the two acquisition approaches, 
where only about 10% of all candidates were 
identified by both methods. On average, 120 
candidate entailment pairs were acquired for 
each target term. 
The SVM classifier was trained on a quite 
small annotated sample of 700 candidate entail-
ment pairs of the 10 training terms. Table 3 pre-
sents comparative results for the classifier, for 
each of the two sets of candidates produced by 
each method alone, and for the union of these 
two sets (referred as Na?ve Combination). The 
results were computed for an annotated random 
sample of about 400 candidate entailment pairs 
of the test terms. Following common pooling 
evaluations in Information Retrieval, recall is 
calculated relatively to the total number of cor-
rect entailment pairs acquired by both methods 
together.  
METHOD P R F 
Pattern-based  0.44 0.61 0.51 
Distributional  
Similarity 0.33 0.53 0.40 
Na?ve Combina-
tion 0.36 1.00 0.53 
Integrated  0.57 0.69 0.62 
Table 3: Precision, Recall and F1 figures for the 
test words under each method. 
 
PATTERN-
BASED 
DISTRIBU-
TIONAL TOTAL 
1186 1420 2350 
Table 2: The numbers of distinct entailment pair 
candidates obtained for the test words by each of 
the methods, and when combined.  
 
584
The first two rows of the table show quite 
moderate precision and recall for the candidates 
of each separate method. The next row shows the 
great impact of method combination on recall, 
relative to the amount of correct entailment pairs 
found by each method alone, validating the com-
plementary yield of the approaches. The inte-
grated classifier, applied to the combined set of 
candidates, succeeds to increase precision sub-
stantially by 21 points (a relative increase of al-
most 60%), which is especially important for 
many precision-oriented applications like Infor-
mation Retrieval and Question Answering. The 
precision increase comes with the expense of 
some recall, yet having F1 improved by 9 points. 
The integrated method yielded on average about 
30 correct entailments per target term. Its classi-
fication accuracy (percent of correct classifica-
tions) reached 70%, which nearly doubles the 
na?ve combination's accuracy.  
It is impossible to directly compare our results 
with those of other works on lexical semantic 
relationships acquisition, since the particular task 
definition and dataset are different. As a rough 
reference point, our result figures do match those 
of related papers reviewed in Section 2, while we 
notice that our setting is relatively more difficult 
since we excluded the easier cases of proper 
nouns. (Geffet and Dagan, 2005), who exploited 
the distributional similarity approach over the 
web to address the same task as ours, obtained 
higher precision but substantially lower recall, 
considering only distributional candidates. Fur-
ther research is suggested to investigate integrat-
ing their approach with ours. 
 
 
 
4.3 Analysis and Discussion 
Analysis of the data confirmed that the two 
methods tend to discover different types of rela-
tions. As expected, the distributional similarity 
method contributed most (75%) of the synonyms 
that were correctly classified as mutually entail-
ing pairs (e.g. assault ? abuse in Table 4). On 
the other hand, about 80% of all correctly identi-
fied hyponymy relations were produced by the 
pattern-based method (e.g. abduction ? abuse). 
The integrated method provides a means to de-
termine the entailment direction for distributional 
similarity candidates which by construction are 
non-directional. Thus, amongst the (non-
synonymous) distributional similarity pairs clas-
sified as entailing, the direction of 73% was cor-
rectly identified. In addition, the integrated 
method successfully filters 65% of the non-
entailing co-hyponym candidates (hyponyms of 
the same hypernym), most of them originated in 
the distributional candidates, which is a large 
portion (23%) of all correctly discarded pairs. 
Consequently, the precision of distributional 
similarity candidates approved by the integrated 
system was nearly doubled, indicating the addi-
tional information that patterns provide about 
distributionally similar pairs. 
Yet, several error cases were detected and 
categorized. First, many non-entailing pairs are 
context-dependent, such as a gap which might 
constitute a hazard in some particular contexts, 
even though these words do not entail each other 
in their general meanings. Such cases are more 
typical for the pattern-based approach, which is 
sometimes permissive with respect to the rela-
tionship captured and may also extract candi-
dates from a relatively small number of pattern 
occurrences. Second, synonyms tend to appear 
less frequently in patterns. Consequently, some 
synonymous pairs discovered by distributional 
similarity were rejected due to insufficient pat-
tern matches. Anecdotally, some typos and spell-
ing alternatives, like privatization ? 
privatisation, are also included in this category 
as they never co-occur in patterns. 
In addition, a large portion of errors is caused 
by pattern ambiguity. For example, the pattern 
"NP1, a|an NP2", ranked among the top IS-A pat-
terns by (Pantel et al, 2004), can represent both 
apposition (entailing) and a list of co-hyponyms 
(non-entailing). Finally, some misclassifications 
can be attributed to technical web-based process-
ing errors and to corpus data sparseness.  
 
Pattern-based Distributional 
abduction ? abuse assault ? abuse 
government ?  
organization 
government ?  
administration 
drug therapy ?  
treatment budget deficit ?gap 
gap ? hazard* broker ? analyst* 
management ? issue* government ?  parliament* 
Table 4: Typical entailment pairs acquired by the 
integrated method, illustrating Section 4.3. The 
columns specify the method that produced the 
candidate pair. Asterisk indicates a non-entailing 
pair. 
 
585
5 Conclusion 
The main contribution of this paper is a novel 
integration of the pattern-based and distributional 
approaches for lexical semantic acquisition, ap-
plied to lexical entailment. Our investigation 
highlights the complementary nature of the two 
approaches and the information they provide. 
Notably, it is possible to extract pattern-based 
information that complements the weaker evi-
dence of distributional similarity. Supervised 
learning was found effective for integrating the 
different information types, yielding noticeably 
improved performance. Indeed, our analysis re-
veals that the integrated approach helps eliminat-
ing many error cases typical to each method 
alone. We suggest that this line of research may 
be investigated further to enrich and optimize the 
learning processes and to address additional lexi-
cal relationships.  
 
Acknowledgement 
We wish to thank Google for providing us with 
an extended quota for search queries, which 
made this research feasible.  
 
References  
Berland, Matthew and Charniak, Eugene. 1999. Find-
ing parts in very large corpora. In Proc. of ACL-99. 
Maryland, USA. 
Chklovski, Timothy and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic 
Verb Relations. In Proc. of EMNLP-04. Barcelona, 
Spain. 
Dagan, Ido, Oren Glickman and Bernardo Magnini. 
2005. The PASCAL Recognizing Textual Entail-
ment Challenge. In Proc. of the PASCAL Chal-
lenges Workshop for Recognizing Textual 
Entailment. Southampton, U.K.  
Etzioni, Oren, M. Cafarella, D. Downey, S. Kok, A.-
M. Popescu, T. Shaked, S. Soderland, D.S. Weld, 
and A. Yates. 2004. Web-scale information extrac-
tion in KnowItAll. In Proc. of WWW-04. NY, 
USA. 
Geffet, Maayan and Ido Dagan. 2004. Feature Vector 
Quality and Distributional Similarity. In Proc. of 
COLING-04. Geneva, Switzerland. 
Geffet, Maayan and Ido Dagan. 2005. The Distribu-
tional Inclusion Hypothesis and Lexical Entail-
ment. In Proc of ACL-05. Michigan, USA. 
Harris, Zelig S. 1968. Mathematical Structures of 
Language. Wiley. 
Hearst, Marti. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proc. of 
COLING-92. Nantes, France. 
Joachims, Thorsten. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.), MIT-Press. 
Lin, Dekang. 1998. Automatic Retrieval and Cluster-
ing of Similar Words.  In Proc. of COLING?
ACL98, Montreal, Canada. 
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and Ming 
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In Proc. of  IJCAI-03. Aca-
pulco, Mexico. 
Pantel, Patrick, Deepak Ravichandran, and Eduard 
Hovy. 2004. Towards Terascale Semantic Acquisi-
tion. In Proc. of COLING-04. Geneva, Switzer-
land. 
Pantel, Patrick and Deepak Ravichandran. 2004. 
Automatically Labeling Semantic Classes. In Proc. 
of HLT/NAACL-04. Boston, MA. 
Ravichandran, Deepak and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question An-
swering System. In Proc. of ACL-02. Philadelphia, 
PA. 
Riloff, Ellen and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In 
Proc. of EMNLP-97. RI, USA. 
Riloff, Ellen and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level 
Bootstrapping. In Proc. of AAAI-99. Florida, USA. 
Shinzato, Kenji and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In 
Proc. of HLT/NAACL-04. Boston, MA. 
Sundblad, H. Automatic Acquisition of Hyponyms 
and Meronyms from Question Corpora. 2002. In 
Proc. of the ECAI-02 Workshop on Natural Lan-
guage Processing and Machine Learning for On-
tology Engineering. Lyon, France. 
Weeds, Julie, David Weir, and Diana McCarthy. 
2004. Characterizing Measures of Lexical Distribu-
tional Similarity. In Proc. of COLING-04. Geneva, 
Switzerland. 
Yangarber, Roman, Ralph Grishman, Pasi Tapanainen 
and Silja Huttunen. 2000. Automatic Acquisition 
of Domain Knowledge for Information Extraction. 
In Proc. of COLING-00. Saarbr?cken, Germany. 
586
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 770?778,
Beijing, August 2010
Recognising Entailment within Discourse
Shachar Mirkin?, Jonathan Berant?, Ido Dagan?, Eyal Shnarch?
? Computer Science Department, Bar-Ilan University
? The Blavatnik School of Computer Science, Tel-Aviv University
Abstract
Texts are commonly interpreted based on
the entire discourse in which they are sit-
uated. Discourse processing has been
shown useful for inference-based applica-
tion; yet, most systems for textual entail-
ment ? a generic paradigm for applied in-
ference ? have only addressed discourse
considerations via off-the-shelf corefer-
ence resolvers. In this paper we explore
various discourse aspects in entailment in-
ference, suggest initial solutions for them
and investigate their impact on entailment
performance. Our experiments suggest
that discourse provides useful informa-
tion, which significantly improves entail-
ment inference, and should be better ad-
dressed by future entailment systems.
1 Introduction
This paper investigates the problem of recognising
textual entailment within discourse. Textual En-
tailment (TE) is a generic framework for applied
semantic inference (Dagan et al, 2009). Under
TE, the relationship between a text (T) and a tex-
tual assertion (hypothesis, H) is defined such that
T entails H if humans reading T would infer that
H is most likely true (Dagan et al, 2006).
TE has been successfully applied to a variety of
natural language processing applications, includ-
ing information extraction (Romano et al, 2006)
and question answering (Harabagiu and Hickl,
2006). Yet, most entailment systems have thus
far paid little attention to discourse aspects of in-
ference. In part, this is the result of the unavail-
ability of adept tools for handling the kind of dis-
course processing required for inference. In addi-
tion in the main TE benchmarks, the Recognising
Textual Entailment (RTE) challenges, discourse
played little role. This state of affairs has started
to change with the recent introduction of the RTE
Pilot ?Search? task (Bentivogli et al, 2009b), in
which assessed texts are situated within complete
documents. In this setting, texts need to be inter-
preted based on their entire discourse (Bentivogli
et al, 2009a), hence attending to discourse issues
becomes essential. Consider the following exam-
ple from the task?s dataset:
(T) The seven men on board were said to have
as little as 24 hours of air.
For the interpretation of T, e.g. the identity and
whereabouts of the seven men, one must consider
T?s discourse. The preceding sentence T?, for in-
stance, provides useful information to that aim:
(T?) The Russian navy worked desperately to
save a small military submarine.
This example demonstrates a common situation in
texts, and is also applicable to the RTE Search
task?s setting. Still, little was done by the task?s
participants to consider discourse, and sentences
were mostly processed independently.
Analyzing the Search task?s development set,
we identified several key discourse aspects that af-
fect entailment in a discourse-dependent setting.
First, we observed that the coverage of available
coreference resolution tools is considerably lim-
ited. To partly address this problem, we extend the
set of coreference relations to phrase pairs with
a certain degree of lexical overlap, as long as no
semantic incompatibility is found between them.
Second, many bridging relations (Clark, 1975) are
realized in the form of ?global information? per-
ceived as known for entire documents. As bridg-
ing falls completely out of the scope of available
resolvers, we address this phenomenon by iden-
tifying and weighting prominent document terms
and allowing their incorporation in inference even
770
when they are not explicitly mentioned in a sen-
tence. Finally, we observed a coherence-related
discourse phenomenon, namely inter-relations be-
tween entailing sentences in the discourse, such
as the tendency of entailing sentences to be ad-
jacent to one another. To that end, we apply a
two-phase classification scheme, where a second-
phase meta-classifier is applied, extracting dis-
course and document-level features based on the
classification of each sentence on its own.
Our results show that, even when simple so-
lutions are employed, the reliance on discourse-
based information is helpful and achieves a sig-
nificant improvement of results. We analyze the
contribution of each component and suggest some
future work to better attend to discourse in entail-
ment systems. To our knowledge, this is the most
extensive effort thus far to empirically explore the
effect of discourse on entailment systems.
2 Background
Discourse plays a key role in text understanding
applications such as question answering or infor-
mation extraction. Yet, such applications typically
only handle a narrow aspect of discourse, address-
ing coreference by term substitution (Dali et al,
2009; Li et al, 2009). The limited coverage and
scope of existing tools for coreference resolution
and the unavailability of tools for addressing other
discourse aspects also contribute to this situation.
For instance, VP anaphora and bridging relations
are usually not handled at all by such resolvers. A
similar situation is seen in the TE research field.
The prominent benchmark for entailment sys-
tems evaluation is the series of RTE challenges.
The main task in these challenges has tradition-
ally been to determine, given a text-hypothesis
pair (T,H), whether T entails H. Discourse played
no role in the first two RTE challenges as
T?s were constructed of short simplified texts.
In RTE-3 (Giampiccolo et al, 2007), where
some paragraph-long texts were included, inter-
sentential relations became relevant for correct in-
ference. Yet the texts in the task were manually
modified to ensure they are self-contained. Con-
sequently, little effort was invested by the chal-
lenges? participants to address discourse issues
beyond the standard substitution of coreferring
nominal phrases, using publicly available tools
such as JavaRap (Qiu et al, 2004) or OpenNLP1,
e.g. (Bar-Haim et al, 2008).
A major step in the RTE challenges towards a
more practical setting of text processing applica-
tions occurred with the introduction of the Search
task in the Fifth RTE challenge (RTE-5). In this
task entailing sentences are situated within doc-
uments and depend on other sentences for their
correct interpretation. Thus, discourse becomes
a substantial factor impacting inference. Surpris-
ingly, discourse hardly received any treatment in
this task beyond the standard use of coreference
resolution (Castillo, 2009; Litkowski, 2009), and
an attempt to address globally-known information
by removing from H words that appear in docu-
ment headlines (Clark and Harrison, 2009).
3 The RTE Search Task
The RTE-5 Search task was derived from the
TAC Summarization task2. The dataset consists
of several corpora, each comprised of news arti-
cles concerning a specific topic, such as the im-
pact of global warming on the Arctic or the Lon-
don terrorist attacks in 2005. Hypotheses were
manually generated based on Summary Content
Units (Nenkova et al, 2007), clause-long state-
ments taken from manual summaries of the cor-
pora. Texts are unmodified sentences in the arti-
cles. Given a topic and a hypothesis, entailment
systems are required to identify all sentences in
the topic?s corpus that entail the hypothesis.
Each sentence-hypothesis pair in both the de-
velopment and test sets was annotated, judging
whether the sentence entails the hypothesis. Out
of 20,104 annotations in the development set, only
810 were judged as positive. This small ratio (4%)
of positive examples, in comparison to 50% in tra-
ditional RTE tasks, better corresponds to the natu-
ral distribution of entailing texts in a corpus, thus
better simulates practical settings.
The task may seem as a variant of information
retrieval (IR), as it requires finding specific texts
in a corpus. Yet, it is fundamentally different from
IR for two reasons. First, the target output is a set
1http://opennlp.sourceforge.net
2http://www.nist.gov/tac/2009/Summarization/
771
of sentences, each evaluated independently, rather
than a set of documents. Second, the decision cri-
terion is entailment rather than relevance.
Despite the above, apparently, IR techniques
provided hard-to-beat baselines for the RTE
Search task (MacKinlay and Baldwin, 2009), out-
performing every other system that relied on in-
ference without IR-based pre-filtering. At the cur-
rent state of performance of entailment systems, it
seems that lexical coverage largely overshadows
any other approach in this task. Still, most (6 out
of 8) participants in the challenge applied their en-
tailment systems to the entire dataset without a
prior retrieval of candidate sentences. F1 scores
for such systems vary between 10% and 33%, in
comparison to over 40% of the IR-based methods.
4 The Baseline RTE System
In this work we used BIUTEE, Bar-Ilan Univer-
sity Textual Entailment Engine (Bar-Haim et al,
2008; Bar-Haim et al, 2009), a state of the art
RTE system, as a baseline and as a basis for our
discourse-based enhancements. This section de-
scribes this system?s architecture; the methods by
which it was augmented to address discourse are
presented in Section 5.
To determine entailment, BIUTEE performs the
following main steps:
Preprocessing First, all documents are parsed
and processed with standard tools for named en-
tity recognition (Finkel et al, 2005) and corefer-
ence resolution. For the latter purpose, we use
OpenNLP and enable the substitution of corefer-
ring terms. This is the only way by which BIUTEE
addresses discourse, representing the state of the
art in entailment systems.
Entailment-based transformations Given a
T-H pair (both represented as dependency
parse trees), the system applies a sequence of
knowledge-based entailment transformations over
T, generating a set of texts which are entailed by
it. The goal is to obtain consequent texts which
are more similar to H. Based on preliminary re-
sults on the development set, in our experiments
(Section 6) we use WordNet (Fellbaum, 1998) as
the system?s only knowledge resource, using its
synonymy, hyponymy and derivation relations.
Classification A supervised classifier, trained
on the development set, is applied to determine
entailment of each pair based on a set of syntactic
and lexical syntactic features assessing the degree
by which T and its consequents cover H.
5 Addressing Discourse
In the following subsections we describe the
prominent discourse phenomena that affect infer-
ence, which we have identified in an analysis of
the development set and addressed in our imple-
mentation. As mentioned, these phenomena are
poorly addressed by available reference resolvers
or fall completely out of their scope.
5.1 Augmented coreference set
A large number of coreference relations are com-
prised of terms which share lexical elements, (e.g.
?airliners?s first flight? and ?Airbus A380?s first
flight?). Although common in coreference rela-
tions, standard resolvers miss many of these cases.
For the purpose of identifying additional corefer-
ring terms, we consider two noun phrases in the
same document as coreferring if: (i) their heads
are identical and (ii) no semantic incompatibil-
ity is found between their modifiers. The types
of incompatibility we handle are: (a) mismatch-
ing numbers, (b) antonymy and (c) co-hyponymy
(coordinate terms), as specified by WordNet. For
example, two nodes of the noun distance would
be considered incompatible if one is modified by
short and the second by its antonym long. Simi-
larly, two modifier co-hyponyms of distance, such
as walking and running would also result such
an incompatibility. Adding more incompatibility
types (e.g. first vs. second flight) may further im-
prove the precision of this method.
5.2 Global information
Key terms or prominent pieces of information that
appear in the document, typically at the title or the
first few sentences, are many times perceived as
?globally? known throughout the document. For
example, the geographic location of the document
theme, mentioned at the beginning of the docu-
ment, is assumed to be known from that point on,
and will often not be mentioned explicitly in fur-
ther sentences. This is a bridging phenomenon
772
that is typically not addressed by available dis-
course processing tools. To compensate for that,
we identify key terms for each document based
on tf-idf scores and consider them as global in-
formation for that document. For example, global
terms for the topic discussing the ice melting in
the Arctic, typically contain a location such as
Arctic or Antarctica and terms referring to ice, like
permafrost or iceshelf.
We use a variant of tf-idf, where term frequency
is computed as follows: tf(ti,j) = ni,j+~?> ? ~fi,j .
Here, ni,j is the frequency of term i in document j
(ti,j), which is incremented by additional positive
weights (~?) for a set of features ( ~fi,j) of the term.
Based on our analysis, we defined the following
features, which correlated mostly with global in-
formation: (i) does the term appear in the title?
(ii) is it a proper name? (iii) is it a location? The
weights for these features are set empirically.
The document?s top-n global terms are added
to each of its sentences. As a result, a global term
that occurs in the hypothesis is matched in each
sentence of the document, regardless of whether
the term explicitly appears in the sentence.
Considering the previous sentence Another
method for addressing missing coreference and
bridging relations is based on the assumption that
adjacent sentences often refer to the same entities
and events. Thus, when extracting classification
features for a given sentence, in addition to the
features extracted from the parse tree of the sen-
tence itself, we extract the same set of features
from the current and previous sentences together.
Recall the example presented in Section 1. T is
annotated as entailing the hypothesis ?The AS-28
mini-submarine was trapped underwater?, but the
word submarine, e.g., appears only in its preced-
ing sentence T?. Thus, considering both sentences
together when classifying T increases its coverage
of the hypothesis. Indeed, a bridging reference re-
lates on board in T with submarine in T?, justify-
ing our assumption in this case.
5.3 Document-level classification
Beyond discourse references addressed above,
further information concerning discourse and doc-
ument structure is available in the Search setting
and may contribute to entailment classification.
We observed that entailing sentences tend to come
in bulks. This reflects a common coherence as-
pect, where the discussion of a specific topic is
typically continuous rather than scattered across
the entire document. This locality phenomenon
may be useful for entailment classification since
knowing that a sentence entails the hypothesis in-
creases the probability that adjacent sentences en-
tail the hypothesis as well.
To capture this phenomenon, we use a two-
phase meta-classification scheme, in which a
meta-classifier utilizes entailment classifications
of the first classification phase to extract meta-
features and determine the final classification de-
cision. This scheme also provides a convenient
way to combine scores from multiple classifiers
used in the first classification phase. We refer
to these as base-classifiers. This scheme and the
meta-features we used are detailed hereunder.
Let us write (s, h) for a sentence-hypothesis
pair. We denote the set of pairs in the development
(training) set asD and in the test set as T . We split
D into two halves, D1 and D2. We make use of n
base-classifiers, C1, . . . , Cn, among which C? is
a designated classifier with additional roles in the
process, as described below. Classifiers may dif-
fer, for example, in their classification algorithm.
An additional meta-classifier is denoted CM . The
classification scheme is shown as Algorithm 1.
Algorithm 1 Meta-classification
Training
1: Extract features for every (s, h) in D
2: Train C1, . . . , Cn on D1
3: Classify D2, using C1, . . . , Cn
4: Extract meta-features for D2 using the
classification of C1, . . . , Cn
5: Train CM on D2
Classification
6: Extract features for every (s, h) in T
7: Classify T using C1, . . . , Cn
8: Extract meta-features for T
9: Classify T using CM
At Step 1, features are extracted for every (s, h)
pair in the training set, as in the baseline system.
773
In Steps 2 and 3 we split the training set into two
halves (taking half of each topic), train n different
classifiers on the first half and then classify the
second half using each of the n classifiers. Given
the classification scores of the n base-classifiers
to the (s, h) pairs in the second half of the train-
ing set, D2, we add in Step 4 the meta-features
described in Section 5.3.1.
After adding the meta-features, we train
(Step 5) a meta-classifier on this new set of fea-
tures. Test sentences then go through the same
process: features are extracted for them and they
are classified by the already trained n classifiers
(Steps 6 and 7), meta-features are extracted in
Step 8, and a final classification decision is made
by the meta-classifier in Step 9.
A retrieval step may precede the actual en-
tailment classification, allowing the processing of
fewer and potentially ?better? candidates.
5.3.1 Meta-features
The following features are extracted in our
meta-classification scheme:
Classification scores The classification score of
each of the n base-classifiers.
Title entailment In many texts, and in news ar-
ticles in particular, the title and the first few sen-
tences often represent the entire document?s con-
tent. Thus, knowing whether these sentences en-
tail the hypothesis may be an indicator to the gen-
eral potential of the document to include entailing
sentences. Two binary features are added accord-
ing to the classification of C? indicating whether
the title entails the hypothesis and whether the first
sentence entails it.
Second-closest entailment Considering the lo-
cality phenomenon described above, we add a fea-
ture assigning higher scores to sentences in the
vicinity of an entailment environment. This fea-
ture is computed as the distance to the second-
closest entailing sentence in the document (count-
ing the sentence itself as well), according to the
classification ofC?. Formally, let i be the index of
the current sentence and J be the set of indices of
entailing sentences in the document according to
C?. For each j ? J we compute di,j = |i?j|, and
choose the second smallest di,j as di. The idea is
Ent?
# 1110987654321
NO NOYESYESYESYESNONONONONO
d
2nd cl
oses
t
8887 or 96 or 8777777
2 3111123456
d
Close
st
8887 or 96 or 8766666
1 2111112345
Figure 1: Comparison of the closest and second-closest
schemes when applied to a bulk of entailing sentences (in
white) situated within a non-entailing environment (in gray).
Unlike the closest one, the second-closest scheme assigns
larger distance values to non-entailing sentences located on
the ?edge? of the bulk (5 and 10) than to entailing ones.
that if entailing sentences indeed always come in
bulks, then di = 1 for all entailing sentences, but
di > 1 for all non-entailing ones. Figure 1 illus-
trates such a case, comparing the second-closest
distance with the distance to the closest entailing
sentence. In the closest scheme we do not count
the sentence as closest to itself since it would dis-
regard the environment of the sentence altogether,
eliminating the desired effect. We scale the dis-
tance and add the feature score: ? log(di).
Smoothed entailment This feature addressed
the locality phenomenon by smoothing the
classification score of sentence i with the scores
of adjacent sentences, weighted by their distance
from the current sentence i. Let s(i) be the
score assigned by C? to sentence i. We add the
Smoothed Entailment feature score:
SE(i) =
?
w(b|w|?s(i+w))?
w(b|w|)
where 0 < b < 1 is the decay parameter and w is
an integer bounded between?N and N , denoting
the distance from sentence i.
1st sentence entailing title Bensley and Hickl
(2008) showed that the first sentence in a news ar-
ticle typically entails the article?s title. We there-
fore assume that in each document, s1 ? s0,
where s1 and s0 are the document?s first sentence
and title respectively. Hence, under entailment
transitivity, if s0 ? h then s1 ? h. The cor-
responding binary feature states whether the sen-
tence being classified is the document?s first sen-
tence and the title entails h according to C?.
774
P (%) R (%) F1 (%)
BIU-BL 14.53 55.25 23.00
BIU-DISC 20.82 57.25 30.53
BIU-BL3 14.86 59.00 23.74
BIU-DISCno?loc 22.35 57.12 32.13All-yes baseline 4.6 100.0 8.9
Table 1: Micro-average results.
Note that the above locality-based features rely
on high accuracy of the base classifier C?. Oth-
erwise, it will provide misleading information to
the features computation. We analyze the effect
of this accuracy in Section 6.
6 Results and Analysis
Using the RTE-5 Search data, we compare
BIUTEE in its baseline configuration (cf. Sec-
tion 4), denoted BIU-BL, with its discourse-aware
enhancement (BIU-DISC) which uses all the com-
ponents described in Section 5. To alleviate the
strong IR effect described in Section 3, both sys-
tems are applied to the complete datasets (both
training and test), without candidates pre-filtering.
BIU-DISC uses three base-classifiers (n = 3):
SVMperf (Joachims, 2006), and Na??ve Bayes and
Logistic Regression from the WEKA package
(Witten and Frank, 2005). The first among these
is set as our designated classifier C?, which is
used for the computation of the document-level
features. SVMperf is also used for the meta-
classifier. For the smoothed entailment score (cf.
Section 5.3), we used b = 0.9 and N = 3. Global
information is added by enriching each sentence
with the highest-ranking term in the document, ac-
cording to tf-idf scores (cf. Section 5.2), where
document frequencies were computed based on
about half a million documents from the TIP-
STER corpus (Harman, 1992). The set of weights
~? equals {2, 1, 4} for title terms, proper names and
locations, respectively. All parameters were tuned
based on a 10-fold cross-validation on the devel-
opment set, optimizing the micro-averaged F1.
The results are presented in Table 1. As can be
seen in the table, BIU-DISC outperforms BIU-BL in
every measure, showing the impact of addressing
discourse in this setting. To rule out the option that
the improvement is simply due to the fact that we
use three classifiers for BIU-DISC and a single one
P (%) R (%) F1 (%)
By Topic
BIU-BL 16.54 55.62 25.50
BIU-DISC 22.69 57.96 32.62
All-yes baseline 4.85 100.00 9.25
By Hypothesis
BIU-BL 22.87 59.62 33.06
BIU-DISC 27.81 61.97 38.39
All-yes baseline 4.96 100.00 9.46
Table 2: Macro-average results.
for BIU-BL, we show (BIU-BL3) the results when
the baseline system is applied in the same meta-
classification configuration as BIU-DISC, with the
same three classifiers. Apparently, without the
discourse information this configuration?s contri-
bution is limited.
As mentioned in Section 5.3, the benefit from
the locality features rely directly on the perfor-
mance of the base classifiers. Hence, considering
the low precision scores obtained here, we applied
BIU-DISC to the data in the meta-classification
scheme, but with locality features removed. The
results, shown as BIU-DISCno?loc in the Table, in-
dicate that indeed performance increases without
these features. The last line of the table shows the
results obtained by a na??ve baseline where all test-
set pairs are considered entailing.
For completeness, Table 2 shows the macro-
averaged results, when averaged over the topics or
over the hypotheses. Although we tuned our sys-
tem to maximize micro-averaged F1, these figures
comply with the ones shown in Table 1.
Analysis of locality As discussed in Section 5,
determining whether a sentence entails a hypothe-
sis should take into account whether adjacent sen-
tences also entail the hypothesis. In the above ex-
periment we were unable to show the contribution
of our system?s component that attempts to cap-
ture this information; on the contrary, the results
show it had a negative impact on performance.
Still, we claim that this information can be use-
ful when used within a more accurate system. We
try to validate this conjecture by understanding
how performance of the locality features varies as
the systems becomes more accurate. We do so via
the following simulation.
When classifying a certain sentence, the classi-
775
2025303540 0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
p
F1
Figure 2: F1 performance of BIU-DISC as a function of
the accuracy in classifying adjacent sentences.
fications of its adjacent sentences are given by an
oracle classifier that provides the correct answer
with probability p. The system is applied using
two locality features: the 1st sentence entailing
title feature and a close variant of the smoothed
entailment feature, which calculates the weighted
average of adjacent sentences, but disregards the
score of the currently evaluated sentence.3 Thus
we supply information about adjacent sentences
and test whether overall performance increases
with the accuracy of this information.
We performed this experiment for p in a range
of [0.5-1.0]. Figure 2 shows the results of this sim-
ulation, based on the average F1 of five runs for
each p. Since performance, from a certain point,
increases with the accuracy of the oracle classi-
fier, we can conclude that indeed precise infor-
mation about adjacent sentences improves perfor-
mance on the current sentence, and that locality is
a true phenomenon in the data. We note, however,
that performance improves only when accuracy is
very high, suggesting the currently limited prac-
tical potential of this information, at least in the
way locality was represented in this work.
Ablation tests Table 3 presents the results of the
ablation tests performed to evaluate the contribu-
tion of each component. Based on the result re-
ported in Table 1 and the above discussion, the
tests were performed relative to BIU-DISCno?loc,
the optimal configuration. As seen in the table,
the removal of each component causes a drop
in results. For global information we see a mi-
3The second-closest entailment feature was not used as it
considers the oracle?s decision for the current sentence, while
we wish to use only information about adjacent sentences.
Component removed F1 (%) ?F1 (%)
Previous sent. features 28.55 3.58
Augmented coref. 26.73 5.40
Global information 31.76 0.37
Table 3: Results of ablation tests relative to
BIU-DISCno?loc. The columns specify the compo-nent removed, the micro-averaged F1 score achieved without
it, and the marginal contribution of the component.
nor difference, which is not surprising considering
the conservative approach we took, using a sin-
gle global term for each sentence. Possibly, this
information is also included in the other compo-
nents, thus proving no marginal contribution rel-
ative to them. Under the conditions of an over-
whelming majority of negative examples, this is
a risky method to use, and should be considered
when the ratio of positive examples is higher. For
future work, we intend to use this information via
classification features (e.g. the coverage obtained
with and without global information), rather than
the crude addition of the term to the sentence.
Analysis of augmented coreferences We an-
alyzed the performance of the component for
augmenting coreference relations relative to the
OpenNLP resolver. Recall that our component
works on top of the resolver?s output and can add
or remove coreference relations. As a complete
annotation of coreference chains in the dataset is
unavailable, we performed the following evalua-
tion. Recall is computed based on the number
of identified pairs from a sample of 100 intra-
document coreference and bridging relations from
the annotated dataset described in (Mirkin et al,
2010). Precision is computed based on 50 pairs
sampled from the output of each method, equally
distributed over topics. The results, shown in Ta-
ble 4, indicate the much higher recall obtained
by our component at some cost in precision. Al-
though rather simple, the ablation test of this com-
ponent shows its usefulness. Still, both methods
achieve low absolute recall, suggesting the need
for more robust tools for this task.
P (%) R (%) F1 (%)
OpenNLP 74 16 26.3
Augmented coref. 60 28 38.2
Table 4: Performance of coreference methods.
776
05101520253035404550 0
10
20
30
40
50
60
70
80
90
100
k
F1
BIU-B
L
BIU-D
ISC
Lucen
e
Figure 3: F1 performance as a function of the number of
retrieved candidates.
Candidate retrieval setting As mentioned in
Section 3, best performance of RTE systems in the
task was obtained when applying a first step of IR-
based candidate filtering. We therefore compare
the performance of BIU-DISC with that of BIU-BL
under this setting as well.4 For candidate retrieval
we used Lucene, a state of the art search engine5,
in a range of top-k retrieved candidates. The re-
sults are shown in Figure 3. For reference, the fig-
ure also shows the performance along this range
of Lucene as-is, when no further inference is ap-
plied to the retrieved candidates.
While BIU-DISC does not outperform BIU-BL at
every point, the area under the curve is clearly
larger for BIU-DISC. The figure also indicates that
BIU-DISC is far more robust, maintaining a stable
F1 and enabling a stable tradeoff between recall
and precision along the whole range (recall ranges
between 42% and 55% for k ? [15 ? 100], with
corresponding precision range of 51% to 33%).
Finally, Table 5 shows the results of the best
systems as determined in our first experiment.
We performed a single experiment to compare
BIU-DISCno?loc and BIU-BL3 under a candidate re-
trieval setting, using k = 20, where both systems
highly perform. We compare these results to the
highest score obtained by Lucene, as well as to the
two best submissions to the RTE-5 Search task6.
BIU-DISCno?loc outperforms all other methods and
its result is significantly better than BIU-BL3 with
p < 0.01 according to McNemar?s test.
4This time, for global information, the document?s three
highest ranking terms were added to each sentence.
5http://lucene.apache.org
6The best one is an earlier version of this work (Mirkin et
al., 2009); the second is MacKinlay and Baldwin?s (2009).
P (%) R (%) F1 (%)
BIU-DISCno?loc 50.77 45.12 47.78
BIU-BL3 51.68 40.38 45.33
Lucene, top-15 35.93 52.50 42.66
RTE-5 best 40.98 51.38 45.59
RTE-5 second-best 42.94 38.00 40.32
Table 5: Performance of best configurations.
7 Conclusions
While it is generally assumed that discourse inter-
acts with semantic entailment inference, the con-
crete impacts of discourse on such inference have
been hardly explored. This paper presented a first
empirical investigation of discourse processing
aspects related to entailment. We argue that avail-
able discourse processing tools should be substan-
tially improved towards this end, both in terms of
the phenomena they address today, namely nom-
inal coreference, and with respect to the cover-
ing of additional phenomena, such as bridging
anaphora. Our experiments show that even rather
simple methods for addressing discourse can have
a substantial positive impact on the performance
of entailment inference. Concerning the local-
ity phenomenon stemming from discourse coher-
ence, we learned that it does carry potentially use-
ful information, which might become beneficial
in the future when better-performing entailment
systems become available. Until then, integrating
this information with entailment confidence may
be useful. Overall, we suggest that entailment sys-
tems should extensively incorporate discourse in-
formation, while developing sound algorithms for
addressing various discourse phenomena, includ-
ing the ones described in this paper.
Acknowledgements
The authors are thankful to Asher Stern and Ilya
Kogan for their help in implementing and evalu-
ating the augmented coreference component, and
to Roy Bar-Haim for useful advice concerning
this paper. This work was partially supported
by the Israel Science Foundation grant 1112/08
and the PASCAL-2 Network of Excellence of the
European Community FP7-ICT-2007-1-216886.
Jonathan Berant is grateful to the Azrieli Foun-
dation for the award of an Azrieli Fellowship.
777
References
Bar-Haim, Roy, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proc. of Text Analysis Conference (TAC).
Bar-Haim, Roy, Jonathan Berant, and Ido Dagan.
2009. A compact forest for scalable inference
over entailment and paraphrase rules. In Proc. of
EMNLP.
Bensley, Jeremy and Andrew Hickl. 2008. Unsuper-
vised resource creation for textual inference appli-
cations. In Proc. of LREC.
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009a. Considering discourse refer-
ences in textual entailment annotation. In Proc. of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009b. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Castillo, Julio J. 2009. Sagan in TAC2009: Using
support vector machines in recognizing textual en-
tailment and TE search pilot task. In Proc. of TAC.
Clark, Peter and Phil Harrison. 2009. An inference-
based approach to recognizing entailment. In Proc.
of TAC.
Clark, Herbert H. 1975. Bridging. In Schank, R. C.
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169?174. As-
sociation of Computing Machinery.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Dagan, Ido, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, pages 15(4):1?17.
Dali, Lorand, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Question
answering based on semantic graphs. In Proc. of the
Workshop on Semantic Search (SemSearch 2009).
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). The MIT Press.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Harabagiu, Sanda and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proc. of ACL.
Harman, Donna. 1992. The DARPA TIPSTER
project. SIGIR Forum, 26(2):26?28.
Joachims, Thorsten. 2006. Training linear SVMs in
linear time. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Li, Fangtao, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of ACL-IJCNLP.
Litkowski, Ken. 2009. Overlap analysis in textual en-
tailment recognition. In Proc. of TAC.
MacKinlay, Andrew and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Mirkin, Shachar, Roy Bar-Haim, Jonathan Berant, Ido
Dagan Eyal Shnarch, Asher Stern, and Idan Szpek-
tor. 2009. Addressing discourse and document
structure in the RTE search task. In Proc. of TAC.
Mirkin, Shachar, Ido Dagan, and Sebastian Pado?.
2010. Assessing the role of discourse references in
entailment inference. In Proc. of ACL.
Nenkova, Ani, Rebecca Passonneau, and Kathleen
Mckeown. 2007. The pyramid method: incorpo-
rating human content selection variation in summa-
rization evaluation. In ACM Transactions on Speech
and Language Processing.
Qiu, Long, Min-Yen Kan, and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proc. of LREC.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor,
and Ido Dagan. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proc. of EACL.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition. Morgan Kaufmann, San Francisco.
778
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1209?1219,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Assessing the Role of Discourse References in Entailment Inference
Shachar Mirkin, Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
{mirkins,dagan}@cs.biu.ac.il
Sebastian Pado?
University of Stuttgart
Stuttgart, Germany
pado@ims.uni-stuttgart.de
Abstract
Discourse references, notably coreference
and bridging, play an important role in
many text understanding applications, but
their impact on textual entailment is yet to
be systematically understood. On the ba-
sis of an in-depth analysis of entailment
instances, we argue that discourse refer-
ences have the potential of substantially
improving textual entailment recognition,
and identify a number of research direc-
tions towards this goal.
1 Introduction
The detection and resolution of discourse refer-
ences such as coreference and bridging anaphora
play an important role in text understanding appli-
cations, like question answering and information
extraction. There, reference resolution is used for
the purpose of combining knowledge from multi-
ple sentences. Such knowledge is also important
for Textual Entailment (TE), a generic framework
for modeling semantic inference. TE reduces the
inference requirements of many text understand-
ing applications to the problem of determining
whether the meaning of a given textual assertion,
termed hypothesis (H), can be inferred from the
meaning of certain text (T ) (Dagan et al, 2006).
Consider the following example:
(1) T: ?Not only had he developed an aversion
to the President1 and politics in general,
Oswald2 was also a failure with Marina, his
wife. [...] Their relationship was supposedly
responsible for why he2 killed Kennedy1.?
H: ?Oswald killed President Kennedy.?
The understanding that the second sentence of the
text entails the hypothesis draws on two corefer-
ence relationships, namely that he is Oswald, and
that the Kennedy in question is President Kennedy.
However, the utilization of discourse information
for such inferences has been so far limited mainly
to the substitution of nominal coreferents, while
many aspects of the interface between discourse
and semantic inference needs remain unexplored.
The recently held Fifth Recognizing Textual
Entailment (RTE-5) challenge (Bentivogli et al,
2009a) has introduced a Search task, where the
text sentences are interpreted in the context of their
full discourse, as in Example 1 above. Accord-
ingly, TE constitutes an interesting framework ?
and the Search task an adequate dataset ? to study
the interrelation between discourse and inference.
The goal of this study is to analyze the roles
of discourse references for textual entailment in-
ference, to provide relevant findings and insights
to developers of both reference resolvers and en-
tailment systems and to highlight promising direc-
tions for the better incorporation of discourse phe-
nomena into inference. Our focus is on a manual,
in-depth assessment that results in a classification
and quantification of discourse reference phenom-
ena and their utilization for inference. On this ba-
sis, we develop an account of formal devices for
incorporating discourse references into the infer-
ence computation. An additional point of inter-
est is the interrelation between entailment knowl-
edge and coreference. E.g., in Example 1 above,
knowing that Kennedy was a president can alle-
viate the need for coreference resolution. Con-
versely, coreference resolution can often be used
to overcome gaps in entailment knowledge.
Structure of the paper. In Section 2, we pro-
vide background on the use of discourse refer-
ences in natural language processing (NLP) in
general and specifically in TE. Section 3 describes
the goals of this study, followed by our analy-
sis scheme (Section 4) and the required inference
1209
mechanisms (Section 5). Section 6 presents quan-
titative findings and further observations. Conclu-
sions are discussed in Section 7.
2 Background
2.1 Discourse in NLP
Discourse information plays a role in a range
of NLP tasks. It is obviously central to dis-
course processing tasks such as text segmenta-
tion (Hearst, 1997). Reference information pro-
vided by discourse is also useful for text under-
standing tasks such as question answering (QA),
information extraction (IE) and information re-
trieval (IR) (Vicedo and Ferrndez, 2006; Zelenko
et al, 2004; Na and Ng, 2009), as well as for the
acquisition of lexical-semantic ?narrative schema?
knowledge (Chambers and Jurafsky, 2009). Dis-
course references have been the subject of atten-
tion in both the Message Understanding Confer-
ence (Grishman and Sundheim, 1996) and the Au-
tomatic Content Extraction program (Strassel et
al., 2008).
The simplest form of information that discourse
provides is coreference, i.e., information that two
linguistic expressions refer to the same entity or
event. Coreference is particularly important for
processing pronouns and other anaphoric expres-
sions, such as he in Example 1. Ability to re-
solve this reference translates directly into, e.g., a
QA system?s ability to answer questions like Who
killed Kennedy?.
A second, more complex type of information
stems from bridging references, such as in the fol-
lowing discourse (Asher and Lascarides, 1998):
(2) ?I?ve just arrived. The camel is outside.?
While coreference indicates equivalence, bridging
points to the existence of a salient semantic rela-
tion between two distinct entities or events. Here,
it is (informally) ?means of transport?, which
would make the discourse (2) relevant for a ques-
tion like How did I arrive here?. Other types of
bridging relations include set-membership, roles
in events and consequence (Clark, 1975).
Note, however, that text understanding systems
are generally limited to the resolution of entity (or
even just pronoun) coreference, e.g. (Li et al,
2009; Dali et al, 2009). An important reason is the
unavailability of tools to resolve the more complex
(and difficult) forms of discourse reference such as
event coreference and bridging.1 Another reason
is uncertainty about their practical importance.
2.2 Discourse in Textual Entailment
Textual Entailment has been introduced in Sec-
tion 1 as a common-sense notion of inference.
It has spawned interest in the computational lin-
guistics community as a common denominator of
many NLP tasks including IE, summarization and
tutoring (Romano et al, 2006; Harabagiu et al,
2007; Nielsen et al, 2009).
Architectures for Textual Entailment. Over
the course of recent RTE challenges (Giampic-
colo et al, 2007; Giampiccolo et al, 2008), the
main benchmark for TE technology, two archi-
tectures for modeling TE have emerged as dom-
inant: transformations and alignment. The goal
of transformation-based TE models is to deter-
mine the entailment relation T ? H by find-
ing a ?proof?, i.e., a sequence of consequents,
(T, T1, . . . , Tn), such that Tn=H (Bar-Haim et al,
2008; Harmeling, 2009), and that in each trans-
formation, Ti? Ti+1, the consequent Ti+1 is en-
tailed by Ti. These transformations commonly in-
clude lexical modifications and the generation of
syntactic alternatives. The second major approach
constructs an alignment between the linguistic en-
tities of the trees (or graphs) of T and H , which
can represent syntactic structure, semantic struc-
ture, or non-hierarchical phrases (Zanzotto et al,
2009; Burchardt et al, 2009; MacCartney et al,
2008). H is assumed to be entailed by T if its en-
tities are aligned ?well? to corresponding entities
in T . Alignment quality is generally determined
based on features that assess the validity of the lo-
cal replacement of the T entity by the H entity.
While transformation- and alignment-based en-
tailment models look different at first glance, they
ultimately have the same goal, namely obtaining
a maximal coverage of H by T , i.e. to identify
matches of as many elements of H within T as
possible.2 To do so, both architectures typically
make use of inference rules such as ?Y was pur-
chased by X? X paid for Y?, either by directly ap-
plying them as transformations, or by using them
1Some studies, e.g. (Markert et al, 2003; Poesio et al,
2004), address the resolution of a few specific kinds of bridg-
ing relations; yet, wide-scope systems for bridging resolution
are unavailable.
2Clearly, the details of how the final entailment decision
is made based on the attained coverage differ substantially
among models.
1210
to score alignments. Rules are generally drawn
from external knowledge resources, such as Word-
Net (Fellbaum, 1998) or DIRT (Lin and Pantel,
2001), although knowledge gaps remain a key ob-
stacle (Bos, 2005; Balahur et al, 2008; Bar-Haim
et al, 2008).
Discourse in previous RTE challenges. The
first two rounds of the RTE challenge used ?self-
contained? texts and hypotheses, where discourse
considerations played virtually no role. A first step
towards a more comprehensive notion of entail-
ment was taken with RTE-3 (Giampiccolo et al,
2007), when paragraph-length texts were first in-
cluded and constituted 17% of the texts in the test
set. Chambers et al (2007) report that in a sample
of T ? H pairs drawn from the development set,
25% involved discourse references.
Using the concepts introduced above, the im-
pact of discourse references can be generally de-
scribed as a coverage problem, independent of the
system?s architecture. In Example 1, the hypoth-
esis word Oswald cannot be safely linked to the
text pronoun he without further knowledge about
he; the same is true for ?Kennedy ? President
Kennedy? which involves a specialization that is
only warranted in the specific discourse.
A number of systems have tried to address the
question of coreference in RTE as a preprocessing
step prior to inference proper, with most systems
using off-the-shelf coreference resolvers such as
JavaRap (Qiu et al, 2004) or OpenNLP3. Gen-
erally, anaphoric expressions were textually re-
placed by their antecedents. Results were in-
conclusive, however, with several reports about
errors introduced by automatic coreference res-
olution (Agichtein et al, 2008; Adams et al,
2007). Specific evaluations of the contribution
of coreference resolution yielded both small nega-
tive (Bar-Haim et al, 2008) and insignificant pos-
itive (Chambers et al, 2007) results.
3 Motivation and Goals
The results of recent studies, as reported in Sec-
tion 2.2, seem to show that current resolution of
discourse references in RTE systems hardly af-
fects performance. However, our intuition is that
these results can be attributed to four major lim-
itations shared by these studies: (1) the datasets,
where discourse phenomena were not well repre-
3http://opennlp.sourceforge.net
sented; (2) the off-the-shelf coreference resolution
systems which may have been not robust enough;
(3) the limitation to nominal coreference; and (4)
overly simple integration of reference information
into the inference engines.
The goal of this paper is to assess the impact of
discourse references on entailment with an anno-
tation study which removes these limitations. To
counteract (1), we use the recent RTE-5 Search
dataset (details below). To avoid (2), we perform
a manual analysis, assuming discourse references
as predicted by an oracle. With regards to (3), our
annotation scheme covers coreference and bridg-
ing relations of all syntactic categories and classi-
fies them. As for (4), we suggest several opera-
tions necessary to integrate the discourse informa-
tion into an entailment engine.
In contrast to the numerous existing datasets
annotated for discourse references (Hovy et al,
2006; Strassel et al, 2008), we do not annotate ex-
haustively. Rather, we are interested specifically in
those references instances that impact inference.
Furthermore, we analyze each instance from an
entailment perspective, characterizing the relevant
factors that have an impact on inference. To our
knowledge, this is the first such in-depth study.4
The results of our study are of twofold interest.
First, they provide guidance for the developers of
reference resolvers who might prioritize the scope
of their systems to make them more valuable for
inference. Second, they point out potential direc-
tions for the developers of inference systems by
specifying what additional inference mechanisms
are needed to utilize discourse information.
The RTE-5 Search dataset. We base our anno-
tation on the Search task dataset, a new addition
to the recent Fifth RTE challenge (Bentivogli et
al., 2009a) that is motivated by the needs of NLP
applications and drawn from the TAC summariza-
tion track. In the Search task, TE systems are re-
quired to find all individual sentences in a given
corpus which entail the hypothesis ? a setting that
is sensible not only for summarization, but also for
information access tasks like QA. Sentences are
judged individually, but ?are to be interpreted in
the context of the corpus as they rely on explicit
and implicit references to entities, events, dates,
places, etc., mentioned elsewhere in the corpus?
(Bentivogli et al, 2009b).
4The guidelines and the dataset are available at
http://www.cs.biu.ac.il/? nlp/downloads/
1211
Text Hypothesis
i T
? Once the reform becomes law, Spain will join the Netherlands
and Belgium in allowing homosexual marriages. Massachusetts allows homosexual
T Such unions are also legal in six Canadian provinces and the
northeastern US state of Massachusetts.
marriages
T ? The official name of 2003 UB313 has yet to be determined.
ii T Brown said he expected to find a moon orbiting Xena because
many Kuiper Belt objects are paired with moons.
2003 UB313 is in the Kuiper Belt
iii
T ?a
All seven aboard the AS-28 submarine appeared to be in satis-
factory condition, naval spokesman said.
T ?b
British crews were working with Russian naval authorities to ma-
neuver the unmanned robotic vehicle and untangle the AS-28.
The AS-28 mini submarine was trapped
underwater
T The Russian military was racing against time early Friday to res-
cue a mini submarine trapped on the seabed.
iv T
? China seeks solutions to its coal mine safety. A mining accident in China has killed
several minersT A recent accident has cost more than a dozen miners their lives.
v
T ??
A remote-controlled device was lowered to the stricken vessel to
cut the cables in which the AS-28 vehicle is caught.
T ?
The mini submarine was resting on the seabed at a depth of about
200 meters.
The AS-28 mini submarine was trapped
underwater
T Specialists said it could have become tangled up with a metal
cable or in sunken nets from a fishing trawler.
vi T . . . dried up lakes in Siberia, because the permafrost beneaththem has begun to thaw.
The ice is melting in the Arctic
Table 1: Examples for discourse-dependent entailment in the RTE-5 dataset, where the inference of H
depends on reference information from the discourse sentences T ? / T ??. Referring terms (in T ) and target
terms (in H) are shown in boldface.
4 Analysis Scheme
For annotating the RTE-5 data, we operationalize
reference relations that are relevant for entailment
as those that improve coverage. Recall from Sec-
tion 2.2 that the concept of coverage is applicable
to both transformation and alignment models, all
of which aim at maximizing coverage of H by T .
We represent T and H as syntactic trees, as
common in the RTE literature (Zanzotto et al,
2009; Agichtein et al, 2008). Specifically, we
assume MINIPAR-style (Lin, 1993) dependency
trees where nodes represent text expressions and
edges represent the syntactic relations between
them. We use ?term? to refer to text expressions,
and ?components? to refer to nodes, edges, and
subtrees. Dependency trees are a popular choice
in RTE since they offer a fairly semantics-oriented
account of the sentence structure that can still be
constructed robustly. In an ideal case of entail-
ment, all nodes and dependency edges of H are
covered by T .
For each T ? H pair, we annotate all relevant
discourse references in terms of three items: the
target component in H , the focus term in T , and
the reference term which stands in a reference re-
lation to the focus term. By resolving this ref-
erence, the target component can usually be in-
ferred; sometimes, however, more than one ref-
erence term needs to be found. We now define
and illustrate these concepts on examples from
Table 1.5
The target component is a tree component in
H that cannot be covered by the ?local? material
from T . An example for a tree component is Ex-
ample (v), where the target component AS-28 mini
submarine in H cannot be inferred from the pro-
noun it in T . Example (vi) demonstrates an edge
as target component. In this case, the edge in H
connecting melt with the modifier in the Arctic is
not found in T . Although each of the hypothesis?
nodes can be covered separately via knowledge-
based rules (e.g. ?Siberia ? Arctic?, ?permafrost
? ice?, ?thaw ? melt?), the resulting fragments
in T are unconnected without the (intra-sentential)
coreference between them and lakes in Siberia.
For each target component, we identify its focus
term as the expression in T that does not cover the
target component itself but participates in a refer-
ence relation that can help covering it.
We follow the focus term?s reference chain to
a reference term which can, either separately or
in combination with the focus term, help covering
the target component. In Example (ii), where the
5In our annotation, we assume throughout that some
knowledge about basic admissible transformations is avail-
able, such as passive to active or derivational transformations;
for brevity, we ignore articles in the examples and treat named
entities as single nodes.
1212
target component in H is 2003 UB313, Xena is the
focus term in T and the reference term is a men-
tion of 2003 UB313 in a previous sentence, T ?. In
this case, the reference term covers the entire tar-
get component on its own.
An additional attribute that we record for each
instance is whether resolving the discourse refer-
ence is mandatory for determining entailment, or
optional. In Example (v), it is mandatory: the in-
ference cannot be completed without the knowl-
edge provided by the discourse. In contrast, in
Example (ii), inferring 2003 UB313 from Xena
is optional. It can be done either by identify-
ing their coreference relation, or by using back-
ground knowledge in the form of an entailment
rule, ?Xena ? 2003 UB313?, that is applicable
in the context of astronomy. Optional discourse
references represent instances where discourse in-
formation and TE knowledge are interchange-
able. As mentioned, knowledge gaps constitute
a major obstacle for TE systems, and we can-
not rely on the availability of any ceratin piece of
knowledge to the inference process. Thus, in our
scheme, mandatory references provide a ?lower
bound? with regards to the necessity to resolve
discourse references, even in the presence of com-
plete knowledge; optional references, on the other
hand, set an ?upper bound? for the contribution of
discourse resolution to inference, when no knowl-
edge is available. At the same time, this scheme
allows investigating how much TE knowledge can
be replaced by (perfect) discourse processing.
When choosing a reference term, we search the
reference chain of the focus term for the nearest
expression that is identical to the target component
or a subcomponent of it. If we find such an expres-
sion, covering the identical part of the target com-
ponent requires no entailment knowledge. If no
identical reference term exists, we choose the se-
mantically ?closest? term from the reference chain,
i.e. the term which requires the least knowledge to
infer the target component. For instance, we may
pick permafrost as the semantically closet term to
the target ice if the latter is not found in the focus
term?s reference chain.
Finally, for each reference relation that we an-
notate, we record four additional attributes which
we assumed to be informative in an evaluation.
First, the reference type: Is the relation a coref-
erence or a bridging reference? Second, the syn-
tactic type of the focus and reference terms. Third,
the focus/reference terms entailment status ? does
some kind of entailment relation hold between the
two terms? Fourth, the operation that should be
performed on the focus and reference terms to ob-
tain coverage of the target component (as specified
in Section 5).
5 Integrating Discourse References into
Entailment Recognition
In initial analysis we found that the standard sub-
stitution operation applied by virtually all previous
studies for integrating coreference into entailment
is insufficient. We identified three distinct cases
for the integration of discourse reference knowl-
edge in entailment, which correspond to different
relations between the target component, the fo-
cus term and the reference term. This section de-
scribes the three cases and characterizes them in
terms of tree transformations. An initial version of
these transformations is described in (Abad et al,
2010). We assume a transformation-based entail-
ment architecture (cf. Section 2.2), although we
believe that the key points of our account are also
applicable to alignment-based architecture. Trans-
formations create revised trees that cover previ-
ously uncovered target components in H . The
output of each transformation, T1, is comprised
of copies of the components used to construct it,
and is appended to the discourse forest, which in-
cludes the dependency trees of all sentences and
their generated consequents.
We assume that we have access to a dependency
tree for H , a dependency forest for T and its dis-
course context, as well as the output of a perfect
discourse processor, i.e., a complete set of both
coreference and bridging relations, including the
type of bridging relation (e.g. part-of, cause).
We use the following notation. We use x, y
for tree nodes, and Sx to denote a (sub-)tree with
root x. lab(x) is the label of the incoming edge
of x (i.e., its grammatical function). We write
C(x, y) for a coreference relation between Sx and
Sy, the corresponding trees of the focus and refer-
ence terms, respectively. We write Br(x, y) for a
bridging relation, where r is its type.
(1) Substitution: This is the most intuitive and
widely-used transformation, corresponding to the
treatment of discourse information in existing sys-
tems. It applies to coreference relations, when an
expression found elsewhere in the text (the refer-
ence term) can cover all missing information (the
1213
be lega
l als
o
unio
n
suc
h
pred
mod
subj
be lega
l
also
mar
riage
s
hom
ose
xua
lpre
d mod
subj
mod
T
T 1
mar
riage
s
hom
ose
xua
l
modT?
pre
Figure 1: The Substitution transformation, demon-
strated on the relevant subtrees of Example (i).
The dashed line denotes a discourse reference.
target component) on its own. In such cases, the
reference term can replace the entire focus term.
Apparently (cf. Section 6), substitution applies
also to some types of bridging relations, such as
set-membership, when the member is sufficient for
representing the entire set for the necessary infer-
ence. For example, in ?I met two people yesterday.
The woman told me a story.? (Clark, 1975), sub-
stituting two people with woman results in a text
which is entailed from the discourse, and which
allows inferring ?I met a woman yesterday.?
In a parse tree representation, given a corefer-
ence relation C(x, y) (or Br(x, y)), the newly gen-
erated tree, T1, consists of a copy of T , where the
entire tree Sx is replaced by a copy of Sy . In Fig-
ure 1, which shows Example (i) from Table 1, such
unions is substituted by homosexual marriages.
Head-substitution. Occasionally, substituting
only the head of the focus term is sufficient. In
such cases, only the root nodes x and y are sub-
stituted. This is the case, for example, with syn-
onymous verbs with identical subcategorization
frames (like melt and thaw). As verbs typically
constitute tree roots in dependency parses, sub-
stituting or merging (see below) their entire trees
might be inappropriate or wasteful. In such cases,
the simpler head-substitution may be applied.
(2) Merge: In contrast to substitution, where a
match for the entire target component is found
elsewhere in the text, this transformation is re-
quired when parts of the missing information are
scattered among multiple locations in the text.
We distinguish between two types of merge trans-
formations: (a) dependent-merge, and (b) head-
merge, depending on the syntactic roles of the
merged components.
(a) Dependent-Merge. This operation is ap-
plicable when the head of either the focus or ref-
erence terms (of both) matches the head node of
subm
arine
mini
ontrapp
ed mod
T
T 1
subm
arine AS-2
8nnT? a
pcom
p-n
pnm
od
mod
sea
bed
subm
arine
mini
trapp
ed
mod
pnm
od
mod
AS-2
8nn
AS-2
8  
T? b
on
pcom
p-n sea
bed
Figure 2: The dependent-merge (T ?a) and head-
merge (T ?b) transformations (Example (iii)).
the target component, but modifiers from both of
them are required to cover the target component?s
dependents. The modifiers are therefore merged
as dependents of a single head node, to create
a tree that covers the entire target component.
Dependent-merge is illustrated in Figure 2, using
Example (iii). The component we wish to cover in
H is the noun phrase AS-28 mini submarine. Un-
fortunately, the focus term in T , ?mini submarine
trapped on the seabed?, covers only the modifier
mini, but not AS-28. This modifier can however be
provided by the coreferent term in T ?a (left upper
corner). Once merged, the inference engine can,
e.g., employ the rule ?on seabed ? underwater?
to cover H completely.
Formally, assume without loss of generality that
y, the reference term?s head, matches the root node
of the target component. Given C(x, y), we define
T1 as a copy of T , where (i) the subtree Sx is re-
placed by Sy, and (ii) for all children c of x, a copy
of Sc is placed under the copy of y in T1 with its
original edge label, lab(c).
(b) Head-merge. An alternative way to recover
the missing information in Example (iii) is to find
a reference term whose head word itself (rather
than one of its modifiers) matches the target com-
ponent?s missing dependent, as with AS-28 in Fig-
ure 2 in the bottom left corner (T ?b). In terms of
parse trees, we need to add one tree as a depen-
dent of the other. Formally, given C(x, y), simi-
larly to dependent-merge, T1 is created as a copy
of T where the subtree Sx is replaced by either Sx
or Sy, depending on whichever of x and y matches
the target component?s head. Assume it is x, for
example. Then, a copy of Sy is added as a new
child to x. In our sample, head-merge operations
correspond to internal coreferences within nomi-
nal target components (such as between AS-28 and
mini submarine in this case). The appropriate la-
bel, lab(y), in these cases is nn (nominal modi-
1214
in
T
T 1
T?
pcom
p-n Chin
acost have
thanmore comp1 pcomp
-nobj
have
doze
n
acc
identsubj recen
t
mod
cos
t have
thanmore co
mp1 pcom
p-n
obj
have
doze
n
acc
identsubj recen
tmod
mod
Solut
ionseek
Chin
a
tomod pco
mp-n
safet
y coa
lmin
e
nn
nn
itsgenobj
subj
Figure 3: The insertion transformation. Dotted
edges mark the newly inserted path (Ex. (iv)).
fier). Further analysis is required to specify what
other dependencies can hold between such core-
ferring heads.
(3) Insertion: The last transformation, insertion,
is used when a relation that is realized in H is
missing from T and is only implied via a bridg-
ing relation. In Example (iv), the location that is
explicitly mentioned in H can only be covered by
T by resolving a bridging reference with China
in T ?. To connect the bridging referents, a new
tree component representing the bridging relation
is inserted into the consequent tree T1. In this ex-
ample, the component connects China and recent
accident via the in preposition. Formally, given
a bridging relation Br(x, y), we introduce a new
subtree Srz into T1, where z is a child of x and
lab(z) = labr. Srz must contain a variable node
that is instantiated with a copy of S(y).
This transformation stands out from the others
in that it introduces new material. For each bridg-
ing relation, it adds a specific subtrees Sr via an
edge labeled with labr. These two items form the
dependency representation of the bridging relation
Br and must be provided by the interface between
the discourse and the inference systems. Clearly,
their exact form depends on the set of bridging re-
lations provided by the discourse resolver as well
as the details of the dependency parses.
As shown in Figure 3, the bridging relation
located-in (r) is represented by inserting a subtree
Srz headed by in (z) into T1 and connecting it to
accident (x) as a modifier (labr). The subtree Srz
consists of a variable node which is connected to
in with a pcomp-n dependency (a nominal head of
a prepositional phrase), and which is instantiated
with the node China (y) when the transformation
is applied. Note that the structure of Srz and the
way it is inserted into T1 are predefined by the
abovementioned interface; only the node to which
it is attached and the contents of the variable node
are determined at transformation-time.
As another example, consider the following
short text from (Clark, 1975): John was murdered
yesterday. The knife lay nearby. Here, the bridg-
ing relation between the murder event and the in-
strument, the knife (x), can be addressed by in-
serting under x a subtree for the clause with which
as Srz , with a variable which is instantiated by the
parse-tree (headed by murdered, y) of the entire
first sentence John was murdered yesterday.
Transformation chaining. Since our transfor-
mations are defined to be minimal, some cases re-
quire the application of multiple transformations
to achieve coverage. Consider Example (v), Ta-
ble 1. We wish to cover AS-28 mini submarine in
H from the coreferring it in T , mini submarine in
T ? and AS-28 vehicle in T ??. A substitution of it by
either coreference does not suffice, since none of
the antecedents contains all necessary modifiers. It
is therefore necessary to substitute it first by one of
the coreferences and then merge it with the other.
6 Results
We analyzed 120 sentence-hypothesis pairs of the
RTE-5 development set (21 different hypotheses,
111 distinct sentences, 53 different documents).
Below, we summarize our findings, focusing on
the relation between our findings and the assump-
tions of previous studies as discussed in Section 3.
General statistics. We found that 44% of the
pairs contained reference relations whose resolu-
tion was mandatory for inference. In another 28%,
references could optionally support the inference
of the hypothesis. In the remaining 28%, refer-
ences did not contribute towards inference. The
total number of relevant references was 137, and
37 pairs (27%) contained multiple relevant refer-
ences. These numbers support our assumption that
discourse references play an important role in in-
ference.
Reference types. 73% of the identified refer-
ences are coreferences and 27% are bridging re-
lations. The most common bridging relation was
the location of events (e.g. Arctic in ice melting
events), generally assumed to be known through-
out the document. Other bridging relations we en-
countered include cause (e.g. between injured and
attack), event participants and set membership.
1215
(%) Pronoun NE NP VP
Focus term 9 19 49 23
Reference term - 43 43 14
Table 2: Syntactic types of discourse references
(%) Sub. Merge Insertion
Coreference 62 38 -
Bridging 30 - 70
Total 54 28 18
Table 3: Distribution of transformation types
Syntactic types. Table 2 shows that 77% of all
focus terms and 86% of the reference terms were
nominal phrases, which justifies their prominent
position in work on anaphora and coreference res-
olution. However, almost a quarter of the focus
terms were verbal phrases. We found these focus
terms to be frequently crucial for entailment since
they included the main predicate of the hypothe-
sis.6 This calls for an increased focus on the reso-
lution of event references.
Transformations. Table 3 shows the relative
frequencies of all transformations. Again, we
found that the ?default? transformation, substitu-
tion, is the most frequent one, and is helpful for
both coreference and bridging relations. Substitu-
tion is particularly useful for handling pronouns
(14% of all substitution instances), the replace-
ment of named entities by synonymous names
(32%), the replacement of other NPs (38%), and
the substitution of verbal head nodes in event
coreference (16%). Yet, in nearly half the cases,
a different transformation had to be applied. In-
sertion accounts for the majority of bridging cases.
Head-merge is necessary to integrate proper nouns
as modifiers of other head nouns. Dependent-
merge, responsible for 85% of the merge transfor-
mations, can be used to complete nominal focus
terms with missing modifiers (e.g., adjectives), as
well as for merging other dependencies between
coreferring predicates. This result indicates the
importance of incorporating other transformations
into inference systems.
Distance of reference terms. The distance be-
tween the focus and the reference terms varied
considerably, ranging from intra-sentential refer-
ence relations and up to several dozen sentences.
For more than a quarter of the focus terms, we
6The lower proportion of VPs among reference terms
stems from bridging relations between VPs and nominal de-
pendents, such as the abovementioned ?location? relation.
had to go to other documents to find reference
terms that, possibly in conjunction with the focus
term, could cover the target components. Interest-
ingly, all such cases involved coreference (about
equally divided between the merge transforma-
tions and substitutions), while bridging was al-
ways ?document-local?. This result reaffirms the
usefulness of cross-document coreference resolu-
tion for inference (Huang et al, 2009).
Discourse resolution as preprocessing? In ex-
isting RTE systems, discourse references are typ-
ically resolved as a preprocessing step. While
our annotation was manual and cannot yield di-
rect results about processing considerations, we
observed that discourse relations often hold be-
tween complex, and deeply embedded, expres-
sions, which makes their automatic resolution dif-
ficult. Of course, many RTE systems attempt to
normalize and simplify H and T , e.g., by split-
ting conjunctions or removing irrelevant clauses,
but these operations are usually considered a part
of the inference rather the preprocessing phase (cf.
e.g., Bar-Haim et al (2007)). Since the resolu-
tion of discourse references is likely to profit from
these steps, it seems desirable to ?postpone? it un-
til after simplification. In transformation-based
systems, it might be natural to add discourse-based
transformations to the set of inference operations,
while in alignment-based systems, discourse ref-
erences can be integrated into the computation of
alignment scores.
Discourse references vs. entailment knowledge.
We have stated before that even if a discourse ref-
erence is not strictly necessary for entailment, it
may be interesting because it represents an alter-
native to the use of knowledge rules to cover the
hypothesis. Sometimes, these rules are generally
applicable (e.g., ?Alaska? Arctic?). However, of-
ten they are context-specific. Consider the follow-
ing sentence as T for the hypothesis H: ?The ice
is melting in the Arctic?:
(3) T : ?The scene at the receding edge of the Exit
Glacier was part festive gathering, part nature
tour with an apocalyptic edge.?
While it is possible to cover melting using a rule
?melting? receding?, this rule is only valid under
quite specific conditions (e.g., for the subject ice).
Instead of determining the applicability of the rule,
a discourse-aware system can take the next sen-
1216
tence into account, which contains a coreferring
event to receding that can cover melting in H:
(4) T ?: ?. . . people moved closer to the rope line
near the glacier as it shied away, practically
groaning and melting before their eyes.?
Discourse relations can in fact encode arbitrar-
ily complex world knowledge, as in the following
pair:
(5) H: ?The serial killer BTK was accused of at
least 7 killings starting in the 1970?s.?
T: ?Police say BTK may have killed as many
as 10 people between 1974 and 1991.?
Here, the H modifier serial, which does not occur
in T , can be covered either by world knowledge
(a person who killed 10 people is a serial killer),
or by resolving the coreference of BTK to the term
the serial killer BTK which occurs in the discourse
around T . Our conclusion is that not only can
discourse references often replace world knowl-
edge in principle, in practice it often seems easier
to resolve discourse references than to determine
whether a rule is applicable in a given context or
to formalize complex world knowledge as infer-
ence rules. Our annotation provides further em-
pirical support to this claim: An entailment rela-
tion exists between the focus and reference terms
in 60% of the focus-reference term pairs, and in
many of the remainder, entailment holds between
the terms? heads. Thus, discourse provides rela-
tions which are many times equivalent to entail-
ment knowledge rules and can therefore be uti-
lized in their stead.
7 Conclusions
This work has presented an analysis of the relation
between discourse references and textual entail-
ment. We have identified a set of limitations com-
mon to the handling of discourse relations in vir-
tually all entailment systems. They include the use
of off-the-shelf resolvers that concentrate on nom-
inal coreference, the integration of reference in-
formation through substitution, and the RTE eval-
uation schemes, which played down the role of
discourse. Since in practical settings, discourse
plays an important role, our goal was to develop
an agenda for improving the handling of discourse
references in entailment-based inference.
Our manual analysis of the RTE-5 dataset
shows that while the majority of discourse refer-
ences that affect inference are nominal coreference
relations, another substantial part is made up by
verbal terms and bridging relations. Furthermore,
we have demonstrated that substitution alone is in-
sufficient to extract all relevant information from
the wide range of discourse references that are
frequently relevant for inference. We identified
three general cases, and suggested matching op-
erations to obtain the relevant inferences, formu-
lated as tree transformations. Furthermore, our ev-
idence suggests that for practical reasons, the res-
olution of discourse references should be tightly
integrated into entailment systems instead of treat-
ing it as a preprocessing step.
A particularly interesting result concerns the
interplay between discourse references and en-
tailment knowledge. While semantic knowledge
(e.g., from WordNet or Wikipedia) has been used
beneficially for coreference resolution (Soon et al,
2001; Ponzetto and Strube, 2006), reference res-
olution has, to our knowledge, not yet been em-
ployed to validate entailment rules? applicability.
Our analyses suggest that in the context of de-
ciding textual entailment, reference resolution and
entailment knowledge can be seen as complemen-
tary ways of achieving the same goal, namely en-
riching T with additional knowledge to allow the
inference of H . Given that both of the technolo-
gies are still imperfect, we envisage the way for-
ward as a joint strategy, where reference resolution
and entailment rules mutually fill each other?s gaps
(cf. Example 3).
In sum, our study shows that textual entailment
can profit substantially from better discourse han-
dling. The next challenge is to translate the the-
oretical gain into practical benefit. Our analy-
sis demonstrates that improvements are necessary
both on the side of discourse reference resolution
systems, which need to cover more types of refer-
ences, as well as a better integration of discourse
information in entailment systems, even for those
relations which are within the scope of available
resolvers.
Acknowledgements
This work was partially supported by the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1112/08.
1217
References
Azad Abad, Luisa Bentivogli, Ido Dagan, Danilo Gi-
ampiccolo, Shachar Mirkin, Emanuele Pianta, and
Asher Stern. 2010. A resource for investigating the
impact of anaphora and coreference on inference. In
Proceedings of LREC.
Rod Adams, Gabriel Nicolae, Cristina Nicolae, and
Sanda Harabagiu. 2007. Textual entailment through
extended lexical overlap and lexico-semantic match-
ing. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
E. Agichtein, W. Askew, and Y. Liu. 2008. Combining
lexical, syntactic, and semantic evidence for textual
entailment classification. In Proceedings of TAC.
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15(1):83?113.
Alexandra Balahur, Elena Lloret, O?scar Ferra?ndez,
Andre?s Montoyo, Manuel Palomar, and Rafael
Mun?oz. 2008. The DLSIUAES team?s participation
in the TAC 2008 tracks. In Proceedings of TAC.
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, and Eyal Shnarch amd
Idan Szpektor. 2008. Efficient semantic deduc-
tion and approximate matching over compact parse
forests. In Proceedings of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009a. The
fifth pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009b. Considering discourse references
in textual entailment annotation. In Proceedings of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Johan Bos. 2005. Recognising textual entailment with
logical inference. In Proceedings of EMNLP.
Aljoscha Burchardt, Marco Pennacchiotti, Stefan
Thater, and Manfred Pinkal. 2009. Assessing
the impact of frame semantics on textual entail-
ment. Journal of Natural Language Engineering,
15(4):527?550.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of ACL-IJCNLP.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Herbert H. Clark. 1975. Bridging. In R. C. Schank
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169?174. As-
sociation of Computing Machinery.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Lorand Dali, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Ques-
tion answering based on semantic graphs. In Pro-
ceedings of the Workshop on Semantic Search (Sem-
Search 2009).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2008. The
fourth pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference-6: a brief history.
In Proceedings of the 16th conference on Computa-
tional Linguistics.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2007. Satisfying information needs with multi-
document summaries. Information Processing &
Management, 43:1619?1642.
Stefan Harmeling. 2009. Inferring textual entailment
with a probabilistically sound calculus. Journal of
Natural Language Engineering, pages 459?477.
Marti A. Hearst. 1997. Segmenting text into multi-
paragraph subtopic passages. Computational Lin-
guistics, 23(1):33?64.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of HLT-NAACL.
Jian Huang, Sarah M. Taylor, Jonathan L. Smith, Kon-
stantinos A. Fotiadis, and C. Lee Giles. 2009. Pro-
file based cross-document coreference using kernel-
ized fuzzy relational clustering. In Proceedings of
ACL-IJCNLP.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of ACL-
IJCNLP.
1218
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 4(7):343?360.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. In Proceedings of ACL.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of EACL Workshop on
the Computational Treatment of Anaphora.
Seung-Hoon Na and Hwee Tou Ng. 2009. A 2-poisson
model for probabilistic coreference of named enti-
ties for improved text retrieval. In Proceedings of
SIGIR.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of ACL.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of HLT.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the rap anaphora
resolution algorithm. In Proceedings of LREC.
Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investigat-
ing a generic paraphrase-based approach for relation
extraction. In Proceedings of EACL.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Stephanie Strassel, Mark Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda. 2008. Linguistic
resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
Proceedings of LREC.
Jose L. Vicedo and Antonio Ferrndez. 2006. Coref-
erence in Q&A. In Tomek Strzalkowski and
Sanda M. Harabagiu, editors, Advances in Open Do-
main Question Answering, pages 71?96. Springer.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learning
approach to textual entailment recognition. Journal
of Natural Language Engineering, 15(4):551?582.
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts.
2004. Coreference resolution for information ex-
traction. In Proceedings of the ACL Workshop on
Reference Resolution and its Applications.
1219
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 85?90,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SORT: An Interactive Source-Rewriting Tool for Improved Translation
Shachar Mirkin, Sriram Venkatapathy, Marc Dymetman, Ioan Calapodescu
Xerox Research Centre Europe
6 Chemin de Maupertuis
38240 Meylan, France
firstname.lastname@xrce.xerox.com
Abstract
The quality of automatic translation is af-
fected by many factors. One is the diver-
gence between the specific source and tar-
get languages. Another lies in the source
text itself, as some texts are more com-
plex than others. One way to handle such
texts is to modify them prior to transla-
tion. Yet, an important factor that is of-
ten overlooked is the source translatabil-
ity with respect to the specific translation
system and the specific model that are be-
ing used. In this paper we present an in-
teractive system where source modifica-
tions are induced by confidence estimates
that are derived from the translation model
in use. Modifications are automatically
generated and proposed for the user?s ap-
proval. Such a system can reduce post-
editing effort, replacing it by cost-effective
pre-editing that can be done by monolin-
guals.
1 Introduction
While Machine Translation (MT) systems are con-
stantly improving, they are still facing many dif-
ficulties, such as out-of-vocabulary words (i.e.
words unseen at training time), lack of sufficient
in-domain data, ambiguities that the MT model
cannot resolve, and the like. An important source
of problems lies in the source text itself ? some
texts are more complex to translate than others.
Consider the following English-to-French
translation by a popular service, BING TRANS-
LATOR:1 Head of Mali defense seeks more arms
? D?fense de la t?te du Mali cherche bras plus.
There, apart from syntactic problems, both head
and arms have been translated as if they were
1http://www.bing.com/translator, accessed
on 4/4/2013.
body parts (t?te and bras). However, suppose
that we express the same English meaning in the
following way: Chief of Mali defense wants more
weapons. Then BING produces a much better
translation: Chef d??tat-major de la d?fense du
Mali veut plus d?armes.
The fact that the formulation of the source can
strongly influence the quality of the translation has
long been known, and there have been studies in-
dicating that adherence to so-called ?Controlled
Language? guidelines, such as Simplified Techni-
cal English2 can reduce the MT post-edition ef-
fort. However, as one such study (O?Brien, 2006)
notes, it is unfortunately not sufficient to just ?ap-
ply the rules [i.e. guidelines] and press Translate.
We need to analyze the effect that rules are hav-
ing on different language pairs and MT systems,
and we need to tune our rule sets and texts ac-
cordingly?.
In the software system presented here, SORT
(SOurce Rewriting Tool), we build on the basic in-
sight that formulation of the source needs to be
geared to the specific MT model being used, and
propose the following approach. First, we assume
that the original source text in English (say) is not
necessarily under the user?s control, but may be
given to her. While she is a fluent English speaker,
she does not know at all the target language, but
uses an MT system; crucially, this system is able
to provide estimates of the quality of its transla-
tions (Specia et al, 2009). SORT then automati-
cally produces a number of rewritings of each En-
glish sentence, translates them with the MT sys-
tem, and displays to the user those rewritings for
which the translation quality estimates are higher
than the estimate for the original source. The user
then interactively selects one such rewriting per
sentence, checking that it does not distort the orig-
inal meaning, and finally the translations of these
2http://www.asd-ste100.org
85
reformulations are made available.
One advantage of this framework is that the
proposed rewritings are implicitly ?aware? of the
underlying strengths and limitations of the spe-
cific MT model. A good quality estimation3
component, for instance, will feel more confident
about the translation of an unambiguous word like
weapon than about that of an ambiguous one such
as arm, or about the translation of a known term
in its domain than about a term not seen during
training.
Such a tool is especially relevant for business
situations where post-edition costs are very high,
for instance because of lack of people both ex-
pert in the domain and competent in the target lan-
guage. Post-edition must be reserved for the most
difficult cases, while pre-edition may be easier to
organize. While the setup cannot fully guarantee
the accuracy of all translations, it can reduce the
number of sentences that need to go through post-
edition and the overall cost of this task.
2 The rewriting tool
In this section we describe SORT, our implemen-
tation of the aforementioned rewriting approach.
While the entire process can in principle be fully
automated, we focus here on an interactive pro-
cess where the user views and approves suggested
rewritings. The details of the rewriting methods
and of the quality estimation used in the current
implementation are described in Sections 3 and 4.
Figure 1 presents the system?s interface, which
is accessed as a web application. With this in-
terface, the user uploads the document that needs
to be translated. The translation confidence of
each sentence is computed and displayed next to
it. The confidence scores are color-coded to en-
able quickly focusing on the sentences that require
more attention. Green denotes sentences for which
the translation confidence is high, and are thus ex-
pected to produce good translations. Red marks
sentences that are estimated to be poorly trans-
lated, and all those in between are marked with
an orange label.
We attempt to suggest rewritings only for sen-
tences that are estimated to be not so well trans-
lated. When we are able to propose rewriting(s)
with higher translation confidence than the origi-
nal, a magnifying glass icon is displayed next to the
sentence. Clicking it displays, on the right side of
3Also known as confidence estimation.
the screen, an ordered list of the more confident
rewritings, along with their corresponding confi-
dence estimations. The first sentence on the list
is always the original one, to let it be edited, and
to make it easier to view the difference between
the original and the rewritings. An example is
shown on the right side of Figure 1, where we see
a rewriting suggestion for the fourth sentence in
the document. Here, the suggestion is simply to
replace the word captured with the word caught, a
rewriting that is estimated to improve the transla-
tion of the sentence.
The user can select one of the suggestions or
choose to edit either the original or one of the
rewritings. The current sentence which is being
examined is marked with a different color and the
alternative under focus is marked with a small icon
(the bidirectional arrows). The differences between
the alternatives and the original are highlighted.
After the user?s confirmation (with the check mark
icon), the display of the document on the left-hand
side is updated based on her selection, including
the updated confidence estimation. At any time,
the user (if she speaks the target language) can
click on the cogwheel icon and view the transla-
tion of the source or of its rewritten version. When
done, the user can save the edited text or its trans-
lation. Moses Release 1.0 of an English-Spanish
Europarl-trained model4 was used in this work to
obtain English-Spanish translations.
2.1 System and software architecture
SORT is implemented as a web application, using
an MVC (Model View Controller) software archi-
tecture. The Model part is formed by Java classes
representing the application state (user input, se-
lected text lines, associated rewriting propositions
and scores). The Controller consists of several
servlet components handling each user interaction
with the backend server (file uploads, SMT tools
calls via XML-RPC or use of the embedded Java
library that handles the actual rewritings). Finally,
the View is built with standard web technologies:
HTML5, JavaScript (AJAX) and CSS style sheets.
The application was developed and deployed on
Linux (CentOS release 6.4), with a Java Runtime
6 (Java HotSpot 64-Bit Server VM), within a Tom-
cat 7.0 Application Server, and tested with Firefox
as the web client both on Linux and Windows 7.
Figure 2 shows the system architecture of SORT,
4http://www.statmt.org/moses/RELEASE-1.0/model/
86
Figure 1: SORT?s interface
Figure 2: SORT?s system architecture. For simplicity, only
partial input-output details are shown.
with some details of the current implementation.
The entire process is performed via a client-server
architecture in order to provide responsiveness, as
required in an interactive system. The user com-
municates with the system through the interface
shown in Figure 1. When a document is loaded,
its sentences are translated in parallel by an SMT
Moses server (Koehn et al, 2007). Then, the
source and the target are sent to the confidence es-
timator, and the translation model information is
also made available to it. The confidence estima-
tor extracts features from that input and returns a
confidence score. Specifically, the language model
features are computed with two SRILM servers
(Stolcke, 2002), one for the source language and
one for the target language. Rewritings are pro-
duced by the rewriting modules (see Section 3 for
the implemented rewriting methods). For each
rewriting, the same process of translation and con-
fidence estimation is performed. Translations are
cached during the session; thus, when the user
wishes to view a translation or download the trans-
lations of the entire document, the response is im-
mediate.
3 Source rewriting
Various methods can be used to rewrite a source
text. In what follows we describe two rewriting
methods, based on Text Simplification techniques,
which we implemented and integrated in the cur-
rent version of SORT. Simplification operations
include the replacement of words by simpler ones,
removal of complicated syntactic structures, short-
ening of sentences etc. (Feng, 2008). Our assump-
tion is that simpler sentences are more likely to
yield higher quality translations. Clearly, this is
not always the case; yet, we leave this decision to
the confidence estimation component.
Sentence-level simplification (Specia, 2010)
has proposed to model text simplification as a Sta-
tistical Machine Translation (SMT) task where the
goal is to translate sentences to their simplified
version in the same language. In this approach, a
simplification model is learnt from a parallel cor-
pus of texts and their simplified versions. Apply-
87
ing this method, we train an SMT model from En-
glish to Simple English, based on the PWKP par-
allel corpus generated from Wikipedia (Zhu et al,
2010);5 we use only alignments involving a single
sentence on each side. This results in a phrase ta-
ble containing many entries where source and tar-
get phrases are identical, but also phrase-pairs that
are mapping complex phrases to their simplified
counterparts, such as the following:
? due to its location on? because it was on
? primarily dry and secondarily cold ? both
cold and dry
? the high mountainous alps? the alps
Also, the language model is trained with Simple
English sentences to encourage the generation of
simpler texts. Given a source text, it is translated
to its simpler version, and its n-best translations
are assessed by the confidence estimation compo-
nent.
Lexical simplification One of the primary oper-
ations for text-simplification is lexical substitution
(Table 2 in (Specia, 2010)). Hence, in addition to
rewriting a full sentence using the previous tech-
nique, we implemented a second method, address-
ing lexical simplification directly, and only modi-
fying local aspects of the source sentence. The ap-
proach here is to extract relevant synonyms from
our trained SMT model of English to Simplified
English, and use them as substitutions to simplify
new sentences. We extract all single token map-
pings from the phrase table of the trained model,
removing punctuations, numbers and stop-words.
We check whether their lemmas were synonyms
in WordNet (Fellbaum, 1998) (with all possible
parts-of-speech as this information was not avail-
able in the SMT model). Only those are left as
valid substitution pairs. When a match of an En-
glish word is found in the source sentence it is re-
placed with its simpler synonym to generate an al-
ternative for the source. For example, using this
rewriting method for the source sentence ?Why the
Galileo research program superseded rival pro-
grams,? three rewritings of the sentence are gen-
erated when rival is substituted by competitor or
superseded by replaced, and when both substitu-
tions occur together.
5Downloaded from:
http://www.ukp.tu-darmstadt.de/data/
sentence-simplification
In the current version of SORT, both sentence-
level and lexical simplification methods are used
in conjunction to suggest rewritings for sentences
with low confidence scores.
4 Confidence estimation
Our confidence estimator is based on the system
and data provided for the 2012 Quality estima-
tion shared task (Callison-Burch et al, 2012). In
this task, participants were required to estimate the
quality of automated translations. Their estimates
were compared to human scores of the translation
which referred to the suitability of the translation
for post-editing. The scores ranged from 1 to 5,
where 1 corresponded to translation that practi-
cally needs to be done from scratch, and 5 to trans-
lations that requires little to no editing.
The task?s training set consisted of approxi-
mately 1800 source sentences in English, their
Moses translations to Spanish and the scores given
to the translations by the three judges. With this
data we trained an SVM regression model using
SVMlight (Joachims, 1999). Features were ex-
tracted with the task?s feature-extraction baseline
module. Two types of features are used in this
module (i) black-box features, which do not as-
sume access to the translation system, such as
the length of the source and the target, number
of punctuation marks and language model prob-
abilities, and (ii) glass-box features, which are ex-
tracted from the translation model, such as the
average number of translations per source word
(Specia et al, 2009).
5 Initial evaluation and analysis
We performed an initial evaluation of our ap-
proach in an English to Spanish translation setting,
using the 2008 News Commentary data.6 First,
two annotators who speak English but not Spanish
used SORT to rewrite an English text. They re-
viewed the proposed rewritings for 960 sentences
and were instructed to ?trust the judgment? of the
confidence estimator; that is, reviewing the sug-
gestions from the most to the least confident one,
they accepted the first rewriting that was fluent and
preserved the meaning of the source document as
a whole. 440 pairs of the original sentence and
the selected alternative were then both translated
to Spanish and were presented as competitors to
6Available at http://www.statmt.org
88
three native Spanish speakers. The sentences were
placed within their context in the original docu-
ment, taken from the Spanish side of the corpus.
The order of presentation of the two competitors
was random. In this evaluation, the translation of
the original was preferred 20.6% of the cases, the
rewriting 30.4% of them, and for 49% of the sen-
tences, no clear winner was chosen.7 Among the
two rewriting methods, the sentence-level method
more often resulted in preferred translations.
These results suggest that rewriting is esti-
mated to improve translation quality. However,
the amount of preferred original translations indi-
cates that the confidence estimator is not always
discriminative enough: by construction, for every
rewriting that is displayed, the confidence compo-
nent estimates the translation of the original to be
less accurate than that of the rewriting; yet, this is
not always reflected in the preferences of the eval-
uators. On a different dimension than translation
quality, the large number of cases with no clear
winner, and the analysis we conducted, indicate
that the user?s cognitive effort would be decreased
if we only displayed those rewritings associated
with a substantial improvement in confidence; due
to the nature of our methods, frequently, identi-
cal or near-identical translations were generated,
with only marginal differences in confidence, e.g.,
when two source synonyms were translated to the
same target word. Also, often a wrong synonym
was suggested as a replacement for a word (e.g.
Christmas air for Christmas atmosphere). This
was somewhat surprising as we had expected the
language model features of the confidence estima-
tor to help removing these cases. While they were
filtered by the English-speaking users, and thus
did not present a problem for translation, they cre-
ated unnecessary workload. Putting more empha-
sis on context features in the confidence estimation
or explicitly verifying context-suitability of a lex-
ical substitutions could help addressing this issue.
6 Related work
Some related approaches focus on the authoring
process and control a priori the range of possible
texts, either by interactively enforcing lexical and
syntactic constraints on the source that simplify
the operations of a rule-based translation system
(Carbonell et al, 1997), or by semantically guid-
7One should consider these figures with caution, as the
numbers may be too small to be statistically meaningful.
ing a monolingual author in the generation of mul-
tilingual texts (Power and Scott, 1998; Dymetman
et al, 2000). A recent approach (Venkatapathy
and Mirkin, 2012) proposes an authoring tool that
consults the MT system itself to propose phrases
that should be used during composition to obtain
better translations. All these methods address the
authoring of the source text from scratch. This
is inherently different from the objective of our
work where an existing text is modified to improve
its translatability. Moving away from authoring
approaches, (Choumane et al, 2005) propose an
interactive system where the author helps a rule-
based translation system disambiguate a source
text inside a structured document editor. The
techniques are generic and are not automatically
adapted to a specific MT system or model. Closer
to our approach of modifying the source text, one
approach is to paraphrase the source or to gener-
ate sentences entailed by it (Callison-Burch et al,
2006; Mirkin et al, 2009; Marton et al, 2009;
Aziz et al, 2010). These works, however, fo-
cus on handling out-of-vocabulary (OOV) words,
do not assess the translatability of the source sen-
tence and are not interactive.8 The MonoTrans2
project (Hu et al, 2011) proposes monolingual-
based editing for translation. Monolingual speak-
ers of the source and target language collaborate
to improve the translation. Unlike our approach,
here both the feedback for poorly translated sen-
tences and the actual modification of the source
is done by humans. This contrasts with the auto-
matic handling (albeit less accurate) of both these
tasks in our work.
7 Conclusions and future work
We introduced a system for rewriting texts for
translation under the control of a confidence esti-
mator. While we focused on an interactive mode,
where a monolingual user is asked to check the
quality of the source reformulations, in an exten-
sion of this approach, the quality of the reformu-
lations could also be assessed automatically, re-
moving the interactive aspects at the cost of an in-
creased risk of rewriting errors. For future work
we wish to add more powerful rewriting tech-
niques that are able to explore a larger space of
possible reformulations, but compensate this ex-
8Another way to use paraphrases for improved translation
has been proposed by (Max, 2010) who uses paraphrasing of
the source text to increase the number of training examples
for the SMT system.
89
panded space by robust filtering methods. Based
on an evaluation of the quality of the generated al-
ternatives as well as on user selection decisions,
we may be able to learn a quality estimator for
the rewriting operations themselves. Such meth-
ods could be useful both in an interactive mode,
to minimize the effort of the monolingual source
user, as well as in an automatic mode, to avoid
misinterpretation. In this work we used an avail-
able baseline feature extraction module for confi-
dence estimation. A better estimator could bene-
fit our system significantly, as we argued above.
Lastly, we wish to further improve the user inter-
face of the tool, based on feedback from actual
users.
References
[Aziz et al2010] Wilker Aziz, Marc Dymetman,
Shachar Mirkin, Lucia Specia, Nicola Cancedda,
and Ido Dagan. 2010. Learning an expert from
human annotations in statistical machine translation:
the case of out-of-vocabularywords. In Proceedings
of EAMT.
[Callison-Burch et al2006] Chris Callison-Burch,
Philipp Koehn, and Miles Osborne. 2006. Improved
statistical machine translation using paraphrases. In
Proceedings of HLT-NAACL.
[Callison-Burch et al2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of WMT.
[Carbonell et al1997] Jaime G Carbonell, Sharlene L
Gallup, Timothy J Harris, James W Higdon, Den-
nis A Hill, David C Hudson, David Nasjleti,
Mervin L Rennich, Peggy M Andersen, Michael M
Bauer, et al 1997. Integrated authoring and transla-
tion system. US Patent 5,677,835.
[Choumane et al2005] Ali Choumane, Herv? Blan-
chon, and C?cile Roisin. 2005. Integrating transla-
tion services within a structured editor. In Proceed-
ings of the ACM symposium on Document engineer-
ing. ACM.
[Dymetman et al2000] Marc Dymetman, Veronika
Lux, and Aarne Ranta. 2000. Xml and multilin-
gual document authoring: Convergent trends. In
Proceedings of COLING.
[Fellbaum1998] Christiane Fellbaum, editor. 1998.
WordNet: An Electronic Lexical Database (Lan-
guage, Speech, and Communication). The MIT
Press.
[Feng2008] Lijun Feng. 2008. Text simplification: A
survey. Technical report, CUNY.
[Hu et al2011] Chang Hu, Philip Resnik, Yakov Kro-
nrod, Vladimir Eidelman, Olivia Buzek, and Ben-
jamin B. Bederson. 2011. The value of monolingual
crowdsourcing in a real-world translation scenario:
simulation using haitian creole emergency sms mes-
sages. In Proceedings of WMT.
[Joachims1999] T. Joachims. 1999. Making large-
scale SVM learning practical. In B. Sch?lkopf,
C. Burges, and A. Smola, editors, Advances in Ker-
nel Methods - Support Vector Learning, chapter 11,
pages 169?184. MIT Press.
[Koehn et al2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade
Shen, Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
ACL, Demo and Poster Sessions.
[Marton et al2009] Yuval Marton, Chris Callison-
Burch, and Philip Resnik. 2009. Improved sta-
tistical machine translation using monolingually-
derived paraphrases. In Proceedings of EMNLP.
[Max2010] Aur?lien Max. 2010. Example-based para-
phrasing for improved phrase-based statistical ma-
chine translation. In Proceedings of EMNLP.
[Mirkin et al2009] Shachar Mirkin, Lucia Specia,
Nicola Cancedda, Ido Dagan, Marc Dymetman, and
Idan Szpektor. 2009. Source-language entailment
modeling for translating unknown terms. In Pro-
ceedings of ACL-IJCNLP.
[O?Brien2006] Sharon O?Brien. 2006. Controlled Lan-
guage and Post-Editing. Multilingual, 17(7):17?19.
[Power and Scott1998] Richard Power and Donia Scott.
1998. Multilingual authoring using feedback texts.
In Proceedings of ACL.
[Specia et al2009] Lucia Specia, Nicola Cancedda,
Marc Dymetman, Marco Turchi, and Nello Cristian-
ini. 2009. Estimating the sentence-level quality
of machine translation systems. In Proceedings of
EAMT.
[Specia2010] Lucia Specia. 2010. Translating from
complex to simplified sentences. In Proceedings of
PROPOR.
[Stolcke2002] Andreas Stolcke. 2002. SRILM - an
extensible language modeling toolkit. In INTER-
SPEECH.
[Venkatapathy and Mirkin2012] Sriram Venkatapathy
and Shachar Mirkin. 2012. An SMT-driven
authoring tool. In Proceedings of COLING 2012:
Demonstration Papers.
[Zhu et al2010] Zhemin Zhu, Delphine Bernhard, and
Iryna Gurevych. 2010. A monolingual tree-based
translation model for sentence simplification. In
Proceedings of COLING.
90
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 75?80,
Dublin, Ireland, August 23-24 2014.
Text Summarization through Entailment-based Minimum Vertex Cover
Anand Gupta
1
, Manpreet Kaur
2
, Adarsh Singh
2
, Aseem Goel
2
, Shachar Mirkin
3
1
Dept. of Information Technology, NSIT, New Delhi, India
2
Dept. of Computer Engineering, NSIT, New Delhi, India
3
Xerox Research Centre Europe, Meylan, France
Abstract
Sentence Connectivity is a textual charac-
teristic that may be incorporated intelli-
gently for the selection of sentences of a
well meaning summary. However, the ex-
isting summarization methods do not uti-
lize its potential fully. The present pa-
per introduces a novel method for single-
document text summarization. It poses
the text summarization task as an opti-
mization problem, and attempts to solve
it using Weighted Minimum Vertex Cover
(WMVC), a graph-based algorithm. Tex-
tual entailment, an established indicator of
semantic relationships between text units,
is used to measure sentence connectivity
and construct the graph on which WMVC
operates. Experiments on a standard sum-
marization dataset show that the suggested
algorithm outperforms related methods.
1 Introduction
In the present age of digital revolution with pro-
liferating numbers of internet-connected devices,
we are facing an exponential rise in the volume
of available information. Users are constantly fac-
ing the problem of deciding what to read and what
to skip. Text summarization provides a practical
solution to this problem, causing a resurgence in
research in this field.
Given a topic of interest, a standard search of-
ten yields a large number of documents. Many of
them are not of the user?s interest. Rather than go-
ing through the entire result-set, the reader may
read a gist of a document, produced via summa-
rization tools, and then decide whether to fully
read the document or not, thus saving a substan-
tial amount of time. According to Jones (2007),
a summary can be defined as ?a reductive trans-
formation of source text to summary text through
content reduction by selection and/or generaliza-
tion on what is important in the source?. Summa-
rization based on content reduction by selection is
referred to as extraction (identifying and includ-
ing the important sentences in the final summary),
whereas a summary involving content reduction
by generalization is called abstraction (reproduc-
ing the most informative content in a new way).
The present paper focuses on extraction-based
single-document summarization. We formulate
the task as a graph-based optimization problem,
where vertices represent the sentences and edges
the connections between sentences. Textual en-
tailment (Giampiccolo et al., 2007) is employed to
estimate the degree of connectivity between sen-
tences, and subsequently to assign a weight to each
vertex of the graph. Then, the Weighted Mini-
mum Vertex Cover, a classical graph algorithm,
is used to find the minimal set of vertices (that is
? sentences) that forms a cover. The idea is that
such cover of well-connected vertices would cor-
respond to a cover of the salient content of the doc-
ument.
The rest of the paper is organized as follows: In
Section 2, we discuss related work and describe
the WMVC algorithm. In Section 3, we propose
a novel summarization method, and in Section 4,
experiments and results are presented. Finally, in
Section 5, we conclude and outline future research
directions.
2 Background
Extractive text summarization is the task of iden-
tifying those text segments which provide impor-
tant information about the gist of the document
? the salient units of the text. In (Marcu, 2008),
salient units are determined as the ones that con-
tain frequently-used words, contain words that are
within titles and headings, are located at the begin-
ning or at the end of sections, contain key phrases
and are the most highly connected to other parts
75
of the text. In this work we focus on the last of
the above criteria, connectivity, to find highly con-
nected sentences in a document. Such sentences
often contain information that is found in other
sentences, and are therefore natural candidates to
be included in the summary.
2.1 Related Work
The connectivity between sentences has been pre-
viously exploited for extraction-based summariza-
tion. Salton et al. (1997) generate intra-document
links between passages of a document using auto-
matic hypertext link generation algorithms. Mani
and Bloedorn (1997) use the number of shared
words, phrases and co-references to measure con-
nectedness among sentences. In (Barzilay and El-
hadad, 1999), lexical chains are constructed based
on words relatedness.
Textual entailment (TE) was exploited recently
for text summarization in order to find the highly
connected sentences in the document. Textual en-
tailment is an asymmetric relation between two
text fragments specifying whether one fragment
can be inferred from the other. Tatar et al. (2008)
have proposed a method called Logic Text Tiling
(LTT), which uses TE for sentence scoring that
is equal to the number of entailed sentences and
to form text segments comprising of highly con-
nected sentences. Another method called Ana-
log Textual Entailment and Spectral Clustering
(ATESC), suggested in (Gupta et al., 2012), also
uses TE for sentence scoring, using analog scores.
We use a graph-based algorithm to produce the
summary. Graph-based ranking algorithms have
been employed for text summarization in the past,
with similar representation to ours. Vertices rep-
resent text units (words, phrases or sentences) and
an edge between two vertices represent any kind
of relationship between two text units. Scores are
assigned to the vertices using some relevant crite-
ria to select the vertices with the highest scores.
In (Mihalcea and Tarau, 2004), content overlap
between sentences is used to add edges between
two vertices and Page Rank (Page et al., 1999) is
used for scoring the vertices. Erkan and Radev
(2004) use inter-sentence cosine similarity based
on word overlap and tf-idf weighting to identify
relations between sentences. In our paper, we use
TE to compute connectivity between nodes of the
graph and apply the weighted minimum vertex
cover (WMVC) algorithm on the graph to select
the sentences for the summary.
2.2 Weighted MVC
WMVC is a combinatorial optimization problem
listed within the classical NP-complete problems
(Garey and Johnson, 1979; Cormen et al., 2001).
Over the years, it has caught the attention of many
researchers, due to its NP-completeness, and also
because its formulation complies with many real
world problems.
Weighted Minimum Vertex Cover Given a
weighted graph G = (V,E,w), such that w is
a positive weight (cost) function on the vertices,
w : V ? R, a weighted minimum vertex cover of
G is a subset of the vertices, C ? V such that for
every edge (u, v) ? E either u ? C or v ? C
(or both), and the total sum of the weights is min-
imized.
C = argmin
C
?
?
v? C
?
w(v) (1)
3 Weighted MVC for text summarization
We formulate the text summarization task as a
WMVC problem. The input document to be sum-
marized is represented as a weighted graph G =
(V,E,w), where each of v ? V corresponds to a
sentence in the document; an edge (u, v) ? E ex-
ists if either u entails v or v entails u with a value
at least as high as an empirically-set threshold. A
weight w is then assigned to each sentence based
on (negated) TE values (see Section 3.2 for further
details). WMVC returns a cover C which is a sub-
set of the sentences with a minimum total weight,
corresponding to the best connected sentences in
the document. The cover is our output ? the sum-
mary of the input document.
Our proposed method, shown in Figure 1, con-
sists of the following main steps.
1. Intra-sentence textual entailment score com-
putation
2. Entailment-based connectivity scoring
3. Entailment connectivity graph construction
4. Application of WMVC to the graph
We elaborate on each of these steps in the fol-
lowing sections.
3.1 Computing entailment scores
Given a document d for which summary is to be
generated, we represent d as an array of sentences
76
Id Sentence
S
1
A representative of the African National Congress said Saturday the South African government may release black nationalist leader Nelson Mandela
as early as Tuesday.
S
2
?There are very strong rumors in South Africa today that on Nov. 15 Nelson Mandela will be released,? said Yusef Saloojee, chief representative in
Canada for the ANC, which is fighting to end white-minority rule in South Africa.
S
3
Mandela the 70-year-old leader of the ANC jailed 27 years ago, was sentenced to life in prison for conspiring to overthrow the South African
government.
S
4
He was transferred from prison to a hospital in August for treatment of tuberculosis.
S
5
Since then, it has been widely rumoured Mandela will be released by Christmas in a move to win strong international support for the South African
government.
S
6
?It will be a victory for the people of South Africa and indeed a victory for the whole of Africa,? Saloojee told an audience at the University of
Toronto.
S
7
A South African government source last week indicated recent rumours of Mandela?s impending release were orchestrated by members of the
anti-apartheid movement to pressure the government into taking some action.
S
8
And a prominent anti-apartheid activist in South Africa said there has been ?no indication (Mandela) would pe released today or in the near future.?
S
9
Apartheid is South Africa?s policy of racial separation.
Summary ?There are very strong rumors in South Africa today that on Nov.15 Nelson Mandela will pe released,? said Yusef Saloojee, chief representative
in Canada for the ANC, which is fighting to end white-minority rule in South Africa. He was transferred from prison to a hospital in August for
treatment of tuberculosis. A South African government source last week indicated recent rumours of Mandela?s impending release were orchestrated
by members of the anti-apartheid movement to pressure the government into taking some action. Apartheid is South Africa?s policy of racial
separation.
Table 1: The sentence array of article AP881113-0007 of cluster do106 in the DUC?02 dataset.
Figure 1: Outline of the proposed method.
D
1?N
. An example article is shown in Table 1.
We use this article to demonstrate the steps of our
algorithm.
Then, we compute a TE score between every
possible pair of sentences in D using a textual en-
tailment tool. TE scores for all the pairs are stored
in a sentence entailment matrix, SE
N?N
. An en-
try SE[i, j] in the matrix represents the extent by
which sentence i entails sentence j. The sentence
entailment matrix produced for our example doc-
ument is shown in Table 2.
S
1
S
2
S
3
S
4
S
5
S
6
S
7
S
8
S
9
S
1
- 0 0 0.04 0 0 0.001 0.02 0.02
S
2
0.02 - 0.01 0.04 0.06 0.01 0 0.01 0.04
S
3
0 0 - 0.09 0 0 0 0 0.04
S
4
0 0 0 - 0 0 0 0 0.01
S
5
0 0 0 0.04 - 0 0.01 0.01 0.04
S
6
0 0 0 0.04 0 - 0 0 0.02
S
7
0 0 0 0.04 0.06 0 - 0.02 0.27
S
8
0 0 0 0.04 0 0 0.01 - 0.02
S
9
0 0 0 0.04 0 0 0 0 -
Table 2: The sentence entailment matrix of the ex-
ample article.
Id ConnScore Id ConnScore
S
1
0.08 S
6
0.06
S
2
0.19 S
7
0.39
S
3
0.13 S
8
0.07
S
4
0.01 S
9
0.04
S
5
0.1
Table 3: Connectivity Scores of the sentences of
article AP881113-0007.
3.2 Connectivity scores
Our assumption is that entailment between sen-
tences indicates connectivity, that ? as mentioned
above ? is an indicator of sentence salience. More
specifically, salience of a sentence is determined
by the degree by which it entails other sentences
in the document. We thus use the sentence entail-
ment matrix to compute a connectivity score for
each sentence by summing the entailment scores
of the sentence with respect to the rest of the sen-
tences in the document, and denote this sum as
ConnScore. Formally, ConnScore for sentence
i is computed as follows.
ConnScore[i] =
?
i 6= j
SE [i, j] (2)
Applying it to each sentence in the document,
we obtain the ConnScore
1?N
vector. The sen-
tence connectivity scores corresponding to Table 2
are shown in Table 3.
3.3 Entailment connectivity graph
construction
The more a sentence is connected, the higher its
connectivity score. To adapt the scores to the
WMVC algorithm, that searches for a minimal so-
lution, we convert the scores into positive weights
77
in inverted order:
w[i] = ?ConnScore[i] + Z (3)
w[i] is the score that is assigned to the vertex of
sentence i; Z is a large constant, meant to keep
the scores positive. In this paper, Z has been as-
signed value = 100. Now, the better a sentence is
connected, the lower its weight.
Given the weights, we construct an undi-
rected weighted entailment connectivity graph,
G(V,E,w), for the document d. V consists of
vertices for the document?s sentences, and E are
edges that correspond to the entailment relations
between the sentences. w is the weight explained
above. We create an edge between two vertices as
explained below. Suppose that S
i
and S
j
are two
sentences in d, with entailment scores SE[i, j] and
SE[j, i] between them. We set a threshold ? for
the entailment scores as the mean of all entailment
values in the matrix SE. We add an edge (i, j) to
G if SE[i, j] > ? OR SE[j, i] > ? , i.e. if at least
one of them is as high as the threshold.
Figure 2 shows the connectivity graph con-
structed for the example in Table 1.
Figure 2: The Entailment connectivity graph of the
considered example with associated Score of each
node shown.
3.4 Applying WMVC
Finally, we apply the weighted minimum vertex
cover algorithm to find the minimal vertex cover,
which would be the document?s summary. We
use integer linear programming (ILP) for find-
ing a minimum cover. This algorithm is a 2-
approximation for the problem, meaning it is an
efficient (polynomial-time) algorithm, guaranteed
to find a solution that is no more than 2 times big-
ger than the optimal solution.
1
The algorithm?s
1
We have used an implementation of ILP for WMVC in
MATLAB, grMinVerCover.
input is G = (V,E,w), a weighted graph where
each vertex v
i
? V (1 ? i ? n) has weight w
i
. Its
output is a minimal vertex cover C of G, contain-
ing a subset of the vertices V . We then list these
sentences as our summary, according to their orig-
inal order in the document.
After applying WMVC to the graph in Fig-
ure 2, the cover C returned by the algorithm is
{S
2
, S
4
, S
7
, S
9
} (highlighted in Figure 2).
Whenever a summary is required, a word-limit
on the summary is specified. We find the threshold
which results with a cover that matches the word
limit through binary search.
4 Experiments and results
4.1 Experimental settings
We have conducted experiments on the single-
document summarization task of the DUC 2002
dataset
2
, using a random sample that contains 60
news articles picked from each of the 60 clus-
ters available in the dataset. The target sum-
mary length limit has been set to 100 words. We
used version 2.1.1 of BIUTEE (Stern and Da-
gan, 2012), a transformation-based TE system to
compute textual entailment score between pairs of
sentences.
3
BIUTEE was trained with 600 text-
hypothesis pairs of the RTE-5 dataset (Bentivogli
et al., 2009).
4.1.1 Baselines
We have compared our method?s performance
with the following re-implemented methods:
1. Sentence selection with tf-idf: In this base-
line, sentences are ranked based on the sum
of the tf-idf scores of all the words except
stopwords they contain, where idf figures are
computed from the dataset of 60 documents.
Top ranking sentences are added to the sum-
mary one by one, until the word limit is
reached.
2. LTT: (see Section 2)
3. ATESC : (see Section 2)
4.1.2 Evaluation metrics
We have evaluated the method?s performance us-
ing ROUGE (Lin, 2004). ROUGE measures the
2
http://www-nlpir.nist.gov/projects/
duc/data/2002_data.html
3
Available at: http://www.cs.biu.ac.il/
?
nlp/downloads/biutee.
78
Method P (%) R (%) F
1
(%)
TF-IDF 13.3 17.6 15.1
LTT 39.9 34.6 37.1
ATESC 37.7 32.5 34.9
WMVC 39.8 38.8 39.2
Table 4: ROUGE-1 results.
Method P (%) R (%) F
1
(%)
TF-IDF 7.4 9.6 8.4
LTT 18.4 15.2 16.6
ATESC 16.3 11.7 13.6
WMVC 16.7 16.8 16.8
Table 5: ROUGE-2 results.
quality of an automatically-generated summary by
comparing it to a ?gold-standard?, typically a hu-
man generated summary. ROUGE-n measures n-
gram precision and recall of a candidate summary
with respect to a set of reference summaries. We
compare the system-generated summary with two
reference summaries for each article in the dataset,
and show the results for ROUGE-1, ROUGE-2 and
ROUGE-SU4 that allows skips within n-grams.
These metrics were shown to perform well for
single document text summarization, especially
for short summaries. Specifically, Lin and Hovy
(2003) showed that ROUGE-1 achieves high cor-
relation with human judgments.
4
4.2 Results
The results for ROUGE-1, ROUGE-2 and
ROUGE-SU4 are shown in Tables 4, 5 and 6, re-
spectively. For each, we show the precision (P),
recall (R) and F
1
scores. Boldface marks the high-
est score in each table. As shown in the tables,
our method achieves the best score for each of the
three metrics.
4.3 Analysis
The entailment connectivity graph generated con-
veys information about the connectivity of sen-
tences in the document, an important parameter
for indicating the salience of a sentences.
The purpose of the WMVC is therefore to find
a subset of the sentences that are well-connected
and cover all the content of all the sentences. Note
that merely selecting the sentences on the basis
of a greedy approach, that picks the those sen-
tences with the highest connectivity score, does
not ensure that all edges of the graph are cov-
4
See (Lin, 2004) for formal definitions of these metrics.
Method P (%) R (%) F
1
(%)
TF-IDF 2.2 4.2 2.9
LTT 16 11.8 13.6
ATESC 15.5 11.1 12.9
WMVC 14.1 14.2 14.2
Table 6: ROUGE-SU4 results.
ered, i.e. it does not ensure that all the infor-
mation is covered in the summary. In Figure 3,
we illustrate the difference between WMVC (left)
and a greedy algorithm (right) over our example
document. The vertices selected by each algo-
rithm are highlighted. The selected set by WMVC,
{S
2
, S
4
, S
7
, S
9
}, covers all the edges in the graph.
In contrast, using the greedy algorithm, the subset
of vertices selected on the basis of highest scores
is {S
2
, S
3
, S
7
, S
8
}. There, several edges are not
covered (e.g. (S
1
? S
9
)).
It is therefore much more in sync with the sum-
marization goal of finding a subset of sentences
that conveys the important information of the doc-
ument in a compressed manner.
S1 S2 S4 
S6 
S7 S9 
S5 
S3 
S8 
Weighted  Minimum Vertex Cover    Greedy   vertex  selection   
S1 S2 S4 
S6 
S7 S9 
S5 
S3 
S8 
Figure 3: Minimum Vertex Cover vs. Greedy se-
lection of sentences.
5 Conclusions and future work
The paper presents a novel method for single-
document extractive summarization. We formu-
late the summarization task as an optimization
problem and employ the weighted minimum ver-
tex cover algorithm on a graph based on textual en-
tailment relations between sentences. Our method
has outperformed previous methods that employed
TE for summarization as well as a frequency-
based baseline. For future work, we wish to ap-
ply our algorithm on smaller segments of the sen-
tences, using partial textual entailment Levy et al.
(2013), where we may obtain more reliable en-
tailment measurements, and to apply the same ap-
proach for multi-document summarization.
79
References
Regina Barzilay and Michael Elhadad. 1999. Using
lexical chains for text summanzauon. In In Inder-
jeet Mani and Mark T. Maybury, editors, Advances
in Automatic Text Summarization, pages 111?121,
The MIT Press, 1999.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
fifth pascal recognizing textual entailment chal-
lenge. In Proceedings of Text Analysis Conference,
pages 14?24, Gaithersburg, Maryland USA.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to
Algorithms. McGraw-Hill, New York, 2nd edition.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intellignece
Research(JAIR), 22(1):457?479.
Michael R. Garey and David S. Johnson. 1979. Com-
puters and Intractability: A Guide to the Theory of
NP-Completeness. FREEMAN, New York.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
Association for Computational Linguistics, ACL?07,
pages 1?9, Prague, Czech Republic.
Anand Gupta, Manpreet Kaur, Arjun Singh, Ashish
Sachdeva, and Shruti Bhati. 2012. Analog textual
entailment and spectral clustering (atesc) based sum-
marization. In Lecture Notes in Computer Science,
Springer, pages 101?110, New Delhi, India.
Karen Spark Jones. 2007. Automatic summarizing:
The state of the art. Information Processing and
Management, 43:1449?1481.
Omer Levy, Torsten Zesch, Ido Dagan, and Iryna
Gurevych. 2013. Recognizing partial textual entail-
ment. In Proceedings of the Association for Compu-
tational Linguistics, pages 17?23, Sofia, Bulgaria.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 71?78,
Edmonta, Canada, 27 May- June 1.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pages 25?26, Barcelona, Spain.
Inderjeet Mani and Eric Bloedorn. 1997. Multi-
document summarization by graph search and
matching. In Proceedings of the Fourteenth Na-
tional Conference on Articial Intelligence (AAAI-
97), American Association for Articial Intelligence,
pages 622?628, Providence, Rhode Island.
Daniel Marcu. 2008. From discourse structure to text
summaries. In Proceedings of the ACL/EACL ?97,
Workshop on Intelligent Scalable Text Summariza-
tion, pages 82?88, Madrid, Spain.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of
EMNLP, volume 4(4), page 275, Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 33:193?207.
Asher Stern and Ido Dagan. 2012. BIUTEE: A mod-
ular open-source system for recognizing textual en-
tailment. In Proceedings of the ACL 2012 System
Demonstrations, pages 73?78, Jeju, Korea.
Doina Tatar, Emma Tamaianu Morita, Andreea Mihis,
and Dana Lupsa. 2008. Summarization by logic
seg-mentation and text entailment. In Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLing 08), pages 15?26, Haifa, Israel.
80
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 20?29,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Classification-based Contextual Preferences
Shachar Mirkin, Ido Dagan, Lili Kotlerman
Bar-Ilan University
Ramat Gan, Israel
{mirkins,dagan,davidol}@cs.biu.ac.il
Idan Szpektor
Yahoo! Research
Haifa, Israel
idan@yahoo-inc.com
Abstract
This paper addresses context matching in tex-
tual inference. We formulate the task under
the Contextual Preferences framework which
broadly captures contextual aspects of infer-
ence. We propose a generic classification-
based scheme under this framework which co-
herently attends to context matching in infer-
ence and may be employed in any inference-
based task. As a test bed for our scheme we use
the Name-based Text Categorization (TC) task.
We define an integration of Contextual Prefer-
ences into the TC setting and present a concrete
self-supervised model which instantiates the
generic scheme and is applied to address con-
text matching in the TC task. Experiments on
standard TC datasets show that our approach
outperforms the state of the art in context mod-
eling for Name-based TC.
1 Introduction
Textual inference is prevalent in text understanding
applications. For example, in Question Answering
(QA) the expected answer should be inferred from
retrieved passages, and in Information Extraction (IE)
the meaning of the target event is inferred from its
mention in the text.
Lexical inferences make a substantial part of the
inference process. In such cases, a target term is
inferred from text expressions based on either one of
two types of lexical matches: (i) a direct match of
the target term in the text. For instance, the IE event
injure may be detected by finding the word injure in
the text; (ii) an indirect match, through a term that
implies the meaning of the target term, e.g. inferring
injure from hurt.
In either case, due to word ambiguity, it is nec-
essary to validate that the context of the match con-
forms with the intended meaning of the target term
before carrying out an inference operation based on
this match. For example, ?You hurt my feelings? con-
stitutes an invalid context for the injure event as hurt
in this text does not refer to a physical injury. Simi-
larly, inferring the protest-related event demonstrate
based on demo is deemed invalid although demo im-
plies the meaning of the word demonstrate in other
contexts, e.g., concerning software demonstration.
Although seemingly equivalent, a closer look re-
veals that the above two examples correspond to two
distinct contextual mismatch situations. While the
match of hurt is invalid for injure in the particular
given context, an inference based on demo is invalid
for the protest demonstrate event in any context.
Thus, several types of context matching are in-
volved in textual inference. While most prior work
addressed only specific context matching scenarios,
Szpektor et al (2008) presented a broader view,
proposing a generic framework for context match-
ing in inference, termed Contextual Preferences (CP).
CP specifies the types of context matching that need
to be considered in inference, allowing a model of
choice to be applied for validating each type of match.
Szpektor et al applied CP to an IE task using differ-
ent models to validate each type of context match.
In this work we adopt CP as our context matching
framework and propose a novel classification-based
scheme which provides unified modeling for CP. We
represent typical contexts of the textual objects that
participate in inference using classifiers; at inference
time, each match is assessed by the respective classi-
fiers which determine its contextual validity.
As a test bed we applied our scheme to the task
20
of Name-based Text Categorization. This is an unsu-
pervised setting of TC where the only input given is
the category name, and in which context validation
is of high importance. We instantiate the scheme
with a novel self-supervised model and apply it to
the TC task. We suggest a method for integrating any
CP-based context matching model into TC and use it
to combine the context matching scores generated by
our model. Results on two standard TC datasets show
that our approach outperforms the state of the art con-
text model for this task and suggest applying this
scheme to additional inference-based applications.
2 Background
2.1 Context matching in inference
Word ambiguity has been traditionally addressed
through Word Sense Disambiguation (WSD) (Nav-
igli, 2009). The WSD task requires selecting the
meaning of a target term from amongst a predefined
set of senses, based on sense-inventories such as
WordNet (Fellbaum, 1998).
An alternative approach eliminates the reliance on
such inventories. Instead of explicit sense identifi-
cation, a direct sense-match between terms is pur-
sued (Dagan et al, 2006). Lexical substitution (Mc-
Carthy and Navigli, 2009) is probably the most com-
monly known task that follows this approach. Con-
text matching is a generalization of lexical substitu-
tion, which seeks a match between terms in context,
not necessarily for the purpose of substitution. For in-
stance, the word played in ?U2 played their first-ever
concert in Russia? contextually matches music, al-
though music cannot substitute played in this context.
The context matching task, therefore, is to determine
(by quantifying or giving a binary decision) the va-
lidity of a match between two terms in context.
In Section 1 we informally presented two cases of
contextual mismatches. A comprehensive view of
context matching types is provided by the Contextual
Preferences framework (Szpektor et al, 2008). CP
is phrased in terms of the Textual Entailment (TE)
paradigm (Dagan et al, 2009). In TE, a text t entails
a textual hypothesis h if the meaning of h can be
inferred from t. Formulating the IE example from
Section 1 within TE, h may be the name of the target
event, injure, and t is a text segment from which h
can be inferred. A direct match occurs when a term
in h is identical to a term in t. An inference based
on an indirect match is viewed as the application of
a lexical entailment rule, r, such as ?hurt? injure?,
where the entailing left-hand side (LHS) of the rule
(hurt) is matched in the text, while the entailed right-
hand side (RHS), injure, is matched in the hypothesis.
Hence, three inference objects take part in infer-
ence operations: t, h and r. Most prior work ad-
dressed only specific contextual matches between
these objects. For example, Harabagiu et al (2003)
matched the contexts of t and h for QA (answer and
question, respectively); Barak et al (2009) matched
t and h (document and category) in TC, while other
works, including those applying lexical substitution,
typically validated the context match between t and r
(Kauchak and Barzilay, 2006; Dagan et al, 2006;
Pantel et al, 2007; Connor and Roth, 2007).
In comparison, in the CP framework, all possible
contextual matches among t, h and r are considered:
t?h, t? r and r?h. The three context matches are
depicted in Figure 1 (left). In CP, the representation
of each inference object is enriched with contextual
information which is used to characterize its valid
contexts. Such information may be the words of the
event description in IE, corpus instances based on
which a rule was learned, or an annotation of relevant
WordNet senses in Name-based TC. For example,
a category name hockey may be assigned with the
sense number corresponding to ice hockey, but not
to field hockey, in order to designate information that
limits the valid contexts of the category to the former
among the two meanings of the name.
Before an inference operation is performed, the
context representations of each pair among the partic-
ipating objects should be matched by a context model
in order to assess the contextual validity of the opera-
tion. Along with the context representation and the
specific context matching models, the way context
model decisions are combined needs to be specified
in a concrete implementation of the CP framework.
2.2 Context matching models
Several approaches were taken in prior work to
model context matching, mostly within the scope
of learning selectional preferences of templatized
lexical-syntactic rules (e.g. ?X
subj
???? hit
obj
??? Y ??
?X
subj
???? attack
obj
??? Y ?).
21
Pantel et al (2007) and Szpektor et al (2008) rep-
resented the context of such rules as the intersection
of preferences of the rule?s LHS and RHS, namely the
observed argument instantiations or their semantic
classes. A rule is deemed applicable to a given text if
the argument instantiations in the text are similar to
the selectional preferences of the rule. To overcome
sparseness, other works represented context in latent
space. Pennacchiotti et al (2007) and Szpektor et al
(2008) measured the similarity between the Latent
Semantic Analysis (LSA) (Deerwester et al, 1990)
representations of matched contexts. Dinu and La-
pata (2010) used Latent Dirichlet Allocation (LDA)
(Blei et al, 2003) to model templates? latent senses,
determining rule applicability based on the similarity
between the two sides of the rule when instantiated
by the context, while Ritter et al (2010) used LDA
to model argument classes, considering a rule valid
for a given argument instantiation if its instantiated
templates are drawn from the same hidden topic.
A different approach is provided by classification-
based models which learn classifiers for inference
objects. A classifier is trained based on positive and
negative examples which represent valid or invalid
contexts of the object; from those, features charac-
terizing the context are extracted, e.g. words in a
window around the target term or syntactic links with
it. Given a new context, the classifier assesses its va-
lidity with respect to the learned classification model.
Classifiers in prior work were applied to determine
rule applicability in a given context (t ? r). Train-
ing a classifier for word paraphrasing, Kauchak and
Barzilay (2006) used occurrences of the rule?s RHS as
positive context examples, and randomly picked neg-
ative examples. A similar approach was applied by
Dagan et al (2006), which used a single-class SVM
to avoid selecting negative examples. In both works,
a resulting classifier represents a word with all its
senses intermixed. Clearly, this poses no problem for
monosemous words, but is biased towards the more
common senses of polysemous words. Indeed, Dagan
et al (2006) report a negative correlation between the
degree of polysemy of a word and the performance of
its classifier. Connor and Roth (2007) used per-rule
classifiers to produce a noisy training set for learning
a global classifier for verb substitution.
In this work we follow the classification-based ap-
proach which seems appealing for several reasons.
t
r
t
C h(
r)
h
C r(
t)
C h(
t)r
h
Figure 1: Left: An illustration of the CP relationships as in
(Szpektor et al, 2008), with arrows indicting the context
matching direction; Right: The application of classifiers
to the tested contexts under our scheme.
First, it allows seamlessly integrating various types
of information via classifiers? features; unlike some
of the above models, it is not inherently dependent on
the type of rules that are utilized and easily accom-
modates to both lexical and lexical-syntactic rules
through the choice of features. In addition, it does
not rely on a predefined similarity measure and pro-
vides flexibility in terms of model?s parameters. Fi-
nally, this approach captures the notion of direction-
ality which is fundamental in textual inference, and
is therefore better suited to applied inference than
previously proposed symmetric context models.
In comparison to prior classification-based models,
our approach addresses all three context matches
specified by CP, rather than only the rule-text match.
It is not limited to substitutable terms or even to
terms with the same part of speech. In addition, we
avoid learning a classifier for all senses combined,
but rather learn it for the specific intended meaning.
2.3 Name-based Text Categorization
Name-based TC (Gliozzo et al, 2009) is an unsu-
pervised setting of Text Categorization in which the
only input provided is the category name, e.g. trade,
?mergers and acquisitions? or guns. When category
names are ambiguous, e.g. space, categories are not
well defined; thus, auxiliary information is expected
to accompany the name for disambiguation, such as
a list of relevant senses or a category description.
Typically, unsupervised TC consists of two steps.
First, an unsupervised method is applied to an unla-
beled corpus, automatically labeling some of the doc-
uments to categories. Then, the labeled documents
from the first step are used to train a supervised TC
classifier which is used to label any document in the
test set (Gliozzo et al, 2009; Downey and Etzioni,
2009; Barak et al, 2009).
22
In this work we focus on the above unsupervised
step. Gliozzo et al (2009) addressed this task by rep-
resenting both documents and categories by LSA vec-
tors which implicitly capture contextual similarities
between terms. Each document was then assigned
to the most similar category based on cosine simi-
larity between the LSA vectors. Barak et al (2009)
required an occurrence of a term entailing the cat-
egory name (or the category name itself) in order
to regard the category as a candidate for the docu-
ment. To assess the contextual validity of the match,
they used LSA document-category similarity as in
(Gliozzo et al, 2009). For example, to classify a doc-
ument into the category medicine, at least one lexical
entailment rule, e.g. ?drug? medicine?, should be
matched in the document. Then, the validity of drug
for medicine in the matched document is assessed by
the LSA context model. In this work we adopt Barak
et al?s requirement for a match for the category in
the document, but address context matching in an
entirely different way.
Name-based TC provides a convenient setting for
evaluating context matching approaches for two main
reasons. First, all types of context matchings are real-
ized in this application (see Section 3); second, as the
hypothesis consists of a single term or a few terms,
the TC gold standard annotation corresponds quite
directly to the context matching task for lexical infer-
ences; in other applications where longer hypotheses
are involved, context matching performance may be
masked by other factors.
3 Contextual Matches in TC
Within Name-based TC, the Textual Entailment ter-
minology is mapped as follows: h is a term denoting
the category name (e.g. merger or acquisition); t is
a matched term in the document to be categorized
from which h may be inferred; and a match refers
to an occurrence in the document of either h (direct
match) or the LHS of an entailment rule r whose RHS
is a category name (indirect match).1
Under the CP view, a context model needs to ad-
dress the following three context matching cases
within a TC setting.
t?h: Assessing the validity of a match in the docu-
ment with respect to the category?s intended meaning.
1Note that t and h both refer here to individual terms.
For example, the occurrence of the category name
space (in the sense of outer space) in ?the server ran
out of disk space? does not indicate a space-related
text, and should be dismissed by the context model.
t? r: This case refers to a rule match in the docu-
ment. A context model should ensure that the mean-
ing of a match is compatible with that of the rule.
For example, ?alien? space? is a valid rule for the
space category. Yet, it should not be applied to ?The
US welcomes a large number of aliens every year?,
since alien in this sentence has a different meaning
than the intended meaning of the rule.
r ? h: The match between the intended meanings
of the category name and the RHS of the rule. For
instance, the rule ?room? space? is not suitable at
all for the (outer) space category.
4 A Classification-based Scheme for CP
Szpektor et al (2008) introduced a vector-space
model to implement CP, in which the text t, the rule
r and the hypothesis h share the same contextual
representation. However, in CP, r, h and t have non-
symmetric roles: the context of t should be tested as
valid for r and h and not vice versa, and the context
of r should be validated for h and not the other way
around. This stems from the need to consider direc-
tionality in context matching. For instance, a text
about football typically constitutes a valid context
for the more general sports context, but not vice
versa. Indeed, directionality may be captured in
vector-space models by using a directional similarity
measure (Kotlerman et al, 2010), but only symmetric
measures were used in context matching work so far.
Based on this distinction between the inference
objects? roles, we present a novel scheme that uses
two types of classifiers to represent context:
Ch: A classifier that identifies valid contexts for h. It
tests contexts of t (for t? h matching) or r (for
r ? h matching), assigning them scores Ch(t)
and Ch(r), respectively.
Cr: A classifier that identifies valid contexts for ap-
plying the rule r. It tests the context of t, assign-
ing it a score Cr(t).
Figure 1 (right) shows the classifiers scores which
are assigned to each of the matching types.
23
Hence, h always acts as the classifying object, t is
always the classified object, while r acts as both. Con-
text matching is quantified by the degree by which
the classified object represents a valid context for the
classifying object in a given inference scenario.
In comparison to the CP implementation in (Szpek-
tor et al, 2008), our approach uses a unified model
which captures directionality in context matching.
To instantiate the scheme, one needs to define the
way training examples are obtained and processed.
This may be done within supervised classification,
where labeled examples are provided, or ? as we do
in this work ? using self-supervised classifiers which
obtain training examples automatically. We present
such an instantiation in Section 5, where a classifier is
trained for each category and each rule. When more
complex hypotheses are involved, Ch classifiers can
be trained separately for each relevant part of the
hypothesis, using the rest for disambiguation.
A combination of the three model scores provides
a final context matching score. In Section 6 we sug-
gest a way to combine the actual classification scores
as part of the integration of CP into TC, but other
combinations are plausible. In particular, binary clas-
sifications (valid vs. invalid) may be used as filters.
That is, the context is classified as valid only if all
relevant models classify it as such.
5 A Self-supervised Context Model
We now turn to demonstrate how our classification-
based scheme may be implemented. The model be-
low is exemplified on Name-based TC, but may be
applicable to other tasks, with few changes.
5.1 Training-set generation
Our implementation is self-supervised as we want
to integrate it within the unsupervised TC setting.
That is, the classifiers automatically obtain training
examples for the classifying object (a category or a
rule) without relying on labeled documents.
We obtain examples by querying the TC training
corpus with automatically-generated search queries.
The difficulty lies in correctly constructing queries
that will retrieve documents representing either valid
or invalid contexts for the classifying object. To this
end, we retrieve examples through a gradual process
in which the most accurate (least ambiguous) query
is used first and less accurate queries follow, until the
designated number of examples is acquired.
5.1.1 Obtaining positive examples
To acquire positive training examples, we con-
struct queries which are comprised of two main
clauses. The first contains the seeds, terms which
characterize the classifying object. Primarily, these
are the category name or the LHS of the rule. The sec-
ond consists of context words which are used when
the seeds are polysemous, and are intended to assist
disambiguation. When context words are used, at
least one seed and at least one context word must
be matched to retrieve a document. For example,
given the highly ambiguous category name space,
we first construct the query using only the monose-
mous term outer space; if the number of retrieved
documents does not meet the requirement, a second
query may be constructed: (?outer space? OR space)
AND (infinite OR science OR . . . ).
To generate a rule classifier Cr, we retrieve posi-
tive examples as follows. If the LHS term is monose-
mous according to WordNet2, we first query using
this term alone (e.g. decrypt), and add its monose-
mous synonyms and hyponyms if more examples are
required (e.g. decrypt OR decode). If the LHS is
polysemous, we carry out Procedure 1. Intuitively,
this procedure tries to minimize ambiguity by using
monosemous terms as much as possible; when poly-
semous terms must be used, it tries to ensure there are
monosemous terms to disambiguate them. Note that
entailment directionality is maintained throughout
the process, as seeds are only expanded with more
specific (entailing) terms, while context words are
only expanded with more general (entailed) terms.
Procedure 1 : Retrieval of Cr positive examples
Apply sequentially until sufficient examples are obtained:
1: Set the LHS as seed and the RHS?s monosemous syn-
onyms, hypernyms and derivations as context words.
2: Add monosemous synonyms and hyponyms of the
LHS to the seeds.
3: As in 2, but use polysemous terms as well.
4: Add polysemous context words.
Positive examples for category classifiers (Ch) are
obtained through a similar procedure as for rule clas-
2Terms not in WordNet are assumed monosemous.
24
sifiers. If the category is part of a hierarchy, we also
use the name of the parent category (e.g. sport for
rec.sport.hockey) as a context word.
5.1.2 Obtaining negative examples
Negative examples are even more challenging to
acquire. In prior work negative examples were se-
lected randomly (Kauchak and Barzilay, 2006; Con-
nor and Roth, 2007). We follow this method, but
also attempt to identify negative examples that are
semantically similar to the positive ones in order to
improve the discriminative power of the classifier
(Smith and Eisner, 2005). We do that by applying
a similar procedure which uses cohyponyms of the
seeds, e.g. baseball for hockey or islam for christian-
ity. Cohyponymy is a non-entailing relation; hence,
by using it we expect to obtain semantically-related,
yet invalid contexts. If not enough negative exam-
ples are retrieved using cohyponyms, we select the
remaining required examples randomly.
As the distribution of positive and negative ex-
amples in the data is unknown, we set the ratio of
negative to positive examples as a parameter of the
model, as in (Bergsma et al, 2008).
5.1.3 Insufficient examples
When the number of training examples for a rule
or a category is below a certain minimum, the re-
sulting classifier is expected to be of poor quality.
This usually happens for positive examples in any of
the following two cases: (i) the seed is rare in the
training set; (ii) the desired sense of the seed is rarely
found in the training set, and unwanted senses were
filtered by our retrieval query. For instance, nazarene
does not occur at all in the training set, and the classi-
fier corresponding to the rule ?nazarene? christian?
cannot be generated. On the other hand, cone does
appear in the corpus but not in the astrophysical sense
the rule ?cone? space? refers to. In such cases we
refrain from generating the classifier and use instead
a default score of 0 for each classified object. The
idea is that rare terms will also occur infrequently in
the test set, while cases where the term is found in
the corpus, but in a different sense than the desired
one, will be blocked.
5.1.4 Feature extraction
We extract global and local lexical features that are
standard in WSD work. Global features include all
the terms in the document or in the sentence in which
a match was found. Local features are extracted
around matches of seeds which comprised the query
that retrieved the document. These features include
the terms in a window around the match, and the
noun, verb, adjective and adverb nearest to the match
in either direction. For randomly sampled negative
examples, where no matched query terms exist, we
randomly select terms in the document as ?matches?
for local feature extraction. If more than one match of
the same term is found in a document, we assume one-
sense-per-discourse (Gale et al, 1992) and jointly
extract features for all matches of the term.
5.2 Applying the classifiers
During inference, for each direct match in a docu-
ment, the corresponding Ch is applied. For an indi-
rect match, the respective Cr is also applied.
In addition, Ch is applied to the matched rules.
Unlike t, a rule is not represented by a single text.
Therefore, to test a rule?s match with the category,
we randomly sample from the training set documents
containing the rule?s LHS. We apply Ch to each sam-
pled example and compute the ratio of positive classi-
fications. The result is a score indicating the domain-
specific probability of the rule to be applicable to
the category, and may be interpreted as an in-domain
prior. For instance, the rule ?check ? hockey? is
assigned a score of 0.05, since the sense of check
as a hockey defense technique is rare in the corpus.
On the other hand, non ambiguous rules, e.g. ?war-
ship ? ship? are assigned a high probability (1.0),
and so are rules whose LHS is ambiguous but its dom-
inant sense in the training corpus is the same one the
rule refers to, e.g. ?margin? earnings?(0.85).
We do not assign negative classifier scores to in-
valid matches but rather set them to zero instead. The
reason is that an invalid context only indicates that
the term cannot be used for entailing the category
name, but not that the document itself is irrelevant.
6 CP for Text Categorization
CP may be employed in any inference-based task,
but the integration with each task is somewhat dif-
ferent and needs to be specified. Below we present
a methodology for integrating CP into Name-based
Text Categorization.
25
As in (Barak et al, 2009) (Barak09 below), we
represent documents and categories by term-vectors
in the following way: a document vector contains
the document terms; a category vector contains two
sets of terms: C, the terms denoting the category
name, and E , their entailing terms. For example, oil
is added to the vector of the category crude by the
rule ?oil? crude? (i.e. crude ? C and oil ? E).
Barak09 assigned equal values of 1 to all vector
entries. We suggest integrating a CP-based context
model into TC by re-weighting the terms in the vec-
tors, prior to determining the final document-category
categorization score through vector similarity. Given
a category c, with term vector C, and a document d
with term vector D, the model re-weights vector en-
tries of matching terms (i.e., terms in C ?D), based
on the validity of the context match. Valid matches
should be assigned with higher scores than invalid
ones, leading to higher overall vector similarity for
documents with valid matches for the given category.
Non-matching terms are ignored as their weights are
canceled out in the subsequent vector product.
Specifically, the model assigns a new weight
wD(u) to a matching term u in the document vec-
tor D based on the model?s assessment of: (a) t? h,
the context match between the (match in the) doc-
ument and the category; and (if an indirect match)
(b) t ? r, the context match between the document
and the rule ?u? ci?, where ci ? C. The model also
sets a new weight wC(v) to a term v in the category
vector C based on the context match for r ? h, be-
tween the rule ?v ? cj? (cj ? C) and the category.
For instance, using our context matching scheme in
TC, wD(u) is set to Ch(u) or
Ch(u)+Cr(u)
2 for direct
and indirect matches, respectively; wC(v) is left as 1
if v ? C and set to Ch(v) when v ? E .
Barak09 assigned a single global context score to
a document-category pair using the LSA representa-
tions of their vectors. In our approach, however, we
consider the actual matches from the three different
views, hence the re-weighting of the vector entries
using three model scores.
7 Experimental Setting
7.1 Datasets and knowledge resources
Following (Gliozzo et al, 2009) and (Barak et al,
2009), we evaluated our method on two standard TC
datasets: Reuters-10 and 20-Newsgroups.
The Reuters-10 (R10, for short) is a sub-corpus
of the Reuters-21578 collection3, constructed from
the ten most frequent categories in the Reuters tax-
onomy. We used the Apte split of the Reuters-21578
collection, often used in TC tasks. The top 10 cate-
gories include about 9,000 documents, split into train-
ing (70%) and test (30%) sets. The 20-Newsgroups
(20NG) corpus is a collection of newsgroup postings
gathered from twenty different categories from the
Usenet Newsgroups hierarchy4. We used the ?by-
date? version of the corpus, which contains approxi-
mately 20,000 documents partitioned (nearly) evenly
across the categories and divided in advance to train-
ing (60%) and test (40%) sets.
As in (Gliozzo et al, 2009; Barak et al, 2009), we
adjusted non-standard category names (e.g. forsale
was renamed to sale) and manually specified for each
category its relevant WordNet senses. The sense tag-
ging properly defines the categories, and is expected
to accompany such hypotheses. Other types of in-
formation may be used for this purpose, e.g. words
from category descriptions, if such exist.
We applied standard preprocessing (sentence split-
ting, tokenization, lemmatization and part of speech
tagging) to all documents in the datasets. All terms,
including those denoting category names and rules,
are represented by their lemma and part of speech.
As sources for lexical entailment rules we used
WordNet 3.0 (synonyms, hyponyms, derivations
and meronyms) and a Wikipedia-derived rule-base
(Shnarch et al, 2009). Unlike Barak09 we did not
limit the rules extracted from WordNet to the most
frequent senses and used all rule types from the
Wikipedia-based resource.
7.2 Self-supervised model tuning
Tuning of the self-supervised context model?s pa-
rameters (number of training examples, negative to
positive ratio, feature set and the way negative exam-
ples are obtained) was performed over development
sets sampled from the training sets. Based on this tun-
ing, some parameters varied between the datasets and
between classifier types (Ch vs. Cr). For example,
3http://kdd.ics.uci.edu/databases/
reuters21578/reuters21578.html
4http://people.csail.mit.edu/jrennie/
20Newsgroups/
26
selection of negative examples based on cohyponyms
was found useful for Cr classifiers in R10, while ran-
dom examples were used in the rest of the cases.
We used SVMperf (Joachims, 2006) with a linear
kernel and binary feature weighting.
For querying the corpus we used the Lucene search
engine5 in its default setting. Up to 150 positive
examples were retrieved for each classifier, with 5
examples set as the required minimum. This resulted
in generating 100% of the hypothesis classifiers for
both datasets and 95% and 70% of the rule classifiers
for R10 and 20NG, respectively.
We computed Ch(r) scores based on up to 20 sam-
pled instances. If less than 2 examples were found in
the training set, we assigned an ?unknown? context
match probability of 0.5, since a rare LHS occurrence
does not indicate anything about its meaning in the
corpus. Such cases constituted 2% (R10) and 11%
(20NG) of the utilized rules.
7.3 Baseline models
To provide a more meaningful comparison with prior
work, we focus on the first unsupervised step in the
typical Name-based TC flow, without the subsequent
supervised training. Our goal is to improve the accu-
racy of this first step, and we therefore compare our
context model?s performance to two unsupervised
methods used by Barak09.
The first baseline, denoted Barakno-cxt, is the co-
sine similarity score between the document and cate-
gory vectors where all terms are equally weighted to
a score of 1.6 This baseline shows the performance
when no context model is employed.
The second baseline, denoted Barakfull, is a repli-
cation of the state of the art context model for Name-
based TC. In this method, LSA vectors are con-
structed for a document by averaging the LSA vectors
of its individual terms, and for a category by averag-
ing the LSA vectors of the terms denoting its name.
The categorization score of a document-category pair
is set to be the product between the cosine similarity
score of the LSA vectors and the score given by the
above Barakno-cxt method. We note that LSA-based
context models performed best also in (Gliozzo et al,
2009) and (Szpektor et al, 2008).
5http://lucene.apache.org
6Other attempted weighting schemes, such as tf-idf, did not
yield better performance.
Model
Reuters-10
Accuracy P R F1
Barakno-cxt 73.2 63.6 77.0 69.7
Barakfull 76.3 68.0 79.2 73.2
Class.-based 79.3 71.8 83.6 77.2
Model
20-Newsgroups
Accuracy P R F1
Barakno-cxt 63.7 44.5 74.6 55.8
Barakfull 69.4 50.1 82.8 62.4
Class.-based 73.4 54.7 76.4 63.7
Table 1: Evaluation results.
All models were constructed based on the TC train-
ing sets, using no external corpora. The vocabulary
consists of terms that appear more than once in the
training set. The terms we consider include nouns,
verbs, adjectives and adverbs, as well as nominal
multi-word expressions.
8 Results and Analysis
Given a document, all categories for which a lexical
match was found in the document are considered,
and the document is classified to the highest scoring
category. If all categories are assigned non-positive
scores, the document is not assigned to any of them.
Based on this requirement that a document con-
tains at least one match for the category, 4862
document-category pairs were considered for clas-
sification in R10 and 9955 pairs in 20NG. We eval-
uated our context model, as well as the baselines,
based on the accuracy of these classifications, i.e.
the percentage of correct decisions among the candi-
date document-category pairs. We also measured the
models? performance in terms of micro-averaged pre-
cision (P ), relative recall (R) and F1. Like Barak09,
recall is computed relative to the potential recall of
the rule-set which provides the entailing terms.
Table 1 presents the evaluation results. As in
Barak09, the LSA-based model outperforms the first
baseline, supporting its usefulness as a context model.
In both datasets our model outperformed the base-
lines in terms of accuracy. This result is statistically
significant with p < 0.01 according to McNemar?s
test (McNemar, 1947). Recall is lower for our model
in 20NG but F1 scores are higher for both datasets.
These results indicate that the classification-based
context model provides a favorable alternative to the
27
Removed
Reuters-10 20-Newsgroups
Accuracy F1 Accuracy F1
- 79.3 77.2 73.4 63.7
Ch(t) 76.2 72.3 71.9 61.0
Cr(t) 80.5 77.6 74.3 64.5
Ch(r) 78.4 75.7 73.1 63.4
Table 2: Ablation tests results.
state of the art LSA-based method.
Table 2 presents ablation tests of our model. In
each test we measured the classification performance
when one of the three classification scores is ignored.
Clearly, Ch(t) is the most beneficial component, and
in general the category classifiers help improving
overall performance. The limited performance of Cr
may be related to higher ambiguity in rules relative to
category names, resulting in noisier training data. In
addition, the small size of the training set limits the
number of training examples for rule classifiers. This
problem affects Cr more than Ch since, by nature,
the corpus includes more occurrences of category
names. Still, Cr contributes to improved recall (this
fact is not visible in Table 2).
The coverage of the utilized rule-set determines
the maximal (absolute) recall that can be achieved
by any model. With the rule-set we used in this ex-
periment, the recall upper bound was 59.1% for R10
and 40.6% for 20NG. However, rule coverage af-
fects precision as well: In many cases documents are
assigned to incorrect categories because the correct
category is not even a candidate as no entailing term
was matched for it in the document. For instance,
a document with the sentence ?For sale or trade!!!
BMW R60US. . . ? was classified by our method to
the category forsale, while its gold-standard category
is motorcycles. Yet, none of the rules in our rule-set
triggered motorcycles as a candidate category for this
document. Ideally, a context model would rule out
all incorrect candidate categories; in practice even a
single low score for one of the competing categories
results in a false positive error in such cases (in addi-
tion to the recall loss). To reduce these problems we
intend to employ additional knowledge resources in
future work.
Our algorithm for retrieving training examples
turned out to be not sufficiently accurate, particularly
for negative examples. This is a challenging task that
requires further research. Although useful for some
classifier types, the use of cohyponyms may retrieve
potentially positive examples as negative ones, since
terms that are considered cohyponyms in WordNet
are often perceived as near synonyms in common
usage, e.g. buyout and purchase in the context of
acquisitions. Likewise, using WordNet senses to de-
termine ambiguity is also inaccurate. Rare or too
fine-grained senses, common in WordNet, cause a
term to be considered ambiguous, which in turn trig-
gers the use of less accurate retrieval methods. For
example, auction has a bridge-related WordNet sense
which is irrelevant for our dataset, but made the term
be considered ambiguous. This calls for develop-
ment of other methods for determining word ambigu-
ity, which consider the actual usage of terms in the
domain rather than relying solely on WordNet.
9 Conclusions
In this paper we presented a generic classification-
based scheme for comprehensively addressing con-
text matching in textual inference scenarios. We
presented a concrete implementation of the proposed
scheme for Name-based TC, and showed how CP
decisions can be integrated within the TC setting.
Utilizing classifiers for context matching offers
several advantages. They naturally incorporate di-
rectionality and allow integrating various types of
information, including ones not used in this work
such as syntactic features. Our results indeed support
this approach. Still, further research is required re-
garding issues raised by the use of multiple classifiers,
scalability in particular.
Hypotheses in TC are available in advance. While
also the case in other applications, it constitutes a
practical challenge when hypotheses are given ?on-
line?, like Information Retrieval queries, since classi-
fiers will have to be generated on the fly. We intend
to address this issue in future work.
Lastly, we plan to apply the generic classification-
based approach to address context matching in other
inference-based applications.
Acknowledgments
This work was partially supported by the Israel Sci-
ence Foundation grant 1112/08 and the NEGEV
project (www.negev-initiative.org).
28
References
Libby Barak, Ido Dagan, and Eyal Shnarch. 2009. Text
Categorization from Category Name via Lexical Refer-
ence. In HLT-NAACL (Short Papers).
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative Learning of Selectional Preference from
Unlabeled Text. In In Proceedings of EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022, March.
Michael Connor and Dan Roth. 2007. Context Sensitive
Paraphrasing with a Global Unsupervised Classifier. In
Proceedings of ECML.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct Word
Sense Matching for Lexical Substitution. In Proceed-
ings of COLING-ACL.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing Textual Entailment: Rational, Eval-
uation and Approaches. Natural Language Engineer-
ing, pages 15(4):1?17.
Scott Deerwester, Scott Deerwester, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by Latent Semantic Anal-
ysis. Journal of the American Society for Information
Science, 41:391?407.
Georgiana Dinu and Mirella Lapata. 2010. Topic Models
for Meaning Similarity in Context. In Proceedings of
Coling 2010: Posters.
Doug Downey and Oren Etzioni. 2009. Look Ma, No
Hands: Analyzing the Monotonic Feature Abstraction
for Text Classification. In Proceedings of NIPS.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense per Discourse. In Proceed-
ings of the workshop on Speech and Natural Language.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2009.
Improving Text Categorization Bootstrapping via Unsu-
pervised Learning. ACM Trans. Speech Lang. Process.,
6:1:1?1:24, October.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain Textual Question Answer-
ing Techniques. Natural Language Engineering, 9:231?
267, September.
Thorsten Joachims. 2006. Training Linear SVMs in
Linear Time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD).
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distributional
Similarity for Lexical Inference. Natural Language
Engineering, 16(4):359?389.
Diana McCarthy and Roberto Navigli. 2009. The English
Lexical Substitution Task. Language Resources and
Evaluation, 43(2):139?159.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157, June.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys, 41(2):1?69.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timo-
thy Chklovski, and Eduard Hovy. 2007. ISP: Learning
Inferential Selectional Preferences. In Proceedings of
NAACL-HLT.
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning Selectional Prefer-
ences for Entailment or Paraphrasing Rules. In Pro-
ceedings of RANLP.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Prefer-
ences. In Proceedings of ACL.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting Lexical Reference Rules from Wikipedia. In
Proceedings of IJCNLP-ACL.
Noah A. Smith and Jason Eisner. 2005. Contrastive
Estimation: Training Log-linear Models on Unlabeled
Data. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Gold-
berger. 2008. Contextual Preferences. In Proceedings
of ACL-08: HLT.
29
