Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 25?31,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Technical Support Dialog Systems: 
Issues, Problems, and Solutions  
 
 
Kate Acomb, Jonathan Bloom, Krishna Dayanidhi, Phillip 
Hunter, Peter Krogh, Esther Levin, Roberto Pieraccini 
SpeechCycle 
535 W 34th Street 
New York, NY 10001 
{kate,jonathanb,krishna,phillip,peter,roberto}@speechcycle.com 
esther@spacegate.com  
 
 
 
 
Abstract 
The goal of this paper is to give a description 
of the state of the art, the issues, the problems, 
and the solutions related to industrial dialog 
systems for the automation of technical sup-
port. After a general description of the evolu-
tion of the spoken dialog industry, and the 
challenges in the development of technical 
support applications, we will discuss two spe-
cific problems through a series of experimental 
results.    The first problem is the identification 
of the call reason, or symptom, from loosely 
constrained user utterances. The second is the 
use of data for the experimental optimization 
of the Voice User Interface (VUI). 
1 Introduction 
Since the beginning of the telephony spoken dialog 
industry, in the mid 1990, we have been witnessing 
the evolution of at least three generations of sys-
tems. What differentiates each generation is not 
only the increase of complexity, but also the dif-
ferent architectures used. Table 1 provides a sum-
mary of the features that distinguish each 
generation. The early first generation systems were 
mostly informational, in that they would require 
some information from the user, and would pro-
vide information in return. Examples of those sys-
tems, mostly developed during the mid and late 
1990s, are package tracking, simple financial ap-
plications, and flight status information. At the 
time there were no standards for developing dialog 
systems, (VoiceXML 1.0 was published as a rec-
ommendation in year 2000) and thus the first gen-
eration dialog applications were implemented on 
proprietary platforms, typically evolutions of exist-
ing touch-tone IVR (Interactive Voice Response) 
architectures.  
 
Since the early developments, spoken dialog sys-
tems were implemented as a graph, called call-
flow. The nodes of the call-flow typically represent 
actions performed by the system and the arcs rep-
resent an enumeration of the possible outcomes. 
Playing a prompt and interpreting the user re-
sponse through a speech recognition grammar is a 
typical action. Dialog modules (Barnard et al, 
1999) were introduced in order to reduce the com-
plexity and increase reusability of call-flows. A 
Dialog Module (or DM) is defined as a call-flow 
object that encapsulates many of the interactions 
needed for getting one piece of information from 
the user, including retries, timeout handling, dis-
ambiguation, etc. Modern commercial dialog sys-
tems use DMs as their active call-flow nodes.  
 
The number of DMs in a call-flow is generally an 
indication of the application complexity. First gen-
eration applications showed a range of complexity 
of a few to tens of DMs, typically spanning a few 
turns of interaction.   
 
The dialog modality is another characterization of 
applications. Early applications supported strict 
directed dialog interaction, meaning that at each 
25
turn the system would direct the user by proposing 
a finite?and typically small?number of choices. 
That would also result in a limited grammar or vo-
cabulary at each turn.  
 
The applications of the second generation were 
typically transactional, in the sense that they could 
perform a transaction on behalf of the user, like 
moving funds between bank accounts, trading 
stocks, or buying tickets. Most of those applica-
tions were developed using the new standards, 
typically as collections of VoiceXML documents. 
The complexity moved to the range of dozens of 
dialog modules, spanning a number of turns of in-
teractions of the order of ten or more. At the same 
time, some of the applications started using a tech-
nology known as Statistical Spoken Language Un-
derstanding, or SSLU (Gorin et al, 1997, Chu-
Carroll et al, 1999, Goel et al 2005), for mapping 
loosely constrained user utterances to a finite num-
ber of pre-defined semantic categories. The natural 
language modality?as opposed to directed dia-
log?was initially used mostly for call-routing, i.e. 
to route calls to the appropriate call center based 
on a more or less lengthy description of the reason 
for the call by the user.  
 
While the model behind the first and second gen-
erations of dialog applications can be described by 
the form-filling paradigm, and the interaction fol-
lows a pre-determined simple script, the systems of 
the third generation have raised to a qualitatively 
different level of complexity. Problem solving ap-
plications, like customer care, help desk, and tech-
nical support, are characterized by a level of 
complexity ranging in the thousands of DMs, for a 
number of turns of dynamic interaction that can 
reach into the dozens. As the sophistication of the 
applications evolved, so did the system architec-
ture by moving the logic from the client 
(VoiceXML browser, or voice-browser) to the 
server (Pieraccini and Huerta, 2005). More and 
more system are today based on generic dialog 
application server which interprets a dialog speci-
fication described by a?typically proprietary?
markup language and serve the voice-browser with 
dynamically generated VoiceXML documents. 
Finally, the interaction modality of the third gen-
eration systems is moving from the strictly directed 
dialog application, to directed dialog, with some 
natural language (SSLU) turns, and some limited 
mixed-initiative (i.e. the possibility for the user to 
change the course of the dialog by making an un-
solicited request).  
2 Technical Support Applications 
Today, automated technical support systems are 
among the most complex types of dialog applica-
tions. The advantage of automation is clear, espe-
cially for high-volume services like broadband-
internet, entertainment (cable or satellite TV), and 
telephony. When something goes wrong with the 
service, the only choice for subscribers is to call a 
technical support center. Unfortunately, staffing a 
call center with enough agents trained to help solve 
even the most common problems results in pro-
hibitive costs for the provider, even when out-
sourcing to less costly locations End users often 
experience long waiting times and poor service 
from untrained agents. With the magnitude of the 
daily increase in the number of subscribers of those 
services, the situation with human agents is bound 
to worsen. Automation and self-service can, and 
does, help reduce the burden constituted by the 
most frequent call reasons, and resort to human 
agents only for the most difficult and less common 
problems.  
GENERATION 
  FIRST SECOND THIRD 
Time Period 1994-2001 2000-2005 2004-today 
Type of Ap-
plication Informational 
Transac-
tional 
Problem 
Solving 
Examples 
Package 
Tracking, 
Flight Status 
Banking, 
Stock 
Trading, 
Train Res-
ervation 
Customer 
Care, 
Technical 
Support, 
Help Desk. 
Architecture Proprietary 
Static 
VoiceXML  
Dynamic 
VoiceXML 
Complexity 
(Number of 
DMs) 10 100 1000 
Interaction 
Turns  few 10 10-100 
Interaction 
Modality directed 
directed + 
natural 
language 
(SSLU) 
directed + 
natural 
language 
(SSLU) + 
limited 
mixed initia-
tive 
 
Table 1: Evolution of spoken dialog systems. 
26
 However, automating technical support is particu-
larly challenging for several reasons. Among them:   
 
- Troubleshooting knowledge is not readily 
available in a form that can be used for 
automation. Most often it is based on the 
idiosyncratic experience of the individual 
agents. 
- End users are typically in a somewhat 
emotionally altered state?something for 
which they paid and that is supposed to 
work is broken. They want it repaired 
quickly by an expert human agent; they 
don?t trust a machine can help them.  
- The description of the problem provided 
by the user can be imprecise, vague, or 
based on a model of the world that may be 
incorrect (e.g. some users of internet can-
not tell their modem from their router). 
- It may be difficult to instruct non-
technically savvy users on how to perform 
a troubleshooting step (e.g. Now renew 
your IP address.) or request technical in-
formation (e.g. Are you using a Voice over 
IP phone service?) 
- Certain events cannot be controlled. For 
instance, the time it would take for a user 
to complete a troubleshooting step, like re-
booting a PC, is often unpredictable.   
- The acoustic environment may be chal-
lenging. Users may be asked to switch 
their TV on, reboot their PC, or check the 
cable connections. All these operations can 
cause noise that can trigger the speech rec-
ognizer and affect the course of the inter-
action. 
 
On the other hand, one can leverage the automated 
diagnosis or troubleshooting tools that are cur-
rently used by human agent and improve the effi-
ciency of the interaction. For instance, if the IP 
address of the digital devices at the user premises 
is available, one can ping them, verify their con-
nectivity, download new firmware, and perform 
automated troubleshooting steps in the background 
without the intervention of the user. However, the 
interplay between automated and interactive op-
erations can raise the complexity of the applica-
tions such as to require higher level development 
abstractions and authoring tools.  
3 High Resolution SSLU 
The identification of the call reason?i.e. the prob-
lem or the symptoms of the problem experienced 
by the caller?is one of the first phases of the in-
teraction in a technical support application. There 
are two possible design choices with today?s spo-
ken language technology: 
 
- Directed dialog. A specific prompt enu-
merates all the possible reasons for a call, 
and the user would choose one of them. 
-  Natural Language: An open prompt asks 
the user to describe the reason for the call. 
The utterance will be automatically 
mapped to one of a number of possible call 
reasons using SSLU technology. 
 
Directed dialog would be the preferred choice in 
terms of accuracy and cost of development. Unfor-
tunately, in most technical support applications, the 
number of call-reasons can be very large, and thus 
prompting the caller through a directed dialog 
menu would be impractical. Besides, even though 
a long menu can be structured hierarchically as a 
cascade of several shorter menus, the terms used 
for indicating the different choices may be mis-
leading or meaningless for some of the users (e.g. 
do you have a problem with hardware, software, or 
networking?).  Natural language with SSLU is 
generally the best choice for problem identifica-
tion.  
 
In practice, users mostly don?t know what the ac-
tual problem with their service is (e.g. modem is 
wrongly configured), but typically they describe 
their observations?or symptoms?which are ob-
servable manifestations of the problem. and not the 
problem itself (e.g. symptom: I can?t connect to the 
Web, problem: modem wrongly configured). Cor-
rectly identifying the symptom expressed in natural 
language by users is the goal of the SSLU module.  
 
SSLU provides a mapping between input utter-
ances and a set of pre-determined categories. 
SSLU has been effectively used in the past to en-
able automatic call-routing. Typically call-routing 
applications have a number of categories, of the 
order of a dozen or so, which are designed based 
on the different routes to which the IVR is sup-
posed to dispatch the callers. So, generally, in call-
27
routing applications, the categories are known and 
determined prior to any data collection.  
 
One could follow the same approach for the prob-
lem identification SSLU, i.e. determine a number 
of a-priori problem categories and then map a col-
lection of training symptom utterances to each one 
of them. There are several issues with this ap-
proach.  
 
First, a complete set of categories?the prob-
lems?may not be known prior to the acquisition 
and analysis of a significant number of utterances. 
Often the introduction of new home devices or ser-
vices (such as DVR, or HDTV) creates new prob-
lems and new symptoms that can be discovered 
only by analyzing large amounts of utterance data.   
 
Then, as we noted above, the relationship between 
the problems?or broad categories of problems?
and the manifestations (i.e. the symptoms) may not 
be obvious to the caller. Thus, confirming a broad 
category in response to a detailed symptom utter-
ance may induce the user to deny it or to give a 
verbose response (e.g. Caller: I cannot get to the 
Web. System: I understand you have a problem 
with your modem configuration, is that right? 
Caller: Hmm?no. I said I cannot get to the Web.). 
 
Finally, caller descriptions have different degrees 
of specificity (e.g. I have a problem with my cable 
service vs. The picture on my TV is pixilated on all 
channels).  Thus, the categories should reflect a 
hierarchy of symptoms, from vague to specific, 
that need to be taken into proper account in the 
design of the interaction.  
 
As a result from the above considerations, SSLU 
for symptom identification needs to be designed in 
order to reflect the high-resolution multitude and 
specificity hierarchy of symptoms that emerge 
from the analysis of a large quantity of utterances. 
Figure 1 shows an excerpt from the hierarchy of 
symptoms for a cable TV troubleshooting applica-
tion derived from the analysis of almost 100,000 
utterance transcriptions.  
 
Each node of the tree partially represented by Fig-
ure 1 is associated with a number of training utter-
ances from users describing that particular 
symptom in their own words. For instance the top-
most node of the hierarchy, ?TV Problem?, corre-
sponds to vague utterances such as I have a 
problem with my TV or My cable TV does not 
work. The? Ordering? node represents requests of 
the type I have a problem with ordering a show, 
which is still a somewhat vague request, since one 
can order ?Pay-per-view? or ?On-demand? events, 
and they correspond to different processes and 
troubleshooting steps. Finally, at the most detailed 
level of the hierarchy, for instance for the node 
?TV Problem-Ordering-On Demand-Error?, one 
finds utterances such as I tried to order a movie on 
demand, but all I get is an error code on the TV. 
 
In the experimental results reported below, we 
trained and tested a hierarchically structured SSLU 
for a cable TV troubleshooting application. A cor-
pus of 97,236 utterances was collected from a de-
ployed application which used a simpler, non 
hierarchical, version of the SSLU. The utterances 
were transcribed and initially annotated based on 
an initial set of symptoms. The annotation was car-
ried out by creating an annotation guide document 
which includes, for each symptom, a detailed ver-
bal description, a few utterance examples, and 
relevant keywords. Human annotators were in-
structed to label each utterance with the correct 
category based on the annotation guide and their 
TV Problem
On Demand
Pay-per-view
Ordering
No Picture
Error
PIN
Other
Error
 
 
Figure 1: Excerpt from the hierarchical symp-
tom description in a cable TV technical support 
application 
 
28
work was monitored systematically by the system 
designer. 
 
After a first initial annotation of the whole corpus, 
the annotation consistency was measured by com-
puting a cluster similarity distance between the 
utterances corresponding to all possible pairs of 
symptoms. When the consistency between a pair of 
symptoms was below a given threshold, the clus-
ters were analyzed, and actions taken by the de-
signer in order to improve the consistency, 
including reassign utterances and, if necessary, 
modifying the annotation guide. The whole process 
was repeated a few times until a satisfactory global 
inter-cluster distance was attained.  
 
Eventually we trained the SSLU on 79 symptoms 
arranged on a hierarchy with a maximum depth of 
3. Table 2 summarizes the results on an independ-
ent test set of 10,332 utterances.  The result shows 
that at the end of the process, a satisfactory batch 
accuracy of 81.43% correct label assignment what 
attained for the utterances which were deemed to 
be in-domain, which constituted 90.22% of the test 
corpus. Also, the system was able to correctly re-
ject 24.56% of out-of-domain utterances. The 
overall accuracy of the system was considered rea-
sonable for the state of the art of commercial 
SSLUs based on current statistical classification 
algorithms. Improvement in the classification per-
formance can result by better language models (i.e. 
some of the errors are due to incorrect word recog-
nition by the ASR) and better classifiers, which 
need to take into account more features of the in-
coming utterances, such as word order1 and con-
textual information. 
                                                          
1
 Current commercial SSLU modules,  as the one used in the 
work described here, use statistical classifiers based only on 
bags of words. Thus the order of the words in the incoming 
utterance is not taken into consideration.  
3.1 Confirmation Effectiveness 
Accuracy is not the only measure to provide an 
assessment of how the symptom described by the 
caller is effectively captured. Since the user re-
sponse needs to be confirmed based on the inter-
pretation returned by the SSLU, the caller always 
has the choice of accepting or denying the hy-
pothesis. If the confirmation prompts are not prop-
erly designed, the user can erroneously deny 
correctly detected symptoms, or erroneously accept 
wrong ones.  
 
The analysis reported below was carried out for a 
deployed system for technical support of Internet 
service. The full symptom identification interac-
tions following the initial open prompt was tran-
scribed and annotated for 895 calls. The SSLU 
used in this application consisted of 36 symptoms 
structured in a hierarchy with a maximum depth of 
3. For each interaction we tracked the following 
events: 
 
- the first user response to the open question 
- successive responses in case of re-
prompting because of speech recognition 
rejection or timeout 
- response to the yes/no confirmation ques-
tion) 
- successive responses to the confirmation 
question in case the recognizer rejected it 
or timed out. 
- Successive responses to the confirmation 
question in case the user denied, and a 
second best hypothesis was offered. 
 
Table 3 summarizes the results of this analysis. 
  
The first row reports the number of calls for which 
the identified symptom was correct (as compared 
with human annotation) and confirmed by the 
caller.  The following rows are the number of calls 
where the identified symptom was wrong and the 
caller still accepted it during confirmation, the 
symptom was correct and the caller denied it, and 
the symptom was wrong and denied, respectively. 
Finally there were 57 calls where the caller did not 
provide any confirmation (e.g. hung up, timed out, 
ASR rejected the confirmation utterance even after 
re-prompting, etc.), and 100 calls in which it was 
not possible to collect the symptom (e.g. rejections 
Utterances 10332 100.00% 
In domain 9322 90.22% 
Correct  in-domain 7591 81.43% 
Out of domain  1010 9.78% 
Correct rejection out-of-
domain 249 24.65% 
 
Table 2: Accuracy results for Hierarchical 
SSLU with 79 symptoms. 
29
of first and second re-prompts, timeouts, etc.) In 
both cases?i.e. no confirmation or no symptom 
collection at all?the call continued with a differ-
ent strategy (e.g. moved to a directed dialog, or 
escalated the call to a human agent). The interest-
ing result from this experiment is that the SSLU 
returned a correct symptom 59.8 + 2.5 = 62.3% of 
the times (considering both in-domain and out-of-
domain utterances), but the actual ?perceived? ac-
curacy (i.e. when the user accepted the result) was 
higher, and precisely 59.8 + 13.2 = 73%. A deeper 
analysis shows that for most of the wrongly ac-
cepted utterances the wrong symptom identified by 
the SSLU was still in the same hierarchical cate-
gory, but with different degree of specificity (e.g. 
Internet-Slow vs. vague Internet) 
 
The difference between the actual and perceived 
accuracy of SSLU has implications for the overall 
performance of the application. One could build a 
high performance SSLU, but a wrongly confirmed 
symptom may put the dialog off course and result 
in reduced automation, even though the perceived 
accuracy is higher. Confirmation of SSLU results 
is definitely an area where new research can poten-
tially impact the performance of the whole system. 
4 Experimental VUI 
Voice User Interface (VUI) is typically considered 
an art. VUI designers acquire their experience by 
analyzing the effect of different prompts on the 
behavior of users, and can often predict whether a 
new prompt can help, confuse, or expedite the in-
teraction. Unfortunately, like all technologies rely-
ing on the anecdotal experience of the designer, in 
VUI it is difficult to make fine adjustments to an 
interface and predict the effect of competing simi-
lar designs before the application is actually de-
ployed. However, in large volume applications, 
and when a global measure of performance is 
available, one can test different non-disruptive de-
sign hypotheses on the field, while the application 
is running. We call this process experimental VUI.  
 
There have been, in the past, several studies aimed 
at using machine learning for the design of dialog 
systems (Levin et al, 2000, Young 2002, Pietquin 
et al 2006). Unfortunately, the problem of full de-
sign of a system based uniquely on machine learn-
ing is a very difficult one, and cannot be fully 
utilized yet for commercial systems. A simpler and 
less ambitious goal is that of finding the optimal 
dialog strategy among a small number of compet-
ing designs, where all the initial designs are work-
ing reasonably well (Walker 2000, Paek et al2004, 
Lewis 2006). Comparing competing designs re-
quires carrying on an exploration based on random 
selection of each design at crucial points of the 
dialog. Once a reward schema is defined, one can 
use it for changing the exploration probability so as 
to maximize a function of the accumulated reward 
using, for instance, one of the algorithms described 
in (Sutton 1998). 
 
Defining many different competing designs at sev-
eral points of the interaction is often impractical 
and costly. Moreover, in a deployed commercial 
application, one needs to be careful about main-
taining a reasonable user experience during explo-
ration. Thus, the competing designs have to be 
chosen carefully and applied to portions of the dia-
log where the choice of the optimal design can 
make a significant difference for the reward meas-
ure in use.  
 
In the experiments described below we selected the 
symptom identification as a point worth exploring. 
in an internet technical support application We 
then defined three prompting schemas 
 
- Schema A: the system plays an open 
prompt 
- Schema B: the system plays an open 
prompt, and then provides some examples 
of requests 
- Schema C: The system plays an open 
prompt, and then suggests a command that 
provides a list of choices. 
 
Accepted correct 535 59.8% 
Accepted wrong 118 13.2% 
Denied correct 22 2.5% 
Denied wrong 63 7.0% 
Unconfirmed 57 6.4% 
No result 100 11.2% 
TOTAL 895 100.0% 
 
Table 3: Result of the confirmation analy-
sis based on the results of 895 calls 
30
The three schemas were implemented on a de-
ployed system for limited time. There was 1/3 
probability for each individual call to go through 
one of the above schemas. The target function cho-
sen for optimization was the average automation 
rate.  
 
Figure 2 shows the effect on the cumulated average 
automation rate for each one of the competing de-
sign. The exploration was carried out until the dif-
ference in the automation rate among the three 
designs reached statistical significance, which was 
after 13 days with a total number of 21,491 calls. 
At that point in time we established that design B 
had superior performance, as compared to A and 
C, with a difference of 0.68 percent points.  
Event though the gain in total automation rate (i.e. 
0.68 percent points) seems to be modest, one has to 
consider that this increase is simply caused only by 
the selection  of the best wording of a single 
prompt in an application with thousands of 
prompts. One can expect to obtain more important 
improvements by at looking to other areas of the 
dialog where experimental VUI can be applied and 
selecting the optimal prompt can have an impact 
on the overall automation rate.  
5 Conclusions 
We started this paper by describing the advances 
achieved in dialog system technology for commer-
cial applications during the past decade. The indus-
try moved from the first generation of systems able 
to handle very structured and simple interactions, 
to a current third generation where the interaction 
is less structured and the goal is to automate com-
plex tasks such as problem solving and technical 
support.We then discussed general issues regarding 
the effective development of a technical support 
application. In particular we focused on two areas: 
the collection of the symptom from natural lan-
guage expressions, and the experimental optimiza-
tion of the VUI strategy. In both cases we 
described how a detailed analysis of live data can 
greatly help optimize the overall performance.  
6 References 
Barnard, E., Halberstadt, A., Kotelly, C., Phillips, M.,  1999 
?A Consistent Approach To Designing Spoken-dialog 
Systems,? Proc. of ASRU99 ? IEEE Workshop, Keystone, 
Colorado, Dec. 1999. 
Gorin, A. L., Riccardi, G.,Wright, J. H.,  1997 Speech Com-
munication, vol. 23, pp. 113-127, 1997. 
Chu-Carroll, J., Carpenter B., 1999. ?Vector-based natural 
language call routing,? Computational Linguistics, 
v.25, n.3, p.361-388, September 1999 
Goel, V., Kuo, H.-K., Deligne, S., Wu S.,  2005 ?Language 
Model Estimation for Optimizing End-to-end Performance 
of a Natural Language Call Routing System,? ICASSP 
2005 
Pieraccini, R., Huerta, J., Where do we go from here? Re-
search and Commercial Spoken Dialog Systems, Proc. of 
6th SIGdial Workshop on Discourse and Dialog, Lisbon, 
Portugal, 2-3 September, 2005. pp. 1-10 
Levin, E., Pieraccini, R., Eckert, W., A Stochastic Model of 
Human-Machine Interaction for Learning Dialog Strate-
gies,  IEEE Trans. on Speech and Audio Processing, Vol. 
8, No. 1, pp. 11-23, January 2000. 
Pietquin, O., Dutoit, T., A Probabilistic Framework for Dialog 
Simulation and Optimal Strategy Learning, In IEEE 
Transactions on Audio, Speech and Language Processing, 
14(2):589-599, 2006 
Young, S., Talking to Machines (Statistically Speaking), Int 
Conf Spoken Language Processing, Denver, Colorado. 
(2002). 
Walker, M., An Application of Reinforcement Learning to 
Dialogue Strategy Selection in a Spoken Dialogue System 
for Email . Journal of Artificial Intelligence Research, 
JAIR, Vol 12., pp. 387-416, 2000 
Paek T., Horvitz E.,. Optimizing automated call routing by 
integrating spoken dialog models with queuing models. 
Proceedings of HLT-NAACL, 2004, pp. 41-48. 
Lewis, C., Di Fabbrizio, G., Prompt Selection with Rein-
forcement Learning in an AT&T Call Routing Applica-
tion, Proc. of Interspeech 2006, Pittsburgh, PA. pp. 1770-
1773, (2006) 
Sutton, R.S., Barto, A.G. (1998). Reinforcement Learning: An 
Introduction. MIT Press. 
 
14.00%
15.00%
16.00%
17.00%
18.00%
19.00%
20.00%
21.00%
1 2 3 4 5 6 7 8 9 10 11 12 13
Time (days)
A
u
to
m
a
tio
n
 
ra
te
A
B
C
Figure 2: Daily average automation rate for com-
peting designs. 
31
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 349?356,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
A Handsome Set of Metrics to Measure Utterance Classification
Performance in Spoken Dialog Systems
David Suendermann, Jackson Liscombe, Krishna Dayanidhi, Roberto Pieraccini?
SpeechCycle Labs, New York, USA
{david, jackson, krishna, roberto}@speechcycle.com
Abstract
We present a set of metrics describing
classification performance for individual
contexts of a spoken dialog system as well
as for the entire system. We show how
these metrics can be used to train and tune
system components and how they are re-
lated to Caller Experience, a subjective
measure describing how well a caller was
treated by the dialog system.
1 Introduction
Most of the speech recognition contexts in com-
mercial spoken dialog systems aim at mapping the
caller input to one out of a set of context-specific
semantic classes (Knight et al, 2001). This is done
by providing a grammar to the speech recognizer
at a given recognition context. A grammar serves
two purposes:
? It constraints the lexical content the recog-
nizer is able to recognize in this context (the
language model) and
? It assigns one out of a set of possible classes
to the recognition hypothesis (the classifier).
This basic concept is independent of the nature of
a grammar: it can be a rule-based one, manually or
automatically generated; it can comprise a statisti-
cal language model and a classifier; it can consist
of sets of grammars, language models, or classi-
fiers; or it can be a holistic grammar, i.e., a sta-
tistical model combining a language model and a
classification model in one large search tree.
Most commercial dialog systems utilize gram-
mars that return a semantic parse in one of these
contexts:
? directed dialogs (e.g., yes/no contexts, menus
with several choices, collection of informa-
tion out of a restricted set [Which type of
modem do you have?]?usually, less than 50
classes)
? open-ended prompts (e.g. for call routing,
problem capture; likewise to collect infor-
mation out of a restricted set [Tell me what
?Patent pending.
you are calling about today]?possibly sev-
eral hundred classes (Gorin et al, 1997; Boye
and Wiren, 2007))
? information collection out of a huge (or infi-
nite) set of classes (e.g., collection of phone
numbers, dates, names, etc.)
When the performance of spoken dialog sys-
tems is to be measured, there is a multitude of
objective metrics to do so, many of which feature
major disadvantages. Examples include
? Completion rate is calculated as the number
of completed calls divided by the total num-
ber of calls. The main disadvantage of this
metric is that it is influenced by many fac-
tors out of the system?s control, such as caller
hang-ups, opt-outs, or call reasons that fall
out of the system?s scope. Furthermore, there
are several system characteristics that impact
this metric, such as recognition performance,
dialog design, technical stability, availability
of back-end integration, etc. As experience
shows, all of these factors can have unpre-
dictable influence on the completion rate. On
the one hand, a simple wording change in the
introduction prompt of a system can make
this rate improve significantly, whereas, on
the other hand, major improvement of the
open-ended speech recognition grammar fol-
lowing this very prompt may not have any
impact.
? Average holding time is a common term for
the average call duration. This metric is often
considered to be quite controversial since it is
unclear whether longer calls are preferred or
dispreferred. Consider the following two in-
congruous behaviors resulting in longer call
duration:
? The system fails to appropriately treat
callers, asking too many questions, per-
forming redundant operations, acting
unintelligently because of missing back-
end integration, or letting the caller wait
in never-ending wait music loops.
? The system is so well-designed that it
engages callers to interact with the sys-
tem longer.
349
? Hang-up and opt-out rates. These metrics
try to encapsulate how many callers choose
not to use the dialog system, either because
they hang up or because they request to speak
with a human operator. However, it is unclear
how such events are related to dialog system
performance. Certainly, many callers may
have a prejudice against speaking with auto-
mated systems and may hang up or request
a human regardless of how well-performing
the dialog system is with cooperative users.
Furthermore, callers who hang up may do so
because they are unable to get their problem
solved or they may hang up precisely because
their problem was solved (instead of waiting
for the more felicitous post-problem-solving
dialog modules).
? Retry rate is calculated as the average num-
ber of times that the system has to re-prompt
for caller input because the caller?s previ-
ous utterance was determined to be Out-of-
Grammar. The intuition behind this metric
is that the lower the retry rate, the better
the system. However, this metric is prob-
lematic because it is tied to grammar per-
formance itself. Consider a well-performing
grammar that correctly accepts In-Grammar
utterances and rejects Out-of-Grammar utter-
ances. This grammar will cause the system to
produce retries for all Out-of-Grammar utter-
ances. Consider a poorly designed grammar
that accepts everything (incorrectly), even
background noise. This grammar would de-
crease the retry rate but would not be indica-
tive of a well-performing dialog system.
As opposed to these objective measures, there is
a subjective measure directly related to the system
performance as perceived by the user:
? Caller Experience. This metric is used to
describe how well the caller is treated by the
system according to its design. Caller Expe-
rience is measured on a scale between 1 (bad)
and 5 (excellent). This is the only subjective
measure in this list and is usually estimated
based on averaging scores given by multi-
ple voice user interface experts which listen
to multiple full calls. Although this metric
directly represents the ultimate design goal
for spoken dialog systems?i.e., to achieve
highest possible user experience?it is very
expensive to be repeatedly produced and not
suitable to be generated on-the-fly.
Our former research has suggested, however,
that it may be possible to automatically esti-
mate Caller Experience based on several ob-
jective measures (Evanini et al, 2008). These
measures include the overall number of no-
matches and substitutions in a call, opera-
tor requests, hang-ups, non-heard speech, the
fact whether the call reason could be suc-
cessfully captured and whether the call rea-
son was finally satisfied. Initial experiments
showed a near-human accuracy of the auto-
matic predictor trained on several hundred
calls with available manual Caller Experi-
ence scores. The most powerful objective
metric turned out to be the overall number
of no-matches and substitutions, indicating a
high correlation between the latter and Caller
Experience.
No-matches and substitutions are objective met-
rics defined in the scope of semantic classification
of caller utterances. They are part of a larger set of
semantic classification metrics which we system-
atically demonstrate in Section 2. The remainder
of the paper examines three case studies exploring
the usefulness and interplay of different evaluation
metrics, including:
? the correlation between True Total (one of the
introduced metrics) and Caller Experience in
Section 3,
? the estimation of speech recognition and clas-
sification parameters based on True Total and
True Confirm Total (another metric) in Sec-
tion 4, and
? the tuning of large-scale spoken dialog sys-
tems to maximize True Total and its effect on
Caller Experience in Section 5.
2 Metrics for Utterance Classification
Acoustic events processed by spoken dialog sys-
tems are usually split into two main categories:
In-Grammar and Out-of-Grammar. In-Grammar
utterances are all those that belong to one of the
semantic classes processable by the system logic
in the given context. Out-of-Grammar utterances
comprise all remaining events, such as utterances
whose meanings are not handled by the grammar
or when the input is non-speech noise.
Spoken dialog systems usually respond to
acoustic events after being processed by the gram-
mar in one of three ways:
? The event gets rejected. This is when the sys-
tem either assumes that the event was Out-
of-Grammar, or it is so uncertain about its
(In-Grammar) finding that it rejects the utter-
ance. Most often, the callers get re-prompted
for their input.
350
Table 1: Event Acronyms
I In-Grammar
O Out-of-Grammar
A Accept
R Reject
C Correct
W Wrong
Y Confirm
N Not-Confirm
TA True Accept
FA False Accept
TR True Reject
FR False Reject
TAC True Accept Correct
TAW True Accept Wrong
FRC False Reject Correct
FRW False Reject Wrong
FAC False Accept Confirm
FAA False Accept Accept
TACC True Accept Correct Confirm
TACA True Accept Correct Accept
TAWC True Accept Wrong Confirm
TAWA True Accept Wrong Accept
TT True Total
TCT True Confirm Total
? The event gets accepted. This is when the
system is certain to have correctly detected
an In-Grammar semantic class.
? The event gets confirmed. This is when the
system assumes to have correctly detected an
In-Grammar class but still is not absolutely
certain about it. Consequently, the caller is
asked to verify the class. Historically, confir-
mations are not used in many contexts where
they would sound confusing or distracting,
for instance in yes/no contexts (?I am sorry.
Did you say NO????No!???This was NO,
yes????No!!!?).
Based on these categories, an acoustic event and
how the system responds to it can be described by
four binary questions:
1. Is the event In-Grammar?
2. Is the event accepted?
3. Is the event correctly classified?
4. Is the event confirmed?
Now, we can draw a diagram containing the first
two questions as in Table 2. See Table 1 for all
Table 2: In-Grammar? Accepted?
A R
I TA FR
O FA TR
Table 3: In-Grammar? Accepted? Correct?
A R
C W C W
I TAC TAW FRC FRW
O FA TR
acoustic event classification types used in the re-
mainder of this paper.
Extending the diagram to include the third ques-
tion is only applicable to In-Grammar events since
Out-of-Grammar is a single class and, therefore,
can only be either falsely accepted or correctly re-
jected as shown in Table 3.
Further extending the diagram to accomodate
the fourth question on whether a recognized class
was confirmed is similarly only applicable if an
event was accepted, as rejections are never con-
firmed; see Table 4. Table 5 gives one example for
each of the above introduced events for a yes/no
grammar.
When the performance of a given recognition
context is to be measured, one can collect a cer-
tain number of utterances recorded in this context,
look at the recognition and application logs to see
whether these utterances where accepted or con-
firmed and which class they were assigned to, tran-
scribe and annotate the utterances for their seman-
tic class and finally count the events and divide
them by the total number of utterances. If X is an
event from the list in Table 1, we want to refer to
x as this average score, e.g., tac is the fraction of
total events correctly accepted. One characteristic
of these scores is that they sum up to 1 for each of
the Diagrams 2 to 4 as for example
a + r = 1, (1)
i + o = 1, (2)
ta + fr + fa + tr = 1. (3)
In order to enable system tuning and to report
system performance at-a-glance, the multitude of
metrics must be consolidated into a single power-
ful metric. In the industry, one often uses weights
to combine metrics since they are assumed to have
different importance. For instance, a False Ac-
cept is considered worse than a False Reject since
the latter allows for correction in the first retry
whereas the former may lead the caller down the
351
Table 5: Examples for utterance classification metrics. This table shows the transcription of an utterance,
the semantic class it maps to (if In-Grammar), a binary flag for whether the utterance is In-Grammar, the
recognized class (i.e. the grammar output), a flag for whether the recognized class was accepted, a flag
for whether the recognized class was correct (i.e. matched the transcription?s semantic class), a flag
for whether the recognized class was confirmed, and the acronym of the type of event the respective
combination results in.
utterance class In-Grammar? rec. class accepted? correct? confirmed? event
yeah YES 1 I
what 0 O
NO 1 A
NO 0 R
no no no NO 1 NO 1 C
yes ma?am YES 1 NO 0 W
1 Y
0 N
i said no NO 1 YES 1 TA
oh my god 0 NO 1 FA
i can?t tell 0 NO 0 TR
yes always YES 1 YES 0 FR
yes i guess so YES 1 YES 1 1 TAC
no i don?t think so NO 1 YES 1 0 TAW
definitely yes YES 1 YES 0 1 FRC
no man NO 1 YES 0 0 FRW
sunshine 0 YES 1 1 FAC
choices 0 NO 1 0 FAA
right YES 1 YES 1 1 1 TACC
yup YES 1 YES 1 1 0 TACA
this is true YES 1 NO 1 0 1 TAWC
no nothing NO 1 YES 1 0 0 TAWA
Table 4: In-Grammar? Accepted? Correct? Con-
firmed?
A R
C W C W
Y TACC TAWC
I N TACA TAWA FRC FRW
Y FACO N FAA TR
wrong path. However, these weights are heavily
negotiable and depend on customer, application,
and even the recognition context, making it im-
possible to produce a comprehensive and widely
applicable consolidated metric. This is why we
propose to split the set of metrics into two groups:
good and bad. The sought-for consolidated met-
ric is the sum of all good metrics (hence, an over-
all accuracy) or, alternatively, the sum of all bad
events (overall error rate). In Tables 3 and 4, good
metrics are highlighted. Accordingly, we define
two consolidated metrics True Total and True Con-
firm Total as follows:
tt = tac + tr, (4)
tct = taca + tawc + fac + tr. (5)
In the aforementioned special case that a recog-
nition context never confirms, Equation 5 equals
Equation 4 since the confirmation terms tawc and
fac disappear.
The following sections report on three case
studies on the applicability of True Total and True
Confirm Total to the tuning of spoken dialog sys-
tems and how they relate to Caller Experience.
3 On the Correlation between True Total
and Caller Experience
As motivated in Section 1, initial experiments on
predicting Caller Experience based on objective
metrics indicated that there is a considerable cor-
relation between Caller Experience and semantic
352
Table 6: Pearson correlation coefficient for sev-
eral utterance classification metrics on the source
data.
A R
C W
I 0.394 -0.160 ......-0.230......
O -0.242 -0.155
r(TT) = 0.378
classification metrics such as those introduced in
Section 2. In the first of our case studies, this effect
is to be deeper analyzed and quantified. For this
purpose, we selected 446 calls from four different
spoken dialog systems of the customer service hot-
lines of three major cable service providers. The
spoken dialog systems comprised
? a call routing application?cf. (Suendermann
et al, 2008),
? a cable TV troubleshooting application,
? a broadband Internet troubleshooting appli-
cation, and
? a Voice-over-IP troubleshooting
application?see for instance (Acomb et
al., 2007).
The calls were evaluated by voice user interface
experts and Caller Experience was rated according
to the scale introduced in Section 1. Furthermore,
all speech recognition utterances (4480) were tran-
scribed and annotated with their semantic classes.
Thereafter, all utterance classification metrics in-
troduced in Section 2 were computed for every call
individually by averaging across all utterances of
a call. Finally, we applied the Pearson correlation
coefficient (Rodgers and Nicewander, 1988) to the
source data points to correlate the Caller Experi-
ence score of a single call to the metrics of the
same call. This was done in Table 6.
Looking at these numbers, whose magnitude is
rather low, one may be suspect of the findings.
E.g., |r(FR)| > |r(TAW)| suggesting that False
Reject has a more negative impact on Caller Expe-
rience than True Accept Wrong (aka Substitution)
which is against common experience. Reasons for
the messiness of the results are that
? Caller Experience is subjective and affected
by inter- and intra-expert inconsistency. E.g.,
in a consistency cross-validation test, we ob-
served identical calls rated by one subject as
1 and by another as 5.
Figure 1: Dependency between Caller Experience
and True Total.
? Caller Experience scores are discrete, and,
hence, can vary by ?1, even in case of strong
consistency.
? Although utterance classification metrics are
(almost) objective metrics measuring the per-
centage of how often certain events happen
in average, this average generated for indi-
vidual calls may not be very meaningful. For
instance, a very brief call with a single yes/no
utterance correctly classified results in the
same True Total score like a series of 50 cor-
rect recognitions in a 20-minutes conversa-
tion. While the latter is virtually impossible,
the former happens rather often and domi-
nates the picture.
? The sample size of the experiment conducted
in the present case study (446 calls) is per-
haps too small for deep analyses on events
rarely happening in the investigated calls.
Trying to overcome these problems, we com-
puted all utterance classification metrics intro-
duced in Section 2, grouping and averaging them
for the five distinct values of Caller Experience.
As an example, we show the almost linear graph
expressing the relationship between True Total and
Caller Experience in Figure 1. Applying the Pear-
son correlation coefficient to this five-point curve
yields r = 0.972 confirming that what we see is
pretty much a straight line. Comparing this value
to the coefficients produced by the individual met-
rics TAC, TAW, FR, FA, and TR as done in Ta-
ble 7, shows that no other line is as straight as the
one produced by True Total supposing its maxi-
mization to produce spoken dialog systems with
highest level of user experience.
353
Table 7: Pearson correlation coefficient for sev-
eral utterance classification metrics after group-
ing and averaging.
A R
C W
I 0.969 -0.917 ......-0.539......
O -0.953 -0.939
r(TT) = 0.972
4 Estimating Speech Parameters by
Maximizing True Total or True
Confirm Total
The previous section tried to shed some light on
the relationship between some of the utterance
classification metrics and Caller Experience. We
saw that, on average, increasing Caller Experience
comes with increasing True Total as the almost lin-
ear curve of Figure 1 supposes. As a consequence,
much of our effort was dedicated to maximizing
True Total in diverse scenarios. Speech recogni-
tion as well as semantic classification with all their
components (such as acoustic, language, and clas-
sification models) and parameters (such as acous-
tic and semantic rejection and confirmation confi-
dence thresholds, time-outs, etc.) was set up and
tuned to produce highest possible scores. This sec-
tion gives two examples of how parameter settings
influence True Total.
4.1 Acoustic Confirmation Threshold
When a speech recognizer produces a hypothesis
of what has been said, it also returns an acoustic
confidence score which the application can utilize
to decide whether to reject the utterance, confirm
it, or accept it right away. The setting of these
thresholds has obviously a large impact on Caller
Experience since the application is to reject as few
valid utterances as possible, not confirm every sin-
gle input, but, at the same time, not falsely accept
wrong hypotheses. It is also known that these set-
tings can strongly vary from context to context.
E.g., in announcements, where no caller input is
expected, but, nonetheless utterances like ?agent?
or ?help? are supposed to be recognized, rejection
must be used much more aggressively than in col-
lection contexts. True Total or True Confirm To-
tal are suitable measures to detect the optimum
tradeoff. Figure 2 shows the True Confirm Total
graph for a collection context with 30 distinguish-
able classes. At a confidence value of 0.12, there
is a local and global maximum indicating the opti-
mum setting for the confirmation threshold for this
grammar context.
Figure 2: Tuning the acoustic confirmation thresh-
old.
4.2 Maximum Speech Time-Out
This parameter influences the maximum time the
speech recognizer keeps recognizing once speech
has started until it gives up and discards the recog-
nition hypothesis. Maximum speech time-out is
primarily used to limit processor load on speech
recognition servers and avoid situations in which
line noise and other long-lasting events keep the
recognizer busy for an unnecessarily long time. As
it anecdotally happened to callers that they were
interrupted by the dialog system, on the one hand,
some voice user interface designers tend to chose
rather large values for this time-out setting, e.g.,
15 or 20 seconds. On the other hand, very long
speech input tends to produce more likely a clas-
sification error than shorter ones. Might there be a
setting which is optimum from the utterance clas-
sification point of view?
To investigate this behavior, we took 115,885
transcribed and annotated utterances collected in
the main collection context of a call routing ap-
plication and aligned them to their utterance dura-
Figure 3: Dependency between utterance duration
and True Total.
354
Figure 4: Dependency between maximum speech
time-out and True Total.
tions. Then, we ordered the utterances in descend-
ing order of their duration, grouped always 1000
successive utterances together, and averaged over
duration and True Total. This generated 116 data
points showing the relationship between the dura-
tion of an utterance and its expected True Total,
see Figure 3.
The figure shows a clear maximum somewhere
around 2.5 seconds and then descends with in-
creasing duration towards zero. Utterances with
a duration of 9 seconds exhibited a very low True
Total score (20%). Furthermore, it would appear
that one should never allow utterances to exceed
four second in this context. However, upon fur-
ther evaluation of the situation, we also have to
consider that long utterances occur much less fre-
quently than short ones. To integrate the frequency
distribution into this analysis, we produced an-
other graph that shows the average True Total ac-
cumulated over all utterances shorter than a cer-
tain duration. This simulates the effect of using
a different maximum speech time-out setting and
is displayed in Figure 4. We also show a graph
on how many of the utterances would have been
interrupted in Figure 5.
The curve shows an interesting down-up-down
trajection which can be explained as follows:
? Acoustic events shorter than 1.0 seconds are
mostly noise events which are correctly iden-
tified since the speech recognizer could not
even build a search tree and returns an empty
hypothesis which the classifier, in turn, cor-
rectly rejects.
? Utterances with a duration around 1.5s are
dominated by single words which cannot
properly evaluated by the (trigram) language
model. So, the acoustic model takes over the
main work and, because of its imperfectness,
lowers the True Total.
Figure 5: Percentage of utterances interrupted by
maximum speech time-out.
? Utterances with a moderate number of words
are best covered by the language model, so
we achieve highest accuracy for them (?3s).
? The longer the utterances continues after 4
seconds, the less likely the language model
and classfier are to have seen such utterances,
and True Total declines.
Evaluating the case from the pure classifier per-
formance perspective, the maximum speech time-
out would have to be set to a very low value
(around 3 seconds). However, at this point, about
20% of the callers would be interrupted. The deci-
sion whether this optimimum should be accepcted
depends on how elegantly the interruption can be
designed:
?I?m so sorry to interrupt, but I?m hav-
ing a little trouble getting that. So, let?s
try this a different way.?
5 Continuous Tuning of a Spoken Dialog
System to Maximize True Total and Its
Effect on Caller Experience
In the last two sections, we investigated the corre-
lation between True Total and Caller Experience
and gave examples on how system parameters can
be tuned by maximizing True Total. The present
section gives a practical example of how rigorous
improvement of utterance classification leads to
real improvement of Caller Experience.
The application in question is a combination of
the four systems listed in Section 3 which work
in an interconnected fashion. When callers access
the service hotline, they are first asked to briefly
describe their call reason. After up to two follow-
up questions to further disambiguate their reason,
they are either connected to a human operator or
one of the three automated troubleshooting sys-
tems. Escalation from one of them can connect
355
Figure 6: Increase of the True Total of a large-
vocabulary grammar with more than 250 classes
over release time.
the caller to an agent, transfer the caller back to
the call router or to one of the other troubleshoot-
ing systems.
When the application was launched in June
2008, its True Total averaged 78%. During the fol-
lowing three months, almost 2.2 million utterances
were collected, transcribed, and annotated for their
semantic classes to train statistical update gram-
mars in a continuously running process (Suender-
mann et al, 2009). Whenever a grammar sig-
nificantly outperformed the most recent baseline,
it was released and put into production leading
to an incremental improvement of performance
throughout the application. As an example, Fig-
ure 6 shows the True Total increase of the top-level
large-vocabulary grammar that distinguishes more
than 250 classes. The overall performance of the
application went up to more than 90% True Total
within three months of its launch.
Having witnessed a significant gain of a spoken
dialog system?s True Total, we would now like to
know to what extent this improvement manifests
itself in an increase of Caller Experience. Fig-
ure 7 shows that, indeed, Caller Experience was
strongly positively affected. Over the same three
month period, we achieved an iterative increase
from an initial Caller Experience of 3.4 to 4.6.
6 Conclusion
Several of our investigations have suggested a con-
siderable correlation between True Total, an objec-
tive utterance classification metric, and Caller Ex-
perience, a subjective score of overall system per-
formance usually rated by expert listeners. This
observation leads to our main conclusions:
? True Total and several of the other utterance
classification metrics introduced in this paper
can be used as input to a Caller Experience
predictor?as tentative results in (Evanini et
al., 2008) confirm.
Figure 7: Increase of Caller Experience over re-
lease time.
? Efforts towards improvement of speech
recognition in spoken dialog applications
should be focused on increasing True Total
since this will directly influence Caller Expe-
rience.
References
K. Acomb, J. Bloom, K. Dayanidhi, P. Hunter,
P. Krogh, E. Levin, and R. Pieraccini. 2007. Techni-
cal Support Dialog Systems: Issues, Problems, and
Solutions. In Proc. of the HLT-NAACL, Rochester,
USA.
J. Boye and M. Wiren. 2007. Multi-Slot Semantics for
Natural-Language Call Routing Systems. In Proc.
of the HLT-NAACL, Rochester, USA.
K. Evanini, P. Hunter, J. Liscombe, D. Suendermann,
K. Dayanidhi, and R. Pieraccini:. 2008. Caller Ex-
perience: A Method for Evaluating Dialog Systems
and Its Automatic Prediction. In Proc. of the SLT,
Goa, India.
A. Gorin, G. Riccardi, and J. Wright. 1997. How May
I Help You? Speech Communication, 23(1/2).
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing Grammar-
Based and Robust Approaches to Speech Under-
standing: A Case Study. In Proc. of the Eurospeech,
Aalborg, Denmark.
J. Rodgers and W. Nicewander. 1988. Thirteen Ways
to Look at the Correlation Coefficient. The Ameri-
can Statistician, 42(1).
D. Suendermann, P. Hunter, and R. Pieraccini. 2008.
Call Classification with Hundreds of Classes and
Hundred Thousands of Training Utterances ... and
No Target Domain Data. In Proc. of the PIT, Kloster
Irsee, Germany.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2009. From
Rule-Based to Statistical Grammars: Continu-
ous Improvement of Large-Scale Spoken Dialog
Systems. In Proc. of the ICASSP, Taipei, Taiwan.
356
