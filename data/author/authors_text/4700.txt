Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 59?64,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Strategies for sustainable MT for Basque:  
incremental design, reusability, standardization and open-source 
 I. Alegria, X. Arregi, X. Artola, A. Diaz de Ilarraza, G. Labaka,  
M. Lersundi, A. Mayor, K. Sarasola 
Ixa taldea.  
University of the Basque Country. 
i.alegria@ehu.es 
 
 
 
Abstract 
We present some Language Technology 
applications that have proven to be effec-
tive tools to promote the use of Basque, a 
European less privileged language. We also 
present the strategy we have followed for 
almost twenty years to develop those appli-
cations as the top of an integrated environ-
ment of language resources, language 
foundations, language tools and other ap-
plications. When we have faced a difficult 
task such as Machine Translation to 
Basque, our strategy has worked well. We 
have had good results in a short time just 
reusing previous works for Basque, reusing 
other open-source tools, and developing 
just a few new modules in collaboration 
with other groups. In addition, new reus-
able tools and formats have been produced.  
1 Introduction and Basque Language 
Basque is a highly inflected minority language 
with free order of sentence constituents. Machine 
Translation for Basque is thus both, a real need and 
a test bed for our strategy to develop NLP tools for 
Basque.          
Basque is an isolate language, and little is 
known of its origins. It is likely that an early form 
of the Basque language was already present in 
Western Europe before the arrival of the Indo-
European languages. 
Basque is an agglutinative language, with a rich 
flexional morphology. In fact for nouns, for 
example, at least 360 word forms are possible for 
each lemma. Each of the declension cases such as 
absolutive, dative, associative? has four different 
suffixes to be added to the last word of the noun 
phrase. These four suffix variants correspond to 
undetermined, determined singular, determined 
plural and ?close? determined plural.  
Basque is also an ergative-absolutive language. 
The subject of an intransitive verb is in the 
absolutive case (which is unmarked), and the same 
case is used for the direct object of a transitive 
verb. The subject of the transitive verb (that is, the 
agent) is marked differently, with the ergative case 
(shown by the suffix -k). This also triggers main 
and auxiliary verbal agreement. 
The auxiliary verb, which accompanies most 
main verbs, agrees not only with the subject, but 
with the direct object and the indirect object, if 
present. Among European languages, this 
polypersonal system (multiple verb agreement) is 
only found in Basque, some Caucasian languages, 
and Hungarian. The ergative-absolutive alignment 
is rare among European languages, but not 
worldwide. 
Although in last centuries Basque suffered 
continuous regression it still remains alive. The 
region in which Basque is spoken is smaller than 
what is known as the Basque Country, and the 
distribution of Basque speakers is not 
homogeneous there. The main reasons of this 
regression (Amorrortu, 2002) are that Basque was 
not an official language, and that it was out of 
educational system, out of media and out of 
industrial environments. Besides, the fact of being 
six different dialects made the wide development 
of written Basque difficult.  
However, after 1980, some of those features 
changed and many citizens and some local 
59
governments promote recovering of Basque 
Language.  
Today, Basque holds co-official language status 
in the Basque regions of Spain: the whole 
autonomous community of the Basque Country 
and some parts of Navarre. Basque has no official 
standing in the Northern Basque Country.   
In the past, Basque was associated with lack of 
education, stigmatized as uneducated, rural, or 
holding low economic and power resources. There 
is not such an association today; Basque speakers 
do not differ from Spanish or French monolinguals 
in any of these characteristics.  
Standard Basque, called Batua (unified) in 
Basque, was defined by the Academy of Basque 
Language (Euskaltzaindia) in 1968. At present, its 
morphology is completely standardized, but the 
lexical standardization process is still underway. 
Now this is the language model taught in most 
schools and used on some media and official 
papers published in Basque.  
Basque speakers are about 700,000, about 25% 
of the total population of the Basque Country, but 
they are not evenly distributed. Still the use of 
Basque in industry and specially in Information 
and Communication Technology is not 
widespread. A language that seeks to survive in the 
modern information society has to be present also 
in such field and this requires language technology 
products. Basque, as other minority languages, has 
to make a great effort to face this challenge (Petek, 
2000; Williams et al, 2001).  
2 Strategy to develop Human Language 
Technology (HLT) in Basque 
IXA group is a research Group created in 1986 by 
5 university lecturers in the computer science fac-
ulty of the University of the Basque Country with 
the aim of laying foundations for research and de-
velopment of NLP software mainly for Basque. 
We wanted to face the challenge of adapting 
Basque to language technology. 
Twenty one years later, now IXA is a group 
composed of 28 computer scientists, 13 linguists 
and 2 research assistants. It works in cooperation 
with more than 7 companies from Basque Country 
and 5 from abroad; it has been involved in the birth 
of two new spin-off companies; and it has devel-
oped more than seven language technology prod-
ucts. 
In recent years, several private companies and 
technology centers in the Basque Country have 
begun to get interested and to invest in this area. At 
the same time, more agents have come to be aware 
of the fact that collaboration is essential to the de-
velopment of language technologies for minority 
languages. One of the fruits of this collaboration 
are HIZKING21 (2002-2005) and ANHITZ (2006-
2008) projects. Both projects were accepted by the 
Government of the Basque Country in a new 
strategical research line called ?Language Infoen-
gineering?. 
At the very beginning, twenty years ago, our 
first goal was just to create a Spanish-Basque 
translation system, but after some preliminary 
work we realized that instead of wasting our time 
in creating an ad hoc MT system with small accu-
racy, we had to invest our effort in creating basic 
tools such as a morphological analyzer/generator 
for Basque, that could later be used to build not 
only a more robust MT system but also other ap-
plications. 
This thought was the seed to design our strategy 
to make progress in the adaptation of Basque to 
Language Technology. Basque language had to 
face up scarcity of resources and tools that could 
make possible its development in Language Tech-
nology at a reasonable and competitive rate. 
We presented an open proposal for making pro-
gress in Human Language Technology (Aduriz et 
al., 1998). Anyway, the steps proposed did not cor-
respond exactly with those observed in the history 
of the processing of English, because the high ca-
pacity and computational power of new computers 
allowed facing problems in a different way.  
Our strategy may be described in two points: 
1) The need for standardization of resources to 
be useful in different researches, tools and applica-
tions 
2) The need for incremental design and devel-
opment of language foundations, tools, and appli-
cations in a parallel and coordinated way in order 
to get the best benefit from them. Language foun-
dations and research are essential to create any tool 
or application; but in the same way tools and ap-
plications will be very helpful in the research and 
improvement of language foundations. 
Following this strategy, our steps on standardi-
zation of resources led us to adopt TEI and XML 
standards and also to define a methodology for 
60
stand-off corpus tagging based on TEI, feature 
structures and XML (Artola et al, 2005). 
In the same way, taking as reference our experi-
ence in incremental design and development we 
proposed four phases as a general strategy for lan-
guage processing. These are the phases defined 
with the products to be developed in each of them. 
1. Initial phase: Foundations. Corpus I (collection 
of raw text with no tagging mark). Lexical da-
tabase I (the first version could be a list of 
lemmas and affixes). Machine-readable dic-
tionaries. Morphological description.  
2. Second phase: Basic tools and applications. 
Statistical tools for the treatment of corpora. 
Morphological analyzer/generator. Lemma-
tizer/tagger. Spelling checker and corrector (al-
though in morphologically simple languages a 
word list could be enough). Speech processing 
at word level. Corpus II (word-forms are 
tagged with their part of speech and lemma). 
Lexical database II (lexical support for the con-
struction of general applications, including part 
of speech and morphological information). 
3. Third phase: Advanced tools and applications. 
An environment for tool integration. Web 
search engine.  A traditional search machine 
that integrates lemmatization and language 
identification. Surface syntax. Corpus III (syn-
tactically tagged text). Grammar and style 
checkers. Structured versions of dictionaries 
(they allow enhanced functionality not avail-
able for printed or raw electronic versions). 
Lexical database III (the previous version is en-
riched with multiword lexical units. Integration 
of dictionaries in text editors). Lexical-
semantic knowledge base. Creation of a con-
cept taxonomy (e.g.: Wordnet). Word-sense 
disambiguation. Speech processing at sentence 
level. Basic Computer Aided Language Learn-
ing (CALL) systems 
4. Fourth phase: Multilingualism and general 
applications. Information extraction. Transla-
tion aids (integrated use of multiple on-line 
dictionaries, translation of noun phrases and 
simple sentences). Corpus IV (semantically 
tagged text after word-sense disambiguation). 
Dialog systems. Knowledge base on multilin-
gual lexico-semantic relations and its applica-
tions.  
We will complete this strategy with some sug-
gestions about what shouldn?t be done when work-
ing on the treatment of minority languages. a) Do 
not start developing applications if linguistic foun-
dations are not defined previously; we recommend 
following the above given sequence: foundations, 
tools and applications. b) When a new system has 
to be planned, do not create ad hoc lexical or syn-
tactic resources; you should design those resources 
in a way that they could be easily extended to full 
coverage and reusable by any other tool or applica-
tion. c) If you complete a new resource or tool, do 
not keep it to yourself; there are many researchers 
working on English, but only a few on each minor-
ity language; thus, the few results should be public 
and shared for research purposes, for it is desirable 
to avoid needless and costly repetition of work. 
3 Machine Translation for Basque 
After years working on basic resources and tools 
we decided it was time to face  the MT task (Hut-
chins and Somers, 1992). Our general strategy was 
more specifically for Machine Translation defined 
bearing in mind the following concepts:  
? reusability of previous resources, specially 
lexical resources and morphology of Basque 
? standardization and collaboration: using a 
more general framework in collaboration 
with other groups working in NLP 
? open-source: this means that anyone having 
the necessary computational and linguistic 
skills will be able to adapt or enhance it to 
produce a new MT system,  
Due to the real necessity for translation in our 
environment the involved languages would be 
Basque, Spanish and English. 
From the beginning we wanted to combine the 
two basic approaches for MT (rule-based and cor-
pus-based) in order to build a hybrid system, be-
cause it is generally agreed that there are not 
enough corpora for a good corpus-based system in 
minority languages like Basque.  
Data-driven Machine Translation (example-
based or statistical) is nowadays the most prevalent 
trend in Machine Translation research. Translation 
results obtained with this approach have already 
reached a high level of accuracy, especially when 
the target language is English. But these Data-
driven MT systems base their knowledge on 
aligned bilingual corpora, and the accuracy of their 
61
output depends heavily on the quality and the size 
of these corpora. Large and reliable bilingual cor-
pora are unavailable for many language pairs. 
3.1 The rule-based approach 
First, we present the main architecture and the pro-
posed standards of an open source MT engine, the 
first implementation of which translates from 
Spanish into Basque using the traditional transfer 
model and based on shallow and dependency pars-
ing. 
The design and the programs are independent 
from the languages, so the software can be used for 
other projects in MT. Depending on the languages 
included in the adaptation, it will be necessary to 
add, reorder and change some modules, but this 
will not be difficult because a unique XML format 
is used for the communication among all the mod-
ules. 
The project has been integrated in the OpenTrad 
initiative (www.opentrad.com), a government-
funded project shared among different universities 
and small companies, which also include MT en-
gines for translation among the main languages in 
Spain. The main objective of this initiative is the 
construction of an open, reusable and interoperable 
framework. 
In the OpenTrad project, two different but coor-
dinated designs have been carried out: 
? A shallow-transfer machine translation en-
gine for similar languages (Spanish, Catalan 
and Galician by the the time being). The 
MT architecture uses finite-state transducers 
for lexical processing, hidden Markov mod-
els for part-of-speech tagging, and chunking 
based on finite-state for structural transfer. 
It is named Apertium and it can be 
downloaded from apertium.sourceforge.net. 
(Armentano-Oller et al, 2004) 
? A deeper-transfer engine for the Spanish-
Basque pair. It is named Matxin (Alegria et 
al., 2007) and it is stored in 
matxin.sourceforge.net. It is an extension of 
previous work in our group. In order to re-
use resources in this Spanish-Basque system 
the analysis module for similar languages 
was not included in Matxin; another open 
source engine, FreeLing (Carreras et al, 
2004), was used here, of course, and its out-
put had to be converted to the proposed in-
terchange format. 
Some of the components (modules, data formats 
and compilers) from the first architecture in Open-
Trad were used in the second one. Indeed, an im-
portant additional goal of this work was testing 
which modules from the first architecture could be 
integrated in deeper-transfer architectures for more 
difficult language pairs. 
The transfer module is also based on three main 
objects in the translation process: words or nodes, 
chunks or phrases, and sentences.  
? First, lexical transfer is carried out using a 
bilingual dictionary compiled into a finite-
state transducer. We use the XML specifica-
tion of Apertium engine.  
? Then, structural transfer at the sentence 
level is applied, and some information is 
transferred from some chunks to others, and 
some chunks may disappear. Grammars 
based on regular expressions are used to 
specify these changes. For example, in the 
Spanish-Basque transfer, the person and 
number information of the object and the 
type of subordination are imported from 
other chunks to the chunk corresponding to 
the verb chain. 
? Finally the structural transfer at the chunk 
level is carried out. This process can be 
quite simple (e.g. noun chains between 
Spanish and Basque) or more complex (e.g. 
verb chains between these same languages). 
The XML file coming from the transfer module 
is passed on the generation module. 
? In the first step, syntactic generation is per-
formed in order to decide the order of 
chunks in the sentence and the order of 
words in the chunks. Several grammars are 
used for this purpose.  
? Morphological generation is carried out in 
the last step. In the generation of Basque, 
the main inflection is added to the last word 
in the phrase (in Basque: the declension 
case, the article and other features are added 
to the whole noun phrase at the end of the 
last word), but in verb chains other words 
need morphological generation. A previous 
morphological analyzer/generator for 
Basque (Alegria et al, 1996) has been 
adapted and transformed to the format used 
in Apertium. 
The results for the Spanish/Basque system using 
FreeLing and Matxin are promising. The quantita-
62
tive evaluation uses the open source evaluation 
tool IQMT and figures are given using Bleu and 
NIST measures (Gim?nez et al, 2005). An user 
based evaluation has been carried out too. 
3.2 The corpus-based approach 
The corpus-based approach has been carried out in 
collaboration with the National Center for Lan-
guage Technology in Dublin.  
The system exploits both EBMT and SMT tech-
niques to extract a dataset of aligned chunks. We 
conducted Basque to English and Spanish to 
Basque translation experiments, evaluated on a 
large corpus (270, 000 sentence pairs).  
Some tools have been reused for this purpose: 
? GIZA++: for word/morpheme alignment we 
used the GIZA++ statistical word alignment 
toolkit, and following the ?refined? method 
of (Och and Ney, 2003), extracted a set of 
high-quality word/ morpheme alignments 
from the original unidirectional alignment 
sets. These along with the extracted chunk 
alignments were passed to the translation 
decoder.                                         
? Pharaoh/Moses decoder: the decoder is also 
a hybrid system which integrates EBMT 
and SMT. It is capable of retrieving already 
translated sentences and also provides a 
wrapper around the PHARAOH SMT de-
coder (Koehn, 2004). 
? MaTrEx: the MATREX (Machine Transla-
tion using Examples) system used in our 
experiments is a data-driven MT engine, 
built following an extremely modular de-
sign. It consists of a number of extensible 
and re-implementable modules (Way and 
Gough, 2005). 
   For this engine, we reuse a toolkit to chunk the 
Basque sentences. After this processing stage, a 
sentence is treated as a sequence of morphemes, in 
which chunk boundaries are clearly visible. Mor-
phemes denoting morphosyntactic features are re-
placed by conventional symbolic strings. After 
some adaptation, the chunks obtained in this man-
ner are actually very comparable to the English 
chunks obtained with the marker-based chunker. 
The experimental results have shown that our 
system significantly outperforms state-of-the-art 
approaches according to several common auto-
matic evaluation metrics: WER, Bleu and PER 
(Stroppa et al, 2006; Labaka et al, 2007). 
4 Conclusions 
A language that seeks to survive in the modern 
information society requires language technology 
products. "Minority" languages have to do a great 
effort to face this challenge. The Ixa group has 
been working since 1986 on adapting Basque to 
language technology, having developed several 
applications that are effective tools to promote the 
use of Basque. Now we are planning to define the 
BLARK for Basque (Krauwer, 2003).  
From our experience, we defend that research 
and development for a minority language should to 
be faced following these points: high standardiza-
tion,  reusing language foundations, tools, and ap-
plications, and their incremental design and devel-
opment. We know that any HLT project related to 
a less privileged language should follow those 
guidelines, but from our experience we know that 
in most cases they do not. We think that if Basque 
is now in an good position in HLT is because those 
guidelines have been applied even  when it was 
easier to define "toy" resources and tools useful to 
get good short term academic results, but not reus-
able in future developments.  
This strategy has been completely useful when 
we have created MT systems for Basque. Reusing 
previous works for Basque (that were defined fol-
lowing XML and TEI standards) and reusing other 
open-source tools have been the key to get satisfac-
tory results in a short time.  
Two results produced in the MT track are pub-
licly available:  
? matxin.sourceforge.net for the free code for 
the Spanish-Basque RBMT system 
? www.opentrad.org for the on-line demo  
Acknowledgments 
This work has been partially funded by the Spanish 
Ministry of Education and Science (OpenMT: 
Open Source Machine Translation using hybrid 
methods,TIN2006-15307-C03-01) and the Local 
Government of the Basque Country (AnHITZ 
2006: Language Technologies for Multingual In-
teraction in Intelligent Environments., IE06-185). 
Andy Way, Declan Groves and Nicolas Stroppa 
from National Centre for Language Technology in 
Dublin are kindly acknowledged for providing 
their expertise on the Matrex system and the 
evaluation of the output. 
63
References 
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, O. Ansa, 
X. Arregi, J. Arriola, X. Artola, A. D?az de Ilarraza, 
N. Ezeiza, K.Gojenola, M. Maritxalar, M. Oronoz, K. 
Sarasola, A. Soroa, R. Urizar. 1998. A framework for 
the automatic processing of Basque. Proceedings of 
Workshop on Lexical Resources for Minority Lan-
guages.  
I. Alegria, X. Artola, K. Sarasola. 1996.Automatic mor-
phological analysis of Basque. Literary & Linguistic 
Computing Vol. 11, No. 4, 193-203. Oxford Univer-
sity Press. Oxford. 1996. 
I. Alegria, A. D?az de Ilarraza, G. Labaka, M Lersundi, 
A. Mayor, K. Sarasola.  2007. Transfer-based MT 
from Spanish into Basque: reusability, standardiza-
tion and open source. LNCS 4394. 374-384. Cicling 
2007.  
E. Amorrortu. 2002. Bilingual Education in the Basque 
Country: Achievements and Challenges after Four 
Decades of Acquisition Planning. Journal of Iberian 
and Latin American Literary and Cultural Stud-
ies.Volume 2 Number 2 (2002) 
C. Armentano-Oller, A. Corb?-Bellot, M. L. Forcada, 
M. Ginest?-Rosell, B. Bonev, S. Ortiz-Rojas, J. A. 
P?rez-Ortiz, G. Ram?rez-S?nchez, F. S?nchez-
Mart?nez, 2005. An open-source shallow-transfer 
machine translation toolbox: consequences of its re-
lease and availability. Proceedings of OSMaTran: 
Open-Source Machine Translation workshop, MT 
Summit X. 
X. Artola, A. D?az de Ilarraza, N. Ezeiza, K. Gojenola, 
G. Labaka, A. Sologaistoa, A. Soroa.  2005. A 
framework for representing and managing linguistic 
annotations based on typed feature structures. Proc. 
of RANLP 2005. 
X. Carreras,, I. Chao, L. Padr? and M. Padr?. 2004. 
FreeLing: An open source Suite of Language Ana-
lyzers, in  Proceedings of the 4th International Con-
ference on Language Resources and Evaluation 
(LREC'04).  
J. Gim?nez, E. Amig?, C. Hori. 2005. Machine 
Translation Evaluation Inside QARLA. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Technology (IWSLT'05) 
W. Hutchins and H. Somers. 1992. An Introduction to 
Machine Translation. Academic Press. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els.  In Proceedings of AMTA-04, pages 115?124, 
Washington, District of Columbia. 
S. Krauwer. 2003. The Basic Language Resource Kit 
(BLARK) as the First Milestone for the Language 
Resources Roadmap. Proc. of the International 
Workshop  Speech and Computer. Moscow, Russia. 
G. Labaka, N. Stroppa, A. Way, K. Sarasola  2007 
Comparing Rule-Based and Data-Driven Approaches 
to Spanish-to-Basque Machine Translation Proc. of 
MT-Summit XI, Copenhagen 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1): 19?51. 
B. Petek. 2000. Funding for research into human lan-
guage technologies for less prevalent languages, Sec-
ond International Conference on Language Re-
sources and Evaluation (LREC 2000). Athens, 
Greece. 
N. Stroppa, D. Groves, A. Way, K. Sarasola K. 2006. 
Example-Based Machine Translation of the Basque 
Language. AMTA. 7th conference of the Association 
for Machine Translation in the Americas.. 
A. Way and N. Gough. 2005. Comparing Example-
Based and Statistical Machine Translation. Natural 
Language Engineering, 11(3):295?309. 
B. Williams, K. Sarasola, D. ??Cr?inin, B. Petek. 2001. 
Speech and Language Technology for Minority Lan-
guages. Proceedings of Eurospeech 2001 
 
 
64
A Multilingual Approach to Disambiguate Prepositions  
and Case Suffixes 
Eneko Agirre, Mikel Lersundi, David Martinez 
 
IxA NLP group 
University of the Basque Country 
649 pk. - 20.080 Donostia (Spain) 
{eneko, jialeaym, jibmaird}@si.ehu.es 
 
Abstract 
This paper presents preliminary 
experiments in the use of translation 
equivalences to disambiguate 
prepositions or case suffixes. The core 
of the method is to find translations of 
the occurrence of the target preposition 
or case suffix, and assign the 
intersection of their set of 
interpretations. Given a table with 
prepositions and their possible 
interpretations, the method is fully 
automatic. We have tested this method 
on the occurrences of the Basque 
instrumental case -z in the definitions of 
a Basque dictionary, looking for the 
translations in the definitions from 3 
Spanish and 3 English dictionaries. The 
results have been that we are able to 
disambiguate with 94.5% accuracy 
2.3% of those occurrences (up to 91). 
The ambiguity is reduced from 7 
readings down to 3.1. The results are 
very encouraging given the simple 
techniques used, and show great 
potential for improvement. 
1 Introduction 
This paper presents some preliminary experiments 
in the use of translation equivalences to 
disambiguate the interpretations of case suffixes in 
Basque. Basque is an agglutinative language, and 
its case suffixes are more or less equivalent to 
prepositions, but are also used to mark the subject 
and objects of verbs. The method is general, and 
could be as easily applied to prepositions in any 
other language. The core of the method is to find a 
preposition in the translation of an occurrence of 
the target case suffix, and select the 
interpretation(s) in the intersection of both as the 
valid interpretation(s). At this point, we have not 
used additional sources for the disambiguation, 
e.g. governing verbs, nouns, etc., but they could 
complement the technique here presented. 
In this particular experiment, the method was 
tested on the definitions of a Basque monolingual 
dictionary, using the -z instrumental as the target 
case suffix. The main reason is that we are in the 
process of building a Lexical Knowledge Base out 
of dictionary definitions, and the disambiguation 
of case suffixes and other semantic dependencies 
is of great interest. 
The method searches for the respective 
definitions in English and Spanish monolingual 
dictionaries and tries to find a preposition that is 
the translation of the target case suffix. Once the 
preposition is found, the intersection of the set of 
interpretations of both the source case suffix and 
the translated preposition is taken, and the 
outcome is stored. 
The resources needed to perform this task are 
the following: lemmatizers, bilingual dictionaries 
and monolingual dictionaries, as well as a table of 
possible interpretations of prepositions and case 
suffixes. In our case, we have used Basque, 
English and Spanish lemmatizers, Basque/English 
and Basque/Spanish bilingual dictionaries, a target 
Basque monolingual dictionary, 3 Spanish and 3 
English monolingual dictionaries.  
The method is fully automatic; the Spanish and 
English monolingual dictionaries are accessed 
from the Internet, and the rest are local, installed 
in our machines. The manual work has been to 
build the table with possible interpretations of the 
prepositions and case suffixes. 
                       July 2002, pp. 1-8.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
The paper is structured as follows. Section 2 
presents the method for disambiguation in detail. 
Section 3 introduces the interpretations for the 
case suffix and the prepositions. The results are 
shown in Section 4, which are further discussed in 
Section 5. Finally, section 6 presents the 
conclusions and future work.  
2 Method for disambiguation 
The goal of the method is to disambiguate 
between the possible interpretations of a case 
suffix appearing in any text. We have taken as the 
target text the definitions from a monolingual 
Basque dictionary Euskal Hiztegia, EH in short 
(Sarasola, 1996). The method consists on five 
steps:  
? Extraction of the definitions in EH where the 
target case suffix occurs.  
? Search of on-line Spanish and English 
dictionaries to obtain the translation 
equivalent of the definitions.  
? Extraction of the target preposition from the 
translation definitions.  
? Disambiguation based on the intersection of 
the interpretations of case suffix and 
prepositions.  
We will explain each step in turn. 
2.1 Extraction of relations from EH 
Given a case suffix, in this step we will search the 
EH dictionary for occurrences of the case suffix. 
We first lemmatize and perform morphological 
analysis of the definitions (Aduriz et. al, 1996). 
The definitions that contain the target case suffix 
in a morphological analysis are extracted, storing 
the following information: the Basque dictionary 
entry of the definition, the lemma that has the case 
suffix, the case suffix, and the following lemma.  
Below we can see a sample definition, its 
lemmatized version, and the two triples extracted 
from this definition. The occurrences of the 
instrumental -z are shown in bold. 
 
Ildo iz. A1 Goldeaz lurra irauliz 
egiten den irekidura luzea1  
                                                     
1 The literal translation of the definition is the 
following : furrow, a long trench produced turning 
 
/<@@lema ildo>/<ID>/ 
/<@@Adiera_string A1.>/<ID>/ 
/<@@Kategoria iz. >/<ID>/ 
"<Goldeaz>" 
  "golde"  IZE ARR DEK INS NUMS MUGM  
"<lurra>" 
  "lur"  IZE ARR DEK ABS NUMS MUGM  
"<irauliz>" 
  "irauli"  ADI SIN AMM PART DEK INS MG  
"<egiten>" 
  "egin"  ADI SIN AMM ADOIN ASP EZBU  
"<den>" 
  "izan"  ADL A1 NOR NR_HU ERL MEN ERLT  
"<irekidura>" 
  "irekidura"  IZE ARR DEK ABS MG  
"<luzea>" 
  "luze"  ADJ IZO DEK ABS NUMS MUGM  
"<$.>" 
  PUNT_PUNT 
 
golde#INS#lur2 
irauli#INS#egin 
 
Extracting lemma-suffix-lemma triples in this 
simple way leads to some errors (cf. section 5.1). 
For instance, the first triple should rather be the 
dependency golde#INS#irauli (plow#with#turn, to 
be read in reverse order). We will see that even in 
this case we will be able to obtain correct 
translations and disambiguate the preposition 
correctly. Nevertheless, in the future we plan to 
use a syntactic parser to identify better the lemmas 
that are related by the case suffix.  
2.2 Search for Spanish/English 
translations 
After we have a list of entries in the Basque 
dictionary that contain the lemma-suffix-lemma 
triple, we search for their equivalent definitions in 
Spanish and English. We first look up the entry in 
the bilingual dictionary, and then retrieve the 
                                                                                  
over the ground with a plow.  
2 The translation of the first triple is plow#with#ground, 
to be read on reverse. The translation of the second is 
turn#NULL#produce, to be also read on reverse. In this 
second triple the instrumental case suffix is not 
translated explicitly by a preposition, but by a syntactic 
construct. 
definitions for each of the possible translations 
from the monolingual dictionaries. 
We use two bilingual and 6 monolingual 
Machine Readable Dictionaries: Morris 
Basque/English dictionary (Morris, 1998) Elhuyar 
Basque/Spanish dictionary (Elhuyar, 1996); 
English monolingual on-line dictionaries are: 
Cambridge (online), Heritage (online), and 
Wordsmyth (online); and Spanish monolingual 
on-line dictionaries are: Colmex (online), Rae 
(online), and Vox (online). The Basque dictionary 
and the bilingual dictionaries are stored in a local 
server, while the monolingual dictionaries are 
accessed from the Internet using a wrapper. 
The incomplete list of the translation of ildo 
(furrow in English, surco in Spanish) is shown 
below. Note that we got two different definitions 
for surco, coming from different Spanish 
dictionaries. 
 
furrow#A long , narrow , shallow 
trench made in the ground by a 
plow  
 
surco#Excavaci?n alargada , angosta y 
poco profunda que se hace 
paralelamente en la tierra con el 
arado , para sembrarla despu?s  
 
surco#Hendedura que se hace en la 
tierra con el arado  
2.3 Extraction of Spanish/English 
equivalent relations 
Given a list of definitions in Spanish and English, 
we search in the definition the translation of the 
Basque triple found in step 2.1, that is, we look for 
a triple of consecutive words where the first word 
is the translation of the last word in the Basque 
triple, the second word is a preposition (which 
corresponds to the Basque suffix) and the third 
word is the translation of the first word in the 
Basque triple. Between the preposition and the last 
word in the triple we allow for the presence of a 
determiner or an adjective in the text. More 
complex patterns could be allowed, up to full 
syntactic analyses, but at this point we follow this 
simple scheme. 
Below we can find the triples for 
golde#INS#lur, obtained from the three definitions 
above. One triple is obtained twice from two 
different definitions. 
 
furrow#ground#by#plow 
surco#tierra#con#arado 
surco#tierra#con#arado 
 
Definitions that do not have a matching triple 
are discarded, leaving Basque triples without 
matching triple ambiguous. For instance we could 
not find triples for irauli#INS#egin(cf. example in 
section 2.1). The instrumental suffix is sometimes 
translated without prepositions (in this case ?? 
made turning ??). 
Looking up the bilingual dictionaries for 
translation requires lemmatization and Part of 
Speech tagging. For English we use the TnT PoS 
tagger (Brants, 2000) and WordNet for 
lemmatization (Miller et al, 1990). For Spanish 
we use (Atserias et al, 1998).  
2.4 Disambiguation 
For each Basque case suffix, Spanish preposition 
and English preposition we have a list of 
interpretations (cf. Table 1). We assign the 
interpretations of the preposition to each 
Spanish/English triple. The intersection of all the 
interpretations is assigned to it. 
Continuing with out example, we can see that 
the intersection between the interpretations of the 
English by preposition (three interpretations) and 
the interpretations of the Spanish con preposition 
(four interpretations) are manner and instrument. 
Therefore, we can say that the Basque 
instrumental case interpretation in this case will 
be manner or instrument. 
 
furrow#ground#by a#plow# 
manner instrument during-time 
surco#tierra#con el#arado# 
manner instrument cause containing 
 
golde#INS#lur#instrument manner 
3 Interpretations for the 
instrumental case suffix and 
equivalent prepositions 
The method explained in the previous section is 
fully automatic, and it only requires the list of 
interpretations for each case suffix and 
preposition. In this work, we want to evaluate if 
the overall approach is feasible, so we selected 
Basque as the target language and a single case 
suffix, -z the instrumental case. Table 1 shows the 
list of possible interpretations and Table 2 and 3 
examples for each interpretation. 
The sources for the interpretations of the 
instrumental case have been a grammar of Basque 
(Euskaltzaindia, 1985) and a bilingual dictionary 
(Elhuyar, 1996). Possible interpretations for 
Spanish and English prepositions have been taken 
from an English dictionary (Cambridge, online), a 
Spanish dictionary (Vox, online) and a Spanish 
grammar (Bosque & Demonte, 1999).  
For this work we have taken a descriptive 
approach, but other more theoretically committed 
approaches are also possible. The overall method 
is independent of the set of interpretations, as it 
only needs a table of possible interpretations in the 
style of Table 1. Section 5.4 further discusses 
other alternatives. 
In order to disambiguate the occurrences of the 
instrumental case suffix we have taken the 
Spanish and English translations for this case 
suffix. The list of possible translations is 
preliminary and covers what we found necessary 
to make this experiment. Table 1 shows the list of 
prepositions and interpretations for Spanish and 
English. Examples of the interpretations can be 
found in Table 2. The Spanish preposition de had 
the same interpretations as the instrumental case 
suffix (cf. Table1), so it was discarded. 
4 Results 
The instrumental case occurs in 4,004 different 
definitions in the EH dictionary. The algorithm in 
Section 2 was applied to all these definitions, 
yielding a result for 125 triples, 3.1% of the total. 
The triples for which we had an answer were 
tagged by hand independently, i.e. not consulting 
the results output by the algorithm. The hand-
tagged set constitutes what we call the gold 
standard. 
A single linguist made the tagging, consulting 
other teammates when in doubt. Apart from 
marking the interpretation, there were some other 
special cases. 
1. In some of the examples, the instrumental 
case was part of a more complex scheme, and 
was tagged accordingly: 
? Part of a postposition (XPOST), e.g. -en 
bidez (by means of) or -en ordez (instead 
of). 
? Part of a conjunction (XLOK), e.g. batez 
ere (specially). 
? Part of a compounded suffix ?zko 
(XZKO), which results from the 
aggregation of the instrumental ?z  with 
the location genitive -ko. 
2. There were three errors in the lemmatization 
process (XLEM), due to lexicalized items, e.g. 
gizonezko (meaning male person).  
3. Finally, the relation in the definition was 
sometimes wrongly retrieved, e.g.  
? The triple would contain the determiner or 
an adjective instead of the dependencies. 
We thought that the algorithm would be 
able to work well even with those cases, 
so we decided to keep them. 
? The triple contains a conjunction (X): 
these were tagged as incorrect. 
Table 4 shows the amount of such cases, 
alongside the frequency of each interpretation. 
The most frequent interpretation is instrument. In 
seven examples, the linguist decided to keep two 
interpretations: instrument and manner. In a single 
example, the linguist was unable to select an 
interpretation, so this example was discarded. 
The output of the algorithm was compared 
with the gold standard, yielding the accuracy 
figures in Table 5. An output was considered 
correct if it yielded at least one interpretation in 
common with the gold standard. The accuracy is 
given for each dictionary in isolation, or merging 
all the results (as mentioned in section 2, when 
two dictionaries propose interpretations for the 
same triple, their intersection is taken). The 
remaining ambiguity is 3.1 overall. 
  Basque English Spanish 
 -z (ins.) of by with in de con a en 
theme x x   x x  x  
during-time x x x   x    
instrument x  x x x x x  x 
manner x  x  x x x x x 
cause x x  x x x x   
containing x x  x x x x   
matter x x    x    
Table 1: interpretations for the instrumental case in Basque and its equivalents in English and Spanish. 
 
 Basque English 
theme Seguru nago horretaz 
Matematikaz asko daki 
I?m sure of that 
He?s an expert in maths 
during-time Arratsaldez lasai egon nahi dut 
Gauez egin dut 
I like to relax of an evening 
I did it by night 
instrument Autobusez etorri naiz 
Belarra segaz moztu 
Euskaraz hitz egin 
I have come by bus 
To cut grass with a scythe 
To speak in Basque 
manner Animali baten hestea betez egindako haragia
 
Ahots ozen batez 
A meat preparation made by filling an 
animal intestine 
In a loud voice 
cause Haren aitzakiez nekatuta nago  
Beldurrez zurbildu 
Kanpoan lan egitea baztertu zuenez, lan-
aukera ederra galdu zuen 
Sick of his excuses 
To turn white of fear 
In refusing to work abroad, she missed an 
excellent job opportunity 
containing Edalontzia ardoz beteta dago 
Txapelaz dagoen gizona 
Ilez estalia 
The glass is full of wine 
The man with the beret on 
Cover in hair 
matter Armairua egurrez egina dago The wardrobe is made of wood 
Table 2: examples in Basque and English for the set of possible interpretations. 
 
 Basque Spanish 
theme Mariaz aritu dira 
Honetaz ziur naiz 
Han mencionado a  Maria 
Estoy seguro de esto 
during-time Gauez egin dut Lo he hecho de noche 
instrument Belarra segaz moztu 
Euskaraz hitz egin 
Hiria harresiz inguratu dute 
Cortar la hierba con la guada?a 
Hablar en vasco 
Han cubierto la ciudad de murallas 
manner Oinez etorri zen 
Ahots ozen batez 
Bere familiaren laguntzaz erosi zuen 
Berdez margotzen ari dira 
Vino a pie 
En voz alta 
Lo compr? con la ayuda de su familia 
Lo estan pintando de verde 
cause Beldurrez zurbildu 
Maitasunez hil 
Con el miedo me qued? p?lido 
Morir de amor 
containing Edalontzia ardoz beteta dago 
Txapelaz dagoen gizona ikusi dut 
El baso esta lleno de vino 
He visto a un hombre con boina 
matter Armairua egurrez egina dago El armario est? hecho de madera 
Table 3: examples in Basque and Spanish for the set of possible interpretations. 
Table 4 also shows the most frequent baseline 
(MF), constructed as follows: for each occurrence 
of the suffix, the three most frequent 
interpretations are chosen. The accuracy of this 
baseline is practically equal to that of the 
algorithm. Note that the frequency is computed on 
the same sample where it is applied, yielding 
better results than it should. 
5 Discussion 
The obtained results show a very good accuracy, 
leaving a remaining ambiguity of 3.1 results per 
example. This means that we were able to discard 
an average of 4 readings for each of the examples, 
introducing only 5.5% of error. The results are 
practically equal to the most frequent baseline, 
which is usually hard to beat using knowledge-
based techniques. 
Coverage of the method is very low, only 
2.3%, but this was not an issue for us, as we plan 
to couple this method with other Machine 
Learning techniques in a bootstrapping 
framework. Nevertheless, we are still interested in 
increasing the coverage, in order to obtain more 
training data. 
Next, we will analyze more in depth the causes 
of the low coverage, the sources of the errors and 
ambiguity and the interpretations of case suffixes 
and prepositions. 
5.1 Sources of low coverage 
As soon as we started devising this method, it was 
clear to us that the coverage will be rather low. 
The main reason is that different dictionaries tend 
to give different details in their definitions, or use 
differing paraphrases. This fact is intrinsic to our 
method, and accounts for the large majority of 
missing answers. 
On the other hand, the simple method used to 
find triples means that a change in the order of the 
complements will cause our method to fail 
looking for a translation triple. Syntactic analysis, 
even shallow parsing methods, will help increase 
the coverage. 
Another source of discarded triples are the 
cases where the suffix is not translated by a 
preposition, e.g. the relation is carried out by a 
subject or direct object. When syntactic analysis is 
performed, we
interpretations o
5.2 Sources
Only five errors
were caused 
especially whe
determiner inste
- xixta/pric
needle 
- luma/feed
a submarine
There errors
parser. Other 
# interpretation 
   8 XPOST 
1 XLOK 
12 XZKO 
   3 XLEM 
   9 X 
1 No interpretation 
34 Total discarded 
  37 instrument 
  35 containing 
   7 instrument manner 
6 manner 
5 theme 
   1 cause 
0 matter 
0 during-time 
Table 4: fre
 
Dictionary 
cambridge 
Am. heritage
wordsmith 
Colmex 
vox_ya 
Rae 
overall  
MF baseline 
Table 5: result  
combination fo also plan to incorporate the 
f the other syntactic relations. 
 of error  
 we made by the algorithm, which 
by the wrong triple pairings, 
n the Basque triple contained a 
ad of the related word. Examples: 
k: punta batez osatua/made by a 
le: odi batez osatua/wake made by 
 
 could be avoided using a syntactic 
wrong pairings were caused by 
91 Total kept 
quency of tags in gold standard. 
total correct accur. ambig.
16 15 0.938 4.0
34 32 0.941 3.2
26 26 1.000 3.7
10 9 0.900 2.6
7 7 1.000 2.8
26 25 0.962 2.8
91 86 0.945 3.1
91 85 0.934 3.0
s for each of the dictionaries, overall
r all and the most frequent baseline. 
errors in the English PoS tagger, or chance made 
the algorithm find an unrelated definition.  
5.3 Remaining ambiguity 
The amount of readings left by our method in this 
experiment is rather high, around 3.1 readings 
compared to 7 possible readings for the 
instrumental. This is a strong reduction but we 
would like to make it even smaller. 
We plan to study which is the source of the 
residual ambiguity. Alternative sets of 
interpretations (cf. Section 5.4) with coarser 
grained differences and smaller ambiguity, could 
yield better results. Another alternative is to 
explore more infrequent translations of the case 
suffixes, which might yield a narrower overlap. 
This is the case for the instrumental case suffix 
being translated with from, up, etc. 
5.4 Interpretations of case suffixes and 
prepositions 
Different authors give differing interpretations for 
prepositions. It has been our choice to take a 
descriptive list of possible interpretations from a 
set of sources, mainly dictionaries and grammar 
books. 
This work covers only the instrumental case 
suffix and its translations to English and Spanish. 
If tables for all case suffixes and prepositions were 
built, the method could be applied to all case 
suffixes and prepositions, yielding disambiguated 
relations in all three languages. 
More theoretically committed lists of 
interpretations (Dorr et al, 1998; Civit et al, 
2000; Sowa, 2000) should also be considered, but 
unfortunately we have not found a full account for 
all prepositions. If such a full table of 
interpretations existed, it could be very easy to 
apply our method, and obtain the outcome in 
terms of these other interpretations. 
6 Conclusion and further work 
This paper presents preliminary experiments in the 
use of translation equivalences to disambiguate 
prepositions or case suffixes. The core of the 
method is to find translations of the occurrence of 
the target preposition or case suffix, and assign the 
intersection of their set of interpretations. The 
method is fully automatic, given a table with 
prepositions and their possible interpretations.  
We have tested this method on the occurrences 
of the Basque instrumental case -z in the 
definitions of a Basque dictionary. We have 
searched the translations in the definitions from 3 
Spanish and 3 English dictionaries.  
The results have been that we are able to 
disambiguate with 94.5% accuracy 2.3% of those 
occurrences (up to 91). The ambiguity is reduced 
from 7 readings down to 3.1. We think that these 
are very good results, especially seeing that there 
is room for improvement.  
More specifically, we plan to apply surface 
syntactic analysis to better extract the dependency 
relations, which is the main source of errors. We 
would like to study other inventories of 
preposition interpretations, both in order to have 
better theoretical foundations as well as to 
investigate whether coarser grained distinctions 
would lead to a reduction in the ambiguity.  
In the future, we plan to explore the possibility 
to feed a Machine Learning algorithm with the 
automatically disambiguated examples, in order to 
construct a full-fledged disambiguation algorithm 
following a bootstrapping approach. On the other 
hand, we would like to apply the method to the set 
of all prepositions and case suffixes, and beyond 
that to all syntactic dependencies. The results will 
be directly loaded in a Lexical Knowledge Base 
extracted from the Basque dictionary (Ansa et al, 
in prep.). 
We also plan to explore whether this method 
can be applied to free running text, removing the 
constraint that the translations have to be 
definitions of the equivalent word. 
Finally, this technique could be coupled with 
techniques that make use of the semantic types of 
the words in the context. 
Overall, we found the results are very 
encouraging given the simple techniques used, 
and we think that it shows great potential for 
improvement and interesting avenues for research. 
Acknowledgments 
Mikel Lersundi and David Martinez were 
supported by Basque Government grants AE-
BFI:98.217 and AE-BFI:01.2485. This work was 
partially funded by the MCYT HERMES project 
(TIC-2000-0335) and the EC MEANING project 
(IST-2001-34460). 
References 
Aduriz I., Aldezabal I., Alegria I., Artola X., 
Ezeiza N., Urizar R., 1996, "EUSLEM: A 
Lemmatiser / Tagger for Basque" Proc. Of 
EURALEX'96, G?teborg (Sweden) Part 1, 17-26. 
Ansa O., Arregi X., Lersundi M., ?A 
Conceptual Schema for a Basque Lexical-
Semantic Framework? (in preparation) 
Bosque, I., Demonte, V., 1999, Gramatica 
descriptiva de la lengua Espa?ola, Espasa, 
Madrid. 
Brants, T. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth 
Applied Natural Language Processing 
Conference, Seattle, WA. 
Cambridge, online. Cambridge 
International Dictionary of English 
http://dictionary.cambridge.org/ 
Civit, M., Castell?n, I., Mart?, M.A. and Taul?, 
M., 2000,  ?LEXPIR: a verb lexicon for Spanish? 
Cuadernos de Filolog?a Inglesa, Vol. 9.1. Corpus-
based Research in English Language and 
Linguistics, University of Granada. 
Colmex, online. Diccionario del espa?ol usual 
en M?xico (Colmex) http://mezcal.colmex.mx 
(also accessible from 
http://www.foreignword.com 
Dorr, Bonnie J., Nizar Habash, and David 
Traum, 1998, ?A Thematic Hierarchy for Efficient 
Generation from Lexical-Conceptual Structure,? 
in Proceedings of the Third Conference of the 
Association for MT in the America's, Langhorne, 
PA, pp. 333--343 
Elhuyar, 1996, Elhuyar Hiztegia, Elhuyar K.E., 
Usurbil. 
Euskaltzaindia, 1985, Euskal Gramatika Lehen 
Urratsak-I (EGLU-I), Euskaltzaindia, Bilbo. 
Heritage, online. The American Heritage? 
Dictionary of the English Language. 
http://www.bartleby.com/61 
J. Atserias, J. Carmona, I. Castellon, S. 
Cervell, M. Civit, L. Marquez, M.A. Marti, L. 
Padro, R.Placer, H. Rodriguez, M. Taule & J. 
Turmo ?Morphosyntactic Analysis and Parsing of 
Unrestricted Spanish Text? First International 
Conference on Language Resources and 
Evaluation (LREC'98). Granada, Spain, 1998. 
Miller, G. A., R. Beckwith, C. Fellbaum, D. 
Gross, and K. Miller. 1990. Five Papers on 
WordNet. Special Issue of International Journal of 
Lexicography, 3(4). 
Morris M., 1998, Morris Student dictionary, 
Klaudio Harluxet Fundazioa, Donostia. 
Rae, online. Diccionario de la Real Academia 
de la Lengua http://buscon.rae.es/drae/drae.htm 
Sarasola, I., 1996, Euskal Hiztegia, 
Gipuzkoako Kutxa, Donostia. 
John F. Sowa, 2000, Knowledge 
Representation: Logical, Philosophical, and 
Computational Foundations, Brooks Cole 
Publishing Co., Pacific Grove, CA 
John F. Sowa, ed. (1992) Knowledge-Based 
Systems, Special Issue on Conceptual Graphs, vol. 
5, no. 3, September 1992 
Vox, online. Diccionario General de la lengua 
espa?ola VOX http://www.vox.es/consultar.html 
Wordsmyth, online. The Wordsmyth 
Educational Dictionary-Thesaurus 
http://www.wordsmyth.net 
The Basque lexical-sample task  
Eneko Agirre, Itziar Aldabe, Mikel Lersundi, David Martinez, Eli Pociello, Larraitz Uria(*) 
IxA NLP group, Basque Country University  
649 pk. 20.080 Donostia, Spain 
eneko@si.ehu.es 
 
Abstract 
In this paper we describe the Senseval 3 
Basque lexical sample task. The task 
comprised 40 words (15 nouns, 15 verbs and 
10 adjectives) selected from the Basque 
WordNet. 10 of the words were chosen in 
coordination with other lexical-sample tasks. 
The examples were taken from newspapers, an 
in-house balanced corpus and Internet texts. 
We additionally included a large set of 
untagged examples, and a lemmatised version 
of the data including lemma, PoS and case 
information. The method used to hand-tag the 
examples produced an inter-tagger agreement 
of 78.2% before arbitration. The eight 
competing systems attained results well above 
the most frequent baseline and the best system 
from Swarthmore College scored 70.4% 
recall. 
1 Introduction 
This paper reviews the Basque lexical-sample task 
organized for Senseval 3. Each participant was 
provided with a relatively small set of labelled 
examples (2/3 of 75+15*senses+7*multiwords) 
and a comparatively large set of unlabelled 
examples (roughly ten times more when possible) 
for around 40 words. The larger number of 
unlabelled data was released with the purpose to 
enable the exploration of semi-supervised systems. 
The test set comprised 1/3 of the tagged examples. 
The sense inventory was taken from the Basque 
WordNet, which is linked to WordNet version 1.6 
(Fellbaum, 1998). The examples came mainly from 
newspaper texts, although we also used a balanced 
in-house corpus and texts from Internet. The words 
selected for this task were coordinated with other 
lexical-sample tasks (such as Catalan, English, 
Italian, Romanian and Spanish) in order to share 
around 10 of the target words.  
The following steps were taken in order to carry 
out the task: 
                                                     
(*) Authors listed in alphabetic order. 
1. set the exercise  
a. choose sense inventory from a pre-existing 
resource 
b. choose target corpora 
c. choose target words  
d. lemmatize the corpus automatically 
e. select examples from the corpus 
2. hand-tagging 
a. define the procedure 
b. revise the sense inventory 
c. tag 
d. analyze the inter-tagger agreement 
e. arbitrate 
This paper is organized as follows: The 
following section presents the setting of the 
exercise. Section 3 reviews the hand-tagging, and 
Section 4 the details of the final release. Section 5 
shows the results of the participant systems. 
Section 6 discusses some main issues and finally, 
Section 7 draws the conclusions. 
2 Setting of the exercise  
In this section we present the setting of the 
Basque lexical-sample exercise. 
2.1 Basque 
As Basque is an agglutinative language, the 
dictionary entry takes each of the elements 
necessary to form the different functions. More 
specifically, the affixes corresponding to the 
determinant, number and declension case are taken 
in this order and independently of each other (deep 
morphological structure). For instance, ?etxekoari 
emaiozu? can be roughly translated as ?[to the one 
in the house] [give it]? where the underlined 
sequence of suffixes in Basque corresponds to ?to 
the one in the?.  
2.2 Sense inventory 
We chose the Basque WordNet, linked to 
WordNet 1.6, for the sense inventory. This way, 
the hand tagging enabled us to check the sense 
coverage and overall quality of the Basque 
WordNet, which is under construction. The Basque 
WordNet is available at http://ixa3.si.ehu.es/ 
wei3.html. 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2.3 Corpora used 
Being Basque a minority language it is not easy 
to find the required number of occurrences for each 
word. We wanted to have both balanced and 
newspaper examples, but we also had to include 
texts extracted from the web, specially for the 
untagged corpus. The procedure to find examples 
from the web was the following: for each target 
word all possible morphological declensions were 
automatically generated, searched in a search-
engine, documents retrieved, automatically 
lemmatized (Aduriz et al 2000), filtered using 
some heuristics to ensure quality of context, and 
finally filtered for PoS mismatches. Table 1 shows 
the number of examples from each source. 
2.4 Words chosen 
Basically, the words employed in this task are 
the same words used in Senseval 2 (40 words, 15 
nouns, 15 verbs and 10 adjectives), only the sense 
inventory changed. Besides, in Senseval 3 we 
replaced 5 verbs with new ones. The reason for this 
is that in the context of the MEANING project1 we 
are exploring multilingual lexical acquisition, and 
there are ongoing experiments that focus on those 
verbs. (Agirre et al 2004; Atserias et al 2004). 
In fact, 10 words in the English lexical-sample 
have translations in the Basque, Catalan, Italian, 
Romanian and Spanish lexical tasks: channel, 
crown, letter, program, party (nouns), simple 
(adjective), play, win, lose, decide (verbs).  
2.5 Selection of examples from corpora 
The minimum number of examples for each 
word according to the task specifications was 
calculated as follows: 
 
N=75+15*senses+7*multiwords  
 
As the number of senses in WordNet is very high, 
we decided to first estimate the number of senses 
and multiwords that really occur in the corpus. The 
taggers were provided with a sufficient number of 
examples, but they did not have to tag all. After 
they had tagged around 100 examples, they would 
count the number of senses and multiwords that 
had occurred and computed the N according to 
those counts.  
The context is constituted of 5 sentences, 
including the sentence with the target word 
appearing in the middle. Links were kept to the 
source corpus, document, and to the newspaper 
section when applicable.  
The occurrences were split at random in training 
set (2/3 of all occurrences) and test set (1/3).  
                                                     
1 http://www.lsi.upc.es/~nlp/meaning/meaning.html 
 Total (N) (B) (I)
# words 40  
# senses 316  
# number of tagged examples 7362 5695 924 743
# number of untagged examples 62498 - - 62498
# tags  9887  
Table 1: Some figures regarding the task. N, B and I 
correspond to the source of the examples: newspaper, 
balanced corpus and Internet respectively. 
3 Hand tagging 
Three persons, graduate linguistics students, 
took part in the tagging. They are familiar with 
word senses, as they are involved in the 
development of the Basque WordNet. The 
following procedure was defined in the tagging of 
each word. 
? Before tagging, one of the linguists (the editor) 
revised the 40 words in the Basque WordNet. 
She had to delete and add senses to the words, 
specially for adjectives and verbs, and was 
allowed to check the examples in the corpus.  
? The three taggers would meet, read the glosses 
and examples given in the Basque WordNet 
and discuss the meaning of each synset. They 
tried to agree and clarify the meaning 
differences among the synsets. For each word 
two hand-taggers and a referee is assigned by 
chance. 
? The number of senses of a word in the Basque 
WordNet might change during this meeting; 
that is, linguists could agree that one of the 
word?s senses was missing, or that a synset did 
not fit with a word. This was done prior to 
looking at the corpus. Then, the editor would 
update the Basque WordNet according to those 
decisions before giving the taggers the final 
synset list. Overall (including first bullet 
above), 143 senses were deleted and 92 senses 
added, leaving a total of 316 senses. This 
reflects the current situation of the Basque 
WordNet, which is still under construction. 
? Two taggers independently tagged all 
examples for the word. No communication was 
allowed while tagging the word. 
? Multiple synset tags were allowed, as well as 
the following tags: the lemma (in the case of 
multiword terms), U (unassignable), P (proper 
noun), and X (incorrectly lemmatized). Those 
with an X were removed from the final release. 
In the case of proper nouns and multiword 
terms no synset tag was assigned. Sometimes 
the U tag was used for word senses which are 
not in the Basque WordNet. For instance, the 
sense of kanal corresponding to TV channel, 
which is the most frequent sense in the 
examples, is not present in the Basque 
WordNet (it was not included in WordNet 1.6).  
? A program was used to compute agreement 
rates and to output those occurrences where 
there was disagreement. Those occurrences 
were  grouped by the senses assigned. 
? A third tagger, the referee, reviewed the 
disagreements and decided which one was the 
correct sense (or senses).  
The taggers were allowed to return more than one 
sense, and they returned 9887 tags (1.34 per 
occurrence). Overall, the two taggers agreed in at 
least one tag 78.2% of the time. Some words 
attained an agreement rate above 95% (e.g. nouns 
kanal or tentsio), but others like herri ?
town/people/nation? attained only 52% agreement. 
On average, the whole tagging task took 54 
seconds per occurrence for the tagger, and 20 
seconds for the referee. However, this average 
does not include the time the taggers and the 
referee spent in the meetings they did to 
understand the meaning of each synset. The 
comprehension of a word with all its synsets 
required 45.5 minutes on average. 
4 Final release 
Table 1 includes the total amount of hand-tagged 
and untagged examples that were released. In 
addition to the usual release, the training and 
testing data were also provided in a lemmatized 
version (Aduriz et al 2000) which included 
lemma, PoS and case information. The motivation 
was twofold: 
? to make participation of the teams easier, 
considering the deep inflection of Basque. 
? to factor out the impact of different 
lemmatizers and PoS taggers in the system 
comparison.  
5 Participants and Results 
5 teams took part in this task: Swarthmore 
College (swat), Basque Country University 
(BCU), Instituto per la Ricerca Scientifica e 
Tecnologica (IRST), University of Minnesota 
Duluth (Duluth) and University of Maryland 
(UMD). All the teams presented supervised systems 
which only used the tagged training data, and no 
other external resource. In particular, no system 
used the pointers to the full texts, or the additional 
untagged texts. All the systems used the lemma, 
PoS and case information provided, except the 
BCU team, which had additional access to number, 
determiner and ellipsis information directly from 
the analyzer. This extra information was not 
provided publicly because of representation issues.  
 
 Prec. Rec. Attempted
basque-swat_hk-bo 71.1  70.4  99.04 %
BCU_Basque_svm 69.9  69.9  100.00 %
BCU_-_Basque_Comb 69.5  69.5  100.00 %
swat-hk-basque 67.0  67.0  100.00 %
IRST-Kernels-bas 65.5  65.5  100.00 %
swat-basque 64.6  64.6  100.00 %
Duluth-BLSS 60.8  60.8  100.00 %
UMD_SST1 65.6  58.7  89.42 %
MFS 55.8  55.8  100.00 %
Table 2: Results of systems and MFS baseline, ordered 
according to Recall. 
We want to note that due to a bug, a few examples 
were provided without lemmas.  
The results for the fine-grained scoring are 
shown in Table 2, including the Most Frequent 
Sense baseline (MFS). We will briefly describe 
each of the systems presented by each team in 
order of best recall.  
? Swat presented three systems based in the 
same set of features: the best one was based on 
Adaboost, the second on a combination of five 
learners (Adaboost, maximum entropy, 
clustering system based on cosine similarity, 
decision lists, and na?ve bayes, combined by 
majority voting), and the third on a 
combination of three systems (the last three).  
? BCU presented two systems: the first one based 
on Support Vector Machines (SVM) and the 
second on a majority-voting combination of 
SVM, cosine based vectors and na?ve bayes.  
? IRST participated with a kernel-based method. 
? Duluth participated with a system that votes 
among three bagged decision trees. 
? UMD presented a system based on SVM. 
The winning system is the one using Adaboost 
from Swat, followed closely by the BCU system 
using SVM. 
6 Discussion 
These are the main issues we think are 
interesting for further discussion. 
Sense inventory. Using the Basque WordNet 
presented some difficulties to the taggers. The 
Basque WordNet has been built using the 
translation approach, that is, the English synsets 
have been ?translated? into Basque. The taggers 
had some difficulties to comprehend synsets, and 
especially, to realize what makes a synset different 
from another. In some cases the taggers decided to 
group some of the senses, for instance, in herri ?
town/people/nation? they grouped 6 senses. This 
explains the relatively high number of tags per 
occurrence (1.34). The taggers think that the 
tagging would be much more satisfactory if they 
had defined the word senses directly from the 
corpus.  
Basque WordNet quality. There was a 
mismatch between the Basque WordNet and the 
corpus: most of the examples were linked to a 
specific genre, and this resulted in i) having a 
handful of senses in the Basque WordNet that did 
not appear in our corpus and ii) having some 
senses that were not included in the Basque 
WordNet. Fortunately, we already predicted this 
and we had a preparation phase where the editor 
enriched WordNet accordingly. Most of the 
deletions in the preliminary part were due to the 
semi-automatic method to construct the Basque 
WordNet. All in all, we think that tagging corpora 
is the best way to ensure the quality of the 
WordNets and we plan to pursue this extensively 
for the improvement of the Basque WordNet.  
7 Conclusions and future work 
5 teams participated in the Basque lexical-
sample task with 8 systems. All of the participants 
presented supervised systems which used lemma, 
PoS and case information provided, but none used 
the large amount of untagged senses provided by 
the organizers. The winning system attained 70.4 
recall. Regarding the organization of the task, we 
found that the taggers were more comfortable 
grouping some of the senses in the Basque 
WordNet. We also found that tagging word senses 
is essential for enriching and quality checking of 
the Basque WordNet. 
Acknowledgements 
The work has been partially funded by the 
European Commission (MEANING project IST-
2001-34460). Eli Pociello has a PhD grant from 
the Basque Government.  
References  
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, X. 
Arregi, J.M. Arriola, X. Artola, K. Gojenola, A. 
Maritxalar, K. Sarasola, M. Urkia. 2000. A 
Word-grammar Based Morphological Analyzer 
for Agglutinative Languages. In Proceedings of 
the International Conference on Computational 
Linguistics (COLING). Saarbrucken, Germany.  
E. Agirre, A. Atutxa, K. Gojenola, K. Sarasola. 
2004. Exploring portability of syntactic 
information from English to Basque. In 
Proceedings of the 4rd International Conference 
on Languages Resources and Evaluations 
(LREC). Lisbon, Portugal. 
J. Atserias, B. Magnini, O. Popescu, E. Agirre, A. 
Atutxa, G. Rigau, J. Carroll and R. Koeling 
2004. Cross-Language Acquisition of Semantic 
Models for Verbal Predicates. In Proceedings of 
the 4rd International Conference on Languages 
Resources and Evaluations (LREC). Lisbon, 
Portugal. 
C. Fellbaum. 1998. WordNet: An electronic 
Lexical Database. The MIT Press, Cambridge, 
Massachusetts.  
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 466?475, Dublin, Ireland, August 23-29 2014.
The annotation of the Central Unit in Rhetorical Structure Trees: A Key
Step in Annotating Rhetorical Relations
Mikel Iruskieta
Dept. Language and
Literature Didactics
mikel.iruskieta@ehu.es
IXA NLP Group, Manuel Lardizabal 1, 48014 Donostia
Arantza D??az de Ilarraza
Dept. Computer Languages
and Systems
a.diazdeilarraza@ehu.es
Mikel Lersundi
Dept. Basque Language
and Communication
mikel.lersundi@ehu.es
Abstract
This article aims to analyze how agreement regarding the central unit (macrostructure) influ-
ences agreement when establishing rhetorical relations (microstructure). To do so, the authors
conducted an empirical study of abstracts from research articles in three domains (medicine, ter-
minology, and science) in the framework of Rhetorical Structure Theory (RST). The results help
to establish a new criteria to be used in RST-based annotation methodology of rhetorical rela-
tions. Furthermore, a set of verbs which can be utilized to detect the central unit of abstracts was
identified and analyzed with the aim of designing a preliminary study of an automatic system for
identifying the central unit in rhetorical structures.
1 Credits
This study was carried out within the framework of the following projects: IXA group, Research Group
of type A (2010-2015): IT344-10 (Basque Government); SKaTeR: Scenario Knowledge Acquisition by
Textual Reading: TIN2012-38584-C06-02 (Spanish Ministry of Economy and Competitiveness); Hib-
rido Sint: Rule-based and Statistical-based syntactic analyzers. Corpus management in an XML standard
based framework: TIN2010-20218 (Spanish Ministry of Science and Innovation); TACARDI: Context-
aware Machine Translation Augmented using Dynamic Resources from Internet: TIN2012-38523-C02-
01 (Spanish Ministry of Science and Innovation).
2 Introduction
One of the biggest challenges in annotating the rhetorical structure of discourse has to do with the reli-
ability of annotation. When two or more individuals annotate a text, discrepancies generally arise as a
result of the way each human annotator interprets the text (Taboada and Mann, 2006). Besides, markers
specifying the rhetorical relations between discourse units do not always exist (Taboada, 2006). Even
if they appear in the text, these markers do not always establish rhetorical relations unequivocally (van
Dijk, 1998; Mann and Thompson, 1987). Despite this ambiguity, discourse markers are considered to
be a form of linguistic evidence which are used to signal coherence relations and which are useful in
detecting certain rhetorical relations (Georg et al., 2009; Iruskieta et al., 2009; Pardo and Nunes, 2004).
In searching for linguistic evidence to determine the rhetorical structure of texts, scholars have ana-
lyzed not only discourse markers but also verbs. For example, Pardo and Nunes (2004) first rhetorically
annotated their Corpus TCC (a Portuguese corpus containing scientific texts in the computational do-
main) and then analyzed verbs related to certain rhetorical relations, finding that verbs such as buscar
?search, look for?, objetivar ?objectify, intend?, pretender ?intend, mean?, procurar ?search, look for?,
servir ?serve, meet the requirements of?, and visar ?aim, drive? are related to the PURPOSE relation.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
466
They also found that other rhetorical relations such as CAUSE, EVIDENCE and RESULT are indi-
cated by other types of verbs.
This paper aims to answer the following research questions:
(i) Does agreement about the central unit affect inter-annotator reliability when annotating rhetorical
relations?
(ii) Are there some types of verbs that can be used as ?indicators? (Paice, 1980) to identify the central
unit of a rhetorical structure?
Besides we focus on how to identify the unit associated with the main node in the rhetorical structure
tree or, in other words, the ?central unit? (CU) (Stede, 2008), the ?central proposition? (Pardo et al.,
2003), the ?central subconstituent? (Egg and Redeker, 2010) or the ?salient unit of the root node? (Marcu,
1999). To our knowledge, no other research has attempted to identify this unit, the central unit of a
rhetorical structure tree, by semantically studying the verb within the framework of RST. This topic,
however, could have both theoretical and methodological implications.
The structure of the paper is as follows: Section 3 describes the theoretical framework, corpus and
methodology utilized in this study. Section 4 lays out the results obtained. Section 5 presents a pre-
liminary study on the semantic classes of the verbs beloging to central unit. The final section presents
conclusions and suggests directions for future research.
3 Theory, corpus and methodology
3.1 Theory
Various theories describe the relational structure of a text (Asher and Lascarides, 2003; Grosz and Sidner,
1986; Mann and Thompson, 1987). This study is based on Mann and Thompson?s (1987) Rhetorical
Structure Theory (RST), an applied, language-independent theory that describes coherence between text
fragments. It combines the idea of nuclearity ?that is, the importance of an individual fragment from
within the discourse? with the presence of rhetorical relations (RR) (hypotactic and paratactic relations)
between these fragments. Mann and Thompson (1987) argue that nuclear units play a more important
role for text coherence than satellites.
This has significant implications for automatic text summarization. Ono et al. (1994) and Rino and
Scott (1996) suggest that the summary of a text can be obtained by deleting optional satellites, an argu-
ment based on the property of nuclearity in hypotactic relations. Da Cunha (2008) describes rules based
on nuclearity which can be used to summarize medical texts. For a more in-depth, critical explanation of
nuclearity, see Stede (2008) and for additional information on RST, see Taboada and Mann (2006) and
Mann and Taboada (2010).
According to RST, hypotactic and paratactic relations connect elementary discourse units (EDUs) or
groups of discourse units (span). Elementary units cannot be divided into simpler units. In this paper, a
?central unit? is defined as the clause which best expresses the topic or main idea of the text. The central
unit of a rhetorical structure tree is the elementary unit or group of elementary units which comprise the
nucleus of its main node. Hypotactic units have a single nucleus in the central unit, while paratactic units
contain multiple nuclei.
For example,
1
in the rhetorical structure tree presented in Figure 1, unit 7 is the central unit of the
elementary units that are numbered from 1 to 7, since it is the nuclear unit of the root node which and
has the relation PREPARATION associated to it. The root node covers the entire structure of the text,
and since it is not linked to any other unit, no other associated nuclei have the same degree of central
importance (Marcu, 1999). The central unit indicates the most important unit in the structure, which is
indicated in Figure 1 by the verb analizatzen ?analyze?.
Determining nuclearity (that is, deciding which of the two associated spans has a more central role
based on the intentions of the writer) is key in assigning rhetorical relations. In fact, Stede (2008) has
questioned the way in which rhetorical structure is represented in RST based on several reasons:
i) It is not clear what grounds are used to make the decision: is it because of nuclearity or because of
the effect of a rhetorical relation?
1
Examples are extracted from the Basque corpus used in this study (Iruskieta, 2014).
467
Figure 1: A rhetorical structure tree for text GMB0301 (Annotator 1)
ii) Nuclearity poses challenges for annotation. This led Carlson et al. (2001) to present multi-nuclear
versions of some nuclear relations from the classic extended classification.
We also identified the same problems. Examples (1) and (2) demonstrate how different choices of
nuclearity affect agreement in rhetorical relations.
(1) [Emaitza:]
1
[Erabiltzaileen perfil orokorra ondokoa dela esan daiteke: gizonezkoa (% 51,4),
heldua (43,2 urteko media) eta patologia traumatologikoagatik kontsultatzen duena (% 50,5).]
2
GMB0401
[Results:]
1
[The average user is as follows: male (51.4%), middle-aged (43.2 years old), and
treated for trauma (50.5%).]
2
Annotator 1 (A1) decides that the second unit in Example (1) is more important than the first unit.
The second annotator (A2), however, makes the exact opposite decision. Both annotators arrive reach
their conclusions based on structural reasons. Disagreements about the importance of each text fragment
influence the rhetorical relation: A1 annotates the relation as PREPARATION while A2 chooses to label
the relation as ELABORATION.
Example (2) demonstrates how different interpretations of nuclearity affect agreement with regard to
the rhetorical relation.
(2) [Erabiltzaileen % 80ak bere kabuz erabakitzen dute larrialdi zerbitzu batetara jotzea]
1
[eta
kontsulta hauen % 70a larritasun gutxikotzat jotzen dituzte zerbitzu hauetako medikuek.]
2
GMB0401
[It is calculated that about 80% of users come to emergency services on their own initiative]
1
[and that 70% of visits are considered minor by health care personnel.]
2
A1 believes that the second unit in Example (2) provides more detailed characteristics about the users
(e.g. the second unit is a satellite of the first unit) and therefore annotates the relation as hypotactic
468
(ELABORATION). A2, on the other hand, annotates the same discourse segment as a paratactic relation
(CONJUNCTION), considering the marker eta ?and? to be the most significant element, indicating that
she or he believes that two different elements of emergency services are being discussed.
According to Bateman and Rondhuis (1997), when determining nuclearity at the higher levels of a tree
structure, RST clearly establishes a global view of a text, since an analysis is by definition incomplete
until all units in the text have a function which is depicted by a single structure. It is logical that if
nuclearity plays a role in determining rhetorical relations at the lower levels of a rhetorical structure, it
will also affect the structure?s higher levels. If two annotators have a different global point of view (e.g.
they annotate different central units), they will also annotate different rhetorical relations. Therefore,
our hypothesis is that trees which have the same global interpretation of text structure will have greater
agreement in the annotation process; i.e., in the labeling of rhetorical relations, while those with differing
global structures will have lower agreement. This hypothesis underpins the methodology used to answer
the first research question of this study.
The next subsection describes the corpus used for this study.
3.2 Corpus
This study sought to analyze short but well structured texts written in Basque in order to determine
linguistic evidence which could be used to indicate the central unit of rhetorical structure. The cor-
pus utilized in this study consists of three corpora from the same genre (abstracts) from three different
specialized domains: medicine, terminology and science. The communicative goal of these texts is to
present specialized knowledge, since both the writer and readers are experts. Medical texts include the
abstracts of all medical articles written in Basque in the Gaceta M?edica de Bilbao (GMB) ?Medical
Journal of Bilbao? between 2000 and 2008. Terminology texts are abstracts from the proceedings of the
Congreso Internacional de Terminolog??a (TERM) ?International Conference on Terminology? organized
by UZEI ?the Basque Centre for Terminology? in 1997, while scientific articles are abstracts of papers
from the University of the Basque Country?s Jornadas de Investigacin de la Facultad de Ciencia y Tec-
nolog??a (ZTF) ?Research Conference of the Faculty of Science and Technology?, which took place in
2008.
After the annotation process (central unit and rhetorical relations among others), the annotated cor-
pus was evaluated (Iruskieta et al., Forthcoming) and harmonized by a judge (Iruskieta, 2014). The
harmonized corpus can be consulted in the RST Basque TreeBank
2
(Iruskieta et al., 2013a).
3.3 Methodology
Before presenting the process followed to get our goals, let us explain that, when we began this research,
the GMB corpus had previously been annotated manually (Iruskieta et al., 2013b) by two linguists using
the extended classification of RST (Mann and Taboada, 2010) while the other two corpora (TERM and
ZTF) were not tagged. The results of the comparison done about the relationship of agreement between
the annotation of the central unit and the annotation of the rhetorical structure in GMB led us to redefine
the annotation strategy for TERM and ZTF in the sense that we asked annotators to identify the central
unit (one or more) before tagging the rhetorical structure.
The steps carried out for the annotation of the corpora were the following:
A. Elementary Discourse Units segmentation. The corpus was segmented at intra-sentential level using
a minimal set of criteria (Iruskieta et al., 2011a) by each annotator using the RSTTool program
(O?Donnell, 1997)
B. Central unit identification (TERM and ZTF). Both annotators determined the central unit
3
and the
verbs present in the central unit of a scientific abstract in TERM and ZTF domains.
4
2
The RST Basque TreeBank is available at http://ixa2.si.ehu.es/diskurtsoa/en/fitxategiak.php.
3
We calculate a baseline to illustrate the complexity of the central unit selection reporting the average number of EDUs:
average number of 22.58 EDUs per central unit candidates per text. The average was calculated based on the number of EDUs,
over the number of texts.
4
The central units (CU) can be consulted also in RST Basque TreeBank.
469
C. Rhetorical tree structure annotation. Rhetorical relations were annotated by each annotator using
the RSTTool program with the extended classification (Mann and Taboada, 2010) of RST.
D. Evaluation. Agreement in rhetorical tree structures were manually evaluated following the qua-
litative methodology proposed in Iruskieta et al. (Forthcoming), but taking into account the struc-
tures with the same central unit and distinguishing between the rhetorical relations linked or not to
central unit.
E. Interpretation. We compared the results of central unit agreement and disagreemens to check for
possible correlations using a t-test formula at 99.5% confidence.
4 Results
Our main hypothesis is that an agreement on central unit leads us to a higher agreement on rhetorical rela-
tions; in other words, identifying the main idea of the text helps the human annotator in the identification
of the structure of the text and, therefore, the agreement between annotators is higher.
5
4.1 Correlation between agreement on rhetorical relations and agreement on central unit
The observation made about the GMB, where we argued that annotators agree more on rhetorical rela-
tions when they annotated the same central unit, remained after considering results of a more extended
corpus with two new corpora (TERM and ZTF) and two additional annotators.
Results confirm this fact even when the difference has been substantially reduced from 0.1497 to
0.0426 when more data (all the corpus) were considered. Table 1 presents the global results of the
comparison between the agreement on central unit (?= CU?)
6
and mean agreement on rhetorical relations
for the corpus as a whole.
GMB Corpus
= CU 6= CU Diff. = CU 6= CU Diff.
Mean 0.7456 0.5959 0.1497 0.5915 0.5489 0.0426
SD 0.1833 0.1749 0.1429 0.1125
Table 1: Mean agreement (and standard deviation) of the central unit and rhetorical relations
We perform a significant test for the differences. We confirmed that the populations being compared
have a normal distribution following the Kolmogorov-Smirnov test (p-value of K-S test was 0.913) and
have the same variance (p-value of F-test was 0.063). Therefore, two tail independent samples t-test was
used with a 0.013 p-value, denying the null hypothesis.
Other hypothesis and combinations were analyzed with positive results: a significant agreement was
observed when we compared agreement in rhetorical relation linked to central unit when annotators
tagged the same central unit and when they tagged different central units. It is very difficult to establish
which rhetorical relation are linked to central unit when annotators do not tag the same central unit.
4.2 Correlation between agreement on rhetorical relations linked or not to central unit
After our main hypothesis was confirmed, we went ahead in the tree structure and we checked whether
there is higher agreement in rhetorical relations linked to the central unit (considering the structures
where there was agreement in central unit), than in the other relations of the tree structure. For example,
in the rhetorical structure tree presented in Figure 1, we consider two relations linked to central unit
PREPARATION (1>2-7) and BACKGROUND (2-6>7), while the other four relations are not linked
to central unit (ELABORATION (2<3), ELABORATION (2-3<4-6), ELABORATION (4-5<6) and
CONJUNCTION (4=5). Table 2 presents the results of relations linked to central unit with relation not
linked to central unit:
In structures with the same central unit we compare between the agreement in rhetorical relations
linked to the central unit and all the other relations. Percent agreement is substantially higher when we
5
The results of all corpora considered indicate that the change in methodology improved central unit agreement between
annotators slightly in TERM and ZTF. This highlights the benefits of a first step followed in TERM and ZTF which entails
detecting the central unit.
6
And ? 6= CU? for disagreement on central unit.
470
GMB Corpus
Linked Not Diff. Linked Not Diff.
Mean 0.7454 0.5881 0.1573 0.7179 0.5449 0.1730
SD 0.2695 0.3344 0.2107 0.1850
Table 2: Comparison between rhetorical relations linked and no-linked to central unit in structures with
the same central unit
observe the relations linked to the central unit: 17.3% higher than the agreement on the relations that are
not linked to the central unit. Populations being compared follow a normal distribution (p-value of K-S
test was 0.93) but they do not have the same variance (p-value of F-test is 0.09). The result of the null
hypothesis was rejected (p-value of t-test was smaller than 0.001), so we can establish a correlation. The
average rhetorical relation agreement on a text according to the central unit, is no different to the average
percentage of agreement in the rhetorical relations linked to the UC to those not linked.
4.3 Discussion of results
To illustrate the results on agreement (or not) on central unit and average agreement on rhetorical rela-
tions linked (or not) from Tables 1 and 2, we present comparisons of the populations in Figure 2:
a. When the central unit was the same, the average agreement on relations is represented with red
crosses.
b. When the central unit was different, the average agreement on relations is represented with blue
circles.
c. When the central unit was the same and the relations are linked to central unit with black crosses.
d. When the central unit was the same and the relations are not linked to central unit with violet
triangles.
Figure 2: Representation of mean agreement between RR (vertical) and the number of relations consid-
ered in a structure (horizontal) according to the central unit.
These results help to answer the first research question of this study and seem to indicate that there
is a correlation between these two kinds of agreement: i) greater agreement on detecting the central
unit correlates with greater agreement on the annotation of rhetorical relations (results from Table 1 are
ilustrated in Figure 2 comparing the distance of the red croses [a] with blue circles [b]), ii) also on those
which are linked to the central unit (results from Table 2 are ilustrated in Figure 2 comparing the distance
471
of the black croses [c] with the violet triangles [d]).
This analysis leads to two conclusions:
i) When considering the methodology for labeling rhetorical structure, annotating the central unit is
an important first step before labeling rhetorical relations at least in short texts such as abstracts.
ii) In Computational Linguistics, a process which helps to automatically identify the central unit is
important for determining some restrictions in rhetorical structure mainly determined by the gen-
re/domain structure.
In order to discuss these results, first of all we have to consider that the central unit is a nuclear unit and
that relations are linked at various levels (intra-sentential level and inter-sentential level); there are more
relations linked at inter-sentential level. For example, in Figure 1 two relations linked to central unit are
only at inter-sentential level. This seems to show that these results (rhetorical relations linked to central
unit) are not so trivial, since the degree of agreement expected at higher level tree structures is lower.
In other words, the agreement at lower levels is higher than in the high level. For example, Marcu and
Echihabi (2002) argue that automatic annotation of certain rhetorical relations should be addressed first
at intra-sentential level because they are less ambiguous. Soricut and Marcu (2003) mention that some
of the rhetorical relations are derived from syntactic structures. These results (11.50% higher agreement
at intra-sentential level, than at inter-sentential level in the GMB corpus) were confirmed in Basque by
Iruskieta et al. (2011b).
5 Identifying the semantic class of verbs in the central unit
Our final goal is the automatic detection of central unit. To this end, we wanted to find lexical-semantic
markers in the central unit
7
in each domain in greater detail. The meanings of the main verbs were
analyzed and their semantic class determined as per the SUMO ontology (Niles, 2003). The relation
between meaning and semantic class was obtained by means of the MCR semantic database, which
includes various lexical-semantic and ontological databases. Data from the GMB, TERM, and ZTF cor-
pora are grouped in Table 3 by semantic classes at the most general level, e.g. ?Intentional Psychological
Process? (IPP), ?Social Interaction? (SI), ?Internal Change? (IC) and ?Predicate?.
SUMO SUMO MCR synset GMB TERM ZTF
IP-IPP Reasoning analyze
1
, show
2
, base
1
0.4615 0.2273 0.0870
Comparing value
2
, compare
1
0.2692
Classifying classify
1
0.0870
Learning review
1
0.0385
Guiding take
3
0.0455
Process gain
4
0.1739
IP-IPP recognize
2
, determine
8
, hold
6
, focus
1
0.0385 0.0909 0.0435
IP-SI Communication present
2
, addres
9
, recount
1
, propose
1
0.0385 0.4545 0.0435
IP perform
1
, target
1
, set-up
15
, work
1
, make
3
, use
1
0.1154 0.0909 0.0870
IP Searching-Investigating investigate
1
0.0435
IP Organizational Process serve
2
0.0435
IC palliate
2
0.0455
Predicate be
1
, develop
5
, constitute
1
, hold
4
0.0385 0.0455 0.3913
Table 3: Summary comparison of verbs by domain
The results of this empirical study indicate that each domain tends to use verbs from the same semantic
class. For example, in the GMB corpus, the central unit was usually marked with verbs from the IPP
category. On the other hand, in the TERM corpus, verbs from the IPP and SI category. Verbs in the
central unit of the ZTF corpus are marked with IPP and Predicate class.
Therefore, the results demonstrate that:
7
Results show that there are multiple EDUs functioning as the central unit of the text in the three corpora: 9 multiple EDU
functioning as central unit in GMB, 2 multiple EDUs in TERM and 3 multiple EDUs in ZTF.
472
i) A study is needed to identify the SUMO class of the verbs used in a specific domain. For example in
our corpus the central units is indicated with verbs that belong to the IPP class for all three domains.
However, other classes also have to be considered, SI for TERM and Predicate for ZTF.
ii) In the case of weak verbs, other indicators
8
help to identify the central unit. The TERM and ZTF
corpora are more marked by noun class indicators than the GMB corpus (Iruskieta, 2014). Another
reason is that the direct observation of the central unit makes the central unit selection more con-
sistent. An evidence of that is that all the verbs in central unit are from the same SUMO class in
TERM and ZTF corpora by both annotators. Futhermore, it could also be argued that the use of
different verbs has to do not only with the field but also with the medium: the GMB corpus derives
from texts published in a periodical while the TERM and ZTF corpora include texts published in
Conference proceedings. In other words, it could be argued that the medium influences the writing
style, and consequently, impacts the verb classes used in the texts. This is in line with the main
argument of this study, since different verbs are used to indicate the central unit in the TERM and
ZTF corpora, which share the same medium but belong to different fields.
So far, this paper has provided a partial answer to the second research question. However, to automat-
ically detect the central unit by means of verbs (with the help of other types of signals) it is necessary to
consider these three issues:
i) The verb form which is used in the central unit might also be used in non-central units in the
rhetorical structure tree.
ii) Tools which disambiguate the sense of analyzed verbs are necessary in order to know what SUMO
class they belong to.
9
iii) The central unit is not always indicated with a verb and, therefore, other types of signals (or combi-
nations) can help in the automatic identification of the central unit.
The next phase of this research considered whether verb forms which appear in the central unit un-
equivocally indicate this unit or whether they can also appear in other types of units. This entailed calcu-
lating the frequency with which each studied verb appeared and counting the percentage of appearances
which correspond to the central unit.
From the results obtained so far we can?t establish any clear tendency but rather some preliminary
conclusions that must be ratified with the analysis of more data.
Phenomena related to the central unit appeared in this study of ambiguity:
i) In GMB corpus verbs that indicate the central unit with a high enough frequency are from IPP cat-
egory baloratu ?value2?; there exist other verbs that can be considered but they are not so frequent,
e.g. alderatu ?compare
1
?, gainbegiratu ?review
1
?, aztertu and analizatu ?analyze
1
?, and ezagutu
?recognize
2
?.
ii) In TERM corpus, the second sense of the verb present in MCR, ?present
2
? (its equivalents in Basque
are the verbs plazaratu, aurkeztu, aipatu, berri eman and jardun), has a high frequency but a high
degree of ambiguity. We can?t identify the central unit on the basis of its occurrence.
iii) In the ZTF corpus, the central unit was not always indicated with a verb.
6 Conclusions and future research
After considering the relationship between identifying the central unit in a text and annotating its rhetor-
ical structure, it has been demonstrated that a correlation exists between these two tasks, since a greater
degree of agreement with regard to the central unit leads to a greater degree of agreement in rhetorical.
Besides there is more agreement in rhetorical relations linked to the central units than in relations that
are not linked.
This study has investigated verbs which mark the central unit of a rhetorical structure and the cor-
relation of the agreement in central unit with the agreement in rhetorical relations. Its goal has been
8
According to Paice (1980) indicators can be nouns (?paper?, ?method?, ?result?), determiners (?this?, ?the?, ?a?) and pronouns
(?we?, ?I?), among others.
9
In attempting to automatically detect coherence relations which are not indicated or vaguely indicated using WordNet
(Miller et al., 1990) Sporleder and Lascarides (2007) obtained better results using morphological strategies than using semantic
generalization strategies. This is due to the fact that, as far as we know, NLP has yet to focus on disambiguating words.
473
to consider aspects which are relevant for establishing a methodology to help set general criteria for
identifying the central unit of texts.
This study also considered which verbs appear in the central units, their semantic classes (according
to SUMO categories), and how they identify the central unit. Verbs used to indicate the central units vary
in different domains: in the GMB corpus, the central unit was more frequently and the least ambiguously
indicated with verbs from the IPP category (SUMO), while in the TERM, SI verbs were most frequent
and the least ambiguous.
Testing these results in a larger corpus (and different domains and text structures) could lead to ap-
plications for automatic text summarization tasks (classifying clauses), since the central unit is the most
important unit in the text.
Furthermore, this study has explained the steps to automatically detect the central unit based on the
ambiguity of the verb which marks the central unit. More studies about other indicators (and their
combinations) are necessary to automatically detect the central unit.
References
[Asher and Lascarides2003] Asher, Nicholas and Alex Lascarides. 2003. Logics of conversation. Cambridge Univ
Pr, Cambridge.
[Bateman and Rondhuis1997] Bateman, John A. and Klaas Jan Rondhuis. 1997. Coherence relations: Towards a
general specification. Discourse Processes, 24(1):3?49.
[Carlson et al.2001] Carlson, Lynn, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged
corpus in the framework of Rhetorical Structure Theory. In 2nd SIGDIAL Workshop on Discourse and Dialogue,
Eurospeech 2001, Aalborg, Denmark, 1-2 September. Association for Computational Linguistics.
[da Cunha2008] da Cunha, Iria. 2008. Hacia un modelo ling?u??stico de resumen autom?atico de art??culos m?edicos
en espa?nol. Phd-thesis, IULA, Universitat Pompeu Fabra.
[Egg and Redeker2010] Egg, Markus and Gisela Redeker. 2010. How complex is discourse structure? In Proceed-
ings of the 7th International Conference on Language Resources and Evaluation (LREC 2010), page 16191623,
Valletta, Malta, 19-21 May.
[Georg et al.2009] Georg, Georg, Hugo Hernault, Marc Cavazza, Helmut Prendinger, and Mitsuru Ishizuka. 2009.
From rhetorical structures to document structure: shallow pragmatic analysis for document engineering. In 9th
ACM symposium on Document engineering, pages 185?192, Munich, Germany, 16-18 September. ACM.
[Grosz and Sidner1986] Grosz, Barbara J. and Candance L. Sidner. 1986. Attention, intentions, and the structure
of discourse. Computational linguistics, 12(3):175?204.
[Iruskieta et al.2009] Iruskieta, Mikel, Arantza Diaz de Ilarraza, and Mikel Lersundi. 2009. Correlaciones en
euskera entre las relaciones ret?oricas y los marcadores del discurso [Correlations between rhetorical relations
and discourse markers]. In 27th AESLA Conference, pages 963?971, Ciudad Real, Spain.
[Iruskieta et al.2011a] Iruskieta, Mikel, Arantza Diaz de Ilarraza, and Mikel Lersundi. 2011a. Bases para la imple-
mentaci?on de un segmentador discursivo para el euskera [Bases for an Implementation of a Discourse Parser for
Basque]. In Workshop A RST e os Estudos do Texto, Mato Grosso, Brazil, 24-26 October.
[Iruskieta et al.2011b] Iruskieta, Mikel, Arantza Diaz de Ilarraza, and Mikel Lersundi. 2011b. Unidad discursiva
y relaciones ret?oricas: un estudio acerca de las unidades de discurso en el etiquetado de un corpus en euskera.
Procesamiento del Lenguaje Natural, 47:144.
[Iruskieta et al.2013a] Iruskieta, Mikel, Mara Jesus Aranzabe, Arantza Diaz de Ilarraza, Itziar Gonzalez, Mikel
Lersundi, and Oier Lopez de la Calle. 2013a. The RST Basque TreeBank: an online search interface to check
rhetorical relations. In 4th Workshop ?RST and Discourse Studies?, Brasil, October 21-23.
[Iruskieta et al.2013b] Iruskieta, Mikel, Arantza Diaz de Ilarraza, and Mikel Lersundi. 2013b. Establishing criteria
for RST-based discourse segmentation and annotation for texts in Basque. Corpus Linguistics and Linguistic
Theory, 0(0):132.
[Iruskieta et al.Forthcoming] Iruskieta, Mikel, Iria da Cunha, and Maite Taboada. Forthcoming. A qualitative
comparison method for rhetorical structures: Identifying different discourse structures in multilingual corpora.
Language Resources and Evaluation.
474
[Iruskieta2014] Iruskieta, Mikel. 2014. Pragmatikako erlaziozko diskurtso-egitura: deskribapena eta bere ebalu-
azioa hizkuntzalaritza konputazionalean (a description of pragmatics rhetorical structure and its evaluation in
computational linguistic). Phd-thesis, Euskal Herriko Unibertsitatea, Donostia. http://ixa2.si.ehu.
es/
?
jibquirm/tesia/tesi_txostena.pdf.
[Mann and Taboada2010] Mann, Willian C. and Maite Taboada. 2010. RST web-site. http://www.sfu.ca/
rst/.
[Mann and Thompson1987] Mann, Willian C. and Sandra A. Thompson. 1987. Rhetorical Structure Theory: A
Theory of Text Organization. Text, 8(3):243?281.
[Marcu and Echihabi2002] Marcu, Daniel and Abdessamad Echihabi. 2002. An unsupervised approach to rec-
ognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, pages 368?375. Association for Computational Linguistics.
[Marcu1999] Marcu, Daniel, 1999. Discourse trees are good indicators of importance in text, pages 123?136.
Advances in Automatic Text Summarization. MIT, Cambridge.
[Miller et al.1990] Miller, George A., Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller.
1990. Introduction to WordNet: An on-line lexical database. International Journal of lexicography, 3(4):235?
244.
[Niles2003] Niles, Ian. 2003. Mapping WordNet to the SUMO ontology. In Proceedings of the IEEE International
Knowledge Engineering conference, pages 23?26.
[O?Donnell1997] O?Donnell, Michael. 1997. RSTTool: An RST analysis tool. In Proceedings of the 6th European
Workshop on Natural Language Generation, Duisburg, Germany.
[Ono et al.1994] Ono, Kenjl, Kazuo Sumita, and Seijl Miike. 1994. Abstract generation based on rhetorical struc-
ture extraction. In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 344?348.
Association for Computational Linguistics.
[Paice1980] Paice, Chris D. 1980. The automatic generation of literature abstracts: An approach based on the
identification of self-indicating phrases. In 3rd annual ACM conference on Research and development in infor-
mation retrieval, pages 172?191, Cambridge, June. Butterworth and Co.
[Pardo and Nunes2004] Pardo, Thiago A. S. and Maria G. V. Nunes. 2004. Relac??oes Ret?oricas e seus Marcadores
Superficiais: An?alise de um Corpus de Textos Cient??ficos em Portugu?es do Brasil [Rhetorical relations and its
surface markers: an analysis of scientific texts corpus in Portuguese of Brazil]. Technical Report NILC-TR-04-
03.
[Pardo et al.2003] Pardo, Thiago A. S., Lucia H. M. Rino, and Maria G. V. Nunes. 2003. GistSumm: A summa-
rization tool based on a new extractive method. Computational Processing of the Portuguese Language, pages
196?196.
[Rino and Scott1996] Rino, Lucia H. M. and Donia R. Scott. 1996. A discourse model for gist preservation.
Advances in Artificial Intelligence, pages 131?140.
[Soricut and Marcu2003] Soricut, R. and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic
and lexical information. In 2003 Conference of the North American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, volume 1, pages 149?156. Association for Computational
Linguistics.
[Sporleder and Lascarides2007] Sporleder, Caroline and Alex Lascarides. 2007. Exploiting linguistic cues to clas-
sify rhetorical relations. In Recent Advances in Natural Language Processing, pages 532?539, Borovets, Bul-
garia, 27-29 September.
[Stede2008] Stede, Manfred, 2008. RST revisited: Disentangling nuclearity, pages 33?57. ?Subordination? versus
?coordination? in sentence and text. John Benjamins, Amsterdam and Philadelphia.
[Taboada and Mann2006] Taboada, Maite and Willian C. Mann. 2006. Rhetorical Structure Theory: looking back
and moving ahead. Discourse Studies, 8(3):423?459.
[Taboada2006] Taboada, Maite. 2006. Discourse markers as signals (or not) of rhetorical relations. Journal of
Pragmatics, 38(4):567?592.
[van Dijk1998] van Dijk, Teun A. 1998. Texto y contexto: sem?antica y pragm?atica del discurso. C
?atedra.
475
