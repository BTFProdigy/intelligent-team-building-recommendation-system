Proceedings of NAACL HLT 2007, Companion Volume, pages 177?180,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Language Modeling for Determiner Selection
Jenine Turner and Eugene Charniak
Department of Computer Science
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{jenine|ec}@cs.brown.edu
Abstract
We present a method for automatic deter-
miner selection, based on an existing lan-
guage model. We train on the Penn Tree-
bank and also use additional data from the
North American News Text Corpus. Our
results are a significant improvement over
previous best.
1 Introduction
Determiner placement (choosing if a noun phrase
needs a determiner, and if so, which one) is a
non-trivial problem in several language processing
tasks. While context beyond that of the current sen-
tence can sometimes be necessary, native speakers
of languages with determiners can select determin-
ers quite well for most NPs. Native speakers of lan-
guages without determiners have a much more diffi-
cult time.
Automating determiner selection is helpful in sev-
eral applications. A determiner selection program
can aid in Machine Translation of determiner-free
languages (by adding determiners after the text has
been translated), correct English text written by non-
native speakers (Lee, 2004), and choose determiners
for text generation programs.
Early work on determiner selection focuses on
rule-based systems (Gawronska, 1990; Murata and
Nagao, 1993; Bond and Ogura, 1994; Heine, 1998).
Knight and Chander (1994) use decision trees to
choose between the and a/an, ignoring NPs with no
determiner, and achieve 78% accuracy on their Wall
Street Journal corpus. (Deciding between a and an
is a trivial postprocessing step.)
Minnen et al (2000) use a memory-based learner
(Daelemans et al, 2000) to choose determiners of
base noun phrases. They choose between no deter-
miner (hencefore null), the, and a/an. They use syn-
tactic features (head of the NP, part-of-speech tag of
the head of the NP, functional tag of the head of the
NP, category of the constituent embedding the NP,
and functional tag of the constituent embedding the
NP), whether the head is a mass or count noun and
semantic classes of the head of the NP (Ikehara et
al., 1991). They report 83.58% accuracy.
In this paper, we use the Charniak language model
(Charniak, 2001) for determiner selection. Our ap-
proach significantly improves upon the work of Min-
nen et al (2000). We also use additional automat-
ically parsed data from the North American News
Text Corpus (Graff, 1995), further improving our re-
sults.
2 The Immediate-Head Parsing Model
The language model we use is described in (Char-
niak, 2001). It is based upon a parser that, for a
sentence s, tries to find the parse pi defined as:
argmaxpip(pi | s) = argmaxpip(pi, s) (1)
The parser can be turned into a language model p(s)
describing the probability distribution over all pos-
sible strings s in the language, by considering all
parses pi of s:
p(s) =
?
pi
p(pi, s) (2)
177
Here p(pi, s) is zero if the yield of pi 6= s.
The parsing model assigns a probability to a parse
pi by a top-down process. For each constituent c in
pi it first guesses the pre-terminal of c, t(c) (t for
?tag?), then the lexical head of c, h(c), and then the
expansion of c into further constituents e(c). Thus
the probability of a parse is given by the equation
p(pi) =
?
c?pi
p(t(c) | l(c),H(c))
? p(h(c) | t(c), l(c),H(c))
? p(e(c) | l(c), t(c), h(c),H(c))
where l(c) is the label of c (e.g., whether it is a noun
phrase NP, verb phrase, etc.) and H(c) is the rel-
evant history of c ? information outside c deemed
important in determining the probability in question.
H(c) approximately consists of the label, head, and
head-part-of-speech for the parent of c: m(c), i(c),
and u(c) respectively and also a secondary head
(e.g., in ?Monday Night Football? Monday would
be conditioned on both the head of the noun-phrase
?Football? and the secondary head ?Night?).
It is usually clear to which constituent we are re-
ferring and we omit the (c) in, e.g., h(c). In this no-
tation the above equation takes the following form:
p(pi) =
?
c?pi
p(t | l,m, u, i) ? p(h | t, l,m, u, i)
? p(e | l, t, h,m, u). (3)
Next we describe how we assign a probability to
the expansion e of a constituent. We break up a tra-
ditional probabilistic context-free grammar (PCFG)
rule into a left-hand side with a label l(c) drawn
from the non-terminal symbols of our grammar, and
a right-hand side that is a sequence of one or more
such symbols. For each expansion we distinguish
one of the right-hand side labels as the ?middle? or
?head? symbol M(c). M(c) is the constituent from
which the head lexical item h is obtained according
to deterministic rules that pick the head of a con-
stituent from among the heads of its children. To the
left of M is a sequence of one or more left labels
Li(c) including the special termination symbol ?,
which indicates that there are no more symbols to
the left. We do the same for the labels to the right,
Ri(c). Thus, an expansion e(c) looks like:
l ? ?Lm...L1MR1...Rn?. (4)
The expansion is generated first by guessing M ,
then in order L1 through Lm+1 (= ?), and then, R1
through Rn+1.
Let us turn to how this works in the case of de-
terminer recovery. Consider a noun-phrase, which,
missing a possible determiner, is simply ?FBI.? The
language model is interested in the probability of the
strings ?the FBI,? ?a/an FBI? and ?FBI.? The ver-
sion with the highest probability will dictate the de-
terminer, or lack thereof. So, consider (most of) the
probability calculation for the answer ?the FBI:?
p(NNP | H) ? p(FBI | NNP,H)
? p(det | FBI,NNP,H)
? p(? | det,FBI,NNP,H)
? p(the | det,FBI,NNP,H) (5)
Of these, the first two terms, the probability that
the head will be an NNP (a singular proper noun)
and the probability that it will be ?FBI?, are shared
by all three competitors, null, the, and a/an. These
terms can therefore be ignored when we only wish to
identify the competitor with the highest probability.
The next two probabilities state that the noun-phrase
contains a determiner to the left of ?FBI? and that
the determiner is the last constituent of the left-hand
side. The last of the probabilities states that the de-
terminer in question is the. Ignoring the first two
probabilities, the critical probabilities for ?the FBI?
are:
p(det | FBI,NNP,H)
? p(? | det,FBI,NNP,H)
? p(the | det,FBI,NNP,H) (6)
Conversely, to evaluate the probability of the noun-
phrase ?FBI? ? i.e., no determiner, we evaluate:
p(? | FBI,NNP,H) (7)
We ask the probability of the NP stopping immedi-
ately to the left of ?FBI.? For ?a/an FBI? we evalu-
ate:
p(det | FBI,NNP,H)
? p(? | det,FBI,NNP,H) (8)
? (p(a | det,FBI,NNP,H) +
p(an | det,FBI,NNP,H))
178
Test Data Method Accuracy
leave-one-out Minnen et al 83.58%
Language Model (LM) 86.74%
tenfold on development LM 84.72%
LM trained on WSJ + 3 million words of NANC 85.83%
LM trained on WSJ + 10 million words of NANC 86.36%
LM trained on WSJ + 20 million words of NANC 86.64%
tenfold on test LM trained on WSJ + 20 million words of NANC 86.63%
Table 1: Results of classification
This equation is very similar to Equation 6 (the
equation for ?the FBI?, except the term for the prob-
ability of the is replaced by the sum of the probabil-
ities for a and an.
To choose between null, the, or a/an, the language
model in effect constructs Equations 6, 7 and 8 and
we pick the one that has the highest probability.
2.1 Training the model
As with (Minnen et al, 2000), we train the lan-
guage model on the Penn Treebank (Marcus et al,
1993). As far as we know, language modeling
always improves with additional training data, so
we add data from the North American News Text
Corpus (NANC) (Graff, 1995) automatically parsed
with the Charniak parser (McClosky et al, 2006) to
train our language model on up to 20 million addi-
tional words.
3 Results and Discussion
The best results of Minnen et al (2000) are using
leave-one-out cross-validation. We also test our lan-
guage model using leave-one-out cross-validation
on the Penn Treebank (Marcus et al, 1993) (WSJ),
giving us 86.74% accuracy (see Table 1).
Leave-one-out cross-validation does not make
sense in this case. When choosing determiners, we
can train a language model on similar data, but not
on other NPs in the article. Therefore, for the rest
of our tests, we use tenfold cross-validation. The
difference between leave-one-out and tenfold cross-
validation is due to the co-occurrence of NPs within
an article. Church (2000) shows that a word appears
with much higher probability when seen elsewhere
in an article. Thus, a rare NP might be unseen in
tenfold cross-validation, but seen in leave-one-out.
For each of our sets in tenfold cross validation,
we use 80% of the Penn Treebank for training, 10%
for development, and 10% for testing. The divisions
occur at article boundaries. On our development set
with tenfold cross-validation, we get 84.72% accu-
racy using the language model (Table 1).
As expected, we achieve significant improvement
when adding NANC data over training on data from
the Penn Treebank alone (Table 1). With 20 mil-
lion additional words, we seem to be approaching
an upper bound on the language model features. We
obtain improvement despite the fact that the parses
were automatic, but there may have been errors in
determiner selection due to parsing error.
Table 2 gives ?error? examples. Some errors are
wrong (either grammatically or yielding a signifi-
cantly different interpretation), but some ?incorrect?
answers are reasonable possibilities. Furthermore,
even all the text of the article is not enough for clas-
sification at times. In particular note Example 5,
where unless you know whether IBM was the world
leader or simply one of the world leaders at the time
of the article, no additional context would help.
4 Conclusions and Future Work
With the Charniak (Charniak, 2001) language
model, our results exceed those of the previous best
(Minnen et al, 2000) on the determiner selection
task. This shows the benefits of the language model
features in determining the most grammatical deter-
miner to use in a noun phrase. Such a language
model looks at much of the structure in individual
sentences, but there may be additional features that
could improve performance. There is a high rate of
ambiguity for many of the misclassified sentences.
The success of using a state-of-the-art language
179
Guess Correct Sentence
the null (1) The computers were crude by today?s standards.
null the (2) In addition, the Apple II was an affordable $1,298.
(3) Highway officials insist the ornamental railings on older bridges aren?t strong enough
to prevent vehicles from crashing through.
a/an the (4) The new carrier can tote as many as four cups at once.
(5) IBM, the world leader in computers, didn?t offer its first PC
until August 1981 as many other companies entered the market.
the a/an (6) In addition, the Apple II was an affordable $1,298.
(7) ?The primary purpose of a railing is to contain a vehicle and not to provide
a scenic view,? says Jack White, a planner with the Indiana Highway Department.
a/an null (8) Crude as they were, these early PCs triggered explosive product development in
desktop models for the home and office.
Table 2: Examples of ?errors?
model in determiner selection also suggests that one
would be helpful in making other decisions in the
surface realization stage of text generation. This is
an avenue worth exploring.
Acknowledgements
This work was supported by NSF PIRE grant OISE-0530118.
We would also like to thank the BLLIP team for their comments.
References
Francis Bond and Kentaro Ogura. 1994. Countability
and number in Japanese-to-English machine transla-
tion. In 15th International Conference on Computa-
tional Linguistics, pages 32?38.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics. The Association for Computational Linguistics.
Kenneth Church. 2000. Empirical estimates of adap-
tation: The chance of Two Noriegas is closer to p/2
than p2. In Proceedings of COLING-2000.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2000. TiMBL: Tilburg memory
based learner, version 3.0, reference guide. ILK Tech-
nical Report ILK-0001, ILK, Tilburg University, The
Netherlands.
Barbara Gawronska. 1990. Translation great problem.
In Proceedings of the 13th International Conference
on Computational Linguistics.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Julia E. Heine. 1998. Definiteness predictions for
Japanese noun phrases. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, pages 519?525.
Satoru Ikehara, Satoship Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing - effects of new methods in ALT-J/E. In Third
Machine Translation Summit, pages 101?106.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
Twelfth National Conference on Artificial Intelligence,
pages 779?784.
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the 2004 NAACL Conference Student Re-
search Workshop, pages 195?200.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT-NAACL 2006.
Guido Minnen, Francis Bond, and Ann Copestake. 2000.
Memory-based learning for article generation. In Pro-
ceedings of the Fourth Conference on Computational
Natural Language Learning and of the Second Learn-
ing Language in Logic Workshop, pages 43?48.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In Fifth International Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion, pages 218?225.
180
Proceedings of the 43rd Annual Meeting of the ACL, pages 290?297,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Supervised and Unsupervised Learning for Sentence Compression
Jenine Turner and Eugene Charniak
Department of Computer Science
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{jenine|ec}@cs.brown.edu
Abstract
In Statistics-Based Summarization - Step
One: Sentence Compression, Knight and
Marcu (Knight and Marcu, 2000) (K&M)
present a noisy-channel model for sen-
tence compression. The main difficulty
in using this method is the lack of data;
Knight and Marcu use a corpus of 1035
training sentences. More data is not easily
available, so in addition to improving the
original K&M noisy-channel model, we
create unsupervised and semi-supervised
models of the task. Finally, we point out
problems with modeling the task in this
way. They suggest areas for future re-
search.
1 Introduction
Summarization in general, and sentence compres-
sion in particular, are popular topics. Knight and
Marcu (henceforth K&M) introduce the task of
statistical sentence compression in Statistics-Based
Summarization - Step One: Sentence Compression
(Knight and Marcu, 2000). The appeal of this prob-
lem is that it produces summarizations on a small
scale. It simplifies general compression problems,
such as text-to-abstract conversion, by eliminating
the need for coherency between sentences. The
model is further simplified by being constrained
to word deletion: no rearranging of words takes
place. Others have performed the sentence compres-
sion task using syntactic approaches to this problem
(Mani et al, 1999) (Zajic et al, 2004), but we fo-
cus exclusively on the K&M formulation. Though
the problem is simpler, it is still pertinent to cur-
rent needs; generation of captions for television and
audio scanning services for the blind (Grefenstette,
1998), as well as compressing chosen sentences for
headline generation (Angheluta et al, 2004) are ex-
amples of uses for sentence compression. In addi-
tion to simplifying the task, K&M?s noisy-channel
formulation is also appealing.
In the following sections, we discuss the K&M
noisy-channel model. We then present our cleaned
up, and slightly improved noisy-channel model. We
also develop unsupervised and semi-supervised (our
term for a combination of supervised and unsuper-
vised) methods of sentence compression with inspi-
ration from the K&M model, and create additional
constraints to improve the compressions. We con-
clude with the problems inherent in both models.
2 The Noisy-Channel Model
2.1 The K&M Model
The K&M probabilistic model, adapted from ma-
chine translation to this task, is the noisy-channel
model. In machine translation, one imagines that a
string was originally in English, but that someone
adds some noise to make it a foreign string. Analo-
gously, in the sentence compression model, the short
string is the original sentence and someone adds
noise, resulting in the longer sentence. Using this
framework, the end goal is, given a long sentence
l, to determine the short sentence s that maximizes
290
P (s | l). By Bayes Rule,
P (s | l) = P (l | s)P (s)P (l) (1)
The probability of the long sentence, P (l) can be ig-
nored when finding the maximum, because the long
sentence is the same in every case.
P (s) is the source model: the probability that s
is the original sentence. P (l | s) is the channel
model: the probability the long sentence is the ex-
panded version of the short. This framework in-
dependently models the grammaticality of s (with
P (s)) and whether s is a good compression of l
(P (l | s)).
The K&M model uses parse trees for the sen-
tences. These allow it to better determine the proba-
bility of the short sentence and to obtain alignments
from the training data. In the K&M model, the
sentence probability is determined by combining a
probabilistic context free grammar (PCFG) with a
word-bigram score. The joint rules used to create the
compressions are generated by aligning the nodes of
the short and long trees in the training data to deter-
mine expansion probabilities (P (l | s)).
Recall that the channel model tries to find the
probability of the long string with respect to the
short string. It obtains these probabilities by align-
ing nodes in the parsed parallel training corpus, and
counting the nodes that align as ?joint events.? For
example, there might be S ? NP VP PP in the long
sentence and S ? NP VP in the short sentence; we
count this as one joint event. Non-compressions,
where the long version is the same as the short, are
also counted. The expansion probability, as used in
the channel model, is given by
Pexpand(l | s) =
count(joint(l, s))
count(s) (2)
where count(joint(l, s)) is the count of alignments
of the long rule and the short. Many compressions
do not align exactly. Sometimes the parses do not
match, and sometimes there are deletions that are too
complex to be modeled in this way. In these cases
sentence pairs, or sections of them, are ignored.
The K&M model creates a packed parse forest of
all possible compressions that are grammatical with
respect to the Penn Treebank (Marcus et al, 1993).
Any compression given a zero expansion probability
according to the training data is instead assigned a
very small probability. A tree extractor (Langkilde,
2000) collects the short sentences with the highest
score for P (s | l).
2.2 Our Noisy-Channel Model
Our starting implementation is intended to follow
the K&M model fairly closely. We use the same
1067 pairs of sentences from the Ziff-Davis cor-
pus, with 32 used as testing and the rest as train-
ing. The main difference between their model and
ours is that instead of using the rather ad-hoc K&M
language model, we substitute the syntax-based lan-
guage model described in (Charniak, 2001).
We slightly modify the channel model equation to
be P (l | s) = Pexpand(l | s)Pdeleted, where Pdeleted
is the probability of adding the deleted subtrees back
into s to get l. We determine this probability also
using the Charniak language model.
We require an extra parameter to encourage com-
pression. We create a development corpus of 25 sen-
tences from the training data in order to adjust this
parameter. That we require a parameter to encourage
compression is odd as K&M required a parameter to
discourage compression, but we address this point in
the penultimate section.
Another difference is that we only generate short
versions for which we have rules. If we have never
before seen the long version, we leave it alone, and
in the rare case when we never see the long version
as an expansion of itself, we allow only the short
version. We do not use a packed tree structure, be-
cause we make far fewer sentences. Additionally,
as we are traversing the list of rules to compress the
sentences, we keep the list capped at the 100 com-
pressions with the highest Pexpand(l | s). We even-
tually truncate the list to the best 25, still based upon
Pexpand(l | s).
2.3 Special Rules
One difficulty in the use of training data is that so
many compressions cannot be modeled by our sim-
ple method. The rules it does model, immediate
constituent deletion, as in taking out the ADVP , of
S ? ADVP , NP VP ., are certainly common, but
many good deletions are more structurally compli-
cated. One particular type of rule, such as NP(1) ?
291
NP(2) CC NP(3), where the parent has at least one
child with the same label as itself, and the resulting
compression is one of the matching children, such
as, here, NP(2). There are several hundred rules of
this type, and it is very simple to incorporate into our
model.
There are other structures that may be common
enough to merit adding, but we limit this experiment
to the original rules and our new ?special rules.?
3 Unsupervised Compression
One of the biggest problems with this model of sen-
tence compression is the lack of appropriate train-
ing data. Typically, abstracts do not seem to con-
tain short sentences matching long ones elsewhere
in a paper, and we would prefer a much larger cor-
pus. Despite this lack of training data, very good
results were obtained both by the K&M model and
by our variant. We create a way to compress sen-
tences without parallel training data, while sticking
as closely to the K&M model as possible.
The source model stays the same, and we still
pay a probability cost in the channel model for ev-
ery subtree deleted. However, the way we determine
Pexpand(l | s) changes because we no longer have a
parallel text. We create joint rules using only the first
section (0.mrg) of the Penn Treebank. We count all
probabilistic context free grammar (PCFG) expan-
sions, and then match up similar rules as unsuper-
vised joint events.
We change Equation 2 to calculate Pexpand(s | l)
without parallel data. First, let us define svo (shorter
version of) to be: r1 svo r2 iff the righthand side of
r1 is a subsequence of the righthand side of r2. Then
define
Pexpand(l | s) =
count(l)
?
l?s.t. s svo l? count(l?)
(3)
This is best illustrated by a toy example. Consider
a corpus with just 7 rules: 3 instances of NP ? DT
JJ NN and 4 instances of NP ? DT NN.
P(NP ? DT JJ NN | NP ? DT JJ NN) = 1. To
determine this, you divide the count of NP ? DT JJ
NN = 3 by all the possible long versions of NP ?
DT JJ NN = 3.
P(NP ? DT JJ NN | NP ? DT NN) = 3/7. The
count of NP ? DT JJ NN = 3, and the possible long
versions of NP ? DT NN are itself (with count of 3)
and NP ? DT JJ NN (with count of 4), yielding a
sum of 7.
Finally, P(NP ? DT NN | NP ? DT NN) = 4/7.
The count of NP ? DT NN = 4, and since the short
(NP ? DT NN) is the same as above, the count of
the possible long versions is again 7.
In this way, we approximate Pexpand(l | s) with-
out parallel data.
Since some of these ?training? pairs are likely
to be fairly poor compressions, due to the artifi-
ciality of the construction, we restrict generation of
short sentences to not allow deletion of the head
of any subtree. None of the special rules are ap-
plied. Other than the above changes, the unsuper-
vised model matches our supervised version. As will
be shown, this rule is not constraining enough and
allows some poor compressions, but it is remarkable
that any sort of compression can be achieved with-
out training data. Later, we will describe additional
constraints that help even more.
4 Semi-Supervised Compression
Because the supervised version tends to do quite
well, and its main problem is that the model tends
to pick longer compressions than a human would,
it seems reasonable to incorporate the unsupervised
version into our supervised model, in the hope of
getting more rules to use. In generating new short
sentences, if we have compression probabilities in
the supervised version, we use those, including the
special rules. The only time we use an unsupervised
compression probability is when there is no super-
vised version of the unsupervised rule.
5 Additional Constraints
Even with the unsupervised constraint from section
3, the fact that we have artificially created our joint
rules gives us some fairly ungrammatical compres-
sions. Adding extra constraints improves our unsu-
pervised compressions, and gives us better perfor-
mance on the supervised version as well. We use a
program to label syntactic arguments with the roles
they are playing (Blaheta and Charniak, 2000), and
the rules for complement/adjunct distinction given
by (Collins, 1997) to never allow deletion of the
complement. Since many nodes that should not
292
be deleted are not labeled with their syntactic role,
we add another constraint that disallows deletion of
NPs.
6 Evaluation
As with Knight and Marcu?s (2000) original work,
we use the same 32 sentence pairs as our Test Cor-
pus, leaving us with 1035 training pairs. After ad-
justing the supervised weighting parameter, we fold
the development set back into the training data.
We presented four judges with nine compressed
versions of each of the 32 long sentences: A human-
generated short version, the K&M version, our first
supervised version, our supervised version with our
special rules, our supervised version with special
rules and additional constraints, our unsupervised
version, our supervised version with additional con-
straints, our semi-supervised version, and our semi-
supervised version with additional constraints. The
judges were asked to rate the sentences in two ways:
the grammaticality of the short sentences on a scale
from 1 to 5, and the importance of the short sen-
tence, or how well the compressed version retained
the important words from the original, also on a
scale from 1 to 5. The short sentences were ran-
domly shuffled across test cases.
The results in Table 1 show compression rates,
as well as average grammar and importance scores
across judges.
There are two main ideas to take away from these
results. First, we can get good compressions without
paired training data. Second, we achieved a good
boost by adding our additional constraints in two of
the three versions.
Note that importance is a somewhat arbitrary dis-
tinction, since according to our judges, all of the
computer-generated versions do as well in impor-
tance as the human-generated versions.
6.1 Examples of Results
In Figure 1, we give four examples of most compres-
sion techniques in order to show the range of perfor-
mance that each technique spans. In the first two ex-
amples, we give only the versions with constraints,
because there is little or no difference between the
versions with and without constraints.
Example 1 shows the additional compression ob-
tained by using our special rules. Figure 2 shows
the parse trees of the original pair of short and long
versions. The relevant expansion is NP ? NP1 ,
PP in the long version and simply NP1 in the short
version. The supervised version that includes the
special rules learned this particular common special
joint rule from the training data and could apply it
to the example case. This supervised version com-
presses better than either version of the supervised
noisy-channel model that lacks these rules. The un-
supervised version does not compress at all, whereas
the semi-supervised version is identical with the bet-
ter supervised version.
Example 2 shows how unsupervised and semi-
supervised techniques can be used to improve com-
pression. Although the final length of the sentences
is roughly the same, the unsupervised and semi-
supervised versions are able to take the action of
deleting the parenthetical. Deleting parentheses was
never seen in the training data, so it would be ex-
tremely unlikely to occur in this case. The unsuper-
vised version, on the other hand, sees both PRN ?
lrb NP rrb and PRN ? NP in its training data, and
the semi-supervised version capitalizes on this par-
ticular unsupervised rule.
Example 3 shows an instance of our initial super-
vised versions performing far worse than the K&M
model. The reason is that currently our supervised
model only generates compressions that it has seen
before, unlike the K&M model, which generates all
possible compressions. S ? S , NP VP . never occurs
in the training data, and so a good compression does
not exist. The unsupervised and semi-supervised
versions do better in this case, and the supervised
version with the added constraints does even better.
Example 4 gives an example of the K&M model
being outperformed by all of our other models.
7 Problems with Noisy Channel Models of
Sentence Compression
To this point our presentation has been rather nor-
mal; we draw inspiration from a previous paper, and
work at improving on it in various ways. We now
deviate from the usual by claiming that while the
K&M model works very well, there is a technical
problem with formulating the task in this way.
We start by making our noisy channel notation a
293
original: Many debugging features, including user-defined break points and
variable-watching and message-watching windows, have been added.
human: Many debugging features have been added.
K&M: Many debugging features, including user-defined points and
variable-watching and message-watching windows, have been added.
supervised: Many features, including user-defined break points and variable-watching
and windows, have been added.
super (+ extra rules, constraints): Many debugging features have been added.
unsuper (+ constraints): Many debugging features, including user-defined break
points and variable-watching and message-watching windows, have been added.
semi-supervised (+ constraints): Many debugging features have been added.
original: Also, Trackstar supports only the critical path method (CPM) of project
scheduling.
human: Trackstar supports the critical path method of project scheduling.
K&M: Trackstar supports only the critical path method (CPM) of scheduling.
supervised: Trackstar supports only the critical path method (CPM) of scheduling.
super (+ extra rules, constraints): Trackstar supports only the critical path method (CPM) of scheduling.
unsuper (+ constraints): Trackstar supports only the critical path method of project scheduling.
semi-supervised (+ constraints): Trackstar supports only the critical path method of project scheduling.
original: The faster transfer rate is made possible by an MTI-proprietary data
buffering algorithm that off-loads lock-manager functions from the Q-bus
host, Raimondi said.
human: The algorithm off-loads lock-manager functions from the Q-bus host.
K&M: The faster rate is made possible by a MTI-proprietary data buffering algorithm
that off-loads lock-manager functions from the Q-bus host, Raimondi said.
supervised: Raimondi said.
super (+ extra rules): Raimondi said.
super (+ extra rules, constraints): The faster transfer rate is made possible by an MTI-proprietary data buffering
algorithm, Raimondi said.
unsuper (+ constraints): The faster transfer rate is made possible, Raimondi said.
semi-supervised (+ constraints): The faster transfer rate is made possible, Raimondi said.
original: The SAS screen is divided into three sections: one for writing programs, one for
the system?s response as it executes the program, and a third for output tables
and charts.
human: The SAS screen is divided into three sections.
K&M: The screen is divided into one
super (+ extra rules): SAS screen is divided into three sections: one for writing programs, and a third
for output tables and charts.
super (+ extra rules, constraints): The SAS screen is divided into three sections.
unsupervised: The screen is divided into sections: one for writing programs, one for the system?s
response as it executes program, and third for output tables and charts.
unsupervised (+ constraints): Screen is divided into three sections: one for writing programs, one for the
system?s response as it executes program, and a third for output tables and charts.
semi-supervised: The SAS screen is divided into three sections: one for writing programs, one for
the system?s response as it executes the program, and a third for output tables
and charts.
semi-super (+ constraints): The screen is divided into three sections: one for writing programs, one for the
system?s response as it executes the program, and a third for output tables
and charts.
Figure 1: Compression Examples294
compression rate grammar importance
humans 53.33% 4.96 3.73
K&M 70.37% 4.57 3.85
supervised 79.85% 4.64 3.97
supervised with extra rules 67.41% 4.57 3.66
supervised with extra rules and constraints 68.44% 4.77 3.76
unsupervised 79.11% 4.38 3.93
unsupervised with constraints 77.93% 4.51 3.88
semi-supervised 81.19% 4.79 4.18
semi-supervised with constraints 79.56% 4.75 4.16
Table 1: Experimental Results
short: (S (NP (JJ Many) (JJ debugging) (NNS features))
(VP (VBP have) (VP (VBN been) (VP (VBN added))))(. .))
long: (S (NP (NP (JJ Many) (JJ debugging) (NNS features))(, ,)
(PP (VBG including) (NP (NP (JJ user-defined)(NN break)(NNS points)
(CC and)(NN variable-watching))
(CC and)(NP (JJ message-watching) (NNS windows))))(, ,))
(VP (VBP have) (VP (VBN been) (VP (VBN added))))(. .))
Figure 2: Joint Trees for special rules
bit more explicit:
argmaxsp(s, L = s | l, L = l) = (4)
argmaxsp(s, L = s)p(l, L = l | s, L = s)
Here we have introduced explicit conditioning
events L = l and L = s to state that that the sen-
tence in question is either the long version or the
short version. We do this because in order to get the
equation that K&M (and ourselves) start with, it is
necessary to assume the following
p(s, L = s) = p(s) (5)
p(l, L = l | s, L = s) = p(l | s) (6)
This means we assume that the probability of, say, s
as a short (compressed) sentence is simply its prob-
ability as a sentence. This will be, in general, false.
One would hope that real compressed sentences are
more probable as a member of the set of compressed
sentences than they are as simply a member of all
English sentences. However, neither K&M, nor we,
have a large enough body of compressed and origi-
nal sentences from which to create useful language
models, so we both make this simplifying assump-
tion. At this point it seems like a reasonable choice
root
vp
vb
buy
np
nns
toys
root
vp
vb
buy
np
jj
large
nns
toys
Figure 3: A compression example ? trees A and B
respectively
to make. In fact, it compromises the entire enter-
prise. To see this, however, we must descend into
more details.
Let us consider a simplified version of a K&M
example, but as reinterpreted for our model: how
the noisy channel model assigns a probability of the
compressed tree (A) in Figure 3 given the original
tree B.
We compute the probabilities p(A) and p(B | A)
as follows (Figure 4): We have divided the probabil-
ities up according to whether they are contributed by
the source or channel models. Those from the source
295
p(A) p(B | A)
p(s ? vp | H(s)) p(s ? vp | s ? vp)
p(vp ? vb np | H(vp)) p(vp ? vb np | vp ? vb np)
p(np ? nns | H(np)) p(np ? jj nns | np ? nns)
p(vb ? buy | H(vb)) p(vb ? buy | vb ? buy)
p(nns ? toys | H(nns)) p(nns ? toys | nns ? toys)
p(jj ? large | H(jj))
Figure 4: Source and channel probabilities for com-
pressing B into A
p(B) p(B | B)
p(s ? vp | H(s)) p(s ? vp | s ? vp)
p(vp ? vb np | H(vp)) p(vp ? vb np | vp ? vb np)
p(np ? jj nns | H(np)) p(np ? jj nns | np ? jj nns)
p(vb ? buy | H(vb)) p(vb ? buy | vb ? buy)
p(nns ? toys | H(nns)) p(nns ? toys | nns ? toys)
p(jj ? large | H(jj)) p(jj ? large | jj ? large)
Figure 5: Source and channel probabilities for leav-
ing B as B
model are conditioned on, e.g. H(np) the history in
terms of the tree structure around the noun-phrase.
In a pure PCFG this would only include the label of
the node. In our language model it includes much
more, such as parent and grandparent heads.
Again, following K&M, contrast this with the
probabilities assigned when the compressed tree is
identical to the original (Figure 5).
Expressed like this it is somewhat daunting, but
notice that if all we want is to see which probability
is higher (the compressed being the same as the orig-
inal or truly compressed) then most of these terms
cancel, and we get the rule, prefer the truly com-
pressed if and only if the following ratio is greater
than one.
p(np ? nns | H(np))
p(np ? jj nns | H(np))
p(np ? jj nns | np ? nns)
p(np ? jj nns | np ? jj nns) (7)
1
p(jj ? large | jj ? large)
In the numerator are the unmatched probabilities
that go into the compressed sentence noisy chan-
nel probability, and in the denominator are those for
when the sentence does not undergo any change. We
can make this even simpler by noting that because
tree-bank pre-terminals can only expand into words
p(jj ? large | jj ? large) = 1. Thus the last fraction
in Equation 7 is equal to one and can be ignored.
For a compression to occur, it needs to be less de-
sirable to add an adjective in the channel model than
in the source model. In fact, the opposite occurs.
The likelihood of almost any constituent deletion is
far lower than the probability of the constituents all
being left in. This seems surprising, considering that
the model we are using has had some success, but
it makes intuitive sense. There are far fewer com-
pression alignments than total alignments: identical
parts of sentences are almost sure to align. So the
most probable short sentence should be very barely
compressed. Thus we add a weighting factor to
compress our supervised version further.
K&M also, in effect, weight shorter sentences
more strongly than longer ones based upon their lan-
guage model. In their papers on sentence compres-
sion, they give an example similar to our ?buy large
toys? example. The equation they get for the channel
probabilities in their example is similar to the chan-
nel probabilities we give in Figures 3 and 4. How-
ever their source probabilities are different. K&M
did not have a true syntax-based language model
to use as we have. Thus they divided the language
model into two parts. Part one assigns probabilities
to the grammar rules using a probabilistic context-
free grammar, while part two assigns probabilities
to the words using a bi-gram model. As they ac-
knowledge in (Knight and Marcu, 2002), the word
bigram probabilities are also included in the PCFG
probabilities. So in their versions of Figures 3 and
4 they have both p(toys | nns) (from the PCFG)
and p(toys | buy) for the bigram probability. In
this model, the probabilities do not sum to one, be-
cause they pay the probabilistic price for guessing
the word ?toys? twice, based upon two different con-
ditioning events. Based upon this language model,
they prefer shorter sentences.
To reiterate this section?s argument: A noisy
channel model is not by itself an appropriate model
for sentence compression. In fact, the most likely
short sentence will, in general, be the same length
as the long sentence. We achieve compression by
weighting to give shorter sentences more likelihood.
In fact, what is really required is some model that
takes ?utility? into account, using a utility model
296
in which shorter sentences are more useful. Our
term giving preference to shorter sentences can be
thought of as a crude approximation to such a utility.
However, this is clearly an area for future research.
8 Conclusion
We have created a supervised version of the noisy-
channel model with some improvements over the
K&M model. In particular, we learned that adding
an additional rule type improved compression, and
that enforcing some deletion constraints improves
grammaticality. We also show that it is possible to
perform an unsupervised version of the compression
task, which performs remarkably well. Our semi-
supervised version, which we hoped would have
good compression rates and grammaticality, had
good grammaticality but lower compression than de-
sired.
We would like to come up with a better utility
function than a simple weighting parameter for our
supervised version. The unsupervised version prob-
ably can also be further improved. We achieved
much success using syntactic labels to constrain
compressions, and there are surely other constraints
that can be added.
However, more training data is always the easi-
est cure to statistical problems. If we can find much
larger quantities of training data we could allow for
much richer rule paradigms that relate compressed
to original sentences. One example of a rule we
would like to automatically discover would allow us
to compress all of our design goals or
(NP (NP (DT all))
(PP (IN of)
(NP (PRP$ our) (NN design) (NNS goals))))}
to all design goals or
(NP (DT all) (NN design) (NNS goals))
In the limit such rules blur the distinction between
compression and paraphrase.
9 Acknowledgements
This work was supported by NSF grant IIS-
0112435. We would like to thank Kevin Knight
and Daniel Marcu for their clarification and test sen-
tences, and Mark Johnson for his comments.
References
Roxana Angheluta, Rudradeb Mitra, Xiuli Jing, and
Francine-Marie Moens. 2004. K.U.Leuven summa-
rization system at DUC 2004. In Document Under-
standing Conference.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In The Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics, pages 234?240.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics. The Association for Computational Linguistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics, San Francisco. Morgan Kauf-
mann.
Gregory Grefenstette. 1998. Producing intelligent tele-
graphic text reduction to provide an audio scanning
service for the blind. In Working Notes of the AAAI
Spring Symposium on Intelligent Text Summarization,
pages 111?118.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: sentence compression. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence, pages 703?71.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. In Artificial Intelli-
gence, 139(1): 91-107.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st Annual Meeting
of the North American Chapter of the Association for
Computationl Linguistics.
Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999.
Improving summaries by revising them. In The Pro-
ceedings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics. The Association
for Computational Linguistics.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David Zajic, Bonnie Dorr, and Richard Schwartz. 2004.
BBN/UMD at DUC 2004: Topiary. In Document Un-
derstanding Conference.
297
