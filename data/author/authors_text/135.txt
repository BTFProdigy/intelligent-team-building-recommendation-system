Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 1?8, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving LSA-based Summarization with Anaphora Resolution
Josef Steinberger
University of West Bohemia
Univerzitni 22, Pilsen 30614,
Czech Republic
jstein@kiv.zcu.cz
Mijail A. Kabadjov
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
malexa@essex.ac.uk
Massimo Poesio
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
poesio@essex.ac.uk
Olivia Sanchez-Graillet
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
osanch@essex.ac.uk
Abstract
We propose an approach to summarization
exploiting both lexical information and
the output of an automatic anaphoric re-
solver, and using Singular Value Decom-
position (SVD) to identify the main terms.
We demonstrate that adding anaphoric
information results in significant perfor-
mance improvements over a previously
developed system, in which only lexical
terms are used as the input to SVD. How-
ever, we also show that how anaphoric in-
formation is used is crucial: whereas using
this information to add new terms does re-
sult in improved performance, simple sub-
stitution makes the performance worse.
1 Introduction
Many approaches to summarization can be very
broadly characterized as TERM-BASED: they at-
tempt to identify the main ?topics,? which gen-
erally are TERMS, and then to extract from the
document the most important information about
these terms (Hovy and Lin, 1997). These ap-
proaches can be divided again very broadly in ?lex-
ical? approaches, among which we would include
LSA-based approaches, and ?coreference-based? ap-
proaches . Lexical approaches to term-based sum-
marization use lexical relations to identify cen-
tral terms (Barzilay and Elhadad, 1997; Gong and
Liu, 2002); coreference- (or anaphora-) based ap-
proaches (Baldwin and Morton, 1998; Boguraev and
Kennedy, 1999; Azzam et al, 1999; Bergler et al,
2003; Stuckardt, 2003) identify these terms by run-
ning a coreference- or anaphoric resolver over the
text.1 We are not aware, however, of any attempt to
use both lexical and anaphoric information to iden-
tify the main terms. In addition, to our knowledge no
authors have convincingly demonstrated that feed-
ing anaphoric information to a summarizer signif-
icantly improves the performance of a summarizer
using a standard evaluation procedure (a reference
corpus and baseline, and widely accepted evaluation
measures).
In this paper we compare two sentence extraction-
based summarizers. Both use Latent Semantic
Analysis (LSA) (Landauer, 1997) to identify the
main terms of a text for summarization; however,
the first system (Steinberger and Jezek, 2004), dis-
cussed in Section 2, only uses lexical information
to identify the main topics, whereas the second sys-
tem exploits both lexical and anaphoric information.
This second system uses an existing anaphora reso-
lution system to resolve anaphoric expressions, GUI-
TAR (Poesio and Kabadjov, 2004); but, crucially,
two different ways of using this information for
summarization were tested. (Section 3.) Both sum-
marizers were tested over the CAST corpus (Orasan
et al, 2003), as discussed in Section 4, and sig-
1The terms ?anaphora resolution? and ?coreference resolu-
tion? have been variously defined (Stuckardt, 2003), but the lat-
ter term is generally used to refer to the coreference task as de-
fined in MUC and ACE. We use the term ?anaphora resolution? to
refer to the task of identifying successive mentions of the same
discourse entity, realized via any type of noun phrase (proper
noun, definite description, or pronoun), and whether such dis-
course entities ?refer? to objects in the world or not.
1
nificant improvements were observed over both the
baseline CAST system and our previous LSA-based
summarizer.
2 An LSA-based Summarizer Using
Lexical Information Only
LSA (Landauer, 1997) is a technique for extracting
the ?hidden? dimensions of the semantic representa-
tion of terms, sentences, or documents, on the basis
of their contextual use. It is a very powerful tech-
nique already used for NLP applications such as in-
formation retrieval (Berry et al, 1995) and text seg-
mentation (Choi et al, 2001) and, more recently,
multi- and single-document summarization.
The approach to using LSA in text summariza-
tion we followed in this paper was proposed in
(Gong and Liu, 2002). Gong and Liu propose to
start by creating a term by sentences matrix A =
[A1, A2, . . . , An], where each column vector Ai rep-
resents the weighted term-frequency vector of sen-
tence i in the document under consideration. If there
are a total of m terms and n sentences in the docu-
ment, then we will have an m ? n matrix A for the
document. The next step is to apply Singular Value
Decomposition (SVD) to matrix A. Given an m? n
matrix A, the SVD of A is defined as:
(1) A = U?V T
where U = [uij ] is an m ? n column-orthonormal
matrix whose columns are called left singular vec-
tors, ? = diag(?1, ?2, . . . , ?n) is an n ? n di-
agonal matrix, whose diagonal elements are non-
negative singular values sorted in descending order,
and V = [vij ] is an n?n orthonormal matrix, whose
columns are called right singular vectors.
From a mathematical point of view, applying
SVD to a matrix derives a mapping between the m-
dimensional space spawned by the weighted term-
frequency vectors and the r-dimensional singular
vector space. From a NLP perspective, what the SVD
does is to derive the latent semantic structure of the
document represented by matrix A: a breakdown
of the original document into r linearly-independent
base vectors (?topics?). Each term and sentence from
the document is jointly indexed by these ?topics?.
A unique SVD feature is that it is capable of cap-
turing and modelling interrelationships among terms
so that it can semantically cluster terms and sen-
tences. Furthermore, as demonstrated in (Berry et
al., 1995), if a word combination pattern is salient
and recurring in document, this pattern will be cap-
tured and represented by one of the singular vec-
tors. The magnitude of the corresponding singular
value indicates the importance degree of this pattern
within the document. Any sentences containing this
word combination pattern will be projected along
this singular vector, and the sentence that best repre-
sents this pattern will have the largest index value
with this vector. As each particular word combi-
nation pattern describes a certain topic in the doc-
ument, each singular vector can be viewed as repre-
senting a salient topic of the document, and the mag-
nitude of its corresponding singular value represents
the degree of importance of the salient topic.
The summarization method proposed by Gong
and Liu (2002) should now be easy to understand.
The matrix V T describes the importance degree of
each ?implicit topic? in each sentence: the summa-
rization process simply chooses the most informa-
tive sentence for each term. In other words, the kth
sentence chosen is the one with the largest index
value in the kth right singular vector in matrix V T .
The summarization method proposed by Gong
and Liu has some disadvantages as well, the main of
which is that it is necessary to use the same number
of dimensions as is the number of sentences we want
to choose for a summary. However, the higher the
number of dimensions of reduced space is, the less
significant topic we take into a summary. In order
to remedy this problem, we (Steinberger and Jezek,
2004) proposed the following modifications to Gong
and Liu?s summarization method. After computing
the SVD of a term by sentences matrix, we compute
the length of each sentence vector in matrix V . This
is to favour the index values in the matrix V that
correspond to the highest singular values (the most
significant topics). Formally:
(2) sk =
?
?r
i=1 v2k,i ? ?2i ,
where sk is the length of the vector of k?th sentence
in the modified latent vector space, and its signif-
icance score for summarization too. The level of
dimensionality reduction (r) is essentially learned
from the data. Finally, we put into the summary the
sentences with the highest values in vector s. We
showed in previous work (Steinberger and Jezek,
2
2004) that this modification results in a significant
improvement over Gong and Liu?s method.
3 Using Anaphora Resolution for
Summarization
3.1 The case for anaphora resolution
Words are the most basic type of ?term? that can
be used to characterize the content of a document.
However, being able to identify the most important
objects mentioned in the document clearly would
lead to an improved analysis of what is important in
a text, as shown by the following news article cited
by Boguraev and Kennedy (1999):
(3) PRIEST IS CHARGED WITH POPE ATTACK
A Spanish priest was charged here today with attempt-
ing to murder the Pope. Juan Fernandez Krohn, aged
32, was arrested after a man armed with a bayonet ap-
proached the Pope while he was saying prayers at Fa-
tima on Wednesday night. According to the police, Fer-
nandez told the investigators today that he trained for
the past six months for the assault. . . . If found guilty,
the Spaniard faces a prison sentence of 15-20 years.
As Boguraev and Kennedy point out, the title of the
article is an excellent summary of the content: an en-
tity (the priest) did something to another entity (the
pope). Intuitively, understanding that Fernandez and
the pope are the central characters is crucial to pro-
vide a good summary of texts like these.2 Among
the clues that help us to identify such ?main charac-
ters?, the fact that an entity is repeatedly mentioned
is clearly important.
Purely lexical methods, including the LSA-based
methods discussed in the previous section, can only
capture part of the information about which enti-
ties are frequently repeated in the text. As exam-
ple (3) shows, stylistic conventions forbid verbatim
repetition, hence the six mentions of Fernandez in
the text above contain only one lexical repetition,
?Fernandez?. The main problem are pronouns, that
tend to share the least lexical similarity with the
form used to express the antecedent (and anyway are
usually removed by stopword lists, therefore do not
2It should be noted that for many newspaper articles, indeed
many non-educational texts, only a ?entity-centered? structure
can be clearly identified, as opposed to a ?relation-centered?
structure of the type hypothesized in Rhetorical Structures The-
ory (Knott et al, 2001; Poesio et al, 2004).
get included in the SVD matrix). The form of defi-
nite descriptions (the Spaniard) doesn?t always over-
lap with that of their antecedent, either, especially
when the antecedent was expressed with a proper
name. The form of mention which more often over-
laps to a degree with previous mentions is proper
nouns, and even then at least some way of dealing
with acronyms is necessary (cfr. European Union
/ E.U.). The motivation for anaphora resolution is
that it should tell us which entities are repeatedly
mentioned.
In this work, we tested a mixed approach to in-
tegrate anaphoric and word information: using the
output of the anaphoric resolver GUITAR to modify
the SVD matrix used to determine the sentences to
extract. In the rest of this section we first briefly in-
troduce GUITAR, then discuss the two methods we
tested to use its output to help summarization.
3.2 GUITAR: A General-Purpose Anaphoric
Resolver
The system we used in these experiments, GUITAR
(Poesio and Kabadjov, 2004), is an anaphora resolu-
tion system designed to be high precision, modular,
and usable as an off-the-shelf component of a NL
processing pipeline. The current version of the sys-
tem includes an implementation of the MARS pro-
noun resolution algorithm (Mitkov, 1998) and a par-
tial implementation of the algorithm for resolving
definite descriptions proposed by Vieira and Poe-
sio (2000). The current version of GUITAR does not
include methods for resolving proper nouns.
3.2.1 Pronoun Resolution
Mitkov (1998) developed a robust approach to
pronoun resolution which only requires input text
to be part-of-speech tagged and noun phrases to be
identified. Mitkov?s algorithm operates on the ba-
sis of antecedent-tracking preferences (referred to
hereafter as ?antecedent indicators?). The approach
works as follows: the system identifies the noun
phrases which precede the anaphor within a distance
of 2 sentences, checks them for gender and number
agreement with the anaphor, and then applies genre-
specific antecedent indicators to the remaining can-
didates (Mitkov, 1998). The noun phrase with the
highest aggregate score is proposed as antecedent.
3
3.2.2 Definite Description Resolution
The Vieira / Poesio algorithm (Vieira and Poesio,
2000) attempts to classify each definite description
as either direct anaphora, discourse-new, or bridg-
ing description. The first class includes definite de-
scriptions whose head is identical to that of their an-
tecedent, as in a house . . . the house. Discourse-
new descriptions are definite descriptions that refer
to objects not already mentioned in the text and not
related to any such object. Bridging descriptions are
all definite descriptions whose resolution depends
on knowledge of relations between objects, such as
definite descriptions that refer to an object related
to an entity already introduced in the discourse by
a relation other than identity, as in the flat . . . the
living room. The Vieira / Poesio algorithm also at-
tempts to identify the antecedents of anaphoric de-
scriptions and the anchors of bridging ones. The
current version of GUITAR incorporates an algorithm
for resolving direct anaphora derived quite directly
from Vieira / Poesio, as well as a statistical version
of the methods for detecting discourse new descrip-
tions (Poesio et al, 2005).
3.3 SVD over Lexical and Anaphoric Terms
SVD can be used to identify the ?implicit topics? or
main terms of a document not only when on the basis
of words, but also of coreference chains, or a mix-
ture of both. We tested two ways of combining these
two types of information.
3.3.1 The Substitution Method
The simplest way of integrating anaphoric in-
formation with the methods used in our earlier
work is to use anaphora resolution simply as a pre-
processing stage of the SVD input matrix creation.
Firstly, all anaphoric relations are identified by the
anaphoric resolver, and anaphoric chains are identi-
fied. Then a second document is produced, in which
all anaphoric nominal expressions are replaced by
the first element of their anaphoric chain. For exam-
ple, suppose we have the text in (4).
(4) S1: Australia?s new conservative government on
Wednesday began selling its tough deficit-slashing bud-
get, which sparked violent protests by Aborigines,
unions, students and welfare groups even before it was
announced.
S2: Two days of anti-budget street protests preceded
spending cuts officially unveiled by Treasurer Peter
Costello.
S3: ?If we don?t do it now, Australia is going to be in
deficit and debt into the next century.?
S4: As the protesters had feared, Costello revealed a
cut to the government?s Aboriginal welfare commission
among the hundreds of measures implemented to claw
back the deficit.
An ideal resolver would find 8 anaphoric chains:
Chain 1 Australia - we - Australia
Chain 2 its new conservative government (Australia?s new
conservative government) - the government
Chain 3 its tough deficit-slashing budget (Australia?s tough
deficit-slashing budget) - it
Chain 4 violent protests by Aborigines, unions, students and
welfare groups - anti-budget street protests
Chain 5 Aborigines, unions, students and welfare groups - the
protesters
Chain 6 spending cuts - it - the hundreds of measures imple-
mented to claw back the deficit
Chain 7 Treasurer Peter Costello - Costello
Chain 8 deficit - the deficit
By replacing each element of the 8 chains above
in the text in (4) with the first element of the chain,
we get the text in (5).
(5) S1: Australia?s new conservative government on
Wednesday began selling Australia?s tough deficit-
slashing budget, which sparked violent protests by Abo-
rigines, unions, students and welfare groups even be-
fore Australia?s tough deficit-slashing budget was an-
nounced.
S2: Two days of violent protests by Aborigines, unions,
students and welfare groups preceded spending cuts of-
ficially unveiled by Treasurer Peter Costello.
S3: ?If Australia doesn?t do spending cuts now, Aus-
tralia is going to be in deficit and debt into the next
century.?
S4: As Aborigines, unions, students and welfare
groups had feared, Treasurer Peter Costello revealed a
cut to Australia?s new conservative government?s Abo-
riginal welfare commission among the spending cuts.
This text is then used to create the SVD input matrix,
as done in the first system.
3.3.2 The Addition Method
An alternative approach is to use SVD to identify
?topics? on the basis of two types of ?terms?: terms in
the lexical sense (i.e., words) and terms in the sense
of objects, which can be represented by anaphoric
4
chains. In other words, our representation of sen-
tences would specify not only if they contain a cer-
tain word, but also if they contain a mention of a
discourse entity (See Figure 1.) This matrix would
then be used as input to SVD.
Figure 1: Addition method.
The chain ?terms? tie together sentences that con-
tain the same anaphoric chain. If the terms are
lexically the same (direct anaphors - like deficit
and the deficit) the basic summarizer works suffi-
ciently. However, Gong and Liu showed that the best
weighting scheme is boolean (i.e., all terms have the
same weight); our own previous results confirmed
this. The advantage of the addition method is the
opportunity to give higher weights to anaphors.
4 Evaluation
4.1 The CAST Corpus
To evaluate our system, we used the corpus of
manually produced summaries created by the CAST
project3 (Orasan et al, 2003). The CAST cor-
pus contains news articles taken from the Reuters
Corpus and a few popular science texts from the
British National Corpus. It contains information
about the importance of the sentences (Hasler et
al., 2003). Sentences are marked as essential or im-
portant. The corpus also contains annotations for
3The goal of this project was to investigate to what extent
Computer-Aided Summarization can help humans to produce
high quality summaries with less effort.
linked sentences, which are not significant enough
to be marked as important/essential, but which have
to be considered as they contain information essen-
tial for the understanding of the content of other sen-
tences marked as essential/important.
Four annotators were used for the annotation,
three graduate students and one postgraduate. Three
of the annotators were native English speakers, and
the fourth had advanced knowledge of English. Un-
fortunately, not all of the documents were annotated
by all of the annotators. To maximize the reliability
of the summaries used for evaluation, we chose the
documents annotated by the greatest number of the
annotators; in total, our evaluation corpus contained
37 documents.
For acquiring manual summaries at specified
lengths and getting the sentence scores (for relative
utility evaluation) we assigned a score 3 to the sen-
tences marked as essential, a score 2 to important
sentences and a score 1 to linked sentences. The
sentences with highest scores are then selected for
ideal summary (at specified lenght).
4.2 Evaluation Measures
Evaluating summarization is a notoriously hard
problem, for which standard measures like Preci-
sion and Recall are not very appropriate. The main
problem with P&R is that human judges often dis-
agree what are the top n% most important sentences
in a document. Using P&R creates the possibility
that two equally good extracts are judged very dif-
ferently. Suppose that a manual summary contains
sentences [1 2] from a document. Suppose also that
two systems, A and B, produce summaries consist-
ing of sentences [1 2] and [1 3], respectively. Us-
ing P&R, system A will be ranked much higher than
system B. It is quite possible that sentences 2 and 3
are equally important, in which case the two systems
should get the same score.
To address the problem with precision and recall
we used a combination of evaluation measures. The
first of these, relative utility (RU) (Radev et al,
2000) allows model summaries to consist of sen-
tences with variable ranking. With RU, the model
summary represents all sentences of the input doc-
ument with confidence values for their inclusion in
the summary. For example, a document with five
sentences [1 2 3 4 5] is represented as [1/5 2/4 3/4
5
Evaluation Lexical LSA Manual Manual
Method Substitution Additition
Relative Utility 0.595 0.573 0.662
F-score 0.420 0.410 0.489
Cosine Similarity 0.774 0.806 0.823
Main Topic Similarity 0.686 0.682 0.747
Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.
Evaluation Lexical LSA Manual Manual
Method Substitution Addition
Relative Utility 0.645 0.662 0.688
F-score 0.557 0.549 0.583
Cosine Similarity 0.863 0.878 0.886
Main Topic Similarity 0.836 0.829 0.866
Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.
4/1 5/2]. The second number in each pair indicates
the degree to which the given sentence should be
part of the summary according to a human judge.
This number is called the utility of the sentence.
Utility depends on the input document, the summary
length, and the judge. In the example, the system
that selects sentences [1 2] will not get a higher score
than a system that chooses sentences [1 3] given
that both summaries [1 2] and [1 3] carry the same
number of utility points (5+4). Given that no other
combination of two sentences carries a higher util-
ity, both systems [1 2] and [1 3] produce optimal
extracts. To compute relative utility, a number of
judges, (N ? 1) are asked to assign utility scores to
all n sentences in a document. The top e sentences
according to utility score4 are then called a sentence
extract of size e. We can then define the following
system performance metric:
(6) RU =
?n
j=1 ?j
?N
i=1 uij
?n
j=1 ?j
?N
i=1 uij
,
where uij is a utility score of sentence j from anno-
tator i, ?j is 1 for the top e sentences according to the
sum of utility scores from all judges and ?j is equal
to 1 for the top e sentences extracted by the system.
For details see (Radev et al, 2000).
The second measure we used is Cosine Similarity,
according to the standard formula:
(7) cos(X,Y ) =
?
i xi?yi
?
?
i(xi)2?
?
?
i(yi)2
,
4In the case of ties, some arbitrary but consistent mecha-
nism is used to decide which sentences should be included in
the summary.
where X and Y are representations of a system sum-
mary and its reference summary based on the vector
space model. The third measure is Main Topic Sim-
ilarity. This is a content-based evaluation method
based on measuring the cosine of the angle between
first left singular vectors of a system summary?s
and its reference summary?s SVDs. (For details see
(Steinberger and Jezek, 2004).) Finally, we mea-
sured ROUGE scores, with the same settings as in the
Document Understanding Conference (DUC) 2004.
4.3 How Much May Anaphora Resolution
Help? An Upper Bound
We annotated all the anaphoric relations in the 37
documents in our evaluation corpus by hand us-
ing the annotation tool MMAX (Mueller and Strube,
2003).5 Apart from measuring the performance of
GUITAR over the corpus, this allowed us to establish
the upper bound on the performance improvements
that could be obtained by adding an anaphoric re-
solver to our summarizer. We tested both methods
of adding the anaphoric knowledge to the summa-
rizer discussed above. Results for the 15% and 30%
ratios6 are presented in Tables 1 and 2. The baseline
is our own previously developed LSA-based sum-
marizer without anaphoric knowledge. The result
is that the substitution method did not lead to sig-
nificant improvement, but the addition method did:
5We annotated personal pronouns, possessive pronouns, def-
inite descriptions and also proper nouns, who will be handled by
a future GUITAR version.
6We used the same summarization ratios as in CAST.
6
Evaluation Lexical LSA CAST GUITAR GUITAR
Method Substitution Addition
Relative Utility 0.595 0.527 0.530 0.640
F-score 0.420 0.348 0.347 0.441
Cosine Similarity 0.774 0.726 0.804 0.805
Main Topic Similarity 0.686 0.630 0.643 0.699
Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.
Evaluation Lexical LSA CAST GUITAR GUITAR
Method Substitution Addittion
Relative Utility 0.645 0.618 0.626 0.678
F-score 0.557 0.522 0.524 0.573
Cosine Similarity 0.863 0.855 0.873 0.879
Main Topic Similarity 0.836 0.810 0.818 0.868
Table 4: Evaluation of the GUITAR improvement - summarization ratio: 30%.
addition could lead to an improvement in Relative
Utility score from .595 to .662 for the 15% ratio, and
from .645 to .688 for the 30% ratio. Both of these
improvements were significant by t-test at 95% con-
fidence.
4.4 Results with GUITAR
To use GUITAR, we first parsed the texts using Char-
niak?s parser (Charniak, 2000). The output of the
parser was then converted into the MAS-XML for-
mat expected by GUITAR by one of the preproces-
sors that come with the system. (This step includes
heuristic methods for guessing agreement features.)
Finally, GUITAR was ran to add anaphoric infor-
mation to the files. The resulting files were then
processed by the summarizer.
GUITAR achieved a precision of 56% and a recall
of 51% over the 37 documents. For definite descrip-
tion resolution, we found a precision of 69% and
a recall of 53%; for possessive pronoun resolution,
the precision was 53%, recall was 53%; for personal
pronouns, the precision was 44%, recall was 46%.
The results with the summarizer are presented
in Tables 3 and 4 (relative utility, f-score, cosine,
and main topic). The contribution of the differ-
ent anaphora resolution components is addressed in
(Kabadjov et al, 2005). All versions of our summa-
rizer (the baseline version without anaphora resolu-
tion and those using substitution and addition) out-
performed the CAST summarizer, but we have to em-
phasize that CAST did not aim at producing a high-
performance generic summarizer; only a system that
could be easily used for didactical purposes. How-
ever, our tables also show that using GUITAR and the
addition method lead to significant improvements
over our baseline LSA summarizer. The improve-
ment in Relative Utility measure was significant by
t-test at 95% confidence. Using the ROUGE mea-
sure we obtained improvement (but not significant).
On the other hand, the substitution method did not
lead to significant improvements, as was to be ex-
pected given that no improvement was obtained with
?perfect? anaphora resolution (see previous section).
5 Conclusion and Further Research
Our main result in this paper is to show that using
anaphora resolution in summarization can lead to
significant improvements, not only when ?perfect?
anaphora information is available, but also when
an automatic resolver is used, provided that the
anaphoric resolver has reasonable performance. As
far as we are aware, this is the first time that such
a result has been obtained using standard evaluation
measures over a reference corpus. We also showed
however that the way in which anaphoric informa-
tion is used matters: with our set of documents at
least, substitution would not result in significant im-
provements even with perfect anaphoric knowledge.
Further work will include, in addition to extend-
ing the set of documents and testing the system with
other collections, evaluating the improvement to be
achieved by adding a proper noun resolution algo-
rithm to GUITAR.
7
References
S. Azzam, K. Humphreys and R. Gaizauskas. 1999. Using
coreference chains for text summarization. In Proceedings
of the ACL Workshop on Coreference. Maryland.
B. Baldwin and T. S. Morton. 1998. Dynamic coreference-
based summarization. In Proceedings of EMNLP. Granada,
Spain.
R. Barzilay and M. Elhadad. 1997. Using lexical chains for text
summarization. In Proceedings of the ACL/EACL Workshop
on Intelligent Scalable Text Summarization. Madrid, Spain.
S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz.
2003. Using Knowledge-poor Coreference Resolution for
Text Summarization. In Proceedings of DUC. Edmonton.
M. W. Berry, S. T. Dumais and G. W. O?Brien. 1995. Using
Linear Algebra for Intelligent IR. In SIAM Review, 37(4).
B. Boguraev and C. Kennedy. 1999. Salience-based content
characterization of text documents. In I. Mani and M. T.
Maybury (eds), Advances in Automatic Text Summarization,
MIT Press. Cambridge, MA.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of NAACL. Philadelphia.
F. Y. Y. Choi, P. Wiemer-Hastings and J. D. Moore. 2001. La-
tent Semantic Analysis for Text Segmentation. In Proceed-
ings of EMNLP. Pittsburgh.
Y. Gong and X. Liu. 2002. Generic Text Summarization Us-
ing Relevance Measure and Latent Semantic Analysis. In
Proceedings of ACM SIGIR. New Orleans.
L. Hasler, C. Orasan and R. Mitkov. 2003. Building better
corpora for summarization. In Proceedings of Corpus Lin-
guistics. Lancaster, United Kingdom.
E. Hovy and C. Lin. 1997. Automated text summarization in
SUMMARIST. In ACL/EACL Workshop on Intelligent Scal-
able Text Summarization. Madrid, Spain.
M. A. Kabadjov, M. Poesio and J. Steinberger. 2005. Task-
Based Evaluation of Anaphora Resolution: The Case of
Summarization. In RANLP Workshop ?Crossing Barriers
in Text Summarization Research?. Borovets, Bulgaria.
A. Knott, J. Oberlander, M. O?Donnell, and C. Mellish. 2001.
Beyond elaboration: The interaction of relations and focus in
coherent text. In Sanders, T., Schilperoord, J., and Spooren,
W. (eds), Text representation: linguistic and psycholinguistic
aspects. John Benjamins.
T. K. Landauer and S. T. Dumais. 1997. A solution to Plato?s
problem: The latent semantic analysis theory of the acqui-
sition, induction, and representation of knowledge. In Psy-
chological Review, 104, 211-240.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proceedings of COLING. Montreal.
C. Mueller and M. Strube. 2001. MMAX: A Tool for the Anno-
tation of Multi-modal Corpora. In Proceedings of the IJCAI
Workshop on Knowledge and Reasoning in Practical Dia-
logue Systems. Seattle.
C. Orasan, R. Mitkov and L. Hasler. 2003. CAST: a Computer-
Aided Summarization Tool. In Proceedings of EACL. Bu-
dapest, Hungary.
M. Poesio and M. A. Kabadjov. 2004. A General-Purpose, off-
the-shelf Anaphora Resolution Module: Implementation and
Preliminary Evaluation. In Proceedings of LREC. Lisbon,
Portugal.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M. Hitzeman.
2004. Centering: A parametric theory and its instantiations.
Computational Linguistics, 30(3).
M. Poesio, M. A. Kabadjov, R. Vieira, R. Goulart, and
O. Uryupina. 2005. Do discourse-new detectors help def-
inite description resolution? In Proceedings of IWCS.
Tilburg, The Netherlands.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple documents. In
ANLP/NAACL Workshop on Automatic Summarization.
Seattle.
J. Steinberger and K. Jezek. 2004. Text Summarization and
Singular Value Decomposition. In Proceedings of ADVIS.
Izmir, Turkey.
R. Stuckardt. 2003. Coreference-Based Summarization and
Question Answering: a Case for High Precision Anaphor
Resolution. In International Symposium on Reference Reso-
lution. Venice, Italy.
R. Vieira and M. Poesio. 2000. An empirically-based system
for processing definite descriptions. In Computational Lin-
guistics, 26(4).
8
DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION
RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL
Massimo Poesio,? Olga Uryupina,? Renata Vieira,?
Mijail Alexandrov-Kabadjov? and Rodrigo Goulart?
?University of Essex, Computer Science and Cognitive Science (UK)
?Universita?t des Saarlandes, Computerlinguistik (Germany)
?Unisinos, Computac?a?o Aplicada (Brazil)
Abstract
Vieira and Poesio (2000) proposed an algorithm for
definite description (DD) resolution that incorpo-
rates a number of heuristics for detecting discourse-
new descriptions. The inclusion of such detec-
tors was motivated by the observation that more
than 50% of definite descriptions (DDs) in an av-
erage corpus are discourse new (Poesio and Vieira,
1998), but whereas the inclusion of detectors for
non-anaphoric pronouns in algorithms such as Lap-
pin and Leass? (1994) leads to clear improvements
in precision, the improvements in anaphoric DD res-
olution (as opposed to classification) brought about
by the detectors were rather small. In fact, Ng and
Cardie (2002a) challenged the motivation for the
inclusion of such detectors, reporting no improve-
ments, or even worse performance. We re-examine
the literature on the topic in detail, and propose a re-
vised algorithm, taking advantage of the improved
discourse-new detection techniques developed by
Uryupina (2003).
1 Introduction
Although many theories of definiteness and many
anaphora resolution algorithms are based on the as-
sumption that definite descriptions are anaphoric,
in fact in most corpora at least half of definite de-
scriptions are DISCOURSE-NEW (Prince, 1992), as
shown by the following examples, both of which are
the first sentences of texts from the Penn Treebank.
(1) a. Toni Johnson pulls a tape measure across
the front of what was once a stately Victorian
home.
b. The Federal Communications Commission
allowed American Telephone & Telegraph
Co. to continue offering discount phone
services for large-business customers
and said it would soon re-examine its
regulation of the long-distance market.
Vieira and Poesio (2000) proposed an algorithm for
definite description resolution that incorporates a
number of heuristics for detecting discourse-new
(henceforth: DN) descriptions. But whereas the
inclusion of detectors for non-anaphoric pronouns
(e.g., It in It?s raining) in algorithms such as Lappin
and Leass? (1994) leads to clear improvements in
precision, the improvements in anaphoric DD reso-
lution (as opposed to classification) brought about
by the detectors were rather small. In fact, Ng
and Cardie (2002a) challenged the motivation for
the inclusion of such detectors, reporting no im-
provements or even worse performance. We re-
examine the literature on the topic in detail, and
propose a revised algorithm, taking advantage of
the improved DN detection techniques developed by
Uryupina (2003).
2 Detecting Discourse-New Definite
Descriptions
2.1 Vieira and Poesio
Poesio and Vieira (1998) carried out corpus stud-
ies indicating that in corpora like the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993), around 52% of DDs are discourse-new
(Prince, 1992), and another 15% or so are bridg-
ing references, for a total of about 66-67% first-
mention. These results led Vieira and Poesio to
propose a definite description resolution algorithm
incorporating independent heuristic strategies for
recognizing DN definite descriptions (Vieira, 1998;
Vieira and Poesio, 2000).
The heuristics proposed by Vieira and Poesio
assumed a parsed input (the Penn Treebank) and
aimed at identifying five categories of DDs licensed
to occur as first mention on semantic or pragmatic
grounds on the basis of work on definiteness includ-
ing Loebner?s account (1987):
1. So-called SEMANTICALLY FUNCTIONAL de-
scriptions (Loebner, 1987). This class included
descriptions with modifiers like first or best
that turned a possibly sortal predicate into a
function (as in the first person to cross the Pa-
cific on a row boat); as well as descriptions
with predicates like fact or belief followed by a
that-clause with the function of specifying the
fact or belief under question. Both types of
definites descriptions were recognized by con-
sulting a hand-coded list of SPECIAL PREDI-
CATES.
2. Descriptions serving as disguised PROPER
NAMES, such as The Federal Communications
Commission or the Iran-Iraq war. The heuris-
tics for recognizing these definite descriptions
were primarily based on capitalization (of the
head or the modifiers).
3. PREDICATIVE descriptions, i.e., descriptions
semantically functioning as predicates rather
than as referring. These include descriptions
occurring in appositive position (as in Glenn
Cox, the president of Phillips Petroleum) and
in certain copular constructions (as in the man
most likely to gain custody of all this is a career
politician named Dinkins). The heuristics used
to recognize these cases examined the syntac-
tic structure of the NP and the clause in which
it appeared.
4. Descriptions ESTABLISHED (i.e., turned
into functions in context) by restric-
tive modification, particularly by es-
tablishing relative clauses (Loebner,
1987) and prepositional phrases, as in
The hotel where we stayed last night was
pretty good. These heuristics, as well,
examined the syntactic structure of the NP.
5. LARGER SITUATION definite descriptions
(Hawkins, 1978), i.e., definite descriptions like
the sun, the pope or the long distance mar-
ket which denote uniquely on the grounds of
shared knowledge about the situation (these are
Loebner?s ?situational functions?). Vieira and
Poesio?s system had a small list of such defi-
nites.
These heuristics were included as tests both of a de-
cision tree concerned only with the task of DN de-
tection, and of decision trees determining the classi-
fication of DDs as anaphoric, bridging or discourse
new. In both cases, the DN detection tests were in-
tertwined with attempts to identify an antecedent for
such DDs. Both hand-coded decision trees and auto-
matically acquired ones (trained using ID3, (Quin-
lan, 1986)) were used for the task of two-way clas-
sification into discourse-new and anaphoric. Vieira
and Poesio found only small differences in the order
of tests in the two decision trees, and small differ-
ences in performance. The hand-coded decision tree
executes in the following order:
1. Try the DN heuristics with the highest accu-
racy (recognition of some types of semanti-
cally functional DDs using special predicates,
and of potentially predicative DDs occurring in
appositions);
2. Otherwise, attempt to resolve the DD as direct
anaphora;
3. Otherwise, attempt the remaining DN heuris-
tics in the order: proper names, descrip-
tions established by relatives and PPs, proper
name modification, predicative DDs occurring
in copular constructions.
If none of these tests succeeds, the algorithm can ei-
ther leave the DD unclassified, or classify it as DN.
The automatically learned decision tree attempts di-
rect anaphora resolution first. The overall results on
the 195 DDs on which the automatically trained de-
cision tree was tested are shown in Table 1. The
baseline is the result achieved by classifying every
DD as discourse-new?with 99 discourse-new DDs
out of 195, this means a precision of 50.8%. Two
results are shown for the hand-coded decision tree:
in one version, the system doesn?t attempt to clas-
sify all DDs; in the other, all unclassified DDs are
classified as discourse-new.
Version of the System P R F
Baseline 50.8 100 67.4
Discourse-new detection only 69 72 70
Hand-coded DT: partial 62 85 71.7
Hand-coded DT: total 77 77 77
ID3 75 75 75
Table 1: Overall results by Vieira and Poesio
2.2 Bean and Riloff
Bean and Riloff (1999) developed a system for iden-
tifying discourse-new DDs1 that incorporates, in ad-
dition to syntax-based heuristics aimed at recogniz-
ing predicative and established DDs using postmod-
ification heuristics similar to those used by Vieira
and Poesio, additional techniques for mining from
corpora unfamiliar DDs including proper names,
larger situation, and semantically functional. Two
1Bean and Riloff use the term EXISTENTIAL for these DDs.
of the techniques proposed by Bean and Riloff are
particularly worth noticing. The SENTENCE-ONE
(S1) EXTRACTION heuristic identifies as discourse-
new every DD found in the first sentence of a text.
More general patterns can then be extracted from
the DDs initially found by S1-extraction, using the
EXISTENTIAL HEAD PATTERN method which, e.g.,
would extract the N+ Government from the
Salvadoran Government and the Guatemalan Gov-
ernment. The DEFINITE ONLY (DO) list contained
NPs like the National Guard or the FBI with a high
DEFINITE PROBABILITY, i.e., whose nominal com-
plex has been encountered at least 5 times with the
definite article, but never with the indefinite. VAC-
CINES were also developed that prevented the use
of patterns identified by S1-extraction or DO-list el-
ements when the definite probability of the definite
was too low. Overall, the algorithm proposed by
Bean and Riloff is as follows:
1. If the head noun of the DD appeared earlier in
the text, classify as anaphoric.
2. Otherwise, if the DD occurs in the S1 list, clas-
sify as discourse-new unless stopped by vac-
cine.
3. Otherwise, classify the DD as DN if one of the
following tests applies:
(a) it occurs in the DO list;
(b) it matches one of the EHP patterns, and is
not stopped by vaccine;
(c) it matches one of the syntactic heuristics
4. Otherwise, classify the DD as anaphoric.
(Note that as in the machine-learned version of the
Vieira and Poesio decision tree, a (simplified) direct
anaphora test is tried first, followed by DN detectors
in decreasing order of accuracy.)
Bean and Riloff trained their system on 1600 ar-
ticles from MUC-4, and tested it on 50 texts. The
S1 extraction methods produced 849 DDs; the DO
list contained 65 head nouns and 321 full NPs. The
overall results are shown in Table 2; the baseline
are the results obtained when classifying all DDs as
discourse-new.
Although the overall precision is not better than
what obtained with the partial hand-coded decision
tree used by Vieira and Poesio, recall is substantially
improved.
2.3 Ng and Cardie
Ng and Cardie (2002a) directly investigate the ques-
tion of whether employing a discourse-new pre-
diction component improves the performance of a
Method R P
Baseline 100 72.2
Syntactic Heuristics 43 93.1
Synt. Heuristics + S1 66.3 84.3
Synt. Heuristics + EHP 60.7 87.3
Synt. Heuristics + DO 69.2 83.9
Synt. Heuristics + S1 + EHP + DO 81.7 82.2
Synt. Heuristics + S1 + EHP + DO + V 79.1 84.5
Table 2: Discourse-new prediction results by Bean
and Riloff
coreference resolution system (specifically, the sys-
tem discussed in (Ng and Cardie, 2002b)). Ng and
Cardie?s work differs from the work discussed so far
in that their system attempts to deal with all types of
NPs, not just definite descriptions.
The discourse-new detectors proposed by Ng and
Cardie are statistical classifiers taking as input 37
features and trained using either C4.5 (Quinlan,
1993) or RIPPER (Cohen, 1995). The 37 features
of a candidate anaphoric expression specify, in ad-
dition to much of the information proposed in pre-
vious work, a few new types of information about
NPs.
? The four boolean so-called LEXICAL features
are actually string-level features: for exam-
ple, str_match is Y if a preceding NP
string-matches the anaphoric expression (ex-
cept for the determiner), and head_match =
Y if a preceding NP?s head string-matches the
anaphoric expression?s. embedded=Y if the
anaphoric expression is a prenominal modifier.
? The second group of 11 (mostly boolean) fea-
tures specifies the type of NP: e.g., pronoun
is Y if the anaphoric expression is a pronoun,
else N.
? The third group of 7 features specifies syn-
tactic properties of the anaphoric expression,
including number, whether NPj is the first of
two NPs in an appositive or predicative con-
struction, whether NPj is pre- or post-modified,
whether it contains a proper noun, and whether
it is modified by a superlative.
? The next group of 8 features are mostly novel,
and capture information not used by previ-
ous DN detectors about the exact composition
of definite descriptions: e.g., the_2n=Y if
the anaphoric expression starts with deter-
miner the followed by exactly two common
nouns, the_num_n=Y if the anaphoric ex-
pression starts with determiner the followed
by a cardinal and a common noun, and
the_sing_n=Y if the anaphoric expression
starts with determiner the followed by a singu-
lar NP not containing a proper noun.
? The next group of features consists of 4 fea-
tures capturing a variety of ?semantic? infor-
mation, including whether a previous NP is an
?alias? of NPj , or whether NPj is the title of a
person (the president).
? Finally, the last three features capture informa-
tion about the position in the text in which NPj
occurs: the header, the first sentence, or the
first paragraph.
Ng and Cardie?s discourse-new predictor was
trained and tested over the MUC-6 and MUC-7 coref-
erence data sets, achieving accuracies of 86.1% and
84%, respectively, against a baseline of 63.8% and
73.2%, respectively. Inspection of the top parts
of the decision tree produced with the MUC-6 sug-
gests that head_match is the most important fea-
ture, followed by the features specifying NP type,
the alias feature, and the features specifying the
structure of definite descriptions.
Ng and Cardie discuss two architectures for the
integration of a DN detector in a coreference sys-
tem. In the first architecture, the DN detector is
run first, and the coreference resolution algorithm
is run only if the DN detector classifies that NP as
anaphoric. In the second architecture, the system
first computes str_match and alias, and runs
the anaphoric resolver if any of them is Y; other-
wise, it proceeds as in the first architecture. The
results obtained on the MUC-6 data with the base-
line anaphoric resolver, the anaphoric resolver aug-
mented by a DN detector as in the first architecture,
and as in the second architecture (using C4.5), are
shown in Table 3. The results for all NPs, pronouns
only, proper names only, and common nouns only
are shown.2
As indicated in the Table, running the DN detector
first leads to worse results?this is because the detec-
tor misclassifies a number of anaphoric NPs as non-
anaphoric. However, looking first for a same-head
antecedent leads to a statistically significant im-
provement over the results of the baseline anaphoric
resolver. This confirms the finding both of Vieira
and Poesio and of Bean and Riloff that the direct
anaphora should be called very early.
2It?s not clear to us why the overall performance of the algo-
rithm is much better than the performance on the three individ-
ual types of anaphoric expressions considered?i.e., which other
anaphoric expressions are handled by the coreference resolver.
MUC-6 MUC-7
R P F R P F
Baseline (no DN detector) 70.3 58.3 63.8 65.5 58.2 61.6
Pronouns 17.9 66.3 28.2 10.2 62.1 17.6
Proper names 29.9 84.2 44.1 27.0 77.7 40.0
Common nouns 25.2 40.1 31.0 26.6 45.2 33.5
DN detector runs first 57.4 71.6 63.7 47.0 77.1 58.4
Pronouns 17.9 67.0 28.2 10.2 62.1 17.6
Proper names 26.6 89.2 41.0 21.5 84.8 34.3
Common nouns 15.4 56.2 24.2 13.8 77.5 23.4
Same head runs first 63.4 68.3 65.8 59.7 69.3 64.2
Pronouns 17.9 67.0 28.2 10.2 62.1 17.6
Proper names 27.4 88.5 41.9 26.1 84.7 40.0
Common nouns 20.5 53.1 29.6 21.7 59.0 31.7
Table 3: Evaluation of the three anaphoric resolvers
discussed by Ng and Cardie.
2.4 Uryupina
Uryupina (2003) trained two separate classifiers (us-
ing RIPPER, (Cohen, 1995)): a DN detector and a
UNIQUENESS DETECTOR, i.e., a classifier that de-
termines whether an NP refers to a unique object.
This is useful to identify proper names (like 1998,
or the United States of America), semantic definites
(like the chairman of Microsoft) and larger situation
definite descriptions (like the pope). Both classi-
fiers use the same set of 32 features. The features of
an NP encode, first, of all, string-level information:
e.g., whether the NP contains capitalized words, dig-
its, or special symbols. A second group of features
specifies syntactic information: whether the NP is
postmodified, and whether it contains an apposition.
Two types of appositions are distinguished, with and
without commas. CONTEXT features specify the
distance between the NP and the previous NP with
the same head, if any. Finally, Uryupina?s system
computes four features specifying the NP?s definite
probability. Unlike the definite probability used by
Bean and Riloff, these features are computed from
the Web, using Altavista. From each NP, its head H
and entire NP without determiner Y are determined,
and four ratios are then computed:
#?the Y?
#Y ,
#?the Y?
#??aY ?? ,
#?the H?
#H ,
#?the H?
#??aH?? .
The classifiers were tested on 20 texts from MUC-
7 (a subset of the second data set used by Ng and
Cardie), parsed by Charniak?s parser. 19 texts were
used for training and for tuning RIPPER?s parame-
ters, one for testing. The results for the discourse
new detection task are shown in Table 4, separat-
ing the results for all NPs and definite NPs only,
and the results without definite probabilities and in-
cluding them. The results for uniqueness detection
are shown in Table 4, in which the results obtained
by prioritizing precision and recall are shown sepa-
rately.
Features P R F
All NPs String+Syn+Context 87.9 86.0 86.9
All 88.5 84.3 86.3
Def NPs String+Syn+Context 82.5 79.3 80.8
All 84.8 82.3 83.5
Table 4: Results of Uryupina?s discourse new clas-
sifier
Features P R F
Best Prec String+Syn+Context 94.0 84.0 88.7
All 95.0 83.5 88.9
Best Rec String+Syn+Context 86.7 96.0 91.1
All 87.2 97.0 91.8
Table 5: Results of Uryupina?s uniqueness classifier
The first result to note is that both of Uryupina?s
classifiers work very well, particularly the unique-
ness classifier. These tables also show that the def-
inite probability helps somewhat the discourse new
detector, but is especially useful for the uniqueness
detector, as one would expect on the basis of Loeb-
ner?s discussion.
2.5 Summary
Quite a lot of consensus on many of the factors play-
ing a role in DN detection for DDs. Most of the al-
gorithms discussed above incorporate methods for:
? recognizing predicative DDs;
? recognizing discourse-new proper names;
? identifying functional DDs;
? recognizing DDs modified by establishing rel-
atives (which may or may not be discourse-
new).
There is also consensus on the fact that DN detection
cannot be isolated from anaphoric resolution (wit-
ness the Ng and Cardie results).
One problem with some of the machine learning
approaches to coreference is that these systems do
not achieve very good results on pronoun and defi-
nite description resolution in comparison with spe-
cialized algorithms: e.g., although Ng and Cardie?s
best version achieves F=65.8 on all anaphoric ex-
pressions, it only achieves F=29.6 for definite de-
scriptions (cfr. Vieira and Poesio?s best result of
F=77), and F=28.2 for pronouns (as opposed to re-
sults as high as F=80 obtained by the pronoun res-
olution algorithms evaluated in (Tetreault, 2001)).
Clearly these systems can only be properly com-
pared by evaluating them all on the same corpora
and the same data, and discussion such as (Mitkov,
2000) suggest caution in interpreting some of the
results discussed in the literature as pre- and post-
processing often plays a crucial role, but we feel that
evaluating DN detectors in conjunction with high-
performing systems would give a better idea of the
improvements that one may hope to achieve.
3 Do Discourse-New Detectors Help?
Preliminary Evaluations
Vieira and Poesio did not test their system with-
out DN-detection, but Ng and Cardie?s results indi-
cate that DN detection does improve results, if not
dramatically, provided that the same_head test is
run first?although their DN detector does not appear
to improve results for pronouns, the one category
for which detection of non-anaphoricity has been
shown to be essential (Lappin and Leass, 1994). In
order to evaluate how much improvement can we
expect by just improving the DN detector, we did
a few preliminary evaluations both with a reimple-
mentation of Vieira and Poesio?s algorithm which
does not include a discourse-new detector, running
over treebank text as the original algorithm, and
with a simple statistical coreference resolver at-
tempting to resolve all anaphoric expressions and
running over unparsed text, using Uryupina?s fea-
tures for discourse-new detection, and over the same
corpus used by Ng and Cardie (MUC-7).
3.1 How much does DN-detection help the
Vieira / Poesio algorithm?
GUITAR (Poesio and Alexandrov-Kabadjov, 2004)
is a general-purpose anaphoric resolver that in-
cludes an implementation of the Vieira / Poesio al-
gorithm for definite descriptions and of Mitkov?s al-
gorithm for pronoun resolution (Mitkov, 1998). It is
implemented in Java, takes its input in XML format
and returns as output its input augmented with the
anaphoric relations it has discovered. GUITAR has
been implemented in such a way as to be fully mod-
ular, making it possible, for example, to replace the
DD resolution method with alternative implementa-
tions. It includes a pre-processor incorporating a
chunker so that it can run over both hand-parsed and
raw text.
A version of GUITAR without the DN detection
aspects of the Vieira / Poesio algorithm was evalu-
ated on the GNOME corpus (Poesio, 2000; Poesio et
al., 2004), which contains 554 definite descriptions,
of which 180 anaphoric, and 305 third-person pro-
nouns, of which 217 anaphoric. The results for defi-
nite descriptions over hand-parsed text are shown in
Table 6.
Total Res Corr NM WM SM R P F
180 182 121 43 16 45 67.2 66.5 66.8
Table 6: Evaluation of the GUITAR system without
DN detection over a hand-annotated treebank
GUITAR without a DN recognizer takes 182 DDs
(Res) as anaphoric, resolving 121 of them cor-
rectly (Corr); of the 182 DDs it attempts to resolve,
only 16 are incorrectly resolved (WM); almost three
times that number (45) are Spurious Matches (SM),
i.e., discourse-new DDs incorrectly interpreted as
anaphoric. (Res=Corr+WM+SM.) The system can?t
find an antecedent for 43 of the 180 anaphoric DDs.
When endowed with a perfect DN detector, GUI-
TAR could achieve a precision P=88.3 which, as-
suming recall stays the same (R=67.2) would mean
a F=76.3.
Of course, these results are obtained assuming
perfect parsing. For a fairer comparison with the
results of Ng and Cardie, we report in Table 7 the
results for both pronouns and definite descriptions
obtained by running GUITAR off raw text.
R P F
Pronouns 65.5 63.0 64.2
DDs 56.7 56.1 56.4
Table 7: Evaluation of the GUITAR system without
DN detection off raw text
Notice that although these results are not partic-
ularly good, they are still better than the results re-
ported by Ng and Cardie for pronouns and definite
NPs.
3.2 How much might DN detection help a
simple statistical coreference resolver?
In order to have an even closer comparison with
the results of Ng and Cardie, we implemented a
simple statistical coreference system, that, like Ng
and Cardie?s system, would resolve all types of
anaphoric expressions, and would run over unparsed
text, but without DN detection. We ran the system
over the MUC-7 data used by Ng and Cardie, and
compared the results with those obtained by using
perfect knowledge about discourse novelty. The re-
sults are shown in Table 8.
R P F
Without DN detection 44.7 54.9 49.3
With DN detection 41.4 80.0 54.6
Table 8: Using an oracle
These results suggest that a DN detector could
lead to substantial improvements for coreference
resolution in general: DN detection might improve
precision by more than 30%, which more than
makes up for the slight deterioration in recall. Of
course, this test alone doesn?t tell us how much im-
provement DN detection would bring to a higher-
performance anaphoric resolver.
4 A New Set of Features for
Discourse-New Detection
Next, we developed a new set of features for dis-
course new detection that takes into account the
findings of the work on DN detection discussed in
the previous sections. This set of features will be
input to an anaphoric resolver for DDs working in
two steps. For each DD,
1. The direct anaphora resolution algorithm from
(Vieira and Poesio, 2000) is run, which at-
tempts to find an head-matching antecedent
within a given window and taking premodifica-
tion into account. The results of the algorithm
(i.e., whether an antecedent was found) is used
as one of the input features of the classifier in
the next step. In addition, a number of features
of the DD that may help recognizing the classes
of DDs discussed above are extracted from the
input. Some of these features are computed ac-
cessing the Web via the Google API.
2. A decision tree classifier is used to classify the
DD as anaphoric (in which case the antecedents
identified at the first step are also returned) or
discourse-new.
The features input to the classifier can be catego-
rized as follows:
Anaphora A single feature,
direct-anaphora, specifying the distance
of the (same-head) antecedent from the DD, if
any (values: none, zero, one, more)
Predicative NPs Two boolean features:
? apposition, if the DD occurs in appos-
itive position;
? copular, if the DD occurs in post-verbal
position in a copular construction.
Proper Names Three boolean features:
? c-head: whether the head is capitalized;
? c-premod: whether one of the premod-
ifiers is capitalized;
? S1: whether the DD occurs in the first sen-
tence of a Web page.
Functionality The four definite probabilities used
by Uryupina (computed accessing the Web),
plus a superlative feature specifying if
one of the premodifiers is a superlative, ex-
tracted from the part of speech tags.
Establishing relative A single feature, specifying
whether NP is postmodified, and by a relative
clause or a prepositional phrase;
Text Position Whether the DD occurs in the title,
the first sentence, or the first paragraph.
We are testing several classifiers in-
cluded in the Weka 3.4 library
(http://www.cs.waikato.ac.nz/?ml/)
including an implementation of C4.5 and a
multi-layer perceptron.
5 Evaluation
Data We are using three corpora for the evalua-
tion, including texts from different genres, in which
all anaphoric relations between (all types of) NPs are
marked. The GNOME corpus includes pharmaceuti-
cal leaflets and museum ?labels? (i.e., descriptions
of museum objects and of the artists that realized
them). As said above, the corpus contains 554 def-
inite descriptions. In addition, we are using the 14
texts from the Penn Treebank included in the cor-
pus used by Vieira and Poesio. We transferred these
texts to XML format, and added anaphoric informa-
tion for all types of NPs according to the GNOME
scheme. Finally, we are testing the system on the
MUC-7 data used by Ng and Cardie
Methods We will compare three versions of the
DD resolution component:
1. The baseline algorithm without DN detection
incorporated in GUITAR described above (i.e.,
only the direct anaphora resolution part of
(Vieira and Poesio, 2000));
2. A complete implementation of the Vieira and
Poesio algorithm, including also the DN detect-
ing heuristics;
3. An algorithm using the statistical classifier dis-
cussed above.
Results Regrettably, the system is still being
tested. We will report the results at the workshop.
6 Discussion and Conclusions
Discussions and conclusions will be based on the
final results.
Acknowledgments
Mijail Alexandrov-Kabadjov is supported by Cona-
cyt. Renata Vieira and Rodrigo Goulart are partially
supported by CNPq.
References
D. L. Bean and E. Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
Proc. of the 37th ACL, pages 373?380, University
of Maryland. ACL.
W. Cohen. 1995. Fast effective rule induction. In
Proc. of ICML.
J. A. Hawkins. 1978. Definiteness and Indefinite-
ness. Croom Helm, London.
S. Lappin and H. J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):535?562.
S. Loebner. 1987. Definites. Journal of Semantics,
4:279?326.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of english: the Penn Treebank.
Computational Linguistics, 19(2):313?330.
R. Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proc. of the 18th COL-
ING, pages 869?875, Montreal.
R. Mitkov. 2000. Towards more comprehensive
evaluation in anaphora resolution. In Proc. of
the 2nd International Conference on Language
Resources and Evaluation, pages 1309?1314,
Athens, May.
V. Ng and C. Cardie. 2002a. Identifying anaphoric
and non-anaphoric noun phrases to improve
coreference resolution. In Proc. of 19th COL-
ING.
V. Ng and C. Cardie. 2002b. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40th Meeting of the ACL.
M. Poesio and M. Alexandrov-Kabadjov. 2004. A
general-purpose, off the shelf anaphoric resolver.
In Proc. of LREC, Lisbon, May.
M. Poesio and R. Vieira. 1998. A corpus-based in-
vestigation of definite description use. Compu-
tational Linguistics, 24(2):183?216, June. Also
available as Research Paper CCS-RP-71, Centre
for Cognitive Science, University of Edinburgh.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M.
Hitzeman. 2004. Centering: A parametric theory
and its instantiations. Computational Linguistics.
To appear.
M. Poesio. 2000. Annotating a corpus to develop
and evaluate discourse entity realization algo-
rithms: issues and preliminary results. In Proc.
of the 2nd LREC, pages 211?218, Athens, May.
E. F. Prince. 1992. The ZPG letter: subjects, defi-
niteness, and information status. In S. Thompson
and W. Mann, editors, Discourse description: di-
verse analyses of a fund-raising text, pages 295?
325. John Benjamins.
J. R. Quinlan. 1986. Induction of decision trees.
Machine Learning, 1(1):81?106.
J. R. Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann, San Mateo, CA.
J. R. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics, 27(4):507?520.
O. Uryupina. 2003. High-precision identification
of discourse-new and unique noun phrases. In
Proc. of the ACL 2003 Student Workshop, pages
80?86.
R. Vieira and M. Poesio. 2000. An empirically-
based system for processing definite descriptions.
Computational Linguistics, 26(4), December.
R. Vieira. 1998. Definite Description Resolution
in Unrestricted Texts. Ph.D. thesis, University of
Edinburgh, Centre for Cognitive Science, Febru-
ary.
Proceedings of the ACL 2010 Conference Short Papers, pages 382?386,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Wrapping up a Summary:
from Representation to Generation
Josef Steinberger and Marco Turchi and
Mijail Kabadjov and Ralf Steinberger
EC Joint Research Centre
21027, Ispra (VA), Italy
{Josef.Steinberger, Marco.Turchi,
Mijail.Kabadjov, Ralf.Steinberger}
@jrc.ec.europa.eu
Nello Cristianini
University of Bristol,
Bristol, BS8 1UB, UK
nello@support-vector.net
Abstract
The main focus of this work is to investi-
gate robust ways for generating summaries
from summary representations without re-
curring to simple sentence extraction and
aiming at more human-like summaries.
This is motivated by empirical evidence
from TAC 2009 data showing that human
summaries contain on average more and
shorter sentences than the system sum-
maries. We report encouraging prelimi-
nary results comparable to those attained
by participating systems at TAC 2009.
1 Introduction
In this paper we adopt the general framework
for summarization put forward by Spa?rck-Jones
(1999) ? which views summarization as a three-
fold process: interpretation, transformation and
generation ? and attempt to provide a clean in-
stantiation for each processing phase, with a par-
ticular emphasis on the last, summary-generation
phase often omitted or over-simplified in the main-
stream work on summarization.
The advantages of looking at the summarization
problem in terms of distinct processing phases are
numerous. It not only serves as a common ground
for comparing different systems and understand-
ing better the underlying logic and assumptions,
but it also provides a neat framework for devel-
oping systems based on clean and extendable de-
signs. For instance, Gong and Liu (2002) pro-
posed a method based on Latent Semantic Anal-
ysis (LSA) and later J. Steinberger et al (2007)
showed that solely by enhancing the first source
interpretation phase, one is already able to pro-
duce better summaries.
There has been limited work on the last sum-
mary generation phase due to the fact that it is
unarguably a very challenging problem. The vast
amount of approaches assume simple sentence se-
lection, a type of extractive summarization, where
often the summary representation and the end
summary are, indeed, conflated.
The main focus of this work is, thus, to in-
vestigate robust ways for generating summaries
from summary representations without recurring
to simple sentence extraction and aiming at more
human-like summaries. This decision is also mo-
tivated by empirical evidence from TAC 2009 data
(see table 1) showing that human summaries con-
tain on average more and shorter sentences than
the system summaries. The intuition behind this is
that, by containing more sentences, a summary is
able to capture more of the important content from
the source.
Our initial experimental results show that our
approach is feasible, since it produces summaries,
which when evaluated against the TAC 2009 data1
yield ROUGE scores (Lin and Hovy, 2003) com-
parable to the participating systems in the Sum-
marization task at TAC 2009. Taking into account
that our approach is completely unsupervised and
language-independent, we find our preliminary re-
sults encouraging.
The remainder of the paper is organised as fol-
lows: in the next section we briefly survey the
related work, in ?3 we describe our approach to
summarization, in ?4 we explain how we tackle
the generation step, in ?5 we present and discuss
our experimental results and towards the end we
conclude and give pointers to future work.
2 Related Work
There is a large body of literature on summariza-
tion (Hovy, 2005; Erkan and Radev, 2004; Kupiec
et al, 1995). The most closely related work to the
approach presented hereby is work on summariza-
tion attempting to go beyond simple sentence ex-
1http://www.nist.gov/tac/
382
traction and to a lesser degree work on sentence
compression. We survey below work along these
lines.
Although our approach is related to sentence
compression (Knight and Marcu, 2002; Clarke
and Lapata, 2008), it is subtly different. Firstly, we
reduce the number of terms to be used in the sum-
mary at a global level, not at a local per-sentence
level. Secondly, we directly exploit the resulting
structures from the SVD making the last genera-
tion step fully aware of previous processing stages,
as opposed to tackling the problem of sentence
compression in isolation.
A similar approach to our sentence reconstruc-
tion method has been developed by Quirk et al
(2004) for paraphrase generation. In their work,
training and test sets contain sentence pairs that
are composed of two different proper English sen-
tences and a paraphrase of a source sentence is
generated by finding the optimal path through a
paraphrases lattice.
Finally, it is worth mentioning that we are aware
of the ?capsule overview? summaries proposed by
Boguraev and Kennedy (1997) which is similar to
our TSR (see below), however, as opposed to their
emphasis on a suitable browsing interface rather
than producing a readable summary, we precisely
attempt the latter.
3 Three-fold Summarization:
Interpretation, Transformation and
Generation
We chose the LSA paradigm for summarization,
since it provides a clear and direct instantiation of
Spa?rck-Jones? three-stage framework.
In LSA-based summarization the interpreta-
tion phase takes the form of building a term-by-
sentence matrix A = [A1, A2, . . . , An], where
each column Aj = [a1j , a2j , . . . , anj ]T represents
the weighted term-frequency vector of sentence j
in a given set of documents. We adopt the same
weighting scheme as the one described in (Stein-
berger et al, 2007), as well as their more general
definition of term entailing not only unigrams and
bigrams, but also named entities.
The transformation phase is done by applying
singular value decomposition (SVD) to the initial
term-by-sentence matrix defined as A = U?V T .
The generation phase is where our main contri-
bution comes in. At this point we depart from stan-
dard LSA-based approaches and aim at produc-
ing a succinct summary representation comprised
only of salient terms ? Term Summary Represen-
tation (TSR). Then this TSR is passed on to an-
other module which attempts to produce complete
sentences. The module for sentence reconstruc-
tion is described in detail in section 4, in what fol-
lows we explain the method for producing a TSR.
3.1 Term Summary Representation
To explain how a term summary representation
(TSR) is produced, we first need to define two con-
cepts: salience score of a given term and salience
threshold. Salience score for each term in matrix
A is given by the magnitude of the corresponding
vector in the matrix resulting from the dot product
of the matrix of left singular vectors with the diag-
onal matrix of singular values. More formally, let
T = U ? ? and then for each term i, the salience
score is given by |~Ti|. Salience threshold is equal
to the salience score of the top kth term, when all
terms are sorted in descending order on the basis
of their salience scores and a cutoff is defined as a
percentage (e.g., top 15%). In other words, if the
total number of terms is n, then 100?k/n must be
equal to the percentage cutoff specified.
The generation of a TSR is performed in two
steps. First, an initial pool of sentences is selected
by using the same technique as in (Steinberger and
Jez?ek, 2009) which exploits the dot product of the
diagonal matrix of singular values with the right
singular vectors: ? ? V T .2 This initial pool of sen-
tences is the output of standard LSA approaches.
Second, the terms from the source matrix A are
identified in the initial pool of sentences and those
terms whose salience score is above the salience
threshold are copied across to the TSR. Thus, the
TSR is formed by the most (globally) salient terms
from each one of the sentences. For example:
? Extracted Sentence: ?Irish Prime Minister Bertie
Ahern admitted on Tuesday that he had held a series of
private one-on-one meetings on the Northern Ireland
peace process with Sinn Fein leader Gerry Adams, but
denied they had been secret in any way.?
? TSR Sentence at 10%: ?Irish Prime Minister
Bertie Ahern Tuesday had held one-on-one meetings
Northern Ireland peace process Sinn Fein leader Gerry
Adams?3
2Due to space constraints, full details on that step are
omitted here, see (Steinberger and Jez?ek, 2009).
3The TSR sentence is stemmed just before feeding it to
the reconstruction module discussed in the next section.
383
Average Human System At 100% At 15% At 10% At 5% At 1%
number of: Summaries Summaries
Sentences/summary 6.17 3.82 3.8 3.95 4.39 5.18 12.58
Words/sentence 15.96 25.01 26.24 25.1 22.61 19.08 7.55
Words/summary 98.46 95.59 99.59 99.25 99.18 98.86 94.96
Table 1: Summary statistics on TAC?09 data (initial summaries).
Metric LSAextract At 100% At 15% At 10% At 5% At 1%
ROUGE-1 0.371 0.361 0.362 0.365 0.372 0.298
ROUGE-2 0.096 0.08 0.081 0.083 0.083 0.083
ROUGE-SU4 0.131 0.125 0.126 0.128 0.131 0.104
Table 2: Summarization results on TAC?09 data (initial summaries).
4 Noisy-channel model for sentence
reconstruction
This section describes a probabilistic approach to
the reconstruction problem. We adopt the noisy-
channel framework that has been widely used in a
number of other NLP applications. Our interpre-
tation of the noisy channel consists of looking at a
stemmed string without stopwords and imagining
that it was originally a long string and that some-
one removed or stemmed some text from it. In our
framework, reconstruction consists of identifying
the original long string.
To model our interpretation of the noisy chan-
nel, we make use of one of the most popular
classes of SMT systems: the Phrase Based Model
(PBM) (Zens et al, 2002; Och and Ney, 2001;
Koehn et al, 2003). It is an extension of the noisy-
channel model and was introduced by Brown et al
(1994), using phrases rather than words. In PBM,
a source sentence f is segmented into a sequence
of I phrases f I = [f1, f2, . . . fI ] and the same is
done for the target sentence e, where the notion of
phrase is not related to any grammatical assump-
tion; a phrase is an n-gram. The best translation
ebest of f is obtained by:
ebest = argmaxe p(e|f) = argmaxe
I?
i=1
?(fi|ei)
??
d(ai ? bi?1)
?d
|e|?
i=1
pLM (ei|e1 . . . ei?1)
?LM
where ?(fi|ei) is the probability of translating
a phrase ei into a phrase fi. d(ai ? bi?1) is
the distance-based reordering model that drives
the system to penalize substantial reorderings of
words during translation, while still allowing some
flexibility. In the reordering model, ai denotes the
start position of the source phrase that was trans-
lated into the ith target phrase, and bi?1 denotes
the end position of the source phrase translated
into the (i?1th) target phrase. pLM (ei|e1 . . . ei?1)
is the language model probability that is based on
the Markov chain assumption. It assigns a higher
probability to fluent/grammatical sentences. ??,
?LM and ?d are used to give a different weight to
each element (for more details see (Koehn et al,
2003)).
In our reconstruction problem, the difference
between the source and target sentences is not in
terms of languages, but in terms of forms. In fact,
our source sentence f is a stemmed sentence with-
out stopwords, while the target sentence e is a
complete English sentence. ?Translate? means to
reconstruct the most probable sentence e given f
inserting new words and reproducing the inflected
surface forms of the source words.
4.1 Training of the model
In Statistical Machine Translation, a PBM system
is trained using parallel sentences, where each sen-
tence in a language is paired with another sentence
in a different language and one is the translation of
the other.
In the reconstruction problem, we use a set, S1
of 2,487,414 English sentences extracted from the
news. This set is duplicated, S2, and for each sen-
tence in S2, stopwords are removed and the re-
maining words are stemmed using Porter?s stem-
mer (Porter, 1980). Our stopword list contains 488
words. Verbs are not included in this list, because
they are relevant for the reconstruction task. To
optimize the lambda parameters, we select 2,000
pairs as development set.
384
An example of training sentence pair is:
? Source Sentence: ?royal mail ha doubl profit 321
million huge fall number letter post?
? Target Sentence: ?royal mail has doubled its prof-
its to 321 million despite a huge fall in the number of
letters being posted?
In this work we use Moses (Koehn et al, 2007),
a complete phrase-based translation toolkit for
academic purposes. It provides all the state-of-the-
art components needed to create a phrase-based
machine translation system. It contains different
modules to preprocess data, train the Language
Models and the Translation Models.
5 Experimental Results
For our experiments we made use of the TAC
2009 data which conveniently contains human-
produced summaries against which we could eval-
uate the output of our system (NIST, 2009).
To begin our inquiry we carried out a phase
of exploratory data analysis, in which we mea-
sured the average number of sentences per sum-
mary, words per sentence and words per summary
in human vs. system summaries in the TAC 2009
data. Additionally, we also measured these statis-
tics of summaries produced by our system at five
different percentage cutoffs: 100%, 15%, 10%,
5% and 1%. 4 The results from this exploration
are summarised in table 1. The most notable thing
is that human summaries contain on average more
and shorter sentences than the system summaries
(see 2nd and 3rd column from left to right). Sec-
ondly, we note that as the percentage cutoff de-
creases (from 4th column rightwards) the charac-
teristics of the summaries produced by our system
are increasingly more similar to those of the hu-
man summaries. In other words, within the 100-
word window imposed by the TAC guidelines, our
system is able to fit more (and hence shorter) sen-
tences as we decrease the percentage cutoff.
Summarization performance results are shown
in table 2. We used the standard ROUGE evalu-
ation (Lin and Hovy, 2003) which has been also
used for TAC. We include the usual ROUGE met-
rics: R1 is the maximum number of co-occurring
unigrams, R2 is the maximum number of co-
occurring bigrams and RSU4 is the skip bigram
measure with the addition of unigrams as counting
4Recall from section ?3 that the salience threshold is a
function of the percentage cutoff.
unit. The last five columns of table 2 (from left to
right) correspond to summaries produced by our
system at various percentage cutoffs. The 2nd col-
umn, LSAextract, corresponds to the performance
of our system at producing summaries by sentence
extraction only.5
In the light of the above, the decrease in per-
formance from column LSAextract to column ?At
100%? can be regarded as reconstruction error.6
Then, as we decrease the percentage cutoff (from
4th column rightwards) we are increasingly cover-
ing more of the content comprised by the human
summaries (as far as the ROUGE metrics are able
to gauge this, of course). In other words, the im-
provement of content coverage makes up for the
reconstruction error, and at 5% cutoff we already
obtain ROUGE scores comparable to LSAextract.
This suggests that if we improve the quality of our
sentence reconstruction we would potentially end
up with a better performing system than a typical
LSA system based on sentence selection. Hence,
we find these results very encouraging.
Finally, we admittedly note that by applying a
percentage cutoff on the initial term set and further
performing the sentence reconstruction we gain in
content coverage, to a certain extent, on the ex-
pense of sentence readability.
6 Conclusion
In this paper we proposed a novel approach to
summary generation from summary representa-
tion based on the LSA summarization framework
and on a machine-translation-inspired technique
for sentence reconstruction.
Our preliminary results show that our approach
is feasible, since it produces summaries which re-
semble better human summaries in terms of the av-
erage number of sentences per summary and yield
ROUGE scores comparable to the participating
systems in the Summarization task at TAC 2009.
Bearing in mind that our approach is completely
unsupervised and language-independent, we find
our results promising.
In future work we plan on working towards im-
proving the quality of our sentence reconstruction
step in order to produce better and more readable
sentences.
5These are, effectively, what we called initial pool of sen-
tences in section 3, before the TSR generation.
6The only difference between the two types of summaries
is the reconstruction step, since we are including 100% of the
terms.
385
References
B. Boguraev and C. Kennedy. 1997. Salience-
based content characterisation of text documents. In
I. Mani, editor, Proceedings of the Workshop on In-
telligent and Scalable Text Summarization at the An-
nual Joint Meeting of the ACL/EACL, Madrid.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1994. The mathematic of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 31:273?318.
G. Erkan and D. Radev. 2004. LexRank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analy-
sis. In Proceedings of ACM SIGIR, New Orleans,
US.
E. Hovy. 2005. Automated text summarization. In
Ruslan Mitkov, editor, The Oxford Handbook of
Computational Linguistics, pages 583?598. Oxford
University Press, Oxford, UK.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
?03, pages 48?54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of ACL ?07, demonstration session.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proceedings of the ACM
SIGIR, pages 68?73, Seattle, Washington.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, Edmonton, Canada.
NIST, editor. 2009. Proceeding of the Text Analysis
Conference, Gaithersburg, MD, November.
F. Och and H. Ney. 2001. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL ?02, pages
295?302, Morristown, NJ, USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation.
In Proceedings of EMNLP, volume 149. Barcelona,
Spain.
K. Spa?rck-Jones. 1999. Automatic summarising: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization.
MIT Press.
J. Steinberger and K. Jez?ek. 2009. Update summariza-
tion based on novel topic distribution. In Proceed-
ings of the 9th ACM DocEng, Munich, Germany.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jez?ek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management,
43(6):1663?1680. Special Issue on Text Summari-
sation (Donna Harman, ed.).
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of KI
?02, pages 18?32, London, UK. Springer-Verlag.
386
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 29?37,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Agile Corpus Annotation in Practice:
An Overview of Manual and Automatic Annotation of CVs
Bea Alex Claire Grover Rongzhou Shen
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Mijail Kabadjov
Joint Research Centre
European Commission
Via E. Fermi 2749, Ispra (VA), Italy
Contact: balex@staffmail.ed.ac.uk
Abstract
This paper describes work testing agile
data annotation by moving away from the
traditional, linear phases of corpus cre-
ation towards iterative ones and by recog-
nizing the potential for sources of error oc-
curring throughout the annotation process.
1 Introduction
Annotated data sets are an important resources for
various research fields, including natural language
processing (NLP) and text mining (TM). While the
detection of annotation inconsistencies in different
data sets has been investigated (e.g. Nova?k and
Raz??mova?, 2009) and their effect on NLP perfor-
mance has been studied (e.g. Alex et al 2006), very
little work has been done on deriving better methods
of annotation as a whole process in order to maxi-
mize both the quality and quantity of annotated data.
This paper describes our annotation project in which
we tested the relatively new approach of agile cor-
pus annotation (Voormann and Gut, 2008) of mov-
ing away from the traditional, linear phases of cor-
pus creation towards iterative ones and of recogniz-
ing the fact that sources of error can occur through-
out the annotation process.
We explain agile annotation and discuss related
work in Section 2. Section 3 describes the en-
tire annotation process and all its aspects. We pro-
vide details on the data collection and preparation,
the annotation tool, the annotators and the annota-
tion phases. Section 4 describes the final annota-
tion scheme and Section 5 presents inter-annotator-
agreement (IAA) figures measured throughout the
annotation. In Section 6, we summarize the per-
formance of the machine-learning (ML)-based TM
components which were trained and evaluated on the
annotated data. We discuss our findings and con-
clude in Section 7.
2 Background and Related Work
The manual and automatic annotation work de-
scribed in this paper was conducted as part of the
TXV project. The technology used was based
on TM components that were originally developed
for the biomedical domain during its predecessor
project (Alex et al, 2008b). In TXV we adapted
the tools to the recruitment domain in a short time
frame. The aim was to extract key information from
curricula vitae (CVs) for matching applicants to job
adverts and to each other. The TM output is visu-
alized in a web application with search navigation
that captures relationships between candidates, their
skills and organizations etc. This web interface al-
lows recruiters to find hidden information in large
volumes of unstructured text.
Both projects were managed using agile, test-
driven software development, i.e. solutions were
created based on the principles of rapid-prototyping
and iterative development cycles of deliverable ver-
sions of the TM system and the web application.1
The same principles were also applied to other
project work, including the manual annotation. The
aim of this annotation was to produce annotated data
for training ML-based TM technology as well as
evaluating system components.
Collecting data, drawing up annotation guidelines
and getting annotators to annotate this data in se-
quential steps is similar to the waterfall model in
software engineering (Royce, 1970). This approach
can be inefficient and costly if annotators unknow-
ingly carried out work that could have been avoided
and it can lead to difficulties if at the end of the pro-
cess the requirements no longer match the annota-
tions. Instead we applied agile software engineering
methods to the process of creating annotated data.
This is a relatively recent philosophy in software
1The agile software development principles are explained in
the Agile Manifesto: http://agilemanifesto.org/
29
Figure 1: The phases of traditional corpus creation (a) and the cyclic approach in agile corpus creation (b).
Reproduction of Figure 2 in Voormann and Gut (2008).
development which was inspired to overcome the
drawbacks of the waterfall model. The idea of ap-
plying agile methods to corpus creation and annota-
tion was first inspired by Voormann and Gut (2008)
but was not tested empirically. Cyclic annotation
was already proposed by Atkins et al (1992) and
Biber (1993) with a focus on data creation rather
than data annotation. In this paper, we describe a
way of testing this agile annotation in practice.
The idea behind an agile annotation process is
to produce useable manually annotated data fast as
well as discover and correct flaws in either the an-
notation guidelines or the annotation setup early on.
Voormann and Gut (2008) propose query-driven an-
notation, a cyclic corpus creation and annotation
process that begins with formulating a query. The
main advantages of this approach are:
? The annotation scheme evolves over time
which ensures that annotations are consistent
and remain focussed on the research that is
carried out. An iterative annotation process
therefore improves the annotation guidelines
but keeps the annotations suitable to the rele-
vant research questions.
? Problems with the annotation guidelines, er-
rors in the annotation and issues with the setup
become apparent immediately and can be cor-
rected early on. This can avoid difficulties later
on and will save time and cost.
? Some annotation data is available early on.
Voormann and Gut compare the cyclical approach
in agile annotation to traditional linear-phrase cor-
pus creation depicted in Figure 1. In the following
section we describe the annotation process in our
project which followed the principles of agile cor-
pus creation.
3 Annotation Process
This section provides an overview of all aspect in-
volved in the annotation of a data set of CVs for
various types of semantic information useful to re-
cruiters when analysing CVs and placing candidates
with particular jobs or organizations. We provide
information on the data collection, the document
preparation, the annotation tool and the annotation
process following agile methods.
3.1 Data Collection
We automatically collected a set of CVs of soft-
ware engineers and programmers which are publicly
available online. This data set was created by firstly
querying Google using the Google API2 for word
documents containing either the terms ?CV?, ?re-
sume? or ?curriculum vitae? as well as the terms
?developer?, ?programmer? or ?software? but ex-
cluding documents containing the word ?template?
or ?sample?. Furthermore, the query was restricted
to a 3-month period from 30/03 to 30/06/2008.3
We automatically downloaded the Word docu-
ments returned by this query, resulting in a pool of
1,000 candidate CVs available for annotation. We
split these documents randomly into a TRAIN, a DE-
VTEST and a TEST set in a ratio of approximately
64:16:20. We used the annotated TRAIN data for
training ML-based models and deriving rules and
the DEVTEST data for system development and op-
timization. We set aside the blind TEST set for
evaluating the final performance of our named en-
tity recognition (NER) and relation extraction (RE)
2http://code.google.com/apis/ajaxsearch
3The exact Google query is: ?(CV OR resume OR ?cur-
riculum vitae?) AND (developer OR programmer OR soft-
ware) AND filetype:doc AND -template AND -sample AND
daterange:2454466-2454647?.
30
CV data set
Set TRAIN DEVTEST TEST ALL
Files 253 72 78 403
Annotations 279 84 91 454
Table 1: Number of files and annotated files in each
section of the CV data set.
components (see Section 6).
The final manually annotated data set contains
403 files, of which 352 are singly and 51 doubly an-
notated, resulting in an overall total of 454 annota-
tions (see Table 1). This does not include the files
used during the pilot annotation. The doubly an-
notated CVs were used to determine inter-annotator
agreement (IAA) in regular intervals (see Section 5).
Some of the documents in the pool were not gen-
uine CVs but either job adverts or CV writing ad-
vice. We let the annotators carry out the filtering
process of only choosing genuine CVs of software
developers and programmers for annotation and re-
ject but record any documents that did not fit this cat-
egory. The annotators rejected 99 files as being ei-
ther not CVs at all (49) or being out-of-domain CVs
from other types of professionals (50). Therefore,
just over 50% of the documents in the pool were
used up during the annotation process.
3.2 Document Preparation
Before annotation, all candidate CVs were then au-
tomatically converted from Word DOC format to
OpenOffice ODT as well as to Acrobat PDF format
in a batch process using OpenOffice macros. The
resulting contents.xml files for each ODT version of
the documents contain the textual information of the
original CVs. An XSLT stylesheet was used to sim-
plify this format to a simpler in-house XML format,
as the input into our pre-processing pipeline. We re-
tained all formatting and style information in span
elements for potential later use.
The pre-processing includes tokenization, sen-
tence boundary detection, part-of-speech tagging,
lemmatization, chunking, abbreviation detection
and rule-based NER for person, location names and
dates. This information extraction system is a mod-
ular pipeline built around the LT-XML24 and LT-
TTT25 toolsets. The NER output is stored as stand-
4http://www.ltg.ed.ac.uk/software/ltxml2
5http://www.ltg.ed.ac.uk/software/lt-ttt2
off annotations in the XML. These pre-processed
files were used as the basis for annotation.
3.3 Annotation Tool
For annotating the text of the CVs we chose
MMAX2, the Java-based open source tool (Mu?ller
and Strube, 2006).6 MMAX2 supports multiple lev-
els of annotation by way of stand-off annotation.
As a result MMAX2 creates one separate file for
each level of annotation for each given base data file.
Only the annotation level files get edited during the
annotation phase. The base data files which con-
tain the textual information of the documents do not
change. In our project, we were interested in three
levels of annotation, one for named entities (NEs),
one for zones and one for relations between NEs.
The MMAX2 GUI allows annotators to mark up
nested structures as well as intra- and inter-sentential
relations. Both of these functionalities were crucial
to our annotation effort.
As the files used for annotation already con-
tained some NEs which were recognized automat-
ically using the rule-based NER system and stored
in standoff XML, the conversion into and out of the
MMAX2 format was relatively straightforward. For
each file to be annotated, we created one base file
containing the tokenized text and one entity file con-
taining the rule-based NEs.7
3.4 Annotation Phases
We employed 3 annotators with various degrees of
experience in annotation and computer science and
therefore familiar with software engineering skills
and terminology. The lead researcher of the project,
the first author of this paper, managed the annotators
and organized regular meetings with them.
We followed the agile corpus creation approach
and carried out cycles of annotations, starting with
a simple paper-based pilot annotation. This first an-
notation of 10 documents enabled us to get a first
impression of the type of information contained in
CVs of software engineers and programmers as well
as the type of information we wanted to capture in
the manual and automatic annotation. We drew up a
first set of potential types of zones that occur within
6http://mmax2.sourceforge.net
7For more information on how this is done see Mu?ller and
Strube (2006).
31
CVs and the types of NEs that can be found within
each zone (e.g. an EDUCATION zone containing NEs
of type LOC, ORG and QUAL).
Using this set of potential markables, we decided
on a subset of NEs and zones to be annotated in fu-
ture rounds. Regarding the zones, we settled on an-
notating zone titles in a similar way as NEs. Our
assumption was that recognizing the beginning of a
zone can sufficiently identify zone boundaries. We
did not include relations between NEs at this stages,
as we wanted to get a clearer idea of the definitions
of relevant NEs first before proceeding to relations.
We then carried out a second pilot annotation us-
ing 10 more CVs selected from the candidate pool.
We used the revised annotation scheme and this
time the annotation was done electronically using
MMAX2. The annotators also had access to the
PDF and DOC versions of each file in case crucial
structural or formatting information was lost in the
conversion. Files were annotated for NEs and zone
titles. We also asked the annotators to answer the
following questions:
? Does it make sense to annotate the proposed
markables and what are the difficulties in doing
so?
? Are there any interesting markables missing
from the list?
? Are there are any issues with using the annota-
tion tool?
Half way through the second pilot we scheduled a
further meeting to discuss their answers, addressed
any question, comments or issues with regard to the
annotation and adjusted the annotation guidelines
accordingly. At this point, as we felt that the defini-
tions of NEs were sufficiently clear and added guide-
lines for annotating various types of binary relations
between NEs, for example a LOC-ORG relation re-
ferring to a particular organization situated at a par-
ticular location, e.g. Google - Dublin. We list the
final set of markables as defined at the end of the
annotation process in Tables 2 and 3.
During the second half of the second pilot we
asked the annotators to time their annotation and es-
tablished that it can take between 30 minutes and 1.5
hours to annotate a CV. We then calculated pairwise
IAA for two doubly annotated files which allowed
us to get some evidence for which definition of NEs,
zone titles and relations were still ambiguous or not
actually relevant.
In parallel with both pilots, we also liaised closely
with a local recruitment company to gain a first-
hand understanding of what information recruiters
are interested in when matching candidates to em-
ployments or employers. This consultation as well
as the conclusions made after the second pilot led
to further adaptions of the annotation scheme before
the main annotation phase began.
Based on the feedback from the second pilot an-
notation, we also made some changes to the data
conversion and the annotation tool setup to reduce
the amount of work for annotators but without re-
stricting the set of markables. In the case of some
nested NEs, we propagated relations between em-
bedded NEs that could be referred from the relations
of the containing NEs. For example, two DATE enti-
ties nested within a DATERANGE entity, the latter of
which the annotator related to an ORG entity, were
related to the same ORG entity automatically. We
also introduced a general GROUP entity which could
be used by the annotators to mark up lists of NEs,
for example, if they were all related to a different
NE mention of type X. In that case, the annotators
only had to mark up a relation between the GROUP
and X. All implicit relations between the NEs nested
in the GROUP and X were propagated during the con-
version from the MMAX2 format back into the in-
house XML format. This proved particularly useful
for annotating relations between SKILL entities and
other types of NEs.
Once those changes had been made, the main an-
notation phase began. Each in-domain CV that was
loaded into the annotation tool already contained
some NEs pre-annotated by the rule-based NER sys-
tem (see Section 3.2). The annotators had to correct
the annotations in case they were erroneous. Over-
all, the annotators reported this pre-annotation to be
useful rather than hindering as they did not have to
do too many corrections. At the end of each day, the
annotators checked in their work into the project?s
subversion (SVN) repository. This provided us with
additional control and backup in case we needed to
go back to previous versions at later stages.
The annotation guidelines still evolved during the
main annotation. Regular annotation meetings were
32
held in case the annotators had questions on the
guidelines or if they wanted to discuss specific ex-
amples. If a change was made to the annotation
guidelines, all annotators were informed and asked
to update their annotations accordingly. Moreover,
IAA was calculated regularly on sub-sections of the
doubly annotated data. This provided more empiri-
cal evidence for the types of markables the annota-
tors found difficult to mark up and where clarifica-
tions where necessary. The reasons for this were that
their definitions were ambiguous or underspecified.
We deliberately kept the initial annotation scheme
simple. The idea was for the annotators to shape the
annotation scheme based on evidence in the actual
data. We believe that this approach made the data
set more useful for its final use to train and evaluate
TM components. As a result of this agile annotation
approach, we became aware of any issues very early
on and were able to correct them accordingly.
4 Annotation Scheme
In this section, we provide a summary of the final
annotation scheme as an overview of all the mark-
ables present in the annotated data set.
4.1 Named Entities
In general, we asked the annotators to mark up ev-
ery mention of all NE types throughout the entire
CV, even if they did not refer to the CV owner. With
some exceptions (DATE in DATERANGE and LOC or
ORG in ADDRESS), annotators were asked to avoid
nested NEs and aim for a flat annotation. Discontin-
uous NEs in coordinated structures had to be marked
as such, i.e. the NE should only contain strings that
refer to it. Finally, abbreviations and their defini-
tions had to be annotated as two separate NEs. The
NE types in the final annotation guidelines are listed
in Table 2. While carrying out the NE annotation,
the annotators were also asked to set the NE at-
tribute of type CANDIDATE (by default set to true)
to false if a certain NE was not an attribute of the
CV owner (e.g. the ADDRESS of a referee).
4.2 Zone Titles
Regarding the zone titles, we provided a list of syn-
onyms for each type as context (see Table 2). The
annotators were asked only to annotate main zone
titles, ignoring sub-zones. They were also asked to
Entity Type Description
ADDRESS Addresses with streets or postcodes.
DATE Absolute (e.g. 10/04/2010), underspec-
ified (e.g. April 2010) or relative dates
(e.g. to date) including DATE entities
within DATERANGE entities.
DATERANGE Date ranges with a specific start and end
date including ones with either point not
explicitly stated (e.g. since 2008).
DOB Dates of birth.
EMAIL Email addresses.
JOB Job titles and roles referring to the of-
ficial name a post (e.g. software devel-
oper) but not a skill (e.g. software de-
velopment).
LOC Geo-political place names.
ORG Names of companies, institutions and
organizations.
PER Person names excluding titles.
PHONE Telephone and fax numbers.
POSTCODE Post codes.
QUAL Qualifications achieved or working to-
wards.
SKILL Skills and areas of expertise incl. hard
skills (e.g. Java, C++, French) or gen-
eral areas of expertise (e.g. software de-
velopment) but not soft or interpersonal
skills (e.g. networking, team work).
TIMESPAN Durations of time (e.g. 7 years, 2
months, over 2 years).
URL URLs
GROUP Dummy NE to group several NEs for
annotating multiple relations at once.
The individual NEs contained within
the group still have to be annotated.
Zone Title Type Synonyms
EDUCATION Education, Qualifications, Training,
Certifications, Courses
SKILLS Skills, Qualifications, Experience,
Competencies
SUMMARY Summary, Profile
PERSONAL Personal Information, Personal Data
EMPLOYMENT Employment, Employment History,
Work History, Career, Career Record
REFERENCES References, Referees
OTHER Other zone titles not covered by this list,
e.g. Publications, Patents, Grants, As-
sociations, Interests, Additional.
Table 2: The types of NEs and zone titles annotated.
mark up only the relevant sub-string of the text re-
ferring to the zone title and not the entire title if it
contained irrelevant information.
4.3 Relations
The binary relations that were annotated (see Table
3) always link two different types of NE mentions.
Annotators were asked to mark up relations within
the same zone but not across zones.
33
Relation Type Description
TEMP-SKILL A skill related to a temporal expression
(e.g. Java - 7 years). TEMP includes any
temporal NE types (DATE, DATERANGE
and TIMESPAN).
TEMP-LOC A location related to a temporal expres-
sion (e.g. Dublin - summer 2004).
TEMP-ORG An organization related to a temporal
expression (e.g. Google - 2001-2004).
TEMP-JOB A job title related to a temporal ex-
pression (e.g. Software Engineer -
Sep. 2001 to Jun. 2004).
TEMP-QUAL A qualification related to a temporal ex-
pression (e.g. PhD - June 2004).
LOC-ORG An organization related to a location
(e.g. Google - Dublin).
LOC-JOB A job title related to a location
(e.g. Software Engineer - Dublin).
LOC-QUAL A qualification related to a location
(e.g. PhD - Dublin).
ORG-JOB A job title related to an organization
(e.g. Software Engineer - Google).
ORG-QUAL A qualification related to an organiza-
tion (e.g. PhD - University of Toronto).
GROUP-X A relation that can be assigned in case
a group of NEs all relate to another NE
X. GROUP-X can be any of the relation
pairs mentioned in this list.
Table 3: The types of relations annotated.
5 Inter-Annotator Agreement
We first calculated pairwise IAA for all markables
at the end of the 2nd pilot and continued doing so
throughout the main annotation phase. For each pair
of annotations on the same document, IAA was cal-
culated by scoring one annotator against another us-
ing precision (P), recall (R) and F1.8 An overall IAA
was calculated by micro-averaging across all anno-
tated document pairs.9 We used F1 rather than the
Kappa score (Cohen, 1960) to measure IAA as the
latter requires comparison with a random baseline,
which does not make sense for tasks such as NER.
Table 4 compares the IAA figures we obtained for
2 doubly annotated documents during the 2nd pilot
phase, i.e. the first time we measured IAA, to those
we obtained on 9 different files once the main an-
notation was completed. For NEs and zone titles,
IAA was calculated using P, R and F1, defining two
mentions as equal if they had the same left and right
8P, R and F1 are calculated in standard fashion from the
number of true positives, false positives and false negatives.
9Micro-averaging was chosen over macro-averaging, since
we felt that the latter would give undue weight to documents
with fewer markables.
boundaries and the same type. Although this com-
parison is done over different sub-sets of the corpus,
it is still possible to conclude that the NE IAA im-
proved considerably over the course of the annota-
tion process.
The IAA scores for the majority of NEs were in-
creased considerably at the end, with the exception
of SKILL for which the IAA ended up being slightly
lower as well as DOB and PER of which there are
not sufficient examples in either sets to obtain re-
liably results.10 There are very large increases in
IAA for JOB and ORG entities, as we discovered dur-
ing the pilot annotation that the guidelines for those
markables were not concrete enough regarding their
boundaries and definitions. Their final IAA figures
show that both of these types of NEs were still most
difficult to annotate at the end. However, a final total
IAA of 84.8 F1 for all NEs is a relatively high score.
In comparison, the final IAA score of 97.1 F1 for the
zone titles shows that recognizing zone titles is an
even easier task for humans to perform compared to
recognizing NEs.
When calculating IAA for relations, only those re-
lations for which both annotators agreed on the NEs
were included. This is done to get an idea of the
difficulty of the RE task independently of NER. Re-
lation IAA was also measured using F1, where rela-
tions are counted as equal if they connect exactly the
same NE pair. The IAA for relations between NEs
within CVs is relatively high both during the pilot
annotation and at the end of the main annotation and
only increased slightly over this time. These figures
show that this task is much easier than annotating re-
lations in other domains, e.g. in biomedical research
papers (Alex et al, 2008a).
The IAA figures show that even with cyclic anno-
tation, evolving guidelines and continuous updating,
human annotators can find it challenging to annotate
some markables consistently. This has an effect on
the results of the automatic annotation where the an-
notated data is used to train ML-based models and
to evaluate their performance.
10The reason why there are no figures for POSTCODE and
TIMESPAN entities for the pilot annotation is that none appeared
in those documents.
34
(1) 2nd Pilot Annotation (2) End of Main Annotation (3) Automatic Annotation
Type P R F1 TPs P R F1 TPs P R F1 TPs
Named Entities
ADDRESS 100.0 100.0 100.0 1 100.0 100.0 100.0 10 13.8 16.0 14.8 8
DATE 62.5 92.6 74.6 25 98.5 98.5 98.5 191 94.1 95.7 94.9 1,850
DATERANGE 91.3 95.5 93.3 21 98.6 97.3 97.9 71 91.4 87.0 89.2 637
DOB 100.0 100.0 100.0 1 75.0 100.0 85.7 3 70.8 70.8 70.8 17
EMAIL 100.0 100.0 100.0 2 100.0 100.0 100.0 8 95.9 100.0 97.9 93
JOB 39.1 52.9 45.0 9 72.5 69.9 71.2 95 70.5 61.4 65.6 742
LOC 88.9 100.0 94.1 16 100.0 95.8 97.9 137 83.2 87.3 85.2 1,259
ORG 68.0 81.0 73.9 17 93.4 86.4 89.8 171 57.1 44.7 50.2 749
PER 100.0 100.0 100.0 2 100.0 95.0 97.4 19 69.8 40.5 51.2 196
PHONE 100.0 100.0 100.0 4 100.0 100.0 100.0 16 90.9 85.7 88.2 90
POSTCODE - - - - 90.9 90.9 90.9 10 98.3 71.3 82.6 57
QUAL 9.1 7.7 8.3 1 68.4 81.3 74.3 13 53.9 27.2 36.1 56
SKILL 76.6 86.8 81.4 210 79.3 79.0 79.2 863 67.9 66.5 67.2 5,645
TIMESPAN - - - - 91.7 91.7 91.7 33 74.0 76.8 75.4 179
URL 100.0 100.0 100.0 2 100.0 100.0 100.0 43 97.2 90.5 93.7 209
All 73.0 84.1 78.1 311 85.4 84.2 84.8 1,683 73.5 69.4 71.4 11,787
Zone Titles
EDUCATION 100.0 100.0 100.0 3 100.0 100.0 100.0 9 86.3 75.0 80.3 63
EMPLOYMENT 100.0 100.0 100.0 1 100.0 88.9 94.1 8 83.1 69.7 75.8 69
OTHER 100.0 100.0 100.0 1 - - - - 39.3 28.2 32.8 22
PERSONAL 25.0 25.0 25.0 1 100.0 100.0 100.0 4 65.4 53.1 58.6 17
REFERENCES 100.0 100.0 100.0 1 100.0 100.0 100.0 3 94.4 89.5 91.9 17
SKILLS 33.3 40.0 36.4 2 100.0 100.0 100.0 7 63.8 38.9 48.4 44
SUMMARY - - - - 75.0 100.0 85.7 3 82.2 64.9 72.6 37
All 56.3 60.0 58.1 9 97.1 97.1 97.1 34 72.7 55.8 63.2 269
Relations
DATE-JOB - - - - 100.0 83.3 90.9 10 28.1 44.7 34.5 110
DATE-LOC - - - - 88.9 72.7 80.0 8 71.3 52.7 60.6 223
DATE-ORG - - - - 100.0 88.2 93.8 15 53.0 51.5 52.3 218
DATE-QUAL - - - - 100.0 100.0 100.0 6 60.6 73.1 66.3 57
DATERANGE-JOB 77.8 100.0 87.5 7 91.7 100.0 95.7 66 80.4 72.5 76.2 663
DATERANGE-LOC 91.7 100.0 95.7 11 85.4 79.6 82.4 70 82.0 82.7 82.4 735
DATERANGE-ORG 93.8 100.0 96.8 15 80.2 76.2 78.2 77 72.2 76.4 74.2 644
DATERANGE-QUAL 100.0 100.0 100.0 1 100.0 100.0 100.0 21 71.1 62.1 66.3 59
DATERANGE-SKILL 89.0 98.1 93.3 105 82.2 100.0 90.5 352 61.1 33.7 43.4 1,574
DATE-SKILL 100.0 9.1 16.7 1 95.0 67.1 78.6 57 23.6 54.5 33.0 368
JOB-LOC NaN 0.0 NaN 0 91.8 65.6 76.5 78 77.0 69.1 72.8 932
JOB-ORG 87.5 100.0 93.3 7 86.8 73.3 79.5 99 64.6 50.7 56.8 758
JOB-TIMESPAN - - - - 85.7 54.6 66.7 6 56.0 61.8 58.8 47
LOC-ORG NaN 0.0 NaN 0 89.6 71.4 79.5 120 79.7 78.9 79.3 1,044
LOC-QUAL NaN 0.0 NaN 0 100.0 100.0 100.0 19 75.6 78.7 77.1 133
LOC-TIMESPAN - - - - 100.0 75.0 85.7 3 48.2 36.1 41.3 13
ORG-QUAL NaN 0.0 NaN 0 95.2 95.2 95.2 20 77.8 71.4 74.5 140
ORG-TIMESPAN - - - - 83.3 55.6 66.7 5 55.9 33.3 41.8 19
SKILL-TIMESPAN - - - - 86.1 74.0 79.6 37 59.5 52.6 55.8 280
All 85.5 83.1 84.2 147 86.8 82.6 84.6 1,069 63.1 55.3 59.0 8,017
Table 4: IAA for NEs, zone titles and relations in precision (P), recall (R) and F1 at two stages in the
annotation process: (1) at the end of the second pilot annotation and (2) at the end of the main annotation
phase; as well as automatic annotation scores (3) on the blind TEST set. The total number of true positives
(TPs) is shown to provide an idea of the quantities of markables in each set.
35
6 Automatic Annotation
Table 4 also lists the final scores of the automatic
ML-based NER and RE components (Alex et al,
2008b) which were adapted to the recruitment do-
main during the TXV project. Following agile
methods, we trained and evaluated models very
early into the annotation process. During the sys-
tem optimization, learning curves helped to investi-
gate for which markables having more training data
available would improve performance.
The NER component recognizes NEs and zone
titles simultaneously with an overall F1 of 71.4
(84.2% of IAA) and 63.2 (65.0% of IAA), respec-
tively. Extremely high or higher than average scores
were obtained for DATE, DATERANGE, EMAIL, LOC,
PHONE, POSTCODE, TIMESPAN and URL entities.
Mid-range to lower scores were obtained for AD-
DRESS, DOB, JOB, ORG, PER, QUAL and SKILL enti-
ties. One reason is the similarity between NE types,
e.g. DOB is difficult to differentiate from DATE. The
layout of CVs and the lack of full sentences also
pose a challenge as the NER component is trained
using contextual features surrounding NEs that are
often not present in CV data. Finally, the strict eval-
uation counts numerous boundary errors for NEs
which can be considered correct, e.g. the system
often recognizes organization names like ?Sun Mi-
crosystems, Inc? whereas the annotator included the
full stop at the end (?Sun Microsystems, Inc.?).
The RE component (Haddow, 2008) performs
with an overall F1 of 59.0 on the CV TEST set
(69.7% of IAA). It yields high or above aver-
age scores for 10 relation types (DATE-LOC, DATE-
QUAL, DATERANGE-JOB, DATERANGE-LOC, DAT-
ERANGE-ORG, DATERANGE-QUAL, JOB-LOC, LOC-
ORG, LOC-QUAL, ORG-QUAL). It yields mid-range
to low scores for the other relation types (DATE-
JOB, DATE-ORG, DATERANGE-SKILL, DATE-SKILL,
JOB-ORG, JOB-TIMESPAN, LOC-TIMESPAN, ORG-
TIMESPAN, SKILL-TIMESPAN). The most frequent
type is DATERANGE-SKILL, a skill obtained during a
particular time period. Its entities tend to be found in
the same zone but not always in immediate context.
Such relations are inter-sentential, i.e. their entities
are in different sentences or what is perceived as sen-
tences by the system. Due to nature of the data, there
are few intra-sentential relations, relations between
NEs in the same sentence. The further apart two re-
lated NEs are, the more difficult it is to recognize
them. Similarly to NER, one challenge for RE from
CVs is their diverse structure and formatting.
7 Discussion and Conclusion
The increase in the IAA figures for the markables
over time show that agile corpus annotation resulted
in more qualitative annotations. It is difficult to
prove that the final annotation quality is higher than
it would have been had we followed the traditional
way of annotation. Comparing two such methods in
parallel is very difficult to achieve as the main aim
of annotation is usually to create a corpus and not to
investigate the best and most efficient method.
However, using the agile approach we identified
problems early on and made improvements to the
annotation scheme and the setup during the process
rather than at the end. Given a fixed annotation time
frame and the proportion of time we spent on cor-
recting errors throughout the annotation process, one
might conclude that we annotated less data than we
may have done, had we not followed the agile ap-
proach. However, Voormann and Hut (2008) argue
that agile annotation actually results in more useable
data at the end and in less data being thrown away.
Had we followed the traditional approach, we
would unlikely have planned a correction phase at
the end. The two main reason for that are cost
and the general belief that the more annotated data
the better. A final major correction phase is usu-
ally viewed as too expensive during an annotation
project. In order to avoid this cost, the traditional ap-
proach taken tends to be to create a set of annotation
guidelines when starting out and hold off the main
annotation until the guidelines are finalized and con-
sidered sufficiently defined. This approach does not
lend itself well to changes and adjustments later on
which are inevitable when dealing with natural lan-
guage. As a result the final less accurate annotated
corpus tends to be accepted as the ground truth or
gold standard and may not be as suitable and useful
for a given purpose as it could have been follow-
ing the agile annotation approach. Besides changing
the way in which annotators work, we recognize the
need for more flexible annotation tools that allow an-
notators to implement changes more rapidly.
36
References
Beatrice Alex, Malvina Nissim, and Claire Grover. 2006.
The impact of annotation on the performance of pro-
tein tagging in biomedical text. In Proceedings of the
5th International Conference on Language Resources
and Evaluation (LREC 2006), Genoa, Italy.
Bea Alex, Claire Grover, Barry Haddow, Mijail Kabad-
jov, Ewan Klein, Michael Matthews, Richard Tobin,
and Xinglong Wang. 2008a. The ITI TXM corpora:
Tissue expressions and protein-protein interactions. In
Proceedings of the Workshop on Building and Evalu-
ating Resources for Biomedical Text Mining at LREC
2008, Marrakech, Morocco.
Beatrice Alex, Claire Grover, Barry Haddow, Mijail
Kabadjov, Ewan Klein, Michael Matthews, Richard
Tobin, and Xinglong Wang. 2008b. Automating cu-
ration using a natural language processing pipeline.
Genome Biology, 9(Suppl 2):S10.
Sue Atkins, Jeremy Clear, and Nicholas Ostler. 1992.
Corpus design criteria. Literary and Linguistic Com-
puting, 7(1):1?16.
Douglas Biber. 1993. Representativeness in corpus de-
sign. Literary and Linguistic Computing, 8(4):243?
257.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37?46.
Barry Haddow. 2008. Using automated feature optimisa-
tion to create an adaptable relation extraction system.
In Proceedings of BioNLP 2008, Columbus, Ohio.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy. New Re-
sources, New Tools, New Methods., pages 197?214.
Peter Lang, Frankfurt. (English Corpus Linguistics,
Vol.3).
Va?clav Nova?k and Magda Raz??mova?. 2009. Unsu-
pervised detection of annotation inconsistencies using
apriori algorithm. In Proceedings of the Third Linguis-
tic Annotation Workshop (LAW III), pages 138?141,
Suntec, Singapore.
Winston Royce. 1970. Managing the development
of large software systems. In Proceedings of IEEE
WESCON, pages 1?9.
Holger Voormann and Ulrike Gut. 2008. Agile corpus
creation. Corpus Linguistics and Linguistic Theory,
4(2):235?251.
37
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 28?36,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Creating Sentiment Dictionaries via Triangulation
Josef Steinberger,
Polina Lenkova, Mohamed Ebrahim,
Maud Ehrmann, Ali Hurriyetoglu,
Mijail Kabadjov, Ralf Steinberger,
Hristo Tanev and Vanni Zavarella
EC Joint Research Centre
21027, Ispra (VA), Italy
Name.Surname@jrc.ec.europa.eu
Silvia Va?zquez
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona
silvia.vazquez@upf.edu
Abstract
The paper presents a semi-automatic approach
to creating sentiment dictionaries in many lan-
guages. We first produced high-level gold-
standard sentiment dictionaries for two lan-
guages and then translated them automatically
into third languages. Those words that can
be found in both target language word lists
are likely to be useful because their word
senses are likely to be similar to that of the
two source languages. These dictionaries can
be further corrected, extended and improved.
In this paper, we present results that verify
our triangulation hypothesis, by evaluating tri-
angulated lists and comparing them to non-
triangulated machine-translated word lists.
1 Introduction
When developing software applications for senti-
ment analysis or opinion mining, there are basi-
cally two main options: (1) writing rules that assign
sentiment values to text or text parts (e.g. names,
products, product features), typically making use of
dictionaries consisting of sentiment words and their
positive or negative values, and (2) inferring rules
(and sentiment dictionaries), e.g. using machine
learning techniques, from previously annotated doc-
uments such as product reviews annotated with an
overall judgment of the product. While movie or
product reviews for many languages can frequently
be found online, sentiment-annotated data for other
fields are not usually available, or they are almost
exclusively available for English. Sentiment dictio-
naries are also mostly available for English only or,
if they exist for other languages, they are not com-
parable, in the sense that they have been developed
for different purposes, have different sizes, are based
on different definitions of what sentiment or opinion
means.
In this paper, we are addressing the resource bot-
tleneck for sentiment dictionaries, by developing
highly multilingual and comparable sentiment dic-
tionaries having similar sizes and based on a com-
mon specification. The aim is to develop such dic-
tionaries, consisting of typically one or two thou-
sand words, for tens of languages, although in this
paper we only present results for eight languages
(English, Spanish, Arabic, Czech, French, German,
Italian and Russian). The task raises the obvious
question how the human effort of producing this re-
source can be minimized. Simple translation, be it
using standard dictionaries or using machine trans-
lation, is not very efficient as most words have two,
five or ten different possible translations, depending
on context, part-of-speech, etc.
The approach we therefore chose is that of trian-
gulation. We first produced high-level gold-standard
sentiment dictionaries for two languages (English
and Spanish) and then translated them automatically
into third languages, e.g. French. Those words that
can be found in both target language word lists (En
Fr and Es Fr) are likely to be useful because their
word senses are likely to be similar to that of the
two source languages. These word lists can then be
used as they are or better they can be corrected, ex-
tended and improved. In this paper, we present eval-
uation results verifying our triangulation hypothesis,
by evaluating triangulated lists and comparing them
28
to non-triangulated machine-translated word lists.
Two further issues need to be addressed. The
first one concerns morphological inflection. Auto-
matic translation will yield one word form (often,
but not always the base form), which is not suffi-
cient when working with highly inflected languages:
A single English adjective typically has four Spanish
or Italian word forms (two each for gender and for
number) and many Russian word forms (due to gen-
der, number and case distinctions). The target lan-
guage word lists thus need to be expanded to cover
all these morphological variants with minimal effort
and considering the number of different languages
involved without using software, such as morpho-
logical analysers or generators. The second issue
has to do with the subjectivity involved in the human
annotation and evaluation effort. First of all, it is im-
portant that the task is well-defined (this is a chal-
lenge by itself) and, secondly, the inter-annotator
agreement for pairs of human evaluators working on
different languages has to be checked in order to get
an idea of the natural variation involved in such a
highly subjective task.
Our main field of interest is news opinion min-
ing. We would like to answer the question how cer-
tain entities (persons, organisations, event names,
programmes) are discussed in different media over
time, comparing different media sources, media in
different countries, and media written in different
languages. One possible end product would be a
graph showing how the popularity of a certain en-
tity has changed over time across different languages
and countries. News differs significantly from those
text types that are typically analysed in opinion min-
ing work, i.e. product or movie reviews: While a
product review is about a product (e.g. a printer)
and its features (e.g. speed, price or printing qual-
ity), the news is about any possible subject (news
content), which can by itself be perceived to be pos-
itive or negative. Entities mentioned in the news can
have many different roles in the events described.
If the method does not specifically separate positive
or negative news content from positive or negative
opinion about that entity, the sentiment analysis re-
sults will be strongly influenced by the news context.
For instance, the automatically identified sentiment
towards a politician would most likely to be low if
the politician is mentioned in the context of nega-
tive news content such as bombings or disasters. In
our approach, we therefore aim to distinguish news
content from sentiment values, and this distinction
has an impact on the sentiment dictionaries: unlike
in other approaches, words like death, killing, award
or winner are purposefully not included in the sen-
timent dictionaries as they typically represent news
content.
The rest of the paper is structured as follows: the
next section (2) describes related work, especially
in the context of creating sentiment resources. Sec-
tion 3 gives an overview of our approach to dic-
tionary creation, ranging from the automatic learn-
ing of the sentiment vocabulary, the triangulation
process, the expansion of the dictionaries in size
and regarding morphological inflections. Section 4
presents a number of results regarding dictionary
creation using simple translation versus triangula-
tion, morphological expansion and inter-annotator
agreement. Section 5 summarises, concludes and
points to future work.
2 Related Work
Most of the work in obtaining subjectivity lexicons
was done for English. However, there were some
authors who developed methods for the mapping of
subjectivity lexicons to other languages. Kim and
Hovy (2006) use a machine translation system and
subsequently use a subjectivity analysis system that
was developed for English. Mihalcea et al (2007)
propose a method to learn multilingual subjective
language via cross-language projections. They use
the Opinion Finder lexicon (Wilson et al, 2005)
and two bilingual English-Romanian dictionaries to
translate the words in the lexicon. Since word am-
biguity can appear (Opinion Finder does not mark
word senses), they filter as correct translations only
the most frequent words. The problem of translat-
ing multi-word expressions is solved by translating
word-by-word and filtering those translations that
occur at least three times on the Web. Another ap-
proach in obtaining subjectivity lexicons for other
languages than English was explored in Banea et al
(2008b). To this aim, the authors perform three dif-
ferent experiments, with good results. In the first
one, they automatically translate the annotations of
the MPQA corpus and thus obtain subjectivity an-
29
notated sentences in Romanian. In the second ap-
proach, they use the automatically translated entries
in the Opinion Finder lexicon to annotate a set of
sentences in Romanian. In the last experiment, they
reverse the direction of translation and verify the as-
sumption that subjective language can be translated
and thus new subjectivity lexicons can be obtained
for languages with no such resources. Finally, an-
other approach to building lexicons for languages
with scarce resources is presented in Banea et al
(2008a). In this research, the authors apply boot-
strapping to build a subjectivity lexicon for Roma-
nian, starting with a set of seed subjective entries,
using electronic bilingual dictionaries and a training
set of words. They start with a set of 60 words per-
taining to the categories of noun, verb, adjective and
adverb obtained by translating words in the Opin-
ion Finder lexicon. Translations are filtered using a
measure of similarity to the original words, based on
Latent Semantic Analysis (Landauer and Dumais,
1997) scores. Wan (2008) uses co-training to clas-
sify un-annotated Chinese reviews using a corpus
of annotated English reviews. He first translates
the English reviews into Chinese and subsequently
back to English. He then performs co-training using
all generated corpora. Banea et al (2010) translate
the MPQA corpus into five other languages (some
with a similar ethimology, others with a very differ-
ent structure). Subsequently, they expand the fea-
ture space used in a Naive Bayes classifier using the
same data translated to 2 or 3 other languages. Their
conclusion is that expanding the feature space with
data from other languages performs almost as well
as training a classifier for just one language on a
large set of training data.
3 Approach Overview
Our approach to dictionary creation starts with semi-
automatic way of colleting subjective terms in En-
glish and Spanish. These pivot language dictionaries
are then projected to other languages. The 3rd lan-
guage dictionaries are formed by the overlap of the
translations (triangulation). The lists are then man-
ually filtered and expanded, either by other relevant
terms or by their morphological variants, to gain a
wider coverage.
3.1 Gathering Subjective Terms
We started with analysing the available English
dictionaries of subjective terms: General Inquirer
(Stone et al, 1966), WordNet Affect (Strapparava
and Valitutti, 2004), SentiWordNet (Esuli and Se-
bastiani, 2006), MicroWNOp (Cerini et al, 2007).
Additionally, we used the resource of opinion words
with associated polarity from Balahur et al (2009),
which we denote as JRC Tonality Dictionary. The
positive effect of distinguishing two levels of inten-
sity was shown in (Balahur et al, 2010). We fol-
lowed the idea and each of the emloyed resources
was mapped to four categories: positive, negative,
highly positive and highly negative. We also got
inspired by the results reported in that paper and
we selected as the base dictionaries the combination
of MicroWNOp and JRC Tonality Dictionary which
gave the best results. Terms in those two dictionar-
ies were manually filtered and the other dictionar-
ies were used as lists of candidates (their highly fre-
quent terms were judged and the relevant ones were
included in the final English dictionary). Keeping in
mind the application of the dictionaries we removed
at this step terms that are more likely to describe bad
or good news content, rather than a sentiment to-
wards an entity. In addition, we manually collected
English diminishers (e.g. less or approximately), in-
tensifiers (e.g. very or indeed) and invertors (e.g.
not or barely). The English terms were translated to
Spanish and the same filtering was performed. We
extended all English and Spanish lists with the miss-
ing morphological variants of the terms.
3.2 Automatic Learning of Subjective Terms
We decided to expand our subjective term lists by
using automatic term extraction, inspired by (Riloff
and Wiebe, 2003). We look at the problem of ac-
quisition of subjective terms as learning of seman-
tic classes. Since we wanted to do this for two dif-
ferent languages, namely English and Spanish, the
multilingual term extraction algorithm Ontopopulis
(Tanev et al, 2010) was a natural choice.
Ontopopulis performs weakly supervised learning
of semantic dictionaries using distributional similar-
ity. The algorithm takes on its input a small set of
seed terms for each semantic class, which is to be
learnt, and an unannotated text corpus. For example,
30
if we want to learn the semantic class land vehicles,
we can use the seed set - bus, truck, and car. Then
it searches for the terms in the corpus and finds lin-
ear context patterns, which tend to co-occur imme-
diately before or after these terms. Some of the
highest-scored patterns, which Ontopopulis learned
about land vehicles were driver of the X, X was
parked, collided with another X, etc. Finally, the
algorithm searches for these context patterns in the
corpus and finds other terms which tend to fill the
slot of the patterns (designated by X). Considering
the land vehicles example, new terms which the sys-
tem learned were van, lorry, taxi, etc. Ontopop-
ulis is similar to the NOMEN algorithm (Lin et al,
2003). However, Ontopopulis has the advantage to
be language-independent, since it does not use any
form of language-specific processing, nor does it use
any language-specific resources, apart from a stop
word list.
In order to learn new subjective terms for each
of the languages, we passed the collected subjective
terms as an input to Ontopopulis. For English, we
divided the seed set in two classes: class A ? verbs
and class B ? nouns and adjectives. It was necessary
because each of these classes has a different syn-
tactic behaviour. It made sense to do the same for
Spanish, but we did not have enough Spanish speak-
ers available to undertake this task, therefore we put
together all the subjective Spanish words - verbs, ad-
jectives and nouns in one class. We ran Ontopopulis
for each of the three classes - the class of subjective
Spanish words and the English classes A and B. The
top scored 200 new learnt terms were taken for each
class and manually reviewed.
3.3 Triangulation and Expansion
After polishing the pivot language dictionaries we
projected them to other languages. The dictionaries
were translated by Google translator because of its
broad coverage of languages. The overlapping terms
between English and Spanish translations formed
the basis for further manual efforts. In some cases
there were overlapping terms in English and Span-
ish translations but they differed in intensity. There
was the same term translated from an English posi-
tive term and from a Spanish very positive term. In
these cases the term was assigned to the positive cat-
egory. However, more problematic cases arose when
the same 3rd language term was assigned to more
than one category. There were also cases with dif-
ferent polarity. We had to review them manually.
However, there were still lots of relevant terms in the
translated lists which were not translated from the
other language. These complement terms are a good
basis for extending the coverage of the dictionaries,
however, they need to be reviewed manually. Even if
we tried to include in the pivot lists all morpholog-
ical variants, in the triangulation output there were
only a few variants, mainly in the case of highly in-
flected languages. To deal with morphology we in-
troduced wild cards at the end of the term stem (*
stands for whatever ending and for whatever char-
acter). This step had to be performed carefully be-
cause some noise could be introduced. See the Re-
sults section for examples. Although this step was
performed by a human, we checked the most fre-
quent terms afterwards to avoid irrelavant frequent
terms.
4 Results
4.1 Pivot dictionaries
We gathered and filtered English sentiment terms
from the available corpora (see Section 3.1). The
dictionaries were then translated to Spanish (by
Google translator) and filtered afterwards. By ap-
plying automatic term extraction, we enriched the
sets of terms by 54 for English and 85 for Spanish,
after evaluating the top 200 candidates suggested by
the Ontopolulis tool for each language. The results
are encouraging, despite the relevance of the terms
(27% for English and 42.5% for Spanish where
some missing morphological variants were discov-
ered) does not seem to be very high, considering the
fact that we excluded the terms already contained
in the pivot lists. If we took them into account, the
precision would be much better. The initial step re-
sulted in obtaining high quality pivot sentiment dic-
tionaries for English and Spanish. Their statistics
are in table 1. We gathered more English terms than
Spanish (2.4k compared to 1.7k). The reason for
that is that some translations from English to Span-
ish have been filtered. Another observation is that
there is approximately the same number of negative
terms as positive ones, however, much more highly
negative than highly positive terms. Although the
31
Language English Spanish
HN 554 466
N 782 550
P 772 503
HP 171 119
INT 78 62
DIM 31 27
INV 15 10
TOTAL 2.403 1.737
Table 1: The size of the pilot dictionaries. HN=highly
negative terms, N=negative, P=positive, HP=highly posi-
tive, INV=invertors, DIM=diminishers, INV=invertors.
frequency analysis we carried out later showed that
even if there are fewer highly positive terms, they are
more frequent than the highly negative ones, which
results in almost uniform distribution.
4.2 Triangulation and Expansion
After running triangulation to other languages the
resulted terms were judged for relevance. Native
speakers could suggest to change term?s category
(e.g. negative to highly negative) or to remove it.
There were several reasons why the terms could
have been marked as ?non-sentiment?. For instance,
the term could tend to describe rather negative news
content than negative sentiment towards an entity
(e.g. dead, quake). In other cases the terms were
too ambiguous in a particular language. Examples
from English are: like or right.
Table 2 shows the quality of the triangulated dic-
tionaries. In all cases except for Italian we had only
one annotator assessing the quality. We can see that
the terms were correct in around 90% cases, how-
ever, it was a little bit worse in the case of Russian
in which the annotator suggested to change category
very often.
Terms translated from English but not from Span-
ish are less reliable but, if reviewed manually, the
dictionaries can be expanded significantly. Table 3
gives the statistics concerning these judgments. We
can see that their correctness is much lower than in
the case of the triangulated terms - the best in Italian
(54.4%) and the worst in Czech (30.7%). Of course,
the translation performance affects the results here.
However, this step extended the dictionaries by ap-
proximately 50%.
When considering terms out of context, the most
common translation error occurs when the original
word has several meanings. For instance, the En-
glish word nobility refers to the social class of no-
bles, as well as to the quality of being morally good.
In the news context we find this word mostly in the
second meaning. However, in the Russian triangu-
lated list we have found dvoryanstvo , which refers
to a social class in Russian. Likewise, we need to
keep in mind that a translation of a monosemantic
word might result polysemantic in the target lan-
guage, thereby leading to confusion. For example,
the Italian translation of the English word champion
campione is more frequently used in Italian news
context in a different meaning - sample, therefore
we must delete it from our sentiment words list for
Italian. Another difficulty we might encounter es-
pecially when dealing with inflectional languages is
the fact that a translation of a certain word might be
homographic with another word form in the target
language. Consider the English negative word ban-
dit and its Italian translation bandito, which is more
frequently used as a form of the verb bandire (to an-
nounce) in the news context. Also each annotator
had different point of view on classifying the bor-
derline cases (e.g. support, agreement or difficult).
Two main reasons are offered to explain the low
performance in Arabic. On the one hand, it seems
that some Google translation errors will be repeated
in different languages if the translated words have
the same etymological root. For example both words
? the English fresh and the Spanish fresca ? are
translated to the Arabic as YK
Yg. meaning new. The
Other reason is a more subtle one and is related to
the fact that Arabic words are not vocalized and to
the way an annotator perceive the meaning of a given
word in isolation. To illustrate this point, consider
the Arabic word ? J. ?A
	
J ?? @ , which could be used
as an adjective, meaning appropriate, or as a noun,
meaning The occasion. It appears that the annotator
would intuitively perceive the word in isolation as a
noun and not as an adjective, which leads to disre-
garding the evaluative aspects of a given word.
We tried to include in the pivot dictionaries all
morphological variants of the terms. However, in
highly inflected languages there are much more vari-
ants than those translated from English or Spanish.
32
We manually introduced wild cards to capture the
variants. We had to be attentive when compiling
wild cards for languages with a rich inflectional sys-
tem, as we might easily get undesirable words in the
output. To illustrate this, consider the third person
plural of the Italian negative word perdere (to lose)
perdono, which is also homographic with the word
meaning forgiveness in English. Naturally, it could
happen that the wildcard captures a non-sentiment
term or even a term with a different polarity. For in-
stance, the pattern care% would capture either care,
careful, carefully, but also career or careless. That
is way we perform the last manual checking after
matching the lists expanded by wildcards against a
large number of texts. The annotators were unable
to check all the variants, but only the most frequent
terms, which resulted in reviewing 70-80% of the
term mentions. This step has been performed for
only English, Czech and Russian so far. Table 5
gives the statistics. By introducing the wildcards,
the number of distinct terms grew up significantly
- 12x for Czech, 15x for Russian and 4x for En-
glish. One reason why it went up also for English
is that we captured compounds like: well-arranged,
well-balanced, well-behaved, well-chosen by a sin-
gle pattern. Another reason is that a single pat-
tern can capture different POSs: beaut% can cap-
ture beauty, beautiful, beautifully or beautify. Not
all of those words were present in the pivot dictio-
naries. For dangerous cases like care% above we
had to rather list all possible variants than using a
wildcard. This is also the reason why the number
of patterns is not much lower than the number of
initial terms. Even if this task was done manually,
some noise was added into the dictionaries (92-94%
of checked terms were correct). For example, highly
positive pattern hero% was introduced by an anno-
tator for capturing hero, heroes, heroic, heroical or
heroism. If not checked afterwards heroin would
score highly positively in the sentiment system. An-
other example is taken from Russian: word meaning
to steal ukra% - might generate Ukraine as one most
frequent negative word in Russian.
4.3 How subjective is the annotation?
Sentiment annotation is a very subjective task. In ad-
dition, annotators had to judge single terms without
any context: they had to think about all the senses of
Metric Percent Agreement Kappa
HN 0.909 0.465
N 0.796 0.368
P 0.714 0.281
HP 0.846 0
N+HN 0.829 0.396
P+HP 0.728 0.280
ALL 0.766 0.318
Table 6: Inter-annotator agreement on checking the trian-
gulated list. In the case of HP all terms were annotated as
correct by one of the annotators resulting in Kappa=0.
Metric Percent Agreement Kappa
HN 0.804 0.523
N 0.765 0.545
P 0.686 0.405
HP 0.855 0.669
N+HN 0.784 0.553
P+HP 0.783 0.559
ALL 0.826 0.614
Table 7: Inter-annotator agreement on checking the can-
didates. In ALL diminishers, intensifiers and invertors
are included as well.
the term. Only if the main sense was subjective they
agreed to leave it in the dictionary. Another sub-
jectivity level was given by concentrating on distin-
guishing news content and news sentiment. Defining
the line between negative and highly negative terms,
and similarly with positive, is also subjective. In the
case of Italian we compared judgments of two anno-
tators. The figures of inter-annotator agreement of
annotating the triangulated terms are in table 6 and
the complement terms in table 7. Based on the per-
cent agreement the annotators agree a little bit less
on the triangulated terms (76.6%) compared to the
complement terms (82.6%). However, if we look at
Kappa figures, the difference is clear. Many terms
translated only from English were clearly wrong
which led to a higher agreement between the annota-
tors (0.318 compared to 0.614). When looking at the
difference between positive and negative terms, we
can see that there was higher agreement on the neg-
ative triangulated terms then on the positive ones.
33
Language Triangulated Correct Removed Changed category
Arabic 926 606 (65.5%) 316 (34.1%) 4 (0.4%)
Czech 908 809 (89.1%) 68 (7.5%) 31 (3.4%)
French 1.085 956 (88.1%) 120 (11.1%) 9 (0.8%)
German 1.053 982 (93.3%) 50 (4.7%) 21 (2.0%)
Italian 1.032 918 (89.0%) 36 (3.5%) 78 (7.5%)
Russian 966 816 (84.5%) 49 (5.1%) 101 (10.4%)
Table 2: The size and quality of the triangulated dictionaries. Triangulated=No. of terms coming directly from triangu-
lation, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive from triangulation, but annotator changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 1.092 335 (30.7%) 675 (61.8%) 82 (7.5%)
French 1.226 617 (50.3%) 568 (46.3%) 41 (3.4%)
German 1.182 548 (46.4%) 610 (51.6%) 24 (2.0%)
Italian 1.069 582 (54.4%) 388 (36.3%) 99 (9.3%)
Russian 1.126 572 (50.8%) 457 (40.6%) 97 (8.6%)
Table 3: The size and quality of the candidate terms (translated from English but not from Spanish). Terms=No. of
terms translated from English but not from Spanish, Correct=terms annotated as correct, Removed=terms not relevant
to sentiment analysis, Change category=terms in wrong category (e.g., positive in the original list, but annotator
changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 2.000 1.144 (57.2%) 743 (37.2%) 113 (5.6%)
French 2.311 1.573 (68.1%) 688 (29.8%) 50 (2.1%)
German 2.235 1.530 (68.5%) 660 (29.5%) 45 (2.0%)
Italian 2.101 1.500 (71.4%) 424 (20.2%) 177 (8.4%)
Russian 2.092 1.388 (66.3%) 506 (24.2%) 198 (9.5%)
Table 4: The size and quality of the translated terms from English. Terms=No. of (distinct) terms translated from En-
glish, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive in the original list, but annotator changed the category to highly positive).
Language Initial terms Patterns Matched terms
Count Correct Checked
Czech 1.257 1.063 15.604 93.0% 74.4%
English 2.403 2.081 10.558 93.8% 81.1%
Russian 1.586 1.347 33.183 92.2% 71.0%
Table 5: Statistics of introducing wild cards and its evaluation. Initial terms=checked triangulated terms extended by
relevant translated terms from English, Patterns=number of patterns after introducing wildcards, Matched terms=terms
matched in the large corpus - their count and correctness + checked=how many mentions were checked (based on the
fact that the most frequent terms were annotated).
34
4.4 Triangulation vs. Translation
Table 4 present the results of simple translation from
English (summed up numbers from tables 2 and 3).
We can directly compare it to table 2 where only
results of triangulated terms are reported. The per-
formance of triangulation is significantly better than
the performance of translation in all languages. The
highest difference was in Czech (89.1% and 57.2%)
and the lowest was in Italian (89.0% and 71.4%).
As a task-based evaluation we used the triangu-
lated/translated dictionaries in the system analysing
news sentiment expressed towards entities. The sys-
tem analyses a fixed word window around entity
mentions. Subjective terms are summed up and the
resulting polarity is attached to the entity. Highly
negative terms score twice more than negative, di-
minishers lower and intensifiers lift up the score. In-
vertors invert the polarity but for instance inverted
highly positive terms score as only negative pre-
venting, for instance, not great to score as worst.
The system searches for the invertor only two words
around the subjective term.
We ran the system on 300 German sentences
taken from news gathered by the Europe Media
Monitor (EMM)1. In all these cases the system at-
tached a polarity to an entity mention. We ran it with
three different dictionaries - translated terms from
English, raw triangulated terms (without the man-
ual checking) and the checked triangulated terms.
This pilot experiment revealed the difference in per-
formance on this task. When translated terms were
used there were only 41.6% contexts with correct
polarity assigned by the system, with raw triangu-
lated terms 56.5%, and with checked triangulated
terms 63.4%. However, the number does not contain
neutral cases that would increase the overall perfor-
mance. There are lots of reasons why it goes wrong
here: the entity may not be the target of the sub-
jective term (we do not use parser because of deal-
ing with many languages and large amounts of news
texts), the system can miss or apply wrongly an in-
vertor, the subjective term is used in different sense,
and irony is hard to detect.
1http://emm.newsbrief.eu/overview.html
4.5 State of progress
We finished all the steps for English, Czech and Rus-
sian. French, German, Italian and Spanish dictio-
naries miss only the introduction of wild cards. In
Arabic we have checked only the triangulated terms.
For other 7 languages (Bulgarian, Dutch, Hungarian,
Polish, Portuguese, Slovak and Turkish) we have
only projected the terms by triangulation. However,
we have capabilities to finish all the steps also for
Bulgarian, Dutch, Slovak and Turkish. We haven?t
investigated using more than two pivot languages for
triangulation. It would probably results in more ac-
curate but shortened dictionaires.
5 Conclusions
We presented our semi-automatic approach and cur-
rent state of work of producing multilingual senti-
ment dictionaries suitable of assessing the sentiment
in news expressed towards an entity. The triangula-
tion approach works significantly better than simple
translation but additional manual effort can improve
it a lot in both recall and precision. We believe that
we can predict the sentiment expressed towards an
entity in a given time period based on large amounts
of data we gather in many languages even if the per-
case performance of the sentiment system as on a
moderate level. Now we are working on improving
the dictionaries in all the discussed languages. We
also run experiments to evaluate the system on vari-
ous languages.
Acknowledgments
We thank Alexandra Balahur for her collaboration
and useful comments. This research was partly sup-
ported by a IULA-Universitat Pompeu Fabra grant.
35
References
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
and Bruno Pouliquen. 2009. Opinion mining from
newspaper quotations. In Proceedings of the Work-
shop on Intelligent Analysis and Processing of Web
News Content at the IEEE / WIC / ACM International
Conferences on Web Intelligence and Intelligent Agent
Technology (WI-IAT).
A. Balahur, R. Steinberger, M. Kabadjov, V. Zavarella,
E. van der Goot, M. Halkia, B. Pouliquen, and
J. Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of LREC?10.
C. Banea, R. Mihalcea, and J. Wiebe. 2008a. A boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC.
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan.
2008b. Multilingual subjectivity analysis using ma-
chine translation. In Proceedings of EMNLP.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of COLING.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Andrea Sanso`,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, English linguis-
tics. Franco Angeli, Milano, IT.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceeding of the 6th International Conference on Lan-
guage Resources and Evaluation, Italy, May.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of the ACL Workshop on
Sentiment and Subjectivity in Text.
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of the ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104:211?240.
W. Lin, R. Yangarber, and R. Grishman. 2003. Boot-
strapped learning of semantic classes from positive
and negative examples. In Proceedings of the ICML-
2003 Workshop on The Continuum from Labeled to
Unlabeled Data, Washington DC.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of ACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceeding of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The general inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics, M.I.T. Press, Cambridge, MA.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceeding of the
4th International Conference on Language Resources
and Evaluation, pages 1083?1086, Lisbon, Portugal,
May.
H. Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-
rski, M. Atkinson, and R.Steinberger. 2010. Exploit-
ing machine learning techniques to build an event ex-
traction system for portuguese and spanish. Lingua-
matica: Revista para o Processamento Automatico das
Linguas Ibericas.
X. Wan. 2008. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics and 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP.
36
