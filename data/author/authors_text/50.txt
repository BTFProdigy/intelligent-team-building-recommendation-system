Processing Broadcast Audio for Information Access
Jean-Luc Gauvain, Lori Lamel, Gilles Adda, Martine Adda-Decker,
Claude Barras, Langzhou Chen, and Yannick de Kercadio
Spoken Language Processing Group
LIMSI-CNRS, B.P. 133, 91403 Orsay cedex, France
(gauvain@limsi.fr http://www.limsi.fr/tlp)
Abstract
This paper addresses recent progress in
speaker-independent, large vocabulary,
continuous speech recognition, which
has opened up a wide range of near and
mid-term applications. One rapidly ex-
panding application area is the process-
ing of broadcast audio for information
access. At LIMSI, broadcast news tran-
scription systems have been developed
for English, French, German, Mandarin
and Portuguese, and systems for other
languages are under development. Au-
dio indexation must take into account
the specificities of audio data, such as
needing to deal with the continuous
data stream and an imperfect word tran-
scription. Some near-term applications
areas are audio data mining, selective
dissemination of information and me-
dia monitoring.
1 Introduction
A major advance in speech processing technology
is the ability of todays systems to deal with non-
homogeneous data as is exemplified by broadcast
data. With the rapid expansion of different me-
dia sources, there is a pressing need for automatic
processing of such audio streams. Broadcast au-
dio is challenging as it contains segments of vari-
ous acoustic and linguistic natures, which require
appropriate modeling. A special section in the
Communications of the ACM devoted to ?News
on Demand? (Maybury, 2000) includes contribu-
tions from many of the sites carrying out active
research in this area.
Via speech recognition, spoken document re-
trieval (SDR) can support random access to rel-
evant portions of audio documents, reducing the
time needed to identify recordings in large multi-
media databases. The TREC (Text REtrieval Con-
ference) SDR evaluation showed that only small
differences in information retrieval performance
are observed for automatic and manual transcrip-
tions (Garofolo et al, 2000).
Large vocabulary continuous speech recogni-
tion (LVCSR) is a key technology that can be used
to enable content-based information access in au-
dio and video documents. Since most of the lin-
guistic information is encoded in the audio chan-
nel of video data, which once transcribed can be
accessed using text-based tools. This research has
been carried out in a multilingual environment in
the context of several recent and ongoing Euro-
pean projects. We highlight recent progress in
LVCSR and describe some of our work in de-
veloping a system for processing broadcast au-
dio for information access. The system has two
main components, the speech transcription com-
ponent and the information retrieval component.
Versions of the LIMSI broadcast news transcrip-
tion system have been developed in American En-
glish, French, German, Mandarin and Portuguese.
2 Progress in LVCSR
Substantial advances in speech recognition tech-
nology have been achieved during the last decade.
Only a few years ago speech recognition was pri-
marily associated with small vocabulary isolated
word recognition and with speaker-dependent (of-
ten also domain-specific) dictation systems. The
same core technology serves as the basis for a
range of applications such as voice-interactive
database access or limited-domain dictation, as
well as more demanding tasks such as the tran-
scription of broadcast data. With the exception of
the inherent variability of telephone channels, for
most applications it is reasonable to assume that
the speech is produced in relatively stable envi-
ronmental and in some cases is spoken with the
purpose of being recognized by the machine.
The ability of systems to deal with non-
homogeneous data as is found in broadcast au-
dio (changing speakers, languages, backgrounds,
topics) has been enabled by advances in a vari-
ety of areas including techniques for robust signal
processing and normalization; improved training
techniques which can take advantage of very large
audio and textual corpora; algorithms for audio
segmentation; unsupervised acoustic model adap-
tation; efficient decoding with long span language
models; ability to use much larger vocabularies
than in the past - 64 k words or more is common
to reduce errors due to out-of-vocabulary words.
With the rapid expansion of different media
sources for information dissemination including
via the internet, there is a pressing need for au-
tomatic processing of the audio data stream. The
vast majority of audio and video documents that
are produced and broadcast do not have associ-
ated annotations for indexation and retrieval pur-
poses, and since most of today?s annotation meth-
ods require substantial manual intervention, and
the cost is too large to treat the ever increasing
volume of documents. Broadcast audio is chal-
lenging to process as it contains segments of vari-
ous acoustic and linguistic natures, which require
appropriate modeling. Transcribing such data re-
quires significantly higher processing power than
what is needed to transcribe read speech data
in a controlled environment, such as for speaker
adapted dictation. Although it is usually as-
sumed that processing time is not a major issue
since computer power has been increasing con-
tinuously, it is also known that the amount of data
appearing on information channels is increasing
at a close rate. Therefore processing time is an
important factor in making a speech transcription
system viable for audio data mining and other re-
lated applications. Transcription word error rates
of about 20% have been reported for unrestricted
broadcast news data in several languages.
As shown in Figure 1 the LIMSI broadcast
news transcription system for automatic indexa-
tion consists of an audio partitioner and a speech
recognizer.
3 Audio partitioning
The goal of audio partitioning is to divide the
acoustic signal into homogeneous segments, la-
beling and structuring the acoustic content of the
data, and identifying and removing non-speech
segments. The LIMSI BN audio partitioner re-
lies on an audio stream mixture model (Gauvain
et al, 1998). While it is possible to transcribe the
continuous stream of audio data without any prior
segmentation, partitioning offers several advan-
tages over this straight-forward solution. First,
in addition to the transcription of what was said,
other interesting information can be extracted
such as the division into speaker turns and the
speaker identities, and background acoustic con-
ditions. This information can be used both di-
rectly and indirectly for indexation and retrieval
purposes. Second, by clustering segments from
the same speaker, acoustic model adaptation can
be carried out on a per cluster basis, as opposed
to on a single segment basis, thus providing more
adaptation data. Third, prior segmentation can
avoid problems caused by linguistic discontinu-
ity at speaker changes. Fourth, by using acoustic
models trained on particular acoustic conditions
(such as wide-band or telephone band), overall
performance can be significantly improved. Fi-
nally, eliminating non-speech segments substan-
tially reduces the computation time. The result
of the partitioning process is a set of speech seg-
ments usually corresponding to speaker turns with
speaker, gender and telephone/wide-band labels
(see Figure 2).
4 Transcription of Broadcast News
For each speech segment, the word recognizer de-
termines the sequence of words in the segment,
associating start and end times and an optional
confidence measure with each word. The LIMSI
system, in common with most of today?s state-of-
the-art systems, makes use of statistical models
of speech generation. From this point of view,
message generation is represented by a language
model which provides an estimate of the probabil-
ity of any given word string, and the encoding of
the message in the acoustic signal is represented
by a probability density function. The speaker-
independent 65k word, continuous speech rec-
ognizer makes use of 4-gram statistics for lan-
guage modeling and of continuous density hidden
Markov models (HMMs) with Gaussian mixtures
for acoustic modeling. Each word is represented
by one or more sequences of context-dependent
phone models as determined by its pronunciation.
The acoustic and language models are trained on
large, representative corpora for each task and
language.
Processing time is an important factor in mak-
ing a speech transcription system viable for au-
tomatic indexation of radio and television broad-
casts. For many applications there are limita-
tions on the response time and the available com-
putational resources, which in turn can signifi-
cantly affect the design of the acoustic and lan-
guage models. Word recognition is carried out in
one or more decoding passes with more accurate
acoustic and language models used in successive
passes. A 4-gram single pass dynamic network
decoder has been developed (Gauvain and Lamel,
2000) which can achieve faster than real-time de-
coding with a word error under 30%, running in
less than 100 Mb of memory on widely available
platforms such Pentium III or Alpha machines.
5 Multilinguality
A characteristic of the broadcast news domain is
that, at least for what concerns major news events,
similar topics are simultaneously covered in dif-
ferent emissions and in different countries and
languages. Automatic processing carried out on
contemporaneous data sources in different lan-
guages can serve for multi-lingual indexation and
retrieval. Multilinguality is thus of particular in-
terest for media watch applications, where news
may first break in another country or language.
At LIMSI broadcast news transcription systems
have been developed for the American English,
French, German, Mandarin and Portuguese lan-
guages. The Mandarin language was chosen be-
cause it is quite different from the other lan-
guages (tone and syllable-based), and Mandarin
resources are available via the LDC as well as ref-
erence performance results.
Our system and other state-of-the-art sys-
tems can transcribe unrestricted American En-
glish broadcast news data with word error rates
under 20%. Our transcription systems for French
and German have comparable error rates for news
broadcasts (Adda-Decker et al, 2000). The
character error rate for Mandarin is also about
20% (Chen et al, 2000). Based on our expe-
rience, it appears that with appropriately trained
models, recognizer performance is more depen-
dent upon the type and source of data, than on the
language. For example, documentaries are partic-
ularly challenging to transcribe, as the audio qual-
ity is often not very high, and there is a large pro-
portion of voice over.
6 Spoken Document Retrieval
The automatically generated partition and word
transcription can be used for indexation and in-
formation retrieval purposes. Techniques com-
monly applied to automatic text indexation can
be applied to the automatic transcriptions of the
broadcast news radio and TV documents. These
techniques are based on document term frequen-
cies, where the terms are obtained after standard
text processing, such as text normalization, tok-
enization, stopping and stemming. Most of these
preprocessing steps are the same as those used to
prepare the texts for training the speech recog-
nizer language models. While this offers advan-
tages for speech recognition, it can lead to IR er-
rors. For better IR results, some words sequences
corresponding to acronymns, multiword named-
entities (e.g. Los Angeles), and words preceded
by some particular prefixes (anti, co, bi, counter)
are rewritten as a single word. Stemming is used
to reduce the number of lexical items for a given
word sense. The stemming lexicon contains about
32000 entries and was constructed using Porter?s
algorithm (Porter80, 1980) on the most frequent
words in the collection, and then manually cor-
rected.
The information retrieval system relies on a un-
Lexicon
Acoustic models
Recognition
Word
Audio signal
Language model
Analysis
Acoustic
partitioned
speech acoustic models
Music, noise and
non speech
Filter out
segments
telephone/non-tel models
word transcription
(SGML file)data
Male/female models
Iterative 
segmentation 
and labelling
Figure 1: Overview of an audio transcription system. The audio partitioner divides the data stream into
homogeneous acoustic segments, removing non-speech portions. The word recognizer identifies the
words in each speech segment, associating time-markers with each word.
 
audiofile filename=19980411 1600 1630 CNN HDL language=english 
 
segment type=wideband gender=female spkr=1 stime=50.25 etime=86.83 
 
wtime stime=50.38 etime=50.77  c.n.n.
 
wtime stime=50.77 etime=51.10  headline
 
wtime stime=51.10 etime=51.44  news
 
wtime stime=51.44 etime=51.63  i?m
 
wtime stime=51.63 etime=51.92  robert
 
wtime stime=51.92 etime=52.46  johnson
it is a day of final farewells in alabama the first funerals for victims of this week?s tornadoes are being held today along
with causing massive property damage the twisters killed thirty three people in alabama five in georgia and one each
in mississippi and north carolina the national weather service says the tornado that hit jefferson county in alabama had
winds of more than two hundred sixty miles per hour authorities speculated was the most powerful tornado ever to hit the
southeast twisters destroyed two churches to fire stations and a school parishioners were in one church when the tornado
struck
  /segment 
 
segment type=wideband gender=female spkr=2 stime=88.37 etime=104.86 
at one point when the table came onto my back i thought yes this is it i?m ready ready protects protect the children because
the children screaming the children were screaming they were screaming in prayer that were screaming god help us
  /segment 
 
segment type=wideband gender=female spkr=1 stime=104.86 etime=132.37 
vice president al gore toured the area yesterday he called it the worst tornado devastation he?s ever seen we will have a
complete look at the weather across the u. s. in our extended weather forecast in six minutes
  /segment 
. . .
 
segment type=wideband gender=male spkr=19 stime=1635.60 etime=1645.71 
so if their computing systems don?t tackle this problem well we have a potential business disruption and either erroneous
deliveries or misdeliveries or whatever savvy businesses are preparing now so the january first two thousand would just be
another day on the town not a day when fast food and everything else slows down rick lockridge c.n.n.
  /segment 
  /audiofile 
Figure 2: Example system output obtained by automatic processing of the audio stream of a CNN show
broadcasted on April 11, 1998 at 4pm. The output includes the partitioning and transcription results. To
improve readability, word time stamps are given only for the first 6 words. Non speech segments have
been removed and the following information is provided for each speech segment: signal bandwidth
(telephone or wideband), speaker gender, and speaker identity (within the show).
Transcriptions Werr Base BRF
Closed-captions - 46.9% 54.3%
10xRT 20.5% 45.3% 53.9%
1.4xRT 32.6% 40.9% 49.4%
Table 1: Impact of the word error rate on the
mean average precision using using a 1-gram doc-
ument model. The document collection contains
557 hours of broadcast news from the period of
February through June 1998. (21750 stories, 50
queries with the associated relevance judgments.)
igram model per story. The score of a story is ob-
tained by summing the query term weights which
are simply the log probabilities of the terms given
the story model once interpolated with a general
English model. This term weighting has been
shown to perform as well as the popular TF  IDF
weighting scheme (Hiemstra and Wessel, 1998;
Miller et al, 1998; Ng, 1999; Spa?rk Jones et al,
1998).
The text of the query may or may not include
the index terms associated with relevant docu-
ments. One way to cope with this problem is to
use query expansion (Blind Relevance Feedback,
BRF (Walker and de Vere, 1990)) based on terms
present in retrieved contemporary texts.
The system was evaluated in the TREC SDR
track, with known story boundaries. The SDR
data collection contains 557 hours of broadcast
news from the period of February through June
1998. This data includes 21750 stories and a set
of 50 queries with the associated relevance judg-
ments (Garofolo et al, 2000).
In order to assess the effect of the recogni-
tion time on the information retrieval results we
transcribed the 557 hours of broadcast news data
using two decoder configurations: a single pass
1.4xRT system and a three pass 10xRT system.
The word error rates are measured on a 10h test
subset (Garofolo et al, 2000). The information
retrieval results are given in terms of mean av-
erage precision (MAP), as is done for the TREC
benchmarks in Table 1 with and without query ex-
pansion. For comparison, results are also given
for manually produced closed captions. With
query expansion comparable IR results are ob-
tained using the closed captions and the 10xRT
0
5
10
15
20
25
30
35
40
45
50
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Pe
rc
en
ta
ge
 o
f s
ec
tio
ns
Number of speaker turns
Figure 3: Histogram of the number of speaker
turns per section in 100 hours of audio data from
radio and TV sources (NPR, ABC, CNN, CSPAN)
from May-June 1996.
transcriptions, and a moderate degradation (4%
absolute) is observed using the 1.4xRT transcrip-
tions.
7 Locating Story Boundaries
The broadcast news transcription system also pro-
vides non-lexical information along with the word
transcription. This information is available in
the partition of the audio track, which identifies
speaker turns. It is interesting to see whether or
not such information can be used to help locate
story boundaries, since in the general case these
are not known. Statistics were made on 100 hours
of radio and television broadcast news with man-
ual transcriptions including the speaker identities.
Of the 2096 sections manually marked as reports
(considered stories), 40% start without a manu-
ally annotated speaker change. This means that
using only speaker change information for detect-
ing document boundaries would miss 40% of the
boundaries. With automatically detected speaker
changes, the number of missed boundaries would
certainly increase. At the same time, 11,160 of
the 12,439 speaker turns occur in the middle of a
document, resulting in a false alarm rate of almost
90%. A more detailed analysis shows that about
50% of the sections involve a single speaker, but
that the distribution of the number of speaker
turns per section falls off very gradually (see Fig-
ure 3). False alarms are not as harmful as missed
detections, since it may be possible to merge ad-
jacent turns into a single document in subsequent
processing. These results show that even perfect
00.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0 30 60 90 120 150 180 210 240 270 300
D
en
si
ty

Duration (seconds)
1997 Hub-4
0
0.005
0.01
0.015
0.02
0.025
0 30 60 90 120 150 180 210 240 270 300
D
en
si
ty

Duration (seconds)
TREC-9 SDR Corpus
Figure 4: Distribution of document durations for
100 hours of data from May-June 1996 (top) and
for 557 hours from February-June 1998 (bottom).
speaker turn boundaries cannot be used as the pri-
mary cue for locating document boundaries. They
can, however, be used to refine the placement
of a document boundary located near a speaker
change.
We also investigated using simple statistics on
the durations of the documents. A histogram of
the 2096 sections is shown in Figure 4. One
third of the sections are shorter than 30 seconds.
The histogram has a bimodal distribution with a
sharp peak around 20 seconds, and a smaller, flat
peak around 2 minutes. Very short documents
are typical of headlines which are uttered by sin-
gle speaker, whereas longer documents are more
likely to contain data from multiple talkers. This
distribution led us to consider using a multi-scale
segmentation of the audio stream into documents.
Similar statistics were measured on the larger cor-
pus (Figure 4 bottom).
As proposed in (Abberley et al, 1999; John-
son et al, 1999), we segment the audio stream
into overlapping documents of a fixed duration.
As a result of optimization, we chose a 30 sec-
ond window duration with a 15 second overlap.
Since there are many stories significantly shorter
than 30s in broadcast shows (see Figure 4) we
conjunctured that it may be of interest to use a
double windowing system in order to better tar-
get short stories (Gauvain et al, 2000). The win-
dow size of the smaller window was selected to
be 10 seconds. So for each query, we indepen-
dently retrieved two sets of documents, one set
for each window size. Then for each document
set, document recombination is done by merging
overlapping documents until no further merges
are possible. The score of a combined document
is set to maximum score of any one of the com-
ponents. For each document derived from the
30s windows, we produce a time stamp located
at the center point of the document. However,
if any smaller documents are embedded in this
document, we take the center of the best scor-
ing document. This way we try to take advantage
of both window sizes. The MAP using a single
30s window and the double windowing strategy
are shown in Table 2. For comparison, the IR re-
sults using the manual story segmentation and the
speaker turns located by the audio partitioner are
also given. All conditions use the same word hy-
potheses obtained with a speech recognizer which
had no knowledge about the story boundaries.
manual segmentation (NIST) 59.6%
audio partitioner 33.3%
single window (30s) 50.0%
double window 52.3%
Table 2: Mean average precision with manual and
automatically determined story boundaries. The
document collection contains 557 hours of broad-
cast news from the period of February through
June 1998. (21750 stories, 50 queries with the
associated relevance judgments.)
From these results we can clearly see the inter-
est of using a search engine specifically designed
to retrieve stories in the audio stream. Using an
a priori acoustic segmentation, the mean aver-
age precision is significantly reduced compared
to a ?perfect? manual segmentation, whereas the
window-based search engine results are much
closer. Note that in the manual segmentation all
non-story segments such as advertising have been
removed. This reduces the risk of having out-of-
topic hits and explains part of the difference be-
tween this condition and the other conditions.
The problem of locating story boundaries is be-
ing further pursued in the context of the ALERT
project, where one of the goals is to identify ?doc-
uments? given topic profiles. This project is in-
vestigating the combined use of audio and video
segmentation to more accurately locate document
boundaries in the continuous data stream.
8 Recent Research Projects
The work presented in this paper has benefited
from a variety of research projects both at the Eu-
ropean and National levels. These collaborative
efforts have enabled access to real-world data al-
lowing us to develop algorithms and models well-
suited for near-term applications.
The European project LE-4 OLIVE: A
Multilingual Indexing Tool for Broadcast
Material Based on Speech Recognition
(http://twentyone.tpd.tno.nl/ olive/) addressed
methods to automate the disclosure of the infor-
mation content of broadcast data thus allowing
content-based indexation. Speech recognition
was used to produce a time-linked transcript of
the audio channel of a broadcast, which was then
used to produce a concept index for retrieval.
Broadcast news transcription systems for French
and German were developed. The French data
come from a variety of television news shows and
radio stations. The German data consist of TV
news and documentaries from ARTE. OLIVE also
developed tools for users to query the database,
as well as cross-lingual access based on off-line
machine translation of the archived documents,
and online query translation.
The European project IST ALERT: Alert sys-
tem for selective dissemination (http://www.fb9-
ti.uni-duisburg.de/alert) aims to associate state-
of-the-art speech recognition with audio and
video segmentation and automatic topic index-
ing to develop an automatic media monitoring
demonstrator and evaluate it in the context of real
world applications. The targeted languages are
French, German and Portuguese. Major media-
monitoring companies in Europe are participating
in this project.
Two other related FP5 IST projects are: CORE-
TEX: Improving Core Speech Recognition Tech-
nology and ECHO: European CHronicles On-
line. CORETEX (http://coretex.itc.it/), aims at
improving core speech recognition technologies,
which are central to most applications involv-
ing voice technology. In particular the project
addresses the development of generic speech
recognition technology and methods to rapidly
port technology to new domains and languages
with limited supervision, and to produce en-
riched symbolic speech transcriptions. The ECHO
project (http://pc-erato2.iei.pi.cnr.it/echo) aims to
develop an infrastructure for access to histori-
cal films belonging to large national audiovisual
archives. The project will integrate state-of-the-
art language technologies for indexing, searching
and retrieval, cross-language retrieval capabilities
and automatic film summary creation.
9 Conclusions
This paper has described some of the ongoing re-
search activites at LIMSI in automatic transcrip-
tion and indexation of broadcast data. Much of
this research, which is at the forefront of todays
technology, is carried out with partners with real
needs for advanced audio processing technolo-
gies.
Automatic speech recognition is a key tech-
nology for audio and video indexing. Most of
the linguistic information is encoded in the au-
dio channel of video data, which once transcribed
can be accessed using text-based tools. This is in
contrast to the image data for which no common
description language is widely adpoted. A va-
riety of near-term applications are possible such
as audio data mining, selective dissemination of
information (News-on-Demand), media monitor-
ing, content-based audio and video retrieval.
It appears that with word error rates on the
order of 20%, comparable IR results to those
obtained on text data can be achieved. Even
with higher word error rates obtained by run-
ning a faster transcription system or by transcrib-
ing compressed audio data (Barras et al, 2000;
J.M. Van Thong et al, 2000) (such as that can be
loaded over the Internet), the IR performance re-
mains quite good.
Acknowledgments
This work has been partially financed by the Eu-
ropean Commission and the French Ministry of
Defense. The authors thank Jean-Jacques Gan-
golf, Sylvia Hermier and Patrick Paroubek for
their participation in the development of differ-
ent aspects of the automatic indexation system de-
scribed here.
References
Dave Abberley, Steve Renals, Dan Ellis and Tony
Robinson, ?The THISL SDR System at TREC-8?,
Proc. of the 8th Text Retrieval Conference TREC-8,
Nov 1999.
Martine Adda-Decker, Gilles Adda, Lori Lamel, ?In-
vestigating text normalization and pronunciation
variants for German broadcast transcription,? Proc.
ICSLP?2000, Beijing, China, October 2000.
Claude Barras, Lori Lamel, Jean-Luc Gauvain, ?Auto-
matic Transcription of Compressed Broadcast Au-
dio Proc. ICASSP?2001, Salt Lake City, May 2001.
Langzhou Chen, Lori Lamel, Gilles Adda and Jean-
Luc Gauvain, ?Broadcast News Transcription in
Mandarin,? Proc. ICSLP?2000, Beijing, China, Oc-
tober 2000.
John S. Garofolo, Cedric G.P. Auzanne, and Ellen
M. Voorhees, ?The TREC Spoken Document Re-
trieval Track: A Success Story,? Proc. of the 6th
RIAO Conference, Paris, April 2000. Also John
S. Garofolo et al, ?1999 Trec-8 Spoken Docu-
ment Retrieval Track Overview and Results,? Proc.
8th Text Retrieval Conference TREC-8, Nov 1999.
(http://trec.nist.gov).
Jean-Luc Gauvain, Lori Lamel, ?Fast Decoding for
Indexation of Broadcast Data,? Proc. ICSLP?2000,
3:794-798, Oct 2000.
Jean-Luc Gauvain, Lori Lamel, Gilles Adda, ?Parti-
tioning and Transcription of Broadcast News Data,?
ICSLP?98, 5, pp. 1335-1338, Dec. 1998.
Jean-Luc Gauvain, Lori Lamel, Claude Barras, Gilles
Adda, Yannick de Kercadio ?The LIMSI SDR sys-
tem for TREC-9,? Proc. of the 9th Text Retrieval
Conference TREC-9, Nov 2000.
Alexander G. Hauptmann and Michael J. Witbrock,
?Informedia: News-on-Demand Multimedia Infor-
mation Acquisition and Retrieval,? Proc Intelli-
gent Multimedia Information Retrieval, M. May-
bury, ed., AAAI Press, pp. 213-239, 1997.
Djoerd Hiemstra, Wessel Kraaij, ?Twenty-One at
TREC-7: Ad-hoc and Cross-language track,? Proc.
of the 8th Text Retrieval Conference TREC-7, Nov
1998.
Sue E. Johnson, Pierre Jourlin, Karen Spa?rck Jones,
Phil C. Woodland, ?Spoken Document Retrieval for
TREC-8 at Cambridge University?, Proc. of the 8th
Text Retrieval Conference TREC-8, Nov 1999.
Mark Maybury, ed., Special Section on ?News on De-
mand?, Communications of the ACM, 43(2), Feb
2000.
David Miller, Tim Leek, Richard Schwartz, ?Using
Hidden Markov Models for Information Retrieval?,
Proc. of the 8th Text Retrieval Conference TREC-7,
Nov 1998.
Kenney Ng, ?A Maximum Likelihood Ratio Informa-
tion Retrieval Model,? Proc. of the 8th Text Re-
trieval Conference TREC-8, 413-435, Nov 1999.
M. F. Porter, ?An algorithm for suffix stripping?, Pro-
gram, 14, pp. 130?137, 1980.
Karen Spa?rk Jones, S. Walker, Stephen E. Robert-
son, ?A probabilistic model of information retrieval:
development and status,? Technical Report of the
Computer Laboratory, University of Cambridge,
U.K., 1998.
J.M. Van Thong, David Goddeau, Anna Litvi-
nova, Beth Logan, Pedro Moreno, Michael Swain,
?SpeechBot: a Speech Recognition based Audio In-
dexing System for the Web?, Proc. of the 6th RIAO
Conference, Paris, April 2000.
S. Walker, R. de Vere, ?Improving subject retrieval in
online catalogues: 2. Relevance feedback and query
expansion?, British Library Research Paper 72,
British Library, London, U.K., 1990.
JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines, pages 1?12,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Mbochi : corpus oral, traitement automatique et exploration phonologique  Annie Rialland 1, Martial Embanga Aborobongui 1, Martine Adda-Decker1, 2, Lori Lamel2.  (1) LPP, UMR 7018, 19, rue des Bernardins 75005 Paris  (2) LIMSI, UPR 3251, b?t. 508, rue John von Neumann, 91403, Orsay  annie.rialland@univ-paris3.fr, aborobongui@yahoo.fr, {madda,lamel}@limsi.fr RESUME ____________________________________________________________________________________________________________  !Nous d?crivons la constitution d?un corpus oral en langue mbochi, une des langues bantoues parl?es au Congo-Brazzaville, catalogu?e comme C25 dans le riche inventaire de ces langues. Le mat?riel enregistr? comprend dans un premier temps de la lecture de contes de tradition orale, transcrits par un des co-auteurs, natif de la langue. Un deuxi?me volet incluant de la parole radiophonique est pr?vu. Le corpus a ?t? ensuite align? automatiquement en mots et en segments phon?miques, permettant des ?tudes acoustico-phon?tiques et phonologiques ? grande ?chelle. Il permettra ? terme d?envisager la mise au point d?un syst?me de transcription automatique pour cette langue sous-dot?e. Dans l?imm?diat, les ressources nous permettent de pr?senter une description de la langue et d??tudier des processus phonologiques entra?nant des ?lisions de voyelles ? la fronti?re de mots. Le corpus cr??, permettant de documenter le mbochi et d?am?liorer sa visibilit? sur la toile, pourra ?tre mis ? disposition d?autres chercheurs. Abstract  Mbochi: oral corpus, automatic processing & phonological mining  This contribution describes ongoing research on Mbochi, a Bantu C language spoken by more than 100000 native speakers in Congo-Brazzaville. A first oral corpus has been collected as read speech corresponding to 3 folktales. It has been transcribed by one of the co-authors and it will be extended to radio broadcasts. The corpus is aligned automatically into words and phonemic segments, allowing acoustic-phonetic and phonological studies on a large scale. It is providing the first step towards an automatic transcription system for this under-resourced language. Currently, these resources allow us to improve the description of the language and to improve our knowledge of the nature and conditions of phonological processes such as vowel elision with or without compensatory lengthening at word junctions. The corpus which will contribute to the documentation of Mbochi and its visibility on the web, will be made available to other researchers. MOTS-CLES : mbochi, alignement automatique, ?lision vocalique, dissimilation consonantique.  KEYWORDS : Mbochi, automatic alignment, vowel elision, consonantal dissimilation 
1
1 Introduction Le mbochi (ou "mb!"s?, son nom dans la langue m?me) est une langue sans standard d??criture, sous-dot?e en termes de ressources, qu?elles soient ?lectroniques ou non. Le but de notre travail est de commencer ? combler cette lacune en constituant un corpus align? automatiquement. L?alignement automatique a d? ?tre adapt? ? des caract?ristiques du mbochi, en particulier, ? ses processus d??lision vocalique qui g?n?rent de nombreuses variantes de mots. L?article pr?sentera cette adaptation avec des r?sultats quantifi?s. Il montrera ?galement comment ces corpus annot?s peuvent devenir des outils permettant de documenter ? grande ?chelle les contextes d?occurrence de processus phonologiques. La langue et ses principales r?gles phonologiques seront d?abord pr?sent?es avant d?aborder la partie exp?rimentale.  2 Pr?sentation de la langue mbochi Le mbochi est une langue bantoue du groupe C, appartenant au sous-groupe mbochi et r?pertori?e comme C 25 dans la classification de  Guthrie (1967-1970). Elle comporte plusieurs dialectes, en particulier le dialecte de Boundji qui retiendra plus particuli?rement notre attention dans la pr?sente ?tude. 2.1 Situation et statut de la langue Le mbochi est principalement parl?e dans le Nord du Congo-Brazzaville, en terre mbochi situ?e dans la r?gion de la cuvette Ouest, mais aussi dans les grandes villes et dans la diaspora. Le nombre de locuteurs de la langue en terre mbochi ?tait estim? ? 108 000 en 2000 d?apr?s le site Ethnologue (http://www.ethnologue.com), chiffre reconnu comme tr?s approximatif.  Le mbochi est une langue qui n?a pas de forme d??criture officielle et qui a ?t? tr?s peu ?crite. Les documents ?crits en mbochi, avec des notations qui sont propres ? chaque auteur ou groupe d?auteurs, sont tr?s peu nombreux : on trouve un recueil de contes (Ob?nga, 1984), des brochures (par la SIL CONGO, en particulier), des textes peu diffus?s pour l??ducation religieuse. La Bible n?est pas disponible en mbochi. La langue ?crite utilis?e dans la r?gion est le fran?ais, qui est la langue de l??ducation et la langue officielle du Congo-Brazzaville. Actuellement, il n?y a pas de projet pour donner une forme ?crite officielle au mbochi ni pour l?utiliser dans l??ducation. Boundji, sous-pr?fecture de la r?gion de la Cuvette, a depuis 2009 une cha?ne de radio-t?l?vision ALIMA FM. Cette cha?ne a un rayon d??mission de 50 km et couvre 32 villages de la sous-pr?fecture de Boundji. Elle diffuse des informations ? la fois locales et internationales ainsi que diverses ?missions en fran?ais, en lingala et dans les deux langues de la r?gion : le mbochi et le teke lima. Cette nouvelle cha?ne commence ? jouer un r?le important dans la revitalisation des langues et des cultures de la r?gion, dans la red?couverte des contes, des chants traditionnels qui n??taient plus chant?s que par les personnes ?g?es dans les r?unions familiales ainsi que des traditions musicales.   
2
Par ailleurs, on note que les t?l?phones portables sont tr?s r?pandus et que les SMS sont ?crits en fran?ais.  Ainsi, ? ce jour, le mbochi est fondamentalement une langue non ?crite, dont les formes orales commencent ? b?n?ficier du soutien d?une diffusion par de nouvelles techniques de communication.  2.2 Principales ?tudes sur le mbochi Un certain nombre de travaux ont port? sur le mbochi, en particulier : Ollassa 1969, Ob?nga 1976, 1984, Fontaney 1988, 1989, Ndinga Oba 2003, 2004, Leitch 1997, Amboulou 1998, Bedrosian 1998,  Chatfield 1999, Beltzung & al 2010, Embanga Aborobongui & al 2011, Embanga Aborobongui & al (sous presse). Le plus complet est actuellement la th?se  d?Amboulou (1998) qui pr?sente une description d?ensemble d?un des dialectes, le dialecte olee. Des questions plus sp?cifiques ont ?t? approfondies par divers auteurs : ainsi, les processus d?harmonie vocalique se sont trouv?s mieux compris gr?ce ? l??tude de Leich (1997), qui a montr? que le trait actif ?tait le trait RTR. Les processus tonals, quant ? eux, ont ?t? ?clair?s par Embanga Aborobongui et al (sous presse) qui ont d?gag? le r?le central d?une contrainte d??vitement de contour dans les modifications tonales observ?es. 2.3 Principales caract?ristiques phonologiques du mbochi Les syllabes possibles en mbochi sont de la forme : CV, CVV, V ou VV. Comme la plupart des langues bantoues, le mbochi n?a pas de syllabes ferm?es, c?est ? dire se terminant par une consonne. Le mbochi a un syst?me vocalique comportant 7 voyelles,  qui peuvent ?tre analys?es selon les traits : [haut], [rond], [bas] et [RTR]. ?  ? i ? e ? ! ? a ? " ? o ? u ? Haut ? * ?  ?  ?  ?  ?  ? * ? Rond ?  ?  ?  ?  ? * ? * ? * ? Bas ?  ?  ?  ? * ?  ?  ?  ? RTR ?  ?  ? * ? * ? * ?  ?  TABLE 1.  Les voyelles du mbochi Les traits [RTR] (Retracted Tongue Root) et [Rond] sont actifs dans les harmonies vocaliques (voir Leich, 1997). Chaque voyelle a un pendant long, qui correspond ? deux mores vocaliques et est not? en doublant la voyelle (aa pour a long, par exemple).   
3
  Le mbochi compte 24 consonnes, pr?sent?es dans le tableau ci-dessous :  Bilabiales Labiodentales Alv?olaires Alv?opalatales et palatales V?laires Occlusives p           b  pf           bv   t               d ts              dz k Pr?-nasalis?e           mb             mbv               nd                ndz ng Nasales           m                  n                   # $ Fricatives  f  s  Approximantes %  r/l   
Semi-voyelles w   y  TABLE 2.  Les consonnes du mbochi Les points marquants du syst?me sont : la s?rie de pr?nasalis?es, la pr?sence d?un ! dans le syst?me ? c?t? d?un b, le grand nombre de consonnes labiales et l?absence de g.  Par ailleurs, le mbochi poss?de deux tons : un ton haut et un ton bas. Chaque ton est port? par une more et toute more porte un ton. Une contrainte absolue interdit tout contour sur une more et d?clenche des processus permettant d??viter toute configuration non conforme, en particulier, ? la suite de l??lision de voyelle (voir Embanga Aborobongui et al, sous presse).  Le mbochi pr?sente des r?gles de dissimilation consonantique et d??lision vocalique tr?s productives. Les r?gles de dissimilation consonantique concernent les pr?fixes de forme CV pr?c?dant les noms (pr?fixes de classe) et les verbes (marqueurs personnels). La dissimilation est totale dans le sens o? la consonne tombe lorsque le pr?fixe pr?c?de une racine commen?ant par une consonne. Cette r?gle g?n?re un ensemble d?allomorphes : ainsi le pr?fixe de la classe 2 ba- appara?t-il sous la forme b(a) devant une racine qui commence par une voyelle et sous la forme a devant une racine commen?ant par une consonne. Les exemples suivants illustrent cette alternance : 1. ba+?s?   & b-?s?   ? ?pouses ? 2.   ba+?na  & b-?na  ? enfants ?             3.   ba+kondzi   & a-kondzi ? chefs ?             4.   ba+ k ?su   ? a-k?su  ? tortues ? (voir Beltzung et al 2010)  
4
Le processus de dissimilation ne s?accompagne pas d?une disparition totale de la consonne : elle laisse une trace, une position consonantique qui entra?ne la formation d?une voyelle longue dans certains contextes (voir ex. 7 et 8). Cette consonne, qu?on peut dire ? flottante ? sera not?e entre parenth?ses. Des processus d??lision vocalique se produisent r?guli?rement ? la rencontre de deux mots phonologiques (c?est ? dire de l?ensemble : mot +clitiques) lorsque le premier mot phonologique (MP) se termine par une voyelle et le deuxi?me commence par une voyelle. Dans le cas g?n?ral (en l?absence de consonne flottante et en dehors de la suite a+i), la derni?re voyelle (ou la derni?re more) du premier MP tombe. Exemples : 5. oy?lal?mbi (m) o -y?lu        ?-l?mb-i   Cl1.femme  Cl1.Pas-cuisiner-R?cent  ? La femme a cuisin?. ?  6.  okondz?s"ri (m)o-kondzi  ?-s"r-i Cl1.chef  Cl1.Pas-dire-R?cent ? Le chef a dit. ? Lorsque a et un i viennent en contact, il y a optionnellement une coalescence, r?sultant en la formation d?une voyelle interm?diaire e ou ! (en fonction de l?harmonie vocalique avec la voyelle suivante). Les consonnes flottantes interviennent dans ces processus d??lision, engendrant la formation de voyelles longues, comme l?illustrent les exemples suivants : 7.   ay?laal?mbi  (b)a-y?lu       (b)?-l?mb-i   Cl2.femme  Cl2.Pas-cuisiner-R?cent  ? Les femmes ont cuisin?. ?   8.  akondza?s"ri (b)a-kondzi  ( b ) ? -s"r-i Cl2.chef  Cl2.Pas-dire-R?cent ? Les chefs ont dit. ?  Les voyelles longues r?sultent d?un allongement compensatoire, dans la mesure o? la voyelle suivante s?est allong?e pour compenser la perte de la premi?re voyelle. La pr?sence de la consonne flottante a pour effet de maintenir la more de la premi?re voyelle et de la prot?ger de l??lision. Ajoutons que des m?canismes de restructuration tonale sont associ?s ? ces processus d??lision. Ces divers m?canismes tonals et segmentaux sont en cours d??tude par M. Embanga Aborobongui (en pr?paration).  
5
 3 Corpus et m?thodes L??tude se situe dans la ligne d??tudes pr?c?dentes sur des langues africaines sous dot?es, fond?es sur des corpus oraux et utilisant une proc?dure d?alignement automatique originellement d?velopp?e pour des langues ? bien dot?es ? (A. Sharma Grover & al. 2010, Gelas & al. 2010). Corpus Le corpus utilis? dans cette ?tude repose sur la lecture de contes traditionnels, une des rares oeuvres transcrites de langue mbochi (Obenga 1984).  Ces contes ont ?t? lus par un locuteur natif. La pr?sente ?tude se limite a trois de ces contes: nd?ng? y? di? ts!"si "mw!n!"  ?Le li?vre et l??l?phant?, !b!"! b? la "n""  ? La main et la bouche ? et  Lek? ?ya? la ay?lu  ?La mort et la femme? d?une dur?e totale de 10 minutes.   Pour ces trois contes, une transcription manuelle avec notation des consonnes flottantes entre parenth?ses a ?t? effectu?e.   Le tableau 3 donne une description du corpus en termes de phon?mes et de mots (types et tokens) inclus dans le corpus, de nombre de jonctions de mots avec deux voyelles venant en contact (V1#V2) ou deux voyelles et consonne flottante (V1#CflottV2):   tokens types  total phon?mes total voyelles total consonnes total labiales (hors w) total /%/ total /b/ 
4035 2438 1597   514   197   128 
30   7 23   6   1   1 
   /%,b,m,mb,bv,mbv/    total mots 1348 460  tot. contextes V1#V2 tot. contextesV1#CflottV2   386   198   -   -   TABLE 3 ? Description du corpus CONTESOBENGA en termes de phon?mes (avec focus sur les labiales) et mots lexicaux (types et leurs occurrences dans le corpus=tokens), nombre de contextes V1#V2 et V1#CflottV2.  Syst?me d?alignement automatique ? partir du fran?ais                            Afin de pouvoir rechercher et ?couter des mots ou des r?alisations de s?quences de phon?mes mbochi sp?cifiques dans le signal, nous avons align? notre corpus par alignement automatique en adaptant le syst?me de reconnaissance du LIMSI (Gauvain et al2005). Ce syst?me, d?velopp? pour le fran?ais, n?a que tr?s peu ?t? modifi? pour traiter la langue mbochi. Des mod?les acoustiques du fran?ais (ind?pendants du contexte) ont ?t? utilis?s pour emprunter ou initialiser des mod?les acoustiques mbochi. Nous rappelons ici rapidement les ?tapes essentielles pour traiter le mbochi:  1. d?finir un transcodage entre inventaires phon?miques fran?ais et mbochi afin 
6
d??tablir une correspondance entre mbochi et fran?ais en s?appuyant sur les correspondances IPA. La bilabiale /%/ a ?t? mod?lis?e par le /w/ fran?ais et la nasale /ng/ comme s?quence /n/ et /g/. 2. emprunter des mod?les acoustiques ? partir de mod?les existants d?une autre langue (fran?ais). Les mod?les acoustiques du fran?ais servent ainsi comme approximation des  sons en mbochi. Les consonnes complexes telles que /mbv/ ont ?t? d?compos?es pour ?tre mod?lis?es comme concat?nation (/m/, /b/ et /v/ fran?ais pour /mbv/). Nous sommes conscients que cette mani?re de proc?der augmente la topologie des mod?les des sons complexes mbochi et ne correspond certainement pas au mieux au d?cours temporels de ces sons. 3. cr?er un vocabulaire pour la langue mbochi (une liste de mots). Notre vocabulaire se limite aux mots pr?sents dans les transcriptions du corpus enregistr?. 4. cr?er un dictionnaire de prononciation. La correspondance graph?me-phon?me est transparente. Nous avons ?crit un script PERL qui transforme les graph?mes (lettres accentu?es indiquant les tons) en phon?mes (correspondant essentiellement ? la m?me lettre sans accent). Les tons n?ont pas ?t? cod?s dans les prononciations, dans la mesure o? nous n?avons pas de mod?les ? tons en fran?ais. 5. la tire lexicale des fichiers .TextGrid a ?t? transform?e par script PERL en format NIST .stm qui permet d??tre comprise par les syst?mes de reconnaissance automatique.  Afin d??tudier les contacts de mots, en particulier les contacts V1#V2 et V1#CflottV2 nous avons explor? notre corpus par alignement automatique en utilisant le syst?me de reconnaissance du LIMSI par alignement automatique.   Alignement automatique Concernant l??tape 4 du dictionnaire de prononciation, nous avons ?labor? deux versions de dictionnaires (voir Table 4). La premi?re version donne pour chaque entr?e lexicale sa prononciation compl?te (ou canonique) telle que d?riv?e de l??criture. Afin de pouvoir rendre compte des processus d??lision vocalique, la deuxi?me version propose ?galement des prononciations plus courtes avec des ?lisions conditionnelles de voyelles en d?but et fin de mot, la condition ?tant que le mot pr?c?dent se termine par une voyelle ou que le mot suivant commence par une voyelle.                       Mots Prononciation canonique Variantes i b??  ts! "si oy ? nga ng? 
ibaa ts"si ojenga nga 
iba(V), (V)baa ts"s(V) ojeng(V), (V)jenga ng(V)  TABLE 4 ? Exemples de mots et prononciations du dictionnaire de prononciation. La deuxi?me colonne indique les prononciations compl?tes, la troisi?me colonne montre des variantes rajout?es pour tester le ph?nom?ne d??lision vocalique. La notation (V) en d?but et fin de prononciation indique une prononciation conditionnelle, d?pendant des 
7
contextes. Nous avons effectu? deux s?ries d?alignement pour ?tudier en particulier les ph?nom?nes d??lision de voyelles en fronti?re de mots. La premi?re s?rie utilise le dictionnaire canonique avec les formes compl?tes tandis que la deuxi?me s?rie s?appuie sur un dictionnaire enrichi des variantes afin de rendre compte des chutes vocaliques. Un exemple des deux alignements en parall?le est illustr? par la figure 1.                       FIGURE 1 ? Spectrogramme montrant un extrait du conte La main et la bouche avec 2 lignes d?alignements/segmentations en phones : 1) en fonction du dictionnaire de prononciations compl?tes ; 2) en fonction du dictionnaire enrichi des variantes ? chute vocalique. Les barres verticales indiquent les fronti?res de mot. L?ellipse rouge montre un  contact V1#CflottV2 avec une chute de voyelle. La figure 1 montre un spectrogramme d?un extrait de parole en mbochi avec en-dessous deux alignements en phones l?g?rement diff?rents. La nasale /ng/ r?sulte ainsi en deux segments cons?cutifs [n] et [g] (que nous avons refusionn?s pour compter le nombre de phon?mes pr?sents dans le corpus). L?ellipse rouge illustre un contact V1#CflottV2, o? se produit une chute vocalique de V1. Le premier alignement (? partir d?un dictionnaire de prononciation sans variantes) n?a pas pr?vu une telle ?lision : en cons?quence un segment [a] de dur?e minimale (30ms) est pr?sent ici, alors qu?il dispara?t dans la ligne du dessous.  4 Exploration phonologique  Les processus d??lision pr?sent?s en 1. ont ?t? ?tablis ? partir de proc?dures communes en phonologie, consistant ? cr?er des exemples afin de valider ou non une hypoth?se. 
8
Ainsi, les rencontres des diverses voyelles en finale et en d?but de mot ont ?t? test?es syst?matiquement dans des exemples pr?sentant les rencontres de voyelles possibles. Le m?me type de travail sur la combinatoire a permis de d?gager le r?le des consonnes flottantes dans la formation des allongements compensatoires. Les effets des divers contextes syntaxiques (sujet+ verbe, verbe+compl?ment, etc)  ont aussi ?t? explor?s et ceux-ci ne paraissent pas avoir d?influence sur ces processus. Le traitement automatique mis en ?uvre pour ce corpus nous permet d?explorer ces processus ? plus grande ?chelle et de v?rifier leur r?gularit? et g?n?ralit?.  La table suivante pr?sente le nombre de mots du corpus avec voyelle initiale et finale ainsi que leurs pourcentages respectifs :  total avec V initiale avec V finale #nombre de mots 1348 460 1348 pourcentage de mots 100% 34.1% 100% Table 5 : Nombre et pourcentages de mots avec voyelle initiale et voyelle finale L?analyse des contacts V1+V2  et V1 C flottante V2 ? la jonction de mots aboutit aux r?sultats suivants :   V1+V2  V1 Cflottante V2 nombre de mots  386 198 pourcentage ?lision V1 85% 83.3% pourcentage ?lision V2 3.9% 6.1% moyenne de la dur?e vocalique   0,16s 0,18s Table 6.  Contacts V1+V2 et V1 Cflottante V2 ? la jonction de mots : nombre de mots, pourcentage d??lision de V1, V2, et moyenne de la dur?e de la voyelle r?sultante. Les chiffres indiquent un processus d??lision dans le corpus tr?s important pour V1 (85% pour le contact V1+V2, et 83,3% pour le contact V1 Cflottante V2) et au contraire faible pour V2 (respectivement 3.9% et 6.1%).  La pr?sence ou non d??lision vocalique a ?t? v?rifi?e ? l??coute et not?e manuellement pour les 80 contacts de mots de la forme V1V2 et V1 CflottV2 pr?sents dans un des contes du corpus (Le li?vre et l??l?phant).  On note 7 non-?lisions correspondant ? des pauses, relev?es de fa?on concordante dans la notation manuelle et la notation automatique. 57 ?lisions reconnues ? l??coute ont ?t? ?galement prises en compte par l?alignement automatique. Dans l??tat actuel de son adaptation au mbochi, les cas 
9
probl?matiques pour l?alignement automatique sont : 1) la coalescence, avec son changement vocalique non pr?vu dans la proc?dure d?alignement (1cas), 2) l??lision de voyelles longues, qui dans l?alignement automatique n?est que partielle du fait de la transcription des voyelles longues par deux voyelles (7 cas), 3) la non-?lision des voyelles des racines monosyllabiques, qui seraient ? prot?ger de l?application des m?canismes d??lision (2 cas).  En d?pit de la limitation de la proc?dure d?alignement aux cas les plus typiques (mais aussi de loin les plus nombreux),  les donn?es ?manant de l?alignement automatique, confirment que les processus d??lision vocaliques sont tr?s g?n?raux dans la langue, qu?ils ne sont pas limit?s ? des constituants prosodiques ou syntaxiques, comme l?est par exemple, la liaison en fran?ais. Ce point est important dans la mesure o?, typologiquement, il est peu fr?quent que ce type de m?canismes se produise dans l?ensemble de la phrase. Les pourcentages d??lision de V1 comme V2 sont comparables pour V1+V2 et V1 Cflottante V2, ce qui ?tait attendu, la pr?sence de la consonne flottante ne semblant pas intervenir dans les ?lisions de timbre vocalique mais au niveau de la dur?e de la voyelle r?sultante. La derni?re ligne du tableau pr?sente la dur?e moyenne des voyelles r?sultant des processus d??lision  sans et avec consonne flottante. On s?attendait ? ce que la voyelle soit plus longue lorsqu?une consonne flottante est pr?sente. Les r?sultats vont dans ce sens mais assez faiblement, l?accroissement n??tant que de 20ms lorsqu?une consonne flottante est impliqu?e. Des investigations suppl?mentaires seraient ici n?cessaires. 5 Conclusion Notre ?tude repr?sente une premi?re tentative d?alignement automatique sur une langue bantoue du Congo-Brazzaville, impliquant quelques difficult?s ? surmonter dans l?adaptation ? ses caract?ristiques propres. Le corpus align? a permis de quantifier la proportion de voyelles et de consonnes, en particulier des consonnes labiales. L?ensemble des mots du corpus se termine en syllabe ouverte et un tiers des occurrences de mots ont une voyelle en d?but de mot. Nous avons impl?ment? les m?canismes d??lision vocalique dans le syst?me d?alignement. Une premi?re ?tude quantifi?e, sur la base des 3 contes enregistr?s, sur ces processus d??lision vocalique et leurs contextes d?occurrence, confirme leurs fr?quences (autour de 85%) et leur non-limitation ? des constituants en dessous de la phrase. Par ailleurs, concernant l?hypoth?se d?allongement compensatoire en cas de consonne flottante, les donn?es tendent ? montrer une augmentation de la dur?e vocalique V2 autour de 20ms, sans pour autant clairement d?montrer l?existence de cet alongement.  Les travaux en cours visent ? la fois ? augmenter le corpus audio en variant les styles et les locuteurs, ? approfondir les descriptions acoustico-phon?tiques et les m?canismes phonologiques ? plus grande ?chelle et ? augmenter nos connaissances sur la langue mbochi et sa visibilit? en particulier sur la toile.  
10
Remerciements Le travail pr?sent? a ?t? en partie soutenu par le projet ANR-DFG BANTUPSYN Phonology/Syntax Interface in Bantu languages (ANR-08-FASHS-005-01) et par le LabEx EFL (Empirical Foundations of Linguistics).  R?f?rences AMBOULOU, C. (1998). Le Mbochi : langue bantoue du Congo Brazzaville (zone C, groupe C20). Th?se de Doctorat, INALCO : Paris. BELTZUNG, J-M, RIALLAND, A, EMBANGA ABOROBONGUI, M. (2010). Les relatives possessives en "mb!"s? (C25). ZAS Papers in Linguistics 53, pages 7-37. BEDROSIAN, P. L. (1998). The Mbochi noun class system. Journal of West African Languages 26, pages 27-47.  CHATFIELD, R. (1999). Temps modes et aspects en mbochi. ms. S.I.L., Congo  EMBANGA ABOROBONGUI, M, RIALLAND, A, BELTZUNG, J-M. (sous presse).Tone and intonation in a Bantu language: Embosi, In Proceedings of the 4th World Conference on African Languages, Cologne, August 2009. EMBANGA ABOROBONGUI, M, BELTZUNG, J-M, FATIMA, H, RIALLAND, A. (2011).Questions partielles en "mb!"s ? . ZASPIL 55, pages 7-21. FONTANEY, L. (1988), Mboshi : Steps toward a Grammar: Part I. Pholia 3, pages 87-169. FONTANEY, L. (1989), Mboshi : Steps toward a Grammar: Part II. Pholia 4, pages 71-131. GAUVAIN, J.L et al (2005), Where are we in transcribing French broadcast news? In Proceedings of Interspeech, Lisbonne, pages 1665-1668. GELAS, H., BESACIER, L., ROSSATO, S. & PELLEGRINO, F., (2010), Using automatic speech recognition for phonological purposes: study of vowel length in Punu (Bantu B40), LabPhon 12, New-Mexico, 8-10 juillet.  GUTHRIE, M. (1967-1971). Comparative Bantu. 4. volumes. Farborough : Gregg LEITCH, M. (1997), Vowel harmonies of the Congo Basin: An Optimality Theory analysis of variation in the Bantu zone C. University British Columbia, Doctoral thesis. NDINGA OBA, A. (2003). Les langues bantoues du Congo Brazzaville : ?tude typologique des langues du groupe C20 (mbosi ou mbochi). Tome 1 : Introduction, Pr?sentation, Phonologie. Paris : L?Harmattan. NDINGA OBA, A. (2004). Les langues bantoues du Congo Brazzaville : ?tude typologique des langues du groupe C20 (mbosi ou mbochi). Tome 2 : Classes nominales, Conclusion g?n?rale. Paris : L?Harmattan. OBENGA, T. (1976), la cuvette congolaise : les hommes et les structures. Paris, Pr?sence Africaine. OBENGA, T. (1984), Litt?rature traditionnelle des mbochi : Etsee leyamba. Paris, Pr?sence  
11
Africaine. OLLASA, P. (1969), Phonologie du mbosi (dialecte du Congo Brazzaville). M?moire de Ma?trise, Facult? des Lettres et Sciences Humaines de Bordeaux. SHARMA GROVER, A., CALTEAUX, K., VAN HUYSSTEEN, K. & PRETORIUS M. (2010), An overview of HLTs for South African Bantu languages? In Proceedings of the 2010 Annual Research Conference of the South African Institute for Computer Scientists and Information Technologists (SAICSIT), Bela-Bela (South Africa), pages 370-375.    
12
