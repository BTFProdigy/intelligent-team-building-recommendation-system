Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 370?376,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Unsupervised Feature Learning for Visual Sign Language Identification
Binyam Gebrekidan Gebre
1
, Onno Crasborn
2
, Peter Wittenburg
1
,
Sebastian Drude
1
, Tom Heskes
2
1
Max Planck Institute for Psycholinguistics,
2
Radboud University Nijmegen
bingeb@mpi.nl,o.crasborn@let.ru.nl,peter.wittenburg@mpi.nl,
sebastian.drude@mpi.nl,t.heskes@science.ru.nl
Abstract
Prior research on language identification fo-
cused primarily on text and speech. In this
paper, we focus on the visual modality and
present a method for identifying sign lan-
guages solely from short video samples. The
method is trained on unlabelled video data (un-
supervised feature learning) and using these
features, it is trained to discriminate between
six sign languages (supervised learning). We
ran experiments on short video samples in-
volving 30 signers (about 6 hours in total). Us-
ing leave-one-signer-out cross-validation, our
evaluation shows an average best accuracy of
84%. Given that sign languages are under-
resourced, unsupervised feature learning tech-
niques are the right tools and our results indi-
cate that this is realistic for sign language iden-
tification.
1 Introduction
The task of automatic language identification is
to quickly identify the identity of the language
given utterances. Performing this task is key in
applications involving multiple languages such as
machine translation and information retrieval (e.g.
metadata creation for large audiovisual archives).
Prior research on language identification is
heavily biased towards written and spoken lan-
guages (Dunning, 1994; Zissman, 1996; Li et al,
2007; Singer et al, 2012). While language iden-
tification in signed languages is yet to be studied,
significant progress has been recorded for written
and spoken languages.
Written languages can be identified to about
99% accuracy using Markov models (Dunning,
1994). This accuracy is so high that current
research has shifted to related more challeng-
ing problems: language variety identification
(Zampieri and Gebre, 2012), native language iden-
tification (Tetreault et al, 2013) and identification
at the extremes of scales; many more languages,
smaller training data, shorter document lengths
(Baldwin and Lui, 2010).
Spoken languages can be identified to accura-
cies that range from 79-98% using different mod-
els (Zissman, 1996; Singer et al, 2003). The
methods used in spoken language identification
have also been extended to a related class of prob-
lems: native accent identification (Chen et al,
2001; Choueiter et al, 2008; Wu et al, 2010) and
foreign accent identification (Teixeira et al, 1996).
While some work exists on sign language
recognition
1
(Starner and Pentland, 1997; Starner
et al, 1998; Gavrila, 1999; Cooper et al, 2012),
very little research exists on sign language iden-
tification except for the work by (Gebre et al,
2013), where it is shown that sign language identi-
fication can be done using linguistically motivated
features. Accuracies of 78% and 95% are reported
on signer independent and signer dependent iden-
tification of two sign languages.
This paper has two goals. First, to present a
method to identify sign languages using features
learned by unsupervised techniques (Hinton and
Salakhutdinov, 2006; Coates et al, 2011). Sec-
ond, to evaluate the method on six sign languages
under different conditions.
Our contributions: a) show that unsupervised
feature learning techniques, currently popular in
many pattern recognition problems, also work for
visual sign languages. More specifically, we show
how K-means and sparse autoencoder can be used
to learn features for sign language identification.
b) demonstrate the impact on performance of vary-
ing the number of features (aka, feature maps or
filter sizes), the patch dimensions (from 2D to 3D)
and the number of frames (video length).
1
There is a difference between sign language recognition
and identification. Sign language recognition is the recogni-
tion of the meaning of the signs in a given known sign lan-
guage, whereas sign language identification is the recognition
of the sign language itself from given signs.
370
2 The challenges in sign language
identification
The challenges in sign language identification
arise from three sources as described below.
2.1 Iconicity in sign languages
The relationship between forms and meanings are
not totally arbitrary (Perniss et al, 2010). Both
signed and spoken languages manifest iconicity,
that is forms of words or signs are somehow mo-
tivated by the meaning of the word or sign. While
sign languages show a lot of iconicity in the lex-
icon (Taub, 2001), this has not led to a universal
sign language. The same concept can be iconi-
cally realised by the manual articulators in a way
that conforms to the phonological regularities of
the languages, but still lead to different sign forms.
Iconicity is also used in the morphosyntax and
discourse structure of all sign languages, however,
and there we see many similarities between sign
languages. Both real-world and imaginary objects
and locations are visualised in the space in front
of the signer, and can have an impact on the artic-
ulation of signs in various ways. Also, the use of
constructed action appears to be used in many sign
languages in similar ways. The same holds for the
rich use of non-manual articulators in sentences
and the limited role of facial expressions in the
lexicon: these too make sign languages across the
world very similar in appearance, even though the
meaning of specific articulations may differ (Cras-
born, 2006).
2.2 Differences between signers
Just as speakers have different voices unique to
each individual, signers have also different sign-
ing styles that are likely unique to each individual.
Signers? uniqueness results from how they articu-
late the shapes and movements that are specified
by the linguistic structure of the language. The
variability between signers either in terms of phys-
ical properties (hand sizes, colors, etc) or in terms
of articulation (movements) is such that it does not
affect the understanding of the sign language by
humans, but that it may be difficult for machines
to generalize over multiple individuals. At present
we do not know whether the differences between
signers using the same language are of a similar or
different nature than the differences between dif-
ferent languages. At the level of phonology, there
are few differences between sign languages, but
the differences in the phonetic realization of words
(their articulation) may be much larger.
2.3 Diverse environments
The visual ?activity? of signing comes in a context
of a specific environment. This environment can
include the visual background and camera noises.
The background objects of the video may also in-
clude dynamic objects ? increasing the ambiguity
of signing activity. The properties and configu-
rations of the camera induce variations of scale,
translation, rotation, view, occlusion, etc. These
variations coupled with lighting conditions may
introduce noise. These challenges are by no means
specific to sign interaction, and are found in many
other computer vision tasks.
3 Method
Our method performs two important tasks. First,
it learns a feature representation from patches of
unlabelled raw video data (Hinton and Salakhut-
dinov, 2006; Coates et al, 2011). Second, it looks
for activations of the learned representation (by
convolution) and uses these activations to learn a
classifier to discriminate between sign languages.
3.1 Unsupervised feature learning
Given samples of sign language videos (unknown
sign language with one signer per video), our sys-
tem performs the following steps to learn a feature
representation (note that these video samples are
separate from the video samples that are later used
for classifier learning or testing):
1. Extract patches. Extract small videos (here-
after called patches) randomly from any-
where in the video samples. We fix the
size of the patches such that they all have r
rows, c columns and f frames and we ex-
tract patches m times. This gives us X =
{x
(1)
, x
(1)
, . . . , x
(m)
}, where x
(i)
? R
N
and
N = r?c?f (the size of a patch). For our ex-
periments, we extract 100,000 patches of size
15 ? 15 ? 1 (2D) and 15 ? 15 ? 2 (3D).
2. Normalize the patches. There is evidence
that normalization and whitening (Hyv?arinen
and Oja, 2000) improve performance in un-
supervised feature learning (Coates et al,
2011). We therefore normalize every patch
x
(i)
by subtracting the mean and dividing by
371
Figure 1: Illustration of feature extraction: convolution and pooling.
the standard deviation of its elements. For vi-
sual data, normalization corresponds to local
brightness and contrast normalization.
3. Learn a feature-mapping. Our unsuper-
vised algorithm takes in the normalized and
whitened datasetX = {x
(1)
, x
(1)
, . . . , x
(m)
}
and maps each input vector x
(i)
to a new fea-
ture vector of K features (f : R
N
? R
K
).
We use two unsupervised learning algorithms
a) K-means b) sparse autoencoders.
(a) K-means clustering: we train K-means
to learns K c
(k)
centroids that mini-
mize the distance between data points
and their nearest centroids (Coates and
Ng, 2012). Given the learned centroids
c
(k)
, we measure the distance of each
data point (patch) to the centroids. Natu-
rally, the data points are at different dis-
tances to each centroid, we keep the dis-
tances that are below the average of the
distances and we set the other to zero:
f
k
(x) = max{0, ?(z)? z
k
} (1)
where z
k
= ||x? c
(k)
||
2
and ?(z) is the
mean of the elements of z.
(b) Sparse autoencoder: we train a sin-
gle layer autoencoder with K hid-
den nodes using backpropagation to
minimize squared reconstruction error.
At the hidden layer, the features are
mapped using a rectified linear (ReL)
function (Maas et al, 2013) as follows:
f(x) = g(Wx+ b) (2)
where g(z) = max(z, 0). Note that ReL
nodes have advantages over sigmoid or
tanh functions; they create sparse repre-
sentations and are suitable for naturally
sparse data (Glorot et al, 2011).
From K-means, we get K R
N
centroids and from
the sparse autoencoder, we get W ? R
KxN
and
b ? R
K
filters. We call both the centroids and
filters as the learned features.
3.2 Classifier learning
Given the learned features, the feature mapping
functions and a set of labeled training videos, we
extract features as follows:
1. Convolutional extraction: Extract features
from equally spaced sub-patches covering the
video sample.
2. Pooling: Pool features together over four
non-overlapping regions of the input video to
reduce the number of features. We perform
max pooling for K-means and mean pooling
for the sparse autoencoder over 2D regions
(per frame) and over 3D regions (per all se-
quence of frames).
3. Learning: Learn a linear classifier to predict
the labels given the feature vectors. We use
logistic regression classifier and support vec-
tor machines (Pedregosa et al, 2011).
The extraction of classifier features through
convolution and pooling is illustrated in figure 1.
372
4 Experiments
4.1 Datasets
Our experimental data consist of videos of 30
signers equally divided between six sign lan-
guages: British sign language (BSL), Danish
(DSL), French Belgian (FBSL), Flemish (FSL),
Greek (GSL), and Dutch (NGT). The data for the
unsupervised feature learning comes from half of
the BSL and GSL videos in the Dicta-Sign cor-
pus
2
. Part of the other half, involving 5 signers, is
used along with the other sign language videos for
learning and testing classifiers.
For the unsupervised feature learning, two types
of patches are created: 2D dimensions (15 ? 15)
and 3D (15 ? 15 ? 2). Each type consists of ran-
domly selected 100,000 patches and involves 16
different signers. For the supervised learning, 200
videos (consisting of 1 through 4 frames taken at a
step of 2) are randomly sampled per sign language
per signer (for a total of 6,000 samples).
4.2 Data preprocessing
The data preprocessing stage has two goals.
First, to remove any non-signing signals that re-
main constant within videos of a single sign lan-
guage but that are different across sign languages.
For example, if the background of the videos is
different across sign languages, then classifying
the sign languages could be done with perfection
by using signals from the background. To avoid
this problem, we removed the background by us-
ing background subtraction techniques and manu-
ally selected thresholds.
The second reason for data preprocessing is to
make the input size smaller and uniform. The
videos are colored and their resolutions vary from
320 ? 180 to 720 ? 576. We converted the videos
to grayscale and resized their heights to 144 and
cropped out the central 144 ? 144 patches.
4.3 Evaluation
We evaluate our system in terms of average accu-
racies. We train and test our system in leave-one-
signer-out cross-validation, where videos from
four signers are used for training and videos of the
remaining signer are used for testing. Classifica-
tion algorithms are used with their default settings
and the classification strategy is one-vs.-rest.
2
http://www.dictasign.eu/
5 Results and Discussion
Our best average accuracy (84.03%) is obtained
using 500 K-means features which are extracted
over four frames (taken at a step of 2). This ac-
curacy obtained for six languages is much higher
than the 78% accuracy obtained for two sign lan-
guages (Gebre et al, 2013). The latter uses lin-
guistically motivated features that are extracted
over video lengths of at least 10 seconds. Our sys-
tem uses learned features that are extracted over
much smaller video lengths (about half a second).
All classification accuracies are presented in ta-
ble 5 for 2D and table 5 for 3D. Classification con-
fusions are shown in table 5. Figure 2 shows fea-
tures learned by K-means and sparse autoencoder.
(a) K-means features (b) SAE features
Figure 2: All 100 features learned from 100,000
patches of size 15?15. K-means learned relatively
more curving edges than the sparse auto encoder.
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 1
100 69.23 70.60 67.42 73.85 74.53 71.8
300 76.08 77.37 74.80 72.27 70.67 68.90
500 83.03 79.88 77.92 67.50 69.38 66.20
# of frames = 2
100 71.15 72.07 67.42 72.78 74.62 72.08
300 77.33 78.27 76.60 71.85 71.07 68.27
500 83.58 79.50 79.90 67.73 70.15 66.45
# of frames = 3
100 71.42 73.10 67.82 65.70 67.52 63.68
300 78.40 78.57 76.50 72.53 71.68 68.18
500 83.48 80.05 80.57 67.85 70.85 66.77
# of frames = 4
100 71.88 73.05 68.70 64.93 67.48 63.80
300 79.32 78.65 76.42 72.27 72.18 68.35
500 84.03 80.38 80.50 68.25 71.57 67.27
K = # of features, SVM = SVM with linear kernel
LR-L? = Logistic Regression with L1 and L2 penalty
Table 1: 2D filters (15?15): Leave-one-signer-out
cross-validation average accuracies.
373
1 2 3 4 5 6 7 8 9 1012345678910
BSL
1 2 3 4 5 6 7 8 9 1012345678910
DSL
1 2 3 4 5 6 7 8 9 1012345678910
FBSL
1 2 3 4 5 6 7 8 9 1012345678910
FSL
1 2 3 4 5 6 7 8 9 1012345678910
GSL
1 2 3 4 5 6 7 8 9 1012345678910
NGT
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 3: Visualization of coefficients of Lasso (logistic regression with L1 penalty) for each sign lan-
guage with respect to each of the 100 filters of the sparse autoencoder. The 100 filters are shown in figure
2(b). Each grid cell represents a frame and each filter is activated in 4 non-overlapping pooling regions.
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 2
100 70.63 69.62 68.87 67.40 66.53 65.73
300 73.73 74.05 73.03 72.83 73.48 70.52
500 75.30 76.53 75.40 72.28 74.65 68.72
# of frames = 3
100 72.48 73.30 70.33 68.68 67.40 68.33
300 74.78 74.95 74.77 74.20 74.72 70.85
500 77.27 77.50 76.17 72.40 75.45 69.42
# of frames = 4
100 74.85 73.97 69.23 68.68 67.80 68.80
300 76.23 76.58 74.08 74.43 75.20 70.65
500 79.08 78.63 76.63 73.50 76.23 70.53
Table 2: 3D filters (15?15?2): Leave-one-signer-
out cross-validation average accuracies.
BSL DSL FBSL FSL GSL NGT
BSL 56.11 2.98 1.79 3.38 24.11 11.63
DSL 2.87 92.37 0.95 0.46 3.16 0.18
FBSL 1.48 1.96 79.04 4.69 6.62 6.21
FSL 6.96 2.96 2.06 60.81 18.15 9.07
GSL 5.50 2.55 1.67 2.57 86.05 1.65
NGT 9.08 1.33 3.98 18.76 4.41 62.44
Table 3: Confusion matrix ? confusions averaged
over all settings for K-means and sparse autoen-
coder with 2D and 3D filters (i.e. for all # of
frames, all filter sizes and all classifiers).
Tables 5 and 5 indicate that K-means performs
better with 2D filters and that sparse autoencoder
performs better with 3D filters. Note that features
from 2D filters are pooled over each frame and
concatenated whereas, features from 3D filters are
pooled over all frames.
Which filters are active for which language?
Figure 3 shows visualization of the strength of fil-
ter activation for each sign language. The figure
shows what Lasso looks for when it identifies any
of the six sign languages.
6 Conclusions and Future Work
Given that sign languages are under-resourced,
unsupervised feature learning techniques are the
right tools and our results show that this is realis-
tic for sign language identification.
Future work can extend this work in two direc-
tions: 1) by increasing the number of sign lan-
guages and signers to check the stability of the
learned feature activations and to relate these to
iconicity and signer differences 2) by comparing
our method with deep learning techniques. In our
experiments, we used a single hidden layer of fea-
tures, but it is worth researching into deeper layers
to improve performance and gain more insight into
the hierarchical composition of features.
Other questions for future work. How good are
human beings at identifying sign languages? Can
a machine be used to evaluate the quality of sign
language interpreters by comparing them to a na-
tive language model? The latter question is partic-
ularly important given what happened at the Nel-
son Mandela?s memorial service
3
.
3
http://www.youtube.com/watch?v=X-DxGoIVUWo
374
References
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Tao Chen, Chao Huang, E. Chang, and Jingchun Wang.
2001. Automatic accent identification using gaus-
sian mixture models. In Automatic Speech Recog-
nition and Understanding, 2001. ASRU ?01. IEEE
Workshop on, pages 343?346.
Ghinwa Choueiter, Geoffrey Zweig, and Patrick
Nguyen. 2008. An empirical study of automatic ac-
cent classification. In Acoustics, Speech and Signal
Processing, 2008. ICASSP 2008. IEEE International
Conference on, pages 4265?4268. IEEE.
Adam Coates and Andrew Y Ng. 2012. Learn-
ing feature representations with k-means. In Neu-
ral Networks: Tricks of the Trade, pages 561?580.
Springer.
Adam Coates, Andrew Y Ng, and Honglak Lee. 2011.
An analysis of single-layer networks in unsuper-
vised feature learning. In International Conference
on Artificial Intelligence and Statistics, pages 215?
223.
H. Cooper, E.J. Ong, N. Pugeault, and R. Bowden.
2012. Sign language recognition using sub-units.
Journal of Machine Learning Research, 13:2205?
2231.
Onno Crasborn, 2006. Nonmanual structures in sign
languages, volume 8, pages 668?672. Elsevier, Ox-
ford.
T. Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Dariu M Gavrila. 1999. The visual analysis of human
movement: A survey. Computer vision and image
understanding, 73(1):82?98.
Binyam Gebrekidan Gebre, Peter Wittenburg, and Tom
Heskes. 2013. Automatic sign language identifica-
tion. In Proceedings of ICIP 2013.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504?507.
Aapo Hyv?arinen and Erkki Oja. 2000. Independent
component analysis: algorithms and applications.
Neural networks, 13(4):411?430.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007. A
vector space modeling approach to spoken language
identification. Audio, Speech, and Language Pro-
cessing, IEEE Transactions on, 15(1):271?284.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In Proceedings of the ICML.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al 2011. Scikit-learn:
Machine learning in python. The Journal of Ma-
chine Learning Research, 12:2825?2830.
Pamela Perniss, Robin L Thompson, and Gabriella
Vigliocco. 2010. Iconicity as a general property
of language: evidence from spoken and signed lan-
guages. Frontiers in psychology, 1.
E. Singer, PA Torres-Carrasquillo, TP Gleason,
WM Campbell, and D.A. Reynolds. 2003. Acous-
tic, phonetic, and discriminative approaches to auto-
matic language identification. In Proc. Eurospeech,
volume 9.
E. Singer, P. Torres-Carrasquillo, D. Reynolds, A. Mc-
Cree, F. Richardson, N. Dehak, and D. Sturim.
2012. The mitll nist lre 2011 language recogni-
tion system. In Odyssey 2012-The Speaker and Lan-
guage Recognition Workshop.
Thad Starner and Alex Pentland. 1997. Real-time
american sign language recognition from video us-
ing hidden markov models. In Motion-Based Recog-
nition, pages 227?243. Springer.
Thad Starner, Joshua Weaver, and Alex Pentland.
1998. Real-time american sign language recogni-
tion using desk and wearable computer based video.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1371?1375.
Sarah Taub. 2001. Language from the body: iconicity
and metaphor in American Sign Language. Cam-
bridge University Press, Cambridge.
C. Teixeira, I. Trancoso, and A. Serralheiro. 1996. Ac-
cent identification. In Spoken Language, 1996. IC-
SLP 96. Proceedings., Fourth International Confer-
ence on, volume 3, pages 1784?1787 vol.3.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. NAACL/HLT 2013, page 48.
Tingyao Wu, Jacques Duchateau, Jean-Pierre Martens,
and Dirk Van Compernolle. 2010. Feature subset
selection for improved native accent identification.
Speech Communication, 52(2):83?98.
Marcos Zampieri and Binyam Gebrekidan Gebre.
2012. Automatic identification of language vari-
eties: The case of portuguese. In Proceedings of
KONVENS, pages 233?237.
375
M.A. Zissman. 1996. Comparison of four approaches
to automatic language identification of telephone
speech. IEEE Transactions on Speech and Audio
Processing, 4(1):31?44.
376
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 216?223,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Improving Native Language Identification with TF-IDF Weighting
Binyam Gebrekidan Gebre1, Marcos Zampieri2, Peter Wittenburg1, Tom Heskes3
1Max Planck Institute for Psycholinguistics
2University of Cologne
3Radboud University
bingeb@mpi.nl,mzampier@uni-koeln.de,
peter.wittenburg@mpi.nl,t.heskes@science.ru.nl
Abstract
This paper presents a Native Language Iden-
tification (NLI) system based on TF-IDF
weighting schemes and using linear classi-
fiers - support vector machines, logistic re-
gressions and perceptrons. The system was
one of the participants of the 2013 NLI Shared
Task in the closed-training track, achieving
0.814 overall accuracy for a set of 11 native
languages. This accuracy was only 2.2 per-
centage points lower than the winner?s perfor-
mance. Furthermore, with subsequent evalua-
tions using 10-fold cross-validation (as given
by the organizers) on the combined training
and development data, the best average accu-
racy obtained is 0.8455 and the features that
contributed to this accuracy are the TF-IDF of
the combined unigrams and bigrams of words.
1 Introduction
Native Language Identification (NLI) is the task of
automatically identifying the native language of a
writer based on the writer?s foreign language pro-
duction. The task is modeled as a classification task
in which automatic methods have to assign class la-
bels (native languages) to objects (texts). NLI is by
no means trivial and it is based on the assumption
that the mother tongue influences Second Language
Acquisition (SLA) and production (Lado, 1957).
When an English native speaker hears someone
speaking English, it is not difficult for him/her to
identify if this person is a native speaker or not.
Moreover, it is, to some extent, possible to assert
the mother tongue of non-native speakers by his/hers
pronunciation patterns, regardless of their language
proficiency. In NLI, the same principle that seems
intuitive for spoken language, is applied to text. If
it is true that the mother tongue of an individual in-
fluences speech production, it should be possible to
identify these traits in written language as well.
NLI methods are particularly relevant for lan-
guages with a significant number of foreign speak-
ers, most notably, English. It is estimated that
the number of non-native speakers of English out-
numbers the number of native speakers by two to
one (Lewis et al, 2013). The written production
of non-native speakers is abundant on the Internet,
academia, and other contexts where English is used
as lingua franca.
This study presents the system that participated in
the 2013 NLI Shared Task (Tetreault et al, 2013)
under the name Cologne-Nijmegen. The novel as-
pect of the system is the use of TF-IDF weighting
schemes. For this study, we experimented with a
number of algorithms and features. Linear SVM and
logistic regression achieved the best accuracies on
the combined features of unigrams and bigrams of
words. The rest of the paper will explain in detail
the features, methods and results achieved.
2 Motivation
There are two main reasons to study NLI. On one
hand, there is a strong linguistic motivation, particu-
larly in the field of SLA and on the other hand, there
is the practical relevance of the task and its integra-
tion to a number of computational applications.
The linguistic motivation of NLI is the possibil-
ity of using classification methods to study the inter-
216
play between native and foreign language acquisi-
tion and performance (Wong and Dras, 2009). One
of the SLA theories that investigate these phenom-
ena is contrastive analysis, which is used to explain
why some structures of L2 are more difficult to ac-
quire than others (Lado, 1957).
Contrastive analysis postulates that the difficulty
in mastering L2 depends on the differences between
L1 and L2. In the process of acquiring L2, lan-
guage transfer (also known as L1 interference) oc-
curs and speakers apply knowledge from their na-
tive language to a second language, taking advan-
tage of their similarities. Computational methods
applied to L2 written production can function as a
corpus-driven method to level out these differences
and serve as a source of information for SLA re-
searchers. It can also be used to provide more tar-
geted feedback to language learners about their er-
rors.
NLI is also a relevant task in computational lin-
guistics and researchers have turned their attention
to it in the last few years. The task is often regarded
as a part of a broader task of authorship profiling,
which consists of the application of automatic meth-
ods to assert information about the writer of a given
text, such as age, gender as well native language.
Authorship profiling is particularly useful for foren-
sic linguistics.
Automatic methods of NLI may be integrated in
NLP applications such as spam detection or machine
translation. NLP tasks such as POS tagging and
parsing might also benefit from NLI, as these re-
sources are trained on standard language written by
native speakers. These tools can be more accurate to
tag non-native speaker?s text if trained with L2 cor-
pora.
3 Related Work
In the last years, a couple of attempts at identifying
native language have been described in the literature.
Tomokiyo and Jones (2001) uses a Naive Bayes al-
gorithm to classify transcribed data from three native
languages: Chinese, Japanese and English. The al-
gorithm reached 96% accuracy when distinguishing
native from non-native texts and 100% when distin-
guishing English native speakers from Chinese na-
tive speakers.
Koppel et al (2005) used machine learning to
identify the native languages of non-native English
speakers with five different mother tongues (Bul-
garian, Czech, French, Russian, and Spanish), us-
ing data retrieved from the International Corpus of
Learner English (ICLE) (Granger et al, 2009). The
features used in this study were function words,
character n-grams, and part-of-speech (POS) bi-
grams.
Tsur and Rappoport (2007) investigated the influ-
ence of the phonology of a writer?s mother tongue
through native language syllables modelled by char-
acter bigrams. Estival et al (2007) addressed NLI as
part of authorship profiling. Authors aim to attribute
10 different characteristics of writers by analysing
a set of English e-mails. The study reports around
84% accuracy in distinguishing e-mails written by
English Arabic and Spanish L1 speakers.
SVM, the algorithm that achieved the best results
in our experiments, was also previously used in NLI
(Kochmar, 2011). In this study, the author identi-
fied error types that are typical for speakers of differ-
ent native languages. She compiled a set of features
based on these error types to improve the classifica-
tion?s performance.
Recently, the TOEFL11 corpus was compiled to
serve as an alternative to the ICLE corpus (Tetreault
et al, 2012). Authors argue that TOEFL11 is more
suitable to NLI than ICLE. This study also experi-
mented with different features to increase results in
NLI and reports best accuracy results of 90.1% on
ICLE and 80.9% on TOEFL11.
4 Methods
We approach the task of native language identifica-
tion as a kind of text classification. In text classifica-
tion, decisions and choices have to be made at three
levels. First, how do we use the training and devel-
opment data? Second, what features do we extract
and how do we select the most informative ones?
Third, which machine learning algorithms perform
best and which parameters can we tune under the
constraints of memory and time? In the following
subsections, we answer these questions.
217
4.1 Dataset: TOEFL11
The dataset used for the shared task is called
TOEFL11 (Blanchard et al, 2013). It consists of
12,100 English essays (about 300 to 400 words long)
from the Test of English as a Foreign Language
(TOEFL). The essays are written by 11 native lan-
guage speakers (L1). Table 1 shows the 11 na-
tive languages. Each essay is labelled with an En-
glish language proficiency level (high, medium, or
low) based on the judgments of human assessment
specialists. We used 9,900 essays for training data
and 1,100 for development (parameter tuning). The
shared task organizers kept 1,100 essays for testing.
Table 1: TOEFL11
L1 languages Arabic, Chinese,
French, German,
Hindi, Italian,
Japanese, Korean,
Spanish, Telugu,
Turkish
# of essays per L1
900 for training
100 for validating
100 for testing
4.2 Features
We explored different kinds and combinations of
features that we assumed to be different for different
L1 speakers and that are also commonly used in the
NLI literature (Koppel et al, 2005; Tetreault et al,
2012). Table 2 shows the sources of the features we
considered. Unigrams and bigrams of words are ex-
plored separately and in combination. One through
four grams of part of speech tags have also been ex-
plored. For POS tagging of the essays, we applied
the default POS tagger from NLTK (Bird, 2006).
Spelling errors have also been treated as features.
We used the collection of words in Peter Norvig?s
website1 as a reference dictionary. The collection
consists of about a million words. It is a concate-
nation of several public domain books from Project
Gutenberg and lists of most frequent words from
Wiktionary and the British National Corpus.
Character n-grams have also been explored for
both the words in the essays and for words with
1http://norvig.com/spell-correct.html
spelling errors. The maximum n-gram size consid-
ered is six.
All features, consisting of either characters or
words or part-of-speech tags or their combinations,
are mapped into normalized numbers (norm L2).
For the mapping, we use TF-IDF, a weighting tech-
nique popular in information retrieval but which is
also finding its use in text classification. Features
that occurred in less than 5 of the essays or those
that occurred in more than 50% of the essays are
removed (all characters are in lower case). These
cut-off values are experimentally selected.
Table 2: A summary of features used in our experiments
Word n-grams Unigrams and bigrams of
words present in the es-
says.
POS n-grams One up to four grams of
POS tags present in the
essays; tagging is done
using default NLTK tag-
ger (Bird, 2006).
Character n-grams One up to six grams of
characters in each essay.
Spelling errors All words that are not
found in the dictionary
of Peter Norvig?s spelling
corrector.
4.2.1 Term Frequency (TF)
Term Frequency refers to the number of times a
particular term appears in an essay. In our experi-
ments, terms are n-grams of characters, words, part-
of-speech tags or any combination of them. The
intuition is that a term that occurs more frequently
identifies/specifies the essay better than another term
that occurs less frequently. This seems a useful
heuristic but what is the relationship between the fre-
quency of a term and its importance to the essay?
From among many relationships, we selected a log-
arithmic relationship (sublinear TF scaling) (Man-
ning et al, 2008):
wft,e =
{
1 + log(tft,e) if tft,e > 0
0 otherwise
(1)
218
where wft,e refers to weight and tft,e refers to the
frequency of term t in essay e.
The wft,e weight tells us the importance of a term
in an essay based on its frequency. But not all terms
that occur more frequently in an essay are equally
important. The effective importance of a term also
depends on how infrequent the term is in other es-
says and this intuition is handled by Inverse Docu-
ment Frequency(IDF).
4.2.2 Inverse Document Frequency(IDF)
Inverse Document Frequency (IDF) quantifies the
intuition that a term which occurs in many essays
is not a good discriminator, and should be given
less weight than one which occurs in fewer essays.
In mathematical terms, IDF is the log of the in-
verse probability of a term being found in any essay
(Salton and McGill, 1984):
idf(ti) = log
N
ni
, (2)
where N is the number of essays in the corpus,
and term ti occurs in ni of them. IDF gives a new
weight when combined with TF to form TF-IDF.
4.2.3 TF?IDF
TF?IDF combines the weights of TF and IDF
by multiplying them. TF gives more weight to a
frequent term in an essay and IDF downscales the
weight if the term occurs in many essays. Equation
3 shows the final weight that each term of an essay
gets before normalization.
wi,e = (1 + log(tft,e))? log(N/ni) (3)
Essay lengths are usually different and this has an
impact on term weights. To abstract from different
essay lengths, each essay feature vector is normal-
ized to unit length. After normalization, the result-
ing essay feature vectors are fed into classifiers.
4.3 Classifiers
We experimented with three linear classifiers - lin-
ear support vector machines, logistic regression and
perceptrons - all from scikit-learn (Pedregosa et al,
2011). These algorithms are suitable for high dimen-
sional and sparse data (text data is high dimensional
and sparse). In the following paragraphs, we briefly
describe the algorithms and the parameter values we
selected.
SVMs have been explored systematically for text
categorization (Joachims, 1998). An SVM classi-
fier finds a hyperplane that separates examples into
two classes with maximal margin (Cortes and Vap-
nik, 1995) (Multi-classes are handled by multi one-
versus-rest classifiers). Examples that are not lin-
early separable in the feature space are mapped to a
higher dimension using kernels. In our experiments,
we used a linear kernel and a penalty parameter of
value 1.0.
In its various forms, logistic regression is also
used for text classification (Zhang et al, 2003;
Genkin et al, 2007; Yu et al, 2011) and native
language identification (Tetreault et al, 2012). Lo-
gistic regression classifies data by using a decision
boundary, determined by a linear function of the fea-
tures. For the implementation of the algorithm, we
used the LIBLINEAR open source library (Fan et
al., 2008) from scikit-learn (Pedregosa et al, 2011)
and we fixed the regularization parameter to 100.0.
For baseline, we used a perceptron classifier
(Rosenblatt, 1957). Perceptron (or single layer net-
work) is the simplest form of neural network. It is
designed for linear separation of data and works well
for text classification. The number of iterations of
the training algorithm is fixed to 70 and the rest of
parameters are left with their default values.
5 Results and Discussion
For each classifier, we ran ten-fold cross-validation
experiments. We divided the training and develop-
ment data into ten folds using the same fold splitting
ids as requested by the shared task organizers and
also as used in (Tetreault et al, 2012). Nine of the
folds were used for training and the tenth for test-
ing the trained model. This was repeated ten times
with each fold being held out for testing. The per-
formance of the classifiers on different features are
presented in terms of average accuracy.
Table 3 gives the average accuracies based on
the TF-IDF of word and character n-grams. Lin-
ear SVM gives the highest accuracy of 84.55% us-
ing features extracted from unigrams and bigrams
of words. Logistic regression also gives comparable
accuracy of 84.45% on the same features.
219
Table 3: Cross-validation results; accuracy in %
N-gram LinearSVM
Logistic
Regression Perceptron
Words
1 74.73 74.18 65.45
2 80.91 80.27 75.45
1 and 2 84.55 84.45 78.82
(1 and 2)* 83.36 83.27 78.73
* minus country and language names
Characters
1 18.45 19.27 9.09
2 43.27 40.82 10.36
3 71.36 68.00 36.91
4 80.36 79.91 59.64
5 83.09 82.64 73.91
6 84.09 84.00 76.45
The size of the feature vector of unigrams and bi-
grams of words is 73,6262. For each essay, only a
few of the features have non-zero values. Which
features are active and most discriminating in the
classifiers? Table 4 shows the ten most informative
features for the 10th run in the cross-validation (as
picked up linear SVM).
Table 4: Ten most informative features for each L1
ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alot
CHI in china / hold / china / time on / may / taiwan / just /still / , the / . take
FRE french / conclude , / even if / in france / france / toconclude / indeed , / ... / . indeed / indeed
GER special / furthermore / might / germany / , because /have to / . but / - / often / , that
HIN which / and concept / various / hence / generation / &/ towards / then / its / as compared
ITA in italy / , for / infact / that a / italy / i think / in fact /italian / think that / :
JPN , and / i disagree / is because / . it / . if / i think /japan , / japanese / in japan / japan
KOR . however / however , / even though / however / thesedays / various / korea , / korean / in korea / korea
SPA an specific / because is / moment / , etc / going to / ,is / necesary / , and / diferent / , but
TEL
may not / the statement / every one / days / the above
/ where as / with out / when compared / i conclude /
and also
TUR ages / istanbul / addition to / conditions / enough / inturkey / the life / ; / . because / turkey
The ten most informative features include coun-
2features that occur less than 5 times or that occur in more
than 50% of the essays are removed from the vocabulary
try and language names. For example, for Japanese
and Korean L1s, four of the ten top features include
Korea or Korean in the unigrams or bigrams. How
would the classification accuracy decrease if we re-
moved mentions of country or language names?
We made a list of the 11 L1 language names and
the countries where they are mainly spoken (for ex-
ample, German, Germany, French, France, etc.). We
considered this list as stop words (i.e. removed them
from corpus) and ran the whole classification exper-
iments. The new best accuracy is 83.36% ( a loss of
just 1.2% ). Table 3 shows the new accuracies for all
classifiers. The new top ten features mostly consist
of function words and some spelling errors. Table 5
shows all of the new top ten features.
The spelling errors seem to have been influenced
by the L1 languages, especially for French and
Spanish languages. The English words example
and developed have similar sounding/looking equiv-
alents in French (exemple and de?veloppe?) . Simi-
larly, the English words necessary and different have
similar sounding/looking words in Spanish (nece-
sario and diferente). These spelling errors made it
to the top ten features. But how discriminating are
they on their own?
Table 5: Ten most informative features (minus country
and language names) for each L1
ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alot
CHI and more / hold / more and / time on / taiwan / may /just / still / . take / , the
FRE conclude / exemple / developped / conclude , / evenif / to conclude / indeed , / ... / . indeed / indeed
GER has to / special / furthermore / might / , because /have to / . but / - / often / , that
HIN and concept / which / various / hence / generation / &/ towards / then / its / as compared
ITA possibility / probably / particular / , for / infact / thata / i think / in fact / think that / &
JPN i agree / the opinion / tokyo / two reasons / is because/ , and / i disagree / . it / . if / i think
KOR creative / , many / ?s / . also / . however / even though/ however , / various / however / these days
SPA activities / an specific / moment / , etc / going to / , is/ necesary / , and / diferent / , but
TEL
may not / the statement / every one / days / the above
/ where as / when compared / with out / i conclude /
and also
TUR enjoyable / being / ages / addition to / istanbul /enough / conditions / the life / ; / . because
We ran experiments with features extracted from
220
Table 6: Confusion matrix: Best accuracy is for German (95%) and the worst is for Hindi (72%)
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
ARA 83 1 4 1 1 3 1 2 3 1 0
CHI 0 88 2 0 2 0 2 5 1 0 0
FRE 3 0 88 2 1 2 0 1 2 0 1
GER 2 0 1 95 0 0 0 0 1 0 1
HIN 2 1 1 1 72 0 0 0 2 18 3
ITA 0 0 6 3 0 84 0 0 6 0 1
JPN 1 2 0 1 1 0 84 10 0 0 1
KOR 0 3 0 2 3 0 8 81 1 1 1
SPA 6 2 5 2 0 4 0 0 79 0 2
TEL 0 0 0 0 16 0 1 0 0 83 0
TUR 1 1 0 1 3 0 0 0 1 0 93
only spelling errors. For comparison, we also ran
experiments with POS tags with and without their
words. None of these experiments beat the best ac-
curacy obtained using unigram and bigram of words
- not even the unigram and bigram of POS tagged
words. See table 7 for the obtained results.
Table 7: Cross-validation results; accuracy in %
N-gram LinearSVM
Logistic
Regression Perceptron
POS
1 17.00 17.09 9.09
2 43.45 40.00 11.18
3 55.27 53.55 35.36
4 56.09 56.18 48.64
POS + Word
1 75.09 74.18 64.09
2 80.45 80.64 76.18
1 and 2 83.00 83.36 79.09
Spelling errors - characters
1 20.36 21.00 9.09
2 34.09 32.64 9.73
3 47.00 44.64 26.82
4 50.82 48.09 41.64
1?4 51.82 48.27 34.18
words 42.73 39.45 28.73
All our reported results so far have been global
classification results. Table 6 shows the confusion
matrix for each L1. The best accuracy is 95% for
German and the worst is for Hindi (72%). Hindi
is classified as Telugu (18%) of the times and Tel-
ugu is classified as Hindi 16% of the times and
only one Telugu essay is classified as any other than
Hindi. More generally, the confusion matrix seems
to suggest that geographically closer countries are
more confused with each other: Hindi and Telugu,
Japanese and Korean, Chinese and Korean.
The best accuracy (84.55%) obtained in our ex-
periments is higher than the state-of-the-art accuracy
reported in (Tetreault et al, 2012) (80.9%). But the
features we used are not different from those com-
monly used in the literature (Koppel et al, 2005;
Tetreault et al, 2012) (n-grams of characters or
words). The novel aspect of our system is the use
of TF-IDF weighting on all of the features including
on unigrams and bigrams of words.
TF-IDF weighting has already been used in na-
tive language identification (Kochmar, 2011; Ahn,
2011). But its importance has not been fully ex-
plored. Experiments in Kochmar (2011) were lim-
ited to character grams and in a binary classifica-
tion scenario. Experiments in Ahn (2011) applied
TF-IDF weighting to identify content words and
showed how their removal decreased performance
(Ahn, 2011). By contrast, in this paper, we applied
TF-IDF weighting consistently to all features - same
type features (e.g. unigrams) or combined features
(e.g. unigram and bigrams).
How would the best accuracy change if TF-IDF
weighting is not applied? Table 8 shows the changes
to the best average accuracies with and without
TF/IDF weighting for the three classifiers.
Table 8: The importance of TF-IDF weighting
TF IDF SVM LR Perceptron
Yes Yes 84.55 84.45 78.82
Yes No 80.82 80.73 63.18
No Yes 82.36 82.27 78.82
No No 79.18 78.55 56.36
221
6 Conclusions
This paper has presented the system that participated
in the 2013 NLI Shared Task in the closed-training
track. Cross-validation testing on the TOEFL11 cor-
pus showed that the system could achieve an accu-
racy of about 84.55% in categorizing unseen essays
into one of the eleven L1 languages.
The novel aspect of the system is the use
of TF-IDF weighting schemes on features ?
which could be any or combination of n-gram
words/characters/POS tags. The feature combina-
tion that gave the best accuracy is the TF-IDF of
unigrams and bigrams of words. The next best fea-
ture class is the TF-IDF of 6-gram characters , which
achieved 84.09%, very close to 84.55%. Both lin-
ear support vector machines and logistic regression
classifiers have performed almost equally.
To improve performance in NLI, future work
should examine new features that can classify ge-
ographically or typologically related languages such
as Hindi and Telugu. Future work should also ana-
lyze the information obtained in NLI experiments to
quantify and investigate differences in the usage of
foreign language lexicon or grammar according to
the individual?s mother tongue.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Commissions
7th Framework Program under grant agreement no
238405 (CLARA). The authors would like to thank
the organizers of the NLI Shared Task 2013 for pro-
viding prompt reply to all our inquiries and for coor-
dinating a very interesting and fruitful shared task.
References
Charles S. Ahn. 2011. Automatically detecting authors?
native language. Ph.D. thesis, Monterey, California.
Naval Postgraduate School.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, pages 69?72. Association for
Computational Linguistics.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author Profiling
for English Emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Alexander Genkin, David D Lewis, and David Madigan.
2007. Large-scale bayesian logistic regression for text
categorization. Technometrics, 49(3):291?304.
Sylviane Granger, Estelle Dagneaux, and Fanny Meu-
nier. 2009. International Corpus of Learner English.
Presses Universitaires de Louvain, Louvain-la-Neuve.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. Springer.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge, United Kingdom.
Moshe Koppel, Jonathan Schler, and Kfir Zigon. 2005.
Automatically determining an anonymous author?s na-
tive language. Lecture Notes in Computer Science,
3495:209?217.
Robert Lado. 1957. Applied Linguistics for Language
Teachers. University of Michigan Press.
Paul Lewis, Gary Simons, and Charles Fennig. 2013.
Ethnologue: Languages of the World, Seventeeth Edi-
tion. SIL International.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval, volume 1. Cambridge University Press Cam-
bridge.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and E?douard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learn-
ing Research, 12:2825?2830.
Frank Rosenblatt. 1957. The perceptron, a perceiv-
ing and recognizing automaton Project Para. Cornell
Aeronautical Laboratory.
Gerard Salton and Michael McGill. 1984. Introduction
to Modern Information Retrieval. McGraw-Hill Book
Company.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
222
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
Summary report on the first shared task on native lan-
guage identification. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NLP, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you?: Naive bayes
detection of non-native utterance text. In Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies (NAACL ?01).
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop, pages 53?61. Citeseer.
Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy models. Machine
Learning, 85(1-2):41?75.
Jian Zhang, Rong Jin, Yiming Yang, and Alexander G.
Hauptmann. 2003. Modified logistic regression: An
approximation to svm and its applications in large-
scale text categorization. In ICML, pages 888?895.
223
