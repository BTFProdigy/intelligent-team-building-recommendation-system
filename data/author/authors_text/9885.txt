Rapid Prototyping of Robust Language Understanding Modules
for Spoken Dialogue Systems
?Yuichiro Fukubayashi, ?Kazunori Komatani, ?Mikio Nakano,
?Kotaro Funakoshi, ?Hiroshi Tsujino, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{fukubaya,komatani}@kuis.kyoto-u.ac.jp
{ogata,okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
nakano@jp.honda-ri.com
{funakoshi,tsujino}@jp.honda-ri.com
Abstract
Language understanding (LU) modules for
spoken dialogue systems in the early phases
of their development need to be (i) easy
to construct and (ii) robust against vari-
ous expressions. Conventional methods of
LU are not suitable for new domains, be-
cause they take a great deal of effort to
make rules or transcribe and annotate a suf-
ficient corpus for training. In our method,
the weightings of the Weighted Finite State
Transducer (WFST) are designed on two
levels and simpler than those for conven-
tional WFST-based methods. Therefore,
our method needs much fewer training data,
which enables rapid prototyping of LU mod-
ules. We evaluated our method in two dif-
ferent domains. The results revealed that our
method outperformed baseline methods with
less than one hundred utterances as training
data, which can be reasonably prepared for
new domains. This shows that our method
is appropriate for rapid prototyping of LU
modules.
1 Introduction
The language understanding (LU) of spoken dia-
logue systems in the early phases of their devel-
opment should be trained with a small amount of
data in their construction. This is because large
amounts of annotated data are not available in the
early phases. It takes a great deal of effort and time
to transcribe and provide correct LU results to a
Figure 1: Relationship between our method and con-
ventional methods
large amount of data. The LU should also be robust,
i.e., it should be accurate even if some automatic
speech recognition (ASR) errors are contained in its
input. A robust LU module is also helpful when col-
lecting dialogue data for the system because it sup-
presses incorrect LU and unwanted behaviors. We
developed a method of rapidly prototyping LU mod-
ules that is easy to construct and robust against var-
ious expressions. It makes LU modules in the early
phases easier to develop.
Several methods of implementing an LU mod-
ule in spoken dialogue systems have been proposed.
Using grammar-based ASR is one of the simplest.
Although its ASR output can easily be transformed
into concepts based on grammar rules, complicated
grammars are required to understand the user?s ut-
terances in various expressions. It takes a great deal
of effort to the system developer. Extracting con-
210
Figure 2: Example of WFST for LU
cepts from user utterances by keyword spotting or
heuristic rules has also been proposed (Seneff, 1992)
where utterances can be transformed into concepts
without major modifications to the rules. However,
numerous complicated rules similarly need to be
manually prepared. Unfortunately, neither method
is robust against ASR errors.
To cope with these problems, corpus-based (Su-
doh and Tsukada, 2005; He and Young, 2005) and
Weighted Finite State Transducer (WFST)-based
methods (Potamianos and Kuo, 2000; Wutiwi-
watchai and Furui, 2004) have been proposed as LU
modules for spoken dialogue systems. Since these
methods extract concepts using stochastic analy-
sis, they do not need numerous complicated rules.
These, however, require a great deal of training data
to implement the module and are not suitable for
constructing new domains.
Here, we present a new WFST-based LU module
that has two main features.
1. A statistical language model (SLM) for ASR
and a WFST for parsing that are automatically
generated from the domain grammar descrip-
tion.
2. Since the weighting for the WFST is simpler
than that in conventional methods, it requires
fewer training data than conventional weight-
ing schemes.
Our method accomplishes robust LU with less ef-
fort using SLM-based ASR and WFST parsing. Fig-
ure 1 outlines the relationships between our method
and conventional schemes. Since rule- or grammar-
based approaches do not require a large amount of
data, they take less effort than stochastic techniques.
However, they are not robust against ASR errors.
Stochastic approaches, on the contrary, take a great
deal of effort to collect data but are robust against
ASR errors. Our method is an intermediate approach
that lies between these. That is, it is more robust than
rule- or grammar-based approaches and takes less
effort than stochastic techniques. This characteristic
makes it easier to rapidly prototype LU modules for
a new domain and helps development in the early
phases.
2 Related Work and WFST-based
Approach
A Finite State Transducer (FST)-based LU is ex-
plained here, which accepts ASR output as its in-
put. Figure 2 shows an example of the FST for a
video recording reservation domain. The input, ?,
means that a transition with no input is permitted at
the state transition. In this example, the LU mod-
ule returns the concept [month=2, day=22] for the
utterance ?It is February twenty second please?.
Here, a FILLER transition in which any word is ac-
cepted is appropriately allowed between phrases. In
Figure 2, ?F? represents 0 or more FILLER tran-
sitions. A FILLER transition from the start to the
end is inserted to reject unreliable utterances. This
FILLER transition enables us to ignore unnecessary
words listed in the example utterances in Table 1.
The FILLER transition helps to suppress the inser-
tion of incorrect concepts into LU results.
However, many output sequences are obtained for
one utterance due to the FILLER transitions, be-
cause the utterance can be parsed with several paths.
We used a WFST to select the most appropriate
path from several output sequences. The path with
the highest cumulative weight, w, is selected in a
211
Table 2: Many LU results for input ?It is February twenty second please?
LU output LU result w
It is February twenty second please month=2, day=22 2.0
It is FILLER twenty second please day=22 1.0
It is FILLER twenty second FILLER day=22 1.0
FILLER FILLER FILLER FILLER FILLER FILLER n/a 0
Table 1: Examples of utterances with FILLERs
ASR output
Well, it is February twenty second please
It is uhm, February twenty second please
It is February, twe-, twenty second please
It is February twenty second please, OK?
(LU result = [month=2, day=22])
WFST-based LU. In the example in Table 2, the
concept [month=2, day=22] has been selected, be-
cause its cumulative weight, w, is 2.0, which is the
highest.
The weightings of conventional WFST-based ap-
proaches used an n-gram of concepts (Potamianos
and Kuo, 2000) and that of word-concept pairs (Wu-
tiwiwatchai and Furui, 2004). They obtained the
n-grams from several thousands of annotated ut-
terances. However, it takes a great deal of ef-
fort to transcribe and annotate a large corpus. Our
method enables prototype LU modules to be rapidly
constructed that are robust against various expres-
sions with SLM-based ASR and WFST-based pars-
ing. The SLM and WFST are generated automat-
ically from a domain grammar description in our
toolkit. We need fewer data to train WFST, because
its weightings are simpler than those in conventional
methods. Therefore, it is easy to develop an LU
module for a new domain with our method.
3 Domain Grammar Description
A developer defines grammars, slots, and concepts
in a domain in an XML file. This description en-
ables an SLM for ASR and parsing WFST to be au-
tomatically generated. Therefore, a developer can
construct an LU module rapidly with our method.
Figure 3 shows an example of a descrip-
tion. A definition of a slot is described in
keyphrase-class tags and its keyphrases and
...
<keyphrase-class name="month">
...
<keyphrase>
<orth>February</orth>
<sem>2</sem>
</keyphrase>
...
</keyphrase-class>
...
<action type="specify-attribute">
<sentence> {It is} [*month] *day [please]
</sentence>
</action>
Figure 3: Example of a grammar description
the values are in keyphrase tags. The month is
defined as a slot in this figure. February and 2 are
defined as one of the phrases and values for the slot
month. A grammar is described in a sequence of
terminal and non-terminal symbols. A non-terminal
symbol represents a class of keyphrases, which is
defined in keyphrase-class. It begins with an
asterisk ?*? in a grammar description in sentence
tags. Symbols that can be skipped are enclosed
by brackets []. The FILLER transition described
in Section 2 is inserted between the symbols un-
less they are enclosed in brackets [] or braces {}.
Braces are used to avoid FILLER transitions from
being inserted. For example, the grammar in Figure
3 accepts ?It is February twenty second please.? and
?It is twenty second, OK??, but rejects ?It is Febru-
ary.? and ?It, uhm, is February twenty second.?.
A WFST for parsing can be automatically gener-
ated from this XML file. The WFST in Figure 2 is
generated from the definition in Figure 3. Moreover,
we can generate example sentences from the gram-
mar description. The SLM for the speech recognizer
is generated with our method by using many exam-
ple sentences generated from the defined grammar.
212
4 Weighting for ASR Outputs on Two
Levels
We define weights on two levels for a WFST. The
first is a weighting for ASR outputs, which is set to
select paths that are reliable at a surface word level.
The second is a weighting for concepts, which is
used to select paths that are reliable on a concept
level. The weighting for concepts reflects correct-
ness at a more abstract level than the surface word
level. The weighting for ASR outputs consists of
two categories: a weighting for ASR N-best outputs
and one for accepted words. We will describe the
definitions of these weightings in the following sub-
sections.
4.1 Weighting for ASR N-Best Outputs
The N-best outputs of ASR are used for an input of
a WFST. Weights are assigned to each sentence in
ASR N-best outputs. Larger weights are given to
more reliable sentences, whose ranks in ASR N-best
are higher. We define this preference as
wis =
e??scorei
?N
j e??scorej
,
where wis is a weight for the i-th sentence in ASRN-best outputs, ? is a coefficient for smoothing, and
scorei is the log-scaled score of the i-th ASR out-put. This weighting reflects the reliability of the
ASR output. We set ? to 0.025 in this study after
a preliminary experiment.
4.2 Weighting for Accepted Words
Weights are assigned to word sequences that have
been accepted by the WFST. Larger weights are
given to more reliable sequences of ASR outputs at
the surface word level. Generally, longer sequences
having more words that are not fillers and more re-
liable ASR outputs are preferred. We define these
preferences as the weights:
1. word(const.): ww = 1.0,
2. word(#phone): ww = l(W ), and
3. word(CM): ww = CM(W ) ? ?w.
The word(const.) gives a constant weight to
all accepted words. This means that sequences
with more words are simply preferred. The
word(#phone) takes the length of each accepted
word into consideration. This length is measured by
its number of phonemes, which are normalized by
that of the longest word in the vocabulary. The nor-
malized values are denoted as l(W ) (0 < l(W ) ?
1). By adopting word(#phone), the length of se-
quences is represented more accurately. We also
take the reliability of the accepted words into ac-
count as word(CM). This uses confidence measures
(Lee et al, 2004) for a word, W , in ASR outputs,
which are denoted as CM(W ). The ?w is the thresh-old for determining whether word W is accepted or
not. The ww obtains a negative value for an unreli-able word W when CM(W ) is lower than ?w. Thisrepresents a preference for longer and more reliable
sequences.
4.3 Weighting for Concepts
In addition to the ASR level, weights on a concept
level are also assigned. The concepts are obtained
from the parsing results by the WFST, and contain
several words. Weights for concepts are defined by
using the measures of all words contained in a con-
cept.
We prepared three kinds of weights for the con-
cepts:
1. cpt(const.): wc = 1.0,
2. cpt(avg):
wc =
?
W (CM(W ) ? ?c)
#W , and
3. cpt(#pCM(avg)):
wc =
?
W (CM(W ) ? l(W ) ? ?c)
#W ,
where W is a set of accepted words, W , in the corre-
sponding concept, and #W is the number of words
in W .
The cpt(const.) represents a preference for
sequences with more concepts. The cpt(avg)
is defined as the weight by using the CM(W )
of each word contained in the concept. The
cpt(#pCM(avg)) represents a preference for longer
and reliable sequences with more concepts. The ?cis the threshold for the acceptance of a concept.
213
Table 3: Examples of weightings when parameter set is: word(CM) and cpt(#pCM(avg))
ASR onput No, it is February twenty second
LU output FILLER it is February twenty second
CM(W ) 0.3 0.7 0.6 0.9 1.0 0.9
l(W ) 0.3 0.2 0.2 0.9 0.6 0.5
Concept - - - month=2 day=22
word - 0.7 ? ?w 0.6 ? ?w 0.9 ? ?w 1.0 ? ?w 0.9 ? ?wcpt - - - (0.9 ? 0.9 ? ?c)/1 (1.0 ? 0.6 ? ?c + 0.9 ? 0.5 ? ?c)/2
'
&
$
%
Reference From June third please
ASR output From June third uhm FIT please LU result
CM(W ) 0.771 0.978 0.757 0.152 0.525 0.741
LU reference From June third FILLER FILLER FILLER month:6, day:3
Our method From June third FILLER FILLER FILLER month:6, day:3
Keyword spotting From June third FILLER FIT please month:6, day:3, car:FIT
(?FIT? is the name of a car.)
Figure 4: Example of LU with WFST
4.4 Calculating Cumulative Weight and
Training
The LU results are selected based on the weighted
sum of the three weights in Subsection 4.3 as
wi = wis + ?w
?
ww + ?c
?
wc
The LU module selects an output sequence with
the highest cumulative weight, wi, for 1 ? i ? N .
Let us explain how to calculate cumulative weight
wi by using the example specified in Table 3. Here,
word(CM) and cpt(#pCM(avg)) are selected as pa-
rameters. The sum of weights in this table for ac-
cepted words is ?w(4.1 ? 5?w), when the input se-quence is ?No, it is February twenty second.?.
The sum of weights for concepts is ?c(1.335 ? 2?c)because the weight for ?month=2? is ?c(0.81 ? ?c)and the weight for ?day=22? is ?c(0.525 ? ?c).Therefore, cumulative weight wi for this input se-
quence is wis + ?w(4.1 ? 5?w) + ?c(1.335 ? 2?c).In the training phase, various combinations of pa-
rameters are tested, i.e., which weightings are used
for each of ASR output and concept level, such as
N = 1 or 10, coefficient ?w,c = 1.0 or 0, and thresh-old ?w,c = 0 to 0.9 at intervals of 0.1, on the train-ing data. The coefficient ?w,c = 0 means that acorresponding weight is not added. The optimal pa-
rameter settings are obtained after testing the various
combinations of parameters. They make the concept
error rate (CER) minimum for a training data set.
We calculated the CER in the following equation:
CER = (S +D + I)/N , where N is the number of
concepts in a reference, and S, D, and I correspond
to the number of substitution, deletion, and insertion
errors.
Figure 4 shows an example of LU with our
method, where it rejects misrecognized concept
[car:FIT], which cannot be rejected by keyword
spotting.
5 Experiments and Evaluation
5.1 Experimental Conditions
We discussed our experimental investigation into the
effects of weightings in Section 4. The user utter-
ance in our experiment was first recognized by ASR.
Then, the i-th sentence of ASR output was input to
WFST for 1 ? i ? N , and the LU result for the
highest cumulative weight, wi, was obtained.
We used 4186 utterances in the video recording
reservation domain (video domain), which consisted
of eight different dialogues with a total of 25 differ-
ent speakers. We also used 3364 utterances in the
rent-a-car reservation domain (rent-a-car domain) of
214
eight different dialogues with 23 different speakers.
We used Julius 1 as a speech recognizer with an
SLM. The language model was prepared by using
example sentences generated from the grammars of
both domains. We used 10000 example sentences in
the video and 40000 in the rent-a-car domain. The
number of the generated sentences was determined
empirically. The vocabulary size was 209 in the
video and 891 in the rent-a-car domain. The average
ASR accuracy was 83.9% in the video and 65.7%
in the rent-a-car domain. The grammar in the video
domain included phrases for dates, times, channels,
commands. That of the rent-a-car domain included
phrases for dates, times, locations, car classes, op-
tions, and commands. The WFST parsing mod-
ule was implemented by using the MIT FST toolkit
(Hetherington, 2004).
5.2 Performance of WFST-based LU
We evaluated our method in the two domains: video
and rent-a-car. We compared the CER on test data,
which was calculated by using the optimal settings
for both domains. We evaluated the results with 4-
fold cross validation. The number of utterances for
training was 3139 (=4186*(3/4)) in the video and
2523 (=3364*(3/4)) in the rent-a-car domain.
The baseline method was simple keyword spot-
ting because we assumed a condition where a large
amount of training data was not available. This
method extracts as many keyphrases as possible
from ASR output without taking speech recogni-
tion errors and grammatical rules into consideration.
Both grammar-based and SLM-based ASR outputs
are used for input in keyword spotting (denoted as
?Grammar & spotting? and ?SLM & spotting? in
Table 4). The grammar for grammar-based ASR
was automatically generated by the domain descrip-
tion file. The accuracy of grammar-based ASR was
66.3% in the video and 43.2% in the rent-a-car do-
main.
Table 4 lists the CERs for both methods. In key-
word spotting with SLM-based ASR, the CERs were
improved by 5.2 points in the video and by 22.2
points in the rent-a-car domain compared with those
with grammar-based ASR. This is because SLM-
based ASR is more robust against fillers and un-
1http://julius.sourceforge.jp/
Table 4: Concept error rates (CERs) in each domain
Domain Grammar &spotting SLM &spotting Ourmethod
Video 22.1 16.9 13.5
Rent-a-car 51.1 28.9 22.0
known words than grammar-based ASR. The CER
was improved by 3.4 and 6.9 points by optimal
weightings for WFST. Table 5 lists the optimal pa-
rameters in both domains. The ?c = 0 in the videodomain means that weights for concepts were not
used. This result shows that optimal parameters de-
pend on the domain for the system, and these need
to be adapted for each domain.
5.3 Performance According to Training Data
We also investigated the relationship between the
size of the training data for our method and the CER.
In this experiment, we calculated the CER in the
test data by increasing the number of utterances for
training. We also evaluated the results by 4-fold
cross validation.
Figures 5 and 6 show that our method outper-
formed the baseline methods by about 80 utterances
in the video domain and about 30 utterances in
the rent-a-car domain. These results mean that our
method can effectively be used to rapidly prototype
LU modules. This is because it can achieve robust
LU with fewer training data compared with conven-
tional WFST-based methods, which need over sev-
eral thousand sentences for training.
6 Conclusion
We developed a method of rapidly prototyping ro-
bust LU modules for spoken language understand-
ing. An SLM for a speech recognizer and a WFST
for parsing were automatically generated from a do-
main grammar description. We defined two kinds
of weightings for the WFST at the word and con-
cept levels. These two kinds of weightings were
calculated by ASR outputs. This made it possi-
ble to create an LU module for a new domain with
less effort because the weighting scheme was rel-
atively simpler than those of conventional methods.
The optimal parameters could be selected with fewer
training data in both domains. Our experiment re-
215
Table 5: Optimal parameters in each domain
Domain N ?w ww ?c wc
Video 1 1.0 word(const.) 0 -
Rent-a-car 10 1.0 word(CM)-0.0 1.0 cpt(#pCM(avg))-0.8
 0
 5
 10
 15
 20
 25
 30
 35
 40
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 5: CER in video domain
 0
 5
 10
 15
 20
 25
 30
 50
 55
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 6: CER in rent-a-car domain
vealed that the CER could be improved compared to
the baseline by training optimal parameters with a
small amount of training data, which could be rea-
sonably prepared for new domains. This means that
our method is appropriate for rapidly prototyping
LU modules. Our method should help developers
of spoken dialogue systems in the early phases of
development. We intend to evaluate our method on
other domains, such as database searches and ques-
tion answering in future work.
Acknowledgments
We are grateful to Dr. Toshihiko Ito and Ms. Yuka
Nagano of Hokkaido University for constructing the
rent-a-car domain system.
References
Yulan He and Steve Young. 2005. Spoken Language
Understanding using the Hidden Vector State Model.Speech Communication, 48(3-4):262?275.
Lee Hetherington. 2004. The MIT finite-state trans-ducer toolkit for speech and language processing. InProc. 6th International Conference on Spoken Lan-guage Processing (INTERSPEECH-2004 ICSLP).
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawahara.
2004. Real-time word confidence scoring using lo-cal posterior probabilities on tree trellis search. InProc. 2004 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2004),volume 1, pages 793?796.
Alexandors Potamianos and Hong-Kwang J. Kuo. 2000.
Statistical recursive finite state machine parsingfor speech understanding. In Proc. 6th Interna-tional Conference on Spoken Language Processing(INTERSPEECH-2000 ICSLP), pages 510?513.
Stephanie Seneff. 1992. TINA: A natural language sys-tem for spoken language applications. ComputationalLinguistics, 18(1):61?86.
Katsuhito Sudoh and Hajime Tsukada. 2005. Tightly in-
tegrated spoken language understanding using word-to-concept translation. In Proc. 9th European Con-ference on Speech Communication and Technology(INTERSPEECH-2005 Eurospeech), pages 429?432.
Chai Wutiwiwatchai and Sadaoki Furui. 2004. Hybridstatistical and structural semantic modeling for Thaimulti-stage spoken language understanding. In Proc.HLT-NAACL Workshop on Spoken Language Under-standing for Conversational Systems and Higher LevelLinguistic Information for Speech Processing, pages
2?9.
216
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 314?321,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Ranking Help Message Candidates Based on Robust Grammar
Verification Results and Utterance History in Spoken Dialogue Systems
Kazunori Komatani Satoshi Ikeda Yuichiro Fukubayashi
Tetsuya Ogata Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jp
Abstract
We address an issue of out-of-grammar
(OOG) utterances in spoken dialogue sys-
tems by generating help messages for
novice users. Help generation for OOG
utterances is a challenging problem be-
cause language understanding (LU) re-
sults based on automatic speech recogni-
tion (ASR) results for such utterances are
always erroneous as important words are
often misrecognized or missed from such
utterances. We first develop grammar ver-
ification for OOG utterances on the ba-
sis of a Weighted Finite-State Transducer
(WFST). It robustly identifies a grammar
rule that a user intends to utter, even when
some important words are missed from the
ASR result. We then adopt a ranking algo-
rithm, RankBoost, whose features include
the grammar verification results and the
utterance history representing the user?s
experience.
1 Introduction
Studies on spoken dialogue systems have recently
proceeded from in-laboratory systems to ones de-
ployed to the open public (Raux et al, 2006; Ko-
matani et al, 2007; Nisimura et al, 2005). Ac-
cordingly, opportunities are increasing as general
citizens use the systems. This situation means
that novice users directly access the systems with
no instruction, which is quite different from in-
laboratory experiments where some instructions
can be given. In such cases, users often experi-
ence situations where their utterances are not cor-
rectly recognized. This is because of a gap be-
tween the actual system and a user?s mental model,
that is, a user?s expectation of the system. Ac-
tually, a user?s utterance often cannot be inter-
preted by the system because of the system?s lim-
ited grammar for language understanding (LU).
We call such an unacceptable utterance an ?out-
of-grammar (OOG) utterance.? When users? ut-
terances are OOG, they cannot change their ut-
terances into acceptable ones unless they are in-
formed what expressions are acceptable by the
system.
We aim to manage the problem of OOG utter-
ances by providing help messages showing an ex-
ample of acceptable language expressions when a
user utterance is not acceptable. We prepare help
messages corresponding to each grammar rule the
system has. We therefore assume that appropri-
ate help messages can be provided if a user?s in-
tention, i.e., a grammar rule the user originally
intends to use by his utterance, is correctly esti-
mated.
Issues for generating such help messages in-
clude:
1. Estimating a grammar rule corresponding to
user intention even from OOG utterances,
and
2. Complementing missing information in a sin-
gle utterance.
The first issue focuses on the fact that automatic
speech recognition (ASR) results, used as main in-
put data, are erroneous for OOG utterances. Es-
timating a grammar rule that the user intends to
use becomes accordingly difficult especially when
content words, which correspond to database en-
tries such as place names and their attributes, are
not correctly recognized. That is, any type of ASR
error in any position should be taken into consid-
eration in ASR results of OOG utterances. On the
314
other hand, the second issue focuses on the fact
that an ASR result for an OOG utterance does not
necessarily contain sufficient information to esti-
mate the user intention. This is because of ASR
errors or that users may omit some elements from
their utterances because they are in context.
We develop a grammar verification method
based on Weighted Finite-State Transducer
(WFST) as a solution to the first issue. The
grammar verification method robustly estimates
which a grammar rule is intended to use by a
user?s utterance. The WFST is automatically
generated to represent an ASR result in which any
possibility of error is taken into consideration. We
furthermore adopt a boosting algorithm, Rank-
Boost (Freund et al, 2003), to put help messages
in order of probability to address the second issue.
Because it is difficult even for human annotators
to uniquely determine which help message should
be provided for each case, we adopt an algorithm
that can be used for training on several data
examples that have a certain order of priority.
We also incorporate features representing the
user?s utterance history for preventing message
repetition.
2 Related Work
Various studies have been done on generating help
messages in spoken dialogue systems. Gorrell et
al. (2002) trained a decision tree to classify causes
of errors for OOG utterances. Hockey et al (2003)
also classified OOG utterances into the three cate-
gories of endpointing errors, unknown vocabulary,
and subcategorization mistakes, by comparing two
kinds of ASR results. This was called Targeted
Help and provided a user with immediate feedback
tailored to what the user said. Lee et al (2007) also
addressed error recovery by generating help mes-
sages in an example-based dialog modeling frame-
work. These studies, however, determined what
help messages should be provided mainly on the
basis of literal ASR results. Therefore, help mes-
sages would be degraded by ASR results in which
a lot of information was missing, especially for
OOG utterances. The same help messages would
be repeated when the same ASR results were ob-
tained.
An example dialogue enabled by our method,
especially the part of the method described in Sec-
tion 4, is shown in Figure 1. Here, user utter-
ances are transcriptions, and utterance numbers
U1: Tell me your recommending sites.
Underlined parts are not in-vocabulary and no
valid LU result is obtained. The estimated gram-
mar is [Obtaining info on a site] although the most
appropriate help message is that corresponding to
[Searching tourist sites].
S1: I did not understand. You can say ?Tell me
the address of Kiyomizu Temple? for example,
if getting information on a site.
The help message corresponding to [Obtaining info
on a site] is provided.
U2: Tell me your recommending sites.
The user repeats the same utterance probably be-
cause the help message (S1) was not helpful. The
estimated grammar is [Obtaining info on a site]
again.
S2: I did not understand. You can say ?Search
shrines or museums? for example, if searching
tourist sites.
Another help message corresponding to [Searching
tourist sites] is provided after ranking candidates
by also using the user?s utterance history.
[] denotes grammar rules.
Figure 1: Example dialogue enabled by our
method
start with ?S? and ?U? denote system and user
utterances, respectively. In this example, ASR
results for the user utterances (U1 and U2) do
not contain sufficient information because the ut-
terances are short and contain out-of-vocabulary
words. These two results are similar, and ac-
cordingly, the help message after U2 provided by
methods like Targeted Help (Gorrell et al, 2002;
Hockey et al, 2003) is the same as Utterance S1
because they are only based on ASR results. Our
method can provide different help messages as Ut-
terance S2 after ranking candidates by consider-
ing the utterance history and grammar verification
results. Because the candidates are arranged in
the order of probability, the most appropriate help
message can be provided in fewer attempts.
This ranking method for help message candi-
dates is also useful in multimodal interfaces with
speech input. Help messages are necessary when
ASR is used as its input modality, and such mes-
sages were actually implemented in City Browser
(Gruenstein and Seneff, 2007), for example. This
system lists template-based help messages on the
screen by using ASR results and internal states of
the system. The order of help messages is impor-
tant, especially in portable devices with a small
screen, on which the number of help messages dis-
315
played at one time is limited, as Hartmann and
Schreiber (2008) pointed out. Even in cases where
sufficiently large screens are available, too many
help messages without any order will distract the
user?s attention and thus spoil its usability.
3 Grammar Verification based on WFST
We estimate a user?s intention even from OOG ut-
terances as a grammar rule that the user intends
to use by his utterance. We call this estimation
grammar verification. This process is applied to
ASR outputs based on a statistical language model
(LM) in this paper. We use two transducers: a
finite-state transducer (FST) representing the task
grammar, and weighted FST (WFST) representing
an ASR result and its confidence score. Hereafter,
we denote these two as ?grammar FST? and ?input
WFST? and depict examples in Figure 2.
A strong point of our method is that it takes
all three types of ASR error into consideration.
The input WFST is designed to represent all cases
where any word in an ASR result is an inserted or
substituted error, or any word is deleted. Its weight
is designed to reflect confidence scores of ASR re-
sults. By composing this WFST and the gram-
mar FST, we can obtain all possible sequences
and their accumulated weights when arbitrary se-
quences represented by the input WFST are input
into the grammar FST. The optimal results having
the maximum accumulated weight consist of the
LU result and the grammar rule that is the nearest
to the ASR result. The result can be obtained even
when any element in it is misrecognized or absent
from the ASR result.
An LU result is a set of concepts that consist
of slots and their values corresponding to database
entries the system handles. For example, an LU
result ?month=2, day=22? consists of two con-
cepts, such as the value of slotmonth is 2, and the
value of slot day is 22.
3.1 Design of input WFST and grammar FST
In input WFSTs and grammar FSTs, each arc rep-
resenting state transitions has a label in the form of
?a:b/c? denoting its input symbol, output symbol,
and weight, in this order. Input symbol ? means a
state transition without any input symbol, that is,
an epsilon transition. Output symbol ? means no
output in the state transition. For example, a state
transition ?please:?/1.0? is executed when an in-
put symbol is ?please,? no output symbol is gen-
erated, and 1.0 is added to the accumulated weight.
Weights are omitted in the grammar FST because
no weight is given in it.
An input WFST is automatically constructed
from an ASR result. Sequential state transitions
are assigned to each word in the ASR result, and
each of them is paralleled by filler transitions, as
shown in Figure 2 where the ASR result was ?Ev-
ery Monday please? for example. Filler transitions
such as INS, DEL, and SUB are assigned to each
state for representing every kind of error such as
insertion, deletion, and substitution errors. All in-
put symbols in the input WFST are ?, by which the
WFST represents all possible sequences contain-
ing arbitrary errors. For example, the input WFST
in Figure 2 represents all possible sequences such
as ?Every Monday please,? ?Every Monday F,? ?F
Monday F,? and so on. Here, every word can be
replaced by the symbol F that represents an inser-
tion or substitution error. Moreover, the error sym-
bol DEL can be inserted into its output symbol se-
quence at any position, which corresponds to dele-
tion errors in ASR results. Each weight per state
transition is summed up and then the optimal re-
sult is determined. The weights will be explained
in Section 3.2.
A grammar FST is generated from a task gram-
mar, which is written by a system developer for
each task. It determines whether an input se-
quence conforms to the task grammar. We also
assign filler transitions to each state for handling
each type of error of ASR results considered in
the input WFST. A filler transition, either of INS,
DEL, or SUB, is added to each state in the FST
except for states within keyphrases, which are ex-
plicitly indicated by a system developer. In the
example shown in Figure 2, ?SUB $ Monday
date-repeat=Mon please? is output for an input
sequence ?SUB Monday please?. Here, date-
repeat=Mon denotes an LU result, and $ is a sym-
bol for marking words corresponding to a concept.
3.2 Weights assigned to input WFST
We defined two kinds of weights:
1. Rewards for accepted words (wacc), and
2. Penalties for each kind of error (wsub, wdel,
wins).
An accumulated weight for a single utterance is
defined as the sum of these weights as shown be-
316
Input WFST
Every:
Every
Monday:
Monday
please:
please
Grammar FST
input:output/weight
ASR result: ?Every Monday please?
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !!
  !"#$%& :
 !!
  !"#$"%& :
 !!
  !"#$%&' :
 !"# !"
 !"# !"
 !"# !"
$:?
repeat-date:?
Mon=
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"# !"  !"# !"
 !"# !"
 !"# !"
 !"# !"
Figure 2: Example of input WFST and grammar FST
low.
w =
?
Eaccepted
wacc +
?
Eerror
(wsub + wdel + wins)
Here, Eaccepted denotes a set of accepted words
corresponding to elements of each grammar rule,
and Eerror denotes a set of words that are not ac-
cepted and that have either error symbol. Note that
the weights are not given beforehand but are cal-
culated and given to the input WFST in runtime
according to each ASR result.
A weight for an accepted word easr is defined
by using its confidence score CM(easr) (Lee et
al., 2004) and its word length. A word length in
mora is denoted as l(?), which is normalized by
that of the longest word in the vocabulary.
wacc = CM(easr)l(easr)
This weight wacc gives preference to sequences
containing longer words with higher confidence
scores.
Weights for each type of error have negative val-
ues because they are penalties:
wsub = ?{CM(easr)l(easr) + l(egram)}/2
wdel = ?{l(e) + l(egram)}/2
wins = ?{CM(easr)l(easr) + l(e)}/2
where l(e) is the average word length in the vocab-
ulary and egram is a grammar element i.e., either a
word or a class. A deletion error is a case when a
grammar element does not correspond to any word
in the ASR result. A substitution error is a case
when an element is replaced by another word in
the ASR result. An insertion error is a case when
no grammar element corresponds to the ASR re-
sult. Every weight is defined as an average of a
word length of a grammar element and the corre-
sponding one in the ASR result multiplied by its
confidence score. When correspondences cannot
be defined in insertion and deletion errors, l(e) is
used instead. In the case when egram is a class in
the grammar, the average word length in that class
is used as l(egram).
3.3 Example of calculating the weights
We show how a weight is calculated by using the
example in Figure 3. In this example, the user ut-
terance was ?Tell me a liaison of Koetsu-ji (a tem-
ple name).? The word ?liaison? was not in the sys-
tem vocabulary. The ASR result accordingly con-
tained errors for that part; the result was ?Tell me
all Sakyo-ward Koetsu-ji.?
Weights are calculated for each grammar rule
the system has. This example shows calcula-
tions for two grammar rules: [get info] accept-
ing ?Tell me ?item name? of ?temple name?,? and
[search ward] accepting ?Tell me ?facility name?
of ?ward name?.? Here, [] and ?? denote a gram-
mar rule and a class in grammars. Two alignment
results are also shown for grammar [get info] in
this example. Weights are calculated for any align-
ment as shown here, and the alignment result with
the largest weight is selected. In this example,
weight +0.16 for the grammar [get info] was the
largest.
We consequently obtained the result that gram-
mar rule [get info] had the highest score for this
OOG utterance and its accumulated weight was
317
User utterance: ?Tell me a liaison of Koetsu-ji?. (Underlined parts denote OOG.)
ASR result tell me all Sakyo-ward Koetsu-ji
(ward) (temple)
grammar [get info] tell me ?item name? of ?temple name?
WFST output tell me INS SUB DEL Koetsu-ji
weights +0.09 +0.06 ?0.04 ?0.11 ?0.02 +0.18 +0.16
grammar [get info] tell me ?item name ? of ?temple name?
WFST output tell me SUB SUB Koetsu-ji
weights +0.09 +0.06 ?0.21 ?0.10 +0.18 +0.02
grammar [search ward] tell me ?facility type? in ?ward name?
WFST output tell me INS SUB DEL SUB
weights +0.09 +0.06 ?0.04 ?0.12 ?0.02 ?0.21 ?0.24
Figure 3: Example of calculating weights in our grammar verification
+0.16. The result also indicated each type of er-
ror as a result of the alignment: ?item name? was
substituted by ?Sakyo-ward?, ?of? in the grammar
[get info] was deleted, and ?all? in the ASR result
was inserted.
4 Ranking Help Message Candidates by
Integrating Dialogue Context
We furthermore develop a method to rank help
message candidates per grammar rule by integrat-
ing the grammar verification result and the user?s
utterance history. This complements information
that is often absent from utterances or misrecog-
nized in ASR and prevents that the same help mes-
sages are repeated. An outline of the method is
depicted in Figure 4.
4.1 Features used in Ranking
Features used in our methods are listed in Table
1. These features are calculated for each help
message candidate corresponding to each gram-
mar rule. Features H1 to H5 represent how reli-
able a grammar verification result is. Feature H1 is
a grammar verification score, that is, the resulting
accumulated weight described in Section 3. Fea-
ture H2 is calculated by normalizing H1 by the
total score of all grammar rules. This represents
how reliable the grammar verification result is rel-
atively compared to others. Features H3 to H5
represent how partially the user utterance matches
with the grammar rule.
Features H6 and H7 correspond to a dialogue
context. Feature H6 reflects the case in which
users tend to repeat similar utterances when their
utterances were not understood by the system.
Feature H7 represents whether and how the user
knows about the language expression of the gram-
mar rule. This feature corresponds to the known
degree we previously proposed (Fukubayashi et
Table 1: Features of each instance (help message
candidate)
H1: accumulated weight of GV (GV score)
H2: GV score normalized by the total GV score of other
instances
H3: ratio of # of accepted words in GV result to # of all
words
H4: maximum number of successively accepted words
in GV result
H5: number of accepted slots in GV result
H6: how before the grammar rule was selected as GV
result (in # of utterances)
H7: maximum GV score for the grammar rule until then
H8: whether it belongs to the ?command? class
H9: whether it belongs to the ?query? class
H10: whether it belongs to the ?request-info? class
H11-H17: products of H8 and each of H1 to H7
H18-H24: products of H9 and each of H1 to H7
H25-H31: products of H10 and each of H1 to H7
GV: grammar verification
al., 2006), and prevents a help message the user
already knows from being provided repeatedly.
Features H8 to H10 represent properties of
utterances corresponding to the grammar rules,
which are categorized into three classes such as
?command,? ?query,? and ?request-info.? In the
sightseeing task, the numbers of grammar rules for
the three classes were 8, 4, and 11, respectively.
More specifically, utterances in either ?query? or
?request-info? class tend to appear successively
because they are used when users try and com-
pare several query conditions; on the other hand,
utterances in ?command? class tend to appear in-
dependently of the context. Features H11 to H31
are the products of features H8, H9, and H10 and
each feature from H1 to H7. These were defined to
consider combinations of properties of utterances
represented by H8, H9, and H10 and their reliabil-
ity represented by H1 to H7, because RankBoost
318
Help candidate 
Help candidate
Ranking
(RankBoost)
?
=
T
t
tt
xhxH )()( ?
1
x
LL )()(
111
xfxf
i
LL )()(
1 nin
xfxf
n
x
User
utterance
Context
deft
qi,,,??
Parameters
Training 
data
p
x
q
x
Grammar
verification
Calculating features
Sorted by H(x)
Statistical LM-based
ASR outputs
Figure 4: Outline of our ranking method for help message candidates
does not consider them.
4.2 Ranking Algorithm
We adopt RankBoost (Freund et al, 2003), a
boosting algorithm based on machine learning, to
rank help message candidates. This algorithm can
be used for training on several data examples hav-
ing a certain order of priority. This attribute fits
for the problem in this paper; it is difficult even
for human annotators to determine the unique ap-
propriate help message to be provided. Target in-
stances x of the algorithm are help message can-
didates corresponding to grammar rules in this pa-
per.
RankBoost trains a score function H(x) and ar-
ranges instances x in the order. Here, H(x?) <
H(x??) means x?? is ranked higher than x?. This
score function is defined as a linear combination
of weak rankers giving partial information regard-
ing the order:
H(x) =
T
?
t
?tht(x)
where T , ht(), and ?t denote the number of boost-
ing iterations, a weak ranker, and its associated
weight, respectively. The weak ranker ht is de-
fined by comparing the value of a feature fi of an
instance x with a threshold ?. That is,
ht(x) =
?
?
?
?
?
1 if fi(x) > ?
0 if fi(x) ? ?
qdef if fi(x) = ?
(1)
where qdef ? {0, 1}. Here, fi(x) denotes the
value of the i-th feature of instance x, and ? de-
notes that no value is given in fi(x).
5 Experimental Evaluation
5.1 Target Data
Data were collected by 30 subjects in total by us-
ing a multi-domain spoken dialogue system that
handles five domains such as restaurant, hotel,
sightseeing, bus, and weather (Komatani et al,
2008). The data consisted of 180 dialogues and
11,733 utterances. Data from five subjects were
used to determine the number of boosting iter-
ations and to improve LMs for ASR. We used
utterances in the restaurant, hotel, and sightsee-
ing domains because the remaining two, bus and
weather, did not have many grammar rules. We
then extracted OOG utterances on the basis of the
grammar verification results to evaluate the per-
formance of our method for such utterances. We
regarded an utterance whose accumulated weight
was negative as OOG. As a result, 1,349 OOG ut-
terances by 25 subjects were used for evaluation,
hereafter. These consisted of 363 utterances in the
restaurant domain, 563 in the hotel domain, and
423 in the sightseeing domain. These data were
collected under the following conditions: subjects
were given no instructions on concrete language
expressions the system accepts. System responses
were made only by speech, and no screen for dis-
playing outputs was used. Subjects were given six
scenarios describing tasks to be completed.
We used Julius1 that is a statistical-LM-based
ASR engine. We constructed class 3-gram LMs
for ASR by using 10,000 sentences generated
from the task grammars and the 600 utterances
collected by the five subjects. The vocabulary
sizes for the restaurant, hotel, and sightseeing do-
mains were 3,456, 2,625, and 3,593, and ASR ac-
curacies for them were 45.8%, 57.1%, and 43.5%,
respectively. These ASR accuracies were not very
high because the target utterances were all OOG.
A set of possible thresholds in the weak rankers
described in Section 4.2 consisted of all feature
values that appeared in the training data. The num-
bers of boosting iterations were determined on the
basis of accuracies for the data by the five sub-
1http://julius.sourceforge.jp/
319
!"#
$"#
%"#
&"#
'"#
("#
)"#
*""#
*+,-./ 0+,-./ !+,-./ $+,-./ %+,-./
1+,-./
!
"
"
#
$
%
"
&
!"#$%&'$ ()*+,$-.(/
Figure 5: Accuracy when N candidates were pro-
vided in sightseeing domain (1 ? N ? 5)
jects. The numbers were 400, 100, and 500 for the
restaurant, hotel, and sightseeing domains.
5.2 Evaluation Criterion
We manually gave five help messages correspond-
ing to grammar rules as reference labels per ut-
terance in the order of having a strong relation to
the utterance. The numbers of candidate help mes-
sages were 28, 27, and 23 for the restaurant, hotel
and sightseeing domains, respectively.
We evaluated our ranking method as the accu-
racy where at least one of the reference labels was
contained in its top N candidates. This corre-
sponds to a probability where at least one appro-
priate help message was contained in a list of N
candidates. The accuracy was calculated by 5-fold
cross validation. In the baseline method we set,
help messages were provided only by using the
grammar verification scores.
5.3 Results
Results in the sightseeing domain are plotted in
Figure 5. We can see that our method outper-
formed the baseline in the accuracies for all N
values. All these differences were statistically sig-
nificant (p < 0.05) by the McNemar test. The ac-
curacies were also better in the other two domains
for all N values, and the average differences for
the three domains were 11.7 points for N=1, 9.7
points for N=2, and 6.7 points for N=3. The dif-
ferences were large especially for small N values.
This result indicates that we can successfully re-
duce the number of help messages when providing
several ones for users. The improvements were
derived from the features we incorporated such as
the estimated user knowledge in addition to gram-
mar verification results. The baseline method was
only based on grammar verification results for sin-
gle utterances, which contained insufficient infor-
mation because OOG utterances were often mis-
recognized or misunderstood.
Table 2: Sum of absolute values of weight ? for
each feature
H7 H17 H19 H2 H6
(H7*H8) (H2*H9)
9.58 6.91 6.61 6.02 6.01
We also investigated dominant features by cal-
culating the sum of absolute values of final weight
? for each feature in RankBoost. Five dominant
features based on the sums are shown in Table
2. These five features include a feature obtained
from grammar verification result (H2), a feature
about the user?s utterance history (H6), a feature
representing estimated user knowledge (H7), and
features representing properties of the utterances.
The most dominant feature was H7, which ap-
peared twice in this table. This was because user
utterances were not likely to be OOG utterances
again after the user had already known an expres-
sion corresponding to the grammar rule, which can
be detected when user utterances for it were cor-
rectly accepted, that is, its grammar verification
score was high. The second dominant feature was
H2, which showed that grammar verification re-
sults worked effectively.
6 Conclusion
We addressed an issue of OOG utterances in spo-
ken dialogue systems by generating help mes-
sages. To manage situations when a user utter-
ance could not be accepted, we robustly estimated
a user?s intention as a grammar rule that the user
intends to use. We furthermore integrated various
information as well as the grammar verification
results for complementing missing information in
single utterances, and then ranked help message
candidates corresponding to the grammar rules for
efficiently providing them.
Our future work includes the following. The
evaluation in this paper was taken place only on
the basis of utterances collected beforehand. Pro-
viding help messages itself should be evaluated by
another experiment through dialogues. Further-
more, we assumed that language expressions of
help messages to show an example language ex-
pression were fixed. We also need to investigate
what kind of expression is more helpful to novice
users.
320
References
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and
Yoram Singer. 2003. An efficient boosting algo-
rithm for combining preferences. Journal of Ma-
chine Learning Research, 4:933?969.
Yuichiro Fukubayashi, Kazunori Komatani, Tetsuya
Ogata, and Hiroshi G. Okuno. 2006. Dynamic
help generation by estimating user?s mental model in
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (INTERSPEECH), pages
1946?1949.
Genevieve Gorrell, Ian Lewin, and Manny Rayner.
2002. Adding intelligent help to mixed-initiative
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (ICSLP), pages 2065?
2068.
Alexander Gruenstein and Stephanie Seneff. 2007.
Releasing a multimodal dialogue system into the
wild: User support mechanisms. In Proc. 8th SIG-
dial Workshop on Discourse and Dialogue, pages
111?119.
Melanie Hartmann and Daniel Schreiber. 2008. Proac-
tively adapting interfaces to individual users for mo-
bile devices. In Adaptive Hypermedia and Adap-
tive Web-Based Systems, 5th International Confer-
ence (AH 2008), volume 5149 of Lecture Notes in
Computer Science, pages 300?303. Springer.
Beth A. Hockey, Oliver Lemon, Ellen Campana, Laura
Hiatt, Gregory Aist, James Hieronymus, Alexander
Gruenstein, and John Dowding. 2003. Targeted
help for spoken dialogue systems: intelligent feed-
back improves naive users? performance. In Proc.
10th Conf. of the European Chapter of the ACL
(EACL2003), pages 147?154.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of real
user?s behaviors in a spoken dialogue system. In
Proc. INTERSPEECH, pages 142?145.
Kazunori Komatani, Satoshi Ikeda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2008. Managing out-of-
grammar utterances by topic estimation with domain
extensibility in multi-domain spoken dialogue sys-
tems. Speech Communication, 50(10):863?870.
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawa-
hara. 2004. Real-time word confidence scoring us-
ing local posterior probabilities on tree trellis search.
In IEEE Int?l Conf. Acoust., Speech & Signal Pro-
cessing (ICASSP), volume 1, pages 793?796.
Cheongjae Lee, Sangkeun Jung, Donghyeon Lee, and
Gary Guenbae Lee. 2007. Example-based error re-
covery strategy for spoken dialog system. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU), pages 538?543.
Ryuichi Nisimura, Akinobu Lee, Masashi Yamada, and
Kiyohiro Shikano. 2005. Operating a public spo-
ken guidance system in real environment. In Proc.
European Conf. Speech Commun. & Tech. (EU-
ROSPEECH), pages 845?848.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
Let?s Go! experience. In Proc. INTERSPEECH.
321
