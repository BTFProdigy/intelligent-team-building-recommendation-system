Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 109?120, Dublin, Ireland, August 23-29 2014.
Query-by-Example Image Retrieval
using Visual Dependency Representations
Desmond Elliott, Victor Lavrenko and Frank Keller
Institute of Language, Communication, and Computation
School of Informatics
University of Edinburgh
d.elliott@ed.ac.uk {vlavrenk,keller}@inf.ed.ac.uk
Abstract
Image retrieval models typically represent images as bags-of-terms, a representation that is well-
suited to matching images based on the presence or absence of terms. For some information
needs, such as searching for images of people performing actions, it may be useful to retain data
about how parts of an image relate to each other. If the underlying representation of an image
can distinguish between images where objects only co-occur from images where people are in-
teracting with objects, then it should be possible to improve retrieval performance. In this paper
we model the spatial relationships between image regions using Visual Dependency Represen-
tations, a structured image representation that makes it possible to distinguish between object
co-occurrence and interaction. In a query-by-example image retrieval experiment on data set
of people performing actions, we find an 8.8% relative increase in MAP and an 8.6% relative
increase in Precision@10 when images are represented using the Visual Dependency Represen-
tation compared to a bag-of-terms baseline.
1 Introduction
Every day millions of people search for images on the web, both professionally and for personal amuse-
ment. The majority of image searches are aimed at finding a particular named entity, such as Justin
Bieber or supernova, and a typical image retrieval system is well-suited to this type of information need
because it represents an image as a bag-of-terms drawn from data surrounding the image, such as text,
manual tags, and anchor text (Datta et al., 2008). It is not always possible to find useful terms in the sur-
rounding data; the last decade has seen advances in automatic methods for assigning terms to images that
have neither user-assigned tags, nor a textual description (Duygulu et al., 2002; Lavrenko et al., 2003;
Guillaumin and Mensink, 2009). These automatic methods learn to associate the presence and absence
of labels with the visual characteristics of an image, such as colour and texture distributions, shape, and
points of interest, and can automatically generate a bag of terms for an unlabelled image.
It is important to remember that not all information needs are entity-based: people also search for im-
ages reflecting a mood, such as people having fun at a party, or an action, such as using a computer. The
bag-of-terms representation is limited to matching images based on the presence or absence of terms,
and not the relation of the terms to each other. Figures 1(a) and (b) highlight the problem with using
unstructured representations for image retrieval: there is a person and a computer in both images but only
(a) depicts a person actually using the computer. To address this problem with unstructured represen-
tations we propose to represent the structure of an image using the Visual Dependency Representation
(Elliott and Keller, 2013). The Visual Dependency Representation is a directed labelled graph over the
regions of an image that captures the spatial relationships between regions. The representation is inspired
by evidence from the psychology literature that people are better at recognising and searching for objects
when the spatial relationships between the objects in the image are consistent with our expectations of
the world.(Biederman, 1972; Bar and Ullman, 1996). In an automatic image description task, Elliott
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://
creativecommons.org/licenses/by/4.0/
109
usingcomputer
ROOT Lamp Picture Girl Laptop Bed
beside
above
(a)
playinginstrument
ROOT Table Laptop Man Trumpet Boy
on beside
beside
(b)
using computer
ROOT Sofa Man Laptop Chair
beside
on
(c)
Figure 1: Three examples of images depicting a person and a computer, alongside a respective Visual
Dependency Representation for each image. The bag-of-terms representation can be observed in the
annotated regions of the Visual Dependency Representations. In (a) and (c) there is a person using a
laptop, whereas in (b) the man is actually using the trumpet. The gold-standard action annotation is
shown in the yellow bounding box.
and Keller (2013) showed that encoding the spatial relationships between objects in the Visual Depen-
dency Representation helped to generate significantly better descriptions than approaches based on the
spatial proximity of objects (Farhadi et al., 2010) or corpus-based models (Yang et al., 2011). In this
paper we study whether the Visual Dependency Representation of images can improve the performance
of query-by-example image retrieval models. The main finding is that encoding images using the Visual
Dependency Representation leads to significantly better retrieval accuracy compared to a bag-of-terms
baseline, and that the improvements are most pronounced for transitive verbs.
2 Related Work
2.1 Representing Images
A central problem in image retrieval is how to abstractly represent images (Datta et al., 2008). A bag-
of-terms representation of an image is created by grouping visual features, such as color, shape (Shi
and Malik, 2000), texture, and interest points (Lowe, 1999), in a vector or as a probability distribution
over the features. Image retrieval can then be performed by trying to find the best matchings of terms
across an image collection. Spatial Pyramid Matching is an approach to constructing low-level image
representations that capture the relationships between features at differently sized partitions of the im-
age (Lazebnik et al., 2006). This approach has proven successful for scene categorisation tasks. An
alternative approach to representing images is to learn a mapping (Duygulu et al., 2002; Lavrenko et al.,
110
2003; Guillaumin and Mensink, 2009) between the bags-of-terms and object tags. An image can then be
represented as a bag-of-terms and image retrieval is similar to text retrieval (Wu et al., 2012).
In this work, we represent an image as a directed acyclic graph over a set of labeled object region
annotations. This representation captures the important spatial relationships between the image regions
and makes it possible to distinguish between co-occurring regions and interacting regions.
2.2 Still-Image Action Recognition
One approach to recognizing actions is to learn appearance models for visual phrases and use these
models to predict actions (Sadeghi and Farhadi, 2011). A visual phrase is defined as the people and the
objects they interact with in an action. In this approach, a fixed number of visual phrase models are
trained using the deformable parts object detector (Felzenszwalb et al., 2010) and used to perform action
recognition.
An alternative approach is to model the relationships between objects in an image, and hence the
visible actions, as a Conditional Random Field (CRF), where each node in the field is an object and the
factors between nodes correspond to features that capture the relationships between the objects (Zitnick
et al., 2013). The factors between object nodes in the CRF include object occurrence, absolute position,
person attributes, and the relative location of pairs of objects. This model has been used to generate novel
images of people performing actions and to retrieve images of people performing actions.
Most recently, actions have been predicted in images by selecting the most likely verb and object pair
given a set of candidate objects detected in an image (Le et al., 2013a). The verb and object is selected
amongst those that maximize the distributional similarity of the pair in a large and diverse collection of
documents. This approach is most similar to ours but it relies on an external corpus and, depending on
the text collections used to train the distributional model, will compound the problem of co-occurrence
of objects instead of the relationships between the objects.
The work presented in this paper uses ground-truth annotation for region labels, an assumption similar
to (Zitnick et al., 2013), but requires no external data to make predictions of the relationships between
objects, unlike the approach of (Le et al., 2013a). The directed acyclic graph representation we propose
for images can be seen as a latent representation of the depicted action in the image, where the spatial
relationships between the regions capture the different types of actions.
3 Task and Baseline
In this paper we study the task of query-by-example image retrieval within the restricted domain of
images depicting actions. More specifically, given an image that depicts a given action, such as using a
computer, the aim of the retrieval model is to find all other images in the image collection that depict the
same action. We define an action as an event involving one or more entities in an image, e.g., a woman
running or boy using a computer, and assume all images have been manually annotated for objects. This
assumption means we can explore the utility of the Visual Dependency Representation without the noise
introduced by automatic computer vision methods. The data available to the retrieval models can be seen
in Figure 1, and Section 5 provides further details about the different sources of data The action label -
which is only used for evaluation - is shown in the labelled bounding box, and the Visual Dependency
Representation - not used by the baseline model - is shown as a tree at the bottom of the figure.
The main hypothesis explored in this paper is that the accuracy of an image retrieval model will
increase if the representation encodes information about the relationships between the objects in images.
This hypothesis is tested by encoding images as either an unstructured bag-of-terms representation or
as the structured Visual Dependency Representation. The Bag-of-Terms baseline represents the query
image and the image collection as an unstructured bags-of-terms vector. All of the models used to test
the main hypothesis use the cosine similarity function is to determine the similarity of the query image
to other images in the collection, and thus to generate a ranked list from the similarity values.
111
4 Visual Dependency Representation
The Visual Dependency Representation (VDR) is a structured representation of an image that captures the
spatial relationships between pairs of image regions in a directed labelled graph. The Visual Dependency
Grammar defines eight possible spatial relationships between pairs of regions, as shown in Table 1.
The relationships in the grammar were designed to provide sufficient coverage of the types of spatial
relationships required to describe the data, and are mathematically defined in terms of pixel overlap,
distance between regions, and the angle between regions. The frame of reference for annotating spatial
relationships is the image itself and not the object in the image, and angles and distance measurements
are taken or estimated from the centroids of the regions. The VDR of an image is created by a trained
human annotator in a two-stage process:
1. The annotator draws and labels boundaries around the parts of the image they think contribute to
defining the action depicted in the image, and the context within which the action occurs;
2. The annotator draws labelled directed edges between the annotated regions that captures how the
relationships between the image convey the action. In Section 4.1, we will explain how to automate
the second stage of the process from a collection of labelled region annotations.
In addition to the annotated image regions, a VDR also contains a ROOT node, which acts as a place-
holder for the image. In the remainder of this section we describe how a gold-standard VDR is created
by a human annotator. The starting point for the VDR in Figure 1(a) is the following set of regions and
the ROOT node:
ROOT Lamp Picture Girl Laptop Bed
First, the regions are attached to each other based on how the relationship between the objects con-
tributes to the depicted action. In Figure 1(a), the Girl is using the Laptop, therefore a labelled directed
edge is created from the Girl region to the Laptop region. The spatial relationship is labelled as BESIDE.
ROOT Lamp Picture Girl Laptop Bed
beside
The Girl is also attached to the Bed because the bed supports her body. The spatial relation label is
ABOVE because it expresses the spatial relationship between the regions, not the semantic relationship
ON. ROOT is attached to the Girl without an edge label to symbolize that she is an actor in the image.
ROOT Lamp Picture Girl Laptop Bed
beside
above
Now the regions that are not concerned with the depicted action are first attached to each other if there
is a clear spatial relationship between them (for an example, see Figure 1(b), where the laptop is attached
to the table because it is sitting on the table), and then to the ROOT node to signify that they do not play
a part in the depicted action. In this example, neither the Lamp nor the Picture are related to the action
of using the computer, so they are attached to the ROOT node.
112
X??
on Y
More than 50% of the pix-
els of region X overlap
with region Y.
X
????
beside Y
The angle between the cen-
troid of X and the centroid
of Y lies between 315
?
and
45
?
or 135
?
and 225
?
.
X
????
above Y
The angle between X and
Y lies between 225
?
and
315
?
.
X
?????
infront Y
The Z-plane relationship
between the regions is
dominant.
X
????????
surrounds Y
The entirety of region X
overlaps with region Y.
X
??????
opposite Y
Similar to beside, but
used when there X and
Y are at opposite sides
of the image.
X
????
below Y
The angle between X
and Y lies between 45
?
and 135
?
.
X
?????
behind Y
Identical to infront ex-
cept X is behind Y in the
Z-plane.
Table 1: Visual Dependency Grammar defines eight relations between pairs of annotated regions. To
simplify explanation, all regions are circles, where X is the grey region and Y is the white region. All
relations are considered with respect to the centroid of a region and the angle between those centroids.
ROOT Lamp Picture Girl Laptop Bed
beside
above
This now forms a completed VDR for the image in Figure 1(a). This structured representation of
an image captures the prominent relationship between the girl, the laptop, and the bed. There is no
prominent relationship defined between the girl and either the lamp of the picture, in effect these regions
have been relegated to background objects. The central hypothesis underpinning the Visual Dependency
Representation is that images that contain similar VDR substructures are more likely to depict the same
action than images that only contain the same set of objects. For example, the VDR for Figure 1(a)
correctly captures the relationship between the people and the laptops, whereas this relationship is not
present in Figure 1(b), where the person is playing a trumpet.
4.1 Predicting Visual Dependency Representations
We follow the approach of Elliott and Keller (2013) and predict the VDR y of an image over a collection
of labelled region annotations x. This task is framed as a supervised learning problem, where the aim is
to construct a Maximum Spanning Tree from a fully-connected directed weighted graph over the labelled
regions (McDonald et al., 2005). Reducing the fully-connected graph to the Maximum Spanning Tree
removes the region?region edges that are not important in defining the prominent relationships between
the regions in an image. The score of the VDR y over the image regions is calculated as the sum of the
scores of the directed labelled edges:
score(x, y) =
?
(a,b)?y
w ? f(a, b) (1)
where the score of an edge between image regions a and b is calculated using a vector of weighted feature
functions f . The feature functions characterize the image regions and the edge between pairs of regions,
and include: the labels of the regions and the spatial relation annotated on the edge; the (normalized)
distance between the centroids of the regions; the angle formed between the annotated regions, which is
113
mapped onto the set of spatial relations; the relative size of the region compared to the image; and the
distance of the region centroid from the center of the image.
The model is trained over i instances of region-annotated images x
i
associated with human-created
VDR structures y
i
, I
train
= {x
i
, y
i
}. The score of each edge a, b is calculated by applying the feature
functions to the data associated with that edge, and this is performed over each edge in a VDR to obtain
a score for a complete gold-standard structure. The parameters of the weight vector w are iteratively
adjusted to maximise the score of the gold-standard structures in the training data using the Margin
Infused Relaxation Algorithm (Crammer and Singer, 2002).
The test data contains i instances of region-annotated images with image regions x
i
, I
test
= {x
i
}.
The parsing model computes the highest scoring structure y?
i
for each instance in the test data by scoring
each possible directed edge between pairs of regions in x
i
. This process forms a fully-connected graph
over the image regions, from which the Maximum Spanning Tree is taken and returned as the predicted
VDR.
We evaluate the performance of this VDR prediction model by comparing how well it can recover
the manually created trees in the data set. This evaluation is performed on the development data in a
10-fold cross validation setting where each fold of the data is split 80%/10%/10%. Unlabelled directed
accuracy means the model correctly proposes an edge between a pair of regions in the correct direction;
Labelled directed accuracy means it additionally proposes the correct edge label. The baseline approach
is to assume no latent image structure and attach all image regions to the ROOT node of the VDR; this
achieves 51.6% labelled and unlabelled directed attachment accuracy. The accuracy of our automatic
approach to VDR prediction is 61.3% labelled and 68.8% unlabelled attachment accuracy.
4.2 Comparing Visual Dependency Representations
It remains to define how to compare the Visual Dependency Representation of a pair of images. The most
obvious approach is to use the labelled directed accuracy measurement used for the VDR prediction
evaluation in the previous section, but we did not find significant improvements in retrieval accuracy
using this method. We hypothesise that the lack of weight given to the edges between nodes in the Visual
Dependency Representation results in this comparison function not distinguishing between object?object
relationships that matter, such as PERSON
?????
beside BIKE, compared to ROOT ?? TREES. The former
is a potential person?object relationship that explains the depicted event, whereas the latter is only a
background object.
The approach we adopted in this paper is to compare Visual Dependency Representations of images
by decomposing the structure into a set of labelled and a unlabelled parent?child subtrees in a depth-first
traversal of the VDR. The decomposition process allows use to use the same similarity function as the
Bag-of-Terms baseline model, removing the confound of choosing different similarity functions. The
subtrees can be transformed into tokens and these tokens can be used as weighted terms in a vector
representation. An example of a labelled transformation is shown below:
Girl Bed ? Girl above Bed
above
We now demonstrate the outcome of comparing images represented using either a vector that con-
catenates the decomposed transformed VDR and bag-of-terms, or a vector that contains only the bag-of-
terms. In this demonstration, each term has a tf-idf weight of 1. The first illustration (Similar) compares
images that depict the same underlying action: Figure 1 (a) and (c). The second illustration (Dissimilar)
compares images that depict different actions: Figure 1 (a) and (b).
Similar : cos(VDR
a
,VDR
c
) = 0.56 > cos(Bag
a
,Bag
c
) = 0.52
Dissimilar : cos(VDR
b
,VDR
a
) = 0.201 cos(Bag
b
,Bag
a
) = 0.4
It can be seen that when the images represent the same action, the decomposed VDR increases the
similarity of the pair of images compared to the bag-of-terms representation; and when images do not
114
represent the same action, the decomposed VDR yields a lower similarity than the bag-of-terms repre-
sentation. These illustrations confirm that Visual Dependency Representations can be used to distinguish
the difference between presence or absence of objects, and the prominent relationships between objects.
5 Data
We use an existing dataset of VDR-annotated images to study whether modelling the structure of an
image can improve image retrieval in the domain of action depictions. The data set of Elliott and Keller
(2013) contains 341 images annotated with region annotations, three visual dependency representations
per image (making a total of 1,023 instances), and a ground-truth action label for each image. An
example of the annotations can be seen in Figure 1. The image collection is drawn from the PASCAL
Visual Object Classification Challenge 2011 action recognition taster and covers a set of 10 actions
(Everingham et al., 2011): riding a bike, riding a horse, reading, running, jumping, walking, playing an
instrument, using a computer, taking a photo, and talking on the phone.
Image Descriptions
Each image is associated with three human-written descriptions collected from untrained annotators
on Amazon Mechanical Turk. The descriptions do not form any part of the models presented in the
current paper; they were used in the automatic image description task of Elliott and Keller (2013). Each
description contains two sentences: the first sentence describes the action depicted in the image, and
the second sentence describes other objects not involved in the action. A two sentence description of
an image helps distinguish objects that are central to depicting the action from objects that may be
distractors.
Region Annotations
The images contain human-drawn labelled region annotations. The annotations were drawn using the
LabelMe toolkit, which allows for arbitrary labelled polygons to be created over an image (Russell
et al., 2008). The annotated regions were restricted to those present in at least one of three human-
written descriptions. To reduce the effects of label sparsity, frequently occurring equivalent labels were
conflated, i.e., man, child, and boy? person; bike, bicycle, motorbike? bike; this reduced the object
label vocabulary from 496 labels to 362 labels. The data set contains a total of 5,034 region annotations,
with a mean of 4.19 ? 1.94 annotations per image.
Visual Dependency Representations
Recall that each image is associated with three descriptions, and that people were free to decide how to
describe the action and background of the image. The differences between how people describe images
leads to the creation of one Visual Dependency Representation per image?description pair in the data
set, resulting in a total of 1,023 instances. The process for creating a visual dependency representation
of an image is described in Section 4. The annotated dataset comprises a total of 5,748 spatial relations,
corresponding to a mean of 4.79 ? 3.51 relations per image. Elliott and Keller (2013) report inter-
annotator agreement on a subset of the data at 84% agreement for labelled directed attachments and
95.1% for unlabelled directed attachments.
Action Labels
The original PASCAL action recognition dataset contains ground truth action class annotations for each
image. These annotations are in the form of labelled bounding boxes around the person performing the
action in the image. The action labels are only used as the gold-standard relevance judgements for the
query-by-example image retrieval experiments.
6 Experiments
In this section we present the results of a query-by-example image retrieval experiment to determine
the utility of the Visual Dependency Representation compared to a bag-of-terms representation. In this
115
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Recall
0.0
0.2
0.4
0.6
0.8
1.0
Prec
ision
Manual VDRAutomatic VDRBag-of-Terms
Figure 2: Average 11-point precision/recall curves show that the VDR-based retrieval models are con-
sistently better than the Bag-of-Terms model.
experiment, a single image (the query image) is used to rank the images in the test collection, where the
goal is to construct a ranking where the top images depict the same action as the query image.
6.1 Protocol
The image retrieval experiment is performed using 10-fold cross-validation in the following manner.
The 341 images in the dataset are randomly partitioned into 80%/10%/10% splits, resulting in 1011 test
queries
1
. For each query we compute average precision and Precision@10 of the ranked list, and use the
resulting values to test the statistical significance of the results.
The training set is used to train the VDR prediction model and to estimate inverse document frequency
statistics. During the training phase, the VDR-based models have access to region boundaries, region
labels and three manually-created VDRs for each training image. In the test set, all models have access to
the region boundaries and labels for each image. Each image in the test set forms a query and the models
produce a ranked list of the remaining images in the test collection. Images are marked for relevance
as follows: a image at rank r is considered relevant if it has the same action label as the query image;
otherwise it is non-relevant. The dev set was used to experiment with different matching functions and
to optimise the feature functions used in the VDR prediction model.
6.2 Models
We compare the retrieval accuracy of three approaches: Bag-of-Terms uses an unstructured representa-
tion for each image. A tf-idf weight is assigned to each region label in an image, and the cosine measure
is used to calculate the similarity of images. This model allows us to compare the usefulness of a struc-
tured vs. unstructured image representation. Automatic VDR is a model using the VDR prediction
method from Section 4.1, and Manual VDR uses the gold-standard data described in Section 5. Both
1
Recall there are three Visual Dependency Representations for each image. The partitions are the same as those used in the
VDR prediction experiment in Section 4.1
116
MAP P@10
Manual VDR 0.514
??
0.454
?
Automatic VDR 0.508
?
0.451
?
Bag-of-Terms 0.467 0.415
Table 2: Overall Mean Average Precision and Precision@10 images. The VDR-based models are sig-
nificantly better than the Bag-of-Terms model, supporting the hypothesis that modelling the structure
of an image using the Visual Dependency Representation is useful for image retrieval. ?: significantly
different than Bag-of-Terms at p < 0.01; ?: significantly different than Automatic VDR at p < 0.01.
of the VDR-based models have a tf-idf weight assigned to the transformed decomposed terms and the
cosine similarity measure is used to calculate the similarity of images.
6.3 Results
Figure 2(a) shows the interpolated precision/recall curve and Table 2 shows the Mean Average Precision
(MAP) and Precision at 10 retrieved images (P@10). The MAP of the Automatic VDR model increases
by 8.8% relative to the Bag-of-Terms model, and a relative improvement up to 10.1% would possible if
we had a better structure prediction model, as evidenced by Manual VDR. Furthermore, if we assume a
user will only view the top results returned by the retrieval model, then P@10 increases by 8.6% when we
model the structure of an image, relative to using an unstructured representation; a relative improvement
of up to 9.4% would be possible if we had a better image parser.
To determine whether the differences are statistically significant, we perform the Wilcoxon Signed
Ranks Test on the average precision and P@10 values over the 1011 queries in our cross-validation
data set. The results support the main hypothesis of this paper: structured image representations allow
us to find images depicting actions more accurately than the standard bag-of-terms representation. We
find significant differences in average precision and P@10 between the Bag-of-Terms baseline and both
Automatic VDR (p < 0.01) and Manual VDR (p < 0.01). This suggests that structure is very useful in
the query-by-example scenario. We find a significant difference in average precision between Automatic
VDR and Manual VDR (p < 0.01), but no difference in P@10 between Automatic VDR and Manual
VDR (p = 0.442).
6.4 Retrieval Performance by Type of Action and Verb
We now analyse whether image structure is useful when the action does not require a direct object. The
analysis presented here compares the Bag-of-Terms model against the Automatic VDR model because
there was no significant difference in P@10 between the Automatic and Manual VDR models. Table 3
shows the MAP and Precision@10 per type of action. Figure 3 shows the precision/recall curves for (a)
transitive verbs, (b) intransitive verbs, and (c) light verbs.
In Figure 3(a), it can be seen that the actions that can be classified as transitive verbs benefit from
exploiting the structure encoded in the Visual Dependency Representation. The only exception is for the
action to read, which frequently behaves as an intransitive verb: the man reads on a train. The consistent
improvement in both the entirety of the ranked list and at the top of the ranked list can be seen in the
MAP and P@10 results in Table 3.
Figure 3(b) shows that there is a small increase in retrieval performance for intransitive verbs compared
to the transitive verbs. We conjecture this is because there are fewer objects to annotate in an image when
the verb does not require a direct object. The summary results for the intransitive verbs in Table 3 confirm
the small but insignificant increase in MAP and P@10.
Finally, the light verbs, shown in Figure 3(c), exhibit variable behaviour in retrieval performance. One
reason for this could be that if the light verb encodes information about the object, as in using a computer,
then the computer can be annotated in the image, and thus it acts as a transitive verb. Conversely, when
117
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.0
0.2
0.4
0.6
0.8
1.0
ride horseride bikeread
(a)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.0
0.2
0.4
0.6
0.8
1.0 jumpwalkrun
(b)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.0
0.2
0.4
0.6
0.8
1.0 use computertake photo
(c)
Figure 3: Precision/recall curves grouped by the type of verb. The solid lines represent the Automatic
VDR model; the dashed lines represent the Bag-of-Terms model; y-axis is Precision, and the x-axis is
Recall. (a) Images depicting transitive verbs benefit the most from the Visual Dependency Representation
and are easiest to retrieve. (b) Intransitive verbs are difficult to retrieve and there is is a negligible
improvement in performance when using Visual Dependency Representation. (c) Light verbs benefit
from the Visual Dependency Representation depending on the type of the object involved in the action.
MAP P@10
VDR Bag VDR Bag
Ride bike 0.721
?
0.601 0.596
?
0.513
Ride horse 0.833
?
0.768 0.787
?
0.726
Talk on phone 0.762
?
0.679 0.666
?
0.582
Play instrument 0.774
?
0.705 0.634
?
0.586
Read 0.483 0.454 0.498 0.475
Walk 0.198 0.186 0.184 0.174
Run 0.193 0.165 0.151 0.132
Jump 0.211 0.189 0.142 0.136
Use computer 0.814
?
0.761 0.694
?
0.648
Take photo 0.241 0.223 0.212 0.198
Table 3: Mean Average Precision and Precision@10 for each action in the data set, grouped into transitive
(top), intransitive (middle), and light (bottom) verbs. VDR is the Automatic VDR model and Bag is the
Bag-of-Terms model. It can be seen that the Automatic VDR retrieval model is consistently better than
the Bag-of-Terms model on both MAP and Precision@10. ?: the Automatic VDR model is significantly
different than Bag-of-Terms at p < 0.01.
118
the light verb conveys information about the outcome of the event, as in the action take a photograph,
the outcome is rarely possible to annotate in an image, and so no improvements can be gained from
structured image representations.
6.5 Discussion
In our experiments we observed that all models can achieve high precision at very low levels of recall. We
found that this happens for testing images that are almost identical to the query image. For such images,
objects that are unrelated to the target action form an effective context, which allows this image to be
placed at the top of the ranking. However, near-identical images are relatively rare, and performance
degrades for higher levels of recall.
It is surprising that image retrieval using automatically predicted VDR model is statistically indistin-
guishable from the manually crafted VDR model, given the relatively low accuracy of our VDR predic-
tion model: 61.3% by the labelled dependency attachment accuracy measure. One possible explanation
could be that not all parts of the VDR structure are useful for retrieval purposes, and our VDR prediction
model does well on the useful ones. This observation also suggests that we are unlikely to achieve better
retrieval performance by continuing to improve the accuracy of VDR prediction. We believe a more
promising direction is refining the current formulation of the VDR, and exploring more sophisticated
ways to measure the similarity of two structured representations.
7 Conclusion
In this paper we argued that a limiting factor of retrieving images depicting actions is the unstructured
bag-of-terms representation typically used for images. In a bag-of-terms representation, images that
share similar sets of regions are deemed to be related even when the depicted actions are different. We
proposed that representing an image using the Visual Dependency Representation (VDR) can prevent
this type of misclassification in image retrieval. The VDR of an image captures the region?region re-
lationships that explain what is happening in an image, and it can be automatically predicted from a
region-annotated image.
In a query-by-example image retrieval task, we found that representing images as automatically pre-
dicted VDRs resulted in statistically significant 8.8% relative improvement in MAP and 8.6% relative
improvement in Precision@10 compared to a Bag-of-Terms model. There was a significant difference
in MAP when using manually or automatically predicted image structures, but no difference in the Pre-
cision@10, suggesting that the proposed automatic prediction model is accurate enough for retrieval
purposes. Future work will focus on using automatically generated visual input, such as the output of
the image tagger (Guillaumin and Mensink, 2009), or an automatic object detector (Felzenszwalb et al.,
2010), which will make it possible to tackle image ranking tasks (Hodosh et al., 2013). It would also be
interesting to explore alternative structure prediction methods, such as predicting the relationships using
a conditional random field (Zitnick et al., 2013), or by leveraging distributional lexical semantics (Le et
al., 2013b).
Acknowledgments
The anonymous reviewers provided valuable feedback on this paper. The research is funded by ERC
Starting Grant SYNPROC No. 203427.
References
Moshe Bar and Shimon Ullman. 1996. Spatial Context in Recognition. Perception, 25(3):343?52, January.
I Biederman. 1972. Perceiving real-world scenes. Science, 177(4043):77?80.
Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector
machines. Journal of Machine Learning Research, 2:265?292.
119
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang. 2008. Image retrieval: Ideas, influences, and trends of
the new age. ACM Computing Surveys, 40(2):1?60.
P Duygulu, Kobus Barnard, J F G de Freitas, and David A Forsyth. 2002. Object Recognition as Machine
Translation: Learning a Lexicon for a Fixed Image Vocabulary. In Proceedings of the 7th European Conference
on Computer Vision, pages 97?112, Copenhagen, Denmark.
Desmond Elliott and Frank Keller. 2013. Image Description using Visual Dependency Representations. In Pro-
ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292?1302,
Seattle, Washington, U.S.A.
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2011. The
PASCAL Visual Object Classes Challenge 2011.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and
David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proceedings of the 15th
European Conference on Computer Vision, pages 15?29, Heraklion, Crete, Greece.
P F Felzenszwalb, R B Girshick, D McAllester, and D Ramanan. 2010. Object Detection with Discriminatively
Trained Part-Based Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627?
1645.
Matthieu Guillaumin and Thomas Mensink. 2009. Tagprop: Discriminative metric learning in nearest neighbor
models for image auto-annotation. In IEEE 12th International Conference on Computer Vision, pages 309?316,
Kyoto, Japan.
Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data,
Models and Evaluation Metrics. Journal of Artificial Intelligence Research, 47:853?899.
Victor Lavrenko, R Manmatha, and Jiwoon Jeon. 2003. A Model for Learning the Semantics of Pictures. In
Advances in Neural Information Processing Systems 16, Vancouver and Whistler, British Columbia, Canada.
S. Lazebnik, C. Schmid, and J. Ponce. 2006. Beyond Bags of Features: Spatial Pyramid Matching for Recog-
nizing Natural Scene Categories. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, pages 2169?2178, New York, NY, USA.
DT Le, R Bernardi, and Jasper Uijlings. 2013a. Exploiting language models to recognize unseen actions. In
Proceedings of the International Conference on Multimedia Retrieval, pages 231?238, Dallas, Texas, U.S.A.
DT Le, Jasper Uijlings, and Raffaella Bernardi. 2013b. Exploiting language models for visual recognition. In
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769?779,
Seattle, Washington, U.S.A.
D G Lowe. 1999. Object recognition from local scale-invariant features. In Proceedings of the International
Conference on Computer Vision, pages 1150?1157, Washington, D.C., USA.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji?c. 2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 523?530, Vancouver, British Columbia, Canada.
Bryan C. Russell, Antonio Torralba, Kevin P. Murphy, and William T. Freeman. 2008. LabelMe: A Database and
Web-Based Tool for Image Annotation. International Journal of Computer Vision, 77(1-3):157?173.
Mohammad A Sadeghi and Ali Farhadi. 2011. Recognition Using Visual Phrases. In 2011 IEEE Conference on
Computer Vision and Pattern Recognition, pages 1745?1752, Colorado Springs, Colorado, U.S.A.
Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts and Image Segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(8):888?905, August.
Lei Wu, Rong Jin, and Anil K Jain. 2012. Tag Completion for Image Retrieval. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 35(3):716?727.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yiannis Aloimonos. 2011. Corpus-Guided Sentence Generation
of Natural Images. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 444?454, Edinburgh, Scotland, UK.
CL Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the Visual Interpretation of Sentences. In IEEE
International Conference on Computer Vision, pages 1681?1688, Sydney, Australia.
120
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292?1302,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Image Description using Visual Dependency Representations
Desmond Elliott
School of Informatics
University of Edinburgh
d.elliott@ed.ac.uk
Frank Keller
School of Informatics
University of Edinburgh
keller@inf.ed.ac.uk
Abstract
Describing the main event of an image in-
volves identifying the objects depicted and
predicting the relationships between them.
Previous approaches have represented images
as unstructured bags of regions, which makes
it difficult to accurately predict meaningful
relationships between regions. In this pa-
per, we introduce visual dependency represen-
tations to capture the relationships between
the objects in an image, and hypothesize that
this representation can improve image de-
scription. We test this hypothesis using a
new data set of region-annotated images, as-
sociated with visual dependency representa-
tions and gold-standard descriptions. We de-
scribe two template-based description gener-
ation models that operate over visual depen-
dency representations. In an image descrip-
tion task, we find that these models outper-
form approaches that rely on object proxim-
ity or corpus information to generate descrip-
tions on both automatic measures and on hu-
man judgements.
1 Introduction
Humans are readily able to produce a description of
an image that correctly identifies the objects and ac-
tions depicted. Automating this process is useful for
applications such as image retrieval, where users can
go beyond keyword-search to describe their infor-
mation needs, caption generation for improving the
accessibility of existing image collections, story il-
lustration, and in assistive technology for blind and
partially sighted people. Automatic image descrip-
tion presents challenges on a number of levels: rec-
ognizing the objects in an image and their attributes
are difficult computer vision problems; while deter-
mining how the objects interact, which relationships
hold between them, and which events are depicted
requires considerable background knowledge.
Previous approaches to automatic description
generation have typically tackled the problem us-
ing an object recognition system in conjunction with
a natural language generation component based on
language models or templates (Kulkarni et al, 2011;
Li et al, 2011). Some approaches have utilised the
visual attributes of objects (Farhadi et al, 2010),
generated descriptions by retrieving the descriptions
of similar images (Ordonez et al, 2011; Kuznetsova
et al, 2012), relied on an external corpus to pre-
dict the relationships between objects (Yang et al,
2011), or combined sentence fragments using a tree-
substitution grammar (Mitchell et al, 2012).
A common aspect of existing work is that an im-
age is represented as a bag of image regions. Bags
of regions encode which objects co-occur in an im-
age, but they are unable to express how the regions
relate to each other, which makes it hard to describe
what is happening. As an example, consider Fig-
ure 1a, which depicts a man riding a bike. If the
man was instead repairing the bike, then the bag-
of-regions representation would be the same, even
though the image would depict a different action and
would have to be described differently. This type
of co-occurrence of regions indicates the need for a
more structured image representation; an image de-
scription system that has access to structured repre-
1292
(a)
A man is riding a bike down the road.
A car and trees are in the background.
(b)
ROOT bike car man road trees
- -
-
on
above
A man is riding a bike down the road.
det
nsubj
aux
root
det
dobj
advmod
det
pobj
(c)
Figure 1: (a) Image with regions marked up: BIKE, CAR,
MAN, ROAD, TREES; (b) human-generated image de-
scription; (c) visual dependency representation express-
ing the relationships between MAN, BIKE, and ROAD
aligned to the syntactic dependency parse of the first sen-
tence in the human-generated description (b).
sentations would be able to correctly infer the action
that is taking place, such as the distinction between
repairing or riding a bike, which would greatly im-
prove the descriptions it is able to generate.
In this paper, we introduce visual dependency rep-
resentations (VDRs) to represent the structure of im-
ages. This representation encodes the geometric re-
lations between the regions of an image. An ex-
ample can be found in Figure 1c, which depicts the
VDR for Figure 1a. It encodes that the MAN is above
the BIKE, and that the BIKE is on the ROAD. These
relationships make it possible to infer that the man
is riding a bike down the road, which corresponds
to the first sentence of the human-generated image
description in Figure 1b.
In order to test the hypothesis that structured im-
age representations are useful for description gener-
ation, we present a series of template-based image
description models. Two of these models are based
on approaches in the literature that represent images
as bags of regions. The other two models use vi-
sual dependency representations, either on their own
or in conjunction with gold-standard image descrip-
tions at training time.
We find that descriptions generated using the
VDR-based models are significantly better than
those generated using bag-of-region models in au-
tomatic evaluations using smoothed BLEU scores
and in human judgements. The BLEU score im-
provements are found at bi-, tri-, and four-gram lev-
els, and humans rate VDR-based image descriptions
1.2 points above the next-best model on a 1?5 scale.
Finally, we also show that the benefit of the vi-
sual dependency representation is maintained when
image descriptions are generated from automatically
parsed VDRs. We use a modified version of the
edge-factored parser of McDonald et al (2005) to
predict VDRs over a set of annotated object regions.
This result reaffirms the potential utility of this rep-
resentation as a means to describe events in images.
Note that throughout the paper, we work with gold-
standard region annotations; this makes it possible
to explore the effect of structured image representa-
tions independently of automatic object detection.
2 Visual Dependency Representation
In analogy to dependency grammar for natural lan-
guage syntax, we define Visual Dependency Gram-
mar to describe the spatial relations between pairs
of image regions. A directed arc between two re-
gions is labelled with the spatial relationship be-
tween those regions, defined in terms of three ge-
ometric properties: pixel overlap, the angle between
regions, and the distance between regions. Table 1
presents a detailed explanation of the spatial rela-
tionships defined in the grammar.
A visual dependency representation of an image
is constructed by creating a directed acyclic graph
1293
X ??on Y
More than 50% of the pixels of re-
gion X overlap with region Y.1
X
???????
surrounds Y
The entirety of region X overlaps
with region Y.
X
????
beside Y
The angle between the centroid of
X and the centroid of Y lies be-
tween 315? and 45? or 135? and
225?.
X
??????
opposite Y
Similar to beside, but used when
there X and Y are at opposite sides
of the image.
X
????
above Y
The angle between X and Y lies be-
tween 225? and 315?.
X
????
below Y
The angle between X and Y lies be-
tween 45? and 135?.
X
?????
infront Y
The Z-plane relationship between
the regions is dominant.
X
?????
behind Y
Identical to infront except X is be-
hind Y in the Z-plane.
Table 1: Visual Dependency Grammar defines eight re-
lations between pairs of annotated regions. To simplify
explanation, all regions are circles, where X is the grey
region and Y is the white region. All relations are consid-
ered with respect to the centroid of a region and the angle
between those centroids. We follow the definition of the
unit circle, in which 0? lies to the right and a turn around
the circle is counter-clockwise.
over the set of regions in an image using the spa-
tial relationships in the Visual Dependency Gram-
mar. It is created from a region-annotated image and
a corresponding image description by first identify-
ing the central actor of the image. The central actor
is the person or object carrying out the depicted ac-
tion; this typically corresponds to the subject of the
sentence describing the image. The region corre-
sponding to the central actor is attached to the ROOT
node of the graph. The remaining regions are then
attached based on their relationship with either the
actor or the other regions in the image as they are
1As per the PASCAL VOC definition of overlap in the object
detection task (Everingham et al, 2011).
mentioned in the description. Each arc introduced
is labelled with one of the spatial relations defined
in the grammar, or with no label if the region is not
described in relation to anything else in the image.
As an example of the output of this annotation
process, consider Figure 1a, its description in 1b,
and its VDR in 1c. Here, the MAN is the central
actor in the image, as he is carrying out the depicted
action (riding a bike). The region corresponding to
MAN is therefore attached to ROOT without a spa-
tial relation. The BIKE region is then attached to the
MAN region using the
????
above relation and BIKE is at-
tached to the ROAD with the ??on relation. In the sec-
ond sentence of the description, CAR and TREES are
mentioned without a relationship to anything else in
the image, so they are attached to the ROOT node. If
these regions were attached to other regions, such as
CAR
????
above ROAD then this would imply structure in
the image that is not conveyed in the description.
2.1 Data
Our data set uses the images from the PASCAL
Visual Object Classification Challenge 2011 action
recognition taster competition (Everingham et al,
2011). This is a closed-domain data set containing
images of people performing ten types of actions,
such as making a phone call, riding a bike, and tak-
ing a photo. We annotated the data set in a three-step
process: (1) collect a description for each image;
(2) annotate the regions in the image; and (3) create a
visual dependency representation of the image. Note
that Steps (2) and (3) are dependent on the image de-
scription, as both the region labels and the relations
between them are derived from the description.
2.2 Image Descriptions
We collected three descriptions of each image in our
data set from Amazon Mechanical Turk. Workers
were asked to describe an image in two sentences.
The first sentence describes the action in the image,
the person performing the action and the region in-
volved in the action; the second sentence describes
any other regions in the image not directly involved
in the action. An example description is given in
Figure 1b.
A total of 2,424 images were described by three
workers each, resulting in a total of 7,272 image de-
1294
ma
n
wo
ma
n
per
son
peo
ple tree
s
hor
se girl wal
l
boy
co
mp
ute
r
chi
ld
boo
k
pho
ne
cha
ir
win
dow gra
ss
ca
me
ra
bic
ycle bik
e
lap
top
Fre
que
ncy
0
100
200
300
400
500
Figure 2: Top 20 annotated regions.
scriptions. The workers, drawn from those regis-
tered in the US with a minimum HIT acceptance rate
of 95%, described an average of 145 ? 93 images;
they were encouraged to describe fewer than 300 im-
ages each to ensure a linguistically diverse data set.
They were paid $0.04 per image and it took on av-
erage 67 ? 123 seconds to describe a single image.
The average length of a description was 19.9 ? 6.5
words in a range of 8?50 words. Dependency parses
of the descriptions were produced using the MST-
Parser (McDonald et al, 2005) trained on sections
2-21 of the WSJ portion of the Penn Treebank.
2.3 Region Annotations
We trained two annotators to draw polygons around
the outlines of the regions in an image using the La-
belMe annotation tool (Russell et al, 2008). The
regions annotated for a given image were limited to
those mentioned in the description paired with the
image. Region annotation was performed on a sub-
set of 341 images and resulted in a total of 5,034
annotated regions with a mean of 4.19 ? 1.94 an-
notations per image. A total of 496 distinct labels
were used to label regions. Figure 2 shows the
distribution of the top 20 region annotations in the
data; people-type regions are the most commonly
annotated regions. Given the prevalence of labels
referring to the same types of regions, we defined
26 sets of equivalent labels to reduce label sparsity
(e.g., BIKE was considered equivalent to BICYCLE).
This normalization process reduced the size of the
region label vocabulary from 496 labels to 362 la-
no
ne
infr
ont
bes
ide
abo
ve on
su
rro
un
ds
beh
ind
bel
ow
opp
osi
te
Fre
que
ncy
0
500
1000
1500
2000
2500
Figure 3: Distribution of the spatial relations.
bels. Inter-annotator agreement was 74.3% for re-
gion annotations, this was measured by computing
polygon overlap over the annotated regions.
2.4 Visual Dependency Representations
The same two annotators were trained to construct
gold-standard visual dependency representations for
annotated image?description pairs. The process for
creating a visual dependency representation of an
image is described earlier in this section of the pa-
per. The 341 region-annotated images resulted in a
set of 1,023 visual dependency representations. The
annotated data set comprised a total of 5,748 spatial
relations, corresponding to a mean of 4.79 ? 3.51
relations per image. Figure 3 shows the distribution
of spatial relation labels in the data set. It can be
seen that the majority of regions are attached to the
ROOT node, i.e., they have the relation label none.
Inter-annotator agreement on a subset of the data
was measured at 84% agreement for labelled de-
pendency accuracy and 95.1% for unlabelled depen-
dency accuracy. This suggests the task of generating
visual dependency representations can be performed
reliably by human annotators. We induced an align-
ment between the annotated region labels and words
in the image description using simple lexical match-
ing augmented with WordNet hyponym lookup. See
Figure 1c for an example of the alignments.
3 Image Description Models
We present four template-based models for gener-
ating image descriptions in this section. Table 2
1295
Regions VDR
External
Corpus
Parallel
text
PROXIMITY X
CORPUS X X
STRUCTURE X X
PARALLEL X X X
Table 2: The data available to each model at training time.
presents an overview of the amount of information
available to each model at training time, ranging
from only the annotated regions of an image to us-
ing visual dependency representation of an image
aligned with the syntactic dependency representa-
tion of its description. At test time, all models have
access to image regions and their labels, and use
these to generate image descriptions. Two of the
models also have access to VDRs at test time, al-
lowing us to test the hypothesis that image structure
is useful for generating good image descriptions.
The aim of each model is to determine what is
happening in the image, which regions are impor-
tant for describing it, and how these regions relate to
each other. Recall that all our images depict actions,
and that the gold-standard annotation was performed
with this in mind. A good description therefore is
one that relates the main actors depicted in the im-
age to each other, typically through a verb; a mere
enumeration of the regions in the image is not suffi-
cient. All models attempt to generate a two-sentence
description, as per the gold standard descriptions.
In the remainder of this section, we will use Fig-
ure 1 as a running example to demonstrate the type
of language each model is capable of generating. All
models share the set of templates in Table 3.
3.1 PROXIMITY
PROXIMITY is based on the assumption that people
describe the relationships between regions that are
near each other. It has access to only the annotated
image regions and their labels.
Region?region relationships that are potentially
relevant for the description are extracted by calculat-
ing the proximity of the annotated regions. Here, oi
is the subject region, o j is the object region, and si j
is the spatial relationship between the regions. Let
T1 DT Oi AUX REL DT O j. T5?
T2 There AUX also {DT Oi}
|unrelated|
i=1 in the image.
T3 DT Oi AUX REL DT O j REL DT Ok. T5?
T4 REL DT O j.
T5 PRP AUX {REL DT Oi}
|dependents|
i=1 .
Table 3: The language generation templates.
R = {(oi, si j, o j), . . .} be the set of possible region?
region relationships found by calculating the near-
est neighbour of each region in Euclidean space be-
tween the centroids of the polygons that mark the re-
gion boundaries. The tuple with the subject closest
to the centre of the image is used to describe what is
happening in the image, and the remaining regions
are used to describe the background.
The first sentence of the description is realised
with template T1 from Table 3. oi is the label of
the subject region and o j is the label of the object
region. DT is a simple determiner chosen from {the,
a}, depending on whether the region label is a plural
noun; AUX is either {is, are}, depending on the num-
ber of the region label; and REL is a word to describe
the relationship between the regions. For this model,
REL is the spatial relationship between the centroids
chosen from {above, below, beside}, depending on
the angle formed between the region centroids, us-
ing the definitions in Table 1. The second sentence
of the description is realised with template T2 over
the subjects oi in R that were not used in the first
sentence. An example of the language generated is:
(1) The man is beside the bike. There is also a
road, a car, and trees in the image.
With the exception of visual attributes to describe
size, colour, or texture, this model is based on the
approach described by Kulkarni et al (2011).
3.2 CORPUS
The biggest limitation of PROXIMITY is that regions
that are near each other are not always in a rele-
vant relationship for a description. For example, in
Figure 1, the BIKE and the CAR regions are near-
est neighbours but they are unlikely to be described
as being in an relationship by a human annotator.
The model CORPUS addresses this issue by using an
1296
external text corpus to determine which pairs of re-
gions are likely to be in a describable relationship.
Furthermore, CORPUS can generate verbs instead of
spatial relations between regions, leading to more
human-like descriptions. CORPUS is based on Yang
et al (2011), except we do not use scene type (in-
door, outdoor, etc.) as part of the model. At training
time, the model has access to the annotated image
regions and labels, and to the dependency-parsed
version of the English Gigaword Corpus (Napoles
et al, 2012). The corpus is used to extract subject?
verb?object subtrees, which are then used to predict
the best pairs of regions, as well as the verb that re-
lates the regions.
The set of region?region relationships
R = {(oi, vi j, o j), . . .} is determined by search-
ing for the most likely o?j ,v
? given an oi over a set
of verbs V extracted from the corpus and the other
regions in the image. This is shown in Equation 1.
o?j ,v
?|oi = argmax
o j ,v
p(oi) ? p(v|oi) ? p(o j|v,oi) (1)
We can easily estimate p(oi), p(v|oi), and p(o j|v,oi)
directly from the corpus. If we cannot find an o?j ,v
?
for a region, we back-off to the spatial relationship
calculation as defined in PROXIMITY. When we
have found the best pairs of regions, we select the
most probable pair and generate the first sentence of
the description using that pair an template T1. The
second sentence is realised with template T2 over the
subjects in R not used in generating the first sen-
tence. An example of the language generated is:
(2) The man is riding the bike. There is also a
car, a road, and trees in the image.
In comparison to PROXIMITY, this model will only
describe pairs of regions that have observed rela-
tions in the external corpus. The corpus also pro-
vides a verb that relates the regions, which pro-
duces descriptions that are more in line with human-
generated text. However, since noun co-occurrence
in the corpus controls which regions can be men-
tioned in the description, this model will be prone
to relating regions simply because their labels occur
together frequently in the corpus.
3.3 STRUCTURE
The model STRUCTURE exploits the visual depen-
dency representation of an image to generate lan-
guage for only the relationships that hold between
pairs of regions. It has access to the image regions,
the region labels, and the visual dependency repre-
sentation of an image.
Region?region relationships are generated during
a depth-first traversal of the VDR using templates
T1, T3, T4, and T5. The VDR of an image is traversed
and language fragments are generated and then com-
bined depending on the number of children of a node
in the tree. If a node has only one child then we
use T1 to generate text for the head-child relation-
ship. If a node has more than one child, we need to
decide how to order the language generated by the
model. We generate sentence fragments using T4 for
each child independently and combine them later. In
STRUCTURE, the sentence fragments are sorted by
the Euclidean distance of the children from the par-
ent. In order to avoid problematic descriptions such
as ?The woman is above the horse is above the field
is beside the house?, we include a special case for
when a node has more than one child. In these cases,
the nearest region is realized in direct relation to the
head using either T3 (two children) or T1 (more than
two children), and the remaining regions form a sep-
arate sentence using T5. This sorting and combing
process would result in ?The woman is above the
horse. She is above field and beside the house? for
the case mentioned above.
An example of the type of description that can be
generated during a traversal is:
(3) The man is above the bike above the road.
There is also a car and trees in the image.
In comparison to PROXIMITY, this model can exploit
a representation of an image that encodes the rela-
tionships between regions in an image (the VDR).
However, it is limited to generating spatial relations,
because it cannot predict verbs to relate regions.
3.4 PARALLEL
The model PARALLEL is an extension of STRUC-
TURE that uses the image descriptions available to
1297
predict verbs that relate regions in parent-child re-
lationships in a VDR. At training time it has ac-
cess to the annotated regions and labels, the visual
dependency representations, and the gold-standard
image descriptions. Recall from Section 2.1 that
the descriptions were dependency-parsed using the
parser of McDonald et al (2005) and alignments
were calculated between the nodes in the VDRs and
the words in the parsed image descriptions.
We estimate two distributions from
the image descriptions using the align-
ments: p(verb|ohead ,ochild ,relhead?child) and
p(verb|ohead ,ochild). The second distribution is used
as a backoff when we do not observe the arc label
between the regions in the training data. The gener-
ation process is similar to that used in STRUCTURE,
with two exceptions: (1) it can generate verbs
during the generation steps, and (2) when a node
has multiple dependents, the sentence fragments
are sorted by the probability of the verb associated
with them. This sorting step governs which child is
in a relationship with its parent. When the model
generates text, it only generates a verb for the
most probable sentence fragment. The remaining
fragments revert back to spatial relationships to
avoid generating language that places the subject
region in multiple relationships with other regions.
An example of the language generated is:
(4) The man is riding the bike on the road. There
is also a car and trees in the image.
In comparison to CORPUS, this model generates de-
scriptions in which the relations between the regions
determined by the image itself and not by an external
corpus. In comparison to PROXIMITY and STRUC-
TURE, this model generates descriptions that express
meaningful relations between the regions and not
simple spatial relationships.
4 Image Parsing
The STRUCTURE and PARALLEL models rely on vi-
sual dependency representations, but it is unreal-
istic to assume gold-standard representations will
always be available because they are expensive to
construct. In this section we describe an image
parser that can induce VDRs automatically from
region-annotated images, providing the input for
the STRUCTURE-PARSED and PARALLEL-PARSED
models at test time.
The parser is based on the arc-factored depen-
dency parsing model of McDonald et al (2005).
This model generates a dependency representation
by maximizing the score s computed over all edges
of the representation. In our notation, xvis is the set
of annotated regions and yvis is a visual dependency
representation of the image; (i, j) is a directed arc
from node i to node j in xvis, f(i, j) is a feature rep-
resentation of the arc (i, j), and w is a vector of fea-
ture weights to be learned by the model. The overall
score of a visual dependency representation is:
s(xvis,yvis) = ?
(i, j)?yvis
w ? f(i, j) (2)
The features in the model are defined over re-
gion labels in the visual dependency representation
as well as the relationship labels. As our depen-
dency representations are unordered, none of the
features encode the linear order of region labels,
unlike the feature set of the original model. Uni-
gram features describe how likely individual region
labels are to appear as either heads or arguments and
bigram feature captures which region labels are in
head-argument relationships. All features are con-
joined with the relationship label.
We evaluate our parser on the 1,023 visual depen-
dency representations from the data set. The evalu-
ation is run over 10 random splits into 80% train-
ing, 10% development, and 10% test data.2 Per-
formance is measured with labelled and unlabelled
directed dependency accuracy. The parser achieves
58.2%? 3.1 labelled accuracy and 65.5%? 3.3 un-
labelled accuracy, significantly better than the base-
line of 51.6% ? 2.5 for both labelled and unlabelled
accuracy (the baseline was calculated by attaching
all image regions to the root node; this is the most
frequent form of attachment in our data).
5 Language Generation Experiments
We evaluate the image description models in an au-
tomatic setting and with human judgements. In
2Different visual dependency representations of the same
image are never split between the training and test data.
1298
Automatic Evaluation Human Judgements
BLEU-1 BLEU-2 BLEU-3 BLEU-4 Grammar Action Scene
PARALLEL-PARSED 45.4 ? 2.0 16.1 ? 0.9 6.4 ? 0.7 2.7 ? 0.5 4.2 ? 1.3 3.3 ? 1.7 3.5 ? 1.3
PROXIMITY 45.1 ? 0.8 10.2 ? 1.0? 2.1 ? 0.6? 0.4 ? 0.2? 3.7 ? 1.5? 2.1 ? 0.3? 3.0 ? 1.4?
CORPUS 46.1 ? 1.1 12.4 ? 1.3? 3.1 ? 0.8? 0.7 ? 0.3? 4.4 ? 1.1 2.2 ? 1.3? 3.4 ? 1.3
STRUCTURE 40.2 ? 3.0? 11.5 ? 1.2? 3.5 ? 0.5? 0.3 ? 0.1? 4.1 ? 1.4 2.1 ? 1.4? 3.0 ? 1.4?
STRUCTURE-PARSED 41.1 ? 2.1? 12.2 ? 0.9? 3.6 ? 0.4? 0.4 ? 0.2? 4.0 ? 1.4 1.6 ? 1.3? 3.2 ? 1.3
PARALLEL 44.6 ? 3.1 16.0 ? 1.5 6.8 ? 1.0 2.9 ? 0.7 4.5 ? 1.0? 3.4 ? 1.6 3.7 ? 1.3
GOLD - - - - 4.8 ? 0.4? 4.8 ? 0.6? 4.6 ? 0.7?
Table 4: Automatic evaluation results averaged over 10 random test splits of the data, and human judgements on the
median scoring BLEU-4 test split for PARALLEL. We find significant differences (?p < 0.05) in the descriptions gener-
ated by PARALLEL-PARSED compared to models that operate over an unstructured bag of image regions representation.
Bold means PARALLEL-PARSED is significantly better than PROXIMITY, CORPUS, and STRUCTURE.
the automatic setting, we follow previous work and
measure how close the model-generated descrip-
tions are to the gold-standard descriptions using the
BLEU metric. Human judgements were collected
from Amazon Mechanical Turk.
5.1 Methodology
The task is to produce a description of an image.
The PROXIMITY and CORPUS models have access
to gold-standard region labels and region bound-
aries at test time. The STRUCTURE and PARALLEL
models have additional access to the visual depen-
dency representation of the image. These represen-
tations are either the gold-standard, or in the case of
STRUCTURE-PARSED and PARALLEL-PARSED, pro-
duced by the image parser described in Section 4.
Table 2 provides a reminder of the information the
different models have access to at training time.
Our data set of 1,023 image?description?VDR
tuples was randomly split into 10 folds of 80%
training data, 10% development data, and 10% test
data. The results we report are means computed
over the 10 splits. The image parser used for mod-
els STRUCTURE-PARSED and PARALLEL-PARSED
is trained on the gold-standard VDRs of the train-
ing splits, and then predicts VDRs on the develop-
ment and test splits. Significant differences were
measured using a one-way ANOVA with PARALLEL-
PARSED as the reference3, with differences between
pairs of mean checked with a Tukey HSD test.
5.2 Automatic Evaluation
The model-generated descriptions are compared
against the human-written gold-standard descrip-
tions using the smoothed BLEU measure (Lin and
Och, 2004). BLEU is commonly used in ma-
chine translation experiments to measure the effec-
tive overlap between a reference sentence and a pro-
posed translation sentence. Table 4 shows the re-
sults on the test data and Figure 4 shows sample out-
puts for two images. PARALLEL, the model with
access to both image structure and aligned image
descriptions at training time outperforms all other
models on higher-order BLEU measures. One rea-
son for this improvement is that PARALLEL can for-
mulate sentence fragments that relate the subject, a
verb, and an object without trying to predict the best
object, unlike CORPUS. The probability associated
with each fragment generated for nodes with mul-
tiple children also tends to lead to a more accurate
order of mentioning image regions. It can also be
seen that PARALLEL-PARSED remains significantly
better than the other models when the VDRs of im-
ages are predicted by an image parser, rather than
being gold-standard.
3Recall that PARALLEL uses gold-standard VDRs and
PARALLEL-PARSED uses the output of the image parser de-
scribed in Section 4.
1299
The weakest results are obtained from a model
that relies on the proximity of regions to generate de-
scriptions. PROXIMITY achieves competitive BLEU-
1 scores but this is mostly due to it correctly gener-
ating region names and determiners. CORPUS is bet-
ter than PROXIMITY at correctly producing higher-
order n-grams than because it has a better model of
the region?region relationships in an image. How-
ever, it has difficulties guessing the correct verb for
a description, as it relies on corpus co-occurrences
for this (see the second example in Table 4). STRUC-
TURE uses the VDR of an image to generate the de-
scription, which this leads to an improvement over
PROXIMITY on some of the BLEU metrics; however,
it is not sufficient to outperform CORPUS.
5.3 Human Judgements
We conducted a human judgement study on Me-
chanical Turk to complement the automatic evalu-
ation. Workers were paid $0.05 to rate the quality of
an image?description pair generated by one of the
models using three criteria on a scale from 1 to 5:
1. Grammaticality: give high scores if the de-
scription is correct English and doesn?t contain
any grammatical mistakes.
2. Action: give high scores if the description cor-
rectly describes what people are doing in the
image.
3. Scene: give high scores if the description cor-
rectly describes the rest of the image (back-
ground, other objects, etc).
A total of 101 images were used for this evalua-
tion and we obtained five judgments for each image-
description pair, resulting in a total of 3,535 judg-
ments. To ensure a fair evaluation, we chose the
images from the split of the data that gave median
BLEU-4 accuracy for PARALLEL, the best perform-
ing model in the automatic evaluations.
The right side of Table 4 shows the mean judge-
ments for each model for across the three evalua-
tion criteria. The gold-standard descriptions elicited
judgements around five, and were significantly bet-
ter than the model outputs on all aspects. Further-
more, all models produce highly grammatical out-
put, with mean ratings of between 3.7 and 4.5. This
can be explained by the fact that the models all relied
on templates to ensure grammatical output.
The ratings of the action descriptions reveal the
usefulness of structural information. PROXIMITY,
CORPUS, and STRUCTURE all perform badly with
mean judgements around two, PARALLEL, which
uses both image structure and aligned descriptions,
significantly outperforms all other models with the
exception of PARALLEL-PARSED, which has very
similar performance. The fact that PARALLEL and
PARALLEL-PARSED perform similarly on all three
human measures confirms that automatically parsed
VDRs are as useful for image description as gold-
standard VDRs.
When we compare the quality of the scene de-
scriptions, we notice that all models perform simi-
larly, around the middle of the scale. This is proba-
bly due to the fact that they all have access to gold-
standard region labels, which enables them to cor-
rectly refer to regions in the scene most of the time.
The additional information about the relationships
between regions that STRUCTURE and PARALLEL
have access to does not improve the quality of the
background scene description.
6 Related Work
Previous work on image description can be grouped
into three approaches: description-by-retrieval, de-
scription using language models, and template-
based description. Ordonez et al (2011), Farhadi
et al (2010), and Kuznetsova et al (2012) gener-
ate descriptions by retrieving the most similar image
from a large data set of images paired with descrip-
tions. These approaches are restricted to generating
descriptions that are only present in the training set;
also, they typically require large amounts of training
data and assume images that share similar properties
(scene type, objects present) should be described in
a similar manner.
Kulkarni et al (2011) and Li et al (2011) generate
descriptions using n-gram language models trained
on a subset of Wikipedia. Both approaches first
determine the attributes and relationships between
regions in an image as region?preposition?region
triples. The disadvantage of relying on region?
preposition?region triples is that they cannot distin-
guish between the main event of the image and the
1300
PROXIMITY A man is beside a phone. There is also a wall and a sign in the image.
CORPUS A man is holding a sign. There is also a wall and a phone in the image.
STRUCTURE A wall is above a wall. A man is beside a sign.
PARALLEL A man is holding a phone. A wall is beside a sign.
GOLD A foreign man with sunglasses talking on a cell phone.
A large building and a mountain in the background.
PROXIMITY A beach is above a beach.
There are also horses, a woman, and a man in the image.
CORPUS A woman is outnumbering a man.
There are also horses and beaches in the image.
STRUCTURE A man is beside a woman above a horse.
A horse is beside a woman beside a beach.
PARALLEL A man is riding a horse above a beach.
A horse is beside a beach beside a woman.
GOLD There is a man and women both on horses.
They are on a beach during the day.
Figure 4: Some example descriptions produced by PROXIMITY, CORPUS, STRUCTURE and PARALLEL.
background regions. Kulkarni et al (2011) is closely
related to our PROXIMITY baseline.
Yang et al (2011) fill in a sentence template
by selecting the likely objects, verbs, prepositions,
and scene types based on a Hidden Markov Model.
Verbs are generated by finding the most likely pair-
ing of object labels in an external corpus. This
model is closely related to our CORPUS baseline.
Mitchell et al (2012) over-generates syntactically
well-formed sentence fragments and then recom-
bines these using a tree-substitution grammar.
Previous research has relied extensively on auto-
matically detecting object regions in an image using
state-of-the art object detectors (Felzenszwalb et al,
2010). We use gold-standard region annotations to
remove this noisy component from the description
generation pipeline, allowing us to focus on the util-
ity of image structure for description generation.
7 Conclusion
In this paper we introduced a novel representation
of an image as a set of dependencies over its an-
notated regions. This visual dependency represen-
tation encodes which regions are related to each
other in an image, and can be used to infer the ac-
tion or event that is depicted. We found that im-
age description models based on visual dependency
representations significantly outperform competing
models in both automatic and human evaluations.
We showed that visual dependency representations
can be induced automatically using a standard de-
pendency parser and that the descriptions generated
from the induced representations are as good as the
ones generated from gold-standard representations.
Future work will focus on improvements to the im-
age parser, on exploring this representation in open-
domain data sets, and on using the output of an ob-
ject detector to obtain a fully automated model.
Acknowledgments
The authors would like to thank M. Lapata and S.
Frank for feedback on an earlier draft of the pa-
per and the anonymous reviewers for their feed-
back. A. M. Enoch, N. Ghahremani-Azghandi, L. S.
McAlpine, and K. Tsagkaridis helped annotate the
data. The research presented here was supported by
the European Research Council under award 203427
Synchronous Linguistic and Visual Processing.
1301
References
Mark Everingham, Luc Van Gool, Christopher K. I.
Williams, John Winn, and Andrew Zisserman. 2011.
The PASCAL Visual Object Classes Challenge 2011
(VOC2011) Results.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In ECCV ?10,
pages 15?29, Heraklion, Crete, Greece.
P F Felzenszwalb, R B Girshick, D McAllester, and
D Ramanan. 2010. Object Detection with Discrimi-
natively Trained Part-Based Models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627?1645.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In CVPR ?11, pages
1601?1608, Colorado Springs, Colorado, U.S.A.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg, and Yejin Choi. 2012. Collective
Generation of Natural Image Descriptions. In ACL
?12, pages 359?368, Jeju Island, South Korea.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
CoNLL ?11, pages 220?228, Portland, Oregon, U.S.A.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
ACL ?04, pages 605?612, Barcelona, Spain.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL ?05, pages 91?98, University of
Michigan, U.S.A.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,
Tamara Berg, and Hal Daum. 2012. Midge : Generat-
ing Image Descriptions From Computer Vision Detec-
tions. In EACL ?12, pages 747?756, Avignon, France.
Courtney Napoles, Matthew Gormley, and Benjamin Van
Durme. 2012. Annotated Gigaword. In AKBC-
WEKEX Workshop at NAACL-HLT ?12, Montreal,
Canada.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In NIPS 24, Granada, Spain.
Bryan C. Russell, Antonio Torralba, Kevin P. Murphy,
and William T. Freeman. 2008. LabelMe: A Database
and Web-Based Tool for Image Annotation. IJCV,
77(1-3):157?173.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yiannis
Aloimonos. 2011. Corpus-Guided Sentence Genera-
tion of Natural Images. In EMNLP ?11, pages 444?
454, Edinburgh, Scotland, UK.
1302
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 452?457,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Comparing Automatic Evaluation Measures for Image Description
Desmond Elliott and Frank Keller
Institute for Language, Cognition, and Computation
School of Informatics, University of Edinburgh
d.elliott@ed.ac.uk, keller@inf.ed.ac.uk
Abstract
Image description is a new natural lan-
guage generation task, where the aim is to
generate a human-like description of an im-
age. The evaluation of computer-generated
text is a notoriously difficult problem, how-
ever, the quality of image descriptions has
typically been measured using unigram
BLEU and human judgements. The focus
of this paper is to determine the correlation
of automatic measures with human judge-
ments for this task. We estimate the correla-
tion of unigram and Smoothed BLEU, TER,
ROUGE-SU4, and Meteor against human
judgements on two data sets. The main
finding is that unigram BLEU has a weak
correlation, and Meteor has the strongest
correlation with human judgements.
1 Introduction
Recent advances in computer vision and natural
language processing have led to an upsurge of re-
search on tasks involving both vision and language.
State of the art visual detectors have made it possi-
ble to hypothesise what is in an image (Guillaumin
et al, 2009; Felzenszwalb et al, 2010), paving
the way for automatic image description systems.
The aim of such systems is to extract and reason
about visual aspects of images to generate a human-
like description. An example of the type of image
and gold-standard descriptions available can be
seen in Figure 1. Recent approaches to this task
have been based on slot-filling (Yang et al, 2011;
Elliott and Keller, 2013), combining web-scale n-
grams (Li et al, 2011), syntactic tree substitution
(Mitchell et al, 2012), and description-by-retrieval
(Farhadi et al, 2010; Ordonez et al, 2011; Hodosh
et al, 2013). Image description has been compared
to translating an image into text (Li et al, 2011;
Kulkarni et al, 2011) or summarising an image
1. An older woman with a small dog in the snow.
2. A woman and a cat are outside in the snow.
3. A woman in a brown vest is walking on the
snow with an animal.
4. A woman with a red scarf covering her head
walks with her cat on snow-covered ground.
5. Heavy set woman in snow with a cat.
Figure 1: An image from the Flickr8K data set and
five human-written descriptions. These descrip-
tions vary in the adjectives or prepositional phrases
that describe the woman (1, 3, 4, 5), incorrect or un-
certain identification of the cat (1, 3), and include
a sentence without a verb (5).
(Yang et al, 2011), resulting in the adoption of the
evaluation measures from those communities.
In this paper we estimate the correlation of hu-
man judgements with five automatic evaluation
measures on two image description data sets. Our
work extends previous studies of evaluation mea-
sures for image description (Hodosh et al, 2013),
which focused on unigram-based measures and re-
ported agreement scores such as Cohen?s ? rather
than correlations. The main finding of our analysis
is that TER and unigram BLEU are weakly corre-
452
lated against human judgements, ROUGE-SU4 and
Smoothed BLEU are moderately correlated, and the
strongest correlation is found with Meteor.
2 Methodology
We estimate Spearman?s ? for five different auto-
matic evaluation measures against human judge-
ments for the automatic image description task.
Spearman?s ? is a non-parametric correlation co-
efficient that restricts the ability of outlier data
points to skew the co-efficient value. The automatic
measures are calculated on the sentence level and
correlated against human judgements of semantic
correctness.
2.1 Data
We perform the correlation analysis on the Flickr8K
data set of Hodosh et al (2013), and the data set of
Elliott and Keller (2013).
The test data of the Flickr8K data set contains
1,000 images paired with five reference descrip-
tions. The images were retrieved from Flickr, the
reference descriptions were collected from Me-
chanical Turk, and the human judgements were
collected from expert annotators as follows: each
image in the test data was paired with the highest
scoring sentence(s) retrieved from all possible test
sentences by the TRI5SEM model in Hodosh et al
(2013). Each image?description pairing in the test
data was judged for semantic correctness by three
expert human judges on a scale of 1?4. We calcu-
late automatic measures for each image?retrieved
sentence pair against the five reference descriptions
for the original image.
The test data of Elliott and Keller (2013) con-
tains 101 images paired with three reference de-
scriptions. The images were taken from the PAS-
CAL VOC Action Recognition Task, the reference
descriptions were collected from Mechanical Turk,
and the judgements were also collected from Me-
chanical Turk. Elliott and Keller (2013) gener-
ated two-sentence descriptions for each of the test
images using four variants of a slot-filling model,
and collected five human judgements of the se-
mantic correctness and grammatical correctness of
the description on a scale of 1?5 for each image?
description pair, resulting in a total of 2,042 human
judgement?description pairings. In this analysis,
we use only the first sentence of the description,
which describes the event depicted in the image.
2.2 Automatic Evaluation Measures
BLEU measures the effective overlap between a
reference sentence X and a candidate sentence Y .
It is defined as the geometric mean of the effective
n-gram precision scores, multiplied by the brevity
penalty factor BP to penalise short translations. p
n
measures the effective overlap by calculating the
proportion of the maximum number of n-grams
co-occurring between a candidate and a reference
and the total number of n-grams in the candidate
text. More formally,
BLEU = BP ? exp
(
N
?
n=1
w
n
log p
n
)
p
n
=
?
c?cand
?
ngram?c
count
clip
(ngram)
?
c?cand
?
ngram?c
count(ngram)
BP =
{
1 if c > r
e
(1?r/c)
if c? r
Unigram BLEU without a brevity penalty has been
reported by Kulkarni et al (2011), Li et al (2011),
Ordonez et al (2011), and Kuznetsova et al (2012);
to the best of our knowledge, the only image de-
scription work to use higher-order n-grams with
BLEU is Elliott and Keller (2013). In this paper we
use the smoothed BLEU implementation of Clark et
al. (2011) to perform a sentence-level analysis, set-
ting n = 1 and no brevity penalty to get the unigram
BLEU measure, or n = 4 with the brevity penalty
to get the Smoothed BLEU measure. We note that a
higher BLEU score is better.
ROUGE measures the longest common subse-
quence of tokens between a candidate Y and refer-
ence X . There is also a variant that measures the co-
occurrence of pairs of tokens in both the candidate
and reference (a skip-bigram): ROUGE-SU*. The
skip-bigram calculation is parameterised with d
skip
,
the maximum number of tokens between the words
in the skip-bigram. Setting d
skip
to 0 is equivalent to
bigram overlap and setting d
skip
to ? means tokens
can be any distance apart. If ? = |SKIP2(X ,Y )|
is the number of matching skip-bigrams between
the reference and the candidate, then skip-bigram
ROUGE is formally defined as:
R
SKIP2
= ? /
(
?
2
)
453
ROUGE has been used by only Yang et al (2011)
to measure the quality of generated descriptions,
using a variant they describe as ROUGE-1. We set
d
skip
= 4 and award partial credit for unigram only
matches, otherwise known as ROUGE-SU4. We use
ROUGE v.1.5.5 for the analysis, and configure the
evaluation script to return the result for the average
score for matching between the candidate and the
references. A higher ROUGE score is better.
TER measures the number of modifications a hu-
man would need to make to transform a candidate
Y into a reference X . The modifications available
are insertion, deletion, substitute a single word, and
shift a word an arbitrary distance. TER is expressed
as the percentage of the sentence that needs to be
changed, and can be greater than 100 if the candi-
date is longer than the reference. More formally,
TER =
|edits|
|reference tokens|
TER has not yet been used to evaluate image de-
scription models. We use v.0.8.0 of the TER evalu-
ation tool, and a lower TER is better.
Meteor is the harmonic mean of unigram preci-
sion and recall that allows for exact, synonym, and
paraphrase matchings between candidates and ref-
erences. It is calculated by generating an alignment
between the tokens in the candidate and reference
sentences, with the aim of a 1:1 alignment between
tokens and minimising the number of chunks ch
of contiguous and identically ordered tokens in the
sentence pair. The alignment is based on exact to-
ken matching, followed by Wordnet synonyms, and
then stemmed tokens. We can calculate precision,
recall, and F-measure, where m is the number of
aligned unigrams between candidate and reference.
Meteor is defined as:
M = (1?Pen) ?F
mean
Pen = ?
(
ch
m
)
?
F
mean
=
PR
?P+(1??)R
P =
|m|
|unigrams in candidate|
R =
|m|
|unigrams in reference|
We calculated the Meteor scores using release 1.4.0
with the package-provided free parameter settings
of 0.85, 0.2, 0.6, and 0.75 for the matching compo-
nents. Meteor has not yet been reported to evaluate
Flickr 8K
co-efficient
n = 17,466
E&K (2013)
co-efficient
n = 2,040
METEOR 0.524 0.233
ROUGE SU-4 0.435 0.188
Smoothed BLEU 0.429 0.177
Unigram BLEU 0.345 0.097
TER -0.279 -0.044
Table 1: Spearman?s correlation co-efficient of au-
tomatic evaluation measures against human judge-
ments. All correlations are significant at p < 0.001.
the performance of different models on the image
description task; a higher Meteor score is better.
2.3 Protocol
We performed the correlation analysis as follows.
The sentence-level evaluation measures were cal-
culated for each image?description?reference tu-
ple. We collected the BLEU, TER, and Meteor
scores using MultEval (Clark et al, 2011), and the
ROUGE-SU4 scores using the RELEASE-1.5.5.pl
script. The evaluation measure scores were then
compared with the human judgements using Spear-
man?s correlation estimated at the sentence-level.
3 Results
Table 1 shows the correlation co-efficients between
automatic measures and human judgements and
Figures 2(a) and (b) show the distribution of scores
for each measure against human judgements. To
classify the strength of the correlations, we fol-
lowed the guidance of Dancey and Reidy (2011),
who posit that a co-efficient of 0.0?0.1 is uncor-
related, 0.11?0.4 is weak, 0.41?0.7 is moderate,
0.71?0.90 is strong, and 0.91?1.0 is perfect.
On the Flickr8k data set, all evaluation measures
can be classified as either weakly correlated or mod-
erately correlated with human judgements and all
results are significant. TER is only weakly cor-
related with human judgements but could prove
useful in comparing the types of differences be-
tween models. An analysis of the distribution of
TER scores in Figure 2(a) shows that differences in
candidate and reference length are prevalent in the
image description task. Unigram BLEU is also only
weakly correlated against human judgements, even
though it has been reported extensively for this task.
454
0 20 40 60 80 100
METEOR ?= 0.524
12
34
0.0 0.2 0.4 0.6
ROUGE-SU4 ?= 0.435
12
34
0 20 40 60 80 100
Smoothed BLEU ?= 0.429
12
34
0 20 40 60 80 100
Unigram BLEU ?= 0.345
12
34
0 100 200 300 400
TER ?= -0.279
12
34
Sentence-level automated measure score
Huma
n Jud
geme
nt
(a) Flick8K data set, n=17,466.
0 20 40 60 80 100
METEOR ?= 0.233
1
3
5
0.0 0.2 0.4 0.6 0.8
ROUGE-SU4 ?= 0.188
1
3
5
0 20 40 60 80 100
Smoothed BLEU ?= 0.177
1
3
5
40 50 60 70 80 90 100
Unigram BLEU ?= 0.0965
1
3
5
0 50 100 150
TER ?= -0.0443
1
3
5
Sentence-level automated measure score
Huma
n Jud
geme
nt
(b) E&K (2013) data set, n=2,042.
Figure 2: Distribution of automatic evaluation measures against human judgements. ? is the correlation
between human judgements and the automatic measure. The intensity of each point indicates the number
of occurrences that fall into that range.
Figure 2(a) shows an almost uniform distribution
of unigram BLEU scores, regardless of the human
judgement. Smoothed BLEU and ROUGE-SU4 are
moderately correlated with human judgements, and
the correlation is stronger than with unigram BLEU.
Finally, Meteor is most strongly correlated mea-
sure against human judgements. A similar pattern
is observed in the Elliott and Keller (2013) data set,
though the correlations are lower across all mea-
sures. This could be caused by the smaller sample
size or because the descriptions were generated
by a computer, and not retrieved from a collection
of human-written descriptions containing the gold-
standard text, as in the Flickr8K data set.
Qualitative Analysis
Figure 3 shows two images from the test collec-
tion of the Flickr8K data set with a low Meteor
score and a maximum human judgement of seman-
tic correctness. The main difference between the
candidates and references are in deciding what to
describe (content selection), and how to describe it
(realisation). We can hypothesise that in both trans-
lation and summarisation, the source text acts as a
lexical and semantic framework within which the
translation or summarisation process takes place.
In Figure 3(a), the authors of the descriptions made
different decisions on what to describe. A decision
has been made to describe the role of the officials in
the candidate text, and not in the reference text. The
underlying cause of this is an active area of research
in the human vision literature and can be attributed
to bottom-up effects, such as saliency (Itti et al,
1998), top-down contextual effects (Torralba et al,
2006), or rapidly-obtained scene properties (Oliva
and Torralba, 2001). In (b), we can see the problem
of deciding how to describe the selected content.
The reference uses a more specific noun to describe
the person on the bicycle than the candidate.
4 Discussion
There are several differences between our analysis
and that of Hodosh et al (2013). First, we report
Spearman?s ? correlation coefficient of automatic
measures against human judgements, whereas they
report agreement between judgements and auto-
matic measures in terms of Cohen?s ?. The use of
? requires the transformation of real-valued scores
into categorical values, and thus loses informa-
tion; we use the judgement and evaluation measure
scores in their original forms. Second, our use of
Spearman?s ? means we can readily use all of the
available data for the correlation analysis, whereas
Hodosh et al (2013) report agreement on thresh-
olded subsets of the data. Third, we report the corre-
lation coefficients against five evaluation measures,
455
Candidate: Football players gathering to con-
test something to collaborating officials.
Reference: A football player in red and white
is holding both hands up.
(a)
Candidate: A man is attempting a stunt with a
bicycle.
Reference: Bmx biker Jumps off of ramp.
(b)
Figure 3: Examples in the test data with low Meteor scores and the maximum expert human judgement.
(a) the candidate and reference are from the same image, and show differences in what to describe, in
(b) the descriptions are retrieved from different images and show differences in how to describe an image.
some of which go beyond unigram matchings be-
tween references and candidates, whereas they only
report unigram BLEU and unigram ROUGE. It is
therefore difficult to directly compare the results
of our correlation analysis against Hodosh et al?s
agreement analysis, but they also reach the conclu-
sion that unigram BLEU is not an appropriate mea-
sure of image description performance. However,
we do find stronger correlations with Smoothed
BLEU, skip-bigram ROUGE, and Meteor.
In contrast to the results presented here, Reiter
and Belz (2009) found no significant correlations
of automatic evaluation measures against human
judgements of the accuracy of machine-generated
weather forecasts. They did, however, find signif-
icant correlations of automatic measures against
fluency judgements. There are no fluency judge-
ments available for Flickr8K, but Elliott and Keller
(2013) report grammaticality judgements for their
data, which are comparable to fluency ratings. We
failed to find significant correlations between gram-
matlicality judgements and any of the automatic
measures on the Elliott and Keller (2013) data. This
discrepancy could be explained in terms of the dif-
ferences between the weather forecast generation
and image description tasks, or because the image
description data sets contain thousands of texts and
a few human judgements per text, whereas the data
sets of Reiter and Belz (2009) included hundreds
of texts with 30 human judges.
5 Conclusions
In this paper we performed a sentence-level corre-
lation analysis of automatic evaluation measures
against expert human judgements for the automatic
image description task. We found that sentence-
level unigram BLEU is only weakly correlated with
human judgements, even though it has extensively
reported in the literature for this task. Meteor was
found to have the highest correlation with human
judgements, but it requires Wordnet and paraphrase
resources that are not available for all languages.
Our findings held when judgements were made on
human-written or computer-generated descriptions.
The variability in what and how people describe
images will cause problems for all of the measures
compared in this paper. Nevertheless, we propose
that unigram BLEU should no longer be used as
an objective function for automatic image descrip-
tion because it has a weak correlation with human
accuracy judgements. We recommend adopting
either Meteor, Smoothed BLEU, or ROUGE-SU4 be-
cause they show stronger correlations with human
judgements. We believe these suggestions are also
applicable to the ranking tasks proposed in Hodosh
et al (2013), where automatic evaluation scores
could act as features to a ranking function.
Acknowledgments
Alexandra Birch and R. Calen Walshe, and the
anonymous reviewers provided valuable feedback
on this paper. The research is funded by ERC
Starting Grant SYNPROC No. 203427.
456
References
Jonathon H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176?181, Portland, Oregon, USA.
Christine Dancey and John Reidy, 2011. Statistics
Without Maths for Psychology, page 175. Prentice
Hall, 5th edition.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1292?1302, Seattle, Washington, U.S.A.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences from im-
ages. In Proceedings of the 11th European Confer-
ence on Computer Vision, pages 15?29, Heraklion,
Crete, Greece.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
Detection with Discriminatively Trained Part-Based
Models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(9):1627?1645.
Matthieu Guillaumin, Thomas Mensink, Jakob J. Ver-
beek, and Cornelia Schmid. 2009. Tagprop: Dis-
criminative metric learning in nearest neighbor mod-
els for image auto-annotation. In IEEE 12th Interna-
tional Conference on Computer Vision, pages 309?
316, Kyoto, Japan.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing Image Description as a Ranking
Task : Data , Models and Evaluation Metrics. Jour-
nal of Artificial Intelligence Research, 47:853?899.
Laurent Itti, Christof Koch, and Ernst Niebur. 1998.
A model of saliency-based visual attention for rapid
scene analysis. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 20(11):1254?1259.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In The 24th IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1601?1608, Colorado Springs, Col-
orado, U.S.A.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective Generation of Natural Image Descriptions.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 359?
368, Jeju Island, South Korea.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Fifteenth Conference on Computational Natural
Language Learning, pages 220?228, Portland, Ore-
gon, U.S.A.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,
Tamara Berg, and Hal Daum?e III. 2012. Midge :
Generating Image Descriptions From Computer Vi-
sion Detections. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 747?756, Avi-
gnon, France.
Aude Oliva and Antonio Torralba. 2001. Modeling the
Shape of the Scene: A Holistic Representation of
the Spatial Envelope. International Journal of Com-
puter Vision, 42(3):145?175.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In Advances in Neural In-
formation Processing Systems 24, Granada, Spain.
Ehud Reiter and A Belz. 2009. An investigation into
the validity of some metrics for automatically evalu-
ating natural language generation systems. Compu-
tational Linguistics, 35(4):529?558.
Antonio Torralba, Aude Oliva, Monica S. Castelhano,
and John M. Henderson. 2006. Contextual guid-
ance of eye movements and attention in real-world
scenes: the role of global features in object search.
Psychologial Review, 113(4):766?786.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-Guided Sentence
Generation of Natural Images. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 444?454, Edinburgh,
Scotland, UK.
457
Proceedings of the 25th International Conference on Computational Linguistics, pages 109?111,
Dublin, Ireland, August 23-29 2014.
Towards Succinct and Relevant Image Descriptions
Desmond Elliott
Institute of Language, Communication, and Computation
School of Informatics
University of Edinburgh
d.elliott@ed.ac.uk
What does it mean to produce a good description of an image? Is a description good because it
correctly identifies all of the objects in the image, because it describes the interesting attributes of the
objects, or because it is short, yet informative? Grice?s Cooperative Principle, stated as ?Make your
contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction
of the talk exchange in which you are engaged? (Grice, 1975), alongside other ideas of pragmatics in
communication, have proven useful in thinking about language generation (Hovy, 1987; McKeown et
al., 1995). The Cooperative Principle provides one possible framework for thinking about the generation
and evaluation of image descriptions.
1
The immediate question is whether automatic image description is within the scope of the Cooperative
Principle. Consider the task of searching for images using natural language, where the purpose of the
exchange is for the user to quickly and accurately find images that match their information needs. In this
scenario, the user formulates a complete sentence query to express their needs, e.g. A sheepdog chasing
sheep in a field, and initiates an exchange with the system in the form of a sequence of one-shot con-
versations. In this exchange, both participants can describe images in natural language, and a successful
outcome relies on each participant succinctly and correctly expressing their beliefs about the images. It
follows from this that we can think of image description as facilitating communication between people
and computers, and thus take advantage of the Principle?s maxims of Quantity, Quality, Relevance, and
Manner in guiding the development and evaluation of automatic image description models.
An overview of the image description literature from the perspective of Grice?s maxims can be found
in Table 1. The most apparent ommission is the lack of research devoted to generating minimally infor-
mative descriptions: the maxim of Quantity. Attending to this maxim will become increasingly important
as the quality and coverage of object, attribute, and scene detectors increases. It would be undesirable to
develop models that describe every detected object in an image because that would be likely to violate the
maxim of Quantity (Spain and Perona, 2010). Similarly, if it is possible to associate an accurate attribute
with each object in the image, it will be important to be sparing in the application of those attributes: is
it relevant to describe ?furry? sheep when there are no sheared sheep in an image?
How should image description models be evaluated with respect to the maxims of the Cooperative
Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram
BLEU and human judgements of semantic correctness (see Hodosh et al. (2013) for discussion of framing
image description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-based
measures against human judgements). The semantic correctness judgements task typically present a
variant of ?Rate the relevance of the description for this image?, which only evaluates the description vis-
`a-vis the maxim of Relevance. One exception is the study of Mitchell et al. (2012), in which judgements
about the ordering of noun phrases (the maxim of Manner) were also collected. The importance of
being able to evaluate according to multiple maxims becomes clearer as computer vision becomes more
accurate. It seems intuitive that a model that describes and relates every object in the image could
be characterised as generating Relevant and Quality descriptions, but not necessarily descriptions of
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://
creativecommons.org/licenses/by/4.0/
1
This discussion primarily applies to image descriptions, and not to image captions. See (Hodosh et al., 2013) and (Panof-
sky, 1939) for a discussion of the differences between descriptions and captions.
109
Category
Maxim Attention in the literature
Quantity
Be as informative as required ???
Do not be more informative than
required
???
Quality
Do not say what you
believe is false
All models exploit some kind of corpus data to
construct descriptions that are maximally probable
(Yang et al., 2011; Li et al., 2011; Kuznetsova et al.,
2012; Le et al., 2013). These approaches typically
use language modelling to construct hypotheses
based on the available evidence, but may eventually
be false.
Do not say that for which
you lack evidence
Relevance
Be relevant
No models try to generate irrelevant descriptions.
Dodge et al. (2012) explored the separation between
what can be seen/not seen in an image/caption pair.
Manner
Avoid obscure expressions
No model has been deliberately obscure.
Avoid ambiguity
Kulkarni et al. (2011) introduced visual attributes to
describe and distinguish objects.
Be brief ???
Be orderly
Mitchell et al. (2012) and Elliott and Keller (2013)
explicitly try to predict the best ordering of objects
in the final description.
Table 1: An overview of Grice?s maxims and the relevant image description models. ??? means that we
are unaware of any models that implicitly or explicitly claim to address this type of maxim.
adequate Quantity. It is not clear that current human judgements capture this distinction, yet the gold-
standard crowdsourced descriptions almost certainly do conform to the maxim of sufficient Quantity. A
further important consideration is how to obtain human judgements for multiple maxims without making
the studies prohibitively expensive.
Using Grice?s maxims to think about image description from the perspective of enabling effective
communication helps us reconsider the state of the art of automatic image description and directions
for future research. In particular, we identified the open problems of determining the minimum and
most relevant aspects of an image, and the challenges of conducting human evaluations along alternative
dimensions to semantic correctness.
Acknowledgments
S. Frank, D. Frassinelli, and the anonymous reviewers provided valuable feedback on this paper. The
research is funded by ERC Starting Grant SYNPROC No. 203427.
References
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, Yejin
Choi, Hal Daum?e III, Alex Berg, and Tamara Berg. 2012. Detecting visual text. In Proceedings of the 2012
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 762?772, Montr?eal, Canada.
Desmond Elliott and Frank Keller. 2013. Image Description using Visual Dependency Representations. In Pro-
ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292?1302,
Seattle, Washington, U.S.A.
110
Desmond Elliott and Frank Keller. 2014. Comparing Automatic Evaluation Measures for Image Description.
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 452?457,
Baltimore, Maryland, U.S.A.
H. Paul Grice. 1975. Logic and Conversation. In P. Cole and J. L. Morgan, editors, Syntax and Semantics 3:
Speech Arts, pages 41?58. Academic Press, Inc.
Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data,
Models and Evaluation Metrics. Journal of Artificial Intelligence Research, 47:853?899.
E Hovy. 1987. Generating natural language under pragmatic constraints. Journal of Pragmatics, 11(6):689?719.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.
2011. Baby talk: Understanding and generating simple image descriptions. In 2011 IEEE Conference on
Computer Vision and Pattern Recognition, pages 1601?1608, Colorado Springs, Colorado, U.S.A.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective
Generation of Natural Image Descriptions. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 359?368, Jeju Island, South Korea.
Dieu Thu Le, Jasper Uijlings, and Raffaella Bernardi. 2013. Exploiting language models for visual recognition.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769?779,
Seattle, Washington, U.S.A.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image
descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural
Language Learning, pages 220?228, Portland, Oregon, U.S.A.
K McKeown, J Robin, and K Kukich. 1995. Generating concise natural language summaries. Information
Processing & Management, 31(5):703?733.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Alyssa Mensch, Alex Berg, Tamara
Berg, and Hal Daum. 2012. Midge : Generating Image Descriptions From Computer Vision Detections. In
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,
pages 747?756, Avignon, France.
Erwin Panofsky. 1939. Studies in Iconology. Oxford University Press.
Merrielle Spain and Pietro Perona. 2010. Measuring and Predicting Object Importance. International Journal of
Computer Vision, 91(1):59?76.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yiannis Aloimonos. 2011. Corpus-Guided Sentence Generation
of Natural Images. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 444?454, Edinburgh, Scotland, UK.
111
