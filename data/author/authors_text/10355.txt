Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 241?248
Manchester, August 2008
Mining Opinions in Comparative Sentences 
Murthy Ganapathibhotla 
Department of Computer Science 
University of Illinois at Chicago 
851 South Morgan Street 
Chicago, IL 60607-7053 
sganapat@cs.uic.edu 
Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
851 South Morgan Street 
Chicago, IL 60607-7053 
liub@cs.uic.edu 
 
 
Abstract 
This paper studies sentiment analysis 
from the user-generated content on the 
Web. In particular, it focuses on mining 
opinions from comparative sentences, i.e., 
to determine which entities in a compari-
son are preferred by its author. A typical 
comparative sentence compares two or 
more entities. For example, the sentence, 
?the picture quality of Camera X is better 
than that of Camera Y?, compares two 
entities ?Camera X? and ?Camera Y? 
with regard to their picture quality. Clear-
ly, ?Camera X? is the preferred entity. 
Existing research has studied the problem 
of extracting some key elements in a 
comparative sentence. However, there is 
still no study of mining opinions from 
comparative sentences, i.e., identifying 
preferred entities of the author. This pa-
per studies this problem, and proposes a 
technique to solve the problem. Our ex-
periments using comparative sentences 
from product reviews and forum posts 
show that the approach is effective. 
1 Introduction 
In the past few years, there was a growing inter-
est in mining opinions in the user-generated con-
tent (UGC) on the Web, e.g., customer reviews, 
forum posts, and blogs. One major focus is sen-
timent classification and opinion mining (e.g., 
Pang et al2002; Turney 2002; Hu and Liu 2004; 
Wilson et al2004; Kim and Hovy 2004; Popescu 
and Etzioni 2005) 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
However, these studies mainly center on direct 
opinions or sentiments expressed on entities. Lit-
tle study has been done on comparisons, which 
represent another type of opinion-bearing text. 
Comparisons are related to but are also quite dif-
ferent from direct opinions. For example, a typi-
cal direct opinion sentence is ?the picture quality 
of Camera X is great?, while a typical compara-
tive sentence is ?the picture quality of Camera X 
is better than that of Camera Y.? We can see that 
comparisons use different language constructs 
from direct opinions. A comparison typically 
expresses a comparative opinion on two or more 
entities with regard to their shared features or 
attributes, e.g., ?picture quality?. Although direct 
opinions are most common in UGC, comparisons 
are also widely used (about 10% of the sen-
tences), especially in forum discussions where 
users often ask questions such as ?X vs. Y? (X 
and Y are competing products). Discussions are 
then centered on comparisons.  
Jindal and Liu (2006) proposed a technique to 
identify comparative sentences from reviews and 
forum posts, and to extract entities, comparative 
words, and entity features that are being com-
pared. For example, in the sentence, ?Camera X 
has longer battery life than Camera Y?, the 
technique extracts ?Camera X? and ?Camera Y? 
as entities, and ?longer? as the comparative 
word and ?battery life? as the attribute of the 
cameras being compared. However, the tech-
nique does not find which entity is preferred by 
the author. For this example, clearly ?Camera Y? 
is the preferred camera with respect to the ?bat-
tery life? of the cameras. This paper aims to 
solve this problem, which is useful in many ap-
plications because the preferred entity is the key 
piece of information in a comparative opinion. 
For example, a potential customer clearly wants 
to buy the product that is better or preferred.  
In this work, we treat a sentence as the basic 
241
information unit. Our objective is thus to identify 
the preferred entity in each comparative sentence. 
A useful observation about comparative sen-
tences is that in each such sentence there is 
usually a comparative word (e.g., ?better?, 
?worse? and ?er word) or a superlative word 
(e.g., ?best?, ?worst? and ?est word). The entities 
being compared often appear on the two sides of 
the comparative word. A superlative sentence 
may only have one entity, e.g., ?Camera X is the 
best?. For simplicity, we use comparative words 
(sentences) to mean both comparative words 
(sentences) and superlative words (sentences). 
Clearly, the preferred entity in a comparative 
sentence is mainly determined by the compara-
tive word in the sentence. Some comparative 
words explicitly indicate user preferences, e.g., 
?better?, ?worse?, and ?best?. We call such 
words opinionated comparative words. For ex-
ample, in the sentence, ?the picture quality of 
Camera X is better than that of Camera Y?, 
Camera X is preferred due to the opinionated 
comparative word ?better?.  
However, many comparative words are not 
opinionated, or their opinion orientations (i.e., 
positive or negative) depend on the context 
and/or the application domain. For instance, the 
word ?longer? is not opinionated as it is normal-
ly used to express that the length of some feature 
of an entity is greater than the length of the same 
feature of another entity. However, in a particular 
context, it can express a desired (or positive) or 
undesired (or negative) state. For example, in the 
sentence, ?the battery life of Camera X is longer 
than Camera Y?, ?longer? clearly expresses a 
desired state for ?battery life? (although this is an 
objective sentence with no explicit opinion). 
?Camera X? is thus preferred with regard to 
?battery life? of the cameras. The opinion in this 
sentence is called an implicit opinion. We also 
say that ?longer? is positive in this context. We 
know this because of our existing domain know-
ledge. However, ?longer? may also be used to 
express an undesirable state in a different context, 
e.g., ?Program X?s execution time is longer than 
Program Y?. longer? is clearly negative here. 
?Program Y? is thus preferred. We call compara-
tive words such as ?longer? and ?smaller? con-
text-dependent opinion comparatives.  
Sentences with opinionated words (e.g., ?bet-
ter?, and ?worse?) are usually easy to handle. 
Then the key to solve our problem is to identify 
the opinion orientations (positive or negative) of 
context-dependent comparative words. To this 
end, two questions need to be answered: (1) what 
is a context and (2) how to use the context to 
help determine the opinion orientation of a com-
parative word?  
The simple answer to question (1) is the whole 
sentence. However, a whole sentence as context 
is too complex because it may contain too much 
irrelevant information, which can confuse the 
system. Intuitively, we want to use the smallest 
context that can determine the orientation of the 
comparative word. Obviously, the comparative 
word itself must be involved. We thus conjecture 
that the context should consist of the entity fea-
ture being compared and the comparative word. 
Our experimental results show that this context 
definition works quite well.   
To answer the second question, we need ex-
ternal information or knowledge because there is 
no way that a computer program can solve the 
problem by analyzing the sentence itself. In this 
paper, we propose to use the external information 
in customer reviews on the Web to help solve the 
problem. There are a large number of such re-
views on almost any product or service. These 
reviews can be readily downloaded from many 
sites. In our work, we use reviews from epi-
nions.com. Each review in epinions.com has sep-
arate Pros and Cons (which is also the case in 
most other review sites). Thus, positive and 
negative opinions are known as they are sepa-
rated by reviewers. However, they cannot be 
used directly because Pros and Cons seldom con-
tain comparative words. We need to deal with 
this problem. Essentially, the proposed method 
computes whether the comparative word and the 
feature are more associated in Pros or in Cons. If 
they are more associated in Pros (or Cons) than 
Cons (or Pros), then the comparative word is 
likely to be positive (or negative) for the feature. 
A new association measure is also proposed to 
suit our purpose. Our experiment results show 
that it can achieve high precision and recall.  
2 Related Work 
Sentiment analysis has been studied by many 
researchers recently. Two main directions are 
sentiment classification at the document and sen-
tence levels, and feature-based opinion mining. 
Sentiment classification at the document level 
investigates ways to classify each evaluative 
document (e.g., product review) as positive or 
negative (Pang et al2002; Turney 2002). Senti-
ment classification at the sentence-level has also 
been studied (e.g., Riloff and Wiebe 2003; Kim 
and Hovy 2004; Wilson et al2004; Gamon et al
242
2005; Stoyanov and Cardie 2006). These works 
are different from ours as we study comparatives.    
The works in (Hu and Liu 2004; Liu et al2005; 
Popescu and Etzioni 2005; Mei et al2007) per-
form opinion mining at the feature level. The 
task involves (1) extracting entity features (e.g., 
?picture quality? and ?battery life? in a camera 
review) and (2) finding orientations (positive, 
negative or neutral) of opinions expressed on the 
features by reviewers. Again, our work is differ-
ent because we deal with comparisons.  
Discovering orientations of context dependent 
opinion comparative words is related to identify-
ing domain opinion words (Hatzivassiloglou and 
McKeown 1997; Kanayama and Nasukawa 
2006). Both works use conjunction rules to find 
such words from large domain corpora. One con-
junction rule states that when two opinion words 
are linked by ?and?, their opinions are the same. 
Our method is different in three aspects. First, we 
argue that finding domain opinion words is prob-
lematic because in the same domain the same 
word may indicate different opinions depending 
on what features it is applied to. For example, in 
the camera domain, ?long? is positive in ?the 
battery life is very long? but negative in ?it takes 
a long time to focus?. Thus, we should consider 
both the feature and the opinion word rather than 
only the opinion word. Second, we focus on 
studying opinionated comparative words. Third, 
our technique is quite different as we utilize rea-
dily available external opinion sources.  
As discussed in the introduction, a closely re-
lated work to ours is (Jindal and Liu 2006). 
However, it does not find which entities are pre-
ferred by authors. Bos and Nissim (2006) pro-
poses a method to extract some useful items from 
superlative sentences. Fiszman et al(2007) stu-
died the problem of identifying which entity has 
more of certain features in comparative sen-
tences. It does not find which entity is preferred.  
3 Problem Statement 
Definition (entity and feature): An entity is the 
name of a person, a product, a company, a lo-
cation, etc, under comparison in a compara-
tive sentence. A feature is a part or attribute 
of the entity that is being compared. 
For example, in the sentence, ?Camera X?s bat-
tery life is longer than that of Camera Y?, ?Cam-
era X? and ?Camera Y? are entities and ?battery 
life? is the camera feature.  
Types of Comparatives 
1)  Non-equal gradable: Relations of the type 
greater or less than that express a total order-
ing of some entities with regard to their 
shared features. For example, the sentence, 
?Camera X?s battery life is longer than that of 
Camera Y?, orders ?Camera X? and ?Camera 
Y? based on their shared feature ?battery life?.  
2)  Equative: Relations of the type equal to that 
state two objects as equal with respect to 
some features, e.g., ?Camera X and Camera Y 
are about the same size?.  
3)  Superlative: Relations of the type greater or 
less than all others that rank one object over 
all others, ?Camera X?s battery life is the 
longest?. 
4)  Non-gradable: Sentences which compare fea-
tures of two or more entities, but do not expli-
citly grade them, e.g., ?Camera X and Cam-
era Y have different features?  
The first three types are called gradable compar-
atives. This paper focuses on the first and the 
third types as they express ordering relationships 
of entities. Equative and non-gradable sentences 
usually do not express preferences.  
Definition (comparative relation): A compara-
tive relation is the following:  
<ComparativeWord, Features, EntityS1, EntityS2, Type> 
ComparativeWord is the keyword used to ex-
press a comparative relation in the sentence. Fea-
tures is a set of features being compared. En-
tityS1 and EntityS2 are sets of entities being 
compared. Entities in EntityS1 appear on the left 
of the comparative word and entities in EntityS2 
appear on the right. Type is non-equal gradable, 
equative or superlative. Let us see an example. 
For the sentence ?Camera X has longer battery 
life than Camera Y,? the extracted relation is:  
<longer, {battery life}, {Camera X}, {Camera Y}, 
non-equal gradable>.  
We assume that the work in (Jindal and Liu 2006) 
has extracted the above relation from a compara-
tive sentence. In this work, we aim to identify the 
preferred entity of the author, which is not stu-
died in (Jindal and Liu 2006).  
Our objective: Given the extracted comparative 
relation from a comparative sentence, we want 
to identify whether the entities in EntityS1 or 
in EntityS2 are preferred by the author.  
4 Proposed Technique 
We now present the proposed technique. As dis-
cussed above, the primary determining factors of 
the preferred entity in a comparative sentence are 
243
the feature being compared and the comparative 
word, which we conjecture, form the context for 
opinions (or preferred entities). We develop our 
ideas from here.  
4.1 Comparatives and superlatives 
In English, comparatives and superlatives are 
special forms of adjectives and adverbs. In gen-
eral, comparatives are formed by adding the suf-
fix ?-er? and superlatives are formed by adding 
the suffix ??est? to the base adjectives and ad-
verbs. We call this type of comparatives and su-
perlatives Type 1 comparatives and superlatives. 
For simplicity, we will use Type 1 comparatives 
to represent both from now on.  
Adjectives and adverbs with two syllables or 
more and not ending in y do not form compara-
tives or superlatives by adding ??er? or ??est?. 
Instead, ?more?, ?most?, ?less? and ?least? are 
used before such words, e.g., ?more beautiful?. 
We call this type of comparatives and superla-
tives Type 2 comparatives and Type 2 superla-
tives. These two types are called regular com-
paratives and superlatives respectively.  
In English, there are also some irregular com-
paratives and superlatives, which do not follow 
the above rules, i.e., ?more?, ?most?, ?less?, 
?least?, ?better?, ?best?, ?worse?, ?worst?, ?fur-
ther/farther? and ?furthest/farthest?. They be-
have similarly to Type 1 comparatives and super-
latives and thus are grouped under Type 1.  
Apart from these comparatives and superla-
tives, there are non-standard words that express 
gradable comparisons, e.g., ?prefer?, and ?supe-
rior?. For example, the sentence, ?in term of bat-
tery life, Camera X is superior to Camera Y?, 
says that ?Camera X? is preferred. We obtained a 
list of 27 such words from (Jindal and Liu 2006) 
(which used more words, but most of them are 
not used to express gradable comparisons). Since 
these words behave similarly to Type 1 compara-
tives, they are thus grouped under Type 1. 
Further analysis also shows that we can group 
comparatives into two categories according to 
whether they express increased or decreased val-
ues: 
Increasing comparatives: Such a comparative 
expresses an increased value of a quantity, e.g., 
?more?, and ?longer?.  
Decreasing comparatives: Such a comparative 
expresses a decreased value of a quantity, e.g., 
?less?, and ?fewer?.  
As we will see later, this categorization is very 
useful in identifying the preferred entity.  
Since comparatives originate from adjectives 
and adverbs, they may carry positive or negative 
sentiments/opinions. Along this dimension, we 
can divide them into two categories.   
1.  Opinionated comparatives: For Type 1 com-
paratives, this category contains words such 
as "better", "worse", etc, which has explicit 
opinions. In sentences involving such words, 
it is normally easy to determine which entity 
is the preferred one of the sentence author.  
In the case of Type 2 comparatives, formed 
by adding ?more?, ?less?, ?most?, and ?least? 
before adjectives or adverbs, the opinion (or 
preferred entity) is determined by both words. 
The following rules apply: 
?increasing comparative? Negative  ?  Negative Opinion 
?increasing comparative? Positive   ?  Positive Opinion 
?decreasing comparative? Negative ?  Positive Opinion 
?decreasing comparative? Positive  ?  Negative Opinion 
 The first rule says that the combination of an 
increasing comparative word (e.g., ?more?) 
and a negative opinion adjective/adverb (e.g., 
?awful?) implies a negative Type 2 compara-
tive. The other rules are similar. These rules 
are intuitive and will not be discussed further.  
2.  Comparatives with context-dependent opi-
nions: These comparatives are used to com-
pare gradable quantities of entities. In the case 
of Type 1 comparatives, such words include 
?higher?, ?lower?, etc. Although they do not 
explicitly describe the opinion of the author, 
they often carry implicit sentiments or prefe-
rences based on contexts. For example, in 
?Car X has higher mileage per gallon than 
Car Y?, it is hard to know whether ?higher? is 
positive or negative without domain know-
ledge. It is only when the two words, ?higher? 
and ?mileage?, are combined we know that 
?higher? is desirable for ?mileage? from our 
domain knowledge.  
In the case of Type 2 comparatives, the sit-
uation is similar. However, the comparative 
word (?more?, ?most?, ?less? or ?least?), the 
adjective/adverb and the feature are all impor-
tant in determining the opinion or the prefe-
rence. If we know whether the comparative 
word is increasing or decreasing (which is 
easy since there are only four such words), 
then the opinion can be determined by apply-
ing the four rules above in (1).  
For this work, we used the opinion word list 
from (Hu and Liu 2004), which was compiled 
using a bootstrapping approach based on Word-
Net. For opinionated comparatives, due to the 
observation below we simply convert the opinion 
244
adjectives/adverbs to their comparative forms, 
which is done automatically based on grammar 
(comparative formation) rules described above 
and WordNet. 
Observation: If a word is positive (or negative), 
then its comparative or superlative form is al-
so positive (or negative), e.g., ?good?, ?bet-
ter? and ?best?.  
After the conversion, these words are manually 
categorized into increasing and decreasing com-
paratives. Although this consumes some time, it 
is only a one-time effort.  
4.2 Contexts 
To deal with comparatives with context depen-
dent opinions, we need contexts. It is conjectured 
that the comparative and the feature in the sen-
tence form the context. This works very well. For 
a Type 2 comparative, we only need the feature 
and the adjective/adverb to form a context. For 
example, in the sentence, ?Program X runs more 
quickly than Program Y?, the context is the pair, 
(?run?, ?quickly?), where ?run? is a verb feature. 
If we find out that (?run?, ?quickly?) is positive 
based on some external information, we can con-
clude that ?Program X? is preferred using one of 
the four rules above since ?more? is an increas-
ing comparative.  
We will use such contexts to find opinion 
orientations of comparatives with regard to some 
features from the external information, i.e., Pros 
and Cons in online reviews. 
4.3 Pros and Cons in Reviews 
Figure 1 shows a popular review format. The 
reviewer first describes Pros and Cons briefly, 
and then writes a full review.  
Pros and Cons are used in our work for two 
main reasons. First, the brief information in Pros 
and Cons contains the essential information re-
lated to opinions. Each phrase or sentence seg-
ment usually contains an entity feature and an 
opinion word. Second, depending on whether it 
is in Pros or in Cons, the user opinion on the 
product feature is clear.  
To use the Pros and Cons phrases, we separate 
them use punctuations and words, i.e., ?,?, ?.?, 
?and?, and ?but?. Pros in Figure 1 can be sepa-
rated into 5 phrases or segments,  
great photos  <photo>   
easy to use    <use> 
good manual  <manual> 
many options <option> 
takes videos <video> 
We can see that each segment describes an entity 
feature on which the reviewer has expressed an 
opinion. The entity feature for each segment is 
listed within <>. 
4.4 Identifying Preferred Entities: The Al-
gorithm 
Since we use Pros and Cons as the external in-
formation source to help determine whether the 
combination of a comparative and an entity fea-
ture is positive or negative, we need to find com-
parative and entity features words in Pros and 
Cons. However, in Pros and Cons, comparatives 
are seldom used (entity features are always 
there). Thus we need to first convert compara-
tives to their base forms. This can be done auto-
matically using WordNet and grammar rules de-
scribed in Section 4.1. We will not discuss the 
process here as it is fairly straightforward.     
We now put everything together to identify the 
preferred entity in a comparative sentence. For 
easy reference, we denote the comparative word 
as C and the feature being compared as F. After 
obtaining the base forms of C, we work on two 
main cases for the two types of comparatives:  
Case 1. Type 1 Comparative or Superlative: 
There are four sub-cases.  
1.A. C is opinionated: If the comparative or su-
perlative C has a positive orientation (e.g., 
?better?), EntityS1 (which appears before C 
in the sentence) is temporarily assigned as the 
preferred entity. Otherwise, EntityS2 is as-
signed as the preferred entity. The reason for 
the temporary assignment is that the sentence 
may contain negations, e.g., ?not?, which is 
discussed below.   
1.B. C is not opinionated but F is opinionated: 
An example is, ?Car X generates more noise 
than Car Y?, which has the feature F ?noise?, 
a negative noun. If the orientation of F is 
positive and C is an increasing comparative 
word, we assign EntityS1 as the preferred ent-
ity. Otherwise, we assign EntityS2 as the pre-
ferred entity. The possibilities are listed as 
four rules below, which are derived from the 
4 rules earlier: 
?increasing C? + Positive ? EntityS1 preferred 
?decreasing C? + Positive ? EntityS2 preferred 
 
Figure 1: An example review  
245
?increasing C? + Negative ? EntityS2 preferred 
?decreasing C? + Negative ? EntityS1 preferred 
?Positive? and ?Negative? stand for the orien-
tation of feature F being positive and negative 
respectively.  
1.C. Both C and F are not opinionated: In this 
case, we need external information to identify 
the preferred entity. We use phrases in Pros 
and Cons from reviews.  
In this case, we look for the feature F and 
comparative word C, (i.e., the context) in the 
list of phrases in Pros and Cons. In order to 
find whether the combination of C and F indi-
cates a positive or negative opinion, we com-
pute their associations in Pros and in Cons. If 
they are more associated in Pros than in Cons, 
we conclude that the combination indicates a 
positive sentiment, and otherwise a negative 
sentiment. The result decides the preferred 
entity. Point-wise mutual information (PMI) 
is commonly used for computing the associa-
tion of two terms (e.g., Turney 2002), which 
is defined as:  
?????, ?? ? ???
????, ??
??????????
. 
However, we argue that PMI is not a suitable 
measure for our purpose. The reason is that 
PMI is symmetric in the sense that PMI(F, C) 
is the same as PMI(C, F). However, in our 
case, the feature F and comparative word C 
association is not symmetric because although 
a feature is usually modified by a particular 
adjective word, the adjective word can modify 
many other features. For example, ?long? can 
be used in ?long lag?, but it can also be used 
in ?long battery life?, ?long execution time? 
and many others. Thus, this association is 
asymmetric. We are more interested in the 
conditional probability of C (including its 
synonyms) given F, which is essentially the 
confidence measure in traditional data mining. 
However, confidence does not handle well the 
situation where C occurs frequently but F ap-
pears rarely. In such cases a high conditional 
probability Pr(C|F) may just represent some 
pure chance, and consequently the resulting 
association may be spurious. We propose the 
following measure, which we call one-side 
association (OSA), and it works quite well: 
?????, ?? ? ???
????, ??????|??
??????????
 
The difference between OSA and PMI is the 
conditional probability Pr(C|F) used in OSA, 
which biases the mutual association of F and 
C to one side.  
Given the comparative word C and the fea-
ture F, we first compute an OSA value for 
positive, denoted by OSAP(F, C), and then 
compute an OSA value for negative, denoted 
by OSAN(F, C). The decision rule is simply 
the following: 
If OSAP(F, C) ? OSAN(F, C) ? 0 then  
EntityS1 is preferred 
Otherwise,  EntityS2 is preferred 
Computing OSAP(F, C): We need to compute 
PrP(F, C), for which we need to count the 
number of times that comparative word C and 
the feature F co-occur. Instead of using C 
alone, we also use its base forms and syn-
onyms and antonyms. Similarly, for F, we al-
so use its synonyms. If C (or a synonym of C) 
and F (or a synonym) co-occur in a Pros 
phrase, we count 1. If an antonym of C and F 
(or a synonym) co-occur in a Cons phrase, we 
also count 1. Thus, although we only evaluate 
for positive, we actually use both Pros and 
Cons. This is important because it allows us 
to find more occurrences to produce more re-
liable results. Synonyms and antonyms are 
obtained from WordNet. Currently, synonyms 
and antonyms are only found for single word 
features.  
We then count the number of occurrences of 
the comparative word C and the feature F 
separately in both Pros and Cons to compute 
PrP(F) and PrP(C). In counting the number of 
occurrences of C, we consider both its syn-
onyms in Pros and antonyms in Cons. In 
counting the number of occurrences of F, we 
consider its synonyms in both Pros and Cons.  
Computing OSAN(F, C): To compute PrN(F, 
C), we use a similar strategy as for computing 
PrP(F, C). In this case, we start with Cons.  
1.D. C is a feature indicator: An example sen-
tence is ?Camera X is smaller than Camera 
Y?, where ?smaller? is the feature indicator 
for feature ?size?. In this case, we simply 
count the number of times (denoted by n+) 
that C appears in Pros and the number of 
times (denoted by n-) that C appears in Cons. 
If n+ ? n-, we temporarily assign EntityS1 as 
the preferred entity. Otherwise, we assign En-
tityS2 as the preferred entity. Note that in 
some sentences, the entity features do not ap-
pear explicitly in the sentences but are im-
plied. The words that imply the features are 
called feature indicators.  
246
Case 2: Type 2 Comparative or Superlative: 
There are two sub-cases: 
2.A. Adjective/adverb in the comparison is opi-
nionated: In this case, the feature F is not im-
portant. An example sentence is: 
?Car X has more beautiful interior than Car Y?, 
?more? is an increasing comparative, and 
?beautiful? is the adjective with a positive 
orientation (the feature F is ?interior?). ?Car 
X? is clearly preferred in this case.  
Another example is: ?Car X is more beautiful 
than Car Y?. In this case, ?beautiful? is a fea-
ture indicator for the feature ?appearance?. 
Again, ?Car X? is preferred. This sub-case 
can be handled similarly as case 1.B. 
2.B. adjective/adverb in the comparison is not 
opinionated: If the adjective/adverb in com-
parison is a feature indicator, we can use the 
method in 1.D. Otherwise, we form a context 
using the feature and adjective/adverb, and 
apply the method in 1.C. We then combine 
the result with the comparative word before 
the adjective/adverb to decide based on the 
rules in 1.B.  
Negations: The steps above temporarily deter-
mine which entity is the preferred entity. How-
ever, a comparative sentence may contain a ne-
gation word or phrase (we have compiled 26 of 
them), e.g., ?Camera X?s battery life is not long-
er than that of Camera Y.? Without considering 
?not?, ?Camera X? is preferred. After consider-
ing ?not?, we assign the preferred entity to 
?Camera Y?. This decision may be problematic 
because ?not longer? does not mean ?shorter? 
(thus it can also be seen to have no preference). 
5 Evaluation 
A system, called PCS (Preferred entities in 
Comparative Sentences), has been implemented 
based the proposed method. Since there is no 
existing system that can perform the task, we 
could not compare with an existing approach. 
Below, we first describe the evaluation datasets 
and then present the results. 
5.1 Evaluation Datasets 
Our comparative sentence dataset consists of two 
subsets. The first subset is from (Jindal and Liu 
2006), which are product review and forum dis-
cussion sentences on digital cameras, DVD play-
ers, MP3 players, Intel vs AMD, Coke vs Pepsi, 
and Microsoft vs Google. The original dataset 
used in (Jindal and Liu 2006) also contains many 
non-gradable comparative sentences, which are 
not used here as most such sentences do not ex-
press any preferences.  
To make the data more diverse, we collected 
more forum discussion data about mobile phones 
from http://www.howardforums.com/, and re-
views from amazon.com and cnet.com on prod-
ucts such as laptops, cameras and mobile phones. 
Table 1 gives the number of sentences from these 
two sources. Although we only have 837 com-
parative sentences, they were collected from 
thousands of sentences in reviews and forums. 
About 10% of the sentences from them are com-
parative sentences.  
Skewed Distribution: An interesting observa-
tion about comparative sentences is that a large 
proportion (based on our data) of them (84%) has 
EntityS1 as the preferred entity. This means that 
when people make comparisons, they tend to put 
the preferred entities first.  
Pros and Cons corpus: The Pros and Cons 
corpus was crawled from reviews of epi-
nions.com. It has 15162 Pros and 15162 Cons 
extracted from 15162 reviews of three types of 
products, i.e., digital cameras (8479), and prin-
ters (5778), and Strollers (905).  
Table 1. Sentences from different sources 
Data Sources No. of Comparative Sentences
(Jindal and Liu 2006) 418 
Reviews and forum posts 419 
Total 837 
5.2 Results 
The results on the whole dataset are given in Ta-
ble 2. Note that 84% of the sentences have En-
tityS1 as the preferred entity. If a system does 
nothing but simply announces that EntityS1 is 
preferred, we will have the accuracy of 84%. 
However, PCS using the OSA measure achieves 
the accuracy of 94.4%, which is much better than 
the baseline of taking the majority. Since in 
skewed datasets accuracy does not reflect the 
prediction well, we will mainly use precision 
(Prec.), recall (Rec.) and F-score (F) in evalua-
tion. For the case that EntityS1 is preferred, the 
algorithm does extremely well. For the case that 
EntityS2 is preferred, the algorithm also does 
well although not as well as for the EntityS1 case. 
Based on our observation, we found that in such 
cases, the sentences are usually more complex.  
Next, we compare with the case that the sys-
tem does not use Pros and Cons (then OSA or 
PMI is not needed) (row 2). When a sentence 
requires context dependency handling, the sys-
tem simply takes the majority as the default, i.e., 
247
assigning EntityS1 as the preferred entity. From 
the results in Table 2, we can see that F-scores 
are all worse. In the case that EntityS1 is the pre-
ferred entity, taking defaults is not so bad, which 
is not surprising because of the skewed data dis-
tribution. Even in this case, the precision im-
provement of PCS(OSA) is statistically signifi-
cant at the 95% confidence level. The recall is 
slight less but their difference is not statistically 
significant. When EntityS2 is the preferred entity, 
its F-score (row 2) is much worse, which shows 
that our technique is effective. The recall im-
provement of PCS (OSA) is dramatic (statistical-
ly significant at the 95% confidence level). The 
two precisions are not statistically different. For 
OSA vs. PMI, see below.  
Table 2: Preferred entity identification: whole data 
 
 
EntityS1 Preferred EntityS2 Preferred
Prec. Rec. F Prec. Rec. F
PCS (OSA) 0.967 0.966 0.966 0.822 0.828 0.825
PCS: No Pros & 
Cons 0.925 0.980 0.952 0.848 0.582 0.690
PCS (PMI) 0.967 0.961 0.964 0.804 0.828 0.816
Now let us look at only the 187 sentences that 
need context dependency handling. The data is 
still skewed. 72.2% of the sentences have En-
tityS1 as the preferred entities. Table 3 shows the 
results of PCS with and without using Pros and 
Cons. The results of PCS without Pros and Cons 
(OSA or PMI is not needed) are based on assign-
ing EntityS1 as preferred for every sentence (tak-
ing the majority). Again, we can see that using 
external Pros and Cons (PCS(OSA)) helps dra-
matically. Not surprisingly, the improvements 
are statistically significant except the recall when 
EntityS1 is preferred.   
Table 3: Preferred entity identification with 187 
sentences that need context dependency handling 
 
 
EntityS1 Preferred EntityS2 Preferred
Prec. Rec. F Prec. Rec. F
PCS (OSA) 0.896 0.877 0.886 0.696 0.736 0.716
PCS: No Pros & 
Cons 0.722 1.000 0.839 0.000 0.000 0.000
PCS (PMI) 0.894 0.855 0.874 0.661 0.736 0.696
OSA vs. PMI: Comparing PCS(OSA) with PCS 
(PMI) (Table 3), OSA is better in F-score when 
EntityS1 is preferred by 1.2%, and better in F-
score when EntityS2 is preferred by 2%. Al-
though OSA?s improvements over PMI are not 
large, we believe that in principle OSA is a more 
suitable measure. Comparing with PMI when the 
whole dataset is used (Table 2), OSA?s gains are 
less because the number of sentences requiring 
context dependency handling is small (22%).  
6 Conclusions 
This paper studied sentiments expressed in com-
parative sentences. To our knowledge, no work 
has been reported on this topic. This paper pro-
posed an effective method to solve the problem, 
which also deals with context based sentiments 
by exploiting external information available on 
the Web. To use the external information, we 
needed a measure of association of the compara-
tive word and the entity feature. A new measure, 
called one-side association (OSA), was then pro-
posed. Experimental results show that the tech-
nique produces accurate results. 
References 
Bos, J. and Nissim, M. An Empirical Approach to the 
Interpretation of Superlatives. EMNLP-06, 2006. 
Esuli, A and Sebastiani, F. Determining Term Subjec-
tivity and Term Orientation for Opinion Mining, 
EACL?06, 2006. 
Fiszman, M., Demner-Fushman, D., Lang, F., Goetz, 
P., and Rindflesch, T. Interpreting Comparative 
Constructions in Biomedical Text. BioNLP, 2007.  
Gamon, M., Aue, A., Corston-Oliver, S. and Ringger, 
E.K. Pulse: Mining customer opinions from free 
text. IDA?2005. 
Hatzivassiloglou, V. and McKeown, K. Predicting the 
Semantic Orientation of Adjectives. ACL-EACL?97.  
Hu, M and Liu, B. Mining and summarizing customer 
reviews. KDD?04, 2004.  
Jindal, N. and Liu, B. Mining Comparative Sentences 
and Relations. AAAI?06, 2006. 
Kanayama, H and Nasukawa, T. Fully automatic lex-
icon expansion for domain-oriented sentiment anal-
ysis. EMNLP?06. 
Kim, S. and Hovy, E. Determining the Sentiment of 
Opinions. COLING?04, 2004.  
Liu, B, Hu, M. and Cheng, J. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. 
WWW?05, 2005.  
Mei, Q., Ling, X., Wondra, W., Su, H. and Zhai, C. 
Topic Sentiment Mixture: Modeling Facets and 
Opinions in Weblogs. WWW?07, 2007. 
Pang, B., Lee, L. and Vaithyanathan, S. Thumbs up? 
Sentiment Classification Using Machine Learning 
Techniques. EMNLP?02, 2002.  
Popescu, A.-M. and Etzioni, O. Extracting Product 
Features and Opinions from Reviews. EMNLP?05.  
Riloff, E & Wiebe, J. Learning extraction patterns for 
subjective expressions. EMNLP?03, 2003.  
Stoyanov, V. and Cardie, C. Toward opinion summa-
rization: Linking the sources. In Proc. of the Work-
shop on Sentiment and Subjectivity in Text, 2006. 
Turney, P. Thumbs Up or Thumbs Down? Semantic 
Orientation Applied to Unsupervised Classification 
of Reviews.ACL-2002.  
Wiebe, J. and Mihalcea, R. Word Sense and Subjec-
tivity. ACL?06, 2006.  
Wilson, T., Wiebe, J. and Hwa, R. Just how mad are 
you? Finding strong and weak opinion clauses. 
AAAI?04, 2004.  
248
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 180?189,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Sentiment Analysis of Conditional Sentences 
 
 
Ramanathan Narayanan 
Dept. of EECS 
Northwestern University 
ramanathan.an@gmail.com 
Bing Liu * 
Dept. of Computer Science
Univ. of Illinois at Chicago
liub@cs.uic.edu 
Alok Choudhary 
Dept. of EECS 
Northwestern University 
alokchoudhary01@gmail.com
 
 
 
 
Abstract 
This paper studies sentiment analysis of condi-
tional sentences. The aim is to determine 
whether opinions expressed on different topics 
in a conditional sentence are positive, negative 
or neutral. Conditional sentences are one of the 
commonly used language constructs in text. In 
a typical document, there are around 8% of 
such sentences. Due to the condition clause, 
sentiments expressed in a conditional sentence 
can be hard to determine. For example, in the 
sentence, if your Nokia phone is not good, buy 
this great Samsung phone, the author is posi-
tive about ?Samsung phone? but does not ex-
press an opinion on ?Nokia phone? (although 
the owner of the ?Nokia phone? may be nega-
tive about it). However, if the sentence does 
not have ?if?, the first clause is clearly nega-
tive. Although ?if? commonly signifies a con-
ditional sentence, there are many other words 
and constructs that can express conditions. 
This paper first presents a linguistic analysis of 
such sentences, and then builds some super-
vised learning models to determine if senti-
ments expressed on different topics in a condi-
tional sentence are positive, negative or neu-
tral. Experimental results on conditional sen-
tences from 5 diverse domains are given to 
demonstrate the effectiveness of the proposed 
approach. 
1 Introduction  
Sentiment analysis (also called opinion mining) 
has been an active research area in recent years. 
There are many research directions, e.g., senti-
ment classification (classifying an opinion doc-
ument as positive or negative) (e.g., Pang, Lee 
and Vaithyanathan, 2002; Turney, 2002), subjec-
tivity classification (determining whether a sen-
tence is subjective or objective, and its associated 
opinion) (Wiebe and Wilson, 2002; Yu and Hat-
zivassiloglou, 2003; Wilson et al 2004; Kim and 
Hovy, 2004; Riloff and Wiebe, 2005), fea-
ture/topic-based sentiment analysis (assigning 
positive or negative sentiments to topics or prod-
uct features) (Hu and Liu 2004; Popescu and Et-
zioni, 2005; Carenini et al, 2005; Ku et al, 
2006; Kobayashi, Inui and Matsumoto, 2007; 
Titov and McDonald. 2008). Formal definitions 
of different aspects of the sentiment analysis 
problem and discussions of major research direc-
tions and algorithms can be found in (Liu, 2006; 
Liu, 2009). A comprehensive survey of the field 
can be found in (Pang and Lee, 2008).  
Our work is in the area of topic/feature-based 
sentiment analysis or opinion mining (Hu and 
Liu, 2004). The existing research focuses on 
solving the general problem. However, we argue 
that it is unlikely to have a one-technique-fit-all 
solution because different types of sentences ex-
press sentiments/opinions in different ways. A 
divide-and-conquer approach is needed, e.g., fo-
cused studies on different types of sentences. 
This paper focuses on one type of sentences, i.e., 
conditional sentences, which have some unique 
characteristics that make it hard to determine the 
orientation of sentiments on topics/features in 
such sentences. By sentiment orientation, we 
mean positive, negative or neutral opinions. By 
topic, we mean the target on which an opinion 
has been expressed. In the product domain, a top-
ic is usually a product feature (i.e., a component 
or attribute). For example, in the sentence, I do 
not like the sound quality, but love the design of 
this MP3 player, the product features (topics) are 
?sound quality? and ?design? of the MP3 player 
as opinions have been expressed on them. The 
sentiment is positive on ?design? but negative on 
?sound quality?.  
Conditional sentences are sentences that de-
scribe implications or hypothetical situations and 
their consequences. In the English language, a 
variety of conditional connectives can be used to 
form these sentences. A conditional sentence 
contains two clauses: the condition clause and 
*  This work was done when Bing Liu was on sabbatical 
leave at Northwestern University. 
180
the consequent clause, that are dependent on 
each other. Their relationship has significant im-
plications on whether the sentence describes an 
opinion. One simple observation is that senti-
ment words (also known as opinion words) (e.g., 
great, beautiful, bad) alone cannot distinguish an 
opinion sentence from a non-opinion one. A 
conditional sentence may contain many senti-
ment words or phrases, but express no opinion.  
Example 1: If someone makes a beautiful and 
reliable car, I will buy it expresses no sentiment 
towards any particular car, although ?beautiful? 
and ?reliable? are positive sentiment words.  
This, however, does not mean that a condition-
al sentence cannot express opinions/sentiments.  
Example 2: If your Nokia phone is not good, 
buy this great Samsung phone is positive about 
the ?Samsung phone? but does not express an 
opinion on the ?Nokia phone? (although the 
owner of the ?Nokia phone? may be negative 
about it). Clearly, if the sentence does not have 
?if?, the first clause is negative. Hence, a method 
for determining sentiments in normal sentences 
will not work for conditional sentences. The ex-
amples below further illustrate the point.  
In many cases, both the condition and conse-
quent together determine the opinion. 
Example 3: If you are looking for a phone 
with good voice quality, don?t buy this Nokia 
phone is negative about the ?voice quality? of the 
?Nokia phone?, although there is a positive sen-
timent word ?good? in the conditional clause 
modifying ?voice quality?. However, in the fol-
lowing example, the opinion is just the opposite.    
Example 4: If you want a phone with good 
voice quality, buy this Nokia phone is positive 
about the ?voice quality? of the ?Nokia phone?.  
As we can see, sentiment analysis of condi-
tional sentences is a challenging problem.  
One may ask whether there is a large percen-
tage of conditional sentences to warrant a fo-
cused study. Indeed, there is a fairly large pro-
portion of such sentences in evaluative text. They 
can have a major impact on the sentiment analy-
sis accuracy. Table 1 shows the percentage of 
conditional sentences (sentences containing the 
words if, unless, assuming, etc) and also the total 
number of sentences from which we computed 
the percentage in several user-forums. The fig-
ures definitely suggest that there is considerable 
benefit to be gained by developing techniques 
that can analyze conditional sentences. 
To the best of our knowledge, there is no fo-
cused study on conditional sentences. This paper 
makes such an attempt. Specifically, we deter-
mine whether a conditional sentence (which is 
also called a conditional in the linguistic litera-
ture) expresses positive, negative or neutral opi-
nions on some topics/features. Since our focus is 
on studying how conditions and consequents af-
fect sentiments, we assume that topics are given, 
which are product attributes since our data sets 
are user comments on different products.  
Our study is conducted from two perspectives. 
We start with the linguistic angle to gain a good 
understanding of existing work on different types 
of conditionals. As conditionals can be expressed 
with other words or phrases than if, we will study 
how they behave compared to if. We will also 
show that the distribution of these conditionals 
based on our data sets.  
With the linguistic knowledge, we perform a 
computational study using machine learning. A 
set of features for learning is designed to capture 
the essential determining information. Note that 
the features here are data attributes used in learn-
ing rather than product attributes or features. 
Three classification strategies are designed to 
study how to best perform the classification task 
due to the complex situation of two clauses and 
their interactions in conditional sentences. These 
three classification strategies are clause-based, 
consequent-based and whole-sentence-based. 
Clause-based classification classifies each clause 
separately and then combines their results. Con-
sequent-based classification only uses conse-
quents for classification as it is observed that in 
conditional sentences, it is often the consequents 
that decide the opinion. Whole-sentence-based 
classification treats the entire sentence as a whole 
in classification. Experimental results on condi-
tional sentences from diverse domains demon-
strate the effectiveness of these classification 
models. The results indicate that the whole-
sentence-based classifier performs the best.  
Since this paper only studies conditional sen-
tences, a natural question is whether the pro-
posed technique can be easily integrated into an 
overall sentiment analysis or opinion mining sys-
tem. The answer is yes because a large propor-
tion of conditional sentences can be detected us-
ing conditional connectives. Keyword search is 
Table 1: Percent of conditional sentences  
Source % of cond. (total #. of sent.)  
Cellphone 8.6 (47711) 
Automobile 5.0 (8113) 
LCD TV 9.92 (258078) 
Audio Systems 8.1 (5702) 
Medicine 8.29 (160259) 
181
thus sufficient to identify such sentences for spe-
cial handling using the proposed approach. There 
are, however, some subtle conditionals which do 
not use normal conditional connectives and will 
need an additional module to identify them, but 
such sentences are very rare as Table 2 indicates. 
2 The Problem Statement  
The paper follows the feature-based sentiment 
analysis model in (Hu and Liu 2004; Popescu 
and Etzioni, 2005). We are particularly interested 
in sentiments on products and services, which are 
called objects or entities. Each object is de-
scribed by its parts and attributes, which are col-
lectively called features in (Hu and Liu, 2004; 
Liu, 2006). For example, in the sentence, If this 
camera has great picture quality, I will buy it, 
?picture quality? is a feature of the camera. For 
formal definitions of objects and features, please 
refer to (Liu, 2006; Liu, 2009). In this paper, we 
use the term topic to mean feature as the feature 
here can confuse with the feature used in ma-
chine learning. The term topic has also been used 
by some researchers (e.g., Kim and Hovy, 2004; 
Stoyanov and Cardie, 2008).  
Our objective is to predict the sentiment 
orientation (positive, negative or neutral) on each 
topic that has been commented on in a sentence.  
The problem of automatically identifying fea-
tures or topics being spoken about in a sentence 
has been studied in (Hu and Liu, 2004; Popescu 
and Etzioni, 2005; Stoyanov and Cardie, 2008). 
In this work, we do not attempt to identify such 
topics automatically. Instead, we assume that 
they are given because our objective is to study 
how the interaction of the condition and conse-
quent clauses affects sentiments. For this pur-
pose, we manually identify all the topics.    
3 Conditional Sentences  
This section presents the linguistic perspective of 
conditional sentences.  
3.1 Conditional Connectives  
A large majority of conditional sentences are 
introduced by the subordinating conjunction If. 
However, there are also many other conditional 
connectives, e.g., even if, unless, in case, assum-
ing/supposing, as long as, etc. Table 2 shows the 
distribution of conditional sentences with various 
connectives in our data. Detailed linguistic dis-
cussions of them are beyond the scope of this 
paper. Interested readers, please refer to (Dec-
lerck and Reed, 2001). Below, we briefly discuss 
some important ones and their interpretations.  
If: This is the most commonly used conditional 
connective. In addition to its own usage, it can 
also be used to replace other conditional connec-
tives, except some semantically richer connec-
tives (Declerck and Reed, 2001). Most (but not 
all) conditional sentences can be logically ex-
pressed in the form ?If P then Q?, where P is the 
condition clause and Q is the consequent clause. 
For practical purposes, we can automatically 
segment the condition and consequent clauses 
using simple rules generated by observing 
grammatical and linguistic patterns. 
Unless: Most conditional sentences containing 
unless can be replaced with equivalent sentences 
with an if and a not. For example, the sentence 
Unless you need clarity, buy the cheaper model 
can be expressed with If you don?t need clarity, 
buy the cheaper model.  
Even if: Linguistic theories claim that even if is 
a special case of a conditional which may not 
always imply an if-then relationship (Gauker 
2005). However, in our datasets, we have ob-
served that the usage of even if almost always 
translates into a conditional. Replacing even if by 
if will yield a sentence that is semantically simi-
lar enough for the purpose of sentiment analysis. 
Only if, provided/providing that, on condition 
that: Conditionals involving these phrases typi-
cally express a necessary condition, e.g., I will 
buy this camera only if they can reduce the price. 
In such sentences, only usually does not affect 
whether the sentence is opinionated or not.  
In case: Conditional sentences containing in 
case usually describe a precaution (I will close 
the window in case it rains), prevention (I wore 
sunglasses in case I was recognized), or a relev-
ance conditional (In case you need a car, you can 
rent one). Identifying the conditional and conse-
quent clauses is not straightforward in many cas-
es. Further, in these instances, replacing in case 
with if may not convey the intended meaning of 
the conditional. We have ignored these cases in 
Table 2: Percentage of sentences with some main 
conditional connectives 
Conditional Connective % of sentences 
If 6.42 
Unless 0.32 
Even if 0.17 
Until 0.10 
As (so) long as 0.09 
Assuming/supposing 0.04 
In case 0.04 
Only if 0.03 
182
our analysis as we believe that they need a sepa-
rate study, and also such sentences are rare.  
As (so) long as: Sentences with these connec-
tives behave similarly to if and can usually be 
replaced with if.  
Assuming/Supposing: These are a category of 
conditionals that behave quite differently. The 
participles supposing and assuming create condi-
tional sentences where the conditional clause and 
the consequent clause can be syntactically inde-
pendent. It is quite difficult to distinguish those 
conditional sentences which contain an explicit 
consequent clause and fit within our analysis 
framework. In our data, most of such sentences 
have no consequent, thus representing assump-
tions rather than opinions. We omit these sen-
tences in our study (they are also rare). 
3.2 Types of Conditionals  
There are extensive studies of conditional sen-
tences (also known as conditionals) in linguis-
tics. Various theories have led to a number of 
classification systems. Popular types of condi-
tionals include actualization conditionals, infe-
rential conditionals, implicative conditionals, etc 
(Declerck and Reed, 2001). However, these clas-
sifications are mainly based on semantic mean-
ings which are difficult to recognize by a com-
puter program. To build classification models, 
we instead exploit canonical tense patterns of 
conditionals, which are often used in pedagogic 
grammar books. They are defined based on tense 
and are associated with general meanings. How-
ever, as described in (Declerck and Reed, 2001), 
their meanings are much more complex and nu-
merous than their associated general meanings. 
However, the advantage of this classification is 
that different types can be detected easily be-
cause they depend on tense which can be pro-
duced by a part-of-speech tagger. As we will see 
in Section 5, canonical tense patterns help senti-
ment classification significantly. Below, we in-
troduce the four canonical tense patterns.  
Zero Conditional:  This conditional form is 
used to describe universal statements like facts, 
rules and certainties. In a zero conditional, both 
the condition and consequent clauses are in the 
simple present tense. An example of such sen-
tences is: If you heat water, it boils. 
First Conditional: Conditional sentences of 
this type are also called potential or indicative 
conditionals. They are used to express a hypo-
thetical situation that is probably true, but the 
truth of which is unverified. In the first condi-
tional, the condition is in the simple present 
tense, and the consequent can be either in past 
tense or present tense, usually with a modal aux-
iliary verb preceding the main verb, e.g., If the 
acceleration is good, I will buy it. 
Second Conditional: This is usually used to 
describe less probable situations, for stating pre-
ferences and imaginary events. The condition 
clause of a second conditional sentence is in the 
past subjunctive (past tense), and the consequent 
clause contains a conditional verb modifier (like 
would, should, might), in addition to the main 
verb, e.g., If the cell phone was robust, I would 
consider buying it. 
Third conditional: This is usually used to de-
scribe contrary-to-fact (impossible) past events. 
The past perfect tense is used in the condition 
clause, and the consequent clause is in the 
present perfect tense, e.g., If I had bought the 
a767, I would have hated it. 
Based on the above definitions, we have devel-
oped approximate part-of-speech (POS) tags 1 for 
the condition and the consequent of each pattern 
(Table 3), which do not cover all sentences, but 
overall they cover a majority of the sentences. 
For those not covered cases, the problem is 
mainly due to incomplete sentences and wrong 
grammars, which are typical for informal writ-
ings in forum postings and blogs. For example, 
the sentence, Great car if you need powerful ac-
celeration, does not fall into any category, but it 
actually means It is a great car if you need po-
werful acceleration, which is a zero conditional. 
To handle such sentences, we designed a set of 
rules to assign them some default types: 
 If condition contains VB/VBP/VBZ ? 0 conditional 
 If consequent contains VB/VBP/VBS ? 0 conditional 
 If condition contains VBG ? 1st conditional 
 If condition contains VBD ? 2nd conditional 
 If conditional contains VBN ? 3rd conditional.  
                                                 
1 The list of Part-Of-Speech (POS) tags can be found at: 
http://www.ling.upenn.edu/courses/Fall_2003/ling001/ 
penn_treebank_pos.html 
Table 3: Tenses for identifying conditional types 
Type Linguistic Rule Condition  
POS tags 
Consequent
POS tags
0 If + simple present
? simple present 
VB/VBP/VBZ VB/VBP/
VBZ 
1 If + simple present
? will + bare infinitive
VB/VBP/VBZ
/VBG 
MD + VB
2 If + past tense
? would + infinitive
VBD MD + VB
3 If + past perfect
? present perfect
VBD+VBN MD + VBD
183
By using these rules, we can increase the sen-
tence coverage from 73% to 95%.  
4 Sentiment Analysis of Conditionals 
We now describe our computational study. We 
take a machine learning approach to predict sen-
timent orientations. Below, we first describe fea-
tures used and then classification strategies.  
4.1 Feature construction  
I.  Sentiment words/phrases and their locations: 
Sentiment words are words used to express 
positive or negative opinions, which are in-
strumental for sentiment classification for ob-
vious reasons. We obtained a list of over 6500 
sentiment words gathered from various 
sources. The bulk of it is from 
http://www.cs.pitt.edu/mpqa. We also added 
some of our own. Our list is mainly from the 
work in (Hu and Liu, 2004; Ding, Liu and Yu, 
2008). In addition to words, there are phrases 
that describe opinions. We have identified a 
set of such phrases. Although obtaining these 
phrases was time-consuming, it was only a 
one-time effort. We will make this list availa-
ble as a community resource. It is possible 
that there is a better automated method for 
finding such phrases, such as the methods in 
(Kanayama and Nasukawa, 2006; Breck, Choi  
and Cardie, 2007). However, automatically 
generating sentiment phrases has not been the 
focus of this work as our objective is to study 
how the two clauses interact to determine 
opinions given the sentiment words and 
phrases are known. Our list of phrases is by 
no means complete and we will continue to 
expand it in the future.  
For each sentence, we also identify wheth-
er it contains sentiment words/phrases in its 
condition or consequent clause. It was ob-
served that the presence of a sentiment 
word/phrase in the consequent clause has 
more effect on the sentiment of a sentence.  
II.  POS tags of sentiment words: Sentiment 
words may be used in several contexts, not all 
of which may correspond to an opinion. For 
example, I trust Motorola and He has a trust 
fund both contain the word trust. But only the 
former contains an opinion. In such cases, the 
POS tags can provide useful information. 
III. Words indicating no opinion: Similar to how 
sentiment words are related to opinions, there 
are also a number of words which imply the 
opposite. Words like wondering, thinking, de-
bating are used when the user is posing a 
question or expressing doubts. Thus such 
phrases usually do not contribute an opinion, 
especially if they are in the vicinity of the if 
connective. We search a window of 3 words 
on either side of if to determine if there is any 
such word. We have compiled a list of these 
words as well and use it in our experiments.  
IV. Tense patterns: These are the canonical tense 
patterns in Section 3.2. They are used to gen-
erate a set of features. We identify the first 
verb in both the condition and consequent 
clauses by searching for the relevant POS tags 
in Table 3. We also search for the words pre-
ceding the main verb to find modal auxiliary 
verbs, which are also used as features.  
V. Special characters: The presence or absence 
of ??? and ?!?. 
VI. Conditional connectives: The conditional 
connective used in the sentence (if, even if, 
unless, only if, etc) is also taken as a feature. 
VII. Length of condition and consequent clauses: 
Using simple linguistic and punctuation rules, 
we automatically segment a sentence into 
condition and consequent clauses. The num-
bers of words in the condition and consequent 
clauses are then used as features. We ob-
served that when the condition clause is short, 
it usually has no impact on whether the sen-
tence expresses an opinion.   
VIII. Negation words: The use of negation words 
like not, don?t, never, etc, often alter the sen-
timent orientation of a sentence. For example, 
the addition of not before a sentiment word 
can change the orientation of a sentence from 
positive to negative. We consider a window of 
3-6 words before an opinion word, and search 
for these kinds of words. 
The following two features are singled out for 
easy reference later. They are only used in one 
classification strategy. The first feature is an in-
dicator, and the second feature has a parameter 
(which will be evaluated separately). 
(1). Topic location: This feature indicates wheth-
er the topic is in the conditional clause or the 
consequent clause.  
(2). Opinion weight: This feature considers only 
sentiment words in the vicinity of the topic, 
since they are more likely to influence the 
opinion on the topic. A window size is used 
to control what we mean by vicinity. The fol-
lowing formula is used to assign a weight to 
each sentiment word, which is inversely pro-
portional to the distance (Dop) of the senti-
ment word to the topic mention. Sentiment 
184
value is +1 for a positive word and -1 for a 
negative word. Sentwords are the set of 
known sentiment words and phrases.  
}{,
1
sentwordsop
D
weight
op op
???=?   
4.2 Classification Strategies 
Since we are interested in topic-based sentiment 
analysis, how to perform classification becomes 
an interesting issue. Due to the two clauses, it 
may not be sufficient to classify the whole sen-
tence as positive or negative as in the same sen-
tence, some topics may be positive and some 
may be negative. We propose three strategies.  
Clause-based classification: Since there are two 
clauses in a conditional sentence, in this case 
we build two classifiers, one for the condition 
and one for the consequent.  
Condition classifier: This method classifies the 
condition clause as expressing positive, nega-
tive or neutral opinion.  
Training data: Each training sentence is 
represented as a feature vector. Its class is posi-
tive, negative or neutral depending on whether 
the conditional clause is positive, negative or 
neutral while considering both clauses.  
Testing: For each test sentence, the resulting 
classifier predicts the opinion of the condition 
clause.  
Topic class prediction: To predict the opi-
nion on a topic, if the topic is in the condition 
clause, it takes the predicted class of the 
clause.  
Consequent classifier: This classifier classi-
fies the consequent clause as expressing posi-
tive, negative or neutral opinion. 
Training data: Each training sentence is 
represented as a feature vector. Its class is posi-
tive, negative or neutral depending on whether 
the consequent clause is positive, negative or 
neutral while considering both clauses.  
Testing: For each test sentence, the resulting 
classifier predicts the opinion of the conse-
quent clause.  
Topic class prediction: To predict the opi-
nion on a topic, if the topic is in the consequent 
clause, it takes the predicted class of the 
clause.  
The combination of these two classifiers is 
called the clause-based classifier. It works as 
follows: If a topic is in the conditional clause, 
the condition classifier is used, and if a topic is 
in the consequent clause, the consequent clas-
sifier is used.  
Consequent-based classification: It is observed 
that in most cases, the condition clause con-
tains no opinion whereas the consequent clause 
reflects the sentiment of the entire sentence. 
Thus, this method uses (in a different way) on-
ly the above consequent classifier. If it classi-
fies the consequent of a testing conditional 
sentence as positive, all the topics in the whole 
sentence are assigned the positive orientation, 
and likewise for negative and neutral.  
Whole-sentence-based classification: In this 
case, a single classifier is built to predict the 
opinion on each topic in a sentence.  
Training data: In addition to the normal fea-
tures, the two features (1) and (2) in Section 
4.1 are used for this classifier. If a sentence 
contains multiple topics, multiple training in-
stances of the same sentence are created in the 
training data. Each instance represents one 
specific topic. The class of the instance de-
pends on whether the opinion on the topic is 
positive, negative or neutral.  
Testing: For each topic in each test sentence, 
the resulting classifier predicts its opinion.  
Topic class prediction: This is not needed as 
the prediction has been done in testing.  
5 Results and Discussions 
5.1 Data sets 
Our data consists of conditional sentences from 5 
different user forums: Cellphone, Automobile, 
LCD TV, Audio systems and Medicine. We ob-
tained user postings from these forums and ex-
tracted the conditional sentences. We then ma-
nually annotated 1378 sentences from this cor-
pus. We also annotated the conditional and con-
sequent clauses and identified the topics (or 
product features) being commented upon, and 
their sentiment orientations. In our annotation, 
we observed that sentences with no sentiment 
words or phrases almost never express opinions, 
i.e., only around 3% of them express opinions. 
There are around 26% sentences containing no 
sentiment words or phrases in our data. To make 
the problem challenging, we restrict our attention 
to only those sentences that contain at least one 
sentiment word or phrase. We have annotated 
topics from around 900 such sentences. Table 4 
shows the class distributions of this data. At the 
clause level (topics are not considered), we ob-
serve that conditional clauses contain few opi-
nions. At the topic-level, 43.5% of the topics 
have positive opinions, 26.4% of the topics have 
negative opinions, and the rest have no opinions.  
185
Table 4: Distribution of classes  
For the annotation of data, we assume that 
topics are known. One student annotated the top-
ics first. Then two students annotated the senti-
ments on the topics. If a student found that a top-
ic annotation is wrong, he will let us know. Some 
mistakes and missing topics were found but there 
were mainly due to oversights rather than disa-
greements. The agreement on sentiment annota-
tions were computed using the Kappa score. We 
achieved the Kappa score of 0.63, which indi-
cates strong agreements. The conflicting cases 
were then solved through discussion to reach 
consensus. We did not find anything that the an-
notators absolutely disagree with each other.   
5.2 Experimental results 
We now present the results for different combi-
nations of features and classification strategies. 
For model building, we used Support Vector 
Machines (SVM), and the LIBSVM implementa-
tion (Chang and Lin, 2001) with a Gaussian ker-
nel, which produces the best results. All the re-
sults are obtained via 10-fold cross validation.  
Two-class classification: We first discuss the 
results for a simpler version of the problem that 
involves only sentences with positive or negative 
orientations on some topics (at least one of the 
clauses must have a positive/negative opinion on 
a topic). Neutral sentences are not used (~28% of 
the total). The results of all three classifiers are 
given in Table 5. The feature sets have been de-
scribed in Section 4.1. For all the experiments 
below, features (1) and (2) are only used by the 
whole-sentence-based classifier, but not used by 
the other two classifiers for obvious reasons.   
{I+II}: This setting uses sentiment words and 
phrases, their positions and POS tags as features 
(we used Brill?s POS tagger). This can be seen as 
the baseline. We observe that both the conse-
quent-based and whole-sentence-based classifiers 
perform dramatically better than the clause-based 
classifier. The consequent-based classifier and 
the whole-sentence-based classifier perform si-
milarly (with the latter being slightly better). The 
precision, recall, and F-score are computed as the 
average of the two classes.  
{I+II+III}: In this setting, the list of special 
non-sentiment related words is added to the fea-
ture set. All three classifiers improve slightly.  
{I+II+III+IV}: This setting includes all the ca-
nonical tense based features. We see marked im-
provements for the consequent-based and whole-
sentence-based classifiers both in term of accura-
cy and F-score, which are statistically significant 
compared to those of {I+II+III} at the 95% con-
fidence level based on paired t-test.  
All: When all the features are used, the results 
of all the classifiers improve further.  
Two main observations worth mentioning: 
1. Both the consequent-based and whole-
sentence-based classifiers outperform the 
clause-based classifier dramatically. This con-
firms our observation that the consequent 
usually plays the key role in determining the 
sentiment of the sentence. This is further rein-
forced by the fact that the consequent-based 
classifier actually performs similarly to the 
whole-sentence-based classifier. The condi-
tion clause seems to give no help.  
2. The second observation is that the linguistic 
knowledge of canonical tense patterns helps 
significantly. This shows that the linguistic 
knowledge is very useful.  
We also noticed that many misclassifications are 
caused by grammatical errors, use of slang 
phrases and improper punctuations, which are 
typical of postings on the Web. Due to language 
irregularities (e.g., wrong grammar, missing 
punctuations, sarcasm, exclamations), the POS 
tagger makes many mistakes as well causing 
some errors in the tense based features.  
Three-class classification: We now move to the 
more difficult and realistic case of three classes: 
positive, negative and neutral (no-opinion). Ta-
ble 6 shows the results. The trend is similar ex-
cept that the whole-sentence-based classifier now 
performs markedly better than the consequent-
based classifier. We believe that this is because 
the neutral class needs information from both the 
condition and consequent clauses. This is evident 
from the fact that there is little or no improve-
ment after {I+II} for the consequent-based clas-
sifier. We also observe that the accuracies and F-
scores for the three-class classification are lower 
than those for the two-class classification. This is 
understandable due to the difficulty of determin-
ing whether a sentence has opinion or not. Again, 
statistical test shows that the canonical tense-
based features help significantly.  
As mentioned in Section 4.1, the whole-
sentence-based classifier only considers those 
sentiment words in the vicinity of the topic under 
 Positive Negative Neutral 
Condition 6.9% 6.7% 86.4% 
Consequent 49.3% 16.5% 34% 
Topic-level 43.5% 26.4% 29.9% 
186
investigation. For this, we search a window of n 
words on either side of the topic mention. To 
study the effect of varying n, we performed an 
experiment with various values of the window 
size and measured the overall accuracy for each 
case. Table 7 shows how the accuracy changes as 
we increase the window size. We found that a 
window size of 6-10 yielded good accuracies. 
This is because lower values of n lead to loss of 
information regarding sentiment words as some 
sentiment words could be far from the topic. We 
finally used 8, which gave the best results.  
We also investigated ways of using the nega-
tion word in the sentence to correctly predict the 
sentiment. One method is to use the negation 
word as a feature, as described in Section 4.1. 
Another technique is to reverse the orientation of 
the prediction for those sentences which contain 
negation words. We found that the former tech-
nique yielded better results. The results reported 
so far are based on the former approach.  
6 Related Work  
There are several research directions in sentiment 
analysis (or opinion mining). One of the main 
directions is sentiment classification, which clas-
sifies the whole opinion document (e.g., a prod-
uct review) as positive or negative (e.g., Pang et 
al, 2002; Turney, 2002; Dave et al 2003; Ng et 
al. 2006; McDonald et al 2007). It is clearly dif-
ferent from our work as we are interested in con-
ditional sentences. 
Another important direction is classifying 
sentences as subjective or objective, and classify-
ing subjective sentences or clauses as positive or 
negative (Wiebe et al 1999; Wiebe and Wilson, 
2002, Yu and Hatzivassiloglou, 2003; Wilson et 
al, 2004; Kim and Hovy, 2004; Riloff and 
Wiebe, 2005; Gamon et al2005; McDonald et al 
2007). Although these works deal with sen-
tences, they aim to solve the general problem. 
This paper argues that there is unlikely a one-
technique-fit-all solution, and advocates dealing 
with specific types of sentences differently by 
exploiting their unique characteristics. Condi-
tional sentences are the focus of this paper. To 
the best of our knowledge, there is no focused 
study on them.  
Several researchers also studied feature/topic-
based sentiment analysis (e.g., Hu and Liu, 2004; 
Popescu and Etzioni, 2005; Ku et al 2006; Care-
nini et al 2006; Mei et al 2007; Ding, Liu and 
Yu, 2008; Titov and R. McDonald, 2008; Stoya-
nov and Cardie, 2008; Lu and Zhai, 2008). Their 
objective is to extract topics or product features 
in sentences and determine whether the senti-
ments expressed on them are positive or nega-
tive. Again, no focused study has been made to 
handle conditional sentences. Effectively han-
dling of conditional sentences can help their ef-
fort significantly.  
Table 5: Two-class classification ? positive and negative 
 Clause-based  
classifier
Consequent-based  
classifier 
Whole-sentence-based 
classifier 
Acc. Prec. Rec. F Acc. Prec. Rec. F Acc. Prec. Rec. F 
I+II (senti. words+POS) 39.9 42.8 34.0 37.9 69.1 72.9 67.1 69.8 68.9 73.7 68.13 70.8
I+II+III (+ non-senti. words)  41.5 44.9 37.1 40.6 69.3 73.9 66.3   69.9 69.2 73.7 63.5 71.0
I+II+III+IV (+ tenses) 42.7 45.2 38.5 41.6 72.7 76.4 72.0 74.1   71.1 77.9 72.2 74.9
All 43.2 46.1 38.9 42.2 73.3 77.0 72.7 74.8 72.3 77.8 73.6 75.6
Table 6: Three-class classification ? positive, negative and neutral (no opinion) 
 Clause-based  
classifier
Consequent-based  
classifier 
Whole-sentence-based 
classifier 
Acc. Prec. Rec. F Acc. Prec. Rec. F Acc. Prec. Rec. F 
I+II (senti. words+POS) 45.2 41.3 35.1 37.9 54.6 57.7 52.9 55.2 59.1 58.1 56.4 57.2
I+II+III (+ non-senti. words)  46.9 42.8 37.8 40.1 55.3 60.0 51.3 55.3 61.4 60.1 60.8 60.4
I+II+III+IV (+ tenses) 50.3 48.7 40.9 44.5 57.3 64.0 50.0 56.1 64.6 63.3 63.9 63.6
All 53.3 49.8 44.1 46.8 58.7 64.5 50.1 56.4 67.8 66.9 65.1 66.0
Table 7: Accuracy of the whole-sentence-based classifier with varying window sizes (n) 
Window size 1 2 3 4 5 6 7 8 9 10 
Accuracy 66.1 62.6 64.1 64.8 65.3 65.7 66.3 67.3 66.9 66.8 
187
In this work, we used many sentiment words 
and phrases. These words and phrases are usually 
compiled using different approaches (Hatzivassi-
loglou and McKeown, 1997; Kaji and Kitsure-
gawa, 2006; Kanayama and Nasukawa, 2006; 
Esuli and Sebastiani, 2006; Breck et al 2007; 
Ding, Liu and Yu. 2008; Qiu et al 2009). There 
are several existing lists produced by researchers. 
We used the one from the MPQA corpus 
(http://www.cs.pitt.edu/mpqa) with added phras-
es of our own from (Ding, Liu and Yu. 2008). In 
our work, we also assume that the topics are 
known. (Hu and Liu, 2004; Popescu and Etzioni, 
2005; Kobayashi, Inui and Matsumoto, 2007; 
Stoyanov and Cardie, 2008) have studied top-
ic/feature extraction. 
One existing focused study is on comparative 
and superlative sentences (Jindal and Liu, 2006; 
Bos and Nissim, 2006; Fiszman et al 2007; Ga-
napathibhotla and Liu, 2008). Their work identi-
fies comparative sentences, extracts comparative 
relations in the sentences and analyzes compara-
tive opinions (Ganapathibhotla and Liu, 2008). 
An example comparative sentence is ?Honda 
looks better than Toyota?. As we can see, com-
parative sentences are entirely different from 
conditional sentences. Thus, their methods can-
not be directly applied to conditional sentences.  
7 Conclusion  
To perform sentiment analysis accurately, we 
argue that a divide-and-conquer approach is 
needed, i.e., focused study on each type of sen-
tences. It is unlikely that there is a one-size-fit-all 
solution. This paper studied one type, i.e., condi-
tional sentences, which have some unique cha-
racteristics that need special handling. Our study 
was carried out from both the linguistic and 
computational perspectives. In the linguistic 
study, we focused on canonical tense patterns, 
which have been showed useful in classification. 
In the computational study, we built SVM mod-
els to automatically predict whether opinions on 
topics are positive, negative or neutral. Experi-
mental results have shown the effectiveness of 
the models.  
In our future work, we will further improve 
the classification accuracy and study related 
problems, e.g., identifying topics/features. Al-
though there are some special conditional sen-
tences that do not use easily recognizable condi-
tional connectives and identifying them are use-
ful, such sentences are very rare and spending 
time and effort on them may not be cost-effective 
at the moment.  
Acknowledgements  
This work was supported in part by DOE SCI-
DAC-2: Scientific Data Management Center for 
Enabling Technologies (CET) grant DE-FC02-
07ER25808, DOE FASTOS award number DE-
FG02-08ER25848, NSF HECURA CCF-
0621443, NSF SDCI OCI-0724599, and NSF 
ST-HEC CCF-0444405. 
References  
J. Bos, and M. Nissim. 2006. An Empirical Ap-
proach to the Interpretation of Superlatives. 
EMNLP-2006. 
E. Breck, Y. Choi, and C. Cardie. 2007. Identify-
ing expressions of opinion in context, IJCAI-
2007.  
C.-C. Chang and C.-J. Lin. 2001. LIBSVM: a 
library for support vector machines. 
http://www.csie.ntu.edu.tw /~cjlin/libsvm 
G. Carenini, R. Ng, and A. Pauls. 2006. Interac-
tive Multimedia Summaries of Evaluative 
Text. IUI-2006. 
C. Gauker. 2005. Conditionals in Context. MIT 
Press. 
D. Dave, A. Lawrence, and D. Pennock. 2003. 
Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product 
Reviews. WWW-2003. 
R. Declerck, and S. Reed. 2001. Conditionals: A 
Comprehensive Empirical Analysis. Berlin: 
Mouton de Gruyter.  
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic 
lexicon-based approach to opinion mining. 
WSDM-2008.  
A. Esuli, and F. 2006. Sebastiani. Determining 
term subjectivity and term orientation for opi-
nion mining, EACL-2006.  
M. Fiszman, D. Demner-Fushman, F. Lang, P. 
Goetz, and T. Rindflesch. 2007. Interpreting 
Comparative Constructions in Biomedical 
Text. BioNLP-2007.  
M. Gamon, A. Aue, S. Corston-Oliver, S. and E. 
Ringger. 2005. Pulse: Mining customer opi-
nions from free text. IDA-2005. 
G. Ganapathibhotla and B. Liu. 2008. Identifying 
Preferred Entities in Comparative Sentences. 
COLING-2008.  
V. Hatzivassiloglou, and K. McKeown, K. 1997. 
188
Predicting the Semantic Orientation of Adjec-
tives. ACL-EACL-1997.  
M. Hu and B. Liu. 2004. Mining and summariz-
ing customer reviews. KDD-2004.  
N. Jindal, and B. Liu. 2006. Mining Comparative 
Sentences and Relations. AAAI-2006. 
N. Kaji, and M. Kitsuregawa. 2006. Automatic 
construction of polarity-tagged corpus from 
HTML documents. ACL-2006. 
H. Kanayama, and T. Nasukawa. 2006. Fully 
Automatic Lexicon Expansion for Domain-
Oriented Sentiment Analysis. EMNLP-2006. 
S. Kim and E. Hovy. 2004. Determining the Sen-
timent of Opinions. COLING-2004.  
N. Kobayashi, K. Inui and Y. Matsumoto. 2007. 
Extracting Aspect-Evaluation and Aspect-of 
Relations in Opinion Mining. EMNLP-2007. 
L.-W. Ku, Y.-T. Liang, and H.-H. Chen. 2006, 
Opinion Extraction, Summarization and 
Tracking in News and Blog Corpora. AAAI-
CAAW. 
B. Liu. 2006. Web Data Mining: Exploring 
Hyperlinks, Content and Usage Data. Sprin-
ger.  
B. Liu. 2009. Sentiment Analysis and Subjectivi-
ty. To appear in Handbook of Natural Lan-
guage Processing, Second Edition, (editors: 
N. Indurkhya and F. J. Damerau), 2009 or 
2010. 
Y. Lu, and C. X. Zhai. 2008. Opinion integration 
through semi-supervised topic modeling. 
WWW-2008. 
R. McDonald, K. Hannan, T. Neylon, M. Wells, 
and J. Reynar. 2007. Structured models for 
fine-to-coarse sentiment analysis.  ACL-2007 
Q. Mei, X. Ling, M. Wondra, H. Su, and C. X.  
Zhai. 2007. Topic Sentiment Mixture: Model-
ing Facets and Opinions in Weblogs. WWW-
2007.  
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. 
Examining the role of linguistic knowledge 
sources in the automatic identification and 
classification of reviews. ACL-2006. 
B. Pang and L. Lee. 2008. Opinion Mining and 
Sentiment Analysis. Foundations and Trends 
in Information Retrieval 2(1-2), pp. 1?135, 
2008. 
B. Pang, L. Lee. and S. Vaithyanathan. 2002. 
Thumbs up? Sentiment Classification Using 
Machine Learning Techniques. EMNLP-
2002.  
A-M. Popescu, and O. Etzioni. 2005. Extracting 
Product Features and Opinions from Reviews. 
EMNLP-2005.  
G. Qiu, B. Liu, J. Bu and C. Chen. 2009. Ex-
panding Domain Sentiment Lexicon through 
Double Propagation. IJCAI-2009. 
E. Riloff, and J. Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. 
EMNLP-2003.  
V. Stoyanov, and C. Cardie. 2008. Topic Identi-
fication for fine-grained opinion analysis. 
COLING-2008.  
I. Titov and R. McDonald. 2008. A Joint Model 
of Text and Aspect Ratings for Sentiment 
Summarization. ACL-2008.  
P. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. ACL-2002.  
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Devel-
opment and use of a gold standard data set for 
subjectivity classifications. ACL-1999. 
J. Wiebe, and T. Wilson. 2002. Learning to Dis-
ambiguate Potentially Subjective Expressions. 
CoNLL-2002. 
T. Wilson, J. Wiebe. and R. Hwa. 2004. Just how 
mad are you? Finding strong and weak 
opinion clauses. AAAI-2004. 
H. Yu, and Y. Hatzivassiloglou. 2003. Towards 
answering opinion questions: Separating facts 
from opinions and identifying the polarity of 
opinion sentences. EMNLP-2003.  
 
 
 
189
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 268?276,
Beijing, August 2010
Resolving Object and Attribute Coreference in Opinion Mining
Xiaowen Ding 
Department of Computer Science 
University of Illinois at Chicago 
dingxwsimon@gmail.com
Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
liub@cs.uic.edu
Abstract
Coreference resolution is a classic NLP 
problem and has been studied extensively by 
many researchers. Most existing studies, 
however, are generic in the sense that they 
are not focused on any specific text. In the 
past few years, opinion mining became a 
popular topic of research because of a wide 
range of applications. However, limited 
work has been done on coreference resolu-
tion in opinionated text. In this paper, we 
deal with object and attribute coreference 
resolution. Such coreference resolutions are 
important because without solving it a great 
deal of opinion information will be lost, and 
opinions may be assigned to wrong entities. 
We show that some important features re-
lated to opinions can be exploited to perform 
the task more accurately. Experimental re-
sults using blog posts demonstrate the effec-
tiveness of the technique.
1 Introduction 
Opinion mining has been actively researched in 
recent years. Researchers have studied the prob-
lem at the document level (e.g., Pang et al, 
2002; Tuney, 2002; Gamon et al, 2005) sen-
tence and clause level (Wilson et al, 2004; Kim 
and Hovy, 2004), word level (e.g., Andreevs-
kaia and Bergler, 2006; Hatzivassiloglou and 
McKeown, 1997; Esuli and Sebastiani, 2006; 
Kanayama and Nasukawa, 2006; Qiu et al, 
2009), and attribute level (Hu and Liu 2004; 
Popescu and Etzioni, 2005; Ku et al, 2006; Mei 
et al, 2007; Titov and McDonald 2008). Here 
attributes mean different aspects of an object 
that has been commented on. Let us use the fol-
lowing example blog to illustrate the problem: 
?I bought a Canon S500 camera yesterday. It 
looked beautiful. I took a few photos last night. 
They were amazing?. ?It? in the second sen-
tence refers to ?Canon S500 camera?, which is 
called an object. ?They? in the fourth sentence 
refers to ?photos?, which is called an attribute
of the object ?Canon S500 camera?. The use-
fulness of coreference resolution in this case is 
clear. Without resolving them, we lose opinions. 
That is, although we know that the second and 
fourth sentences express opinions, we do not 
know on what. Without knowing the opinion 
target, the opinion is of limited use. In (Nicolov 
et al, 2008), it was shown based on manually 
annotated data that opinion mining results can 
be improved by 10% if coreference resolution is 
used (the paper did not provide an algorithm).  
In this paper, we propose the problem of ob-
ject and attribute coreference resolution ? the 
task of determining which mentions of objects 
and attributes refer to the same entities. Note 
that here entities refer to both objects and 
attributes, not the traditional named entities. To 
our knowledge, limited work has been done on 
this problem in the opinion mining context apart 
from a prior study on resolving opinion sources 
(or holders) (Stoyanov and Cardie 2006). Opi-
nion sources or holders are the persons or or-
ganizations that hold some opinions on objects 
and attributes. In this paper, we do not deal with 
source resolution as we are mainly interested in 
opinion texts on the web, e.g., reviews, discus-
sions and blogs. In such environments opinion 
sources are usually the authors of the posts, 
which are displayed in Web pages.   
This work follows the attribute-based opi-
nion mining model in (Hu and Liu 2004; Popes-
cu and Etzioni, 2005). In their work, attributes 
are called features. We do not use the term ?fea-
ture? in this paper to avoid confusion with the 
term ?feature? used in machine learning.  
Our primary interests in this paper are opi-
268
nions expressed on products and services, which 
are called objects. Each object is described by 
its parts/components and attributes, which are 
all called attributes for simplicity.  
This paper takes the supervised learning ap-
proach to solving the problem. The key contri-
bution of this paper is the design and testing of 
two novel opinion related features for learning. 
The first feature is based on sentiment analysis 
of normal sentences (non-comparative sen-
tences), comparative sentences, and the idea of 
sentiment consistency. For example, we have 
the sentences, ?The Sony camera is better than 
the Canon camera. It is cheap too.? It is clear 
that ?It? means ?Sony? because in the first sen-
tence, the opinion on ?Sony? is positive (com-
parative positive), but negative (comparative 
negative) on ?Canon?, and the second sentence 
is positive. Thus, we can conclude that ?It? re-
fers to ?Sony? because people usually express 
sentiments in a consistent way. It is unlikely 
that ?It? refers to ?Canon?. This is the idea of 
sentiment consistency. As we can see, this fea-
ture requires the system to have the ability to 
determine positive and negative opinions ex-
pressed in normal and comparative sentences.  
The second feature considers what objects 
and attributes are modified by what opinion 
words. Opinion words are words that are com-
monly used to express positive or negative opi-
nions, e.g., good, best, bad, and poor. Consider 
the sentences, ?The picture quality of the Canon 
camera is very good. It is not expensive either.?
The question is what ?It? refers to, ?Canon 
camera? or ?picture quality?. Clearly, we know 
that ?It? refers to ?Canon camera? because ?pic-
ture quality? cannot be expensive. To make this 
feature work, we need to identify what opinion 
words are usually associated with what objects 
or attributes, which means that the system needs 
to discover such relationships from the corpus.  
These two features give significant boost to 
the coreference resolution accuracy. Experimen-
tal results based on three corpora demonstrate 
the effectiveness of the proposed features. 
2 Related Work 
Coreference resolution is an extensively studied 
NLP problem (e.g., Morton, 2000; Ng and Car-
die, 2002; Gasperin and Briscoe, 2008). Early 
knowledge-based approaches were domain and 
linguistic dependent (Carbonell and Brown 
1988), where researchers focused on diverse 
lexical and grammatical properties of referring 
expressions (Soon et al, 2001; Ng and Cardie, 
2002; Zhou et al, 2004). Recent research relied 
more on exploiting semantic information. For 
example, Yang et al (2005) used the semantic 
compatibility information, and Yang and Su 
(2007) used automatically discovered patterns 
integrated with semantic relatedness informa-
tion, while Ng (2007) employed semantic class 
knowledge acquired from the Penn Treebank. 
Versley et al (2008) used several kernel func-
tions in learning. 
Perhaps, the most popular approach is based 
on supervised learning. In this approach, the 
system learns a pairwise function to predict 
whether a pair of noun phrases is coreferent. 
Subsequently, when making coreference resolu-
tion decisions on unseen documents, the learnt 
pairwise noun phrase coreference classifier is 
run, followed by a clustering step to produce the 
final clusters (coreference chains) of coreferent 
noun phrases. For both training and testing, co-
reference resolution algorithms rely on feature 
vectors for pairs of noun phrases that encode 
lexical, grammatical, and semantic information 
about the noun phrases and their local context.  
Soon et al (2001), for example, built a noun 
phrase coreference system based on decision 
trees and it was tested on two standard corefe-
rence resolution data sets (MUC-6, 1995; MUC-
7, 1998), achieving performance comparable to 
the best-performing knowledge based corefe-
rence engines at that time. The learning algo-
rithm used 12 surface-level features. Our pro-
posed method builds on this system with addi-
tional sentiment related features. The features 
inherit from this paper includes: 
Distance Feature: Its possible values are 0, 
1, 2, 3 and so on which captures the sentence 
distance between two entities. 
Antecedent-pronoun feature, anaphor-
pronoun feature: If the candidate antecedent or 
anaphor is a pronoun, it is true; false otherwise. 
Definite noun phrase feature: The value is 
true if the noun phrase starts with ?the?; false 
otherwise.
Demonstrative noun phrase feature: The 
value is true if the noun phrase starts with the 
word ?this?, ?that?, ?these?, or ?those?; false 
otherwise.
269
Number agreement feature: If the candidate 
antecedent and anaphor are both singular or 
both plural, the value is true; otherwise false. 
Both-proper-name feature: If both the can-
didates are proper nouns, which are determined 
by capitalization, return true; otherwise false. 
Alias feature: It is true if one candidate is an 
alias of the other or vice versa; false otherwise. 
Ng and Cardie (2002) expanded the feature 
set of Soon et al (2001) from 12 to 53 features. 
The system was further improved by Stoyanov 
and Cardie (2006) who gave a partially super-
vised clustering algorithm and tackled the prob-
lem of opinion source coreference resolution.  
Centering theory is a linguistic approach tried 
to model the variation or shift of the main sub-
ject of the discourse in focus. In (Grosz et al, 
1995; Tetreault, 2001), centering theory was 
applied to sort the antecedent candidates based 
on the ranking of the forward-looking centers, 
which consist of those discourse entities that 
can be interpreted by linguistic expressions in 
the sentences. Fang et al (2009) employed the 
centering theory to replace the grammatical role 
features with semantic role information and 
showed superior accuracy performances. 
Ding et al (2009) studied the entity assign-
ment problem. They tried to discover the prod-
uct names discussed in forum posts and assign 
the product entities to each sentence. The work 
did not deal with product attributes.  
Unsupervised approaches were also applied 
due to the cost of annotating large corpora. Ng 
(2008) used an Expectation-Maximization (EM) 
algorithm, and Poon and Domingos (2008) ap-
plied Markov Logic Network (MLN).  
Another related work is the indirect anapho-
ra, known as bridging reference. It arises when 
an entity is part of an earlier mention. Resolving 
indirect anaphora requires background know-
ledge (e.g. Fan et al, 2005), and it is thus not in 
the scope of this paper.
Our work differs from these existing studies 
as we work in the context of opinion mining, 
which gives us extra features to enable us to 
perform the task more effectively.  
3 Problem of Object and Attribute Co-
reference Resolution 
In general, opinions can be expressed on any-
thing, e.g., a product, an individual, an organi-
zation, an event, a topic, etc. Following (Liu, 
2006), we also use the term object to denote an 
named entity that has been commented on. The 
object has a set of components (or parts) and 
also a set of attributes. For simplicity, attribute
is used to denote both component and attribute 
in this paper. Thus, we have the two concepts, 
object and attribute.
3.1 Objective 
Task objective: To carry out coreference reso-
lution on objects and attributes in opinion text.   
As we discussed in the introduction section, 
coreference resolution on objects and attributes 
is important because they are the core entities 
on which people express opinions. Due to our 
objective, we do not evaluate other types of co-
references. We assume that objects and entities 
have been discovered by an existing system 
(e.g., Hu and Liu 2004, Popescu and Etzioni 
2005). Recall that a coreference relation holds 
between two noun phrases if they refer to the 
same entity. For example, we have the follow-
ing three consecutive sentences: 
s1: I love the nokia n95 but not sure how good 
the flash would be? 
s2: and also it is quite expensive so anyone got 
any ideas? 
s3: I will be going on contract so as long as i can 
get a good deal of it.
?it? in s2 refers to the entity ?the nokia n95? 
in s1. In this case, we call ?the nokia n95? the 
antecedent and pronoun ?it? in s2 the anaphor.
The referent of ?it? in s3 is also ?the nokia n95?, 
so the ?it? in s3 is coreferent with the ?it? in s2.
Our task is thus to decide which mentions of 
objects and attributes refer to the same entities. 
3.2 Overview of Our Approach 
Like traditional conference resolution, we em-
ploy the supervised learning approach by in-
cluding additional new features. The main steps 
of our approach are as follows:  
Preprocessing: We first preprocess the cor-
pus by running a POS tagger 1 , and a Noun 
Phrase finder2. We then produce the set O-NP 
which includes both possible objects, attributes 
and other noun phrases. The noun phrases are 
1 http://nlp.stanford.edu/software/tagger.shtml
2 http://crfchunker.sourceforge.net/ 
270
found using the Noun Phrase finder and the ob-
ject names are consecutive NNPs. O-NP thus 
contains everything that needs to be resolved.  
Feature vector construction: To perform 
machine learning, we need a set of features. 
Similar to previous supervised learning ap-
proaches (Soon et al, 2001), a feature vector is 
formed for every pair of phrases in O-NP ex-
tracted in the preprocessing step. We use some 
of the features introduced by Soon et al (2001) 
together with some novel new features that we 
propose in this work. Since our focus is on 
products and attributes in opinionated docu-
ments, we do not use personal pronouns, the 
gender agreement feature, and the appositive 
feature, as they are not essential in blogs and 
forum posts discussing products.  
Classifier construction: Using the feature 
vectors obtained from the previous step, we 
construct the training data, which includes all 
pairs of manually tagged phrases that are either 
object names or attributes. More precisely, each 
pair contains at least one object or one attribute. 
Using the training data, a decision tree is con-
structed using WEKA3.
Testing: The testing phase employs the same 
preprocessing and feature vector construction 
steps as described above, followed by the appli-
cation of the learnt classifier on all candidate 
coreference pairs (which are represented as fea-
ture vectors). Since we are only interested in 
coreference information for objects and attribute 
noun phrases, we discard non-object and non-
attribute noun phrases. 
4 The Proposed New Features  
On surface, object and attribute coreference res-
olution seems to be the same as the traditional 
noun phrase coreference resolution. We can ap-
ply an existing coreference resolution technique. 
However, as we mentioned earlier, in the opi-
nion mining context, we can have a better solu-
tion by integrating opinion information into the 
traditional lexical and grammatical features. 
Below are several novel features that we have 
proposed. We use ?i to denote an antecedent 
candidate and ?j an anaphor candidate. Note that 
we will not repeat the features used in previous 
systems, but only focus on the new features.  
3 http://www.cs.waikato.ac.nz/ml/weka/
4.1 Sentiment Consistency 
Intuitively, in a post, if the author starts express-
ing opinions on an object, he/she will continue 
to have the same opinion on that object or its 
attributes unless there are contrary words such 
as ?but? and ?however?. For example, we have 
the following blog (an id is added before each 
sentence to facilitate later discussion):  
?(1) I bought Camera-A yesterday. (2) I 
took a few pictures in the evening in my living 
room. (3) The images were very clear. (4) 
They were definitely better than those from 
my old Camera-B. (5a) It is cheap too. (5b) 
The pictures of that camera were blurring for 
night shots, but for day shots it was ok?  
The comparative sentence (4) says that Cam-
era-A is superior to Camera-B. If the next sen-
tence is (5a) ((5a) and (5b) are alternative sen-
tences), ?it? should refer to the superior prod-
uct/object (Camera-A) because sentence (5a) 
expresses a positive opinion. Similarly, if the 
next sentence is sentence (5b) which expresses a 
negative opinion in its first clause, ?that cam-
era? should refer to the inferior product (Cam-
era-B). We call this phenomenon sentiment con-
sistency (SC), which says that consecutive sen-
timent expressions should be consistent with 
each other unless there are contrary words such 
as ?but? and ?however?. It would be ambiguous 
if such consistency is not observed. 
Following the above observation, we further 
observe that if the author wants to introduce a 
new object o, he/she has to state the name of the 
object explicitly in a sentence si-1. The question 
is what happens to the next sentence si if we 
need to resolve the pronouns in si.   
We consider several cases: 
1. si-1 is a normal sentence (not a comparative 
sentence). If si expresses a consistent senti-
ment with si-1, it should refer to the same ob-
ject as si-1.  For example, we have  
 si-1: The N73 is my favorite.
 si: It can produce great pictures. 
Here ?It? in si clearly refers to ?The N73? in 
the first sentence si-1.
2. si-1 is a normal sentence and si does not ex-
press a consistent sentiment, then ?i and ?j
introduced in these two sentences may not be 
coreferenced. For example, we have
 si-1:  The K800 is awesome.
 si: That phone has short battery life. 
271
Here ?The K800? and ?That phone? may not 
be a coreference pair according to sentiment 
consistency. ?That phone? should refer to an 
object appeared in an earlier sentence.  
3.  si-1 is a comparative sentence. If si expresses 
a positive (respectively negative) sentiment, 
the pronoun in si should refer to the superior 
(or inferior) entity in si-1 to satisfy sentiment 
consistency. This situation is depicted in the 
earlier example blog. For completeness, we 
give another example.   
 si-1: The XBR4 is brighter than the 5080.
 si: Overall, it is a great choice.  
Here ?it? in si should refer to ?The XBR4? in 
si-1 since they both have positive sentiments 
expressed on them. 
Opinion Mining of Comparative Sentences:
To deal with case (3), we need to identify supe-
rior entities from comparative sentences. In fact, 
we first need to find such comparative sen-
tences. There is a prior work on identifying 
comparative sentences (Jindal and Liu. 2006). 
Since our focus is not to identify such sen-
tences, we used several heuristic rules based on 
some comparative keywords, e.g. than, win,
superior, etc. They achieve the F-score of 0.9. 
We then followed the opinion mining method 
introduced in (Ding et al 2009) to find superior 
entities. Since a comparative sentence typically 
has entities on the two sides of a comparative 
keyword, i.e., ?Camera-X is better than Cam-
era-Y?, based on opinion mining, if the sentence 
is positive, then the entities before the compara-
tive keyword is superior and otherwise they are 
inferior (with the negation considered).  
SC Feature: The possible value for this fea-
ture is 0, 1, or 2. If ?i and ?j have the same opi-
nion, return 1; different opinions, return 0; and 
if the opinions cannot be identified for one or 
both of them, return 2. Here is an example ex-
plaining how the feature is used in our system:
?My wife has currently got a Nokia 7390, 
which is terrible. My 6233 would always get 
great reception, hers would get no signal.?
Using our algorithm for opinion mining, ?hers? 
gets a negative opinion in the second sentence. 
So the value for this feature for the pair, ?hers? 
and ?a Nokia 7390?, is 1. The feature value for 
the pair ?hers? and ?My 6233? is 0. The idea is 
that because the first sentence expresses a nega-
tive sentiment on ?a Nokia 7390?, and there is 
no discourse connective (such as ?but? and 
?however?) between these two sentences. 
?Hers? should be talking about ?a Nokia 7390? 
so as to satisfy sentiment consistency. 
4.2 Entity and Opinion Word Association
One of the most important factors determining 
the orientation of opinions is the opinion words 
that opinion holders use to express their opi-
nions. Different entities may be modified by 
different opinion words. We can use their asso-
ciation information with entities (both objects 
and attributes) to identify their coreferences. 
Opinion Words: In most cases, opinions in 
sentences are expressed using opinion words.
For example, the sentence, ?The picture quality 
is amazing?, expresses a positive opinion on the 
?picture quality? attribute because of the posi-
tive opinion word ?amazing?.  
Researchers have compiled sets of such 
words for adjectives, adverbs, verbs, and nouns 
respectively. Such lists are collectively called 
the opinion lexicon. We obtained an opinion 
lexicon from the authors of (Ding et al 2009).  
It is useful to note that opinion words used to 
express opinions on different entities are usually 
different apart from some general opinion words 
such as good, great, bad, etc, which can express 
opinions on almost anything. For example, we 
have the following passage:  
?i love the nokia n95 but not sure how 
strong the flash would be? And also it is quite 
expensive, so anyone got any ideas??
Here ?strong? is an opinion word that expresses 
a positive opinion on ?the flash?, but is seldom 
used to describe ?the nokia n95?. ?expensive?, 
on the other hand, should not be associated with 
?the flash?, but is an opinion word that indicates 
a negative opinion on ?the nokia n95?. So ?the 
nokia n95? is more likely to be the antecedent 
of ?it? in the second sentence.  
The question is how to find such associations 
of entities and opinion words. We use their co-
occurrence information to measure, i.e., the 
pointwise mutual information of the two terms. 
First, we estimate the probability of P(NP),
P(OW) and P(NP&OW). Here NP means a noun 
phrase, e.g., an object (attribute) after removing 
determiners, and OW means an opinion word. 
To compute the probability, we first count the 
occurrences of the words. Then the probability 
is computed as follow: 
272
????? ??? ? 
?????????????
??????????????????
where NumofS is a function that gives the num-
ber of sentences that contain the particular word 
string. P(NP, OW) is computed in the same 
way. Let us use the previous example again. We 
compute P(?nokia n95?,?expensive?) as the 
number of sentences containing both ?nokia 
n95? and ?expensive? divided by the total num-
ber of sentences in the whole corpus. 
Then we use the pointwise mutual informa-
tion between a noun phrase and an opinion word 
to measure the association. 
??????? ??? ? ???
????? ???
??????????

However, this PMI value cannot be encoded 
directly as a feature as it only captures the local 
information between antecedent candidates and 
opinion words. That is, it cannot be used as a 
global feature in the classifier. We thus rank all 
possible antecedents of anaphor ?j based on 
their PMI values and use the ranking as the fea-
ture value. The highest ranked antecedent ?i has 
value 1; the second one has value 2 and so on. 
The candidates ranked below the fourth place 
all have the value 5. In the example above, if 
PMI(?nokia n95?, ?expensive?) is greater than 
PMI(?flash?, ?expensive?), the feature for ?no-
kia n95? and ?it? pair will have a smaller value 
than the feature for the ?flash? and ?it? pair.
One may ask if we can use all adjectives and 
adverbs to associate with objects and attributes 
rather than just opinion words since most opi-
nion words are adjectives and adverbs. We 
tested that, but the results were poor. We be-
lieve the reason is that there are many adjectives 
and adverbs which are used for all kinds of pur-
poses and may not be meaningful for our task.  
4.3 String Similarity Feature
Soon et al (2001) has a string match feature 
(SOON STR), which tests whether the two noun 
phrases are the same string after removing de-
terminers from each. Ng and Cardie (2002) split 
this feature into several primitive features, de-
pending on the type of noun phrases. They re-
place the SOON STR feature with three features 
? PRO STR, PN STR, and WORDS STR ? 
which restrict the application of string matching 
to pronouns, proper names, and non-pronominal 
noun phrases, respectively.  
In the user generated opinion data, these may 
not be sufficient. For a certain product, people 
can have a large number of ways to express it. 
For example, we have 
?Panasonic TH50PZ700U VS TH50PZ77U, 
Which Plasma tv should I go for. The TH77U 
is about $500.00 more than the 700U.?
Here ?TH77U? is the same entity as ?Panasonic 
TH50PZ77U?, and ?TH50PZ700U? is the same 
as ?700U?. But they cannot be easily identified 
by ?same string? features mentioned above. Al-
though ?700U? can be solved using substring 
features, ?TH77U? is difficult to deal with. 
We employ a modified edit distance to com-
puting a similarity score between different men-
tions and use that as a feature in our system. 
When one candidate is a substring of another, 
return 1; otherwise, 1 plus the edit distance. 
4.4 Other Useful Features 
In the machine learning approach introduced by 
Soon et al (2001), they had several general fea-
tures that can deal with various kinds of entities, 
e.g., semantic class agreement features dealing 
with different semantic classes like date, loca-
tion, etc., and the gender agreement feature re-
lated to personal entities. However, these fea-
tures are not so useful for our task because the 
semantic class of a product in one domain is 
usually consistent, and dates and locations are 
unlikely to be of any products that people will 
express their opinions. Moreover, we do not 
study opinion holders (as they are known in the 
Web environment), so personal entities are not 
the aspect that we concentrate on. Thus we did 
not use the following features: semantic class 
agreement features, the gender agreement fea-
ture, and appositive feature.
However, we added some specific features, 
which are based on two extracted entities, ?i and 
?j, where ?i is the potential antecedent and ?j is 
the potential anaphor:  
Is-between feature: Its possible values are 
true and false. If the words between ?i and ?j
have an is-like verb (i.e., is, are, was, were, and 
be) between them and there is no comparative 
indicators, this feature has the value of true, 
e.g., ?The nokia e65 is a good handset.?
In sentences similar to this example, the enti-
ties before and after ?is? usually refer to the 
same object or attribute by a definition relation. 
273
And the value of this feature will be true. 
If ?is? appears together with a comparative 
word, it is probably an indication that the two 
entities are different, and the value for this fea-
ture will be false, e.g., ?Overall the K800 is far 
superior to the W810.?
Has-between feature: Its possible values are 
also true and false. If the words between ?i and 
?j have a has-like verb (i.e., has, have, and had), 
the value is true, and otherwise false, e.g., ?The
k800 has a 3.2 megapixel camera.?
This feature usually indicates a ?part-of? rela-
tion if ?has? appears between two entities. They 
do not refer to the same entity. Table 1 gives a 
summary of all the features used in our system. 
5 Experiments and Discussions 
5.1 Datasets 
For evaluation, we used forum discussions from 
three domains, mobile phones, plasma and LCD 
TVs, and cars. Table 2 shows the characteristics 
of the three data sets. Altogether, we down-
loaded 64 discussion threads, which contain 453 
individual posts with a total of 3939 sentences. 
All the sentences and product names were anno-
tated strictly following the MUC-7 coreference 
task annotation standard4. Here is an example: 
?Phil had <COREF ID = "6" TYPE = 
"OBJ">a z610</COREF> which has <COREF 
ID = "7" TYPE = "ATTR">a 2MP cema-
ra</COREF>, and he never had a problem 
with <COREF ID = "8" TYPE = "OBJ" REF = 
"6">it</COREF>.?
ID and REF features are used to indicate that 
there is a coreference link between two strings. 
ID is arbitrary but uniquely assigned to each 
noun phrase. REF uses the ID to indicate a core-
ference link. ?TYPE? can be ?OBJ? (an object 
or a product), or ?ATTR? (an attribute of an 
object). The annotation was done by the first 
author and another student before the algorithm 
construction, and the annotated data sets will be 
made public for other researchers to use. 
For our experiments, we used the J48-
decision tree builder in WEKA, a popular 
of machine learning suite developed at the  Uni-
versity of Waikato. We conducted 10-fold cross 
validation on each dataset.  
4 http://www-nlpir.nist.gov/related_projects/muc/procee- 
dings/co_task.html
The performances are measured using the 
standard evaluation measures of precision (p),
recall (r) and F-score (F), F = 2pr/(p+r). As we 
stated in Section 3, we are only interested in 
object and attributes noun phrases. So in the 
testing phrases, we only compute the precision 
and recall based on those pairs of candidates 
that contain at least one object or attribute noun 
phrase in each pair. If both of the candidates are 
not an object or an attribute, we ignore them. 
5.2 Baseline
As the baseline systems, we duplicated two rep-
resentative systems. Baseline1 is the decision 
tree system in Soon et al (2001). We do not use 
the semantic class agreement feature, gender 
agreement feature and appositive feature in the 
original 12 features for the reason discussed in 
Section 4.4. Thus, the total number of features 
in Baseline1 is 9. The second baseline (base-
line2) is based on the centering theory from the 
semantic perspective introduced by Fang et al 
(2009). Centering theory is a theory about the 
local discourse structure that models the interac-
tion of referential continuity and the salience of 
discourse entities in the internal organization of 
a text. Fang et al (2009) extended the centering 
theory from the grammar level to the semantic 
level in tracking the local discourse focus. 
5.3 Results Analysis 
Table 3 gives the experimental results of the 
two baseline systems and our system with dif-
ferent features included. From Table 3, we can 
make several observations.  
(1) Comparing the results of Baseline1 and our 
system with all features (Our System (All)), 
the new features introduced in this paper 
improves Baseline1 on average by more 
than 9% in F-score.
(2) Comparing the results of Baseline2 and our 
system with all features (Our System (All)), 
our system performs better than Baseline2 
by about 3 - 5%. We also observe that cen-
tering theory (Baseline2) is indeed better 
than the traditional decision tree. 
(3) Our system with sentiment consistency (SC) 
makes a major difference. It improves Base-
line1 (our method is based on Baseline1) by 
5-6% in F-score.  
(4) With the additional feature of entity and 
opinion association (EOA), the results are 
274
improved further by another 2-4%. 
(5)  Our system with all features (row 5) per-
forms the best. 
Paired t-tests were performed on the three 
systems, i.e., baseline1, baseline2, and our sys-
tem (row 5). The tests show that the improve-
ments of our method over both Baseline1 and 
Baseline2 are significant at the confidence level 
of 95% for the first two datasets. For the third 
dataset, the improvement over Baseline1 is also 
significant at the confidence level of 95%, while 
the improvement over Baseline2 is significant at 
the confidence level of 90%.  
In summary, we can conclude that the new 
technique is effective and is markedly better 
than the existing methods. It is clear that the 
new features made a major difference.  
6 Conclusion
This paper investigated the coreference resolu-
tion problem in the opinion mining context. In 
particular, it studied object and attribute resolu-
tions which are crucial for improving opinion 
mining results. Although we still took the su-
pervised learning approach, we proposed sev-
eral novel features in the opinion mining con-
text, e.g., sentiment consistency, and ob-
ject/attribute and opinion word associations. 
Experimental results using forum posts demon-
strated the effectiveness of the proposed tech-
nique. In our future work, we plan to further 
improve the method and discover some other 
opinion related features that can be exploited to 
produce more accurate results. 
Feature category Feature Remark 
Opinion mining 
based features 
Opinion consistency 1, if the opinion orientation of ?i is the same as ?j, 0 if 
the opinions are different, else 2  
Entity and opinion words 
association
1, 2, 3, 4, 5 which indicate the rank positive based on the 
PMI value introduced in Section 4.2 
 grammatical i-Pronoun feature 1, if ?i is a pronoun, else 0 
j-Pronoun feature 1, if ?j is a pronoun, else 0 
Number agreement feature 1, if both of the noun phrases agree in numbers, else 0 
Definite feature 1, if ?j starts with the word ?the?, else 0 
Demonstrative feature 1, if ?j starts with the word ?this?, ?that?, ?those?, or 
?these?, else 0 
Both proper-name feature 1, if ?i and ?j are both proper names, else 0 
lexical String similarity The string similarity score between ?i and ?j
Alias feature  1, If ?i is an alias of ?j or vice versa, else 0 
Others Distance feature The sentence distance between the pair of noun phrases, 
0 if they are in the same sentence 
Keywords between features 1, if some keywords exist between ?i and ?j, else 0. De-
tails are discussed in Section 4.5 
Table 1: Feature list: ?i denotes the antecedent candidate and ?j the anaphor candidate 
 Posts Sentences 
Phone 168 1498 
TVs 173 1376 
Cars 112 1065 
Total 453 3939 
Table 2: Characteristics of the datasets
  Cellphone TVs Cars 
  p r F p r F p r F
1 Baseline1 0.66 0.57 0.61 0.67 0.61 0.64 0.70 0.63 0.66 
2 Baseline2 0.70 0.64 0.67 0.72 0.65 0.68 0.76 0.70 0.73 
3 Our System (SC) 0.71 0.64 0.67 0.73 0.66 0.69 0.74 0.69 0.72 
4 Our System (SC+EOA) 0.74 0.68 0.71 0.74 0.68 0.71 0.77 0.71 0.74 
5 Our System (All) 0.75 0.70 0.72 0.76 0.70 0.73 0.78 0.73 0.75 
Table 3: Results of object and attribute coreference resolution 
275
References  
A. Andreevskaia and S. Bergler. 2006. Mining 
WordNet for Fuzzy Sentiment: Sentiment Tag Ex-
traction from WordNet Glosses. EACL?06. 
J. Carbonell and R. Brown. 1988. Anaphora resolu-
tion: a multi-strategy approach. COLING?1988.
G. Carenini, R. Ng, and A. Pauls. 2006. Interactive 
multimedia summaries of evaluative text. IUI?06.
X. Ding, B. Liu and L. Zhang. 2009. Entity Discov-
ery and Assignment for Opinion Mining Applica-
tion. KDD?09.  
A. Esuli and F. Sebastiani. 2006. Determining Term 
Subjectivity and Term Orientation for Opinion 
Mining, EACL?06. 
J. Fan, K. Barker and B. Porter. 2005. Indirect ana-
phora resolution as semantic path search. K-
CAP?05. 
C. Gasperin and T. Briscoe. 2008. Statistical ana-
phora resolution in biomedical texts. COLING'08 
B. J. Grosz, A. K. Joshi and S. Weinstein. 1995. 
Centering: a framework for modeling the local 
coherence of discourse. Computational Linguis-
tics, 21(2). 
V. Hatzivassiloglou and K. McKeown. 1997. Pre-
dicting the Semantic Orientation of Adjectives.
ACL-EACL?97.  
M. Hu and B. Liu. 2004. Mining and summarizing 
customer reviews. KDD?04. 
N. Jindal, and B. Liu. 2006. Mining Comparative 
Sentences and Relations. AAAI?06. 
H. Kanayama and T. Nasukawa. 2006. Fully Auto-
matic Lexicon Expansion for Domain-Oriented 
Sentiment Analysis. EMNLP?06.  
S. Kim and E. Hovy. 2004. Determining the Senti-
ment of Opinions. COLING?04. 
N. Kobayashi, K. Inui and Y. Matsumoto. 2007. Ex-
tracting Aspect-Evaluation and Aspect-of Rela-
tions in Opinion Mining. EMNLP-CoNLL?07. 
F. Kong, G. Zhou, Q. Zhu and P. Qian. 2009. Em-
ploying the Centering Theory in Pronoun Resolu-
tion from the Semantic Perspective. EMNLP?09. 
L.-W. Ku, Y.-T. Liang and H.-H. Chen. 2006. Opi-
nion Extraction, Summarization and Tracking in 
News and Blog Corpora. CAAW'06. 
B. Liu. 2006. Web Data Mining, Springer.  
R. McDonald, K. Hannan, T Neylon, M. Wells, and 
J.Reynar. 2007. Structured Models for Fine-to-
Coarse Sentiment Analysis. ACL-07. 
Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. 
2007. Topic Sentiment Mixture: Modeling Facets 
and Opinions in Weblogs. WWW?07. 
T. S. Morton. 2000. Coreference for NLP applica-
tions. ACL?00. 
V. Ng and C. Cardie. 2002. Improving machine 
learning approaches to coreference resolution.
ACL?02. 
V. Ng. 2007. Semantic Class Induction and Corefe-
rence Resolution. ACL?07. 
V. Ng. 2008. Unsupervised Models for Coreference 
Resolution. EMNLP?08. 
N. Nicolov, F. Salvetti and S. Ivanova, Sentiment 
analysis: Does coreference matter? AISB'2008. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. 
Thumbs up? Sentiment Classification Using Ma-
chine Learning Techniques. EMNLP?02.  
H. Poon and P. Domingos. 2008. Joint Unsupervised 
Coreference Resolution with Markov Logic.
EMNLP?08, 650?659. 
A-M. Popescu and O. Etzioni. 2005. Extracting 
product features and opinions from reviews.
EMNLP?05. 
Qiu, Guang, B. Liu, J. Bu and C. Chen. 2009 Ex-
panding Domain Sentiment Lexicon through 
Double Propagation. IJCAI 2009. 
W. M. Soon, H. T. Ng and D. Lim. 2001. A machine 
learning approach to coreference resolution of 
noun phrase. Computational Linguistics, 27(4). 
V. Stoyanov, C. Cardie. 2006. Partially supervised 
coreference resolution for opinion summarization 
through structured rule learning. EMNLP?06.
I. Titov and R. McDonald. 2008, A joint model of 
text and aspect ratings for sentiment summariza-
tion, ACL?08. 
J. Tetreault. 2001. A corpus-based evaluation of cen-
tering and pronoun resolution. Computational 
Linguistics. 27(4):507-520. 
P. Turney. 2002. Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL?02.  
Y. Versley, A. Moschitti, M. Poesio and X. Yang. 
2008. Coreference systems based on kernels me-
thods. COLING?08. 
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how 
mad are you? Finding strong and weak opinion 
clauses. AAAI?04. 
X. F. Yang? J. Su and C. L. Tan. 2005. Improving 
Pronoun Resolution Using Statistics - Based Se-
mantic Compatibility Information. ACL?05. 
X. F. Yang and J. Su. 2007. Coreference Resolution 
Using Semantic Relatedness Information from Au-
tomatically Discovered Patterns. ACL?07. 
G. D. Zhou and J. Su. 2004. A high-performance 
coreference resolution system using a multi-agent 
strategy. COLING?04. 
276
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1272?1280,
Beijing, August 2010
Grouping Product Features Using Semi-Supervised Learning
with Soft-Constraints* 
Zhongwu Zhai?, Bing Liu?, Hua Xu? and Peifa Jia?
?State Key Lab of Intelligent Tech. & Sys. 
Tsinghua National Lab for Info. Sci. and Tech. 
Dept. of Comp. Sci. & Tech., Tsinghua Univ. 
zhaizhongwu@gmail.com
?Dept. of Comp. Sci.
University of Illinois at Chicago 
liub@cs.uic.edu
Abstract
In opinion mining of product reviews, one of-
ten wants to produce a summary of opinions 
based on product features/attributes. Howev-
er, for the same feature, people can express it 
with different words and phrases. To produce 
a meaningful summary, these words and 
phrases, which are domain synonyms, need to 
be grouped under the same feature group. 
This paper proposes a constrained semi-
supervised learning method to solve the prob-
lem. Experimental results using reviews from 
five different domains show that the proposed 
method is competent for the task. It outper-
forms the original EM and the state-of-the-art 
existing methods by a large margin. 
1 Introduction*
One form of opinion mining in product reviews 
is to produce a feature-based summary (Hu and 
Liu, 2004a; Liu, 2010). In this model, product 
features are first identified, and positive and neg-
ative opinions on them are aggregated to produce 
a summary on the features. Features of a product 
are attributes, components and other aspects of 
the product, e.g., ?picture quality?, ?battery life? 
and ?zoom? of a digital camera. 
In reviews (or any writings), people often use 
different words and phrases to describe the same 
product feature. For example, ?picture? and 
?photo? refer to the same feature for cameras. 
Grouping such synonyms is critical for effective 
opinion summary. Although WorldNet and other 
*Supported by National Natural Science Foundation of Chi-
na (Grant No: 60875073). 
    This work was done when the first author was visiting 
Bing Liu?s group at the University of Illinois at Chicago.  
thesaurus dictionaries can help to some extent, 
they are far from sufficient due to a few reasons. 
First, many words and phrases that are not syn-
onyms in a dictionary may refer to the same fea-
ture in an application domain. For example, ?ap-
pearance? and ?design? are not synonymous, but 
they can indicate the same feature, design.
Second, many synonyms are domain dependent. 
For example, ?movie? and ?picture? are syn-
onyms in movie reviews, but they are not syn-
onyms in camera reviews as ?picture? is more 
likely to be synonymous to ?photo? while ?mov-
ie? to ?video?. Third, determining which expres-
sions indicate the same feature can be dependent 
on the user?s application need. For example, in 
car reviews, internal design and external design 
can be regarded as two separate features, but can 
also be regarded as one feature, called ?design?, 
based to the level of details that the user needs to 
study. In camera reviews, one may want to study 
battery as a whole (one feature), or as more than 
one feature, e.g., battery weight, and battery life. 
Due to this reason, in applications the user needs 
to be involved in synonym grouping.  
Before going further, let us introduce two con-
cepts, feature group and feature expression. Fea-
ture group (or feature for short) is the name of a 
feature (given by the user), while a feature ex-
pression of a feature is a word or phrase that ac-
tually appears in a review to indicate the feature. 
For example, a feature group could be named 
?picture quality?, but there are many possible 
expressions indicating the feature, e.g., ?picture?, 
?photo?, ?image?, and even the ?picture quality? 
itself. All the feature expressions in a feature 
group signify the same feature.  
Grouping feature expressions manually into 
suitable groups is time consuming as there are 
1272
often hundreds of feature expressions. This paper 
helps the user to perform the task more efficient-
ly. To focus our research, we assume that feature 
expressions have been discovered from a review 
corpus by an existing system such as those in 
(Hu and Liu, 2004b; Popescu and Etzioni, 2005; 
Kim and Hovy, 2006; Kobayashi et al, 2007; 
Mei et al, 2007; Stoyanov and Cardie, 2008; Jin
et al, 2009; Ku et al, 2009). 
To reflect the user needs, he/she can manually 
label a small number of seeds for each feature 
group. The feature groups are also provided by 
the user based on his/her application needs. The 
system then assigns the rest of the feature ex-
pressions to suitable groups. To the best of our 
knowledge, this problem has not been studied in 
opinion mining (Pang and Lee, 2008).  
The problem can be formulated as semi-
supervised learning. The small set of seeds la-
beled by the user is the labeled data, and the rest 
of the discovered feature expressions are the un-
labeled data. This is the transductive setting 
(Joachims, 1999) because the unlabeled set is 
used in learning and also in testing since our ob-
jective is to assign unlabeled expressions to the 
right feature groups.  
Any semi-supervised learning method can be 
applied to tackle the problem. In this work, we 
use the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). Specifically, we 
use the na?ve Bayesian EM formulation in 
(Nigam et al, 2000), which runs a Bayesian clas-
sifier iteratively on the labeled and unlabeled 
data until the probabilities for the unlabeled data 
converge. When the algorithm ends, each unla-
beled example is assigned a posterior probability 
of belonging to each group.  
However, we can do better since the EM algo-
rithm only achieves local optimal. What local 
optimal it achieves depends on the initialization, 
i.e., the initial seeds. We show that some prior 
knowledge can help provide a better initialization, 
and consequently generate better grouping results. 
Thus, we propose to create another set of data 
extracted from the unlabeled set based on two 
pieces of natural language knowledge: 
1. Feature expressions sharing some common 
words are likely to belong to the same group, 
e.g., ?battery life? and ?battery power?. 
2. Feature expressions that are synonyms in a 
dictionary are likely to belong to the same 
group, e.g., ?movie? and ?picture?.  
We call these two pieces of prior knowledge soft 
constraints because they constrain the feature 
expressions to be in the same feature group. The 
constraints are soft (rather than hard) as they can 
be relaxed in the learning process. This relaxa-
tion is important because the above two con-
straints can result in wrong groupings. The EM 
algorithm is allowed to re-assign them to other 
groups in the learning process.  
We call the proposed framework constrained 
semi-supervised learning. Since we use EM and 
soft constraints, we call the proposed method SC-
EM. Clearly, the problem can also be attempted 
using some other techniques, e.g., topic modeling 
(e.g, LDA (Blei et al, 2003)), or clustering using 
distributional similarity (Pereira et al, 1993; Lin, 
1998; Chen et al, 2006; Sahami and Heilman, 
2006). However, our results show that these me-
thods do not perform as well. 
The input to the proposed algorithm consists 
of: a set of reviews R, and a set of discovered 
feature expressions F from R (using an existing 
algorithm). The user labels a small set of feature 
expressions, i.e., assigning them to the user-
specified feature groups. The system then assigns 
the rest of the discovered features to the feature 
groups. EM is run using the distributional (or 
surrounding words) contexts of feature expres-
sions in review set R to build a na?ve Bayesian 
classifier in each iteration.  
Our evaluation was conducted using reviews 
from 5 different domains (insurance, mattress, 
vacuum, car and home-theater). The results show 
that the proposed method outperforms different 
variations of the topic modeling method LDA, k-
means clustering, and the recent unsupervised 
feature grouping method mLSA.  
In summary, this paper makes three main con-
tributions:
1. It proposes a new sub-problem of opinion 
mining, i.e., grouping feature expressions in 
the context of semi-supervised learning. Al-
though there are existing methods for solving 
the problem based on unsupervised learning, 
we argue that for practical use some form of 
supervision from the user is necessary to let 
the system know what the user wants.  
2. An EM formulation is used to solve the prob-
lem. We augment EM with two soft con-
straints. These constraints help guide EM to 
1273
produce better solutions. We note that these 
constraints can be relaxed in the process to 
correct the imperfection of the constraints.  
3. It is shown experimentally the new method 
outperforms the main existing state-of-the-art 
methods that can be applied to the task.  
2 Related Work 
This work is mainly related to existing research 
on synonyms grouping, which clusters words and 
phrases based on some form of similarity.  
The methods for measuring word similarity 
can be classified into two main types (Agirre et 
al., 2009): those relying on pre-existing know-
ledge resources (e.g., thesauri, or taxonomies) 
(Yang and Powers, 2005; Alvarez and Lim, 2007; 
Hughes and Ramage, 2007), and those based on 
distributional properties (Pereira et al, 1993; 
Lin, 1998; Chen et al, 2006; Sahami and 
Heilman, 2006; Pantel et al, 2009).   
In the category that relies on existing know-
ledge sources, the work of Carenini et al (2005) 
is most related to ours. The authors proposed a 
method to map feature expressions to a given 
domain feature taxonomy, using several similari-
ty metrics on WordNet. This work does not use 
the word distribution information, which is its 
main weakness because many expressions of the 
same feature are not synonyms in WordNet as 
they are domain/application dependent. Dictiona-
ries do not contain domain specific knowledge, 
for which a domain corpus is needed.
Another related work is distributional similari-
ty, i.e., words with similar meaning tend to ap-
pear in similar contexts (Harris, 1968). As such, 
it fetches the surrounding words as context for 
each term. Similarity measures such as Cosine,
Jaccard, Dice, etc (Lee, 1999), can be employed 
to compute the similarities between the seeds and 
other feature expressions. To suit our need, we 
tested the k-means clustering with distributional 
similarity. However, it does not perform as well 
as the proposed method.  
Recent work also applied topic modeling (e.g., 
LDA) to solve the problem. Guo et al (2009) 
proposed a multilevel latent semantic association 
technique (called mLSA) to group product feature 
expressions, which runs LDA twice. However, 
mLSA is an unsupervised approach. For our eval-
uation, we still implemented the method and 
compared it with our SC-EM method.  
Our work is also related to constrained cluster-
ing (Wagstaff et al, 2001), which uses two forms 
of constraints, must-link and cannot-link. Must-
links state that some data points must be in the 
same cluster, and cannot-links state that some 
data points cannot be in the same cluster. In 
(Andrzejewski et al, 2009), the two constraints 
are added to LDA, called DF-LDA. We show 
that both these methods do not perform as well as 
our semi-supervised learning method SC-EM.
3 The Proposed Algorithm 
Since our problem can be formulated as semi-
supervised learning, we briefly describe the set-
ting in our context. Given a set C of classes (our 
feature groups), we use L to denote the small set 
of labeled examples (labeled feature expressions 
or seeds), and U the set of unlabeled examples 
(unlabeled feature expressions). A classifier is 
built using L and U to classify every example in 
U to a class. Several existing algorithms can be 
applied. In this work, we use EM as it is efficient 
and it allows prior knowledge to be used easily. 
Below, we first introduce the EM algorithm that 
we use, and then present our augmented EM. The 
constraints and their conflict handling are dis-
cussed in Section 4.  
3.1 Semi-Supervised Learning Using EM 
EM is a popular iterative algorithm for maximum 
likelihood estimation in problems with missing 
data. In our case, the group memberships of the 
unlabeled expressions are considered missing 
because they come without group labels.  
We use the EM algorithm based on na?ve 
Bayesian classification (Nigam et al, 2000). Al-
though it is involved to derive, using it is simple. 
First, a classifier f is learned using only the la-
beled data L (Equations 1 and 2). Then, f is ap-
plied to assign a probabilistic label to each unla-
beled example in U (see Equation 3). Next, a 
new classifier f is learned using both L and the 
newly probabilistically labeled unlabeled exam-
ples in UPL, again using Equations 1 and 2. These 
last two steps iterate until convergence. 
We now explain the notations in the Equations. 
Given a set of training documents D, each docu-
ment di in D is considered as an ordered list of 
words. ????? denotes the k
th word in di, where 
each word is from the vocabulary V={w1, w2,?,
w|V|}. C={c1, c2,?, c|C|} is the set of pre-defined 
1274
classes or groups. Nti is the number of times the 
word wt occurs in document di.
For our problem, the surrounding words con-
texts of the labeled seeds form L, while the sur-
rounding words of the non-seed feature expres-
sions form U. When EM converges, the classifi-
cation labels of the unlabeled feature expressions 
give us the final grouping. Surrounding words 
contexts will be discussed in Section 5. 
3.2 Proposed Soft-Constrained EM 
Although EM can be directly applied to deal with 
our problem, we can do better. As we discussed 
earlier, EM only achieves local optimal based on 
the initialization, i.e., the labeled examples or 
seeds. We show that natural languages con-
straints can be used to provide a better initializa-
tion, i.e., to add more seeds that are likely to be 
correct, called soft-labeled examples or soft seeds 
(SL). Soft-labeled examples are handled diffe-
rently from the original labeled examples in L.
With the soft seeds, we have the proposed soft-
constrained EM (called SC-EM). 
Compared with the original EM, SC-EM has 
two main differences:
y Soft constraints are applied to L and U to pro-
duce a set SL of soft-labeled examples (or soft 
seeds) to initialize EM in addition to L. SL is 
thus a subset of U. The training set size is in-
creased, which helps produce better results as 
our experimental results show.  
y In the first iteration of EM, soft-labeled ex-
amples SL are treated in the same way as the 
labeled examples in L. Thus both SL and L are 
used as labeled examples to learn the initial 
classifier f0. However, in the subsequent itera-
tions, SL is treated in the same way as any ex-
amples in U. That is, the classifier fx from 
each iteration x (including f0) will predict U.
After that, a new classifier is built using both 
L and UPL (which is U with probabilistic la-
1 Laplace smoothing is used to prevent zero probabilities for 
infrequently occurring words. 
bels). Clearly, this implies that the class labels 
of the examples in SL are allowed to change. 
That is also why we call SL the soft-labeled 
set in contrast to the hard-labeled set L, i.e., 
the examples in L will not change labels in 
EM. The reason that SL is allowed to change 
labels/classes is because the constraints can 
make mistakes. EM may be able to correct 
some of the mistakes. 
The detailed algorithm is given in Figure 1. The 
constraints are discussed in Section 4. 
4 Generating SL Using Constraints 
As mentioned earlier, two forms of constraints 
are used to induce the soft-labeled set SL. For 
easy reference, we reproduce them here:  
1. Feature expressions sharing some common 
words are likely to belong to the same group. 
2. Feature expressions that are synonyms in a 
dictionary are likely to belong to one group.  
According to the number of words, feature ex-
pressions can be categorized into single-word 
expressions and phrase expressions. They are 
handled differently. The detailed algorithm is 
given in Figure 2. In the algorithm, L is the la-
beled set and U is the unlabeled set. L, in fact, 
consists of a set of sets, L = {L1, L2, ?, L|L|}. 
Each Li contains a set of labeled examples (fea-
ture expressions) of the ith class (feature group). 
Similarly, the output set SL (the soft-labeled set) 
also consists of a set of sets, i.e., SL = {SL1,
SL2, ?, SL|L|}. Each SLi is a set of soft-labeled 
examples (feature expressions) of the ith class 
???? ??? ?
? ? ? ???? ??????
???
???
??? ? ? ? ???? ??????
???
???
???
???
(11)
? ??? ?
? ? ? ? ??????
???
???
??? ? ???
(21)
? ?????? ?
? ???? ??????? ???
????
???
? ????
???
??? ? ??????????
????
???
(3)
Input:
- Labeled examples L
- Unlabeled examples U
1 Extract SL from U using constraints (Section 4); 
2 Learn an initial na?ve Bayesian classifier f0 using L
? SL and Equations 1 and 2; 
3 repeat
4 // E-Step 
5 for each example di in U (including SL) do
6 Using the current classifier fx to compute 
P(cj|di) using Equation 3. 
7 end
8 // M-Step 
9 Learn a new na?ve Bayesian classifier fx from L
and U by computing P(wt|cj) and P(cj) using 
Equations 1 and 2. 
10 until the classifier parameters stabilize 
Output: the classifier fx from the last iteration.
 Figure 1. The proposed SC-EM algorithm  
1275
(feature group). Thus Li and SLi correspond to 
each other as they represent the original labeled 
examples and the newly soft-labeled examples of 
the ith class (or feature group) respectively.  
The algorithm basically compares each fea-
ture expression u in U (line 1) with each feature 
expression e (line 4) in every labeled subset Li
(line 2) based on the above two constraints. If 
any of the constraints is satisfied (lines 5-17), it 
means that u is likely to belong to Li (or the i
th
class or feature group), and it is added to SLi.
There are conflict situations that need to be re-
solved. That is, u may satisfy a constraint of 
more than one labeled sub-set Li. For example, if 
u is a single word, it may be synonyms of feature 
expressions from more than one feature groups. 
The question is which group it is likely to belong. 
Further, u may be synonyms of a few single-
word feature expressions in Li. Clearly, u being a 
synonym of more than one word in Li is better 
than it is only the synonym of one word in Li.
Similar problems also occur when u is an ele-
ment of a feature expression phrase e.
To match u and e, there are a few possibilities. 
If both u and e are single words (lines 5-6), the 
algorithm checks if they are synonyms (line 7). 
The score in line 8 is discussed below. When one 
of u and e is a phrase, or both of them are phrases, 
we see whether they have shared words. Again, 
conflict situations can happen with multiple 
classes (feature groups) as discussed above. Note 
that in these cases, we do not use the synonym 
constraint, which does not help in our test.  
Given these complex cases, we need to decide 
which class that u should be assigned to or 
should not be assigned to any class (as it does not 
meet any constraint). We use a score to record 
the level of satisfaction. Once u is compared with 
each e in every class, the accumulated score is 
used to determine which class Li has the strong-
est association with u. The class j with the high-
est score is assigned to u. In other words, u is 
added to SLj. Regarding the score value, syn-
onyms gets the score of 1 (line 8), and intersec-
tion (shared words) gets the score equal to the 
size of the intersection (lines 10-17). 
5 Distributional Context Extraction 
To apply the proposed algorithm, a document di
needs to be prepared for each feature expression 
ei for na?ve Bayesian learning. di is formed by 
aggregating the distributional context of each 
sentence sij in our corpus that contains the ex-
pression ei. The context of a sentence is the sur-
rounding words of ei in a text window of [-t, t], 
including the words in ei. Given a relevant cor-
pus R, the document di for each feature expres-
sion ei in L (or U) is generated using the algo-
rithm in Figure 3. Stopwords are removed. 
1 for each feature expression ei in L (or U) do
2       Si ? all sentences containing ei in R;
3       for each sentence sij ? Si do
4            dij ? words in a window of [-t, t] on the left 
and right (including the words in ei);
5       di ? words from all dij, j = 1, 2, ?, |Si|; 
          // duplicates are kept as it is not union
Figure 3. Distributional context extraction 
For example, a feature expression from L (or 
U) is ei = ?screen? and there are two sentences in 
our corpus R that contain ?screen?
si1 = ?The LCD screen gives clear picture?.
si2 = ?The picture on the screen is blur?
We use the window size of [-3, 3]. Sentence si1,
gives us di1 = <LCD, screen, give, clear, picture> 
as a bag of words. ?the? and ?is? are removed as 
stopwords. si2 gives us di2 = <picture, screen, 
blur>. ?on?, ?the? and ?is? are removed as stop-
words. Finally, we obtain the document di for 
feature expression ei as a bag of words: 
di = <LCD, screen, give, clear, picture,
picture, screen, blur> 
6 Empirical Evaluation 
This section evaluates the SC-EM algorithm and 
compares it with the main existing methods that 
can be applied to solve the problem.   
1  for each feature expression u ? U do
2 for each feature group Li ? L do
3 score(Li) ? 0; 
4 for each feature expression e ? Li do
5 if u is a single word expression then
6 if e is a single word expression then
7 if u and e are synonyms then
8 score(Li) ? score(Li) + 1; 
9 else if w ? e then  // e is a phrase 
10 score(Li) ? score(Li) + 1 
11 else  // u is a phrase 
12 if e is a single word expression then
13 if e ? u then  // u is a phrase 
14 score(Li) ? score(Li) + 1 
15 else
16 s ? e ? u;
17 score(Li) ? score(Li) + |s|
18 u is added to SLj s.t. ???????? ?????????
Figure 2. Generating the soft-labeled set SL
1276
6.1 Review Data Sets and Gold Standards 
To demonstrate the generality of the proposed 
method, experiments were conducted using re-
views from five domains: Hometheater, Insur-
ance, Mattress, Car and Vacuum. All the data 
sets and the gold standard feature expressions 
and groups were from a company that provides 
opinion mining services. The details of the data 
sets and the gold standards are given in Table 1.  
Hometheater Insurance Mattress Car Vacuum
#Sentences 6355 12446 12107 9731 8785
#Reviews 587 2802 933 1486 551
#Feature  
expressions 237 148 333 317 266 
#Feature 
groups 15 8 15 16 28 
Table 1. Data sets and gold standards 
6.2 Evaluation Measures 
Since SC-EM is based on semi-supervised learn-
ing, we can use classification accuracy to eva-
luate it. We can also see it as clustering with ini-
tial seeds. Thus we also use clustering evaluation 
methods. Given gold standards, two popular 
clustering evaluation measures are Entropy and 
Purity (Liu, 2006). As accuracy is fairly standard, 
we will not discuss it further. Below, we briefly 
describe entropy and purity. 
Given a data set DS, its gold partition is G =
{??,?,??,?,??}, where k is the known number 
of clusters. The groups partition DS into k dis-
joint subsets, DS1,?, DSi, ?, DSk.
Entropy: For each resulting cluster, we can 
measure its entropy using Equation 4, where 
Pi(??) is the proportion of ?? data points in DSi.
The total entropy of the clustering (considering 
all clusters) is calculated by Equation 5. 
????????? ??? ? ?? ??????????????
?
???
(4)
???????????? ??
?? ???
????
????????? ???
?
???
(5)
Purity: Purity measures the extent that a clus-
ter contains only data from one gold-partition. 
Each cluster?s purity is computed by Equation 6, 
and the total purity of the whole clustering is 
computed with Equation 7. 
???????? ??? ? ???? ????? (6)
??????????? ??
?? ???
????
???????? ???
?
???
 (7)
In testing, the unlabeled set U is also our test 
set. This is justified because our purpose is to 
assign unlabeled data to appropriate groups.  
6.3 Baseline Methods and Settings 
The proposed SC-EM method is compared with 
a set of existing methods, which can be catego-
rized into unsupervised and semi-supervised me-
thods. We list the unsupervised methods first.  
LDA: LDA is a popular topic modeling me-
thod (see Section 2). Given a set of documents, it 
outputs groups of terms of different topics. In our 
case, each feature expression is a term, and the 
documents refer to the distributional contexts of 
each feature expressions (see Section 5).  
mLSA: This is a state-of-the-art unsupervised 
method for solving the problem. It is based on 
LDA, and has been discussed in related work. 
Kmeans: This is the k-means clustering me-
thod (MacQueen, 1966) based on distributional 
similarity with cosine as the similarity measure. 
In the semi-supervised category, the methods 
are further classified into un-constrained, hard-
constrained, and soft-constrained methods. 
For the un-constrained subclass (no con-
straints are used), we have the following: 
LDA(L, H): This method is based on LDA,
but the labeled examples L are used as seeds for 
each group/topic. All examples in L will always 
stay in the same topic. We call this hard initiali-
zation (H). L is handled similarly below. 
DF-LDA(L, H). DF-LDA is the LDA method 
(Andrzejewski et al, 2009) that takes must-links 
and cannot-links. Our L set can be expressed as a 
combination of must-links and cannot-links. Un-
fortunately, only must-links can be used because 
the number of cannot-links is huge and crashes 
the system. For example, for the car data, the 
number of cannot-links is 194,400 for 10% la-
beled data (see Section 6.4) and for 20% it is 
466,560,000. DF-LDA also has a parameter ?
controlling the link strength, which is set very 
high (=1000) to reflect the hard initialization. We 
did not use DF-LDA in the unsupervised subclass 
above as without constraints it reduces to LDA.
Kmeans(L, H): This method is based on 
Kmeans, but the clusters of the labeled seeds are 
fixed at the initiation and remain unchanged. 
EM(L, H): This is the original EM for semi-
supervised learning. Only the labeled examples 
are used as the initial seeds.  
For the hard-constrained (H) subclass (our 
1277
two constraints are applied and cannot be vi-
olated), we have the following methods (LC is L
plus SL produced by the constraints (C): 
Rand(LC, H): This is an important baseline. It 
shows whether the constraints alone are suffi-
cient to produce good results. That is, the final 
result is the expanded seeds SL plus the rest of U
assigned randomly to different groups.
LDA(LC, H): It is similar to LDA(L,H), but 
both the initial seeds L and the expanded seeds 
SL are considered as labeled examples. They also 
stay in the same topics/groups in the process. 
Note that although SL is called a set of soft-
labeled examples (seeds) in the proposed algo-
rithm, they are treated as hard-labeled examples 
here just for experimental comparison.  
DF-LDA(LC, H): This is DF-LDA with both 
L and SL expressed as must-links. Again, a large 
? (= 1000) is used to make sure that must-links 
for L and SL will not be violated.  
Kmeans(LC,H): It is similar to Kmeans(L,H), 
but both L and SL stay in their assigned clusters.  
EM(LC, H): It is similar to SC-EM, but SL is 
added to the labeled set L, and their classes are 
not allowed to change in the EM iterations.  
For the soft-constrained (S) subclass, our two 
constraints can be violated. Initially, both the 
initial seeds L and the expanded seeds SL are 
considered as labeled data, but subsequently, on-
ly L is taken as the labeled data (i.e., staying in 
the same classes). The algorithm will re-estimate 
the label of each feature expression in SL. This 
subclass has the following methods: 
LDA(LC, S): This is in contrast to LDA(LC, 
H). It allows the SL set to change topics/groups. 
Kmeans(LC, S): This is in contrast to 
Kmeans(LC, H).
A soft DF-LDA is not included here because 
different ? values give different results, and they 
are generally worse than DF-LDA(LC, H).
For all LDA based methods, the topic model-
ing parameters were set to their default values. 
The number of iteration is 1000. We used the 
LDA in MALLET2, and modified it to suit differ-
ent LDA-based methods except DF-LDA, which 
was downloaded from its authors? website3. We 
implemented mLSA, Kmeans and changed EM4
to take soft seeds. For all Kmeans based methods, 
the distance function is the cosine similarity. 
2 http://mallet.cs.umass.edu/ 
3 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html 
4 http://alias-i.com/lingpipe/ 
6.4 Evaluation Results 
We now compare the results of SC-EM and the 
14 baseline methods. To see the effects of differ-
ent numbers of labeled examples (seeds), we ex-
perimented with 10%, 20%, 30%, 40%, and 50% 
of the feature expressions from the gold standard 
data as the labeled set L, and the rest as the unla-
beled set U. All labeled data were selected ran-
domly. For each setting, we run the algorithms 
30 times and report the average results. Due to 
space limitations, we can only show the detailed 
purity (Pur), entropy (Ent) and accuracy (Acc) 
results for 30% as the labeled data (70% as unla-
beled) in Table 2. For the other proportions of 
labeled data, we summarize them in Table 3. 
Each result in Table 3 is thus the average of the 5 
data sets. All the results were obtained from the 
unlabeled set U, which was our test set. For en-
tropy, the smaller the value is the better, but for 
purity and accuracy, the larger the better. For 
these experiments, we used the window size t = 5. 
Section 6.5 studies the effects of window sizes.  
Tables 2 and 3 clearly show that the proposed 
algorithm (SC-EM) outperforms all 14 baseline 
methods by a large margin on every dataset. In 
detail, we observe the following:  
? LDA, mLSA and Kmeans with no seeds (la-
beled data) perform the worst. Seeds help to 
improve the results, which is intuitive. With-
out seeds, DF-LDA is the same as LDA.
? LDA based methods seems to be the weakest. 
Kmeans based methods are slightly better, but 
EM based methods are the best. This clearly 
indicates that classification (EM) performs 
better than clustering. Comparing DF-LDA
and Kmeans, their results are similar.  
? For LDA, and Kmeans, hard-constrained me-
thods (i.e., LDA(L, H), and Kmeans(L, H))
perform better than soft-constrained methods 
(i.e., LDA(LC, S) and Kmeans(LC, S)). This 
indicates that soft-constrained versions may 
change some correctly constrained expres-
sions into wrong groups. However, for the 
EM based methods, the soft-constrained me-
thod (SC-EM) performs markedly better than 
the hard-constrained version (EM(LC, H)). 
This indicates that Bayesian classifier used in 
EM can take advantage of the soft constraints 
and correct some wrong assignments made by 
constraints. Much weaker results of Rand(LC,
H) than SC-EM in different settings show that 
1278
constraints alone (i.e., synonyms and sharing 
of words) are far from sufficient. EM can im-
prove it considerably.  
? Comparing EM based methods, we can see 
that soft seeds in SL make a big difference for 
all data sets. SC-EM is clearly the best.  
? As the number of labeled examples increases 
(from 10% to 50%), the results improve for 
every method (except those for DF-LDA,
which does not change much).  
6.5 Varying the Context Window Size 
We varied the text window size t from 1 to 10 to 
see how it impacts on the performance of SC-EM.
The results are given in Figure 4 (they are aver-
ages of the 5 datasets). Again for purity and ac-
curacy, the greater the value the better, while for 
entropy it is the opposite. It is clear that the win-
dow sizes of 2~6 produce similar good results. 
All evaluations reported above used t = 5. 
7 Conclusion
This paper proposed the task of feature grouping 
in a semi-supervised setting. It argued that some 
form of supervision is needed for the problem 
because its solution depends on the user applica-
tion needs. The paper then proposed to use the 
EM algorithm to solve the problem, which was 
improved by considering two soft constraints. 
Empirical evaluations using 5 real-life data sets 
show that the proposed method is superior to 14 
baselines. In our future work, we will focus on 
further improving the accuracy.  
Methods 
Hometheater Insurance Mattress Car Vacuum 
Acc Pur Ent Acc Pur Ent Acc Pur Ent Acc Pur Ent Acc Pur Ent 
LDA 0.06 0.31 2.54 0.11 0.36 2.24 0.05 0.32 2.57 0.06 0.37 2.39 0.03 0.36 2.09
mLSA 0.06 0.31 2.53 0.14 0.38 2.19 0.06 0.34 2.55 0.09 0.37 2.40 0.03 0.37 2.11
Kmeans 0.21 0.42 2.14 0.25 0.45 1.90 0.15 0.39 2.32 0.25 0.44 2.16 0.24 0.47 1.78
LDA(L, H) 0.10 0.32 2.50 0.16 0.37 2.22 0.10 0.34 2.57 0.19 0.39 2.36 0.10 0.39 2.09
DF-LDA(L, H) 0.27 0.37 2.32 0.25 0.41 2.00 0.19 0.39 2.35 0.28 0.45 2.15 0.31 0.40 1.98
Kmeans(L, H) 0.20 0.42 2.12 0.25 0.43 1.92 0.17 0.42 2.26 0.27 0.48 2.04 0.20 0.48 1.76
EM(L, H) 0.48 0.50 1.93 0.50 0.53 1.69 0.52 0.56 1.87 0.56 0.58 1.80 0.49 0.52 1.79
Rand(CL, H) 0.41 0.46 2.07 0.40 0.46 1.94 0.40 0.47 2.07 0.34 0.41 2.31 0.39 0.52 1.59
LDA(CL, H) 0.44 0.50 1.96 0.42 0.48 1.89 0.42 0.49 1.97 0.44 0.52 1.87 0.43 0.55 1.48
DF-LDA(CL, H) 0.35 0.49 1.86 0.33 0.49 1.71 0.23 0.39 2.26 0.34 0.51 1.88 0.37 0.52 1.58
Kmeans(CL, H) 0.49 0.55 1.70 0.48 0.55 1.62 0.44 0.51 1.91 0.47 0.54 1.80 0.44 0.58 1.42
EM(CL, H) 0.59 0.60 1.62 0.58 0.60 1.46 0.56 0.59 1.74 0.62 0.64 1.54 0.55 0.60 1.44
LDA(CL, S) 0.24 0.35 2.44 0.27 0.40 2.14 0.23 0.37 2.44 0.27 0.41 2.33 0.23 0.41 2.01
Kmeans(CL, S) 0.33 0.46 2.04 0.34 0.45 1.90 0.25 0.43 2.20 0.29 0.47 2.07 0.37 0.50 1.68
SC-EM 0.67 0.68 1.30 0.66 0.68 1.18 0.68 0.70 1.27 0.70 0.71 1.24 0.67 0.68 1.18
Table 2. Comparison results (L = 30% of the gold standard data) 
Methods 
Acc Pur Ent
10% 20% 30% 40% 50% 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%
LDA 0.07 0.07 0.06 0.06 0.08 0.33 0.33 0.34 0.35 0.38 2.50 2.44 2.37 2.28 2.11
mLSA 0.07 0.07 0.08 0.07 0.07 0.34 0.35 0.35 0.37 0.38 2.48 2.42 2.36 2.26 2.12
Kmeans 0.22 0.23 0.22 0.22 0.22 0.42 0.43 0.44 0.44 0.46 2.16 2.11 2.06 1.98 1.86
LDA(L, H) 0.10 0.10 0.13 0.14 0.15 0.34 0.34 0.36 0.37 0.39 2.48 2.43 2.35 2.25 2.11
DF-LDA(L, H) 0.23 0.25 0.26 0.27 0.30 0.41 0.40 0.41 0.41 0.44 2.23 2.23 2.16 2.10 1.94
Kmeans(L, H) 0.13 0.16 0.22 0.24 0.28 0.42 0.43 0.45 0.45 0.48 2.15 2.11 2.02 1.95 1.79
EM(L, H) 0.35 0.44 0.51 0.55 0.58 0.43 0.49 0.54 0.57 0.61 2.22 1.99 1.81 1.65 1.49
Rand(CL, H) 0.28 0.35 0.39 0.42 0.45 0.39 0.43 0.47 0.50 0.54 2.33 2.15 2.00 1.82 1.63
LDA(CL, H) 0.31 0.38 0.43 0.46 0.49 0.43 0.47 0.51 0.54 0.58 2.16 1.99 1.83 1.69 1.49
DF-LDA(CL, H) 0.32 0.33 0.33 0.34 0.36 0.49 0.50 0.48 0.48 0.48 1.90 1.85 1.86 1.83 1.82
Kmeans(CL, H) 0.33 0.41 0.46 0.49 0.52 0.47 0.51 0.55 0.57 0.61 1.98 1.82 1.69 1.56 1.42
EM(CL, H) 0.44 0.54 0.58 0.61 0.64 0.49 0.57 0.61 0.64 0.67 1.98 1.72 1.56 1.40 1.25
LDA(CL, S) 0.17 0.21 0.25 0.30 0.34 0.34 0.36 0.39 0.42 0.46 2.47 2.37 2.27 2.09 1.87
Kmeans(CL, S) 0.23 0.28 0.32 0.36 0.42 0.43 0.44 0.46 0.48 0.51 2.15 2.08 1.98 1.86 1.70
SC-EM 0.45 0.58 0.68 0.75 0.81 0.50 0.61 0.69 0.76 0.82 1.95 1.56 1.24 0.94 0.69
Table 3. Influence of the seeds? proportion (which reflects the size of the labeled set L)
Figure 4. Influence of context window size 
1.0
1.1
1.2
1.3
1.4
1.5
62%
64%
66%
68%
70%
1 2 3 4 5 6 7 8 9 10
E
n
tr
op
y
P
u
ri
ty
/A
cc
u
ra
cy
Window Size t
SC-EM
Purity
Accuracy
Entropy
1279
References 
Agirre E., E. Alfonseca, K. Hall, J. Kravalova, M. Pa 
ca and A. Soroa 2009. A study on similarity and 
relatedness using distributional and WordNet-
based approaches. Proceedings of ACL. 
Alvarez M. and S. Lim 2007. A Graph Modeling of 
Semantic Similarity between Words. Proceeding of 
the Conference on Semantic Computing. 
Andrzejewski D., X. Zhu and M. Craven 2009. 
Incorporating domain knowledge into topic 
modeling via Dirichlet forest priors. Proceedings 
of ICML. 
Blei D., A. Y. Ng and M. I. Jordan 2003. "Latent 
Dirichlet Allocation." JMLR 3: 993-1022. 
Carenini G., R. Ng and E. Zwart 2005. Extracting 
knowledge from evaluative text. Proceedings of 
International Conference on Knowledge Capture. 
Chen H., M. Lin and Y. Wei 2006. Novel association 
measures using web search with double checking.
Proceedings of ACL. 
Dempster A., N. Laird and D. Rubin 1977. "Maximum
likelihood from incomplete data via the EM 
algorithm." Journal of the Royal Statistical Society 
39(1): 1-38. 
Guo H., H. Zhu, Z. Guo, X. Zhang and Z. Su 2009. 
Product feature categorization with multilevel 
latent semantic association. Proc. of CIKM. 
Harris Z. S. 1968. Mathematical structures of 
language. New York, Interscience Publishers. 
Hu M. and B. Liu 2004a. Mining and summarizing 
customer reviews. Proceedings of SIGKDD. 
Hu M. and B. Liu 2004b. Mining Opinion Features in 
Customer Reviews. Proceedings of AAAI. 
Hughes T. and D. Ramage 2007. Lexical semantic 
relatedness with random graph walks. EMNLP. 
Jin W., H. Ho and R. Srihari 2009. OpinionMiner: a 
novel machine learning system for web opinion 
mining and extraction. Proceedings of KDD. 
Joachims T. 1999. Transductive inference for text 
classification using support vector machines.
Proceedings of ICML. 
Kim S. and E. Hovy 2006. Extracting opinions, 
opinion holders, and topics expressed in online 
news media text. Proceedings of EMNLP. 
Kobayashi N., K. Inui and Y. Matsumoto 2007. 
Extracting aspect-evaluation and aspect-of 
relations in opinion mining. Proceedings of 
EMNLP.
Ku L., H. Ho and H. Chen 2009. "Opinion mining and 
relationship discovery using CopeOpi opinion 
analysis system." Journal of the American Society 
for Information Science and Technology 60(7): 
1486-1503. 
Lee L. 1999. Measures of distributional similarity,
Proceedings of ACL. 
Lin D. 1998. Automatic retrieval and clustering of 
similar words, Proceedings of ACL. 
Liu B. 2006. Web data mining; Exploring hyperlinks, 
contents, and usage data, Springer. 
Liu B. 2010. Sentiment Analysis and Subjectivity.
Handbook of Natural Language Processing N.
Indurkhya and F. J. Damerau. 
MacQueen J. 1966. Some methods for classification 
and analysis of multivariate observations. Proc. of 
Symposium on Mathematical Statistics and 
Probability. 
Mei Q., X. Ling, M. Wondra, H. Su and C. Zhai 2007. 
Topic sentiment mixture: modeling facets and 
opinions in weblogs. Proceedings of WWW. 
Nigam K., A. McCallum, S. Thrun and T. Mitchell 
2000. "Text classification from labeled and 
unlabeled documents using EM." Machine 
Learning 39(2). 
Pang B. and L. Lee 2008. "Opinion mining and 
sentiment analysis." Foundations and Trends in 
Information Retrieval 2(1-2): 1-135. 
Pantel P., E. Crestan, A. Borkovsky, A. Popescu and 
V. Vyas 2009. Web-scale distributional similarity 
and entity set expansion. EMNLP. 
Pereira F., N. Tishby and L. Lee 1993. Distributional 
clustering of English words. Proceedings of ACL. 
Popescu A.-M. and O. Etzioni 2005. Extracting 
Product Features and Opinions from Reviews.
EMNLP.
Sahami M. and T. Heilman 2006. A web-based kernel 
function for measuring the similarity of short text 
snippets. Proceedings of WWW. 
Stoyanov V. and C. Cardie 2008. Topic identification 
for fine-grained opinion analysis. COLING. 
Wagstaff K., C. Cardie, S. Rogers and S. Schroedl 
2001. Constrained k-means clustering with 
background knowledge. In Proceedings of ICML. 
Yang D. and D. Powers 2005. Measuring semantic 
similarity in the taxonomy of WordNet,
Proceedings of the Australasian conference on 
Computer Science. 
1280
Coling 2010: Poster Volume, pages 757?765,
Beijing, August 2010
Dependency-Driven Feature-based Learning for Extracting 
Protein-Protein Interactions from Biomedical Text 
Bing Liu   Longhua Qian   Hongling Wang   Guodong Zhou 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University 
Email: liubingnlp@gmail.com 
{qianlonghua,redleaf,gdzhou}@suda.edu.cn
                                                          
 Corresponding author 
Abstract
Recent kernel-based PPI extraction 
systems achieve promising perform-
ance because of their capability to 
capture structural syntactic informa-
tion, but at the expense of computa-
tional complexity. This paper incorpo-
rates dependency information as well 
as other lexical and syntactic knowl-
edge in a feature-based framework. 
Our motivation is that, considering the 
large amount of biomedical literature 
being archived daily, feature-based 
methods with comparable performance 
are more suitable for practical applica-
tions. Additionally, we explore the 
difference of lexical characteristics be-
tween biomedical and newswire do-
mains. Experimental evaluation on the 
AIMed corpus shows that our system 
achieves comparable performance of 
54.7 in F1-Score with other 
state-of-the-art PPI extraction systems, 
yet the best performance among all the 
feature-based ones.  
1 Introduction 
In recent years, automatically extracting 
biomedical information has been the subject of 
significant research efforts due to the rapid 
growth in biomedical development and 
discovery. A wide concern is how to 
characterize protein interaction partners since 
it is crucial to understand not only the 
functional role of individual proteins but also 
the organization of the entire biological 
process. However, manual collection of 
relevant Protein-Protein Interaction (PPI) 
information from thousands of research papers 
published every day is so time-consuming that 
automatic extraction approaches with the help 
of Natural Language Processing (NLP) 
techniques become necessary.  
Various machine learning approaches for 
relation extraction have been applied to the 
biomedical domain, which can be classified 
into two categories: feature-based methods 
(Mitsumori et al, 2006; Giuliano et al, 2006; 
S?tre et al, 2007) and kernel-based methods 
(Bunescu et al, 2005; Erkan et al, 2007; 
Airola et al, 2008; Kim et al, 2010). 
Provided a large-scale manually annotated 
corpus, the task of PPI extraction can be 
formulated as a classification problem. 
Typically, for featured-based learning each 
protein pair is represented as a vector whose 
features are extracted from the sentence 
involving two protein names. Early studies 
identify the existence of protein interactions 
by using ?bag-of-words? features (usually 
uni-gram or bi-gram) around the protein 
names as well as various kinds of shallow 
linguistic information, such as POS tag, 
lemma and orthographical features. However, 
these systems do not achieve promising results 
since they disregard any syntactic or semantic 
information altogether, which are very useful 
for the task of relation extraction in the 
newswire domain (Zhao and Grishman, 2005; 
Zhou et al, 2005). Furthermore, feature-based 
methods fail to effectively capture the 
structural information, which is essential to 
757
identify the relationship between two proteins 
in a syntactic representation. 
With the wide application of kernel-based 
methods to many NLP tasks, various kernels 
such as subsequence kernels (Bunescu and 
Mooney, 2005) and tree kernels (Li et al, 
2008), are also applied to PPI detection.. 
Particularly, dependency-based kernels such 
as edit distance kernels (Erkan et al, 2007) 
and graph kernels (Airola et al, 2008; Kim et 
al., 2010) show some promising results for PPI 
extraction. This suggests that dependency 
information play a critical role in PPI 
extraction as well as in relation extraction 
from newswire stories (Culotta and Sorensen, 
2004). In order to appreciate the advantages of 
both feature-based methods and kernel-based 
methods, composite kernels (Miyao et al, 
2008; Miwa et al, 2009a; Miwa et al, 2009b) 
are further employed to combine structural 
syntactic information with flat word features 
and significantly improve the performance of 
PPI extraction. However, one critical 
challenge for kernel-based methods is their 
computation complexity, which prevents them 
from being widely deployed in real-world 
applications regarding the large amount of 
biomedical literature being archived everyday.  
Considering the potential of dependency in-
formation for PPI extraction and the challenge 
of computation complexity of kernel-based 
methods, one may naturally ask the question: 
?Can the essential dependency information be 
maximally exploited in featured-based PPI 
extraction so as to enhance the performance 
without loss of efficiency?? ?If the answer is 
Yes, then How?? 
This paper addresses these problems, focus-
ing on the application of dependency informa-
tion to feature-based PPI extraction. Starting 
from a baseline system in which common 
lexical and syntactic features are incorporated 
using Support Vector Machines (SVM), we 
further augment the baseline with various fea-
tures related to dependency information, 
including predicates in the dependency tree. 
Moreover, in order to reveal the linguistic 
difference between distinct domains we also 
compare the effects of various features on PPI 
extraction from biomedical texts with those on 
relation extraction from newswire narratives. 
Evaluation on the AIMed and other PPI cor-
pora shows that our method achieves the best 
performance among all feature-based systems. 
The rest of the paper is organized as follows. 
A feature-based PPI extraction baseline system 
is given in Section 2 while Section 3 describes 
our dependency-driven method. We report our 
experiments in Section 4, and compare our 
work with the related ones in Section 5.  
Section 6 concludes this paper and gives some 
future directions. 
2 Feature-based PPI extraction: 
Baseline
For feature-based methods, PPI extraction task 
is re-cast as a classification problem by first 
transforming PPI instances into 
multi-dimensional vectors with various fea-
tures, and then applying machine learning ap-
proaches to detect whether the potential 
relationship exists for a particular protein pair. 
In training, a feature-based classifier learning 
algorithm, such as SVM or MaxEnt, uses the 
annotated PPI instances to learn a classifier 
while, in testing, the learnt classifier is in turn 
applied to new instances to determine their PPI 
binary classes and thus candidate PPI instances 
are extracted. 
As a baseline, various linguistic features, 
such as words, overlap, chunks, parse tree fea-
tures as well as their combined ones are ex-
tracted from a sentence and formed as a vector 
into the feature-based learner. 
1) Words 
Four sets of word features are used in our sys-
tem: 1) the words of both the proteins; 2) the 
words between the two proteins; 3) the words 
before M1 (the 1st protein); and 4) the words 
after M2 (the 2nd protein). Both the words be-
fore M1 and after M2 are classified into two 
bins: the first word next to the proteins and the 
second word next to the proteins. This means 
that we only consider the two words before M1 
and after M2. Words features include: 
x MW1: bag-of-words in M1 
x MW2: bag-of-words in M2 
x BWNULL: when no word in between 
x BWO: other words in between except 
first and last words when at least three 
words in between 
x BWM1FL: the only word before M1 
758
x BWM1F: first word before M1 
x BWM1L: second word before M1 
x BWM1: first and second word before 
M1
x BWM2FL: the only word after M2 
x BWM2F: first word after M2 
x BWM2L: second word after M2 
x BWM2: first and second word after M2 
2) Overlap 
The numbers of other protein names as well as 
the words that appear between two protein 
names are included in the overlap features. 
This category of features includes: 
x #MB: number of other proteins in be-
tween
x #WB: number of words in between 
x E-Flag: flag indicating whether the two 
proteins are embedded or not 
3) Chunks
It is well known that chunking plays an 
important role in the task of relation extraction 
in the ACE program (Zhou et al, 2005). How-
ever, its significance in PPI extraction has not 
fully investigated. Here, the Stanford Parser1
is first employed for full parsing, and then 
base phrase chunks are derived from full parse 
trees using the Perl script2. The chunking fea-
tures usually concern about the head words of 
the phrases between the two proteins, which 
are further classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between. In 
addition, the path of phrasal labels connecting 
two proteins is also a common syntactic 
indicator of the polarity of the PPI instance, 
just as the path NP_VP_PP_NP in the sen-
tence ?The ability of PROT1 to interact with 
the PROT2 was investigated.? is likely to sug-
gest the positive interaction between two pro-
teins. These base phrase chunking features 
contain:
x CPHBNULL: when no phrase in be-
tween.
x CPHBFL: the only phrase head when 
only one phrase in between 
x CPHBF: the first phrase head in between 
when at least two phrases in between. 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
2 http://ilk.kub.nl/~sabine/chunklink/ 
x CPHBL: the last phrase head in between 
when at least two phrase heads in be-
tween.
x CPHBO: other phrase heads in between 
except first and last phrase heads when 
at least three phrases in between. 
x CPP: path of phrase labels connecting 
the two entities in the chunking 
Furthermore, we also generate a set of 
bi-gram features which combine the above 
chunk features except CPP with their corre-
sponding chunk types.  
4) Parse Tree 
It is obvious that full pares trees encompass 
rich structural information of a sentence. 
Nevertheless, it is much harder to explore 
such information in featured-based methods 
than in kernel-based ones. Thus so far only 
the path connecting two protein names in the 
full-parse tree is considered as a parse tree 
feature.
x PTP: the path connecting two protein 
names in the full-parse tree. 
 Again, take the sentence ?The ability of 
PROT1 to interact with the PROT2 was 
investigated.? as an example, the parse path 
between PROT1 and PROT2 is 
NP_S_VP_PP_NP, which is slightly different 
from the CPP feature in the chunking feature 
set.
3 Dependency-Driven PPI Extraction 
The potential of dependency information for 
PPI extraction lies in the fact that the depend-
ency tree may well reveal non-local or 
long-range dependencies between the words 
within a sentence. In order to capture the 
necessary information inherent in the 
depedency tree for identifying their 
relationship, various kernels, such as edit 
distance kernel based on dependency path 
(Erkan et al, 2007), all-dependency-path 
graph kernel (Airola et al, 2008), and 
walk-weighted subsequence kernels (Kim et 
al., 2010) as well as other composite kernels 
(Miyao et al, 2008; Miwa et al, 2009a; Miwa 
et al, 2009b), have been proposed to address 
this problem. It?s true that these methods 
achieve encouraging results, neverthless, they 
suffer from prohibitive computation burden. 
759
Thus, our solution is to fold the structural 
dependency information back into flat 
features in a feature-based framework so as to 
speed up the learning process while retaining 
comparable performance. This is what we 
refer to as dependency-driven PPI extraction. 
 First, we construct dependency trees from 
grammatical relations generated by the Stan-
ford Parser. Every grammatical relation has the 
form of dependent-type (word1, word2),
Where word1 is the head word, word2 is de-
pendent on word1, and dependent-type denotes 
the pre-defined type of dependency. Then, 
from these grammatical relations the following 
features called DependenecySet1 are taken 
into consideration as illustrated in Figure 1: 
x DP1TR: a list of words connecting 
PROT1 and the dependency tree root. 
x DP2TR: a list of words connecting 
PROT2 and the dependency tree root. 
x DP12DT: a list of dependency types 
connecting the two proteins in the 
dependency tree. 
x DP12: a list of dependent words com-
bined with their dependency types con-
necting the two proteins in the depend-
ency tree. 
x DP12S: the tuple of every word com-
bined with its dependent type in DP12. 
x DPFLAG: a boolean value indicating 
whether the two proteins are directly 
dependent on each other. 
The typed dependencies produced by the 
Stanford Parser for the sentence ?PROT1 
contains a sequence motif binds to PROT2.? 
are listed as follows: 
nsubj(contains-2,PROT1-1)
det(motif-5, a-3) 
nn(motif-5, sequence-4) 
nsubj(binds-6, motif-5) 
ccomp(contains-2, binds-6) 
prep_to(binds-6, PROT2-8) 
Each word in a dependency tuple is fol-
lowed by its index in the original sentence, 
ensuring accurate positioning of the head 
word and dependent word. Figure 1 shows the 
dependency tree we construct from the above 
grammatical relations.  
contains 
PROT1 
motif 
binds 
PROT2
a sequence 
nsubj ccomp 
prep_to
nsubj 
det nn 
Figure 1: Dependency tree for the sentence 
?PROT1 contains a sequence motif binds to 
PROT2.? 
Erkan et al (2007) extract the path 
information between PROT1 and PROT2 in 
the dependency tree for kernel-based PPI 
extraction and report promising results, 
neverthless, such path is so specific for 
feature-based methods that it may incure 
higher precision but lower recall. Thus we 
alleviate this problem by collapsing the feature 
into multiple ones with finer granularity, 
leading to the features such as DP12S. 
It is widely acknowledged that predicates 
play an important role in PPI extraction. For 
example, the change of a pivot predicate 
between two proteins may easily lead to the 
polarity reversal of a PPI instance. Therefore, 
we extract the predicates and their positions in 
the dependency tree as predicate features 
called DependencySet2:  
x FVW: the predicates in the DP12 feature 
occurring prior to the first protein. 
x LVW: the predicates in the DP12 feature 
occurring next to the second entity. 
x MVW: other predicates in the DP12 
features. 
x #FVW: the number of FVW 
x #LVW: the number of LVW 
x #MVW: the number of MVW 
4 Experimentation
This section systematically evaluates our fea-
ture-based method on the AIMed corpus as 
well as other commonly used corpus and re-
ports our experimental results. 
760
4.1 Data Sets 
We use five corpora3 with the AIMed corpus 
as the main experimental data, which contains 
177 Medline abstracts with interactions be-
tween two interactions, and 48 abstracts with-
out any PPI within single sentences. There are 
4,084 protein references and around 1,000 
annotated interactions in this data set.  
For corpus pre-procession, we first rename 
two proteins of a pair as PROT1 and PROT2 
respectively in order to blind the learner for 
fair comparison with other work.  Then, all 
the instances are generated from the sentences 
which contain at least two proteins,  that is, if 
a sentence contains n different proteins, there 
are n2 different pairs of proteins and these 
pairs are considered untyped and undirected. 
For the purpose of comparison with previous 
work, all the self-interactions (59 instances) 
are removed, while all the PPI instances with 
nested protein names are retained (154 in-
stances). Finally, 1002 positive instances and 
4794 negative instances are generated and 
their corresponding features are extracted.  
We select Support Vector Machines (SVM) 
as the classifier since SVM represents the 
state-of-the-art in the machine learning re-
search community. In particular, we use the 
binary-class SVMLigh 4 developed by 
Joachims (1998) since it satisfies our require-
ment of detecting potential PPI instances. 
Evaluation is done using 10-fold docu-
ment-level cross-validation. Particularly, we 
apply the extract same 10-fold split that was 
used by Bunescu et al (2005) and Giuliano et 
al. (2006). Furthermore, OAOD (One Answer 
per Occurrence in the Document) strategy is 
adopted, which means that the correct interac-
tion must be extracted for each occurrence. 
This guarantees the maximal use of the avail-
able data, and more important, allows fair 
comparison with earlier relevant work.  
The evaluation metrics are commonly used 
Precision (P), Recall (R) and harmonic 
F1-score (F1). As an alternative to F1-score, 
the AUC (area under the receiver operating 
characteristics curve) measure is proved to be 
invariant to the class distribution of the train-
ing dataset. Thus we also provide AUC scores 
                                                          
3 http://mars.cs.utu.fi/PPICorpora/GraphKernel.html 
4 http://svmlight.joachims.org/
for our system as Airola et al (2008) and 
Miwa et al (2009a). 
4.2 Results and Discussion 
Features P(%) R(%) F1 
Baseline features 
Words 59.4 40.6 47.6
+Overlap 60.4 39.9 47.4
+Chunk 59.2 44.5 50.6
+Parse 60.9 44.8 51.4
Dependency-driven features 
+Dependency Set1 62.9 48.0 53.9
+Dependency Set2 63.4 48.8 54.7
Table 1: Performance of PPI extraction with vari-
ous features in the AIMed corpus 
We present in Table 1 the performance of our 
system using document-wise evaluation 
strategies and 10-fold cross-validation with 
different features in the AIMed corpus, where 
the plus sign before a feature means it is 
incrementally added to the feature set. Table 1 
reports that our system achieves the best per-
formance of 63.4/48.8/54.7 in P/R/F scores. It 
also shows that: 
x Words features alone achieve a relatively 
low performance of 59.4/40.9/47.6 in 
P/R/F, particularly with fairly low recall 
score. This suggests the difficulty of PPI 
extraction and words features alone can?t 
effectively capture the nature of protein 
interactions.
x Overlap features slightly decrease the per-
formance. Statistics show that both the 
distributions of #MB and #WB between 
positives and negatives are so similar that 
they are by no means the discriminators for 
PPI extraction. Hence, we exclude the 
overlap features in the succeeding experi-
ments.
x Chunk features significantly improves the 
F-measure by 3 units largely due to the in-
crease of recall by 3.9%, though at the 
slight expense of precision. This suggests 
the effectiveness of shallow parsing infor-
mation in the form of headwords captured 
by chunking on PPI extraction.  
x The usefulness of the parse tree features is 
quite limited. It only improves the 
F-measure by 0.8 units. The main reason 
may be that these paths are usually long 
761
and specific, thus they suffer from the 
problem of data sparsity. Furthermore, 
some of the parse tree features are already 
involved in the chunk features.  
x The DependencySet1 features are very 
effective in that it can increase the preci-
sion and recall by 2.0 and 3.2 units 
respectively, leading to the increase of F1 
score by 2.5 units. This means that the de-
pendency-related features can effectively 
retrieve more PPI instances without intro-
ducing noise that will severely harm the 
precision. According to our statistics, there 
are over 60% sentences with more than 5 
words between their protein entities in the 
AIMed corpus. Therefore, dependency in-
formation exhibit great potential to PPI 
extraction since they can capture 
long-range dependencies within sentences. 
Take the aforementioned sentence 
?PROT1 contains a sequence motif binds 
to PROT2.? as an example, although the 
two proteins step over a relatively long 
distance, the dependency path between 
them is concise and accurate, reflecting the 
essence of the interaction. 
x The predicate features also contribute to 
the F1-score gain of 0.8 units. It is not 
surprising since some predicates, such as 
?interact?, ?activate? and ?inhibit? etc, are 
strongly suggestive of the interaction 
polarity between two proteins. 
We compare in Table 2 the performance of 
our system with other systems in the AIMed 
corpus using the same 10-fold cross validation 
strategy. These systems are grouped into three 
distinct classes: feature-based, kernel-based 
and composite kernels. Except for Airola et al 
(2008) Miwa et al (2009a) and Kim et al 
(2010), which adopt graph kernels, our system 
performs comparably with other systems. In 
particular, our dependency-driven system 
achieves the best F1-score of 54.7 among all 
feature-based systems. 
In order to measure the generalization abil-
ity of our dependency-driven PPI extraction 
system across different corpora, we further 
apply our method to other four publicly avail-
able PPI corpora: BioInfer, HPRD50, IEPA 
and LLL.  
Table 2: Comparison with other PPI extraction 
systems in the AIMed corpus 
The corresponding performance of 
F1-score and AUC metrics as well as their 
standard deviations is present in Table 3.  
Comparative available results from Airola et 
al. (2008) and Miwa et al (2009a) are also 
included in Table 3 for comparison. This table 
shows that our system performs almost 
consistently with the other two systems, that is, 
the LLL corpus gets the best performance yet 
with the greatest variation, while the AIMed 
corpus achieves the lowest performance with 
reasonable variation. 
It is well known that biomedical texts ex-
hibit distinct linguistic characteristics from 
newswire narratives, leading to dramatic per-
formance gap between PPI extraction and 
relation detection in the ACE corpora. How-
ever, no previous work has ever addressed this 
problem and empirically characterized this 
difference. In this paper, we devise a series of 
experiments over the ACE RDC corpora using 
our dependency-driven feature-based method 
as a touchstone task. In order to do that, a sub-
                                                          
5 Airola et al (2008) repeat the method published by 
Giuliano et al (2006) with a correctly preprocessed 
AIMed and reported an F1-score of 52.4%. 
6 The results from Table 1 (Miyao et al, 2009) with the 
most similar settings to ours (Stanford Parser with SD 
representation) are reported. 
Systems P(%) R(%) F1
Feature-based methods 
Our system 63.4 48.8 54.7
Giuliano et al, 20065 60.9 57.2 59.0
S?tre et al, 2007 64.3 44.1 52.0
Mitsumori et al, 2006 54.2 42.6 47.7
Yakushiji et al, 2005 33.7 33.1 33.4
Kernel-based methods 
Kim et al, 2010 61.4 53.3 56.7
Airola et al, 2008 52.9 61.8 56.4
Bunescu et al, 2006  65.0 46.4 54.2
Composite kernels 
Miwa et al, 2009a - - 62.0
Miyao et al, 20086 51.8 58.1 54.5
762
set of 5796 relation instances is randomly 
sampled from the ACE 2003 and 2004 cor-
pora respectively.  The same cross-validation 
and evaluation metrics are applied to these 
two sets as PPI extraction in the AIMed cor-
pus.
Our system Airola et al (2008) 7 Miwa et al (2009a) 
Corpus F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC
AIMed 54.7 4.5 82.4 3.5 56.4 5.0 84.8 2.3 60.8 6.6 86.8 3.3
BioInfer 59.8 3.5 80.9 3.3 61.3 5.3 81.9 6.5 68.1 3.2 85.9 4.4
HPRD50 64.9 13.4 79.8 8.5 63.4 11.4 79.7 6.3 70.9 10.3 82.2 6.3
IEPA 62.1 6.2 74.8 6.6 75.1 7.0 85.1 5.1 71.7 7.8 84.4 4.2
LLL 78.1 15.8 85.1 8.3 76.8 17.8 83.4 12.2 80.1 14.1 86.3 10.8
Table 3: Comparison of performance across the five PPI corpora 
AIMed ACE2003 ACE2004 
Features
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 
Words 59.4 40.6 47.6 66.5 51.6 57.9 68.1 59.6 63.4
+Overlap +1.0 -0.7 -0.2 +5.4 +1.8 +3.2 +4.6 +1.2 +2.7
+Chunk -1.7 +4.6 +3.2 +2.3 +5.1 +4.0 +1.5 +1.9 +1.7
+Parse +1.7 +0.3 +0.8 +0.3 +0.6 +0.5 +0.6 +0.4 +0.5
+Dependency Set1 +2.0 +3.2 +2.5 +0.8 +0.7 +0.7 +0.5 +0.9 +0.7
+Dependency Set2 +0.5 +0.8 +0.8 +0.3 +0.2 +0.3 +0.2 +0.4 +0.3
Table 4: Comparison of contributions of different features to relation detection across multiple domains 
Table 4 compares the performance of our 
method over different domains. The table re-
ports that the words features alone achieve the 
best F1-score of 63.4 in ACE2004 but the low-
est F1-score of 47.6 in AIMed. This suggests 
the wide difference of lexical distribution be-
tween these domains. We extract the words 
appearing before the 1st mention, between the 
two mentions and after the 2nd mention from 
the training sets of these corpora respectively, 
and summarize the statistics (the number of 
tokens, the number of occurrences) in Table 5, 
where the KL divergence between positives 
and negatives is summed over the distribution 
of the 500 most frequently occurring words. 
                                                          
7 The performance results of F1 and AUC on the BioInfer corpus are slightly adjusted according to Table 3 in Miwa et 
al. (2009b) 
Table 5: Lexical statistics on three corpora 
The table shows that AIMed uses the most 
kinds of words and the most words around the 
two mentions than the other two. More impor-
tant, AIMed has the least distribution differ-
ence between the words appearing in positives 
and negatives, as indicated by its least KL 
divergence. Therefore, the lexical words in 
AIMed are less discriminative for relation 
detection than they do in the other two. This 
naturally explains the reason why the perform-
ance by words feature alone is 
AIMed<ACE2003<ACE2004. In addition, 
Table 4 also shows that: 
x The overlap features significantly improve 
the performance in ACE while slightly 
deteriorating that in AIMed. The reason is 
that, as indicated in Zhou et al (2005), most 
of the positive relation instances in ACE 
exist in local contexts, while the positive 
interactions in AIMed occur in relative 
long-range just as the negatives, therefore 
these features are not discriminative for 
AIMed.
Statistics AIMed ACE2003 ACE2004
# of tokens 2,340 2,064 2,099
# of occurrences 69,976 53,744 49,570
KL divergence  0.22 0.28 0.33 
x The chunk features consistently greatly 
boost the performance across multiple cor-
pora. This implies that the headwords in 
chunk phrases can well capture the partial 
nature of relation instances regardless of 
their genre. 
x It?s not surprising that the parse feature 
attain moderate performance gain in all do-
mains since these parse paths are usually 
763
long and specificity, leading to data 
sparseness problem. 
x It is interesting to note that the depend-
ency-related features exhibit more signifi-
cant improvement in AIMed than that in 
ACE. The reason may be that, these 
dependency features can effectively cap-
ture long-range relationships prevailing in 
AIMed, while in ACE a large number of 
local relationships dominate the corpora. 
5 Related Work 
Among feature-based methods, the PreBIND 
system (Donaldson et al, 2003) uses words and 
word bi-grams features to identify the existence 
of protein interactions in abstracts and such 
information is used to enhance manual expert 
reviewing for the BIND database. Mitsumori et 
al. (2006) use SVM to extract protein-protein 
interactions, where bag-of-words features, spe-
cifically the words around the protein names, 
are employed. Sugiyama et al (2003) extract 
various features from the sentences based on 
the verbs and nouns in the sentences such as the 
verbal forms, and the part-of-speech tags of the 
20 words surrounding the verb. In addition to 
word features, Giuliano et al (2006) extract 
shallow linguistic information such as POS tag, 
lemma, and orthographic features of tokens for 
PPI extraction. Unlike our dependency-driven 
method, these systems do not consider any 
syntactic information.  
For kernel-based methods, there are several 
systems which utilize dependency information. 
Erkan et al (2007) defines similarity functions 
based on cosine similarity and edit distance 
between dependency paths, and then incorpo-
rate them in SVM and KNN learning for PPI 
extraction. Airola et al (2008) introduce 
all-dependency-paths graph kernel to capture 
the complex dependency relationships between 
lexical words and attain significant perform-
ance boost at the expense of computational 
complexity. Kim et al (2010) adopt 
walk-weighted subsequence kernel based on 
dependency paths to explore various substruc-
tures such as e-walks, partial match, and 
non-contiguous paths. Essentially, their kernel 
is also a graph-based one. 
For composite kernel methods, S?tre et al 
(2007) combine a ?bag-of-words? kernel with 
dependency and PAS (Predicate Argument 
Structure) tree kernels to exploit both the words 
features and the structural syntactic information. 
Hereafter, Miyao et al (2008) investigate the 
contribution of various syntactic features using 
different representations from dependency 
parsing, phrase structure parsing and deep 
parsing by different parsers. Miwa et al 
(2009a) integrate ?bag-of-words? kernel, PAS 
tree kernel and all-dependency-paths graph 
kernel to achieve the higher performance. They 
(Miwa et al, 2009b) also use similar compos-
ite kernels for corpus weighting learning 
across multiple PPI corpora.  
6 Conclusion and Future Work 
In this paper, we have combined various lexical 
and syntactic features, particularly dependency 
information, into a feature-based PPI extraction 
system. We find that the dependency informa-
tion as well as the chunk features contributes 
most to the performance improvement.  The 
predicate features involved in the dependency 
tree can also moderately enhance the perform-
ance. Furthermore, comparative study between 
biomedical domain and the ACE newswire 
domain shows that these domains exhibit 
different lexical characteristics, rendering the 
task of PPI extraction much more difficult than 
that of relation detection from the ACE cor-
pora.
In future work, we will explore more syntac-
tic features such as PAS information for fea-
ture-based PPI extraction to further boost the 
performance. 
Acknowledgment 
This research is supported by Projects 
60873150 and 60970056 under the National 
Natural Science Foundation of China and Pro-
ject BK2008160 under the Natural Science 
Foundation of Jiangsu, China. We are also very 
grateful to Dr. Antti Airola from Truku 
University for providing partial experimental 
materials. 
References 
A. Airola, S. Pyysalo, J. Bj?rne, T. Pahikkala, F. 
Ginter, and T. Salakoski. 2008. All-paths graph 
kernel for protein-protein interaction extraction 
764
with evaluation of cross corpus learning. BMC
Bioinformatics.
R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney, 
A. Ramani, and Y. Wong. 2005. Comparative 
Experiments on learning information extractors 
for Proteins and their interactions. Journal of 
Artificial Intelligence In Medicine, 33(2).  
R. Bunescu and R. Mooney. 2005. Subsequence 
kernels for relation extraction. In Proceedings of  
NIPS?05, pages 171?178. 
A. Culotta and J. Sorensen. 2004.  Dependency 
Tree Kernels for Relation Extraction. In 
Proceedings of ACL?04.
I. Donaldson, J. Martin, B. de Bruijn, C. Wolting, V. 
Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. 
Bader, K. Michalockova, T. Pawson, and C. W. V. 
Hogue. 2003. Prebind and textomy - mining the 
biomedical literature for protein-protein 
interactions using a support vector machine. 
Journal of BMC Bioinformatics, 4(11). 
G. Erkan, A. ?zg?r, and D.R. Radev. 2007. 
Semi-Supervised Classification for Extracting 
Protein Interaction Sentences using Dependency 
Parsing, In Proceedings of EMNLP-CoNLL?07,
pages 228?237. 
C. Giuliano, A. Lavelli, and L. Romano. 2006. 
Exploiting Shallow Linguistic Information for 
Relation Extraction from Biomedical Literature. 
In Proceedings of EACL?06, pages 401?408. 
 S. Kim, J. Yoon, J. Yang, and S. Park. 2010. 
Walk-weighted subsequence kernels for 
protein-protein interaction extraction. Journal of 
BMC Bioinformatics, 11(107). 
J. Li, Z. Zhang, X. Li, and H. Chen. 2008. 
Kernel-Based Learning for Biomedical Relation 
extraction. Journal of the American Society for 
Information Science and Technology, 59(5). 
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. 
Doi. 2006. Extracting protein-protein interaction 
information from biomedical text with SVM. 
IEICE Transactions on Information and System, 
E89-D (8).
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009a. 
Protein-Protein Interaction Extraction by 
Leveraging Multiple Kernels and Parsers. 
Journal of Medical Informatics, 78(2009). 
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009b.  
A Rich Feature Vector for Protein-Protein 
Interaction Extraction from Multiple Corpora. In 
Proceedings of EMNLP?09, pages 121?130.  
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and 
J.Tsujii. 2008. Task-oriented evaluation of 
syntactic parsers and their representations. In 
Proceedings of ACL?08, pages 46?54. 
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi. 
2001. Automated extraction of information on 
protein-protein interactions from the biological 
literature. Journal of Bioinformatics, 17(2). 
 K. Sugiyama, K. Hatano, M. Yoshikawa, and S. 
Uemura. 2003. Extracting information on 
protein-protein interactions from biological 
literature based on machine learning approaches. 
Journal of Genome Informatics, (14): 699?700. 
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic 
features for protein-protein interaction extraction. 
In Proceedings of LBM?07, pages 6.1?6.14. 
A. Yakushiji, M. Yusuke, T. Ohta, Y. Tateishi, J. 
Tsujii. 2006. Automatic construction of 
predicate-argument structure patterns for 
biomedical information extraction. In 
Proceedings of EMNLP?06, pages 284?292. 
S.B. Zhao and R. Grishman. 2005. Extracting 
Relations with Integrated Information Using 
Kernel Methods. In Proceedings of ACL?05,
pages 419-426.  
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation 
extraction.  In Proceedings of ACL?05, pages 
427-434.  
765
Coling 2010: Poster Volume, pages 1462?1470,
Beijing, August 2010
              Extracting and Ranking Product Features 
                 in Opinion Documents 
Lei  Zhang 
Department of Computer Science 
University of Illinois at Chicago 
lzhang3@cs.uic.edu
Suk Hwan Lim 
Hewlett-Packard Labs 
suk-hwan.lim@hp.com
                           Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
liub@cs.uic.edu
Eamonn O?Brien-Strain 
Hewlett-Packard Labs 
eob@hpl.hp.com
Abstract
An important task of opinion mining is 
to extract people?s opinions on features 
of an entity. For example, the sentence, 
?I love the GPS function of Motorola 
Droid? expresses a positive opinion on 
the ?GPS function? of the Motorola 
phone. ?GPS function? is the feature. 
This paper focuses on mining features. 
Double propagation is a state-of-the-art 
technique for solving the problem. It 
works well for medium-size corpora. 
However, for large and small corpora, it 
can result in low precision and low re-
call. To deal with these two problems, 
two improvements based on part-whole
and ?no? patterns are introduced to in-
crease the recall. Then feature ranking is 
applied to the extracted feature candi-
dates to improve the precision of the 
top-ranked candidates. We rank feature 
candidates by feature importance which 
is determined by two factors: feature re-
levance and feature frequency. The 
problem is formulated as a bipartite 
graph and the well-known web page 
ranking algorithm HITS is used to find 
important features and rank them high. 
Experiments on diverse real-life datasets 
show promising results. 
1 Introduction 
In recent years, opinion mining or sentiment 
analysis (Liu, 2010; Pang and Lee, 2008) has 
been an active research area in NLP. One task is 
to extract people?s opinions expressed on 
features of entities (Hu and Liu, 2004). For 
example, the sentence, ?The picture of this 
camera is amazing?, expresses a positive 
opinion on the picture of the camera. ?picture?
is the feature. How to extract features from a 
corpus is an important problem. There are 
several studies on feature extraction (e.g., Hu 
and Liu, 2004, Popescu and Etzioni, 2005, 
Kobayashi et al, 2007, Scaffidi et al, 2007, 
Stoyanov and Cardie. 2008, Wong et al, 2008, 
Qiu et al, 2009). However, this problem is far 
from being solved.  
Double Propagation (Qiu et al, 2009) is a 
state-of-the-art unsupervised technique for 
solving the problem. It mainly extracts noun 
features, and works well for medium-size 
corpora. But for large corpora, this method can 
introduce a great deal of noise (low precision), 
and for small corpora, it can miss important 
features. To deal with these two problems, we 
propose a new feature mining method, which 
enhances that in (Qiu et al, 2009). Firstly, two 
improvements based on part-whole patterns and 
?no? patterns are introduced to increase recall. 
Part-whole or meronymy is an important 
semantic relation in NLP, which indicates that 
one or more objects are parts of another object. 
1462
For example, the phrase ?the engine of the car?
contains the part-whole relation that ?engine? is 
part of ?car?. This relation is very useful for 
feature extraction, because if we know one 
object is part of a product class, this object 
should be a feature. ?no? pattern is another 
extraction pattern. Its basic form is the word 
?no? followed by a noun/noun phrase, for 
instance, ?no noise?. People often express their 
short comments or opinions on features using 
this pattern. Both types of patterns can help find 
features missed by double propagation. As for 
the low precision problem, we present a feature 
ranking approach to tackle it. We rank feature 
candidates based on their importance which 
consists of two factors: feature relevance and 
feature frequency. The basic idea of feature 
importance ranking is that if a feature candidate 
is correct and frequently mentioned in a corpus, 
it should be ranked high; otherwise it should be 
ranked low in the final result. Feature frequency 
is the occurrence frequency of a feature in a 
corpus, which is easy to obtain. However, 
assessing feature relevance is challenging. We 
model the problem as a bipartite graph and use 
the well-known web page ranking algorithm 
HITS (Kleinberg, 1999) to find important 
features and rank them high. Our experimental 
results show superior performances. In practical 
applications, we believe that ranking is also 
important for feature mining because ranking 
can help users to discover important features 
from the extracted hundreds of fine-grained 
candidate features efficiently. 
2 Related work 
Hu and Liu (2004) proposed a technique based 
on association rule mining to extract product 
features. The main idea is that people often use 
the same words when they comment on the 
same product features. Then frequent itemsets 
of nouns in reviews are likely to be product fea-
tures while the infrequent ones are less likely to 
be product features. This work also introduced 
the idea of using opinion words to find addi-
tional (often infrequent) features.
   Popescu and Etzioni (2005) investigated the 
same problem. Their algorithm requires that the 
product class is known. The algorithm deter-
mines whether a noun/noun phrase is a feature 
by computing the pointwise mutual information 
(PMI) score between the phrase and class-
specific discriminators, e.g., ?of xx?, ?xx has?,
?xx comes with?, etc., where xx is a product 
class. This work first used part-whole patterns 
for feature mining, but it finds part-whole based 
features by searching the Web. Querying the 
Web is time-consuming. In our method, we use 
predefined part-whole relation patterns to ex-
tract features in a domain corpus. These patterns 
are domain-independent and fairly accurate.  
   Following the initial work in (Hu and Liu 
2004), several researchers have further explored 
the idea of using opinion words in product fea-
ture mining. A dependency based method was 
proposed in (Zhuang et al, 2006) for a movie 
review analysis application. Qiu et al (2009) 
proposed a double propagation method, which 
exploits certain syntactic relations of opinion 
words and features, and propagates through 
both opinion words and features iteratively. The 
extraction rules are designed based on different 
relations between opinion words and features, 
and among opinion words and features them-
selves. Dependency grammar was adopted to 
describe these relations. In (Wang and Wang, 
2008), another bootstrapping method was pro-
posed. In (Kobayashi et al 2007), a pattern min-
ing method was used. The patterns are relations 
between feature and opinion pairs (they call as-
pect-evaluation pairs). The patterns are mined 
from a large corpus using pattern mining. Statis-
tics from the corpus are used to determine the 
confidence scores of the extraction.  
In general information extraction, there are 
two approaches: rule-based and statistical. Early 
extraction systems are mainly based on rules 
(e.g., Riloff, 1993). In statistical methods, the 
most popular models are Hidden Markov Mod-
els (HMM) (Rabiner, 1989), Maximum Entropy 
Models (ME) (Chieu et al, 2002) and Condi-
tional Random Fields (CRF) (Lafferty et al, 
2001). CRF has been shown to be the most ef-
fective method. It was used in (Stoyanov et al, 
2008). However, a limitation of CRF is that it 
only captures local patterns rather than long 
range patterns. It has been shown in (Qiu et al, 
2009) that many feature and opinion word pairs 
have long range dependencies. Experimental 
results in (Qiu et al, 2009) indicate that CRF 
does not perform well.  
Other related works on feature extraction 
mainly use topic modeling to capture topics in 
1463
reviews (Mei et al, 2007). In (Su et al, 2008), 
the authors also proposed a clustering based 
method with mutual reinforcement to identify 
features. However, topic modeling or clustering 
is only able to find some general/rough features, 
and has difficulty in finding fine-grained or pre-
cise features, which is more related to informa-
tion extraction.  
3 The Proposed Method 
As discussed in the introduction section, our 
proposed method deals with the problems of 
double propagation. So let us give a short ex-
planation why double propagation can cause 
problems in large or small corpora. 
 Double propagation assumes that features are 
nouns/noun phrases and opinion words are ad-
jectives. It is shown that opinion words are 
usually associated with features in some ways. 
Thus, opinion words can be recognized by iden-
tified features, and features can be identified by 
known opinion words. The extracted opinion 
words and features are utilized to identify new 
opinion words and new features, which are used 
again to extract more opinion words and fea-
tures. This propagation or bootstrapping process 
ends when no more opinion words or features 
can be found. The biggest advantage of the me-
thod is that it requires no additional resources 
except an initial seed opinion lexicon, which is 
readily available (Wilson et al, 2005, Ding et 
al., 2008). Thus it is domain independent and 
unsupervised, avoiding laborious and time-
consuming work of labeling data for supervised 
learning methods. It works well for medium?
size corpora. But for large corpora, this method 
may extract many nouns/noun phrases which 
are not features. The precision of the method 
thus drops. The reason is that during propaga-
tion, adjectives which are not opinionated will 
be extracted as opinion words, e.g., ?entire? and 
?current?. These adjectives are not opinion 
words but they can modify many kinds of 
nouns/noun phrases, thus leading to extracting 
wrong features. Iteratively, more and more 
noises may be introduced during the process. 
The other problem is that for certain domains, 
some important features do not have opinion 
words modifying them. For example, in reviews 
of mattresses, a reviewer may say ?There is a 
valley on my mattress?, which implies a nega-
tive opinion because ?valley? is undesirable for 
a mattress. Obviously, ?valley? is a feature, but 
?valley? may not be described by any opinion 
adjective, especially for a small corpus. Double 
propagation is not applicable in this situation.  
   To deal with the problem, we propose a novel 
method to mine features, which consists of two 
steps: feature extraction and feature ranking. 
For feature extraction, we still adopt the double 
propagation idea to populate feature candidates. 
But two improvements based on part-whole re-
lation patterns and a ?no? pattern are made to 
find features which double propagation cannot 
find. They can solve part of the recall problem. 
For feature ranking, we rank feature candidates 
by feature importance.        
     A part-whole pattern indicates one object is 
part of another object. For the previous example 
?There is a valley on my mattress?, we can find 
that it contains a part-whole relation between 
?valley? and ?mattress?. ?valley? belongs to 
?mattress?, which is indicated by the preposi-
tion ?on?. Note that ?valley? is not actually a 
part of mattress, but an effect on the mattress. It 
is called a pseudo part-whole relation. For sim-
plicity, we will not distinguish it from an actual 
part-whole relation because for our feature min-
ing task, they have little difference. In this case, 
?noun1 on noun2? is a good indicative pattern 
which implies noun1 is part of noun2. So if we 
know ?mattress? is a class concept, we can infer 
that ?valley? is a feature for ?mattress?. There 
are many phrase or sentence patterns 
representing this type of semantic relation 
which was studied in (Girju et al 2006). Beside 
part-whole patterns, ?no? pattern is another im-
portant and specific feature indicator in opinion 
documents. We introduce these patterns in de-
tail in Sections 3.2 and 3.3. 
   Now let us deal with the first problem: noise. 
With opinion words, part-whole and ?no? pat-
terns, we have three feature indicators at hands, 
but all of them are ambiguous, which means 
that they are not hard rules. We will inevitably 
extract wrong features (also called noises) by 
using them. Pruning noises from feature candi-
dates is a hard task. Instead, we propose a new 
angle for solving this problem: feature ranking. 
The basic idea is that we rank the extracted fea-
ture candidates by feature importance. If a fea-
ture candidate is correct and important, it should 
be ranked high. For unimportant feature or 
1464
noise, it should be ranked low in the final result. 
Ranking is also very useful in practice. In a 
large corpus, we may extract hundreds of fine-
grained features. But the user often only cares 
about those important ones, which should be 
ranked high. We identified two major factors 
affecting the feature importance: one is feature 
relevance and the other is feature frequency. 
Feature relevance: it describes how possible 
a feature candidate is a correct feature. We find 
that there are three strong clues to indicate fea-
ture relevance in a corpus. The first clue is that 
a correct feature is often modified by multiple 
opinion words (adjectives or adverbs). For ex-
ample, in the mattress domain, ?delivery? is 
modified by ?quick? ?cumbersome? and ?time-
ly?. It shows that reviewers put emphasis on the 
word ?delivery?.  Thus we can infer that ?deli-
very? is a possible feature. The second clue is 
that a feature could be extracted by multiple 
part-whole patterns. For example, in the car 
domain, if we find following two phrases, ?the
engine of the car? and ?the car has a big en-
gine?, we can infer that ?engine? is a feature for 
car, because both phrases contain part-whole 
relations to indicate ?engine? is a part of ?car?. 
The third clue is the combination of opinion 
word modification, part-whole pattern extrac-
tion and ?no? pattern extraction. That is, if a 
feature candidate is not only modified by opi-
nion words but also extracted by part-whole or 
?no? patterns, we can infer that it is a feature 
with high confidence. For example, for sentence 
?there is a bad hole in the mattress?, it strongly 
indicates that ?hole? is a feature for a mattress 
because it is modified by opinion word ?bad?
and also in the part-whole pattern. What is 
more, we find that there is a mutual enforce-
ment relation between opinion words, part-
whole and ?no? patterns, and features. If an ad-
jective modifies many correct features, it is 
highly possible to be a good opinion word. Si-
milarly, if a feature candidate can be extracted 
by many opinion words, part-whole patterns, or 
?no? pattern, it is also highly likely to be a cor-
rect feature. This indicates that the Web page 
ranking algorithm HITS is applicable.  
Feature frequency: This is another important 
factor affecting feature ranking. Feature fre-
quency has been considered in (Hu and Liu, 
2004; Blair-Goldensohn et al, 2008). We con-
sider a feature f1 to be more important than fea-
ture f2 if f1 appears more frequently than f2 in 
opinion documents. In practice, it is desirable to 
rank those frequent features higher than infre-
quent features. The reason is that missing a fre-
quently mentioned feature in opinion mining is 
bad, but missing a rare feature is not a big issue.  
   Combining the above factors, we propose a 
new feature mining method. Experiments show 
good results on diverse real-life datasets. 
3.1 Double Propagation 
As we described above, double propagation is 
based on the observation that there are natural 
relations between opinion words and features 
due to the fact that opinion words are often used 
to modify features. Furthermore, it is observed 
that opinion words and features themselves have 
relations in opinionated expressions too (Qiu et 
al., 2009). These relations can be identified via 
a dependency parser (Lin, 1998) based on the 
dependency grammar. The identification of the 
relations is the key to feature extraction. 
Dependency grammar: It describes the de-
pendency relations between words in a sentence. 
After parsed by a dependency parser, words in a 
sentence are linked to each other by a certain 
relation. For a sentence, ?The camera has a 
good lens?, ?good? is the opinion word and 
?lens? is the feature of camera. After parsing, 
we can find that ?good? depends on ?lens? with 
relation mod. Here mod means that ?good? is 
the adjunct modifier for ?lens?. In some cases, 
an opinion word and a feature are not directly 
dependent, but they directly depend on a same 
word. For example, from the sentence ?The lens 
is nice?, we can find that both feature ?lens? and 
opinion word ?nice? depend on the verb ?is?
with the relation s and pred respectively. Here s
means that ?lens? is the surface subject of ?is?
while pred means that ?nice? is the predicate of 
the ?is? clause.    
   In (Qiu et al, 2009), it defines two categories 
of dependency relations to summarize all types 
of dependency relations between two words, 
which are illustrated in Figure 1. Arrows are 
used to represent dependencies. 
Direct relations: It represents that one word 
depends on the other word directly or they both 
depend on a third word directly, shown in (a) 
and (b) of Figure 1. In (a), B depends on A di-
rectly, and in (b) they both directly depend on D.
    Indirect relation: It represents that one word 
1465
depends on the other word through other words 
or they both depend on a third word indirectly. 
For example, in (c) of Figure 1, B depends on A
through D; in (d) of Figure 1, A depends on D
through I1 while B depends on D through I2. For 
some complicated situations, there can be more 
than one I1 or I2.    
Fig.1 Different relations between A and B 
    
      Parsing indirect relations is error-prone for 
Web corpora. Thus we only use direct relation 
to extract opinion words and feature candidates 
in our application. For detailed extraction rules, 
please refer to the paper (Qiu et al, 2009). 
3.2 Part-whole relation 
As we discussed above, a part-whole relation is 
a good indicator for features if the class concept 
word (the ?whole? part) is known. For example, 
the compound nominal ?car hood? contains the 
part-whole relation. If we know ?car? is the 
class concept word, then we can infer that 
?hood? is a feature for car. Part-whole patterns 
occur frequently in text and are expressed by a 
variety of lexico-syntactic structures (Girju et 
al, 2006; Popescu and Etzioni, 2005). There are 
two types of lexico-syntactic structures convey-
ing part-whole relations: unambiguous structure 
and ambiguous structure. The unambiguous 
structure clearly indicates a part-whole relation. 
For example, for sentences ?the camera consists 
of lens, body and power cord.? and ?the bed 
was made of wood?. In these cases, the detec-
tion of the patterns leads to the discovery of real 
part-whole relations. We can easily find features 
of the camera and the bed. Unfortunately, this 
kind of patterns is not very frequent in a corpus. 
However, there are many ambiguous expres-
sions that are explicit but convey part-whole 
relations only in some contexts. For example, 
for two phrases ?valley on the mattress? and 
?toy on the mattress?, ?valley? is a part of ?mat-
tress? whereas ?toy? is not a part of ?mattress?.
Our idea is to use both the unambiguous and 
ambiguous patterns. Although ambiguous pat-
terns may bring some noise, we can rank them 
low in the ranking procedure. The following 
two kinds of patterns are what we have utilized 
for feature extraction.
3.2.1 Phrase pattern 
In this case, the part-whole relation exists in a 
phrase.
NP + Prep + CP:  noun/noun phrase (NP) 
contains the part word and the class concept 
phrase (CP) contains the whole word. They are 
connected by the preposition word (Prep). For 
example, ?battery of the camera? is an instance 
of this pattern where NP (battery) is the part
noun and CP (camera) is the whole noun. For 
our application, we only use three specific pre-
positions: ?of?, ?in? and ?on?.  
CP + with + NP:   likewise, CP is the class 
concept phrase, and NP is the noun/noun phrase. 
They are connected by the word ?with?. Here 
NP is likely to be a feature. For example, in a 
phrase, ?mattress with a cover?, ?cover? is a 
feature for mattress.
NP CP or CP NP: noun/noun phase (NP) 
and class concept phrase (CP) forms a com-
pound word. For example, ?mattress pad?. Here 
?pad? is a feature of ?mattress?. 
3.2.2 Sentence pattern 
In these patterns, the part-whole relation is indi-
cated in a sentence. The patterns contain specif-
ic verbs. The part word and the whole word can 
be found inside noun phrases or prepositional 
phrases which contain specific prepositions. We 
utilize the following patterns in our application. 
   ?CP Verb NP?:  CP is the class concept 
phrase that contains the whole word, NP is the 
noun phrase that contains the part word and the 
verb is restricted and specific. For example, in a 
sentence, ?the phone has a big screen?, we can 
infer that ?screen? is a feature for ?phone?,
which is a class concept. In sentence patterns, 
verbs play an important role. We use indicative 
verbs to find part-whole relations in a sentence, 
A D
A BB
B
A
D
A
D
I1
B
I2
(a) (b) 
(c) (d) 
1466
i.e., ?has?, ?have? ?include? ?contain? ?consist?, 
?comprise? and so on (Girju et al 2006). 
It is worth mentioning that in order to use 
part-whole relations, the class concept word for 
a corpus is needed, which is fairly easy to find 
because the noun with the most frequent occur-
rences in a corpus is always the class concept 
word based on our experiments.  
3.3 ?no? Pattern 
Besides opinion word and part-whole relation, 
?no? pattern is also an important pattern indicat-
ing features in a corpus. Here ?no? represents 
word no.  The basic form of the pattern is ?no? 
word followed by noun/noun phrase. This sim-
ple pattern actually is very useful to feature ex-
traction. It is a specific pattern for product re-
views and forum posts. People often express 
their comments or opinions on features by this 
short pattern. For example, in a mattress domain, 
people always say that ?no noise? and ?no in-
dentation?. Here ?noise? and ?indentation? are 
all features for the mattress. We discover that 
this pattern is frequently used in corpora and a 
very good indicator for features with a fairly 
high precision. But we have to take care of the 
some fixed ?no? expression, like ?no problem?
?no offense?. In these cases, ?problem? and ?of-
fense? should not be regarded as features. We 
have a list of such words, which are manually 
compiled.    
3.4 Bipartite Graph and HITS Algorithm 
Hyperlink-induced topic search (HITS) is a link 
analysis algorithm that rates Web pages. As 
discussed in the introduction section, we can 
apply the HITS algorithm to compute feature 
relevance for ranking.  
   Before illustrating how HITS can be applied 
to our scenario, let us first give a brief 
introduction to HITS. Given a broad search 
query q, HITS sends the query to a search 
engine system, and then collects k (k = 200 in 
the original paper) highest ranked pages, which 
are assumed to be highly relevant to the search 
query. This set is called the root set R; then it 
grows R by including any page pointed to a 
page in R, then forms a base set S. HITS then 
works on the pages in S. It assigns every page in 
S an authority score and a hub score. Let the 
number of pages to be studied be n. We use G = 
(V, E) to denote the (directed) link graph of S. V
is the set of pages (or nodes) and E is the set of 
directed edges (or links). We use L to denote the 
adjacency matrix of the graph.  
??? ?  ?
?????? ?? ? ?
??????????
                 (1) 
Let the authority score of the page i be A(i), and 
the hub score of page i be H(i). The mutual rein-
forcing relationship of the two scores is 
represented as follows: 
                  ???? ? ? ???????????             (2)    
                  ???? ? ? ???????????                (3)
We can write them in a matrix form. We use A
to denote the column vector with all the authori-
ty scores, A = (A(1), A(2), ?, A(n))T, and use H
to denote the column vector with all the hub 
scores, H = (H(1), H(2), ?, H(n))T,
                             ? ?4)                         ???) 
                             ? ? 5)                            ??)
To solve the problem, the widely used method 
is power iteration, which starts with some ran-
dom values for the vectors, e.g., A0 = H0 = (1, 1, 
1, ?1,). It then continues to compute iteratively 
until the algorithm converges.  
   From the formulas, we can see that the author-
ity score estimates the importance of the content 
of the page, and the hub score estimates the val-
ues of its links to other pages. An authority 
score is computed as the sum of the scaled hub 
scores that point to that page. A hub score is the 
sum of the scaled authority scores of the pages 
it points to. The key idea of HITS is that a good 
hub points to many good authorities and a good 
authority is pointed by many good hubs. Thus, 
authorities and hubs have a mutual reinforce-
ment relationship. 
   For our scenario, we have three strong clues 
for features in a corpus: opinion words, part-
whole patterns, and the ?no? pattern. Although 
all these three clues are not hard rules, there 
exist mutual enforcement relations between 
them. If an adjective modify many features, it is 
highly likely to be a good opinion word. If a 
feature candidate is modified by many opinion 
words, it is likely to be a genuine feature. The 
same goes with part-whole patterns, the ?no? 
pattern, or the combination for these three clues. 
This kind of mutual enforcement relation can be 
naturally modeled in the HITS framework.  
1467
Applying the HITS algorithm: Based on the 
key idea of HITS algorithm and feature indica-
tors, we can apply the HITS algorithm to obtain 
the feature relevance ranking. Features act as 
authorities and feature indicators act as hubs. 
Different from the general HITS algorithm, fea-
tures only have authority scores and feature in-
dicators only have hub scores in our case. They 
form a directed bipartite graph, which is illu-
strated in Figure 2. We can run the HITS algo-
rithm on this bipartite graph. The basic idea is 
that if a feature candidate has a high authority 
score, it must be a highly-relevant feature. If a 
feature indicator has a high hub score, it must be 
a good feature indicator. 
Fig. 2 Relations between feature indicators and 
features 
3.5 Feature Ranking 
Although the HITS algorithm can rank features 
by feature relevance, the final ranking is not 
only determined by relevance. As we discussed 
before, feature frequency is another important 
factor affecting the final ranking. It is highly 
desirable to rank those correct and frequent 
features at top because they are more important 
than the infrequent ones in opinion mining (or 
even other applications). With this in mind, we 
put everything together to present the final 
algorithm that we use. We use two steps: 
Step 1:  Compute feature score using HITS 
without considering frequency. Initially, we use 
three feature indicators to populate feature 
candidates, which form a directed bipartite 
graph. Each feature candidate acts as an 
authority node in the graph; each feature 
indicator acts as a hub node. For node s in the 
graph, we let ?? be the hub score and ?? be the 
authority score. Then, we initialize ?? and ?? to 
1 for all nodes in the graph. We update the 
scores of ??  and ??  until they converge using 
power iteration. Finally, we normalize ??  and 
compute the score S for a feature.
Step 2: The final score function considering 
the feature frequency is given in Equation (6).  
? ? ????????????????                       (6)
where ???????  is the frequency count of 
ture?, and S(f) is the authority score of the can-
didate feature f. The idea is to push the frequent 
candidate features up by multiplying the log of 
frequency. Log is taken in order to reduce the 
effect of big frequency count numbers.    
4 Experiments
This section evaluates the proposed method. We 
first describe the data sets, evaluation metrics 
and then the experimental results. We also com-
pare our method with the double propagation
method given in (Qiu et al, 2009).  
4.1 Data Sets 
We used four diverse data sets to evaluate our 
techniques. They were obtained from a com-
mercial company that provides opinion mining 
services. Table 1 shows the domains (based on 
their names) and the number of sentences in 
each data set (?Sent.? means the sentence). The 
data in ?Cars? and ?Mattress? are product re-
views extracted from some online review sites. 
?Phone? and ?LCD? are forum discussion posts 
extracted from some online forum sites. We 
split each review/post into sentences and the 
sentences are POS-tagged using the Brill?s tag-
ger (Brill, 1995). The tagged sentences are the 
input to our system.  
Data  Sets Cars Mattress Phone LCD 
# of Sent. 2223 13233 15168 1783 
                 Table 1.  Experimental data sets 
4.2 Evaluation Metrics 
Besides precision and recall, we adopt the pre-
cision@N metric for experimental evaluation 
(Liu, 2006). It gives the percentage of correct 
features that are among the top N feature candi-
dates in a ranked list. We compare our method?s 
results with those of double propagation which 
ranks extracted candidates only by occurrence 
frequency.  
4.3 Experimental Results 
We first compare our results with double propa-
    Feature Indicators                      Features 
1468
gation on recall and precision for different cor-
pus sizes. The results are presented in Tables 2, 
3, and 4 for the four data sets. They show the 
precision and recall of 1000, 2000, and 3000 
sentences from these data sets. We did not try 
more sentences because manually checking the 
recall and precision becomes prohibitive. Note 
that there are less than 3000 sentences for ?Cars? 
and ?LCD? data sets. Thus, the columns for 
?Cars? and ?LCD? are empty in Table 4. In the 
Tables, ?DP? represents the double propagation 
method; ?Ours? represents our proposed method; 
?Pr? represents precision, and ?Re? represents 
recall. 
    
 Cars Mattress Phone LCD 
Pr Re Pr Re Pr Re Pr Re 
DP 0.79 0.55 0.79 0.54 0.69 0.23 0.68 0.43
Ours 0.78 0.56 0.77 0.64 0.68 0.44 0.66 0.55
Table 2. Results of 1000 sentences 
 Cars Mattress Phone LCD 
Pr Re Pr Re Pr Re Pr Re
DP 0.70 0.65 0.70 0.58 0.67 0.42 0.64 0.52
Ours 0.66 0.69 0.70 0.66 0.70 0.50 0.62 0.56
Table 3. Results of 2000 sentences  
 Cars Mattress Phone LCD
Pr Re Pr Re 
DP 0.65 0.59 0.64 0.48
Ours 0.66 0.67 0.62 0.51
Table 4. Results of 3000 sentences  
From the tables, we can see that for corpora in 
all domains, our method outperforms double 
propagation on recall with only a small loss in 
precision. In data sets for ?Phone? and ?Mat-
tress?, the precisions are even better. We also 
find that with the increase of the data size, the 
recall gap between the two methods becomes 
smaller gradually and the precisions of both me-
thods also drop. However, in this case, feature 
ranking plays an important role in discovering 
important features. 
   Ranking comparison between the two me-
thods is shown in Tables 5, 6, and 7, which give 
the precisions of top 50, 100 and 200 results 
respectively. Note that the experiments reported 
in these tables were run on the whole data sets. 
There were no more results for the ?LCD? data 
beyond top 200 as there were only a limited 
number of features discussed in the data. So the 
column for ?LCD? in Table 7 is empty. We rank 
the extracted feature candidates based on fre-
quency for the double propagation method (DP). 
Using occurrence frequency is the natural way 
to rank features. The more frequent a feature 
occurs in a corpus, the more important it is. 
However, frequency-based ranking assumes the 
extracted candidates are correct features. The 
tables show that our proposed method (Ours) 
outperforms double propagation considerably. 
The reason is that some highly-frequent feature 
candidates extracted by double propagation are 
not correct features. Our method considers the 
feature relevance as an important factor. So it 
produces much better rankings.  
 Cars Mattress Phone LCD 
DP 0.84 0.81 0.64 0.68 
Ours 0.94 0.90 0.76 0.76 
Table 5. Precision at top 50
 Cars Mattress Phone LCD 
DP      0.82      0.80      0.65      0.68 
Ours      0.88      0.85      0.75      0.73 
Table 6. Precision at top 100
 Cars Mattress Phone LCD 
DP      0.75      0.71      0.70  
Ours      0.80      0.79      0.76  
Table 7. Precision at top 200 
5 Conclusion
Feature extraction for entities is an important 
task for opinion mining. The paper proposed a 
new method to deal with the problems of the 
state-of-the-art double propagation method for 
feature extraction. It first uses part-whole and 
?no? patterns to increase recall. It then ranks the 
extracted feature candidates by feature impor-
tance, which is determined by two factors: fea-
ture relevance and feature frequency. The Web 
page ranking algorithm HITS was applying to 
compute feature relevance. Experimental results 
using diverse real-life datasets show promising 
results. In our future work, apart from improv-
ing the current methods, we also plan to study 
the problem of extracting features that are verbs 
or verb phrases.
Acknowledgement 
This work was funded by a HP Labs Innovation 
Research Program Award (CW165044).  
1469
References 
Blair-Goldensohn, Sasha., Kerry, Hannan., Ryan, 
McDonald., Tyler, Neylon., George A. Reis, Jeff, 
Reyna. 2008. Building Sentiment Summarizer for 
Local Service Reviews In Proceedings of the 
Workshop of NLPIX . WWW, 2008
Brill, Eric. 1995. Transformation-Based Error-
Driven Learning and Natural Language 
Processing: a case study in part of speech tagging. 
Computational Linguistics, 1995.   
Chieu, Hai Leong and Hwee-Tou Ng. 2002. Name 
Entity Recognition: a Maximum Entropy Ap-
proach Using Global Information. In Proceedings 
of the 6th Workshop on Very Large Corpora, 
2002.  
Ding, Xiaowen., Bing Liu and Philip S. Yu. 2008. A 
Holistic Lexicon-Based Approach to Opinion 
Mining  In Proceedings of WSDM 2008.
Girju, Roxana., Adriana Badulescu and Dan Moldo-
van. 2006. ?Automatic Discovery of Part-Whole 
Relations?  Computational Linguistics ,32(1):83-
135 2006 
Hu, Mingqin and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of
KDD 2004
Kleinberg, Jon. 1999. ?Authoritative sources in 
hyperlinked environment? Journal of the ACM 46 
(5): 604-632 1999
Kobayashi, Nozomi., Kentaro Inui and Yuji Matsu-
moto. 2007 Extracting Aspect-Evaluation and As-
pect-of Relations in Opinion Mining. In Proceed-
ings of EMNLP, 2007.
Lafferty, John., Andrew McCallum and Fernando 
Pereira. 2001 Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Se-
quence Data.  In Proceedings of ICML, 2001. 
Lin, Dekang. 1998. Dependency-based evaluation of 
MINIPAR. In Proceedings of the Workshop on 
Evaluation of Parsing System at ICLRE 1998.
Liu, Bing. 2006. Web Data Mining: Exploring 
Hyperlinks, contents and usage data. Springer, 
2006.
Liu, Bing. 2010. Sentiment analysis and subjectivity. 
Handbook of Natural Language Processing, 
second edition, 2010. 
Mei, Qiaozhu, Ling Xu, Matthew Wondra, Hang Su 
and ChengXiang Zhai. 2007. Topic Sentiment 
Mixture: Modeling Facets and Opinions in Web-
logs. In Proceedings of WWW, pages 171?180, 
2007.
Pang, Bo., Lillian Lee. 2008. Opinion Mining and 
Sentiment Analysis. Foundations and Trends in 
Information Retrieval  pp. 1-135 2008
Pantel, Patrick., Eric Crestan, Arkady Borkovsky, 
Ana-Maria  Popescu, Vishunu Vyas.  2009. Web-
Scale Distributional Similarity and Entity Set Ex-
pansion.  In Proceedings of. EMNLP, 2009 
Popescu, Ana-Maria and Oren, Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of EMNLP, 2005.
Qiu, Guang., Bing, Liu., Jiajun Bu and Chun Chen. 
2009. Expanding Domain Sentiment Lexicon 
through Double Propagation. In Proceedings of 
IJCAI 2009.
Rabiner, Lawrenence. 1989. A Tutorial on Hidden 
Markov Models and Selected Applications in 
Speech Recognition. In Proceedings of the IEEE, 
77(2), 1989.
Riloff, Ellen. 1993. Automatically Constructing a 
Dictionary for Information Extraction Tasks. In 
Proceedings of AAAI 1993.
Scaffidi, Christopher., Kevin Bierhoff, Eric Chang, 
Mikhael Felker, Herman Ng and Chun Jin. 2007. 
Red opal: Product-feature Scoring from Reviews. 
In Proceedings of EC 2007
Stoyanov, Veselin and Claire Cardie. 2008. Topic 
Identification for Fine-grained Opinion Analysis. 
In Proceedings of COLING 2008 
Su, Qi., Xinying Xu., Honglei Guo, Zhili Guo, Xian 
Wu, Xiaoxun Zhang, Bin Swen and Zhong Su. 
2008. Hidden Sentiment Association in Chinese 
Web Opinion Mining. In Proceedings of WWW 
2008.
Wang, Bo., Houfeng Wang. 2008.  Bootstrapping 
both Product Features and Opinion Words from 
Chinese Customer Reviews with Cross-Inducing  
In Proceedings of IJCNLP 2008 
Wilson, Theresa., Janyce Wiebe and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of 
HLT/EMNLP 2005
Wong, Tak-Lam., Wai Lam and Tik-Sun Wong.  
2008. An Unsupervised Framework for Extracting 
and Normalizing Product Attributes from Multiple 
Web Sites In Proceedings of SIGIR 2008
Zhuang, Li., Feng Jing, Xiao-yan Zhu. 2006. Movie 
Review Mining and Summarization. In Proceed-
ings of CIKM 2006
1470
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 667?676, Dublin, Ireland, August 23-29 2014.
Review Topic Discovery with Phrases using the P?lya Urn Model 
 
 
Geli Fei 
Department of Computer 
Science, University of Illi-
nois at Chicago, Chicago, 
USA 
gfei2@uic.edu 
Zhiyuan Chen 
Department of Computer  
Science, University of Illi-
nois at Chicago, Chicago, 
USA 
czyuanacm@gmail.com 
Bing Liu 
Department of Computer  
Science, University of Illi-
nois at Chicago, Chicago, 
USA 
liub@cs.uic.edu 
  
 
Abstract 
Topic modelling has been popularly used to discover latent topics from text documents. Most existing 
models work on individual words. That is, they treat each topic as a distribution over words. However, 
using only individual words has several shortcomings. First, it increases the co-occurrences of words 
which may be incorrect because a phrase with two words is not equivalent to two separate words. These 
extra and often incorrect co-occurrences result in poorer output topics. A multi-word phrase should be 
treated as one term by itself. Second, individual words are often difficult to use in practice because the 
meaning of a word in a phrase and the meaning of a word in isolation can be quite different. Third, 
topics as a list of individual words are also difficult to understand by users who are not domain experts 
and do not have any knowledge of topic models. In this paper, we aim to solve these problems by 
considering phrases in their natural form. One simple way to include phrases in topic modelling is to 
treat each phrase as a single term. However, this method is not ideal because the meaning of a phrase is 
often related to its composite words. That information is lost. This paper proposes to use the generalized 
P?lya Urn (GPU) model to solve the problem, which gives superior results. GPU enables the connection 
of a phrase with its content words naturally. Our experimental results using 32 review datasets show 
that the proposed approach is highly effective. 
1 Introduction 
Topic models such as LDA (Blei et al., 2003) and pSLA (Hofmann 1999) and their extensions have 
been popularly used to find topics in text documents. These models are mostly governed by the phe-
nomenon called ?higher-order co-occurrence? (Heinrich 2009), i.e., how often terms co-occur in differ-
ent contexts. Word w1 co-occurring with word w2 which in turn co-occurs with word w3 denotes a sec-
ond-order co-occurrence between w1 and w3. Almost all these models regard each topic as a distribution 
over words. The words under each topic are often sorted according to their associated probabilities. 
Those top ranked words are used to represent the topic. However, this representation of topics as a list 
of individual words has some 1major shortcomings: 
? Topics are often difficult to understand or interpret by users unless they are domain experts and also 
knowledgeable about topic models. In most real-life situations, these are not the case. In some of our 
applications, we show users several good topics, but they have no idea what they are because many 
domain phrases cannot be split to individual words. For example, ?battery? and ?life? are put under 
the same topic, which is not bad. But the users wondered why ?battery? and ?life? are the same 
because they thought words under a topic should somehow have similar meanings. We had to explain 
that it is due to ?battery life.? As another example, sentences such as ?This hotel has a very nice 
sandy beach? may cause a topic model to put ?hotel? and ?sandy? in a topic, which is not wrong but 
again it is hard to understand by a user who may not be able to connect the two words. Thus in order 
to interpret topics well, the user must know the phrases (they are split into individual words) that may 
                                                 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
 
667
be used in a domain and how words may be associated with each other. To make the matters worse, 
in most cases, the topics generated from a topic model are not perfect. There are some wrong words 
under a topic, which make the interpretation even harder.  
? Individual words are difficult to use in practice because in some cases a word under a topic may not 
have its intended meaning for the topic in a particular sentence context. This can cause many mis-
takes. For example, in sentiment analysis of product reviews, a topic is often regarded as a set of 
words indicating a product feature or attribute. This is not true in many cases. For example, if ?bat-
tery? and ?life? are put in one topic, when the system sees ?life,? it assumes it is related to ?battery.? 
But in the sentence ?The life expectancy of the machine is about 2 years,? this ?life? has nothing to 
do with battery or battery life. This causes an error. If the system can directly use phrases, ?battery 
life? and ?life expectancy,? the error will not occur.   
? Splitting phrases into multiple individual words causes extra co-occurrences that may result in poor 
or wrong topics involving other words. For example, due to sentences like ?Beach staffs are rude? 
and ?The hotel has a nice sandy beach,? a topic model may put ?staff? and ?sandy? under a topic for 
staff and/or put ?beach? and ?rude? together under the topic of beach views.   
Based on our experiences in opinion mining and social media mining, these are major issues with 
topic models. We believe that they must be dealt with before wide spread adaptation of topic models in 
real-life applications. In this paper, we make an attempt to solve this problem. We will use term to 
represent both word and phrase, and use word or phrase when we want to distinguish them.  
One obvious way to consider phrases is to use a natural language parser to find all phrases and then 
treat each phrase as one term, e.g., ?battery life,? ?sandy beach? and ?beach staff.? However, the prob-
lem with this approach is that it may lose the connection of many related words or phrases in a topic. 
For example, under the topic for beach, we may not find ?sandy beach? because there is no co-occur-
rence of ?sandy beach? and ?beach? if we treat ?sandy beach? as a single term. This is clearly not a good 
solution as it may miss a lot of topical terms (words or phrases) for a topic. It can also result in poor 
topics due to the loss of co-occurrences.  
Another obvious solution is to use individual words as they are, but add an extra term representing 
the phrase. For example, we can turn the sentence ?This hotel has a nice sandy beach? to ?This hotel 
has a nice sandy beach <sandy beach>.? This solution helps deal with the problem of losing co-occur-
rences to some extent, but because the words are still treated individually, the three problems discussed 
above still exist, although the phrase ?sandy beach? now can show up in some topics. However, due to 
the fact that phrases are obviously less frequent than individual words, they may be ranked very low, 
which make little difference to solving the three problems. 
In this paper, we propose a novel approach to solve the problem, which is based on the generalized 
P?lya urn (GPU) model (Mahmoud 2008). GPU was first introduced into LDA in (Mimno et al., 2011) 
to concentrate words with high co-document frequency. However, Mimno et al. (2011) and other re-
searchers Chen et al., (2013) still use them in the framework of individual words. In the GPU model, we 
can deal with the problems above by treating phrases as individual terms and allowing their component 
words to have some connections or co-occurrences with them. Furthermore, we can push phrases up in 
a topic as phrases are important for understanding but are usually less frequent than individual words 
and ranked low in a topic. The intuition here is that when we see a phrase, we also see a small fraction 
of their component words; and when we see each individual word, we also see a small fraction of its 
related phrases. Further, in a phrase not all words are equally important. For example, in ?hotel staff?, 
?staff? is more important as it is the head noun, which represents the semantic category of the phrase. 
Our experiments are conducted using online review collections from 32 domains. We will see that 
the proposed method produces significantly better results both quantitatively based on the statistical 
measure of topic coherence and qualitatively based on human labeling of topics and topical terms. 
In summary, this paper makes the following contributions: 
1. It proposes to consider phrases in topic models, which as we have explained above, is important 
for accurate topic generation, the use of the resulting topics and human interpretation. As we will 
see in Section 2, although some prior works exist, they are based on n-grams (Mukherjee and Liu, 
2013). They are different from our approach. N-grams can generate many non-understandable 
phrases. Furthermore, due to infrequency of n-grams (much less frequent than individual words), 
668
typically a huge amount of data is needed in order to produce reasonable topics, which many ap-
plications simply do not have.  
2. It proposes to use the generalized P?lya Urn (GPU) model to deal with the problems arising in 
considering phrases. To the best of our knowledge, the GPU model has not been used in the context 
of phrases. This model not only generates better topics, but also rank phrases relatively high in 
their topics, which greatly helps understanding of the generated topics. 
3. Comprehensive experiments conducted using product and service review collections from 32 do-
mains demonstrate the effectiveness of the proposed model. 
2 Related Work 
GPU was first introduced to topic modelling in (Mimno et al., 2011), in which GPU is used to concen-
trate words with high co-document frequency based on corpus-specific co-occurrence statistics. Chen et 
al. (2013) applied GPU to deal with the adverse effect of using prior domain knowledge in topic 
modeling by increasing the counts of rare words in the knowledge sets. However, these works still use 
only individual words. 
Topics in most topic models like LDA are unigram distributions over words and assume words to be 
exchangeable at the word level. However, there exists some work that tries to take word order into 
consideration by including n-gram language models. Wallach (2006) proposed the Bigram Topic Model 
(BTM) which integrates bigram statistics with topic-based approaches to document modeling. Wang et 
al. (2007) proposed the Topical N-gram Model (TNG), which is a generalization of the BTM. It 
generates words in their textual order by first sampling a topic, then sampling its status as a unigram or 
bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Although the 
?bag-of-words? assumption does not always hold in real-life applications, it offers a great computational 
advantage over more complex models taking word order into account for discovering significant n-
grams. Our approach is different from these works in two ways. First, we still follow the ?bag-of-words? 
or rather ?bag-of-terms? assumption. Second, we find actual phrases rather than just n-grams. Most n-
grams are still hard to understand because they are not natural phrases.   
Blei and Lafferty (2009), Liu et al. (2010) and Zhao et al. (2011) also try to extract keyphrases from 
texts. Their methods, however, are very different because they identify multi-word phrases using 
relevance and likelihood scores in the post-processing step based on the discovered topical unigrams. 
Mukherjee and Liu (2013) and Mukherjee et al. (2013) all try to include n-grams to enhance the 
expressiveness of their models while preserving the advantages of ?bag-of-words? assumption, which 
has a similar idea as our paper. However, as we point out in the introduction, this way of including 
phrases/n-grams suffers from several shortcomings. Solving these problems is the goal of our paper. 
Finally, since we use product reviews as our datasets, our work is also related to opinion mining using 
topic models, e.g. (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008; Zhao et al., 2010; 
Li et al., 2010; Sauper and Barzilay, 2013; Lin and He, 2009; Jo and Oh, 2011). However, none of these 
models uses phrases. 
3 Proposed Model 
We start by briefly reviewing the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003). Then we 
describe the simple P?lya urn (SPU) model, which is embedded in LDA. After that, we present the 
generalized P?lya urn (GPU) model and discuss how it can be applied to our context. The proposed 
model uses GPU for its inference. It shares the same graphical model as LDA. However, the GPU in-
ference mechanism is very different from that of LDA, which cannot be reflected in the graphical model 
or the generative process as it only helps to infer more desirable posterior distributions of topic models. 
3.1 Latent Dirichlet Allocation 
LDA is a generative probabilistic model for a document collection. It assumes that documents are rep-
resented as a mixture of latent topics, and each latent topic is characterized by a distribution over terms. 
In order to generate a term ??
(?)
 in document ?, where ? is its position, we first draw a discrete topic 
assignment ??
(?)
 from a document-specific distribution over ? topics ??, which is drawn from a prior 
Dirichlet distribution with hyperparameter ?. Then we draw a term from the topic-specific distribution 
669
over the vocabulary ?
??
(?), which is drawn from a prior Dirichlet distribution with hyperparameter ?. 
For inference, instead of directly estimating ? and ?, Gibbs sampling is used to approximate them 
based on the posterior estimates of latent topic assignment ?. The Gibbs sampling procedure considers 
each term in the documents in turn, and estimates the probability of assigning the current term to each 
topic, conditioned on the topic assignments to all other terms. Griffiths and Steyvers (2004) showed this 
could be calculated by: 
 
? (??
(?)
= ?|???,?,?, ?, ?) ?
??|? + ?
?? + ??
?
?
??
(?)
|?
+ ?
?? + ??
 (1) 
where  ??
(?)
= ? represents the topic assignment of term ??
(?)
 to topic ?, and  ???,? refers to the topic 
assignments of all other terms. ? denotes all terms in the document collection, ? denotes the size of 
vocabulary of the collection, ? is the number of topics in the corpus, ??|? is the count of term ? under 
topic ?, ?? = ? ???|??? , and ??|? refers the count of topic ? being assigned to some terms in document 
?, ?? = ? ???|??? . All these counts exclude the current term. 
3.2 Simple P?lya Urn Model 
Traditionally, the P?lya urn model is designed in the context of colored balls and urns. In the context of 
topic models, a term can be seen as a ball of a certain color and the urn contains a mixture of balls with 
various colors. The classic topic-word (or topic-term) distribution can be reflected by the color propor-
tion of balls in the urn. LDA follows the simple P?lya urn (SPU) model, which works as follows: when 
a ball of a particular color is drawn from an urn, that ball is put back to the urn along with another ball 
of the same color. This process corresponds to assigning a topic to a term in the Gibbs sampler of LDA. 
Based on the topic-specific ?collapsed? probability of a term ? given topic ?, 
??|?+?
??+??
, which is essen-
tially the second ratio in (1), drawing a term ? will only increase the probability of seeing ? in the 
future sampling process. This self-reinforcing property is known as ?the rich get richer?. In the next 
subsection, we will introduce the generalized P?lya urn (GPU) model, which increases the probability 
of seeing certain other terms when we sample a term. 
3.3 Generalized P?lya Urn Model 
The generalized P?lya urn (GPU) model differs from SPU in that, when a ball of a certain color is 
drawn, two balls of that color is put back along with a certain number of balls of some other colors. 
Unlike SPU, GPU sampling not only allows us to see a ball of the same color again with higher proba-
bility, but also increases the probability of seeing balls with certain other colors. These additional balls 
of certain other colors added to the urn increase their proportions in the urn. We call this the promotion 
of these colored balls. Applying the idea, there are two directions of promotion in our application (Note 
that in each sentence, we need to identify each phrase, but do not need to add any extra information): 
1. Word to phrase: When an individual word is assigned to a topic (analogous to drawing a ball of 
a certain color), each phrase containing the word will be promoted, meaning that the phrase will 
be added to the same topic with a small count. That is, a fraction of the phrase will be assigned to 
the topic. This is justified because it is reasonable to assume that the phrase is related to the word 
to some extent in meaning.  
2. Phrase to word: When a phrase is assigned to a topic, each component word in it is also promoted 
with a certain small count. That is, each word is also assigned the topic by a certain amount. In 
most cases, the head nouns are more important. Thus, we promote the head nouns more. For 
example, in ?hotel staff?, ?staff? is the head noun that determines the category of the noun phrase. 
The rationale of this promotion is similar to that above.  
Let ??
(?)
 be a word and ?_? be the word itself or a phrase containing the word ??
(?)
. ? represents a 
term, and ?_? indicates all the related terms of ?. The new GPU sampling is as follows:  
 
? (??
(?)
= ?|???,?,?, ?, ?, ?) ?
??|? + ?
?? + ??
?
? ??_?|???_?,??
(?) + ??_?
? ? ??_?|???_?,??_?? + ??
 (2) 
670
where ?  is a ? ? ? real-value matrix, each cell of which contains a real value virtualcount, indicating 
the amount of promotion of a term under a topic when assigning this topic to another term. ? is size of 
all terms. The new model retains the document-topic component of standard LDA, which is the first 
ratio in (1), but replaces the usual P?lya urn topic-word (topic-term) component, the second ratio in (1), 
with a generalized P?lya urn framework (Mahmoud 2008; Mimno et al., 2011). The simple P?lya urn 
model is a simplified version of GPU in which matrix ? is an identity matrix. In this paper, ? is an 
asymmetric matrix because the main goal of using GPU is to promote the less frequent phrases in the 
documents. 
4 EXPERIMENTS 
In this section, we evaluate the proposed method of considering phrases in topic discovery, and compare 
it with three baselines. The first baseline discovers topics using LDA in a traditional way without con-
sidering phrases, i.e., using only individual words. We refer to this baseline as LDA(w). The second 
baseline considers phrases by treating each whole phrase as a separate term in the corpus. We refer to 
this baseline as LDA(p). The third baseline considers phrases by keeping individual component words 
in the phrases as they are, but also adding phrases as extra terms. We refer to this baseline as LDA(w_p). 
We refer to our proposed method as LDA(p_GPU). Note that for those words that are not in any phrases, 
they are treated as individual words (or unigrams). 
Data Set: We use product reviews from 30 sub-categories (types of product) in the electronics domain 
from Amazon.com. The sub-categories are ?Camera?, ?Mouse?, ?Cellphone,? etc (see the whole list 
below Figure 1). Each domain contains 1,000 reviews. Besides, we also use a collection of hotel reviews 
and a collection of restaurant reviews from TripAdvisor.com and Yelp.com. The hotel review data con-
tains 101,234 reviews, and the restaurant review data contains 25,459 reviews. We thus have a total of 
32 domains. We ran the Stanford Parser to perform sentence detection, lemmatization and POS tagging. 
Punctuations, stopwords, numbers and words appearing less than 5 times in each dataset are removed. 
Domain names are also removed, e.g., word ?camera? for the domain Camera, since it co-occurs with 
most words in the dataset, leading to high similarity among topics/aspects. 
Sentences as Documents: As noted in (Titov and McDonald, 2008), when standard topic models are 
applied to reviews as documents, they tend to produce topics that correspond to global properties of 
products (e.g., product brand name), but cannot separate different product aspects or features well. The 
reason is that all reviews of the same product type basically evaluate the same aspects of the product 
type. Only the brand names and product names are different. Thus, using individual reviews for model-
ling is ineffective for finding product aspects or features, which are our topics. Although there are ap-
proaches which model sentences (Jo and Oh, 2011; Zhao et al., 2010; Titov and McDonald, 2008), we 
take the approach in (Brody and Elhadad, 2010; Chen et al., 2013), dividing each review into sentences 
and treating each sentence as an independent document. 
Noun Phrase Detection: Although there are different types of phrases, in this first work we focus 
only on noun phrases as they are more representative of topics in online reviews. We will deal with other 
types of phrases in the future. Our first step is thus to obtain all noun phrases from each domain. Due to 
the efficiency issue of full natural language parser with a huge number of reviews, instead of applying 
the Stanford Parser to recognize noun phrases, we design a rule-based approach to recognize noun 
phrases as consecutive nouns based on POS tags of sentences. Although the Stanford Parser may give 
us better noun phrases, our simple method serves the purpose and gives us very good results. In fact, 
based on our initial experiments, the Stanford Parser also gives many wrong phrases. 
Parameter Settings: In all our experiments, the posterior inference was drawn after 2000 Gibbs 
sampling iterations with a burn-in of 400 iterations. Following (Griffiths and Steyvers, 2004), we fix the 
Dirichlet priors as follows: for all document-topic distributions, we set ?=50/?, where ? is the number 
of topics. And for all topic-term distributions, we set ?=0.1. We also experimented with other settings 
of these priors and did not notice much difference. 
Setting the number of topics/aspects in topic models is often tricky as it is difficult to know the exact 
number of topics that a corpus has. While non-parametric Bayesian approaches (Teh et al., 2005) do 
exist for estimating the number of topics, it?s not the focus of this paper. We empirically set the number 
of topics to 15. Although 15 may not be optimum, since all models use the same number, there is no 
bias against any model. 
671
In Section 3.3, we introduced the promotion concept for the GPU model. When we sample a topic for 
a word, we add virtualcount of topic assignment to all its related phrases. However, not all words in a 
phrase are equally important. For example, in phrase ?hotel staff?, ?staff? is more important, and we call 
such words the head nouns. In this work, we apply a simple method used in (Wang et al., 2007), which 
is to always assume that the last word in a noun phrase is the head noun. Although we are aware of the 
potential harm to our model when we promote a wrong word, we will leave it as our future work. Again, 
because we want to connect phrases with their component words and promote the rank of phrases in 
their topics, we add less virtual counts to individual words. Thus, we add 0.5 * virtualcount to the last 
word in a phrase and add 0.25 * virtualcount to all other words. We set virtualcount = 0.1 in our exper-
iments empirically. 
Based on the discovered topics, we conduct statistical evaluation using topic coherence, human eval-
uation and also a case study to quantitatively and qualitatively show the superiority of the proposed 
method in terms of both interpretability and topic wellness. 
4.1 Statistical Evaluation 
Perplexity and KL-divergence are often used to evaluate topic models statistically. However, researchers 
have found that perplexity on held-out documents is not always a good predictor of human judgments 
of topics (Chang et al., 2009). In our application, we are not concerned with the test on future data using 
the hold-out set. KL-divergence measures the difference of distributions, and thus can be used to meas-
ure the distinctiveness of topics. However, distinctiveness of topics does not necessarily mean human 
agreeable topics. Recently, Mimno et al. (2011) proposed a new measure called topic coherence, which 
has been shown to correlate with human judgments of topic quality quite well. Higher topic coherence 
score indicates higher quality of topics, i.e., better topic coherence. Topic coherence is computed as 
below. 
 
??(?; ?(?)) = ? ? ???
? (??
(?)
, ??
(?)
) + 1
? (??
(?)
)
??1
?=1
?
?=2
 (3) 
in which ?(?) is the document frequency of term ? (i.e., the number of documents with at least one 
term ?) and ?(?, ??) is the co-document frequency of term ? and term ?? (i.e., the number of documents 
containing both term ? and term ??). Also, ?(?) = (?1
(?)
, ? , ??
(?)
) is the list of ? most probable terms 
in topic ?. 1 is added as a smoothing count to avoid taking the logarithm of zero. 
We thus use this measure to score all four experiments. Figure 1 and Figure 2 show the topic coher-
ence using top 15 terms and top 30 terms respectively on the 32 different domains. Notice the topic 
coherence is a negative value, and a smaller absolute value is better than a larger one. Firstly, we can 
see from both charts that our proposed model LDA(p_GPU) is better than all other three baselines by a 
large margin. Secondly, the performance of the other three baselines are quite similar. In general, 
LDA(p) is slightly worse than the other two baselines. It is because replacing many words with phrases 
decreases the number of co-occurrences in the corpus. In contrast, LDA(w_p) is slightly better than the 
other two baselines on most domains because some frequent phrases add more reliable co-occurrences 
in the corpus. However, as we point out in the introduction, some problems still exist. Firstly, it does 
not solve the problem of phrases and their component words having different meanings, and thus artifi-
cially creating such wrong co-occurrences may damage the overall performance. Secondly, even if the 
number of co-occurrences increases, most of the phrases are still too infrequent to be ranked high in 
their associated topics to be useful in helping users understand the topic. 
In order to test the significance of the improvement, we conduct paired t-tests on the topic coherence 
results. Using both 15 top terms and 30 top terms, statistical tests show that our proposed method, 
LDA(p_GPU), outperforms all three baselines significantly (p < 0.01). However, there?s no significant 
improvement between any pair of the three baselines. 
4.2 Manual Evaluation 
Although several statistical measures, such as perplexity, KL-divergence and topic coherence, have been 
used to statistically evaluate topic models, since topic models are mostly (including ours) unsupervised, 
672
statistical measures may not always correlate with human interpretations or judgments. Thus, in this 
sub-section, we perform a manual evaluation through manual labeling of topics and topical terms. 
Manual labeling was done by two annotators, who are familiar with reviews and topic models. The 
labeling was carried out in two stages sequentially: (1) labeling of topics and (2) labeling of topical 
terms in each topic. After the first stage, an annotator agreement is computed and then the two annotators 
discuss about the disagreed topics to reach a consensus. Then, they move on to the next stage to label 
the top ranked topical terms in each topic (based on their probabilities in the topic). For the annotator 
 
Figure 1: Topic coherence of the top 15 terms of each model on each of the 32 datasets. Notice that since topic coherence 
is a negative value, a smaller absolute value is better than a larger one.  
Domain/dataset names are listed as follows (1:Amplifier; 2:BluRayPlayer; 3:Camera; 4:CellPhone; 5:Computer; 
6:DVDPlayer; 7:GPS; 8:HardDrive; 9:Headphone; 10:Keyboard; 11:Kindle; 12:MediaPlayer; 13:Microphone; 14:Monitor; 
15:Mouse; 16:MP3Player; 17:NetworkAdapter; 18:Printer; 19:Projector; 20:RadarDetector; 21:RemoteControl; 22:Scan-
ner; 23:Speaker; 24:Subwoofer; 25:Tablet; 26:TV; 27:VideoPlayer; 28:VideoRecorder; 29:Watch; 30:WirelessRouter; 
31:Hotel; 32:Restaurant). 
 
Figure 2: Topic coherence of the top 30 terms of each model on each dataset. Notice again that since topic coherence is a 
negative value, a smaller absolute value is better than a larger one. X-axis indicates the domain id numbers, whose names 
are listed below Figure 1. 
  
Figure 3: Human evaluation on five domains using top 15 and top 30 terms. X-axis indicates the domain id numbers, whose 
corresponding domain names are listed below Figure 1. Y-axis indicates the ratio of correct topic terms. 
  
 
 
 
-440
-390
-340
-290
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
To
p
ic
 C
o
h
er
en
ce
Top 15 Terms Topic Coherence
LDA(w) LDA(p) LDA(w_p) LDA(p_GPU)
-1750
-1550
-1350
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
To
p
ic
 C
o
h
er
en
ce
Top 30 Terms Topic Coherence
LDA(w) LDA(p) LDA(w_p) LDA(p_GPU)
0.4
0.6
0.8
1
31 32 29 25 16
Top 15 Terms
LDA(w) LDA(p)
LDA(w_p) LDA(p_GPU)
0.3
0.5
0.7
0.9
31 32 29 25 16
Top 30 Terms
LDA(w) LDA(p)
LDA(w_p) LDA(p_GPU)
673
agreement, we compute Kappa scores. The Kappa score for topic labeling is 0.838, and the Kappa score 
for topical terms labeling is 0.846. Both scores indicate strong agreement in the labeling. 
Evaluation measure. A commonly used evaluation measure in human evaluation is precision@n (or 
P@n for short), which is the precision at a particular rank position n in a topic. For example, Preci-
sion@5 means the precision of the top ranked 5 terms for a topic. To be consistent with the automatic 
evaluation, we use Precision@15 and 30. Top 15 terms is usually sufficient to represent the topic. How-
ever, since we include phrases in our experiments which may lead to some other terms ranked lower 
than using only words, we labeled up to top 30 terms. The Precision@n measure is also used in (Zhao 
et al., 2010) and some others, e.g., (Chen et al., 2013). 
In our experiments, we labeled four results for each domain, i.e., those of LDA(w), LDA(p), LDA(w_p) 
and LDA(p_GPU). Due to the large amount of human labeling effort, we only labeled 5 domains. We 
find that it is sometimes hard to figure out what some of the topics are about and whether some terms 
are related to a topic or not, so we give the results to our human evaluators together with the phrases in 
each domain extracted by our rules in order to let them be familiar with the domain vocabulary. The 
human evaluation results are shown in Figure 3. 
Results and Discussions. Again, we conduct paired t-tests on the human evaluation results of top 15 
and 30 terms. Statistical tests show that our proposed method, LDA(p_GPU), outperforms all other three 
methods significantly (p < 0.05) using both top 15 and top 30 terms. However, there?s no significant 
improvement between any pair of the three baselines. 
4.3 Case Study 
In order to illustrate the importance of phrases in enhancing human readability, we conduct case study 
using one topic from each of the five manually labeled domains. Due to space limitations, we only 
compare the results of our model LDA(p_GPU) with LDA(w). 
Table 1: Example topics discovered by LDA(w) and LDA(p_GPU) 
Hotel Restaurant Watch 
LDA(w) LDA(p_GPU) LDA(w) LDA(p_GPU) LDA(w) LDA(p_GPU) 
bed clean service service hand big 
comfortable comfortable star friendly minute hand 
small quiet staff server hour minute 
sleep sleep atmosphere staff beautiful cheap 
size large friendly atmosphere casual hour 
large spacious server waiter christmas automatic 
tv size waiter attentive setting seconds 
pillow king size bed attentive star condition line 
king pillow reason service staff worth hour hand 
chair queen size bed decor star service weight durable 
table bed size quick customer service red analog hand 
mattress bed nd pillow customer table service press hand move 
clean bed sheet waitress delivery service gift hand line 
double bed linen tip rush hour service run seconds hand 
big sofa bed pleasant service attitude functionality hand sweep 
Tablet MP3Player 
LDA(w) LDA(p_GPU) LDA(w) LDA(p_GPU) 
screen screen battery battery 
touch size headphone hour 
software easier life battery life 
hard pro media price 
pad touch screen car worth 
option bigger windows charge 
version area hour replacement 
website inch decent free 
angle screen protector reason market 
car screen size xp aaa battery 
charger inch screen program aa battery 
ipod draw aaa purchase 
worth home screen window hour battery 
gb screen look set aaa 
drive line pair life 
 
 
 
 
 
 
 
 
674
In the above table, we notice that with phrases, the topics are much more interpretable than only 
reading individual words given by LDA(w). For example, ?hand? in ?Watch? domain given by LDA(w) 
is quite confusing at first, but in LDA(p_GPU), ?hour hand? makes it more understandable. Another 
example is ?aaa? in ?MP3Player? domain. It is quite confusing at first, but ?aaa battery? should make it 
more interpretable by an application user who is not familiar with topic models or does not have exten-
sive domain knowledge. Also, due to wrong co-occurrences created by individual words in a phrase, the 
LDA(w) results contain much more noise than those of LDA(p_GPU). 
5 CONCLUSION 
This paper proposed a new method to consider phrases in discovering topics using topic models. The 
method is based on the generalized P?lya urn (GPU) model, which allows us to connect phrases with 
their component words during the inference and rank phrases higher in their related topics. Our method 
preserves the advantages of ?bag-of-words? assumption while preventing the side effects that traditional 
methods have when considering phrases. We tested our method against three baselines across 32 differ-
ent domains, and demonstrated the superiority of our method in improving the topic quality and human 
interpretability both quantitatively and qualitatively. 
 
 
References 
 
David M. Blei and John D. Lafferty. 2009. ?Visualizing Topics with Multi-Word Expressions.? Tech. Report. 
(arXiv:0907.1013). 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. ?Latent Dirichlet Allocation.? Journal of Machine 
Learning Research 993-1022. 
Samuel Brody and Noemie Elhadad. 2010. ?An Unsupervised Aspect-Sentiment Model for Online Reviews.? 
NAACL. Los Angeles, California: ACL. 
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. 2009. ?Reading Tea 
Leaves: How Humans Interpret Topic Models.? Neural Information Processing Systems.  
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013. ?Ex-
ploiting Domain Knowledge in Aspect Extraction.? EMNLP. 
Thomas L. Griffiths, and Mark Steyvers. 2004. ?Finding scientific topics.? Proceedings of National Academy of 
Sciences.  
Gregor Heinrich. 2009. ?A Generic Approach to Topic Models.? ECML PKDD. ACM. Pages 517 - 532. 
Thomas Hofmann. 1999. ?Probabilistic latent semantic analysis.? UAI.  
Yohan Jo and Alice Oh. 2011. ?Aspect and Sentiment Unification Model for Online Review Analysis.? WSDM. 
Hong Kong, China: ACM. 
Chenghua Lin and Yulan He. 2009. ?Joint Sentiment/Topic Model for Sentiment Analysis?. CIKM. Hong Kong, 
China. 
Fangtao Li, Minlie Huang, Xiaoyan Zhu. 2010. ?Sentiment Analysis with Global Topics and Local Dependency?. 
AAAI 
Yue Lu and Chengxiang Zhai. 2008. ?Opinion Integration Through Semi-supervised Topic Modeling.? WWW. 
2008, Beijing, China: ACM. 
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. ?Automatic Keyphrase Extraction via Topic 
Decomposition.? EMNLP.  
Arjun Mukherjee and Bing Liu. 2013. ?Discovering User Interactions in Ideological Discussions.? ACL.  
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and Sharon Meraz. 2013. ?Public Dialogue: Analysis of Tol-
erance in Online Discussions.? ACL.  
David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. ?Optimizing 
Semantic Coherence in Topic Models.? EMNLP. Edinburgh, Scotland, UK: ACL. 
675
Hosan Mahmoud. 2008. Polya Urn Models. Chapman & Hall/CRC Texts in Statistical Science.  
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. ?Topic Sentiment Mixture: 
Modeling Facets and Opinions in Weblogs.? WWW. Banff, Alberta, Canada: ACM. 
Christina Sauper and Regina Barzilay. 2013. ?Automatic Aggregation by Joint Modeling of Aspects and Values?. 
Journal of Artificial Intelligence Research 46 (2013) 89-127 
Ivan Titov and Ryan McDonald. 2008. ?Modeling Online Reviews with Multi-grain Topic Models.? WWW. 2008, 
Beijing, China: ACM. 
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2005. ?Hierarchical Dirichlet Processes.? 
Journal of the American Statistical Association.  
Hanna M. Wallach. 2006. ?Topic Modeling: Beyond Bag-of-Words.? ICML. Pittsburgh, PA: ACM. 
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. ?Topical N-grams: Phrase and Topic Discovery, with an 
Application to Information Retrieval.? ICDM.  
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. ?Jointly Modeling Aspects and Opinions with 
a MaxEnt-LDA Hybrid.? EMNLP. Massachusetts, USA: ACL. 
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li. 2011. 
?Topical Keyphrase Extraction from Twitter.? ACL. 
676
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 207?217,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Gender Classification of Blog Authors 
 
Arjun Mukherjee Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
851 South Morgan Street 
Chicago, IL 60607, USA 
amukherj@cs.uic.edu 
 
Department of Computer Science 
University of Illinois at Chicago 
851 South Morgan Street 
Chicago, IL 60607, USA 
liub@cs.uic.edu 
 
 
 
 
 
 
 
Abstract 
The problem of automatically classifying the 
gender of a blog author has important appli-
cations in many commercial domains. Exist-
ing systems mainly use features such as 
words, word classes, and POS (part-of-
speech) n-grams, for classification learning. 
In this paper, we propose two new techniques 
to improve the current result. The first tech-
nique introduces a new class of features 
which are variable length POS sequence pat-
terns mined from the training data using a se-
quence pattern mining algorithm. The second 
technique is a new feature selection method 
which is based on an ensemble of several fea-
ture selection criteria and approaches. Empir-
ical evaluation using a real-life blog data set 
shows that these two techniques improve the 
classification accuracy of the current state-of-
the-art methods significantly.  
1 Introduction 
Weblogs, commonly known as blogs, refer to on-
line personal diaries which generally contain in-
formal writings. With the rapid growth of blogs, 
their value as an important source of information 
is increasing. A large amount of research work 
has been devoted to blogs in the natural language 
processing (NLP) and other communities. There 
are also many commercial companies that exploit 
information in blogs to provide value-added ser-
vices, e.g., blog search, blog topic tracking, and 
sentiment analysis of people?s opinions on prod-
ucts and services. Gender classification of blog 
authors is one such study, which also has many 
commercial applications. For example, it can help 
the user find what topics or products are most 
talked about by males and females, and what 
products and services are liked or disliked by men 
and women. Knowing this information is crucial 
for market intelligence because the information 
can be exploited in targeted advertising and also 
product development. 
In the past few years, several authors have stu-
died the problem of gender classification in the 
natural language processing and linguistic com-
munities. However, most existing works deal with 
formal writings, e.g., essays of people, the Reuters 
news corpus and the British National Corpus 
(BNC). Blog posts differ from such text in many 
ways. For instance, blog posts are typically short 
and unstructured, and consist of mostly informal 
sentences, which can contain spurious information 
and are full of grammar errors, abbreviations, 
slang words and phrases, and wrong spellings. 
Due to these reasons, gender classification of blog 
posts is a harder problem than gender classifica-
tion of traditional formal text. 
Recent work has also attempted gender classi-
fication of blog authors using features such as 
content words, dictionary based content analysis 
results, POS (part-of-speech) tags and feature se-
lection along with a supervised learning algorithm 
(Schler et al, 2006; Argamon et al, 2007; Yan 
and Yan, 2006). This paper improves these exist-
ing methods by proposing two novel techniques. 
The first technique adds a new class of pattern 
based features to learning, which are not used in 
any existing work. The patterns are frequent se-
quences of POS tags which can capture complex 
stylistic characteristics of male and female au-
thors. We note that these patterns are very differ-
ent from the traditional n-grams because the 
207
patterns are of variable lengths and need to satisfy 
some criteria in order for them to represent signif-
icant regularities. We will discuss them in detail 
in Section 3.5. 
The second technique is a new feature selec-
tion algorithm which uses an ensemble of feature 
selection criteria and methods. It is well known 
that each individual feature selection criterion and 
method can be biased and tends to favor certain 
types of features. A combination of them should 
be able to capture the most useful or discrimina-
tive features. 
Our experimental results based on a real life 
blog data set collected from a large number of 
blog hosting sites show that the two new tech-
niques enable classification algorithms to signifi-
cantly improve the accuracy of the current state-
of-the-art techniques (Argamon et al, 2007; 
Schler et al, 2006; Yan and Yan, 2006). We also 
compare with two publicly available systems, 
Gender Genie (BookBlog, 2007) and Gender 
Guesser (Krawetz, 2006). Both systems imple-
mented variations of the method given in (Arga-
mon et al, 2003). Here, the improvement of our 
techniques is even greater. 
2 Related Work 
There have been several recent papers on gender 
classification of blogs (e.g., Schler et al, 2006, 
Argamon et al, 2007; Yan and Yan, 2006; Now-
son et al, 2005). These systems use func-
tion/content words, POS tag features, word classes 
(Schler et al, 2006), content word classes (Arga-
mon et al, 2007), results of dictionary based con-
tent analysis, POS unigram (Yan and Yan, 2006), 
and personality types (Nowson et al, 2005) to 
capture stylistic behavior of authors? writings for 
classifying gender. (Koppel et al 2002) also used 
POS n-grams together with content words on the 
British National Corpus (BNC). (Houvardas and 
Stamatatos, 2006) even applied character (rather 
than word or tag) n-grams to capture stylistic fea-
tures for authorship classification of news articles 
in Reuters.  
However, these works use only one or a subset 
of the classes of features. None of them uses all 
features for classification learning. Given the 
complexity of blog posts, it makes sense to apply 
all classes of features jointly in order to classify 
genders. Moreover, having many feature classes is 
very useful as they provide features with varied 
granularities and diversities. However, this also 
results in a huge number of features and many of 
them are redundant and may obscure classifica-
tion. Feature selection is thus needed. Following 
the idea, this paper proposes a new ensemble fea-
ture selection method which is capable of extract-
ing good features from different feature classes 
using multiple criteria.  
We also note some less relevant literature. For 
example, (Tannen, 1990) deals with gender differ-
ences in ?conversational style? and in ?formal 
written essays?, and (Gefen and Straub, 1997) 
reports differences in perception of males and fe-
males in the use of emails. 
Our new POS pattern features are related to 
POS n-grams used in (Koppel et al, 2002; Arga-
mon et al, 2007), which considered POS 3-grams, 
2-grams and unigrams as features. As shown in 
(Baayen et. al. 1996), POS n-grams are very ef-
fective in capturing the fine-grained stylistic and 
heavier syntactic information. In this work, we go 
further by finding POS sequence patterns. As dis-
cussed in the introduction, our patterns are entire-
ly different from POS n-grams. First of all, they 
are of variable lengths depending on whatever 
lengths can catch the regularities. They also need 
to satisfy some constraints to ensure that they tru-
ly represent some significant regularity of male or 
female writings. Furthermore, our POS sequence 
patterns can take care of n-grams and capture ad-
ditional sequence regularities. These automatical-
ly mined pattern features are thus more 
discriminating for classification. 
3 Feature Engineering and Mining 
There are different classes of features that have 
been experimented for gender classification, e.g., 
F-measure, stylistic features, gender preferential 
features, factor analysis and word classes (Now-
son et al, 2005; Schler et al, 2006; Corney et al, 
2002; Argamon et al, 2007). We use all these ex-
isting features and also propose a new class of 
features that are POS sequence patterns, which 
replace existing POS n-grams. Also, as mentioned 
before, using all feature classes gives us features 
with varied granularities. Upon extracting all 
these classes of features, a new ensemble feature 
selection (EFS) algorithm is proposed to select a 
subset of good or discriminative features.  
208
Below, we first introduce the existing features, 
and then present the proposed class of new pattern 
based features and how to discover them.  
3.1 F-measure 
The F-measure feature was originally proposed in 
(Heylighen and Dewaele, 2002) and has been used 
in (Nowson et al, 2005) with good results. Note 
that F-measure here is not the F-score or F-
measure used in text classification or information 
retrieval for measuring the classification or re-
trieval effectiveness (or accuracy). 
F-measure explores the notion of implicitness 
of text and is a unitary measure of text?s relative 
contextuality (implicitness), as opposed to its 
formality (explicitness). Contextuality and formal-
ity can be captured by certain parts of speech. A 
lower score of F-measure indicates contextuality, 
marked by greater relative use of pronouns, verbs, 
adverbs, and interjections; a higher score of F-
measure indicates formality, represented by great-
er use of nouns, adjectives, prepositions, and ar-
ticles. F-measure is defined based on the 
frequency of the POS usage in a text (freq.x below 
means the frequency of the part-of-speech x):  
F = 0.5 * [(freq.noun + freq.adj + freq.prep + 
freq.art) ? (freq.pron + freq.verb + 
freq.adv + freq.int) + 100] 
(Heylighen and Dewaele, 2002) applied the F-
measure to a corpus with known author genders 
and found a distinct difference between the sexes. 
Females scored lower preferring a more contex-
tual style while males scored higher preferring a 
more formal style. F-measure values for male and 
female writings reported in (Nowson et al, 2005) 
also demonstrated a similar trend. In our work, we 
also use F-measure as one of the features. 
3.2 Stylistic Features 
These are features which capture people?s writing 
styles. The style of writing is typically captured 
by three types of features: part of speech, words, 
and in the blog context, words such as lol, hmm, 
and smiley that appear with high frequency. In this 
work, we use words and blog words as stylistic 
features. Part of speech features are mined using 
our POS sequence pattern mining algorithm. POS 
n-grams can also be used as features. However, 
since we mine all POS sequence patterns and use 
them as features, most discriminative POS n-
grams are already covered. In Section 5, we will 
also show that POS n-grams do not perform as 
well as our POS sequence patterns. 
3.3 Gender Preferential Features  
Gender preferential features consist of a set of 
signals that has been used in an email gender clas-
sification task (Corney et al, 2002). These fea-
tures come from various studies that have been 
undertaken on the issue of gender and language 
use (Schiffman, 2002). It was suggested by these 
studies and also various other works that women?s 
language makes more frequent use of emotionally 
intensive adverbs and adjectives like ?so?, ?terri-
bly?, ?awfully?, ?dreadfully? and women?s lan-
guage is more punctuated. On the other hand, 
men?s conversational patterns express ?indepen-
dence? (Corney et al, 2002). In brief, the lan-
guage expressed by males is more proactive at 
solving problems while the language used by fe-
males is more reactive to the contribution of oth-
ers - agreeing, understanding and supporting. We 
used the gender preferential features listed in Ta-
ble 1, which indicate adjectives and adverbs based 
on the presence of suffixes and apologies as used 
in (Corney et al, 2002). The feature value as-
signment will be discussed in Section 5. 
f1 words ending with able  
f2 words ending with al  
f3 words ending with ful 
f4 words ending with ible 
f5 words ending with ic  
f6 words ending with ive  
f7 words ending with less  
f8 words ending with ly  
f9 words ending with ous  
f10 sorry words   
Table 1: Gender preferential features 
3.4 Factor Analysis and Word Classes 
Factor or word factor analysis refers to the process 
of finding groups of similar words that tend to 
occur in similar documents. This process is re-
ferred to as meaning extraction in (Chung and 
Pennebaker, 2007). Word lists for twenty factors, 
along with suggested labels/headings (for refer-
ence) were used as features in (Argamon et al, 
2007). Here we list some of those features (word 
209
classes) in Table 2. For the detailed list of such 
word classes, the reader is referred to (Argamon et 
al., 2007). We also used these word classes as fea-
tures in our work. In addition, we added three 
more new word classes implying positive, nega-
tive and emotional connotations and used them as 
features in our experiments. These are listed in 
Table 3. 
Factor Words 
Conversa-
tion 
know, people, think, person, tell, feel, friends, talk, 
new, talking, mean, ask, understand, feelings, care, 
thinking, friend, relationship, realize, question, an-
swer, saying 
Home 
woke, home, sleep, today, eat, tired, wake, watch, 
watched, dinner, ate, bed, day, house, tv, early, bor-
ing, yesterday, watching, sit 
Family 
years, family, mother, children, father, kids, parents, 
old, year, child, son, married, sister, dad, brother, 
moved, age, young, months, three, wife, living, col-
lege, four, high, five, died, six, baby, boy, spend, 
Christmas 
Food / 
Clothes 
food, eating, weight, lunch, water, hair, life, white, 
wearing, color, ice, red, fat, body, black, clothes, 
hot, drink, wear, blue, minutes, shirt, green, coffee, 
total, store, shopping 
Romance forget, forever, remember, gone, true, face, spent, times, love, cry, hurt, wish, loved 
Table 2: Words in factors 
Positive 
absolutely, abundance, ace, active, admirable, adore, 
agree, amazing, appealing, attraction, bargain, beam-
ing, beautiful, best, better, boost, breakthrough, breeze, 
brilliant, brimming, charming, clean, clear, colorful, 
compliment, confidence, cool, courteous, cuddly, daz-
zling, delicious, delightful, dynamic, easy, ecstatic, 
efficient, enhance, enjoy, enormous, excellent, exotic, 
expert, exquisite, flair, free, generous, genius, great, 
graceful, heavenly, ideal, immaculate, impressive, in-
credible, inspire, luxurious, outstanding, royal, speed, 
splendid, spectacular, superb, sweet, sure, supreme, 
terrific, treat, treasure, ultra, unbeatable, ultimate, 
unique, wow, zest 
Negative 
wrong, stupid, bad, evil, dumb, foolish, grotesque, 
harm, fear, horrible, idiot, lame, mean, poor, heinous, 
hideous, deficient, petty, awful, hopeless, fool, risk, 
immoral, risky, spoil, spoiled, malign, vicious, wicked, 
fright, ugly, atrocious, moron, hate, spiteful, meager, 
malicious, lacking 
Emotion 
aggressive, alienated, angry, annoyed, anxious, careful, 
cautious, confused, curious, depressed, determined, 
disappointed, discouraged, disgusted, ecstatic, embar-
rassed, enthusiastic, envious,  excited,  exhausted, 
frightened, frustrated, guilty, happy,  helpless, hopeful, 
hostile, humiliated, hurt, hysterical,  innocent, interest-
ed, jealous, lonely, mischievous,  miserable, optimistic, 
paranoid, peaceful, proud,  puzzled, regretful, relieved, 
sad, satisfied, shocked,  shy, sorry, surprised, suspi-
cious, thoughtful, undecided,  withdrawn 
Table 3: Words implying positive, negative and emo-
tional connotations 
3.5 Proposed POS Sequence Pattern Fea-
tures 
We now present the proposed POS sequence pat-
tern features and the mining algorithm. This re-
sults in a new feature class. A POS sequence 
pattern is a sequence of consecutive POS tags that 
satisfy some constraints (discussed below). We 
used (Tsuruoka and Tsujii, 2005) as our POS tag-
ger. 
As shown in (Baayen et. al., 1996), POS n-
grams are good at capturing the heavy stylistic 
and syntactic information. Instead of using all 
such n-grams, we want to discover all those pat-
terns that represent true regularities, and we also 
want to have flexible lengths (not fixed lengths as 
in n-grams). POS sequence patterns serve these 
purposes. Its mining algorithm mines all such pat-
terns that satisfy the user-specified minimum sup-
port (minsup) and minimum adherence 
(minadherence) thresholds or constraints. These 
thresholds ensure that the mined patterns represent 
significant regularities.   
The main idea of the algorithm is to perform a 
level-wise search for such patterns, which are 
POS sequences with minsup and minadherence. 
The support of a pattern is simply the proportion 
of documents that contain the pattern. If a pattern 
appears too few times, it is probably spurious. A 
sequence is called a frequent sequence if it satis-
fies minsup. The adherence of a pattern is meas-
ured using the symmetrical conditional 
probability (SCP) given in (Silva et al, 1999). 
The SCP of a sequence with two elements |xy| is 
the product of the conditional probability of each 
given the other, 
)()(
),(
)|()|(),(
2
yPxP
yxP
xyPyxPyxSCP ==  
Given a consecutive sequence of POS tags 
|x1?xn|, called a POS sequence of length n, a dis-
persion point defines two subparts of the se-
quence. A sequence of length n contains n-1 
possible dispersion points. The SCP of the se-
quence |x1?xn| given the dispersion point (denoted 
by *) |x1?xn-1*xn| is: 
)()...(
)...(
)),...((
11
2
1
11
nn
n
nn xPxxP
xxP
xxxSCP
?
? =  
The SCP measure can be extended so that all 
possible dispersion points are accounted for. 
210
Hence the fairSCP of the sequence |x1?xn| is giv-
en by: 
??
=
+?
=
1
1
11
2
1
1
)...()...(
1
1
)...(
)...(
n
i
nii
n
n
xxPxxP
n
xxP
xxfairSCP  
fairSCP measures the adherence strength of POS 
tags in a sequence. The higher the fairSCP value, 
the more dominant is the sequence. Our POS se-
quence pattern mining algorithm is given below. 
Input: Corpus D = {d | d is a document containing a 
sequence of POS tags}, Tagset T = {t | t is a POS 
tag}, and the user specified minimum support (min-
sup) and minimum adherence (minadherence). 
Output: All POS sequence patterns (stored in SP) 
mined from D that satisfy minsup and minadhe-
rence.  
Algorithm mine-POS-pats(D, T, minsup, minadhe-
rence) 
1.  C1 ? count each t (? T) in D;  
2.  F1 ? {f | f ? C1 , f .count / n ? minsup};    // n = |D| 
3. SP1 ? F1; 
4.  for (k = 2; k ? MAX-length; k++) 
5. Ck = candidate-gen(Fk-1); 
6. for each document d ? D 
7. for each candidate POS sequence c ? Ck 
8. if (c is contained in d) 
9. c.count++; 
10. endfor 
11.    endfor 
12.   Fk ? {c ? Ck | c.count / n ? minsup}; 
13 SPk ? {f ? Fk | fairSCP(f) ? minadherence} 
14.  endfor 
15.  return SP ? U
k
kSP ; 
Function candidate-gen(Fk-1) 
1.   Ck ? ?;  
2.   for each POS n-gram c ? Fk-1 
3.      for each t ? T 
4.         c?? addsuffix(c, t);  // adds tag t to c as suffix 
5.         add c?  to Ck ; 
6.      endfor 
7.   endfor 
We now briefly explain the mine-POS-pats algo-
rithm. The algorithm is based on level-wise 
search. It generates all POS patterns by making 
multiple passes over data. In the first pass, it 
counts the support of individual POS tags and de-
termines which of them have minsup (line 2). 
Multiple occurrences of a tag in a document are 
counted only once. Those in F1 are called length 1 
frequent sequences. All length 1 sequence patterns 
are stored in SP1. Since adherence is not defined 
for a single element, we have SP1 = F1 (line 3). In 
each subsequent pass k until MAX-length (which 
is the maximum length limit of the mined pat-
terns), there are three steps: 
1.  Using Fk-1 (frequent sequences found in the (k-
1) pass) as a set of seeds, the algorithm applies 
candidate-gen() to generate all possibly fre-
quent POS k-sequences (sequences of length k) 
(line 5). Those infrequent sequences (which are 
not in Fk-1) are discarded as adding more POS 
tags will not make them frequent based on the 
downward closure property in (Agrawal and 
Srikant, 1994). 
2.  D is then scanned to compute the actual sup-
port count of each candidate in Ck (lines 6-11).  
3.  At the end of each scan, it determines which 
candidate sequences have minsup and minad-
herence (lines 12 - 13). We compute Fk and SPk 
separately because adherence does not have the 
downward closure property as the support.   
Finally, the algorithm returns the set of all se-
quence patterns (line 15) that meet the minsup and 
minadherence thresholds.  
The candidate-gen() function generates all pos-
sibly frequent k-sequences by adding each POS 
tag t to c as suffix. c is a k-1-sequence in Fk-1.  
In our experiments, we used MAX-length = 7, 
minsup = 30%, and minadherence = 20% to mine 
all POS sequence patterns. All the mined patterns 
are used as features.   
Finally, it is worthwhile to note that mine-
POS-pat is very similar to the well-known GSP 
algorithm (Srikant and Agrawal, 1996). Likewise, 
it has linear scale up with data size. If needed, one 
can use MapReduce (Dean and Ghemawat, 2004) 
with suitable modifications in mine-POS-pats to 
speed things up by distributing to multiple ma-
chines for large corpora. Moreover, mining is a 
part of preprocessing of the algorithm and its 
complexity does not affect the final prediction, as 
it will be later shown that for model building and 
prediction, standard machine learning methods are 
used. 
4 Ensemble Feature Selection 
Since all classes of features discussed in Section 3 
are useful, we want to employ all of them. This 
results in a huge number of features. Many of 
211
them are redundant and even harmful. Feature 
selection thus becomes important. There are two 
common approaches to feature selection: the filter 
and the wrapper approaches (Blum and Langley, 
1997; Kohavi and John, 1997). In the filter ap-
proach, features are first ranked based on a feature 
selection criterion such as information gain, chi-
square (?2) test, and mutual information. A set of 
top ranked features are selected. On the contrary, 
the wrapper model chooses features and adds to 
the current feature pool based on whether the new 
features improve the classification accuracy.  
Both these approaches have drawbacks. While 
the wrapper approach becomes very time consum-
ing and impractical when the number of features 
is large as each feature is tested by building a new 
classifier. The filter approach often uses only one 
feature selection criterion (e.g., information gain, 
chi-square, or mutual information). Due to the 
bias of each criterion, using only a single one may 
result in missing out some good features which 
can rank high based on another criterion. In this 
work, we developed a novel feature selection me-
thod that uses multiple criteria, and combines both 
the wrapper and the filter approaches. Our method 
is called ensemble feature selection (EFS). 
4.1 EFS Algorithm 
EFS takes the best of both worlds. It first uses a 
number of feature selection criteria to rank the 
features following the filter model. Upon ranking, 
the algorithm generates some candidate feature 
subsets which are used to find the final feature set 
based on classification accuracy using the wrapper 
model. Since our framework generates much few-
er candidate feature subsets than the total number 
of features, using wrapper model with candidate 
feature sets is scalable. Also, since the algorithm 
generates candidate feature sets using multiple 
criteria and all feature classes jointly, it is able to 
capture most of those features which are discrimi-
nating. We now detail our EFS algorithm. 
The algorithm takes as input, a set of n features 
F = {f1, ?, fn}, a set of t feature selection criteria 
? = {?1, ?, ?t}, a set of t thresholds ? = {?1, ?, 
?t} corresponding to the criteria in ?, and a win-
dow w. ?i is the base number of features to be se-
lected for criterion ?i. w is used to vary ?i (thus the 
number of features) to be used by the wrapper 
approach. 
Algorithm: EFS (F, ?, ?, w) 
1. for each ?i ? ? 
2. Rank all features in F based on criterion ?i and 
let ?i denotes the ranked features  
3. endfor 
4. for i = 1 to t 
5. Ci ? ? 
6. for ? = ?i ? w to ? = ?i + w 
7. select first ? features ?i from ?i and add ?i to Ci 
in order 
8. endfor 
9. endfor 
10. // Ci = {?1,  ?, ?2w + 1}, where ?i is a set of fea-
tures 
11. OptCandFeatures ? ?; 
12. Repeat steps 13 ? 18 
13. ? ? ? 
14. for i = 1 to t 
15. select and remove the first feature set ?i ? Ci 
from Ci in order 
16. ? ? ? ? ?i  
17. endfor 
18. add ? to OptCandFeatures  
19. // ? is a set of features comprising of features in 
// feature sets ?i ? Ci in the same position ? i 
20. until Ci = ? ? i 
21. for each ? ? OptCandFeatures 
22. ?.score ? accuracy of 10-fold CV on training 
data on a chosen classifier (learning algo-
rithm) 
23. endfor 
24. return 
score.
maxarg
?
{ ? | ? ? OptCandFeatures} 
We now explain our EFS algorithm. Using a set of 
different feature selection measures, ?, we rank 
all features in our feature pool, F, using the set of 
criteria (lines 1?3). This is similar to the filter ap-
proach. In lines 4?9, we generate feature sets Ci, 1 
? i ? t for each of the t criteria. Each set Ci con-
tains feature subsets, and each subset ?i is the set of 
top ? features in ?i ranked based on criterion ?i in 
lines 1?2. ? varies from ?i ? w to ?i + w where ?i is 
the threshold for criterion ?i and w the window 
size. We vary ? and generate 2w + 1 feature sets 
and add all such feature sets ?i to Ci (in lines 6?8) 
in order. We do so because it is difficult to know 
the optimal threshold ?i for each criterion ?i. It 
should be noted that ?adding in order? ensures the 
ordering of feature sets ?i  as shown in line 10, 
which will be later used to ?select and remove in 
order? in line 15. In lines 11?20 we generate can-
didate feature sets using Ci and add each such 
212
candidate feature set ? to OptCandFeatures. Each 
candidate feature set ? is a collection of top 
ranked features based on multiple criteria. It is 
generated by unioning the features in the first fea-
ture subset ?i, which is then removed from Ci for 
each criterion ?i (lines 14-17). Each candidate fea-
ture set is added to OptCandFeatures in line 18. 
Since each Ci has 2w+1 feature subsets ?i, there 
are a total of 2w+1 candidate feature sets ? in 
OptCandFeatures. Lines 21?23 assign an accuracy 
to each candidate feature set ? ? OptCandFeatures 
by running 10-fold cross validation on the training 
data using a chosen classifier with the features in 
?. Finally, the optimal feature set ? ? OptCand-
Features is returned in line 24. 
An interesting question arising in the EFS al-
gorithm is: How does one select the threshold ?i 
for each criterion ?i and the window size w? Intui-
tively, suppose that for criterion ?i, the optimal 
subset of features is Sopt_i based on some optimal 
threshold ?i. Then the final feature set is a collec-
tion of all features f ? Sopt_i ? i. However, finding 
such optimal feature set Sopt_i or optimal threshold 
?i is a difficult problem. To counter this, we use 
the window w to select various feature subsets 
close to the top ?i features in ?i. Thus, the thre-
shold values ?i and window size w should be ap-
proximated by experiments. In our experiments, 
we used ?i = top 1/20th of the features ranked in ?i 
for ? i and window size w = |F|/100, and got good 
results. Fortunately, as we will see in Section 6.2, 
these parameters are not sensitive at all, and any 
reasonably large size feature set seems to work 
equally well.  
Finally, we are aware that there are some exist-
ing ensemble feature selection methods in the ma-
chine learning literature (Gargant? et al, 2007; Tuv 
et al, 2009). However, they are very different 
from our approach. They mainly use ensemble 
classification methods to help choose good fea-
tures rather than combining different feature se-
lection criteria and integrating different feature 
selection approaches as in our method.   
4.2 Feature Selection Criteria 
The set of feature selection criteria ? = {?1??t} 
used in our work are those commonly used indi-
vidual selection criteria in the filter approach.  
 Let C ={c1, c2, ?, cm} denotes the set of 
classes, and F = {f1, f2, ?, fn} the set of features. 
We list the criteria in ? used in our work below. 
Information Gain (IG): This is perhaps the most 
commonly used criterion, which is based on en-
tropy. The scoring function for information gain 
of a feature f is given by: 
? ??
==
+?=
ff
m
i
iii
m
i
i fcPfcPfPcPcPfIG
, 11
)|(log)|()()(log)()(
Mutual Information (MI): This metric is com-
monly used in statistical language modeling. The 
mutual information MI(f, c) between a class c and 
a feature f is defined as: 
??=
ff cc cPfP
cfP
cfPcfMI
, , )()(
),(
log),(),(  
The scoring function generally used as the crite-
rion is the max among all classes. MI(f) = maxi 
{MI (f, ci)} (which we use). The weighted average 
over all classes can also be applied as the scoring 
function. 
?2 Statistic: The ?2 statistic measures the lack of 
independence between a feature f and class c, and 
can be compared to the ?2 distribution with one 
degree of freedom. We use a 2x2 contingency ta-
ble of a feature f and a class c to introduce ?2 test. 
 c c  
f W X 
f  Y Z 
Table 4: Two-way contingency table of f and c 
In the table, W denotes the number of documents 
in the corpus in which feature f and class c co-
occur, X  the number of documents in which f oc-
curs without c, Y the number of documents in 
which c occurs without f, and Z the number of 
documents in which neither c nor f occurs. Thus, 
N = W + X + Y + Z is the total number of docu-
ments in the corpus. 
?2 test is defined as: 
))()()((
)(
),(
2
2
ZYXWZXYW
YXWZN
cf ++++
?=?  
The scoring function using the ?2 statistic is either 
the weighted average or max over all classes. In 
our experiments, we use the weighted average: 
?2(f) = ?=mi ii cfcP1 2 ),()( ?  
Cross Entropy (CE): This metric is similar to 
mutual information (Mladenic and Grobelnik, 
213
1998): 
?
=
=
m
i
i
i fP
fcP
fcPfPfCE
1 )(
)|(
log)|()()(  
Weight of Evidence for Text (WET): This crite-
rion is based on the average absolute weight of 
evidence (Mladenic and Grobelnik, 1998): 
|
))|(1)((
))(1)(|(
log|)()()(
1 fcPcP
cPfcP
fPcPfWET
ii
ii
m
i
i ?
?=?
=
 
5 Feature Value Assignments 
After selecting features belonging to different 
classes, values are assigned differently to different 
classes of features. There are three common ways 
of feature value assignments: Boolean, TF (Term 
Frequency) and TF-IDF (product of term and in-
verted document frequency). For details of feature 
value assignments, interested readers are referred 
to (Joachims, 1997). While the Boolean scheme 
assigns a 1 to the feature value if the feature is 
present in the document and a 0 otherwise, the TF 
scheme assigns the relative frequency of the num-
ber of times that the feature occurs in the docu-
ment. We did not use TF-IDF as it did not yield 
good results in our preliminary experiments.  
The feature value assignment to different 
classes of features is done as follows: The value 
of F-measure was assigned based on its actual 
value. Stylistic features such words, and blog 
words were assigned values 1 or 0 in the Boolean 
scheme and the relative frequency in the TF 
scheme (we experimented with both schemes). 
Feature values for gender preferential features 
were also assigned in a similar way. Factor and 
word class features were assigned values accord-
ing to the Boolean or TF scheme if any of the 
words belonging to the feature class exists (factor 
or word class appeared in that document). Each 
POS sequence pattern feature was assigned a val-
ue according to the Boolean (or TF) scheme based 
on the appearances of the pattern in the POS 
tagged document. 
6 Experimental Results 
This section evaluates the proposed techniques 
and sees how they affect the classification accura-
cy. We also compare with the existing state-of-
the-art algorithms and systems. For algorithms, 
we compared with three representatives in (Arga-
mon et al, 2007), (Schler et al, 2006) and (Yan 
and Yan, 2006). Since they do not have publicly 
available systems, we implemented them. Each of 
them just uses a subset of the features used in our 
system. Recall our system includes all their fea-
tures and our own POS pattern based features. For 
systems, we compared with two public domain 
systems, Gender Genie (BookBlog, 2007) and 
Gender Guesser (Krawetz, 2006), which imple-
mented variations of the algorithm in (Argamon 
et. al, 2003).  
We used SVM classification, SVM regression, 
and Na?ve Bayes (NB) as learning algorithms. 
Although SVM regression is not designed for 
classification, it can be applied based on the out-
put of positive or negative values. It actually 
worked better than SVM classification for our 
data. For SVM classification and regression, we 
used SVMLight (Joachims, 1999), and for NB we 
used (Borgelt, 2003). In all our experiments, we 
used accuracy as the evaluation measure as the 
two classes (male and female) are roughly ba-
lanced (see the data description below), and both 
classes are equally important.  
6.1 Blog Data Set  
To keep the problem of gender classification of 
informal text as general as possible, we collected 
blog posts from many blog hosting sites and blog 
search engines, e.g., blogger.com, technorati.com, 
etc. The data set consists of 3100 blogs. Each blog 
is labeled with the gender of its author. The gend-
er of the author was determined by visiting the 
profile of the author. Profile pictures or avatars 
associated with the profile were also helpful in 
confirming the gender especially when the gender 
information was not available explicitly. To en-
sure quality of the labels, one group of students 
collected the blogs and did the initial labeling, and 
the other group double-checked the labels by visit-
ing the actual blog pages. Out of 3100 posts, 1588 
(51.2%) were written by men and 1512 (48.8%) 
were written by women. The average post length 
is 250 words for men and 330 words for women.  
6.2 Results  
We used all features from different feature classes 
(Section 3) along with our POS patterns as our 
214
pool of features. We used ? and w values stated in 
Section 4.1 and criteria mentioned in Section 4.2 
for our EFS algorithm. EFS was compared with 
three commonly used feature selection methods 
on SVM classification (denoted by SVM), SVM 
regression (denoted by SVM_R) and the NB clas-
sifier. The results are shown in Table 5. All results 
were obtained through 10-fold cross validation. 
Also, the total number of features selected by 
IG, MI, ?2, and EFS were roughly the same. Thus, 
the improvement in accuracy brought forth by 
EFS was chiefly due to the combination of fea-
tures selected (based on multi-criteria). 
To measure the accuracy improvement of using 
our POS patterns over common POS n-grams, we 
also compared our results with those from POS n-
grams (Koppel et al, 2002). The comparison re-
sults are given in Table 6. Table 6 also includes 
results to show the overall improvement in accu-
racy with our two new techniques. We tested our 
system without any feature selection and without 
using the POS sequence patterns as features. 
The comparison results with existing algo-
rithms and public domain systems using our real-
life blog data set are tabulated in Table 7. 
Also, to see whether feature selection helps and 
how many features are optimal, we varied ? and w 
of the EFS algorithm and plotted the accuracy vs. 
no. of features. These results are shown in Figure 
1. 
Feature  
Selection  
Value 
Assignment NB SVM SVM_R 
IG Boolean 71.32 76.61 78.32 
IG TF 66.01 72.84 74.13 
MI  Boolean 72.01 78.62 79.48 
MI TF 70.86 73.14 74.58 
?2 Boolean 72.90 80.71 81.52 
?2 TF 71.84 73.57 75.24 
EFS Boolean 73.57 86.24 88.56 
EFS TF 72.82 82.05 83.53 
Table 5: Accuracies of SVM, SVM_R and NB with 
different feature selection methods 
 
Settings NB SVM SVM_R
All features 63.01 68.84 70.03 
All features, no POS patterns 60.73 65.17 66.17 
POS 1,2,3-grams + EFS 71.24 82.71 83.86 
POS Patterns + EFS 73.57 86.24 88.56 
Table 6: Accuracies of POS n-grams and POS patterns 
with or without EFS (Boolean value assignment) 
 
System Accuracy (%)
Gender Genie 61.69 
Gender Guesser 63.78 
(Argamon et al, 2007) 77.86 
(Schler et al, 2006) 79.63 
(Yan and Yan, 2006) 68.75 
Our method 88.56 
Table 7: Accuracy comparison with other systems 
50
60
70
80
90
100
25 12
8
21
0
35
0
18
07
64
68
23
97
4
26
02
9
No. of features
A
cc
ur
ac
y
SVM Classification with EFS
SVM Regression with EFS
Na?ve Bayes with EFS
Figure 1: Accuracy vs. no. of features using EFS 
6.3 Observations and Discussions  
Based on the results given in the previous section, 
we make the following observations:  
? SVM regression (SVM_R) performs the best 
(Table 5). SVM classification (SVM) also 
gives good accuracies. NB did not do so well.  
? Table 5 also shows that our EFS feature selec-
tion method brings about 6-10% improvement 
in accuracy over the other feature selection me-
thods based on SVM classification and SVM 
regression. The reason has been explained in 
the introduction section. Paired t-tests showed 
that all the improvements are statistically sig-
nificant at the confidence level of 95%. For 
NB, the benefit is less (3%).   
? Keeping all other parameters constant, Table 5 
also shows that Boolean feature values yielded 
better results than the TF scheme across all 
classifiers and feature selection methods.  
? Row 1 of Table 6 tells us that feature selection 
is very useful. Without feature selection (All 
features), SVM regression only achieves 70% 
accuracy, which is way inferior to the 88.56% 
accuracy obtained using EFS feature selection. 
Row 2 shows that without EFS and without 
POS sequence patterns, the results are even 
worse.  
215
? Keeping all other parameters intact, Table 6 
also demonstrated the effectiveness of our POS 
pattern features over POS n-grams. We have 
discussed the reason in Section 3.2 and 3.5.  
? From Tables 5 and 6, we can infer that the 
overall accuracy improvement using EFS and 
all feature classes described in Section 3 is 
about 15% for SVM classification and regres-
sion and 10% for NB. Also, using POS se-
quence patterns with EFS brings about a 5% 
improvement over POS n-grams (Table 6). The 
improvement is more pronounced for SVM 
based methods than NB. 
? Table 7 summarizes the accuracy improvement 
brought by our proposed techniques over the 
existing state-of-art systems. Our techniques 
have resulted in substantial (around 9%) accu-
racy improvement over the best of the existing 
systems. Note that (Argamon et al, 2007) used 
Logistic Regression with word classes and 
POS unigrams as features. (Schler et al, 2006) 
used Winnow classifier with function words, 
content word classes, and POS features. (Yan 
and Yan, 2006) used Naive Bayes with content 
words and blog-words as features. For all these 
systems, we used their features and ran their 
original classifiers and also the three classifiers 
in this paper and report the best results.  For 
example, for (Argamon et al, 2007), we ran 
Logistic Regression and our three methods. 
SVM based methods always gave slightly bet-
ter results. We could not run Winnow due to 
some technical issues. SVM and SVM_R gave 
comparable results to those given in their orig-
inal papers. These results again show that our 
techniques are useful. All the gains are statisti-
cally significant at the confidence level of 
95%. 
? From Figure 1, we see that when the number of 
features selected is small (<100) the classifica-
tion accuracy is lower than that obtained by us-
ing all features (no feature selection). 
However, the accuracy increases rapidly as the 
number of selected features increases. After 
obtaining the best case accuracy, it roughly 
maintains the accuracy over a long range. The 
accuracies then gradually decrease with the in-
crease in the number of features. This trend is 
consistent with the prior findings in (Mladenic, 
1998; Rogati and Yang, 2002; Forman 2003; 
Riloff et al, 2006; Houvardas and Stamatatos, 
2006).  
It is important to note here that over a long 
range of 2000 to 20000 features, the accuracy 
is high and stable. This means that the thre-
sholds of EFS are easy to set. As long as they 
are in the range, the accuracy will be good. 
Finally, we would like to mention that (Herring 
and Paolillo, 06) has used genre relationships with 
gender classification. Their finding that subgenre 
?diary? contains more ?female? and subgenre ?fil-
ter? having more ?male? stylistic features inde-
pendent of the author gender, may obscure gender 
classification as there are many factors to be con-
sidered. Herring and Paolillo referred only words 
as features which are not as fine grained as our 
POS sequence patterns. We are also aware of oth-
er factors influencing gender classification like 
genre, age and ethnicity. However, much of such 
information is hard to obtain reliably in blogs. 
They definitely warren some future studies. Also, 
EFS being a useful method for feature selection in 
machine learning, it would be useful to perform 
further experiments to investigate how well it per-
forms on a variety of classification datasets. This 
again will be an interesting future work. 
7  Conclusions 
This paper studied the problem of gender classifi-
cation. Although there have been several existing 
papers studying the problem, the current accuracy 
is still far from ideal. In this work, we followed 
the supervised approach and proposed two novel 
techniques to improve the current state-of-the-art. 
In particular, we proposed a new class of features 
which are POS sequence patterns that are able to 
capture complex stylistic regularities of male and 
female authors. Since there are a large number 
features that have been considered, it is important 
to find a subset of features that have positive ef-
fects on the classification task. Here, we proposed 
an ensemble feature selection method which takes 
advantage of many different types of feature se-
lection criteria in feature selection. Experimental 
results based on a real-life blog data set demon-
strated the effectiveness of the proposed tech-
niques. They help achieve significantly higher 
accuracy than the current state-of-the-art tech-
niques and systems.  
216
References 
Agrawal, R. and Srikant, R. 1994. Fast Algorithms for 
Mining Association Rules. VLDB. pp. 487-499. 
Argamon, S., Koppel, M., J Fine, AR Shimoni. 2003. 
Gender, genre, and writing style in formal written 
texts. Text-Interdisciplinary Journal, 2003. 
Argamon, S., Koppel, M., Pennebaker, J. W., Schler, J. 
2007. Mining the Blogosphere: Age, Gender and 
the varieties of self-expression, First Monday, 2007 
- firstmonday.org 
Baayen, H., H van Halteren, F Tweedie. 1996. Outside 
the cave of shadows: Using syntactic annotation to 
enhance authorship attribution, Literary and Lin-
guistic Computing, 11, 1996. 
Blum, A. and Langley, P. 1997. Selection of relevant 
features and examples in machine learning. Artifi-
cial Intelligence, 97(1-2):245-271. 
BookBlog, Gender Genie, Copyright 2003-2007, 
http://www.bookblog.net/gender/genie.html 
Borgelt, C. 2003. Bayes Classifier Induction. 
http://www.borgelt.net/doc/bayes/bayes.html 
Chung, C. K. and Pennebaker, J. W. 2007. Revealing 
people?s thinking in natural language: Using an au-
tomated meaning extraction method in open?ended 
self?descriptions, J. of Research in Personality. 
Corney, M., Vel, O., Anderson, A., Mohay, G. 2002. 
Gender Preferential Text Mining of E-mail Dis-
course. 18th annual Computer Security Applica-
tions Conference (ACSAC), 2002. 
J. Dean and S. Ghemawat. 2004. Mapreduce: Simpli-
fied data processing on large clusters, Operating 
Systems Design and Implementation, 2004. 
Forman, G., 2003. An extensive empirical study of fea-
ture selection metrics for text classification. JMLR, 
3:1289 - 1306 , 2003. 
Gargant?, R. A., Marchiori, T. E., and Kowalczyk, S. 
R. W., 2007. A Genetic Algorithm to Ensemble Fea-
ture Selection. Masters Thesis. Vrije Universiteit, 
Amsterdam. 
Gefen, D., D. W. Straub. 1997. Gender differences in 
the perception and use of e-mail: An extension to 
the technology acceptance model. MIS Quart. 21(4) 
389?400. 
Herring, S. C., & Paolillo, J. C. 2006. Gender and ge-
nre variation in weblogs, Journal of Sociolinguis-
tics, 10 (4), 439-459. 
Heylighen, F., and Dewaele, J. 2002. Variation in the 
contextuality of language: an empirical measure. 
Foundations of Science, 7, 293?340. 
Houvardas, J. and Stamatatos, E. 2006. N-gram Fea-
ture Selection for Authorship Identification, Proc. of 
the 12th Int. Conf. on Artificial Intelligence: Me-
thodology, Systems, Applications, pp. 77-86. 
Joachims, T. 1999. Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and 
A. Smola (ed.), MIT-Press, 1999. 
Joachims, T. 1997. Text categorization with support 
vector machines, Technical report, LS VIII Number 
23, University of Dortmund, 1997 
Kohavi, R. and John, G. 1997. Wrappers for feature 
subset selection. Artificial Intelligence, 97(1-
2):273-324. 
Koppel, M., Argamon, S., Shimoni, A. R.. 2002. Auto-
matically Categorizing Written Text by Author 
Gender. Literary and Linguistic Computing. 
Krawetz, N. 2006. Gender Guesser. Hacker Factor 
Solutions. http://www.hackerfactor.com/ Gender-
Guesser.html 
Mladenic, D. 1998. Feature subset selection in text 
learning. In Proc. of ECML-98, pp. 95?100. 
Mladenic, D. and Grobelnik, D.1998. Feature selection 
for classification based on text hierarchy. Proceed-
ings of the Workshop on Learning from Text and 
the Web, 1998 
Nowson, S., Oberlander J., Gill, A. J., 2005. Gender, 
Genres, and Individual Differences. In Proceedings 
of the 27th annual meeting of the Cognitive Science 
Society (p. 1666?1671). Stresa, Italy. 
Riloff, E., Patwardhan, S., Wiebe, J.. 2006. Feature 
Subsumption for opinion Analysis. EMNLP,  
Rogati, M. and Yang, Y.2002. High performing and 
scalable feature selection for text classification. In 
CIKM, pp. 659-661, 2002. 
Schiffman, H. 2002. Bibliography of Gender and Lan-
guage. http://ccat.sas.upenn.edu/~haroldfs/ pop-
cult/bibliogs/gender/genbib.htm 
Schler, J., Koppel, M., Argamon, S, and Pennebaker J. 
2006. Effects of age and gender on blogging, In 
Proc. of the AAAI Spring Symposium Computa-
tional Approaches to Analyzing Weblogs. 
Silva, J., Dias, F., Guillore, S., Lopes, G. 1999. Using 
LocalMaxs Algortihm for the Extraction of Conti-
guous and Noncontiguous Multiword Lexical Units. 
Springer Lecture Notes in AI 1695, 1999 
Srikant, R. and Agrawal, R. 1996. Mining sequential 
patterns: Generalizations and performance im-
provements, In Proc. 5th Int. Conf. Extending Data-
base Technology (EDBT?96), Avignon, France. 
Tannen, D. (1990). You just don?t understand, New 
York: Ballantine. 
Tsuruoka, Y. and Tsujii, J. 2005. Bidirectional Infe-
rence with the Easiest-First Strategy for Tagging 
Sequence Data, HLT/EMNLP 2005, pp. 467-474. 
Tuv, E., Borisov, A., Runger, G., and Torkkola, K. 
2009. Feature selection with ensembles, artificial 
variables, and redundancy elimination. JMLR, 10. 
Yan, X., Yan, L. 2006. Gender Classification of Web-
log Authors. Computational Approaches to Analyz-
ing Weblogs, AAAI. 
217
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 218?228,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Negative Training Data can be Harmful to Text Classification 
 
 
Xiao-Li  Li Bing Liu See-Kiong  Ng 
Institute for Infocomm Research University of Illinois at Chicago Institute for Infocomm Research 
1 Fusionopolis Way #21-01,  
Connexis Singapore 138632 
851 South Morgan Street,  
Chicago, IL 60607-7053, USA 
1 Fusionopolis Way #21-01,  
Connexis Singapore 138632 
xlli@i2r.a-star.edu.sg liub@cs.uic.edu skng@i2r.a-star.edu.sg
 
 
Abstract 
This paper studies the effects of training data 
on binary text classification and postulates 
that negative training data is not needed and 
may even be harmful for the task. Traditional 
binary classification involves building a clas-
sifier using labeled positive and negative 
training examples. The classifier is then ap-
plied to classify test instances into positive 
and negative classes. A fundamental assump-
tion is that the training and test data are iden-
tically distributed. However, this assumption 
may not hold in practice. In this paper, we 
study a particular problem where the positive 
data is identically distributed but the negative 
data may or may not be so. Many practical 
text classification and retrieval applications fit 
this model. We argue that in this setting nega-
tive training data should not be used, and that 
PU learning can be employed to solve the 
problem. Empirical evaluation has been con-
ducted to support our claim. This result is im-
portant as it may fundamentally change the 
current binary classification paradigm.  
1 Introduction 
Text classification is a well-studied problem in 
machine learning, natural language processing, and 
information retrieval. To build a text classifier, a 
set of training documents is first labeled with pre-
defined classes. Then, a supervised machine learn-
ing algorithm (e.g., Support Vector Machines 
(SVM), na?ve Bayesian classifier (NB)) is applied 
to the training examples to build a classifier that is 
subsequently employed to assign class labels to the 
instances in the test set. In this paper, we focus on 
binary text classification with two classes (i.e. pos-
itive and negative classes).  
Most learning methods assume that the training 
and test data have identical distributions. However, 
this assumption may not hold in practice, i.e., the 
training and the test distributions can be different. 
The problem is called covariate shift or sample 
selection bias (Heckman 1979; Shimodaira 2000; 
Zadrozny 2004; Huang et al 2007; Sugiyama et al 
2008; Bickel et al 2009). In general, this problem 
is not solvable because the two distributions can be 
arbitrarily far apart from each other. Various as-
sumptions were made to solve special cases of the 
problem. One main assumption was that the condi-
tional distribution of the class given an instance is 
the same over the training and test sets (Shimodai-
ra 2000; Huang et al 2007; Bickel et al 2009).  
In this paper, we study another special case of 
the problem in which the positive training and test 
samples have identical distributions, but the nega-
tive training and test samples may have different 
distributions. We believe this scenario is more ap-
plicable for binary text classification. As the focus 
in many applications is on identifying positive in-
stances correctly, it is important that the positive 
training and the positive test data have the same 
distribution. The distributions of the negative train-
ing and negative test data can be different. We be-
lieve that this special case of the sample selection 
bias problem is also more applicable for machine 
learning. We will show that a partially supervised 
learning model, called PU learning (learning from 
Positive and Unlabeled examples) fits this special 
case quite well (Liu et al 2002).  
Following the notations in (Bickel et al 2009), 
our special case of the sample selection bias prob-
lem can be formulated as follows: We are given a 
training sample matrix XL with row vectors x1, ?, 
xk. The positive and negative training instances are 
governed by different unknown distributions p(x|?) 
218
and p(x|?) respectively. The element yi of vector y 
= (y1,  y2, ?, yk) is the class label for training in-
stance xi (yi ?{+1, -1}, where +1 and -1 denote 
positive and negative classes respectively) and is 
drawn based on an unknown target concept p(y|x). 
In addition, we are also given an unlabeled test set 
in matrix XT with rows xk+1, ?, xk+m. The (hidden) 
positive test instances in XT are also governed by 
the unknown distribution p(x|?), but the (hidden) 
negative test instances in XT are governed by an 
unknown distribution, p(x|?), where ? may or may 
not be the same as ?. p(x|?) and p(x|?) can differ 
arbitrarily, but there is only one unknown target 
conditional class distribution p(y|x).  
This problem setting is common in many appli-
cations, especially in those applications where the 
user is interested in identifying a particular type of 
documents (i.e. binary text classification). For ex-
ample, we want to find sentiment analysis papers 
in the literature. For training a text classifier, we 
may label the papers in some EMNLP proceedings 
as sentiment analysis (positive) and non-sentiment 
analysis (negative) papers. A classifier can then be 
built to find sentiment analysis papers from ACL 
and other EMNLP proceedings. However, this la-
beled training set will not be appropriate for identi-
fying sentiment analysis papers from the WWW, 
KDD and SIGIR conference proceedings. This is 
because although the sentiment analysis papers in 
these proceedings are similar to those in the train-
ing data, the non-sentiment analysis papers in these 
conferences can be quite different. Another exam-
ple is email spam detection. A spam classification 
system built using the training data of spam and 
non-spam emails from a university may not per-
form well in a company. The reason is that al-
though the spam emails (e.g., unsolicited 
commercial ads) are similar in both environments, 
the non-spam emails in them can be quite different.  
One can consider labeling the negative data in 
each environment individually so that only the 
negative instances relevant to the testing environ-
ment are used to train the classifier.  However, it is 
often impractical (if not impossible) to do so. For 
example, given a large blog hosting site, we want 
to classify its blogs into those that discuss stock 
markets (positive), and those that do not (nega-
tive). In this case, the negative data covers an arbi-
trary range of topics. It is clearly impractical to 
label all the negative data. 
Most existing methods for addressing the sam-
ple selection bias problem work as follows.  First, 
they estimate the bias of the training data based on 
the given test data using statistical methods. Then, 
a classifier is trained on a weighted version of the 
original training set based on the estimated bias. In 
this paper, we show that our special case of the 
sample selection bias problem can be solved in a 
much simpler and somewhat radical manner?by 
simply discarding the negative training data alto-
gether. We can use the positive training data and 
the unlabeled test data to build the classifier using 
the PU learning model  (Liu et al 2002).  
PU learning was originally proposed to solve the 
learning problem where no labeled negative train-
ing data exist. Several algorithms have been devel-
oped in the past few years that can learn from a set 
of labeled positive examples augmented with a set 
of unlabeled examples. That is, given a set P of 
positive examples of a particular class (called the 
positive class) and a set U of unlabeled examples 
(which contains both hidden positive and hidden 
negative examples), a classifier is built using P and 
U to classify the data in U as well as future test 
data into two classes, i.e., those belonging to P 
(positive) and those not belonging to P (negative). 
In this paper, we also propose a new PU learning 
method which gives more consistently accurate 
results than the current methods.  
Our experimental evaluation shows that when 
the distributions of the negative training and test 
samples are different, PU learning is much more 
accurate than traditional supervised learning from 
the positive and negative training samples. This 
means that the negative training data actually 
harms classification in this case. In addition, when 
the distributions of the negative training and test 
samples are identical, PU learning is shown to per-
form equally well as supervised learning, which 
means that the negative training data is not needed.   
This paper thus makes three contributions. First, 
it formulates a new special case of the sample se-
lection bias problem, and proposes to solve the 
problem using PU learning by discarding the nega-
tive training data. Second, it proposes a new PU 
learning method which is more accurate than the 
existing methods. Third, it experimentally demon-
strates the effectiveness of the proposed method 
and shows that negative training data is not needed 
and can even be harmful. This result is important 
as it may fundamentally change the way that many 
practical classification problems should be solved.  
219
2 Related Work  
A key assumption made by most machine learning 
algorithms is that the training and test samples 
must be drawn from the same distribution. As 
mentioned, this assumption can be violated in prac-
tice. Some researchers have addressed this problem 
under covariate shift or sample selection bias. 
Sample selection bias was first introduced in the 
econometrics by Heckman (1979). It came into the 
field of machine learning through the work of Za-
drozny (2004). The main approach in machine 
learning is to first estimate the distribution bias of 
the training data based on the test data, and then 
learn using weighted training examples to compen-
sate for the bias (Bickel et al 2009).  
Shimodaira (2000) and Sugiyama and Muller 
(2005) proposed to estimate the training and test 
data distributions using kernel density estimation. 
The estimated density ratio could then be used to 
generate weighted training examples. Dudik et al 
(2005) and Bickel and Scheffer (2007) used maxi-
mum entropy density estimation, while Huang et 
al. (2007) proposed kernel mean matching. Su-
giyama et al (2008) and Tsuboi et al (2008) esti-
mated the weights for the training instances by 
minimizing the Kullback-Leibler divergence be-
tween the test and the weighted training distribu-
tions. Bickel et al (2009) proposed an integrated 
model. In this paper, we adopt an entirely different 
approach by dropping the negative training data 
altogether in learning. Without the negative train-
ing data, we use PU learning to solve the problem 
(Liu et al 2002; Yu et al 2002; Denis et al 2002; 
Li et al 2003; Lee and Liu, 2003; Liu et al 2003; 
Denis et al 2003; Li et al 2007; Elkan and Noto, 
2008; Li et al 2009; Li et al 2010). We will dis-
cuss this learning model further in Section 3.  
Another related work to ours is transfer learning 
or domain adaptation. Unlike our problem setting, 
transfer learning addresses the scenario where one 
has little or no training data for the target domain, 
but has ample training data in a related domain 
where the data could be in a different feature space 
and follow a different distribution. A survey of 
transfer learning can be found in (Pan and Yang 
2009). Several NLP researchers have studied trans-
fer learning for different applications (Wu et al 
2009a; Yang et al 2009; Agirre & Lacalle 2009; 
Wu et al 2009b; Sagae & Tsujii 2008; Goldwasser 
& Roth 2008; Li and Zong 2008; Andrew et al 
2008; Chan and Ng 2007; Jiang and Zhai 2007; 
Zhou et al 2006), but none of them addresses the 
problem studied here.  
3 PU Learning Techniques 
In traditional supervised learning, ideally, there is a 
large number of labeled positive and negative ex-
amples for learning. In practice, the negative ex-
amples can often be limited or unavailable. This 
has motivated the development of the model of 
learning from positive and unlabeled examples, or 
PU learning, where P denotes a set of positive ex-
amples, and U a set of unlabeled examples (which 
contains both hidden positive and hidden negative 
instances). The PU learning problem is to build a 
classifier using P and U in the absence of negative 
examples to classify the data in U or a future test 
data T. In our setting, the test set T will also act as 
the unlabeled set U.  
PU learning has been investigated by several re-
searchers in the past decade. A study of PAC learn-
ing for the setting under the statistical query model 
was given in (Denis, 1998). Liu et al reported the 
sample complexity result and showed how the 
problem may be solved (Liu et al, 2002).  Subse-
quently, a number of practical algorithms (e.g., Liu 
et al, 2002; Yu et al, 2002; Li and Liu, 2003) 
were proposed. They generally follow a two-step 
strategy: (i) identifying a set of reliable negative 
documents RN from the unlabeled set; and then (ii) 
building a classifier using P (positive set), RN (re-
liable negative set) and U-RN (unlabelled set) by 
applying an existing learning algorithm (such as 
naive Bayesian classifier or SVM) iteratively. 
There are also some other approaches based on 
unbalanced errors (e.g., Liu et al 2003; Lee and 
Liu, 2003; Elkan and Noto, 2008). 
In this section, we first introduce a representa-
tive PU learning technique S-EM, and then present 
a new technique called CR-SVM. 
3.1 S-EM Algorithm  
S-EM (Liu et al 2002) is based on na?ve Bayesian 
classification (NB) (Lewis, 1995; Nigam et al, 
2000) and the EM algorithm (Dempster et al 
1977). It has two steps. The first step uses a spy 
technique to identify some reliable negatives (RN) 
from the unlabeled set U and the second step uses 
the EM algorithm to learn a Bayesian classifier 
from P, RN and U?RN. 
220
Step 1: Extracting reliable negatives RN from U 
using a spy technique 
The spy technique in S-EM works as follows (Fig-
ure 1): First, a small set of positive examples (de-
noted by SP) called ?spies? is randomly sampled 
from P (line 2). The default sampling ratio in S-
EM is s = 15%. Then, an NB classifier is built us-
ing P?SP as the positive set and U?SP as the neg-
ative set (lines 3-5). The NB classifier is applied to 
classify each u ? U?SP, i.e., to assign a probabil-
istic class label p(+|u) (+ means positive) to u. The 
idea of the spy technique is as follows. Since the 
spy examples were from P and were put into U as 
negatives in building the NB classifier, they should 
behave similarly to the hidden positive instances in 
U. We thus can use them to find the reliable nega-
tive set RN from U. Using the probabilistic labels 
of spies in SP and an input parameter l (noise lev-
el), a probability threshold t is determined. Due to 
space constraints, we are unable to explain l. De-
tails can be found in (Liu et al 2002). t is then used 
to find RN from U (lines 8-10).  
1.  RN ? ?;                  // Reliable negative set 
2.  SP ? Sample(P, s%);          // spy set 
3.  Assign each example in P ? SP the class label +1; 
4.  Assign each example in U ?SP the class label -1; 
5.  C ?NB(P ? SP, U?SP);   // Produce a NB classifier  
6.  Classify each u ?U?SP using C; 
7.  Decide a probability threshold t using SP and l; 
8.  For each u ?U do 
9.       If its probability p(+|u) < t then 
10.          RN ? RN ? {u}; 
Figure 1. Spy technique for extracting RN from U 
Step 2: Learning using the EM algorithm 
Given the positive set P, the reliable negative set 
RN, and the remaining unlabeled set U?RN, we run 
EM using NB as the base learning algorithm. 
The naive Bayesian (NB) method is an effective 
text classification algorithm. There are two differ-
ent NB models, namely, the multinomial NB and 
the multi-variate Bernoulli NB. In this paper, we 
use the multinomial NB since it has been observed 
to perform consistently better than the multi-
variate Bernoulli NB (Nigam et al, 2000).  
Given a set of training documents D, each doc-
ument di ? D is an ordered list of words. We use 
wdi,k to denote the word in position k of di, where 
each word is from the vocabulary V = {w1, ? , w|v|}, 
which is the set of all words considered in classifi-
cation. We also have a set of classes C = {c1, c2} 
representing positive and negative classes. For 
classification, we compute the posterior probability 
Pr(cj|di). Based on the Bayes rule and multinomial 
model, we have 
      
   (1) 
and with Laplacian smoothing, 
    (2) 
where N(wt,di) is the number of times that the word 
wt occurs in document di, and Pr(cj|di) {0,1} de-
pending on the class label of the document. As-
suming that probabilities of words are independent 
given the class, we have the NB classifier:  
 
(3) 
EM (Dempster et al 1977) is a popular class of 
iterative algorithms for maximum likelihood esti-
mation in problems with incomplete data. It is of-
ten used to address missing values in the data by 
computing expected values using the existing val-
ues. The EM algorithm consists of two steps, the 
E-step and the M-step. The E-step fills in the miss-
ing data, and M-step re-estimated the parameters. 
This process is iterated till satisfaction (i.e. con-
vergence). For NB, the steps used by EM are iden-
tical to those used to build the classifier (equations 
(3) for the E-step, and equations (1) and (2) for the 
? ?
?
= =
=
+
+= |V|
s
|D|
i ijis
|D|
i ijit
jt
d|cd,wN|V|
d|cd,wN
c|w
1 1
1
))Pr((
))Pr((1
)?r(
?
1. Each document in P is assigned the class label 1; 
2. Each document in RN is assigned the class label ?1; 
3. Learn an initial NB classifier f from P and RN, us-
ing Equations (1) and (2); 
4. Repeat 
5. For each document di in U-RN do     // E-Step 
6. Using the current classifier f  compute 
Pr(cj|di) using Equation (3); 
7. Learn a new NB classifier f from P, RN and U-
RN by computing Pr(cj) and Pr(wt|cj), using 
Equations (1) and (2);                       // M-Step 
8. Until the classifier parameters stabilize  
9. The last iteration of EM gives the final classifier f ; 
10. For each document di in U do  
11. If its probability Pr(+|di) ? 0.5 then 
12. Output di as a positive document; 
13. else Output di as a negative document 
Figure 2. EM algorithm with the NB classifier 
||
)|(r
)(r
||
1
D
dc
c
D
i ij
j
? = ?=?
? ?
?
= =
=
??
??=?
||
1
||
1 ,
||
1 ,
)|(r)(r
)|(r)(r
)|(r
C
r
d
k rkdr
d
k jkdj
ij i
i
i
i
cwc
cwc
dc
221
M-step). In EM, Pr(cj|di) takes the value in [0, 1] 
instead of {0, 1} in all the three equations.                                                                               
The algorithm for the second step of S-EM is 
given in Figure 2. Lines 1-3 build a NB classifier f 
using P and RN. Lines 4-8 run EM until conver-
gence. Finally, the converged classifier is used to 
classify the unlabeled set U (lines 10-13).     
3.2 Proposed CR-SVM  
As we will see in the experiment section, the per-
formance of S-EM can be weak in some cases. 
This is due to the mixture model assumption of its 
NB classifier (Nigam et al 2000), which requires 
that the mixture components and classes be of one-
to-one correspondence. Intuitively, this means that 
each class should come from a distinctive distribu-
tion rather than a mixture of multiple distributions. 
In our setting, however, the negative class often 
has documents of mixed topics, e.g., representing 
the broad class of everything else except the top-
ic(s) represented by the positive class.  
There are some existing PU learning methods 
based on SVM which can deal with this problem, 
e.g., Roc-SVM (Li and Liu, 2003). Like S-EM, 
Roc-SVM also has two steps. The first step uses 
Rocchio classification (Rocchio, 1971) to find a set 
of reliable negatives RN from U. In particular, this 
method treats the entire unlabeled set U as negative 
documents and then uses the positive set P and the 
unlabeled set U as the training data to build a Roc-
chio classifier. The classifier is subsequently ap-
plied to classify the unlabeled set U. Those 
documents that are classified as negative are then 
considered as reliable negative examples RN. The 
second step of Roc-SVM runs SVM iteratively 
(instead of EM). Unlike NB, SVM does not make 
any distributional assumption. 
However, Roc-SVM does not do well due to the 
weakness of its first step in finding a good set of 
reliable negatives RN. This motivates us to propose 
a new SVM based method CR-SVM to detect a 
better quality RN set. The second step of CR-SVM 
is similar to that in Roc-SVM.  
Step 1: Extracting reliable negatives RN from U 
using Cosine and Rocchio  
The first step of the proposed CR-SVM algorithm 
for finding a RN set consists of two sub-steps:  
Sub-step 1 (extracting the potential negative set 
PN using the cosine similarity): Given the positive 
set P and the unlabeled set U, we extract a set of 
potential negatives PN from U by computing the 
similarities of the unlabeled documents in U and 
the positive documents in P. The idea is that those 
documents in U that are very dissimilar to the doc-
uments in P are likely to be negative.  
1. PN = ?;  
2. Represent each document in P and U as vectors us-
ing the TF-IDF representation; 
3. For each dj ? P do 
4.  
5. ; 
6. For each dj ? P  do 
7. compute cos(pr, dj) using Equation (4); 
8. Sort all the documents dj?P according to cos(pr, dj) 
in decreasing order; 
9. ? = cos(pr, dp) where dp is ranked in the position of 
(1- l)*|P|; 
10. For each di ? U  do 
11. If cos(pr, di)< ? then 
12. PN = PN ?{di} 
Figure 3. Extracting potential negatives PN from U 
The detailed algorithm is given in Figure 3. 
Each document in P and U is first represented as a 
vector d = (q1, q2, ?, qn) using the TF-IDF scheme 
(Salton 1986). Each element qi (i=1, 2, ?, n) in d 
represents a word feature wi. A positive representa-
tive vector (pr) is built by summing up the docu-
ments in P and normalizing it (lines 3-5). Lines 6-7 
compute the similarities of each document dj in P 
with pr using the cosine similarity, cos(pr, dj).  
Line 8 sorts the documents in P according to 
their cos(pr, dj) values. We want to filter away as 
many as possible hidden positive documents from 
U so that we can obtain a very pure negative set.  
Since the hidden positives in U should have the 
same behaviors as the positives in P in terms of 
their similarities to pr, we set their minimum simi-
larity as the threshold value ? which is the mini-
mum similarity before a document is considered as 
a potential negative document: 
           
Pjj
P
j
?= = ddpr  ),,(cosmin
||
1
?
 
(4) 
In a noiseless scenario, using the minimum simi-
larity is acceptable. However, most real-life appli-
cations contain outliers and noisy artifacts. Using 
the absolute minimum similarity may be unrelia-
ble; the similarity cos(pr, dj) of an outlier docu-
2
||
1 ||||
*
||
1 ?
=
=
P
j j
j
P d
d
pr
2 ||||/ prprpr =
222
ment dj in P could be near 0 or smaller than most 
(or even all) negative documents. It would there-
fore be prudent to ignore a small percentage l of 
the documents in P most dissimilar to the repre-
sentative positive (pr) and assume them as noise or 
outliers.  Since we do not know the noise level of 
the data, to be safe, we use a noise level l = 5% as 
the default. The final classification result is not 
sensitive to l as long as it is not too big. In line 9, 
we use the noise level l to decide on a suitable ?. 
Then, for each document di in U, if its cosine simi-
larity cos(pr, di) < ?, we regard it as a potential 
negative and store it in PN (lines 10-12). 
Our experiment results showed that PN is still 
not sufficient or big enough for accurate PU learn-
ing. Thus, we need to do a bit more work to find 
the final RN.   
Sub-step 2 (extracting the final reliable negative 
set RN from U using Rocchio with PN): At this 
point, we have a positive set P and a potential neg-
ative set PN where PN is a purer negative set than 
U. To extract the final reliable negatives, we em-
ploy the Rocchio classification to build a classifier 
RC using P and PN (We do not use SVM here as it 
is very sensitive to the noise in PN). Those docu-
ments in U that are classified as negatives by RC 
will then be regarded as reliable negatives, and 
stored in set RN.   
The algorithm for this sub-step is given in Fig-
ure 4. Following the Rocchio formula, a positive 
and a negative prototype vectors p and n are built 
(lines 3 and 4), which are used to classify the doc-
uments in U (lines 5-7). ? and ? are parameters for 
adjusting the relative impact of the positive and 
negative examples. In this work, we use ? = 16 and 
? = 4 as recommended in (Buckley et al 1994).  
Step 2:  Learning by running SVM iteratively 
This step is similar to that in Roc-SVM, building 
the final classifier by running SVM iteratively with 
the sets P, RN and the remaining unlabeled set Q 
(Q = U ? RN).  
The algorithm is given in Figure 5. We run 
SVM classifiers Si (line 3) iteratively to extract 
more and more negative documents from Q. The 
iteration stops when no more negative documents 
can be extracted from Q (line 5). There is, howev-
er, a danger in running SVM iteratively, as SVM is 
quite sensitive to noise. It is possible that during 
some iteration, SVM is misled by noisy data to 
extract many positive documents from Q and put 
them in the negative set RN. If this happens, the 
final SVM classifier will be inferior. As such, we 
employ a test to decide whether to keep the first 
SVM classifier or the final one. To do so, we use 
the final SVM classifier obtained at convergence 
(called Slast, line 9) to classify the positive set P to 
see if many positive documents in P are classified 
as negatives. Roc-SVM chooses 5% as the thre-
shold, so CR-SVM also uses this threshold. If there 
are 5% of positive documents (5%*|P|) in P that 
are classified as negative, it indicates that SVM has 
gone wrong and we should use the first SVM clas-
sifier (S1). In our experience, the first classifier is 
always quite strong; good results can therefore be 
achieved even without catching the last (possibly 
better) classifier.  
The main difference between Roc-SVM and 
CR-SVM is that Roc-SVM does not produce PN. It 
simply treats the unlabeled set U as negatives for 
extracting RN. Since PN is clearly a purer negative 
set than U, the use of PN by CR-SVM helps ex-
tract a better quality reliable negative set RN which 
subsequently allows the final classifier of CR-
SVM to give better results than Roc-SVM.   
Note that the methods (S-EM and CR-SVM) are 
all two-step algorithms in which the first step and 
the second step are independent of each other. The 
algorithm for the second step basically needs a 
good set of reliable negatives RN extracted from U. 
This means that one can pick any algorithm for the 
first step to work with any algorithm for the second 
step. For example, we can also have CR-EM which 
uses the algorithm (shown in Figures 3 and 4) of 
the first step of CR-SVM to combine with the al-
gorithm of the second step of S-EM. CR-EM ac-
tually works quite well as it is also able to exploit 
the more accurate reliable negative set RN ex-
tracted using cosine and Rocchio. 
 
1. RN = ?;  
2. Represent each document in P, PN and U as vectors 
using the TF-IDF representation; 
3. ; 
4. ; 
5. For each di ? U  do 
6. If  cos(di, n)> cos(di, p) then 
7. RN  = RN ?{di} 
Figure 4. Identifying RN using the Rocchio classifier 
??
??
?=
PN i
i
P j
j
ij
PNP dd d
d
d
d
p
||||||
1
||||||
1 ??
??
??
?=
P j
j
PN i
i
ji
PPN dd d
d
d
d
n
||||||
1
||||||
1 ??
223
4 Empirical Evaluation 
We now present the experimental results to support 
our claim that negative training data is not needed 
and can even harm text classification. We also 
show the effectiveness of the proposed PU learning 
methods CR-SVM and CR-EM. The following 
methods are compared: (1) traditional supervised 
learning methods SVM and NB which use both 
positive and negative training data; (2) PU learning 
methods, including two existing methods S-EM 
and Roc-SVM and two new methods CR-SVM and 
CR-EM, and (3) one-class SVM (Sch?lkop et al, 
1999) where only positive training data is used in 
learning (the unlabeled set is not used at all).  
We used LIBSVM 1  for SVM and one-class 
SVM, and two publicly available 2  PU learning 
techniques S-EM and Roc-SVM. Note that we do 
not compare with some other PU learning methods 
such as those in (Liu et al 2003, Lee and Liu, 2003 
and Elkan and Noto, 2008) as the purpose of this 
paper is not to find the best PU learning method 
but to show that PU learning can address our spe-
cial sample selection bias problem. Our current 
methods already do very well for this purpose.  
4.1 Datasets and Experimental Settings 
We used two well-known benchmark data collec-
tions for text classification, the Reuters-21578 col-
lection 3  and the 20 Newsgroup collection 4 . 
Reuters-21578 contains 21578 documents. We 
used the most populous 10 out of the 135 catego-
ries following the common practice of other re-
searchers. 20 Newsgroup has 11997 documents 
from 20 discussion groups. The 20 groups were 
also categorized into 4 main categories.  
We have performed two sets of experiments, 
and just used bag-of-words as features since our 
objective in this paper is not feature engineering.  
(1) Test set has other topic documents. This set 
of experiments simulates the scenario in which the 
negative training and test samples have different 
distributions. We select positive, negative and oth-
er topic documents for Reuters and 20 Newsgroup, 
and produce various data sets. Using these data 
sets, we want to show that PU learning can do bet-
                                                          
1 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
2 http://www.cs.uic.edu/~liub/LPU/LPU-download.html 
3 http://www.research.att.com/~lewis/reuters21578.html 
4 http://people.csail.mit.edu/jrennie/20Newsgroups/ 
ter than traditional learning that uses both positive 
and negative training data. 
For the Reuters collection, each of the 10 cate-
gories is used as a positive class. We randomly 
select one or two of the remaining categories as the 
negative class (denoted by Neg 1 or Neg 2), and 
then we randomly choose some documents from 
the rest of the categories as other topic documents. 
These other topic documents are regarded as nega-
tives and added to the test set but not to the nega-
tive training data. They thus introduce a different 
distribution to the negative test data. We generated 
20 data sets (10*2) for our experiments this way. 
The 20 Newsgroup collection has 4 main cate-
gories with sub-categories5; the sub-categories in 
the same main category are relatively similar to 
each other. We are able to simulate two scenarios: 
(1) the other topic documents are similar to the 
negative class documents (similar case), and (2) 
the other topic documents are quite different from 
the negative class documents (different case). This 
allows us to investigate whether the classification 
results will be affected when the other topic docu-
ments are somewhat similar or vastly different 
from the negative training set. To create the train-
ing and test data for our experiments, we randomly 
select one sub-category from a main category (cat 
1) as the positive class, and one (or two) sub-
category from another category (cat 2) as the nega-
tive class (again denoted by Neg 1 or Neg 2). For 
the other topics, we randomly choose some docu-
                                                          
5  The four main categories and their corresponding sub-
categories are: computer (graphics, os, ibmpc.hardware, 
mac.hardware, windows.x), recreation (autos, motorcycles, 
baseball, hockey), science (crypt, electronics, med, space), and 
talk (politics.misc, politics.guns, politics.mideast, religion). 
1. Every document in P is assigned the class label +1; 
2. Every document in RN is assigned the label ?1; 
3. Use P and RN  to train a SVM classifier Si, with i = 
1 initially and i = i+1 with each iteration (line 3-7);  
4. Classify Q using Si. Let the set of documents in Q 
that are classified as negative be W;  
5. If (W = ?) then  stop; 
6. else Q = Q ? W; 
7. RN = RN ?W 
8. goto (3); 
9. Use the last SVM classifier Slast to classify P; 
10. If more than 5% positives are classified as negative  
11. then use S1 as the final classifier; 
12. else use Slast as the final classifier; 
Figure 5.  Constructing the final classifier using SVM 
224
ments from the remaining sub-categories of cat 2 
for the similar case, and some documents from a 
randomly chosen different category (cat 3) (as the 
other topic documents) for the different case. We 
generated 8 data sets (4*2) for the similar case, 
and 8 data sets (4*2) for the different case.   
The training and test sets are then constructed as 
follows: we partition the positive (and similarly for 
the negative) class documents into two standard 
subsets: 70% for training and 30% for testing. In 
order to create different experimental settings, we 
vary the number of the other topic documents that 
are added to the test set as negatives, controlled by 
a parameter ?, which is a percentage of |TN|, where 
|TN| is the size of the negative test set without the 
other topic documents. That is, the number of oth-
er topic documents added is ? ? |TN|.  
(2) Test set has no other topic documents. This 
set of experiments is the traditional classification 
in which the training and test data have the same 
distribution. We employ the same data sets as in 
(1) but without having any other topic documents 
in the test set. Here we want to show that PU learn-
ing can do equally well without using the negative 
training data even in the traditional setting.  
4.2 Results with Other Topic Documents in 
Test Set 
We show the results for experiment set (1), i.e. the 
distributions of the negative training and test data 
are different (caused by the inclusion of other topic 
documents in the test set, or the addition of other 
topic documents to complement existing negatives 
in the test set). The evaluation metric is the F-score 
on the positive class (Bollmann and Cherniavsky, 
1981), which is commonly used for evaluating text 
classification.  
4.2.1  Results on the Reuters data 
Figure 6 shows the comparison results when the 
negative class contains only one category of doc-
uments (Neg 1), while Figure 7 shows the results 
when the negative class contains documents from 
two categories (Neg 2) in the Reuters collection. 
The data points in the figures are the averages of 
the results from the corresponding datasets.  
Our proposed method CR-SVM is shown to per-
form consistently better than the other techniques. 
When the size of the other topic documents (x-
axis) in the test set increases, the F-scores of the 
two traditional learning methods SVM and NB 
decreased much more dramatically as compared 
with the PU learning techniques. The traditional 
learning models were clearly unable to handle dif-
ferent distributions for training and test data. 
Among the PU learning techniques, the proposed 
CR-SVM gave the best results consistently. Roc-
SVM did not do consistently well as it did not 
manage to find high quality reliable negatives RN 
sometimes. The EM based methods (CR-EM and 
S-EM) performed well in the case when we had 
only one negative class (Figure 6). However, it did 
not do well in the situation where there were two 
negative classes (Figure 7) due to the underlying 
mixture model assumption of the na?ve Bayesian 
classifier. One-class SVM (OSVM) performed 
poorly because it did not exploit the useful infor-
mation in the unlabeled set at all.   
 
Figure 6. Results of Neg 1 using the Reuter data 
 
Figure 7. Results of Neg 2 using the Reuter data 
4.2.2  Results on 20 Newsgroup data 
Recall that for the 20 Newsgroup data, we have 
two settings: similar case and different case.  
Similar case: Here, the other topic documents are 
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.5
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
225
similar to the negative class documents, as they 
belong to the same main category.  
The comparison results are given in Figure 8 
(Neg 1) and Figure 9 (Neg 2). We observe that 
CR-EM, S-EM and CR-SVM all performed well. 
EM based methods (CR-EM and S-EM) have a 
slight edge over CR-SVM. Again, the F-scores of 
the traditional supervised learning (SVM and NB) 
deteriorated when more other topic documents 
were added to the test set, while CR-EM, S-EM 
and CR-SVM were able to remain unaffected and 
maintained roughly constant F-scores. When the 
negative class contained documents from two cate-
gories (Neg 2), the F-scores of the traditional 
learning dropped even more rapidly. Both Roc-
SVM and One-class SVM (OSVM) performed 
poorly, due to the same reasons given previously.  
 
Figure 8. Results of Neg 1, similar case ? using the 20 
Newsgroup data 
 
Figure 9. Results of Neg 2, similar case ? using the 20 
Newsgroup data 
Different case: In this case, the other topic docu-
ments are quite different from the negative class 
documents, since they are originated from different 
main categories.  
The results are shown in Figures 10 (Neg 1) and 
11 (Neg 2). The trends are similar to those for the 
similar case, except that the performance of the 
traditional supervised learning methods (SVM and 
NB) dropped even more rapidly with more other 
topic documents. As the other topic documents 
have very different distributions from the negatives 
in the training set in this case, they really confused 
the traditional classifiers. In contrast, the three PU 
learning techniques were still able to perform con-
sistently well, regardless of the number of other 
topic documents added to the test data.  
 
Figure 10. Results of Neg 1, different case ? using the 
20 Newsgroup data  
 
Figure 11.  Results of Neg 2, different case ? using the 
20 Newsgroup data 
In summary, the results showed that learning 
with negative training data based on the traditional 
paradigm actually harms classification when the 
identical distribution assumption does not hold.  
4.3 Results without Other Topic Documents in 
Test Set 
Given an application, one may not know whether 
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN| of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN| of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
226
the identical distribution assumption holds. The 
above results showed that PU learning is better 
when it does not hold.  How about when the as-
sumption does hold? To find out, we compared the 
results of SVM, NB, and three PU learning me-
thods using the datasets without any other topic 
documents added to the test set. In this case, the 
training and test data distributions are the same. 
Table 1 shows the results for this scenario. Note 
that for PU learning, the negative training data 
were not used. The traditional supervised learning 
techniques (SVM and NB), which made full use of 
the positive and negative training data, only per-
formed just about 1-2% better than the PU learning 
method CR-SVM (which is not statistically signifi-
cant based on paired t-test). This suggests that we 
can do away with negative training data, since PU 
learning can perform equally well without them.  
This has practical importance since the full cover-
age of negative training data is hard to find and to 
label in many applications. 
From the results in Figures 6?11 and Table 1, 
we can conclude that PU learning can be used for 
binary text classification without the negative 
training data (which can be harmful for the task). 
CR-SVM is our recommended PU learning method 
based on its generally consistent performance. 
Table 1. Comparison of methods without other docu-
ments in test set 
 Methods 
Reuters 
 (Neg 1) 
Reuters 
 (Neg 2) 
20News 
(Neg 1) 
20News 
(Neg 2) 
SVM 0.971 0.964 0.988 0.990 
NB 0.972 0.947 0.988 0.992 
S-EM 0.952 0.921 0.974 0.975 
CR-EM 0.955 0.897 0.983 0.986 
CR-SVM 0.960 0.959 0.967 0.974 
5 Conclusions 
This paper studied a special case of the sample se-
lection bias problem in which the positive training 
and test distributions are the same, but the negative 
training and test distributions may be different. We 
showed that in this case, the negative training data 
should not be used in learning, and PU learning 
can be applied to this setting. A new PU learning 
algorithm (called CR-SVM) was also proposed to 
overcome the weaknesses of the current two-step 
algorithms.  
 Our experiments showed that the traditional 
classification methods suffered greatly when the 
distributions are different for the negative training 
and test data, but PU learning does not. We also 
showed that PU learning performed equally well in 
the ideal case where the training and test data have 
identical distributions. As such, it can be advanta-
geous to discard the potentially harmful negative 
training data and use PU learning for classification.  
 In our future work, we plan to do more compre-
hensive experiments to compare the classic super-
vised learning and PU learning techniques with 
different kinds of settings, for example, by varying 
the ratio between positive and negative examples, 
as well as their sizes. It is also important to explore 
how to catch the best iteration of the SVM/NB 
classifier in the iterative running process of the 
algorithms. Finally, we would like to point out that 
it is conceivable that negative training data could 
still be useful in many cases. An interesting direc-
tion to explore is to somehow combine the ex-
tracted reliable negative data from the unlabeled 
set and the existing negative training data to further 
enhance learning algorithms.  
 
References  
Agirre E., Lacalle L.O. 2009. Supervised Domain Adap-
tion for WSD. Proceedings of the 12th Conference of 
the European Chapter for Computational Linguistics 
(EACL09), pp 42-50.  
Andrew A., Nallapati R., Cohen W., 2008. Exploiting 
Feature Hierarchy for Transfer Learning in Named 
Entity Recognition, ACL. 
Bickel, S., Bruckner, M., and Scheffer. 2009. T. Dis-
criminative learning under covariate shift. Journal of 
Machine Learning Research.  
Bickel S. and Scheffer T. 2007. Dirichlet-enhanced 
spam filtering based on biased samples. In Advances 
in Neural Information Processing Systems. 
Bollmann, P.,& Cherniavsky, V. 1981. Measurement-
theoretical investigation of the mz-metric. Informa-
tion Retrieval Research. 
Buckley, C., Salton, G., & Allan, J. 1994. The effect of 
adding relevance information in a relevance feed-
back environment, SIGIR. 
Blum, A. and Mitchell, T. 1998. Combining labeled and 
unlabeled data with co-training. In Proc. of Compu-
tational Learning Theory, pp. 92?10. 
Chan Y. S., Ng H. T. 2007. Domain Adaptation with 
Active Learning for Word Sense Disambiguation, 
ACL. 
Dempster A., Laird N. and Rubin D.. 1977. Maximum 
likelihood from incomplete data via the EM algorithm, 
Journal of the Royal Statistical Society. 
Denis F., PAC learning from positive statistical queries. 
ALT, 1998. 
227
Denis F., Laurent A., R?mi G., Marc T. 2003. Text clas-
sification and co-training from positive and unlabeled 
examples. ICML. 
Denis, F, R?mi G, and Marc T. 2002. Text Classifica-
tion from Positive and Unlabeled Examples. In Pro-
ceedings of the 9th International Conference on 
Information Processing and Management of Uncer-
tainty in Knowledge-Based Systems. 
Downey, D., Broadhead, M. and Etzioni, O. 2007. Lo-
cating complex named entities in Web Text. IJCAI.  
Dudik M., Schapire R., and Phillips S. 2005. Correcting 
sample selection bias in maximum entropy density 
estimation. In Advances in Neural Information Proc-
essing Systems.  
Elkan, C. and Noto, K. 2008. Learning classifiers from 
only positive and unlabeled data. KDD, 213-220.  
Goldwasser, D., Roth D. 2008. Active Sample Selection 
for Named Entity Transliteration, ACL. 
Heckman J. 1979. Sample selection bias as a specifica-
tion error. Econometrica, 47:153?161. 
Huang J., Smola A., Gretton A., Borgwardt K., and 
Scholkopf B. 2007. Correcting sample selection bias 
by unlabeled data. In Advances in Neural Informa-
tion Processing Systems. 
Jiang J. and Zhai C. X. 2007. Instance Weighting for 
Domain Adaptation in NLP, ACL. 
Lee, W. S. and Liu, B. 2003. Learning with Positive and 
Unlabeled Examples Using Weighted Logistic Re-
gression. ICML.  
Lewis D. 1995. A sequential algorithm for training text 
classifiers: corrigendum and additional data. SIGIR 
Forum, 13-19. 
Li, S., Zong C., 2008. Multi-Domain Sentiment Classifi-
cation, ACL. 
Li, X., Liu, B. 2003. Learning to classify texts using 
positive and unlabeled data, IJCAI. 
Li, X., Liu, B., 2005. Learning from Positive and Unla-
beled Examples with Different Data Distributions. 
ECML. 
Li, X., Liu, B., 2007. Learning to Identify Unexpected 
Instances in the Test Set. IJCAI.  
Li, X., Yu, P. S., Liu B., and Ng, S. 2009. Positive 
Unlabeled Learning for Data Stream Classification, 
SDM. 
Li, X., Zhang L., Liu B., and Ng, S. 2010. Distribution-
al Similarity vs. PU Learning for Entity Set Expan-
sion, ACL. 
Liu, B, Dai, Y., Li, X., Lee, W-S., and Yu. P. 2003. 
Building text classifiers using positive and unlabeled 
examples. ICDM, 179-188. 
Liu, B, Lee, W-S, Yu, P. S, and Li, X. 2002. Partially 
supervised text classification. ICML, 387-394. 
Nigam, K., McCallum, A., Thrun, S. and Mitchell, T. 
2000. Text classification from labeled and unlabeled 
documents using EM. Machine Learning, 39(2/3), 
103?134.  
Pan, S. J. and Yang, Q. 2009. A survey on transfer 
learning. IEEE Transactions on Knowledge and Da-
ta Engineering, Vol. 99, No. 1. 
Rocchio, J. 1971. Relevant feedback in information 
retrieval. In G. Salton (ed.). The smart retrieval sys-
tem: experiments in automatic document processing, 
Englewood Cliffs, NJ, 1971.Sagae K., Tsujii J. 2008. 
Online Methods for Multi-Domain Learning and 
Adaptation, EMNLP. 
Salton G. and McGill M. J. 1986. Introduction to Mod-
ern Information Retrieval.  
Sch?lkop f B., Platt J.C., Shawe-Taylor J., Smola A.J., 
and Williamson R.C. 1999. Estimating the support 
of a high-dimensional distribution. Technical report, 
Microsoft Research, MSR-TR-99-87. 
Shimodaira H. 2000. Improving predictive inference 
under covariate shift by weighting the log-likelihood 
function. Journal of Statistical Planning and Infer-
ence, 90:227?244. 
Sugiyama M. and Muller K.-R. 2005. Input-dependent 
estimation of generalization error under covariate 
shift. Statistics and Decision, 23(4):249?279. 
Sugiyama M., Nakajima S., Kashima H., von Bunau P., 
and Kawanabe M. 2008. Direct importance estima-
tion with model selection and its application to co-
variate shift adaptation. In Advances in Neural 
Information Processing Systems. 
Tsuboi J., Kashima H., Hido S., Bickel S., and Sugi-
yama M. 2008. Direct density ratio estimation for 
large-scale covariate shift adaptation. In Proceed-
ings of the SIAM International Conference on Data 
Mining, 2008. 
Wu D., Lee W.S., Ye N. and Chieu H. L. 2009. Domain 
adaptive bootstrapping for named entity recognition, 
ACL. 
Wu Q., Tan S. and Cheng X. 2009. Graph Ranking for 
Sentiment Transfer, ACL. 
Yang Q., Chen Y., Xue G., Dai W., Yu Y. 2009. Hete-
rogeneous Transfer Learning for Image Clustering 
via the SocialWeb, ACL  
Yu, H., Han, J., K. Chang. 2002. PEBL: Positive exam-
ple based learning for Web page classification using 
SVM. KDD, 239-248. 
Zadrozny B. 2004. Learning and evaluating classifiers 
under s ample selection bias, ICML.  
Zhou Z., Gao J., Soong F., Meng H. 2006. A Compara-
tive Study of Discriminative Methods for Reranking 
LVCSR N-best Hypotheses in Domain Adaptation 
and Generalization. ICASSP. 
228
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1124?1135,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Identifying Multiple Userids of the Same Author 
 
 
Tieyun Qian Bing Liu 
State Key Laboratory of Software Eng.  
Wuhan University 
Department of Computer Science 
University of Illinois at Chicago 
16 Luojiashan Road 851 South Morgan St., Chicago 
Wuhan, Hubei 430072, China  IL, USA, 60607 
qty@whu.edu.cn liub@cs.uic.edu 
 
  
 
Abstract 
This paper studies the problem of identifying 
users who use multiple userids to post in so-
cial media. Since multiple userids may belong 
to the same author, it is hard to directly apply 
supervised learning to solve the problem. This 
paper proposes a new method, which still uses 
supervised learning but does not require train-
ing documents from the involved userids. In-
stead, it uses documents from other userids 
for classifier building. The classifier can be 
applied to documents of the involved userids. 
This is possible because we transform the 
document space to a similarity space and 
learning is performed in this new space. Our 
evaluation is done in the online review do-
main. The experimental results using a large 
number of userids and their reviews show that 
the proposed method is highly effective. 
1 Introduction 
It is common knowledge that some users in social 
media register multiple accounts/userids to post 
articles, blogs, reviews, etc. There are many rea-
sons for doing this. For example, due to past post-
ings, a user may become despised by others. 
He/she then registers another userid in order to 
regain his/her status. A user may also use multiple 
userids to instigate controversy or debates to popu-
larize a topic to make it ?hot? or even just to pro-
mote activities at a website. Yet, a user may also 
use multiple userids to post fake or deceptive opin-
ions to promote or demote some products (Liu, 
2012). It is thus important to develop technologies 
to identify such multi-id users. This paper deals 
with this problem based on writing style and other 
linguistic clues.  
Problem definition: Given a set of userids ID = 
{id1, ?, idn} and each idi has a set of documents 
Di, we want to identify userids that belong to the 
same physical author.  
The main related works to ours are in the area of 
authorship attribution (AA), which aims to identify 
authors of documents. AA is often solved using 
supervised learning. Let A = {a1, ?, ak} be a set of 
authors (or classes) and each author ai ? A has a 
set of training documents Di. A classifier is then 
built to decide the author a of each test document 
d, where a ? A. We will discuss this and other re-
lated works in Section 2.  
This supervised AA formulation, however, is 
not suitable for our task because we only have 
userids but not real authors. Since some of the 
userids may belong to the same author, we cannot 
treat each userid as a class because in that case, we 
will be classifying based on userids, which won?t 
help us find authors with multiple userids (see Sec-
tion 7 also).  
This paper proposes a novel algorithm. To sim-
plify the presentation, we assume that at most two 
userids can belong to a single author, but the algo-
rithm can be extended to handle more than two 
userids from the same author. Using this assump-
tion, the algorithm works in two steps:  
1.  Candidate identification: For each userid idi, 
we first find the most likely userid idj (i ? j) that 
may have the same author as idi. We call idj the 
candidate of idi. We also call this function can-
did-iden, i.e., idj = candid-iden(idi). For easy 
presentation, here we only use one argument for 
*  The work was mainly done when the first author was visit-
ing the University of Illinois at Chicago.  
1124
candid-iden. In the computation, it needs more 
arguments (see Section 4).  
2.  Candidate confirmation: In the reverse order, 
we apply the function candid-iden on idj, which 
produces idk, i.e., idk = candid-iden(idj). 
Decision making: If k = i, we conclude that idi 
and idj are from the same author. Otherwise, idi 
and idj are not from the same author.   
The key of the algorithm is candid-iden. An ob-
vious approach for candid-iden is to use an infor-
mation retrieval method. We can first split the 
documents Di of each idi into two subsets, a query 
set Qi and a sample set Si. We then compare each 
query document in Qi with each sample document 
in Sj from other userids idj (? ID ? {idi}). Cosine 
can be used here for similarity comparison. All the 
similarity scores are then aggregated and used to 
rank the userids in ID ? {idi}. The top ranked 
userid is the candidate for idi. Note that partition-
ing the documents of a userid idi into the query set 
Qi and the sample set Si is crucial here. We cannot 
use all documents in Di to compare with all docu-
ments in Dj. If so and we get candid-iden(idi) = idj, 
we will definitely get candid-iden(idj) = idi since 
the similarity function is symmetric.  
This cosine similarity based method, however, 
does not work well (see Section 7). We propose a 
supervised learning method to compute the scores. 
For this, we need to reformulate the problem.  
The idea of this reformulation is to learn in a 
similarity space rather than in the original docu-
ment space as in traditional AA. In the new formu-
lation, each document d is still represented as a 
feature vector, but the vector no longer represents 
the document d itself. Instead, it represents a set of 
similarities between the document d and a query q. 
We call this method learning in the similarity 
space (LSS).  
Specifically, in LSS, each document d is first 
represented with a document space vector (called a 
d-vector) based on the document itself as in the 
traditional classification learning of AA. Each fea-
ture in the d-vector is called a d-feature (docu-
ment-feature). A query document q is represented 
in the same way. We then produce a similarity vec-
tor sv (called s-vector) for d. sv consists of a set of 
similarity values between document d (in a d-
vector) and query q (in a d-vector):  
sv =Sim(d, q), 
where Sim is a similarity function consists of a set 
of similarity measures. Thus, the d-vector for doc-
ument d in the document space is transformed to 
an s-vector sv for d in the similarity space. Each 
feature in sv is called an s-feature. For example, 
we have the following d-vector for query q:  
 q: 1:1 2:1 6:2 
where x:z represents a d-feature x (a word) and its 
frequency z in q. We also have two non-query 
documents, one is d1 which is written by the author 
of query q and the other is d2 which is not written 
by query author q. Their d-vectors are: 
 d1:  1:2 2:1 3:1  d2:  2:2 3:1 5:2   
If we use cosine as the first similarity measure in 
Sim, we can generate an s-feature 1:0.50 for d1 
(cosine(q, d1) = 0.50) and an s-feature 1:0.27 for d2 
(cosine(q, d2) = 0.27). If we have more similarity 
measures more s-features can be produced. The 
resulting two s-vectors for d1 and d2 with their 
class labels, 1 and -1, are as follows:  
 d1: 1 1:0.50 ? d2:  -1  1:0.27 ? 
Class 1 means ?written by author of query q?, also 
called q-positive, and class -1 means ?not written 
by author of query q?, also called q-negative.  
LSS gives us a two-class classification problem. 
In this formulation, a test userid and his/her docu-
ments do not have to be seen in training as long as 
a set of known documents from this userid is 
available. Any supervised learning method can be 
used to build a classifier. We use SVM. The result-
ing classifier is employed to compute a score for 
each review to be used in the two-step algorithm 
above to find the candidate for each userid and 
then the userids with the same authors.  
Due to the use of query documents, the LSS 
formulation has some resemblance to document 
ranking based on learning to rank (Li, 2011; Liu, 
2011). However, LSS is very different because we 
turn the problem into a supervised classification 
problem. The key difference between learning to 
rank and classification is that ranking will always 
put some documents at the top even if the desired 
documents do not exist. However, classification 
will not return any document if the desired docu-
ments do not exist in the test data (unless there are 
classification errors). Our Type II experiments in 
Section 7 were specifically designed for testing 
such non-existence situations. 
1125
Using online review as the application domain, 
we conduct experiments on a large number of re-
views and their author/reviewer userids from Am-
azon.com. The results show that the proposed 
algorithm is highly accurate and outperforms three 
strong baselines markedly. 
2 Related Work 
A similar problem was attempted in (Chen et al, 
2004) in the context of open forums where users 
interact with each other in their discussions. Their 
method is based on post relationships and intervals 
between posts. It does not use any linguistic clues. 
It is thus not applicable to domains like online re-
views. Reviews do not involve user interactions 
since each review is independent of other reviews. 
Novak et al also solved the same problem under 
the name of ?Anti-aliasing? (Novak et al, 2004). 
They used a clustering based method which as-
sumed the number of actual authors is known. This 
is unrealistic in practice as there is no way to know 
which author has and does not have multiple ids. 
Our work is also related to authorship attribu-
tion (AA). However, to our knowledge, our prob-
lem has not been attempted in AA. Existing works 
focused on two main themes: finding good writing 
style features, and developing effective classifica-
tion methods. On finding good features (d-features 
in our case), it was found that the most promising 
features are function words (Mosteller, 1964; Ar-
gamon and Levitan, 2004; Argamon et al, 2007) 
and rewrite rules (Halteren et al, 1996). Length 
(Gamon 2004; Graham et al, 2005), richness (Hal-
teren et al, 1996; Koppel and Schler, 2004), punc-
tuations (Graham et al, 2005), character n-grams 
(Grieve, 2007; Hedegaard and Simonsen, 2011), 
word n-grams (Burrows, 1992; Sanderson and 
Guenter 2006), POS n-grams (Gamon, 2004; Hirst 
and Feiguina, 2007), syntactic category pairs (Na-
rayanan et al, 2012) are also useful.  
On classification, numerous methods have been 
tried, e.g., Bayesian analysis (Mosteller, 1964), 
discriminant analysis (Stamatatos et al, 2000), 
PCA (Hoover, 2001), neural networks (Graham et 
al., 2005; Zheng et al, 2006; Graham et al, 2005), 
clustering (Sanderson and Guenter, 2006), decision 
trees (Uzuner and Katz, 2005; Zhao and Zobel, 
2005), regularized least squares classification 
(Narayanan et al, 2012),   and SVM (Diederich et 
al., 2000; Gamon 2004; Koppel and Schler, 2004; 
Hedegaard and Simonsen, 2011). Among them, 
SVM was found to be most accurate (Li et al, 
2006; Kim et al, 2011). Although we also use 
supervised learning, we do not learn in the original 
document space as these existing methods do. The 
transformation is important because it enables us 
to use documents from other authors in training. 
The traditional supervised learning (TSL) cannot 
do that. In our case, the only documents that TSL 
can use for training are the queries in the testing 
set. However, as we will see in our experiments, 
such a method performs poorly.  
Since we use online reviews as our experiment 
domain, our work is related to fake review detec-
tion (Jindal and Liu, 2008) as imposters can use 
multiple userids to post fake reviews. Existing re-
search has proposed many methods to detect fake 
reviewers (Lim et al, 2010; Wang et al, 2011; 
Mukherjee et al, 2012) and fake reviews (Jindal 
and Liu, 2008; Ott et al, 2011, 2012; Li et al, 
2011; Feng et al, 2012). However, none of them 
identifies userids belonging to the same person. 
3 Learning in the Similarity Space 
We now formulate the proposed supervised 
learning in the similarity space (LSS), which will 
be used in the candid-iden function in our algo-
rithm to be discussed in Section 4.  
The key difference between LSS and the classic 
document space learning is in the document repre-
sentation. Another difference is in the testing 
phase. We discuss testing first.  
Test data: We are given: 
? A query q from query author (userid) aq 
? A set of test documents DT = {dt1, ?, dtm}. 
Goal: classify the test documents into those au-
thored by aq and those not authored by aq.  
We note the following points:  
i)  This is like a retrieval scenario, but we use su-
pervised learning to perform the task.  
ii) Unlike traditional supervised classification, 
here the test query author aq does not have to 
be used in training. But we are given a query 
document q from aq. Clearly, in practice, we 
can have multiple query documents from aq, 
which we will discuss in Section 4.  
Training document representation: As noted 
earlier, each document is represented with a simi-
larity vector (s-vector) computed using a similarity 
1126
function Sim. Sim takes a query document and a 
non-query document and produces a vector of sim-
ilarity values or s-features to represent the non-
query document. We present the detail below:  
Let the set of training authors be AR = {ar1, .., 
arn}. Each author ari has a set of documents DRi. 
Each document in DRi is first represented with a 
document vector (or d-vector). The algorithm for 
producing the training set, called s-training set, is 
given in Figure 1.  
We randomly select a small set of queries Qi 
from documents DRi of each author ari (lines 1, 
and 2). For each query qij ? Qi (line 3), it selects a 
set of documents DRij also from DRi (excluding qij) 
of the same author (line 4) to be the positive doc-
uments for qij, called q-positive and labeled 1. 
Then, for each document drijk in DRij, a q-positive 
s-training example with the label 1 is generated for 
drijk by computing the similarities of qij and drijk 
using the similarity function Sim (lines 5, 6). In 
line 7, it selects a set of documents DRij,rest from 
other authors to be the negative documents for qij, 
called q-negative and labeled -1. For each docu-
ment drijk,rest in DRij,rest (line 8), a q-negative s-
training example with label -1 is generated for drijk 
by computing the similarities of qij and drijk,rest us-
ing Sim (line 9). How to select Qi, DRij and DRij,rest 
(lines 2, 4 and 7) is left open intentionally to give 
flexibility in implementation.  
This formulation gives us a two-class classifica-
tion problem. The classes are 1 (q-positive mean-
ing ?written by author of query qij?) and -1 (q-
negative meaning ?not written by author of query 
qij.?  Figure 2 shows what the s-training data looks 
like. For easy presentation, we assume that there 
are k queries in every Qi, and p documents in every 
DRij and u documents in every DRij,rest. The num-
ber of authors is n. Each author ari generates 
k?(p+u) s-training examples. As we will see in 
Section 7, k can be very small, even 1. 
Complexity: In the worst case, every document 
1. For each document set Di of idi ? ID do 
2.  partition Di into two subsets:    
 (1) query set Qi and (2) sample set Si;   
3. For each document set Di of idi ? ID do 
 // step 1: candidate identification 
4. idj = candid-iden(idi, ID), i < j; 
 // step 2: candidate confirmation  
5. idk = candid-iden(idj, ID), k ? j; 
6. If k = i then idi and idj are from the same author 
8. else  idi and idj are not from the same author  
Figure 3: Identifying userids from the same authors 
Function candidate-iden(idi, ID) 
1. For each sample document set Sj of idj ? ID-{idi} do 
2.  pcount[idj],  psum[idj], psqsum[idj], max[idj] = 0; 
3.  For each query qi ? Qi do 
4. For each sample sjf ? Sj do 
5.  ssjf = <(idi, qi), (Sim(sjf, qi), ?)>;   
6.  Classify ssjf  using the classifier built earlier; 
7. If ssjf is classified positive, i.e., 1 then 
8.  pcount[idj] = pcount[idj] + 1; 
9.  psum[idj] = psum[idj] + ssjf.score 
10 psqsum[idj] = psqsum[idj] + (ssjf.score)2 
11. If ssif.score > max[idj] then 
12. max[idj] = srjf.score 
// Four methods to decide which idj is the candidate for idi 
13. If for all idj ? ID-{idi}, pcount[idi] = 0 then 
14.  
])(max[maxarg }{ jidIDid idcid ij ???
 
15. Else 
)||
][(maxarg
}{ j
j
idIDid S
idpcountcid
ij ??
?
 // 1. Voting 
16. 
)||
][(maxarg
}{ j
j
idIDid S
idpsumcid
ij ??
?
 // 2. ScoreSum 
17. 
)||
])[((maxarg
2
}{ j
j
idIDid S
idpsumcid
ij ??
?
 // 3. ScoreSqSum 
18.         
])(max[maxarg }{ jidIDid idcid ij ???
 // 4. ScoreMax 
19. return cid; 
Figure 4: Identifying the candidate 
1. For each author ari ? AR 
2. select a set of query documents Qi  ? DRi  
3.  For each query qij ? Qi  
 // produce positive s-training examples 
4. select a set of documents from author ari  
 DRij ? DRi ? {qij} 
5. For each document drijk ? DRij  
6. produce an s-training example for drijk,  
 (Sim(drijk, qij), 1) 
 // produce negative s-training examples 
7. select a set of documents from the rest of authors  
 DRij,rest ? (DR1 ? ? ? DRn) ? DRi  
8. For each document drijk,rest ? DRij,rest  
9. produce an s-training example for drijk,rest,  
 (Sim(drijk,rest, qij), -1) 
Figure 1: Generating s-training examples 
//  Author ar1 ?  
// positive (1) s-training examples 
(Sim(dr111, q11), 1),  ?,  (Sim(dr11p, q11), 1)  
? 
(Sim(dr1k1, q1k), 1),   ?,   (Sim(dr1kp, q1k), 1)  
// negative (-1) s-training examples 
(Sim(dr111.rest, q11), -1),  ?,  (Sim(dr11u.rest, q11), -1)  
? 
(Sim(dr1k1.rest, q1k), -1),  ?,   (Sim(dr1ku.rest, q1k), -1)  
? 
Figure 2: s-training examples 
1127
can serve as a query or a non-query document. 
Then we need to compute all pairwise document 
similarities. If the number of training documents is 
m, the complexity is O(m2), which is both space 
and computation expensive. However, in practice, 
we don?t need all pairwise comparisons. Only a 
small subset is sufficient (see Section 7).  
Test document representation: Like training 
documents, test documents are represented as s-
vectors as well in the similarity space.  
Given a query q from author aq and a set of test 
documents DT, each test document dti is converted 
to a s-vector svi = Sim(dti, q). To reflect svi is com-
puted based on query q from author aq, a s-test 
case is thus represented as <(aq, q), (svi, ?)>.  
Training: A binary classifier is learned using the 
s-training data. Each s-training example is repre-
sented with (sv, y), where sv is an s-vector and y 
(? {1, -1}) is its class. Any supervised learning 
algorithm, e.g., SVM, can be applied.  
Testing: The classifier is applied to each s-test 
case <(aq, q), (svi, ?)> (where svi = S(dti, q)) to 
give it a class q-positive or q-negative. Note that 
the classifier is only applied on svi.  
In most cases, classification based on a single que-
ry is inaccurate. Using multiple queries of an au-
thor can classify much more accurately.   
4 Identify Userids of the Same Author 
We now expand the sketch of the two-step algo-
rithm in Section 1 based on the problem statement 
in Section 1. The algorithm is given in Figure 3.   
Lines 1-2 partitions the documents set Di of 
each idi in ID = {id1, id2, ?, idn}, the set of userids 
that we are working on. How to do the partition is 
flexible (see Section 7). Line 4 is the step 1 of 
candidate identification, and line 5 is the step2 of 
candidate confirmation. Lines 6-8 is the decision 
making of step 2 (see Section 1). Line 6 produced 
a classification score using the classifier described 
in Section 3. The key function here is candid-iden. 
Its algorithm is in Figure 4.  
The candid-iden function takes two arguments: 
the query userid idi and the whole set of userids 
ID. It classifies each sample ssjf in sample set Sj of 
idj ? ID-{idi} to positive (qi-positive) or negative 
(qi-negative) (lines 4, 5, 6). We then aggregate the 
classification results to determine which userid is 
likely to have the same author as idi.   
One simple aggregation method is voting. We 
count the total number of positive classifications of 
the sample documents of each userid in ID-{idi}. 
The userid idj with the highest count is the candi-
date cid which may share the same author as query 
idi. cid is returned as the candidate.  
There are also other methods, which can depend 
on what output value the classifier produces. Here 
we propose four methods including the voting 
method above. The other three methods requires 
the classifier to produces a prediction score, which 
reflects the positive and negative certainty. Many 
classification algorithms produce such a score. 
Here we use SVM. For each classification, SVM 
outputs a positive or negative score indicating the 
certainty that the test case is positive or negative.  
To save space, all four alternative methods are 
given in Figure 4. Line 2 initializes some variables 
for recording the aggregated values for the final 
decision making. The four methods are as follows:  
1). Voting: For each sample from userid idj, if it is 
classified as positive, one vote/count is added 
to pcount[idj]. The userid with the highest 
pcount is regarded as the candidate userid, cid 
(line 15). Note that the normalization is ap-
plied because the sizes of the sample sets Sj 
can be different for different userids. Lines 13 
and 14 mean that if all documents of all 
userids are classified as negative (pcount[idj] = 
0, which also implies psum[idj] = psqsum[idj] 
= 0), we use method 4).  
2). ScoreSum: This method works similarly to the 
voting method above except that instead of 
counting positive classifications, this method 
sums up all scores of positive classifications in 
psum[idj] for each userid (line 9). The decision 
is also made similarly (line 16). 
3). ScoreSqSum: This method works similarly to 
ScoreSum above except that it sums up the 
squared scores of positive classifications in 
psqsum[idj] for each userid (line 10). The deci-
sion is also made similarly (line 17). 
4). ScoreMax: This method works similarly to the 
voting method as well except that it finds the 
maximum classification score for the docu-
ments of each userid (lines 11 and 12). The 
decision is made in line 18. 
5 D-features 
We now compute s-features (similarity features) 
1128
for each non-query document based on a query 
document. Since s-features are calculated using d-
features of a non-query document and a query 
document, we thus discuss d-features first, which 
are extracted from each document itself. We em-
ploy 26 d-features in four categories: length d-
features, frequency based d-features, tf.idf based d-
features, and richness d-features. Although many 
features below have been used in various tasks 
before, our key contribution is solving a new prob-
lem based on a new learning formulation (LSS).  
Length d-feature: We derive three length d-
features from each raw document: (1) average 
sentence length (in terms of word count); (2) 
average word length (in terms of character count 
in one word); (3) average document length (in 
terms of word count in one document). 
Frequency based d-features: We extract lexical, 
syntactic, and stylistic tokens from the raw docu-
ments and the parsed syntactic trees to produce the 
following features:   
? Lexical tokens: word unigrams 
? Syntactic tokens: content-independent struc-
tures: POS n-grams (1 ? n ? 3) and rewrite rules 
(Halteren et al, 1996; Hirst and Feiguina, 2007). 
A rewrite rule is a combination of a node and 
its immediate constituents in a syntactic tree. 
For example, the rewrite rule for "the best 
book" is NP->DT+JJS+NN. 
? Common stylistic token: K-length word (1 ? K ? 
15), punctuations, and 157 function words 
(www.flesl.net/Vocabulary/SinglewordLists/fun
ctionwordlist.php). 
? Review specific stylistic tokens: These tokens 
reflect styles of reviews: all cap words, pairs of 
quotation marks, pairs of brackets, exclamatory 
marks, contractions, two or more consecutive 
non-alphanumeric characters, model auxilia-
ries (e.g., should, must), word ?recommend? or 
?recommended?, sentences with the first letter 
capitalized, sentences starting with This is (this 
is) or This was (this was). We then treat these 
tokens as pseudo-words and count their fre-
quency to form frequency d-features.  
TF-IDF based d-feature: For the tokens listed in 
the frequency based features above, we also com-
pute their tf.idf values. We list these two kinds of 
d-features separately because they will be used for 
different s-features later.  
Richness d-features: This is a set of vocabulary 
richness functions used to quantify the diversity of 
vocabulary in text (Holmes and Forsyth, 1995). In 
this paper, we apply them to the counts of word 
unigrams, POS n-grams (1 ? n ? 3), and rewrite 
rules. Here POS n-grams and rewrite rules are 
treated as pseudo-words. Let T be the total number 
of tokens (words or pseudo-words), and V(T) be 
the number of different tokens in a document, v be 
the highest frequency of occurrence of a token, and 
V(m, T) be the number of tokens which occur m 
times in the document. We use the following six 
richness measures (Yule, 1944; Burrows, 1992; 
Halteren et al, 1996) given in Table 1: Yule?s 
characteristic (K), Hapax dislegomena (S), Simp-
son?s index (D), Honor?s measure (R), Brunet?s 
measure (W), and Hapax legomena (H). They give 
us a set of richness d-features about word uni-
grams, POS n-grams, and rewrite rules. 
Table 1. Richness metrics 
2
4 1
2
( * ( , ) )
10 *
v
m
m V m T T
K T
?
?
?
?  (2, )( )V TS V T?
 
1
( *( 1)* ( , ))
*( 1)
v
m
m m V m T
D T T
?
?
? ?
?
 
100*log( )
1 (1, ) / ( )
TR V T V T? ?  
(1, )H V T?  ( ) , 0.17aV TW T a?? ? 
 
6 S-Features  
The extracted d-features are transformed into s-
features, which are a set of similarity functions on 
two documents. We adopt five types of s-features.  
Sim4 Length s-features: This is a set of four simi-
larity functions defined by us. They are used for d-
feature vectors of length. The four formulae are 
given in Table 2, where lwq. (lwd), lsq. (lsd), and  lrq. 
(lrd) denote the average word, sentence, and docu-
ment length respectively, either in query q or non-
query document d. They produce four s-features.  
Table 2. Sim4 for computing length s-features 
1/ (1 log(1 | |))wq wdl l? ? ?
 
1/ (1 log(1 | |))sq sdl l? ? ?
 
1/ (1 log(1 | |))rq rdl l? ? ? 
{ , , } { , , } { , , }
22( * ) / ( ) * ( )
m w s r m w s r m w s r
mq md mq mdl l l l
? ? ?
? ? ?
 
1129
Sim3 Sentence s-features: This is a set of three 
sentence similarity functions (Metzler et al, 2005). 
We apply them (called Sim3) to documents. Sim3 
s-features are used for frequency based d-features. 
The three formulae are given in Table 3, where f(t, 
s) is the frequency count of token t in a document s, 
and lq and ld are the average document length of 
the query and non-query document, respectively. 
Table 3. Sim3 for computing sentence s-features 
( , ) / ( ( , ) ( , ) ( , ))
t q d t q t d t q d
f t d f t q f t d f t d
? ? ? ? ? ?
? ?? ? ? ?
 
( , )
log ( )*( , ) ( , ) ( , ) ( , )
t q d
t q d
t q t d t q d
f t d
N
f t d f t q f t d f t d
? ?
? ?
? ? ? ?
? ?
?
? ? ?
 
1 * ( )*1 log(1 | |) 1 | ( , ) ( , ) |t q dq d
N idf t
l l f t q f t d? ?? ? ? ? ?? 
Sim7 Retrieval s-features: This is a set of seven 
similarity functions (Table 4) applicable to all fre-
quency based d-features. These functions were 
used in information retrieval (Cao et al, 2006).  
Table 4. Sim7 for computing retrieval s-features 
log( ( , ) 1)
t q d
f t d
? ?
??
 
| |log( 1)( , )t q d
D
f t d? ? ??
 
log( ( ))
t q d
idf t
? ??
 
( , )log( 1)| |t q d
f t d
d? ? ??
 
( , )log( * ( ) 1)| |t q d
f t d idf td? ? ??
 
log( 25 )BM score 
( , ) | |log( * 1)| | ( , )t q d
f t d D
d f t d? ? ??
 
 
In Table 4, f(t, d) denotes the frequency count of 
token t in a non-query document d, q denotes the 
query, D is the entire collection, |.| is the size of a 
set, and idf is the inverse document frequency. 
These 7 formulae can produce 7 s-features. 
SimC tf-idf s-feature: This is the cosine similarity 
used for d-vectors represented by the tf.idf based 
d-features. SimC tf-idf produces one s-feature. 
SimC Richness s-feature: This is also cosine sim-
ilarity. However, it is applied to the richness d-
feature vectors, and produces one s-feature. 
7 Experimental Evaluation  
We now evaluate the proposed approach and com-
pare it with baselines. All our experiments use the 
SVMperf classifier (Joachims, 2006). 
7.1  Experiment Setup 
Experiment Data: We use a set of reviews and 
their authors/reviewers from Amazon.com as our 
experiment data. We select the authors who have 
posted more than 30 reviews in the book category. 
After cleaning, we have 831 authors, 731 authors 
for training and 100 authors for testing. The num-
bers of reviews in the training and test author set 
are 59256 and 14308, respectively. We use the 
Stanford parser (Klein and Manning, 2003) to gen-
erate the grammar structure of review sentences 
for extracting syntactic d-features. Note that the 
authors here are in fact userids. However, since 
they are randomly selected from a large number of 
userids, the probability that two sampled userids 
belong to the same person is very small. Thus, it 
should be safe to assume that each userid here rep-
resents a unique author.  
Training data: We randomly choose 1 (one) re-
view for each author as the query and all of his/her 
other reviews as q-positive reviews. The q-
negative reviews consist of reviews randomly se-
lected from the other 730 authors, two reviews per 
author. We also tried to use more queries from 
each author, but they make little difference.  
Test data: The test authors are all unseen, i.e., 
their reviews have not been used in training. We 
prepare the test case for each author as follows.  
We first divide the reviews of each author into 
two equal subsets. The purpose is to simulate the 
situation where there are two userids idia and idib 
from the same author ai. Our objective is that giv-
en one userid idia and its query set, we want to find 
the other userid idib from the same author.   
For the review subset of idia (or idib), we ran-
domly select 9 reviews as the query set and anoth-
er 10 reviews as the sample set for the userid. The 
two sets are disjoint. We don?t use more queries or 
sample reviews from each author since in the re-
view domain most authors do not have many re-
views (Jindal and Liu, 2008). In the experiments, 
we will vary the number of test userids, the num-
ber of queries, and the number of samples. We use 
the following format to describe each test data: 
T<n>_Q<n>S<n>, where T denotes the total num-
ber of test userids, Q the query set and S the sam-
ple set, and <n> a number. For example, 
T50_Q9S10 stands for a test data with 50 userids, 
and for each userid, 9 reviews are selected as que-
ries and 10 reviews are selected as samples. * rep-
1130
resents a wildcard whose value we can vary. 
Note that we use this ?artificial? data rather than 
manually labeled data for our experiments because 
it is very hard to reliably label any gold-standard 
data manually in this case. The problem is similar 
to labeling fake reviews. In the fake review detec-
tion research, researchers have manually label fake 
reviews and reviewers (Yoo and Gretzel 2009; 
Lim et al, 2010; Li et al, 2011; Wang et al, 2011). 
However, based on the actual fake reviews written 
using Amazon Mechanical Turk, Ott et al (2011) 
have showed that the accuracy of human labeling 
of fake reviews is very poor. We also believe that 
our test data is realistic for evaluation as we can 
image that the two sets of reviews are from two 
accounts (userids) of the same author (reviewer).   
Two types of experiments: For each author with 
two userids, we conduct two types of tests.   
? Type I: Identify two userids belong to the same 
author. The experiment runs iteratively to test 
every userid. In each iteration, we plant one 
userid of an author in the test set and use the 
other userid of the same author as the query 
userid. That is, in the ith run, the test data con-
sist of the following two components: 
1.  Query userid idia and its query set Qia 
2. Test userids {id1a, ?, id(i-1)a, idib, ?, idma} 
and their corresponding sample review sets 
{S1a, ?, S(i-1)a, Sib, ?, Sma}.  
Note that the query userid idia and the test 
userid idib are from the same author. Our objec-
tive is to use Qia to find idib through Sib. 
Evaluation measure: We use precision, recall, 
and F1 score to evaluate Type I experiments as 
we want to identify all matching pairs. The er-
rors are ?no pair? and ?wrong pair? found.  
? Type II: Type II experiments test the cases 
when no pair exists. That is, we do not plant 
any matching userid for the query userid. Then, 
the algorithm should not find anything. For the 
ith run, the test data has these components: 
1.  Query userid idia and its query set Qia 
2.Test userids {id1a, ?, id(i-1)a, id(i+1)a, ?, idma} 
and their sample review sets {S1a, ?, S(i-1)a, 
S(i+1)a, ?, Sma}. idib is not planted.  
Evaluation measure: Here we cannot use pre-
cision and recall because we are not trying to 
find any pairs. We thus use accuracy as our 
measure. For each idi, if no pair is found, it is 
correct. If a pair is found, it is wrong.  
Baseline methods:  As mentioned eariler, there 
are  only two works that tried to identify multi-id 
users. The first is that in (Chen et al, 2004). 
However, as we discussed in related work, their 
approach is not applicable to reviews. The other is 
that in (Novak et al, 2004), which used clustering 
but assumed that the number of actual authors (or 
clusters) is known. This is unrealistic in practice. 
Thus we designed three new baselines:  
TSL: This baseline is based on the traditional su-
pervised learning (TSL). We use it to evaluate 
how the traditional approach performs in the 
original feature space. In this case, each docu-
ment in TSL has to be represented as a vector of 
d-features or traditional n-gram features. For 
each test userid id, we build a SVM classifier 
based on the one vs. all strategy. That is, for 
training we use id?s queries in T*_Q*S10 as the 
positive documents, and all queries of the other 
test userids (e.g., 99 userids if the test data has 
100 userids) as the negative documents. Note 
that TSL cannot use the 731 userids for training 
as in LSS because they do not appear in the test 
data. In testing, userid id?s sample (non-query) 
documents in T*_Q*S10 are used as positive 
documents, and the sample documents of all oth-
er test userids are used as negative documents. 
SimUG: It uses the word unigrams to compare the 
cosine similarity of queries and samples. Cosine 
similarity with unigrams is the most widely used 
document similarity measure.  
SimAD: It uses all d-features to compare the cosine 
similarity of queries and samples.  
For both SimUG and SimAD, their cosine simi-
larity values are used in place of SVM scores of 
LSS or TSL. We then apply the same 4 strategies 
to decide the final author attribution except voting 
as cosine similarity cannot classify.  
7.2  Results and analysis 
1) Effects of positive/total ratio in training set: 
Since our data is highly skewed and too many neg-
ative cases may not be good for classification, we 
thus performed this experiment to find a good ratio.  
Table 5 shows the results for Type I experiments. 
From Table 5, we can see that the results are high-
ly accurate. Even for 100 userids, our method can 
correctly identify 85% cases. Here we use the data 
sets T*_Q9S10 and the decision method is 
ScoreSqSum, which produces the best result. The 
1131
results for Type II experiments (Table 6) are also 
accurate. In most cases, the values of accuracy are 
higher than 90%. For all our experiments below, 
we use the model/classifier trained with 0.4 ratio. 
Table 5. Positive(p)/total(t) ratio in training (Type I) 
 
F1 
 
p/t 10 30 50 80 100 
0.3 100.00 84.62 86.36 88.89 83.72 
0.4 100.00 91.91 90.11 88.89 85.71 
0.5 100.00 90.91 91.30 88.89 87.01 
0.6 94.74 82.35 87.64 85.71 86.36 
0.7 94.74 84.62 86.36 86.53 87.64 
Table 6. Positive(p)/total(t) ratio in training (Type II) 
 
Accuracy 
 
p/t 10 30 50 80 100 
0.3 90.00 90.00 92.00 97.50 94.00 
0.4 90.00 90.00 94.00 98.75 95.00 
0.5 80.00 86.67 94.00 97.75 95.00 
0.6 80.00 86.67 90.00 93.75 92.00 
0.7 80.00 86.67 90.00 95.00 92.00 
(2) Effects of different decision methods: We 
show the results of the four proposed decision 
methods: Voting, ScoreSum, ScoreSqSum, and 
ScoreMax, using our basic data of T*_Q9S10 with 
varied number of test userids. Figure 5(a) shows 
that ScoreSqSum is the best for Type I experi-
ments. Figure 5(b) shows ScoreMax is the best for 
Type II, but ScoreSqSum also does very well. Be-
low, ScoreSqSum is used as our default method 
because Type I is more important than Type II.  
 
              (a)  Type I                   (b) Type II 
Figure 5: Effect of different decision methods 
(3) Effects of number of queries per userid: 
Figure 6 shows the results of different numbers of 
queries. We see that more queries give better re-
sults, which is easy to understand because more 
queries give more information. We use 9 queries 
per userid in all other experiments. 
 
 (a)  Type I                   (b) Type II 
Figure 6: Effect of different numbers of queries 
(4) Effects of number of samples per userid: We 
tried 2, 4, 6, 8, 10 samples per userid. Although 
there are some fluctuations for Type II (Fig.7(b)), 
we can see an upward trend for Type I in Fig. 7(a). 
This indicates that more sample documents give 
better results in general. The main reason again is 
that more samples from a userid give more identi-
fying information about the userid. We use 10 test 
documents (samples) per userid in all experiments. 
 
 (a)  Type I                   (b) Type II 
Figure 7: Effect of different number of samples 
(5) Impact of individual s-feature sets: Here we 
show the effectiveness of individual s-feature sets. 
From Table 7, we see that Sim7Retrieval s-
features are extremely important for Type I test. 
Removing Sim7Retrieval causes about 10% to 
20% F1 score drop on different datasets. SimCT-
fidf s-features are also useful. The impacts of other 
s-features are small. The same applies to Type II 
test (Table 8). On average, using all features is the 
best. Hence we use all features in all other experi-
ments above.  
Table 7. Using different s-features (Type I) 
T*_Q9S10 F1 
10  
F1 
30 
F1 
50  
F1 
80 
F1 
100  
All features 100.00 90.91 90.11 88.89 85.71 
No Sim4Len 100.00 88.89 86.36 87.32 85.06 
No SimCRichness 100.00 88.89 91.30 88.89 85.71 
No SimCTfidf 100.00 80.00 86.36 86.53 83.72 
No Sim7Retrieval 82.35 72.34 75.80 78.79 77.30 
No Sim3Sent 94.74 84.62 86.36 88.11 87.64 
Table 8. Using different s-features (Type II) 
T*_Q9S10 Acc. 
10  
Acc 
30 
Acc. 
50  
Acc 
80 
Acc. 
100  
All features 90.00 90.00 94.00 98.75 99.00 
No Sim4Len 90.00 93.33 96.00 96.25 96.00 
No SimCRichness 90.00 90.00 94.00 96.25 96.00 
No SimCTfidf 90.00 86.67 94.00 93.75 97.00 
No Sim7Retrieval 80.00 90.00 94.00 94.00 96.00 
No Sim3Sent 90.00 93.33 92.00 98.75 93.00 
(6) Comparing with the three baselines: Similar 
to our method, the training data for TSL is highly 
skewed as it uses a one-vs.-all strategy. Hence we 
also investigate the effect of p/t ratio in training for 
TSL. Results show that 0.4 ratio is the best setting. 
1132
Thus this setting is adopted for TSL in the follow-
ing experiments. Note that we cannot conduct p/t 
ratio experiments for SimAD and SimUG as they 
are unsupervised methods. We use ScoreMax for 
TSL, ScoreSqSum for SimUG and SimAD, re-
spectively, since they perform the best for their 
corresponding approaches. Tables 9 and 10 show 
the results of our LSS method and the baseline 
methods for Type I and II tests respectively. For 
TSL, we use all d-features. Unigram features gave 
TSL much worse results and are thus not included 
here.  
Table 9: Comparison with baselines (Type I) 
 10 30 50 80 100 
LSS Pre 100.00 100.00 100.00 100.00 98.68 
Rec 100.00 83.33 82.00 80.00 75.76 
F1 100.00 90.91 90.11 88.89 85.71 
TSL Pre 50.00 50.00 33.33 0.00 0.00 
Rec 11.11 3.45 2.08 0.00 0.00 
F1 18.18 6.45 3.92 0.00 0.00 
SimUG Pre 100.00 100.00 100.00 100.00 100.00 
Rec 70.00 46.67 48.00 48.75 43.00 
F1 82.35 63.64 64.86 65.55 60.14 
SimAD Pre 100.00 75.00 100.00 33.33 0.00 
Rec 20.00 10.35 2.00 1.28 0.00 
F1 33.33 18.18 3.92 2.47 0.00 
Table 10: Comparison with baselines (Type II) 
Accuracy 10 30 50 80 100 
LSS 90.00 90.00 94.00 98.75 95.00 
TSL 90.00 96.67 98.00 98.75 99.00 
SimUG 96.00 93.33 96.00 96.25 97.00 
SimAD 90.00 96.67 98.00 98.75 99.00 
From Tables 9 and 10, we can make the follow-
ing observations. 
? For Type I, F1 scores of LSS are markedly bet-
ter than those of the three baselines. The results 
of SimUG also drop more quickly than LSS 
with the increased number of userids. SimAD?s 
results are extremely poor. These show that 
LSS is much more superior to the unsupervised 
methods. TSL performed the worst, indicating 
that traditional supervised learning is inappro-
priate for this task. There are two main reasons: 
First, for one vs. all learning, the negative train-
ing data actually contain positive documents 
which are written by the same author using an-
other userid as the positive data, which confus-
es the classifier. Second, TSL is unable to build 
an accurate classifier using the small number of 
queries (which are training data). In contrast, 
our LSS method can exploit a large number of 
other authors who do not have to appear in test-
ing and thus achieves the huge improvements.  
? For Type II, LSS also performs very well. The 
baselines perform well too and even better, 
which is not surprising because they have diffi-
culty in finding matching pairs for Type I. 
Since Type II datasets have no author with mul-
tiple userids, naturally the baselines will do 
well for Type II. But that is useless because 
when there are authors with multiple usersids 
(Type I), they are unable to find them well.  
In summary, we can conclude that for Type I tests 
(there are authors with multiple userids), LSS is 
dramatically better than all baseline methods. For 
Type II tests (there is no author with multiple 
userids), it also performs very well.   
8 Conclusion  
This paper proposed a novel method to identify 
userids that may be from the same author. The 
core of the method is a supervised learning method 
which learns in a similarity space rather than the 
document space. This learning method is able to 
better determine whether a document may be writ-
ten by a known author, although no document 
from the author has been used in training (as long 
as we have some documents from the author to 
serve as queries). To the best of our knowledge, 
there is no existing method based on linguistic 
analysis for solving the problem. Our experimental 
results based on a large number of reviewers and 
their reviews show that the proposed algorithm is 
highly accurate. It outperforms three baselines 
markedly.  
 
Acknowledgements 
We are grateful to the anonymous reviewers for 
their thoughtful comments. Tieyun Qian was sup-
ported in part by the NSFC Projects (61272275, 
61272110, 61202036), and the 111 Project 
(B07037). Bing Liu was supported in part by a 
grant from National Science Foundation (NSF) 
under no. IIS-1111092. 
 
References  
Shlomo Argamon and Shlomo Levitan. 2004. 
Measuring the usefulness of function words for 
authorship attribution. Literary and Linguistic 
Computing 1-3. 
1133
Shlomo Argamon, Casey Whitelaw, Paul Chase, 
Sobhan Raj Hota, Navendu Garg, and Shlomo 
Levitan. 2007. Stylistic text classification using 
functional lexical features: Research articles. J. 
Am. Soc. Inf. Sci. Technol. 58:802-822.  
John F. Burrows. 1992. Not unless you ask nicely: 
The interpretative nexus between analysis and 
information. Literary and Linguistic Computing 
7:91-109. 
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou 
Huang, and Hsiao-Wuen Hon. 2006. Adapting 
ranking svm to document retrieval. Proc. of 
SIGIR, Pages 186-193. 
Hung-Ching Chen, Mark K. Goldberg, Malik 
Magdon-Ismail. 2004. Identifying multi-ID users 
in open forums. Intelligence and Security 
Informatics, Pages 176-186. 
Joachim Diederich, J?rg Kindermann, Edda 
Leopold, and Gerhard Paass, 2000. Authorship 
attribution with support vector machines. 
Applied Intelligence 19:109-123. 
Hugo Jair Escalante, Thamar Solorio, and Manuel 
Montes-y-G?mez. 2011. Local histograms of 
character n-grams for authorship attribution. 
Proc. of ACL-HLT, Volume I: 288-298. 
Song Feng, Longfei Xing, Anupam Gogar, and 
Yejin Choi. 2012. Distributional Footprints of 
Deceptive Product Reviews.  Proc. of ICWSM.  
Michael Gamon. 2004. Linguistic correlates of 
style: authorship classification with deep 
linguistic analysis features. Proc. of Coling. 
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 
2005. Segmenting documents by stylistic 
character. Natural Language Engineering, 
11:397-415.  
Jack Grieve. 2007. Quantitative authorship 
attribution: An evaluation of techniques. Literary 
and Linguistic Computing 22:251-270. 
Hans van Halteren, Fiona Tweedie, and Harald 
Baayen. 1996. Outside the cave of shadows: 
using syntactic annotation to enhance authorship 
attribution. Literary and Linguistic Computing 
11:121-132. 
Steffen Hedegaard and Jakob Grue Simonsen. 
2011. Lost in translation: authorship attribution 
using frame semantics. Proc. of ACL-HLT, short 
papers - Volume 2, 65-70. 
Graeme Hirst and Ol?ga Feiguina. 2007. Bigrams 
of syntactic labels for authorship discrimination 
of short texts. Literary and Linguistic Computing 
22:405-417. 
David I. Holmes and R. S. Forsyth. 1995. The 
Federalist Revisited: New Directions in 
Authorship Attribution, Literary and Linguistic 
Computing, 10(2): 111-127. 
David L. Hoover. 2001. Statistical stylistics and 
authorship attribution: an empirical 
investigation. Literary and Linguistic Computing 
16:421-424. 
Nitin Jindal and Bing Liu. 2008. Opinion Spam 
and Analysis. Proc. of WSDM, California, USA. 
Thorsten Joachims. 2006. Training linear svms in 
linear time. Proc. of KDD. 
Sangkyum Kim, Hyungsul Kim, Tim Weninger, 
Jiawei Han, and Hyun Duk Kim. 2011. 
Authorship classification: a discriminative 
syntactic tree mining approach. Proc. of SIGIR, 
Pages 455-464. 
Dan Klein, and Christopher D. Manning. 2003. 
Accurate unlexicalized parsing. Proc. of ACL, 
423-430.  
Moshe Koppel and Jonathan Schler. 2004. 
Authorship verification as a one-class 
classification problem. Proc. of ICML.  
Moshe Koppel, Jonathan Schler, Shlomo 
Argamon. 2011. Authorship attribution in the 
wild. Lang Resources & Evaluation, 45:83-94 
Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan 
Zhu. 2011. Learning to identify review Spam. 
Proc. of IJCAI. 
Hang Li. 2011. Learning to Rank for Information 
Retrieval and Natural Language Processing. 
Morgan & Claypool publishers. 
Jiexun Li, Rong Zheng, and Hsinchun Chen. 2006. 
From fingerprint to writeprint. Communications 
of the ACM, 49:76-82. 
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing 
Liu, Hady W. Lauw. 2010. Detecting product 
review spammers using rating behaviors. Proc. 
of CIKM, 2010. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining, Morgan & Claypool publishers.  
1134
Tieyan Liu. 2011. Learning to Rank for Infor-
mation Retrieval. Springer. 
Kim Luyckx, Walter Daelemans. 2008. Authorship 
Attribution and Verification with Many Authors 
and Limited Data. Proc. of Coling, pages 513-
520. 
David Madigan, Alexander Genkin, David D. 
Lewis, Shlomo Argamon, Dmitriy Fradkin, and 
Li Ye. 2005. Author Identification on the Large 
Scale. Proc. of CSNA.  
Donald Metzler, Yaniv Bernstein, W. Bruce Croft, 
Alistair Moffat, and Justin Zobel. 2005. 
Similarity measures for tracking information 
flow. Proc. of CIKM. Pages 517-524. 
Frederick Mosteller, David Lee Wallace. 1964. 
Inference and disputed authorship: The 
Federalist. Addison-Wesley.  
Arjun Mukherjee, Bing Liu, and Natalie Glance. 
2012. Spotting Fake Reviewer Groups in Con-
sumer Reviews. Proc. of WWW, Pages 191-200. 
Arvind Narayanan, Hristo Paskov, Neil Zhenqiang 
Gong, et al 2012. On the feasibility of internet-
scale author identification. Proceedings of the 
2012 IEEE Symposium on Security and Privacy. 
Pages 300-314 
Jasmine Novak, Prabhakar Raghavan, Andrew 
Tomkins. 2004. Anti-aliasing on the web. Proc. 
of WWW, Pages 30-39 
Myle Ott, Yejin Choi, Claire Cardie, Jeffrey T. 
Hancock. 2011. Finding Deceptive Opinion 
Spam by Any Stretch of the Imagination. Proc. 
of ACL. 
Myle Ott, Claire Cardie, Jeffrey T. Hancock. 2012. 
Estimating the prevalence of deception in online 
review communities. Proc. of WWW. 
Fuchun Peng, Dale Schuurmans, Shaojun Wang, 
and Vlado Keselj. 2003. Language independent 
authorship attribution using character level 
language models. Proc. of EACL, Pages 267-
274. 
Conrad Sanderson and Simon Guenter. 2006. 
Short text authorship attribution via sequence 
kernels, markov chains and author unmasking: 
an investigation. Proc. of EMNLP, Pages 482-
491. 
Yanir Seroussi, Fabian Bohnert, Ingrid Zukerman. 
2012. Authorship Attribution with Author-
aware Topic Models. Proc. of ACL, 2:264-269. 
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, 
Manuel Montes y G?omez. 2011. Modality 
Specific Meta Features for Authorship 
Attribution in Web Forum Posts. Proc. of 
IJCNLP, Pages 156-164.  
Efstathios Stamatatos. 2009. A Survey of Modern 
Authorship Attribution Methods. Journal of the 
American Society for Information Science and 
Technology, 60(3):538-556, Wiley. 
Efstathios Stamatatos, George Kokkinakis, and 
Nikos Fakotakis. 2000. Automatic text 
categorization in terms of genre and author. 
Comput. Linguist. 26:471-495. 
?zlem Uzuner and Boris Katz. 2005. A 
comparative study of language models for book 
and author recognition. Proc. of IJCNLP, Pages 
969-980.  
Vladimir N. Vapnik. 1998. Statistical Learning 
Theory. Wiley-Interscience, NY.  
O. de Vel, A. Anderson, M. Corney and G. 
Mohay. 2001. Mining Email Content for Author 
Identification Forensics. Sigmod Record, 30:55-
64. 
Kyung-Hyan Yoo and Ulrike Gretzel. 2009. 
Comparison of Deceptive and Truthful Travel 
Reviews. Information and Communication 
Technologies in Tourism, Pages 37-47. 
Georgy Udnv Yule. 1944. The statistical study of 
literary vocabulary. Cambridge University 
Press. 
Guan Wang, Sihong Xie, Bing Liu, Philip S. Yu. 
2011. Review Graph based Online Store 
Review Spammer Detection. Proc. of ICDM. 
Ying Zhao and Justin Zobel. 2005. Effective and 
scalable authorship attribution using function 
words. Proceeding of Information Retrival 
Technology, Pages 174-189.  
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan 
Huang. 2006. A framework for authorship iden-
tification of online messages: Writing style fea-
tures and classification techniques. Journal of 
the American Society of Information Science 
and Technology 57:378-393. 
1135
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1655?1667,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Domain Knowledge in Aspect Extraction 
 
 
Zhiyuan Chen, Arjun Mukherjee, 
Bing Liu 
Meichun Hsu, Malu Castellanos, 
Riddhiman Ghosh 
University of Illinois at Chicago HP Labs 
Chicago, IL 60607, USA Palo Alto, CA 94304, USA 
{czyuanacm,arjun4787}@gmail.com, 
liub@cs.uic.edu 
{meichun.hsu, malu.castellanos, 
riddhiman.ghosh}@hp.com 
 
 
 
Abstract 
Aspect extraction is one of the key tasks in 
sentiment analysis. In recent years, statistical 
models have been used for the task. However, 
such models without any domain knowledge 
often produce aspects that are not interpreta-
ble in applications. To tackle the issue, some 
knowledge-based topic models have been 
proposed, which allow the user to input some 
prior domain knowledge to generate coherent 
aspects. However, existing knowledge-based 
topic models have several major shortcom-
ings, e.g., little work has been done to incor-
porate the cannot-link type of knowledge or 
to automatically adjust the number of topics 
based on domain knowledge. This paper pro-
poses a more advanced topic model, called 
MC-LDA (LDA with m-set and c-set), to ad-
dress these problems, which is based on an 
Extended generalized P?lya urn (E-GPU) 
model (which is also proposed in this paper).  
Experiments on real-life product reviews 
from a variety of domains show that MC-
LDA outperforms the existing state-of-the-art 
models markedly. 
1 Introduction 
In sentiment analysis and opinion mining, aspect 
extraction aims to extract entity aspects or features 
on which opinions have been expressed (Hu and 
Liu, 2004; Liu, 2012). For example, in a sentence 
?The picture looks great,? the aspect is ?picture.? 
Aspect extraction consists of two sub-tasks: (1) 
extracting all aspect terms (e.g., ?picture?) from 
the corpus, and (2) clustering aspect terms with 
similar meanings (e.g., cluster ?picture? and ?pho-
to? into one aspect category as they mean the 
same in the domain ?Camera?). In this work, we 
adopt the topic modeling approach as it can per-
form both sub-tasks simultaneously (see ? 2). 
Topic models, such as LDA (Blei et al, 2003), 
provide an unsupervised framework for extracting 
latent topics in text documents. Topics are aspect 
categories (or simply aspects) in our context. 
However, in recent years, researchers have found 
that fully unsupervised topic models may not pro-
duce topics that are very coherent for a particular 
application. This is because the objective functions 
of topic models do not always correlate well with 
human judgments and needs (Chang et al, 2009). 
To address the issue, several knowledge-based 
topic models have been proposed. The DF-LDA 
model (Andrzejewski et al, 2009) incorporates 
two forms of prior knowledge, also called two 
types of constraints: must-links and cannot-links. 
A must-link states that two words (or terms) 
should belong to the same topic whereas a cannot-
link indicates that two words should not be in the 
same topic. In (Andrzejewski et al, 2011), more 
general knowledge can be specified using first-
order logic. In (Burns et al, 2012; Jagarlamudi et 
al., 2012; Lu et al, 2011; Mukherjee and Liu, 
2012), seeded models were proposed. They enable 
the user to specify prior knowledge as seed 
words/terms for some topics. Petterson et al (2010) 
also used word similarity as priors for guidance.  
However, none of the existing models is capable 
of incorporating the cannot-link type of knowledge 
except DF-LDA (Andrzejewski et al, 2009). Fur-
thermore, none of the existing models, including 
DF-LDA, is able to automatically adjust the num-
ber of topics based on domain knowledge. The 
domain knowledge, such as cannot-links, may 
change the number of topics. There are two types 
of cannot-links: consistent and inconsistent with 
the domain corpus. For example, in the reviews of 
1655
domain ?Computer?, a topic model may generate 
two topics Battery and Screen that represent two 
different aspects. A cannot-link {battery, screen} 
as the domain knowledge is thus consistent with 
the corpus. However, words Amazon and Price 
may appear in the same topic due to their high co-
occurrences in the Amazon.com review corpus. To 
separate them, a cannot-link {amazon, price} can 
be added as the domain knowledge, which is in-
consistent with the corpus as these two words have 
high co-occurrences in the corpus. In this case, the 
number of topics needs to be increased by 1 since 
the mixed topic has to be separated into two indi-
vidual topics Amazon and Price. Apart from the 
above shortcoming, earlier knowledge-based topic 
models also have some major shortcomings: 
Incapability of handling multiple senses: A 
word typically has multiple meanings or senses. 
For example, light can mean ?of little weight? or 
?something that makes things visible.? DF-LDA 
cannot handle multiple senses because its defini-
tion of must-link is transitive. That is, if A and B 
form a must-link, and B and C form a must-link, it 
implies a must-link between A and C, indicating A, 
B, and C should be in the same topic. This case 
also applies to the models in (Andrzejewski et al, 
2011), (Petterson et al, 2010), and (Mukherjee and 
Liu, 2012). Although the model in (Jagarlamudi et 
al., 2012) allows multiple senses, it requires that 
each topic has at most one set of seed words (seed 
set), which is restrictive as the amount of 
knowledge should not be limited. 
Sensitivity to the adverse effect of knowledge: 
When using must-links or seeds, existing models 
basically try to ensure that the words in a must-
link or a seed set have similar probabilities under a 
topic. This causes a problem: if a must-link com-
prises of a frequent word and an infrequent word, 
due to the redistribution of probability mass, the 
probability of the frequent word will decrease 
while the probability of the infrequent word will 
increase. This can harm the final topics because 
the attenuation of the frequent (often domain im-
portant) words can result in some irrelevant words 
being ranked higher (with higher probabilities). 
To address the above shortcomings, we define 
m-set (for must-set) as a set of words that should 
belong to the same topic and c-set (cannot-set) as a 
set of words that should not be in the same topic. 
They are similar to must-link and cannot-link but 
m-sets do not enforce transitivity. Transitivity is 
the main cause of the inability to handle multiple 
senses. Our m-sets and c-sets are also more con-
cise providing knowledge in the context of a set. 
As in (Andrzejewski et al, 2009), we assume that 
there is no conflict between m-sets and c-sets, i.e., 
if ?1  is a cannot-word of ?2  (i.e., shares a c-set 
with ?2), any word that shares an m-set with ?1 is 
also a cannot-word of ?2. Note that knowledge as 
m-sets has also been used in (Chen et al, 2013a) 
and (Chen et al, 2013b). 
We then propose a new topic model, called MC-
LDA (LDA with m-set and c-set), which is not on-
ly able to deal with c-sets and automatically adjust 
the number of topics, but also deal with the multi-
ple senses and adverse effect of knowledge prob-
lems at the same time. For the issue of multiple 
senses, a new latent variable ? is added to LDA to 
distinguish multiple senses (? 3). Then, we employ 
the generalized P?lya urn (GPU) model 
(Mahmoud, 2008) to address the issue of adverse 
effect of knowledge (? 4). Deviating from the 
standard topic modeling approaches, we propose 
the Extended generalized P?lya urn (E-GPU) 
model (? 5). E-GPU extends the GPU model to 
enable multi-urn interactions. This is necessary for 
handling c-sets and for adjusting the number of 
topics. E-GPU is the heart of MC-LDA. Due to the 
extension, a new inference mechanism is designed 
for MC-LDA (? 6). Note that E-GPU is generic 
and can be used in any appropriate application. 
In summary, this paper makes the following 
three contributions: 
1. It proposed a new knowledge-based topic mod-
el called MC-LDA, which is able to use both 
m-sets and c-sets, as well as automatically ad-
just the number of topics based on domain 
knowledge. At the same time, it can deal with 
some other major shortcomings of early exist-
ing models. To our knowledge, none of the ex-
isting knowledge-based models is as compre-
hensive as MC-LDA in terms of capabilities. 
2. It proposed the E-GPU model to enable multi-
urn interactions, which enables c-sets to be nat-
urally integrated into a topic model. To the best 
of our knowledge, E-GPU has not been pro-
posed and used before.  
3. A comprehensive evaluation has been conduct-
ed to compare MC-LDA with several state-of-
the-art models. Experimental results based on 
both qualitative and quantitative measures 
demonstrate the superiority of MC-LDA. 
1656
2  Related Work 
Sentiment analysis has been studied extensively in 
recent years (Hu and Liu, 2004; Pang and Lee, 
2008; Wiebe and Riloff, 2005; Wiebe et al, 2004). 
According to (Liu, 2012), there are three main ap-
proaches to aspect extraction: 1) Using word fre-
quency and syntactic dependency of aspects and 
sentiment words for extraction (e.g., Blair-
goldensohn et al, 2008; Hu and Liu, 2004; Ku et 
al., 2006; Popescu and Etzioni, 2005; Qiu et al, 
2011; Somasundaran and Wiebe, 2009; Wu et al, 
2009; Yu et al, 2011; Zhang and Liu, 2011; 
Zhuang et al, 2006); 2) Using supervised se-
quence labeling/classification (e.g., Choi and 
Cardie, 2010; Jakob and Gurevych, 2010; 
Kobayashi et al, 2007; Li et al, 2010); 3) Topic 
models (Branavan et al, 2008; Brody and Elhadad, 
2010; Fang and Huang, 2012; Jo and Oh, 2011; 
Kim et al, 2013; Lazaridou et al, 2013; Li et al, 
2011; Lin and He, 2009; Lu et al, 2009, 2012, 
2011; Lu and Zhai, 2008; Mei et al, 2007; 
Moghaddam and Ester, 2011; Mukherjee and Liu, 
2012; Sauper et al, 2011; Titov and McDonald, 
2008; Wang et al, 2010, 2011; Zhao et al, 2010). 
Other approaches include shallow semantic pars-
ing (Li et al, 2012b), bootstrapping (Xia et al, 
2009), Non-English techniques (Abu-Jbara et al, 
2013; Zhou et al, 2012), graph-based representa-
tion (Wu et al, 2011), convolution kernels 
(Wiegand and Klakow, 2010) and domain adap-
tion (Li et al, 2012). Stoyanov and Cardie (2011), 
Wang and Liu (2011), and Meng et al (2012) 
studied opinion summarization outside the reviews. 
Some other works related with sentiment analysis 
include (Agarwal and Sabharwal, 2012; Kennedy 
and Inkpen, 2006; Kim et al, 2009; Mohammad et 
al., 2009). 
In this work, we focus on topic models owing to 
their advantage of performing both aspect extrac-
tion and clustering simultaneously. All other ap-
proaches only perform extraction. Although there 
are several related works on clustering aspect 
terms (e.g., Carenini et al, 2005; Guo et al, 2009; 
Zhai et al, 2011), they all assume that the aspect 
terms have been extracted beforehand. We also 
notice that some aspect extraction models in sen-
timent analysis separately discover aspect words 
and aspect specific sentiment words (e.g., Sauper 
and Barzilay, 2013; Zhao et al, 2010). Our pro-
posed model does not separate them as most sen-
timent words also imply aspects and most adjec-
tives modify specific attributes of objects. For ex-
ample, sentiment words expensive and beautiful 
imply aspects price and appearance respectively. 
Regarding the knowledge-based models, be-
sides those discussed in ? 1, the model (Hu et al, 
2011) enables the user to provide guidance interac-
tively. Blei and McAuliffe (2007) and Ramage et 
al. (2009) used document labels in supervised set-
ting. In (Chen et al, 2013a), we proposed MDK-
LDA to leverage multi-domain knowledge, which 
serves as the basic mechanism to exploit m-sets in 
MC-LDA. In (Chen et al, 2013b), we proposed a 
framework (called GK-LDA) to explicitly deal 
with the wrong knowledge when exploring the 
lexical semantic relations as the general (domain 
independent) knowledge in topic models. But 
these models above did not consider the 
knowledge in the form of c-sets (or cannot-links). 
The generalized P?lya urn (GPU) model 
(Mahmoud, 2008) was first introduced in LDA by 
Mimno et al (2011). However, Mimno et al (2011) 
did not use domain knowledge. Our results in ? 7 
show that using domain knowledge can signifi-
cantly improve aspect extraction. The GPU model 
was also employed in topic models in our work of 
(Chen et al, 2013a, 2013b). In this paper, we pro-
pose the Extended GPU (E-GPU) model. The E-
GPU model is more powerful in handling complex 
situations in dealing with c-sets. 
3 Dealing with M-sets and Multiple Senses 
Since the proposed MC-LDA model is a major 
extension to our earlier work in (Chen et al, 
2013a), which can deal with m-sets, we include 
this earlier work here as the background.     
To incorporate m-sets and deal with multiple 
senses of a word, the MDK-LDA(b) model was 
proposed in (Chen et al, 2013a), which adds a 
new latent variable ? into LDA. The rationale here 
is that this new latent variable ? guides the model 
to choose the right sense represented by an m-set. 
The generative process of MDK-LDA(b) is (the 
notations are explained in Table 1): 
                     ?  ~ ?????????(?) 
                     ??|??  ~ ???????????(??) 
                     ? ~ ?????????(?) 
                     ??|??,? ~ ???????????????? 
                     ? ~ ?????????(?) 
                     ??|?? , ??,? ~ ???????????????,??? 
1657
The corresponding plate is shown in Figure 1. Un-
der MDK-LDA(b), the probability of word ? giv-
en topic ?, i.e., ??(?), is given by: 
 ??(?) = ? ??(?) ? ??,?(?)
?
?=1    (1) 
where ??(?)  denotes the probability of m-set ? 
occurring under topic ? and ??,?(?) is the proba-
bility of word ? appearing in m-set ? under topic ?. 
According to (Chen et al, 2013a), the condi-
tional probability of Gibbs sampler for MDK-
LDA(b) is given by (see notations in Table 1): 
    ???? = ?, ?? = ? ???? , ??? ,?,?,?, ?? ? 
??,?
?? + ?
? ???,??
?? + ?????=1
?
??,?
?? + ?
? ???,??
?? + ?????=1
?
??,?,??
?? + ??
? ???,?,??
?? + ???
?
??=1
 (2) 
The superscript ??  denotes the counts excluding 
the current assignments (?? and ??) for word ??. 
4 Handling Adverse Effect of Knowledge 
4.1 Generalized P?lya urn (GPU) Model 
The P?lya urn model involves an urn containing 
balls of different colors. At discrete time intervals, 
balls are added or removed from the urn according 
to their color distributions. 
In the simple P?lya urn (SPU) model, a ball is 
first drawn randomly from the urn and its color is 
recorded, then that ball is put back along with a 
new ball of the same color. This selection process 
is repeated and the contents of the urn change over 
time, with a self-reinforcing property sometimes 
expressed as ?the rich get richer.? SPU is actually 
exhibited in the Gibbs sampling for LDA.  
The generalized P?lya urn (GPU) model differs 
from the SPU model in the replacement scheme 
during sampling. Specifically, when a ball is ran-
domly drawn, certain numbers of additional balls 
of each color are returned to the urn, rather than 
just two balls of the same color as in SPU. 
4.2 Promoting M-sets using GPU 
To deal with the issue of sensitivity to the adverse 
effect of knowledge, MDK-LDA(b) is extended to 
MDK-LDA which employs the generalized P?lya 
urn (GPU) sampling scheme. 
As discussed in ? 1, due to the problem of the 
adverse effect of knowledge, important words may 
suffer from the presence of rare words in the same 
m-set. This problem can be dealt with the very 
sampling scheme of the GPU model (Chen et al, 
2013a). Specifically, by adding additional ??,??,? 
balls of color ? into ??
?  while keeping the drawn 
ball, we increase the proportion (probability) of 
seeing the m-set ? under topic ? and thus promote 
m-set ? as a whole. Consequently, each word in ? 
is more likely to be emitted. We define ??,??,? as: 
 ??,??,? = ?
1           ? = ??                                   
?           ? ? ?,?? ? ?,? ? ??        
 0           otherwise                             
 (3) 
The corresponding Gibbs sampler for MDK-LDA 
will be introduced in ? 6.  
Hyperparameters 
?, ?, ? Dirichlet priors for ?,  ?,  ? 
Latent & Visible Variables 
? Topic (Aspect) 
? M-set 
? Word 
? Document-Topic distribution 
?? Topic distribution of document ? 
? Topic-M-set distribution 
?? M-set distribution of topic ? 
? Topic-M-set-Word distribution 
??,? Word distribution of topic ?, m-set ? 
Cardinalities 
? Number of documents 
?? Number of words in document ? 
? Number of topics 
? Number of m-sets 
? The vocabulary size 
Sampling & Count Notations 
?? Topic assignment for word ??  
??  M-set assignment for word ??  
??? Topic assignments for all words except ??  
??? M-set assignments for all words except ??  
??,?  
Number of times that topic ? is assigned 
to word tokens in document ? 
??,? 
Number of times that m-set ? occurs un-
der topic ? 
??,?,? 
Number of times that word ? appears in 
m-set ? under topic ? 
Table 1. Meanings of symbols. 
 
 
 
 
 
 
 
 
 
Figure 1. Plate notation for MDK-LDA(b) and MC-LDA. 
T?S Nm 
M 
? 
T 
 ?  z 
 w 
 ? 
? 
 s ? 
 ? 
1658
5 Incorporating C-sets 
5.1 Extended Generalized P?lya urn Model 
To handle the complex situation resulted from in-
corporating c-sets, we propose an Extended gener-
alized P?lya urn (E-GPU) model. Instead of 
involving only one urn as in SPU and GPU, E-
GPU model considers a set of urns in the sampling 
process. The E-GPU model allows a ball to be 
transferred from one urn to another, enabling mul-
ti-urn interactions. Thus, during sampling, the 
populations of several urns will evolve even if on-
ly one ball is drawn from one urn. This capability 
makes the E-GPU model more powerful as it 
models relationships among multiple urns. 
We define three sets of urns which will be used 
in the new sampling scheme in the proposed MC-
LDA model. The first set of urns is the topic urns 
???{1??}
? , where each topic urn contains ? colors 
(topics) and each ball inside has a color ? ?
 {1 ??}. It corresponds to the document-topic dis-
tribution ? in Table 1. The second set of urns (m-
set urn ??? {1??}
? )  corresponds to the topic-m-set 
distribution ? , with balls of colors (m-sets) 
? ?  {1 ? ?}  in each m-set urn. The third set of 
urns is the word urns ??,?
? ,where ? ?  {1 ??} and 
? ?  {1 ? ?} . Each ball inside a word urn has a 
color (word) ? ?  {1 ??}. The distribution ? can 
be reflected in this set of urns. 
5.2 Handling C-sets using E-GPU 
As MDK-LDA can only use m-sets but not c-sets, 
we now extend MDK-LDA to the MC-LDA model 
in order to exploit c-sets. As pointed out in ? 1, c-
sets may be inconsistent with the corpus domain, 
which makes them considerably harder to deal 
with. To tackle the issue, we utilize the proposed 
E-GPU model and incorporate c-sets handling in-
side the E-GPU sampling scheme, which is also 
designed to enable automated adjustment of the 
number of topics based on domain knowledge. 
Based on the definition of c-set, each pair of 
words in a c-set cannot both have large probabili-
ties under the same topic. As the E-GPU model 
allows multi-urn interactions, when sampling a 
ball represents word ? from a word urn ??,?
? , we 
want to transfer the balls representing cannot-
words of ? (sharing a c-set with ?) to other urns 
(see Step 3 a below). That is, decrease the proba-
bilities of those cannot-words under this topic 
while increasing their corresponding probabilities 
under some other topics. In order to correctly 
transfer a ball that represents word ?, it should be 
transferred to an urn which has a higher proportion 
of ? and its related words (i.e., words sharing m-
sets with ?). That is, we randomly sample an urn 
that has a higher proportion of any m-set of ? to 
transfer ? to (Step 3 b below). However, the situa-
tion becomes more involved when a c-set is not 
consistent with the corpus. For example, aspects 
price and amazon may be mixed under one topic 
(say ?) in LDA. The user may want to separate 
them by providing a c-set {price, amazon}. In this 
case, according to LDA, word price has no topic 
with a higher proportion of it (and its related 
words) than topic ?. To transfer it, we need to in-
crement the number of topics by 1 and then trans-
fer the word to this new topic urn (step 3 c below). 
Based on these ideas, we propose the E-GPU sam-
pling scheme for the MC-LDA model below: 
1. Sample a topic ? from ??
? , an m-set ? from ??
?, and 
a word ?  from ??,?
?  sequentially, where ?  is the 
?th document. 
2. Record ?, ? and ?, put back two balls of color ? in-
to urn ??
? , one ball of color ? into urn ??
?, and two 
balls of color ? into urn ??,?
? . Given the matrix ? 
(in Equation 3), for each word ?? ? ?, we put back 
??,?? ,? number of balls of color ? into urn ??
?. 
3.  For each word ?? that shares a c-set with ?: 
a) Sample an m-set ??  from ??
?  which satisfies 
?? ? ?? . Draw a ball ? of color ?? (to be trans-
ferred) from ??,??
?  and remove it from ??,??
? . The 
document of ball ? is denoted by ??. If no ball 
of color ?? can be drawn (i.e., there is no ball 
of color ?? in ??,??
? ), skip steps b) to d). 
b) Produce an urn set {???,??
? } such that each urn in 
it satisfies the following conditions: 
i)   ?? ? ?, ?? ? ?
? 
ii) The proportion of balls of color ?? in ???
?  is    
higher than that of balls of color ??  in ??
?. 
c) If {???,??
? } is not empty, randomly select one urn 
???,??
?  from it. If {???,??
? } is empty, set ? = ? +
1, ?? = ?, draw an m-set ?? from ???
?  which sat-
isfies ?? ? ?
?. Record ?? for step d). 
d) Put the ball ? drawn from Step a) into ???,??
? , as 
well as a ball of color ?? into ???
?  and a ball of 
color ?? into ???
? . 
Note that the E-GPU model cannot be reflected in 
the graphical model in Figure 1 as it is essentially 
1659
sampling scheme, and hence MC-LDA shares the 
same plate as MDK-LDA(b). 
6 Collapsed Gibbs Sampling 
We now describe the collapsed Gibbs sampler 
(Griffiths and Steyvers, 2004) with the detailed 
conditional distributions and algorithms for MC-
LDA. Inference of ? and ? can be computationally 
expensive due to the non-exchangeability of words 
under the E-GPU models. We take the approach of 
(Mimno et al, 2011) which approximates the true 
Gibbs sampling distribution by treating each word 
as if it were the last. 
For each word ?? , we perform hierarchical 
sampling consisting of the following three steps 
(the detailed algorithms are given in Figures 2 and 
3): 
Step 1 (Lines 1-11 in Figure 2): We jointly 
sample a topic ??  and an m-set ??  (containing ??) 
for ?? , which gives us a blocked Gibbs sampler 
(Ishwaran and James, 2001), with the conditional 
probability given by: 
     ?(?? = ?, ?? = ?|?
?? , ??? ,?,?,?, ?,?) ?
     
??,?
?? +?
? ??
?,??
?? +???
??=1
?
? ? ??,??,?? ???,?,??
???
??=1
?
??=1 +?
? ?? ? ???,??,?????,??,??
???
??=1
?
??=1
+???
??=1
?
     
??,?,??
?? +??
? ??
?,?,??
?? +???
?
??=1
  
 (4) 
This step is the same as the Gibbs sampling for the 
MDK-LDA model. 
Step 2 (lines 1-5 in Figure 3): For every cannot-
word (say ??) of ??, randomly pick an urn ???,??
?  
from the urn set {???,??
? } where ??? ? ??. If there ex-
ists at least one ball of color ?? in urn ???,??
? , we 
sample one ball (say ?? ) of color ??  from urn 
???,??
? , based on the following conditional distribu-
tion: 
     ?(? = ??|?, ?,?,?,?, ?,?) ?
???,?+?
? ????,??
+???
??=1
  (5) 
where ??  denotes the document of the ball ??  of 
color ??. 
Step 3 (lines 6-12 in Figure 3): For each drawn 
ball ? from Step 2, resample a topic ? and an m-set 
?  (containing ?? ) based on the following condi-
tional distribution: 
?(?? = ?, ?? = ?|?
?? , ??? ,?,?,?, ?,?, ? = ??)
? ?
?0,???
?????
(????
??)?
????(??)? ?
??,?
?? + ?
? ???,??
?? + ?????=1
?
? ? ??,??,?? ? ??,?,??
???
??=1
?
??=1 + ?
? ?? ? ???,??,?? ? ??,??,??
???
??=1
?
??=1 + ??
?
??=1
?
??,?,??
?? + ??
? ???,?,??
?? + ???
?
??=1
 
(6) 
where ??  (same as ??  in Figure 3) and ??  are the 
original topic and m-set assignments. The super-
script ?? denotes the counts excluding the original 
Algorithm 1. GibbsSampling(?, ?? , ?, ?, ?) 
Input: Document ?, Word ?? , Matrix ?, 
           Transfer cannot-word flag ?, 
           A set of valid topics ? to be assigned to ??  
1:   ??,?? ? ??,?? ? 1; 
2:   for each word ?? in ?? do 
3:       ???,?? ? ???,?? ? ???,?? ,??; 
4:   end for 
5:   ???,??,?? ? ???,??,?? ? 1; 
6:   Jointly sample ?? ? ? and ?? ? ??  using Equation 2; 
7:   ??,?? ? ??,?? + 1; 
8:   for each word ?? in ?? do 
9:       ???,?? ? ???,?? + ???,?? ,??; 
10: end for 
11: ???,??,?? ? ???,??,?? + 1; 
12: if ? is true then 
13:     TransferCannotWords(?? , ??); 
14: end if 
Figure 2. Gibbs sampling for MC-LDA. 
Algorithm 2.TransferCannotWords(?? , ??) 
Input: Word ?? , Topic ??, 
1:   for each cannot-word ?? of ??  do 
2:       Randomly select an m-set ??  from all m-sets of ??; 
3:       Build a set ? containing all the instances of ?? 
from the corpus with topic and m-set assign-
ments being ?? and ??; 
4:       if ? is not empty then 
5:            Draw an instance of ?? from ? (denoting the 
document of this instance by ??) using 
Equation 5; 
6:            Generate a topic set ?? that each topic ?? inside 
satisfies ????????(???(?? )) > ???(??). 
7:            if ?? is not empty then 
8:                GibbsSampling(??, ??, ?, false, ??); 
9:            else 
10:              ????? = ? + 1; // ? is #Topics. 
11:              GibbsSampling(??, ??, ?, false, {?????}); 
12:          end if 
13:     end if 
14: end for 
Figure 3. Transfer cannot-words in Gibbs sampling. 
1660
assignments. ?()  is an indicator function, which 
restricts the ball to be transferred only to an urn 
that contains a higher proportion of its m-set. 
When no topic ? can be successfully sampled and 
the current sweep (iteration) of Gibbs sampling 
has the same number of topic (?) as the previous 
sweep, we increment ? by 1. And then assign ? to 
?? . The counts and parameters are also updated 
accordingly. 
7 Experiments 
We now evaluate the proposed MC-LDA model 
and compare it with state-of-the-art existing mod-
els. Two unsupervised baseline models that we 
compare with are:  
? LDA: LDA is the basic unsupervised topic 
model (Blei et al, 2003). 
? LDA-GPU: LDA with GPU (Mimno et al, 
2011). Specifically, LDA-GPU applies GPU in 
LDA using co-document frequency.  
As for knowledge-based models, we focus on 
comparing with DF-LDA model (Andrzejewski et 
al., 2009), which is perhaps the best known 
knowledge-based model and it allows both must-
links and cannot-links.  
For a comprehensive evaluation, we consider 
the following variations of MC-LDA and DF-LDA:  
? MC-LDA: MC-LDA with both m-sets and c-
sets. This is the newly proposed model.   
? M-LDA: MC-LDA with m-sets only. This is 
the MDK-LDA model in (Chen et al, 2013a). 
? DF-M: DF-LDA with must-links only. 
? DF-MC: DF-LDA with both must-links and 
cannot-links. This is the full DF-LDA model in 
(Andrzejewski et al, 2009).  
We do not compare with seeded models in (Burns 
et al, 2012; Jagarlamudi et al, 2012; Lu et al, 
2011; Mukherjee and Liu, 2012) as seed sets are 
special cases of must-links and they also do not 
allow c-sets (or cannot-links). 
7.1 Datasets and Settings 
Datasets: We use product reviews from four do-
mains (types of products) from Amazon.com for 
evaluation. The corpus statistics are shown in Ta-
ble 2 (columns 2 and 3). The domains are ?Cam-
era,? ?Food,? ?Computer,? and ?Care? (short for 
?Personal Care?). We have made the datasets pub-
lically available at the website of the first author. 
Pre-processing: We ran the Stanford Core NLP 
Tools1 to perform sentence detection and lemmati-
zation. Punctuations, stopwords 2 , numbers and 
words appearing less than 5 times in each corpus 
were removed. The domain name was also re-
moved, e.g., word camera in the domain ?Camera?, 
since it co-occurs with most words in the corpus, 
leading to high similarity among topics/aspects. 
Sentences as documents: As noted in (Titov and 
McDonald, 2008), when standard topic models are 
applied to reviews as documents, they tend to pro-
duce topics that correspond to global properties of 
products (e.g., brand name), which make topics 
overlapping with each other. The reason is that all 
reviews of the same type of products discuss about 
the same aspects of these products. Only the brand 
names and product names are different. Thus, us-
ing individual reviews for modeling is not very 
effective. Although there are approaches which 
model sentences (Jo and Oh, 2011; Titov and 
McDonald, 2008), we take the approach of (Brody 
and Elhadad, 2010), dividing each review into sen-
tences and treating each sentence as an independ-
ent document. Sentences can be used by all three 
baselines without any change to their models. Alt-
hough the relationships between sentences are lost, 
the data is fair to all models. 
Parameter settings: For all models, posterior in-
ference was drawn using 1000 Gibbs iterations 
with an initial burn-in of 100 iterations. For all 
models, we set ? = 1 and ? = 0.1. We found that 
small changes of ? and ? did not affect the results 
much, which was also reported in (Jo and Oh, 
2011) who also used online reviews. For the num-
ber of topics T, we tried different values (see ?7.2) 
as it is hard to know the exact number of topics. 
While non-parametric Bayesian approaches (Teh 
et al, 2006) aim to estimate ?  from the corpus, 
they are often sensitive to the hyper-parameters 
(Heinrich, 2009). 
1 http://nlp.stanford.edu/software/corenlp.shtml 
2 http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list 
Domain #Reviews #Sentences #M-sets #C-sets 
Camera 500 5171 173 18 
Food 500 2416 85 10 
Computer 500 2864 92 6 
Care 500 3008 119 13 
Average 500 3116 103 9 
Table 2. Corpus statistics with #m-sets and #c-sets 
having at least two words. 
                                                          
1661
For DF-LDA, we followed (Andrzejewski et al, 
2009) to generate must-links and cannot-links 
from our domain knowledge. We then ran DF-
LDA3 while keeping its parameters as proposed in 
(Andrzejewski et al, 2009) (we also experimented 
with different parameter settings but they did not 
produce better results). For our proposed model, 
we estimated the thresholds using cross validation 
in our pilot experiments. Estimated value ? = 0.2 
in equation 3 yielded good results. The second 
stage (steps 2 and 3) of the Gibbs sampler for MC-
LDA (for dealing with c-sets) is applied after 
burn-in phrase. 
Domain knowledge: User knowledge about a do-
main can vary a great deal. Different users may 
have very different knowledge. To reduce this var-
iance for a more reliable evaluation, instead of 
asking a human user to provide m-sets, we obtain 
the synonym sets and the antonym sets of each 
word that is a noun or adjective (as words of other 
parts-of-speech usually do not indicate aspects) 
from WordNet (Miller, 1995) and manually verify 
the words in those sets for the domain. Note that if 
a word ?  is not provided with any m-set, it is 
treated as a singleton m-set {?}. For c-sets, we ran 
LDA in each domain and provide c-sets based on 
the wrong results of LDA as in (Andrzejewski et 
al., 2009). Then, the knowledge is provided to 
each model in the format required by each model. 
The numbers of m-sets and c-sets are listed in col-
umns 4 and 5 of Table 2. Duplicate sets have been 
removed. 
7.2 Objective Evaluation 
In this section, we evaluate our proposed MC-
3 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html 
LDA model objectively. Topic models are often 
evaluated using perplexity on held-out test data. 
However, the perplexity metric does not reflect the 
semantic coherence of individual topics learned by 
a topic model (Newman et al, 2010). Recent re-
search has shown potential issues with perplexity 
as a measure: (Chang et al, 2009) suggested that 
the perplexity can sometimes be contrary to human 
judgments. Also, perplexity does not really reflect 
our goal of finding coherent aspects with accurate 
semantic clustering. It only provides a measure of 
how well the model fits the data. 
The Topic Coherence metric (Mimno et al, 
2011) (also called the ?UMass? measure (Stevens 
and Buttler, 2012)) was proposed as a better alter-
native for assessing topic quality. This metric re-
lies upon word co-occurrence statistics within the 
documents, and does not depend on external re-
sources or human labeling. It was shown that topic 
coherence is highly consistent with human expert 
labeling by Mimno et al (2011). Higher topic co-
herence score indicates higher quality of topics, 
i.e., better topic interpretability. 
Effects of Number of Topics 
Since our proposed models and the baseline mod-
els are all parametric models, we first compare 
each model given different numbers of topics. 
Figure 4 shows the average Topic Coherence score 
of each model given different numbers of topics. 
From Figure 4, we note the following: 
1. MC-LDA consistently achieves the highest To-
pic Coherence scores given different numbers 
of topics. M-LDA also works better than the 
other baseline models, but not as well as MC-
LDA. This shows that both m-sets and c-sets 
are beneficial in producing coherent aspects. 
2. DF-LDA variants, DF-M and DF-MC, do not 
perform well due to the shortcomings discussed 
 
Figure 4. Avg. Topic Coherence score of each model 
across different number of topics. 
 
Figure 5. Avg. Topic Coherence score for different 
proportions of knowledge. 
-1600
-1500
-1400
-1300
-1200
3 6 9 12 15
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
-1320
-1300
-1280
-1260
-1240
0% 25% 50% 100%
MC-LDA M-LDA DF-M DF-MC
                                                          
1662
in ? 1. It is slightly better than LDA when ? = 
15, but worse than LDA in other cases. We will 
further analyze the effects of knowledge on 
MC-LDA and DF-LDA shortly. 
3. LDA-GPU does not perform well due to its use 
of co-document frequency. As frequent words 
usually have high co-document frequency with 
many other words, the frequent words are 
ranked top in many topics. This shows that the 
guidance using domain knowledge is more ef-
fective than using co-document frequency. 
In terms of improvements, MC-LDA outperforms 
M-LDA significantly ( ? < 0.03 ) and all other 
baseline models significantly (? < 0.01) based on 
a paired t-test. It is important to note that by no 
means do we say that LDA-GPU and DF-LDA are 
not effective. We only say that for the task of as-
pect extraction and leveraging domain knowledge, 
these models do not generate as coherent aspects 
as ours because of their shortcomings discussed in 
? 1. In general, with more topics, the Topic Coher-
ence scores increase. We found that when ?  is 
larger than 15, aspects found by each model be-
came more and more overlapping, with several 
aspects expressing the same features of products. 
So we fix ? = 15 in the subsequent experiments.  
Effects of Knowledge 
To further analyze the effects of knowledge on 
models, in each domain, we randomly sampled 
different proportions of knowledge (i.e., different 
numbers of m-sets/must-links and c-sets/cannot-
links) as shown in Figure 5, where 0% means no 
knowledge (same as LDA and LDA-GPU, which 
do not incorporate knowledge) and 100% means 
all knowledge. From Figure 5, we see that MC-
LDA and M-LDA both perform consistently better 
than DF-MC and DF-M across different propor-
tions of knowledge. With the increasing number of 
knowledge sets, MC-LDA and M-LDA achieve 
higher Topic Coherence scores (i.e., produce more 
coherent aspects). In general, MC-LDA performs 
the best. For both DF-MC and DF-M, the Topic 
Coherence score increases from 0% to 25% 
knowledge, but decreases with more knowledge 
(50% and 100%). This shows that with limited 
amount of knowledge, the shortcomings of DF-
LDA are not very obvious, but with more 
knowledge, these issues become more serious and 
thus degrade the performance of DF-LDA.  
7.3 Human Evaluation 
Since our aim is to make topics more interpretable 
and conformable to human judgments, we worked 
with two judges who are familiar with Amazon 
products and reviews to evaluate the models sub-
jectively. Since topics from topic models are rank-
ings based on word probability and we do not 
know the number of correct topical words, a natu-
ral way to evaluate these rankings is to use Preci-
sion@n (or p@n) which was also used in 
(Mukherjee and Liu, 2012; Zhao et al, 2010), 
where n is the rank position. We give p@n for n = 
5 and 10. There are two steps in human evaluation: 
topic labeling and word labeling. 
Topic Labeling: We followed the instructions in 
(Mimno et al, 2011) and asked the judges to label 
each topic as good or bad. Each topic was present-
ed as a list of 10 most probable words in descend-
ing order of their probabilities under that topic. 
The models which generated the topics for label-
ing were obscure to the judges. In general, each 
topic was annotated as good if it had more than 
half of its words coherently related to each other 
representing a semantic concept together; other-
wise bad. Agreement of human judges on topic 
 
Figure 6. Avg. p@5 of good topics for each model 
across different domains. 
The models of each bar from left to rights are MC-LDA, M-
LDA, LDA, DF-M, DF-MC, LDA-GPU. (Same for Figure 7) 
 
Figure 7. Avg. p@10 of good topics for each model 
across different domains. 
0.5
0.6
0.7
0.8
0.9
1.0
Camera Food Computer Care
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
0.5
0.6
0.7
0.8
0.9
1.0
Camera Food Computer Care
MC-LDA M-LDA LDA
DF-M DF-MC LDA-GPU
1663
labeling using Cohen?s Kappa yielded a score of 
0.92 indicating almost perfect agreements accord-
ing to the scale in (Landis and Koch, 1977). This 
is reasonable as topic labeling is an easy task and 
semantic coherence can be judged well by humans. 
Word Labeling: After topic labeling, we chose 
the topics, which were labeled as good by both 
judges, as good topics. Then, we asked the two 
judges to label each word of the top 10 words in 
these good topics. Each word was annotated as 
correct if it was coherently related to the concept 
represented by the topic; otherwise incorrect. 
Since judges already had the conception of each 
topic in mind when they were labeling topics, la-
beling each word was not difficult which explains 
the high Kappa score for this labeling task (score = 
0.892). 
Quantitative Results 
Figures 6 and 7 give the average p@5 and p@10 
of all good topics over all four domains. The num-
bers of good topics generated by each model are 
shown in Table 3. We can see that the human 
evaluation results are highly consistent with Topic 
Coherence results in ?7.2. MC-LDA improves 
over M-LDA significantly (? < 0.01) and both 
MC-LDA and M-LDA outperforms the other base-
line models significantly ( ? < 0.005 ) using a 
paired t-test. We also found that when the domain 
knowledge is simple with one word usually ex-
pressing only one meaning/sense (e.g., in the do-
main ?Computer?), DF-LDA performs better than 
LDA. In other domains, it performs similarly or 
worse than LDA. Again, it shows that DF-LDA is 
not effective to handle complex knowledge, which 
is consistent with the results of effects of 
knowledge on DF-LDA in ?7.2. 
Qualitative Results 
We now show some qualitative results to give an 
intuitive feeling of the outputs from different mod-
els. There are a large number of aspects that are 
dramatically improved by MC-LDA. Due to space 
constraints, we only show some examples. To fur-
ther focus, we just show some results of MC-LDA, 
M-LDA and LDA. The results from LDA-GPU 
and DF-LDA were inferior and hard for the human 
judges to match them with aspects found by the 
other models for qualitative comparison. 
Table 4 shows three aspects Amazon, Price, 
Battery generated by each model in the domain 
?Camera?. Both LDA and M-LDA can only dis-
cover two aspects but M-LDA has a higher aver-
age precision. Given the c-set {amazon, price, 
battery}, MC-LDA can discover all three aspects 
with the highest average precision. 
8 Conclusion  
This paper proposed a new model to exploit do-
main knowledge in the form of m-sets and c-sets 
to generate coherent aspects (topics) from online 
reviews. The paper first identified and character-
ized some shortcomings of the existing 
knowledge-based models. A new model called 
MC-LDA was then proposed, whose sampling 
scheme was based on the proposed Extended GPU 
(E-GPU) model enabling multi-urn interactions. A 
comprehensive evaluation using real-life online 
reviews from multiple domains shows that MC-
LDA outperforms the state-of-the-art models sig-
nificantly and discovers aspects with high seman-
tic coherence. In our future work, we plan to 
incorporate aspect specific sentiments in the MC-
LDA model. 
Acknowledgments 
This work was supported in part by a grant from 
National Science Foundation (NSF) under grant no. 
IIS-1111092, and a grant from HP Labs Innova-
tion Research Program. 
#Good 
Topics 
MC-LDA M-LDA LDA DF-M DF-MC LDA-GPU 
Camera 15/18 12 11 9 7 3 
Food 8/16 7 7 5 4 5 
Computer 12/16 10 7 9 6 4 
Care 11/16 10 9 10 9 3 
Average 11.5/16.5 9.75/15 8.5/15 8.25/15 6.5/15 3.75/15 
Table 3. Number of good topics of each model.             
In x/y, x is the number of discovered good topics, and y is the 
total number of topics generated.  
MC-LDA M-LDA LDA 
Amazon Price Battery Price Battery Amazon Battery 
review price battery price battery card battery 
amazon perform life lot review day screen 
software money day money amazon amazon life 
customer expensive extra big life memory lcd 
month cost charger expensive extra product water 
support week water point day sd usb 
warranty cheap time cost power week cable 
package purchase power photo time month case 
product deal hour dot support item charger 
hardware product aa purchase customer class hour 
Table 4. Example aspects in the domain ?Camera?; 
errors are marked in red/italic. 
1664
References  
Amjad Abu-Jbara, Ben King, Mona Diab, and 
Dragomir Radev. 2013. Identifying Opinion 
Subgroups in Arabic Online Discussions. In 
Proceedings of ACL. 
Apoorv Agarwal and Jasneet Sabharwal. 2012. End-to-
End Sentiment Analysis of Twitter Data. In 
Proceedings of the Workshop on Information 
Extraction and Entity Analytics on Social Media 
Data, at the 24th International Conference on 
Computational Linguistics (IEEASMD-COLING 
2012), Vol. 2. 
David Andrzejewski, Xiaojin Zhu, and Mark Craven. 
2009. Incorporating domain knowledge into topic 
modeling via Dirichlet Forest priors. In Proceedings 
of ICML, pages 25?32. 
David Andrzejewski, Xiaojin Zhu, Mark Craven, and 
Benjamin Recht. 2011. A framework for 
incorporating general domain knowledge into latent 
Dirichlet alocation using first-order logic. In 
Proceedings of IJCAI, pages 1171?1177. 
Sasha Blair-goldensohn, Tyler Neylon, Kerry Hannan, 
George A. Reis, Ryan Mcdonald, and Jeff Reynar. 
2008. Building a sentiment summarizer for local 
service reviews. In Proceedings of In NLP in the 
Information Explosion Era. 
David M. Blei and Jon D. McAuliffe. 2007. Supervised 
Topic Models. In Proceedings of NIPS. 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent Dirichlet Allocation. Journal of 
Machine Learning Research, 3, 993?1022. 
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and 
Regina Barzilay. 2008. Learning Document-Level 
Semantic Properties from Free-Text Annotations. In 
Proceedings of ACL, pages 263?271. 
Samuel Brody and Noemie Elhadad. 2010. An 
unsupervised aspect-sentiment model for online 
reviews. In Proceedings of NAACL, pages 804?812. 
Nicola Burns, Yaxin Bi, Hui Wang, and Terry 
Anderson. 2012. Extended Twofold-LDA Model for 
Two Aspects in One Sentence. Advances in 
Computational Intelligence, Vol. 298, pages 265?
275. Springer Berlin Heidelberg. 
Giuseppe Carenini, Raymond T. Ng, and Ed Zwart. 
2005. Extracting knowledge from evaluative text. In 
Proceedings of K-CAP, pages 11?18. 
Jonathan Chang, Jordan Boyd-Graber, Wang Chong, 
Sean Gerrish, and David Blei, M. 2009. Reading Tea 
Leaves: How Humans Interpret Topic Models. In 
Proceedings of NIPS, pages 288?296. 
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun 
Hsu, Malu Castellanos, and Riddhiman Ghosh. 
2013a. Leveraging Multi-Domain Prior Knowledge 
in Topic Models. In Proceedings of IJCAI, pages 
2071?2077. 
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun 
Hsu, Malu Castellanos, and Riddhiman Ghosh. 
2013b. Discovering Coherent Topics Using General 
Knowledge. In Proceedings of CIKM. 
Yejin Choi and Claire Cardie. 2010. Hierarchical 
Sequential Learning for Extracting Opinions and 
their Attributes, pages 269?274. 
Lei Fang and Minlie Huang. 2012. Fine Granular 
Aspect Analysis using Latent Structural Models. In 
Proceedings of ACL, pages 333?337. 
Thomas L. Griffiths and Mark Steyvers. 2004. Finding 
Scientific Topics. PNAS, 101 Suppl, 5228?5235. 
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, 
and Zhong Su. 2009. Product feature categorization 
with multilevel latent semantic association. In 
Proceedings of CIKM, pages 1087?1096. 
Gregor Heinrich. 2009. A Generic Approach to Topic 
Models. In Proceedings of ECML PKDD, pages 517 
? 532. 
Minqing Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. In Proceedings of 
KDD, pages 168?177. 
Yuening Hu, Jordan Boyd-Graber, and Brianna 
Satinoff. 2011. Interactive Topic Modeling. In 
Proceedings of ACL, pages 248?257. 
Hemant Ishwaran and LF James. 2001. Gibbs sampling 
methods for stick-breaking priors. Journal of the 
American Statistical Association, 96(453), 161?173. 
Jagadeesh Jagarlamudi, Hal Daum? III, and 
Raghavendra Udupa. 2012. Incorporating Lexical 
Priors into Topic Models. In Proceedings of EACL, 
pages 204?213. 
Niklas Jakob and Iryna Gurevych. 2010. Extracting 
Opinion Targets in a Single- and Cross-Domain 
Setting with Conditional Random Fields. In 
Proceedings of EMNLP, pages 1035?1045. 
Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment 
unification model for online review analysis. In 
Proceedings of WSDM, pages 815?824. 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence, 22(2), 
110?125. 
Jungi Kim, Jinji Li, and Jong-Hyeok Lee. 2009. 
Discovering the Discriminative Views: Measuring 
Term Weights for Sentiment Analysis. In 
Proceedings of ACL/IJCNLP, pages 253?261. 
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and 
Shixia Liu. 2013. A Hierarchical Aspect-Sentiment 
Model for Online Reviews. In Proceedings of AAAI. 
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 
2007. Extracting Aspect-Evaluation and Aspect-of 
Relations in Opinion Mining. In Proceedings of 
EMNLP, pages 1065?1074. 
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. 
Opinion Extraction, Summarization and Tracking in 
1665
News and Blog Corpora. In Proceedings of AAAI 
Spring Symposium: Computational Approaches to 
Analyzing Weblogs, pages 100?107. 
JR Landis and GG Koch. 1977. The measurement of 
observer agreement for categorical data. biometrics, 
33. 
Angeliki Lazaridou, Ivan Titov, and Caroline Sporleder. 
2013. A Bayesian Model for Joint Unsupervised 
Induction of Sentiment, Aspect and Discourse 
Representations. In Proceedings of ACL. 
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, 
Yingju Xia, Shu Zhang, and Hao Yu. 2010. 
Structure-Aware Review Mining and 
Summarization. In Proceedings of COLING, pages 
653?661. 
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and 
Xiaoyan Zhu. 2012a. Cross-Domain Co-Extraction 
of Sentiment and Topic Lexicons. In Proceedings of 
ACL (1), pages 410?419. 
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011. 
Generating Aspect-oriented Multi-Document 
Summarization with Event-aspect model. In 
Proceedings of EMNLP, pages 1137?1146. 
Shoushan Li, Rongyang Wang, and Guodong Zhou. 
2012b. Opinion Target Extraction Using a Shallow 
Semantic Parsing Framework. In Proceedings of 
AAAI. 
Chenghua Lin and Yulan He. 2009. Joint 
sentiment/topic model for sentiment analysis. In 
Proceedings of CIKM, pages 375?384. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Morgan & Claypool Publishers. 
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K. 
Tsou. 2011. Multi-aspect Sentiment Analysis with 
Topic Models. In Proceedings of ICDM Workshops, 
pages 81?88. 
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan 
Roth. 2012. Unsupervised discovery of opposing 
opinion networks from forum discussions. In 
Proceedings of CIKM, pages 1642?1646. 
Yue Lu and Chengxiang Zhai. 2008. Opinion 
integration through semi-supervised topic modeling. 
In Proceedings of WWW, pages 121?130. 
Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009. 
Rated aspect summarization of short comments. In 
Proceedings of WWW, pages 131?140. 
Hosam Mahmoud. 2008. Polya Urn Models. Chapman 
& Hall/CRC Texts in Statistical Science. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and ChengXiang Zhai. 2007. Topic sentiment 
mixture: modeling facets and opinions in weblogs. In 
Proceedings of WWW, pages 171?180. 
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, 
Sujian Li, and Houfeng Wang. 2012. Entity-centric 
topic-oriented opinion summarization in twitter. In 
Proceedings of KDD, pages 379?387. 
George A. Miller. 1995. WordNet: A Lexical Database 
for English. Commun. ACM, 38(11), 39?41. 
David Mimno, Hanna M. Wallach, Edmund Talley, 
Miriam Leenders, and Andrew McCallum. 2011. 
Optimizing semantic coherence in topic models. In 
Proceedings of EMNLP, pages 262?272. 
Samaneh Moghaddam and Martin Ester. 2011. ILDA: 
interdependent LDA model for learning latent 
aspects and their ratings from online product 
reviews. In Proceedings of SIGIR, pages 665?674. 
Saif Mohammad, Cody Dunne, and Bonnie J. Dorr. 
2009. Generating High-Coverage Semantic 
Orientation Lexicons From Overtly Marked Words 
and a Thesaurus. In Proceedings of EMNLP, pages 
599?608. 
Arjun Mukherjee and Bing Liu. 2012. Aspect 
Extraction through Semi-Supervised Modeling. In 
Proceedings of ACL, pages 339?348. 
David Newman, Youn Noh, Edmund Talley, Sarvnaz 
Karimi, and Timothy Baldwin. 2010. Evaluating 
topic models for digital libraries. In Proceedings of 
JCDL, pages 215?224. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1-2), 1?135. 
James Petterson, Alex Smola, Tib?rio Caetano, Wray 
Buntine, and Shravan Narayanamurthy. 2010. Word 
Features for Latent Dirichlet Allocation. In 
Proceedings of NIPS, pages 1921?1929. 
AM Popescu and Oren Etzioni. 2005. Extracting 
product features and opinions from reviews. In 
Proceedings of HLT, pages 339?346. 
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. 
Opinion Word Expansion and Target Extraction 
through Double Propagation. Computational 
Linguistics, 37(1), 9?27. 
Daniel Ramage, David Hall, Ramesh Nallapati, and 
Christopher D. Manning. 2009. Labeled LDA: a 
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP, pages 
248?256. 
Christina Sauper and Regina Barzilay. 2013. Automatic 
Aggregation by Joint Modeling of Aspects and 
Values. J. Artif. Intell. Res. (JAIR), 46, 89?127. 
Christina Sauper, Aria Haghighi, and Regina Barzilay. 
2011. Content Models with Attitude. In Proceedings 
of ACL, pages 350?358. 
Swapna Somasundaran and J. Wiebe. 2009. 
Recognizing stances in online debates. In 
Proceedings of ACL, pages 226?234. 
Keith Stevens and PKDAD Buttler. 2012. Exploring 
Topic Coherence over many models and many 
topics. In Proceedings of EMNLP-CoNLL, pages 
952?961. 
Veselin Stoyanov and Claire Cardie. 2011. 
Automatically Creating General-Purpose Opinion 
1666
Summaries from Text. In Proceedings of RANLP, 
pages 202?209. 
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, 
and David M. Blei. 2006. Hierarchical dirichlet 
processes. Journal of the American Statistical 
Association, 1?30. 
Ivan Titov and Ryan McDonald. 2008. Modeling online 
reviews with multi-grain topic models. In 
Proceedings of WWW, pages 111?120. 
Dong Wang and Yang Liu. 2011. A Pilot Study of 
Opinion Summarization in Conversations. In 
Proceedings of ACL, pages 331?339. 
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. 
Latent aspect rating analysis on review text data: a 
rating regression approach. In Proceedings of KDD, 
pages 783?792. 
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. 
Latent aspect rating analysis without aspect keyword 
supervision. In Proceedings of KDD, pages 618?626. 
Janyce Wiebe and Ellen Riloff. 2005. Creating 
Subjective and Objective Sentence Classifiers from 
Unannotated Texts. In Proceedings of CICLing, 
pages 486?497. 
Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce, 
Matthew Bell, and Melanie Martin. 2004. Learning 
Subjective Language. Computational Linguistics, 
30(3), 277?308. 
Michael Wiegand and Dietrich Klakow. 2010. 
Convolution Kernels for Opinion Holder Extraction. 
In Proceedings of HLT-NAACL, pages 795?803. 
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide 
Wu. 2009. Phrase dependency parsing for opinion 
mining. In Proceedings of EMNLP, pages 1533?
1541. 
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide 
Wu. 2011. Structural Opinion Mining for Graph-
based Sentiment Representation. In Proceedings of 
EMNLP, pages 1332?1341. 
Yunqing Xia, Boyi Hao, and Kam-Fai Wong. 2009. 
Opinion Target Network and Bootstrapping Method 
for Chinese Opinion Target Extraction. In 
Proceedings of AIRS, pages 339?350. 
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect Ranking: Identifying 
Important Product Aspects from Online Consumer 
Reviews. In Proceedings of ACL, pages 1496?1505. 
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011. 
Constrained LDA for grouping product features in 
opinion mining. In Proceedings of the 15th Pacific-
Asia Conference on Knowledge Discovery and Data 
Mining (PAKDD), pages 448?459. 
Lei Zhang and Bing Liu. 2011. Identifying Noun 
Product Features that Imply Opinions. In 
Proceedings of ACL (Short Papers), pages 575?580. 
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and 
Xiaoming Li. 2010. Jointly Modeling Aspects and 
Opinions with a MaxEnt-LDA Hybrid. In 
Proceedings of EMNLP, pages 56?65. 
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2012. 
Cross-Language Opinion Target Extraction in 
Review Texts. In Proceedings of ICDM, pages 
1200?1205. 
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
CIKM, pages 43?50. ACM Press. 
 
1667
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1139?1145,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Exploiting Social Relations and Sentiment for Stock Prediction 
 
Jianfeng Si* Arjun Mukherjee? Bing Liu? Sinno Jialin Pan* Qing Li? Huayi Li? 
* Institute for Infocomm Research, Singapore 
{ thankjeff@gmail.com, jspan@i2r.a-star.edu.sg} 
?Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA 
{ arjun4787@gmail.com, liub@cs.uic.edu, lhymvp@gmail.com} 
? Department of Computer Science, City University of Hong Kong, Hong Kong, China 
qing.li@cityu.edu.hk 
 
 
Abstract 
In this paper we first exploit cash-tags (?$? fol-
lowed by stocks? ticker symbols) in Twitter to 
build a stock network, where nodes are stocks 
connected by edges when two stocks co-occur 
frequently in tweets. We then employ a labeled 
topic model to jointly model both the tweets and 
the network structure to assign each node and 
each edge a topic respectively. This Semantic 
Stock Network (SSN) summarizes discussion 
topics about stocks and stock relations. We fur-
ther show that social sentiment about stock 
(node) topics and stock relationship (edge) topics 
are predictive of each stock?s market. For predic-
tion, we propose to regress the topic-sentiment 
time-series and the stock?s price time series. Ex-
perimental results demonstrate that topic senti-
ments from close neighbors are able to help im-
prove the prediction of a stock markedly. 
1 Introduction 
Existing research has shown the usefulness of 
public sentiment in social media across a wide 
range of applications. Several works showed so-
cial media as a promising tool for stock market 
prediction (Bollen et al., 2011; Ruiz et al., 2012; 
Si et al., 2013). However, the semantic relation-
ships between stocks have not yet been explored. 
In this paper, we show that the latent semantic 
relations among stocks and the associated social 
sentiment can yield a better prediction model.  
On Twitter, cash-tags (e.g., $aapl for Apple 
Inc.) are used in a tweet to indicate that the tweet 
talks about the stocks or some other related in-
formation about the companies. For example, 
one tweet containing cash-tags: $aapl and $goog 
(Google Inc.), is ?$AAPL is loosing customers. 
everybody is buying android phones! $GOOG?. 
Such joint mentions directly reflect some kind of 
latent relationship between the involved stocks, 
which motivates us to exploit such information 
for the stock prediction.  
We propose a notion of Semantic Stock Net-
work (SSN) and use it to summarize the latent 
semantics of stocks from social discussions. To 
our knowledge, this is the first work that uses 
cash-tags in Twitter for mining stock semantic 
relations. Our stock network is constructed based 
on the co-occurrences of cash-tags in tweets. 
With the SSN, we employ a labeled topic model 
to jointly model both the tweets and the network 
structure to assign each node and each edge a 
topic respectively. Then, a lexicon-based senti-
ment analysis method is used to compute a sen-
timent score for each node and each edge topic. 
To predict each stock?s performance (i.e., the 
up/down movement of the stock?s closing price), 
we use the sentiment time-series over the SSN 
and the price time series in a vector autoregres-
sion (VAR) framework.  
We will show that the neighbor relationships in 
SSN give very useful insights into the dynamics 
of the stock market. Our experimental results 
demonstrate that topic sentiments from close 
neighbors of a stock can help improve the predic-
tion of the stock market markedly. 
2 Related work 
2.1 Social Media & Economic Indices 
Many algorithms have been proposed to produce 
meaningful insights from massive social media 
data. Related works include detecting and sum-
marizing events (Weng and Lee, 2011; Weng et 
al., 2011; Baldwin et al., 2012; Gao et al., 2012) 
and analyzing sentiments about them (Pang and 
Lee, 2008; Liu, 2012), etc. Some recent literature 
also used Twitter as a sentiment source for stock 
market prediction (Bollen et al., 2011; Si et al., 
2013). This paper extends beyond the correlation 
between social media and stock market, but fur-
1139
ther exploits the social relations between stocks 
from the social media context. 
  Topic modeling has been widely used in social 
media. Various extensions of the traditional LDA 
model (Blei et al., 2003) has been proposed for 
modeling social media data (Wang et al., 2011, 
Jo and Oh, 2011; Liu et al., 2007; Mei et al., 
2007; Diao et al., 2012). Ramage et al. (2009; 
2011) presented a partially supervised learning 
model called Labeled LDA to utilize supervision 
signal in topic modeling. Ma et al. (2013) pre-
dicted the topic popularity based on hash-tags on 
Twitter in a classification framework. 
2.2 Financial Networks for Stock 
Financial network models study the correlations 
of stocks in a graph-based view (Tse et al., 2010; 
Mantegna, 1999; Vandewalle et al., 2001; On-
nela et al., 2003; Bonanno et al., 2001). The usu-
al approach is to measure the pairwise correla-
tion of stocks? historical price series and then 
connect the stocks based on correlation strengths 
to build a correlation stock network (CSN). 
However, our approach leverages social media 
posts on stock tickers. The rationale behind is 
that micro-blogging activities have been shown 
to be highly correlated with the stock market 
(Ruiz et al., 2012; Mao et al., 2012). It is more 
informative, granular to incorporate latest devel-
opments of the market as reflected in social me-
dia instead of relying on stocks? historical price.  
3 Semantic Stock Network (SSN) 
3.1 Construction of SSN 
We collected five months (Nov. 2 2012 - Apr. 3 
2013) of English tweets for a set of stocks in the 
Standard & Poor's 100 list via Twitter?s REST 
API, using cash-tags as query keywords. For 
preprocessing, we removed tweets mentioning 
more than five continuous stock tickers as such 
tweets usually do not convey much meaning for 
our task. Finally, we obtained 629,977 tweets in 
total. Table 1 shows the top five most frequent 
stocks jointly mentioned with Apple Inc. in our 
dataset. Formally, we define the stock network as 
an undirected graph ? = {? , ?}. The node set 
? comprises of stocks, ??,? ? ?  stands for the 
edge between stock nodes ? and ? and the edge 
weight is the number of co-occurrences. On ex-
ploring the co-occurrence statistics in pilot stud-
ies, we set a minimum weight threshold of 400 to 
filter most non-informative edges. Figure 1 
demonstrates a segment of the stock network 
constructed from our dataset. 
3.2 Semantic Topics over the Network 
Figure 2 illustrates our annotation for each tweet. 
For a tweet, ? with three cash-tags: {?1, ?2, ?3}, we annotate ?  with the label set, ?? =
 {?1, ?2, ?3, ?1,2, ?1,3, ?2,3}. (?1,2 is ?aapl_goog? 
if ?1is ?aapl? and ?2 is ?goog?). Then, the topic assignments of words in ? are constrained to top-
ics indexed by its label set, ??. Given the annota-tions as labels, we use the Labeled LDA model 
(Ramage et al., 2009) to jointly learn the topics 
over nodes and edges. Labeled-LDA assumes 
that the set of topics are the distinct labels in a 
labeled set of documents, and each label corre-
sponds to a unique topic. Similar to LDA (Blei et 
al., 2003), Labeled-LDA models each document 
as an admixture of latent topics and generates 
each word from a chosen topic. Moreover, La-
beled-LDA incorporates supervision by simply 
constraining the model to use only those topics 
that correspond to a document?s observed label 
set (Ramage et al., 2009). For model inference, 
we use collapsed Gibbs sampling (Bishop, 2006) 
and the symmetric Dirichlet Priors are set to: 
? = 0.01, ? = 0.01 as suggested in (Ramage et 
al., 2010). The Gibbs Sampler is given as: 
?(?? = ?|???)~
 ?(??,?)?1+ ?
?(??,?)?1+ |???|??
? ?(?,??)?1+??(?,?)?1+ |? |?? (1) 
where ?(??, ?) is the number of words in ?? as-signed to topic ?, while ?(??,?) is the marginal-ized sum. |??? | is the size of label subset of ??. 
 Figure 2. Tweet label design. 
$goog $amzn $ebay $msft $intc
43263 23266 14437 11891 2486
Table 1. co-occurrence statistics with $aapl. 
 
Figure 1. An example stock network. 
1140
?(?, ?) is the term frequency of word ? in topic 
?. |? | is the vocabulary size. The subscript -1 is 
used to exclude the count assignment of the cur-
rent word ?? . The posterior on the document?s topic distribution {??,?} and topic?s word distri-
bution {??,?} can be estimated as follows: 
??,? =  
?(??,?)+ ?
?(??,?)+ |???|??
                (2) 
??,? =  
?(?,??)+?
?(?,?)+ |? |??                   (3) 
Later, parameters {??,?} will be used to compute 
the sentiment score for topics. 
3.3 Leveraging Sentiment over SSN for 
Stock Prediction 
We define a lexicon based sentiment score in the 
form of opinion polarity for each node-indexed 
and edge-indexed topic as follows: 
?(?) = ? ??,?
|? |
?=1
?(?), ?(?) ? [?1,1]  (4) 
where ?(?) denotes the opinion polarity of word 
?. ??,?  is the word probability of ? in topic ? 
(Eq.3). Based on an opinion lexicon ?, ?(?) = 
+1 if ? ? ????, ?(?) = -1 if ? ? ???? and ?(?) 
= 0 otherwise. We use the opinion English lexi-
con contributed by Hu and Liu (2004).  
Considering the inherent dynamics of both the 
stock markets and social sentiment, we organize 
the tweets into daily based sub-sets according to 
their timestamps to construct one ????  ( ? ?
[1, ? ]) for each day. Then, we apply a Labeled 
LDA for each ???? and compute the sentiment scores for each ???? ?s nodes and edges. This yields a sentiment time series for the node, ? , 
{?(?)1, ?(?)2, ? , ?(?)? } and for the edge, ??,?, 
{?(??,?)1, ?(??,?)2, ? , ?(??,?)? } . We intro-
duce a vector autoregression model (VAR) 
(Shumway and Stoffer, 2011) by regressing sen-
timent time series together with the stock price 
time series to predict the up/down movement of 
the stock?s daily closing price. 
As usual in time series analysis, the regression 
parameters are learned during a training phase 
and then are used for forecasting under sliding 
windows, i.e., to train in period [?, ? + ?] and to 
predict on time ? + ? + 1. Here the window size 
? refers to the number of days in series used in 
model training. A VAR model for two variables 
{??} and {??} can be written as: 
?? =  ? (??????? + ???????)????=1 + ??  (5) where {?} are white noises, {?} are model pa-
rameters, and ??? notes the time steps of histori-
cal information to use. In our experiment, {??} is the target stock?s price time series, {??} is the covariate sentiment/price time series, and we will 
try ??? ? ?2,3?. We use the ?dse? library in R 
language to fit our VAR model based on least 
square regression. 
4 Experiments 
4.1 Tweets in Relation to the Stock Market 
Micro-blogging activities are well correlated 
with the stock market. Figure 3 shows us how the 
Twitter activities response to a report announce-
ment of $aapl (Jan. 23 2013). The report was 
made public soon after the market closed at 
4:00pm, while the tweets volume rose about two 
hours earlier and reached the peak at the time of 
announcement, then it arrived the second peak at 
the time near the market?s next opening (9:30am). 
By further accumulating all days? tweet volume 
in our dataset as hourly based statistics, we plot 
the volume distribution in Figure 4. Again, we 
note that trading activities are well reflected by 
tweet activities. The volume starts to rise drasti-
cally two or three hours before the market opens, 
and then reaches a peak at 9:00pm. It drops dur-
ing the lunch time and reaches the second peak 
around 2:00pm (after lunch). Above observations 
clearly show that market dynamics are discussed 
in tweets and the content in tweets? discussion 
very well reflects the fine-grained aspects of 
stock market trading, opening and closing. 
 
Figure 3. Tweet activity around $aapl?s earnings 
report date on Jan. 23 2013. 
 
Figure 4. Tweet volume distribution in our data 
over hours averaged across each day. 
0
500
1000
1500
2000
2500
Time (date-hour)
0
0.02
0.04
0.06
0.08
0.1
0 2 4 6 8 10 12 14 16 18 20 22
Time (hourly)
1141
4.2 Stock Prediction 
This section demonstrates the effectiveness of 
our SSN based approach for stock prediction. 
We leverage the sentiment time-series on two 
kinds of topics from SSN: 1). Node topic from 
the target stock itself, 2). Neighbor node/edge 
topics. We note that the price correlation stock 
network (CSN) (e.g., Bonanno et al., 2001; Man-
tegna, 1999) also defines neighbor relationships 
based on the Pearson's correlation coefficient 
(Tse et al., 2010) between pair of past price se-
ries (We get the stock dataset from Yahoo! Fi-
nance, between Nov. 2 2012 and Apr. 3 2013).  
 We build a two variables VAR model to pre-
dict the movement of a stock?s daily closing 
price. One variable is the price time series of the 
target stock ({??} in Eq.5); another is the covari-ate sentiment/price time series ({??}  in Eq.5). We setup two baselines according to the sources 
of the covariate time series as follows: 
1. Covariate price time series from CSN, we try 
the price time series from the target stock?s 
closest neighbor which takes the maximum 
historical price correlation in CSN. 
2. With no covariate time series, we try the tar-
get stock?s price only based on the univariate 
autoregression (AR) model. 
 To summarize, we try different covariate sen-
timent (?(. )) or price (?(. )) time series from 
SSN or CSN together with the target stock?s 
price time series (? ?) to predict the movement of 
one day ahead price (???). The accuracy is com-
puted based on the correctness of the predicted 
directions as follows, i.e., if the prediction ??? 
takes the same direction as the actual price value, 
we increment #(???????) by 1, #(?????????) is 
the total number of test.  
???????? = #(???????)#(?????????)       (6) 
 Figure 5 details the prediction of $aapl on dif-
ferent training window sizes of [15, 60] and lags. 
{?(????), ?(????), ?(????), ?(????_????)} are 
from SSN, ?(????)  is from CSN ($dell (Dell 
Inc.) takes the maximum price correlation score 
of 0.92 with $aapl), and ? ? =  ?(????)  is the 
univariate AR model, using the target stock?s 
price time series only. Table 2 further summariz-
es the performance comparison of different ap-
proaches reporting the average (and best) predic-
tion accuracies over all time windows and dif-
ferent lag settings. Comparing to the univariate 
AR model (?? only), we see that the sentiment 
based time-series improve performances signifi-
cantly. Among SSN sentiment based approach-
es, the ?(????) helps improve the performance 
mostly and gets the best accuracy of 0.78 on ??? 
2 and training window size of 53. On average, 
?(????) achieves a net gain over ?(????) in the 
range of 29% with lag 2 (0.62 = 1.29 x 0.48) and 
14% with lag 3 (0.57 = 1.14 x 0.50). Also, 
?(????_????)  performs better than ?(????) . 
The result indicates that $aapl?s stock perfor-
mance is highly influenced by its competitor. 
?(????) also performs well, but we will see rela-
tionships from CSN may not be so reliable. 
We further summarize some other prediction 
cases in Table 3 to show how different covariate 
sentiment sources ( ?(. ) ) and price sources 
(?(. )) from their closest neighbor nodes help 
predict their stocks, which gives consistent con-
clusions. We compute the ?-test for SSN based 
prediction accuracies against that of CSN or 
price only based approaches among all testing 
 Source Lag = 2 Lag = 3 
?? only self 0.49(0.57)	 0.47(0.52)
CSN: 
P(.)+??	 dell	 0.55(0.64)	 0.57(0.67)	
 
SSN: 
S(.)+?? 
aapl 0.48(0.56)	 0.50(0.61)
goog 0.62(0.78) 0.57(0.69) 
aapl_goog 0.55(0.65) 0.52(0.56) 
msft 0.52(0.65) 0.54(0.61) 
Table 2. Performance comparison of the average and 
best (in parentheses) prediction accuracies over all 
training window sizes for prediction on $aapl. 
 
 
Figure 5. Prediction on $aapl. (x-axis is the training 
window size, y-axis is the prediction accuracy) 
with different covariate sources. 
0.2
0.3
0.4
0.5
0.6
0.7
0.8
15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60
(a) Prediction of $aapl on lag 2
P* P(dell)+P*
S(aapl)+P* S(goog)+P*
S(aapl_goog)+P* S(msft)+P*
0.2
0.3
0.4
0.5
0.6
0.7
15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60
(b) Prediction of $aapl on lag 3
P* P(dell)+P*
S(aapl)+P* S(goog)+P*
S(aapl_goog)+P* S(msft)+P*
1142
window sizes ([15, 60]), and find that SSN based 
approaches are significantly (? -value < 0.001) 
better than others.  
We note that tweet volumes of most S&P100 
stocks are too small for effective model building, 
as tweets discuss only popular stocks, other 
stocks are not included due to their deficient 
tweet volume.  
We make the following observations: 
  1. CSN may find some correlated stock pairs 
like $ebay and $amzn, $wmt and $tgt, but some-
times, it also produces pairs without real-world 
relationships like $tgt and $vz, $qcom and $pfe, 
etc. In contrast, SSN is built on large statistics of 
human recognition in social media, which is like-
ly to be more reliable as shown. 
  2. Sentiment based approaches {?(?)} consist-
ently perform better than all price based ones 
{??, ? (?)}. For ?(?)  based predictions, senti-
ment discovered from the target stock?s closest 
neighbors in SSN performs best in general. This 
empirical finding dovetails with qualitative re-
sults in the financial analysis community (Mizik 
& Jacobson, 2003; Porter, 2008), where compa-
nies? market performances are more likely to be 
influenced by their competitors. But for Google, 
its stock market is not so much influenced by 
other companies (it gets the best prediction accu-
racy on ?(????), i.e., the internal factor). It can 
be explained by Google Inc.?s relatively stable 
revenue structure, which is well supported by its 
leading position in the search engine market. 
  3. The business of offline companies like Target 
Corp. ($tgt) and Wal-Mart Stores Inc. ($wmt) are 
highly affected by online companies like $amzn. 
Although competition exists between $tag and 
$wmt, their performances seem to be affected 
more by a third-party like $amzn (In Table 3, 
??????? predicts the best for both). Not surpris-
ingly, these offline companies have already been 
trying to establish their own online stores and 
markets. 
5 Conclusion 
This paper proposed to build a stock network 
from co-occurrences of ticker symbols in tweets. 
The properties of SSN reveal some close rela-
tionships between involved stocks, which pro-
vide good information for predicting stocks 
based on social sentiment. Our experiments show 
that SSN is more robust than CSN in capturing 
the neighbor relationships, and topic sentiments 
from close neighbors of a stock significantly im-
prove the prediction of the stock market.   
Acknowledgments 
This work was supported in part by a grant from 
the National Science Foundation (NSF) under 
grant no. IIS-1111092). 
Target ? ???? only CSN:  P(.)+?? SSN:  S(.)+?? 
 
goog 
  dis(0.96) goog aapl amzn 
2 0.48(0.59) 0.53(0.60) 0.59(0.65) 0.44(0.53) 0.42(0.49) 
3 0.46(0.54) 0.53(0.62) 0.56(0.67) 0.50(0.59) 0.43(0.49) 
 
amzn 
  csco(0.90) amzn goog msft 
2 0.48(0.54) 0.48(0.55) 0.47(0.54) 0.57(0.66) 0.60(0.68) 
3 0.46(0.53) 0.49(0.53) 0.43(0.50) 0.55(0.63) 0.57(0.66) 
 
ebay 
  amzn(0.81) ebay amzn goog 
2 0.49(0.55) 0.51(0.57) 0.44(0.53) 0.57(0.64) 0.56(0.62) 
3 0.48(0.58) 0.49(0.54) 0.45(0.58) 0.54(0.64) 0.54(0.61) 
 
tgt 
  vz(0.88) tgt wmt amzn 
2 0.43(0.53) 0.43(0.54) 0.46(0.55) 0.49(0.56) 0.49(0.59) 
3 0.44(0.50) 0.40(0.53) 0.44(0.48) 0.41(0.48) 0.48(0.54) 
 
wmt 
  tgt(0.86) wmt tgt amzn 
2 0.53(0.59) 0.53(0.63) 0.52(0.61) 0.52(0.60) 0.60(0.65) 
3 0.53(0.64) 0.48(0.57) 0.55(0.66) 0.48(0.58) 0.58(0.66) 
 
qcom 
  pfe(0.88) qcom aapl intc 
2 0.53(0.6) 0.55(0.63) 0.57(0.61) 0.46(0.54) 0.63(0.70) 
3 0.54(0.61) 0.48(0.55) 0.56(0.65) 0.51(0.61) 0.61(0.67) 
Table 3. Average and best (in parentheses) prediction accuracies (over window sizes of [15, 
60]) of some other cases with different covariates, cell of dis(0.96) means ?$dis? takes the 
maximum price correlation strength of 0.96 with ?$goog? (similar for others in column 
CSN). The best performances are highlighted in bold.  
1143
References 
Baldwin T., Cook P., Han B., Harwood A., Karuna-
sekera S., and Moshtaghi M. 2012. A support plat-
form for event detection using social intelligence. 
In Proceedings of the Demonstrations at the 13th 
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL '12). 
Association for Computational Linguistics, 
Stroudsburg, PA, USA, 69-72. 
Bishop C.M. 2006. Pattern Recognition and Machine 
Learning. Springer. 
Blei D., NG A., and Jordan M. 2003. Latent Dirichlet 
allocation. Journal of Machine Learning Research 
3:993-1022. 
Bollen J., Mao H., and Zeng X.J. 2011. Twitter mood 
predicts the stock market. Journal of Computer 
Science 2(1):1-8.  
Bonanno G., Lillo F., and Mantegna R.N. 2001. High- 
frequency cross-correlation in a set of stocks, 
Quantitative Finance, Taylor and Francis Journals, 
vol. 1(1), 96-104. 
Cohen J., Cohen P., West S.G., and Aiken L.S. 2003. 
Applied Multiple Regression/Correlation Analysis 
for the Behavioral Sciences, (3rd ed.) Hillsdale, NJ: 
Lawrence Erlbaum Associates. 
Diao Q., Jiang J., Zhu F., and Lim E.P. 2012. Finding 
bursty topics from microblogs. In Proceedings of 
the 50th Annual Meeting of the Association for 
Computational Linguistics: Long Papers - Volume 
1 (ACL '12), Vol. 1. Association for Computational 
Linguistics, Stroudsburg, PA, USA, 536-544. 
Gao W., Li P., and Darwish K. 2012. Joint topic mod-
eling for event summarization across news and so-
cial media streams. CIKM 2012: 1173-1182 
Hu M. and Liu B. 2004. Mining and summarizing 
customer reviews.  In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining, 22-25. Seattle, Wash-
ington (KDD-2004). 
Jo Y. and Oh A. 2011. Aspect and sentiment unifica-
tion model for online review analysis. In ACM 
Conference in Web Search and Data Mining 
(WSDM-2011). 
Liu B. 2012. Sentiment analysis and opinion mining. 
Morgan & Claypool Publishers. 
Liu Y., Huang X., An A., and Yu X. 2007. ARSA: a 
sentiment-aware model for predicting sales per-
formance using blogs. In Proceedings of the 30th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retriev-
al, 607-614. ACM, New York, NY. 
Ma Z., Sun A., and Cong G. 2013. On predicting the 
popularity of newly emerging hashtags in Twitter. 
In Journal of the American Society for Information 
Science and Technology, 64(7): 1399-1410 (2013) 
Mantegna R. 1999. Hierarchical structure in financial 
markets, The European Physical Journal B - Con-
densed Matter and Complex Systems, Springer, vol. 
11(1), pages 193-197, September. 
Mao Y., Wei W., Wang B., and Liu B. 2012. Corre-
lating S&P 500 stocks with Twitter data. In Pro-
ceedings of the First ACM International Workshop 
on Hot Topics on Interdisciplinary Social Net-
works Research (HotSocial '12). ACM, New York, 
NY, USA, 69-72 
Mei Q., Ling X., Wondra M., Su H., and Zhai C. 2007. 
Topic sentiment mixture: modeling facets and 
opinions in weblogs. In Proceedings of Interna-
tional Conference on World Wide Web (WWW-
2007). 
Mizik N. and Jacobson R. 2003. Trading off between 
value creation and value appropriation: The finan-
cial implications of shifts in strategic emphasis. 
Journal of Marketing, 63-76. 
Onnela J.P., Chakraborti A., and Kaski K. 2003. Dy-
namics of market correlations: taxonomy and port-
folio analysis, Phys. Rev. E 68, 056110. 
Pang B. and Lee L. 2008. Opinion Mining and Senti-
ment Analysis. Now Publishers Inc. 
Porter M.E. 2008. The Five Competitive Forces That 
Shape Strategy.HBR, Harvard Business Review. 
Ramage D., Dumais S.T., and Liebling D. 2010. 
Characterizing microblogging using latent topic 
models. In Proceedings of ICWSM 2010. 
Ramage D., Hall D., Nallapati R., and Manning C.D. 
2009. Labeled LDA: A supervised topic model for 
credit attribution in multi-labeled corpora. In Pro-
ceedings of the 2009 Conference on Empirical 
Methods in Natural Language Processing (EMNLP 
2009). 
Ramage D., Manning C.D., and Dumais S.T. 2011. 
Partially labeled topic models for interpretable text 
mining. In Proceedings of KDD 2011 
Ruiz E.J., Hristidis V., Castillo C., Gionis A., and 
Jaimes A. 2012. Correlating financial time series 
with micro-blogging activity. In Proceedings of the 
fifth ACM international conference on Web search 
and data mining, pp. 513-522. ACM Press, NY 
(WSDM-2012). 
Shumway R.H. and Stoffer D.S. 2011. Time Series 
Analysis and Its Applications: With R Examples, 
3rd ed. 
Si J., Mukherjee A., Liu B., Li Q., Li H., and Deng X. 
2013. Exploiting Topic based Twitter Sentiment 
for Stock Prediction. In Proceedings of the 51st 
1144
Annual Meeting of the Association for Computa-
tional Linguistics. ACL?13, Sofia, Bulgaria, 24-29.   
Tse C.K., Liu J., and Lau F.C.M. 2010. A network 
perspective of the stock market, Journal of Empiri-
cal Finance. 17(4): 659-667. 
Vandewalle N., Brisbois F., and Tordoir X. 2001. 
Self-organized critical topology of stock markets, 
Quantit. Finan., 1, 372?375. 
Wang X., Wei F., Liu X., Zhou M., and Zhang M. 
2011. Topic sentiment analysis in twitter: a graph-
based hashtag sentiment classification approach. 
CIKM 2011: 1031-1040 
Weng J. and Lee B.S. 2011. Event Detection in Twit-
ter. In Proceedings of the International AAAI Con-
ference on Weblogs and Social Media 2011. 
Weng J.Y., Yang C.L., Chen B.N., Wang Y.K., and 
Lin S.D. 2011. IMASS: An Intelligent Microblog 
Analysis and Summarization System. ACL (Sys-
tem Demonstrations) 2011: 133-138. 
 
1145
Opinion Word Expansion and Target
Extraction through Double Propagation
Guang Qiu?
Zhejiang University, China
Bing Liu??
University of Illinois at Chicago
Jiajun Bu?
Zhejiang University, China
Chun Chen?
Zhejiang University, China
Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great
deal of attention recently due to many practical applications and challenging research problems.
In this article, we study two important problems, namely, opinion lexicon expansion and
opinion target extraction. Opinion targets (targets, for short) are entities and their attributes
on which opinions have been expressed. To perform the tasks, we found that there are several
syntactic relations that link opinion words and targets. These relations can be identified using a
dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.
This proposed method is based on bootstrapping. We call it double propagation as it propagates
information between opinion words and targets. A key advantage of the proposed method is that
it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is
semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed
method with several state-of-the-art methods using a standard product review test collection. The
results show that our approach outperforms these existing methods significantly.
1. Introduction
Opinion mining (or sentiment analysis) has attracted a great deal of attention from
researchers of natural language processing and data mining in the past few years due
? College of Computer Science, Zhejiang University, 38 Zheda Rd., Hangzhou 310027, Zhejiang, China.
E-mail: qiuguang@zju.edu.cn.
?? Department of Computer Science, University of Illinois, 851 South Morgan Street Chicago, IL 60607-7053.
E-mail: liub@cs.uic.edu.
? College of Computer Science, Zhejiang University, 38 Zheda Rd., Hangzhou 310027, Zhejiang, China.
E-mail: bjj@zju.edu.cn.
? College of Computer Science, Zhejiang University, Corresponding author, 38 Zheda Rd., Hangzhou
310027, Zhejiang, China. E-mail: chenc@zju.edu.cn.
Submission received: 2 September 2009; revised submission received: 20 January 2010; accepted for
publication: 20 July 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
to many challenging research problems and practical applications. Two fundamental
problems in opinion mining are opinion lexicon expansion and opinion target extraction
(Liu 2006; Pang and Lee 2008). An opinion lexicon is a list of opinion words such
as good, excellent, poor, and bad which are used to indicate positive or negative senti-
ments. It forms the foundation of many opinion mining tasks, for example, sentence
(Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and
Vaithyanathan 2002; Turney 2002) sentiment classification, and feature-based opinion
summarization (Hu and Liu 2004). Although there are several opinion lexicons pub-
licly available, it is hard, if not impossible, to maintain a universal opinion lexicon
to cover all domains as opinion expressions vary significantly from domain to do-
main. A word can be positive in one domain but has no opinion or even negative
opinion in another domain. Therefore, it is necessary to expand a known opinion
lexicon for applications in different domains using text corpora from the corresponding
domains.
Opinion targets are topics on which opinions are expressed. They are important
because without knowing the targets, the opinions expressed in a sentence or docu-
ment are of limited use. For example, in the opinion sentence I am not happy with the
battery life of this phone, battery life is the target of the opinion. If we do not know that,
this opinion is of little value. Although several researchers have studied the opinion
lexicon expansion and opinion target extraction (also known as topic, feature, or aspect
extraction) problems, their algorithms either need additional and external resources or
impose strong constraints and are of limited success. Detailed discussions of existing
works will be given in Section 2.
In this article, we propose a novel propagation based method to solve the opinion
lexicon expansion and target extraction problems simultaneously. Our approach differs
from existing approaches in that it requires no additional resources except an initial seed
opinion lexicon, which is readily available. Thus, it can be seen as a semi-supervised
method due to the use of the seeds. It is based on the observation that there are natural
relations between opinion words and targets due to the fact that opinion words are used
to modify targets. Furthermore, we find that opinion words and targets themselves
have relations in opinionated expressions too. These relations can be identified via
a dependency parser based on the dependency grammar (Tesniere 1959), and then
exploited to perform the extraction tasks.
The basic idea of our approach is to extract opinion words (or targets) itera-
tively using known and extracted (in previous iterations) opinion words and targets
through the identification of syntactic relations. The identification of the relations is
the key to the extractions. As our approach propagates information back and forth
between opinion words and targets, we call it double propagation. Opinion word
sentiment or polarity assignment (positive, negative, or neutral) and noisy target prun-
ing methods are also designed to refine the initially extracted results. In evaluation,
we compare our approach with several state-of-the-art existing approaches in opin-
ion lexicon expansion (or opinion word extraction) and target (or feature/topic) ex-
traction. The results show that our approach outperforms these existing approaches
significantly.
2. Related Work
Our work is related to opinion word extraction and target (or topic) extraction in
opinion mining.
10
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
2.1 Opinion Word Extraction
Extensive work has been done on sentiment analysis at word, expression (Breck,
Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and
Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and
Vaithyanathan 2002; Turney 2002) levels. We only describe work at word level as it is
most relevant to our work. In general, the existing work can be categorized as corpora-
based (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Wiebe et al 2004; Turney
and Littman 2003; Kanayama and Nasukawa 2006; Kaji and Kitsuregawa 2007) and
dictionary-based (Hu and Liu 2004; Kim and Hovy 2004; Kamps et al 2004; Esuli and
Sebastiani 2005; Takamura, Inui, and Okumura 2005) approaches. Our work falls into
the corpora-based category.
Hatzivassiloglou and McKeown (1997) proposed the first method for determining
adjective polarities or orientations (positive, negative, and neutral). The method pre-
dicts orientations of adjectives by detecting pairs of such words conjoined by conjunc-
tions such as and and or in a large document set. The underlying intuition is that the
orientations of conjoined adjectives are subject to some linguistic constraints. For exam-
ple, in the sentence This car is beautiful and spacious, if we know that beautiful is positive,
we can infer that spacious is positive too. The weakness of this method is that as it relies
on the conjunction relations it is unable to extract adjectives that are not conjoined.
Wiebe (2000) and Wiebe et al (2004) proposed an approach to finding subjective ad-
jectives using the results of word clustering according to their distributional similarity.
However, they did not tackle the prediction of sentiment polarities of the found subjec-
tive adjectives. Turney and Littman (2003) compute the point wise mutual information
(PMI) of the target term with each seed positive and negative term as a measure of their
semantic association. Their work requires additional access to the Web (or any other
corpus similar to the Web to ensure sufficient coverage), which is time consuming.
Another recent corpora-based approach is proposed by Kanayama and Nasukawa
(2006). Their work first uses clause level context coherency to find candidates, then uses
a statistical estimation method to determine whether the candidates are appropriate
opinion words. Their method for finding candidates would have low recall if the occur-
rences of seed words in the data are infrequent or an unknown opinion word has no
known opinion words in its context, however. Besides, the statistical estimation can be
unreliable if the corpus is small, which is a common problem for statistical approaches.
We will compare our approach with this approach in our experiments.
In dictionary-based approaches, Kamps et al (2004) take advantage of WordNet
to construct a synonymy network by connecting pairs of synonymous words. The
semantic orientation of a word is decided by its shortest paths to two seed words good
and bad which are chosen as representatives of positive and negative orientations.
Esuli and Sebastiani (2005) use text classification techniques to classify orientations.
Their method is based on the glosses (textual definitions) in an on-line ?glossary? or
dictionary. The work of Takamura, Inui, and Okumura (2005) also exploits the gloss
information from dictionaries. The method constructs a lexical network by linking two
words if one appears in the gloss of the other. The weights of links reflect if these two
connected words are of the same orientation. The works of Hu and Liu (2004) and Kim
and Hovy (2004) are simpler as they simply used synonyms and antonyms. However,
all dictionary-based methods are unable to find domain dependent sentiment words
because most entries in dictionaries are domain-independent. For example, unpredictable
is often a positive opinion word in movie reviews, as in unpredictable plot, but in car
reviews unpredictable is likely to be negative, as in unpredictable steering. Our approach
11
Computational Linguistics Volume 37, Number 1
extracts opinion words using domain dependent corpora; thus we are able to find
domain-dependent opinion words.
2.2 Opinion Target Extraction
Opinion target (or topic) extraction is a difficult task in opinion mining. Several methods
have been proposed, mainly in the context of product review mining (Hu and Liu 2004;
Popescu and Etzioni 2005; Kobayashi, Inui, and Matsumoto 2007; Mei et al 2007; Scaffidi
et al 2007; Wong, Lam, and Wong 2008; Stoyanov and Cardie 2008). In this mining
task, opinion targets usually refer to product features, which are defined as product
components or attributes, as in Liu (2006).
In the work of Hu and Liu (2004), frequent nouns and noun phrases are treated as
product feature candidates. In our work, we also extract only noun targets. Different
pruning methods are proposed to remove the noise. To cover infrequent features that are
missed, they regard the nearest nouns/noun phrases of the opinion words identified by
frequent features as infrequent features. In Popescu and Etzioni (2005), the authors in-
vestigated the same problem. Their extraction method, however, requires that the prod-
uct class is known in advance. The algorithm determines whether a noun/noun phrase
is a feature by computing the PMI score between the phrase and class-specific discrimi-
nators through a Web search. Querying the Web is a problem, as discussed earlier. We
will compare these two representative methods with our approach in the experiments.
In Scaffidi et al (2007), the authors proposed a language model approach to product
feature extraction with the assumption that product features are mentioned more often
in a product review than they are mentioned in general English text. However, statistics
may not be reliable when the corpus is small, as pointed out earlier.
The recent work by Kobayashi, Inui, and Matsumoto (2007) focused on the
aspect-evaluation (aspect and evaluation mean the opinion target and opinion word
respectively in our context) and aspect-of extraction problems in blogs. Their aspect-
evaluation extraction uses syntactic patterns learned via pattern mining to extract
?aspect, evaluation? pairs. Our work differs from theirs in that we make use of syntactic
relations from dependency trees. Additionally, we consider not only the relations of
opinion targets and opinion words, but also many other types of relations, as we will
see in Section 3.
In Stoyanov and Cardie (2008), the authors treated target extraction as a topic
coreference resolution problem. The key to their approach is to cluster opinions sharing
the same target together. They proposed to train a classifier to judge if two opinions are
on the same target, which indicates that their approach is supervised. Our work differs
from theirs in that our approach is semi-supervised.
Other related work on target extraction mainly uses the idea of topic modeling to
capture targets in reviews (Mei et al 2007). Topic modeling is to model the generation
of a document set and mine the implied topics in the documents. However, our experi-
ments with topic modeling show that it is only able to find some general or coarse
topics in texts and represent them as clusters of words. Their aim is thus different from
our fine-grained opinion target extraction task.
3. Relation Identification
As stated previously, identification of the relations between opinion words/targets and
other opinion words/targets is the key to our opinion lexicon expansion and target
12
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
extraction methods. In this section, we will describe the relation identification in detail.
Hereafter, for convenience, we refer to the relations between opinion words and tar-
gets as OT-Rel, between opinion words themselves as OO-Rel, and between targets as
TT-Rel.
In this work, we employ a dependency grammar to describe the relations syntacti-
cally. In the dependency grammar, a syntactic relation between two words A and B can
be described as A (or B) depends on B (or A). We define two categories to summarize all
possible dependencies between two words in sentences.
Definition 1 (Direct Dependency (DD))
A direct dependency indicates that one word depends on the other word without any
additional words in their dependency path (i.e., directly) or they both depend on a third
word directly.
Some examples are given in Figures 1 (a) and (b). In (a), A depends on B directly and
they both depend on H directly in (b).
Definition 2 (Indirect Dependency (IDD))
An indirect dependency indicates that one word depends on the other word through
some additional words (i.e., indirectly) or they both depend on a third word through
additional words.
Some examples are shown in Figures 1 (c) and (d). In (c), A depends on B through H1;
in (d), A depends on H through H1 and B depends on H through H2. Actually, IDDs
denote all possible relations apart from DDs.
Note that DDs and IDDs describe only the topology of all possible dependencies.
We then impose some constraints of the Part-of-speech (POS) tags on the opinion words
Figure 1
Different dependencies between words A and B.
13
Computational Linguistics Volume 37, Number 1
and targets, and also the potential syntactic relations on the dependency path. In this
work, we employ the Stanford POS tagging tool1 to do the POS tagging and Minipar2
as the sentence parser. We consider opinion words to be adjectives and targets to be
nouns/noun phrases, which has been widely adopted in previous work (Hu and Liu
2004; Popescu and Etzioni 2005; Mei et al 2007). Thus the potential POS tags for opinion
words are JJ (adjectives), JJR (comparative adjectives), and JJS (superlative adjectives),
whereas those for targets are NN (singular nouns) and NNS (plural nouns). The de-
pendency relations describing relations between opinion words and targets include
mod, pnmod, subj, s, obj, obj2 and desc; and the relations for opinion words and targets
themselves contain only the conjunction relation conj. Therefore, we formulate OT-Rel,
OO-Rel, or TT-Rel as a quadruple ? POS(wi), DT, R, POS(wj) ?, in which POS(wi) is the
POS tag of word wi, DT is the dependency type (i.e., DD or IDD), and R is the syntactic
relation. The values of POS(wi) and R are listed as described here.
4. Opinion Lexicon Expansion and Target Extraction
We perform the opinion lexicon expansion and target extraction tasks iteratively based
on propagation using the relations defined herein. To bootstrap the propagation, we
only require a seed opinion lexicon. Currently, we focus on one major type of opin-
ionated content, namely, product reviews, in which targets refer to product features.
Hereafter, we use target and product feature (or feature for short) interchangeably for
convenience.
Our extraction approach adopts the rule-based strategy which is quite natural given
the well-defined relations. For example, in an opinion sentence Canon G3 takes great
pictures, the adjective great is parsed as directly depending on the noun pictures through
mod, formulated as a OT-Rel quadruple ?JJ,DD,mod,NN?. If we know great is an opinion
word and are given a rule like ?a noun on which an opinion word directly depends
through mod is taken as the target,? we can easily extract pictures as the target. Similarly,
if we know pictures is a target, we could extract the adjective great as an opinion word
using a similar rule. Based on such observations, the idea of the whole propagation
approach is first to extract opinion words and targets using the seed opinion lexicon and
then use the newly extracted opinion words and targets for further target and opinion
word extraction. The propagation ends until no more new opinion words or targets can
be identified. In this way, even if the seed opinion lexicon is small, targets can still be
extracted with high recall (as we will see in the experiments) and at the same time the
opinion lexicon is also expanded.
In the following sections, we first describe the extraction rules in detail and
then demonstrate the whole propagation algorithm with a walk-through example to
show how the propagation works. For the opinion lexicon expansion, one important
issue is to assign sentiment polarities to the newly found opinion words. We pro-
pose a novel polarity assignment method to perform this task. In target extraction,
we also propose several pruning methods to remove different types of noise intro-
duced during the propagation process. We will describe these methods in Sections 4.3
and 4.4.
1 http://nlp.stanford.edu/software/tagger.shtml.
2 http://webdocs.cs.ualberta.ca/lindek/minipar.htm.
14
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
4.1 Propagation Rules Defined Based on Relations
In our propagation, there are four subtasks: (1) extracting targets using opinion words;
(2) extracting targets using the extracted targets; (3) extracting opinion words using the
extracted targets; (4) extracting opinion words using both the given and the extracted
opinion words. OT-Rels are used for tasks (1) and (3), TT-Rels are used for task (2), and
OO-Rels are used for task (4). Four types of rules are defined respectively for these four
subtasks and the details are shown in Table 1. As parsing is considerably more difficult
and error prone with informal expressions used in the Web environment, we only utilize
DD dependencies in our current approach. IDDs are more suitable for formal texts. Note
that the rules here are all domain-independent.
In the table, o (or t) stands for the output (extracted) opinion word (or target). {O}
(or {T}) is the set of known opinion words (or the set of targets) either given or extracted.
H means any word. POS(O(or T)) and O(or T)-Dep stand for the POS information and
dependency relation of the word O (or T) respectively. {JJ} and {NN} are sets of POS
tags of potential opinion words and targets, respectively. As discussed previously, {JJ}
contains JJ, JJR, and JJS; {NN} contains NN and NNS. {MR} consists of dependency
Table 1
Rules for target and opinion word extraction. Column 1 is the rule ID, column 2 is the observed
dependency and the constraint that it must satisfy (after s.t.), column 3 is the output, and
column 4 is an example. In each example, the underlined word is the known word and the
word with double quotes is the extracted word. We also show the corresponding instantiated
dependency in the parentheses.
RuleID Observations output Examples
R11 O ? O-Dep ? T s.t. O ? {O}, O-Dep ?
{MR}, POS(T) ? {NN}
t = T The phone has a good ?screen?.
(good ? mod ? screen)
R12 O ? O-Dep ? H ? T-Dep ? T s.t. O ?
{O}, O/T-Dep? {MR}, POS(T) ? {NN}
t = T ?iPod? is the best mp3 player.
(best ? mod ? player ? subj ?
iPod)
R21 O ? O-Dep ? T s.t. T ? {T}, O-Dep ?
{MR}, POS(O) ? {JJ}
o = O same as R11 with screen as the
known word and good as the
extracted word
R22 O ? O-Dep ? H ? T-Dep ? T s.t. T ?
{T}, O/T-Dep ? {MR}, POS(O) ? {JJ}
o = O same as R12 with iPod as the
known word and best as the
extract word
R31 Ti( j) ? Ti( j)-Dep ? Tj(i) s.t. Tj(i) ? {T},Ti( j)-
Dep? {CONJ},POS(Ti( j) ) ? {NN}
t = Ti( j) Does the player play dvd with
audio and ?video?? (video ?
conj ? audio)
R32 Ti ?Ti-Dep? H ?Tj-Dep? Tj s.t. Ti ?
{T},Ti-Dep==Tj-Dep,POS(Tj) ? {NN}
t = Tj Canon ?G3? has a great lens.
(lens ? obj ? has ? subj ? G3)
R41 Oi( j) ?Oi( j)-Dep? Oj(i) s.t. Oj(i) ? {O},
Oi( j)-Dep? {CONJ},POS(Oi( j) ) ? {JJ}
o = Oi( j) The camera is amazing and
?easy? to use. (easy ? conj ?
amazing)
R42 Oi ?Oi-Dep? H ?Oj-Dep? Oj s.t. Oi ?
{O},Oi-Dep==Oj-Dep,POS(Oj) ? {JJ}
o = Oj If you want to buy a sexy, ?cool?,
accessory-available mp3 player,
you can choose iPod. (sexy ?
mod ? player ? mod ? cool)
15
Computational Linguistics Volume 37, Number 1
relations describing relations between opinion words and targets (mod, pnmod, subj, s,
obj, obj2 and desc). {CONJ} contains conj only. The arrows represent dependency. For
example, O ? O-Dep ? T means O depends on T through a syntactic relation O-Dep.
?==? represents the same or equivalent (Here equivalent specifically means mod is the
same as pnmod, and s or subj is the same as obj). For example, Ti-Dep==Tj-Dep means
Ti-Dep being the same as Tj-Dep or equivalent (e.g., subj and obj in R32).
Specifically, we employ R1i to extract targets (t) using opinion words (O), R2i to
extract opinion words (o) using targets (T), R3i to extract targets (t) using extracted
targets (Ti) and R4i to extract opinion words (o) using known opinion words (Oi).
Take R11 as an example. Given the opinion word O, the word with NN as its POS tag
and satisfying the relation O-Dep is extracted as the target. For example, we have the
sentence The phone has a good screen whose corresponding dependency tree is shown in
Figure 2. If we know that good is an opinion word, and it depends on screen through mod
which is contained in {MR} and screen is tagged as NN, R11 can be applied to extract
screen as a target.
4.2 The Propagation Algorithm
Figure 3 shows the detailed algorithm. In the algorithm, the opinion word lexicon O and
review data R about a product are provided as the input. The steps are set following the
propagation order. It stops when no more new opinion words or targets can be added.
The algorithm has been explained in the preceding text. We will not repeat it here. We
now use an example to illustrate the working of the algorithm. Assume we have the
following four sentences in a review: Canon G3 takes great pictures, The picture is amazing,
You may have to get more storage to store high quality pictures and recorded movies, and The
software is amazing. We only have one input opinion word great. Using lines 4 to 6 in
the algorithm, we can extract picture as a product feature (or target) based on R11. Given
this extracted feature, we can then determine that amazing is also an opinion word using
lines 16 to 18 based on R22, and movies also as a feature using lines 13 to 15 based on
R31. In the second iteration, as amazing is recognized as an opinion word, software can
be extracted as a feature using lines 4 to 6 based on R12. The propagation then stops
as no more features or opinion words can be extracted. As we can see, through the
propagation, the three product features (i.e., the targets) and new opinion words in the
review are discovered using only a single opinion word.
4.3 Opinion Word Polarity Assignment
Polarities of opinion words are important for many opinion mining tasks. Thus, the
newly extracted opinion words should be assigned with polarities. We now propose
a polarity assignment method based on the contextual evidence. The basic intuition is
Figure 2
The dependency tree for the sentence The phone has a good screen.
16
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
Figure 3
The propagation algorithm.
that people often express their opinions in a consistent manner unless there are explicit
contrary words such as but and however. In practice, the assignment is done during the
opinion word extraction process. Before we describe our method, let us make some
observations about opinion words and targets:
Observation 1 (same polarity for same target in a review): A review is a document
written by a single reviewer. It is usually the case that the reviewer has the same senti-
ment or polarity on the same target, although the target may appear more than once in
the review.
Observation 2 (same polarity for same opinion word in a domain corpus): It is
usually the case that the same opinion word has the same polarity in one domain
corpus.
Based on these observations, we assign polarities to both newly extracted targets
and opinion words. The polarity of a target in a review is the identified sentiment
polarity on the target given in the review. The following rules are exploited to infer
polarities for extracted opinion words and targets:
1. Heterogeneous rule: For opinion words extracted by known targets, and targets
extracted by known opinion words, we assign them the same polarities as the known
ones. For example, if word A is an opinion word (or target) and B is a target (or opinion
word) and A is extracted through B, A will be assigned with the same polarity as B. Note
that targets themselves convey no polarities and opinion words are the only expressions
that people use to show their attitudes towards targets. Therefore, the polarities of
targets inherit those of associated opinion words. We also consider whether there are
negations/contrary words associated with the opinion words (by examining each word
in the surrounding 5-word window). (In our current work, the negation/contrary word
set consists of not, n?t, ?t, however, but, despite, though, except, although, oddly, and aside. We
compiled these words based on our experiences.)
2. Homogeneous rule: For opinion words extracted by known opinion words and
targets extracted by known targets, we assign them the same polarities as the known
17
Computational Linguistics Volume 37, Number 1
ones unless there are contrary words between them. For example, considering words
A and B both targets (or opinion words) and A being extracted through B, if there
are no contrary words between A and B, A will be assigned with the same polarity
as B, otherwise the opposite polarity. We also observe that these words can cancel the
polarity change when they are used together or associated with negations. Therefore, we
consider that the polarity changes only when there is an odd number of such contrary
words and negations between the two opinion words or targets.
3. Intra-review rule: There are new opinion words that are extracted by some targets
from other reviews. These targets should convey no polarities in the current review
because they do not conform to Observation 1. Hence, no polarities will be assigned to
the opinion words. Observation 2 cannot be applied either if these opinion words are
found only in the current review. To assign polarities to such opinion words, we make
use of the overall review polarity to infer. We assume that the opinion word takes the
polarity of the review, that is, if the review is positive, the opinion word is assigned with
positive polarity, otherwise negative polarity. The review polarity value is computed
as the sum of polarity values of the contained known opinion words (+1 for positive
polarity and ?1 for negative polarity). If the final sum is greater than 0, the review is
positive, and negative otherwise.
Note that, due to the two observations, multiple polarities may be assigned to an
opinion word or target. To resolve conflict, we sum the polarity values. A positive
polarity is +1 and a negative polarity is ?1. If the sum is larger than 0, the final polarity
is positive, otherwise negative.
4.4 Opinion Target Pruning
During the propagation, noise (incorrect targets and opinion words) may be introduced
besides genuine targets and opinion words. We now describe some methods for remov-
ing noisy target words. We do not perform pruning of extracted opinion words as our
current results give balanced precision and recall, which is desirable. We still do not
have an effective method for pruning opinion words to achieve better results. We will
study it in our future work.
One major type of target noise is the ordinary nouns that are not targets but are
extracted as targets due to parsing errors or their associations with opinion words or
targets. Another major kind of error in product reviews is the name of other competing
products or dealers on which the reviewers also expressed opinions. We propose two
corresponding pruning methods to identify these two types of noise. So far, all extracted
targets are individual words (such as weight, size). However, because many targets are
phrases (such as battery life), we need to identify them from the extracted individual
words. A third pruning technique is to filter the remaining non-targets after the target
phrase identification. Note that the first and third pruning techniques can be used for
other kinds of opinion texts as well as product reviews.
4.4.1 Pruning Based on Clauses. We correct the first type of errors by using the following
observation: A sentence clause usually contains only one target unless there are conjunctions
such as ?and? and ?or.? For example, in the sentence I bought apex a month ago in a
review for Apex DVD Player, both apex and month were extracted as potential targets
in some other sentences based on the rules. As these two potential targets are in the
same clause (we identify the boundary of a clause using Minipar) and are not connected
by a conjunction, one of them has to be removed. We call this method clause pruning.
18
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
In this work, we filter non-targets based on frequency. That is, the one which is less
frequent in the data set is removed. The reason for using the frequency-based pruning
is that although reviewers usually have different things to say, when they comment on
the same product features, they tend to use similar words (Hu and Liu 2004).
4.4.2 Pruning of Other Products and Dealers. We use a heuristic method to prune such
non-targets. The basic idea is that when people compare the product under review with
other products, there are indications such as compare to, better than; when they mention
dealers/merchants, the indications are patterns such as shop with. We denote the in-
dications for products as {ProductINDI} (currently including compare to, compare with,
better than, worse than) and indications for dealers as {DealerINDI} (currently including
shop with and buy from). In this heuristic method, we take the nearest nouns following
any indication in {ProductINDI} as product names and those behind any indication in
{DealerINDI} as dealer names. The distance is measured by the number of words in
between. This simple heuristic method works quite well in practice for product reviews.
However, we should note that this domain-specific pruning method does not always
need to be employed as it trades recall for precision, but does not improve F-score. Our
target extraction method is already quite strong. This pruning, however, does present
some useful options in applications.
4.4.3 Identifying Target Phrases and Global Pruning. So far all the extracted targets are
individual words. After removal of non-target individual words, we identify target
phrases. As we consider targets to be nouns/noun phrases, we identify target phrases
by combining each target word with Q consecutive nouns right before and after the
target word, and K adjectives before the target word. We set Q = 2, K = 1 in our
experiments. After obtaining the target phrases, we conduct another frequency-based
pruning to remove targets that appear only once in the whole opinion data. This global
pruning compensates for the clause pruning in case that a clause contains only one target
which is kept no matter what its frequency is. Note that in clause pruning, it is possible
to prune some words in a target phrase (with two or more words), but they will be
recovered in this step by the combination as long as one of the words in the phrase is not
pruned.
5. Experiments and Discussions
We now present the experimental results on opinion lexicon expansion and target
extraction. We use the customer review collection3 from Hu and Liu (2004) as the testing
data. The collection contains five review data sets: two on two digital cameras, one on a
DVD player, one on an mp3 player, and one on a cell phone. The detailed information
of each review data set is shown in Table 2. The targets (i.e., product features) in these
reviews are already labeled. Although the opinion expressed on each target in each
sentence is also labeled, the polarities (or orientations) of opinion words are not labeled.
In our experiments, we manually labeled the opinion words and their polarities. The
seed opinion lexicon is also provided by Hu and Liu (2004), which contains 654 positive
and 1,098 negative opinion words.
3 http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html.
19
Computational Linguistics Volume 37, Number 1
Table 2
Detailed information of the five review data sets.
Data set Number of reviews Number of sentences
D1 45 597
D2 34 346
D3 41 546
D4 95 1,716
D5 99 740
Avg 62.8 789
5.1 Experiments on Opinion Lexicon Expansion
For the comparison of our approach in opinion lexicon expansion, we implemented the
approach in Kanayama and Nasukawa (2006; referred to as KN06 hereafter). Details
about this approach were given in Section 2. We only considered adjectives as the
candidates in our experiments because our method is only concerned with adjective
opinion words. As propagation is not performed in KN06, we also implemented a non-
propagation version of our approach, in which opinion words are only extracted by the
seed words and targets which are extracted by both the seeds and extracted opinion
words. Furthermore, as our tasks can be regarded as a sequential labeling problem (to
label if a word is an opinion word, a target, or an ordinary word), we experimented with
the conditional random fields (CRF) technique (Lafferty, McCallum, and Pereira 2001)
for extraction, which is a popular information extraction method and has been success-
fully used in labeling tasks such as POS tagging (Lafferty, McCallum, and Pereira 2001)
and Named Entity Recognition (Finkel, Grenager, and Manning 2005). The well-known
toolkit CRF++4 is employed. We consider two kinds of processing windows, one using
the whole sentence (CRF); the other using words between any pair of adjective and noun
(CRF-D). In the first case, we designed seven labels for training, product features, non-
feature nouns, opinion adjectives, non-opinion adjectives, verbs, prepositions/conjunctions, and
others. In the second case, we took advantage of the relations on the shortest dependency
path between the two words and used them as labels. In this way, CRF is made to
capture long range dependencies between words. For both cases, we use the default
parameter settings in CRF++.
To train CRF for the extraction task, we use one data set for training and the
remaining four sets for testing. Consequently, we have five runs. The average results
are reported here. In the set-up of our approaches and KN06, to examine the accuracy in
extracting opinion words with different numbers of seeds we divide the initial opinion
lexicon into 10 subsets, each with roughly the same number of words. We call these lists
of opinion words the 10p lists. These ten 10p lists are combined to produce 20p, 50p,
and 80p lists which mean 20%, 50%, and 80% of the original set (1,752 opinion words),
respectively. The experiments using four kinds of seed lists are performed separately.
Note that all metrics (precision, recall, and F-score) are computed on the newly
extracted opinion words. This is an important point because only the new extractions
are meaningful. Using all the extracted words to compute precision and recall is not
4 http://crfpp.sourceforge.net/.
20
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
Figure 4
Precisions of CRF, CRF-D, KN06, noProp-dep, and Prop-dep.
appropriate as they can include many words that are already in the seed list or the
labeled training set in the case of CRF.
5.1.1 Comparison Results and Discussions. Figures 4, 5, and 6 show the average results
of precision, recall, and F-score of different approaches using different numbers of
seed opinion words. CRF and CRF-D are not evaluated against the number of seed
opinion words because that results in too few training data for CRF and CRF-D and
consequently poorer results. That is also why their results stay the same for all cases.
Prop-dep is our propagation approach and noProp-dep is the non-propagation version
of our technique.
Observing from Figure 4, we can see that our approaches, both propagation and
non-propagation versions, outperform others in all the four cases in precision. This
indicates that our rules based on the dependency relations are effective in extracting
correct opinion words. The precision of CRF is low, which means CRF has difficulty in
distinguishing ordinary adjectives from opinion ones. The better performance of CRF-D
over CRF indicates that long-range dependency relations are helpful. KN06 is reported
to have around 60% precision in the Japanese test data, but it does not perform as well in
Figure 5
Recalls of Init, CRF, CRF-D, KN06, noProp-dep, and Prop-dep.
21
Computational Linguistics Volume 37, Number 1
Figure 6
F-scores of CRF, CRF-D, KN06, noProp-dep, and Prop-dep.
our experiments. One reason could be that the statistical estimation of KN06 measures
a word?s positive or negative occurrences compared to its total occurrences, which can
introduce unreliability if words are infrequent when the corpus is small. Considering
the size of the testing data in our experiments (which is much smaller than theirs), the
estimation can be quite unreliable. Many infrequent non-opinion words are identified
as opinion words, which lowers the precision. In our technique, rules are applied in
terms of single sentences. Thus it is not sensitive to the size of the testing data. Another
observation is that in our approaches, the best performance is gained at 10p rather than
80p. This is because at 80p most of the opinion words are already known (in the seed
list) and the number of remaining ones to be extracted is small and they are usually
harder to identify.
From Figure 5, we can see that our approach makes significant improvement over
others in recall except CRF-D. Clearly, the propagation is at work. In the 10p case,
the new opinion words extracted by our approach could cover almost 75% of the
whole opinion set whereas the corresponding seed words only cover 8% of the opinion
words in the data (see the Init line). Thus our propagation method is quite powerful
in identifying a large number of new opinion words. We also notice that the results
do not change dramatically in different cases (all around 75%), which shows that the
propagation performs steadily in extracting new opinion words with a different number
of seeds (although it does introduce more noise as the number of seeds increases, as
shown in Figure 4.). CRF is found to cover only about 30% of the opinion words.
Technically, CRF captures only local patterns rather than long-range patterns. Many de-
pendency relationships are long range (i.e., there are many words between the opinion
word and the feature that it modifies), which explains the weak performance of CRF.
CRF-D performs the best (about 78%), which confirms the usefulness of long range
patterns. However, considering the large size of training data for CRF-D (the training
data set aleady contains most of the correct opinion words) and poorer precision
(Figure 4), its result is weaker than our approach. KN06 performs poorly in finding
new opinion words, which we believe is due to its strategy in selecting candidates. The
strategy only considers adjectives in successive sentences and does not use features or
any dependency relationships. Such relationships clearly exist and are useful.
Figure 6 shows the F-score results. In all four cases, our propagation approach
(Prop-dep) achieves the highest F-score. We can thus draw the conclusion that our
22
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
Figure 7
Average polarity assignment accuracy on correct new opinion words.
approach is superior to the existing methods. It indicates that rules defined based on
the relations are effective and the propagation idea is powerful.
5.1.2 Results of Polarity Assignment. Figure 7 shows the accuracy of polarity assignment of
different approaches computed on the newly discovered correct opinion words by each
approach. From the results, we can see that noProp-dep performs the best before around
65p (which means 65% of the complete opinion lexicon). Prop-dep performs worse than
KN06 but approaches it when the number of seeds increases, and outperforms KN06
from around 50p and noProp-dep from around 65p. Considering our approach has a
much higher recall, about 20% higher at 80p (Figure 5), this result is useful. At 10p,
20p, and 50p, the recall values of our methods are even higher than KN06. In the cases
of 50p and 80p, our method becomes more accurate than KN06 (Figure 7). We consider
those two cases to be realistic for practical applications because there are already several
existing opinion lexicons compiled by researchers. Thus, in practice, one does not need
to start with a very small number of seeds. Note that this does not conflict with our
earlier statement about our propagation approach?s ability in extracting a large number
of new opinion words with only a small set of seeds.
5.2 Experiments on Opinion Target Extraction
For performance evaluation on opinion target extraction, we compare our work (and
also the non-propagation version, i.e., extracting targets using only the opinion words)
with those in Hu and Liu (2004, henceforth Hu) and Popescu and Etzioni (2005, hence-
forth Popescu), which also considered only explicit noun targets and experimented with
the same data sets. Details of both approaches have been described in Section 2. Addi-
tionally, we experimented with the popular topic modeling algorithm PLSA (Hofmann
1999), using a public domain program,5 and CRF (CRF-D) using the toolkit CRF++. The
parameters and training set-ups were set the same as in the opinion word extraction
experiment. Note in this set of experiments, all our initial opinion words were used.
5 http://www.kyb.mpg.de/bs/people/pgehler/code/index.html.
23
Computational Linguistics Volume 37, Number 1
In PLSA, the maximum number of iterations was set to 500. As PLSA only clusters
words of the same rough topic together but does not perform fine-grained target ex-
traction directly, we computed the precision, recall, and F-score results by combining
the top M nouns of each cluster together as the extracted targets by PLSA. The value of
M and the number of clusters were chosen empirically. We set M as 10, 20, and 30 and
number of clusters as 10, 20, 30, 40, and 50. We used the best results based on F-scores
as the final results of each data set for PLSA. For the five data sets, the best results for
the DVD player were gained at M = 10 with the number of clusters being 30, and those
for the remaining four data sets were all gained at M = 20 with the number of clusters
being 10.
5.2.1 Comparison Results and Discussions. Tables 3, 4, and 5 show the precision, recall,
and F-score results respectively of our propagation approach (Prop-dep), the non-
propagation version (noProp-dep), Hu, Popescu, PLSA, and CRF (CRF-D).
From Table 3, we can see that on average our approach has a 16% improvement
in precision over Hu, 34% over PLSA, 28% over CRF, and 30% over CRF-D, and has
similar results to Popescu. noProp-dep performs the best, which indicates that the
rules are helpful in extracting targets, but the propagation introduces noise. The results
of Hu and Popescu are taken from their respective papers as their systems are not
available for experimentation. The Popescu approach is much more time consuming
as it needs to complete an extensive Web search and also must know the product
class in advance. Our proposed approach relies only on the review data itself and
Table 3
Precisions of our propagation approach (Prop-dep), the non-propagation version (noProp-dep),
Hu, Popescu, PLSA, CRF, and CRF-D.
Precision
Hu Popescu PLSA CRF CRF-D noProp-dep Prop-dep
D1 0.75 0.89 0.63 0.62 0.59 0.94 0.87
D2 0.71 0.87 0.48 0.64 0.58 0.97 0.90
D3 0.72 0.89 0.56 0.58 0.57 0.97 0.90
D4 0.69 0.86 0.53 0.53 0.54 0.88 0.81
D5 0.74 0.90 0.49 0.64 0.62 0.92 0.92
Avg 0.72 0.88 0.54 0.60 0.58 0.94 0.88
Table 4
Recalls of our propagation approach (Prop-dep), the non-propagation version (noProp-dep), Hu,
Popescu, PLSA, CRF, and CRF-D.
Recall
Hu Popescu PLSA CRF CRF-D noProp-dep Prop-dep
D1 0.82 0.80 0.53 0.37 0.52 0.70 0.81
D2 0.79 0.74 0.59 0.41 0.59 0.59 0.81
D3 0.76 0.74 0.56 0.30 0.45 0.67 0.86
D4 0.82 0.80 0.47 0.35 0.50 0.72 0.84
D5 0.80 0.78 0.59 0.27 0.45 0.64 0.86
Avg 0.80 0.78 0.55 0.34 0.50 0.66 0.83
24
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
Table 5
F-scores of our propagation approach (Prop-dep), the non-propagation version (noProp-dep),
Hu, Popescu, PLSA, CRF, and CRF-D.
F-score
Hu Popescu PLSA CRF CRF-D noProp-dep Prop-dep
D1 0.78 0.84 0.58 0.46 0.55 0.80 0.84
D2 0.75 0.80 0.53 0.50 0.58 0.73 0.85
D3 0.74 0.81 0.56 0.40 0.50 0.79 0.88
D4 0.75 0.83 0.49 0.42 0.52 0.79 0.82
D5 0.77 0.84 0.54 0.38 0.52 0.75 0.89
Avg 0.76 0.82 0.54 0.43 0.54 0.77 0.86
no external information is needed. The poor results of PLSA show that many of the
top-ranked terms in clusters are not genuine opinion targets but other words, such
as telephone, message, location, buyer for the DVD player data set. The low precisions
of CRF and CRF-D show that CRF performs poorly in distinguishing targets and
non-targets.
Table 4 shows that our approach outperforms all the other approaches in recall.
We have 3% improvement over Hu, 5% over Popescu, 28% over PLSA, 49% over CRF,
and 33% over CRF-D. However, if we do not perform propagation, the recall results are
much worse than Hu and Popescu, as shown in noProp-dep. Therefore, propagation
is necessary in extracting a large number of targets. The reason for the improvement
of our approach over Popescu is quite evident. Although some of the product features
tend to appear with product discriminators like product has, of product, there are many
other features that do not have high co-occurrences with these discriminators. The better
performance of CRF-D over CRF again indicates that dependency relations are helpful
in identifying more targets.
F-scores in Table 5 show that our approach outperforms Hu (by 10%), Popescu (4%),
PLSA (32%), CRF (43%), and CRF-D (32%). Paired t-tests show that all the improve-
ments are statistically significant at the confidence level of 95%. noProp-dep also has
good F-score results (even a little better than Hu) due to its high precision. The poor
results seen with PLSA and CRF indicate that these approaches may not be suitable for
this task. As analyzed earlier, PLSA is known for its ability to mine rough topics in a
large text collection by clustering topic words together based on the probability distri-
bution in the corpus. It is not suitable for the fine-grained extraction task. Therefore, a
large number of non-targets are included. CRF and CRF-D perform poorly as explained
in the opinion word extraction task.
6. Conclusions
This article focuses on two important tasks in opinion mining, namely, opinion lexicon
expansion and target extraction. We propose a propagation approach to extract opinion
words and targets iteratively given only a seed opinion lexicon of small size. The ex-
traction is performed using identified relations between opinion words and targets, and
also opinion words/targets themselves. The relations are described syntactically based
on the dependency grammar. We also propose novel methods for new opinion word
polarity assignment and noisy target pruning. In the evaluation, we compared our new
25
Computational Linguistics Volume 37, Number 1
approach with others on standard testing data sets. The results show that our approach
outperforms other state-of-the-art methods in these two tasks. In the future, we plan to
first focus on improving the precision of opinion word extraction by working on opin-
ion word pruning methods. We will then also try to learn syntactic relations automat-
ically from large corpuses using pattern mining techniques to improve the relation
coverage.
Acknowledgments
We thank Xiaowen Ding, and Lei Zhang from
University of Illinois at Chicago for helpful
discussions for our work; Xiyun Gan, Xiao
Cheng, Dazhou Wang, and Xuan Zhao from
Zhejiang University for their contributions to
the annotations. This work was supported by
National Key Technology R&D Programs
2008BAH26B00 and 2007BAH11B06.
References
Breck, Eric, Yejin Choi, and Claire Cardie.
2007. Identifying expressions of opinion
in context. In Proceedings of IJCAI?07,
pages 2683?2688. Menlo Park, CA.
Esuli, Andrea and Fabrizio Sebastiani.
2005. Determining the semantic
orientation of terms through gloss
classification. In Proceedings of CIKM?05,
pages 617?624. New York, NY.
Finkel, Jenny Rose, Trond Grenager,
and Christopher Manning. 2005.
Incorporating non-local information
into information extraction systems by
gibbs sampling. In Proceedings of ACL?05,
pages 363?370. Stroudsburg, PA.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
ACL?97, pages 174?181. Stroudsburg, PA.
Hofmann, Thomas. 1999. Probabilistic latent
semantic analysis. In Proceedings of UAI?99,
pages 289?296. San Francisco, CA.
Hu, Mingqing and Bing Liu. 2004.
Mining and summarizing customer
reviews. In Proceedings of SIGKDD?04,
pages 168?177.
Kaji, Nobuhiro and Masaru Kitsuregawa.
2007. Building lexicon for sentiment
analysis from massive collection of html
documents. In Proceedings of EMNLP?07,
pages 1075?1083.
Kamps, Jaap, Maarten Marx, Robert J.
Mokken, and Maarten de Rijke. 2004.
Using Wordnet to measure semantic
orientation of adjectives. In Proceedings
of LREC?04, pages 1115?1118.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis.
In Proceedings of EMNLP?06,
pages 355?363.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In Proceedings of COLING?04,
pages 1367?1373.
Kobayashi, Nozomi, Kentaro Inui, and
Yuji Matsumoto. 2007. Extracting
aspect-evaluation and aspect-of relations
in opinion mining. In Proceedings of
EMNLP?07, pages 1065?1074.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models
for segmenting and labeling sequence
data. In Proceedings of ICML?01,
pages 282?289.
Liu, Bing. 2006. Web Data Mining: Exploring
Hyperlinks, Contents and Usage Data.
Springer, Berlin.
Mei, Qiaozhu, Xu Ling, Matthew Wondra,
Hang Su, and ChengXiang Zhai. 2007.
Topic sentiment mixture: Modeling facets
and opinions in weblogs. In Proceedings
of WWW?07, pages 171?180.
Pang, Bo and Lillian Lee. 2008. Opinion
Mining and Sentiment Analysis. Now
Publishers Inc., Hanover, MA.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of
EMNLP?02, pages 79?86.
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
from reviews. In Proceedings of EMNLP?05,
pages 339?346.
Scaffidi, Christopher, Kevin Bierhoff,
Eric Chang, Mikhael Felker, Herman Ng,
and Chun Jin. 2007. Red opal: Product-
feature scoring from reviews. In
Proceedings of EC?07, pages 182?191.
Stoyanov, Veselin and Claire Cardie. 2008.
Topic identification for fine-grained
opinion analysis. In Proceedings of
COLING?08, pages 817?824.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
semantic orientations of words using
spin model. In Proceedings of ACL?05,
pages 133?140.
26
Qiu et al Opinion Word Expansion and Target Extraction through Double Propagation
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2007. Extracting
semantic orientations of phrases from
dictionary. In Proceedings of NAACL
HLT?07, pages 292?299.
Tesniere, Lucien. 1959. Elements de
Syntaxe Structurale. Librairie C.
Klincksieck, Paris.
Turney, Peter D. 2002. Thumbs up or
thumbs down? Semantic orientation
applied to unsupervised classification
of reviews. In Proceedings of ACL?02,
pages 417?424.
Turney, Peter D. and Michael L. Littman.
2003. Measuring praise and criticism:
Inference of semantic orientation from
association. ACM Transactions on
Information System, 21(4):315?346.
Wiebe, Janyce. 2000. Learning subjective
adjective from corpora. In Proceedings of
AAAI?00, pages 735?740.
Wiebe, Janyce, Theresa Wilson, Rebecca
Bruce, Matthew Bell, and Melanie Martin.
2004. Learning subjective language.
Computational Linguistics, 30(3):277?308.
Wong, Tak-Lam, Wai Lam, and Tik-Shun
Wong. 2008. An unsupervised framework
for extracting and normalizing product
attributes from multiple Web sites. In
Proceedings of SIGIR?08, pages 35?42.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of EMNLP?03,
pages 129?136.
27

Proceedings of NAACL-HLT 2013, pages 1041?1050,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Identifying Intention Posts in Discussion Forums 
 
 
Zhiyuan Chen, Bing Liu Meichun Hsu, Malu Castellanos,  
Riddhiman Ghosh 
Department of Computer Science HP Labs 
University of Illinois at Chicago Palo Alto, CA 94304, USA 
Chicago, IL 60607, USA {meichun.hsu, malu.castellanos, 
riddhiman.ghosh}@hp.com czyuanacm@gmail.com,liub@cs.uic.edu 
 
 
 
 
Abstract 
This paper proposes to study the problem of 
identifying intention posts in online discus-
sion forums. For example, in a discussion fo-
rum, a user wrote ?I plan to buy a camera,? 
which indicates a buying intention. This in-
tention can be easily exploited by advertisers. 
To the best of our knowledge, there is still no 
reported study of this problem. Our research 
found that this problem is particularly suited 
to transfer learning because in different do-
mains, people express the same intention in 
similar ways. We then propose a new transfer 
learning method which, unlike a general 
transfer learning algorithm, exploits several 
special characteristics of the problem. Exper-
imental results show that the proposed meth-
od outperforms several strong baselines, 
including supervised learning in the target 
domain and a recent transfer learning meth-
od. 
1 Introduction 
Social media content is increasingly regarded as 
an information gold mine. Researchers have stud-
ied many problems in social media, e.g., senti-
ment analysis (Pang & Lee, 2008; Liu, 2010) and 
social network analysis (Easley & Kleinberg, 
2010). In this paper, we study a novel problem 
which is also of great value, namely, intention 
identification, which aims to identify discussion 
posts expressing certain user intentions that can be 
exploited by businesses or other interested parties. 
For example, one user wrote, ?I am looking for a 
brand new car to replace my old Ford Focus?. 
Identifying such intention automatically can help 
social media sites to decide what ads to display so 
that the ads are more likely to be clicked. 
This work focuses on identifying user posts 
with explicit intentions. By explicit we mean that 
the intention is explicitly stated in the text, no 
need to deduce (hidden or implicit intention). For 
example, in the above sentence, the author clearly 
expressed that he/she wanted to buy a car. On the 
other hand, an example of an implicit sentence is 
?Anyone knows the battery life of iPhone?? The 
person may or may not be thinking about buying 
an iPhone. 
To our knowledge, there is no reported study of 
this problem in the context of text documents. The 
main related work is in Web search, where user 
(or query) intent classification is a major issue 
(Hu et al, 2009; Li, 2010; Li, Wang, & Acero, 
2008). Its task is to determine what the user is 
searching for based on his/her keyword queries (2 
to 3 words) and his/her click data. We will discuss 
this and other related work in Section 2. 
We formulate the proposed problem as a two-
class classification problem since an application 
may only be interested in a particular intention. 
We define intention posts (positive class) as the 
posts that explicitly express a particular intention 
of interest, e.g., the intention to buy a product. 
The other posts are non-intention posts (negative 
class). Note that we do not exploit intention spe-
cific knowledge since our aim is to propose a ge-
neric method applicable to different types of 
intentions. 
There is an important feature about this prob-
lem which makes it amenable to transfer learning 
so that we do not need to label data in every do-
main. That is, for a particular kind of intention 
such as buying, the ways to express the intention 
in different domains are often very similar. This 
1041
fact can be exploited to build a classifier based on 
labeled data in some domains and apply it to a 
new/target domain without labeling any training 
data in the target domain. However, this problem 
also has some special difficulties that existing 
general transfer learning methods do not deal 
with. The two special difficulties of the proposed 
problem are as follows: 
1. In an intention post, the intention is typically 
expressed in only one or two sentences while 
most sentences do not express intention, which 
provide very noisy data for classifiers. Fur-
thermore, words/phrases used for expressing 
intention are quite limited compared to other 
types of expressions. These mean that the set 
of shared (or common) features in different 
domains is very small. Most of the existing ad-
vanced transfer learning methods all try to ex-
tract and exploit these shared features. The 
small number of such features in our task 
makes it hard for the existing methods to find 
them accurately, which in turn learn poorer 
classifiers. 
2. As mentioned above, in different domains, the 
ways to express the same intention are often 
similar. This means that only the positive (in-
tention) features are shared among different 
domains, while features indicating the negative 
class in different domains are very diverse. We 
then have an imbalance problem, i.e., the 
shared features are almost exclusively features 
indicating the positive class. To our 
knowledge, none of the existing transfer learn-
ing methods deals with this imbalance problem 
of shared features, which also results in inaccu-
rate classifiers.  
We thus propose a new transfer learning (or do-
main adaptation) method, called Co-Class, which, 
unlike a general transfer learning method, is able 
to deal with these difficulties in solving the prob-
lem. Co-Class works as follows: we first build a 
classifier   using the labeled data from existing 
domains, called the source data, and then apply 
the classifier to classify the target (domain) data 
(which is unlabeled). Based on the target data la-
beled by  , we perform a feature selection on the 
target data. The selected set of features is used to 
build two classifiers, one (  ) from the labeled 
source data and one (  ) from the target data 
which has been labeled by  . The two classifiers 
(   and   ) then work together to perform classi-
fication of the target data. The process then runs 
iteratively until the labels assigned to the target 
data stabilize. Note that in each iteration both 
classifiers are built using the same set of features 
selected from the target domain in order to focus 
on the target domain. The proposed Co-Class ex-
plicitly deals with the difficulties mentioned 
above (see Section 3). Our experiments using four 
real-life data sets extracted from four forum dis-
cussion sites show that Co-Class outperforms sev-
eral strong baselines. What is also interesting is 
that it works even better than fully supervised 
learning in the target domain itself, i.e., using both 
training and test data in the target domain. It also 
outperforms a recent state-of-the-art transfer 
learning method (Tan et al, 2009), which has 
been successfully applied to the NLP task of sen-
timent classification.  
In summary, this paper makes two main contri-
butions: 
1. It proposes to study the novel problem of inten-
tion identification. User intention is an im-
portant type of information in social media 
with many applications. To our knowledge, 
there is still no reported study of this problem.  
2. It proposes a new transfer learning method Co-
Class which is able to exploit the above two 
key issues/characteristics of the problem in 
building cross-domain classifiers. Our experi-
mental results demonstrate its effectiveness. 
2 Related Work 
Although we have not found any paper studying 
intention classification of social media posts, there 
are some related works in the domain of Web 
search, where user or query intent classification is 
a major issue (Hu et al, 2009; Li, 2010; Li et al, 
2008). The task there is to classify a query submit-
ted to a search engine to determine what the user 
is searching for. It is different from our problem 
because they classify based on the user-submitted 
keyword queries (often 2 to 3 words) together 
with the user?s click-through data (which repre-
sent the user?s behavior). Such intents are typical-
ly implicit because people usually do not issue a 
search query like ?I want to buy a digital cam-
era.? Instead, they may just type the keywords 
?digital camera?. Our interest is to identify explic-
it intents expressed in full text documents (forum 
posts). Another related problem is online com-
mercial intention (OCI) identification (Dai et al, 
1042
2006; Hu et al, 2009), which focuses on capturing 
commercial intention based on a user query and 
web browsing history. In this sense, OCI is still a 
user query intent problem. 
In NLP, (Kanayama & Nasukawa, 2008) stud-
ied users? needs and wants from opinions. For 
example, they aimed to identify the user needs 
from sentences such as ?I?d be happy if it is 
equipped with a crisp LCD.? This is clearly dif-
ferent from our explicit intention to buy or to use 
a product/service, e.g., ?I plan to buy a new TV.? 
Our proposed Co-Class technique is related to 
transfer learning or domain adaptation. The pro-
posed method belongs to ?feature representation 
transfer" from source domain to target domain 
(Pan & Yang, 2010). Aue & Gamon (2005) tried 
training on a mixture of labeled reviews from oth-
er domains where such data are available and test 
on the target domain. This is basically one of our 
baseline methods 3TR-1TE in Section 4. Their 
work does not do multiple iterations and does not 
build two separate classifiers as we do. Some re-
lated methods were also proposed in (W. Dai, 
Xue, Yang & Yu, 2007; Tan et al, 2007; Yang, Si 
& Callan, 2006). More sophisticated transfer 
learning methods try to find common features in 
both the source and target domains and then try to 
map the differences of the two domains (Blitzer, 
Dredze, & Pereira, 2007; Pan, et al 2010; Bolle-
gala, Weir & Carroll, 2011; Tan et al, 2009). 
Some researchers also used topic modeling of 
both domains to transfer knowledge (Gao & Li, 
2011; He, Lin & Alani, 2011). However, none of 
these methods deals with the two prob-
lems/difficulties of our task. Co-Class tackles 
them explicitly and effectively (Section 4). 
The proposed Co-Class method is also related 
to Co-Training method in (Blum & Mitchell, 
1998). We will compare them in detail in Section 
3.3. 
3 The Proposed Technique 
We now present the proposed technique. Our ob-
jective is to perform classification in the target 
domain by utilizing labeled data from the source 
domains. We use the term ?source domains? as 
we can combine labeled data from multiple source 
domains. The target domain has no labeled data. 
Only the source domain data are labeled. 
To deal with the first problem in Section 1 (i.e., 
the difficulty of finding common features across 
different domains), Co-Class avoids it by using an 
EM-based method to iteratively transfer from the 
source domains to the target domain while ex-
ploiting feature selection in the target domain to 
focus on important features in the target domain. 
Since our ideas are developed starting from the 
EM (Expectation Maximization) algorithm and its 
shortcomings, we now introduce EM. 
3.1 EM Algorithm 
EM (Dempster, Laird, & Rubin, 1977) is a popu-
lar class of iterative algorithms for maximum like-
lihood estimation in problems with incomplete 
data. It is often used to address missing values in 
the data by computing expected values using ex-
isting values. The EM algorithm consists of two 
steps, the Expectation step (E-step) and the Max-
imization step (M-step). E-step basically fills in 
the missing data, and M-step re-estimates the pa-
rameters. This process iterates until convergence. 
Since our target data have no labels, which can be 
treated as missing values/data, the EM algorithm 
naturally applies. For text classification, each iter-
ation of EM (Nigam, McCallum, Thrun, & Mitch-
ell, 2000) usually uses the na?ve Bayes (NB) 
classifier. Below, we first introduce the NB classi-
fier. 
Given a set of training documents  , each doc-
ument      is an ordered list of words. We use 
      to denote the word in the position   of   , 
where each word is from the vocabulary    
         | | , which is the set of all words con-
sidered in classification. We also have a set of 
classes         representing positive and neg-
ative classes. For classification, we compute the 
posterior probability      |   . Based on the 
Bayes rule and multinomial model, we have: 
      
   (1) 
 
and with Laplacian smoothing: 
    (2) 
where          is the number of times that the 
word    occurs in document   , and   (  |  )  
      is the probability of assigning class    to   . 
Assuming that word probabilities are independent 
given a class, we have the NB classifier: 
? ?
?
? ?
?
?
?? |V|
s
|D|
i ijis
|D|
i ijit
jt d|cd,wN|V|
d|cd,wNc|w
1 1
1
))Pr((
))Pr((1)?r(
||
)|(r)(r
||
1
D
dcc
D
i ij
j
? ? ???
1043
   
  (3) 
The EM algorithm basically builds a classifier 
iteratively using NB and both the labeled source 
data and the unlabeled target data. However, the 
major shortcoming is that the feature set, even 
with feature selection, may fit the labeled source 
data well but not the target data because the target 
data has no labels to be used in feature selection. 
Feature selection is shown to be very important 
for this application as we will see in Section 4. 
3.2 FS-EM 
Based on the discussion above, the key to solve 
the problem of EM is to find a way to reflect the 
features in the target domain during the iterations. 
We propose two alternatives, FS-EM (Feature 
Selection EM) and Co-Class (Co-Classification). 
This sub-section presents FS-EM. 
EM can select features only before iterations 
using the labeled source data and keep using the 
same features in each iteration. However, these 
features only fit the labeled source data but not the 
target data. We then propose to select features 
during iterations, i.e., after each iteration, we re-
do feature selection. For this, we use the predicted 
classes of the target data. In na?ve Bayes, we de-
fine the predicted class for document    as 
 
        
    
     |    (4) 
The detailed algorithm for FS-EM is given in 
Figure 1. First, we select a feature set from the 
labeled source data    and then build an initial 
NB classifier (lines 1 and 2). The feature selection 
is based on Information Gain, which will be intro-
duced in Section 3.4. After that, we classify each 
document in the target data    to obtain its pre-
dicted class (lines 4-6). A new target data set    
is produced in line 7, which is    with added 
classes (predicted in line 5). Line 8 selects a new 
feature set   from the data    (which is discussed 
below), from which a new classifier   is built 
(line 9). The iteration stops when the predicted 
classes of    do not change any more (line 10). 
We now turn to the data set   , which can be 
formed with one of the two methods: 
1.          
2.       
The first method (called FS-EM1) merges the 
labeled source data    and the target data    
(with predicted classes). However, this method 
does not work well because the labeled source 
data can dominate    and the target domain fea-
tures are still not well represented. 
The second method (     ), denoted as FS-
EM2, selects features from the target domain data 
   only based on the predicted classes. The clas-
sifiers are built in iterations (lines 3-10) using on-
ly the target domain data. The weakness of this is 
that it completely ignores the labeled source data 
after initialization, but the source data does con-
tain some valuable information. Our final pro-
posed method Co-Class is able to solve this 
problem. 
3.3 Co-Class 
Co-Class is our final proposed algorithm. It con-
siders both the source labeled data and the target 
data with predicted classes. It uses the idea of FS-
EM, but is also inspired by Co-Training in (Blum 
& Mitchell, 1998). It additionally deals with the 
second issue identified in Section 1 (i.e., the im-
balance of shared positive and negative features). 
Co-Training is originally designed for semi-
supervised learning to learn from a small labeled 
and a large unlabeled set of training examples, 
which assumes the set of features in the data can 
be partitioned into two subsets, and each subset is 
sufficient for building an accurate classifier. The 
proposed Co-Class model is similar to Co-
Training in that it also builds two classifiers. 
However, unlike Co-Training, Co-Class does not 
partition the feature space. Instead, one classifier 
is built based on the target data with predicted 
classes (  ), and the other classifier is built using 
only the source labeled data (  ). Both classifiers 
use the same features (this is an important point) 
that are selected from the target data    only, in 
order to focus on the target domain. The final 
classification is based on both classifiers. Fur-
thermore, Co-Training only uses the data from the 
same domain. 
The detailed Co-Class algorithm is given in 
Figure 2. Lines 1-6 are the same as lines 1, 2 and 
4-7 in FS-EM. Line 8 selects new features   from 
  . Two na?ve Bayes classifiers,    and   , are 
then built using the source data    and predicted 
target data    respectively with the same set of 
? ?
?
? ?
?
??
???? ||
1
||
1 ,
||
1 ,
)|(r)(r
)|(r)(r)|(r C
r
d
k rkdr
d
k jkdj
ij i
i
i
i
cwc
cwcdc
1044
features   (lines 9-10). Lines 11-13 classify each 
target domain document    using the two classifi-
ers.  (             ) is the aggregate function to 
combine the results of two classifiers. It is defined 
as: 
 (             )  {
                          
                                      
  
This aims to deal with the imbalanced feature 
problem. As discussed before, the expressions for 
stating a particular intention (e.g., buying) are 
very similar across domains but the non-intention 
expressions across domains are highly diverse, 
which result in strong positive features and weak 
negative features. We then need to restrict the 
positive class by requiring both classifiers to give 
positive predictions. If we use the method in Co-
Training (multiplying the probabilities of the two 
NB classifiers), the classification results deterio-
rate from iteration to iteration because the positive 
class recall gets higher and higher due to strong 
positive features, but the precision gets lower and 
lower. 
Since we build and use two classifiers for the 
final classification, we call the method Co-Class, 
short for Co-Classification. Co-Class is different 
from EM (Nigam et al, 2000) in two main aspects. 
First, it integrates feature selection into the itera-
tions, which has not been done before. Feature 
selection refines features to enhance the correla-
tion between the features and classes. Second, two 
classifiers are built based on different domains 
and combined to improve the classification. Only 
one classifier is built in existing EM methods, 
which gives poorer results (Section 4). 
3.4 Feature Selection 
As feature selection is important for our task, we 
briefly introduce the Information Gain (IG) meth-
od given in (Yang & Pedersen, 1997), which is a 
popular feature selection algorithm for text classi-
fication. IG is based on entropy reflecting the pu-
rity of the categories or classes by knowing the 
presence or absence of each feature, which is de-
fined as: 
? ??
??
???
ff
m
i
ii
m
i
ii fcPfcPfPcPcPfIG
, 11
)|(log)|()()(log)()(
    
Using the IG value of each feature  , all fea-
tures can be ranked. As in normal classification 
tasks, the common practice is to use a set of top 
ranked features for classification. 
4 Evaluation 
We have conducted a comprehensive set of exper-
iments to compare the proposed Co-Class method 
with several strong baselines, including a state-of-
the-art transfer learning method. 
4.1 Experiment Settings 
Datasets: We created 4 different domain datasets 
crawled from 4 different forum discussion sites: 
Cellphone: http://www.howardforums.com/forums.php 
Electronics: http://www.avsforum.com/avs-vb/ 
Camera: http://forum.digitalcamerareview.com/ 
Algorithm FS-EM 
     Input:  Labeled data    and unlabeled data    
1   Select a feature set   based on IG from   ; 
2   Learn an initial na?ve Bayes classifier   from     
         based on   (using Equations (1) and (2)); 
3   repeat 
4       for each document    in    do 
5                  ;   // predict the class of    using   
6       end 
7       Produce data    based on predicted class of   ; 
8       Select a new feature set   from   ; 
9       Learn a new classifier   on    
based on the new feature set  ; 
10 until the predicted classes of    stabilize 
11 Return the classifier   from the last iteration. 
Figure 1 ? The FS-EM algorithm 
Algorithm Co-Class 
     Input:  Labeled data    and unlabeled data    
1   Select a feature set   based on IG from   ; 
2   Learn an initial na?ve Bayes classifier   from            
         based on   (using Equations (1) and (2)); 
3  for each document    in    do 
4              ;   // predict the class of    using   
5    end 
6   Produce data    based on the predicted class of   ; 
7   repeat 
8       Select a new feature set   from   ; 
9       Build a na?ve Bayes classifier    using   and   ; 
10     Build a na?ve Bayes classifier    using   and   ; 
11     for each document    in    do 
12             (             ); // Aggregate function 
13     end 
14     Produce data    based on predicted class of   ; 
15 until the prediction classes of    stabilize 
16 Return classifiers    and    from the last iteration. 
Figure 2 ? The Co-Class algorithm 
1045
TV: http://www.avforums.com/forums/tvs/  
For our experiments, we are interested in the in-
tention to buy, which is our intention or positive 
class. For each dataset, we manually labeled 1000 
posts. 
Labeling: We initially labeled about one fifth of 
posts by two human annotators. We found their 
labels highly agreed. We then used only one anno-
tator to complete the remaining labeling. The rea-
son for the strong labeling agreement is that we 
are interested in only explicit buying intentions, 
which are clearly expressed in each post, e.g., ?I 
am in the market for a new smartphone.? There is 
little ambiguity or subjectivity in labeling. 
To ensure that the task is realistic, for all da-
tasets we keep their original class distributions as 
they are extracted from their respective websites 
to reflect the real-life situation. The intention class 
is always the minority class, which makes it much 
harder to predict due to the imbalanced class dis-
tribution. Table 1 gives the statistics of each da-
taset. On average, each post contains about 7.5 
sentences and 122 words. We have made the da-
tasets used in this paper publically available at the 
websites of the first two authors.  
Evaluation measures: For all experiments, we 
use precision, recall and F1-score as the evalua-
tion measures. They are suitable because our ob-
jective is to identify intention posts. 
4.2 One Domain Learning 
The objective of our work is to classify the target 
domain instances without labeling any target do-
main data. To set the background, we first give 
the results of one domain learning, i.e., assuming 
that there is labeled training data in the target do-
main (which is the traditional fully supervised 
learning).  We want to see how the results of Co-
Class compare with the fully supervised learning. 
For this set of experiments, we use na?ve Bayes 
and SVM. For na?ve Bayes, we use the Lingpipe 
implementation (http://alias-i.com/lingpipe/). For 
SVM, we use SVMLight (Joachims, 1999) from 
(http://svmlight.joachims.org/) with the linear 
kernel as it has been shown by many researchers 
that linear kernel is sufficient for text classifica-
tion (Joachims, 1998; Yang and Liu, 1999). 
During labeling, we observed that the intention 
in an intention (positive) post is often expressed in 
the first few or the last few sentences. Hence, we 
tried to use the full post (denoted by Full), the first 
5 sentences (denoted by (5, 0)), and first 5 and last 
5 sentences (denoted by (5, 5)). We also experi-
mented with the first 3 sentences, and first 3 and 
last 3 sentences but their results were poorer. 
The experiments were done using 10-fold cross 
validation. For the number of selected features, 
we tried 500, 1000, 1500, 2000, 2500 and all. We 
also tried unigrams, bigrams, trigrams, and 4-
grams. To compare na?ve Bayes with SVM, we 
tried each combination, i.e. number of features 
and n-grams, and found the best model for each 
method. We found that na?ve Bayes works best 
when using trigrams with 1500 selected features. 
Bigrams with 1000 features are the best combina-
tion for SVM. Figure 3 shows the comparison of 
the best results (F1-scores) of na?ve Bayes and 
SVM. 
From Figure 3, we make the following observa-
tions: 
1. SVM does not do well for this task. We tuned 
the parameters of SVM, but the results were 
similar to the default setting, and all were 
worse than na?ve Bayes. We believe the main 
reason is that the data for this application is 
highly noisy because apart from one or two in-
tention sentences, other sentences in an inten-
tion post have little difference from those in a 
non-intention post. SVM does not perform well 
with very noisy data. When there are data 
points far away from their own classes, SVM 
Dataset 
No. of 
Intention 
No. of 
Non-Intention 
Total No. 
of posts 
Cellphone 184 816 1000 
Electronics 280 720 1000 
Camera 282 718 1000 
TV 263 737 1000 
Table 1: Datasets statistics with the buy intention 
 
 
Figure 3 ? Na?ve Bayes vs. SVM 
1046
tends to be strongly affected by such points 
(Wu & Liu, 2007). Na?ve Bayes is more robust 
in the presence of noise due to its probabilistic 
nature. 
2. SVM using only the first few and/or last few 
sentences performs better than using full posts 
because full posts have more noise. However, 
it is still worse than na?ve Bayes. 
3. For na?ve Bayes, using full posts and the first 5 
and last 5 (5, 5) sentences give similar results, 
which is not surprising as (5, 5) has almost all 
the information needed. Without using the last 
5 sentence (5, 0), the results are poorer. 
We also found that without feature selection (us-
ing all features), the results are markedly worse 
for both na?ve Bayes and SVM. This is under-
standable (as we discussed earlier) because most 
words and sentences in both intention and non-
intention posts are very similar. Thus, feature se-
lection is highly desirable for this application. 
Effect of different combinations: Table 2 gives 
the detailed F1-score results of na?ve Bayes with 
best results in different n-grams (with best number 
of features). We can see that using trigrams pro-
duces the best results on average, but bigrams and 
4-grams are quite similar. It turns out that using 
trigrams with 1500 selected features performs the 
best. SVM results are not shown as they are poor-
er. 
In summary, we say that na?ve Bayes is more 
suitable than SVM for our application and feature 
selection is crucial. In our experiments reported 
below, we will only use na?ve Bayes with feature 
selection. 
4.3 Evaluation of Co-Class 
We now compare Co-Class with the baseline 
methods listed below. Note that for this set of ex-
periments, the source data all contain labeled 
posts from three domains and the target data con-
tain unlabeled posts in one domain. That is, for 
each target domain, we merge three other domains 
for training and the target domain for testing. For 
example, for the target of ?Cellphone?, the model 
is built using the data from the other three do-
mains (i.e., ?Electronics?, ?Camera? and ?TV?). 
The results are the classification of the model on 
the target domain ?Cellphone?. Several strong 
baselines are described as follows: 
3TR-1TE: Use labeled data from three do-
mains to train and then classify the target (test) 
domain. There is no iteration. This method was 
used in (Aue & Gamon, 2005). 
EM: This is the algorithm in Section 3.1. The 
combined data from three domains are used as the 
labeled source data. The data of the remaining one 
domain are used as the unlabeled target data, 
which is also used as the test data (since it is unla-
beled). 
ANB: This is a recent transfer learning method 
(Tan et al, 2009). ANB uses frequently co-
occurring entropy (FCE) to pick out generalizable 
(or shared) features that occur frequently in both 
the source and target domains. Then, a weighted 
transfer version of na?ve Bayes classifier is ap-
plied. We chose this method for comparison as it 
is a recent method, also based on na?ve Bayes, and 
has been applied to the NLP task of sentiment 
Na?ve Bayes 
(n-grams, features) 
Cellphone Electronics Camera TV 
Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 
Unigrams, 2000 59.91 55.21 56.76 71.31 70.10 71.24 71.57 71.53 75.78 74.96 74.45 74.13 
Bigrams, 1500 61.97 54.29 59.17 70.71 71.46 72.48 77.02 74.12 77.38 79.76 77.71 79.72 
Trigrams, 1500 61.50 55.78 60.15 71.38 71.07 71.61 77.66 75.71 78.74 80.24 75.66 79.92 
4-grams, 2000 58.94 51.94 57.72 72.03 71.98 73.05 79.84 75.09 79.46 79.12 76.61 79.88 
Table 2: One-domain learning using na?ve Bayes with n-grams (with best no. of features) 
Na?ve Bayes 
(n-grams, features) 
Cellphone Electronics Camera TV 
Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 
Trigrams, 2000 57.98 57.60 58.67 71.85 69.74 71.51 74.45 73.58 74.24 74.07 71.34 73.65 
Trigrams, 2500 58.08 57.48 59.12 72.27 69.65 71.82 76.15 73.64 76.31 74.02 71.25 73.49 
Trigrams, 3000 56.74 56.94 56.74 72.27 70.76 72.43 77.62 74.65 77.62 75.64 71.65 74.73 
Trigrams, 3500 56.60 56.81 57.21 71.86 70.40 72.24 77.17 74.85 76.68 74.25 71.10 73.37 
4-grams, 2000 58.94 51.94 57.72 72.03 71.98 73.05 79.84 75.09 79.46 79.12 76.61 79.88 
Table 3: F1-scores of 3TR-1TE with trigrams and different no. of features 
 
1047
classification, which to some extend is related to 
the proposed task of intention classification. ANB 
was also shown to perform better than EM and 
na?ve Bayes transfer learning method (Dai et al, 
2007). 
We look at the results of 3TR-1TE first, which 
are shown in Table 3. Due to space limitations, we 
only show the trigrams F1-scores as they perform 
the best on average. Table 3 gives the number of 
features with trigrams. We can observe that on 
average using 3000 features gives the best F1-
score results. It has 1000 more features than one 
domain learning because we now combine three 
domains (3000 posts) for training and thus more 
useful features. 
From Table 3, we observe that the F1-score re-
sults of 3TR-1TE are worse than those of one do-
main learning (Table 2), which is intuitive 
because no training data are used from the target 
domain. But the results are not dramatically worse 
which indicate that there are some common fea-
tures in different domains, meaning people ex-
pressing the same intention in similar ways. 
Since we found that trigrams with 3000 features 
perform the best on average, we run EM, FS-
EM1, FS-EM2 and Co-Class based on trigrams 
with 3000 features. For the baseline ANB, we 
tuned the parameters using a development set 
(1/10 of the training data). We found that select-
ing 2000 generalizable/shared features gives the 
best results (the default is 500 in (Tan et al, 
2009)). We kept ANB?s other original parameter 
values. The F1-scores (averages over all 4 da-
tasets) with the number of iterations are shown in 
Figure 4. Iteration 0 is the result of 3TR-1TE. 
From Figure 4, we can make the following obser-
vations: 
1. EM makes a little improvement in iteration 1. 
After that, the results deteriorate. The gain of 
iteration 1 shows that incorporating the target 
domain data (unlabeled) is helpful. However, 
the selected features from source domains can 
only fit the labeled source data but not the tar-
get data, which was explained in Section 3.1. 
2. ANB improves slightly from iteration 1 to iter-
ation 6, but the results are all worse than those 
of Co-Class. We checked the generaliza-
ble/shared features of ANB and found that they 
were not suitable for our problem since they 
were mainly adjectives, nouns and sentiment 
verbs, which do not have strong correlation 
with intentions. This shows that it is hard to 
find the truly shared features indicating inten-
tions. Furthermore, ANB?s results are almost 
the same as those of EM. 
3. FS-EM2 behaves similarly to FS-EM1. After 
two iterations, the results start to deteriorate. 
Selecting features only from the target domain 
makes sense since it can reflect target domain 
data well. However, it also becomes worse 
with the increased number of iterations, due to 
strong positive features. With increased itera-
tions, positive features get stronger due to the 
imbalanced feature problem discussed in Sec-
tion 1. 
4. Co-Class performs much better than all other 
methods. With the increased number of itera-
tions, the results actually improve. Starting 
from iteration 7, the results stabilize. Co-Class 
solves the problem of strong positive features 
by requiring strong conditions for positive 
classification and focusing on features in the 
target domain only. Although the detailed re-
sults of precision and recall are not shown, the 
Co-Class model actually improves the F1-score 
by improving both the precision and recall.  
Significance of improvement: We now discuss 
the significance of improvements by comparing 
the results of Co-Class with other models. Table 4 
summarizes the results among the models. For 
Co-Class, we use the converged models at itera-
tion 7. We also include the One Domain learning 
results which are from fully supervised classifica-
tion in the target domains with trigrams and 1500 
features. The results of 3TR-1TE, EM, ANB, FS-
EM1, and FS-EM2 are obtained based on their 
settings which give the best results in Figure 4. 
 
Figure 4 ? Comparison EM, ANB, FS-EM1, FS-EM2, 
and Co-Class across iterations (0 is 3TR-1TE) 
1048
It is clear from Table 4 that Co-Class is the best 
method in general. It is even better than the fully 
supervised One-Domain learning, although their 
results are not strictly comparable because One-
Domain learning uses training and test data from 
the same domain via 10-fold cross validation, 
while all other methods use one domain as the test 
data (the labeled data are from the other three do-
mains). One possible reason is that the labeled 
data are much bigger than those in One-Domain 
learning, which contain more expressions of buy-
ing intention. Note that FS-EM1 and FS-EM2 
work slightly better than Co-Class in domain 
?Camera? because it is the least noisy domain 
with very short posts while other domains (as 
source data) are quite noisy. With good quality 
data, FS-EM1 and FS-EM2 (also proposed in this 
paper) can do slightly better than Co-Class. Statis-
tical paired t-test shows that Co-Class performs 
significantly better than baseline methods 3TR-
1TE, EM, ANB and FS-EM1 at the confidence 
level of 95%, and better than FS-EM2 at the con-
fidence level of 94%. 
Effect of the number of training domains: In 
our experiments above, we used 3 source domain 
data and tested on one target domain. We now 
show what happens if we use only one or two 
source domain data and test on one target domain. 
We tried all possible combinations of source and 
target data. Figure 5 gives the average results over 
the four target/test domains. We can see that using 
more source domains is better due to more labeled 
data. With more domains, Co-Class also improves 
more over 3TR-1TE. 
5 Conclusion 
This paper studied the problem of identifying in-
tention posts in discussion forums. The problem 
has not been studied in the social media context. 
Due to special characteristics of the problem, we 
found that it is particularly suited to transfer learn-
ing. A new transfer learning method, called Co-
Class, was proposed to solve the problem. Unlike 
a general transfer learning method, Co-Class can 
deal with two specific difficulties of the problem 
to produce more accurate classifiers. Our experi-
mental results show that Co-Class outperforms 
strong baselines including classifiers trained using 
labeled data in the target domains and classifiers 
from a state-of-the-art transfer learning method. 
Acknowledgments 
This work was supported in part by a grant from 
National Science Foundation (NSF) under grant 
no. IIS-1111092, and a grant from HP Labs Inno-
vation Research Program.  
References 
Aue, A., & Gamon, M. (2005). Customizing Sentiment 
Classifiers to New Domains: A Case Study. Pro-
ceedings of Recent Advances in Natural Language 
Processing (RANLP). 
Blitzer, J., Dredze, M., & Pereira, F. (2007). Biog-
raphies, Bollywood, Boom-boxes and Blenders: 
Domain Adaptation for Sentiment Classification. 
Proceedings of Annual Meeting of the Association 
for Computational Linguistics (ACL). 
Model 
Cellphone Electronics Camera TV 
Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 
One-Domain 61.50 55.78 60.15 71.38 71.07 71.61 77.66 75.71 78.74 80.24 75.66 79.92 
3TR-1TE 56.74 56.94 56.74 72.27 70.76 72.43 77.62 74.65 77.62 75.64 71.65 74.73 
EM 60.28 59.59 60.45 70.47 69.90 71.33 79.38 77.01 80.31 74.96 70.76 74.31 
ANB 62.53 59.29 62.41 66.58 68.29 68.36 78.37 77.49 78.83 78.70 75.73 78.26 
FS-EM1 59.01 57.69 59.41 70.75 71.74 72.00 80.58 76.13 80.37 79.29 73.75 77.34 
FS-EM2 59.54 60.09 61.33 71.19 72.09 72.07 80.14 77.93 81.09 78.90 74.21 77.53 
Co-Class 62.69 61.10 62.69 73.38 73.23 73.95 79.69 74.65 78.66 81.12 76.40 81.60 
Table 4: F1-score results of One-Domain, 3TR-1TE, EM, ANB, FS-EM1, FS-EM2, and Co-Class 
 
Figure 5 ? Effect of number of source domains 
using 3TR-1TE and Co-Class. 
 
 
 
 
 
 
1049
Blum, A., & Mitchell, T. (1998). Combining Labeled 
and Unlabeled Data with Co-Training. COLT: Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory. 
Bollegala, D., Weir, D. J., & Carroll, J. (2011). Using 
Multiple Sources to Construct a Sentiment Sensitive 
Thesaurus for Cross-Domain Sentiment Classifica-
tion. Proceedings of Annual Meeting of the Associa-
tion for Computational Linguistics (ACL). 
Dai, H. K., Zhao, L., Nie, Z., Wen, J. R., Wang, L., & 
Li, Y. (2006). Detecting online commercial inten-
tion (OCI). Proceedings of the 15th international 
conference on World Wide Web (WWW). 
Dai, W., Xue, G., Yang, Q., & Yu, Y. (2007). Transfer-
ring naive bayes classifiers for text classification. In 
Proceedings of the 22nd AAAI Conference on Artifi-
cial Intelligence (AAAI). 
Dempster, A., Laird, N., & Rubin, D. (1977). Maxi-
mum likelihood from incomplete data via the EM 
algorithm. Journal of the Royal Statistical Society. 
Series B, 39(1), 1?38. 
Easley, D., & Kleinberg, J. (2010). Networks, Crowds, 
and Markets: Reasoning About a Highly Connected 
World. Cambridge University Press. 
Gao, S., & Li, H. (2011). A cross-domain adaptation 
method for sentiment classification using probabilis-
tic latent analysis. Proceedings of the 20th ACM in-
ternational conference on Information and 
knowledge management (CIKM). 
He, Y., Lin, C., & Alani, H. (2011). Automatically 
Extracting Polarity-Bearing Topics for Cross-
Domain Sentiment Classification. Proceedings of 
the 49th Annual Meeting of the Association for 
Computational Linguistics: Human Language Tech-
nologies (ACL). 
Hu, D. H., Shen, D., Sun, J.-T., Yang, Q., & Chen, Z. 
(2009). Context-Aware Online Commercial Inten-
tion Detection. Proceedings of the 1st Asian Confer-
ence on Machine Learning: Advances in Machine 
Learning (ACML). 
Hu, J., Wang, G., Lochovsky, F., tao Sun, J., & Chen, 
Z. (2009). Understanding user?s query intent with 
wikipedia. Proceedings of the 18th international 
conference on World wide web (WWW). 
Joachims, T. (1998). Text Categorization with Support 
Vector Machines: Learning with Many Relevant 
Features. European Conference on Machine Learn-
ing (ECML). 
Joachims, T. (1999). Making large-Scale SVM Learn-
ing Practical. Advances in Kernel Methods - Support 
Vector Learning. MIT Press. 
Kanayama, H., & Nasukawa, T. (2008). Textual De-
mand Analysis: Detection of Users? Wants and 
Needs from Opinions. Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING). 
Li, X. (2010). Understanding the Semantic Structure of 
Noun Phrase Queries. Proceedings of Annual Meet-
ing of the Association for Computational Linguistics 
(ACL). 
Li, X., Wang, Y.-Y., & Acero, A. (2008). Learning 
query intent from regularized click graphs. Proceed-
ings of the 31st annual international ACM SIGIR 
conference on Research and development in infor-
mation retrieval (SIGIR). 
Liu, B. (2010). Sentiment Analysis and Subjectivity. 
(N. Indurkhya & F. J. Damerau, Eds.) Handbook of 
Natural Language Processing, 2nd ed. 
Nigam, K., McCallum, A. K., Thrun, S., & Mitchell, T. 
(2000). Text Classification from Labeled and Unla-
beled Documents using EM. Mach. Learn., 39(2-3), 
103?134. 
Pan, S. J., Ni, X., Sun, J.-T., Yang, Q., & Chen, Z. 
(2010). Cross-domain sentiment classification via 
spectral feature alignment. Proceedings of the 19th 
international conference on World wide web 
(WWW). 
Pan, S. J., & Yang, Q. (2010). A Survey on Transfer 
Learning. IEEE Trans. Knowl. Data Eng., 22(10), 
1345?1359. 
Pang, B., & Lee, L. (2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2), 1?135. 
Tan, S., Cheng, X., Wang, Y., & Xu, H. (2009). Adapt-
ing Naive Bayes to Domain Adaptation for Senti-
ment Analysis. Proceedings of the 31th European 
Conference on IR Research on Advances in Infor-
mation Retrieval (ECIR). 
Tan, S., Wu, G., Tang, H., & Cheng, X. (2007). A nov-
el scheme for domain-transfer problem in the con-
text of sentiment analysis. Proceedings of the 
sixteenth ACM conference on Conference on infor-
mation and knowledge management (CIKM). 
Wu, Y., & Liu, Y. (2007). Robust truncated-hinge-loss 
support vector machines. Journal of the American 
Statistical Association, 102(479), 974?983. 
Yang, H., Si, L., & Callan, J. (2006). Knowledge 
Transfer and Opinion Detection in the TREC 2006 
Blog Track. Proceedings of TREC. 
Yang, Y., & Liu, X. (1999). A re-examination of text 
categorization methods. Proceedings of the 22nd 
annual international ACM SIGIR conference on Re-
search and development in information retrieval 
(SIGIR). 
Yang, Y., & Pedersen, J. O. (1997). A Comparative 
Study on Feature Selection in Text Categorization. 
Proceedings of the Fourteenth International Con-
ference on Machine Learning (ICML). 
1050
Proceedings of the ACL 2010 Conference Short Papers, pages 359?364,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Distributional Similarity vs. PU Learning for Entity Set Expansion 
 
 
Xiao-Li  Li 
Institute for Infocomm Research,  
1 Fusionopolis Way #21-01 Connexis 
Singapore 138632 
xlli@i2r.a-star.edu.sg 
Lei Zhang 
University of Illinois at Chicago,  
851 South Morgan Street, Chicago, 
Chicago, IL 60607-7053, USA 
zhang3@cs.uic.edu 
 
Bing Liu 
University of Illinois at Chicago,  
851 South Morgan Street, Chicago, 
Chicago, IL 60607-7053, USA 
liub@cs.uic.edu 
See-Kiong  Ng 
Institute for Infocomm Research,  
1 Fusionopolis Way #21-01 Connexis 
Singapore 138632 
skng@i2r.a-star.edu.sg 
 
Abstract 
Distributional similarity is a classic tech-
nique for entity set expansion, where the 
system is given a set of seed entities of a 
particular class, and is asked to expand the 
set using a corpus to obtain more entities 
of the same class as represented by the 
seeds. This paper shows that a machine 
learning model called positive and unla-
beled learning (PU learning) can model 
the set expansion problem better. Based 
on the test results of 10 corpora, we show 
that a PU learning technique outperformed 
distributional similarity significantly.   
1 Introduction 
The entity set expansion problem is defined as 
follows: Given a set S of seed entities of a partic-
ular class, and a set D of candidate entities (e.g., 
extracted from a text corpus), we wish to deter-
mine which of the entities in D belong to S. In 
other words, we ?expand? the set S based on the 
given seeds. This is clearly a classification prob-
lem which requires arriving at a binary decision 
for each entity in D (belonging to S or not). 
However, in practice, the problem is often solved 
as a ranking problem, i.e., ranking the entities in 
D based on their likelihoods of belonging to S.  
The classic method for solving this problem is 
based on distributional similarity (Pantel et al 
2009; Lee, 1998). The approach works by com-
paring the similarity of the surrounding word 
distributions of each candidate entity with the 
seed entities, and then ranking the candidate enti-
ties using their similarity scores.   
In machine learning, there is a class of semi-
supervised learning algorithms that learns from 
positive and unlabeled examples (PU learning for 
short). The key characteristic of PU learning is 
that there is no negative training example availa-
ble for learning. This class of algorithms is less 
known to the natural language processing (NLP) 
community compared to some other semi-
supervised learning models and algorithms.  
PU learning is a two-class classification mod-
el. It is stated as follows (Liu et al 2002): Given 
a set P of positive examples of a particular class 
and a set U of unlabeled examples (containing 
hidden positive and negative cases), a classifier 
is built using P and U for classifying the data in 
U or future test cases. The results can be either 
binary decisions (whether each test case belongs 
to the positive class or not), or a ranking based 
on how likely each test case belongs to the posi-
tive class represented by P. Clearly, the set ex-
pansion problem can be mapped into PU learning 
exactly, with S and D as P and U respectively. 
This paper shows that a PU learning method 
called S-EM (Liu et al 2002) outperforms distri-
butional similarity considerably based on the 
results from 10 corpora. The experiments in-
volved extracting named entities (e.g., product 
and organization names) of the same type or 
class as the given seeds. Additionally, we also 
compared S-EM with a recent method, called 
Bayesian Sets (Ghahramani and Heller, 2005), 
which was designed specifically for set expan-
sion. It also does not perform as well as PU 
learning. We will explain why PU learning per-
forms better than both methods in Section 5. We 
believe that this finding is of interest to the NLP 
community.  
359
There is another approach used in the Web 
environment for entity set expansion. It exploits 
Web page structures to identify lists of items us-
ing wrapper induction or other techniques. The 
idea is that items in the same list are often of the 
same type. This approach is used by Google Sets 
(Google, 2008) and Boo!Wa! (Wang and Cohen, 
2008). However, as it relies on Web page struc-
tures, it is not applicable to general free texts.  
2 Three Different Techniques  
2.1 Distributional Similarity 
Distributional similarity is a classic technique for 
the entity set expansion problem. It is based on 
the hypothesis that words with similar meanings 
tend to appear in similar contexts (Harris, 1985). 
As such, a method based on distributional simi-
larity typically fetches the surrounding contexts 
for each term (i.e. both seeds and candidates) and 
represents them as vectors by using TF-IDF or 
PMI (Pointwise Mutual Information) values (Lin, 
1998; Gorman and Curran, 2006; Pa?ca et al 
2006; Agirre et al 2009; Pantel et al 2009). Si-
milarity measures such as Cosine, Jaccard, Dice, 
etc, can then be employed to compute the simi-
larities between each candidate vector and the 
seeds centroid vector (one centroid vector for all 
seeds). Lee (1998) surveyed and discussed vari-
ous distribution similarity measures.  
2.2 PU Learning and S-EM 
PU learning is a semi-supervised or partially su-
pervised learning model. It learns from positive 
and unlabeled examples as opposed to the model 
of learning from a small set of labeled examples 
of every class and a large set of unlabeled exam-
ples, which we call LU learning (L and U stand 
for labeled and unlabeled respectively) (Blum 
and Mitchell, 1998; Nigam et al 2000)  
There are several PU learning algorithms (Liu 
et al 2002; Yu et al 2002; Lee and Liu, 2003; Li 
et al 2003; Elkan and Noto, 2008). In this work, 
we used the S-EM algorithm given in (Liu et al 
2002). S-EM is efficient as it is based on na?ve 
Bayesian (NB) classification and also performs 
well. The main idea of S-EM is to use a spy 
technique to identify some reliable negatives 
(RN) from the unlabeled set U, and then use an 
EM algorithm to learn from P, RN and U?RN.  
The spy technique in S-EM works as follows 
(Figure 1): First, a small set of positive examples 
(denoted by SP) from P is randomly sampled 
(line 2). The default sampling ratio in S-EM is s 
= 15%, which we also used in our experiments. 
The positive examples in SP are called ?spies?. 
Then, a NB classifier is built using the set P? SP 
as positive and the set U?SP as negative (line 3, 
4, and 5). The NB classifier is applied to classify 
each u ? U?SP, i.e., to assign a probabilistic 
class label p(+|u) (+ means positive). The proba-
bilistic labels of the spies are then used to decide 
reliable negatives (RN). In particular, a probabili-
ty threshold t is determined using the probabilis-
tic labels of spies in SP and the input parameter l 
(noise level). Due to space constraints, we are 
unable to explain l. Details can be found in (Liu 
et al 2002). t is then used to find RN from U 
(lines 8-10). The idea of the spy technique is 
clear. Since spy examples are from P and are put 
into U in building the NB classifier, they should 
behave similarly to the hidden positive cases in 
U. Thus, they can help us find the set RN.  
Algorithm Spy(P, U, s, l) 
1.  RN ? ?;            // Reliable negative set 
2.  SP ? Sample(P, s%); 
3.  Assign each example in P ? SP the class label +1; 
4.  Assign each example in U ? SP the class label -1; 
5.  C ?NB(P ? S, U?SP); // Produce a NB classifier  
6.  Classify each u ?U?SP using C; 
7.  Decide a probability threshold t using SP and l; 
8.  for each u ?U do 
9.       if its probability p(+|u) < t then 
10.          RN ? RN ? {u}; 
Figure 1. Spy technique for extracting reliable 
negatives (RN) from U. 
Given the positive set P, the reliable negative 
set RN and the remaining unlabeled set U?RN, an 
Expectation-Maximization (EM) algorithm is 
run. In S-EM, EM uses the na?ve Bayesian clas-
sification as its base method. The detailed algo-
rithm is given in (Liu et al 2002). 
2.3 Bayesian Sets 
Bayesian Sets, as its name suggests, is based on 
Bayesian inference, and was designed specifical-
ly for the set expansion problem (Ghahramani 
and Heller, 2005). The algorithm learns from a 
seeds set (i.e., a positive set P) and an unlabeled 
candidate set U. Although it was not designed as 
a PU learning method, it has similar characteris-
tics and produces similar results as PU learning. 
However, there is a major difference. PU learn-
ing is a classification model, while Bayesian Sets 
is a ranking method. This difference has a major 
implication on the results that they produce as we 
will discuss in Section 5.3.  
In essence, Bayesian Sets learns a score func-
360
tion using P and U to generate a score for each 
unlabeled case u ? U. The function is as follows:  
                    
)(
)|(
)(
up
Pup
uscore =  (1) 
where p(u|P) represents how probable u belongs 
to the positive class represented by P. p(u) is the 
prior probability of u. Using the Bayes? rule, eq-
uation (1) can be re-written as:              
               
)()(
),(
)(
Ppup
Pup
uscore =                    (2)  
Following the idea, Ghahramani and Heller 
(2005) proposed a computable score function. 
The scores can be used to rank the unlabeled 
candidates in U to reflect how likely each u ? U 
belongs to P. The mathematics for computing the 
score is involved. Due to the limited space, we 
cannot discuss it here. See (Ghahramani and Hel-
ler, 2005) for details. In (Heller and Ghahramani, 
2006), Bayesian Sets was also applied to an im-
age retrieval application.  
3 Data Generation for Distributional 
Similarity, Bayesian Sets and S-EM 
Preparing the data for distributional similarity is 
fairly straightforward. Given the seeds set S, a 
seeds centroid vector is produced using the sur-
rounding word contexts (see below) of all occur-
rences of all the seeds in the corpus (Pantel et al 
2009). In a similar way, a centroid is also pro-
duced for each candidate (or unlabeled) entity.  
Candidate entities: Since we are interested in 
named entities, we select single words or phrases 
as candidate entities based on their correspond-
ing part-of-speech (POS) tags. In particular, we 
choose the following POS tags as entity indica-
tors ? NNP (proper noun), NNPS (plural proper 
noun), and CD (cardinal number). We regard a 
phrase (could be one word) with a sequence of 
NNP, NNPS and CD POS tags as one candidate 
entity (CD cannot be the first word unless it 
starts with a letter), e.g., ?Windows/NNP 7/CD? 
and ?Nokia/NNP N97/CD? are regarded as two 
candidates ?Windows 7? and ?Nokia N97?. 
Context: For each seed or candidate occurrence, 
the context is its set of surrounding words within 
a window of size w, i.e. we use w words right 
before the seed or the candidate and w words 
right after it. Stop words are removed.  
For S-EM and Bayesian Sets, both the posi-
tive set P (based on the seeds set S) and the unla-
beled candidate set U are generated differently. 
They are not represented as centroids.  
Positive and unlabeled sets: For each seed si ?S, 
each occurrence in the corpus forms a vector as a 
positive example in P. The vector is formed 
based on the surrounding words context (see 
above) of the seed mention. Similarly, for each 
candidate d ? D (see above; D denotes the set of 
all candidates), each occurrence also forms a 
vector as an unlabeled example in U. Thus, each 
unique seed or candidate entity may produce 
multiple feature vectors, depending on the num-
ber of times that it appears in the corpus. 
The components in the feature vectors are 
term frequencies for S-EM as S-EM uses na?ve 
Bayesian classification as its base classifier. For 
Bayesian Sets, they are 1?s and 0?s as Bayesian 
Sets only takes binary vectors based on whether 
a term occurs in the context or not.  
4 Candidate Ranking 
For distributional similarity, ranking is done us-
ing the similarity value of each candidate?s cen-
troid and the seeds? centroid (one centroid vector 
for all seeds). Rankings for S-EM and Bayesian 
Sets are more involved. We discuss them below.  
After it ends, S-EM produces a Bayesian clas-
sifier C, which is used to classify each vector u ? 
U and to assign a probability p(+|u) to indicate 
the likelihood that u belongs to the positive class. 
Similarly, Bayesian Sets produces a score 
score(u) for each u (not a probability).  
Recall that for both S-EM and Bayesian Sets, 
each unique candidate entity may generate mul-
tiple feature vectors, depending on the number of 
times that the candidate entity occurs in the cor-
pus. As such, the rankings produced by S-EM 
and Bayesian Sets are not the rankings of the 
entities, but rather the rankings of the entities? 
occurrences. Since different vectors representing 
the same candidate entity can have very different 
probabilities (for S-EM) or scores (for Bayesian 
Sets), we need to combine them and compute a 
single score for each unique candidate entity for 
ranking.  
To this end, we also take the entity frequency 
into consideration. Typically, it is highly desira-
ble to rank those correct and frequent entities at 
the top because they are more important than the 
infrequent ones in applications. With this in 
mind, we define a ranking method. 
Let the probabilities (or scores) of a candidate 
entity d ? D be Vd = {v1 , v2 ?, vn} for the n fea-
ture vectors of the candidate. Let Md be the me-
dian of Vd. The final score (fs) for d is defined as:  
    )1log()( nMdfs d +?=         (3) 
361
The use of the median of Vd can be justified 
based on the statistical skewness (Neter et al 
1993). If the values in Vd are skewed towards the 
high side (negative skew), it means that the can-
didate entity is very likely to be a true entity, and 
we should take the median as it is also high 
(higher than the mean). However, if the skew is 
towards the low side (positive skew), it means 
that the candidate entity is unlikely to be a true 
entity and we should again use the median as it is 
low (lower than the mean) under this condition.  
Note that here n is the frequency count of 
candidate entity d in the corpus. The constant 1 is 
added to smooth the value. The idea is to push 
the frequent candidate entities up by multiplying 
the logarithm of frequency. log is taken in order 
to reduce the effect of big frequency counts. 
The final score fs(d) indicates candidate d?s 
overall likelihood to be a relevant entity. A high 
fs(d) implies a high likelihood that d is in the 
expanded entity set. We can then rank all the 
candidates based on their fs(d) values.  
5 Experimental Evaluation 
We empirically evaluate the three techniques in 
this section. We implemented distribution simi-
larity and Bayesian Sets. S-EM was downloaded 
from http://www.cs.uic.edu/~liub/S-EM/S-EM-
download.html. For both Bayesian Sets and S-
EM, we used their default parameters. EM in S-
EM ran only two iterations. For distributional 
similarity, we tested TF-IDF and PMI as feature 
values of vectors, and Cosine and Jaccard as si-
milarity measures. Due to space limitations, we 
only show the results of the PMI and Cosine 
combination as it performed the best. This com-
bination was also used in (Pantel et al, 2009). 
5.1 Corpora and Evaluation Metrics 
We used 10 diverse corpora to evaluate the tech-
niques. They were obtained from a commercial 
company. The data were crawled and extracted 
from multiple online message boards and blogs 
discussing different products and services. We 
split each message into sentences, and the sen-
tences were POS-tagged using Brill?s tagger 
(Brill, 1995). The tagged sentences were used to 
extract candidate entities and their contexts. Ta-
ble 1 shows the domains and the number of sen-
tences in each corpus, as well as the three seed 
entities used in our experiments for each corpus. 
The three seeds for each corpus were randomly 
selected from a set of common entities in the ap-
plication domain.  
Table 1. Descriptions of the 10 corpora 
Domains # Sentences Seed Entities 
 Bank 17394 Citi, Chase, Wesabe 
 Blu-ray 7093 S300, Sony, Samsung 
 Car 2095 Honda, A3, Toyota 
 Drug 1504 Enbrel, Hurmia, Methotrexate 
 Insurance 12419 Cobra, Cigna, Kaiser 
 LCD 1733 PZ77U, Samsung, Sony 
 Mattress 13191 Simmons, Serta, Heavenly 
 Phone 14884 Motorola, Nokia, N95 
 Stove 25060 Kenmore, Frigidaire, GE 
 Vacuum 13491 Dc17, Hoover, Roomba 
The regular evaluation metrics for named enti-
ty recognition such as precision and recall are not 
suitable for our purpose as we do not have the 
complete sets of gold standard entities to com-
pare with. We adopt rank precision, which is 
commonly used for evaluation of entity set ex-
pansion techniques (Pantel et al, 2009):  
Precision @ N: The percentage of correct enti-
ties among the top N entities in the ranked list.  
5.2 Experimental Results 
The detailed experimental results for window 
size 3 (w=3) are shown in Table 2 for the 10 cor-
pora. We present the precisions at the top 15-, 
30- and 45-ranked positions (i.e., precisions 
@15, 30 and 45) for each corpus, with the aver-
age given in the last column. For distributional 
similarity, to save space Table 2 only shows the 
results of Distr-Sim-freq, which is the distribu-
tional similarity method with term frequency 
considered in the same way as for Bayesian Sets 
and S-EM, instead of the original distributional 
similarity, which is denoted by Distr-Sim. This 
is because on average, Distr-Sim-freq performs 
better than Distr-Sim. However, the summary 
results of both Distr-Sim-freq and Distr-Sim are 
given in Table 3.  
From Table 2, we observe that on average S-
EM outperforms Distr-Sim-freq by about 12 ? 
20% in terms of Precision @ N. Bayesian-Sets 
is also more accurate than Distr-Sim-freq, but S-
EM outperforms Bayesian-Sets by 9 ? 10%. 
To test the sensitivity of window size w, we 
also experimented with w = 6 and w = 9. Due to 
space constraints, we present only their average 
results in Table 3. Again, we can see the same 
performance pattern as in Table 2 (w = 3): S-EM 
performs the best, Bayesian-Sets the second, and 
the two distributional similarity methods the 
third and the fourth, with Distr-Sim-freq slightly 
better than Distr-Sim.  
362
5.3 Why does S-EM Perform Better? 
From the tables, we can see that both S-EM and 
Bayesian Sets performed better than distribution-
al similarity. S-EM is better than Bayesian Sets. 
We believe that the reason is as follows: Distri-
butional similarity does not use any information 
in the candidate set (or the unlabeled set U). It 
tries to rank the candidates solely through simi-
larity comparisons with the given seeds (or posi-
tive cases). Bayesian Sets is better because it 
considers U. Its learning method produces a 
weight vector for features based on their occur-
rence differences in the positive set P and the 
unlabeled set U (Ghahramani and Heller 2005). 
This weight vector is then used to compute the 
final scores used in ranking. In this way, Baye-
sian Sets is able to exploit the useful information 
in U that was ignored by distributional similarity. 
S-EM also considers these differences in its NB 
classification; in addition, it uses the reliable 
negative set (RN) to help distinguish negative 
and positive cases, which both Bayesian Sets and 
distributional similarity do not do. We believe 
this balanced attempt by S-EM to distinguish the 
positive and negative cases is the reason for the 
better performance of S-EM. This raises an inter-
esting question. Since Bayesian Sets is a ranking 
method and S-EM is a classification method, can 
we say even for ranking (our evaluation is based 
on ranking) classification methods produce better 
results than ranking methods? Clearly, our single 
experiment cannot answer this question. But in-
tuitively, classification, which separates positive 
and negative cases by pulling them towards two 
opposite directions, should perform better than 
ranking which only pulls the data in one direc-
tion. Further research on this issue is needed. 
6 Conclusions and Future Work 
Although distributional similarity is a classic 
technique for entity set expansion, this paper 
showed that PU learning performs considerably 
better on our diverse corpora. In addition, PU 
learning also outperforms Bayesian Sets (de-
signed specifically for the task). In our future 
work, we plan to experiment with various other 
PU learning methods (Liu et al 2003; Lee and 
Liu, 2003; Li et al 2007; Elkan and Noto, 2008) 
on this entity set expansion task, as well as other 
tasks that were tackled using distributional simi-
larity. In addition, we also plan to combine some 
syntactic patterns (Etzioni et al 2005; Sarmento 
et al 2007) to further improve the results.  
Acknowledgements: Bing Liu and Lei Zhang 
acknowledge the support of HP Labs Innovation 
Research Grant 2009-1062-1-A, and would like 
to thank Suk Hwan Lim and Eamonn O'Brien- 
Strain for many helpful discussions.   
Table 2.  Precision @ top N (with 3 seeds, and window size w = 3) 
 Bank Blu-ray Car  Drug Insurance LCD Mattress Phone Stove  Vacuum Avg. 
 Top 15 
Distr-Sim-freq 0.466 0.333 0.800 0.666 0.666 0.400 0.666 0.533 0.666 0.733 0.592
Bayesian-Sets 0.533 0.266 0.600 0.666 0.600 0.733 0.666 0.533 0.800 0.800 0.617
S-EM 0.600 0.733 0.733 0.733 0.533 0.666 0.933 0.533 0.800 0.933 0.720 
 Top 30 
Distr-Sim-freq 0.466 0.266 0.700 0.600 0.500 0.333 0.500 0.466 0.600 0.566 0.499 
Bayesian-Sets 0.433 0.300 0.633 0.666 0.400 0.566 0.700 0.333 0.833 0.700 0.556 
S-EM 0.500 0.700 0.666 0.666 0.566 0.566 0.733 0.600 0.600 0.833 0.643 
 Top 45 
Distr-Sim-freq 0.377 0.288 0.555 0.500 0.377 0.355 0.444 0.400 0.533 0.400 0.422 
Bayesian-Sets 0.377 0.333 0.666 0.555 0.377 0.511 0.644 0.355 0.733 0.600 0.515 
S-EM 0.466 0.688 0.644 0.733 0.533 0.600 0.644 0.555 0.644 0.688 0.620 
Table 3. Average precisions over the 10 corpora of different window size (3 seeds) 
Window-size w = 3   Window-size  w = 6  Window-size  w = 9 
Top Results Top 15 Top 30 Top 45  Top 15 Top 30 Top 45  Top 15 Top 30 Top 45
Distr-Sim 0.579 0.466 0.410  0.553 0.483 0.439  0.519 0.473 0.412 
Distr-Sim-freq 0.592 0.499 0.422  0.553 0.492 0.441  0.559 0.476 0.410 
Bayesian-Sets 0.617 0.556 0.515  0.593 0.539 0.524  0.539 0.522 0.497 
S-EM 0.720 0.643 0.620  0.666 0.606 0.597  0.666 0.620 0.604 
 
363
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., 
Pasca, M., and Soroa, A. 2009. A study on si-
milarity and relatedness using distributional 
and WordNet-based approaches. NAACL 
HLT.  
Blum, A. and Mitchell, T. 1998. Combining la-
beled and unlabeled data with co-training. In 
Proc. of Computational Learning Theory, pp. 
92?100, 1998. 
Brill, E. 1995. Transformation-Based error-
Driven learning and natural language 
processing: a case study in part of speech 
tagging. Computational Linguistics.  
Bunescu, R. and Mooney, R. 2004. Collective 
information extraction with relational Markov 
Networks. ACL.  
Cheng T., Yan X. and Chang C. K. 2007. Entity-
Rank: searching entities directly and holisti-
cally.  VLDB.  
Chieu, H.L. and Ng, H. Tou. 2002. Name entity 
recognition: a maximum entropy approach 
using global information. In The 6th Work-
shop on Very Large Corpora. 
Downey, D., Broadhead, M. and Etzioni, O. 
2007. Locating complex named entities in 
Web Text. IJCAI.  
Elkan, C. and Noto, K. 2008. Learning classifi-
ers from only positive and unlabeled data. 
KDD, 213-220.  
Etzioni, O., Cafarella, M., Downey. D., Popescu, 
A., Shaked, T., Soderland, S., Weld, D. Yates. 
2005. A. Unsupervised named-entity extrac-
tion from the Web: An Experimental Study. 
Artificial Intelligence, 165(1):91-134.  
Ghahramani, Z and Heller, K.A. 2005. Bayesian 
sets. NIPS.  
Google Sets. 2008.  System and methods for au-
tomatically creating lists. US Patent: 
US7350187, March 25. 
Gorman, J. and Curran, J. R. 2006. Scaling dis-
tributional similarity to large corpora. ACL. 
Harris, Z. Distributional Structure. 1985. In: 
Katz, J. J. (ed.), The philosophy of linguistics. 
Oxford University Press.  
Heller, K. and Ghahramani, Z. 2006. A simple 
Bayesian framework for content-based image 
retrieval. CVPR. 
Isozaki, H. and Kazawa, H. 2002. Efficient sup-
port vector classifiers for named entity recog-
nition. COLING.  
Jiang, J. and Zhai, C. 2006. Exploiting domain 
structure for named entity recognition.  HLT-
NAACL.  
Lafferty J., McCallum A., and Pereira F. 2001. 
Conditional random fields: probabilistic 
models for segmenting and labeling sequence 
data. ICML.  
Lee, L. 1999. Measures of distributional similar-
ity. ACL.  
Lee, W-S. and Liu, B. 2003. Learning with Posi-
tive and Unlabeled Examples Using Weighted 
Logistic Regression. ICML.  
Li, X., Liu, B. 2003. Learning to classify texts 
using positive and unlabeled data, IJCAI. 
Li, X., Liu, B., Ng, S. 2007. Learning to identify 
unexpected instances in the test sSet. IJCAI. 
Lin, D. 1998. Automatic retrieval and clustering 
of similar words. COLING/ACL. 
Liu, B, Lee, W-S, Yu, P. S, and Li, X. 2002. 
Partially supervised text classification. ICML, 
387-394. 
Liu, B, Dai, Y., Li, X., Lee, W-S., and Yu. P. 
2003. Building text classifiers using positive 
and unlabeled examples. ICDM, 179-188. 
Neter, J., Wasserman, W., and Whitmore, G. A. 
1993. Applied Statistics. Allyn and Bacon.  
Nigam, K., McCallum, A., Thrun, S. and Mit-
chell, T. 2000. Text classification from la-
beled and unlabeled documents using EM. 
Machine Learning, 39(2/3), 103?134.  
Pantel, P., Eric Crestan, Arkady Borkovsky, 
Ana-Maria Popescu, Vishnu, Vyas. 2009. 
Web-Scale Distributional similarity and entity 
set expansion, EMNLP.  
Pa?ca, M. Lin, D. Bigham, J. Lifchits, A. Jain, A. 
2006. Names and similarities on the web: fast 
extraction in the fast lane. ACL.  
Sarmento, L., Jijkuon, V. de Rijke, M. and 
Oliveira, E. 2007. ?More like these?: growing 
entity classes from seeds. CIKM. 
Wang, R. C. and Cohen, W. W. 2008. Iterative 
set expansion of named entities using the web. 
ICDM.  
Yu, H., Han, J., K. Chang. 2002. PEBL: Positive 
example based learning for Web page classi-
fication using SVM. KDD, 239-248. 
364
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 575?580,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying Noun Product Features that Imply Opinions 
 
 
Lei Zhang Bing Liu 
University of Illinois at Chicago University of Illinois at Chicago 
851 South Morgan Street 851 South Morgan Street 
Chicago, IL 60607, USA Chicago, IL 60607, USA 
lzhang3@cs.uic.edu liub@cs.uic.edu 
 
 
 
 
 
 
Abstract 
Identifying domain-dependent opinion 
words is a key problem in opinion mining 
and has been studied by several researchers. 
However, existing work has been focused 
on adjectives and to some extent verbs. 
Limited work has been done on nouns and 
noun phrases. In our work, we used the 
feature-based opinion mining model, and we 
found that in some domains nouns and noun 
phrases that indicate product features may 
also imply opinions. In many such cases, 
these nouns are not subjective but objective. 
Their involved sentences are also objective 
sentences and imply positive or negative 
opinions. Identifying such nouns and noun 
phrases and their polarities is very 
challenging but critical for effective opinion 
mining in these domains. To the best of our 
knowledge, this problem has not been 
studied in the literature. This paper proposes 
a method to deal with the problem. 
Experimental results based on real-life 
datasets show promising results. 
1 Introduction 
Opinion words are words that convey positive or 
negative polarities. They are critical for opinion 
mining (Pang et al, 2002; Turney, 2002; Hu and 
Liu, 2004; Wilson et al, 2004; Popescu and 
Etzioni, 2005; Gamon et al, 2005; Ku et al, 2006; 
Breck et al, 2007; Kobayashi et al, 2007; Ding et 
al., 2008; Titov and McDonald, 2008; Pang and 
Lee, 2008; Lu et al, 2009). The key difficulty in 
finding such words is that opinions expressed by 
many of them are domain or context dependent.  
Several researchers have studied the problem of 
finding opinion words (Liu, 2010). The approaches 
can be grouped into corpus-based approaches 
(Hatzivassiloglou and McKeown, 1997; Wiebe, 
2000; Kanayama and Nasukawa, 2006; Qiu et al, 
2009) and dictionary-based approaches (Hu and 
Liu 2004; Kim and Hovy, 2004; Kamps et al, 
2004; Esuli and Sebastiani, 2005; Takamura et al, 
2005; Andreevskaia and Bergler, 2006; Dragut et 
al., 2010). Dictionary-based approaches are 
generally not suitable for finding domain specific 
opinion words as dictionaries contain little domain 
specific information.  
Hatzivassiloglou and McKeown (1997) did the 
first work to tackle the problem for adjectives 
using a corpus. The approach exploits some 
conjunctive patterns, involving and, or, but, either-
or, or neither-nor, with the intuition that the 
conjoining adjectives subject to linguistic 
constraints on the orientation or polarity of the 
adjectives involved. Using these constraints, one 
can infer opinion polarities of unknown adjectives 
based on the known ones. Kanayama and 
Nasukawa (2006) improved this work by using the 
idea of coherency. They deal with both adjectives 
and verbs. Ding et al (2008) introduced the 
concept of feature context because the polarities of 
many opinion bearing words are sentence context 
dependent rather than just domain dependent. Qiu 
et al (2009) proposed a method called double 
propagation that uses dependency relations to 
extract both opinion words and product features. 
575
However, none of these approaches handle nouns 
or noun phrases. Although Zagibalov and Carroll 
(2008) noticed the issue, they did not study it.  
Esuli and Sebastiani (2006) used WordNet to 
determine polarities of words, which can include 
nouns. However, dictionaries do not contain 
domain specific information.  
Our work uses the feature-based opinion mining 
model in (Hu and Liu, 2004) to mine opinions in 
product reviews. We found that in some 
application domains product features which are 
indicated by nouns have implied opinions although 
they are not subjective words.  
This paper aims to identify such opinionated 
noun features. To make this concrete, let us see an 
example from a mattress review: ?Within a month, 
a valley formed in the middle of the mattress.?  
Here ?valley? indicates the quality of the mattress 
(a product feature) and also implies a negative 
opinion. The opinion implied by ?valley? cannot 
be found by current techniques.  
Although Riloff et al (2003) proposed a method 
to extract subjective nouns, our work is very 
different because many nouns implying opinions 
are not subjective nouns, but objective nouns, e.g., 
?valley? and ?hole? on a mattress. Those sentences 
involving such nouns are usually also objective 
sentences. As much of the existing opinion mining 
research focuses on subjective sentences, we 
believe it is high time to study objective words and 
sentences that imply opinions as well. This paper 
represents a positive step towards this direction.  
 Objective words (or sentences) that imply 
opinions are very difficult to recognize because 
their recognition typically requires the 
commonsense or world knowledge of the 
application domain. In this paper, we propose a 
method to deal with the problem, specifically, 
finding product features which are nouns or noun 
phrases and imply positive or negative opinions. 
Our experimental results show promising results. 
2 The Proposed Method  
We start with some observations. For a product 
feature (or feature for short) with an implied 
opinion, there is either no adjective opinion word 
that modifies it directly or the opinion word that 
modify it usually have the same opinion.  
Example 1: No opinion adjective word modifies 
the opinionated product feature (?valley?):  
?Within a month, a valley formed in the middle 
of the mattress.?   
Example 2: An opinion adjective modifies the 
opinionated product feature: 
?Within a month, a bad valley formed in the 
middle of the mattress.?   
Here, the adjective ?bad? modifies ?valley?. It is 
unlikely that a positive opinion word will modify 
?valley?, e.g., ?good valley? in this context. Thus, 
if a product feature is modified by both positive 
and negative opinion adjectives, it is unlikely to be 
an opinionated product feature.  
Based on these examples, we designed the 
following two steps to identify noun product 
features which imply positive or negative opinions: 
1. Candidate Identification: This step determines 
the surrounding sentiment context of each noun 
feature. The intuition is that if a feature occurs 
in negative (respectively positive) opinion 
contexts significantly more frequently than in 
positive (or negative) opinion contexts, we can 
infer that its polarity is negative (or positive). A 
statistical test is used to test the significance. 
This step thus produces a list of candidate 
features with positive opinions and a list of 
candidate features with negative opinions.  
2. Pruning: This step prunes the two lists. The 
idea is that when a noun product feature is 
directly modified by both positive and negative 
opinion words, it is unlikely to be an 
opinionated product feature.  
Basically, step 1 needs the feature-based sentiment 
analysis capability. We adopt the lexicon-based 
approach in (Ding et al 2008) in this work.  
2.1 Feature-Based Sentiment Analysis  
To use the lexicon-based sentiment analysis 
method, we need a list of opinion words, i.e., an 
opinion lexicon. Opinion words are words that 
express positive or negative sentiments. As noted 
earlier, there are also many words whose polarities 
depend on the contexts in which they appear.  
Researchers have compiled sets of opinion 
words for adjectives, adverbs, verbs and nouns 
respectively, called the opinion lexicon. In this 
paper, we used the opinion lexicon complied by 
Ding et al (2008). It is worth mentioning that our 
task is to find nouns which imply opinions in a 
specific domain, and such nouns do not appear in 
any general opinion lexicon.  
576
2.1.1.  Aggregating Opinions on a Feature  
Using the opinion lexicon, we can identify opinion 
polarity expressed on each product feature in a 
sentence. The lexicon based method in (Ding et al 
2008) basically combines opinion words in the 
sentence to assign a sentiment to each product 
feature. The sketch of the algorithm is as follows.  
Given a sentence s which contains a product 
feature f, opinion words in the sentence are first 
identified by matching with the words in the 
opinion lexicon. It then computes an orientation 
score for f. A positive word is assigned the 
semantic orientation (polarity) score of +1, and a 
negative word is assigned the semantic orientation 
score of -1. All the scores are then summed up 
using the following score formula: 
      ,),(
.)(
:
?
???
?
Lwsww i
i
iii fwdis
SOwfscore  (1) 
where wi is an opinion word, L is the set of all 
opinion words (including idioms) and s is the 
sentence that contains the feature f, and dis(wi, f) is 
the distance between feature f and opinion word wi 
in s. wi.SO is the semantic orientation (polarity) of 
word wi. The multiplicative inverse in the formula 
is used to give low weights to opinion words that 
are far away from the feature f. 
If the final score is positive, then the opinion on 
the feature in s is positive. If the score is negative, 
then the opinion on the feature in s is negative.  
2.1.2. Rules of Opinions  
Several language constructs need special handling, 
for which a set of rules is applied (Ding et al, 
2008; Liu, 2010). A rule of opinion is an 
implication with an expression on the left and an 
implied opinion on the right. The expression is a 
conceptual one as it represents a concept, which 
can be expressed in many ways in a sentence.  
Negation rule. A negation word or phrase 
usually reverses the opinion expressed in a 
sentence. Negation words include ?no,? ?not?, etc.  
In this work, we also discovered that when 
applying negation rules, a special case needs extra 
care. For example, ?I am not bothered by the hump 
on the mattress? is a sentence from a mattress 
review. It expresses a neutral feeling from the 
person. However, it also implies a negative opinion 
about ?hump,? which indicates a product feature. 
We call this kind of sentences negated feeling 
response sentences. A sentence like this normally 
expresses the feeling of a person or a group of 
persons towards some items which generally have 
positive or negative connotations in the sentence 
context or the application domain. Such a sentence 
usually consists of four components: a noun 
representing a person or a group of persons (which 
includes personal pronoun and proper noun), a 
negation word, a feeling verb, and a stimulus word. 
Feeling verbs include ?bother,? ?disturb,? ?annoy,? 
etc. The stimulus word, which stimulates the 
feeling, also indicates a feature. In analyzing such 
a sentence, for our purpose, the negation is not 
applied. Instead, we regard the sentence bearing 
the same opinion about the stimulus word as the 
opinion of the feeling verb. These opinion contexts 
will help the statistical test later.  
But clause rule. A sentence containing ?but? 
also needs special treatment. The opinion before 
?but? and after ?but? are usually the opposite to 
each other. Phrases such as ?except that? and 
?except for? behave similarly. 
Deceasing and increasing rules. These rules 
say that deceasing or increasing of some quantities 
associated with opinionated items may change the 
orientations of the opinions. For example, ?The 
drug eased my pain?. Here ?pain? is a negative 
opinion word in the opinion lexicon, and the 
reduction of ?pain? indicates a desirable effect of 
the drug. We have compiled a list of such words, 
which include ?decease?, ?diminish?, ?prevent?, 
?remove?, etc. The basic rules are as follows:   
Decreased Neg ? Positive 
E.g: ?My problem have certainly diminished? 
Decreased Pos ?  Negative 
E.g: ?These tires reduce the fun of driving.? 
Neg and Pos represent respectively a negative 
and a positive opinion word. Increasing rules do 
not change opinion directions (Liu, 2010).   
2.1.3. Handing Context-Dependent Opinions 
As mentioned earlier, context-dependent opinion 
words (only adjectives and adverbs) must be 
determined by its contexts. We solve this problem 
by using the global information rather than only 
the local information in the current sentence. We 
use a conjunction rule. For example, if someone 
writes a sentence like ?This camera is very nice 
and has a long battery life?, we can infer that 
577
?long? is positive for ?battery life? because it is 
conjoined with the positive word ?nice.? This 
discovery can be used anywhere in the corpus.   
2.2 Determining Candidate Noun Product 
Features that Imply Opinions  
Using the sentiment analysis method in section 2.1, 
we can identify opinion sentences for each product 
feature in context, which contains both positive-
opinionated sentences and negative-opinionated 
sentences. We then determine candidate product 
features implying opinions by checking the 
percentage of either positive-opinionated sentences 
or negative-opinionated sentences among all 
opinionated sentences. Through experiments, we 
make an empirical assumption that if either the 
positive-opinionated sentence percentage or the 
negative-opinionated sentence percentage is 
significantly greater than 70%, we regard this noun 
feature as a noun feature implying an opinion. The 
basic heuristic for our idea is that if a noun feature 
is more likely to occur in positive (or negative) 
opinion contexts (sentences), it is more likely to be 
an opinionated noun feature. We use a statistic 
method test for population proportion to perform 
the significant test. The details are as follows. We 
compute the Z-score statistic with one-tailed test. 
 n
pp
ppZ )1( 00
0
?
??
 (2)
 
where p0 is the hypothesized value (0.7 in our 
case), p is the sample proportion, i.e., the 
percentage of positive (or negative) opinions in our 
case, and n is the sample size, which is the total 
number of opinionated sentences that contain the 
noun feature. We set the statistical confidence level 
to 0.95, whose corresponding Z score is -1.64. It 
means that Z score for an opinionated feature must 
be no less than -1.64. Otherwise we do not regard 
it as a feature implying opinion.   
2.3 Pruning Non-Opinionated Features  
Many of candidate noun features with opinions 
may not indicate any opinion. Then, we need to 
distinguish features which have implied opinions 
and normal features which have no opinions, e.g., 
?voice quality? and ?battery life.? For normal 
features, people often can have different opinions. 
For example, for ?voice quality?, people can say 
?good voice quality? or ?bad voice quality.? 
However, for features with context dependent 
opinions, people often have a fixed opinion, either 
positive or negative but not both. With this 
observation in mind, we can detect features with 
no opinion by finding direct modification relations 
using a dependency parser. To be safe, we use only 
two types of direct relations:  
Type1: O  ? O-Dep ? F 
It means O depends on F through a relation O-
Dep. E.g: ?This TV has a good picture quality.? 
Type 2: O ? O-Dep ? H ? F-Dep ? F 
It means both O and F depends on H through 
relation O-Dep and F-Dep respectively. E.g: 
?The springs of the mattress are bad.? 
Here O is an opinion word, O-Dep / F-Dep is a 
dependency relation, which describes a relation 
between words, and includes mod, pnmod, subj, s, 
obj, obj2 and desc (detailed explanations can be 
found in http://www.cs.ualberta.ca/~lindek/ 
minipar.htm). F is a noun feature. H means any 
word. For the first example, given feature ?picture 
quality?, we can extract its modification opinion 
word ?good?. For the second example, given 
feature ?springs?, we can get opinion word ?bad?. 
Here H is the word ?are?. 
Among these extracted opinion words for the 
feature noun, if some belong to the positive 
opinion lexicon and some belong to the negative 
opinion lexicon, we conclude the noun feature is 
not an opinionated feature and is thus pruned.  
3 Experiments  
We conducted experiments using four diverse real-
life datasets of reviews. Table 1 shows the domains 
(based on their names) of the datasets, the number 
of sentences, and the number of noun features. The 
first two datasets were obtained from a commercial 
company that provides opinion mining services, 
and the other two were crawled by us. 
Product Name Mattress    Drug Router Radio 
# Sentences 13191 1541 4308 2306 
# Noun features 326 38 173 222 
Table 1.  Experimental datasets 
    An issue for judging noun features implying 
opinions is that it can be subjective. So for the gold 
standard, a consensus has to be reached between 
the two annotators.  
578
For comparison, we also implemented a baseline 
method, which decides a noun feature?s polarity 
only by its modifying opinion words (adjectives). 
If its corresponding adjective is positive-orientated, 
then the noun feature is positive-orientated. The 
same goes for a negative-orientated noun feature. 
Then using the same techniques in section 2.3 for 
statistical test (in this case, n in equation 2 is the 
total number of sentences containing the noun 
feature) and for pruning, we can determine noun 
features implying opinions from the data corpus.       
Table 2 gives the experimental results. The 
performances are measured using the standard 
evaluation measures of precision and recall. From 
Table 2, we can see that the proposed method is 
much better than the baseline method on both the 
recall and precision. It indicates many noun 
features that imply opinions are not directly 
modified by adjective opinion words. We have to 
determine their polarities based on contexts. 
Product 
Name 
Baseline Proposed Method
Precision Recall Precision Recall 
Mattress 0.35 0.07 0.48 0.82 
Drug 0.40 0.15 0.58 0.88 
Router 0.20 0.45 0.42 0.67 
Radio 0.18 0.50 0.31 0.83 
Table 2. Experimental results for noun features  
    Table 3 and Table 4 give the results of noun 
features implying positive and negative opinions 
separately. No baseline method is used here due to 
its poor results. Because for some datasets, there is 
no noun feature implying a positive/negative 
opinion, their precision and recall are zeros. 
Product Name Precision Recall 
Mattress 0.42 0.95 
Drug 0.33 1.0 
Router 0.43 0.60 
Radio 0.38 0.83 
Table 3. Features implying positive opinions 
Product Name Precision Recall 
Mattress 0.56 0.72 
Drug 0.67 0.86 
Router 0.40 1.00 
Radio 0 0 
Table 4. Features implying negative opinions 
    From Tables 2 - 4, we observe that the precision 
of the proposed method is still low, although the 
recalls are good. To better help the user find such 
words easily, we rank the extracted feature 
candidates. The purpose is to rank correct noun 
features that imply opinions at the top of the list, so 
as to improve the precision of the top-ranked 
candidates. Two ranking methods are used:  
1. rank based on the statistical score Z in equation 
2. We denote this method with Z-rank. 
2. rank based on negative/positive sentence ratio. 
We denote this method with R-rank. 
Tables 5 and 6 show the ranking results. We adopt 
the rank precision, also called the precision@N, 
metric for evaluation. It gives the percentage of 
correct noun features implying opinions at the rank 
position N. Because some domains may not 
contain positive or negative noun features, we 
combine positive and negative candidate features 
together for an overall ranking for each dataset. 
 Mattress Drug Router Radio
Z-rank 0.70 0.60 0.60 0.70 
R-rank 0.60 0.60 0.50 0.40 
Table 5. Experimental results: Precision@10 
 Mattress Drug Router Radio
Z-rank 0.66  0.46 0.53 
R-rank 0.60  0.46 0.40 
     Table 6. Experimental results: Precision@15 
    From Tables 5 and 6, we can see that the 
ranking by statistical value Z is more accurate than 
negative/positive sentence ratio. Note that in Table 
6, there is no result for the Drug dataset because no 
noun features implying opinions were found 
beyond the top 10 results because there are not 
many such noun features in the drug domain. 
4 Conclusions 
This paper proposed a method to identify noun 
product features that imply opinions. Conceptually, 
this work studied the problem of objective nouns 
and sentences with implied opinions. To the best of 
our knowledge, this problem has not been studied 
in the literature. This problem is important because 
without identifying such opinions, the recall of 
opinion mining suffers. Our proposed method 
determines feature polarity not only by opinion 
words that modify the features but also by its 
surrounding context. Experimental results show 
that the proposed method is promising. Our future 
work will focus on improving the precision.    
579
References  
Andreevskaia, A. and S. Bergler. 2006. Mining 
WordNet for fuzzy sentiment: Sentiment tag 
extraction from WordNet glosses. Proceedings of 
EACL 2006. 
Eric Breck, Yejin Choi, and Claire Cardie. 2007. 
Identifying Expressions of Opinion in Context. 
Proceedings of IJCAI 2007. 
Xiaowen Ding, Bing Liu and Philip S. Yu. 2008 A 
Holistic Lexicon-Based Approach to Opinion 
Mining. Proceedings of WSDM 2008. 
Eduard C. Dragut, Clement Yu, Prasad Sistla, and 
Weiyi Meng. 2010. Construction of a sentimental 
word dictionary. In Proceedings of CIKM 
2010.Andrea Esuli and Fabrizio Sebastiani. 2005. 
Determining the Semantic Orientation of Terms 
through Gloss Classification. Proceedings of CIKM 
2005. 
Andrea Esuli and Fabrizio Sebastiani. 2006. 
SentiWorkNet: A Publicly Available Lexical 
Resource for Opinion Mining. Proceedings of LREC 
2006. 
Michael Gamon. 2004. Sentiment Classification on 
Customer Feedback Data: Noisy Data, Large Feature 
Vectors and the Role of Linguistic Analysis. 
Proceedings of COLING 2004. 
Murthy Ganapathibhotla. and Bing Liu. 2008. Mining 
opinions in comparative sentences. Proceedings of 
COLING 2008. 
Vasileios Hatzivassiloglou and Kathleen, McKeown. 
1997. Predicting the Semantic Orientation of 
Adjectives. Proceedings of ACL 1997. 
Minqing Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. Proceedings of 
KDD 2004. 
Jaap Kamps, Maarten Marx, Robert J, Mokken and 
Maarten de Rijke. 2004. Proceedings of LREC 2004. 
Hiroshi Kanayama, Tetsuya Nasukawa 2006. Fully 
Automatic Lexicon Expansion for Domain-Oriented 
Sentiment Analysis. Proceedings of EMNLP 2006.  
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 
2007. Extracting aspect-evaluation and aspect-of 
relations in opinion mining. Proceedings of EMLP 
2007 
Soo-Min Kim and Eduard Hovy. 2004. Determining the 
Sentiment of Opinions. Proceedings of COLING 
2004. 
Lun-Wei Ku,Yu-Ting Liang, and Hsin-Hsi Chen. 2006. 
Opinion extraction, summarization and tracking in 
news and blog corpora. Proceedings of AAAI-CAAW 
2006. 
Bing Liu. 2010. Sentiment analysis and subjectivity. A 
chapter in Handbook of Natural Language 
Processing, Second edition. 
Yue Lu, Chengxiang Zhai, and Neel Sundaresan. 2009. 
Rated aspect summarization of short comments. 
Proceedings of WWW 2009. 
Bo Pang and Lillian Lee. 2008. Opinion Mining and 
sentiment Analysis. Foundations and Trends in 
Information Retrieval 2(1-2), 2008. 
Bo Pang,  Lillian Lee and Shivakumar  Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. Proceedings of  
EMNLP 2002. 
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting 
Product Features and Opinions from Reviews. 
Proceedings of EMNLP 2005. 
Guang Qiu, Bing Liu, Jiajun  Bu and Chun Chen. 2009. 
Expanding Domain Sentiment Lexicon through 
Double Propagation. Proceedings of IJCAI 2009.  
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. Proceedings of CoNLL 2003. 
Hiroya Takamura, Takashi Inui and Manabu Okumura. 
2007. Extracting Semantic Orientations of Phrases 
from Dictionary. Proceedings of HLT-NAACL 2007. 
Ivan Titov and Ryan McDonald. 2008. A joint model of 
text and aspect ratings for sentiment summarization. 
In Proceedings of ACL 2008.Peter D. Turney. 2002. 
Thumbs Up or Thumbs Down? Semantic Orientation 
Applied to Unsupervised Classification of Reviews. 
Proceedings of ACL 2002.  
Janyce Wiebe. 2000. Learning Subjective Adjectives 
from Corpora. Proceedings of AAAI 2000. 
Theresa Wilson, Janyce Wiebe, Rebecca Hwa. 2004. 
Just how mad are you? Finding strong and weak 
opinion clauses. Proceedings of AAAI 2004. 
Taras Zagibalov and John Carroll. 2008. Unsupervised 
Classification of Sentiment and Objectivity in 
Chinese Text. Proceedings of IJCNLP 2008.     
 
 
580
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 320?329,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modeling Review Comments 
 
 
Arjun Mukherjee Bing Liu 
Department of Computer Science Department of Computer Science 
University of Illinois at Chicago University of Illinois at Chicago 
Chicago, IL 60607, USA Chicago, IL 60607, USA 
arjun4787@gmail.com liub@cs.uic.edu 
 
 
 
 
 
 
Abstract 
Writing comments about news articles, 
blogs, or reviews have become a popular 
activity in social media. In this paper, we 
analyze reader comments about reviews. 
Analyzing review comments is important 
because reviews only tell the experiences 
and evaluations of reviewers about the 
reviewed products or services. Comments, 
on the other hand, are readers? evaluations 
of reviews, their questions and concerns. 
Clearly, the information in comments is 
valuable for both future readers and brands. 
This paper proposes two latent variable 
models to simultaneously model and 
extract these key pieces of information. 
The results also enable classification of 
comments accurately. Experiments using 
Amazon review comments demonstrate the 
effectiveness of the proposed models. 
1. Introduction 
Online reviews enable consumers to evaluate the 
products and services that they have used. These 
reviews are also used by other consumers and 
businesses as a valuable source of opinions.  
However, reviews only give the evaluations and 
experiences of the reviewers. Often a reviewer may 
not be an expert of the product and may misuse the 
product or make other mistakes. There may also be 
aspects of the product that the reviewer did not 
mention but a reader wants to know. Some 
reviewers may even write fake reviews to promote 
some products, which is called opinion spamming 
(Jindal and Liu 2008). To improve the online 
review system and user experience, some review 
hosting sites allow readers to write comments 
about reviews (apart from just providing a 
feedback by clicking whether the review is helpful 
or not). Many reviews receive a large number of 
comments. It is difficult for a reader to read them 
to get a gist of them. An automated comment 
analysis would be very helpful. Review comments 
mainly contain the following information:   
Thumbs-up or thumbs-down: Some readers may 
comment on whether they find the review 
useful in helping them make a buying decision.  
Agreement or disagreement: Some readers who 
comment on a review may be users of the 
product themselves. They often state whether 
they agree or disagree with the review. Such 
comments are valuable as they provide a second 
opinion, which may even identify fake reviews 
because a genuine user often can easily spot 
reviewers who have never used the product.  
Question and answer: A commenter may ask for 
clarification or about some aspects of the 
product that are not covered in the review. 
In this paper, we use statistical modeling to model 
review comments. Two new generative models are 
proposed. The first model is called the Topic and 
Multi-Expression model (TME). It models topics 
and different types of expressions, which represent 
different types of comment posts: 
1. Thumbs-up (e.g., ?review helped me?) 
2. Thumbs-down (e.g., ?poor review?) 
3. Question (e.g., ?how to?) 
320
4. Answer acknowledgement (e.g., ?thank you for 
clarifying?). Note that we have no expressions 
for answers to questions as there are usually no 
specific phrases indicating that a post answers 
a question except starting with the name of the 
person who asked the question. However, there 
are typical phrases for acknowledging answers, 
thus answer acknowledgement expressions.  
5. Disagreement (contention) (e.g., ?I disagree?)  
6. Agreement (e.g., ?I agree?). 
For ease of presentation, we call these 
expressions the comment expressions (or C-
expressions). TME provides a basic model for 
extracting these pieces of information and topics. 
Its generative process separates topics and C-
expression types using a switch variable and treats 
posts as random mixtures over latent topics and C-
expression types. The second model, called ME-
TME, improves TME by using Maximum-Entropy 
priors to guide topic/expression switching. In short, 
the two models provide a principled and integrated 
approach to simultaneously discover topics and C-
expressions, which is the goal of this work. Note 
that topics are usually product aspects in this work.  
The extracted C-expressions and topics from 
review comments are very useful in practice. First 
of all, C-expressions enable us to perform more 
accurate classification of comments, which can 
give us a good evaluation of the review quality and 
credibility. For example, a review with many 
Disagreeing and Thumbs-down comments is 
dubious. Second, the extracted C-expressions and 
topics help identify the key product aspects that 
people are troubled with in disagreements and in 
questions. Our experimental results in Section 5 
will demonstrate these capabilities of our models. 
With these pieces of information, comments for 
a review can be summarized. The summary may 
include, but not limited to, the following: (1) 
percent of people who give the review thumbs-up 
or thumbs-down; (2) percent of people who agree 
or disagree (or contend) with the reviewer; (3) 
contentious (disagreed) aspects (or topics); (4) 
aspects about which people often have questions. 
To the best of our knowledge, there is no 
reported work on such a fine-grained modeling of 
review comments. The related works are mainly in 
sentiment analysis (Pang and Lee, 2008; Liu 
2012), e.g., topic and sentiment modeling, review 
quality prediction and review spam detection. 
However, our work is different from them. We will 
compare with them in detail in Section 2.  
The proposed models have been evaluated both 
qualitatively and quantitatively using a large 
number of review comments from Amazon.com. 
Experimental results show that both TME and ME-
TME are effective in performing their tasks. ME-
TME also outperforms TME significantly.   
2. Related Work 
We believe that this work is the first attempt to 
model review comments for fine-grained analysis. 
There are, however, several general research areas 
that are related to our work. 
Topic models such as LDA (Latent Dirichlet 
Allocation) (Blei et al, 2003) have been used to 
mine topics in large text collections. There have 
been various extensions to multi-grain (Titov and 
McDonald, 2008a), labeled (Ramage et al, 2009), 
partially-labeled (Ramage et al, 2011), constrained 
(Andrzejewski et al, 2009) models, etc. These 
models produce only topics but not multiple types 
of expressions together with topics. Note that in 
labeled models, each document is labeled with one 
or multiple labels. For our work, there is no label 
for each comment. Our labeling is on topical terms 
and C-expressions with the purpose of obtaining 
some priors to separate topics and C-expressions. 
In sentiment analysis, researchers have jointly 
modeled topics and sentiment words (Lin and He, 
2009; Mei et al, 2007; Lu and Zhai, 2008; Titov 
and McDonald, 2008b; Lu et al, 2009; Brody and 
Elhadad, 2010; Wang et al, 2010; Jo and Oh, 
2011; Maghaddam and Ester, 2011; Sauper et al, 
2011; Mukherjee and Liu, 2012a). Our model is 
more related to the ME-LDA model in (Zhao et al, 
2010), which used a switch variable trained with 
Maximum-Entropy to separate topic and sentiment 
words. We also use such a variable. However, 
unlike sentiments and topics in reviews, which are 
emitted in the same sentence, C-expressions often 
interleave with topics across sentences and the 
same comment post may also have multiple types 
of C-expressions. Additionally, C-expressions are 
mostly phrases rather than individual words. Thus, 
a different model is required to model them. 
There have also been works aimed at putting 
authors in debate into support/oppose camps, e.g., 
(Galley et al, 2004; Agarwal et al, 2003; 
Murakami and Raymond, 2010), modeling debate 
discussions considering reply relations (Mukherjee 
and Liu, 2012b), and identifying stances in debates 
(Somasundaran and Wiebe, 2009; Thomas et al, 
321
2006; Burfoot et al, 2011). (Yano and Smith, 
2010) also modeled the relationship of a blog post 
and the number of comments it receives. These 
works are different as they do not mine C-
expressions or discover the points of contention 
and questions in comments. 
In (Kim et al, 2006; Zhang and Varadarajan, 
2006; Ghose and Ipeirotis, 2007; Liu et al, 2007; 
Liu et al, 2008; O?Mahony and Smyth, 2009; Tsur 
and Rappoport 2009), various classification and 
regression approaches were taken to assess the 
quality of reviews. (Jindal and Liu, 2008; Lim et 
al., 2010; Li et al 2011; Ott et al, 2011; 
Mukherjee et al, 2012) detect fake reviews and 
reviewers. However, all these works are not 
concerned with review comments. 
3. The Basic TME Model   
This section discusses TME. The next section 
discusses ME-TME, which improves TME. These 
models belong to the family of generative models 
for text where words and phrases (n-grams) are 
viewed as random variables, and a document is 
viewed as a bag of n-grams and each n-gram takes 
a value from a predefined vocabulary. In this work, 
we use up to 4-grams, i.e., n = 1, 2, 3, 4. For 
simplicity, we use terms to denote both words 
(unigrams or 1-grams) and phrases (n-grams). We 
denote the entries in our vocabulary by ???? where ? is the number of unique terms in the vocabulary. 
The entire corpus contains ???? documents. A document (e.g., comment post) ? is represented as 
a vector of terms ?? with ?? entries. ? is the set of all observed terms with cardinality, |?| ? ? ??? . The TME (Topic and Multi-Expression) model is 
a hierarchical generative model motivated by the 
joint occurrence of various types of expressions 
indicating Thumbs-up, Thumbs-down, Question, 
Answer acknowledgement, Agreement, and 
Disagreement and topics in comment posts. As 
before, these expressions are collectively called C-
expressions. A typical comment post mentions a 
few topics (using semantically related topical 
terms) and expresses some viewpoints with one or 
more C-expression types (using semantically 
related expressions). This observation motivates 
the generative process of our model where 
documents (posts) are represented as random 
mixtures of latent topics and C-expression types. 
Each topic or C-expression type is characterized by 
a distribution over terms (words/phrases). Assume 
we have ???? topics and ???? expression types in our corpus. Note that in our case of Amazon 
review comments, based on reading various posts, 
we hypothesize that E = 6 as in such review 
discussions, we mostly find 6 expression types 
(more details in Section 5.1). Let ?? denote the distribution of topics and C-expressions in a 
document ? with ??,? ? ???, ??? denoting the binary 
indicator variable (topic or C-expression) for the 
??? term of ?, ??,?. ??,?denotes the appropriate 
topic or C-expression type index to which ??,? 
belongs. We parameterize multinomials over topics 
using a matrix ????? whose elements ??,??  signify the 
probability of document ? exhibiting topic ?. For 
simplicity of notation, we will drop the latter 
subscript (? in this case) when convenient and use 
???  to stand for the ??? row of ??. Similarly, we define multinomials over C-expression types using 
a matrix ????? . The multinomials over terms associated with each topic are parameterized by a 
matrix ????? , whose elements ??,??  denote the 
probability of generating ? from topic ?. Likewise, 
multinomials over terms associated with each C-
expression type are parameterized by a matrix 
????? . We now define the generative process of TME (see Figure 1(a)). 
A. For each C-expression type ?, draw ???~??????? B. For each topic t, draw ??? ~??????? C. For each comment post ? ? ?1???: 
i. Draw ??~????????  
ii. Draw ???~??????? iii. Draw ???~??????? iv. For each term ??,?, ? ? ?1? ???: 
a. Draw ??,?~????????????? 
b. if (??,? ? ? ?? // ??,?is a C-expression term 
Draw ??,?~?????????) 
else  // ??,? ? ? ? ,???,?is a topical term 
Draw ??,?~????????? ) 
c. Emit ??,?~????????,?
??,?) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 (a) TME Model (b) ME-TME Model  
Figure 1: Graphical Models in plate notations.  
D 
?E?E 
?
? u 
?T ?T
?T T ?
E E ?E?T
z
r
w Nd
x 
? 
z 
r 
w Nd 
D 
?E ?E 
? 
?T ?T 
?T T ?
E E ?E?T
322
To learn the TME model from data, as exact 
inference is not possible, we resort to approximate 
inference using collapsed Gibbs sampling 
(Griffiths and Steyvers, 2004). Gibbs sampling is a 
form of Markov Chain Monte Carlo method where 
a Markov chain is constructed to have a particular 
stationary distribution. In our case, we want to 
construct a Markov chain which converges to the 
posterior distribution over ? and ? conditioned on 
the data. We only need to sample ? and ? as we use 
collapsed Gibbs sampling and the dependencies of 
? and ? have been integrated out analytically in the 
joint. Denoting the random variables ??, ?, ?? by 
singular subscripts???, ??, ???, ????, where ? ?
? ??? , a single iteration consists of performing the following sampling: 
???? ? ?, ?? ? ??| ???, ???, ???,?? ? ?? ? 
       ??
?
?????
???????????? ???
? ??,?
??
?????
??,????? ??????
? ??,?
??
?????
??,????? ??????
   (1) 
???? ? ?, ?? ? ??| ???, ???, ???, ?? ? ?? ? 
       ??
?
?????
???????????? ???
? ??,?
??
?????
??,????? ??????
? ??,?
??
?????
??,????? ??????
   (2) 
where ? ? ??, ?? denotes the ??? term of document 
? and the subscript ?? denotes assignments 
excluding the term at ??, ??. Counts??,??? and ??,???  
denote the number of times term ? was assigned to 
topic ? and expression type ? respectively. ??,??? and 
??,???  denote the number of terms in document ? that 
were assigned to topic ? and C-expression type ? 
respectively. Lastly, ??? and ??? are the number of terms in ? that were assigned to topics and C-
expression types respectively. Omission of the 
latter index denoted by ??? represents the 
marginalized sum over the latter index. We employ 
a blocked sampler jointly sampling ? and ? as this 
improves convergence and reduces autocorrelation 
of the Gibbs sampler (Rosen-Zvi et al, 2004). 
Asymmetric Beta priors: Based on our initial 
experiments with TME, we found that properly 
setting the smoothing hyper-parameter ?? is 
crucial as it governs the topic/expression switch. 
According to the generative process, ?? is the (success) probability (of the Bernoulli distribution) 
of emitting a topical/aspect term in a comment post 
??and1 ? ??, the probability of emitting a C-expression term in ?. Without loss of generality, 
we draw ??~???????? where ? is the concentration parameter and ? ? ???, ??? is the base measure. Without any prior belief, one resorts 
to uniform base measure ?? ? ?? ?0.5 (i.e., assumes that both topical and C-expression terms 
are equally likely to be emitted in a comment post). 
This results in symmetric Beta priors 
??~???????, ??? where ?? ? ???, ?? ? ??? and 
?? ? ?? ? 2/?. However, knowing the fact that topics are more likely to be emitted than 
expressions in a post apriori motivates us to take 
guidance from asymmetric priors (i.e., we now 
have a non-uniform base measure?).  This 
asymmetric setting of ? ensures that samples of ?? are more close to the actual distribution of topical 
terms in posts based on some domain knowledge. 
Symmetric ? cannot utilize any prior knowledge. In 
(Lin and He, 2009), a method was proposed to 
incorporate domain knowledge during Gibbs 
sampling initialization, but its effect becomes weak 
as the sampling progresses (Jo and Oh, 2011). 
For asymmetric priors, we estimate the hyper-
parameters from labeled data. Given a labeled set 
??, where we know the per post probability of C-expression emission (1 ? ???, we use the method of moments to estimate ? ? ???, ??? as follows: 
?? ? ? ???????? ? 1? , ?? ? ?? ?
?
? ? 1? ; ?? ? ?????, ? ? ???????   (3) 
4. ME-TME Model 
The guidance of Beta priors, although helps, is still 
relatively coarse and weak. We can do better to 
produce clearer separation of topical and C-
expression terms. An alternative strategy is to 
employ Maximum-Entropy (Max-Ent) priors 
instead of Beta priors. The Max-Ent parameters 
can be learned from a small number of labeled 
topical and C-expression terms (words and 
phrases) which can serve as good priors. The idea 
is motivated by the following observation: topical 
and C-expression terms typically play different 
syntactic roles in a sentence. Topical terms (e.g. 
?ipod? ?cell phone?, ?macro lens?, ?kindle?, etc.) 
tend to be noun and noun phrases while expression 
terms (?I refute?, ?how can you say?, ?great 
review?) usually contain pronouns, verbs, wh-
determiners, adjectives, and modals. In order to 
utilize the part-of-speech (POS) tag information, 
we move the topic/C-expression distribution ?? (the prior over the indicator variable ??,?) from the 
document plate to the word plate (see Figure 1 (b)) 
and draw it from a Max-Ent model conditioned on 
the observed feature vector ??,???????? associated with 
??,? and the learned Max-Ent parameters ??.??,? can 
323
encode arbitrary contextual features for learning. 
With Max-Ent priors, we have the new model ME-
TME. In this work, we encode both lexical and 
POS features of the previous, current and next POS 
tags/lexemes of the term ??,?. More specifically, 
 ??,???????? ? ??????,???, ?????,? , ?????,???, ??,? ? 1,??,?, ??,? ? 1? 
For phrasal terms (n-grams), all POS tags and 
lexemes of ??,?are considered as features. 
Incorporating Max-Ent priors, the Gibbs sampler 
of ME-TME is given by: 
???? ? ?, ?? ? ??| ???, ???, ???,?? ? ?? ? 
       ????? ???????,?,??????? ?? ????? ???????,?,?????? ??????,??? ?
??,????????
??,????? ??????
? ??,?
??
?????
??,????? ??????
    (4) 
???? ? ?, ?? ? ??| ???, ???, ???, ?? ? ?? ? 
       ????? ???????,?,??????? ?? ????? ???????,?,?????? ??????,??? ?
??,????????
??,????? ??????
? ??,?
??
?????
??,????? ??????
   (5) 
where ???? are the parameters of the learned Max-Ent model corresponding to the ? binary feature 
functions ???? from Max-Ent. 
5. Evaluation 
We now evaluate the proposed TME and ME-TME 
models. Specifically, we evaluate the discovered 
C-expressions, contentious aspects, and aspects 
often mentioned in questions. 
5.1 Dataset and Experiment SettingsWe crawled 
comments of reviews in Amazon.com for a variety 
of products. For each comment we extracted its id, 
the comment author id, the review id on which it 
commented, and the review author id. Our 
database consisted of 21,316 authors, 37,548 
reviews, and 88,345 comments with an average of 
124 words per comment post. 
For all our experiments, the hyper-parameters 
for TME and ME-TME were set to the heuristic 
values ?T = 50/T, ?E = 50/E, ?T = ?E = 0.1 as 
suggested in (Griffiths and Steyvers, 2004). For ?, 
we estimated the asymmetric Beta priors using the 
method of moments discussed in Section 3. We 
sampled 1000 random posts and for each post we 
identified the C-expressions emitted. We thus 
computed the per-post probability of C-expression 
emission (1 ? ??? and used Eq. (3) to get the final estimates, ?? = 3.66, ??= 1.21. To learn the Max-Ent parameters ?, we randomly sampled 500 terms 
from our corpus appearing at least 10 times and 
labeled them as topical (332) or C-expressions 
(168) and used the corresponding feature vector of 
each term (in the context of posts where it occurs) 
to train the Max-Ent model. We set the number of 
topics, T = 100 and the number of C-expression 
types, E = 6 (Thumbs-up, Thumbs-down, Question, 
Answer acknowledgement, Agreement and 
Disagreement) as in review comments, we usually 
find these six dominant expression types. Note that 
knowing the exact number of topics, T and 
expression types, E in a corpus is difficult. While 
non-parametric Bayesian approaches (Teh et al, 
2006) aim to estimate T from the corpus, in this 
work the heuristic values obtained from our initial 
experiments produced good results. We also tried 
increasing E to 7, 8, etc. However, it did not 
produce any new dominant expression type. 
Instead, the expression types became less specific 
as the expression term space became sparser. 
5.2 C-Expression Evaluation 
We now evaluate the discovered C-expressions. 
We first evaluate them qualitatively in Tables 1 
and 2. Table 1 shows the top terms of all 
expression types using the TME model. We find 
that TME can discover and cluster many correct C-
expressions, e.g., ?great review?, ?review helped 
me? in Thumbs-up; ?poor review?, ?very unfair 
review? in Thumbs-down; ?how do I?, ?help me 
decide? in Question; ?good reply?, ?thank you for 
clarifying? in Answer Acknowledgement; ?I 
disagree?, ?I refute? in Disagreement; and ?I 
agree?, ?true in fact? in Agreement. However, with 
the guidance of Max-Ent priors, ME-TME did 
much better (Table 2). For example, we find ?level 
headed review?, ?review convinced me? in 
Thumbs-up; ?biased review?, ?is flawed? in 
Thumbs-down; ?any clues?, ?I was wondering 
how? in Question; ?clears my?, ?valid answer? in 
Answer-acknowledgement; ?I don?t buy your?, 
?sheer nonsense? in Disagreement; ?agree 
completely?, ?well said? in Agreement. These 
newly discovered phrases by ME-TME are marked 
in blue in Table 3. ME-TME also has fewer errors. 
Next, we evaluate them quantitatively using the 
metric precision @ n, which gives the precision at 
different rank positions. This metric is appropriate 
here because the C-expressions (according to top 
terms in ?E) produced by TME and ME-TME are 
rankings. Table 3 reports the precisions @ top 25, 
50, 75, and 100 rank positions for all six 
expression types across both models. We evaluated 
till top 100 positions because it is usually 
324
important to see whether a model can discover and 
rank those major expressions of a type at the top. 
We believe that top 100 are sufficient for most 
applications. From Table 3, we observe that ME-
TME consistently outperforms TME in precisions 
across all expression types and all rank positions. 
This shows that Max-Ent priors are more effective 
in discovering expressions than Beta priors. Note 
that we couldn?t compare with an existing baseline 
because there is no reported study on this problem. 
5.3 Comment Classification 
Here we show that the discovered C-expressions 
can help comment classification. Note that since a 
comment can belong to one or more types (e.g., a 
comment can belong to both Thumbs-up and 
Agreement types), this task is an instance of multi-
label classification, i.e., an instance can have more 
than one class label. In order to evaluate all the 
expression types, we follow the binary approach 
which is an extension of one-against-all method for 
multi-label classification. Thus, for each label, we 
build a binary classification problem. Instances 
associated with that label are in one class and the 
rest are in the other class. To perform this task, we 
randomly sampled 2000 comments, and labeled 
each of them into one or more of the following 8 
labels: Thumbs-up, Thumbs-down, Disagreement, 
Agreement, Question, Answer-Acknowledgement, 
Answer, and None, which have 432, 401, 309, 276, 
305, 201, 228, and 18 comments respectively. We 
disregard the None category due to its small size. 
This labeling is a fairly easy task as one can almost 
certainly make out to which type a comment 
belongs. Thus we didn?t use multiple labelers. The 
distribution reveals that the labels are overlapping. 
For instance, we found many comments belonging 
to both Thumbs-down and Disagreement, Thumbs-up 
with Acknowledgement and with Question. 
For supervised classification, the choice of 
feature is a key issue. While word and POS n-
grams are traditional features, such features may 
not be the best for our task. We now compare such 
features with the C-expressions discovered by the 
proposed models. We used the top 1000 terms 
from each of the 6 C-expression rankings as 
features. As comments in Question type mostly use 
the punctuation ???, we added it in our feature set. 
We use precision, recall and F1 as our metric to 
compare classification performance using a trained 
SVM (linear kernel). All results (Table 4) were 
computed using 10-fold cross-validation (CV). We 
also tried Na?ve Bayes and Logistic Regression 
classifiers, but they were poorer than SVM. Hence 
their results are not reported due to space 
constraints. As a separate experiment (not shown 
here also due to space constraints), we analyzed 
the classification performance by varying the 
number of top terms from 200, 400,?, 1000, 1200, 
etc. and found that the F1 scores stabilized after top 
 
 
 
        Figure 5: Precision @ top 50,  
Thumbs-up (e1): review, thanks, great review, nice review, time, best review, appreciate, you, your review helped, nice, terrific, 
review helped me, good critique, very, assert, wrong, useful 
review, don?t, misleading, thanks a lot, ? 
Thumbs-down (e2): review, no, poor review, imprecise, you, complaint, very, suspicious, bogus review, absolutely, credible, 
very unfair review, criticisms, true, disregard this review, disagree 
with, judgment, without owning, ? 
Question (e3): question, my, I, how do I, why isn?t, please explain, good answer, clarify, don?t understand, my doubts, I?m confused, 
does not, understand, help me decide, how to,  yes, answer, how 
can I, can?t explain, ? 
Answer Acknowledgement (e4): my, informative, answer, good reply, thank you for clarifying, answer doesn?t, good answer, 
vague, helped me choose, useful suggestion, don?t understand, 
cannot explain, your answer, doubts, answer isn?t, ? 
Disagreement (e5): disagree, I, don?t, I disagree, argument claim, I reject, I refute, I refuse, oppose, debate, accept, don?t agree, quote, 
sense, would disagree, assertions, I doubt, right,  your, really, 
you, I?d disagree, cannot, nonsense,... 
Agreement (e6): yes, do, correct, indeed, no, right, I agree, you, agree, I accept, very, yes indeed, true in fact, indeed correct, I?d 
agree, completely, true, but, doesn?t, don?t, definitely, false, 
completely agree, agree with your, true, ? 
Table 1: Top terms (comma delimited) of six expression types 
e1, e2, e3, e4, e5, e6 (?E) using TME model. Red (bold) colored terms denote possible errors 
Thumbs-up (e1): review, you, great review, I'm glad I read, best review, review convinced me, review helped me,  good review, terrific 
review, job, thoughtful review, awesome review, level headed review, 
good critique, good job, video review,... 
Thumbs-down (e2): review, you, bogus review, con, useless review, ridiculous,  biased review, very unfair review, is flawed, completely, 
skeptical, badmouth, misleading review, cynical review, wrong, 
disregard this review, seemingly honest, ? 
Question (e3): question, I, how do I, why isn?t, please explain, clarify, any clues, answer, please explain, help me decide, vague, how to, how 
do I, where can I, how to set, I was wondering how, could you explain, 
how can I, can I use, ? 
Answer Acknowledgement (e4): my, good reply, , answer, reply, helped me choose, clears my,  valid answer, answer doesn?t, 
satisfactory answer, can you clarify, informative answer, useful 
suggestion, perfect answer, thanks for your reply, doubts, ? 
Disagreement (e5): disagree, I, don?t, I disagree, doesn?t, I don?t buy your, credible, I reject, I doubt, I refuse, I oppose, sheer nonsense, 
hardly, don?t agree, can you prove, you have no clue, how do you say, 
sense, you fail, contradiction, ? 
Agreement (e6): I, do, agree, point, yes, really, would agree, you, agree, I accept, claim, agree completely, personally agree, true in fact, 
indeed correct, well said, valid point, correct, never meant, might not, 
definitely agree,? 
Table 2: Top terms (comma delimited) of six expression types 
using ME-TME model. Red (bold) terms denote possible errors. 
Blue (italics) terms denote those newly discovered by the model; 
rest (black) were used in Max-Ent training. 
325
1000 terms. From Table 4, we see that F1 scores 
dramatically increase with C-expression (??) 
features for all expression types. TME and ME-
TME progressively improve the classification. 
Improvements of TME and ME-TME being 
significant (p<0.001) using a paired t-test across 
10-fold cross validations shows that the discovered 
C-expressions are of high quality and useful.  
We note that the annotation resulted in a new 
label ?Answer? which consists of mostly replies to 
comments with questions. Since an ?answer? to a 
question usually does not show any specific 
expression, it does not attain very good F1 scores. 
Thus, to improve the performance of the Answer 
type comments, we added three binary features for 
each comment c on top of C-expression features: 
i) Is the author of c the review author too? The 
idea here is that most of the times the reviewer 
answers the questions raised in comments. 
ii) Is there any comment posted before c by some 
author a which has been previously classified 
as a question post? 
iii) Is there any comment posted after c by author 
a that replies to c (using @name) and is an 
Answer-Acknowledgement comment (which 
again has been previously classified as such)? 
Using these additional features, we obtained a 
precision of 0.78 and a recall of 0.73 yielding an F1 
C-Expression Type P@25 P@50 P@75 P@100 
TME ME-TME TME ME-TME TME ME-TME TME ME-TME
Thumbs-up 0.60 0.80 0.66 0.78 0.60 0.69 0.55 0.64 
Thumbs-down 0.68 0.84 0.70 0.80 0.63 0.67 0.60 0.65 
Question 0.64 0.80 0.68 0.76 0.65 0.72 0.61 0.67 
Answer-Acknowledgement 0.68 0.76 0.62 0.72 0.57 0.64 0.54 0.58 
Disagreement 0.76 0.88 0.74 0.80 0.68 0.73 0.65 0.70 
Agreement 0.72 0.80 0.64 0.74 0.61 0.70 0.60 0.69 
Table 3: Precision @ top 25, 50, 75, and 100 rank positions for all C-expression types. 
Features Thumbs-up Thumbs-down Question Answer-Ack. Disagreement Agreement Answer 
 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 
W+POS 1-gram 0.68 0.66 0.67 0.65 0.65 0.65 0.71 0.68 0.69 0.64 0.61 0.62 0.73 0.72 0.72 0.67 0.65 0.66 0.58 0.57 0.57
W+POS 1-2 gram 0.72 0.69 0.70 0.68 0.67 0.67 0.74 0.69 0.71 0.69 0.63 0.65 0.76 0.75 0.75 0.71 0.69 0.70 0.60 0.57 0.58
W+POS, 1-3 gram 0.73 0.71 0.72 0.69 0.68 0.68 0.75 0.69 0.72 0.70 0.64 0.66 0.76 0.76 0.76 0.72 0.70 0.71 0.61 0.58 0.59
W+POS, 1-4 gram 0.74 0.72 0.73 0.71 0.68 0.69 0.75 0.70 0.72 0.70 0.65 0.67 0.77 0.76 0.76 0.73 0.70 0.71 0.61 0.58 0.59
C-Expr. ?E, TME 0.82 0.74 0.78 0.77 0.71 0.74 0.83 0.75 0.78 0.75 0.72 0.73 0.83 0.80 0.81 0.78 0.75 0.76 0.66 0.61 0.63
C-Expr. ?E, ME-TME 0.87 0.79 0.83 0.80 0.73 0.76 0.87 0.76 0.81 0.77 0.72 0.74 0.86 0.81 0.83 0.81 0.77 0.79 0.67 0.61 0.64
Table 4: Precision (P), Recall (R), and F1 scores of binary classification using SVM and different features. The improvements of our models are significant (p<0.001) over paired t-test across 10-fold cross validation. 
D 
?E  + Noun/Noun Phrase TME ME-TME 
J1 J2 J1 J2 J1 J2 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 
D1 0.62 0.70 0.66 0.58 0.67 0.62 0.66 0.75 0.70 0.62 0.70 0.66 0.67 0.79 0.73 0.64 0.74 0.69
D2 0.61 0.67 0.64 0.57 0.63 0.60 0.66 0.72 0.69 0.62 0.67 0.64 0.68 0.75 0.71 0.64 0.71 0.67
D3 0.60 0.69 0.64 0.56 0.64 0.60 0.64 0.73 0.68 0.60 0.67 0.63 0.67 0.76 0.71 0.63 0.72 0.67
D4 0.59 0.68 0.63 0.55 0.65 0.60 0.63 0.71 0.67 0.59 0.68 0.63 0.65 0.73 0.69 0.62 0.71 0.66
Avg. 0.61 0.69 0.64 0.57 0.65 0.61 0.65 0.73 0.69 0.61 0.68 0.64 0.67 0.76 0.71 0.63 0.72 0.67
Table 5 (a) 
D ?
E  + Noun/Noun Phrase TME ME-TME 
J1 J2 J1 J2 J1 J2 
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 D1 0.57 0.65 0.61 0.54 0.63 0.58 0.61 0.69 0.65 0.58 0.66 0.62 0.64 0.73 0.68 0.61 0.70 0.65
D2 0.61 0.66 0.63 0.58 0.61 0.59 0.64 0.68 0.66 0.60 0.64 0.62 0.68 0.70 0.69 0.65 0.69 0.67
D3 0.60 0.68 0.64 0.57 0.64 0.60 0.64 0.71 0.67 0.62 0.68 0.65 0.67 0.72 0.69 0.64 0.69 0.66
D4 0.56 0.67 0.61 0.55 0.65 0.60 0.60 0.72 0.65 0.58 0.68 0.63 0.63 0.75 0.68 0.61 0.71 0.66
Avg. 0.59 0.67 0.62 0.56 0.63 0.59 0.62 0.70 0.66 0.60 0.67 0.63 0.66 0.73 0.69 0.63 0.70 0.66
Table 5 (b) 
Table 5: Points of Contention (a), Questioned aspects (b). D1: Ipod, D2: Kindle, D3: Nikon, D4: Garmin. We report the 
average precision (P), recall (R), and F1 score over 100 comments for each particular domain.  
Statistical significance: Differences between Nearest Noun Phrase and TME for both judges (J1, J2) across all domains were significant at 97% confidence level (p<0.03). Differences among TME and ME-TME for both judges (J1, J2) across all domains were significant at 95% confidence level (p<0.05). A paired t-test was used for testing significance. 
326
score of 0.75 which is a dramatic increase beyond 
0.64 achieved by ME-TME in Table 4. 
5.4 Contention Points and Questioned Aspects  
We now turn to the task of discovering points of 
contention in disagreement comments and aspects 
(or topics) raised in questions. By ?points?, we 
mean the topical terms on which some contentions 
or disagreements have been expressed. Topics 
being the product aspects are also indirectly 
evaluated in this task. We employ the TME and 
ME-TME models in the following manner.  
We only detail the approach for disagreement 
comments. The same method is applied to question 
comments. Given a disagreement comment post ?, 
we first select the top k topics that are mentioned in 
d according to its topic distribution, ??? . Let ?? be the set of these top ? topics in ?. Then, for each 
disagreement expression?? ? ? ? ???????????????? , 
we emit the topical terms (words/phrases) of topics 
in ??which appear within a word window of ? from ? in ?. More precisely, we emit the set ? ? ??|? ?
? ? ??? , ? ? ??, |??????? ? ???????| ? ??, where posi(?) returns the position index of the word or 
phrase in document ?. To compute the intersection 
? ? ? ? ??? , we need a threshold. This is so because the Dirichlet distribution has a smoothing 
effect which assigns some non-zero probability 
mass to every term in the vocabulary for each topic 
?. So for computing the intersection, we considered 
only terms in ???  which have ???|?? ? ???,??  > 0.001 
as probability masses lower than 0.001 are more 
due to the smoothing effect of the Dirichlet 
distribution than true correlation. In an actual 
application, the values for ? and ? can be set 
according to the user?s need. In our experiment, we 
used ??= 3 and 5 = ?, which are reasonable because 
a post normally does not talk about many topics 
(?), and the contention points (aspect terms) appear 
quite close to the disagreement expressions. 
For comparison, we also designed a baseline. 
For each disagreement (or question) expression 
? ? ? ? ???????????????? (???????????? ), we emit the 
nouns and noun phrases within the same window ? 
as the points of contention (question) in ?. This 
baseline is reasonable because topical terms are 
usually nouns and noun phrases and are near 
disagreement (question) expressions. We note that 
this baseline cannot stand alone because it has to 
rely on our expression models ?? of ME-TME. 
Next, to evaluate the performance of these 
methods in discovering points of contention, we 
randomly selected 100 disagreement (contentious) 
(and 100 question) comment posts on reviews from 
each of the 4 product domains: Ipod, Kindle, 
Nikon Cameras, and Garmin GPS in our database 
and employed the aforementioned methods to 
discover the points of contention (question) in each 
post. Then we asked two human judges (graduate 
students fluent in English) to manually judge the 
results produced by each method for each post. We 
asked them to report the precision of the 
discovered terms for a post by judging them as 
being indeed valid points of contention and report 
recall in a post by judging how many of actually 
contentious points in the post were discovered. In 
Table 5 (a), we report the average precision and 
recall for 100 posts in each domain by the two 
judges J1 and J2 for different methods on the task 
of discovering points (aspects) of contention. In 
Table 5 (b), similar results are reported for the task 
of discovering questioned aspects in 100 question 
comments for each product domain. Since this 
judging task is subjective, the differences in the 
results from the two judges are not surprising. Our 
judges were made to work in isolation to prevent 
any bias. We observe that across all domains, ME-
TME again performs the best consistently. Note 
that agreement study using Kappa is not used here 
as our problem is not to label a fixed set of items 
categorically by the judges. 
6. Conclusion 
This paper proposed the problem of modeling 
review comments, and presented two models TME 
and ME-TME to model and to extract topics 
(aspects) and various comment expressions. These 
expressions enable us to classify comments more 
accurately, and to find contentious aspects and 
questioned aspects. These pieces of information 
also allow us to produce a simple summary of 
comments for each review as discussed in Section 
1. To our knowledge, this is the first attempt to 
analyze comments in such details. Our experiments 
demonstrated the efficacy of the models. ME-TME 
also outperformed TME significantly. 
Acknowledgments 
This work is supported in part by National Science 
Foundation (NSF) under grant no. IIS-1111092.  
327
References  
Agarwal, R., S. Rajagopalan, R. Srikant, Y. Xu. 2003. 
Mining newsgroups using networks arising from 
social behavior. Proceedings of International 
Conference on World Wide Web 2003. 
Andrzejewski, D., X. Zhu, M. Craven. 2009. 
Incorporating domain knowledge into topic 
modeling via Dirichlet forest priors. Proceedings of 
International Conference on Machine Learning.  
Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirichlet 
Allocation. Journal of Machine Learning Research. 
Brody, S. and S. Elhadad. 2010. An Unsupervised 
Aspect-Sentiment Model for Online Reviews. 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL. 
Burfoot, C., S. Bird, and T. Baldwin. 2011. Collective 
Classification of Congressional Floor-Debate 
Transcripts. Proceedings of the 49th Annual Meeting 
of the Association for Computational Linguistics.  
Galley, M., K. McKeown, J. Hirschberg, E. Shriberg. 
2004. Identifying agreement and disagreement in 
conversational speech: Use of Bayesian networks to 
model pragmatic dependencies. Proceedings of the 
42th Annual Meeting of the Association of 
Computational Linguistics.  
Ghose, A. and P. Ipeirotis. 2007. Designing novel 
review ranking systems: predicting the usefulness 
and impact of reviews. Proceedings of International 
Conference on Electronic Commerce. 
Griffiths, T. and M. Steyvers. 2004. Finding scientific 
topics. Proceedings of National Academy of 
Sciences.  
Kim, S., P. Pantel, T. Chklovski, and M. Pennacchiotti. 
2006. Automatically assessing review helpfulness. 
Proceedings of Empirical Methods in Natural 
Language Processing.  
Jindal, N. and B. Liu. 2008. Opinion spam and analysis. 
Proceedings of the ACM International Conference on 
Web Search and Web Data Mining.  
Jo, Y. and A. Oh. 2011. Aspect and sentiment 
unification model for online review analysis. 
Proceedings of the ACM International Conference 
on Web Search and Web Data Mining. 
Li, F., M. Huang, Y. Yang, and X. Zhu. 2011. Learning 
to Identify Review Spam. in Proceedings of the 
International Joint Conference on Artificial 
Intelligence. 
Lim, E., V. Nguyen, N. Jindal, B. Liu, and H. Lauw. 
2010. Detecting Product Review Spammers using 
Rating Behaviors. Proceedings of the ACM 
International Conference on Information and 
Knowledge Management. 
Lin, C. and Y. He. 2009. Joint sentiment/topic model for 
sentiment analysis. Proceedings of the ACM 
International Conference on Information and 
Knowledge Management.  
Liu, J., Y. Cao, C. Lin, Y. Huang, and M. Zhou. 2007. 
Low-quality product review detection in opinion 
summarization. Proceedings of Empirical Methods in 
Natural Language Processing.  
Liu, B. 2012. Sentiment Analysis and Opinion Mining. 
Morgan & Claypool publishers (to appear in June 
2012).  
Liu, Y., X. Huang, A. An, and X. Yu. 2008. Modeling 
and predicting the helpfulness of online reviews. 
Proceedings of IEEE International Conference on 
Data Mining. 
Lu, Y. and C. Zhai. 2008. Opinion integration through 
semi-supervised topic modeling. Proceedings of 
International Conference on World Wide Web.  
Lu, Y., C. Zhai, and N. Sundaresan. 2009. Rated aspect 
summarization of short comments. Proceedings of 
International Conference on World Wide.  
Mei, Q. X. Ling, M. Wondra, H. Su and C. Zhai. 2007. 
Topic sentiment mixture: modeling facets and 
opinions in weblogs.? Proceedings of International 
Conference on World Wide.  
Moghaddam, S. and M. Ester. 2011. ILDA: 
interdependent LDA model for learning latent 
aspects and their ratings from online product reviews. 
Proceedings of Annual ACM SIGIR Conference on 
Research and Development in Information Retrieval.  
Mukherjee, A. and B. Liu. 2012a. Aspect Extraction 
through Semi-Supervised Modeling. Proceedings of 
50th Annual Meeting of Association for 
Computational Linguistics (to appear in July 2012). 
Mukherjee, A. and B. Liu. 2012b. Mining Contentions 
from Discussions and Debates. Proceedings of ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (to appear in August 
2012). 
Mukherjee, A., B. Liu and N. Glance. 2012. Spotting 
Fake Reviewer Groups in Consumer Reviews. 
Proceedings of International World Wide Web 
Conference. 
Murakami A., and R. Raymond, 2010. Support or 
Oppose? Classifying Positions in Online Debates 
from Reply Activities and Opinion Expressions. 
Proceedings of International Conference on 
328
Computational Linguistics. 
O'Mahony, M. P. and B. Smyth. 2009. Learning to 
recommend helpful hotel reviews. Proceedings of the 
third ACM conference on Recommender systems.  
Ott, M., Y. Choi, C. Cardie, and J. T. Hancock. 2011. 
Finding deceptive opinion spam by any stretch of the 
imagination. Proceedings of the 49th Annual Meeting 
of the Association for Computational Linguistics. 
Pang, B. and L. Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval. 
Ramage, D., D. Hall, R. Nallapati, and C. Manning. 
2009. Labeled LDA: A supervised topic model for 
credit attribution in multi-labeled corpora. 
Proceedings of Empirical Methods in Natural 
Language Processing.  
Ramage, D., C. Manning, and S. Dumais. 2011 Partially 
labeled topic models for interpretable text mining. 
Proceedings of the 17th ACM SIGKDD 
international conference on Knowledge discovery 
and data mining.  
Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smith. 
2004. The author-topic model for authors and 
documents. Uncertainty in Artificial Intelligence.  
Sauper, C. A. Haghighi and R. Barzilay. 2011. Content 
models with attitude. Proceedings of the 49th Annual 
Meeting of the Association for Computational 
Linguistics. 
Somasundaran, S., J. Wiebe. 2009. Recognizing stances 
in online debates. Proceedings of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP 
Teh, Y., M. Jordan, M. Beal and D. Blei. 2006. 
Hierarchical Dirichlet Processes. Journal of the 
American Statistical Association. 
Thomas, M., B. Pang and L. Lee. 2006. Get out the 
vote: Determining support or opposition from 
Congressional floor-debate transcripts. Proceedings 
of Empirical Methods in Natural Language 
Processing.  
Titov, I. and R. McDonald. 2008a. Modeling online 
reviews with multi-grain topic models. Proceedings 
of International Conference on World Wide Web.  
Titov, I. and R. McDonald. 2008b. A joint model of text 
and aspect ratings for sentiment summarization. 
Proceedings of Annual Meeting of the Association 
for Computational Linguistics.  
Tsur, O. and A. Rappoport. 2009. Revrank: A fully 
unsupervised algorithm for selecting the most helpful 
book reviews. Proceedings of the International AAAI 
Conference on Weblogs and Social Media. 
Wang, H., Y. Lu, and C. Zhai. 2010. Latent aspect 
rating analysis on review text data: a rating 
regression approach. Proceedings of ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining. 
Yano, T and N. Smith. 2010. What?s Worthy of 
Comment? Content and Comment Volume in 
Political Blogs. Proceedings of the International 
AAAI Conference on Weblogs and Social Media. 
Zhang, Z. and B. Varadarajan. 2006. Utility scoring of 
product reviews. Proceedings of ACM International 
Conference on Information and Knowledge 
Management.  
Zhao, X., J. Jiang, H. Yan, and X. Li. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-LDA 
hybrid. Proceedings of Empirical Methods in Natural 
Language Processing. 
329
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 339?348,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Aspect Extraction through Semi-Supervised Modeling 
 
 
Arjun Mukherjee Bing Liu 
Department of Computer Science Department of Computer Science 
University of Illinois at Chicago University of Illinois at Chicago 
Chicago, IL 60607, USA Chicago, IL 60607, USA 
arjun4787@gmail.com liub@cs.uic.edu 
 
  
 
 
 
 
Abstract 
Aspect extraction is a central problem in 
sentiment analysis. Current methods either 
extract aspects without categorizing them, 
or extract and categorize them using 
unsupervised topic modeling. By 
categorizing, we mean the synonymous 
aspects should be clustered into the same 
category. In this paper, we solve the 
problem in a different setting where the 
user provides some seed words for a few 
aspect categories and the model extracts 
and clusters aspect terms into categories 
simultaneously. This setting is important 
because categorizing aspects is a subjective 
task. For different application purposes, 
different categorizations may be needed. 
Some form of user guidance is desired. In 
this paper, we propose two statistical 
models to solve this seeded problem, which 
aim to discover exactly what the user 
wants. Our experimental results show that 
the two proposed models are indeed able to 
perform the task effectively.  
1 Introduction 
Aspect-based sentiment analysis is one of the main 
frameworks for sentiment analysis (Hu and Liu, 
2004; Pang and Lee, 2008; Liu, 2012). A key task 
of the framework is to extract aspects of entities 
that have been commented in opinion documents. 
The task consists of two sub-tasks. The first sub-
task extracts aspect terms from an opinion corpus. 
The second sub-task clusters synonymous aspect 
terms into categories where each category 
represents a single aspect, which we call an aspect 
category. Existing research has proposed many 
methods for aspect extraction. They largely fall 
into two main types. The first type only extracts 
aspect terms without grouping them into categories 
(although a subsequent step may be used for the 
grouping, see Section 2). The second type uses 
statistical topic models to extract aspects and group 
them at the same time in an unsupervised manner. 
Both approaches are useful. However, in practice, 
one also encounters another setting, where 
grouping is not straightforward because for 
different applications the user may need different 
groupings to reflect the application needs. This 
problem was reported in (Zhai et al, 2010), which 
gave the following example. In car reviews, 
internal design and external design can be regarded 
as two separate aspects, but can also be regarded as 
one aspect, called ?design?, based on the level of 
details that the user wants to study. It is also 
possible that the same word may be put in different 
categories based on different needs. However, 
(Zhai et al, 2010) did not extract aspect terms. It 
only categorizes a set of given aspect terms. 
In this work, we propose two novel statistical 
models to extract and categorize aspect terms 
automatically given some seeds in the user 
interested categories. It is thus able to best meet the 
user?s specific needs. Our models also jointly 
model both aspects and aspect specific sentiments. 
The first model is called SAS and the second 
model is called ME-SAS. ME-SAS improves SAS 
by using Maximum-Entropy (or Max-Ent for short) 
priors to help separate aspects and sentiment terms. 
However, to train Max-Ent, we do not need 
manually labeled training data (see Section 4).  
339
In practical applications, asking users to provide 
some seeds is easy as they are normally experts in 
their trades and have a good knowledge what are 
important in their domains.  
Our models are related to topic models in 
general (Blei et al, 2003) and joint models of 
aspects and sentiments in sentiment analysis in 
specific (e.g., Zhao et al, 2010). However, these 
current models are typically unsupervised. None of 
them can use seeds. With seeds, our models are 
thus semi-supervised and need a different 
formulation. Our models are also related to the DF-
LDA model in (Andrzejewski et al, 2009), which 
allows the user to set must-link and cannot-link 
constraints. A must-link means that two terms must 
be in the same topic (aspect category), and a 
cannot-link means that two terms cannot be in the 
same topic. Seeds may be expressed with must-
links and cannot-links constraints. However, our 
models are very different from DF-LDA. First of 
all, we jointly model aspect and sentiment, while 
DF-LDA is only for topics/aspects. Joint modeling 
ensures clear separation of aspects from sentiments 
producing better results. Second, our way of 
treating seeds is also different from DF-LDA. We 
discuss these and other related work in Section 2. 
The proposed models are evaluated using a large 
number of hotel reviews. They are also compared 
with two state-of-the-art baselines. Experimental 
results show that the proposed models outperform 
the two baselines by large margins. 
2 Related Work  
There are many existing works on aspect 
extraction. One approach is to find frequent noun 
terms and possibly with the help of dependency 
relations (Hu and Liu, 2004; Popescu and Etzioni, 
2005; Zhuang et al, 2006; Blair-Goldensohn et al, 
2008; Ku et al, 2006; Wu et al, 2009; 
Somasundaran and Wiebe, 2009; Qiu et al, 2011). 
Another approach is to use supervised sequence 
labeling (Liu, Hu and Cheng 2005; Jin and Ho, 
2009; Jakob and Gurevych, 2010; Li et al, 2010; 
Choi and Cardie, 2010; Kobayashi et al, 2007; Yu 
et al, 2011). Ma and Wan (2010) also exploited 
centering theory, and (Yi et al, 2003) used 
language models. However, all these methods do 
not group extracted aspect terms into categories. 
Although there are works on grouping aspect terms 
(Carenini et al, 2005; Zhai et al, 2010; Zhai et al, 
2011; Guo et al, 2010), they all assume that aspect 
terms have been extracted beforehand. 
In recent years, topic models have been used to 
perform extraction and grouping at the same time. 
Existing works are based on two basic models, 
pLSA (Hofmann, 1999) and LDA (Blei et al, 
2003). Some existing works include discovering 
global and local aspects (Titov and McDonald, 
2008), extracting key phrases (Branavan et al, 
2008), rating multi-aspects (Wang et al, 2010; 
Moghaddam and Ester, 2011), summarizing 
aspects and sentiments (Lu et al, 2009), and 
modeling attitudes (Sauper et al, 2011). In (Lu and 
Zhai, 2008), a semi-supervised model was 
proposed. However, their method is entirely 
different from ours as they use expert reviews to 
guide the analysis of user reviews. 
 Aspect and sentiment extraction using topic 
modeling come in two flavors: discovering aspect 
words sentiment wise (i.e., discovering positive 
and negative aspect words and/or sentiments for 
each aspect without separating aspect and 
sentiment terms) (Lin and He, 2009; Brody and 
Elhadad, 2010, Jo and Oh, 2011) and separately 
discovering both aspects and sentiments (e.g., Mei 
et al, 2007; Zhao et al, 2010). Zhao et al (2010) 
used Maximum-Entropy to train a switch variable 
to separate aspect and sentiment words. We adopt 
this method as well but with no use of manually 
labeled data in training. One problem with these 
existing models is that many discovered aspects 
are not understandable/meaningful to users. Chang 
et al (2009) stated that one reason is that the 
objective function of topic models does not always 
correlate well with human judgments. Our seeded 
models are designed to overcome this problem.   
Researchers have tried to generate ?meaningful? 
and ?specific? topics/aspects. Blei and McAuliffe 
(2007) and Ramage et al (2009) used document 
label information in a supervised setting. Hu et al 
(2011) relied on user feedback during Gibbs 
sampling iterations. Andrzejewski et al (2011) 
incorporated first-order logic with Markov Logic 
Networks. However, it has a practical limitation 
for reasonably large corpora since the number of 
non-trivial groundings can grow to O(N2) where N 
is the number of unique tokens in the corpus. 
Andrzejewski et al (2009) used another approach 
(DF-LDA) by introducing must-link and cannot-
link constraints as Dirichlet Forest priors. Zhai et 
al. (2011) reported that the model does not scale up 
340
when the number of cannot-links go beyond 1000 
because the number of maximal cliques Q(r) in a 
connected component of size |r| in the cannot-link 
graph is exponential in r. Note that we could still 
experiment with DF-LDA as our problem size is 
not so large. We will show in Section 4 that the 
proposed models outperform it by a large margin. 
3 Proposed Seeded Models  
The standard LDA and existing aspect and 
sentiment models (ASMs) are mostly governed by 
the phenomenon called ?higher-order co-
occurrence? (Heinrich, 2009), i.e., based on how 
often terms co-occur in different contexts1. This 
unfortunately results in many ?non-specific? terms 
being pulled and clustered. We employ seed sets to 
address this issue by ?guiding? the model to group 
semantically related terms in the same aspect thus 
making the aspect more specific and related to the 
seeds (which reflect the user needs). For easy 
presentation, we will use aspect to mean aspect 
category from now on. We replace the multinomial 
distribution over words for each aspect (as in 
ASMs) with a special two-level tree structured 
distribution. The generative process of ASMs 
assumes that each vocabulary word is 
independently (i.e., not dependent upon other 
word-aspect association) and equally probable to 
be associated with any aspect. Due to higher-order 
co-occurrences, we find conceptually different 
terms yet related in contexts (e.g., in hotel domain 
terms like stain, shower, walls in aspect 
                                                          1 w1 co-occurring with w2 which in turn co-occurs with w3 denotes a 
second-order co-occurrence between w1 and w3. 
Maintenance; bed, linens, pillows in aspect 
Cleanliness) equally probable of emission for any 
aspect. Figure 1(a) shows an example tree. Upon 
adding the seed sets {bed, linens, pillows} and 
{staff, service}, the prior structure now changes to 
the correlated distribution in Figure 1 (b). Thus, 
each aspect has a top level distribution over non-
seed words and seed sets. Each seed set in each 
aspect further has a second level distribution over 
seeds in that seed set. The aspect term (word) 
emission now requires two steps: first sampling at 
level one to obtain a non-seed word or a seed set. If 
a non-seed word is sampled we emit it else we 
further sample at the second seed set level and emit 
a seed word. This ensures that seed words together 
have either all high or low aspect associations. 
Furthermore, seed sets preserve conjugacy between 
related concepts and also shape more specific 
aspects by clustering based on higher order co-
occurrences with seeds rather than only with 
standard one level multinomial distribution over 
words (or terms) alone. 
3.1 SAS Model 
We now present the proposed Seeded Aspect and 
Sentiment model (SAS). Let ???? denote the entries in our vocabulary where ??is the number of 
unique non-seed terms. Let there be ? seed sets 
?????? where each seed set ?? is a group of semantically related terms. Let ??????? ,????????  denote T aspect and aspect specific sentiment 
models. Also let ??,? denote the aspect specific distribution of seeds in the seed set ??. Following the approach of (Zhao et al, 2010), we too assume 
that a review sentence usually talks about one 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Prior structure:  (a) Standard ASMs, (b) Two-level tree structured distribution. Graphical models in plate 
notation: (c) SAS and (d) ME-SAS. 
a? a? a? 
D 
aw 
ar 
au Nd, s 
az a? 
Sd 
a?A a?O 
T 
C a? a?
A 
T 
a?O 
a? a?A a?O 
T 
C a? a?
A 
T 
a?O 
a? 
a? 
a? 
a? 
D 
az 
a? 
Sd 
aw 
ar 
au Nd, s a
x 
(a) 
(b) (d) (c) 
Seeds: 
{staff, service} 
{linens, bed, pillows} 
room stain bed staff linens service shower pillows walls friendly
?A 
staff service bed linens pillows 
?A 
stain walls            shower            friendly room ? ?
341
aspect. A review document ???? comprises of ?? sentences and each sentence ? ? ?? has ??,?words. Also, let ?????? denote the sentence ? of document ?. To distinguish between aspect and sentiment 
terms, we introduce an indicator (switch) variable 
??,?,? ? ? ??, ??? for the ???term of ??????,???,?,?. Further, let ??,? denote the distribution of aspects and sentiments in ??????. The generative process of the SAS model (see Figure 1(c)) is given by: 
1. For each aspect ?? ? ?1, ? , ??: 
i. Draw    ????~???????? ii. Draw a distribution over terms and seed sets ????~???????? a) For each seed set ? ? ???, ? , ??? Draw a distribution over seeds ??,??~??????? 
2. For each (review) document ? ? ?1, ? , ??: 
i. Draw ???~??????? ii. For each sentence ? ? ?1, ? , ???: a) Draw ??,??~????????? 
b) Draw ??,??~???????? 
c) For each term ??,?,??where??? ? ?1, ? , ??,??: 
I. Draw ??,?,??~?????????????,??, ??,?,? ? ? ??, ??? 
II. if ??,?,? ? ? ?? // ??,?,? is a sentiment 
Emit ??,?,??~?????????,?? ? 
else // ??,?,? ? ? ???, ??,?,? is an aspect 
A. Draw ??,?,??~?????????,?? ? 
B. if ??,?,? ? ? ? // non-seed term 
Emit ??,?,? ? ???,?,? 
else // ??,?,? is some seed set index say ??,?,? 
Emit ??,?,??~????,??,??,?,? 
We employ collapsed Gibbs sampling (Griffiths 
and Steyvers, 2004) for posterior inference. As ? 
and ? are at different hierarchical levels, we derive 
their samplers separately as follows: 
????,? ? ??????,?, ???,?, ???,??, ???,?? ?
????,??? ? ???
? ???,??? ??,? ? ???
? 
????,???,??????
????,???,???,????
?? ? ?
????,?,???,? ????
????,?,???,? ??,?????
???? ?
??,?????.??,???
??,???????.??,????
  (1) 
????,?,? ? ??????,?, ???,?,?, ???,?,??, ???,?,?, ??,? ? ?, ??,?,? ? ?? ? 
??,?? ??,?,????
??,???? ??,?,??|??? ??? |??
? ??,?
?
??,?,????
??,?? ??,?,????????,?? ??,?,????
    (2) 
????,?,? ? ??? ? ? ?
?
??
?
?
??
?
? ??,?,?
?,?
??,?,???
??,?,????,? ??,?,??|??|?
? ??,?? ?????,???? ???????? ??
??,?? ??,?,????
??,?? ??,?,????????,?
?
??,?,????
??? ; ??? ? ??
??,??,?????
??,????,??????????
? ??,?
?
??,?,????
??,?? ??,?,????????,?
?
??,?,????
; ??, ? ? ??
 (3) 
where ????? ? ?? ???????????????????? ?????????????? ?is the multinomial Beta function. ??,??  is the number of times term ? was 
assigned to aspect ? as an opinion/sentiment word. 
??,??,? is the number of times non-seed term ? ? ?? was assigned to aspect ? as an aspect. ??,?,??,?  is the number of times seed term ? ? ? ?? was assigned to aspect ? as an aspect. ??,?????. is the number of sentences in document ? that were assigned to 
aspect ?. ??,??  and ??,??  denote the number of terms in ?????? that were assigned to aspects and opinions respectively. ??,??  is the number of times any term of seed set ?? was assigned to aspect ?. Omission of a latter index denoted by [] in the above notation 
represents the corresponding row vector spanning 
over the latter index. For example, ??,???,? ?
???,????,? , ? , ??,????,? ? and (?) denotes the marginalized sum over the latter index. The subscript ??, ? 
denotes the counts excluding assignments of all 
terms in ??????. ??, ?, ? denotes counts excluding 
??,?,?.We perform hierarchical sampling. First, an 
aspect is sampled for each sentence ??,? using Eq. (1). After sampling the aspect, we sample ??,?,?. 
The probability of ??,?,? being an opinion or 
sentiment term, ????,?,? ? ??? is given by Eq. (2). 
However, for ????,?,? ? ??? we have two cases: (a) 
the observed term ? ? ??,?,? ? ?? or (b) does not belong to any seed set, ??, ? ? ??, i.e., w is an non-seed term. These cases are dealt in Eq. (3). 
Asymmetric Beta priors: Hyper-parameters ?, ?O, 
?A are not very sensitive and the heuristic values 
suggested in (Griffiths and Steyvers, 2004) usually 
hold well in practice (Wallach et al 2009). 
However, the smoothing hyper-parameter ? 
(Figure 1(c)) is crucial as it governs the aspect or 
sentiment switch. Essentially, ??,?~????????? is the probability of emitting an aspect term2 in ?????? with concentration parameter ? and base measure 
?? ? ???, ???. Without any prior belief, uniform base measures ?? ? ?? ? 0.5 are used resulting in symmetric Beta priors. However, aspects are often 
more probable than sentiments in a sentence (e.g., 
?The beds, sheets, and bedding were dirty.?). Thus, 
it is more principled to employ asymmetric priors. 
Using a labeled set of sentences, ????????, where we know the per sentence probability of aspect 
emission (??,?), we can employ the method of moments to estimate the smoothing hyper-
parameter ? ? ???, ???: 
?? ? ? ???????? ? 1? , ?? ? ?? ?
?
? ? 1? ; ?? ? ????,??, ? ? ??????,?? 
(4) 
                                                          
2 ??,?,?~?????????????,??. ??,? , 1 ? ??,? are the success and failure 
probability of emitting an aspect/sentiment term. 
342
3.2 ME-SAS Model 
We can further improve SAS by employing 
Maximum Entropy (Max-Ent) priors for aspect and 
sentiment switching. We call this new model ME-
SAS. The motivation is that aspect and sentiment 
terms play different syntactic roles in a sentence. 
Aspect terms tend to be nouns or noun phrases 
while sentiment terms tend to be adjectives, 
adverbs, etc. POS tag information can be elegantly 
encoded by moving ??,? to the term plate (see Figure 1(d)) and drawing it from a Max-
Ent??,??,?; ?? model. Let  
???,?,??????????? ? ??????,?,???, ?????,?,? , ?????,?,???, ??,?,? ? 1, ??,?,?, ??,?,? ?
1?  denote the feature vector associated with ??,?,?? encoding lexical and POS features of the previous, 
current and next term. Using a training data set, we 
can learn Max-Ent priors. Note that unlike 
traditional Max-Ent training, we do not need 
manually labeled data for training (see Section 4 
for details). For ME-SAS, only the sampler for the 
switch variable r changes as follows: 
????,?,? ? ??????,?, ???,?,?, ???,?,??, ???,?,?, ??,? ? ?, ??,?,? ? ?? ? 
??,?? ??,?,????
??,???? ??,?,??|??? ??? |??
? ????? ???????,?,?,??????? ?? ????? ???????,?,?,?????? ??????,???       (5) 
????,?,? ? ??? ? ? ?
?
??
?
??
? ??,?,?
?,?
??,?,???
??,?,????,? ??,?,??|??|?
? ??,?? ?????,???? ???????? ??
????? ???????,?,?,??????? ?
? ????? ???????,?,?,?????? ??????,???
??? ; ??? ? ??
??,??,?????
??,????,??????????
? ????? ???????,?,?,??????? ?? ????? ???????,?,?,?????? ??????,??? ; ??, ? ? ??
   (6) 
where ???? are the parameters of the learned Max-Ent model corresponding to the ? binary feature 
functions ???? of Max-Ent. 
4 Experiments 
This section evaluates the proposed models. Since 
the focus in this paper is to generate high quality 
aspects using seeds, we will not evaluate 
sentiments although both SAS and ME-SAS can 
also discover sentiments. To compare the 
performance with our models, we use two existing 
state-of-the-art models, ME-LDA (Zhao et al 
2010) and DF-LDA (Andrzejewski et al, 2009). 
As discussed in Section 2, there are two main 
flavors of aspect and sentiment models. The first 
flavor does not separate aspect and sentiment, and 
the second flavor uses a switch to perform the 
separation. Since our models also perform a 
switch, it is natural to compare with the latter 
flavor, which is also more advanced. ME-LDA is 
the representative model in this flavor. DF-LDA 
adds constraints to LDA. We use our seeds to 
generate constraints for DF-LDA. While ME-LDA 
cannot consider constraints, DF-LDA does not 
separate sentiments and aspects. Apart from other 
modeling differences, our models can do both, 
which enable them to produce much better results. 
Dataset and Settings: We used hotel reviews from 
tripadvisor.com. Our corpus consisted of 101,234 
reviews and 692,783 sentences. Punctuations, stop 
words 3, and words appearing less than 5 times in 
the corpus were removed. 
For all models, the posterior inference was 
drawn after 5000 Gibbs iterations with an initial 
burn-in of 1000 iterations. For SAS and ME-SAS, 
we set ? = 50/T, ?A = ?O = 0.1 as suggested in 
(Griffiths and Steyvers, 2004). To make the seeds 
more effective, we set the seed set word-
distribution hyper-parameter ? to be much larger 
than ?A, the hyper-parameter for the distribution 
over seed sets and aspect terms. This results in 
higher weights to seeded words which in turn 
guide the sampler to cluster relevant terms better. 
A more theoretical approach would involve 
performing hyper-parameter estimation (Wallach 
et al, 2009) which may reveal specific properties 
of the dataset like the estimate of ? (indicating how 
different documents are in terms of their latent 
semantics), ? (suggesting how large the groups of 
frequently appearing aspect and sentiment terms 
are) and ? (giving a sense of which and how large 
groupings of seeds are good). These are interesting 
questions and we defer it to our future work. In this 
work, we found that the setting ? = 250, a larger 
value compared to ?A, produced good results. 
For SAS, the asymmetric Beta priors were 
estimated using the method of moments (Section 
3.1). We sampled 500 random sentences from the 
corpus and for each sentence identified the aspects. 
We thus computed the per-sentence probability of 
aspect emission (??,?) and used Eq. (4) to compute the final estimates, which give ?a = 2.35, ?b = 3.44.  To learn the Max-Ent parameters ? of ME-SAS, 
we used the sentiment lexicon 4 of (Hu and Liu, 
2004) to automatically generate training data (no 
manual labeling). We randomly sampled 1000 
terms from the corpus which have appeared at least 
                                                          3 http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-
list/english.stop 4 http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar 
343
20 times (to ensure that the training set is 
reasonably representative of the corpus). Of those 
1000 terms if they appeared in the sentiment 
lexicon, they were treated as sentiment terms, else 
aspect terms. Clearly, labeling words not in the 
sentiment lexicon as aspect terms may not always 
be correct. Even with this noisy automatically-
labeled data, the proposed models can produce 
good results. Since ME-LDA used manually 
labeled training data for Max-Ent, we again 
randomly sampled 1000 terms from our corpus 
appearing at least 20 times and labeled them as 
aspect terms or sentiment terms, so this labeled 
data clearly has less noise than our automatically 
labeled data. For both ME-SAS and ME-LDA we 
used the corresponding feature vector of each 
labeled term (in the context of sentences where it 
occurs) to train the Max-Ent model. As DF-LDA 
requires must-link and cannot-link constraints, we 
used our seed sets to generate intra-seed set must-
link and inter-seed set cannot-link constraints. For 
its hyper-parameters, we used the default values in 
the package5 (Andrzejewski et al, 2009). 
Setting the number of topics/aspects in topic 
models is often tricky as it is difficult to know the 
                                                          
5 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html 
exact number of topics that a corpus has. While 
non-parametric Bayesian approaches (Teh et al, 
2006) do exist for estimating the number of topics, 
T, they strongly depend on the hyper-parameters 
(Heinrich, 2009). As we use fixed hyper-
parameters, we do not learn T from Bayesian non-
parametrics. We used 9 major aspects (T = 9) 
based on commonsense knowledge of what people 
usually talk about hotels and some experiments. 
These are Dining, Staff, Maintenance, Check In, 
Cleanliness, Comfort, Amenities, Location and 
Value for Money (VFM). However, it is important 
to note that the proposed models are flexible and 
do not need to have seeds for every aspect/topic. 
Our experiments simulate the real-life situation 
where the user may not know all aspects or have 
no seeds for some aspects. Thus, we provided 
seeds only to the first 6 of the 9 aspects/topics. We 
will see that without seeds for all aspects, our 
models not only can improve the seeded aspects 
but also improve the non-seeded aspects. 
4.1 Qualitative Results  
This section shows some qualitative results to give 
an intuitive feeling of the results from different 
models. Table 1 shows the aspect terms and 
sentiment terms discovered by the 4 models for 
Aspect 
(seeds) 
ME-SAS SAS ME-LDA DF-LDA
Aspect Sentiment Aspect Sentiment Aspect Sentiment Topic
 Staff  (staff service waiter hospitality upkeep)  
attendantmanager waitress maintenance bartender waiters housekeepingreceptionist waitstaff janitor 
friendly attentive polite nice 
clean pleasant slow courteous rude professional 
attendantwaiter waitress manager maintenance 
helpful waiters housekeepingreceptionist 
polite
friendlynice dirty comfortablenice 
clean polite 
extremely courteous efficient
staffmaintenance room upkeep 
linens room-service receptionist 
wait pillow waiters
friendly nice courteous extremely nice 
clean polite 
little helpful 
better  
stafffriendly helpful beds front room comfortable large receptionist housekeeping
 Cleanliness 
 (curtains restroom floor beds cleanliness) 
carpets hall towels bathtub couch mattress linens wardrobe spa pillow 
clean dirty comfortable fresh wet filthy extra stain 
front worn 
hallcarpets towels pillow stain mattress 
filthy linens 
interior bathtub
cleandirty fresh old nice good 
enough new 
front friendly
cleanlinessfloor carpets bed lobby bathroom 
staff closet spa d?cor
clean good dirty 
hot large nice 
fresh thin new little 
cleanpool beach carpets parking bed bathroom nice comfortable suite
 Comfort 
 (comfort mattress furniture couch pillows) 
bedding bedcover sofa linens bedroom suites d?cor comforter blanket futon 
comfortable clean soft nice uncomfortable spacious hard comfy dirty quiet 
bedlinens sofa bedcover 
hard bedroom privacy double comfy futon
nicedirty comfortable large clean 
best spacious 
only big 
extra
bedmattress suites furniture 
lighting d?cor room bedroom hallway carpet
great clean awesome dirty best comfortable soft nice only extra 
bedmattress 
nice stay lighting lobby comfort room dirty sofa
Table 1: Top ranked aspect and sentiment words in three aspects (please see the explanation in Section 4.1).?
344
three aspects. Due to space limitations, we are 
unable to show all 6 aspects for which we have 
seeds. Since DF-LDA cannot separate aspects and 
sentiments, we only show its topics (aspects). Red 
(bold) colored words show semantic clustering 
errors or inappropriate terms for different groups.  
It is important to note that we judge the results 
based on how they are related to the user seeds 
(which represent the user need). The judgment is to 
some extent subjective. What we reported here are 
based on our judgments what are appropriate and 
what are not for each aspect. For SAS, ME-SAS 
and ME-LDA, we mark sentiment terms as errors 
when they are grouped under aspects as these 
models are supposed to separate sentiments and 
aspects. For DF-LDA, the situation is different as it 
is not meant to separate sentiment and aspect 
terms, we use red italic font to indicate those 
adjectives which are aspect specific adjectives (see 
more discussion below). Our judgment may be 
slightly unfair to ME-LDA and DF-LDA as their 
results may make sense in some other ways. 
However, that is precisely the purpose of this 
work, to produce results that suit the user?s need 
rather than something generic. 
We can see from Table 1 that ME-SAS performs 
the best. Next in order are SAS, ME-LDA, and 
DF-LDA. We see that only providing a handful of 
seeds (5) for the aspect Staff, ME-SAS can 
discover highly specific words like manager, 
attendant, bartender, and janitor. By specific, we 
mean they are highly related to the given seeds. 
While SAS also discovers specific words 
benefiting from seeds, relying on Beta priors for 
aspect and sentiment switching was less effective. 
Next in performance is ME-LDA which although 
produces reasonable results in general, several 
aspect terms are far from what the user wants 
based on the seeds, e.g., room, linens, wait, pillow. 
Finally, we observe that DF-LDA does not perform 
well either. One reason is that it is unable to 
separate aspects and sentiments. Although 
encoding the intra-seed set must-link and inter-
seed set cannot-link constraints in DF-LDA 
discovers some specific words as ME-SAS, they 
are much lower in the ranked order and hence do 
not show up in the top 10 words in Table 1. As 
DF-LDA is not meant to perform extraction and to 
group both aspect and sentiment terms, we relax 
the errors of DF-LDA due to correct aspect 
specific sentiments (e.g., friendly, helpful for Staff 
are correct aspect specific sentiments, but still 
regard incorrect sentiments like front, comfortable, 
large as errors) placed in aspect models. We call 
this model DF-LDA-Relaxed. 
4.2 Quantitative Results   
Topic models are often evaluated quantitatively 
using perplexity and likelihood on held-out test 
data (Blei et al, 2003). However, perplexity does 
not reflect our purpose since our aim is not to 
predict whether an unseen document is likely to be 
a review of some particular aspect. Nor are we 
trying to evaluate how well the unseen review data 
fits our seeded models. Instead our focus is to 
evaluate how well our learned aspects perform in 
clustering specific terms guided by seeds. So we 
directly evaluate the discovered aspect terms. Note 
again we do not evaluate sentiment terms as they 
are not the focus of this paper 6. Since aspects 
produced by the models are rankings and we do 
not know the number of correct aspect terms, a 
natural way to evaluate these rankings is to use 
precision @ n (or p@n), where n is a rank position. 
Varying number of seeds: Instead of a fixed 
number of seeds, we want to see the effect of the 
number of seeds on aspect discovery. Table 2 
reports the average p@n vs. the number of seeds. 
The average is a two-way averaging. The first 
average was taken over all combinations of actual 
seeds selected for each aspect, e.g., when the 
number of seeds is 3, out of the 5 seeds in each 
aspect, all ?53? combinations of seeds were tried and the results averaged. The results were further 
averaged over p@n for 6 aspects with seeds. We 
start with 2 seeds and progressively increase them 
to 5. Using only 1 seed per seed set (or per aspect) 
has practically no effect because the top level 
distribution ?? encodes which seed sets (and non-
seed words) to include; the lower-level distribution 
? constrains the probabilities of the seed words to 
be correlated for each of the seed sets. Thus, 
having only one seed per seed set will result in 
sampling that single word whenever that seed set is 
chosen which will not have the effect of correlating 
seed words so as to pull other words based on co-
occurrence with constrained seed words. From 
Table 2, we can see that for all models p@n 
progressively improves as the number of seeds 
increases. Again ME-SAS performs the best 
followed by SAS and DF-LDA. 
                                                          6 A qualitative evaluation of sentiment extraction based on Table 1 yields 
the following order: ME-SAS, SAS, ME-LDA. 
345
Effect of seeds on non-seeded aspects: Here we 
compare all models aspect wise and see the results 
of seeded models SAS and ME-SAS on non-
seeded aspects (Table 3).  Shaded cells in Table 3 
give the p@n values for DF-LDA, DF-LDA-
Relaxed, SAS, and ME-SAS on three non-seeded 
aspects (Amenities, Location, and VFM)7.  
We see that across all the first 6 aspects with (5) 
seeds ME-SAS outperforms all other models by 
large margins in all top 3 ranked buckets p@10, 
p@20 and p@30. Next in order are SAS, ME-LDA 
and DF-LDA. For the last three aspects which did 
not have any seed guidance, we find something 
interesting. Seeded models SAS and especially 
ME-SAS result in improvements of non-seeded 
aspects too. This is because as seeds facilitate 
clustering specific and appropriate terms in seeded 
aspects, which in turn improves precision on non-
seeded aspects. This phenomenon can be clearly 
seen in Table 1. In aspect Staff of ME-LDA, we 
find pillow and linens being clustered. This is not a 
?flaw? of the model per se, but the point here is 
pillow and linens happen to co-occur many times 
with other words like maintenance, staff, and 
upkeep because ?room-service? generally includes 
staff members coming and replacing linens and 
pillow covers. Although pillow and linens are 
related to Staff, strictly speaking they are 
semantically incorrect because they do not 
represent the very concept ?Staff? based on the 
seeds (which reflect the user need). Presence of 
                                                          
7 Note that Tables 2 and 3 are different runs of the model. The variations in the 
results are due to the random initialization of the Gibbs sampler. 
seed sets in SAS and ME-SAS result in pulling 
such words as linens and pillow (due to seeds like 
beds and cleanliness in the aspect Cleanliness) and 
ranking them higher in the aspect Cleanliness (see 
Table 1) where they make more sense than Staff. 
Lastly, we also note that the improvements in non-
seeded aspects are more pronounced for ME-SAS 
than SAS as SAS encounters more switching errors 
which counters the improvement gained by seeds.  
In summary, the averages over all aspects (Table 
3 last row) show that the proposed seeded models 
SAS and ME-SAS outperform ME-LDA, DF-LDA 
and even DF-LDA-Relaxed considerably. 
5 Conclusion 
This paper studied the issue of using seeds to 
discover aspects in an opinion corpus. To our 
knowledge, no existing work deals with this 
problem. Yet, it is important because in practice 
the user often has something in mind to find. The 
results obtained in a completely unsupervised 
manner may not suit the user?s need. To solve this 
problem, we proposed two models SAS and ME-
SAS which take seeds reflecting the user needs to 
discover specific aspects. ME-SAS also does not 
need any additional help from the user in its Max-
Ent training. Our results showed that both models 
outperformed two state-of-the-art existing models 
ME-LDA and DF-LDA by large margins. 
Acknowledgments 
This work is supported in part by National Science 
Foundation (NSF) under grant no. IIS-1111092.  
No. of Seeds DF-LDA DF-LDA-Relaxed SAS ME-SAS P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 
2 0.51 0.53 0.49 0.67 0.69 0.70 0.69 0.71 0.67 0.74 0.72 0.70 
3 0.53 0.54 0.50 0.71 0.70 0.71 0.71 0.72 0.70 0.78 0.75 0.72 
4 0.57 0.56 0.53 0.73 0.73 0.73 0.75 0.74 0.73 0.83 0.79 0.76 
5 0.59 0.57 0.54 0.75 0.74 0.75 0.77 0.76 0.74 0.86 0.81 0.77 
Table 2: Average p@n of the seeded aspects with the no. of seeds. 
Aspect ME-LDA DF-LDA DF-LDA-Relaxed SAS ME-SAS
P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30 P@10 P@20 P@30
Dining 0.70 0.65 0.67 0.50 0.60 0.63 0.70 0.70 0.70 0.80 0.75 0.73 0.90 0.85 0.80
Staff 0.60 0.70 0.67 0.40 0.65 0.60 0.60 0.75 0.67 0.80 0.80 0.70 1.00 0.90 0.77
Maintenance 0.80 0.75 0.73 0.40 0.55 0.56 0.60 0.70 0.73 0.70 0.75 0.76 0.90 0.85 0.80
Check In 0.70 0.70 0.67 0.50 0.65 0.60 0.80 0.75 0.70 0.80 0.70 0.73 0.90 0.80 0.76
Cleanliness 0.70 0.75 0.67 0.70 0.70 0.63 0.70 0.75 0.70 0.80 0.75 0.70 1.00 0.85 0.83
Comfort 0.60 0.70 0.63 0.60 0.65 0.50 0.70 0.75 0.63 0.60 0.75 0.67 0.90 0.80 0.73
Amenities 0.80 0.80 0.67 0.70 0.65 0.53 0.90 0.75 0.73 0.90 0.80 0.70 1.00 0.85 0.73
Location 0.60 0.70 0.63 0.50 0.60 0.56 0.70 0.70 0.67 0.60 0.70 0.63 0.70 0.75 0.67
VFM 0.50 0.55 0.50 0.40 0.50 0.46 0.60 0.60 0.60 0.50 0.50 0.50 0.60 0.55 0.53
Avg. 0.67 0.70 0.65 0.52 0.62 0.56 0.70 0.72 0.68 0.72 0.72 0.68 0.88 0.80 0.74
Table 3: Effect of performance on seeded and non-seeded aspects (5 seeds were used for the 6 seeded aspects).?
346
References  
Andrzejewski, D., Zhu, X. and Craven, M. 2009. 
Incorporating domain knowledge into topic modeling 
via Dirichlet forest priors. Proceedings of 
International Conference on Machine Learning 
(ICML). 
 Andrzejewski, D., Zhu, X. and Craven, M. and Recht, 
B. 2011. A framework for incorporating general 
domain knowledge into latent Dirichlet alocation 
using first-order logic. Proceedings of the 22nd 
International Joint Conferences on Artificial 
Intelligence (IJCAI).  
Blair-Goldensohn, S., Hannan, K., McDonald, R., 
Neylon, T., Reis, G. A. and Reynar, J. 2008. Building 
a sentiment summarizer for local service reviews. 
Proceedings of WWW-2008 workshop on NLP in the 
Information Explosion Era. 
Blei, D., Ng, A. and Jordan, M. 2003. Latent dirichlet 
allocation. The Journal of Machine Learning 
Research 3: 993-1022. 
Blei D. and McAuliffe, J. 2007. Supervised topic 
models. Neural Information Processing Systems 
(NIPS). 
Branavan, S., Chen, H., Eisenstein J. and Barzilay, R. 
2008. Learning document-level semantic properties 
from free-text annotations. Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL). 
Brody, S. and Elhadad, S. 2010. An Unsupervised 
Aspect-Sentiment Model for Online Reviews. 
Proceedings of The 2010 Annual Conference of the 
North American Chapter of the ACL (NAACL). 
Carenini, G., Ng, R. and Zwart, E. 2005. Extracting 
knowledge from evaluative text. Proceedings of 
Third Intl. Conf. on Knowledge Capture (K-CAP-
05). 
Chang, J., Boyd-Graber, J., Wang, C.  Gerrish, S. and 
Blei, D. 2009. Reading tea leaves: How humans 
interpret topic models. In Neural Information 
Processing Systems (NIPS). 
Choi, Y. and Cardie, C. 2010. Hierarchical sequential 
learning for extracting opinions and their attributes. 
Proceedings of Annual Meeting of the Association 
for Computational (ACL). 
Griffiths, T. and Steyvers, M. 2004. Finding scientific 
topics. Proceedings of National Academy of Sciences 
(PNAS). 
Guo, H., Zhu, H., Guo, Z., Zhang, X. and Su, X. 2009. 
Product feature categorization with multilevel latent 
semantic association. Proceedings of ACM 
International Conference on Information and 
Knowledge Management (CIKM). 
Heinrich, G. 2009. A Generic Approach to Topic 
Models. Proceedings of the European Conference on 
Machine Learning and Principles and Practice of 
Knowledge Discovery in Databases (ECML/PKDD). 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. Proceedings of Conference on Uncertainty 
in Artificial Intelligence (UAI). 
Hu, Y., Boyd-Graber, J. and Satinoff, B. 2011. 
Interactive topic modeling. Proceedings of Annual 
Meeting of the Association for Computational 
Linguistics (ACL), 2011. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. International Conference on 
Knowledge Discovery and Data Mining (ICDM). 
Jakob, N. and Gurevych, I. 2010. Extracting Opinion 
Targets in a Single-and Cross-Domain Setting with 
Conditional Random Fields. Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP). 
Jin, W. and Ho, H. 2009. A novel lexicalized HMM-
based learning framework for web opinion mining. 
Proceedings of International Conference on Machine 
Learning (ICML). 
Jo, Y. and Oh, A. 2011. Aspect and sentiment 
unification model for online review analysis. ACM 
Conference in Web Search and Data Mining 
(WSDM). 
Kobayashi, N., Inui, K. and Matsumoto, K. 2007. 
Extracting aspect-evaluation and aspect-of relations 
in opinion mining. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL). 
Ku, L., Liang, Y. and Chen, H. 2006. Opinion 
extraction, summarization and tracking in news and 
blog corpora. Proceedings of AAAI Symposium on 
Computational Approaches to Analyzing Weblogs 
(AAAI-CAAW'06). 
Li, F., Han, C., Huang, M., Zhu, X. Xia, Y., Zhang, S.  
and Yu, H. 2010. Structure-aware review mining and 
summarization. International Conference on 
Computational Linguistics (COLING). 
Lin, C. and He, Y. 2009. Joint sentiment/topic model for 
sentiment analysis. Proceedings of ACM 
International Conference on Information and 
Knowledge Management (CIKM). 
Liu, B. 2012. Sentiment Analysis and Opinion Mining. 
347
Morgan & Claypool publishers (to appear in June 
2012).  
Liu, B, M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and comparing opinions on the web. 
Proceedings of International Conference on World 
Wide Web (WWW).  
Lu, Y., Zhai, C. and Sundaresan, N. 2009. Rated aspect 
summarization of short comments. Proceedings of 
International Conference on World Wide Web 
(WWW). 
Lu, Y. and Zhai, C. 2008. Opinion Integration Through 
Semi-supervised Topic Modeling. Proceedings of the 
17th International World Wide Web Conference 
(WWW). 
Ma, T. and Wan, X. 2010. Opinion target extraction in 
Chinese news comments. Proceedings of Coling 
2010 Poster Volume (COLING). 
Mei, Q., Ling, X., Wondra, M., Su, H. and Zhai, C. 
2007. Topic sentiment mixture: modeling facets and 
opinions in weblogs. Proceedings of International 
Conference on World Wide Web (WWW). 
Moghaddam, S. and Ester, M. 2011. ILDA: 
interdependent LDA model for learning latent 
aspects and their ratings from online product reviews. 
Proceedings of the Annual ACM SIGIR International 
conference on Research and Development in 
Information Retrieval (SIGIR). 
Pang, B. and Lee, L. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval. 
Popescu, A. and Etzioni, O. 2005. Extracting product 
features and opinions from reviews. Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP). 
Qiu, G., Liu, B., Bu, J. and Chen, C. 2011. Opinion 
Word Expansion and Target Extraction through 
Double Propagation. Computational Linguistics. 
Ramage, D., Hall, D., Nallapati, R. and Manning, C. 
2009. Labeled LDA: a supervised topic model for 
credit attribution in multi-labeled corpora. 
Proceedings of the Conference on Empirical Methods 
in Natural Language Processing (EMNLP). 
Sauper, C., Haghighi, A. and Barzilay, R. 2011. Content 
models with attitude. Proceedings of the 49th Annual 
Meeting of the Association for Computational 
Linguistics (ACL). 
Somasundaran, S. and Wiebe, J. 2009. Recognizing 
stances in online debates, Proceedings of the 47th 
Annual Meeting of the ACL and the 4th IJCNLP of 
the AFNLP. 
Teh, Y., Jordan, M., Beal, M. and Blei, D. 2006. 
Hierarchical Dirichlet Processes. In Journal of the 
American Statistical Association (JASA). 
Titov, I. and McDonald, R. 2008. Modeling online 
reviews with multi-grain topic models. Proceedings 
of International Conference on World Wide Web 
(WWW). 
Wallach, H., Mimno, D. and McCallum, A. 2009. 
Rethinking LDA: Why priors matter. In Neural 
Information Processing Systems (NIPS). 
Wang, H., Lu, Y. and Zhai, C. 2010. Latent aspect 
rating analysis on review text data: a rating 
regression approach. Proceedings of ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD). 
Wu, Y., Zhang, Q., Huang, X. and Wu, L. 2009. Phrase 
dependency parsing for opinion mining. Proceedings 
of Conference on Empirical Methods in Natural 
Language Processing (EMNLP). 
Yi, J., Nasukawa, T., Bunescu, R. and Niblack, W. 
2003. Sentiment analyzer: Extracting sentiments 
about a given topic using natural language processing 
techniques. Proceedings of IEEE International 
Conference on Data Mining (ICDM). 
Yu, J., Zha, Z. J., Wang, M. and Chua, T. S. 2011. 
Aspect ranking: identifying important product 
aspects from online consumer reviews. Proceedings 
of the 49th Annual Meeting of the Association for 
Computational Linguistics, Association for 
Computational Linguistics (ACL). 
Zhai, Z., Liu, B. Xu, H. and Jia, P. 2010. Grouping 
Product Features Using Semi-Supervised Learning 
with Soft-Constraints. Proceedings of International 
Conference on Computational Linguistics 
(COLING). 
Zhai, Z., Liu, B. Xu, H. and Jia, P. 2011. Constrained 
LDA for Grouping Product Features in Opinion 
Mining. Proceedings of Pacific-Asia Conference on 
Knowledge Discovery and Data Mining (PAKDD). 
Zhao, W., Jiang, J., Yan, Y. and Li, X. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-LDA 
hybrid. Proceedings of Conference on Empirical 
Methods in Natural Language Processing (EMNLP). 
Zhuang, L., Jing, F. and Zhu, X. 2006. Movie review 
mining and summarization. Proceedings of 
International Conference on Information and 
Knowledge Management (CIKM). 
 
348
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671?681,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discovering User Interactions in Ideological Discussions 
Arjun Mukherjee     Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
arjun4787@gmail.com  liub@cs.uic.edu 
 
Abstract 
Online discussion forums are a popular 
platform for people to voice their opinions on 
any subject matter and to discuss or debate 
any issue of interest. In forums where users 
discuss social, political, or religious issues, 
there are often heated debates among users or 
participants. Existing research has studied 
mining of user stances or camps on certain 
issues, opposing perspectives, and contention 
points. In this paper, we focus on identifying 
the nature of interactions among user pairs. 
The central questions are: How does each 
pair of users interact with each other? Does 
the pair of users mostly agree or disagree? 
What is the lexicon that people often use to 
express agreement and disagreement? We 
present a topic model based approach to 
answer these questions. Since agreement and 
disagreement expressions are usually multi-
word phrases, we propose to employ a 
ranking method to identify highly relevant 
phrases prior to topic modeling. After 
modeling, we use the modeling results to 
classify the nature of interaction of each user 
pair. Our evaluation results using real-life 
discussion/debate posts demonstrate the 
effectiveness of the proposed techniques.  
1 Introduction 
Online discussion/debate forums allow people 
with common interests to freely ask and answer 
questions, to express their views and opinions on 
any subject matter, and to discuss issues of 
common interest. A large part of such 
discussions is about social, political, and 
religious issues. On such issues, there are often 
heated discussions/debates, i.e., people agree or 
disagree and argue with one another. Such 
ideological discussions on a myriad of social and 
political issues have practical implications in the 
fields of communication and political science as 
they give social scientists an opportunity to study 
real-life discussions/debates of almost any issue 
and analyze participant behaviors in a large scale. 
In this paper, we present such an application, 
which aims to perform fine-grained analysis of 
user-interactions in online discussions.  
There have been some related works that focus 
on discovering the general topics and ideological 
perspectives in online discussions (Ahmed and 
Xing, 2010), placing users in support/oppose 
camps (Agarwal et al, 2003), and classifying 
user stances (Somasundaran and Wiebe, 2009). 
However, these works are at a rather coarser 
level and have not considered more fine-grained 
characteristics of debates/discussions where users 
interact with each other by quoting/replying each 
other to express agreement or disagreement and 
argue with one another. In this work, we want to 
mine the following information: 
1. The nature of interaction of each pair of users 
or participants who have engaged in the 
discussion of certain issues, i.e., whether the 
two persons mostly agree or disagree with 
each other in their interactions. 
2. What language expressions are often used to 
express agreement (e.g., ?I agree? and ?you?re 
right?) and disagreement (e.g., ?I disagree? 
and ?you speak nonsense?).  
We note that although agreement and 
disagreement expressions are distinct from 
traditional sentiment expressions (words and 
phrases) such as good, excellent, bad, and 
horrible, agreement and disagreement clearly 
express a kind of sentiment as well. They are 
usually emitted during interactive exchanges of 
arguments in ideological discussions. This idea 
prompted us to introduce the concept of AD-
sentiment. We define the polarity of agreement 
expressions as positive and the polarity of 
disagreement expressions as negative. We refer 
agreement and disagreement expressions as AD-
sentiment expressions, or AD-expressions for 
short. AD-expressions are crucial for the analysis 
of interactive discussions and debates just as 
sentiment expressions are instrumental in 
sentiment analysis (Liu, 2012). We thus regard 
this work as an extension to traditional sentiment 
671
analysis (Pang and Lee, 2008; Liu, 2012).  
In our earlier work (Mukherjee and Liu, 
2012a), we proposed three topic models to mine 
contention points, which also extract AD-
expressions. In this paper, we further improve the 
work by coupling an information retrieval 
method to rank good candidate phrases with topic 
modeling in order to discover more accurate AD-
expressions. Furthermore, we apply the resulting 
AD-expressions to the new task of classifying the 
arguing or interaction nature of each pair of 
users. Using discovered AD-expressions for 
classification has an important advantage over 
traditional classification because they are domain 
independent. We employ a semi-supervised 
generative model called JTE-P to jointly model 
AD-expressions, pair interactions, and discussion 
topics simultaneously in a single framework. 
With such complex interactions mined, we can 
produce many useful summaries of discussions. 
For example, we can discover the most 
contentious pairs for each topic and ideological 
camps of participants, i.e., people who often 
agree with each other are likely to belong to the 
same camp. The proposed framework also 
facilitates tracking users? ideology shifts and the 
resulting arguing nature. 
The proposed methods have been evaluated 
both qualitatively and quantitatively using a large 
number of real-life discussion/debate posts from 
four domains. Experimental results show that the 
proposed model is highly effective in performing 
its tasks and outperforms several baselines. 
2 Related Work 
There are several research areas that are related 
to our work. We compare with them below.  
Sentiment analysis: Sentiment analysis 
determines positive and negative opinions 
expressed on entities and aspects (Hu and Liu, 
2004). Main tasks include aspect extraction (Hu 
and Liu, 2004; Popescu and Etzioni, 2005), 
polarity identification (Hassan and Radev, 2010; 
Choi and Cardie, 2010) and subjectivity analysis 
(Wiebe, 2000). As discussed earlier, agreement 
and disagreement are a special form of 
sentiments and are different from the sentiment 
studied in the mainstream research. Traditional 
sentiment is mainly expressed with sentiment 
terms (e.g., great and bad), while agreement and 
disagreement are inferred by AD-expressions 
(e.g., I agree and I disagree), which we also call 
AD-sentiment expressions. Thus, this work 
expands the sentiment analysis research.  
Topic models: Our work is also related to topic 
modeling and joint modeling of topics and other 
information as we jointly model several aspects 
of discussions/debates.  
Topic models like pLSA (Hofmann, 1999) and 
LDA (Blei et al, 2003) have proved to be very 
successful in mining topics from large text 
collections. There have been various extensions 
to multi-grain (Titov and McDonald, 2008), 
labeled (Ramage et al, 2009), and sequential (Du 
et al, 2010) topic models. Yet other approaches 
extend topic models to produce author specific 
topics (Rosen-Zvi et al, 2004), author persona 
(Mimno and McCallum, 2007), social roles 
(McCallum et al, 2007), etc. However, these 
models do not model debates and hence are 
unable to discover AD-expressions and 
interaction natures of author pairs.  
Also related are topic models in sentiment 
analysis which are often referred to as Aspect 
and Sentiment models (ASMs). ASMs come in 
two main flavors: Type-1 ASMs discover aspect 
(or topic) words sentiment-wise (i.e., discovering 
positive and negative topic words and sentiments 
for each topic without separating topic and 
sentiment terms) (e.g., Lin and He, 2009; Brody 
and Elhadad, 2010, Jo and Oh, 2011). Type-2 
ASMs separately discover both aspects and 
sentiments (e.g., Mei et al, 2007; Zhao et al, 
2010). Recently, domain knowledge induced 
ASMs have also been proposed (Mukherjee and 
Liu, 2012b; Chen et al, 2013). The generative 
process of ASMs is, however, different from our 
model. Specifically, Type-1 ASMs use 
asymmetric hyper-parameters for aspects while 
Type-2 assumes that sentiments and aspects are 
emitted in the same sentence. However, AD-
expressions are emitted differently. They are 
mostly interleaved with users? topical viewpoints 
and span different sentences. Further, we capture 
the key characteristic of discussions by encoding 
pair-wise user interactions. Existing models do 
not model pair interactions. 
In terms of discussions and comments, Yano 
et al, (2009) proposed the CommentLDA model 
which builds on the work of LinkLDA (Erosheva 
et al, 2004). Mukherjee and Liu (2012d) mined 
comment expressions. These works, however, 
don?t model pair interactions in debates. 
Support/oppose camp classification: Several 
works have attempted to put debate authors into 
support/oppose camps. Agrawal et al (2003) 
used a graph based method. Murakami and 
Raymond (2010) used a rule-based method. In 
(Galley et al, 2004; Hillard et al, 2003), speaker 
672
utterances were classified into agreement, 
disagreement and backchannel classes. 
Stances in online debates: Somasundaran and 
Wiebe (2009), Thomas et al (2006), Bansal et al 
(2008), Burfoot et al (2011), and Anand et al 
(2011) proposed methods to recognize stances in 
online debates. Some other research directions 
include subgroup detection (Abu-Jbara et al, 
2012), tolerance analysis (Mukherjee et al, 
2013), mining opposing perspectives (Lin and 
Hauptmann, 2006), linguistic accommodation 
(Mukherjee and Liu, 2012c), and contention 
point mining (Mukherjee and Liu, 2012a). For 
this work, we adopt the JTE-P model in 
(Mukherjee and Liu, 2012a), and make two 
major advances. We propose a new method to 
improve the AD-expression mining  and a new 
task of classifying pair interaction nature to 
determine whether each pair of users who have 
interacted based on replying relations mostly 
agree or disagree with each other. 
3 Model  
We now introduce the JTE-P model with 
additional details. JTE-P is a semi-supervised 
generative model motivated by the joint 
occurrence of expression types (agreement and 
disagreement), topics in discussion posts, and 
user pairwise interactions. Before proceeding, we 
make the following observation about online 
discussions. 
In a typical debate/discussion post, the user 
(author) mentions a few topics (using 
semantically related topical terms) and expresses 
some viewpoints with one or more AD-
expression types (using agreement and 
disagreement expressions). AD-expressions are 
directed towards other user(s), which we call 
target(s). In this work, we focus on explicit 
mentions (i.e., using @name or quoting other 
authors? posts). In our crawled dataset, 77% of 
all posts exhibit explicit quoting/reply-to 
relations excluding the first posts of threads 
which start the discussions and usually have 
nobody to quote/reply-to. Such author-target 
exchanges usually go back and forth between 
pairs of users populating a thread of discussion. 
The discussion topics and AD-expressions 
emitted are thus caused by the author-pairs? 
topical interests and their nature of interaction 
(agreeing vs. disagreeing).  
In our discussion data obtained from 
Volconvo.com, we found that a pair of users 
typically exhibited a dominant arguing nature 
(agreeing vs. disagreeing) towards each other 
across various topics or threads. We believe this 
is because our data consists of topics like 
elections, theism, terrorism, vegetarianism, etc. 
which are often heated and attract people with 
pre-determined, strong, and polarized stances1. 
This observation motivates the generative 
process of our model. Referring to the notations 
in Table 1, we explain the generative process of 
JTE-P. Given a document (post) ?, its author, ??, 
and the list of targets to whom ?? replies/quotes 
                                                          
1 These hardened perspectives are supported by theoretical 
studies in communications like the polarization effect 
(Sunstein, 2002), and the hostile media effect, a scenario 
where partisans rigidly hold on to their stances (Hansen and 
Hyunjung, 2011). 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: JTE-P Model in plate notation. 
Variable/Function Description 
?; ?? 
A document (post) ? ; author ?  of 
document, ? 
?? = [?1? ??] 
List of targets to whom ?? 
replies/quotes in d. 
? = (?, ??) 
Pair of two authors interacting by 
reply/quote. 
??
?; 
??
?(??,??
?  , 
??,?????
? ) 
Pair ? ?s distribution over topics ; 
expression types (Agreement: ??,??
? , 
Disagreement: ??,?????
? ) 
??
?;  ???{??,?????}
?  Topic ? ?s ; Expression type ? ?s 
distribution over vocabulary terms 
?;? Total number  of topics; expression types 
?;? Total number of vocabulary terms; pairs 
??,?; ?? ??? term in ?;  Total # of terms in ? 
? ?,?  Distribution over topics and AD-
expressions 
??,? 
Associated feature context of the 
observed term ??,? 
? Learned Max-Ent parameters 
??,? ? {???, ???} 
Binary indicator/switch variable ( topic 
(???) or AD-expression (???) ) for ??,? 
??,? Topic/Expression type of ??,? 
??; ??; ??; ?? Dirichlet priors of ??
?;  ??
? ;??
?;  ??
? 
??,?
??; ??,?
??  
# of times topic ? ; expression type ? 
assigned to ? 
??,?
??; ??,?
??  
# of times term ?  appears in topic ? ; 
expression type ? 
Table 1: List of Notations 
?T 
 
  
T 
?E 
 
  
E  
?
E
 
 
?
T
 
   
?
E
 ?
E
  
?
T
  P  
?
T
 
  
z 
 
r  
w  
  
c 
 
p 
 
D 
  
N
d
 
? 
 
 
x 
 
? 
w 
 
a
d
 
 
bd 
673
in ? , ?? = [?1? ??] , the document ?  exhibits 
shared topics and arguing nature of various pairs, 
? = (?? , ?)   , where ? ? ?? . More precisely, the 
pair specific topic and AD-expression 
distributions (??
? ; ??
? ) ?shape? the topics and 
AD-expressions emitted in ?  as agreement and 
disagreement on topical viewpoints are directed 
towards certain target authors. Each topic (???) 
and AD-expression type (???) is characterized by 
a multinomial distribution over terms 
(words/phrases). Assume we have ? = 1 ?? 
topics and ? = 1 ??  expression types in our 
corpus. Note that in our case of discussion/debate 
forums, we hypothesize ? = 2 as in debates, we 
mostly find two expression types: agreement and 
disagreement (more details in ?6.1). Like most 
generative models for text, a post (document) is 
viewed as a bag of n-grams and each n-gram 
(word/phrase) takes one value from a predefined 
vocabulary. In this work, we use up to 4-grams, 
i.e., n = 1, 2, 3, 4. Instead of using all n-grams, a 
relevance based ranking method is proposed to 
select a subset of highly relevant n-grams for 
model building (details in ?4). For notational 
convenience, we use terms to denote both words 
(unigrams) and phrases (n-grams). 
JTE-P is a switching graphical model (Ahmed 
and Xing, 2010; Zhao et al, 2010) performing a 
switch between AD-expressions and topics. ??,? 
denotes the distribution over topics and AD-
expressions with ??,? ? {???, ???} denoting the binary 
indicator/switch variable (topic or AD-
expression) for the ? th term of ? , ??,? .  To 
perform the switch we use a maximum entropy 
(Max-Ent) model. The idea is motivated by the 
observation that topical and AD-expression terms 
usually play different roles in a sentence. Topical 
terms (e.g., ?elections? and ?income tax?) tend to 
be noun and noun phrases while AD-expression 
terms (?I refute?, ?how can you say?, and 
?probably agree?) usually contain pronouns, 
verbs, wh-determiners, and modals. In order to 
utilize the part-of-speech (POS) tag information, 
we place the topic/AD-expression distribution 
??,? (the prior over the indicator variable ??,?) in 
the term plate (see Figure 1) and set it from a 
Max-Ent model conditioned on the observed 
feature context ??,?  associated with ??,?  and the 
learned Max-Ent parameters, ? (details in ?6.1). 
In this work, we use both lexical and POS 
features of the previous, current, and next POS 
tags/lexemes of the term ??,?  as the contextual 
information, i.e., ??,? = [?????,??1 , ?????,? ,
?????,?+1 , ??,??1,??,? , ??,?+1], which is used to 
produce the feature functions for Max-Ent. For 
phrasal terms (n-grams), all POS tags and 
lexemes of ??,?  are considered as contextual 
information for computing feature functions in 
Max-Ent. We now detail the generative process 
of JTE-P (plate notation in Figure 1) as follows: 
1. For each AD-expression type ?, draw ???~???(??) 
2. For each topic ?, draw ???~???(??) 
3. For each pair ?, draw ???~???(??); ???~???(??) 
4. For each forum discussion post ? ? {1 ??}: 
i. Given the author ?? and the list of targets ?? , for 
each term ??,?, ? ? {1 ???}: 
a. Draw a target ?~???(??) 
b. Form pair ? = (?? , ?), ? ? ??   
c. Set ??,? ? ??????(??,?; ?) 
d. Draw ??,?~????(??,?) 
e. if (??,? = ???) // ??,? is an AD-expression term 
Draw ??,?~????(??
?) 
else // ??,? = ???, ??,? is a topical term 
Draw ??,?~????(??
?) 
f. Emit ??,?~????(???,?
??,?) 
??? , ???? , ???? , and ???  correspond to the 
Dirichlet, Multinomial, Bernoulli, and Uniform 
distributions respectively. To learn JTE-P, we 
employ approximate posterior inference using 
Monte Carlo Gibbs sampling. Denoting the 
random variables {?, ?,?, ?} associated with each 
term by singular subscripts {??, ??,??, ??}, ?1?? , 
? = ? ??? , a single Gibbs sweep consists of 
performing the following sampling. 
?(?? = ?,?? = ?, ?? = ???| ? ) ?
 
1
|??|
????? ???????,?,????
?
?=1 ?
? ????? ???????,?,??
?
?=1 ???{??,??}
?   
??,?
??
??
+??
??,(?)
??
??
+???
??,?
??
??
+??
??,(?)
??
??
+???
               (1) 
?(?? = ?,?? = ?, ?? = ???| ? ) ? 
  
1
|??|
????? ???????,?,????
?
?=1 ?
? ????? ???????,?,??
?
?=1 ???{??,??}
? 
??,?
??
??
+??
??,(?)
??
??
+???
??,?
??
??
+??
??,(?)
??
??
+???
                  (2) 
Count variables ??,??? , ??,??? , ??,??? , and ??,???   are 
detailed in Table 1. Omission of a latter index 
denoted by (?)  represents the marginalized sum 
over the latter index. ? = (?, ?)  denotes the ? th 
term of document ? and the subscript ?? denotes 
the counts excluding the term at (?, ?). ?1??  are 
the parameters of the learned Max-Ent model 
corresponding to the ?  binary feature functions 
?1??  for Max-Ent. These learned Max-Ent ? 
parameters in conjunction with the observed 
feature context, ??,?  feed the supervision signal 
for topic/expression switch parameter, r which is 
updated during inference in equations (1) and (2). 
674
4 Phrase Ranking based on Relevance 
We now detail our method of pre-processing n-
grams (phrases) based on relevance to select a 
subset of highly relevant n-grams for model 
building. This has two advantages: (i). A large 
number of irrelevant n-grams slow inference. (ii). 
Filtering irrelevant terms in the vocabulary 
improves the quality of AD-expressions. Before 
proceeding, we review some existing approaches. 
Topics in most topic models like LDA are 
usually unigram distributions. This offers a great 
computational advantage compared to more 
complex models which consider word ordering 
(Wallach, 2006; Wang et al, 2007). This thread 
of research models bigrams by encoding them 
into the generative process. For each word, a 
topic is sampled first, then its status as a unigram 
or bigram is sampled, and finally the word is 
sampled from a topic-specific unigram or bigram 
distribution. This method, however, is expensive 
computationally and has a limitation for arbitrary 
length n-grams. In (Tomokiyo and Hurst, 2003), 
a language model approach is used for bigram 
phrase extraction. 
Yet another thread of research post-processes 
the discovered topical unigrams to form multi-
word phrases using likelihood scores (Blei and 
Lafferty, 2009). This approach considers adjacent 
word pairs and identifies n-grams which occur 
much more often than one would expect by 
chance alone by computing likelihood ratios. 
While this is reasonable, a significant n-gram 
with high likelihood score may not necessarily be 
relevant to the problem domain. For instance, in 
our case of discovering AD-expressions, the 
likelihood score 2  of ?1  = ?the government of? 
happens to be more than ?2  = ?I completely 
disagree?. Clearly, the former is irrelevant for the 
task of discovering AD-expressions. The reason 
for this is that likelihood scores or other 
statistical test scores rely on the relative counts in 
the multi-way contingency table to compute 
significance. Since the relative counts of different 
fragments of the irrelevant phrase ?1 , e.g. ?the 
government?, and ?government of?, happen to 
appear more than the corresponding counts in the 
contingency table of ?2, the tests assign a higher 
score. This is nothing wrong per se because the 
statistical tests only judge significance of an n-
gram, but a significant n-gram may not 
necessarily be relevant in a given problem 
domain. 
                                                          
2 Computed using N-gram statistics package, NSP; http://n-
gram.sourceforge.net 
Thus, the existing approaches have some 
major shortcomings for our task. As our goal is 
to enhance the expressiveness of our models by 
considering relevant n-grams preserving the 
advantages of exchangeable modeling, we 
employ a pre-processing technique to rank n-
grams based on relevance and consider certain 
number of top ranked n-grams based on coverage 
(details follow) in our vocabulary. The idea 
works as follows. 
We first induce a unigram JTE-P whereby we 
cluster the relevant AD-expression unigrams in 
???
?  and ??????
? . Our notion of relevance of AD-
expressions is already encoded into the model 
using priors set from Max-Ent. Next, we rank the 
candidate phrases (n-grams) using our 
probabilistic ranking function. The ranking 
function is grounded on the following 
hypothesis: a relevant phrase is one whose 
unigrams are closely related to (or appear with 
high probabilities in) the given AD-expression 
type, ? : Agreement ( ?? ) or disagreement 
(?????). Continuing from the previous example, 
given the expression type ??=?????
? , ?2 is relevant 
while ?1 is not as ?government? and ?disagree? 
are highly unlikely and likely respectively to be 
clustered in ??=?????
? . Thus, we want to rank 
phrases based on ?(??? = 1|?,?) where ? denotes 
the expression type (Agreement/Disagreement), 
?  denotes a candidate phrase. Following the 
probabilistic relevance model in (Lafferty and 
Zhai, 2003), we use a similar technique to that in 
(Zhao et al, 2011) for deriving our relevance 
ranking function as follows: 
 ?(??? = 1|?,?) =
?(???=1|?,?)
?(???=0|?,?)+?(???=1|?,?)
=
1
1+
?(???=0|?,?)
?(???=1|?,?)
=
1
1+
?(???=0,?| ?)
?(???=1,?|?)
=
 
1
1+
[?(?|???=0,?)??(???=0|?)]
[?(?|???=1,?)??(???=1|?)]
               (3) 
We further define ? = ?(???=0|?)
?(???=1|?)
. Without loss of 
generality, one can say that ?(??? = 0|?) ?
?(??? = 1|?) , because there are many more 
irrelevant phrases than relevant ones, i.e., ? ? 1. 
Thus, taking log, from equation (3), we get, 
log?(??? = 1|?,?) = log?
1
1+??
?(?|???=0,?)
?(?|???=1,?)
? ?
log ?
?(?|???=1,?)
?(?|???=0,?)
?
1
?
? = log ?
?(?|???=1,?)
?(?|???=0,?)
? ? log ?    (4) 
Thus, our ranking function actually computes the 
relevance score log ??
(?|???=1,?)
?(?|???=0,?)
? . The last term, 
log ?  being a constant is ignored because it 
cancels out while comparing candidate n-grams. 
675
We now estimate the relevance score of a phrase 
? = (?1,?2, ? ,??). Using the conditional 
independence assumption of words given the 
indicator variable ??? and expression type ?, we 
have: 
log ?
?(?|???=1,?)
?(?|???=0,?)
? = ? log
?(??|???=1,?)
?(??|???=0,?)
?
?=1              (5) 
Given the expression model ???  previously 
learned by inducing the unigram JTE-P, it is 
intuitive to set ?(??|??? = 1, ?)  to the point 
estimate of the posterior on ??,??
? =
??,??
?? +??
??,(?)
?? +???
, 
where ??,??
??  is the number of times ??  was 
assigned to AD-expression type ?  and ??,(?)
??  
denotes the marginalized sum over the latter 
index. On the other hand, ?(??|??? = 0, ?) can be 
estimated using a Laplace smoothed ( ?  = 1) 
background model, i.e., (??|??? = 0, ?) =
???+?
??+??
 , 
where ???  denotes the number of times ?? 
appears in the whole corpus and ?? denotes the 
number of terms in the entire corpus. 
Next, we throw light on the issue of choosing 
the number of top k phrases from the ranked 
candidate n-grams. Precisely, we want to analyze 
the coverage of our proposed ranking based on 
relevance models. By coverage, we mean that 
having selected top k candidate n-grams based on 
the proposed relevance ranking, we want to get 
an estimate of how many relevant terms from a 
sample of the collection were covered. To 
compute coverage, we randomly sampled 500 
documents from the corpus and listed the 
candidate n-grams3 in the collection of sampled 
500 documents. For this and subsequent human 
judgment tasks, we use two judges (graduate 
students well versed in English). We asked our 
judges to mark all relevant AD-expressions. 
Agreement study yielded ?Cohen = 0.77 showing 
substantial agreement according to scale 4 
provided in (Landis and Koch, 1977). This is 
understandable as identifying AD-expressions is 
a relatively easy task. Finally, a term was 
considered to be relevant if both judges marked it 
so. We then computed the coverage to see how 
many of the relevant terms in the random sample 
were also present in top k phrases from the 
ranked candidate n-grams. We summarize the 
                                                          
3 These are terms appearing at least 20 times in the entire 
collection. We do this for computational reasons as there 
can be many n-grams and n-grams with very low frequency 
are less likely to be relevant. 
4 No agreement (? < 0), slight agreement (0 < ? ? 0.2), fair 
agreement (0.2 < ? ? 0.4), moderate agreement (0.4 < ? ? 
0.6), substantial agreement (0.6 < ? ? 0.8), and almost 
perfect agreement 0.8 < ? ? 1.0. 
coverage results below in Table 2. 
k 3000 4000 5000 
JTE-P 
Agreement 81.34 84.24 87.01 
Disagreement 84.96 87.86 89.64 
Table 2: Coverage (in %) of AD-expressions. 
We find that choosing top k = 5000 candidate n-
grams based on our proposed ranking, we obtain 
a coverage of 87% for agreement and 89.64 for 
disagreement expression types which are 
reasonably good. Thus, we choose top 5000 
candidate n-grams for each expression type and 
add them to the vocabulary beyond all unigrams.  
 Like expression types ?1?? , we also ranked 
candidate phrases for topics ?1??  using 
?(??? = 1|?,?). However, for topics, selecting k 
based on coverage of each topic is more difficult 
because we induce 50 topics and it is also much 
more difficult to manually find relevant topical 
phrases in the sampled data as a topical phrase 
may belong to more than one topic. We selected 
top 2000 ranked candidate phrases for each topic 
using ?(??? = 1|?,?) as we feel that is sufficient 
for a topic. Note that phrases for topics are not as 
crucial as for AD-expressions because topics can 
more or less be defined by unigrams. 
5 Classifying Pair Interaction Nature 
We now determine whether two users (also 
called a user pair) mostly agree or disagree with 
each other in their exchanges, i.e., their pair 
interaction or arguing nature. This is a relatively 
new task. We first summarize the closest related 
works. In (Galley et al, 2004; Hillard et al, 
2003; Thomas et al, 2006, Bansal et al, 2008), 
conversational speeches (i.e., U.S. Congress 
meeting transcripts) are classified into for or 
against an issue using various types of features: 
durational (e.g., time taken by a speaker; speech 
rate, etc.), structural (e.g., no. of speakers per 
side, no. of votes cast by a speaker on a bill, etc.), 
and lexical (e.g., first word, last word, n-grams, 
etc.). Burfoot et al, (2011) builds on the work of 
(Thomas et al, 2006) and proposes collective 
classification using speaker contextual features 
(e.g., speaker intentions based on vote labels). 
However, above works do not discover pair 
interactions (arguing nature) in debate authors. 
Online discussion forums are textual rather than 
conversational (e.g., U.S. Congress meeting 
transcripts). Thus, the durational, structural, and 
contextual features used in prior works are not 
directly applicable.  
Instead, the model posterior on ??
?  for JTE-P 
676
can actually give an estimate of the overall 
interaction nature of a pair, i.e., the probability 
masses assigned to expression types, ? =
??(Agreement) and ? = ????? (Disagreement). 
As ???~???(??), we have ??,?=??? + ??,?=?????
? = 1. 
Hence, if the probability mass assigned to any 
one of the expression types (agreement, 
disagreement) > 0.5 then according to the model 
posterior, that expression type is dominant, i.e., if 
??,??
?  > 0.5, the pair is agreeing else disagreeing.  
However, this approach is not the best. As we 
will see in the experiment section, supervised 
classification using labeled training data with 
discovered AD-expressions as features performs 
better.  
6 Empirical Evaluation 
We now evaluate the proposed techniques in the 
context of the JTE-P model. We first evaluate the 
discovered AD-expressions by comparing results 
with and without using the phrase ranking 
method in Section 4, and then evaluate the 
classification of interaction nature of pairs. 
6.1 Dataset and Experiment Settings 
We crawled debate/discussion forum posts from 
Volconvo.com. The forum is divided into various 
domains. Each domain consists of multiple 
threads of discussions. For each post, we 
extracted the post id, author, domain, ids of all 
posts to which it replies/quotes, and the post 
content. In all, we extracted 26137, 34986, 
22354, and 16525 posts from Politics, Religion, 
Society and Science domains respectively.  
Experiment Data: As it is not interesting to 
study pairs who only exchanged a few posts, we 
restrict to pairs with at least 20 post exchanges. 
This resulted in 1241 authors and 1461 pairs. The 
reduced dataset consists of 1095586 tokens (after 
n-gram preprocessing in ?4), 40102 posts with an 
average of 27 posts or interactions per pair. Data 
from all 4 domains are combined for modeling. 
Parameter Settings: For all our experiments, we 
set the hyper-parameters to the heuristic values 
??  = 50/?, ??  = 50/?, ??  = ??  = 0.1 suggested 
in (Griffiths and Steyvers, 2004). We set the 
number of topics, ? = 50 and the number of AD-
expression types, ? = 2 (agreement and 
disagreement) as in discussion/debate forums, 
there are usually two expression types5. To learn 
                                                          
5 Values for ? > 2 were also tried. However, they did not 
produce any new dominant expression type. There was also 
a slight increase in the model perplexity showing that values 
of ? > 2 do not fit the debate forum data well. 
the Max-Ent parameters ?, we randomly sampled 
500 terms from the held-out data (10 threads in 
our corpus which were excluded from the 
evaluation of tasks in ?6.2, ?6.3) appearing at 
least 10 times and labeled them as topical (361) 
or AD-expressions (139) and used the 
corresponding features of each term (in the 
context of posts where it occurs, ?3) to train the 
Max-Ent model. 
6.2 AD-Expression Evaluation 
We first list some discovered top AD-expressions 
in Table 3 for qualitative inspection. From Table 
3, we can see that JTE-P can cluster many correct 
AD-expressions, e.g., ?I accept?, ?I agree?, 
?you?re correct?, etc. in agreement and ?I 
disagree?, ?don?t accept?, ?I refute?, etc. in 
disagreement. In addition, it also discovers and 
clusters highly specific and more ?distinctive? 
expressions beyond those used in Max-Ent 
training, e.g., ?valid point?, ?I do support?, and 
?rightly said? in agreement; and phrases like ?can 
you prove?, ?I don?t buy your?, and ?you fail to? 
in disagreement. Note that terms in black in 
Table 3 were used in Max-Ent training. The 
newly discovered terms are marked blue in 
italics. Clustering errors are in red (bold). 
For quantitative evaluation, topic models are 
often compared using perplexity. However, 
perplexity does not reflect our purpose since we 
are not trying to evaluate how well the AD-
expressions in an unseen discussion data fit our 
learned models. Instead our focus is to evaluate 
how well our learned AD-expression types 
perform in clustering semantic phrases of 
agreement/disagreement. Since AD-expressions 
(according to top terms in ??) produced by JTE-
P are rankings, we choose precision @ n (p@n) 
as our metric. p@n is commonly used to evaluate 
a ranking when the total number of correct items 
is unknown (e.g., Web search results, aspect 
terms in topic models for sentiment analysis 
(Zhao et al, 2010), etc.). This situation is similar 
to our AD-expression rankings, ?? . Further, as 
??~???, the Dirichlet smoothing effect ensures 
that every term in the vocabulary has some non-
zero mass to agreement or disagreement 
expression type. Thus, it is the ranking of terms 
in each AD-expression type that matters (i.e., 
whether the model is able to rank highly relevant 
terms at the top).  
The above method evaluates the original 
ranking. Another way of evaluating the AD-
expression rankings is to evaluate only those 
newly discovered terms, i.e., beyond those 
677
labeled terms used in Max-Ent training. For this 
evaluation, we remove those terms that have 
been used in Max-Ent (ME) training. We report 
both results in Table 4. We also studied inter-
rater agreement using two judges who 
independently labeled the top n terms as correct 
or incorrect. A term was marked correct if both 
judges deemed it so which was then used to 
compute p@n. Agreement using ??????  was 
greater than 0.78 for all p@n computations 
implying substantial and good agreements as 
identifying whether a phrase implies agreement 
or disagreement or none is an easy task. P@n 
excluding ME labeled terms (Table 4, second 
column) are slightly lower than those using all 
terms but are still decent. This is because p@n 
excluding ME labeled terms removes many 
correct AD-expressions used in training. 
Further to evaluate the sensitivity of 
performance on the amount of labeled terms for 
Max-Ent, we computed p@n across different 
sizes of labeled terms. Table 4 shows p@n for 
agreement and disagreement expressions across 
different sizes of labeled terms (L). We find that 
more labeled terms improves p@n which is 
intuitive. We used 500 labeled terms in all our 
subsequent experiments. The result in Table 4 
uses relevance ranking (?4). 
Disagreement expressions (??=????????????
?  ) 
I, disagree, I don?t, I disagree, argument, reject, claim, I reject, I refute, and, your, I refuse, won?t, the claim, 
nonsense, I contest, dispute, I think, completely disagree, don?t accept, don?t agree, incorrect, doesn?t, hogwash, I 
don?t buy your, I really doubt, your nonsense, true, can you prove, argument fails, you fail to, your assertions, 
bullshit, sheer nonsense, doesn?t make sense, you have no clue, how can you say, do you even, contradict yourself, ? 
Agreement expressions (??=?????????
? ) 
agree, I, correct, yes, true, accept, I agree, don?t, indeed correct, your, I accept, point, that, I concede, is valid, your 
claim, not really, would agree, might, agree completely, yes indeed, absolutely, you?re correct, valid point, 
argument, the argument, proves, do accept, support, agree with you, rightly said, personally, well put, I do 
support, personally agree, doesn?t necessarily, exactly, very well put, kudos, point taken, ... 
Table 3: Top terms (comma delimited) of two expression types. Red (bold) terms denote possible errors. 
Blue (italics) terms are newly discovered; rest (black) terms have been used in Max-Ent training. 
    P@n 
 
     L 
JTE-P (all terms) JTE-P (excluding labeled ME terms) 
Agreement Disagreement Agreement Disagreement 
50 100 150 50 100 150 50 100 150 50 100 150 
100 0.62 0.63 0.61 0.64 0.62 0.63 0.58 0.56 0.57 0.60 0.59 0.58 
200 0.66 0.67 0.65 0.68 0.66 0.67 0.62 0.59 0.60 0.64 0.63 0.62 
300 0.70 0.70 0.71 0.70 0.68 0.67 0.66 0.66 0.65 0.66 0.66 0.65 
400 0.72 0.72 0.73 0.74 0.71 0.70 0.68 0.67 0.69 0.70 0.68 0.69 
500 0.76 0.77 0.75 0.76 0.73 0.74 0.70 0.71 0.70 0.72 0.71 0.70 
Table 4: Results using terms based on phrase relevance ranking for P @ n= 50, 100, 150 across 100, 200, 
?, 500 labeled examples (L) used for Max-Ent (ME) training.  
    P@n 
 
     L 
JTE-P (all terms) JTE-P (excluding ME terms) 
Agreement Disagreement Agreement Disagreement 
50 100 150 50 100 150 50 100 150 50 100 150 
500 0.66 0.69 0.69 0.72 0.70 0.70 0.66 0.65 0.64 0.68 0.66 0.65 
Table 5: Results using all tokens (without applying phrase relevance ranking) for P@50, 100, 150 and 500 
labeled examples were used for Max-Ent (ME) training). 
Feature Setting 
Agreeing Disagreeing 
P R F1 P R F1 
JTE-P-posterior 0.59 0.61 0.60 0.81 0.70 0.75 
W+POS 1-4 grams 0.63 0.66 0.64 0.83 0.82 0.82 
W+POS 1-4grams + IG (top 1%) 0.64 0.67 0.65 0.84 0.82 0.83 
W+POS 1-4 grams + IG (top 2%) 0.65 0.67 0.66 0.84 0.82 0.83 
W+POS 1-4 grams + ?2 (top 1%) 0.65 0.68 0.66 0.84 0.83 0.83 
W+POS 1-4 grams + ?2(top 2%) 0.64 0.68 0.69 0.84 0.82 0.83 
AD-Expressions, ?? (top 1000) 0.73 0.74 0.73 0.87 0.87 0.87 
AD-Expressions, ?? (top 2000) 0.77 0.81 0.78 0.90 0.88 0.89 
Table 6: Precision (P), recall (R), and F1 scores of pair interaction evaluation. Improvements in F1 using 
AD-expression features (??) are statistically significant (p<0.01) using paired t-test across 5-fold CV. 
 
678
We now compare with the performance of the 
model without using phrase relevance ranking. 
P@n results using all tokens (4356787) are 
shown in Table 5 (with 500 labeled terms for 
Max-Ent training). Clearly, P@n is lower than in 
Table 4 (last row; with phrase relevance ranking) 
because without phrase relevance ranking (Table 
5) many irrelevant terms can rank high due to co-
occurrences which may not be semantically 
related. This shows that relevance ranking of 
phrases is beneficial.   
6.3 Pair Interaction Nature 
We now evaluate the overall interaction nature of 
each pair of users. The evaluation of this task 
requires human judges to read all the posts where 
the two users forming the pair have interacted.  
Thus, it is hard to evaluate all 1461 pairs in our 
dataset. Instead, we randomly sampled 500 pairs 
(? 34% of the population) for evaluation. Two 
human judges were asked to independently read 
all the post interactions of 500 pairs and label 
each pair as overall ?disagreeing? or overall 
?agreeing? or ?none?. The ??????  for this task 
was 0.81. Pairs were finally labeled as agreeing 
or disagreeing if both judges deemed them so. 
This resulted in 320 disagreeing and 152 
agreeing pairs. Out of the rest 28 pairs, 10 were 
marked ?none? by both judges while 18 pairs had 
disagreement in labels. We only focus on the 472 
agreeing and disagreeing pairs. 
As we have labeled data for 472 pairs, we can 
treat identifying pair arguing nature as a text 
classification problem where all interactions 
between a pair are merged in one document 
representing the pair along with the label given 
by judges: agreeing or disagreeing. To compare 
classification performance, we use two feature 
sets: (i) standard word + POS 1-4 grams and (ii) 
AD-expressions from ??. We use TF-IDF as our 
feature value assignment scheme. We also try 
two well-known feature selection schemes Chi-
Squared Test (?2) and Information Gain (IG). We 
use the linear kernel6 SVM (SVMlight system in 
(Joachims, 1999)) as our text classifier. For 
feature selection using ?2 and IG, we use two 
settings: top 1% and 2% of all features ranked 
according to the selection metric. Also, for 
estimated AD-expressions (according to 
probabilities in ?? ), we experiment with top 
1000 and 2000 AD-expressions terms for both 
agreement and disagreement. We summarize 
                                                          
6  Other kernels polynomial, RBF, and sigmoid did not 
perform as well. 
comparison results using 5-fold Cross Validation 
(CV) with two classes: agreeing and disagreeing 
in Table 6. JTE-P-posterior represents the 
method using simply the model posterior on ??
? 
to make the decision (see ?5). From Table 6, we 
can make the following observations.  
Predicting agreeing arguing nature is harder 
than that of disagreeing across all feature 
settings. Feature selection improves performance. 
?2 and IG perform similarly. AD-expressions, 
??yields the best performance showing that the 
discovered AD-expressions are of high quality 
and reflect the user pair arguing nature well. 
Selecting certain top terms in ??  can also be 
viewed as a form of feature selection. Although 
prediction performance using model posterior 
(JTE-P-posterior) is slightly lower than 
supervised SVM (Table 6, second row), the F1 
scores are decent. Using the discovered AD-
expressions (Table 6, last low) as features 
renders a statistically significant (see Table 6 
caption) improvement over other baseline feature 
settings. This shows that discovered AD-
expressions are useful for downstream 
applications, e.g., the task of identifying pair 
interactions. 
7 Conclusion 
This paper studied the problem of modeling user 
pair interactions in online discussions with the 
purpose of discovering the interaction or arguing 
nature of each author pair and various AD-
expressions emitted in debates. A novel 
technique was also proposed to rank n-gram 
phrases where relevance based ranking was used 
in conjunction with a semi-supervised generative 
model. This method enables us to find better AD-
expressions. Experiments using real-life online 
debate data showed the effectiveness of the 
model. In our future work, we intend to extend 
the model to account for stances, and issue 
specific interactions which would pave the way 
for user profiling and behavioral modeling. 
Acknowledgments 
We would like thank Sharon Meraz (Department 
of communication, University of Illinois at 
Chicago) and Dennis Chong (Department of 
Political Science, Northwestern University) for 
several valuable discussions. This work was 
supported in part by a grant from the National 
Science Foundation (NSF) under grant no. IIS-
1111092. 
679
References  
Abu-Jbara, A., Dasigi, P., Diab, M. and Dragomir 
Radev. 2012. Subgroup detection in ideological 
discussions. In Proceedings of the Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2012). 
Agrawal, R., Rajagopalan, S., Srikant, R., and Xu. Y. 
2003. Mining newsgroups using networks arising 
from social behavior. In Proceedings of the 
International Conference on World Wide Web 
(WWW-2003). 
Ahmed, A and Xing, E. 2010. Staying informed: 
supervised and semi-supervised multi-view topical 
analysis of ideological perspective. In Proceedings 
of the Empirical Methods in Natural Language 
Processing (EMNLP-2010). 
Anand, P., Walker, M., Abbott, R., Tree, J., Bowmani, 
R., and Minor, M. 2011. Cats rule and dogs drool!: 
Classifying stance in online debate. In Proceedings 
of the 2nd Workshop on Computational Approaches 
to Subjectivity and Sentiment Analysis. 
Bansal, M., Cardie, C., and Lee, L. 2008. The power 
of negative thinking: Exploiting label disagreement 
in the min-cut classification framework. In 
Proceedings of the International Conference on 
Computational Linguistics (Short Paper). 
Blei, D., Ng, A., and Jordan, M. 2003. Latent 
Dirichlet Allocation. Journal of Machine Learning 
Research. 
Blei, D. and Lafferty J. 2009. Visualizing topics with 
multi-word expressions. Tech. Report. 
arXiv:0907.1013v1. 
Brody, S. and Elhadad, S. 2010. An Unsupervised 
Aspect-Sentiment Model for Online Reviews. In 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL (NAACL-2010). 
Burfoot, C., Bird, S., and Baldwin, T. 2011. Collective 
Classification of Congressional Floor-Debate 
Transcripts. In Proceedings of the Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2001). 
Chang, J., Boyd-Graber, J., Wang, C.  Gerrish, S. 
Blei, D. 2009. Reading tea leaves: How humans 
interpret topic models. In Proceedings of the Neural 
Information Processing Systems (NIPS-2009). 
Chen, Z., Mukherjee, A., Liu, B., Hsu, M., 
Castellanos, M., Ghosh, R. 2013. Leveraging Multi-
Domain Prior Knowledge in Topic Models. In 
Proceedings of the International Joint Conference in 
Artificial Intelligence (IJCAI-2013). 
Choi, Y. and Cardie, C. 2010. Hierarchical sequential 
learning for extracting opinions and their attributes 
(Short Paper). In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics (ACL-2010). 
Du, L., Buntine, W. L., and Jin, H. 2010. Sequential 
Latent Dirichlet Allocation: Discover Underlying 
Topic Structures within a Document. In 
Proceedings of the IEEE International Conference 
on Data Mining (ICDM-2010). 
Erosheva, E., Fienberg, S. and Lafferty, J. 2004. 
Mixed membership models of scientific 
publications. In Proceedings of the National 
Academy of Sciences (PNAS-2004). 
Galley, M., McKeown, K., Hirschberg, J., and 
Shriberg, E. 2004. Identifying agreement and 
disagreement in conversational speech: Use of 
Bayesian networks to model pragmatic 
dependencies. In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics (ACL-2004). 
Griffiths, T. and Steyvers, M. 2004. Finding scientific 
topics. In Proceedings of the National Academy of 
Sciences (PNAS-2004). 
Hansen, G. J., and Hyunjung, K. 2011. Is the media 
biased against me? A meta-analysis of the hostile 
media effect research. Communication Research 
Reports, 28, 169-179. 
Hillard, D., Ostendorf, M., and Shriberg, E. 2003. 
Detection of agreement vs. disagreement in 
meetings: Training with unlabeled data. In 
Proceedings of the Conference of the North 
American Chapter of the Association for 
Computational Linguistics: Human Language 
Technologies (NAACL-HLT-2003). 
Hassan, A. and Radev, D. 2010. Identifying text 
polarity using random walks. In Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2010). 
Hofmann, T. 1999. Probabilistic latent semantic 
analysis. In Proceedings of the Conference on 
Uncertainty in Artificial Intelligence (UAI-1999). 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In Proceedings of the SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004). 
Jo, Y. and Oh, A. 2011. Aspect and sentiment 
unification model for online review analysis. In 
Proceedings of the International Conference on 
Web Search and Data Mining (WSDM-2011). 
Joachims, T. Making large-Scale SVM Learning 
Practical. 1999. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.), MIT-Press, 1999. 
Lafferty, J. and Zhai, C. 2003. Probabilistic relevance 
models based on document and query generation. 
Language Modeling and Information Retrieval. 
Landis, J. R. and Koch, G. G. 1977. The measurement 
of observer agreement for categorical data. 
Biometrics. 
Lin, C. and He, Y. 2009. Joint sentiment/topic model 
for sentiment analysis. In Proceedings of the 
680
International Conference on Knowledge 
Management (CIKM-2009). 
Lin, W. H., and Hauptmann, A. 2006. Are these 
documents written from different perspectives?: a 
test of different perspectives based on statistical 
distribution divergence. In Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2006). 
Liu, B. 2012. Sentiment Analysis and Opinion Mining. 
Morgan & Claypool Publisher, USA. 
McCallum, A., Wang, X., and Corrada-Emmanuel, A. 
2007. Topic and Role Discovery in Social Networks 
with Experiments on Enron and Academic Email. 
Journal of Artificial Intelligence Research. 
Mei, Q., Ling, X., Wondra, M., Su, H., and Zhai, C. 
2007. Topic sentiment mixture: modeling facets and 
opinions in weblogs. In Proceedings of the 
International Conference on World Wide Web 
(WWW-2007). 
Mimno, D. and McCallum, A. 2007. Expertise 
modeling for matching papers with reviewers. In 
Proceedings of the SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining (KDD-2007). 
Mukherjee, A., Venkataraman, V., Liu, B., Meraz, S. 
2013. Public Dialogue: Analysis of Tolerance in 
Online Discussions. In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics (ACL-2013). 
Mukherjee, A. and Liu, B. 2012a. Mining Contentions 
from Discussions and Debates. Proceedings of 
SIGKDD Conference on Knowledge Discovery and 
Data Mining (KDD-2012). 
Mukherjee, A. and Liu, B. 2012b. Aspect Extraction 
through Semi-Supervised Modeling. In Proceedings 
of the Annual Meeting of the Association for 
Computational Linguistics (ACL-2012). 
Mukherjee, A. and Liu, B. 2012c. Analysis of 
Linguistic Style Accommodation in Online Debates. 
In Proceedings of the International Conference on 
Computational Linguistics (COLING-2012). 
Mukherjee, A. and Liu, B. 2012d. Modeling Review 
Comments. In Proceedings of the Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2012). 
Murakami A. and Raymond, R. 2010. Support or 
Oppose? Classifying Positions in Online Debates 
from Reply Activities and Opinion Expressions. In 
Proceedings of the International Conference on 
Computational Linguistics (Coling-2010).  
Pang, B. and Lee, L. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval. 
Popescu, A. and Etzioni, O. 2005. Extracting product 
features and opinions from reviews. In Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2005). 
Ramage, D., Hall, D., Nallapati, R, Manning, C. 2009. 
Labeled LDA: A supervised topic model for credit 
attribution in multi-labeled corpora. In Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2009). 
Rosen-Zvi, M., Griffiths, T., Steyvers, M., and Smith, 
P. 2004. The author-topic model for authors and 
documents. In Proceedings of the Conference on 
Uncertainty in Artificial Intelligence (UAI-2004). 
Sunstein, C. R. 2002. The law of group polarization. 
Journal of political philosophy.  
Somasundaran, S. and Wiebe, J. 2009. Recognizing 
stances in online debates. In Proceedings of the 
Joint Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Conference on 
Natural Language Processing (ACL-IJCNLP-2009). 
Titov, I. and R. McDonald. 2008. Modeling online 
reviews with multi-grain topic models. In 
Proceedings of the International Conference on 
World Wide Web (WWW-2008). 
Thomas, M., Pang, B., and Lee, L. 2006. Get out the 
vote: Determining support or opposition from 
congressional floor-debate transcripts. In Proc. of 
the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2006). 
Tomokiyo, T., and Hurst, M. 2003. A language model 
approach to keyphrase extraction. In Proceedings of 
the ACL 2003 workshop on Multiword expressions: 
analysis, acquisition and treatment-Volume 18. 
Wallach, H. 2006. Topic modeling: Beyond bag of 
words. In Proceedings of the International 
Conference on Machine Learning (ICML-2006). 
Wang, X., McCallum, A., Wei, X. 2007. Topical N-
grams: Phrase and topic discovery, with an 
application to information retrieval. In Proceedings 
of the IEEE International Conference on Data 
Mining (ICDM-2007). 
Wiebe, J. 2000. Learning subjective adjectives from 
corpora. In Proc. of National Conference on AI 
(AAAI-2000). 
Yano, T., Cohen, W. and Smith, N. 2009. Predicting 
response to political blog posts with topic models. 
In Proceedings of the N. American Chapter of the 
Association for Computational Linguistics: Human 
Language Technologies (NAACL-HLT-2009). 
Zhao, X., J. Jiang, J. He, Y. Song, P. Achananuparp, 
E.P. LiM, and X. Li. 2011. Topical keyphrase 
extraction from twitter. In Proceedings of the 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2011). 
Zhao, X., Jiang, J., Yan, H., and Li, X. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP-2010). 
681
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1680?1690,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Public Dialogue: Analysis of Tolerance in Online Discussions 
Arjun Mukherjee?   Vivek Venkataraman?   Bing Liu?   Sharon Meraz? 
?Department of Computer Science  ?Department of Communication 
University of Illinois at Chicago 
arjun4787@gmail.com {vvenka6, liub, smeraz}@uic.edu 
 
 
Abstract 
Social media platforms have enabled people to 
freely express their views and discuss issues of 
interest with others. While it is important to dis-
cover the topics in discussions, it is equally use-
ful to mine the nature of such discussions or de-
bates and the behavior of the participants. There 
are many questions that can be asked. One key 
question is whether the participants give rea-
soned arguments with justifiable claims via 
constructive debates or exhibit dogmatism and 
egotistic clashes of ideologies. The central idea 
of this question is tolerance, which is a key 
concept in the field of communications. In this 
work, we perform a computational study of tol-
erance in the context of online discussions. We 
aim to identify tolerant vs. intolerant partici-
pants and investigate how disagreement affects 
tolerance in discussions in a quantitative 
framework. To the best of our knowledge, this 
is the first such study. Our experiments using 
real-life discussions demonstrate the effective-
ness of the proposed technique and also provide 
some key insights into the psycholinguistic 
phenomenon of tolerance in online discussions. 
1 Introduction 
Social media platforms have enabled people 
from anywhere in the world to express their 
views and discuss any issue of interest in online 
discussions/debates. Existing works in this con-
text include recognition of support and oppose 
camps (Agrawal et al, 2003), mining of authori-
ties and subgroups (Mayfield and Ros?, 2011; 
Abu-Jbara et al (2012), dialogue act segmenta-
tion and classification (Morbini and Sagae, 2011; 
Boyer et al, 2011), etc. 
This paper probes further to study a different 
and important angle, i.e., the psycholinguistic 
phenomenon of tolerance in online discussions. 
Tolerance is an important concept in the field of 
communications. It is a subfacet of deliberation 
which refers to critical thinking and exchange of 
rational arguments on an issue among partici-
pants that seek to achieve consensus/solution 
(Habermas, 1984). 
Perhaps the most widely accepted definition 
of tolerance is that of Gastil (2005; 2007), who 
defines tolerance as a means to engage (in writ-
ten or spoken communication) in critical think-
ing, judicious argument, sound reasoning, and 
justifiable claims through constructive discus-
sion as opposed to mere coercion/egotistic clash-
es of ideologies.  
In this work, we adopt this definition, and also 
employ the following characteristics of tolerance 
(also known as ?code of conduct?) (Crocker, 
2005; Gutmann and Thompson, 1996) to guide 
our work.  
Reciprocity: Each member (or participant) offers 
proposals and justifications in terms that others 
could understand and accept. 
Publicity: Each member engages in a process 
that is transparent to all and each member 
knows with whom he is agreeing or disagree-
ing.  
Accountability: Each member gives acceptable 
and sound reasons to others on the various 
claims or proposals suggested by him. 
Mutual respect and civic integrity: Each mem-
ber?s speech should be morally acceptable, i.e., 
using proper language irrespective of agree-
ment or disagreement of views. 
The issue of tolerance has been actively re-
searched in the field of communications for the 
past two decades, and has been investigated in 
multiple dimensions. However, existing studies 
are typically qualitative and focus on theorizing 
the socio-linguistic aspects of tolerance (more 
details in ?2).  
With the rapid growth of social media, the 
large volumes of online discussions/debates offer 
a golden opportunity to investigate people?s im-
plicit psyche in discussions quantitatively based 
on the real-life data, i.e., their tolerance levels 
and their arguing nature, which are of fundamen-
tal interest to several fields, e.g., communica-
tions, marketing, politics, and sociology 
(Dahlgren, 2005; Gastil, 2005; Moxey and 
1680
Sanford, 2000). Communication and political 
scholars are hopeful that technologies capable of 
identifying tolerance levels of people on social 
issues (often discussed in online discussions) can 
render vital statistics which can be used in pre-
dicting political outcomes in elections and help-
ful in tailoring voting campaigns and agendas to 
maximize winning chances (Dahlgren, 2002). 
Objective: The objective of this work is two-
fold:  
1. Identifying tolerant and intolerant participants 
in discussions.  
2. Analyzing how disagreement affects toler-
ance and estimating the tipping point of such 
effects.  
To the best of our knowledge, these tasks have 
not been attempted quantitatively before. The 
first task is a classification/prediction problem. 
Due to the complex and interactive nature of dis-
cussions, the traditional n-gram features are no 
longer sufficient for accurate classification. We 
thus propose a generative model, called DTM, to 
discover some key pieces of information which 
characterize the nature of discussions and their 
participants, e.g., the arguing nature (agreeing 
vs. disagreeing), topic and expression distribu-
tions. These allow us to generate a set of novel 
features from the estimated latent variables of 
DTM capable of capturing authors? tolerance 
psyche during discussions. The features are then 
used in learning to identify tolerant and intoler-
ant authors. Our experimental results show that 
the proposed approach is effective and outper-
forms several strong baselines significantly. 
The second task studies the interplay of toler-
ance and disagreement. It is well-known that 
tolerance facilitates constructive disagreements, 
but sustained disagreements often result in a 
transition to destructive disagreement leading to 
polarization and intolerance (Dahlgren, 2005). 
An interesting question is: What is the tipping 
point of disagreement to exhibit intolerance? We 
take a Bayesian approach to seek an answer and 
discover issue-specific tipping points. Our em-
pirical results discover some interesting relation-
ships which are supported by theoretical studies 
in psychology and linguistic communications. 
Finally, this work also produces an annotated 
corpus of tolerant and intolerant users in online 
discussions across two domains: politics and re-
ligion. We believe this is the first such dataset 
and will be a valuable resource to the communi-
ty. 
2 Related Work 
Although limited work has been done on analy-
sis of tolerance in online discussions, there are 
several general research areas that are related to 
our work.  
Communications: Tolerance has been an active 
research area in the field of communications for 
the past two decades. Ryfe (2005) provided a 
comprehensive survey of the literature. The topic 
has been studied in multiple dimensions, e.g., 
opinion and attitude (Luskin et al, 2004; Price et 
al., 2002), public engagement (Escobar, 2012), 
psychoanalysis (Slavin and Kriegman, 1992), 
argument repertoire (Cappella et al, 2002), etc. 
Tolerance has also been investigated in the 
domain of political communications with an em-
phasis on political sophistication (Gastil and 
Dillard, 1999), civic culture (Dahlgren, 2002), 
and democracy (Fishkin, 1991). These existing 
works study tolerance from the qualitative per-
spective. Our focus is quantitative analysis. 
Sentiment analysis: Sentiment analysis deter-
mines positive or negative opinions expressed on 
topics (Liu, 2012; Pang and Lee, 2008). Main 
tasks include aspect extraction (Hu and Liu, 
2004; Popescu and Etzioni, 2005; Mukherjee and 
Liu, 2012c; Chen et al, 2013), opinion polarity 
identification (Hassan and Radev, 2010; Choi 
and Cardie, 2010) and subjectivity analysis 
(Wiebe, 2000). Although related, tolerance is 
different from sentiment. Sentiments are mainly 
indicated by sentiment terms (e.g., great, good, 
bad, and poor). Tolerance in discussions refers 
to the reception of certain views and often indi-
cated by agreement and disagreement expres-
sions and other features (?5). 
Online discussions or debates: Several works 
put authors in debate into support and oppose 
camps. Agrawal et al (2003) used a graph based 
method, and Murakami and Raymond (2010) 
used a rule-based method. In (Mukherjee and 
Liu, 2012a), contention points were identified, in 
(Mukherjee and Liu, 2012b), various expressions 
in review comment discussions were mined, and 
in (Galley et al, 2004; Hillard et al, 2003), 
speaker utterances were classified into agree-
ment, disagreement, and backchannel classes. 
Also related are studies on linguistic style ac-
commodation (Mukherjee and Liu, 2012d) and 
user pair interactions (Mukherjee and Liu, 2013) 
in online debates. However, these works do not 
consider tolerance analysis in debate discussions, 
which is the focus of this work. 
1681
In a similar vein, several classification meth-
ods have been proposed to recognize opinion 
stances and speaker sides in online debates (So-
masundaran and Wiebe, 2009; Thomas et al, 
2006; Bansal et al, 2008; Burfoot et al, 2011; 
Yessenalina et al, 2010). Lin and Hauptmann 
(2006) also proposed a method to identify oppos-
ing perspectives. Abu-Jbara et al (2012) identi-
fied subgroups. Kim and Hovy (2007) studied 
election prediction by analyzing online discus-
sions. Other related works studying dialogue and 
discourse in discussions include authority recog-
nition (Mayfield and Ros?, 2011), dialogue act 
segmentation and classification (Morbini and 
Sagae, 2011; Boyer et al, 2011), discourse struc-
ture prediction (Wang et al, 2011). 
All these prior works are valuable. But they 
are not designed to identify tolerance or to ana-
lyze tipping points of disagreements for intoler-
ance in discussions which are the focus of this 
work. 
3 Discussion/Debate Data 
For this research, we used discussion posts from 
Volconvo.com. This forum is divided into vari-
ous domains: Politics, Religion, Science, etc. 
Each domain consists of multiple discussion 
threads. Each thread consists of a list of posts. 
Our experimental data is from two domains, Pol-
itics and Religion. The data is summarized in 
Table 1(a). In this work, the terms users, authors 
and participants are used interchangeably. The 
full data is used for modeling, but 436 and 501 
authors from Politics and Religion domains were 
manually labeled as being tolerant or intolerant 
(Table 1(c)) respectively for classification exper-
iments.   
Two judges (graduate students) were used to 
label the data. The judges are fluent in English 
and were briefed on the definition of tolerance 
(see ?1). From each domain (Politics, Religion), 
we randomly sampled authors having not more 
than 60 posts in order to reduce the labeling bur-
den as the judges need to read all posts and see 
all interactions of each author before providing a 
label. Given all posts by an author, ? and his/her 
associated interactions (posts by other authors 
replying or quoting ?), the judges were asked to 
provide a label for author ? as being tolerant or 
intolerant. In our labeling, we found that users 
strongly exhibit one dominant trait: tolerant or 
intolerant, as our data consists of topics like elec-
tions, immigration, theism, terrorism, and vege-
tarianism across politics and religion domains, 
which are often heated and thus attract people 
with pre-determined, strong, and polarized 
stances1.  
The judges worked in isolation (to prevent bi-
as) during annotation/labeling and were also 
asked to provide a short reason for their judg-
ment. The agreement statistics using Cohen?s 
kappa are given in Table 1(b), which shows sub-
stantial agreements according to the scale 2  in 
(Landis and Koch, 1977). This shows that toler-
ance as defined in ?1 is quite decisive and one 
can decide whether a debater is exhibiting toler-
ant vs. intolerant quite well. To account for disa-
greements in labels, the judges discussed their 
reasons to reach a consensus. The final labeled 
data is reported in Table 1(c). 
4 Model 
We now present our generative model to capture 
the key aspects of discussions/debates and their 
intricate relationships, which enable us to (1) 
design sophisticated features for classification 
and (2) perform an in-depth analysis of the inter-
play of disagreement and tolerance. The model is 
called Debate Topic Model (DTM).  
DTM is a semi-supervised generative model 
motivated by the joint occurrence of various top-
ics; and agreement and disagreement expressions 
(abbreviated AD-expressions hereon) in debate 
posts. A typical debate post mentions a few top-
ics (using similar topical terms) and expresses 
some viewpoints with one or more AD-
expression types (Agreement and Disagreement) 
using semantically related expressions. This ob-
servation forms the basis of the generative pro-
cess of our model where documents (posts) are 
represented as admixtures of latent topics and 
AD-expression types (Agreement and Disagree-
ment). This key observation and the motivation 
of modeling debates are from our previous work 
in (Mukherjee and Liu, 2012a). In the new set-
                                                          
1  These hardened perspectives are theoretically supported 
by the polarization effect (Sunstein, 2002), and the hostile 
media effect, a scenario where partisans rigidly hold on to 
their stances (Hansen and Hyunjung, 2011). 
2  Agreement levels are as follows. ? ? [0, 0.2]: Poor, 
? ? (0.2, 0.4]:Fair, ? ? (0.4, 0.6]: Moderate, ? ? (0.6, 0.8]: 
Substantial, and ? ? (0.8, 1.0]: Almost perfect agreement. 
Domain Posts Authors  Cohen?s ?   Tol.  Intol. Total 
Politics 48605 1027  0.74    213  223  436 
 Religion 66835 1370  0.77    207  294  501 
             (a) Full Data     (b) Agreement   (c) Labeled data 
Table 1: Data statistics (Tol: Tolerant users; Intol: 
Intolerant users. Total = Tol. + Intol). 
 
1682
ting, we model topics and debate expression dis-
tributions specific to authors as this work is con-
cerned with modeling authors? (in)tolerance na-
ture. Making latent variable ?? and ?? author 
specific facilitates modeling user behaviors 
(?5.3). 
Assume we have ?1??  topics and ?1??  expres-
sion types in our corpus. In our case of debate 
posts, based upon reading various posts, we hy-
pothesize that ? = 2 as in debates as we mostly 
find 2 dominant expression types: Agreement 
and Disagreement. Meanings of variables used in 
the following discussion are detailed in Table 2. 
In this work, a document/post is viewed as a bag 
of n-grams and we use terms to denote both 
words (unigrams) and phrases (n-grams)3. DTM 
is a switching graphical model performing a 
switch between topics and AD-expressions simi-
lar to that in (Zhao et al, 2010). The switch is 
done using a learned maximum entropy (Max-
Ent) model. The rationale here is that topical and 
AD-expression terms usually play different syn-
tactic roles in a sentence. Topical terms (e.g., 
?U.S. elections,? ?government,? ?income tax?) 
tend to be noun and noun phrases while expres-
sion terms (?I refute,? ?how can you say,? ?I?d 
agree?) usually contain pronouns, verbs, wh-
determiners, and modals. In order to utilize the 
part-of-speech (POS) tag information, we place 
the topic/AD-expression distribution, ??,?,?  (the 
prior over the indicator variable ??,?,?) in the term 
plate (Figure 1)  and set it using a Max-Ent mod-
el conditioned on the observed context ??,?,?  as-
sociated with ??,?,?  and the learned Max-Ent 
parameters ? (details in ?4.1). In this work, we 
use both lexical and POS features of the previ-
ous, current and next POS tags/lexemes of the 
term ??,?,?  as the contextual information, 
i.e., ??,?,? = [?????,?,??1 , ?????,?,? , ?????,?,?+1 ,
??,?,??1,??,?,? , ??,?,?+1], which is used to produce 
feature functions for Max-Ent. For phrasal terms 
(n-grams), all POS tags and lexemes of ??,?  are 
considered as contextual information for compu-
ting feature functions in Max-Ent. DTM has the 
following generative process: 
A. For each AD-expression type ?, draw ???~???(??) 
B. For each topic t, draw ???~???(??) 
C. For each author ? ? {1 ??}: 
i. Draw ???~???(??) 
ii. Draw ???~???(??) 
iii. For each document/post ? ? {1 ???}: 
I. For each term ??,?,?, ? ? {1 ???,?}: 
a. Set ??,?,? ? ??????(??,?,?; ?) 
b. Draw ??,?,?~?????????(??,?,?) 
c. if (??,?,? =  ???) // ??,?is an AD-expression term 
Draw ??,?,?~ ????(??
?) 
else // ??,?,? =  ???, ??,?,?is a topical term 
Draw ??,?,?~ ????(??
?) 
d. Emit ??,?,?~????(???,?,?
??,?,?) 
4.1 Inference 
We employ posterior inference using Monte Car-
                                                          
3 Topics in most topic models (e.g., LDA (Blei et al, 2003)) 
are unigram distributions and a document is treated as an 
exchangeable bag-of-words. This offers a computational 
advantage over models considering word orders (Wallach, 
2006). As our goal is to enhance the expressiveness of 
DTM (rather than ?modeling? word order), we use 1-4 
grams preserving the advantages of exchangeable modeling. 
 
 
 
 
 
 
 
 
 
Figure 1: Plate notation of DTM 
Variable/Function Description 
?; ?; ? 
An author ?; set of all authors; docu-
ment, ? 
(?,?); ?? 
Post ? by author ?; Set of all posts by 
? 
?;?;? 
# of topics; expression types; vocabu-
lary 
??,?,?; ??,? 
??? term in (?,?); Total # of terms in 
(?,?) 
??,?,?  Distribution over topics and AD-
expressions 
??,?,? 
Associated feature context of observed 
??,?,? 
? Learned Max-Ent parameters 
??,?,? ? {???, ???} 
Binary indicator/switch variable ( topic 
(???) or AD-expression (???) ) for ??,?,? 
??
?; 
??
?(??,??
?  , 
??,?????
? ) 
??s distribution over topics ; expression 
types (Agreement: ??,??
? , Disagree-
ment: ??,?????
? ) 
??,?
? ;??,?,?
?  
Topic distribution of post ? by author 
?; Probability mass of topic ? in ??,?
? . 
??,?,??{??,?????}
?  
??,?
? ; 
Expression type distribution of post ? 
by author ?; Corresponding probability 
masses of Agreement: ??,?,?=??
?  and 
Disagreement in ??,?,?=?????
? . 
??,?,? Topic/Expression type of ??,?,? 
??
?;  ??
?  
Topic ??s ; Expression type ??s distri-
bution over vocabulary terms 
??; ??; ??; ?? Dirichlet priors of ??
?;  ??
? ;??
?;  ??
? 
??,?
??; ??,?
??  
# of times topic ?; expression type ? 
assigned to ? 
??,?
??; ??,?
??  
# of times term ? appears in topic ?; 
expression type ? 
Table 2: List of notations 
 
x 
? 
 
z 
 
r 
 
 
w Na, d 
? 
Da 
?E ?E  
A 
?T ?T  
?T  T 
?E  E 
?E ?T 
1683
lo Gibbs sampling. Denoting the random varia-
bles {?, ?, ?}  by singular 
scripts{??, ??, ??} ,?1?? , where ? = ? ? ??,??? , a 
single iteration consists of performing the fol-
lowing sampling: 
?(?? = ?, ?? = ???|???,???,???,?? = ?) ?
exp (? ????(??,?,?,???)
?
?=1 )
? exp (? ????(??,?,?,?)
?
?=1 )??{??,??}
?
??,?
??
??
+??
??,(?)
??
??
+???
?
??,?
??
??
+??
??,(?)
??
??
+???
  (1) 
?(?? = ?, ?? = ???|???,???,??? ,?? = ?) ?
exp (? ????(??,?,?,???)
?
?=1 )
? exp (? ????(??,?,?,?)
?
?=1 )??{??,??}
?
??,?
??
??
+??
??,(?)
??
??
+???
?
??,?
??
??
+??
??,(?)
??
??
+???
  (2) 
where ? = (?,?, ?) denotes the ???  term of docu-
ment ? by author ? and the subscript ?? denotes 
assignments excluding the term at (?,?, ?). Omis-
sion of the latter index denoted by (?) represents 
the marginalized sum over the latter index. 
Count variables are detailed in Table 1 (last two 
rows). ?1??  are the parameters of the learned 
Max-Ent model corresponding to the ?  binary 
feature functions ?1?? for Max-Ent. The learned 
Max-Ent ?  parameters in conjunction with the 
observed context, ??,?,? feed the supervision sig-
nal for updating the topic/expression switch pa-
rameter, ? in equations (1) and (2).  
The hyper-parameters for the model were set 
to the values ??= ??= 0.1 and ??  = 50/?, ??  = 
50/ ? , suggested in (Griffiths and Steyvers, 
2004). Model parameters were estimated after 
5000 Gibbs iterations with a burn-in of 1000 it-
erations. The Max-Ent parameters ?  were 
learned using 500 labeled terms in each domain 
(politics:- topical: 376 and AD-expression: 124; 
religion:- topical: 349 and AD-expression: 151) 
appearing at least 10 times in debate threads oth-
er than the data in Table 1 (we do so since the 
data in Table 1(c) is later used in the classifica-
tion experiments in ?6.1). 
Table 3 lists some top AD-expressions discov-
ered by DTM. We see that DTM can cluster 
many correct AD-expressions, e.g., ?I disagree?, 
?I refute?, ?don?t accept?, etc. in disagreement; 
and ?I agree?, ?you?re correct?, ?agree with 
you?, etc. in agreement. Further, it also discovers 
highly specific and more distinctive expressions 
beyond those used in Max-Ent training (marked 
blue in italics), e.g., ?I don?t buy your?, ?can you 
prove,? ?you fail to?, and ?you have no clue? in 
disagreement; and phrases like ?valid point?, 
?rightly said?, ?I do support?, and ?very well 
put? in agreement. In ?6.1, we will see that these 
AD-expressions serve as high quality features 
for predicting tolerance. 
Lastly, we note that DTM also estimates sev-
eral pieces of useful information (e.g., AD-
expressions, posterior estimates of author?s argu-
ing nature, ??? ; latent topics and expressions, 
??
?;  ??
? , etc.). These will be used to produce a 
rich set of user behavioral features for character-
izing tolerance in ?5.3. 
5 Feature Engineering 
We now propose features which will be used for 
model building to classify tolerant and intolerant 
authors in Table 1(c). We use three sets of fea-
tures. 
5.1 Language based Features of Tolerance 
Word and POS n-grams: As tolerance in com-
munication is directly reflected in language us-
age, word n-grams are obvious features. We also 
use POS tags (obtained using Stanford Tagger4) 
as features. The rationale of using POS tag based 
features is that intolerant communications are 
often characterized by hate/egotistic speech 
which have pronounced use of specific part of 
speech (e.g., pronouns) (Zingo, 1998). 
Heuristic Factor Analysis: In psycholinguistics, 
factor analysis refers to the process of finding 
groups of semantically similar linguistic con-
structs (words/phrases). It is also called meaning 
extraction in (Chung and Pennebaker, 2007). As 
tolerance in discussions is characterized by rea-
soned expressions which often accompany 
sourcing (e.g., providing a hyperlink, making an 
attempt to clarify with some evidence, etc.), we 
compiled a list of reasoned and sourced expres-
sions (shown in Table 4) from prior works 
                                                          
4 http://nlp.stanford.edu/software/tagger.shtml 
Disagreement expressions (??=????????????
?  ) 
I, disagree, I don?t, I disagree, argument, reject, claim, I reject, 
I refute, and, your, I refuse, won?t, the claim, nonsense, I con-
test, dispute, I think, completely disagree, don?t accept, don?t 
agree, incorrect, doesn?t, hogwash, I don?t buy your, I really 
doubt, your nonsense, true, can you prove, argument fails, you 
fail to, your assertions, bullshit, sheer nonsense, doesn?t make 
sense, you have no clue, how can you say, do you even, contra-
dict yourself, ? 
Agreement expressions (??=?????????
? ) 
agree, I, correct, yes, true, accept, I agree, don?t, indeed correct, 
your, point, that, I concede, is valid, your claim, not really, 
would agree, might, agree completely, yes indeed, absolutely, 
you?re correct, valid point, argument, the argument, proves, do 
accept, support, agree with you, rightly said, personally, well 
put, I do support, personally agree, doesn?t necessarily, exactly, 
very well put, absolutely correct, kudos, point taken,... 
Table 3: Top terms (comma delimited) of two expres-
sion types. Red (bold) terms denote possible errors. 
Blue (italics) terms are newly discovered; rest (black) 
terms have been used in Max-Ent training. 
 
1684
(Chung and Pennebaker, 2007; Flor and Hadar, 
2005; Moxey and Sanford, 2000; Pennebaker, et 
al.,  2007).  
5.2 Debate Expression Features 
AD-expressions: As we have seen in ?4, DTM 
can discover specific agreement and disagree-
ment expressions in debates. We use these ex-
pressions as another feature set. Estimated AD-
expressions (Table 3) serve as a principled way 
of performing factor analysis in debates instead 
of heuristic factor analysis as in Table 4 used in 
prior works.  
As the AD-expression types are modeled as 
Dirichlet distributions (??~???(??)), due to the 
smoothing effect, each term in the vocabulary 
has some non-zero probability mass associated 
with the expression types. To ensure that the dis-
covered expressions are representative AD-
expressions, we only consider the terms in ?? 
with ?(?|?) = ??,?
? > 0.001  as probability 
masses lower than 0.001 are more due to the 
smoothing effect of Dirichlet distribution than 
true correlation. 
5.3 User Behavioral Features 
Here we propose several features of user interac-
tion which reflect the socio-psychological state 
of tolerance while participating in discussions. 
We note that these features rely on the posterior 
estimates of latent variables ??, ?, and ? in DTM 
(?4) and are thus difficult to obtain without 
modeling. 
Overall Arguing Nature: The posterior on ??
? 
(Table 2) for each author, ? gives an estimate of 
??s overall arguing nature (agreeing or disagree-
ing). We use the probability mass assigned to 
each arguing nature type as a user behavioral 
feature. This gives us two features ?1, ?2 as fol-
lows: 
?1(?) =  ??,??
?  ;   ?2(?) =  ??,?????
?      (3) 
Behavioral Response: As intolerant users are 
likely to attract more disagreement, it is naturally 
useful to estimate the response (agreeing vs. dis-
agreeing) a user receives from other users. For 
computing behavioral response, we first use the 
posterior on ? to compute the distribution of AD-
expressions (i.e., the relative probability masses 
of agreeing and disagreeing expressions) in a 
document ? by an author ? as follows: 
??,?,??
? =
??????,?,?=??,1?????,???
??????,?,?=???,1?????,???
; 
??,?,?????
? =
??????,?,?=?????,1?????,???
??????,?,?=???,1?????,???
     (4) 
Now to get the overall behavioral response of an 
author, ?  we take the expected value of the 
agreeing and disagreeing responses that ?  re-
ceived from other authors ??  who replied to or 
quoted ? ?s posts. The expectations below are 
taken over all posts ??  by ??  which reply/quote 
posts of ?. 
?3(?) =  ?[??? ??,??
? ]; ?4(?) =  ????? ??,?????
? ?  (5) 
Equality of Speech: In communication literature 
(Dahlgren, 2005; Habermas, 1984), equality is 
theorized as an essential element of tolerance. 
Each participant must be able to participate on an 
equal footing with others without anybody domi-
nating the discussion. In online debates, we can 
measure this phenomenon using the following 
feature: 
?5(?) = ? ??
# ?? ????? ?? ? ?? ?????? ?
# ?? ????? ?? ?????? ?
? ?[??,?,?????
? ]?  (6) 
where the inner expectation is taken over all 
posts of ? in thread ? and the outer expectation is 
taken over all threads ?  in which ?  participated. 
The above definition computes the aggressive 
posting behavior of author ? whereby he tires to 
dominate the thread by posting more than others. 
The aggressive posting behavior is weighted by 
author?s disagreeing nature because a person 
usually exhibits a dominating nature when he 
pushes hard to establish his ideology (which is 
often in disagreement with others) (Moxey and 
Sanford, 2000).  
Topic Shifts: An interesting phenomenon of hu-
man (social) psyche is that when people are una-
ble to logically argue their stances and feel they 
are losing the debate, they often try to belit-
tle/deride others by pulling unrelated topics into 
discussion (Slavin and Kriegman, 1992). This is 
Factor: Reasoning words/phrases 
because, because of, since, reason, reason being, reason is, 
reason why, due to, owing to, as in, therefore, thus, hence-
forth, hence, implies, implies that, implying, hints, hinting, 
hints towards, it follows that, it turns out, conclude, conse-
quence, consequently, the cause, rationale, the rationale, justi-
fication, the justification, provided, premise, assumption, on 
the proviso, in spite, ? 
Factor: Sourcing words/phrases 
presence of hyperlinks/urls, source, reference, for example, 
for instance, namely, to explain, to detail, to clarify, to eluci-
date, to illustrate, to be precise, furthermore, moreover, apart 
from, besides, we find, ? 
 
Table 4: Heuristic Factor Analysis (HFA). 
Words/Phrases in each factor compiled from prior 
works in psycholinguistics. 
1685
referred to as topic shifts. Topic shifts thus have a 
relation with tolerance in deliberation. Stromer-
Galley (2005) reported that if the discussion is 
off topic, then tolerance or deliberation cannot 
meet its objective of deep consideration of an 
issue. Hence, the average topic shifts of an au-
thor, ? across various posts in a thread can serve 
as a good feature for measuring tolerance. We 
use the posterior on per-document topic distribu-
tion, ??,?,?? =
??????,?,?=?,1?????,???
??????,?,?=???,1?????,???
 to measure topic 
shifts using KL-Divergence as follows: 
?6 = ? ?avg?,??? ?????? ? ???????,?
? ||??,??
? ???     (7) 
We first compute author, ??s average topic shifts 
in a thread, ? which measures his topic shifts in ?. 
But this only gives us his behavior in one thread. 
To capture his overall behavior, we take the ex-
pected value of this behavior over all threads in 
which ?  participated. We take average KL-
divergence (KL-Div.) over all pairs of posts by ? 
in a given thread to account for the asymmetry of 
KL-Div. 
Finally, we note that by no means do we claim 
that the mere presence and a large value of any of 
the above features imply that a user is intolerant 
or tolerant. They are indicators of the phenome-
non of tolerance in discussions/debates. The ac-
tual prediction is done using the learned models 
in ?6.1. 
6 Experimental Evaluation 
We now detail the experiments that investigate 
the strengths of features in ?5. In particular, we 
first consider the task of classifying whether an 
author is tolerant or intolerant in discussions. 
Then, we analyze how disagreement affects tol-
erance. 
6.1 Tolerant and Intolerant Classification 
Here, we show that the features in ?5 can help 
build accurate models for predicting tolerance. 
We employ a linear kernel 5  SVM (using the 
SVMLight system (Joachims, 1999)) and report 5-
fold cross validation (CV) results on the task of 
predicting the socio-psychological nature of us-
ers? communication: tolerant vs. intolerant in 
politics and religion domains (Table 1(c)). Note 
that for each fold of 5-fold CV, DTM was run on 
the full data of each domain (Table 1(a)) exclud-
ing the users (and their associated posts) in the 
test set of that fold for generating the features of 
the training instances (users). The learned DTM 
                                                          
5 Other kernels (rbf, poly, sigmoid) did not perform as well. 
was then fitted (using the approach in (Hofmann, 
1999)) to the test set users and their posts for 
generating the features of the test instances.  
To investigate the effectiveness of the pro-
posed framework, we incrementally add feature 
sets starting with the baseline features.  Word 
unigrams and bigrams (inclusive of unigrams)6  
serve as our first baseline (B1a, B1b). Word + 
POS bigrams is our second baseline (B2). 
?Word? in B2 uses bigrams as B1b gives better 
results. B2 + Heuristic Factor Analysis (HFA) 
(Table 4) serve as our third baseline (B3). Table 
5 shows the experiment results. We note the fol-
lowing: 
1. Across both domains, adding POS bigrams 
slightly improves classification accuracy and 
F1-score beyond standard word unigrams and 
bigrams. Feature selection using information 
gain (IG) does not help much. 
2. Using heuristic factor analyses (HFA) of rea-
soned and sourced expressions (Table 4) 
brings about 1% and 2% improvement in ac-
curacy in politics and religion domains re-
spectively. 
3. Debate expression features (DE) in ?5.2 and 
user behavioral features (UB) in ?5.3 pro-
duced from DTM progressively improve clas-
sification accuracies by 4% and 8% in politics 
domains and 5% and 6% in religion domains. 
The improvements are also statistically signif-
icant.  
In summary, we can see that modeling made a 
major impact. It improved the accuracy by about 
10% than traditional unigram and bigram base-
lines. This shows that the debate expressions and 
user behaviors computed using the DTM model 
can capture various dimensions of (in)tolerance 
not captured by n-grams. 
6.2 How Disagreement affects Tolerance? 
We now quantitatively study the effect of disa-
greement on tolerance. We recall from ?1 that 
tolerance indicates constructive discussion and 
allows disagreement. Some level of disagree-
ment is often times an integral component of 
deliberation and tolerance (Cappella et al, 
2002). 
Disagreements, however, can be either con-
structive or destructive. The distinction is that 
the former is aimed at arriving at a consensus or 
solution, while the latter leads to polarization 
and intolerance (Sunstein, 2002). It was also 
shown in (Dahlgren, 2005) that sustained disa-
                                                          
6 Higher order n-grams did not result in better results. 
1686
greement often takes a transition towards de-
structive disagreement and is likely to lead to 
intolerance. Similar phenomena was also identi-
fied in psychology literature (Critchley, 1964). 
In such cases, the participants often stubbornly 
stick to an extreme attitude, which eventually 
results in intolerance and defeats the very pur-
pose of deliberative discussion.  
An intriguing research question is: What is the 
relationship between disagreement and intoler-
ance? The question is interesting from both the 
communication and psycholinguistic perspec-
tives. The best of our knowledge, this is the first 
attempt towards seeking an answer. We work in 
the context of five issues/threads in real-life 
online debates. To derive quantitative and defi-
nite conclusions, it is required to perform the 
following tasks: 
? For each issue, empirically investigate in ex-
pectation the tipping point of disagreement 
beyond which a user tends to be intolerant. 
? Further, investigate the confidence on the es-
timated tipping point (i.e., what is the likeli-
hood that the estimated tipping point is statis-
tically significant instead of chance alone). 
We formalize the above tasks in the Bayesian 
setting. Recall from Table 2 of ?4, that ??,???  (re-
spectively, ??,?????
? ) are the estimates of agreeing 
and disagreeing nature of an author and ??,???  + 
??,?????
?  = 1. Let ??(?) denote the event that in 
expectation a threshold value of 0 < ? < 1 
serves as a tipping point of disagreement beyond 
which intolerance is exhibited. Note that we em-
phasize the term ?in expectation? (taken over all 
authors). We do not mean that every author 
whose disagreement, ??,?????
? >  ? , is intolerant. 
The empirical likelihood of ??(?)  can be ex-
pressed by the following probability expression: 
????(?)? = 
??????,?????
? > ?|? = ?? ? ????,?????
? > ?|? = ??? (8) 
The events ? = ? and ? = ? denote that author 
? is intolerant and tolerant respectively. The ex-
pectation is taken over authors. Showing that ? 
indeed serves as the tipping point of disagree-
ment to exhibit intolerance corresponds is to re-
jecting the null hypothesis that the probabilities 
in (8) are equal. We employ a Fisher?s exact test 
to test significance and report confidence 
measures (using p-values) for the tipping point 
thresholds. The results are shown in Table 6. 
The threshold ? is computed using the entropy 
method in (Fayyad and Irani, 1993) as follows: 
We first fit our previously learned model (using 
the data in Table 1 (a)) to the new threads in Ta-
ble 6 and its users and posts to obtain the esti-
mates of ??,??????  and other latent variables for 
feature generation. The learned classifier in ?6.1 
is used to predict the nature of users (tolerant vs. 
Feature Setting 
Politics Religion 
Precision Recall F1 Accuracy Precision Recall F1 Accuracy 
B1a: Word unigrams 64.1 86.3 73.7 70.1 61.9 86.8 72.6 71.9 
Word unigram + IG 64.5 86.2 73.9 70.2 62.7 86.9 72.9 71.9 
B1b: Word bigrams 66.8 87.8 75.9 72.4 64.9 89.1 75.9 75.1 
B2: W+POS bigrams 68.5 86.8 76.4 73.7 66.6 88.4 76.8 76.7 
B3: B2 + HFA(Table 4) 69.2 90.5 78.1 75.2 66.4 90.6 76.8 77.5 
B3 + DE (?5.2) 74.7 91.3 82.4? 79.5? 70.2 92.8 80.8? 82.1? 
B3 + DE + UB (?5.3) 76.1 92.2 83.1? 83.2? 71.7 93.4 82.1? 83.3? 
Table 5: Precision, Recall, F1 score on the tolerant class, and Accuracy for different feature settings across 2 
domains. DE: Debate expression features (AD-expressions, Table3, ?5.2). UB: User behavioral features 
(?5.3). Improvements in F1 and Accuracy using DTM features (beyond baselines, B1-B3) are statistically 
significant (?: p<0.02; ?: p<0.01) using paired t-test with 5-fold CV. 
Thread/Issue # Posts # Users % InTol. ????,?,?????
? ? ? p-value 
Repeal Healthcare 1823 33 39.9 0.57 0.65 0.02 
Europe?s Collapse 1824 33 42.5 0.61 0.61 0.01 
Obama Euphoria 1244 26 30.7 0.66 0.71 0.01 
Socialism 831 49 44.8 0.69 0.48 0.03 
Abortion 1232 58 48.4 0.78 0.37 0.01 
Table 6: Tipping points of disagreements for intolerance (?) of different issues. ????,?,?????
? ?: the expected 
disagreement over all posts in each issue/thread, # Posts: the total number of posts, # Users: the total number 
of users/authors, % Intol: % of intolerant users in each thread, ?: the estimated tipping point, and p-value: 
computed from two-tailed Fisher?s exact test. 
1687
intolerant) in the new threads7. Then, for each 
user we have his predicted deliberative (social) 
psyche (Tolerant vs. Intolerant) and also his 
overall disagreeing nature exhibited in that 
thread (the posterior on ??,?????
? ? [0, 1]). For a 
thread, tolerant and intolerant users (data points) 
span the range [0, 1] attaining different values 
for ??,?????
? . Each candidate tipping point of disa-
greement, 0 ? ?? ? 1 results in a binary partition 
of the range with each partition containing some 
proportion of tolerant and intolerant users. We 
compute the entropy of the partition for every 
candidate tipping point in the range [0, 1]. The 
final tipping point threshold, ?  is chosen such 
that it minimizes the partition entropy based on 
the binary cut-point method in (Fayyad and 
Irani, 1993).  
Since we perform a thread level analysis, the 
results in Table 6 are thread/issue specific. We 
note the following from Table 6: 
1. Across all threads/issues, we find that the ex-
pected disagreement over all posts, ?, 
????,?,?????
? ? > 0.5 showing that in discussions 
of the reported issues, disagreement predomi-
nates. 
2. ????,?,?????
? ? also gives an estimate of overall 
heat in the issue being discussed. We find 
sensitive issues like abortion and socialism 
being more heated than healthcare, Obama, 
etc. 
3. The percentage of intolerant users increases 
with the expected overall disagreement in the 
issue except for the issue Obama euphoria. 
4. The estimated tipping point of disagreement 
to exhibit intolerance, ?  happens to vary in-
versely with the expected disagreement, 
????,?,?????
? ? except the issue Obama euphoria. 
This reflects that as overall disagreement in 
the issue increases, the tipping point of intol-
erance decreases, i.e., due to high discussion 
heat, people are likely to turn intolerant even 
with relatively small amount of disagreement. 
This finding dovetails with prior studies in 
psychology (Rokeach and Fruchter, 1956) that 
heated discussions are likely to reduce thresh-
                                                          
7 Although this prediction may not be perfect, it can be 
regarded as considerably reliable to study the trend of toler-
ance across different issues as our classifier (in ?6.1) attains 
a high (83%) classification accuracy using the full feature 
set. As judging all users across all threads would require 
reading about 7000 posts, for confirmation, we randomly 
sampled 30 authors across various threads for labeling by 
our judges. 28 out of 30 predictions produced by the classi-
fier correlated with the judges' labels, which should be suf-
ficiently accurate for our analysis. 
olds of reception leading to dogmatism, ego-
tism, and intolerance. Table 6 shows that for 
moderately heated issues (healthcare, Eu-
rope?s collapse), in expectation, author?s dis-
agreement ??,?????
?  should exceed 61-65% to 
exhibit intolerance. However, for sensitive is-
sues, we find that the tipping point is much 
lower, abortion: 37%; socialism: 48%. 
5. The issue Obama Euphoria is an exception to 
other issues? trends. Even though in expecta-
tion, it has ????,?,?????
? ?  = 66% overall disa-
greement, the percentage of intolerant users 
remains the lowest (30%) and the tipping 
point attains a highest value (? = 0.71), show-
ing more tolerance on the issue. A plausible 
reason could be that Obama is somewhat more 
liked and hence attracts less intolerance from 
users8. 
6. The p-values of the estimated tipping points, ? 
across all issues are statistically significant at 
98-99% confidence levels. 
7 Conclusion 
This work performed a deep analysis of the soci-
opsychological and psycholinguistic phenome-
non of tolerance in online discussions, which is 
an important concept in the field of communica-
tions. A novel framework is proposed, which is 
capable of characterizing and classifying toler-
ance in online discussions. Further, a novel tech-
nique was also proposed to quantitatively evalu-
ate the interplay of tolerance and disagreement. 
Our empirical results using real-life online dis-
cussions render key insights into the psycholin-
guistic process of tolerance and dovetail with 
existing theories in psychology and communica-
tions. To the best of our knowledge, this is the 
first such quantitative study. In our future work, 
we want to further this research and study the 
role of diversity of opinions in the context of 
tolerance and its relation to polarization. 
Acknowledgments 
This work was supported in part by a grant from 
National Science Foundation (NSF) under grant 
no. IIS-1111092. 
 
 
                                                          
8 This observation may be linked to the political phenome-
non of ?democratic citizenship through exposure to diverse 
perspectives? (Mutz, 2006) where it was shown that expo-
sure to heterogeneous opinions (i.e., greater disagreement), 
often enhances tolerance.  
1688
References 
Abu-Jbara, A., Dasigi, P., Diab, M. and Dragomir 
Radev. 2012. Subgroup detection in ideological 
discussions. ACL. 
Agrawal, R. Rajagopalan, S. Srikant, R. Xu. Y. 2003. 
Mining newsgroups using networks arising from 
social behavior. WWW.  
Bansal, M., Cardie, C., and Lee, L. 2008. The power 
of negative thinking: Exploiting label disagreement 
in the min-cut classification framework. In COL-
ING. 
Blei, D., A. Ng,  and M. Jordan. 2003. Latent Di-
richlet Allocation. In JMLR. 
Boyer, K.; Grafsgaard, J.; Ha, E. Y.; Phillips, R.; and 
Lester, J. 2011. An affect-enriched dialogue act 
classification model for task-oriented dialogue. In 
ACL. 
Burfoot, C.,  S. Bird,  and T. Baldwin. 2011. Collec-
tive Classification of Congressional Floor-Debate 
Transcripts. In ACL. 
Cappella, J. N., Price, V., and Nir, L. 2002. Argument 
repertoire as a reliable and valid measure of 
opinion quality: electronic dialogue during 
campaign 2000. Political Communication. Political 
Communication. 
Chen, Z., Mukherjee, A., Liu, B., Hsu, M., 
Castellanos, M., Ghosh, R. 2013. Leveraging 
Multi-Domain Prior Knowledge in Topic Models. 
In IJCAI. 
Chung, C. K., and Pennebaker, J. W. 2007. Revealing 
people?s thinking in natural language: Using an 
automated meaning extraction method in open?
ended self?descriptions,. J. of Research in 
Personality. 
Choi, Y. and Cardie, C. 2010. Hierarchical sequential 
learning for extracting opinions and their attributes. 
In ACL. 
Critchley, M. 1964. The neurology of psychotic 
speech. The British Journal of Psychiatry. 
Crocker, D. A. 2005. Tolerance and Deliberative 
Democracy. UMD Technical Report. 
Dahlgren, P. 2002. In search of the talkative public: 
Media, deliberative democracy and civic culture. 
Javnost/The Public. 
Dahlgren, Peter. 2005. The Internet, Public Spheres, 
and Political Communication: Dispersion and 
Deliberation. Political Communication. 
Escobar, O. 2012. Public Dialogue and Deliberation: 
A communication perspective for 
publicengagement practitioners. Handbook and 
Technical Report. 
Fayyad, U., and Irani, K. 1993. Multi-interval 
discretization of continuous-valued attributes for 
classification learning. In UAI. 
Fishkin, J. 1991. Democracy and deliberation. New 
Haven, CT: Yale University Press. 
Flor, M., and Hadar, U. 2005. The production of 
metaphoric expressions in spontaneous speech: A 
controlled-setting experiment. Metaphor and 
Symbol.  
Galley, M.,  K.  McKeown, J. Hirschberg,  E. 
Shriberg. 2004. Identifying agreement and 
disagreement in conversational speech: Use of 
Bayesian networks to model pragmatic 
dependencies. In ACL. 
Gastil, J. 2005. Communication as Deliberation: A 
Non-Deliberative Polemic on Communication 
Theory. Univ. of  Washington, Technical Report. 
Gastil, J., and Dillard, J. P. 1999. Increasing political 
sophistication through public deliberation. Political 
Communication. 
Gastil, John. 2007. Political communication and 
deliberation. Sage Publications. 
Griffiths, T. and Steyvers, M. 2004. Finding scientific 
topics. In PNAS. 
Gutmann, A., and Thompson, D. F. 1996. Democracy 
and disagreement. Harvard University Press. 
Habermas. 1984. The theory of communicative 
action: Reason and rationalization of society. (T. 
McCarthy, Trans. Vol. 1). Boston, MA: Beacon 
Press. 
Hillard, D., Ostendorf, M., and Shriberg, E. 2003. 
Detection of Agreement vs. Disagreement in 
Meetings: Training with Unlabeled Data. HLT-
NAACL. 
Hansen, G. J., and Hyunjung, K. 2011. Is the media 
biased against me? A meta-analysis of the hostile 
media effect research. Communication Research 
Reports, 28, 169-179. 
Hassan, A. and Radev, D. 2010. Identifying text 
polarity using random walks.In ACL. 
Hofmann, T. 1999. Probabilistic latent semantic 
analysis. In UAI.  
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In SIGKDD.  
Joachims, T. Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and 
A. Smola (ed.), MIT-Press, 1999. 
Kim, S. and Hovy, E. 2007. Crystal: Analyzing 
predictive opinions on the web. In EMNLP-CoNLL.  
Landis, J. R. and Koch, G. G. 1977. The 
measurement of observer agreement for categorical 
data. Biometrics, 159?174.  
Lin, W. H., and Hauptmann, A. 2006. Are these 
documents written from different perspectives?: a 
test of different perspectives based on statistical 
distribution divergence. In ACL. 
Liu, B. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publisher, USA. 
Luskin, R. C., Fishkin, J. S., and Iyengar, S. 2004. 
Considered Opinions on U.S. Foreign Policy: Face-
to-Face versus Online Deliberative Polling. 
International Communication Association, New 
Orleans, LA.  
Mayfield, E. and Rose, C. P. 2011. Recognizing 
Authority in Dialogue with an Integer Linear 
Programming Constrained Model. In ACL.  
Moxey, L. M., and Sanford, A. J. 2000. 
Communicating quantities: A review of 
psycholinguistic evidence of how expressions 
determine perspectives. Applied Cognitive 
Psychology.  
Morbini, F. and Sagae, K. 2011. Joint Identification 
and Segmentation of Domain-Specific Dialogue 
Acts for Conversational Dialogue Systems. In ACL. 
1689
Murakami,  A.,  and Raymond, R. 2010. Support or 
Oppose? Classifying Positions in Online Debates 
from Reply Activities and Opinion Expressions. In 
COLING. 
Mukherjee, A. and Liu, B. 2013. Discovering User 
Interactions in Ideological Discussions. In ACL. 
Mukherjee, A. and Liu, B. 2012a. Mining 
Contentions from Discussions and Debates. In 
KDD. 
Mukherjee, A. and Liu, B. 2012b. Modeling review 
Comments. In ACL. 
Mukherjee, A. and Liu, B. 2012c. Aspect Extraction 
through Semi-Supervised Modeling. In ACL. 
Mukherjee, A. and Liu, B. 2012d. Analysis of 
Linguistic Style Accommodation in Online 
Debates. In COLING. 
Mutz, D. 2006. Hearing the Other Side: Deliberative 
Versus Participatory Democracy. Cambridge: 
Cambridge University Press, 2006.  
Pang, B. and Lee, L. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval.  
Pennebaker, J. W., Chung, C. K., Ireland, M., 
Gonzales, A., and Booth, R. J. 2007. The 
development and psychometric properties of 
LIWC2007. LIWC.Net.  
Popescu, A. and Etzioni, O. 2005. Extracting product 
features and opinions from reviews.  In EMNLP.  
Price, V., Cappella, J. N., and Nir, L. 2002. Does 
disagreement contribute to more deliberative 
opinion? Political Communication.  
Rokeach, M., and Fruchter, B. 1956. A factorial study 
of dogmatism and related concepts. The Journal of 
Abnormal and Social Psychology. 
Ryfe, D. M. (2005). Does deliberative democracy 
work? Annual review of political science.  
Slavin, M. O., and Kriegman, D. 1992. The adaptive 
design of the human psyche: Psychoanalysis, 
evolutionary biology, and the therapeutic process. 
Guilford Press.  
Somasundaran, S., J. Wiebe. 2009. Recognizing 
stances in online debates. In ACL-IJCNLP. 
Stromer-Galley, J. 2005. Conceptualizing and 
Measuring Coherence in Online Chat. Annual 
Meeting of the International Communication 
Association.  
Sunstein, C. R. 2002. The law of group polarization. 
Journal of political philosophy.  
Thomas, M.,  B.  Pang and  L.  Lee. 2006. Get out the 
vote: Determining support or opposition from 
Congressional floor-debate transcripts. In EMNLP. 
Wang, L., Lui, M., Kim, S. N., Nivre, J., and 
Baldwin, T. 2011. Predicting thread discourse 
structure over technical web forums. In EMNLP. 
Wiebe, J. 2000. Learning subjective adjectives from 
corpora. In Proc. of National Conference on AI. 
Yessenalina, A., Yue, A., Cardie, C. 2010. Multi-
level structured models for document-level 
sentiment classification. In EMNLP. 
Zhao, X.,  J.  Jiang, H. Yan,  and X.  Li. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In EMNLP. 
Zingo, M. T. (1998). Sex/gender Outsiders, Hate 
Speech, and Freedom of Expression: Can They Say 
that about Me? Praeger Publishers.  
1690
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 24?29,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploiting Topic based Twitter Sentiment for Stock Prediction 
Jianfeng Si* Arjun Mukherjee? Bing Liu? Qing Li* Huayi Li? Xiaotie Deng? 
*Department of Computer Science, City University of Hong Kong, Hong Kong, China 
*{ thankjeff@gmail.com, qing.li@cityu.edu.hk} 
?Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA 
?{ arjun4787@gmail.com, liub@cs.uic.edu, lhymvp@gmail.com} 
?AIMS Lab, Department of Computer Science, Shanghai Jiaotong University, Shanghai, China 
?deng-xt@cs.sjtu.edu.cn  
 
Abstract 
This paper proposes a technique to leverage 
topic based sentiments from Twitter to help 
predict the stock market. We first utilize a con-
tinuous Dirichlet Process Mixture model to 
learn the daily topic set. Then, for each topic 
we derive its sentiment according to its opin-
ion words distribution to build a sentiment 
time series. We then regress the stock index 
and the Twitter sentiment time series to predict 
the market. Experiments on real-life S&P100 
Index show that our approach is effective and 
performs better than existing state-of-the-art 
non-topic based methods. 
1 Introduction 
Social media websites such as Twitter, Facebook, 
etc., have become ubiquitous platforms for social 
networking and content sharing. Every day, they 
generate a huge number of messages, which give 
researchers an unprecedented opportunity to uti-
lize the messages and the public opinions con-
tained in them for a wide range of applications 
(Liu, 2012). In this paper, we use them for the 
application of stock index time series analysis. 
Here are some example tweets upon querying 
the keyword ?$aapl? (which is the stock symbol 
for Apple Inc.) in Twitter: 
1. ?Shanghai Oriental Morning Post confirm-
ing w Sources that $AAPL TV will debut 
in May, Prices range from $1600-$3200, 
but $32,000 for a 50"wow.? 
2. ?$AAPL permanently lost its bid for a ban 
on U.S. sales of the Samsung Galaxy Nex-
us http://dthin.gs/XqcY74.? 
3. ?$AAPL is loosing customers. everybody is 
buying android phones! $GOOG.? 
As shown, the retrieved tweets may talk about 
Apple?s products, Apple?s competition relation-
ship with other companies, etc. These messages 
are often related to people?s sentiments about 
Apple Inc., which can affect or reflect its stock 
trading since positive sentiments can impact 
sales and financial gains. Naturally, this hints 
that topic based sentiment is a useful factor to 
consider for stock prediction as they reflect peo-
ple?s sentiment on different topics in a certain 
time frame. 
This paper focuses on daily one-day-ahead 
prediction of stock index based on the temporal 
characteristics of topics in Twitter in the recent 
past. Specifically, we propose a non-parametric 
topic-based sentiment time series approach to 
analyzing the streaming Twitter data. The key 
motivation here is that Twitter?s streaming mes-
sages reflect fresh sentiments of people which 
are likely to be correlated with stocks in a short 
time frame. We also analyze the effect of training 
window size which best fits the temporal dynam-
ics of stocks. Here window size refers to the 
number of days of tweets used in model building. 
Our final prediction model is built using vec-
tor autoregression (VAR). To our knowledge, 
this is the first attempt to use non-parametric 
continuous topic based Twitter sentiments for 
stock prediction in an autoregressive framework. 
2 Related Work 
2.1 Market Prediction and Social Media 
Stock market prediction has attracted a great deal 
of attention in the past. Some recent researches 
suggest that news and social media such as blogs, 
micro-blogs, etc., can be analyzed to extract pub-
lic sentiments to help predict the market (La-
vrenko et al, 2000; Schumaker and Chen, 2009). 
Bollen et al (2011) used tweet based public 
mood to predict the movement of Dow Jones 
*   The work was done when the first author was visiting 
University of Illinois at Chicago. 
 
 
 
 
 
 
24
Industrial Average index. Ruiz et al (2012) stud-
ied the relationship between Twitter activities 
and stock market under a graph based view. 
Feldman et al (2011) introduced a hybrid ap-
proach for stock sentiment analysis based on 
companies? news articles.  
2.2 Aspect and Sentiment Models 
Topic modeling as a task of corpus exploration 
has attracted significant attention in recent years. 
One of the basic and most widely used models is 
Latent Dirichlet Allocation (LDA) (Blei et al, 
2003). LDA can learn a predefined number of 
topics and has been widely applied in its extend-
ed forms in sentiment analysis and many other 
tasks (Mei et al, 2007; Branavan et al, 2008; Lin 
and He, 2009; Zhao et al, 2010; Wang et al, 
2010; Brody and Elhadad, 2010; Jo and Oh, 2011; 
Moghaddam and Ester, 2011; Sauper et al, 2011; 
Mukherjee and Liu, 2012; He et al, 2012).  
The Dirichlet Processes Mixture (DPM) model 
is a non-parametric extension of LDA (Teh et al, 
2006), which can estimate the number of topics 
inherent in the data itself. In this work, we em-
ploy topic based sentiment analysis using DPM 
on Twitter posts (or tweets). First, we employ a 
DPM to estimate the number of topics in the 
streaming snapshot of tweets in each day.  
Next, we build a sentiment time series based 
on the estimated topics of daily tweets. Lastly, 
we regress the stock index and the sentiment 
time series in an autoregressive framework. 
3 Model 
We now present our stock prediction framework. 
3.1 Continuous DPM Model 
Comparing to edited articles, it is much harder to 
preset the number of topics to best fit continuous 
streaming Twitter data due to the large topic di-
versity in tweets. Thus, we resort to a non-
parametric approach: the Dirichlet Process Mix-
ture (DPM) model, and let the model estimate the 
number of topics inherent in the data itself. 
Mixture model is widely used in clustering and 
can be formalized as follows: 
   ?      (       )
 
              (1) 
where    is a data point,    is its cluster label, K 
is the number of topics,  (       ) is the sta-
tistical (topic) models: *  +   
  and     is the 
component weight satisfying      and  
?      . 
In our setting of DPM, the number of mixture 
components (topics) K is unfixed apriori but es-
timated from tweets in each day. DPM is defined 
as in (Neal, 2010): 
               (  )  
              
         (   )                 (2) 
where    is the parameter of the model that      
belongs to, and   is defined as a Dirichlet Pro-
cess with the base measure H and the concentra-
tion parameter   (Neal, 2010). 
We note that neighboring days may share the 
same or closely related topics because some top-
ics may last for a long period of time covering 
multiple days, while other topics may just last for 
a short period of time. Given a set of time-
stamped tweets, the overall generative process 
should be dynamic as the topics evolve over time. 
There are several ways to model this dynamic 
nature (Sun et al, 2010; Kim and Oh, 2011; 
Chua and Asur, 2012; Blei and Lafferty, 2006; 
Wang et al, 2008). In this paper, we follow the 
approach of Sun et al (2010) due to its generality 
and extensibility. 
Figure 1 shows the graphical model of our con-
tinuous version of DPM (which we call cDPM). 
As shown, the tweets set is divided into daily 
based collections: *         +  *    +   
     are the 
observed tweets and *    +   
     are the model pa-
rameters (latent topics) that generate these tweets. 
For each subset of tweets,    (tweets of day  ), 
we build a DPM on it. For the first day (   ), 
the model functions the same as a standard DPM, 
i.e., all the topics use the same base measure, 
      ( ). However, for later days (   ), 
besides the base measure,      ( ), we make 
use of topics learned from previous days as pri-
ors. This ensures smooth topic chains or links 
(details in ?3.2). For efficiency, we only consider 
topics of one previous day as priors. 
We use collapsed Gibbs sampling (Bishop, 
2006) for model inference. Hyper-parameters are 
set to:              ;       as in 
(Sun et al, 2010; Teh et al, 2006) which have 
been shown to work well. Because a tweet has at 
most 140 characters, we assume that each tweet 
contains only one topic. Hence, we only need to 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM. 
? 
 
   
 
   
        
 
   
 
     
 
     
 
   
 
    
    
    
   
 
 
     
 
     
 
 
     
 
           
25
sample the topic assignment    for each tweet   . 
According to different situations with respect 
to a topic?s prior, for each tweet    in   , the 
conditional distribution for    given all other 
tweets? topic assignments, denoted by    , can be 
summarized as follows: 
1.    is a new topic: Its candidate priors contain 
the symmetric base prior    ( )  and topics 
*      +   
     learned from            .  
? If    takes a symmetric base prior: 
 (     
           )  
 
     
 (    )
 (       )
?  (      )
   
   
?  ( )
   
   
           (3) 
where the first part denotes the prior proba-
bility according to the Dirichlet Process and 
the second part is the data likelihood (this 
interpretation can similarly be applied to the 
following three equations).  
? If    takes one topic k from *      +   
     as 
its prior: 
 (      
            )   
 
       
     
 (    )
 (       )
?  (         ( )     )
   
   
?  (         ( ))
   
   
 (4) 
2. k is an existing topic: We already know its 
prior. 
? If k takes a symmetric base prior: 
  (               )  
 
  
  
     
 (        ( )
  )
 (           ( )
  )
?  (           
  )
   
   
?  (      
  )
   
   
 (5) 
? If k takes topic        as its prior:  
  (               )  
  
  
     
 .        ( )
  /
 .           ( )
  /
?  (         ( )          
  )
   
   
?  (         ( )     
  )
   
   
 (6) 
Notations in the above equations are listed as 
follows: 
?      is the number of topics learned in day t-1. 
? |V| is the vocabulary size. 
?    is the document length of   . 
?      is the term frequency of word   in   . 
?       ( ) is the probability of word   in pre-
vious day?s topic k.  
?   
   is the number of tweets assigned to topic k 
excluding the current one   .  
?     
   is the term frequency of word   in topic k, 
with statistic from    excluded. While    ( )
   
denotes the marginalized sum of all words in 
topic k with statistic from    excluded. 
Similarly, the posteriors on *    ( )+  (topic 
word distributions) are given according to their 
prior situations as follows: 
? If topic k takes the base prior: 
           ( )   (      ) (         ( )) ?     (7) 
where      is the frequency of word   in topic 
k and    ( )  is the marginalized sum over all 
words. 
? otherwise, it is defined recursively as: 
    ( )  (          ( )      ) (         ( ))?  (8) 
where       serves as the topic prior for     . 
Finally, for each day we estimate the topic 
weights,    as follows:  
        ?      ?                              (9) 
where    is the number of tweets in topic k. 
3.2 Topic-based Sentiment Time Series 
Based on an opinion lexicon   (a list of positive 
and negative opinion words, e.g., good and bad), 
each opinion word,     is assigned with a po-
larity label  ( ) as ?+1? if it is positive and ?-1? 
if negative. We spilt each tweet?s text into opin-
ion part and non-opinion part. Only non-opinion 
words in tweets are used for Gibbs sampling. 
Based on DPM, we learn a set of topics from 
the non-opinion words space  . The correspond-
ing tweets? opinion words share the same topic 
assignments as its tweet. Then, we compute the 
posterior on opinion word probability,     
 ( ) 
for topic   analogously to equations (7) and (8). 
Finally, we define the topic based sentiment 
score  (   ) of topic   in day t as a weighted 
linear combination of the opinion polarity labels: 
 (   )   ?     
 ( )
   
    ( );  (   )   ,    -    (10) 
According to the generative process of cDPM, 
topics between neighboring days are linked if a 
topic k takes another topic as its prior. We regard 
this as evolution of topic k. Although there may 
be slight semantic variation, the assumption is 
reasonable. Then, the sentiment scores for each 
topic series form the sentiment time series {?, 
S(t-1, k), S(t, k), S(t+1, k), ...}. 
Figure 2 demonstrates the linking process 
where a triangle denotes a new topic (with base 
symmetric prior), a circle denotes a middle topic 
(taking a topic from the previous day as its prior, 
 
 
 
 
           0      ?       t-1          t         t+1    ?    N 
Figure 2: Linking the continuous topics via 
neighboring priors. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM  
 
 
? 
... 
 
 
? 
 
 
.
.
. 
 
 
? 
 
 
? ? 
26
while also supplying prior for the next day) and 
an ellipse denotes an end topic (no further topics 
use it as a prior). In this example, two continuous 
topic chains or links (via linked priors) exist for 
the time interval [t-1, t+1]: one in light grey color, 
and the other in black. As shown, there may be 
more than one topic chain/link (5-20 in our ex-
periments) for a certain time interval1.Thus, we 
sort multiple sentiment series according to their 
accumulative weights of topics over each link: 
?     
  
    
. In our experiments, we try the top 
five series and use the one that gives the best re-
sult, which is mostly the first (top ranked) series 
with a few exceptions of the second series. The 
topics mostly focus on hot keywords like: news, 
stocknews, earning, report, which stimulate ac-
tive discussions on the social media platform. 
3.3 Time Series Analysis with VAR 
For model building, we use vector autoregression 
(VAR). The first order (time steps of historical 
information to use: lag = 1) VAR model for two 
time series *  + and *  + is given by:  
                                                   
                                               (11) 
where * + are the white noises and * + are model 
parameters. We use the ?dse? library2 in the R 
language to fit our VAR model based on least 
square regression. 
 Instead of training in one period and predicting 
over another disjointed period, we use a moving 
training and prediction process under sliding 
windows3 (i.e., train in [t, t + w] and predict in-
dex on t + w + 1) with two main considerations: 
? Due to the dynamic and random nature of both 
the stock market and public sentiments, we are 
more interested in their short term relationship. 
? Based on the sliding windows, we have more 
training and testing points.  
Figure 3 details the algorithm for stock index 
prediction. The accuracy is computed based on 
the index up and down dynamics, the function 
     (    ) returns True only if   (our predic-
tion) and   (actual value) share the same index 
up or down direction. 
 
 
                                                 
1 The actual topic priors for topic links are governed by the 
four cases of the Gibbs Sampler. 
2 http://cran.r-project.org/web/packages/dse 
3  This is similar to the autoregressive moving average 
(ARMA) models. 
4 Dataset 
We collected the tweets via Twitter?s REST API 
for streaming data, using symbols of the Stand-
ard & Poor's 100 stocks (S&P100) as keywords. 
In this study, we focus only on predicting the 
S&P100 index. The time period of our dataset is 
between Nov. 2, 2012 and Feb. 7, 2013, which 
gave us 624782 tweets. We obtained the S&P100 
index?s daily close values from Yahoo Finance. 
5 Experiment 
5.1 Selecting a Sentiment Metric 
Bollen et al (2011) used the mood dimension, 
Calm together with the index value itself to pre-
dict the Dow Jones Industrial Average. However, 
their Calm lexicon is not publicly available. We 
thus are unable to perform a direct comparison 
with their system. We identified and labeled a 
Calm lexicon (words like ?anxious?, ?shocked?, 
?settled? and ?dormant?) using the opinion lexi-
con4 of Hu and Liu (2004) and computed the sen-
timent score using the method of Bollen et al 
(2011) (sentiment ratio). Our pilot experiments 
showed that using the full opinion lexicon of Hu 
and Liu (2004) actually performs consistently 
better than the Calm lexicon. Hence, we use the 
entire opinion lexicon in Hu and Liu (2004). 
5.2 S&P100INDEX Movement Prediction 
We evaluate the performance of our method by 
comparing with two baselines. The first (Index) 
uses only the index itself, which reduces the 
VAR model to the univariate autoregressive 
model (AR), resulting in only one index time 
series {  } in the algorithm of Figure 3.  
                                                 
4 http://cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar 
Parameter:  
w: training window size; lag: the order of VAR;  
Input:   : the date of time series; {  }: sentiment time 
series; {  }: index time series; 
Output: prediction accuracy. 
1. for t = 0, 1, 2, ?, N-w-1 
2. { 
3.        = VAR( ,     -  ,     -, lag); 
4.             
 =      .Predict(x[t+w+1-lag, t+w],  
  y[t+w+1-lag, t+w]); 
5.       if (     (      
        ) )   
 rightNum++;  
6.     } 
7.    Accuracy = rightNum / (N-w); 
8.    Return Accuracy; 
Figure 3: Prediction algorithm and accuracy 
 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM  
27
 When considering Twitter sentiments, existing 
works (Bollen et al, 2011, Ruiz et al, 2012) 
simply compute the sentiment score as ratio of 
pos/neg opinion words per day. This generates a 
lexicon-based sentiment time series, which is 
then combined with the index value series to give 
us the second baseline Raw.  
 In summary, Index uses index only with the 
AR model while Raw uses index and opinion 
lexicon based time series. Our cDPM uses index 
and the proposed topic based sentiment time se-
ries. Both Raw and cDPM employ the two di-
mensional VAR model. We experiment with dif-
ferent lag settings from 1-3 days. 
 We also experiment with different training 
window sizes, ranging from 15 - 30 days, and 
compute the prediction accuracy for each win-
dow size. Table 1 shows the respective average 
and best accuracies over all window sizes for 
each lag and Table 2 summarizes the pairwise 
performance improvements of averaged scores 
over all training window sizes. Figure 4 show the 
detailed accuracy comparison for lag 1 and lag 3.  
    From Table 1, 2, and Figure 4, we note: 
i. Topic-based public sentiments from tweets 
can improve stock prediction over simple sen-
timent ratio which may suffer from backchan-
nel noise and lack of focus on prevailing top-
ics. For example, on lag 2, Raw performs 
worse by 8.6% than Index itself. 
ii. cDPM outperforms all others in terms of both 
the best accuracy (lag 3) and the average ac-
curacies for different window sizes. The max-
imum average improvement reaches 25.0% 
compared to Index at lag 1 and 15.1% com-
pared to Raw at lag 3. This is due to the fact 
that cDPM learns the topic based sentiments 
instead of just using the opinion words? ratio 
like Raw, and in a short time period, some 
topics are more correlated with the stock mar-
ket than others. Our proposed sentiment time 
series using cDPM can capture this phenome-
non and also help reduce backchannel noise 
of raw sentiments.  
iii. On average, cDPM gets the best performance 
for training window sizes within [21, 22], and 
the best prediction accuracy is 68.0% on win-
dow size 22 at lag 3. 
6 Conclusions 
Predicting the stock market is an important but 
difficult problem. This paper showed that Twit-
ter?s topic based sentiment can improve the pre-
diction accuracy beyond existing non-topic based 
approaches. Specifically, a non-parametric topic-
based sentiment time series approach was pro-
posed for the Twitter stream. For prediction, vec-
tor autoregression was used to regress S&P100 
index with the learned sentiment time series. Be-
sides the short term dynamics based prediction, 
we believe that the proposed method can be ex-
tended for long range dependency analysis of 
Twitter sentiments and stocks, which can render 
deep insights into the complex phenomenon of 
stock market. This will be part of our future work. 
Acknowledgments 
This work was supported in part by a grant from 
the National Science Foundation (NSF) under 
grant no. IIS-1111092 and a strategic research 
grant from City University of Hong Kong (pro-
ject number: 7002770). 
Lag Index Raw cDPM 
1 0.48(0.54) 0.57(0.59) 0.60(0.64) 
2 0.58(0.65) 0.53(0.62) 0.60(0.63) 
3 0.52(0.56) 0.53(0.60) 0.61(0.68) 
Table 1: Average (best) accuracies over all 
training window sizes and different lags 1, 2, 3. 
Lag Raw vs. Index cDPM vs. Index cDPM vs. Raw 
1 18.8% 25.0% 5.3% 
2 -8.6% 3.4% 13.2% 
3 1.9% 17.3% 15.1% 
Table 2: Pairwise improvements among Index, 
Raw and cDPM averaged over all training win-
dow sizes. 
 
Figure 4: Comparison of prediction accuracy of 
up/down stock index on S&P 100 index for dif-
ferent training window sizes. 
0.25
0.45
0.65
15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
A
cc
u
ra
cy
 
Training window size 
Comparison on Lag 1 
Index Raw cDPM
0.25
0.35
0.45
0.55
0.65
0.75
18 19 20 21 22 23 24 25 26 27 28 29 30
A
cc
u
ra
cy
 
Training Window size 
Comparison on Lag 3 
Index Raw cDPM
28
References 
Bishop, C. M. 2006. Pattern Recognition and Machine 
Learning. Springer. 
Blei, D., Ng, A. and Jordan, M. 2003. Latent Dirichlet 
allocation. Journal of Machine Learning Research 
3:993?1022. 
Blei, D. and Lafferty, J. 2006. Dynamic topic models. 
In Proceedings of the 23rd International Confer-
ence on Machine Learning (ICML-2006). 
Bollen, J., Mao, H. N., and Zeng, X. J. 2011. Twitter 
mood predicts the stock market. Journal of Com-
puter Science 2(1):1-8.  
Branavan, S., Chen, H., Eisenstein J. and Barzilay, R. 
2008. Learning document-level semantic properties 
from free-text annotations. In Proceedings of the 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2008). 
Brody, S. and Elhadad, S. 2010. An unsupervised 
aspect-sentiment model for online reviews. In Pro-
ceedings of the 2010 Annual Conference of the 
North American Chapter of the ACL (NAACL-
2010). 
Chua, F. C. T. and Asur, S. 2012. Automatic Summa-
rization of Events from Social Media, Technical 
Report, HP Labs. 
Feldman, R., Benjamin, R., Roy, B. H. and Moshe, F. 
2011. The Stock Sonar - Sentiment analysis of 
stocks based on a hybrid approach. In Proceedings 
of 23rd IAAI Conference on Artificial Intelligence 
(IAAI-2011). 
He, Y., Lin, C., Gao, W., and Wong, K. F. 2012. 
Tracking sentiment and topic dynamics from social 
media. In Proceedings of the 6th International 
AAAI Conference on Weblogs and Social Media 
(ICWSM-2012). 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004). 
Jo, Y. and Oh, A. 2011. Aspect and sentiment unifica-
tion model for online review analysis. In Proceed-
ings of ACM Conference in Web Search and Data 
Mining (WSDM-2011). 
Kim, D. and Oh, A. 2011. Topic chains for under-
standing a news corpus. CICLing (2): 163-176. 
Lavrenko, V., Schmill, M., Lawrie, D., Ogilvie, P., 
Jensen, D. and Allan, J. 2000. Mining of concur-
rent text and time series. In Proceedings of the 6th 
KDD Workshop on Text Mining, 37?44. 
Lin, C. and He, Y. 2009. Joint sentiment/topic model 
for sentiment analysis. In Proceedings of ACM In-
ternational Conference on Information and 
Knowledge Management (CIKM-2009). 
Liu, B. 2012. Sentiment analysis and opinion mining. 
Morgan & Claypool Publishers.  
Mei, Q., Ling, X., Wondra, M., Su, H. and Zhai, C. 
2007. Topic sentiment mixture: modeling facets 
and opinions in weblogs. In Proceedings of Interna-
tional Conference on World Wide Web (WWW-
2007). 
Moghaddam, S. and Ester, M. 2011. ILDA: Interde-
pendent LDA model for learning latent aspects and 
their ratings from online product reviews.  In Pro-
ceedings of the Annual ACM SIGIR International 
conference on Research and Development in In-
formation Retrieval (SIGIR-2011). 
Mukherjee A. and Liu, B. 2012. Aspect extraction 
through semi-supervised modeling. In Proceedings 
of the 50th Annual Meeting of the Association for 
Computational Linguistics (ACL-2012).  
Neal, R.M. 2000. Markov chain sampling methods for 
dirichlet process mixture models. Journal of Com-
putational and Graphical Statistics, 9(2):249-265. 
Ruiz, E. J., Hristidis, V., Castillo, C., Gionis, A. and 
Jaimes, A. 2012. Correlating financial time series 
with micro-blogging activity. In Proceedings of the 
fifth ACM international conference on Web search 
and data mining (WSDM-2012), 513-522.  
Sauper, C., Haghighi, A. and Barzilay, R. 2011. Con-
tent models with attitude. Proceedings of the 49th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). 
Schumaker, R. P. and Chen, H. 2009. Textual analysis 
of stock market prediction using breaking financial 
news. ACM Transactions on Information Systems 
27(February (2)):1?19. 
Sun, Y. Z., Tang, J. Han, J., Gupta M. and Zhao, B. 
2010. Community Evolution Detection in Dynamic 
Heterogeneous Information Networks. In Proceed-
ings of KDD Workshop on Mining and Learning 
with Graphs (MLG'2010), Washington, D.C. 
Teh, Y., Jordan M., Beal, M. and Blei, D. 2006. Hier-
archical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101[476]:1566-1581. 
Wang, C. Blei, D. and Heckerman, D. 2008. Continu-
ous Time Dynamic Topic Models. Uncertainty in 
Artificial Intelligence (UAI 2008), 579-586 
Wang, H., Lu, Y.  and Zhai, C. 2010. Latent aspect 
rating analysis on review text data: a rating regres-
sion approach. Proceedings of ACM SIGKDD In-
ternational Conference on Knowledge Discovery 
and Data Mining (KDD-2010). 
Zhao, W. Jiang, J. Yan, Y. and Li, X. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP-2010). 
29
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 347?358,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Aspect Extraction with Automated Prior Knowledge Learning
Zhiyuan Chen Arjun Mukherjee Bing Liu
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607, USA
{czyuanacm,arjun4787}@gmail.com,liub@cs.uic.edu
Abstract
Aspect extraction is an important task in
sentiment analysis. Topic modeling is a
popular method for the task. However,
unsupervised topic models often generate
incoherent aspects. To address the is-
sue, several knowledge-based models have
been proposed to incorporate prior knowl-
edge provided by the user to guide mod-
eling. In this paper, we take a major
step forward and show that in the big data
era, without any user input, it is possi-
ble to learn prior knowledge automatically
from a large amount of review data avail-
able on the Web. Such knowledge can
then be used by a topic model to discover
more coherent aspects. There are two key
challenges: (1) learning quality knowl-
edge from reviews of diverse domains,
and (2) making the model fault-tolerant
to handle possibly wrong knowledge. A
novel approach is proposed to solve these
problems. Experimental results using re-
views from 36 domains show that the pro-
posed approach achieves significant im-
provements over state-of-the-art baselines.
1 Introduction
Aspect extraction aims to extract target entities
and their aspects (or attributes) that people have
expressed opinions upon (Hu and Liu, 2004, Liu,
2012). For example, in ?The voice is not clear,?
the aspect term is ?voice.? Aspect extraction has
two subtasks: aspect term extraction and aspect
term resolution. Aspect term resolution groups ex-
tracted synonymous aspect terms together. For ex-
ample, ?voice? and ?sound? should be grouped to-
gether as they refer to the same aspect of phones.
Recently, topic models have been extensively
applied to aspect extraction because they can per-
form both subtasks at the same time while other
existing methods all need two separate steps (see
Section 2). Traditional topic models such as
LDA (Blei et al, 2003) and pLSA (Hofmann,
1999) are unsupervised methods for extracting la-
tent topics in text documents. Topics are aspects
in our task. Each aspect (or topic) is a distribution
over (aspect) terms. However, researchers have
shown that fully unsupervised models often pro-
duce incoherent topics because the objective func-
tions of topic models do not always correlate well
with human judgments (Chang et al, 2009).
To tackle the problem, several semi-supervised
topic models, also called knowledge-based topic
models, have been proposed. DF-LDA (Andrze-
jewski et al, 2009) can incorporate two forms
of prior knowledge from the user: must-links
and cannot-links. A must-link implies that two
terms (or words) should belong to the same topic
whereas a cannot-link indicates that two terms
should not be in the same topic. In a similar but
more generic vein, must-sets and cannot-sets are
used in MC-LDA (Chen et al, 2013b). Other re-
lated works include (Andrzejewski et al, 2011,
Chen et al, 2013a, Chen et al, 2013c, Mukher-
jee and Liu, 2012, Hu et al, 2011, Jagarlamudi et
al., 2012, Lu et al, 2011, Petterson et al, 2010).
They all allow prior knowledge to be specified by
the user to guide the modeling process.
In this paper, we take a major step further. We
mine the prior knowledge directly from a large
amount of relevant data without any user inter-
vention, and thus make this approach fully au-
tomatic. We hypothesize that it is possible to
learn quality prior knowledge from the big data
(of reviews) available on the Web. The intuition
is that although every domain is different, there
is a decent amount of aspect overlapping across
domains. For example, every product domain
has the aspect/topic of ?price,? most electronic
products share the aspect ?battery? and some also
share ?screen.? Thus, the shared aspect knowl-
347
edge mined from a set of domains can poten-
tially help improve aspect extraction in each of
these domains, as well as in new domains. Our
proposed method aims to achieve this objective.
There are two major challenges: (1) learning qual-
ity knowledge from a large number of domains,
and (2) making the extraction model fault-tolerant,
i.e., capable of handling possibly incorrect learned
knowledge. We briefly introduce the proposed
method below, which consists of two steps.
Learning quality knowledge: Clearly, learned
knowledge from only a single domain can be er-
roneous. However, if the learned knowledge is
shared by multiple domains, the knowledge is
more likely to be of high quality. We thus propose
to first use LDA to learn topics/aspects from each
individual domain and then discover the shared as-
pects (or topics) and aspect terms among a sub-
set of domains. These shared aspects and aspect
terms are more likely to be of good quality. They
can serve as the prior knowledge to guide a model
to extract aspects. A piece of knowledge is a set
of semantically coherent (aspect) terms which are
likely to belong to the same topic or aspect, i.e.,
similar to a must-link, but mined automatically.
Extraction guided by learned knowledge: For
reliable aspect extraction using the learned prior
knowledge, we must account for possible errors
in the knowledge. In particular, a piece of au-
tomatically learned knowledge may be wrong or
domain specific (i.e., the words in the knowledge
are semantically coherent in some domains but
not in others). To leverage such knowledge, the
system must detect those inappropriate pieces of
knowledge. We propose a method to solve this
problem, which also results in a new topic model,
called AKL (Automated Knowledge LDA), whose
inference can exploit the automatically learned
prior knowledge and handle the issues of incorrect
knowledge to produce superior aspects.
In summary, this paper makes the following
contributions:
1. It proposes to exploit the big data to learn prior
knowledge and leverage the knowledge in topic
models to extract more coherent aspects. The
process is fully automatic. To the best of our
knowledge, none of the existing models for as-
pect extraction is able to achieve this.
2. It proposes an effective method to learn qual-
ity knowledge from raw topics produced using
review corpora from many different domains.
3. It proposes a new inference mechanism for
topic modeling, which can handle incorrect
knowledge in aspect extraction.
2 Related Work
Aspect extraction has been studied by many re-
searchers in sentiment analysis (Liu, 2012, Pang
and Lee, 2008), e.g., using supervised sequence
labeling or classification (Choi and Cardie, 2010,
Jakob and Gurevych, 2010, Kobayashi et al, 2007,
Li et al, 2010, Yang and Cardie, 2013) and us-
ing word frequency and syntactic patterns (Hu
and Liu, 2004, Ku et al, 2006, Liu et al, 2013,
Popescu and Etzioni, 2005, Qiu et al, 2011, So-
masundaran and Wiebe, 2009, Wu et al, 2009, Xu
et al, 2013, Yu et al, 2011, Zhao et al, 2012, Zhou
et al, 2013, Zhuang et al, 2006). However,
these works only perform extraction but not as-
pect term grouping or resolution. Separate aspect
term grouping has been done in (Carenini et al,
2005, Guo et al, 2009, Zhai et al, 2011). They
assume that aspect terms have been extracted be-
forehand.
To extract and group aspects simultaneously,
topic models have been applied by researchers
(Branavan et al, 2008, Brody and Elhadad, 2010,
Chen et al, 2013b, Fang and Huang, 2012, He
et al, 2011, Jo and Oh, 2011, Kim et al, 2013,
Lazaridou et al, 2013, Li et al, 2011, Lin and
He, 2009, Lu et al, 2009, Lu et al, 2012, Lu and
Zhai, 2008, Mei et al, 2007, Moghaddam and Es-
ter, 2013, Mukherjee and Liu, 2012, Sauper and
Barzilay, 2013, Titov and McDonald, 2008, Wang
et al, 2010, Zhao et al, 2010). Our proposed AKL
model belongs to the class of knowledge-based
topic models. Besides the knowledge-based topic
models discussed in Section 1, document labels
are incorporated as implicit knowledge in (Blei
and McAuliffe, 2007, Ramage et al, 2009). Ge-
ographical region knowledge has also been con-
sidered in topic models (Eisenstein et al, 2010).
All of these models assume that the prior knowl-
edge is correct. GK-LDA (Chen et al, 2013a) is
the only knowledge-based topic model that deals
with wrong lexical knowledge to some extent. As
we will see in Section 6, AKL outperformed GK-
LDA significantly due to AKL?s more effective er-
ror handling mechanism. Furthermore, GK-LDA
does not learn any prior knowledge.
Our work is also related to transfer learning to
some extent. Topic models have been used to help
348
Input: Corpora DL for knowledge learning
Test corpora DT
1: // STEP 1: Learning prior knowledge.
2: for r = 0 to R do // Iterate R+ 1 times.
3: for each domain corpus D
i
?DL do
4: if r = 0 then
5: A
i
? LDA(D
i
);
6: else
7: A
i
? AKL(D
i
,K);
8: end if
9: end for
10: A? ?
i
A
i
;
11: TC ? Clustering(A);
12: for each cluster T
j
? TC do
13: K
j
? FPM(T
j
);
14: end for
15: K ? ?
j
K
j
;
16: end for
17: // STEP 2: Using the learned knowledge.
18: for each test corpus D
i
?DT do
19: A
i
? AKL(D
i
,K);
20: end for
Figure 1: The proposed overall algorithm.
transfer learning (He et al, 2011, Pan and Yang,
2010, Xue et al, 2008). However, transfer learn-
ing in these papers is for traditional classification
rather than topic/aspect extraction. In (Kang et al,
2012), labeled documents from source domains
are transferred to the target domain to produce
topic models with better fitting. However, we do
not use any labeled data. In (Yang et al, 2011), a
user provided parameter indicating the technical-
ity degree of a domain was used to model the lan-
guage gap between topics. In contrast, our method
is fully automatic without human intervention.
3 Overall Algorithm
This section introduces the proposed overall algo-
rithm. It consists of two main steps: learning qual-
ity knowledge and using the learned knowledge.
Figure 1 gives the algorithm.
Step 1 (learning quality knowledge, Lines 1-
16): The input is the review corpora DL from
multiple domains, from which the knowledge is
automatically learned. Lines 3 and 5 run LDA on
each review domain corpus D
i
? DL to gener-
ate a set of aspects/topics A
i
(lines 2, 4, and 6-
9 will be discussed below). Line 10 unions the
topics from all domains to give A. Lines 11-14
cluster the topics in A into some coherent groups
(or clusters) and then discover knowledgeK
j
from
each group of topics using frequent pattern mining
(FPM) (Han et al, 2007). We will detail these in
Section 4. Each piece of the learned knowledge
is a set of terms which are likely to belong to the
same aspect.
Iterative improvement: The above process can
actually run iteratively because the learned knowl-
edge K can help the topic model learn better top-
ics in each domain D
i
? DL, which results in
better knowledge K in the next iteration. This it-
erative process is reflected in lines 2, 4, 6-9 and 16.
We will examine the performance of the process at
different iterations in Section 6.2. From the sec-
ond iteration, we can use the knowledge learned
from the previous iteration (lines 6-8). The learned
knowledge is leveraged by the new model AKL,
which is discussed below in Step 2.
Step 2 (using the learned knowledge, Lines 17-
20): The proposed model AKL is employed to use
the learned knowledge K to help topic modeling
in test domains DT , which can be DL or other
unseen domains. The key challenge of this step is
how to use the learned prior knowledge K effec-
tively in AKL and deal with possible errors in K.
We will elaborate them in Section 5.
Scalability: the proposed algorithm is naturally
scalable as both LDA and AKL run on each do-
main independently. Thus, for all domains, the
algorithm can run in parallel. Only the resulting
topics need to be brought together for knowledge
learning (Step 1). These resulting topics used in
learning are much smaller than the domain corpus
as only a list of top terms from each topic are uti-
lized due to their high reliability.
4 Learning Quality Knowledge
This section details Step 1 in the overall algorithm,
which has three sub-steps: running LDA (or AKL)
on each domain corpus, clustering the resulting
topics, and mining frequent patterns from the top-
ics in each cluster. Since running LDA is simple,
we will not discuss it further. The proposed AKL
model will be discussed in Section 5. Below we
focus on the other two sub-steps.
4.1 Topic Clustering
After running LDA (or AKL) on each domain cor-
pus, a set of topics is obtained. Each topic is
a distribution over terms (or words), i.e., terms
with their associated probabilities. Here, we use
only the top terms with high probabilities. As dis-
cussed earlier, quality knowledge should be shared
349
by topics across several domains. Thus, it is nat-
ural to exploit a frequency-based approach to dis-
cover frequent set of terms as quality knowledge.
However, we need to deal with two issues.
1. Generic aspects, such as price with aspect
terms like cost and pricy, are shared by many
(even all) product domains. But specific as-
pects such as screen, occur only in domains
with products having them. It means that dif-
ferent aspects may have distinct frequencies.
Thus, using a single frequency threshold in the
frequency-based approach is not sufficient to
extract both generic and specific aspects be-
cause the generic aspects will result in numer-
ous spurious aspects (Han et al, 2007).
2. A term may have multiple senses in different
domains. For example, light can mean ?of little
weight? or ?something that makes things visi-
ble?. A good knowledge base should have the
capacity of handling this ambiguity.
To deal with these two issues, we propose to
discover knowledge in two stages: topic clustering
and frequent pattern mining (FPM).
The purpose of clustering is to group raw topics
from a topic model (LDA or AKL) into clusters.
Each cluster contains semantically related topics
likely to indicate the same real-world aspect. We
then mine knowledge from each cluster using an
FPM technique. Note that the multiple senses of a
term can be distinguished by the semantic mean-
ings represented by the topics in different clusters.
For clustering, we tried k-means and k-
medoids (Kaufman and Rousseeuw, 1990), and
found that k-medoids performs slightly better.
One possible reason is that k-means is more sen-
sitive to outliers. In our topic clustering, each data
point is a topic represented by its top terms (with
their probabilities normalized). The distance be-
tween two data points is measured by symmetrised
KL-Divergence.
4.2 Frequent Pattern Mining
Given topics within each cluster, this step finds
sets of terms that appear together in multiple top-
ics, i.e., shared terms among similar topics across
multiple domains. Terms in such a set are likely
to belong to the same aspect. To find such sets of
terms within each cluster, we use frequent pattern
mining (FPM) (Han et al, 2007), which is suited
for the task. The probability of each term is ig-
nored in FPM.
FPM is stated as follows: Given a set of trans-
actions T, where each transaction t
i
? T is a set
of items from a global item set I , i.e., t
i
? I . In
our context, t
i
is the topic vector comprising the
top terms of a topic (no probability attached). T
is the collection of all topics within a cluster and
I is the set of all terms in T. The goal of FPM is
to find all patterns that satisfy some user-specified
frequency threshold (also called minimum support
count), which is the minimum number of times
that a pattern should appear in T. Such patterns
are called frequent patterns. In our context, a pat-
tern is a set of terms which have appeared multiple
times in the topics within a cluster. Such patterns
compose our knowledge base as shown below.
4.3 Knowledge Representation
As the knowledge is extracted from each cluster
individually, we represent our knowledge base as
a set of clusters, where each cluster consists of a
set of frequent 2-patterns mined using FPM, e.g.,
Cluster 1: {battery, life}, {battery, hour},
{battery, long}, {charge, long}
Cluster 2: {service, support}, {support, cus-
tomer}, {service, customer}
Using two terms in a set is sufficient to cover the
semantic relationship of the terms belonging to the
same aspect. Longer patterns tend to contain more
errors since some terms in a set may not belong to
the same aspect as others. Such partial errors hurt
performance in the downstream model.
5 AKL: Using the Learned Knowledge
We now present the proposed topic model AKL,
which is able to use the automatically learned
knowledge to improve aspect extraction.
5.1 Plate Notation
Differing from most topic models based on topic-
term distribution, AKL incorporates a latent clus-
ter variable c to connect topics and terms. The
plate notation of AKL is shown in Figure 2. The
inputs of the model are M documents, T top-
ics and C clusters. Each document m has N
m
terms. We model distribution P (cluster|topic)
as ? and distribution P (term|topic, cluster) as
? with Dirichlet priors ? and ? respectively.
P (topic|document) is modeled by ? with a
Dirichlet prior ?. The terms in each document are
assumed to be generated by first sampling a topic
z, and then a cluster c given topic z, and finally
350
? ? z c w NmM
? T ? TXC? ?
Figure 2: Plate notation for AKL.
a term w given topic z and cluster c. This plate
notation of AKL and its associated generative pro-
cess are similar to those of MC-LDA (Chen et al,
2013b). However, there are three key differences.
1. Our knowledge is automatically mined which
may have errors (or noises), while the prior
knowledge for MC-LDA is manually provided
and assumed to be correct. As we will see in
Section 6, using our knowledge, MC-LDA does
not generate as coherent aspects as AKL.
2. Our knowledge is represented as clusters. Each
cluster contains a set of frequent 2-patterns
with semantically correlated terms. They are
different from must-sets used in MC-LDA.
3. Most importantly, due to the use of the new
form of knowledge, AKL?s inference mecha-
nism (Gibbs sampler) is entirely different from
that of MC-LDA (Section 5.2), which results in
superior performances (Section 6). Note that
the inference mechanism and the prior knowl-
edge cannot be reflected in the plate notation
for AKL in Figure 2.
In short, our modeling contributions are (1) the
capability of handling more expressive knowledge
in the form of clusters, (2) a novel Gibbs sampler
to deal with inappropriate knowledge.
5.2 The Gibbs Sampler
As the automatically learned prior knowledge may
contain errors for a domain, AKL has to learn
the usefulness of each piece of knowledge dy-
namically during inference. Instead of assigning
weights to each piece of knowledge as a fixed prior
in (Chen et al, 2013a), we propose a new Gibbs
sampler, which can dynamically balance the use
of prior knowledge and the information in the cor-
pus during the Gibbs sampling iterations.
We adopt a Blocked Gibbs sampler (Rosen-Zvi
et al, 2010) as it improves convergence and re-
duces autocorrelation when the variables (topic z
and cluster c in AKL) are highly related. For each
term w
i
in each document, we jointly sample a
topic z
i
and cluster c
i
(containing w
i
) based on the
conditional distribution in Gibbs sampler (will be
detailed in Equation 4). To compute this distribu-
tion, instead of considering how well z
i
matches
with w
i
only (as in LDA), we also consider two
other factors:
1. The extent c
i
corroborates w
i
given the corpus.
By ?corroborate?, we mean whether those fre-
quent 2-patterns in c
i
containing w
i
are also
supported by the actual information in the do-
main corpus to some extent (see the measure in
Equation 1 below). If c
i
corroborates w
i
well,
c
i
is likely to be useful, and thus should also
provide guidance in determining z
i
. Otherwise,
c
i
may not be a suitable piece of knowledge for
w
i
in the domain.
2. Agreement between c
i
and z
i
. By agreement
we mean the degree that the terms (union of all
frequent 2-patterns of c
i
) in cluster c
i
are re-
flected in topic z
i
. Unlike the first factor, this is
a global factor as it concerns all the terms in a
knowledge cluster.
For the first factor, we measure how well c
i
corroborates w
i
given the corpus based on co-
document frequency ratio. As shown in (Mimno
et al, 2011), co-document frequency is a good in-
dicator of term correlation in a domain. Follow-
ing (Mimno et al, 2011), we define a symmetric
co-document frequency ratio as follows:
Co-Doc(w,w
?
) =
D(w,w
?
) + 1
(D(w) +D(w
?
))?
1
2
+ 1
(1)
where (w,w
?
) refers to each frequent 2-pattern in
the knowledge cluster c
i
. D(w,w
?
) is the number
of documents that contain both termsw andw
?
and
D(w) is the number of documents containing w.
A smoothing count of 1 is added to avoid the ratio
being 0.
For the second factor, if cluster c
i
and topic z
i
agree, the intuition is that the terms in c
i
(union of
all frequent 2-patterns of c
i
) should appear as top
terms under z
i
(i.e., ranked top according to the
term probability under z
i
). We define the agree-
ment using symmetrised KL-Divergence between
the two distributions (DIST
c
and DIST
z
) cor-
responding to c
i
and z
i
respectively. As there is
no prior preference on the terms of c
i
, we use
the uniform distribution over all terms in c
i
for
DIST
c
. For DIST
z
, as only top 20 terms un-
der z
i
are usually reliable, we use these top terms
351
with their probabilities (re-normalized) to repre-
sent the topic. Note that a smoothing probability
(i.e., a very small value) is also given to every term
for calculating KL-Divergence. GivenDIST
c
and
DIST
z
, the agreement is computed with:
Agreement(c, z) =
1
KL(DIST
c
, DIST
z
)
(2)
The rationale of Equation 2 is that the lesser di-
vergence between DIST
c
and DIST
z
implies the
more agreement between c
i
and z
i
.
We further employ the Generalized Plya urn
(GPU) model (Mahmoud, 2008) which was shown
to be effective in leveraging semantically related
words (Chen et al, 2013a, Chen et al, 2013b,
Mimno et al, 2011). The GPU model here ba-
sically states that assigning topic z
i
and cluster c
i
to term w
i
will not only increase the probability
of connecting z
i
and c
i
with w
i
, but also make
it more likely to associate z
i
and c
i
with term w
?
where w
?
shares a 2-pattern with w
i
in c
i
. The
amount of probability increase is determined by
matrixA
c,w
?
,w
defined as:
A
c,w
?
,w
=
?
??
??
1, if w = w
?
?, if (w,w
?
) ? c, w 6= w
?
0, otherwise
(3)
where value 1 controls the probability increase of
w by seeingw itself, and ? controls the probability
increase of w
?
by seeing w. Please refer to (Chen
et al, 2013b) for more details.
Putting together Equations 1, 2 and 3 into a
blocked Gibbs Sampler, we can define the follow-
ing sampling distribution in Gibbs sampler so that
it provides helpful guidance in determining the
usefulness of the prior knowledge and in selecting
the semantically coherent topic.
P (z
i
= t, c
i
= c|z?i, c?i,w, ?, ?, ?,A)
?
?
(w,w
?
)?c
Co-Doc(w,w
?
)?Agreement(c, t)
?
n
?i
m,t
+ ?
?
T
t
?
=1
(n
?i
m,t
?
+ ?)
?
?
V
w
?
=1
?
V
v
?
=1
A
c,v
?
,w
?
? n
?i
t,c,v
?
+ ?
?
C
c
?
=1
(
?
V
w
?
=1
?
V
v
?
=1
A
c
?
,v
?
,w
?
? n
?i
t,c
?
,v
?
+ ?)
?
?
V
w
?
=1
A
c,w
?
,w
i
? n
?i
t,c,w
?
+ ?
?
V
v
?
=1
(
?
V
w
?
=1
A
c,w
?
,v
?
? n
?i
t,c,w
?
+ ?)
(4)
where n
?i
denotes the count excluding the current
assignment of z
i
and c
i
, i.e., z
?i
and c
?i
. n
m,t
de-
notes the number of times that topic twas assigned
to terms in document m. n
t,c
denotes the times
that cluster c occurs under topic t. n
t,c,v
refers to
the number of times that term v appears in cluster
c under topic t. ?, ? and ? are predefined Dirichlet
hyperparameters.
Note that although the above Gibbs sampler is
able to distinguish useful knowledge from wrong
knowledge, it is possible that there is no cluster
corroborates for a particular term. For every term
w, apart from its knowledge clusters, we also add
a singleton cluster for w, i.e., a cluster with one
pattern {w,w} only. When no knowledge cluster
is applicable, this singleton cluster is used. As a
singleton cluster does not contain any knowledge
information but only the word itself, Equations 1
and 2 cannot be computed. For the values of sin-
gleton clusters for these two equations, we assign
them as the averages of those values of all non-
singleton knowledge clusters.
6 Experiments
This section evaluates and compares the pro-
posed AKL model with three baseline models
LDA, MC-LDA, and GK-LDA. LDA (Blei et
al., 2003) is the most popular unsupervised topic
model. MC-LDA (Chen et al, 2013b) is a re-
cent knowledge-based model for aspect extrac-
tion. GK-LDA (Chen et al, 2013a) handles wrong
knowledge by setting prior weights using the ratio
of word probabilities. Our automatically extracted
knowledge is provided to these models. Note that
cannot-set of MC-LDA is not used in AKL.
6.1 Experimental Settings
Dataset. We created a large dataset containing
reviews from 36 product domains or types from
Amazon.com. The product domain names are
listed in Table 1. Each domain contains 1, 000 re-
views. This gives us 36 domain corpora. We have
made the dataset publically available at the web-
site of the first author.
Pre-processing. We followed (Chen et al, 2013b)
to employ standard pre-processing like lemmatiza-
tion and stopword removal. To have a fair compar-
ison, we also treat each sentence as a document as
in (Chen et al, 2013a, Chen et al, 2013b).
Parameter Settings. For all models, posterior es-
timates of latent variables were taken with a sam-
pling lag of 20 iterations in the post burn-in phase
(first 200 iterations for burn-in) with 2, 000 itera-
tions in total. The model parameters were tuned
on the development set in our pilot experiments
352
Amplifier DVD Player Kindle MP3 Player Scanner Video Player
Blu-Ray Player GPS Laptop Network Adapter Speaker Video Recorder
Camera Hard Drive Media Player Printer Subwoofer Watch
CD Player Headphone Microphone Projector Tablet Webcam
Cell Phone Home Theater System Monitor Radar Detector Telephone Wireless Router
Computer Keyboard Mouse Remote Control TV Xbox
Table 1: List of 36 domain names.
and set to ? = 1, ? = 0.1, T = 15, and ? = 0.2.
Furthermore, for each cluster, ? is set proportional
to the number of terms in it. The other param-
eters for MC-LDA and GK-LDA were set as in
their original papers. For parameters of AKL, we
used the top 15 terms for each topic in the clus-
tering phrase. The number of clusters is set to
the number of domains. We will test the sensitiv-
ity of these clustering parameters in Section 6.4.
The minimum support count for frequent pattern
mining was set empirically to min(5, 0.4?#T),
where #T is the number of transactions (i.e., the
number of topics from all domains) in a cluster.
Test Settings: We use two test settings as below:
1. (Sections 6.2, 6.3 and 6.4) Test on the same cor-
pora as those used in learning the prior knowl-
edge. This is meaningful as the learning phrase
is automatic and unsupervised (Figure 1). We
call this self-learning-and-improvement.
2. (Section 6.5) Test on new/unseen domain cor-
pora after knowledge learning.
6.2 Topic Coherence
This sub-section evaluates the topics/aspects gen-
erated by each model based on Topic Coher-
ence (Mimno et al, 2011) in test setting 1. Tra-
ditionally, topic models have been evaluated us-
ing perplexity. However, perplexity on the held-
out test set does not reflect the semantic coher-
ence of topics and may be contrary to human judg-
ments (Chang et al, 2009). Instead, the met-
ric Topic Coherence has been shown in (Mimno
-1510
-1490
-1470
-1450
-1430
0 1 2 3 4 5 6
To
pic
 Co
her
enc
e
AKL GK-LDAMC-LDA LDA
Figure 3: Average Topic Coherence of each model
at different learning iterations (Iteration 0 is equiv-
alent to LDA).
et al, 2011) to correlate well with human judg-
ments. Recently, it has become a standard prac-
tice to use Topic Coherence for evaluation of topic
models (Arora et al, 2013). A higher Topic Coher-
ence value indicates a better topic interpretability,
i.e., semantically more coherent topics.
Figure 3 shows the average Topic Coherence of
each model using knowledge learned at different
learning iterations (Figure 1). For MC-LDA or
GK-LDA, this is done by replacing AKL in lines
7 and 19 of Figure 1 with MC-LDA or GK-LDA.
Each value is the average over all 36 domains.
From Figure 3, we can observe the followings:
1. AKL performs the best with the highest Topic
Coherence values at all iterations. It is actu-
ally the best in all 36 domains. These show that
AKL finds more interpretable topics than the
baselines. Its values stabilize after iteration 3.
2. Both GK-LDA and MC-LDA perform slightly
better than LDA in iterations 1 and 2. MC-
LDA does not handle wrong knowledge. This
shows that the mined knowledge is of good
quality. Although GK-LDA uses large word
probability differences under a topic to detect
wrong lexical knowledge, it is not as effective
as AKL. The reason is that as the lexical knowl-
edge is from general dictionaries rather than
mined from relevant domain data, the words
in a wrong piece of knowledge usually have a
very large probability difference under a topic.
However, our knowledge is mined from top
words in related topics including topics from
the current domain. The words in a piece of in-
correct (or correct) knowledge often have sim-
ilar probabilities under a topic. The proposed
dynamic knowledge adjusting mechanism in
AKL is superior.
Paired t-test shows that AKL outperforms all
baselines significantly (p < 0.0001).
6.3 User Evaluation
As our objective is to discover more coherent as-
pects, we recruited two human judges. Here we
also use the test setting 1. Each topic is annotated
as coherent if the judge feels that most of its top
353
0.6
0.7
0.8
0.9
1.0
Camera Computer Headphone GPS
Pre
cisi
on
@ 5
AKL GK-LDA MC-LDA LDA
0.6
0.7
0.8
0.9
1.0
Camera Computer Headphone GPS
Pre
cisi
on
@ 10
AKL GK-LDA MC-LDA LDA
Figure 4: Average Precision@5 (Left) and Precision@10 (Right) of coherent topics from four models
in each domain. (Headphone has a lot of overlapping topics in other domains while GPS has little.)
terms coherently represent a real-world product
aspect; otherwise incoherent. For a coherent topic,
each top term is annotated as correct if it reflects
the aspect represented by the topic; otherwise in-
correct. We labeled the topics of each model
at learning iteration 1 where the same pieces of
knowledge (extracted from LDA topics at learn-
ing iteration 0) are provided to each model. After
learning iteration 1, the gap between AKL and the
baseline models tends to widen. To be consistent,
the results later in Sections 6.4 and 6.5 also show
each model at learning iteration 1. We also notice
that after a few learning iterations, the topics from
AKL model tend to have some resemblance across
domains. We found that AKL with 2 learning it-
erations achieved the best topics. Note that LDA
cannot use any prior knowledge.
We manually labeled results from four domains,
i.e., Camera, Computer, Headphone, and GPS. We
chose Headphone as it has a lot of overlapping
of topics with other domains because many elec-
tronic products use headphone. GPS was cho-
sen because it does not have much topic overlap-
ping with other domains as its aspects are mostly
about Navigation and Maps. Domains Camera and
Computer lay in between. We want to see how
domain overlapping influences the performance of
AKL. Cohen?s Kappa scores for annotator agree-
ment are 0.918 (for topics) and 0.872 (for terms).
To measure the results, we compute
Precision@n (or p@n) based on the anno-
tations, which was also used in (Chen et al,
2013b, Mukherjee and Liu, 2012).
Figure 4 shows the precision@n results for
n = 5 and 10. We can see that AKL makes im-
provements in all 4 domains. The improvement
varies in domains with the most increase in Head-
phone and the least in GPS as Headphone overlaps
more with other domains than GPS. Note that if a
domain shares aspects with many other domains,
its model should benefit more; otherwise, it is rea-
sonable to expect lesser improvements. For the
baselines, GK-LDA and MC-LDA perform simi-
larly to LDA with minor variations, all of which
are inferior to AKL. AKL?s improvements over
other models are statistically significant based on
paired t-test (p < 0.002).
In terms of the number of coherent topics, AKL
discovers one more coherent topic than LDA in
Computer and one more coherent topic than GK-
LDA and MC-LDA in Headphone. For the other
domains, the numbers of coherent topics are the
same for all models.
Table 2 shows an example aspect (battery) and
its top 10 terms produced by AKL and LDA for
each domain to give a flavor of the kind of im-
provements made by AKL. The results for GK-
LDA and MC-LDA are about the same as LDA
(see also Figure 4). Table 2 focuses on the as-
pects generated by AKL and LDA. From Table 2,
we can see that AKL discovers more correct and
meaningful aspect terms at the top. Note that
those terms marked in red and italicized are er-
rors. Apart from Table 2, many aspects are dra-
matically improved by AKL, including some com-
monly shared aspects such as Price, Screen, and
Customer Service.
6.4 Sensitivity to Clustering Parameters
This sub-section investigates the sensitivity of the
clustering parameters of AKL (again in test setting
1). The top sub-figure in Figure 5 shows the aver-
age Topic Coherence values versus the top k terms
per topic used in topic clustering (Section 4.1).
The number of clusters is set to the number of
domains (see below). We can observe that using
k = 15 top terms gives the highest value. This is
intuitive as too few (or too many) top terms may
generate insufficient (or noisy) knowledge.
The bottom sub-figure in Figure 5 shows the
average Topic Coherence given different number
354
Camera Computer Headphone GPS
AKL LDA AKL LDA AKL LDA AKL LDA
battery battery battery battery hour long battery trip
life card hour cable long battery hour battery
hour memory life speaker battery hour long hour
long life long dvi life comfortable model mile
charge usb speaker sound charge easy life long
extra hour sound hour amp uncomfortable charge life
minute minute charge connection uncomfortable headset trip destination
charger sd dvi life comfortable life purchase phone
short extra tv hdmus period money older charge
aa device hdmus tv output hard compass mode
Table 2: Example aspect Battery from AKL and LDA in each domain. Errors are italicized in red.
-1510
-1490
-1470
-1450
-1430
5 10 15 20 25 30T
opi
c C
ohe
ren
ce
#Top Terms for Clustering
-1510
-1490
-1470
-1450
-1430
20 30 40 50 60 70T
opi
c C
ohe
ren
ce
#Clusters
Figure 5: Average topic coherence of AKL versus
#top k terms (Top) and #clusters (Bottom).
-1490
-1480
-1470
-1460
-1450
AKL GK-LDA MC-LDA LDA
To
pic
 Co
her
enc
e
Figure 6: Average topic coherence of each model
tested on new/unseen domain.
of clusters. We fix the number of top terms per
topic to 15 as it yields the best result (see the top
sub-figure in Figure 5). We can see that the per-
formance is not very sensitive to the number of
clusters. The model performs similarly for 30 to
50 clusters, with lower Topic Coherence for less
than 30 or more than 50 clusters. The significance
test indicates that using 30, 40, and 50 clusters,
AKL achieved significant improvements over all
baseline models (p < 0.0001). With more do-
mains, we should expect a larger number of clus-
ters. However, it is difficult to obtain the optimal
number of clusters. Thus, we empirically set the
number of clusters to the number of domains in
our experiments. Note that the number of clus-
ters (C) is expected to be larger than the number
of topics in one domain (T ) because C is for all
domains while T is for one particular domain.
6.5 Test on New Domains
We now evaluate AKL in test setting 2, i.e., the au-
tomatically extracted knowledge K (Figure 1) is
applied in new/unseen domains other than those in
domainsDL used in knowledge learning. The aim
is to see how K can help modeling in an unseen
domain. In this set of experiments, each domain
is tested by using the learned knowledge from the
rest 35 domains. Figure 6 shows the average Topic
Coherence of each model. The values are also av-
eraged over the 36 tested domains. We can see that
AKL achieves the highest Topic Coherence value
while LDA has the lowest. The improvements of
AKL over all baseline models are significant with
p < 0.0001.
7 Conclusions
This paper proposed an advanced aspect extraction
framework which can learn knowledge automati-
cally from a large number of review corpora and
exploit the learned knowledge in extracting more
coherent aspects. It first proposed a technique to
learn knowledge automatically by clustering and
FPM. Then a new topic model with an advanced
inference mechanism was proposed to exploit the
learned knowledge in a fault-tolerant manner. Ex-
perimental results using review corpora from 36
domains showed that the proposed method outper-
forms state-of-the-art methods significantly.
Acknowledgments
This work was supported in part by a grant from
National Science Foundation (NSF) under grant
no. IIS-1111092.
355
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet Forest priors. In Proceedings
of ICML, pages 25?32.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent Dirich-
let alocation using first-order logic. In Proceedings
of IJCAI, pages 1171?1177.
Sanjeev Arora, Rong Ge, Yonatan Halpern, David
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A Practical Algorithm for
Topic Modeling with Provable Guarantees. In Pro-
ceedings of ICML, pages 280?288.
David M. Blei and Jon D McAuliffe. 2007. Supervised
Topic Models. In Proceedings of NIPS, pages 121?
128.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
S R K Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning Document-Level
Semantic Properties from Free-Text Annotations. In
Proceedings of ACL, pages 263?271.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812.
Giuseppe Carenini, Raymond T Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of K-CAP, pages 11?18.
Jonathan Chang, Jordan Boyd-Graber, Wang Chong,
Sean Gerrish, and David Blei, M. 2009. Reading
Tea Leaves: How Humans Interpret Topic Models.
In Proceedings of NIPS, pages 288?296.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013a. Discovering Coherent Topics Using General
Knowledge. In Proceedings of CIKM, pages 209?
218.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013b. Exploiting Domain Knowledge in Aspect
Extraction. In Proceedings of EMNLP, pages 1655?
1667.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013c. Leveraging Multi-Domain Prior Knowledge
in Topic Models. In Proceedings of IJCAI, pages
2071?2077.
Yejin Choi and Claire Cardie. 2010. Hierarchical Se-
quential Learning for Extracting Opinions and their
Attributes. In Proceedings of ACL, pages 269?274.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A Latent Variable Model
for Geographic Lexical Variation. In Proceedings of
EMNLP, pages 1277?1287.
Lei Fang and Minlie Huang. 2012. Fine Granular As-
pect Analysis using Latent Structural Models. In
Proceedings of ACL, pages 333?337.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
and Zhong Su. 2009. Product feature categoriza-
tion with multilevel latent semantic association. In
Proceedings of CIKM, pages 1087?1096.
Jiawei Han, Hong Cheng, Dong Xin, and Xifeng Yan.
2007. Frequent pattern mining: current status and
future directions. Data Mining and Knowledge Dis-
covery, 15(1):55?86.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Au-
tomatically Extracting Polarity-Bearing Topics for
Cross-Domain Sentiment Classification. In Pro-
ceedings of ACL, pages 123?131.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Analysis. In Proceedings of UAI, pages 289?296.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of KDD,
pages 168?177.
Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-
noff. 2011. Interactive Topic Modeling. In Pro-
ceedings of ACL, pages 248?257.
Jagadeesh Jagarlamudi, Hal Daum?e III, and Raghaven-
dra Udupa. 2012. Incorporating Lexical Priors into
Topic Models. In Proceedings of EACL, pages 204?
213.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
Opinion Targets in a Single- and Cross-Domain Set-
ting with Conditional Random Fields. In Proceed-
ings of EMNLP, pages 1035?1045.
Yohan Jo and Alice H. Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM, pages 815?824.
Jeon-hyung Kang, Jun Ma, and Yan Liu. 2012. Trans-
fer Topic Modeling with Ease and Scalability. In
Proceedings of SDM, pages 564?575.
L Kaufman and P J Rousseeuw. 1990. Finding groups
in data: an introduction to cluster analysis. John
Wiley and Sons.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A Hierarchical Aspect-Sentiment
Model for Online Reviews. In Proceedings of AAAI,
pages 526?533.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting Aspect-Evaluation and Aspect-of
Relations in Opinion Mining. In Proceedings of
EMNLP, pages 1065?1074.
356
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion Extraction, Summarization and
Tracking in News and Blog Corpora. In Proceed-
ings of AAAI-CAAW, pages 100?107.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A Bayesian Model for Joint
Unsupervised Induction of Sentiment, Aspect and
Discourse Representations. In Proceedings of ACL,
pages 1630?1639.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-Aware Review Mining and Summariza-
tion. In Proceedings of COLING, pages 653?661.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011.
Generating Aspect-oriented Multi-Document Sum-
marization with Event-aspect model. In Proceed-
ings of EMNLP, pages 1137?1146.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM, pages 375?384.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic
Patterns versus Word Alignment: Extracting Opin-
ion Targets from Online Reviews. In Proceedings of
ACL, pages 1754?1763.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Yue Lu and Chengxiang Zhai. 2008. Opinion inte-
gration through semi-supervised topic modeling. In
Proceedings of WWW, pages 121?130.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In Proceedings of WWW, pages 131?140.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K Tsou.
2011. Multi-aspect Sentiment Analysis with Topic
Models. In Proceedings of ICDM Workshops, pages
81?88.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of CIKM, pages 1642?1646.
Hosam Mahmoud. 2008. Polya Urn Models. Chap-
man & Hall/CRC Texts in Statistical Science.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW, pages 171?180.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of EMNLP, pages 262?272.
Samaneh Moghaddam and Martin Ester. 2013. The
FLDA Model for Aspect-based Opinion Mining:
Addressing the Cold Start Problem. In Proceedings
of WWW, pages 909?918.
Arjun Mukherjee and Bing Liu. 2012. Aspect Extrac-
tion through Semi-Supervised Modeling. In Pro-
ceedings of ACL, pages 339?348.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on
Transfer Learning. IEEE Trans. Knowl. Data Eng.,
22(10):1345?1359.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
James Petterson, Alex Smola, Tib?erio Caetano, Wray
Buntine, and Shravan Narayanamurthy. 2010. Word
Features for Latent Dirichlet Allocation. In Pro-
ceedings of NIPS, pages 1921?1929.
AM Popescu and Oren Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of HLT, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion Word Expansion and Target Extrac-
tion through Double Propagation. Computational
Linguistics, 37(1):9?27.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: a su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP, pages
248?256.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas
Griffiths, Padhraic Smyth, and Mark Steyvers.
2010. Learning author-topic models from text cor-
pora. ACM Transactions on Information Systems,
28(1):1?38.
Christina Sauper and Regina Barzilay. 2013. Auto-
matic Aggregation by Joint Modeling of Aspects and
Values. J. Artif. Intell. Res. (JAIR), 46:89?127.
Swapna Somasundaran and J Wiebe. 2009. Recog-
nizing stances in online debates. In Proceedings of
ACL, pages 226?234.
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL, pages 308?316.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a
rating regression approach. In Proceedings of KDD,
pages 783?792.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of EMNLP, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Mining Opinion Words and Opinion
Targets in a Two-Stage Framework. In Proceedings
of ACL, pages 1764?1773.
GR Xue, Wenyuan Dai, Q Yang, and Y Yu. 2008.
Topic-bridged PLSA for cross-domain text classifi-
cation. In Proceedings of SIGIR, pages 627?634.
357
Bishan Yang and Claire Cardie. 2013. Joint Inference
for Fine-grained Opinion Extraction. In Proceed-
ings of ACL, pages 1640?1649.
Shuang Hong Yang, Steven P. Crain, and Hongyuan
Zha. 2011. Bridging the language gap: Topic adap-
tation for documents with different technicality. In
Proceedings of AISTATS, pages 823?831.
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect Ranking: Identifying
Important Product Aspects from Online Consumer
Reviews. In Proceedings of ACL, pages 1496?1505.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Constrained LDA for grouping product features in
opinion mining. In Proceedings of PAKDD, pages
448?459.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly Modeling Aspects and Opin-
ions with a MaxEnt-LDA Hybrid. In Proceedings of
EMNLP, pages 56?65.
Yanyan Zhao, Bing Qin, and Ting Liu. 2012. Col-
location polarity disambiguation using web-based
pseudo contexts. In Proceedings of EMNLP-
CoNLL, pages 160?170.
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2013.
Collective Opinion Target Extraction in Chinese Mi-
croblogs. In Proceedings of EMNLP, pages 1840?
1850.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of CIKM, pages 43?50.
358
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 345?351,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Tri-Training for Authorship Attribution with Limited Training Data 
 
Tieyun Qian 
State Key Laboratory 
of Software Eng., 
Wuhan University 
430072, Hubei, China 
qty@whu.edu.cn 
Bing Liu 
Dept. of Computer Sci-
ence, Univ. of Illinois at 
Chicago 
IL, USA, 60607 
liub@cs.uic.edu 
Li Chen 
State Key Laboratory of 
Software Eng., 
Wuhan University 
430072, Hubei, China 
ccnuchenli@163.com 
Zhiyong Peng 
Computer School, 
Wuhan University 
430072, Hubei, China 
peng@whu.edu.cn 
 
  
 
Abstract 
Authorship attribution (AA) aims to identify 
the authors of a set of documents. Traditional 
studies in this area often assume that there are 
a large set of labeled documents available for 
training. However, in the real life, it is often 
difficult or expensive to collect a large set of 
labeled data. For example, in the online review 
domain, most reviewers (authors) only write a 
few reviews, which are not enough to serve as 
the training data for accurate classification. In 
this paper, we present a novel three-view tri-
training method to iteratively identify authors 
of unlabeled data to augment the training set. 
The key idea is to first represent each docu-
ment in three distinct views, and then perform 
tri-training to exploit the large amount of un-
labeled documents. Starting from 10 training 
documents per author, we systematically eval-
uate the effectiveness of the proposed tri-
training method for AA. Experimental results 
show that the proposed approach outperforms 
the state-of-the-art semi-supervised method 
CNG+SVM and other baselines.  
1 Introduction 
Existing approaches to authorship attribution 
(AA) are mainly based on supervised classifica-
tion (Stamatatos, 2009, Kim et al, 2011, Serous-
si et al, 2012). Although this is an effective ap-
proach, it has a major weakness, i.e., for each 
author a large number of his/her articles are 
needed as the training data. This is possible if the 
author has written a large number of articles, but 
will be difficult if he/she has not. For example, in 
the online review domain, most authors (review-
ers) only write a few reviews (documents). It was 
shown that on average each reviewer only has 
2.72 reviews in amazon.com, and only 8% of the 
reviewers have at least 5 reviews (Jindal and Liu, 
2008). The small number of labeled documents 
makes it extremely challenging for supervised 
learning to train an accurate classifier. 
In this paper, we consider AA with only a few 
labeled examples. By exploiting the redundancy 
in human languages, we tackle the problem using 
a new three-view tri-training algorithm (TTA). 
Specifically, we first represent each document in 
three distinct views, and then tri-train three clas-
sifiers in these views. The predictions of two 
classifiers on unlabeled examples are used to 
augment the training set for the third classifier. 
This process repeats until a termination condition 
is met. The enlarged labeled sets are finally used 
to train classifiers to classify the test data.  
To our knowledge, no existing work has ad-
dressed AA in a tri-training framework. The AA 
problem with limited training data was attempted 
in (Stamatatos, 2007; Luyckx and Daelemans, 
2008). However, neither of them used a semi-
supervised approach to augment the training set 
with additional documents. Kourtis and Stama-
tatos (2011) introduced a variant of the self-
training method in (Nigam and Ghani, 2000). 
Note that the original self-training uses one clas-
sifier on one view. However, the self-training 
method in (Kourtis and Stamatatos, 2011) uses 
two classifiers (CNG and SVM) on one view. 
Both the self-training and tri-training are semi-
supervised learning methods. However, the pro-
posed approach is not a simple extension of the 
self-training method CNG+SVM of (Kourtis and 
Stamatatos, 2011). There are key differences. 
First, in their experimental setting, about 115 
and 129 documents per author on average are 
used for two experimental corpora. This number 
of labeled documents is still very large. We con-
sider a much more realistic problem, where the 
size of the training set is very small. Only 10 
samples per author are used in training.  
Second, CNG+SVM uses two learning methods 
on a single character n-gram view. In contrast, 
besides the character n-gram view, we also make 
use of the lexical and syntactic views. That is, 
345
three distinct views are used for building classi-
fiers. The redundant information in human lan-
guage is combined in the tri-training procedure.  
Third, in each round of self-training in 
CNG+SVM, each classifier is refined by the same 
newly labeled examples. However, in the pro-
posed tri-training method (TTA), the examples 
labeled by the classifiers of every two views are 
added to the third view. By doing so, each classi-
fier can borrow information from the other two 
views. And the predictions made by two classifi-
ers are more reliable than those by one classifier. 
The main contribution of this paper is thus the 
proposed three-view tri-training scheme which 
has a much better generalization ability by ex-
ploiting three different views of the same docu-
ment. Experimental results on the IMDb review 
dataset show that the proposed method dramati-
cally improves the CNG+SVM method. It also 
outperforms the co-training method (Blum and 
Mitchell, 1998) based on our proposed views.  
2 Related Work 
Existing AA methods either focused on finding 
suitable features or on developing effective 
techniques. Example features include function 
words (Argamon et al, 2007), richness features 
(Gamon 2004), punctuation frequencies (Graham 
et al, 2005), character (Grieve, 2007), word 
(Burrows, 1992) and POS n-grams (Gamon, 
2004; Hirst and Feiguina, 2007), rewrite rules 
(Halteren et al, 1996), and similarities (Qian and 
Liu, 2013). On developing effective learning 
techniques, supervised classification has been the 
dominant approach, e.g., neural networks 
(Graham et al, 2005; Zheng et al, 2006), 
decision tree (Uzuner and Katz, 2005; Zhao and 
Zobel, 2005), logistic regression (Madigan et al, 
2005), SVM (Diederich et al, 2000; Gamon 
2004; Li et al, 2006; Kim et al, 2011), etc. 
The main problem in the traditional research is 
the unrealistic size of the training set. A size of 
about 10,000 words per author is regarded as a 
reasonable training set size (Argamon et al, 
2007, Burrows, 2003). When no long documents 
are available, tens or hundreds of short texts are 
used (Halteren, 2007; Hirst and Feiguina, 2007; 
Schwartz et al, 2013).  
Apart from the existing works dealing with 
limited data discussed in the introduction, our 
preliminary study in (Qian et al, 2014) used one 
learning method on two views, but it is inferior 
to the proposed method in this paper.  
 
3 Proposed Tri-Training Algorithm  
3.1 Overall Framework 
We represent each document in three feature 
views: the character view, the lexical view and 
the syntactic view. Each view consists of a set of 
features in the respective type. A classifier can 
be learned from any of these views. We propose 
a three-view training algorithm to deal with the 
problem of limited training data. Logistic 
regression (LR) is used as the learner. The 
overall framework is shown in Figure 1. 
Given the labeled, unlabeled, and test sets L, 
U, and T, step 1 extracts the character, lexical, 
and syntactic views from L, U, and T, 
respectively. Steps 2-13 iteratively tri-train three 
classifiers by adding the data which are assigned 
the same label by two classifiers into the training 
set of the third classifier. The algorithm first 
randomly selects u unlabeled documents from U 
to create a pool U? of examples. Note that we can 
directly select from the large unlabeled set U. 
However, it is shown in (Blum and Mitchell 
2008) that a smaller pool can force the classifiers 
to select instances that are more representative of 
the underlying distribution that generates U. 
Hence we set the parameter u to a size of about 
1% of the whole unlabeled set, which allows us 
to observe the effects of different number of 
iterations. It then iterates over the following 
steps. First, use character, lexical and syntactic 
views on the current labeled set to train three 
classifiers C1, C2, and C3. See Steps 4-9. Second, 
Input: A small set of labeled documents L = {l1,?, lr}, a large 
set of unlabeled documents U = {u1,?, us}, and a set of test 
documents T = {t1,?, tt}, 
Parameters: the number of iterations k, the size of selected un-
labeled documents u 
Output: tk?s class assignment 
1   Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T 
2  Loop for k iterations: 
3  Randomly select u unlabeled documents U' from U; 
4       Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls); 
5        Use C1 to label docs in U' based on U1(U1=Uc, Ul, or Us) 
6        Learn the second view classifier C2 from L2 (L2?L1) 
7        Use C2 to label documents in U' based on U2 (U2?U1); 
8        Learn the third view classifier C3 from L3 (L2?L1, L2) 
9        Use C3 to label documents in U' based on U3 (U2?U1, U2); 
10      Up1 = {u | u? U', u.label by C2 = u.label by C3}; 
11      Up2 = {u | u? U', u.label by C1 = u.label by C3}; 
12      Up3 = {u | u? U', u.label by C1 = u.label by C2}; 
13      U = U - U', Li = Li ? Upi (i=1..3);             
14 Learn three classifiers C1, C2, C3 from L1, L2, L3; 
15 Use Ci to label tk in Ti (i=1..3); 
16  Aggregate results from three views 
Figure 1: The tri-training algorithm (TTA) 
346
allow two of these three classifiers to classify the 
unlabeled set U? and choose p documents with 
agreed labels. See Steps 10-12. The selected 
documents are then added to the third labeled set 
for the label assigned (a label is an author here), 
and the u documents are removed from the 
unlabeled pool U? (line 13). We call this way of 
augmenting the training sets InterAdding. The 
one used in (Kourtis and Stamatatos, 2011) is 
called SelfAdding as it uses only a single view 
and adds to the same training set. Steps 14-15 
assign the test document to a category (author) 
using the classifier learned from the three views 
in the augmented labeled data, respectively. Step 
16 aggregates the results from three classifiers. 
3.2 Character View 
The features in the character view are the 
character n-grams of a document. Character n-
grams are simple and easily available for any 
natural language. For a fair comparison with the 
previous work in (Kourtis and Stamatatos, 2011), 
we extract frequencies of 3-grams at the 
character-level. The vocabulary size for character 
3-grams in our experiment is 28584.  
3.3 Lexical View 
The lexical view consists of word unigrams of a 
document. We represent each article by a vector 
of word frequencies. The vocabulary size for 
unigrams in our experiment is 195274.  
3.4 Syntactic View 
The syntactic view consists of the syntactic 
features of a document. We use four content-
independent structures including n-grams of POS 
tags (n = 1..3) and rewrite rules (Kim et al, 
2011). The vocabulary sizes for POS 1-grams, 
POS 2-grams, POS 3-grams, and rewrite rules in 
our experiment are 63, 1917, 21950, and 19240, 
respectively. These four types of syntactic 
structures are merged into a single vector. Hence 
the syntactic view of a document is represented 
as a vector of 43140 components. 
3.5 Aggregating Results from Three Views 
In testing, once we obtain the prediction values 
from three classifiers for a test document tk, an 
additional algorithm is used to decide the final 
author attribution. One simple method is voting. 
However, this method is weaker than the three 
methods below. It is also hard to compare with 
the self-training method CNG+SVM in (Kourtis 
and Stamatatos, 2011) as it only has two classifi-
ers. Hence we present three other strategies to 
further aggregate the results from the three 
views. These methods require the classifier to 
produce a numeric score to reflect the positive or 
negative certainty. Many classification algo-
rithms give such scores, e.g., SVM and logistic 
regression. The three methods are as follows:  
1)  ScoreSum: The learned model first classifies 
all test cases in T. Then for each test case tk, 
this method sums up all scores of positive 
classifications from the three views. It then 
assigns tk to the author with the highest score. 
2)  ScoreSqSum: This method works similarly to 
ScoreSum above except that it sums up the 
squared scores of positive classifications. 
3)  ScoreMax: This method works similarly to the 
ScoreSum method as well except that it finds 
the maximum classification score for each test 
document. 
4 Experimental Evaluation  
We now evaluate the proposed method. We use 
logistic regression (LR) with L2 regularization 
(Fan et al, 2008) and the SVMmulticlass (SVM) 
system (Joachims, 2007) with its default settings 
as the classifiers.  
4.1 Experiment Setup 
We conduct experiments on the IMDb dataset 
(Seroussi et al, 2010). This data set contains the 
IMDb reviews in May 2009. It has 62,000 re-
views by 62 users (1,000 reviews per user). For 
each author/reviewer, we further split his/her 
documents into the labeled, unlabeled, and test 
sets. 1% of one author?s documents, i.e., 10 doc-
uments per author, are used as the labeled data 
for training, 79% are used as unlabeled data, and 
the rest 20% are used for testing. We extract and 
compute the character and lexical features direct-
ly from the raw data, and use the Stanford PCFG 
parser (Klein and Manning, 2003) to generate the 
grammar structures of sentences in each review 
for extracting syntactic features. We normalize 
each feature?s value to the [0, 1] interval by di-
viding the maximum value of this feature in the 
training set. We use the micro-averaged classifi-
cation accuracy as the evaluation metric. 
4.2 Baseline methods 
We use six self-training baselines and three co-
training baselines. Self-training in (Kourtis and 
Stamatatos, 2011) uses two different classifiers 
on one view, and co-training uses one classifier 
on two views. All baselines except CNG+SVM 
347
on the character view are our extensions. 
Self-training using CNG+SVM on character, 
lexical and syntactic views respectively: This 
gives three baselines. It self-trains two classifi-
ers from the character 3-gram, lexical, and syn-
tactic views using CNG and SVM classifiers 
(Kourtis and Stamatatos, 2011). CNG is a pro-
file-based method which represents the author 
as the N most frequent character n-grams of all 
his/her training texts. The original method ap-
plied only CNG and SVM on the character n-
gram view. Since our results show that its per-
formance is extremely poor, we are curious 
what the reason is. Can this be due to the clas-
sifier or to the view? In order to differentiate 
the effects of views and classifiers, we present 
two additional types of baselines. The first type 
is to extend CNG+SVM method to lexical and 
syntactic views as well. The second type is to 
extend CNG+SVM method by replacing CNG 
with LR to show a fair comparison with our 
framework.  
Self-training using LR+SVM on character, lexi-
cal, and syntactic views: This is the second 
type extension. It also gives us three baselines. 
It again uses the character, lexical and syntac-
tic view and SVM as one of the two classifiers. 
The other classifier uses LR rather than CNG.  
Co-training using LR on Char+Lex, Char+Syn, 
and Lex+Syn views: This also gives us three 
baselines. Each baseline co-trains two classifi-
ers from every two views of the character 3-
gram, lexical, and syntactic views. 
4.3 Results and analysis 
(1) Effects of learning algorithms  
We first evaluate the effects of learning algo-
rithms on tri-training. We use SVM and LR as 
the learners as they are among the best methods.  
Figure 2. Effects of SVM and LR on tri-training 
The effects of SVM and LR on tri-training are 
shown in Fig. 2. For the aggregation results, we 
draw the curves for ScoreSum. The results for 
other two stratigies are similar. It is clear that LR 
outperforms SVM by a large margin for tri-
training when the number of iterations (k) is 
small. One possible reason is that LR is more 
tolerant to over-fitting caused by the small 
number of training samples. Hence, we use LR 
for tri-training in all experiments. 
(2) Effects of aggregation strategies  
We show the effects of the three proposed 
aggregation strategies. Table 1 indicates that 
ScoreSum (SS) is the best.  
Table 1. Effects of three aggregation strategies: 
ScoreMax(SM), ScoreSum(SS), and ScoreSq-Sum(SQ) 
We also observe that both ScoreSum and 
ScoreSqSum (SQ) perform better than ScoreMax 
(SM) and all single view cases. This suggests 
that the decision made from a number of scores 
is much more reliable than that made from only 
one score. ScoreSum is our default strategy. 
(3) Effects of data augmenting strategies  
We now see the effects of data adding methods 
to augment the labeled set in Fig. 3.  
 
Figure 3. Effects of data augmenting methods on 
tri-training 
We use two strategies. One is our InterAdding 
approach and the other is the SelfAdding 
approach in (Kourtis and Stamatatos, 2011), as 
introduced in Section 3.1. We can see that by 
adding newly classified samples by two 
classifiers to the third view, tri-training gets 
better and better results rapidly. For example, the 
accuracy for k = 10 iterations grows from 61.24 
for SelfAdding to 78.82 for InterAdding, an 
absolute increase of 17.58%. This implies that by 
integrating more information from other views, 
learning can improve greatly.  
(4) Comparison with self-training baselines 
We show the results of CNG+SVM in Table 2. It 
is clear that CNG is almost unable to correctly 
k 
Single  View Results Aggregated Results 
Lex Char Syn SM SS SQ 
0 45.75 32.88 33.96 41.11 46.85 44.61 
10 74.63 66.05 56.99 73.41 78.82 76.41 
20 82.30 74.92 65.05 81.63 86.19 84.05 
30 86.86 79.12 68.85 85.29 89.69 87.74 
40 89.16 81.81 70.85 87.83 91.52 89.99 
50 90.56 83.14 72.06 89.11 92.58 91.17 
60 91.69 84.13 73.23 90.05 93.15 91.82 
348
classify any test case. Its accuracy is only 1.26% 
at the start. This directly leads to the failure of 
the self-training. The reason is that the other 
classifier SVM can augment nearly 0 documents 
from the unlabeled set. We also tuned the param-
eter N for CNG, but it makes little difference. 
k 
Self-Training on Char  Aggregated Results 
CNG SVM SM SS SQ 
0 1.26 33.22 32.35 32.47 27.00 
10 1.26 32.35 32.35 32.47 27.00 
20 1.26 32.35 32.35 32.47 27.00 
30 1.26 32.35 32.35 32.47 27.00 
40 1.26 33.60 33.60 33.69 29.07 
50 1.26 33.60 33.60 33.69 29.07 
60 1.27 33.54 33.60 33.69 29.07 
Table 2. Results for the CNG+SVM baseline 
To distinguish the effects of views from classi-
fiers, we conduct two more types of experiments. 
First, we apply CNG+SVM to the lexical and 
syntactic views. The results are even worse. Its 
accuracy drops to 0.58% and 1.21%, respectively. 
Next, we replace CNG with LR and apply 
LR+SVM to all three views. We only show their 
best results in Table 3, either on a single view or 
aggregation. The details are omitted due to space 
limitations. We can see significant improvements 
over their corresponding results of CNG+SVM. 
This demonstrates that the learning methods are 
critical to self-training as well.  
k Tri 
Train 
SelfTrain:CNG+SVM SelfTrain:LR+SVM 
Char lex Syn Char Lex Syn 
0 46.85 33.22 45.44 34.50 33.22 45.75 34.48 
10 78.82 32.47 45.44 34.50 62.56 73.78 51.94 
20 86.19 32.47 45.44 34.09 71.21 81.44 59.88 
30 89.69 32.47 45.44 34.09 75.21 84.68 63.70 
40 91.52 33.69 45.44 34.09 77.46 88.25 65.74 
50 92.58 33.69 45.44 34.09 78.64 88.25 67.45 
60 93.15 33.69 45.44 34.09 79.54 89.31 68.37 
Table 3. Self-training variations 
From Table 3, we can also see that our tri-
training approach outperforms all self-training 
baselines by a large margin. For example, the 
accuracy for LR+SVM on the lexical view is 
89.31%.Although this is the best for self-training, 
it is worse than 93.15% of tri-training.  
The reason that self-training does not work 
well in general is the following: When the train-
ing set is small, the available data may not reflect 
the true distribution of the whole data. Then clas-
sifiers will be biased and their classifications will 
be biased too. In testing, the biased classifiers 
will not have good accuracy. However, in tri-
training, and co-training, each individual view 
may be biased but the views are independent. 
Then each view is more likely to produce ran-
dom samples for the other views and thus reduce 
the bias of each view as the iterations progress.  
(5) Comparison with co-training baselines 
We now compare tri-training with co-training 
(Blum and Mitchell, 1998) in Table 4. Again, tri-
training beats co-training consistently. The best 
performance of co-training is 92.81% achieved 
on the character and lexical views after 60 itera-
tions. However, the accuracy is worse than that 
of tri-training. The key reason is that tri-training 
considers three views, while co-training uses on-
ly two. Also, the predictions by two classifiers 
are more reliable than those by one classifier. 
k Tri 
Train 
Co-Train 
Char+Lex Char+Syn Lex+Syn 
0 46.85 45.75 42.02 45.75 
10 78.82 78.84 75.89 78.85 
20 86.19 86.02 82.59 85.63 
30 89.69 89.32 85.77 88.98 
40 91.52 91.14 87.52 91.16 
50 92.58 92.19 88.46 92.02 
60 93.15 92.81 89.21 92.50 
Table 4. Co-training vs. tri-training 
In (Qian, et al, 2014), we systematically inves-
tigated the effects of learning methods and views 
using a special co-training approach with two 
views. Learning was applied on two views but 
the data augmentation method was like that in 
self-training. The best result there was 91.23%, 
worse than 92.81% of co-training here in Table 4, 
which is worse than 93.15% of Tri-Training.   
Overall, Tri-training performs the best and co-
training is better than self-training and co-self-
training. This indicates that learning on different 
views can better exploit the redundancy in texts 
to achieve superior classification results. 
5 Conclusion  
In this paper, we investigated the problem of au-
thorship attribution with very few labeled exam-
ples. A novel three-view tri-training method was 
proposed to utilize natural views of human lan-
guages, i.e., the character, lexical and syntactic 
views, for classification. We evaluated the pro-
posed method and compared it with state-of-the-
art baselines. Results showed that the proposed 
method outperformed all baseline methods.  
Our future work will extend the work by in-
cluding more views such as the stylistic and vo-
cabulary richness views. Additional experiments 
will also be conducted to determine the general 
behavior of the tri-training approach. 
Acknowledgements 
This work was supported in part by the NSFC 
projects (61272275, 61232002, 61379044), and 
the 111 project (B07037).  
349
References  
S. Argamon, C. Whitelaw,  P. Chase, S. R. Hota,  N. 
Garg, and S. Levitan. 2007. Stylistic text 
classification using functional lexical features. 
JASIST 58, 802?822 
 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In: COLT. pp. 
92?100  
 
J. Burrows. 1992. Not unless you ask nicely: The 
interpretative nexus between analysis and 
information. Literary and Linguistic Computing 
7:91-109. 
 
J. Burrows. 2007. All the way through: Testing for 
authorship in different frequency data. LLC 22, 
27?47 
  
R-E. Fan, K-W. Chang,   C-J. Hsieh,  X-R. Wang, and 
C-J. Lin. 2008. Liblinear: A library for large linear 
classification. JMLR 9, 1871?1874 
 
J. Diederich, J. Kindermann, E. Leopold, G. Paass, G. 
F. Informationstechnik, and D-S. Augustin. 2000. 
Authorship attribution with support vector 
machines. Applied Intelligence 19:109-123. 
 
M. Gamon. 2004. Linguistic correlates of style: 
authorship classification with deep linguistic 
analysis features. In COLING. 
 
N. Graham, G. Hirst, and B. Marthi. 2005. 
Segmenting documents by stylistic character. 
Natural Language Engineering, 11:397-415.  
 
J. Grieve. 2007. Quantitative authorship attribution: 
An evaluation of techniques. LLC 22:251-270. 
 
H. van Halteren, F. Tweedie, and H. Baayen. 1996. 
Outside the cave of shadows: using syntactic 
annotation to enhance authorship attribution.  
Literary and Linguistic Computing 11:121-132. 
 
H. van Halteren. 2007. Author verification by 
linguistic profiling: An exploration of the 
parameter space. TSLP 4, 1?17 
 
G. Hirst, and O. Feiguina. 2007. Bigrams of syntactic 
labels for authorship discrimination of short texts. 
LLC 22, 405?417  
 
N. Jindal and B. Liu. 2008. Opinion spam and 
analysis. In: WSDM. pp. 29?230 
 
T. Joachims. 2007. www.cs.cornell.edu/people 
/tj/svmlight/old/svmmulticlassv2.12.html  
 
S. Kim,  H. Kim,  T. Weninger,  J. Han, and H. D. 
Kim. 2011. Authorship classification: a 
discriminative syntactic tree mining approach. In: 
SIGIR. pp. 455?464 
  
D. Klein and C. D. Manning. 2003 Accurate 
unlexicalized parsing. In: ACL. pp. 423?430  
 
I. Kourtis and E. Stamatatos, 2011. Author 
identification using semi-supervised learning. In: 
Notebook for PAN at CLEF 2011  
 
J. Li, R. Zheng, and H. Chen. 2006. From fingerprint 
to writeprint. Communications of the ACM 49:76-
82. 
 
K. Luyckx and W. Daelemans, 2008. Authorship 
attribution and verification with many authors and 
limited data. In: COLING. pp. 513?520 
  
D. Madigan, A. Genkin, D. Lewis, A. Argamon, D. 
Fradkin, and L. Ye, 2005. Author Identification on 
the Large Scale. In CSNA. 
 
K. Nigam and R. Ghani. 2000. Analyzing the 
effectiveness and applicability  of co-training.  In 
Proc. of CIKM, pp.86?93  
 
T. Qian, B. Liu. 2013 Identifying Multiple Userids of 
the Same Author. EMNLP, pp. 1124-1135 
 
T. Qian, B. Liu, M. Zhong, G. He. 2014. Co-Training 
on Authorship Attribution with Very Few Labeled 
Examples: Methods. vs. Views. In SIGIR, to 
appear. 
 
R. Schwartz, O. Tsur, A. Rappoport, M. Koppel. 2013. 
Authorship Attribution of Micro-Messages. 
EMNLP. pp. 1880-1891 
 
Y. Seroussi, F. Bohnert and Zukerman,.2012. 
Authorship attribution with author-aware topic 
models. In: ACL. pp. 264?269  
 
Y. Seroussi,  I. Zukerman, and F. Bohnert. 2010. 
Collaborative inference of sentiments from texts. 
In: UMAP. pp. 195?206 
 
E. Stamatatos. 2007. Author identification using 
imbalanced and limited training texts. In: TIR. pp. 
237?241 
 
E. Stamatatos. 2009. A survey of modern authorship 
attribution methods. JASIST 60:538?556 
 
?. Uzuner and B. Katz. 2005. A comparative study of 
language models for book and author recognition. 
Proc. of the 2nd IJCNLP, 969-980. 
350
 Y. Zhao and J. Zobel. 2005. Effective and scalable 
authorship attribution using function words. In 
Proc. of Information Retrival Technology, 174-
189.  
 
R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A 
framework for authorship identification of online 
messages: Writing style features and classification 
techniques. JASIST 57:378-393. 
351
Soochow University: Description and Analysis of the Chinese 
Word Sense Induction System for CLP2010 
Hua Xu   Bing Liu   Longhua Qian?   Guodong Zhou 
Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
Email: 
{20094227034,20084227065055,qianlonghua,gdzhou}@suda.edu.cn
 
                                                 
? Corresponding author 
Abstract 
Recent studies on word sense induction 
(WSI) mainly concentrate on European 
languages, Chinese word sense induction 
is becoming popular as it presents a new 
challenge to WSI. In this paper, we 
propose a feature-based approach using 
the spectral clustering algorithm to this 
problem. We also compare various 
clustering algorithms and similarity 
metrics. Experimental results show that 
our system achieves promising 
performance in F-score. 
1 Introduction 
Word sense induction (WSI) is an open problem 
of natural language processing (NLP), which 
governs the process of automatic discovery of 
the possible senses of a word. WSI is similar to 
word sense disambiguation (WSD) both in 
methods employed and in problem encountered. 
In the procedure of WSD, the senses are as-
sumed to be known and the task focuses on 
choosing the correct one for an ambiguous word 
in a context. The main difference between them 
is that the task of WSD generally requires large-
scale manually annotated lexical resources while 
WSI does not. As WSI doesn?t rely on the 
manually annotated corpus, it has become one of 
the most important topics in current NLP re-
search (Pantel and Lin, 2002; Neill, 2002; Rapp, 
2003). Typically, the input to a WSI algorithm is 
a target word to be disambiguated. The task of 
WSI is to distinguish which target words share 
the same meaning when they appear in different 
contexts. Such result can be at the very least 
used as empirically grounded suggestions for 
lexicographers or as input for WSD algorithm. 
Other possible uses include automatic thesaurus 
or ontology construction, machine translation or 
information retrieval. Compared with European 
languages, the study of WSI in Chinese is scarce. 
Furthermore, as Chinese has its special writing 
style and Chinese word senses have their own 
characteristics, the methods that work well in 
English may not perform effectively in Chinese 
and the usefulness of WSI in real-world applica-
tions has yet to be tested and proved. 
The core idea behind word sense induction is 
that contextual information provides important 
cues regarding a word?s meaning. The idea dates 
back to (at least) Firth (1957) (?You shall know 
a word by the company it keeps?), and under-
lies most WSD and lexicon acquisition work to 
date. For example, when the adverb phrase oc-
curring prior to the ambiguous word????, 
then the target word is more likely to be a verb 
and the meaning of which is ?to hold something?; 
Otherwise, if an adjective phrase locates in the 
same position, then it probably means ?confi-
dence? in English. Thus, the words surrounds 
the target word are main contributor to sense 
induction. 
The bake off task 4 on WSI in the first CIPS-
SIGHAN Joint Conference on Chinese Lan-
guage Processing (CLP2010) is intended to 
promote the exchange of ideas among partici-
pants and improve the performance of Chinese 
WSI systems. Generally, our WSI system also 
adopts a clustering algorithm to group the con-
texts of a target word. Differently, after generat-
ing feature vectors of words, we compute a simi-
larity matrix with each cell denoting the similar-
ity between two contexts. Furthermore, the set of 
similarity values of a context with other contexts 
is viewed as another kind of feature vector, 
which we refer to as similarity vector. Both fea-
ture vectors and similarity vectors can be sepa-
rately used as the input to clustering algorithms. 
Experimental results show our system achieves 
good performances on the development dataset 
as well as on the final test dataset provided by 
the CLP2010. 
2 System Description 
This section sequentially describes the architec-
ture of our WSI system and its main components. 
2.1 System Architecture 
Figure 1 shows the architecture of our WSI 
system. The first step is to preprocess the raw 
dataset for feature extraction. After that, we 
extract ?bag of words? from the sentence 
containing a target word (feature extraction) and 
transform them into high-dimension vectors 
(feature vector generation). Then, similarities of 
every two vectors could be computed based on 
the feature vectors (similarity measurement). the 
similarities of an instance can be viewed as 
another vector?similarity vector. Both feature 
vectors and similarity vectors can be served as 
the input for clustering algorithms. Finally, we 
perform three clustering algorithms, namely, k-
means, HAC and spectral clustering.  
Dataset
Preprocess
Feature
Extraction
Vector
Generation
Similarity
Measurement
Similarity
As VectorClustering
WSI
Results
 
Figure 1  Architecture of our Chinese 
WSI system 
2.2 Feature Engineering 
In the task of WSI, the target words with their 
topical context are first transformed into multi-
dimensional vectors with various features, and 
then applying clustering algorithm to detect the 
relevance of each other. 
Corpus Preprocessing 
For each raw file, we first extract each sentence 
embedded in the tag <instance>, including 
the <head> and </head> tags which are used 
to identify the ambiguous word. Then, we put all 
the sentences related to one target word into a 
file, ordered by their instance IDs. The next step 
is word segmentation, which segments each sen-
tence into a sequence of Chinese words and is 
unique for Chinese WSI. Here, we use the soft-
ware from Hylanda1 since it is ready to use and 
considered an efficient word segmentation tool. 
Finally, since we retain the <head> tag in the 
sentence, the <head> and </head> tags are 
usually separated after word segmentation, thus 
we have to restore them in order to correctly lo-
cate the target word during the process of feature 
extraction. 
Feature Extraction 
After word segmentation, for a context of a par-
ticular word, we extract all the words around it 
in the sentence and build a feature vector based 
on a ?bag-of-words? Boolean model. ?Bag-of-
words? means that we don?t consider the order 
of words. Meanwhile, in the Boolean model, 
each word in the context is used to generate a 
feature. This feature will be set to 1 if the word 
appears in the context or 0 if it does not. Finally, 
we get a number of feature vectors, each of them 
corresponds to an instance of the target word. 
One problem with this feature-based method is 
that, since the size of word set may be huge, the 
dimension is also very high, which might lead to 
data sparsity problem.  
Similarity measurement 
One commonly used metric for similarity meas-
urement is cosine similarity, which measures the 
angle between two feature vectors in a high-
dimensional space. Formally, the cosine similar-
ity can be computed as follows: 
cos ,ine similarity ?< > = ?
x yx y
x y
 
where ,x y are two vectors in the vector space 
and x , y are the lengths of  ,x y  respectively. 
                                                 
1 http://www.hylanda.com/
Some clustering algorithms takes feature vec-
tors as the input and use cosine similarity as the 
similarity measurement between two vectors. 
This may lead to performance degradation due 
to data sparsity in feature vectors. To avoid this 
problem, we compute the similarities of every 
two vectors and generate an  similarity 
matrix, where  is the number of all the in-
stances containing the ambiguous word. Gener-
ally, is usually much smaller than the dimen-
sion size and may alleviate the data sparsity 
problem. Moreover, we view every row of this 
matrix (i.e., an ordered set of similarities of an 
instance with other instances) as another kind of 
feature vector. In other words, each instance it-
self is regarded as a feature, and the similarity 
with this instance reflects the weight of the fea-
ture. We call this vector similarity vector, which 
we believe will more properly represent the in-
stance and achieve promising performance. 
*N N
N
N
2.3 Clustering Algorithm 
Clustering is a very popular technique which 
aims to partition a dataset into such subgroups 
that samples in the same group share more simi-
larities than those from different groups. Our 
system explores various cluster algorithms for 
Chinese WSI, including K-means, hierarchical 
agglomerative clustering (HAC), and spectral 
clustering (SC). 
K-means (KM) 
K-means is a very popular method for general 
clustering used to automatically partition a data 
set into k groups. K-means works by assigning 
multidimensional vectors to one of K clusters, 
where is given as a priori. The aim of the al-
gorithm is to minimize the variance of the vec-
tors assigned to each cluster.  
K
K-means proceeds by selecting k  initial clus-
ter centers and then iteratively refining them as 
follows: 
(1) Choose cluster centers to coincide with 
k randomly-chosen patterns or k  ran-
domly defined points. 
k
(2) Assign each pattern to the closest cluster 
center. 
(3) Recompute the cluster centers using the 
current cluster memberships. 
(4) If a convergence criterion is not met, go 
to step 2. 
Hierarchical Agglomerative Clustering (HAC) 
Different from K-means, hierarchical clustering 
creates a hierarchy of clusters which can be 
represented in a tree structure called a 
dendrogram. The root of the tree consists of a 
single cluster containing all objects, and the 
leaves correspond to individual object.  
Typically, hierarchical agglomerative 
clustering (HAC) starts at the leaves and 
successively merges two clusters together as 
long as they have the shortest distance among all 
the pair-wise distances between any two clusters.  
Given a specified number of clusters, the key 
problem is to determine where to cut the hierar-
chical tree into clusters. In this paper, we gener-
ate the final flat cluster structures greedily by 
maximizing the equal distribution of instances 
among different clusters. 
Spectral Clustering (SC) 
Spectral clustering refers to a class of techniques 
which rely on the eigen-structure of a similarity 
matrix to partition points into disjoint clusters 
with points in the same cluster having high simi-
larity and points in different clusters having low 
similarity.  
Compared to the ?traditional algorithms? such 
as K-means or single linkage, spectral clustering 
has many fundamental advantages. Results ob-
tained by spectral clustering often outperform 
the traditional approaches, spectral clustering is 
very simple to implement and can be solved ef-
ficiently by standard linear algebra methods. 
3 System Evaluation 
This section reports the evaluation dataset and 
system performance for our feature-based Chi-
nese WSI system. 
3.1  Dataset and Evaluation Metrics 
We use the CLP2010 bake off task 4 sample 
dataset as our development dataset. There are 
2500 examples containing 50 target words and 
each word has 50 sentences with different mean-
ings. The exact meanings of the target words are 
blind, only the number of the meanings is pro-
vided in the data. We compute the system per-
formance with the sample dataset because it con-
tains the answers of each candidate meaning. 
The test dataset provided by the CLP2010 is 
similar to the sample dataset. It contains 100 
target words and 5000 instances in total. How-
ever, it doesn?t provide the answers. 
The F-score measurement is the same as Zhao 
and Karypis (2005). Given a particular 
class rL of size and a particular cluster  of 
size , suppose  in the cluster  belong to
rn iS
in irn iS rL , 
then the value of this class and cluster is de-
fined to be 
F
2 ( , ) ( ,( , )
( , ) ( , )
r i r i
r i
r i r i
)R L S P L SF L S
R L S P L S
? ?= +  
( , ) /r i ir rR L S n n=  
( , ) /r i ir iP L S n n=  
where ( , )r iR L S is the recall value and  
is the precision value. The F-score of class 
( , )r iP L S
rL is 
the maximum value and F-score value follow: F
( ) max ( , )
ir S r i
F score L F L S? =  
1
( )
c
r
r
r
nF score F score L
n=
? = ??  
where  is the total number of classes and n  is 
the total size. 
c
3.2 Experiment Results 
Table 1 reports the F-score of our feature-based 
Chinese WSI for different feature sets with 
various window sizes using K-means clustering. 
Since there are different results for each run of 
K-means clustering algorithm, we perform 20 
trials and compute their average as the final 
results. The columns denote different window 
size n, that is, the n words before and after the 
target word are extracted as features. Particularly, 
the size of infinity (?) means that all the words 
in the sentence except the target word are 
considered. The rows represent various 
combinations of feature sets and similarity 
measurements, currently, four of which are 
considered as follows: 
F-All: all the words are considered as features 
and from them feature vectors are constructed. 
F-Stop: the top 150 most frequently occurring 
words in the total ?word bags? of the corpus are 
regarded as stop words and thus removed from 
the feature set. Feature vectors are then formed 
from these words. 
S-All: the feature set and the feature vector 
are the same as those of F-All, but instead the 
similarity vector is used for clustering (c.f. Sec-
tion 2.2). 
S-Stop: the feature set and the feature vector 
are the same as those of F-Stop, but instead the 
similarity vector is used for clustering. 
Table 1 Experimental results for differ-
ent feature sets with different window sizes us-
ing K-means clustering 
 
This table shows that S-Stop achieves the best 
performance of 0.7320 in F-score. This suggests 
that for K-means clustering, Chinese WSI can 
benefit much from removing stop words and 
adopting similarity vector. It also shows that: 
Feature/ 
Similarity 3 7 10 ? 
F-All 0.5949 0.6199 0.6320 0.6575
F-Stop 0.6384 0.6500 0.6493 0.6428
S-All 0.5856 0.6044 0.6186 0.6843
S-Stop 0.6532 0.6696 0.6804 0.7320
z As the window size increases, the perform-
ance is almost consistently enhanced. This 
indicates that all the words in the sentence 
more or less help disambiguate the target 
word. 
z Removing stop words consistently improves 
the F-score for both similarity metrics. This 
means some high frequent words do not help 
discriminate the meaning of the target words, 
and further work on feature selection is thus 
encouraged. 
z Similarity vector consistently outperforms 
feature vector for stop-removed features, but 
not so for all-words features. This may be 
due to the fact that, when the window size is 
limited, the influence of frequently occur-
ring stop words is relatively high, thus the 
similarity vector misrepresent the context of 
the target word. On the contrary, when stop 
words are removed or the context is wide, 
the similarity vector can better reflect the 
target word?s context, leading to better per-
formance. 
In order to intuitively explain why the simi-
larity vector is more discriminative than the fea-
ture vector, we take two sentences containing 
the Chinese word ???? (hold, grasp) as an ex-
ample (Figure 2). These two sentences have few 
common words, so clustering via feature vectors 
puts them into different classes. However, since 
the similarities of these two feature vectors with 
other feature vectors are much similar, cluster-
ing via similarity vectors group them into the 
same class.  
 
Figure 2  An example from the dataset 
 
According to the conclusion of the above ex-
periments, it is better to include all the words 
except stop words in the sentence as the features 
in the subsequent experiment. Table 2 lists the 
results using various clustering algorithms with 
this same experimental setting. It shows that the 
spectral clustering algorithm achieves the best 
performance of 0.7692 in F-score for Chinese 
WSI using the S-All setup. Additionally, there 
are some interesting findings: 
mi-
 of 
han 
ing 
this 
nto 
lus-
lly 
er-
re. 
ex-
der 
ers the density information, therefore S-All 
will not significantly improve the perform-
ance. 
 
Feature/ 
Similarity 
KM HAC SC 
F-All 0.6428 0.6280 0.7686 
S-All 0.7320 0.6332 0.7692 
Table 2 Experiments results using dif-
ferent clustering algorithms 
<lexelt item="??" snum="4"> 
<instance id="0012"> 
???????????????????
?????????????????
<head>??</head>???????????
?? 
 </instance>  
<instance id="0015">  
???????????????????
???????????????<head>?
?</head>???????????????
??????????????????? 
</instance>  
</lexelt> 
3.3 Final System Performance 
For the CLP2010 task 4 test dataset which con-
tains 100 target words and 5000 instances in to-
tal, we first extract all the words except stop 
words in a sentence containing the target word, 
then produce the feature vector for each context 
and generate the similarity matrix, finally we 
perform the spectral cluster algorithm. Probably 
because the distribution of the target word in the 
test dataset is different from that in the develop-
ment dataset, the F-score of our system on the 
test dataset is 0.7108, about 0.05 units lower 
than that we got on the sample dataset. 
4 Conclusions and Future Work 
In our Chinese WSI system, we extract all the 
words except stop words in the sentence, con-
struct feature vectors and similarity vectors, and 
apply the spectral clustering algorithm to this 
problem. Experimental results show that our 
simple and efficient system achieve a promising 
result. Moreover, we also compare various clus-
tering algorithms and similarity metrics. We find 
that although the spectral clustering algorithm 
outperforms other clustering algorithms, the K-
means clustering with similarity vectors can also 
achieve comparable results. 
For future work, we will incorporate more 
linguistic features, such as base chunking, parse 
tree feature as well as dependency information 
into our system to further improve the perform-
ance. 
Acknowledgement 
This research is supported by Project 60873150, 
60970056 and 90920004 under the National 
Natural Science Foundation of China. We would z Although SC performs best, KM with si
larity vectors achieves comparable results
0.7320 units in F-score, slightly lower t
that of SC. 
z HAC performs worst among all cluster
algorithms. An observation reveals that 
algorithm always groups the instances i
highly skewed clusters, i.e., one or two c
ters are extremely large while others usua
have only one instance in each cluster. 
z It is surprising that S-All slightly outp
forms F-All by only 0.0006 units in F-sco
The truth is that, as discussed in the first 
periment, KM using F-All doesn?t consi
instance density while S-All does. On the 
contrary, SC identifies the eign-structure in 
the instance space and thus already consid-
also like to thank other contributors in the NLP 
lab at Soochow University. 
References 
Jain A, Murty M. 1999.Flynn P. Data clustering : A 
Review [J]. ACM Computing Surveys,1999,31 
(3) :2642323 
F. Bach and M. Jordan.2004. Learning spectral clus-
tering. In Proc. of NIPS-16. MIT Press, 2004. 
Samuel Brody and Mirella Lapata. 2009. Bayesian 
word sense induction. In Proceedings of the 12th 
Conference of the European Chapter of the ACL 
(EACL 2009), pages 103?111. 
Neill, D. B. 2002. Fully Automatic Word Sense In-
duction by Semantic Clustering. Cambridge Uni-
versity, Master?s Thesis, M.Phil. in Computer 
Speech. 
Agirre, E. and Soroa, A. 2007.  Semeval-2007 task 02: 
Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations:7-12 
Ioannis P. Klapaftis and Suresh Manandhar. 2008. 
Word sense induction using graphs of collocations. 
In Proceedings of the 18th European Conference 
On Artificial Intelligence (ECAI-2008), Patras, 
Greece, July. IOS Press. 
Kannan, R., Vempala, S and Vetta, A. 2004. On clus-
terings: Good, bad and spectral. J. ACM, 51(3), 
497?515. 
Reinhard Rapp.2004. A practical solution to the 
problem of automatic word sense induction. Pro-
ceedings of the ACL 2004 on Interactive poster 
and demonstration sessions, p.26-es, July 21-26, 
2004, Barcelona, Spain 
Bordag, S. 2006. Word sense induction: Triplet-based 
clustering and automatic evaluation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics (EACL, Trento, Italy). 137--144. 
Ying Zhao, and George Karypis.2005. Hierarchical 
Clustering Algorithms for Document Datasets. Da-
ta Mining and Knowledge Discovery, 10, 141?168. 
 
