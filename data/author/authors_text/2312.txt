A cross-comparison of two clustering methods
Olivier Ferret
CEA Saclay
DTI/SITI
91191 Gif-sur-Yvette Cedex
ferret@sphinx.cea.fr
Brigitte grau and Miche`le Jardino
LIMSI CNRS
BP133
91403 Orsay, France
bg,jardino@limsi.fr
Abstract
Many Natural Language Processing ap-
plications require semantic knowledge
about topics in order to be possible or to
be efficient. So we developed a system,
SEGAPSITH, that acquires it automat-
ically from text segments by using an
unsupervised and incremental cluster-
ing method. In such an approach, an
important problem consists of the vali-
dation of the learned classes. To do that,
we applied another clustering method,
that only needs to know the number
of classes to build, on the same sub-
set of text segments and we reformu-
late our evaluation problem in compar-
ing the two classifications. So, we es-
tablished different criteria to compare
them, based either on the words as class
descriptors or on the thematic units.
Our first results lead to show a great
correlation between the two classifica-
tions.
1 Introduction
Among all the applications in Natural Language
Processing (NLP), many require semantic knowl-
edge about topics in order to be possible or to
be efficient. These applications are, for exam-
ple, topic segmentation and identification or text
classification. As this kind of knowledge is not
easy to build manually, we developed a system,
SEGAPSITH (Ferret and Grau, 1998a), (Fer-
ret and Grau, 1998b), to acquire it automatically.
In this field, there are two classes of approaches.
Supervised learning that requires to know a pri-
ori which topics have to be learned and to pos-
sess a tagged corpus as a learning set. It is the
approach generally adopted by the different sys-
tems, as those participating to TREC or TDT.
However, we wanted to design a system allow-
ing us to work in open domain, without any re-
striction about the subjects to be represented and,
thus, to be recognized in texts. SEGAPSITH
is grounded on an unsupervised and incremental
learning based on a conceptual clustering method.
After a thematic segmentation of the texts that
divides a text in segments made of lemmatized
words, i. e. thematic units, the system aggregates
similar enough thematic units. Aggregation con-
sists of regrouping all the words of the different
similar units and associating to them a weight ac-
cording to their occurrence number. This weight
represents the importance of a word relative to the
described topic. The incremental aspect allows us
to augment topic knowledge by treating succes-
sive corpora without reconsidering the knowledge
already existing.
In such an approach, an important problem
consists of the validation of the learned classes.
As we do not possess an existing classifica-
tion that agrees with the granularity level of our
classes, we decided to accomplish this evaluation
by using a second classification method on the
same data and by comparing their results. This
second method is an entropy-based method, and
requires to know the number of classes to form.
So, if both results are similar enough, although
the methods applied are different, we could con-
clude that the learned classes are quite relevant
and that the two methods are efficient.
After applying the second method, we pos-
sess two sets composed by the same number of
classes. Each class regroups thematic units and
is described by a set of words. So, we established
different criteria to compare them, based either on
the words as class descriptors or on the thematic
units they gather. After the presentation of the
two methods, we shall present our tests and the
first results we obtained.
2 Semantic domain learning
This description aims at showing the data used for
learning, and the specificity of the learned classes.
2.1 The thematic segmentation:
SEGCOHLEX
Studied texts are newspaper articles coming from
two corpora: ?Le Monde? and ?AFP?. Some of
these texts have been used to build a lexical net-
work where links between two words represent an
evaluation of their mutual information to capture
semantic and pragmatic relations between them,
computed from their co-occurrence count. In or-
der to build class of words linked to a same topic,
we first realize a topic segmentation of the texts
in thematic units (TU) whose words refer to the
same topic, and learning is applied on these the-
matic units.
Text segmentation is based on the use of the
collocation network. A topic is detected by com-
puting a cohesion value for each word resulting
from the relations found in the network between
these words and their neighbors in a text. As in
Kozima?s work (Kozima, 1993), this computa-
tion operates on words belonging to a focus win-
dow that is moved all over the text.The cohe-
sion values lead to build a graph and by succes-
sive transformations applied to it, texts are auto-
matically divided in discourse segments. Such a
method leads to delimit small segments, whose
size is equivalent to a paragraph, i. e. capa-
ble of retrieving topic variations in short texts,
as newswires for example. Table 1 shows an ex-
tract of the words belonging to a cohesive seg-
ment about a dedication of a book.
2.2 Semantic Domain learning in
SEGAPSITH
Learning a semantic domain consists of aggre-
gating all the most cohesive thematic units, TUs,
that are related to a same subject, i. e. a same
kind of situation. We only retain segments whose
cohesion value is higher than a threshold, in or-
der to ground our learning on the more reliable
units. Similarity between a thematic unit and
a semantic domain is evaluated from their com-
mon words. When the similarity value exceeds a
threshold, the thematic unit is aggregated to the
semantic domain, otherwise a new domain is cre-
ated. Each aggregation of a new thematic unit in-
creases the system?s knowledge about one topic
by reinforcing recurrent words and adding new
ones. Weights on words represent their impor-
tance relative to the topic and are computed from
the number of occurrences of these words in the
TUs.
Units related to a same topic are found in dif-
ferent texts and often develop different points of
view of a same type of subject. To ensure a better
similarity between them, SEGAPSITH enriches
a particular description given by a text segment
by adding to these units those words of the collo-
cation network that are particularly linked to the
words found in the segment. Table 2 gives an ex-
tract of the words added to the segment of Table
1.
This method leads SEGAPSITH to learn spe-
cific topic representations (see Table 3) as op-
posed to (Lin, 1997) for example, whose method
builds general topic descriptions as for economy,
sport, etc. Moreover, it does not depend on any a
priori classification of the texts.
We applied the learning module of SEGAP-
SITH on one month (May 1994) of AFP
newswires, corresponding to 7823 TUs. In our
experiments (Ferret and Grau, 1998a), (Ferret
and Grau, 1998b), we showed that domains reach
a stability at 15 to 20 aggregations, and that words
having a weight below 0.1 are rarely related to the
domain. Thus, we only selected domains result-
ing from at least 15 aggregations for our cross-
comparison, i.e. 71 domains regrouping 4935
TUs and 4380 words having a weight upon 0.1.
A lot of domains share common words, and are
close enough to be considered as different repre-
sentations of specific points of view of a general
topic, as economy, sport, etc.
3 Entropy-based clustering
The second clustering method gives an optimal
partition of the 4935 thematic units in 71 non-
words weight words weight
strider 0.683 entourer (to surround) 0.368
toward 0.683 signature (signature) 0.366
de?dicacer (to dedicate) 0.522 exemplaire (exemplar) 0.357
apposer (to append) 0.467 page (page) 0.332
pointu (sharp-pointed) 0.454 train (train) 0.331
relater (to relate) 0.445 centaine (hundred) 0.330
boycottage (boycotting) 0.436 sentir (to feel) 0.328
autobus (bus) 0.435 livre (book) 0.289
enfoncer (to drive in) 0.410 personne (person) 0.267
Table 1: Extract of a segment about a dedication
inferred words weight inferred words weight
paraphe (paraph) 0.522 imprimerie (press) 0.418
presse parisien (parisian-press) 0.480 e?diter (to publish) 0.407
best seller (best seller) 0.477 biographie (biography) 0.406
maison d?e?dition (publishing house) 0.450 librairie (bookshop) 0.405
libraire (bookseller) 0.447 poche (pocket) 0.389
tome (tome) 0.445 e?diteur (publisher) 0.363
Grasset (a publisher) 0.440 lecteur (reader) 0.355
re?e?diter (to republish) 0.428 israe?lien (Israeli) 0.337
parution (appearance) 0.427 e?dition (publishing) 0.333
Table 2: Extract of words selected in the collocation network for the segment of Table 1
words occurrences weight
juge d?instruction (examining judge) 58 0.501
garde a` vue (police custody) 50 0.442
bien social (public property) 46 0.428
inculpation (charging) 49 0.421
e?crouer (to imprison) 45 0.417
chambre d?accusation (court of criminal appeal) 47 0.412
recel (receiving stolen goods) 42 0.397
pre?sumer (to presume) 45 0.382
police judiciaire (criminal investigation department) 42 0.381
escroquerie (fraud) 42 0.381
Table 3: The most representative words of a domain about justice
overlapping clusters according to the word dis-
tributions in the units. It is realized with an al-
gorithm which looks like K-means (here K=71).
Each cluster is the merge of several thematic units
and is represented by its centroid. We search
for the partition which minimizes the Kullback-
Leibleir divergence (Cover and Thomas, 1991)
between the word distributions of the thematic
units and those of of their centroids. This entropy-
based measure is convex (Jardino, 2000), this pro-
priety permits to get an optimal partition whatever
the initial conditions.
3.1 Entropy
We assume that each thematic unit is represented
by one quantitative vector whose components are
the relative occurrences of a selection of words re-
lated to the unit. The advantages of this normal-
ization is that the representation of the thematic
units does not depend on the length of the units
and can be modelized in the frame of the infor-
mation theory (Cover and Thomas, 1991).
Assuming that O
w;tu
is the occurrence of the
word labelled w in the thematic unit labelled tu
and that O
tu
is the occurrence of all the words in
the thematic unit tu, such that O
tu
=
P
w
O
w;tu
,
each thematic unit vector component , p(w=tu),
is :
p(w=tu) =
O
w;tu
O
tu
(1)
When the thematic units are unclassed, their
entropy is given by (Cover and Thomas, 1991):
H
TU
=  
X
w;tu
p(w; tu)  ln[p(wjtu)] (2)
with p(w; tu) = Ow;tuP
w;tu
O
w;tu
When the thematic units are gathered in K clus-
ters, labelled k, the cluster entropy is, H
K
:
H
K
=  
X
w;k
p(w; k)  ln[p(wjk)] (3)
where p(wjk) is defined as :
p(wjtu 2 k) = p(wjk) =
O
w;k
O
k
(4)
with O
w;k
=
P
tu2k
O
w;tu
, O
k
=
P
tu2k
O
tu
and
p(w; k) =
O
w;k
P
w;k
O
w;k
The cluster entropy is always higher than or
equal to the unit entropy (log-sum rule (Cover and
Thomas, 1991)), so that the Kullback-Leibleir di-
vergence defined as:
D
KL
= H
K
 H
TU
(5)
is always higher than or equal to 0.
3.2 Clustering algorithm
Minimizing the Kullback-Leibler divergence
amounts to minimize the entropy H
K
because
H
TU
does not depend on the clusters.
The number of possible partitions is huge,
roughly 493571. We have observed that a random
search is faster than a systematic one (Jardino,
2000), and we have used this paradigm to build
the algorithm described below:
1- Define a priori, K, the cluster number, here
K=71.
2- Initialize: put all the thematic units in one clus-
ter, calculate the entropy H
K
(equation 3). The
remaining K-1 clusters are empty.
3- Do the random selection of one thematic unit
and of another cluster for this unit.
4- Move the unit from its former cluster to the new
randomly selected one, calculate the new entropy.
5- If the new entropy is lower, leave the unit in its
new cluster, otherwise move it back to its initial
cluster.
6- Repeat 3 to 5 until there is no more change.
The optimal clustering of the 4935 thematic
units in 71 clusters is performed on a workstation
(SGI Indy) within twenty minutes.
4 Comparing two classifications
We established different criteria for comparing
the two classifications, based on the elements
used to describe the classes. First, each class is a
set of words with an occurrence number for each
of them; second it is also a set of thematic units.
So, the comparison can be done along these two
points of view.
In order to evaluate the overlapping of the
classes of words, we applied each classifica-
tion method on the two classification results:
the classes of words resulting from the second
method are classified relative to the semantic do-
mains. For comparing the classes of TUs, we ap-
plied the entropy measure on one hand to measure
the overlapping of the classes, and  and Mantel
tests on the other hand to evaluate the differences
in the repartition of all the TUs.
4.1 The word point of view
4.1.1 Classification by similarity
The classification of the clusters relative to the
semantic domains exploits the same similarity
measure than the one used for the learning phase.
In a first step, some domains are selected accord-
ing to the value of the activ function:
activ(d) =
X
i
W
d;i
W
c;i
(6)
where W
d;i
is the weight of the word i in the
semantic domain d and W
c;i
is the weight of the
same word in the cluster c. This first step was
used in the learning phase because the number of
semantic domains was increasing rapidly and this
measure leads to a first fast selection of interesting
domains before evaluating an in-depth similarity.
We kept this step, even if it was not necessary,
in order to apply exactly the same method in the
evaluation phase. Afterwards, each selected do-
main can be compared to the cluster by using the
similarity measure given below. If one of these
similarity values is greater than a given threshold,
fixed to 0.25 in our tests, the cluster is linked to
the domain that is the most similar to it. The sim-
ilarity measure is:
sim(d; c) =
4
r
P
w
W
d;w
P
t
W
d;t
P
w
O
d;w
P
t
O
d;t
P
w
W
c;w
P
t
W
c;t
P
w
O
c;w
P
t
O
cl;t
(7)
where the w index is used for indicating com-
mon words between the cluster c and the seman-
tic domain d and the t index, for indicating all
the words of the cluster or the domain. W is the
weight of a word and O its occurrence number.
The similarity measure is only based on the
common words. As learning is unsupervised and
incremental, differences at time t might disappear
at time t+1. The comparison is based on the pro-
portion of common words relative to the total of
words of each entity to be compared. The evalu-
ation of the common words in each entity is done
according to their occurrence number and their
weight. So, we avoid to obtain a high similarity
value between two entities that only share very
few words having a high weight. We combine
these criteria in a geometrical mean for evaluating
each entity and for computing the global similar-
ity from the evaluation of the two entities in order
to smooth the effect of few recurrent words when
the domains are in their formation phase, words
that would act as attractors otherwise.
4.1.2 Entropy-based classification
For each of the 71 clusters, we have searched
for the nearest domain obtained with the same
kind of entropy-based measure defined above. We
assume that we have a probabilistic model which
gives the predictions of the words according to the
domains. In order to avoid the null value, non-
learned events are infered using the Witten-Bell
interpolation (Witten and Bell, 1991). The inter-
polated value of the prediction of a word w, know-
ing the domain d is p0(wjd) such that:
p
0
(wjd) =
O(w; d) + n
sw
(d)=V
O(d) + n
sw
(d)
(8)
where n
sw
(d) is the number of words seen in each
domain and V the size of the vocabulary. Each
cluster is also defined by a set of words and we
compare the distribution of the words in the clus-
ter with the distributions of the words in the do-
mains (equation 8) with the Kullback-Leibler di-
vergence. Each cluster is associated with the near-
est domain.
4.1.3 Comparison
The results of the two classifications described
above are given in Table 4. For the similarity-
based classification, only 3 clusters do not match
with any domain and 47 different domains are se-
lected for the 68 remaining clusters with 34 links
that are one cluster-one domain. For the entropy-
based classification, 44 clusters have been associ-
ated to the 71 domains.
Several clusters are linked to the same domain.
This can been explained by the closeness of some
of the domains. This is shown when they are hi-
erarchically classified; we obtain then 34 general
domains that regroup 1 to 5 domains each. We
also observe that most clusters are only linked to
one domain. The two methods give almost the
same results and show that the two classifications
are similar.
domain)cluster links links
link (similarity) (entropy)
no link 3
1!1 34 29
1!2 8 8
1!3 3 4
1!4 1 2
1!5 1
1!6 1
Table 4: Number of links between one domain
and the clusters
4.2 The TU point of view
4.2.1 A simple comparison
One domain and one cluster are associated to
each thematic unit. It is then possible to calcu-
late the number of domains and clusters which
partially or fully overlap. Table 5 represents the
intersection between the two partitions. For each
domain we chose the cluster which has the highest
intersection with the domain. Then we calculated
the percentage of thematic units of this domain
which are both in the domain and in this chosen
cluster.
coverage number of clusters
cov=100% 8
80%cov?100% 16
60%cov?80% 19
40%cov?60% 19
20%cov?40% 8
cov<20% 1
Table 5: Coverage rates of UT which are common
to each domain and those of the associated clus-
ters which correspond to the highest intersection
Height domains are identical to height clusters.
The lowest coverage (18%) is obtained for one
domain. The other seventy domains cover more
than 20% of the clusters.
4.2.2 Comparison with the  coefficient
In order to compare more precisely our two
classifications, we used the  coefficient as it was
done by Dietterich in (Dietterich, 2000) and as it
is often done in the field of remote sensing for ex-
ample. The  coefficient measures the degree of
agreement among several judgements and is ex-
pressed as follows:
k =

1
  
2
1  
2
(9)
where 
1
is the proportion of times that the
judgments agree and 
2
is the proportion of times
that we could expect the judgments to agree by
chance. As we are in a case of unsupervised clas-
sification whereas Dietterich?s work was about
supervised classification (building of decision
trees), we have first set a one-to-one mapping
between the semantic domains and the clusters.
This was done by a very simple procedure: we
computed the size of the intersection between
each cluster and each domain; then we iteratively
mapped the cluster and the domain that had the
largest intersection until each cluster was mapped
with a domain. Of course, this is not an optimal
procedure in order to ensure that the intersection
of each couple of classes is the largest one but it
can be considered as a baseline.
Then, the evaluation of the  coefficient was
done by building a matrix K  K, with K, the
number of classes (clusters or domains), such that
each element k
i;j
is equal to the number of TUs
assigned to the class i by SEGAPSITH and to the
class j by the entropy-based clustering. 
1
, which
estimates the probability that the two classifica-
tions agree, is defined by:

1
=
P
K
i=1
k
i;i
N
(10)
where N is the total number of TUs. It eval-
uates the proportion of TUs that were put in the
same classes by the two clustering algorithms.

2
, which estimates the probability that the two
algorithms agree by chance, is given by:

2
=
K
X
i=1
(
k
i+
N

k
+i
N
) (11)
where k+i
N
and ki+
N
are the marginal distribu-
tions.
The  coefficient that results from the evalu-
ation of 
1
and 
2
is equal to 0 when the two
clustering algorithms agree only by chance and
to 1 when they really agree for each TU. Negative
values occur when there is a systematic disagree-
ment.
For the 71 classes of our test set, we computed
the  coefficient in two cases. First, with a ran-
dom mapping of the clusters and domains. We
got K = -0.013, which is very close to the agree-
ment by chance. Second, we applied the above
mapping procedure and got K = 0.484, which in-
dicates a significant correlation between the two
classifications. We think that with a more com-
plex mapping procedure, the  would be higher.
4.2.3 Application of the Mantel Test
In this paradigm, each classified thematic unit,
tu, is described according to its position in the
classification in relation to all the classified el-
ements. This position is characterized by a dis-
tance between tu and each other element. In the
work we present here, we choose a simple dis-
tance: dist(tu
1
; tu
2
) = 0 if tu
1
and tu
2
are part
of the same class; otherwise, dist(tu
1
; tu
2
) = 1.
However more complex distances may be used
when the classifications are hierarchical ones for
example. After this first step, each tu
i
of the two
classifications to compare is characterized by a
vector, each element of which, d
ij
, is equal to
the distance between tu
i
and tu
j
. Hence, each
classification is characterized by a distance ma-
trix, which is a square symmetric matrix of size
N
2
= 4935
2
. Comparing the two classifica-
tions amounts to compare their distance matrices.
In the ecology field, such kind of comparison is
achieved by a statistical test, called the Mantel
test (Mantel, 1967). In (Legendre, 2000), Legen-
dre defines the Mantel test as ? a procedure to test
the hypothesis that the distances among objects
in a matrix A are linearly independent of the dis-
tances among the same objects in another matrix
B. The result of this test may be used as support
for or against the hypothesis that the process that
generated the first set of distances is independent
of the process that generated the second set. The
unique feature of the Mantel test is the use of a
linear statistic to assess the relationship between
two distance matrices?. The basic statistic used in
the Mantel test is the Z statistic:
ZS =
i=N
X
i=1
j=N
X
j=1
x
ij
y
ij
As the elements of a distance matrix are not in-
dependent, the significance of ZS, the Z statistic
that is computed for the two distance matrices to
compare, is evaluated by comparing this value to
the Z statistic that is computed for matrices whose
rows and columns are randomly permuted. A dis-
tribution of random values is obtained by comput-
ing the statistic for many permuted matrices and
if ZS is significantly above this distribution, the
hypothesis that the two matrices are independent
is rejected 1.
1The Z statistic is maximal when the two distance matri-
ces are identical: the x
ij
y
ij
term is not equal to zero only if
x
ij
and y
ij
are equal to 1. Hence, each difference that could
be introduced between the two matrices, decreases its value.
As an exploratory step, we applied the Man-
tel test in order to compare the results of the two
classification methods we presented in this arti-
cle. We used the software developed by Adam
Liedloff (Liedloff, 1999). As the number of
TUs is too large in comparison with the capabil-
ities of this software, we experimented the Man-
tel test only on a subset of 1000 TUs. With the
distance matrix computed from the results of the
two classification methods, we got a Z statistic
(ZS) equal to 940; 894. The maximum value of
ZS is 978; 460 for the domains and 948; 608 for
the clusters. The random distribution was built
from 99 permuted matrices and its ZS value is
937; 708  232. As the proportion of the val-
ues from the random distribution that are above
ZS is equal to zero, we can reject the hypothe-
sis that the two matrices are independent and as a
consequence, we can think that the two compared
classifications are globally similar. However, as
the results of the Mantel test are not easy to inter-
pret, further tests must be performed to see what
are the relations between these results and those
of the other comparing methods and to determine
if this test is actually suited for comparing such
kind of classifications.
5 Conclusion
We presented in this paper an approach for eval-
uating the results of an unsupervised learning
method, when no human evaluation is possible or
when no classification exists as a reference. As
a result, this method builds classes of weighted
words that regroup thematic units. We defined
in a previous work a stability threshold of these
classes, thus we aim at evaluating this subset of
classes. To do that, we applied another clustering
method that only needs to know the number of
classes to build on the same subset of TUs and we
reformulate our evaluation problem in comparing
the two classifications. Our first results lead to
show a great correlation between the two results.
We now have to develop other tests, for exam-
ple on a different number of classes, to verify our
first results. A second step will be to evaluate the
methods on the same task, as a classification task
for example, whose protocol has to be defined.
References
T. Cover and J. Thomas. 1991. Elements of Informa-
tion Theory. Wiley & sons, New York.
T. G. Dietterich. 2000. An experimental comparison
of three methods for constructing ensembles of de-
cision trees: Bagging, boosting, and randomization.
Machine Learning, 40:139?158.
O. Ferret and B. Grau. 1998a. A thematic segmen-
tation procedure for extracting semantic domains
from texts. In ECAI, Brighton, UK.
O. Ferret and B. Grau. 1998b. Structuration d?un
re?seau de cooccurrences lexicales en domaines se?-
mantiques par analyse de textes. In NLPIA, Monc-
ton, Canada.
M. Jardino. 2000. Unsupervised non-hierarchical
entropy-based clustering. In H.A.L.Kiers, J.-
Rasson, P.J.F.Groenen, and M.Schader, editors,
Data Analysis, Classification, and Related Meth-
ods. Springer.
H. Kozima. 1993. Text segmentation based on sim-
ilarity between words. In ACL (Student Session),
Ohio, USA.
P. Legendre. 2000. Comparison of permutation meth-
ods for the partial correlation and partial correla-
tion and partial mantel tests. Statistical Computa-
tion and Simulation, 67:37?73.
Liedloff. 1999. Mantel nonparametric test calculator.
http://www.sci.qut.edu.au/nrs/mantel.htm.
C.-Y. Lin. 1997. Robust Automated Topic Identifica-
tion. Ph.D. thesis, University of Southern Califor-
nia.
N. Mantel. 1967. The detection of disease cluster-
ing and a generalized regression approach. Cancer
Res., 27:209?220.
I.T. Witten and T.C. Bell. 1991. The zero-frequency
problem: estimating the probabilities of novel
events in adaptative text compression. IEEE Trans-
actions on Information Theory, 37(3):1085?1094.
Terminological variants for document selection and
question/answer matching
Olivier Ferret Brigitte Grau Martine Hurault-Plantet
Gabriel Illouz Christian Jacquemin
LIMSI-CNRS
Bat.508 Universit? ParisXI
91403 Orsay, France
{ferret, grau, mhp, gabrieli, jacquemin}@limsi.fr
Abstract
Answering precise questions requires
applying Natural Language techniques
in order to locate the answers inside
retrieved documents. The QALC
system, presented in this paper,
participated to the Question Answering
track of the TREC8 and TREC9
evaluations. QALC exploits an analysis
of documents based on the search for
multi-word terms and their variations.
These indexes are used to select a
minimal number of documents to be
processed and to give indices when
comparing question and sentence
representations. This comparison also
takes advantage of a question analysis
module and recognition of numeric and
named entities in the documents.
1 Introduction
The Question Answering (QA) track at TREC8
and TREC9 is due to the recent need for more
sophisticated paradigms in Information
Retrieval (IR). Question answering generally
refers to encyclopedic or factual questions that
require concise answers. But current IR
techniques do not yet enable a system to give
precise answers to precise questions. Question
answering is thus an area of IR that calls for
Natural Language Processing (NLP) techniques
that can provide rich linguistic features as
output. Such NLP modules should be deeply
integrated in search and matching components
so that answer selection can be performed on
such linguistic features and take advantage of
them. In addition, IR and NLP techniques have
to collaborate in the resulting system in order to
cope with large-scale and broad coverage text
databases while deriving benefit from added
knowledge.
We developed a system for question
answering, QALC, evaluated in the framework
of the QA tracks at TREC8 and TREC9. The
QALC system comprises NLP modules for
multi-word term and named entity extraction
with a specific concern for term conflation
through variant recognition. Since named entity
recognition has already been described
extensively in other publications (Baluja 1999),
we present the contribution of terminological
variants to adding knowledge to our system.
The two main activities involving
terminology in NLP are term acquisition and
term recognition. Basically, terms can be viewed
as a particular type of lexical data. Term
variation may involve structural, morphological,
and semantic transformations of single or multi-
words terms (Fabre and Jacquemin, 2000).
In this paper, we describe how QALC uses
high level indexes, made of terms and variants,
to select among documents the most relevant
ones with regard to a question, and then to
match candidate answers with this question. In
the selection process, the documents first
retrieved by a search engine, are then
postfiltered and ranked through a weighting
scheme based on high level indexes, in order to
retain the top ranked ones. Similarly, all systems
that participated in TREC9 have a search engine
component that firstly selects a subset of the
provided database of about one million
documents. Since a search engine produces a
ranked list of relevant documents, systems then
have to define the highest number of documents
to retain. Indeed, having too many documents
leads to a question processing time that is too
long, but conversely, having too few documents
reduces the possibility of obtaining the correct
answer. For reducing the amount of text to
process, one approach consists of keeping one or
more relevant text paragraphs from each
document retrieved. Kwok et al(2000), for
instance use an IR engine that retrieves the top
300 sub-documents of about 300-550 words and,
on the other hand, the FALCON system
(Harabagiu et al 2000) performs a paragraph
retrieval stage after the application of a boolean
retrieval engine. These systems work on the
whole database and apply a bag-of-words
technique to select passages whereas QALC first
retains a large subset of documents, among
which it then selects relevant documents by
applying richer criteria based on the use of the
linguistic structures of the words.
QALC indexes, used for document selection,
are made of single and multi-word terms
retrieved by a 2-step procedure: (1)?automatic
term extraction from questions through part-of-
speech tagging and pattern matching and
(2)?automatic document indexing through term
recognition and variant conflation. As a result,
linguistic variation is explicitly addressed
through the exploitation of word paradigms,
contrarily to other approaches like the one taken
in COPSY (Schwarz 1988) where an
approximate matching technique between the
query and the documents implicitly takes it into
account. Finally, terms acquired at step?(1) and
indexes from step?(2) are also used by the
matching procedure between a question and the
relevant document sentences.
In the next section, we describe the
architecture of the QALC system. Then, we
present the question processing for term
extraction. We continue with the description of
FASTR, a transformational shallow parser that
recognizes and marks the extracted terms as well
as their linguistic variants within the documents.
The two following sections present the modules
of the QALC system where terms and variants
are used, namely the document selection and
question/answer matching modules. Finally, we
present the results obtained by the QALC
system as well as an evaluation of the
contribution of this NLP technique to the QA
task through the use of the reference collections
for the QA track. In conclusion, suggestions for
more ambitious, but still realistic, developments
using NLP are outlined.
2 System Overview
Natural Language Processing components in the
QALC system (see Figure 1) enrich the selected
documents with terminological indexes in order
to go beyond reasoning about single words. Rich
linguistic features are also used to deduce what a
question is about.
Tagged Questions:
Named entity tags
Vocabulary &
  frequencies
Named entity
 recognition
Candidate
terms
Retrieved
documents
Tagged sentences: named entity
    tags and term indexation
Ordered sequences of 250 and
           50 characters
Question analysis Search engine
Questions
Subset of ranked documents
Corpus
Re-indexing and selection of
      documents (FASTR)
Question/Sentence pairing
Figure 1. The QALC system
The analysis of a question relies on a shallow
parser which spots discriminating patterns and
assigns categories to the question. The
categories correspond to the types of entities that
are likely to constitute the answer to the
question.
In order to select the best documents from
the results given by the search engine and to
locate the answers inside them, we work with
terms and their variants, i.e. morphologic,
syntactic and semantic equivalent expressions.
A term extractor has been developed, based on
syntactic patterns which describe complex
nominal phrases and their subparts. These terms
are used by FASTR (Jacquemin 1999), a
shallow transformational natural language
analyzer that recognizes their occurrences and
their variants. Each occurrence or variant
constitutes an index that is subsequently used in
the processes of document ranking and
question/document matching.
Documents are ordered according to a weight
computed thanks to the number and the quality
of the terms and variants they contain. For
example, original terms with proper names are
considered more reliable than semantic variants.
An analysis of the weight graph enables the
system to select a relevant subpart of the
documents, whose size varies along the
questions. This selection takes all its importance
when applying the last processes which consist
of recognizing named-entities and analyzing
each sentence to decide whether it is a possible
answer or not. As such processes are time
consuming we attempt to limit their application
to a minimal number of documents.
Named entities are recognized in the
documents and used to measure the similarity
between the document sentences and a question.
Named entities receive one of the following
types: person, organization, location (city or
place), number (a time expression or a number
expression). They are defined in a way similar to
the MUC task and recognized through a
combination of lexico-syntactic patterns and
significantly large lexical data.
Finally, the question/answer matching
module uses all the data extracted from the
questions and the documents by the preceding
modules. We developed a similarity measure
that attributes weights to each characteristic, i.e.
named entity tags and terms and variants, and
makes a combination of them. The QALC
system proposes long and short answers.
Concerning the short ones, the system focuses
on parts of sentences that contain the expected
named entity tags, when they are known, or on
the largest subpart without any terms of the
question.
3 Terms and Variants
3.1 Term extraction
For automatic acquisition of terms from
questions, we use a simple technique of filtering
through patterns of part-of-speech categories.
No statistical ranking is possible because of the
small size of the questions from which terms are
extracted. First, questions are tagged with the
help of the TreeTagger (Schmid 1999). Patterns
of syntactic categories are then used to extract
terms from the tagged questions. They are very
close to those described by Justeson and
Katz?(1995), but we do not include post-posed
prepositional phrases. The pattern used for
extracting terms is:
(((((JJ | NN | NP | VBG)) ? (JJ | NN | NP | VBG) (NP
| NN))) | (VBD) | (NN) | (NP) | (CD))
where NN are common nouns, NP proper nouns,
JJ adjectives, VBG gerunds, VBD past
participles and CD numeral determiners.
The longest string is acquired first and
substrings can only be acquired if they do not
begin at the same word as the superstring. For
instance, from the sequence nameNN ofIN theDT
USNP helicopterNN pilotNN shotVBD downRP,
the following four terms are acquired: U S
helicopter pilot, helicopter pilot, pilot, and
shoot.
The mode of acquisition chosen for terms
amounts to considering only the substructures
that correspond to an attachment of modifiers to
the leftmost constituents (the closest one). For
instance, the decomposition of US helicopter
pilot into helicopter pilot and pilot is equivalent
to extracting the subconstituents of the structure
[US [helicopter [pilot]]].
3.2 Variant recognition through FASTR
The automatic indexing of documents is
performed by FASTR (Jacquemin 1999), a
transformational shallow parser for the
recognition of term occurrences and variants.
Terms are transformed into grammar rules and
the single words building these terms are
extracted and linked to their morphological and
semantic families.
The morphological family of a single word w
is the set M(w) of terms in the CELEX database
(CELEX 1998) which have the same root
morpheme as w. For instance, the morphological
family of the noun maker is made of the nouns
maker, make and remake, and the verbs to make
and to remake.
The semantic family of a single word w is the
union S (w ) of the synsets  of WordNet1.6
(Fellbaum 1998) to which w belongs. A synset is
a set of words that are synonymous for at least
one of their meanings. Thus, the semantic family
of a word w is the set of the words w' such that
w' is considered as a synonym of one of the
meanings of w. The semantic family of maker,
obtained from WordNet1.6, is composed of
three nouns: maker, manufacturer, shaper and
the semantic family of c a r is car, auto,
automobile, machine, motorcar.
Variant patterns that rely on morphological
and semantic families are generated through
metarules. They are used to extract terms and
variants from the document sentences in the
TREC corpus. For instance, the following
pattern, named NtoSemArg, extracts the
occurrence making many automobiles as a
variant of the term car maker:
VM('maker') RP? PREP? (ART (NN|NP)? PREP)?
ART? (JJ?|?NN?|?NP |?VBD?|?VBG)[0-3] NS('car')
where RP are particles, PREP prepositions, ART
articles, and VBD, VBG verbs. VM('maker') is
any verb in the morphological family of the
noun maker and NS('car') is any noun in the
semantic family of car.
Relying on the above morphological and
semantic families, auto maker, auto parts
maker , car manufacturer, make autos, and
making many automobiles are extracted as
correct variants of the original term car maker
through the set of metarules used for the QA
track experiment. Unfortunately, some incorrect
variants are extracted as well, such as make
those cuts in auto produced by the preceding
metarule.
3.3 Document selection
The output of NLP-based indexing is a list of
term occurrences composed of a document
identifier d, a term identifier?a pair t(q,i)
composed of a question number q and a unique
index i?, a text sequence, and a variation
identifier v (a metarule). For instance, the
following index :
LA092690-0038 t(131,1)
making many automobiles NtoVSemArg
means that the occurrence making many
automobiles from document d=LA092690-0038
is obtained as a variant of term i=1 in question
q=131 (car maker) through the variation
NtoVSemArg given in Section 3.2.
Each document d selected for a question q is
associated with a weight. The weighting scheme
relies on a measure of quality of the different
families of variations described by
Jacquemin?(1999): non-variant occurrences are
weighted 3.0, morphological and morpho-
syntactic variants are weighted 2.0, and
semantic and morpho-syntactico-semantic
variants are weighted 1.0.
Since proper names are more reliable indices
than common names, each term t(q,i) receives a
weight P(t(q , i )) between 0 and 1.0
corresponding to its proportion of proper names.
For instance, President Cleveland's wife is
weighted 2/3=0.66. Since another factor of
reliability is the length of terms, a factor |t(q,i)|
in the weighting formula denotes the number of
words in term t(q,i). The weight Wq(d) of a
query q  in a document d  is given by the
following formula (1). The products of the
weightings of each term extracted by the indexer
are summed over the indices I(d) extracted from
document d and normalized according to the
number of terms |T(q)| in query q.
  
W (d)
( ) ( ( ( , ))) ( , )
( )
q
( ( , ), ) ( )
=
? + ?
?
? w v P t q i t q i
T q
t q i v I d
1 2
         (1)
Mainly two types of weighting curves are
observed for the retrieved documents: curves
with a plateau and a sharp slope at a given
threshold (Figure 2.a) and curves with a slightly
decreasing weight (Figure 2.b).
The edge of a plateau is detected by examining
simultaneously the relative decrease of the slope
with respect to the preceding one, and the
relative decrease of the value with respect to the
preceding one. When a threshold is detected, we
only select documents before this threshold,
otherwise a fixed cutoff threshold is used. In our
experiments, for each query q, the 200 best
ranked documents retrieved by the search
engine1 were subsequently processed by the re-
indexing module. Our studies (Ferret et al 2000)
show that 200 is a minimum number such as
almost all the relevant documents are kept.
When no threshold was detected, we fixed the
value of the threshold to 100.
0
0
10
10
20
20
30
30
40
40
50
50
60
60
70
70
80
80
90
90
100
100
0
0
1
1
2
2
3
3
4
4
5
5
6
6
7
8
9
10
rank of the document
w
ei
gh
t
Question #87
rank of the document
Truncation of the ranked list
Question #86
w
ei
gh
t
(a)
(b)
Figure 2. Two types of weighting curve.
Through this method, the cutoff threshold is
8 for question #87 (Who followed Willy Brandt
as chancellor of the Federal Republic of
Germany?, Figure 2(a))2 and 100 for question
#86 (Who won two gold medals in skiing in the
Olympic Games in Calgary?, Figure 2(b)). As
indicated by Figure??2(a), there is an important
difference of weight between documents #8 and
#9. The weight of document #8 is 9.57 while the
                                                           
1 We used in particular Indexal (Loupy et al1998), a search
engine provided by Bertin Technologie.
2 Questions come from the TREC8 data.
weight of document #9 is 7.29 because the term
Federal Republic only exists in document #8.
This term has a high weight because it is
composed of two proper names.
4 Question-Answer Matching
4.1 Question type categorization
Question type categorization is performed in
order to assign features to questions and use
these features for the similarity measurement
between a question and potential answer
sentences. Basically, question categorization
allows the prediction of the kind(s) of answer,
called target (for instance, NUMBER).
Sentences inside the retrieved documents are
labeled with the same tags as questions. During
the similarity measurement, the more the
question and a sentence share the same tags, the
more they are considered as involved in a
question-answer relation. For example:
Question:
How many people live in the Falklands?
?> target = NUMBER
Answer:
F a l k l a n d s  p o p u l a t i o n  o f  <bnumex
TYPE=NUMBER> 2,100 <enumex> is
concentrated.
We established 17 types of answer. Some
systems define more categories. For instance
Prager et al (2000) identify about 50 types of
answer.
4.2 Answer Selection
In the QALC system, we have taken the
sentence as a basic unit because it is large
enough to contain the answer to questions about
simple facts and to give a context that permits
the user to judge if the suggested answer is
actually correct. The module associates each
question with the Na most similar sentences (Na
is equal to 5 for the QA task at TREC).
The overall principle of the selection process
is the following: each sentence from the
documents selected for a question is compared
with this question. To perform this comparison,
sentences and questions are turned into vectors
that contain three kinds of elements: content
words, term identifiers and named entity tags. A
specific weight (between 0 and 1.0) is associated
with each of these elements in order to express
their relative importance.
The content words are the lemmatized forms
of mainly adjectives, verbs and nouns such as
they are given by the TreeTagger. Each content
word in a vector is weighted according to its
degree of specificity in relation to the corpus in
which answers are searched through the tf.idf
weighting scheme. For questions, the term
identifiers refer to the terms extracted by the
term extractor described in Section?3.1 and
receive a fixed weight. In sentence vectors, term
identifiers are associated with the normalized
score from the ranking module (see Section 3.3).
The named entity tags correspond to the possible
types of answers, provided by the question
analysis module. In each sentence these tags
delimit the named entities that were recognized
by the corresponding module of the QALC
system and specify their type. Unlike term
identifiers, named entity tags are given the same
fixed weight in both sentence and question
vectors because the matching module uses the
types of the named entities and not their values.
In our experiments, the linguistic features
(terms and named entities) are used to favor
appropriate sentences when they have not
enough content words in common with the
question or when the question only contains a
few content words. Thus, the weights of term
identifiers or named entity tags are reduced by
applying a coefficient in order to be globally
lower than the weights of the content words.
Finally, the comparison between a sentence
vector Vd and a question vector Vq is achieved
by computing the following similarity measure:
?
?
=
j j
i i
dq
wq
wd
VVsim ),( (2)
where wqj is the weight of an element in the
question vector and wdi is the weight of an
element in a sentence vector that is also in the
question vector. This measure evaluates the
proportion and the importance of the elements in
the question vector that are found in the
sentence vector with regards to all the elements
of the question vector. Moreover, when the
similarity value is nearly the same for two
sentences, we favor the one in which the content
words of the question are the least scattered.
The next part gives an example of the
matching operations for the TREC8 question
Q16 What two US biochemists won the Nobel
Prize in medicine in 1992? This question is
turned into the following vector:
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) <PERSON> (0.5)
16.01 (0.5) 16.04 (0.5)
where <PERSON> is the expected type of the
answer, 16.01 is the identifier of the U S
biochemist term and 16.04 is the identifier of the
Nobel Prize term.
The same kind of vector is built for the
sentence <NUMBER> Two </NUMBER> US
biochemists, <PERSON> Edwin Krebs
</PERSON> and <CITY> Edmond </CITY>
Fischer, jointly won the <NUMBER> 1992
</NUMBER> Nobel Medicine Prize for work
that could advance the search for an anti-cancer
drug, coming from the document FT924-14045
that was selected for the question Q163 :
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) Edwin (0.0)
Krebs (0.0) Edmond (0.0) Fischer (0.0)
work (0.0) advance (0.0) search (0.0)
anti-cancer (0.0) jointly (0.0) drug (0.0)
<PERSON> (0.5) <NUMBER> (0.0) <CITY>(0.0)
16.01 (0.5) 16.04 (0.3)
where the weight 0.0 is given to the elements
that are not part of the question vector. The term
US biochemist is found with no variation and
Nobel Prize appears as a syntactic variant.
Finally, according to (2), the similarity measure
between theses two vectors is equal to 0.974.
5 Results and Evaluation
We sent to TREC9 three runs whose variations
concern the searched engine used and the length
of the answer (250 or 50 characters). Among
those runs, the best one obtained a score of
0.407 with 375 correct answers among 682
questions, for answers of 250 characters length.
The score computed by NIST is the reciprocal
mean of the rank, from 1 to 5, of the correct
                                                           
3 This sentence is taken from the output of the named entity
recognizer.
answer. With this score, the QALC system was
ranked 6th among 25 participants at TREC 9
QA task.
Document selection relies on a quantitative
measure, i.e. the document weight, whose
computation is based on syntactic and semantic
indices, i.e. the terms and the terminological
variants. Those indices allow the system to take
into account words as well as group of words
and their internal relations within the
documents. Following examples, that we have
got from selected documents for TREC9 QA
task, show what kind of indices are added to the
question words.
For the question 252 When was the first flush
toilet invented? , one multi-word extracted term
is flush toilet. This term is marked by FASTR
when recognized in a document, but it is also
marked when a variant is found, as for instance
low-flush toilet in the following document
sentence where low-flush is recognized as
equivalent to flush:
Santa Barbara , Calif. , is giving $ 80 to
anyone who converts to a low-flush toilet.
252.01   flush toilet[JJ][NN]
             low-flush[flush][JJ] toilet[toilet][NN]
             1.00
In the given examples, after the identification
number of the term, appears the reference term,
made of the lemmatized form of the words and
their syntactic category, followed by the variant
found in the sentence, with each word, its
lemmatized form and its category, and finally its
weight.
In the example above, the term found in the
sentence is equivalent to the reference term, and
thus its weight is 1.00.
The second example shows a semantic
variant. Salary and average salary are terms
extracted from the question 337, What's the
average salary of a professional baseball player
?. The semantic variant pay, got from WordNet,
was recognized in the following sentence?:
Did the NBA union opt for the courtroom
because its members, whose average pay tops
$500000 a year, wouldn't stand still for a
strike over free agency ?
337.01    salary[NN] pay[pay][NN] 0.25
337.00    average [JJ]salary[NN]
               average[average][JJ] pay[pay][NN]
               0.40
In order to evaluate the efficiency of the
selection process, we proceeded to several
measures. We apply our system on the material
given for the TREC8 evaluation, one time with
the selection process, and another time without
this process. At each time, 200 documents were
returned by the search engine for each of the 200
questions. When selection was applied, at most
100 documents were selected and subsequently
processed by the matching module. Otherwise,
the 200 documents were processed. The system
was scored by 0.463 in the first case, and by
0.452 in the second case. These results show
that the score increases when processing less
documents above all because it is just the
relevant documents that are selected.
The benefit from performing such a selection
is also illustrated by the results given in Table 1,
computed on the TREC9 results.
Number of documents selected
by ranking
100 <<100
Distribution among the
questions
342
(50%)
340
(50%)
Number of correct answers 175
(51%)
200
(59%)
Number of correct answer at
rank 1
88
(50%)
128
(64%)
Table 1. Evaluation of the ranking process
We see that the selection process discards a
lot of documents for 50% of the questions (340
questions are processed from less than 100
documents). The document set retrieved for
those questions had a weighting curve with a
sharp slope and a plateau as in Figure 2(a).
QALC finds more often the correct answer and
in a better position for these 340 questions than
for the 342 remaining ones. The average number
of documents selected, when there are less than
100, is 37. These results are very interesting
when applying such time-consuming processes
as  named ent i ty  recogni t ion and
question/sentence matching. Document selection
will also enable us to apply later on syntactic
and semantic sentence analysis.
6 Conclusion
The goal of a question-answering system is to
find an answer to a precise question, with a
response time short enough to satisfy the user.
As the answer is searched within a great amount
of documents, it seems relevant to apply mainly
numerical methods because they are fast. But, as
we said in the introduction, precise answers
cannot be obtained without adding NLP tools to
IR techniques. In this paper, we proposed a
question answering system which uses
terminological variants first to reduce the
number of documents to process while
increasing the system performance, and then to
improve the matching between a question and its
potential answers. Furthermore, reducing the
amount of text to process will afterwards allow
us to apply more complex methods such as
semantic analysis. Indeed, TREC organizers
foresee a number of possible improvements for
the future?: real-time answering, evaluation and
justification of the answer, completeness of the
answer which could result from answers
distributed along multiple documents, and
finally interactive question answering so that the
user could specify her/his intention. All those
improvements require more data sources as well
as advanced reasoning about pragmatic and
semantic knowledge.
Thus, the improvements that we now want to
bring to our system will essentially pertain to a
semantic and pragmatic approach. For instance,
WordNet that we already use to get the semantic
variants of a word, will be exploited to refine
our set of question types. We also plan to use a
shallow syntactico-semantic parser in order to
construct a semantic representation of both the
potential answer and the question. This
representation will allow QALC to select the
answer not only from the terms and variants but
also from the syntactic and semantic links that
terms share with each other.
References
Baluja, S., Vibhu O. M., Sukthankar, R. 1999
Applying machine learning for high performance
named-entity extraction. P r o c e e d i n g s
PACLING'99 Waterloo, CA. 365-378.
CELEX. 1998.
http://www.ldc.upenn.edu/readme_files/celex.read
me.html. Consortium for Lexical Resources,
UPenns, Eds.
Fabre C., Jacquemin C, 2000. Boosting variant
recognition with light semantics. Proceedings
COLING?2000, pp. 264-270, Luxemburg.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA, MIT Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G.,
Jacquemin C. (2000), QALC ? the Question-
Answering system of LIMSI-CNRS, pre-
proceedings of TREC9, NIST, Gaithersburg, CA.
Harabagiu S., Pasca M., Maiorano J. 2000.
Experiments with Open-Domain Textual Question
Answering. Proceedings of  Coling'2000,
Saarbrucken, Germany.
Jacquemin C. 1999. Syntagmatic and paradigmatic
representations of term variation. Proceedings of
ACL'99. 341-348.
Justeson J., Katz S. 1995. Technical terminology:
some linguistic properties and an algorithm for
identification in texte. Natural Language
Engineering. 1: 9-27.
Kwok K.L., Grunfeld L., Dinstl N., Chan M. 2000.
TREC9 Cross Language, Web and Question-
Answering Track experiments using PIRCS. Pre-
proceedings of TREC9, Gaithersburg, MD, NIST
Eds. 26-35.
Loupy C. , Bellot P., El-B?ze M., Marteau P.-F..
Query Expansion and Classification of Retrieved
Documents, TREC (1998), 382-389.
Prager J., Brown, E., Radev, D., Czuba, K. (2000),
One Search Engine or two for Question-
Answering, NISTs, Eds., Proceedings of TREC9,
Gaithersburg, MD. 250-254.
Schmid H. 1999. Improvments in Part-of-Speech
Tagging with an Application To German.
Natural?Language Processing Using Very Large
Corpora, Dordrecht, S. Armstrong, K. W. Chuch,
P. Isabelle, E. Tzoukermann,  D. Yarowski, Eds.,
Kluwer Academic Publisher.
Schwarz C. 1988. The TINA Project: text content
analysis at the Corporate Research Laboratories at
Siemens. Proceedings of Intelligent Multimedia
Information Retrieval Systems and Management
(RIAO?88) Cambridge, MA. 361-368.
Using collocations for topic segmentation and link detection
Olivier FERRET
CEA ? LIST/LIC2M
Route du Panorama ? BP 6
92265 Fontenay-aux-Roses Cedex
olivier.ferret@cea.fr
Abstract
We present in this paper a method for achiev-
ing in an integrated way two tasks of topic
analysis: segmentation and link detection. This
method combines word repetition and the lexi-
cal cohesion stated by a collocation network to
compensate for the respective weaknesses of
the two approaches. We report an evaluation
of our method for segmentation on two cor-
pora, one in French and one in English, and
we propose an evaluation measure that spe-
cifically suits that kind of systems.
1 Introduction
Topic analysis, which aims at identifying the top-
ics of a text, delimiting their extend and finding
the relations between the resulting segments, has
recently raised an important interest. The largest
part of it was dedicated to topic segmentation, also
called linear text segmentation, and to the TDT
(Topic Detection and Tracking) initiative (Fiscus
et al, 1999), which addresses all the tasks we have
mentioned but from a domain-dependent view-
point and not necessarily in an integrated way.
Systems that implement this work can be catego-
rized according to what kind of knowledge they
use. Most of those that achieve text segmentation
only rely on the intrinsic characteristics of texts:
word distribution, as in (Hearst, 1997), (Choi,
2000) and (Utiyama and Isahara, 2001), or lin-
guistic cues as in (Passonneau and Litman, 1997).
They can be applied without restriction about do-
mains but have low results when a text doesn?t
characterize its topical structure by surface clues.
Some systems exploit domain-independent knowl-
edge about lexical cohesion: a network of words
built from a dictionary in (Kozima, 1993); a large
set of collocations collected from a corpus in (Fer-
ret, 1998), (Kaufmann, 1999) and (Choi, 2001). To
some extend, this knowledge permits these sys-
tems to discard some false topical shifts without
losing their independence with regard to domains.
The last main type of systems relies on knowledge
about the topics they may encounter in the texts
they process. This is typically the kind of approach
developed in TDT where this knowledge is auto-
matically built from a set of reference texts. The
work of Bigi (Bigi et al, 1998) stands in the same
perspective but focuses on much larger topics than
TDT. These systems have a limited scope due to
their topic representations but they are also more
precise for the same reason.
Hybrid systems that combine the approaches we
have presented were also developed and illustrated
the interest of such a combination: (Jobbins and
Evett, 1998) combined word recurrence, colloca-
tions and a thesaurus; (Beeferman et al, 1999)
relied on both collocations and linguistic cues.
The topic analysis we propose implements such a
hybrid approach: it relies on a general language
resource, a collocation network, but exploits it
together with word recurrence in texts. Moreover,
it simultaneously achieves topic segmentation and
link detection, i.e. determining whether two seg-
ments discuss the same topic.
We detail in this paper the implementation of this
analysis by the TOPICOLL system, we report
evaluations of its capabilities concerning segmen-
tation for two languages, French and English, and
finally, we propose an evaluation measure that
integrates both segmentation and link detection.
2 Overview of TOPICOLL
In accordance with much work about discourse
analysis, TOPICOLL processes texts linearly: it de-
tects topic shifts and finds links between segments
without delaying its decision, i.e., by only taking
into account the part of text that has been already
analyzed. A window that delimits the current focus
of the analysis is moved over each text to be proc-
essed. This window contains the lemmatized con-
tent words of the text, resulting from its
pre-processing. A topic context is associated to
this focus window. It is made up of both the words
of the window and the words that are selected
from a collocation network1 as strongly linked to
the words of the window. The current segment is
also given a topic context. This context results
from the fusion of the contexts associated to the
focus window when this window was in the seg-
ment space. A topic shift is then detected when the
context of the focus window and the context of the
current segment are not similar any more for seve-
ral successive positions of the focus window. This
process also performs link detection by comparing
the topic context of each new segment to the con-
text of the already delimited segments.
The use of a collocation network permits
TOPICOLL to find relations beyond word recur-
rence and to associate a richer topical representa-
tion to segments, which facilitates tasks such as
link detection or topic identification. But work
such as (Kozima, 1993), (Ferret, 1998) or (Kauf-
mann, 1999) showed that using a domain-
independent source of knowledge for text seg-
mentation doesn?t necessarily lead to get better
results than work that is only based on word distri-
bution in texts. One of the reasons of this fact is
that these methods don?t precisely control the re-
lations they select or don?t take into account the
sparseness of their knowledge. Hence, while they
discard some incorrect topic shifts found by meth-
ods based on word recurrence, they also find in-
correct shifts when the relevant relations are not
present in their knowledge or don?t find some cor-
rect shifts because of the selection of non relevant
relations from a topical viewpoint. By combining
word recurrence and relations selected from a
collocation network, TOPICOLL aims at exploiting
a domain-independent source of knowledge for
text segmentation in a more accurate way.
3 Collocation networks
TOPICOLL depends on a resource, a collocation
network, that is language-dependent. Two collo-
cation networks were built for it: one for French,
                                                     
1 A collocation network is a set of collocations between
words. This set can be viewed as a network whose
nodes are words and edges are collocations.
from the Le Monde newspaper (24 months be-
tween 1990 and 1994), and one for English, from
the L.A. Times newspaper (2 years, part of the
TREC corpus). The size of each corpus was around
40 million words.
The building process was the same for the two
networks. First, the initial corpus was pre-
processed in order to characterize texts by their
topically significant words. Thus, we retained only
the lemmatized form of plain words, that is, nouns,
verbs and adjectives. Collocations were extracted
according to the method described in (Church and
Hanks, 1990) by moving a window on texts. Pa-
rameters were chosen in order to catch topical
relations: the window was rather large, 20-word
wide, and took into account the boundaries of
texts; moreover, collocations were indifferent to
word order. We also adopted an evaluation of
mutual information as a cohesion measure of each
collocation. This measure was normalized ac-
cording to the maximal mutual information rela-
tive to the considered corpus.
After filtering the less significant collocations
(collocations with less than 10 occurrences and
cohesion lower than 0.1), we got a network with
approximately 23,000 words and 5.2 million col-
locations for French, 30,000 words and 4.8 million
collocations for English.
4 Description of TOPICOLL
TOPICOLL is based on the creation, the update and
the use of a topical representation of both the seg-
ments it delimits and the content of its focus win-
dow at each position of a text. These topical repre-
sentations are called topic contexts. Topic shifts
are found by detecting that the topic context of the
focus window is not similar anymore to the topic
context of the current segment. Link detection is
performed by comparing the context of a new
segment to the context of the previous segments.
4.1 Topic contexts
A topic context characterizes the topical dimen-
sion of the entity it is associated to by two vectors
of weighted words. One of these vectors, called
text vector, is made up of words coming from the
text that is analyzed. The other one, called collo-
cation vector, contains words selected from a col-
location network and strongly linked to the words
of the processed text. For both vectors, the weight
of a word expresses its importance with regard to
the other words of the vector.
4.1.1 Topic context of the focus window
The text vector of the context associated to the
focus window is made up of the content words of
the window. Their weight is given by:
)()()( wsignifwoccNbwwght txt ?= (1)
where occNb(w) is the number of occurrences of
the word w in the window and signif(w) is the sig-
nificance of w. The weight given by (1) combines
the importance of w in the part of text delimited by
the window and its general significance. This sig-
nificance is defined as in (Kozima, 1993) as its
normalized information in a reference corpus2:
)1(log
)(log
)(
2
2
c
cw
S
Sf
wsignif
-
-= (2)
where fw is the number of occurrences of the word
w in the corpus and Sc, the size of the corpus.
0.14
0.21 0.10
0.18 0.13
0.17
w5w4w3w2w1
0.48 = pw3x0.18+pw4x0.13
   +pw5x0.17
selected word from the collocation network (with its weight)
1.0
word from text (with p its weight in the window, equal to
0.21 link in the collocation network (with its cohesion value)
1.0 1.0 1.0 1.0 1.0
wi,
n1 n2
1.0 for all words of the window in this example)
0.48
Figure 1 ? Selection and weighting of words from
the collocation network
The building of the collocation vector for the win-
dow?s context comes from the procedure presented
in (Ferret, 1998) for evaluating the lexical cohe-
sion of a text. It consists in selecting words of the
collocation network that are topically close to
those in the window. We assume that this close-
ness is related to the number of links that exist
between a word of the network and the words of
the window. Thus, a word of the network is se-
                                                     
2 In our case, this is the corpus used for building the
collocation network.
lected if it is linked to at least wst (3 in our ex-
periments) words of the window. A collocation
vector may also contain some words of the win-
dow as they are generally part of the collocation
network and may be selected as its other words.
Each selected word from the network is then as-
signed a weight. This weight is equal to the sum of
the contributions of the window words to which it
is linked to. The contribution of a word of the
window to the weight of a selected word is equal
to its weight in the window, given by (1), modu-
lated by the cohesion measure between these two
words in the network (see Figure 1). More pre-
cisely, the combination of these two factors is
achieved by a geometric mean:
? ?=
i
iitxtcoll wwcohwwgthwwght ),()()( (3)
where coh(w,wi) is the measure of the cohesion
between w and wi in the collocation network.
4.1.2 Topic context of a segment
The topic context of a segment results from the
fusion of the contexts associated to the focus win-
dow when it was inside the segment. The fusion is
achieved as the segment is extended: the context
associated to each new position of the segment is
combined with the current context of the segment.
This combination, which is done separately for
text vectors and collocation vectors, consists in
merging two lists of weighted words. First, the
words of the window context that are not in the
segment context are added to it. Then, the weight
of each word of the resulting list is computed ac-
cording to its weight in the window context and its
previous weight in the segment context:
)),,()((
)1,,(),,(
tCwwwghtwsignif
tCswwghttCswwght
x
xx
?
+-=
(4)
with Cw, the context of the window, Cs, the con-
text of the segment and wghtx(w,C{s,w},t), the
weight of the word w in the vector x (txt or coll) of
the context C{s,w} for the position t. For the words
from the window context that are not part of the
segment context, wghtx(w,Cs,t-1) is equal to 0.
The revaluation of the weight of a word in a seg-
ment context given by (4) is a solution halfway
between a fast and a slow evolution of the content
of segment contexts. The context of a segment has
to be stable because if it follows too narrowly the
topical evolution of the window context, topic
shifts could not be detected. However, it must also
adapt itself to small variations in the way a topic is
expressed when progressing in the text in order not
to detect false topic shifts.
4.1.3 Similarity between contexts
In order to determine if the content of the focus
window is topically coherent or not with the cur-
rent segment, the topic context of the window is
compared to the topic context of the segment. This
comparison is performed in two stages: first, a
similarity measure is computed between the vec-
tors of the two contexts; then, the resulting values
are exploited by a decision procedure that states if
the two contexts are similar.
As (Choi, 2000) or (Kaufmann, 1999), we use the
cosine measure for evaluating the similarity be-
tween a vector of the context window (Vw) and the
equivalent vector in the segment context (Vs):
??
?
?
?
=
i
ix
i
ix
ix
i
ix
xx
CwwwgCswwg
CwwgwCswwg
VwVssim
22 ),(),(
),(),(
),(
(5)
where wgx(wi,C{s,w}) is the weight of the word wi in
the vector x (txt or coll) of the context C{s,w}.
As we assume that the most significant words of a
segment context are the most recurrent ones, the
similarity measure takes into account only the
words of a segment context whose the recurrence3
is above a fixed threshold. This one is higher for
text vectors than for collocation vectors. This fil-
tering is applied only when the context of a seg-
ment is considered as stable (see 4.2).
The decision stage takes root in work about com-
bining results of several systems that achieve the
same task. In our case, the evaluation of the simi-
larity between Cs and Cw at each position is based
on a vote that synthesizes the viewpoint of the text
vector and the viewpoint of the collocation vector.
First, the value of the similarity measure for each
vector is compared to a fixed threshold and a posi-
                                                     
3 The recurrence of a word in a segment context is gi-
ven by the ratio between the number of window
contexts in which the word was present and the number
of window contexts gathered by the segment context.
tive vote in favor of the similarity of the two con-
texts is decided if the value exceeds this threshold.
Then, the global similarity of the two contexts is
rejected only if the votes for the two vectors are
negative.
4.2 Topic segmentation
The algorithm for detecting topic shifts is taken
from (Ferret and Grau, 2000) and basically relies
on the following principle: at each text position, if
the similarity between the topic context of the
focus window and the topic context of the current
segment is rejected (see 4.1.3), a topic shift is as-
sumed and a new segment is opened. Otherwise,
the active segment is extended up to the current
position.
This algorithm assumes that the transition between
two segments is punctual. As TOPICOLL only oper-
ates at word level, its precision is limited. This
imprecision makes necessary to set a short delay
before deciding that the active segment really ends
and similarly, before deciding that a new segment
with a stable topic begins. Hence, the algorithm
for detecting topic shifts distinguishes four states:
? the NewTopicDetection state takes place when a
new segment is going to be opened. This opening
is then confirmed provided that the content of the
focus window context doesn?t change for several
positions. Moreover, the core of the segment con-
text is defined when TOPICOLL is in this state;
? the InTopic state is active when the focus win-
dow is inside a segment with a stable topic;
? the EndTopicDetection state occurs when the
focus window is inside a segment but a difference
between the context of the window and the context
of the current segment suggests that this segment
could end soon. As for the NewTopicDetection
state, this difference has to be confirmed for sev-
eral positions before a change of state is decided;
? the OutOfTopic state is active between two
segments. Generally, TOPICOLL stays in this state
no longer than 1 or 2 positions but when neither
the words from text nor the words selected from
the collocation network are recurrent, i.e. no stable
topic can be detected according to these features,
this number of positions may be equal to the size
of a segment.
The transition from one state to another follows
the automaton of Figure 2 according to three pa-
rameters:
? its current state;
? the similarity between the context of the focus
window and the context of the current segment:
Sim or no Sim;
? the number of successive positions of the focus
window for which the current state doesn?t
change: confirmNb. It must exceed the Tconfirm
threshold (equal to 3 in our experiments) for leav-
ing the NewTopicDetection or the EndTopicDe-
tection state.
NewTopic
Detection
EndTopic
Detection
OutOfTopic
InTopic
Sim
Sim
Sim
no Sim
no Sim
no Sim
no Sim
&
confirmNb =
Tconfirm
no Sim
&
confirmNb <
Tconfirm
Sim
&
confirmNb =
Tconfirm
Sim
&
confirmNb <
Tconfirm
Figure 2 ? Automaton for topic shift detection
The processing of a segment starts with the Ou-
tOfTopic state, after the end of the previous seg-
ment or at the beginning of the text. As soon as the
context of the focus window is stable enough be-
tween two successive positions, TOPICOLL enters
into the NewTopicDetection state. The InTopic
state can then be reached only if the window con-
text is found stable for the next confirmNb-1 posi-
tions. Otherwise, TOPICOLL assumes that it is a
false alarm and returns to the OutOfTopic state.
The detection of the end of a segment is symmetri-
cal to the detection of its beginning. TOPICOLL
goes into the EndTopicDetection state as soon as
the content of the window context begins to
change significantly between two successive posi-
tions but the transition towards the OutOfTopic
state is done only if this change is confirmed for
the next confirmNb-1 next positions.
This algorithm is completed by a specific mecha-
nism related to the OutOfTopic state. When
TOPICOLL stays in this state for a too long time
(this time is defined as 10 positions of the focus
window in our experiments), it assumes that the
topic of the current part of text is difficult to char-
acterize by using word recurrence or selection
from a collocation network and it creates a new
segment that covers all the concerned positions.
4.3 Link detection
The algorithm of TOPICOLL for detecting identity
links between segments is closely associated to its
algorithm for delimiting segments. When TO-
PICOLL goes from the NewTopicDetection state to
the InTopic state, it first checks whether the cur-
rent context of the new segment is similar to one
of the contexts of the previous segments. In this
case, the similarity between contexts only relies on
the similarity measure (see (5)) between their col-
location vectors. A specific threshold is used for
the decision. If the similarity value exceeds this
threshold, the new segment is linked to the corre-
sponding segment and takes the context of this one
as its own context. In this way, TOPICOLL assumes
that the new segment continues to develop a pre-
vious topic. When several segments fulfills the
condition for link detection, TOPICOLL selects the
one with the highest similarity value.
5 Experiments
5.1 Topic segmentation
For evaluating TOPICOLL about segmentation, we
applied it to the ?classical? task of discovering
boundaries between concatenated texts. TOPICOLL
was adapted for aligning boundaries with ends of
sentences. We used the probabilistic error metric
Pk proposed in (Beeferman et al, 1999) for meas-
uring segmentation accuracy4. Recall and precision
was computed for the Le Monde corpus to com-
pare TOPICOLL with older systems5. In this case,
the match between a boundary from TOPICOLL and
a document break was accepted if the boundary
was not farther than 9 plain words.
5.1.1 Le Monde corpus
The evaluation corpus for French was made up of
49 texts, 133 words long on average, from the Le
                                                     
4 Pk evaluates the probability that a randomly chosen
pair of words, separated by k words, is wrongly classi-
fied, i.e. they are found in the same segment by
TOPICOLL while they are actually in different ones (miss
of a document break) or they are found in different
segments by TOPICOLL while they are actually in the
same one (false alarm).
5 Precision is given by Nt / Nb and recall by Nt / D, with
D the number of document breaks, Nb the number of
boundaries found by TOPICOLL and Nt the number of
boundaries that are document breaks.
Monde newspaper. Results in Tables 1 and 2 are
average values computed from 10 different se-
quences of them. The baseline procedure consisted
in randomly choosing a fixed number of sentence
ends as boundaries. Its results in Tables 1 and 2
are average values from 1,000 draws.
Systems Recall Precision F1-measure
baseline 0.51 0.28 0.36
SEGCOHLEX 0.68 0.37 0.48
SEGAPSITH 0.92 0.52 0.67
TextTiling 0.72 0.81 0.76
TOPICOLL1 0.86 0.74 0.80
TOPICOLL2 0.86 0.78 0.81
TOPICOLL3 0.66 0.60 0.63
Table 1 ? Precision/recall for Le Monde corpus
TOPICOLL1 is the system described in section 4.
TOPICOLL2 is the same system but without its link
detection part. The results of these two variants
show that the search for links between segments
doesn?t significantly debase TOPICOLL?s capabili-
ties for segmentation. TOPICOLL3 is a version of
TOPICOLL that only relies on word recurrence.
SEGCOHLEX and SEGAPSITH are the systems de-
scribed in (Ferret, 1998) and (Ferret and Grau,
2000). TextTiling is our implementation of
Hearst?s algorithm with its standard parameters.
Systems Miss False alarm Error
baseline 0.46 0.55 0.50
TOPICOLL1 0.17 0.24 0.21
TOPICOLL2 0.17 0.22 0.20
Table 2 ? Pk for Le Monde corpus
First, Table 1 shows that TOPICOLL is more accu-
rate when its uses both word recurrence and collo-
cations. Furthermore, it shows that TOPICOLL gets
better results than a system that only relies on a
collocation network such as SEGCOHLEX. It also
gets better results than a system such as TextTiling
that is based on word recurrence and as TOPICOLL,
works with a local context. Thus, Table 1 confirms
the fact reported in (Jobbins and Evett, 1998) that
using collocations together with word recurrence
is an interesting approach for text segmentation.
Moreover, TOPICOLL is more accurate than a sys-
tem such as SEGAPSITH that depends on topic rep-
resentations. Its accuracy is also slightly higher
than the one reported in (Bigi et al, 1998) for a
system that uses topic representations in a prob-
abilistic way: 0.75 as precision, 0.80 as recall and
0.77 as f1-measure got on a corpus made of Le
Monde?s articles too.
5.1.2 C99 corpus
For English, we used the artificial corpus built by
Choi (Choi, 2000) for comparing several segmen-
tation systems. This corpus is made up of 700
samples defined as follows: ?A sample is a con-
catenation of ten text segments. A segment is the
first n sentences of a randomly selected document
for the Brown corpus?. Each column of Table 3
states for an interval of values for n.
Systems 3-11 3-5 6-8 9-11
baseline 0.45 0.38 0.39 0.36
CWM 0.09 0.10 0.07 0.05
U00 0.10 0.09 0.07 0.05
C99 0.12 0.11 0.09 0.09
DotPlot 0.18 0.20 0.15 0.12
Segmenter 0.36 0.23 0.33 0.43
TextTiling 0.46 0.44 0.43 0.48
TOPICOLL1 0.30 0.28 0.27 0.34
TOPICOLL2 0.31 0.28 0.28 0.34
Table 3 ? Pk for C99 corpus
The first seven lines of Table 3 results from Choi?s
experiments (Choi, 2001). The baseline is a proce-
dure that partitions a document into 10 segments
of equal length. CWM is described in (Choi, 2001),
U00 in (Utiyama and Isahara, 2001), C99 in (Choi,
2000), DotPlot in (Reynar, 1998) and Segmenter
in (Kan et al, 1998).
Table 3 confirms first that the link detection part
of TOPICOLL doesn?t debase its segmentation ca-
pabilities. It also shows that TOPICOLL?s results on
this corpus are significantly lower than its results
on the Le Monde corpus. This is partially due to
our collocation network for English: its density,
i.e. the ratio between the size of its vocabulary and
its number of collocations, is 30% lower than the
density of the network for French, which has cer-
tainly a significant effect. Table 3 also shows that
TOPICOLL has worse results than systems such as
CWM, U00, C99 or DotPlot. This can be explained
by the fact that TOPICOLL only works with a local
context whereas these systems rely on the whole
text they process. As a consequence, they have a
global view on texts but are more costly than
TOPICOLL from an algorithmic viewpoint. Moreo-
ver, link detection makes TOPICOLL functionally
richer than they are.
5.2 Global evaluation
The global evaluation of a system such as
TOPICOLL faces a problem: a reference for link
detection is relative to a reference for segmenta-
tion. Hence, mapping it onto the segments delim-
ited by a system to evaluate is not straightforward.
To bypass this problem, we chose an approach
close the one adopted in TDT for the link detection
task: we evaluated the probability of an error in
classifying each couple of positions in a text as
being part of the same topic (Cpsame) or belonging
to different topics (Cpdiff). A miss is detected if a
couple is found about different topics while they
are about the same topic and a false alarm corre-
sponds to the complementary case.
Systems Miss False alarm Error
baseline 0.85 0.06 0.45
TOPICOLL 0.73 0.01 0.37
Table 4 ? Error rates for Le Monde corpus
As the number of Cpdiff couples is generally much
larger than the number of Cpsame couples, we ran-
domly selected a number of Cpdiff couples equal to
the number of Cpsame couples in order to have a
large range of possible values. Table 4 shows the
results of TOPICOLL for the considered measure
and compares them to a baseline procedure that
randomly set a fixed number of boundaries and a
fixed number of links between the delimited seg-
ments. This measure is a first proposition that
should certainly be improved, especially for bal-
ancing more soundly misses and false alarms.
6 Conclusion
We have proposed a method for achieving both
topic segmentation and link detection by using
collocations together with word recurrence in
texts. Its evaluation showed the soundness of this
approach for working with a local context. We
plan to extend it to methods that rely on the whole
text they process. We also aim at extending the
evaluation part of this work by improving the
global measure we have proposed and by com-
paring our results to human judgments.
References
Beeferman D., Berger A. and Lafferty J. (1999) Statis-
tical Models for Text Segmentation, Machine Learn-
ing, 34/1, pp. 177?210.
Bigi B., de Mori R., El-B?ze M. and Spriet T. (1998)
Detecting topic shifts using a cache memory, 5th In-
ternational Conference on Spoken Language Proc-
essing, pp. 2331?2334.
Church K. W. and Hanks P. (1990) Word Association
Norms, Mutual Information, And Lexicography.
Computational Linguistics, 16/1, pp. 22?29.
Choi F., Wiemer-Hastings P. and Moore J. (2001) La-
tent Semantic Analysis for Text Segmentation,
NAACL?01, pp. 109?117.
Choi F. (2000) Advances in domain independent linear
text segmentation, NAACL?00, pp. 26?33.
Ferret O. and Grau B. (2000) A Topic Segmentation of
Texts based on Semantic Domains, ECAI 2000,
pp. 426?430.
Ferret O. (1998) How to thematically segment texts by
using lexical cohesion?, ACL-COLING?98,
pp. 1481?1483.
Fiscus J., Doddington G., Garofolo J. and Martin A.
(1999) NIST?s 1998 Topic Detection and Tracking
Evaluation, DARPA Broadcast News Workshop.
Hearst M. (1997) TextTiling: Segmenting Text into
Multi-paragraph Subtopic Passages, Computational
Linguistics, 23/1, pp. 33?64.
Jobbins A. and Evett L. (1998) Text Segmentation Us-
ing Reiteration and Collocation, ACL-COLING?98,
pp. 614?618.
Kan M-Y., Klavans J. and McKeown K. (1998) Linear
segmentation and segment significance, 6th Workshop
on Very Large Corpora, pp. 197?205.
Kaufmann S. (1999) Cohesion and Collocation: Using
Context Vectors in Text Segmentation, ACL?99,
pp. 591?595.
Kozima H. (1993) Text Segmentation Based on Simi-
larity between Words, ACL?93, pp. 286?288.
Passonneau R. and Litman D. (1997) Discourse Seg-
mentation by Human and Automated Means, Com-
putational Linguistics, 23/1, pp. 103?139.
Reynar R. (1998) Topic segmentation: Algorithms and
applications, Ph.D. thesis, Computer and Information
Science, University of Pennsylvania.
Utiyama M. and Isahara H. (2001) A Statistical Model
for Domain-Independent Text Segmentation,
ACL?2001, pp. 491?498.
Discovering word senses from a network of lexical cooccurrences 
Olivier Ferret 
CEA ? LIST/LIC2M 
18, route du Panorama 
92265 Fontenay-aux-Roses, France 
ferreto@zoe.cea.fr 
 
Abstract 
Lexico-semantic networks such as WordNet 
have been criticized about the nature of the 
senses they distinguish as well as on the way 
they define these senses. In this article, we pre-
sent a possible solution to overcome these lim-
its by defining the sense of words from the way 
they are used. More precisely, we propose to 
differentiate the senses of a word from a net-
work of lexical cooccurrences built from a 
large corpus. This method was tested both for 
French and English and was evaluated for Eng-
lish by comparing its results with WordNet. 
1 Introduction 
Semantic resources have proved to be useful in 
information retrieval and information extraction 
for applications such as query expansion (Voor-
hees, 1998), text summarization (Harabagiu and 
Maiorano, 2002) or question/answering (Pasca and 
Harabagiu, 2001). But this work has also shown 
that these resources must be used with caution: 
they bring on an improvement of results only if 
word sense disambiguation is performed with a 
great accuracy. These findings bring one of the 
first roles of a semantic resource to light: discrimi-
nating and characterizing the senses of a set of 
words. The main semantic resources with a wide 
coverage that can be exploited by computers are 
lexico-semantic networks such as WordNet. Be-
cause of the way they were built, mainly by hand, 
these networks are not fundamentally different 
from traditional dictionaries. Hence, it is not very 
surprising that they were criticized, as in (Hara-
bagiu et al, 1999), for not being suitable for Natu-
ral Language Processing. They were criticized 
both about the nature of the senses they discrimi-
nate and the way they characterize them. Their 
senses are considered as too fine-grained but also 
incomplete. Moreover, they are generally defined 
through their relations with synonyms, hyponyms 
and hyperonyms but not by elements that describe 
the contexts in which they occur. 
One of the solutions for solving this problem 
consists in automatically discovering the senses of 
words from corpora. Each sense is defined by a list 
of words that is not restricted to synonyms or hy-
peronyms. The work done in this area can be di-
vided into three main trends. The first one, repre-
sented by (Pantel and Lin, 2002), is not focused on 
the problem of discovering word senses: its main 
objective is to build classes of equivalent words 
from a distributionalist viewpoint, hence to gather 
words that are mainly synonyms. In the case of 
(Pantel and Lin, 2002), the discovering of word 
senses is a side effect of the clustering algorithm, 
Cluster By Committee, used for building classes of 
words: as a word can belong to several classes, 
each of them can be considered as one of its 
senses. The second main trend, found in (Sch?tze, 
1998), (Pedersen and Bruce, 1997) and (Puran-
dare, 2003), represents each instance of a target 
word by a set of features that occur in its 
neighborhood and applies an unsupervised cluster-
ing algorithm to all its instances. Each cluster is 
then considered as a sense of the target word. The 
last trend, explored by (V?ronis, 2003), (Dorow 
and Widdows, 2003) and (Rapp, 2003), starts from 
the cooccurrents of a word recorded from a corpus 
and builds its senses by gathering its cooccurrents 
according to their similarity or their dissimilarity. 
Our work takes place in this last trend. 
2 Overview 
The starting point of the method we present in this 
article is a network of lexical cooccurrences, that 
is a graph whose vertices are the significant words 
of a corpus and edges represent the cooccurrences 
between these words in the corpus. The discove-
ring of word senses is performed word by word 
and the processing of a word only relies on the 
subgraph that contains its cooccurrents. The first 
step of the method consists in building a matrix of 
similarity between these cooccurrents by exploit-
ing their relations in the subgraph. An unsuper-
vised clustering algorithm is then applied for 
grouping these cooccurrents and giving rise to the 
senses of the considered word. This method, as the 
ones presented in (V?ronis, 2003), (Dorow and 
Widdows, 2003) and (Rapp, 2003), relies on the 
following hypothesis: in the subgraph gathering 
the cooccurrents of a word, the number of relations 
between the cooccurrents defining a sense is 
higher than the number of relations that these 
cooccurrents have with those defining the other 
senses of the considered word. The clustering al-
gorithm that we use is an adaptation of the Shared 
Nearest Neighbors (SNN) algorithm presented in 
(Ert?z et al, 2001). This algorithm particularly fits 
our problem as it automatically determines the 
number of clusters, in our case the number of 
senses of a word, and does not take into account 
the elements that are not representative of the clus-
ters it builds. This last point is especially important 
for our application as there is a lot of ?noise? 
among the cooccurrents of a word. 
3 Networks of lexical cooccurrences 
The method we present in this article for discover-
ing word senses was applied both for French and 
English. Hence, two networks of lexical cooccur-
rences were built: one for French, from the Le 
Monde newspaper (24 months between 1990 and 
1994), and one for English, from the L.A. Times 
newspaper (2 years, part of the TREC corpus). The 
size of each corpus was around 40 million words. 
The building process was the same for the two 
networks. First, the initial corpus was pre-
processed in order to characterize texts by their 
topically significant words. Thus, we retained only 
the lemmatized form of plain words, that is, nouns, 
verbs and adjectives. Cooccurrences were classi-
cally extracted by moving a fixed-size window on 
texts. Parameters were chosen in order to catch 
topical relations: the window was rather large, 20-
word wide, and took into account the boundaries 
of texts; moreover, cooccurrences were indifferent 
to word order. As (Church and Hanks, 1990), we 
adopted an evaluation of mutual information as a 
cohesion measure of each cooccurrence. This 
measure was normalized according to the maximal 
mutual information relative to the considered cor-
pus. After filtering the less significant cooccur-
rences (cooccurrences with less than 10 occur-
rences and cohesion lower than 0.1), we got a net-
work with approximately 23,000 words and 
5.2 million cooccurrences for French, 30,000 
words and 4.8 million cooccurrences for English. 
4 Word sense discovery algorithm 
4.1 Building of the similarity matrix be-
tween cooccurrents 
The number and the extent of the clusters built by 
a clustering algorithm generally depend on a set of 
parameters that can be tuned in one way or an-
other. But this possibility is implicitly limited by 
the similarity measure used for comparing the ele-
ments to cluster. In our case, the elements to 
cluster are the cooccurrents in the network of lexi-
cal cooccurrences of the word whose senses have 
to be discriminated. Within the same framework, 
we tested two measures for evaluating the similar-
ity between the cooccurrents of a word in order to 
get word senses with different levels of granular-
ity. The first measure corresponds to the cohesion 
measure between words in the cooccurrence net-
work. If there is no relation between two words in 
the network, the similarity is equal to zero. This 
measure has the advantage of being simple and 
efficient from an algorithmic viewpoint but some 
semantic relations are difficult to catch only from 
cooccurrences in texts. For instance, we experi-
mentally noticed that there are few synonyms of a 
word among its cooccurrents1. Hence, we can ex-
pect that some senses that are discriminated by the 
algorithm actually refer to one sense. 
To overcome this difficulty, we also tested a 
measure that relies not only on first order cooccur-
rences but also on second order cooccurrences, 
which are known to be ?less sparse and more ro-
bust? than first order ones (Sch?tze, 1998). This 
measure is based on the following principle: a vec-
tor whose size is equal to the number of cooccur-
rents of the considered word is associated to each 
of its cooccurrents. This vector contains the cohe-
sion values between this cooccurrent and the other 
ones. As for the first measure, a null value is taken 
when there is no relation between two words in the 
cooccurrence network. The similarity matrix is 
then built by applying the cosine measure between 
 
1 This observation comes from the intersection, for each word 
of the L.A. Times network, of its cooccurrents in the network 
and its synonyms in WordNet. 
each couple of vectors, i.e. each couple of cooc-
currents. With this second measure, two cooccur-
rents can be found strongly linked even though 
they are not directly linked in the cooccurrence 
network: they just have to share a significant num-
ber of words with which they are linked in the 
cooccurrence network. 
4.2 The Shared Nearest Neighbors (SNN) 
algorithm 
The SNN algorithm is representative of the algo-
rithms that perform clustering by detecting the 
high-density areas of a similarity graph. In such a 
graph, each vertex represents an element to cluster 
and an edge links two vertices whose similarity is 
not null. In our case, the similarity graph directly 
corresponds to the cooccurrence network with the 
first order cooccurrences whereas with the second 
order cooccurrences, it is built from the similarity 
matrix described in Section 4.1. The SNN algo-
rithm can be split up into two main steps: the first 
one aims at finding the elements that are the most 
representative of their neighborhood by masking 
the less important relations in the similarity graph. 
These elements are the seeds of the final clusters 
that are built in the second step by aggregating the 
remaining elements to those selected by the first 
step. More precisely, the SNN algorithm is applied 
to the discovering of the senses of a target word as 
follows: 
1. sparsification of the similarity graph: for each 
cooccurrent of the target word, only the links 
towards the k (k=15 in our experiments) most 
similar other cooccurrents are kept. 
2. building of the shared nearest neighbor graph: 
this step only consists in replacing, in the spar-
sified graph, the value of each edge by the 
number of direct neighbors shared by the two 
cooccurrents linked by this edge. 
3. computation of the distribution of strong links 
among cooccurrents: as for the first step, this 
one is a kind of sparsification. Its aim is to 
help finding the seeds of the senses, i.e. the 
cooccurrents that are the most representative of 
a set of cooccurrents. This step is also a means 
for discarding the cooccurrents that have no 
relation with the other ones. More precisely, 
two cooccurrents are considered as strongly 
linked if the number of the neighbors they 
share is higher than a fixed threshold. The 
higher than a fixed threshold. The number of 
strong links of each cooccurrent is then com-
puted. 
4. identification of the sense seeds and filtering 
of noise: the sense seeds and the cooccurrents 
to discard are determined by comparing their 
number of strong links with a fixed threshold. 
5. building of senses: this step mainly consists in 
associating to the sense seeds identified by the 
previous step the remaining cooccurrents that 
are the most similar to them. The result is a set 
of clusters that each represents a sense of the 
target word. For associating a cooccurrent to a 
sense seed, the strength of the link between 
them must be higher than a given threshold. If 
a cooccurrent can be tied to several seeds, the 
one that is the most strongly linked to it is cho-
sen. Moreover, the seeds that are considered as 
too close from each other for giving rise to 
separate senses can also be grouped during this 
step in accordance with the same criteria than 
the other cooccurrents. 
6. extension of senses: after the previous steps, a 
set of cooccurrents that are not considered as 
noise are still not associated to a sense. The 
size of this set depends on the strictness of the 
threshold controlling the aggregation of a 
cooccurrent to a sense seed but as we are inter-
ested in getting homogeneous senses, the value 
of this threshold cannot be too low. Neverthe-
less, we are also interested in having a defini-
tion as complete as possible of each sense. As 
senses are defined at this point more precisely 
than at step 4, the integration into these senses 
of cooccurrents that are not strongly linked to a 
sense seed can be performed on a larger basis, 
hence in a more reliable way. 
4.3 Adaptation of the SNN algorithm 
For implementing the SNN algorithm presented in 
the previous section, one of the points that must be 
specified more precisely is the way its different 
thresholds are fixed. In our case, we chose the 
same method for all of them: each threshold is set 
as a quantile of the values it is applied to. In this 
way, it is adapted to the distribution of these val-
ues. For the identification of the sense seeds 
(threshold equal to 0.9) and for the definition of 
the cooccurrents that are noise (threshold  equal to 
LM-1 LM-2 LAT-1 LAT-1.no LAT-2.no
number of words 17,261 17,261 13,414 6,177 6,177 
percentage of words with at least one sense 44.4% 42.7% 39.8% 41.8% 39% 
average number of senses by word 2.8 2.2 1.6 1.9 1.5 
average number of words describing a sense 16.1 16.3 18.7 20.2 18.9 
Table 1: Statistics about the results of our word sense discovery algorithm 
0.2), the thresholds are quantiles of the number of 
strong links of cooccurrents. For defining strong 
links (threshold equal to 0.65), associating cooc-
currents to sense seeds (threshold equal to 0.5) and 
aggregating cooccurrent to senses (threshold equal 
to 0.7), the thresholds are quantiles of the strength 
of the links between cooccurrents in the shared 
nearest neighbor graph. 
We also introduced two main improvements to 
the SNN algorithm. The first one is the addition of 
a new step between the last two ones. This comes 
from the following observation: although a sense 
seed can be associated to another one during the 
step 5, which means that the two senses they rep-
resent are merged, some clusters that actually cor-
respond to one sense are not merged. This problem 
is observed with the first and the second order 
cooccurrences and cannot be solved, without 
merging unrelated senses, only by adjusting the 
threshold that controls the association of a cooc-
current to a sense seed. In most of these cases, the 
?split? sense is scattered over one large cluster and 
one or several small clusters that only contain 3 or 
4 cooccurrents. More precisely, the sense seeds of 
the small clusters are not associated to the seed of 
the large cluster while most of the cooccurrents 
that are linked to them are associated to this seed. 
Instead of defining a specific mechanism for deal-
ing with these small clusters, we chose to let the 
SNN algorithm to solve the problem by only delet-
ing these small clusters (size < 6) after the step 5 
and marking their cooccurrents as unclassified. 
The last step of the algorithm aggregates in most 
of the cases these cooccurrents to the large cluster. 
Moreover, this new step makes the built senses 
more stable when the parameters of the algorithm 
are only slightly modified. 
The second improvement, which has a smaller 
impact than the first one, aims at limiting the noise 
that is brought into clusters by the last step. In the 
algorithm of (Ert?z et al, 2001), an element can 
be associated to a cluster when the strength of its 
link with one of the elements of this cluster is 
higher than a given threshold. This condition is 
stricter in our case as it concerns the average 
strength of the links between the unclassified 
cooccurrent and those of the cluster. 
5 Experiments 
We applied our algorithm for discovering word 
senses to the two networks of lexical cooccur-
rences we have described in Section 3 (LM: 
French; LAT: English) with the parameters given 
in Section 4. For each network, we tested the use 
of first order cooccurrences (LM-1 and LAT-1) 
and second order ones (LM-2 and LAT-2). For 
English, the use of second order cooccurrences 
was tested only for the subpart of the words of the 
network that was selected for the evaluation of 
Section 6 (LAT-2.no). Table 1 gives some statis-
tics about the results of the discovered senses for 
the different cases. We can notice that a significant 
percentage of words do not have any sense, even 
with second order cooccurrences. This comes from 
the fact that their cooccurrents are weakly linked 
to each other in the cooccurrence network they are 
part of, which probably means that their senses are 
not actually represented in this network. We can 
also notice that the use of second order cooccur-
rence actually leads to have a smaller number of 
senses by word, hence to have senses with a larger 
definition. As V?ronis (2003), we give in Table 2 
as an example of the results of our algorithm some 
of the words defining the senses of the polysemous 
French word barrage, which was part of the 
ROMANSEVAL evaluation. Whatever the kind of 
cooccurrences it relies on, our algorithm finds 
three of the four senses distinguished in (V?ronis, 
2003): dam (senses 1.3 and 2.1); barricading, 
blocking (senses 1.1, 1.2 and 2.2); barrier, frontier 
(senses 1.4 and 2.3). The sense play-off game 
(match de barrage), which refers to the domain of 
sport, is not found as it is weakly represented in 
the cooccurrence network and is linked to words, 
such as division, that are also ambiguous (it  refers 
LM-1 1.1 manifestant, forces_de_l?ordre, pr?fecture, agriculteur, protester, incendier, calme, pierre 
(demonstrator, the police, prefecture, farmer, to protest, to burn, quietness, stone)
1.2 conducteur, routier, v?hicule, poids_lourd, camion, permis, trafic, bloquer, voiture, autoroute 
(driver, lorry driver, vehicule, lorry, truck, driving licence, traffic, to block, car, highway)
1.3 fleuve, rivi?re, lac, bassin, m?tre_cube, crue, amont, pollution, affluent, saumon, poisson 
(river(2), lake, basin, cubic meter, swelling, upstream water, pollution, affluent, salmon, fish)
1.4 bless?, casque_bleu, soldat, tir, milice, convoi, ?vacuer, croate, milicien, combattant 
(wounded, U.N. soldier, soldier, firing, militia, convoy, to evacuate, Croatian, militiaman, combatant)
LM-2 2.1 eau, m?tre, lac, pluie, rivi?re, bassin, fleuve, site, poisson, affluent, montagne, crue, vall?e 
(water, meter, lake, rain, river(2), basin, setting, fish, affluent, mountain, swelling, valley)
2.2 conducteur, trafic, routier, route, camion, chauffeur, voiture, chauffeur_routier, poids_lourd 
(driver, traffic, lorry driver(3), road, lorry, car, truck)
2.3 casque_bleu, soldat, tir, convoi, milicien, blind?, milice, a?roport, bless?, incident, croate 
(U.N. soldier, soldier, firing, convoy, militiaman, tank, militia, airport, wounded, incident, Croatian)
Table 2: Senses found by our algorithm for the word barrage 
both to the sport and the military domains). It 
should be note that barrage has only 1,104 occur-
rences in our corpus while it has 7,000 occurrences 
in the corpus of (V?ronis, 2003), built by crawling 
from the Internet the pages found by a meta search 
engine queried with this word and its morphologi-
cal variants. This example is also a good illustra-
tion of the difference of granularity of the senses 
built from first order cooccurrences and those built 
from the second order ones. The sense 1.1, which 
is close to the sense 1.2 as the two refers to dem-
onstrations in relation to a category of workers, 
disappears when the second order cooccurrences 
are used. Table 3 gives examples of discovered 
senses from first order cooccurrences only, one for 
French (LM-1) and two for English (LAT-1). 
6 Evaluation 
The discovering of word senses, as most of the 
work dedicated to the building of linguistic re-
sources, comes up against the problem of evaluat-
ing its results. The most direct way of doing it is to 
compare the resource to evaluate with a similar 
resource that is acknowledged as a golden stan-
dard. For word senses, the WordNet-like lexico-
semantic networks can be considered as such a 
standard. Using this kind of networks for evaluat-
ing the word senses that we find is of course criti-
cizable as our aim is to overcome their insufficien-
cies. Nevertheless, as these networks are carefully 
controlled, such an evaluation provides at least a 
first judgment about the reliability of the discov-
ered senses. We chose to take up the evaluation 
method proposed in (Pantel and Lin, 2002). This 
method relies on WordNet and shows a rather 
good agreement between its results and human 
judgments (88% for Pantel and Lin). As a conse-
quence, our evaluation was done only for English, 
and more precisely with WordNet 1.7. For each 
considered word, the evaluation method tries to 
map one of its discovered senses with one of its 
synsets in WordNet by applying a specific similar-
ity measure. Hence, only the precision of the word 
sense discovering algorithm is evaluated but 
Pantel and Lin indicate that recall is not very sig-
nificant in this context: a discovered sense may be 
correct and not present in WordNet and con-
versely, some senses in WordNet are very close 
and should be joined for most of the applications 
using WordNet. They define a recall measure but 
only for ranking the results of a set of systems. 
Hence, it cannot be applied in our case. 
The similarity measure between a sense and a 
synset used for computing precision relies on the 
Lin?s similarity measure between two synsets: 
 )2(log)1(log
)(log2)2,1( sPsP
sPsssim
+
?= (1)
 
where s is the most specific synset that subsumes 
s1 and s2 in the WordNet hierarchy and P(s) 
represents the probability of the synset s estimated 
from a reference corpus, in this case the SemCor 
corpus. We used the implementation of this meas-
ure provided by the Perl module WordNet 
::Similarity v0.06 (Patwardhan and Pedersen, 
2003). The similarity between a sense and a synset 
is more precisely defined as the average value of 
the similarity values between the words that char-
acterize the sense, or a subset of them, and the 
synset. The similarity between a word and a synset 
organe (1300)2 patient, transplantation, greffe, malade, th?rapeutique, m?dical, m?decine, greffer, rein 
(patient, transplantation, transplant, sick person, therapeutic, medical, medicine, to transplant, 
kidney)
procr?ation, embryon, ?thique, humain, relatif, bio?thique, corps_humain, g?ne, cellule 
(procreation, embryo, ethical, human, relative, bioethics, human body, gene, cell)
constitutionnel, consultatif, constitution, instituer, ex?cutif, l?gislatif, si?ger, disposition 
(constitutional, consultative, constitution, to institute, executive, legislative, to sit, clause)
article, hebdomadaire, publication, r?daction, quotidien, journal, ?ditorial, r?dacteur  
(article, weekly, publication, editorial staff, daily, newspaper, editorial, sub-editor)
mouse (563) compatible, software, computer, machine, user, desktop, pc, graphics, keyboard, device 
laboratory, researcher, cell, gene, generic, human, hormone, research, scientist, rat 
party (16999) candidate, democrat, republican, gubernatorial, presidential, partisan, reapportionment 
ballroom, cocktail, champagne, guest, bash, gala, wedding, birthday, invitation, festivity 
caterer, uninvited, party-goers, black-tie, hostess, buffet, glitches, napkins, catering 
Table 3: Senses found by our algorithm from first order cooccurrences (LM-1 and LAT-1) 
is equal to the highest similarity value among 
those between the synset and the synsets to which 
the word belongs to. Each of these values is given 
by (1). A sense is mapped to the synset that is the 
most similar to it, providing that the similarity 
between them is higher than a fixed threshold 
(equal to 0.25 as in (Pantel and Lin, 2002)). Fi-
nally, the precision for a word is given by the pro-
portion of its senses that match one of its synsets. 
Table 4 gives the results of the evaluation of our 
algorithm for the words of the English cooccur-
rence network that are nouns only and for which at 
least one sense was discovered. As Pantel and Lin, 
we only take into account for evaluation 4 words 
of each sense, whatever the number of words that 
define it. But, because of the way our senses are 
built, we have not a specific measure of the simi-
larity between a word and the words that charac-
terize its senses. Hence, we computed two variants 
of the precision measure. The first one selects the 
four words of each sense by relying on their num-
ber of strong links in the shared nearest neighbor 
graph. The second one selects the four words that 
have the highest similarity score with one of the 
synsets of the target word, which is called ?opti-
mal choice? in Table 43. A clear difference can be 
noted between the two variants. With the optimal 
choice of the four words, we get results that are 
similar to those of Pantel and Lin: their precision 
is equal to 60.8 with an average number of words 
defining a sense equal to 14. 
 
2 Each word is given with its frequency in the corpus used for 
building the cooccurrence network. 
3 This selection procedure is only used for evaluation and we 
do no rely on WordNet for building our senses. 
On the other hand, Table 4 shows that the words 
selected on the basis of their number of strong 
links are not strongly linked in WordNet (accord-
ing to Lin?s measure) to their target word. This 
does not mean that the selected words are not in-
teresting for describing the senses of the target 
word but more probably that the semantic relations 
that they share with the target word are different 
from hyperonymy. The results of Pantel and Lin 
can be explained by the fact that their algorithm is 
based on the clustering of similar words, i.e. words 
that are likely to be synonyms, and not on the clus-
tering of the cooccurrents of a word, which are not 
often synonyms of that word. Moreover, their ini-
tial corpus is much larger (around 144 millions 
words) than ours and they make use of more 
elaborated tools, such as a syntactic analyzer. 
 
LAT-1.no LAT-2.no
number of strong links 19.4 20.8 
optimal choice 56.2 63.7 
Table 4: Average precision of discovered senses 
for English in relation with WordNet 
As expected, the results obtained with first order 
cooccurrences (LAT-1.no), which produce a 
higher number of senses by word, are lower than 
the results obtained with second order cooccur-
rences (LAT-2.no). However, without a recall 
measure, it is difficult to draw a clear conclusion 
from this observation: some senses of LAT-1.no 
probably result from the artificial division of an 
actual word sense but the fact to have more homo-
geneous senses in LAT-2.no also facilitates in this 
case the mapping with WordNet?s synsets. 
7 Related work 
As they rely on the detection of high-density areas 
in a network of cooccurrences, (V?ronis, 2003) 
and (Dorow and Widdows, 2003) are the closest 
methods to ours. Nevertheless, two main differ-
ences can be noted with our work. The first one 
concerns the direct use they make of the network 
of cooccurrences. In our case, we chose a more 
general approach by working at the level of a simi-
larity graph: when the similarity of two words is 
given by their relation of cooccurrence, our situa-
tion is comparable to the one of (V?ronis, 2003) 
and (Dorow and Widdows, 2003); but in the same 
framework, we can also take into account other 
kinds of similarity relations, such as the second 
order cooccurrences. 
The second main difference is the fact they dis-
criminate senses in an iterative way. This approach 
consists in selecting at each step the most obvious 
sense and then, to update the graph of cooccur-
rences by discarding the words that make up the 
new sense. The other senses are then easier to dis-
criminate. We preferred to put emphasis on the 
ability to gather close or identical senses that are 
artificially distinguished (see Section 4.3). From a 
global viewpoint, these two differences lead 
(V?ronis, 2003) and (Dorow and Widdows, 2003) 
to build finer senses than ours. Nevertheless, as 
methods for discovering word senses from a cor-
pus tend to find a too large number of close senses, 
it was more important from our viewpoint to fa-
vour the building of stable senses with a clear 
definition rather than to discriminate very fine 
senses. 
8 Conclusion and future work 
In this article, we have presented a new method for 
discriminating and defining the senses of a word 
from a network of lexical cooccurrences. This 
method consists in applying an unsupervised clus-
tering algorithm, in this case the SNN algorithm, 
to the cooccurrents of the word by relying on the 
relations that these cooccurrents have in the cooc-
currence network. We have achieved a first 
evaluation based on the methodology defined in 
(Pantel and Lin, 2002). This evaluation has shown 
that in comparison with WordNet taken as a refer-
ence, the relevance of the discriminated senses is 
comparable to the relevance of Pantel and Lin?s 
word senses. But it has also shown that the 
similarity between a discovered sense and a synset 
larity between a discovered sense and a synset of 
WordNet must be evaluated in our case by taking 
into account a larger set of semantic relations, es-
pecially those implicitly present in the glosses. 
Moreover, an evaluation based on the use of the 
built senses in an application such as query expan-
sion is necessary to determine the actual interest of 
this kind of resources in comparison with a lexico-
semantic network such as WordNet. 
References 
K.W. Church and P. Hanks. 1990. Word Association 
Norms, Mutual Information, And Lexicography. 
Computational Linguistics, 16(1): 22?29. 
Dorow B. and D. Widdows. 2003. Discovering Corpus-
Specific Word Senses. In EACL 2003.
L. Ert?z, M. Steinbach and V. Kumar. 2001. Finding 
Topics in Collections of Documents: A Shared Near-
est Neighbor Approach. In Text Mine?01, Workshop 
of the 1st SIAM International Conference on Data 
Mining.
S. Harabagiu and S. Maiorano. 2002. Multi-Document 
Summarization with GISTEXTER. In LREC 2002.
S Harabagiu, G.A. Miller and D. Moldovan. 1999. 
WordNet 2 - A Morphologically and Semantically 
Enhanced Resource. In SIGLEX?99.
M. Pasca and S. Harabagiu. 2001. The informative role 
of WordNet in Open-Domain Question Answering. 
In NAACL 2001 Workshop on WordNet and Other 
Lexical Resources.
P. Pantel and D. Lin. 2002. Discovering Word Senses 
from Text. In ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining 2002.
S. Patwardhan and T. Pedersen. 2003. Word-
Net::Similarity, http://www.d.umn.edu/~tpederse/ si-
milarity.html. 
T. Pedersen and R. Bruce. 1997. Distinguishing Word 
Senses in Untagged Text. In EMNLP'97.
A. Purandare. 2003. Discriminating Among Word 
Senses Using Mcquitty's Similarity Analysis. In 
HLT-NAACL 03 - Student Research Workshop.
R. Rapp. 2003. Word Sense Discovery Based on Sense 
Descriptor Dissimilarity. In Machine Translation 
Summit IX.
H. Sch?tze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24(1): 97-123. 
J. V?ronis. 2003. Cartographie lexicale pour la recher-
che d?information. In TALN 2003.
E.M. Voorhees. 1998. Using WordNet for text retrieval,
In ?WordNet: An Electronic Lexical Database?, 
Cambridge, MA, MIT Press, pages 285-303. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 281?288,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Enhancing electronic dictionaries with an index based on associations 
 
Olivier Ferret 
CEA ?LIST/LIC2M 
18 Route du Panorama 
F-92265 Fontenay-aux-Roses 
ferreto@zoe.cea.fr 
Michael Zock1
LIF-CNRS 
163 Avenue de Luminy 
F-13288 Marseille Cedex 9 
michael.zock@lif.univ-mrs.fr
Abstract 
A good dictionary contains not only 
many entries and a lot of information 
concerning each one of them, but also 
adequate means to reveal the stored in-
formation. Information access depends 
crucially on the quality of the index. We 
will present here some ideas of how a 
dictionary could be enhanced to support a 
speaker/writer to find the word s/he is 
looking for. To this end we suggest to 
add to an existing electronic resource an 
index based on the notion of association. 
We will also present preliminary work of 
how a subset of such associations, for ex-
ample, topical associations, can be ac-
quired by filtering a network of lexical 
co-occurrences extracted from a corpus. 
1 Introduction 
A dictionary user typically pursues one of two 
goals (Humble, 2001): as a decoder (reading, 
listening), he may look for the definition or the 
translation of a specific target word, while as an 
encoder (speaker, writer) he may want to find a 
word that expresses well not only a given con-
cept, but is also appropriate in a given context.  
Obviously, readers and writers come to the 
dictionary with different mindsets, information 
and expectations concerning input and output. 
While the decoder can provide the word he wants 
additional information for, the encoder (language 
producer) provides the meaning of a word for 
which he lacks the corresponding form. In sum, 
users with different goals need access to different 
indexes, one that is based on form (decoding), 
 
1 In alphabetical order 
the other being based on meaning or meaning 
relations (encoding). 
Our concern here is more with the encoder, i.e. 
lexical access in language production, a feature 
largely neglected in lexicographical work. Yet, a 
good dictionary contains not only many entries 
and a lot of information concerning each one of 
them, but also efficient means to reveal the 
stored information. Because, what is a huge dic-
tionary good for, if one cannot access the infor-
mation it contains? 
2 Lexical access on the basis of what: 
concepts (i.e. meanings) or words?
Broadly speaking, there are two views concern-
ing lexicalization: the process is conceptually-
driven (meaning, or parts of it are the starting 
point) or lexically-driven2 : the target word is 
accessed via a source word. This is typically the 
case when we are looking for a synonym, anto-
nym, hypernym (paradigmatic associations), or 
any of its syntagmatic associates (red-rose, cof-
fee-black), the kind of association we will be 
concerned with here. 
Yet, besides conceptual knowledge, people 
seem also to know a lot of things concerning the 
lexical form (Brown and Mc Neill, 1966): num-
ber of syllables, beginning/ending of the target 
word, part of speech (noun, verb, adjective, etc.), 
origin (Greek or Latin), gender (Vigliocco et al, 
 
2 Of course, the input can also be hybrid, that is, it can be 
composed of a conceptual and a linguistic component. For 
example, in order to express the notion of intensity, MAGN in 
Mel?Auk?s theory (Mel?Auk et al, 1995), a speaker or writer 
has to use different words (very, seriously, high) depending 
on the form of the argument (ill, wounded, price), as he says 
very ill, seriously wounded, high price. In each case he ex-
presses the very same notion, but by using a different word. 
While he could use the adverb very for qualifying the state 
of somebody?s health (he is ill), he cannot do so when quali-
fying the words injury or price. Likewise, he cannot use this 
specific adverb to qualify the noun illness.
281
1997). While in principle, all this information 
could be used to constrain the search space, we 
will deal here only with one aspect, the words? 
relations to other concepts or words (associative 
knowledge). 
Suppose, you were looking for a word expressing 
the following ideas: domesticated animal, pro-
ducing milk suitable for making cheese. Suppose 
further that you knew that the target word was 
neither cow, buffalo nor sheep. While none of 
this information is sufficient to guarantee the 
access of the intended word goat, the information 
at hand (part of the definition) could certainly be 
used3. Besides this type of information, people 
often have other kinds of knowledge concerning 
the target word. In particular, they know how the 
latter relates to other words. For example, they 
know that goats and sheep are somehow con-
nected, sharing a great number of features, that 
both are animals (hypernym), that sheep are ap-
preciated for their wool and meat, that they tend 
to follow each other blindly, etc., while goats 
manage to survive, while hardly eating anything, 
etc. In sum, people have in their mind a huge 
lexico-conceptual network, with words 4 , con-
cepts or ideas being highly interconnected. 
Hence, any one of them can evoke the other. The 
likelihood for this to happen depends on such 
factors as frequency (associative strength), sali-
ency and distance (direct vs. indirect access). As 
one can see, associations are a very general and 
powerful mechanism. No matter what we hear, 
read or say, anything is likely to remind us of 
something else. This being so, we should make 
use of it. 
 
3 For some concrete proposals going in this direction, see 
dictionaries offering reverse lookup: http://www.ultralingua. 
net/ ,http://www.onelook.com/reverse-dictionary.shtml.
4 Of course, one can question the very fact that people store 
words in their mind. Rather than considering the human 
mind as a wordstore one might consider it as a wordfactory.
Indeed, by looking at some of the work done by psycholo-
gists who try to emulate the mental lexicon (Levelt et al, 
1999) one gets the impression that words are synthesized 
rather than located and call up. In this case one might con-
clude that rather than having words in our mind we have a 
set of highly distributed, more or less abstract information. 
By propagating energy rather than data ?(as there is no 
message passing, transformation or cumulation of informa-
tion, there is only activation spreading, that is, changes of 
energy levels, call it weights, electronic impulses, or what-
ever),? that we propagate signals, activating ultimately 
certain peripherical organs (larynx, tongue, mouth, lips, 
hands) in such a way as to produce movements or sounds, 
that, not knowing better, we call words. 
3 Accessing the target word by navigat-
ing in a huge associative network 
If one agrees with what we have just said, one 
could view the mental lexicon as a huge semantic 
network composed of nodes (words and con-
cepts) and links (associations), with either being 
able to activate the other5. Finding a word in-
volves entering the network and following the 
links leading from the source node (the first 
word that comes to your mind) to the target word 
(the one you are looking for). Suppose you 
wanted to find the word nurse (target word), yet 
the only token coming to your mind is hospital.
In this case the system would generate internally 
a graph with the source word at the center and all 
the associated words at the periphery. Put differ-
ently, the system would build internally a seman-
tic network with hospital in the center and all its 
associated words as satellites (see Figure 1, next 
page). 
Obviously, the greater the number of associa-
tions, the more complex the graph. Given the 
diversity of situations in which a given object 
may occur we are likely to build many associa-
tions. In other words, lexical graphs tend to be-
come complex, too complex to be a good repre-
sentation to support navigation. Readability is 
hampered by at least two factors: high connec-
tivity (the great number of links or associations 
emanating from each word), and distribution:
conceptually related nodes, that is, nodes acti-
vated by the same kind of association are scat-
tered around, that is, they do not necessarily oc-
cur next to each other, which is quite confusing 
for the user. In order to solve this problem, we 
suggest to display by category (chunks) all the 
words linked by the same kind of association to 
the source word (see Figure 2). Hence, rather 
than displaying all the connected words as a flat 
list, we suggest to present them in chunks to al-
low for categorial search. Having chosen a cate-
gory, the user will be presented a list of words or 
categories from which he must choose. If the 
target word is in the category chosen by the user 
(suppose he looked for a hypernym, hence he 
checked the ISA-bag), search stops, otherwise it 
continues. The user could choose either another 
category (e.g. AKO or TIORA), or a word in the 
current list, which would then become the new 
starting point. 
 
5 While the links in our brain may only be weighted, they 
need to be labelled to become interpretable for human be-
ings using them for navigational purposes in a lexicon. 
282
DENTIST
assistant
near-synonym
GYNECOLOGIST
PHYSICIAN
HEALTH
INSTITUTION
CLINIC
DOCTOR
SANATORIUM
PSYCHIATRIC HOSPITAL
MILITARY HOSPITAL
ASYLUM
treat
A OK
take care of
treat
HOSPITAL
PATIENT
INMATE
TIORA
synonym
ISA A OK A OK
A OK
A OK
ISA
ISA ISA
ISA
ISA
TIORA
TIORA
nurse
Internal Representation
 
Figure 1: Search based on navigating in a network (internal representation) 
AKO: a kind of; ISA: subtype; TIORA: Typically Involved Object, Relation or Actor.
list of potential target words (LOPTW)
source word
link
link
link
link
link
LOPTW
LOPTW
list of potential target words (LOPTW)
...
Abstract representation of the search graph
hospital
TIORA
ISA
AKO clinic, sanatorium, ...
military hospital, psychiatric hospital
inmateSYNONYM
nurse
doctor, ...
patient
...
A concrete example
 
Figure 2: Proposed candidates, grouped by fam-
ily, i.e. according to the nature of the link 
As one can see, the fact that the links are labeled 
has some very important consequences:  
(a) While maintaining the power of a highly 
connected graph (possible cyclic navigation), 
it has at the interface level the simplicity of a 
tree: each node points only to data of the 
same type, i.e. to the same kind of associa-
tion.  
(b) With words being presented in clusters, 
navigation can be accomplished by clicking 
on the appropriate category.  
The assumption being that the user generally 
knows to which category the target word belongs 
(or at least, he can recognize within which of the 
listed categories it falls), and that categorical 
search is in principle faster than search in a huge 
list of unordered (or, alphabetically ordered) 
words6.
Obviously, in order to allow for this kind of 
access, the resource has to be built accordingly. 
This requires at least two things: (a) indexing 
words by the associations they evoke, (b) identi-
 
6 Even though very important, at this stage we shall not 
worry too much for the names given to the links. Indeed, 
one might question nearly all of them. What is important is 
the underlying rational: help users to navigate on the basis 
of symbolically qualified links. In reality a whole set of 
words (synonyms, of course, but not only) could amount to 
a link, i.e. be its conceptual equivalent. 
283
fying and labelling the most frequent/useful as-
sociations. This is precisely our goal. Actually, 
we propose to build an associative network by 
enriching an existing electronic dictionary (es-
sentially) with (syntagmatic) associations coming 
from a corpus, representing the average citizen?s 
shared, basic knowledge of the world (encyclo-
paedia). While some associations are too com-
plex to be extracted automatically by machine, 
others are clearly within reach. We will illustrate 
in the next section how this can be achieved. 
4 Automatic extraction of topical rela-
tions 
4.1 Definition of the problem 
We have argued in the previous sections that dic-
tionaries must contain many kinds of relations on 
the syntagmatic and paradigmatic axis to allow 
for natural and flexible access of words. Synon-
ymy, hypernymy or meronymy fall clearly in this 
latter category, and well known resources like 
WordNet (Miller, 1995), EuroWordNet (Vossen, 
1998) or MindNet (Richardson et al, 1998) con-
tain them. However, as various researchers have 
pointed out (Harabagiu et al, 1999), these net-
works lack information, in particular with regard 
to syntagmatic associations, which are generally 
unsystematic. These latter, called TIORA (Zock 
and Bilac, 2004) or topical relations (Ferret, 
2002) account for the fact that two words refer to 
the same topic, or take part in the same situation 
or scenario. Word-pairs like doctor?hospital,
burglar?policeman or plane?airport, are exam-
ples in case. The lack of such topical relations in 
resources like WordNet has been dubbed as the 
tennis problem (Roger Chaffin, cited in Fell-
baum, 1998). Some of these links have been in-
troduced more recently in WordNet via the do-
main relation. Yet their number remains still very 
small. For instance, WordNet 2.1 does not con-
tain any of the three associations mentioned here 
above, despite their high frequency. 
The lack of systematicity of these topical rela-
tions makes their extraction and typing very dif-
ficult on a large scale. This is why some re-
searchers have proposed to use automatic learn-
ing techniques to extend lexical networks like 
WordNet. In (Harabagiu & Moldovan, 1998), 
this was done by extracting topical relations from 
the glosses associated to the synsets. Other re-
searchers used external sources: Mandala et al 
(1999) integrated co-occurrences and a thesaurus 
to WordNet for query expansion; Agirre et al 
(2001) built topic signatures from texts in rela-
tion to synsets; Magnini and Cavagli? (2000) 
annotated the synsets with Subject Field Codes. 
This last idea has been taken up and extended by 
(Avancini et al, 2003) who expanded the do-
mains built from this annotation. 
Despite the improvements, all these ap-
proaches are limited by the fact that they rely too 
heavily on WordNet and some of its more so-
phisticated features (such as the definitions asso-
ciated with the synsets). While often being ex-
ploited by acquisition methods, these features are 
generally lacking in similar lexico-semantic net-
works. Moreover, these methods attempt to learn 
topical knowledge from a lexical network rather 
than topical relations. Since our goal is different, 
we have chosen not to rely on any significant 
resource, all the more as we would like our 
method to be applicable to a wide array of lan-
guages. In consequence, we took an incremental 
approach (Ferret, 2006): starting from a network 
of lexical co-occurrences7 collected from a large 
corpus, we used these latter to select potential 
topical relations by using a topical analyzer. 
4.2 From a network of co-occurrences to a 
set of Topical Units 
We start by extracting lexical co-occurrences 
from a corpus to build a network. To this end we 
follow the method introduced by (Church and 
Hanks, 1990), i.e. by sliding a window of a given 
size over some texts. The parameters of this ex-
traction were set in such a way as to catch the 
most obvious topical relations: the window was 
fairly large (20-words wide), and while it took 
text boundaries into account, it ignored the order 
of the co-occurrences. Like (Church and Hanks, 
1990), we used mutual information to measure 
the cohesion between two words. The finite size 
of the corpus allows us to normalize this measure 
in line with the maximal mutual information 
relative to the corpus.  
This network is used by TOPICOLL (Ferret, 
2002), a topic analyzer, which performs simulta-
neously three tasks, relevant for this goal: 
 it segments texts into topically homogene-
ous segments;  
 it selects in each segment the most repre-
sentative words of its topic; 
 
7 Such a network is only another view of a set of co-
occurrences: its nodes are the co-occurrent words and its 
edges are the co-occurrence relations. 
284
 it proposes a restricted set of words from 
the co-occurrence network to expand the 
selected words of the segment. 
These three tasks rely on a common mecha-
nism: a window is moved over the text to be ana-
lyzed in order to limit the focus space of the 
analysis. This latter contains a lemmatized ver-
sion of the text?s plain words. For each position 
of this window, we select only words of the co-
occurrence network that are linked to at least 
three other words of the window (see Figure 3). 
This leads to select both words that are in the 
window (first order co-occurrents) and words 
coming from the network (second order co-
occurrents). The number of links between the 
selected words of the network, called expansion 
words, and those of the window is a good indica-
tor of the topical coherence of the window?s con-
tent. Hence, when their number is small, a seg-
ment boundary can be assumed. This is the basic 
principle underlying our topic analyzer. 
 
0.14
0.21 0.10
0.18 0.13
0.17
w5w4w3w2w1
0.48 = pw3x0.18+pw4x0.13
+pw5x0.17
selected word from the co-occurrence network (with its weight)
1.0
word from text (with p its weight in the window, equal to 
0.21 link in the co-occurrence network (with its cohesion value)
1.0 1.0 1.0 1.0 1.0
wi,
n1 n2
1.0 for all words of the window in this example)
0.48
Figure 3: Selection and weighting of words 
from the co-occurrence network 
The words selected for each position of the 
window are summed, to keep only those occur-
ring in 75% of the positions of the segment. This 
allows reducing the number of words selected 
from non-topical co-occurrences. Once a corpus 
has been processed by TOPICOLL, we obtain a 
set of segments and a set of expansion words for 
each one of them. The association of the selected 
words of a segment and its expansion words is 
called a Topical Unit. Since both sets of words 
are selected for reasons of topical homogeneity, 
their co-occurrence is more likely to be a topical 
relation than in our initial network. 
4.3 Filtering of Topical Units 
Before recording the co-occurrences in the Topi-
cal Units built in this way, the units are filtered 
twice. The first filter aims at discarding hetero-
geneous Topical Units, which can arise as a side 
effect of a document whose topics are so inter-
mingled that it is impossible to get a reliable lin-
ear segmentation of the text. We consider that 
this occurs when for a given text segment, no 
word can be selected as a representative of the 
topic of the segment. Moreover, we only keep 
the Topical Units that contain at least two words 
from their original segment. A topic is defined 
here as a configuration of words. Note that the 
identification of such a configuration cannot be 
based solely on a single word. 
 
Text words Expansion words 
surveillance 
(watch)
police_judiciaire 
(judiciary police)
t?l?phonique 
(telephone)
?crouer 
(to imprison)
juge 
(judge)
garde_?_vue 
(police custody)
policier 
(policeman)
?coute_t?l?phonique 
(phone tapping)
brigade 
(squad)
juge_d?instruction 
(examining judge)
enqu?te 
(investigation)
contr?le_judiciaire 
(judicial review)
placer 
(to put)
Table 1: Content of a filtered Topical Unit 
The second filter is applied to the expansion 
words of each Topical Unit to increase their topi-
cal homogeneity. The principle of the filtering of 
these words is the same as the principle of their 
selection described in Section 4.2: an expansion 
word is kept if it is linked in the co-occurrence 
network to at least three text words of the Topi-
cal Unit. Moreover, a selective threshold is ap-
plied to the frequency and the cohesion of the co-
occurrences supporting these links: only co-
occurrences whose frequency and cohesion are 
respectively higher or equal to 15 and 0.15 are 
used. For instance in Table 1, which shows an 
example of a Topical Unit after its filtering, 
?crouer (to imprison) is selected, because it is 
linked in the co-occurrence network to the fol-
lowing words of the text: 
juge (judge): 52 (frequency) ? 0.17 (cohesion) 
policier (policeman): 56 ? 0.17
enqu?te (investigation): 42 ? 0.16
285
word freq. word freq. word freq. word freq.
sc?ne 
(stage) 884 
th??tral 
(dramatic) 62 
cynique
(cynical) 26 
sc?nique 
(theatrical) 14
th??tre 
(theater) 679 
sc?nariste 
(scriptwriter) 51 
miss
(miss) 20 
Chabol 
(Chabol) 13
r?alisateur 
(director) 220 
comique 
(comic) 51 
parti_pris
(bias) 16 
Tchekov 
(Tchekov) 13
cin?aste 
(film-marker) 135 
oscar 
(oscar) 40 
monologue 
(monolog) 15 
allocataire
(beneficiary) 13
com?die 
(comedy) 104 
film_am?ricain 
(american film) 38 
revisiter
(to revisit) 14 
satirique
(satirical) 13
costumer 
(to dress up) 63 
hollywoodien 
(Hollywood) 30 
gros_plan 
(close-up) 14  
Table 2: Co-occurrents of the word acteur (actor) with a cohesion of 0.16  
(the co-occurrents removed by our filtering method are underlined) 
4.4 From Topical Units to a network of 
topical relations 
After the filtering, a Topical Unit gathers a set of 
words supposed to be strongly coherent from the 
topical point of view. Next, we record the co-
occurrences between these words for all the 
Topical Units remaining after filtering. Hence, 
we get a large set of topical co-occurrences, de-
spite the fact that a significant number of non-
topical co-occurrences remains, the filtering of 
Topical Units being an unsupervised process. 
The frequency of a co-occurrence in this case is 
given by the number of Topical Units containing 
both words simultaneously. No distinction con-
cerning the origin of the words of the Topical 
Units is made. 
The network of topical co-occurrences built 
from Topical Units is a subset of the initial net-
work. However, it also contains co-occurrences 
that are not part of it, i.e. co-occurrences that 
were not extracted from the corpus used for set-
ting the initial network or co-occurrences whose 
frequency in this corpus was too low. Only some 
of these ?new? co-occurrences are topical. Since 
it is difficult to estimate globally which ones are 
interesting, we have decided to focus our atten-
tion only on the co-occurrences of the topical 
network already present in the initial network. 
Thus, we only use the network of topical co-
occurrences as a filter for the initial co-
occurrence network. Before doing so, we filter 
the topical network in order to discard co-
occurrences whose frequency is too low, that is, 
co-occurrences that are unstable and not repre-
sentative. From the use of the final network by 
TOPICOLL (see Section 4.5), we set the thresh-
old experimentally to 5. Finally, the initial net-
work is filtered by keeping only co-occurrences 
present in the topical network. Their frequency 
and cohesion are taken from the initial network. 
While the frequencies given by the topical net-
work are potentially interesting for their topical 
significance, we do not use them because the 
results of the filtering of Topical Units are too 
hard to evaluate. 
4.5 Results and evaluation 
We applied the method described here to an ini-
tial co-occurrence network extracted from a cor-
pus of 24 months of Le Monde, a major French 
newspaper. The size of the corpus was around 39 
million words. The initial network contained 
18,958 words and 341,549 relations. The first run 
produced 382,208 Topical Units. After filtering, 
we kept 59% of them. The network built from 
these Topical Units was made of 11,674 words 
and 2,864,473 co-occurrences. 70% of these co-
occurrences were new with regard to the initial 
network and were discarded. Finally, we got a 
filtered network of 7,160 words and 183,074 re-
lations, which represents a cut of 46% of the ini-
tial network. A qualitative study showed that 
most of the discarded relations are non-topical. 
This is illustrated by Table 2, which gives the co-
occurrents of the word acteur (actor) that are 
filtered by our method among its co-occurrents 
with a high cohesion (equal to 0.16). For in-
stance, the words cynique (cynical) or allocataire 
(beneficiary) are cohesive co-occurrents of the 
286
word actor, even though they are not topically 
linked to it. These words are filtered out, while 
we keep words like gros_plan (close-up) or sc?-
nique (theatrical), which topically cohere with 
acteur (actor) despite their lower frequency than 
the discarded words. 
 
Recall8 Precision F1-measure
Error 
(Pk)9
initial (I) 0.85 0.79 0.82 0.20 
topical 
filtering 
(T) 
0.85 0.79 0.82 0.21 
frequency 
filtering 
(F) 
0.83 0.71 0.77 0.25 
Table 3: TOPICOLL?s results  
with different networks 
 
In order to evaluate more objectively our 
work, we compared the quantitative results of 
TOPICOLL with the initial network and its fil-
tered version. The evaluation showed that the 
performance of the segmenter remains stable, 
even if we use a topically filtered network (see 
Table 3). Moreover, it became obvious that a 
network filtered only by frequency and cohesion 
performs significantly less well, even with a 
comparable size. For testing the statistical sig-
nificance of these results, we applied to the Pk
values a one-side t-test with a null hypothesis of 
equal means. Levels lower or equal to 0.05 are 
considered as statistically significant: 
pval (I-T): 0.08 
pval (I-F): 0.02 
pval (T-F): 0.05 
These values confirm that the difference be-
tween the initial network (I) and the topically 
filtered one (T) is actually not significant, 
whereas the filtering based on co-occurrence fre-
quencies leads to significantly lower results, both 
compared to the initial network and the topically 
filtered one. Hence, one may conclude that our 
 
8 Precision is given by Nt / Nb and recall by Nt / D, with D
being the number of document breaks, Nb the number of 
boundaries found by TOPICOLL and Nt the number of 
boundaries that are document breaks (the boundary should 
not be farther than 9 plain words from the document break). 
9 Pk (Beeferman et al, 1999) evaluates the probability that a 
randomly chosen pair of words, separated by k words, is 
wrongly classified, i.e. they are found in the same segment 
by TOPICOLL, while they are actually in different ones (miss 
of a document break), or they are found in different seg-
ments, while they are actually in the same one (false alarm). 
method is an effective way of selecting topical 
relations by preference. 
5 Discussion and conclusion 
We have raised and partially answered the ques-
tion of how a dictionary should be indexed in 
order to support word access, a question initially 
addressed in (Zock, 2002) and (Zock and Bilac, 
2004). We were particularly concerned with the 
language producer, as his needs (and knowledge 
at the onset) are quite different from the ones of 
the language receiver (listener/reader). It seems 
that, in order to achieve our goal, we need to do 
two things: add to an existing electronic diction-
ary information that people tend to associate with 
a word, that is, build and enrich a semantic net-
work, and provide a tool to navigate in it. To this 
end we have suggested to label the links, as this 
would reduce the graph complexity and allow for 
type-based navigation. Actually our basic pro-
posal is to extend a resource like WordNet by 
adding certain links, in particular on the syntag-
matic axis. These links are associations, and their 
role consists in helping the encoder to find ideas 
(concepts/words) related to a given stimulus 
(brainstorming), or to find the word he is think-
ing of (word access). 
One problem that we are confronted with is to 
identify possible associations. Ideally we would 
need a complete list, but unfortunately, this does 
not exist. Yet, there is a lot of highly relevant 
information out there. For example, Mel?cuk?s 
lexical functions (Mel?cuk, 1995), Fillmore?s 
FRAMENET10, work on ontologies (CYC), thesau-
rus (Roget), WordNets (the original version from 
Princeton, various Euro-WordNets, BalkaNet), 
HowNet11, the work done by MICRA, the FACTO-
TUM project 12 , or the Wordsmyth diction-
ary/thesaurus13.
Since words are linked via associations, it is 
important to reveal these links. Once this is done, 
words can be accessed by following these links. 
We have presented here some preliminary work 
for extracting an important subset of such links 
from texts, topical associations, which are gener-
ally absent from dictionaries or resources like 
WordNet. An evaluation of the topic segmenta-
tion has shown that the relations extracted are 
sound from the topical point of view, and that 
they can be extracted automatically. However, 
 
10 http://www.icsi.berkeley.edu/~framenet/
11 http://www.keenage.com/html/e_index.html
12 http://humanities.uchicago.edu/homes/MICRA/
13 http://www.wordsmyth.com/
287
they still contain too much noise to be directly 
exploitable by an end user for accessing a word 
in a dictionary. One way of reducing the noise of 
the extracted relations would be to build from 
each text a representation of its topics and to re-
cord the co-occurrences in these representations 
rather than in the segments delimited by a topic 
segmenter. This is a hypothesis we are currently 
exploring. While we have focused here only on 
word access on the basis of (other) words, one 
should not forget that most of the time speakers 
or writers start from meanings. Hence, we shall 
consider this point more carefully in our future 
work, by taking a serious look at the proposals 
made by Bilac et al (2004); Durgar and Oflazer 
(2004), or Dutoit and Nugues (2002). 
References 
Eneko Agirre, Olatz Ansa, David Martinez and Edu-
ard Hovy. 2001. Enriching WordNet concepts with 
topic signatures. In NAACL?01 Workshop on 
WordNet and Other Lexical Resources: Applica-
tions, Extensions and Customizations.
Henri Avancini, Alberto Lavelli, Bernardo Magnini, 
Fabrizio Sebastiani and Roberto Zanoli. 2003. Ex-
panding Domain-Specific Lexicons by Term Cate-
gorization. In 18th ACM Symposium on Applied 
Computing (SAC-03).
Doug Beeferman, Adam Berger and Lafferty. 1999. 
Statistical Models for Text Segmentation. Machine 
Learning, 34(1): 177-210. 
Slaven Bilac, Wataru Watanabe, Taiichi Hashimoto, 
Takenobu Tokunaga and Hozumi Tanaka. 2004. 
Dictionary search based on the target word descrip-
tion. In Tenth Annual Meeting of The Association 
for Natural Language Processing (NLP2004),
pages 556-559. 
Roger Brown and David McNeill. 1996. The tip of the 
tongue phenomenon. Journal of Verbal Learning 
and Verbal Behaviour, 5: 325-337. 
Kenneth Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, And Lexi-
cography. Computational Linguistics, 16(1): 177-
210. 
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2004. 
Use of Wordnet for Retrieving Words from Their 
Meanings, In 2nd Global WordNet Conference,
Brno 
Dominique Dutoit and Pierre Nugues. 2002. A lexical 
network and an algorithm to find words from defi-
nitions. In 15th European Conference on Artificial 
Intelligence (ECAI 2002), Lyon, pages 450-454, 
IOS Press. 
Christiane Fellbaum. 1998. WordNet - An Electronic 
Lexical Database, MIT Press. 
Olivier Ferret. 2006. Building a network of topical 
relations from a corpus. In LREC 2006.
Olivier Ferret. 2002. Using collocations for topic 
segmentation and link detection. In COLING 2002,
pages 260-266. 
Sanda M. Harabagiu, George A. Miller and Dan I. 
Moldovan. 1999. WordNet 2 - A Morphologically 
and Semantically Enhanced Resource. In ACL-
SIGLEX99: Standardizing Lexical Resources,
pages 1-8. 
Sanda M. Harabagiu and Dan I. Moldovan. 1998. 
Knowledge Processing on an Extended WordNet. 
In WordNet - An Electronic Lexical Database,
pages 379-405. 
Philip Humble. 2001. Dictionaries and Language 
Learners, Haag and Herchen. 
William Levelt, Ardi Roelofs and Antje Meyer. 1999. 
A theory of lexical access in speech production, 
Behavioral and Brain Sciences, 22: 1-75. 
Bernardo Magnini and Gabriela Cavagli?. 2000. Inte-
grating Subject Field Codes into WordNet. In 
LREC 2000.
Rila Mandala, Takenobu Tokunaga and Hozumi 
Tanaka. 1999. Complementing WordNet with Ro-
get?s and Corpus-based Thesauri for Information 
Retrieval. In EACL 99.
Igor Mel?Auk, Arno Clas and Alain Polgu?re. 1995. 
Introduction ? la lexicologie explicative et combi-
natoire, Louvain, Duculot. 
George A. Miller. 1995. WordNet: A lexical Data-
base, Communications of the ACM. 38(11): 39-41. 
Stephen D. Richardson, William B. Dolan and Lucy 
Vanderwende. 1998. MindNet: Acquiring and 
Structuring Semantic Information from Text. In 
ACL-COLING?98, pages 1098-1102. 
Piek Vossen. 1998. EuroWordNet: A Multilingual 
Database with Lexical Semantic Networks. Kluwer 
Academic Publisher. 
Gabriella Vigliocco, Antonini, T., and Merryl Garrett. 
1997. Grammatical gender is on the tip of Italian 
tongues. Psychological Science, 8: 314-317. 
Michael Zock. 2002. Sorry, what was your name 
again, or how to overcome the tip-of-the tongue 
problem with the help of a computer? In SemaNet 
workshop, COLING 2002, Taipei. 
http://acl.ldc.upenn.edu /W/W02/W02-1118.pdf
Michael Zock and Slaven Bilac. 2004. Word lookup 
on the basis of associations: from an idea to a 
roadmap. In COLING 2004 workshop: Enhancing 
and using dictionaries, Geneva. 
http://acl.ldc.upenn.edu/ coling2004/W10/pdf/5.pdf
288
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 480?487,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Finding document topics for improving topic segmentation
Olivier Ferret
CEA LIST, LIC2M
18 route du Panorama, BP6
Fontenay aux Roses, F-92265 France
ferreto@zoe.cea.fr
Abstract
Topic segmentation and identification are of-
ten tackled as separate problems whereas
they are both part of topic analysis. In this
article, we study how topic identification can
help to improve a topic segmenter based on
word reiteration. We first present an unsu-
pervised method for discovering the topics
of a text. Then, we detail how these topics
are used by segmentation for finding topical
similarities between text segments. Finally,
we show through the results of an evaluation
done both for French and English the inter-
est of the method we propose.
1 Introduction
In this article, we address the problem of linear topic
segmentation, which consists in segmenting doc-
uments into topically homogeneous segments that
does not overlap each other. This part of the Dis-
course Analysis field has received a constant interest
since the initial work in this domain such as (Hearst,
1994). One criterion for classifying topic segmen-
tation systems is the kind of knowledge they de-
pend on. Most of them only rely on surface features
of documents: word reiteration in (Hearst, 1994;
Choi, 2000; Utiyama and Isahara, 2001; Galley et
al., 2003) or discourse cues in (Passonneau and Lit-
man, 1997; Galley et al, 2003). As such systems do
not require external knowledge, they are not sensi-
tive to domains but they are limited by the type of
documents they can be applied to: lexical reiteration
is reliable only if concepts are not too frequently ex-
pressed by several means (synonyms, etc.) and dis-
course cues are often rare and corpus-specific.
To overcome these difficulties, some systems
make use of domain-independent knowledge about
lexical cohesion: a lexical network built from a dic-
tionary in (Kozima, 1993); a thesaurus in (Mor-
ris and Hirst, 1991); a large set of lexical co-
occurrences collected from a corpus in (Choi et al,
2001). To a certain extent, these lexical networks
enable topic segmenters to exploit a sort of concept
reiteration. However, their lack of any explicit topi-
cal structure makes this kind of knowledge difficult
to use when lexical ambiguity is high.
The most simple solution to this problem is to ex-
ploit knowledge about the topics that may occur in
documents. Such topic models are generally built
from a large set of example documents as in (Yam-
ron et al, 1998), (Blei and Moreno, 2001) or in one
component of (Beeferman et al, 1999). These sta-
tistical topic models enable segmenters to improve
their precision but they also restrict their scope.
Hybrid systems that combine the approaches
we have presented were also developed and illus-
trated the interest of such a combination: (Job-
bins and Evett, 1998) combined word recurrence,
co-occurrences and a thesaurus; (Beeferman et al,
1999) relied on both lexical modeling and discourse
cues; (Galley et al, 2003) made use of word reitera-
tion through lexical chains and discourse cues.
The work we report in this article takes place in
the first category we have presented. It does not
rely on any a priori knowledge and exploits word
usage rather than discourse cues. More precisely,
we present a new method for enhancing the results
480
of segmentation systems based on word reiteration
without relying on any external knowledge.
2 Principles
In most of the algorithms in the text segmentation
field, documents are represented as sequences of ba-
sic discourse units. When they are written texts,
these units are generally sentences, which is also the
case in our work. Each unit is turned into a vector of
words, following the principles of the Vector Space
model. Then, the similarity between the basic units
of a text is evaluated by computing a similarity mea-
sure between the vectors that represent them. Such a
similarity is considered as representative of the top-
ical closeness of the corresponding units. This prin-
ciple is also applied to groups of basic units, such as
text segments, because of the properties of the Vec-
tor Space model. Segments are finally delimited by
locating the areas where the similarity between units
or groups of units is weak.
This quick overview highlights the important role
of the evaluation of the similarity between discourse
units in the segmentation process. When no exter-
nal knowledge is used, this similarity is only based
on the strict reiteration of words. But it can be en-
hanced by taking into account semantic relations be-
tween words. This was done for instance in (Jobbins
and Evett, 1998) by taking semantic relations from
Roget?s Thesaurus. This resource was also used in
(Morris and Hirst, 1991) where the similarity be-
tween discourse units was more indirectly evaluated
through the lexical chains they share. The same ap-
proach was adopted in (Stokes et al, 2002) but with
WordNet as the reference semantic resource.
In this article, we propose to improve the detec-
tion of topical similarity between text segments but
without relying on any external knowledge. For each
text to segment, we first identify its topics by per-
forming an unsupervised clustering of its words ac-
cording to their co-occurrents in the text. Thus, each
of its topics is represented by a subset of its vocab-
ulary. When the similarity between two segments is
evaluated during segmentation, the words they share
are first considered but the presence of words of the
same topic is also taken into account. This makes
it possible to find similar two segments that refer to
the same topic although they do not share a lot of
words. It is also a way to exploit long-range rela-
tions between words at a local level. More globally,
it helps to reduce the false detection of topic shifts.
3 Unsupervised Topic Identification
The approach we propose first requires to discover
the topics of texts. For performing such a task with-
out using a priori knowledge, we assume that the
most representative words of each of the topics of
a text occur in similar contexts. Hence, for each
word of the text with a minimal frequency, we col-
lect its co-occurrents, we evaluate the pairwise simi-
larity of these selected text words by relying on their
co-occurrents and finally, we build topics by apply-
ing an unsupervised clustering method to them.
3.1 Building the similarity matrix of text words
The first step for discovering the topics of a text is
a linguistic pre-processing of it. This pre-processing
splits the text into sentences and represents each of
them as the sequence of its lemmatized plain words,
that is, nouns (proper and common nouns), verbs
and adjectives. After filtering the low frequency
words of the text (frequency < 3), the co-occurrents
of the remaining words are classically collected by
recording the co-occurrences in a fixed-size win-
dow (15 plain words) moved over the pre-processed
text. As a result, each text word is represented by
a vector that contains its co-occurrents and their co-
occurrence frequency. The pairwise similarity be-
tween all the selected text words is then evaluated
for building their similarity matrix. We classically
apply the Cosine measure between the vectors that
represent them for this evaluation.
3.2 From a similarity matrix to text topics
The final step for discovering the topics of a text is
the unsupervised clustering of its words from their
similarity matrix. We rely for this task on an adap-
tation of the Shared Nearest Neighbor (SNN) algo-
rithm described in (Ert?z et al, 2001). This algo-
rithm particularly fits our needs as it automatically
determines the number of clusters ? in our case the
number of topics of a text ? and does not take into
account the elements that are not representative of
the clusters it builds. This last point is important for
our application as all the plain words of a text are
not representative of its topics. The SNN algorithm
481
yearyearly
bovine
case become
BES
human
market
pair
ski
swiss
animal
carcass
declare
federal
infect
disease
director
indicate
shaking
cow
maker
production
streule
last
mad
company
stockli
Figure 1: Similarity graph after its sparsification
(see Algorithm 1) performs clustering by detecting
high-density areas in a similarity graph. In our case,
the similarity graph is directly built from the simi-
larity matrix: each vertex represents a text word and
an edge links two words whose similarity is not null.
The SNN algorithm splits up into two main stages:
the first one finds the elements that are the most rep-
resentative of their neighborhood. These elements
are the seeds of the final clusters that are built in the
second stage by aggregating the remaining elements
to those selected by the first stage. This first stage
Algorithm 1 SNN algorithm
1. sparsification of the similarity graph
2. building of the SNN graph
3. computation of the distribution of strong links
4. search for topic seeds and filtering of noise
5. building of text topics
6. removal of insignificant topics
7. extension of text topics
starts by sparsifying the similarity graph, which is
done by keeping only the links towards the k (k=10)
most similar neighbors of each text word (step 1).
Figure 1 shows the resulting graph for a two-topic
document of our evaluation framework (see Sec-
tion 5.1). Then, the similarity graph is transposed
into a shared nearest neighbor (SNN) graph (step 2).
In this graph, the similarity between two words is
given by the number of direct neighbors they share
in the similarity graph. This transposition makes the
similarity values more reliable, especially for high-
dimensional data like textual data. Strong links in
the SNN graph are finally detected by applying a
fixed threshold to the distribution of shared neigh-
bor numbers (step 3). A word with a high number
of strong links is taken as the seed of a topic as it is
representative of the set of words that are linked to
it. On the contrary, a word with few strong links is
supposed to be outlier (step 4).
The second stage of the SNN algorithm first
builds text topics by associating to topic seeds the
remaining words that are the most similar to them
provided that their number of shared neighbors is
high enough (step 5). Moreover, the seeds that are
judged as too close to each other are also grouped
during this step in accordance with the same crite-
ria. The last two steps bring small improvements to
the results of this clustering. First, when the num-
ber of words of a topic is too small (size < 3), this
topic is judged as insignificant and it is discarded
(step 6). Its words are added to the set of words with-
out topic after step 5. We added this step to the SNN
algorithm to balance the fact that without any ex-
ternal knowledge, all the semantic relations between
text words cannot be found by relying only on co-
occurrence. Finally, the remaining text topics are
extended by associating to them the words that are
neither noise nor already part of a topic (step 7). As
topics are defined at this point more precisely than at
step 4, the integration of words that are not strongly
linked to a topic seed can be safely performed by
relying on the average strength of their links in the
SNN graph with the words of the topic. After the
SNN algorithm is applied, a set of topics is associ-
ated to the text to segment, each of them being de-
fined as a subset of its vocabulary.
4 Using Text Topics for Segmentation
4.1 Topic segmentation using word reiteration
As TextTiling, the topic segmentation method of
Hearst (Hearst, 1994), the topic segmenter we pro-
pose, called F06, first evaluates the lexical cohesion
of texts and then finds their topic shifts by iden-
tifying breaks in this cohesion. The first step of
this process is the linguistic pre-processing of texts,
which is identical for topic segmentation to the pre-
482
processing described in Section 3.1 for the discover-
ing of text topics. The evaluation of the lexical cohe-
sion of a text relies as for TextTiling on a fixed-size
focus window that is moved over the text to segment
and stops at each sentence break. The cohesion in
the part of text delimited by this window is evalu-
ated by measuring the word reiteration between its
two sides. This is done in our case by applying the
Dice coefficient between the two sides of the focus
window, following (Jobbins and Evett, 1998). This
cohesion value is associated to the sentence break at
the transition between the two sides of the window.
More precisely, if Wl refers to the vocabulary of the
left side of the focus window and Wr refers to the
vocabulary of its right side, the cohesion in the win-
dow at position x is given by:
LCrec(x) =
2 ? card(Wl ? Wr)
card(Wl) + card(Wr)
(1)
This measure was adopted instead of the Cosine
measure used in TextTiling because its definition in
terms of sets makes it easier to extend for taking into
account other types of relations, as in (Jobbins and
Evett, 1998). A cohesion value is computed for each
sentence break of the text to segment and the final
result is a cohesion graph of the text.
The last part of our algorithm is mainly taken
from the LCseg system (Galley et al, 2003) and is
divided into three steps:
? computation of a score evaluating the probabil-
ity of each minimum of the cohesion graph to
be a topic shift;
? removal of segments with a too small size;
? selection of topic shifts.
The computation of the score of a minimum m be-
gins by finding the pair of maxima l and r around it.
This score is then given by:
score(m) = LC(l) + LC(r) ? 2 ? LC(m)2 (2)
This score, whose values are between 0 and 1, is a
measure of how high is the difference between the
minimum and the maxima around it. Hence, it fa-
vors as possible topic shifts minima that correspond
to sharp falls of lexical cohesion.
The next step is done by removing as a possible
topic shift each minimum that is not farther than 2
sentences from its preceding neighbor. Finally, the
selection of topic shifts is performed by applying a
threshold computed from the distribution of mini-
mum scores. Thus, a minimum m is kept as a topic
shift if score(m) > ??? ??, where ? is the average
of minimum scores, ? their standard deviation and ?
is a modulator (? = 0.6 in our experiments).
4.2 Using text topics to enhance segmentation
The heart of the algorithm we have presented above
is the evaluation of lexical cohesion in the focus win-
dow, as given by Equation 1. This evaluation is
also a weak point as card(Wl ? Wr) only relies on
word reiteration. As a consequence, two different
words that respectively belongs to Wl and Wr but
also belong to the same text topic cannot contribute
to the identification of a possible topical similarity
between the two sides of the focus window.
The algorithm F06T is based on the same princi-
ples as F06 but it extends the evaluation of lexical
cohesion by taking into account the topical proxim-
ity of words. The reference topics for judging this
proximity are of course the text topics discovered by
the method of Section 3. In this extended version,
the evaluation of the cohesion in the focus window
is made of three steps:
? computation of the word reiteration cohesion;
? determination of the topic(s) of the window;
? computation of the cohesion based on text top-
ics and fusion of the two kinds of cohesion.
The first step is identical to the computation of the
cohesion in F06. The second one aims at restrict-
ing the set of topics that are used in the last step
to the topics that are actually representative of the
content of the focus window, i.e. representative of
the current context of discourse. This point is espe-
cially important in the areas where the current topic
is changing because amplifying the influence of the
surrounding topics can lead to the topic shift being
missed. Hence, a topic is considered as represen-
tative of the content of the focus window only if it
matches each side of this window. In practice, this
matching is evaluated by applying the Cosine mea-
sure between the vector that represents one side of
483
the window and the vector that represents the topic1
and by testing if the resulting value is higher than a
fixed threshold (equal to 0.1 in the experiments of
Section 5). It must be noted that several topics may
be associated to the focus window. As the discov-
ering of text topics is done in an unsupervised way
and without any external knowledge, a theme of a
text may be scattered over several identified topics
and then, its presence can be characterized by sev-
eral of them.
The last step of the cohesion evaluation first con-
sists in determining for each side of the focus win-
dow the number of its words that belong to one of
the topics associated to the window. The cohesion
of the window is then given by Equation 3, that es-
timates the significance of the presence of the text
topics in the window:
LCtop(x) =
card(TWl) + card(TWr)
card(Wl) + card(Wr)
(3)
where TWi?{l,r} = (Wi ?Tw)? (Wl ?Wr) and Tw
is the union of all the representations of the topics
associated to the window. TWi corresponds to the
words of the i side of the window that belong to the
topics of the window (Wi?Tw) but are not part of the
vocabulary from which the lexical cohesion based
on word reiteration is computed (Wl ? Wr).
Finally, the global cohesion in the focus window
is computed as the sum of the two kinds of cohesion,
the one computed from word reiteration (see Equa-
tion 1) and the one computed from text topics (see
Equation 3).
5 Evaluation
5.1 Evaluation framework
The main objective of our evaluation was to verify
that taking into account text topics discovered with-
out relying on external knowledge can actually im-
prove a topic segmentation algorithm that is initially
based on word reiteration. Since the work of Choi
(Choi, 2000), the evaluation framework he proposed
has become a kind of standard for the evaluation of
topic segmentation algorithms. This framework is
1Each word of the topic vector has a weight equal to 1. In
the window vector, this weight is equal to the frequency of the
word in the corresponding side of the window.
based on the building of artificial texts made of seg-
ments extracted from different documents. It has at
least two advantages: the reference corpus is easy
to build as it does not require human annotations;
parameters such as the size of the documents or the
segments can be precisely controlled. But it has also
an obvious drawback: its texts are artificial. This is a
problem in our case as our algorithm for discovering
text topics exploits the fact that the words of a topic
tend to co-occur at the document scale. This hypoth-
esis is no longer valid for documents built accord-
ing to the procedure of Choi. It is why we adapted
his framework for having more realistic documents
without losing its advantages. This adaptation con-
French English
# source doc. 128 87
# source topics 11 3
segments/doc. 10 (84%) 10 (97%)8 (16%) 8 (3%)
sentences/doc. 65 68
plain words/doc. 797 604
Table 1: Data about our evaluation corpora
cerns the way the document segments are selected.
Instead of taking each segment from a different doc-
ument, we only use two source documents. Each of
them is split into a set of segments whose size is be-
tween 3 and 11 sentences, as for Choi, and an eval-
uation document is built by concatenating these seg-
ments in an alternate way from the beginning of the
source documents, i.e. one segment from a source
document and the following from the other one, un-
til 10 segments are extracted. Moreover, in order
to be sure that the boundary between two adjacent
segments of an evaluation document actually corre-
sponds to a topic shift, the source documents are se-
lected in such a way that they refer to different top-
ics. This point was controlled in our case by taking
documents from the corpus of the CLEF 2003 eval-
uation for crosslingual information retrieval: each
evaluation document was built from two source doc-
uments that had been judged as relevant for two dif-
ferent CLEF 2003 topics. Two evaluation corpora
made of 100 documents each, one in French and one
in English, were built following this procedure. Ta-
ble 1 shows their main characteristics.
484
5.2 Topic identification
As F06T exploits document topics, we also evalu-
ated our method for topic identification. This evalu-
ation is based on the corpus of the previous section.
For each of its documents, a reference topic is built
from each group of segments that come from the
same source document by gathering the words that
only appear in these segments. A reference topic is
associated to the discovered topic that shares with it
the largest number of words. Three complementary
measures were computed to evaluate the quality of
discovered topics. The main one is purity, which is
classically used for unsupervised clustering:
Purity =
k
?
i=1
vi
V P (Tdi) (4)
where P (Tdi), the purity of the discovered topic
Tdi, is equal to the fraction of the vocabulary of Tdi
that is part of the vocabulary of the reference topic
Tdi is assigned to, V is the vocabulary of all the dis-
covered topics and vi is the vocabulary of Tdi. The
second measure evaluates to what extent the refer-
ence topics are represented among the discovered
topics and is equal to the ratio between the num-
ber of discovered topics that are assigned to a refer-
ence topic (assigned discovered topics) and the num-
ber of reference topics. The last measure estimates
how strongly the vocabulary of reference topics is
present among the discovered topics and is equal to
the ratio between the size of the vocabulary of the
assigned discovered topics and the size of the vo-
cabulary of reference topics. Table 2 gives the mean
purity reference
topics (%)
ref. topic
vocab. (%)
French 0.771 (0.117) 89.5 (23.9) 29.9 (7.8)
English 0.766 (0.082) 99.0 (10.0) 31.6 (5.3)
Table 2: Evaluation of topic identification
of each measure, followed by its standard deviation.
Results are globally similar for French and English.
They show that our method for topic identification
builds topics that are rather pure, i.e. each of them is
strongly tied to a reference topic, but their content is
rather sparse in comparison with the content of their
associated reference topics.
5.3 Topic segmentation
For validating the hypothesis that underlies our
work, we applied F06 and F06T to find the topic
bounds in the documents of our two evaluation cor-
pora. Moreover, we also tested four well known seg-
menters on our corpora to compare the results of F06
and F06T with state-of-the-art algorithms. We clas-
sically used the error metric Pk proposed in (Beefer-
man et al, 1999) to measure segmentation accuracy.
Pk evaluates the probability that a randomly cho-
sen pair of sentences, separated by k sentences, is
wrongly classified, i.e. they are found in the same
segment while they are actually in different ones
(miss) or they are found in different segments while
they are actually in the same one (false alarm). We
also give the value of WindowDiff (WD), a variant of
Pk proposed in (Pevzner and Hearst, 2002) that cor-
rects some of its insufficiencies. Tables 3 and 4 show
systems Pk pval(F06) pval(F06T) WD
U00 25.91 0.003 1.3e-07 27.42
C99 27.57 4.2e-05 3.6e-10 35.42
TextTiling* 21.08 0.699 0.037 27.43
LCseg 20.55 0.439 0.111 28.31
F06 21.58  0.013 27.83
F06T 18.46 0.013  24.05
Table 3: Evaluation of topic segmentation for the
French corpus (Pk and WD as percentages)
the results of our evaluations for topic segmentation
(smallest values are best results). U00 is the sys-
tem described in (Utiyama and Isahara, 2001), C99
the one proposed in (Choi, 2000) and LCseg is pre-
sented in (Galley et al, 2003). TextTiling* is a vari-
ant of TextTiling in which the final identification of
topic shifts is taken from (Galley et al, 2003). All
these systems were used as F06 and F06T without
fixing the number of topic shifts to find. Moreover,
their parameters were tuned for our evaluation cor-
pus to obtain their best results. For each result, we
also give the significance level pval of its difference
for Pk with F06 and F06T, evaluated by a one-side
t-test with a null hypothesis of equal means. Lev-
els lower than 0.05 are considered as statistically
significant (bold-faced values). The first important
point to notice about these tables is the fact that
485
systems Pk pval(F06) pval(F06T) WD
U00 19.42 0.048 4.3e-05 21.22
C99 21.63 1.2e-04 1.8e-09 30.64
TextTiling* 15.81 0.308 0.111 19.80
LCseg 14.78 0.043 0.496 19.73
F06 16.90  0.010 20.93
F06T 14.06 0.010  18.31
Table 4: Evaluation of topic segmentation for the
English corpus (Pk and WD as percentages)
F06T has significantly better results than F06, both
for French and English. Hence, it confirms our hy-
pothesis about the interest of taking into account the
topics of a text for its segmentation, even if these
topics were discovered in an unsupervised way and
without using external knowledge. Moreover, F06T
have the best results among all the tested algorithms,
with a significant difference in most of the cases.
Another notable point about these results is their
stability across our two corpora, even if these cor-
pora are quite similar. Whereas F06 and F06T were
initially developed on a corpus in French, their re-
sults on the English corpus are comparable to their
results on the French test corpus, both for the dif-
ference between them and the difference with the
four other algorithms. The comparison with these
algorithms also illustrates the relationships between
them: TextTiling*, LCseg, F06 and F06T share a
large number of principles and their overall results
are significantly higher than the results of U00 and
C99. This trend is different from the one observed
from the Choi corpus for which algorithms such C99
or U00 have good results (Pk for C99, U00, F06 and
F06T is respectively equal to 12%, 10%, 14% and
14%). This means probably that algorithms with
good results on a corpus built as the Choi corpus will
not necessarily have good results on ?true? texts,
which agrees with (Georgescul et al, 2006). Finally,
we can observe that all these algorithms have better
results on the English corpus than on the French one.
As the two corpora are quite similar, this difference
seems to come from their difference of language,
perhaps because repetitions are more discouraged in
French than in English from a stylistic viewpoint.
This tends to be confirmed by the ratio between the
size of the lemmatized vocabulary of each corpus
and their number of tokens, equal to 8% for the
French corpus and to 5.6% for the English corpus.
6 Related Work
One of the main problems addressed by our work
is the detection of the topical similarity of two text
units. We have tackled this problem following an
endogenous approach, which is new in the topic seg-
mentation field to our knowledge. The main advan-
tage of this option is that it does not require external
knowledge. Moreover, it can integrate relations be-
tween words, such as proper nouns for instance, that
are unlikely to be found in an external resource.
Other solutions have been already proposed to
solve the problem we consider. Most of them consist
of two steps: first, they automatically build a seman-
tic representation of words from the co-occurrences
collected from a large corpus; then, they use this
representation for enhancing the representation of
each text unit to compare. This overall principle is
implemented with different forms by several topic
segmenters. In CWM (Choi et al, 2001), a variant
of C99, each word of a sentence is replaced by its
representation in a Latent Semantic Analysis (LSA)
space. In the work of Ponte and Croft (Ponte and
Croft, 1997), the representations of sentences are ex-
panded by adding to them words selected from an
external corpus by the means of the Local Context
Analysis (LCA) method. Finally in (Caillet et al,
2004), a set of concepts are learnt from a corpus
in an unsupervised way by using the X-means clus-
tering algorithm and the paragraphs of documents
are represented in the space defined by these con-
cepts. In fact, the way we use relations between
words is closer to (Jobbins and Evett, 1998), even
if the relations in this work come from a network of
co-occurrences or a thesaurus rather than from text
topics. In both cases the similarity of two text units
is determined by the proportion of their words that
are part of a relation across the two units.
More globally, our work exploits the topics of a
text for its segmentation. This kind of approach
was also explored in (Blei and Moreno, 2001) where
probabilistic topic models were built in an unsuper-
vised way. More recently, (Purver et al, 2006) has
also proposed a method for unsupervised topic mod-
eling to address both topic segmentation and identi-
486
fication. (Purver et al, 2006) is closer to our work
than (Blei and Moreno, 2001) because it does not re-
quire to build topic models from a corpus but as in
our case, its results do not outperform LCseg (Galley
et al, 2003) while its model is far more complex.
7 Conclusion and Future Work
In this article, we have first proposed an unsuper-
vised method for discovering the topics of a text
without relying on external knowledge. Then, we
have shown how these topics can be used for im-
proving a topic segmentation method based on word
reiteration. Moreover, we have proposed an adapta-
tion of the evaluation framework of Choi that aims
at building more realistic evaluation documents. Fi-
nally, we have demonstrated the interest of the
method we present through its evaluation both on a
French and an English corpus.
However, the solution we have proposed for im-
proving the identification of topical similarities be-
tween text excerpts cannot completely make up for
not using any external knowledge. Hence, we plan
to use a network of lexical co-occurrences, which is
a source of knowledge that is easy to build automati-
cally from a large corpus. More precisely, we intend
to extend our method for discovering text topics by
combining the co-occurrence graph of a document
with such a network. This network could also be
used more directly for topic segmentation as in (Job-
bins and Evett, 1998).
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1):177?210.
David M. Blei and Pedro J. Moreno. 2001. Topic seg-
mentation with an aspect hidden markov model. In
24th ACM SIGIR, pages 343?348.
Marc Caillet, Jean-Fran?ois Pessiot, Massih Amini, and
Patrick Gallinari. 2004. Unsupervised learning with
term clustering for thematic segmentation of texts. In
RIAO?04, pages 1?11.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent semantic analysis for text seg-
mentation. In EMNLP?01, pages 109?117.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In NAACL?00, pages
26?33.
Levent Ert?z, Michael Steinbach, and Vipin Kuma. 2001.
Finding topics in collections of documents: A shared
nearest neighbor approach. In Text Mine?01, Work-
shop of the 1st SIAM International Conference on
Data Mining.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In ACL?03, pages 562?569.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. An analysis of quantitative aspects in
the evaluation of thematic segmentation algorithms.
In 7th SIGdial Workshop on Discourse and Dialogue,
pages 144?151.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In ACL?94, pages 9?16.
Amanda C. Jobbins and Lindsay J. Evett. 1998. Text seg-
mentation using reiteration and collocation. In ACL-
COLING?98, pages 614?618.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In ACL?93 (Student Session),
pages 286?288.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103?139.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36.
Jay M. Ponte and Bruce W. Croft. 1997. Text segmen-
tation by topic. In First European Conference on re-
search and advanced technology for digital libraries.
Matthew Purver, Konrad P. K?rding, Thomas L. Grif-
fiths, and Joshua B. Tenenbaum. 2006. Unsupervised
topic modelling for multi-party spoken discourse. In
COLING-ACL 2006, pages 17?24.
N. Stokes, J. Carthy, and A.F. Smeaton. 2002. Segment-
ing broadcast news streams using lexical chains. In
STAIRS?02, pages 145?154.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
ACL?01, pages 491?498.
J.P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mul-
bregt. 1998. A hidden markov model approach to text
segmentation and event tracking. In ICASSP, pages
333?336.
487
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 709?720, Dublin, Ireland, August 23-29 2014.
Improving distributional thesauri by exploring the graph of neighbors
Vincent Claveau
IRISA - CNRS
Campus de Beaulieu
35042 Rennes, France
vincent.claveau@irisa.fr
Ewa Kijak
IRISA - Univ. of Rennes 1
Campus de Beaulieu
35042 Rennes, France
ewa.kijak@irisa.fr
Olivier Ferret
CEA, LIST
LVIC
91191 Gif-sur-Yvette, France
olivier.ferret@cea.fr
Abstract
In this paper, we address the issue of building and improving a distributional thesaurus. We
first show that existing tools from the information retrieval domain can be directly used in order
to build a thesaurus with state-of-the-art performance. Secondly, we focus more specifically
on improving the obtained thesaurus, seen as a graph of k-nearest neighbors. By exploiting
information about the neighborhood contained in this graph, we propose several contributions.
1) We show how the lists of neighbors can be globally improved by examining the reciprocity of
the neighboring relation, that is, the fact that a word can be close of another and vice-versa. 2)
We also propose a method to associate a confidence score to any lists of nearest neighbors (i.e.
any entry of the thesaurus). 3) Last, we demonstrate how these confidence scores can be used
to reorder the closest neighbors of a word. These different contributions are validated through
experiments and offer significant improvement over the state-of-the-art.
1 Introduction
Distributional thesauri are useful for many NLP tasks and their construction is an issue widely discussed
for several years (Grefenstette, 1994). However this is still a very active research field, maintained by
the increasingly large number of available corpus and by many applications. These thesauri associate
each of their entry with a list of words that are desired semantically close to the entry. This notion of
proximity varies (synonymy, other paradigmatic relations, syntagmatic relations (Budanitsky and Hirst,
2006; Adam et al., 2013, for a discussion)), but the methods used for the automatic construction of
thesauri are often shared. For the most part, these methods rely on the distributional hypothesis of (Firth,
1957): each word is characterized by the set of contexts in which it appears, and the semantic proximity
of two words can be inferred from the proximity of their contexts. This hypothesis has been implemented
in different ways, and several propositions to improve the results have been explored (see next section
for a state of the art).
The work presented in this article are part of this framework. We propose several contributions on
the creation of these distributional thesauri and their improvement. We first show that models from
information retrieval (IR) can provide information on semantic relationships, and are thus adapted to the
task of creating these thesauri. In addition, they offer very competitive results compared to the state of
the art, while enjoying existing tools (Section 3).
The most important part of our work then focuses on the exploitation of such semantic neighborhood
relations. The IR models indeed provide lists ordering all words by decreasing similarity, that form a
graph of nearest neighbors. We propose to take advantage of some of the neighborhood information
contained in this graph and we derive three contributions.
1) We globally improve neighbor lists by taking into account the reciprocity of the neighborhood rela-
tionship, that is to say the fact that a word is a close neighbor of another and vice versa (Section 4).
2) We also propose a method that associates each neighbor list (i.e. each entry of the thesaurus built) with
a confidence score (Section 5). This method uses the nearest neighbor graph to estimate the probabilities
that a given word is the i-th neighbor of another word.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
709
3) Finally, on the basis of this work, we show how to use this confidence score and these probabilities
to reorder the list of nearest neighbors (Section 6). To achieve this goal, we model the reranking as an
optimization problem of assignments, solved by the Hungarian algorithm (Kuhn and Yaw, 1955).
2 Related work
The notion of distributional thesaurus, as it was initially defined by Grefenstette (1994), followed by Lin
(1998a) and Curran and Moens (2002), is not often considered specifically, probably because of its strong
link with the notion of semantic similarity. As a consequence, the improvement of distributional thesauri
has been first a side effect of the improvement of the distributional similarity measures used for their
building and more precisely, of the distributional data they rely on. Both the nature of the constituents of
distributional contexts and their weighting have been considered in this regard. Concerning their weight-
ing, Broda et al. (2009) proposed to turn the weights of context constituents into ranks to make them less
dependent on a specific weighting function while Zhitomirsky-Geffet and Dagan (2009), extended by Ya-
mamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituents
in the distributional context of a word according to the similarity with its semantic neighbors.
The nature of distributional contexts has been first considered through the distinction between window-
based and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002). However, most of the
work related to this issue has focused on the fact that the ?traditional? representation of distributional
contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several meth-
ods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer and
Dumais, 1997), extended for syntactic co-occurrents (Pad?o and Lapata, 2007), to Random Indexing
(Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical
representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013).
The work we present in this article follows a different perspective as our objective is to improve an
existing distributional thesaurus by relying on its structure through a reranking of its neighbors. Such ap-
proach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighbors
of an entry in an initial thesaurus for reweighting its distributional representation and finally, reranking
its neighbors. Ferret (2013) proposed a more indirect method in which the reranking is based on the
downgrading of the neighbors that are detected as not similar to their entry through a pseudo word sense
disambiguation task: such detection occurs if a certain proportion of the occurrences of a neighbor are
not tagged as the entry. Finally, the closest work to ours is (Ferret, 2012), which selects in an unsu-
pervised way a set of examples of semantically similar words from an initial thesaurus for training a
classifier whose decision function is used for reranking the neighbors of each entry. Its unsupervised
selection of examples is more precisely based on the symmetry of semantic similarity relations.
As Ferret (2012), our work exploits a certain kind of symmetry in the relation of distributional neigh-
borhood between words but extends it to a larger scale by considering the initial thesaurus as a k-nearest
neighbor graph and using the relations in this graph for reranking the neighbors of each entry, similarly
to Pedronette et al. (2014) in the context of image retrieval.
3 IR models for building distributional thesauri
3.1 Principles
As mentioned in the state of the art, distributional approaches aim to calculate similarities between
textual representations of word contexts. Methods to calculate similarities from IR seem then relevant
for this problem. For a given word, the set of contexts of all its occurrences is considered as a document.
The proximity between two words is then measured on their contexts by a similarity function from IR.
This idea has many links with the work from the state of the art, but seems relatively unexplored, with
the exeption of (Vechtomova and Robertson, 2012) in the specific context of similar named entities. It
offers the advantage of being easily implementable because of the numerous IR tools available. Some
adaptations are of course required. In contrast to IR, the stop words are kept as well as their positions
relative to the considered occurrence. Lemmatization instead of stemming is performed. For example, in
the excerpt: ?... all forms of restrictions on freedom of expression, threats ...?, the indexing terms restriction-2,
710
on-1, of+1, expression+2 are added to the context of freedom noted C(freedom). The whole set of collected
contexts for a word is used as a query in order to find its distributional neighbors. According to an IR
similarity measure, the nearest words of this query (those whose contexts are closest) are returned.
We tested some of the most classical similarity measures used in IR: Hellinger (Escoffier, 1978;
Domeng`es and Volle, 1979), TF-IDF/cosinus, and Okapi-BM-25 (Robertson et al., 1998). The last
model can be seen as a variation of TF-IDF that better takes into account the difference between doc-
ument sizes. This point is of importance since in our case the documents (namely the set of contexts
of a word) are actually of very variable sizes, due to the very variable number of occurrences of each
word. The Okapi-BM25 similarity between a word w
i
(C(w
i
) being the query), and w
j
(C(w
j
) being a
document), is given in Eqn 1.
similarity(w
i
, w
j
) =
?
t?C(w
i
)
(k
3
+ 1) ? qtf
k
3
+ qtf
?
tf ? (k
1
+ 1)
tf + k
1
? (1? b+ b ?
dl(C(w
j
))
dl
avg
)
? log
n? df(t) + 0.5
df(t) + 0.5
(1)
qtf is the number of occurrences of the word t in the query (C(w
i
)), dl is the size of C(w
j
)), dl
avg
the average size of all contexts, n is the number of documents (that means in our case the number of
considered words/thesaurus entries). df(t) is the number of contexts (C(?)) containing t. Finally, k
1
, k
3
and b are some constants, with default values k
1
= 2, k
3
= 1000 and b = 0.75. Details of these classical
IR models are not given here but can be found in (Manning et al., 2008).
In the following experiments, the context of an occurrence is defined by the two words before and
after the occurrence, and we also use an adjusted version of Okapi-BM25 similarity that enhances the
influence of the document size and gives more importance to the most discriminating context words by
setting b = 1 and putting the IDF squared to give more importance to the most discriminating context
words.
3.2 Experimental setup
For the sake of comparison, we use in our experiments the data and baselines provided by Ferret (2013).
The corpus used to build the distributional thesaurus is AQUAINT-2. It is a collection of articles from
press containing about 380 million words. The thesaurus entries are all the nouns in the corpus with
a frequency > 10. That represents 25,000 entries (i.e. unique nouns), denoted by n in the remaining.
The corpus is labeled in parts of speech by TreeTagger (Schmid, 1994). In this way, we can identify the
names that form the thesaurus entries and thus compare to existing work. However this information is
not used to build the thesaurus, ensuring the portability of the method to other languages, similarly to
(Freitag et al., 2005).
To evaluate the built thesauri, WordNet 3.0 synonyms (Miller, 1990) and Moby (Ward, 1996) are used
as references, either separately, or jointly. These two resources exhibit quite different and complementary
characteristics: on the one hand, WordNet indicates strong paradigmatic links between words (synonyms
or quasi-synonyms). On the other hand, Moby groups words sharing more extended syntagmatic and
paradigmatic relations, including synonymy, hyper/hypo-nymy, meronymy, but also many more complex
types such as the composition of co-hyponymy and hyponymy (abolition ? annulment, cataclysm ?
debacle) or hypernymy and co-hyponymy (abyss ? rift, algorithm ? routine). As a result, WordNet
provides lists of 3 neighbors on average for the 10,473 names of the corpus it covers, while Moby
provides lists of 50 neighbors on average for 9,216 names. When combined, the two resources provide a
reference of 38 neighbors on average for 12,243 names. It is this combination of WordNet and Moby that
will be used as the main reference in all evaluations of this article. Some results restricted to WordNet or
Moby only as reference are also given in some cases to illustrate the impact of our methods on semantic
simlarity versus semantic relatedness relations.
Through this intrinsic evaluation framework, the semantic neighbors of about half of the entries of
our thesauri are evaluated, which can be considered as a very large evaluation set compared to classical
benchmarks such as WordSim 353 for instance (Gabrilovich and Markovitch, 2007). This kind of intrin-
sic evaluation is of course limited by the relations that are present in the resources used as gold standards,
often restricted to ?classical? relation types such as synonymy or hypernymy. In our case, this limitation
711
Reference Method MAP R-Prec P@1 P@5 P@10 P@50 P@100
WordNet + Moby
Ferret 2013 base 5.6 7.7 22.5 14.1 10.8 5.3 3.8
Ferret 2013 best rerank 6.1 8.4 24.8 15.4 11.7 5.7 3.8
Hellinger 2.45 2.89 9.73 6.28 5.31 4.12 3.30
TF-IDF 5.40 7.28 21.73 13.74 9.59 5.17 3.49
Okapi-BM25 6.72 8.41 24.82 14.65 10.85 5.16 3.66
Okapi-BM25 ajusted 8.97 10.94 31.05 18.44 13.76 6.46 4.54
Ferret 2014 synt 7.9 10.7 29.4 18.9 14.6 7.3 5.2
WordNet
Ferret 2013 base 9.8 8.2 11.7 5.1 3.4 1.1 0.7
Ferret 2013 best rerank 10.7 9.1 12.8 5.6 3.7 1.2 0.7
Okapi-BM25 ajusted 14.17 12.22 16.97 7.10 4.47 1.41 0.84
Ferret 2014 synt 13.3 11.5 15.6 6.9 4.5 1.5 0.9
Moby
Ferret 2013 base 3.2 6.7 24.1 16.4 13.0 6.6 4.8
Ferret 2013 best rerank 3.5 7.2 26.5 17.9 14.0 6.9 4.8
Okapi-BM25 ajusted 5.69 9.14 32.18 21.37 16.42 8.02 5.69
Ferret 2014 synt 4.8 9.4 30.6 21.7 17.3 8.9 6.5
Table 1: Performance of IR models for distributional thesaurus building with the references WordNet,
Moby and WordNet+Moby
holds true for WordNet?s synonyms but can be considered as far less restrictive for the related words of
Moby, due to the diversity of their underlying relation types.
3.3 Results
For a given name, our approach by IR models returns a list of names ordered by decreasing similarity.
This list is compared to the reference one by computing several classical measures (expressed in % in the
following): the precision after k first names, denoted P@k, the Mean Average Precision (MAP) which
is the mean of the average precision scores for each query after a reference synonym is found, and the
R-precision (precision atR-th position in the ranking of results, whereR is the number of relevant names
for the query).
Table 1 indicates the performance of different models of IR similarities. For purposes of comparison,
we show the results obtained under the same conditions by Ferret (2013), with both a state of the art
approach based on using cosine similarity over pointwise mutual information between contexts (referred
as base in the table), and an improved version by learning as described in section 2 (referred as best
rerank). We also give the results on the same corpus on an approach based on syntactic co-occurrents
(Ferret, 2014 in press), extracted with the Minipar syntactic parser as in (Lin, 1998b).
In these early results, it is worth noting that some IR similarities are quite inefficient, including the
TF alone or Hellinger similarity. This is hardly surprising since these similarities use very basic weights
that do not enhance the discriminative contexts of words. The similarities that include a notion of IDF
get better results in this. Okapi BM25-based similarities offer good results. The standard Okapi version
yields performance similar to the state of the art, and the adjusted version even widely outperforms the
two systems from Ferret (2013), in particular in terms of overall quality (measured by the MAP). More-
over, the results of this adjusted version are comparable to those obtained with syntactic co-occurrents
while it only exploits window-based co-occurents, known to give usually worst results than syntactic
co-occurrents, without even lemmatization. This latest version of the system serves as reference for the
rest of this article.
4 Reciprocity in the graph of k-NN
Computing all the similarities between all pairs of words produces a weighted graph of neighbors: Each
word is connected with certain strength to the n other words. The results above do not reflect this
structure. The following sections aim to examine how take advantage of the neighborhood relations
embedded in this graph. It must be first noted that some of the IR similarity measures we used are not
symmetric, including Okapi-BM25. The similarity between a word w
i
, used as query, and another word
w
j
does not give the same value as the similarity between the query w
j
and w
i
. Apart from that, even if
712
the similarity measure it-self is symmetric, nearest neighbor relationships are not.
It seems then reasonable to assume that the reciprocity between two adjacent words (each belonging to
the k nearest neighbors of the other) is a sign of confidence on the proximity between these words. Using
this information to improve the previous results is discussed in this section. In the following, ?
w
i
(w
j
)
denotes the rank of the word w
j
in the list of neighbors of w
i
. ?
w
i
(w
j
) thus varies from 1 to n.
4.1 Distributional neighborhood graph
Reciprocal relationship in distributional neighborhood has already been discussed and used in some
work (Ferret, 2013) on distributional semantic, or more generally, on nearest neighbors graphs (Pe-
dronette et al., 2014). In these papers, the reciprocity was considered for giving a new similarity score in
a simple way. For a word w
i
and its neighbor w
j
, the maximal or the minimal rank between ?
w
i
(w
j
) and
?
w
j
(w
i
) is taken as the new rank. These two operators have too severe effects as only one rank is taken
into consideration to decide the final score. This leads to highly degraded performance as shown later.
Many other aggregation operators have however been proposed in other contexts with a behavior may
be more appropriate to the task, including fuzzy logic (Detyniecki, 2000). These operators carry some
semantic that allow to comprehend their behavior, such as T-norms (fuzzy logic AND) and S-norms (or
T-conorms, fuzzy OR).
In this section, we test some of these operators without claiming to be exhaustive. These are defined
on [0, 1]
2
, 1 being the certainty. They are used to generate a new similarity score according to:
score
w
i
(w
j
) = Aggreg(1? ?
w
i
(w
j
)/n, 1? ?
w
j
(w
i
)/n) (2)
where Aggreg is an aggregation operator. The new scores are then used to produce a new list of nearest
neighbors of w
i
(the higher the score, the greater proximity is proven). We thus perceive the semantic
associated with these operators. For example, if the aggregation function is max, we get the expected
behavior of the fuzzy OR associated with this S-norm: w
j
will be ranked very close to w
i
in the new list
if w
j
was close to w
i
or if w
i
was close to w
j
. For the T-norm min, this happens if w
j
is close to w
i
and
w
i
is close to w
j
.
4.2 Results
Besides the min and max aggregation operators, Figure 1 reports the results obtained with the following
T-norms (or T-norm families dependent on a parameter ?) used as aggregation function Aggreg:
T
Prob
(x, y) = x ? y
T
Lukasiewicz
(x, y) = max(x+ y ? 1, 0)
T
Hamacher
(x, y) =
x?y
?+(1??)?(x+y?x?y)
; ? ? 0
T
Yager
(x, y) = max(0, 1?
?
?
(1? x)
?
+ (1? y)
?
) ; ? > 0
We also tested the standard related S-norms, obtained by generalization of the De Morgan?s law:
S(x, y) = 1 ? T (1 ? x, 1 ? y). For the T-norm families dependent on a parameter, we varied this
parameter value in a systematic way. The results reported correspond to the parameter values that maxi-
mize the MAP.
All these operators get very different results. Some operators, such as min, max, Lukasiewicz, and
others for some ?, induce a threshold effect which degrades the performance: they return a default value
generating too much ex aequo among the neighbors, for some values of ?
w
i
(w
j
) and ?
w
j
(w
i
). T-norms,
focusing on pairs of words symmetrically close to each other, are too restrictive. This is consistent with
the conclusions of the work cited: if the reciprocity condition is applied too strictly, it does not improve
the nearest neighbor lists over all the words. In contrast, S-norms seem better able to take advantage
of the ranking. The improvements are modest in terms of overall quality (MAP), but important at some
ranks (e.g. P@10).
Finally, it is important to note that these results depend heavily on the resource used as reference.
We tested the aggregation rank with S
Hamacher
, ? = 0.95, on Moby and WordNet references separately.
Results are given in Table 2. Because Wordnet is based on a synonymy relationship strong enough (and
therefore reciprocal), the performance gains on WordNet are much higher than on Moby.
713
Figure 1: Performance of reciprocal rank aggregation, on the reference WordNet+Moby
Reference MAP R-Prec P@1 P@5 P@10 P@50 P@100
WordNet 9.30 (+3.75) 11.06 (+2.03) 30.42 (-2.53) 19.29 (+4.58) 14.71 (+6.92) 7.09 (+9.78) 4.86 (+7.07)
+ Moby
WordNet 15.05 (+6.23) 12.81 (+4.81) 17.55 (+3.41) 7.96 (+12.16) 5.07 (+13.30) 1.63 (+15.69) 0.94 (+12.23)
Moby 5.90 (+3.65) 11.86 (+4.14) 31.77 (-1.27) 21.65 (+1.34) 17.0 (+3.53) 8.42 (+5.01) 5.92 (+4.12)
Table 2: Performance and gains (%) of reciprocal rank aggregation, relatively to adjusted Okapi-BM25,
on the references WordNet and Moby taken separately, with aggregation operator S
Hamacher
, ? = 0.95
5 Confidence estimation for a distributional neighborhood list
In the previous section, the rank ofw
i
in the list of neighbors ofw
j
is used to improve the ranking ofw
j
in
the list of neighbors of w
i
. We can also be interested in a more general way to the relative positions of w
i
and w
j
in all neighbor lists of all the words. Thereby, we expect to derive a more complete information.
As a first step, we define a confidence criterion associated with each list of nearest neighbors, only based
on the neighborhood graph.
5.1 Principle
We make the following assumption: the nearest neighbor list of a word w is probably of good quality if
the distance (in terms of rank) between w and each of its neighbors w
i
, denoted ?(w,w
i
), is consistent
with the distance observed between these same words (w, w
i
) in other lists. The intuition here is that
words supposed to be close should also be found close to the same other words. If k nearest neighbors
of w have this property, then we attribute a high confidence to the neighbor list of w.
Formally, we define the confidence of the k-nearest neighbor list of w by:
Q(w) =
?
{w
i
|?
w
(w
i
)?k}
p(?(w,w
i
) = ?
w
(w
i
)) (3)
where p(?(w,w
i
) = ?
w
(w
i
)) is the probability that w
i
is the ?
w
(w
i
)-th neighbor of w. The problem is
then to estimate the probability distribution p(?(w,w
i
)) for each pair of words (w,w
i
). To achieve this
goal, we use the Parzen windows which is a method for nonparametric density estimation. We describe
below how this classic method (Parzen, 1962; Wasserman, 2005) is applied in our context.
5.2 Parzen-window density estimation
Let x
ab
= ?(w
a
, w
b
) be the distance (in terms of ranks) between two words w
a
and w
b
in a list of neigh-
bors of any given word. Considering the n words of the thesaurus, we have a sample of n realizations
assumed iid: (x
1
ab
, x
2
ab
, ... , x
n
ab
), which are the observed distances between w
a
et w
b
in each (complete)
neighbor list of each word. These counts can be represented by an histogram as illustrated in Figure 2 (a).
Using the Parzen window technique, we can then estimate the probability density of x
ab
with a kernel
714
(a) (b)
Figure 2: (a) Example of two distributions of distances x
ab
and x
ac
between a word w
a
and two of its
neighbors w
b
and w
c
, represented as histograms (blue and red) (b) Same distributions represented by
densities estimated with the Parzen-windows method.
density estimator with Eqn 4 where h is a smoothing parameter called the bandwidth, andK(
?
) is a kernel
that we choose Gaussian. The resulting density is illustrated in Figure 2 (b).
p?
h
(x
ab
) =
1
nh
n
?
i=1
K
(
x
ab
? x
i
ab
h
)
with K(u) =
1
?
2pi
e
?u
2
2
(4)
Thus, the resulting probability is a mixture of Gaussians centered on each x
i
ab
. These methods are
known to be sensitive to the bandwidth h, which controls the regularity of the estimation. The problem of
choosing h is crucial in density estimation and was widely discussed in the literature. We use Silverman?s
rule of thumb (Silverman, 1986, page 48, eqn (3.31)) to set its value. Under the assumption of normality
of the underlying distribution, this rule provides a simple way to calculate the optimal parameter h when
Gaussian functions are used to approximate univariate data (Eqn 5 where ?? is the standard deviation of
the samples, and q
3
? q
1
is the interquartile range).
?
h = 0.9 min(??,
q
3
? q
1
1.34
) n
?
1
5
(5)
Once these probabilities have been estimated on each of the k-nearest neighbors ofw, we can calculate
the confidence score Q(w). The complexity of this estimation for all neighbor lists is O(k ? n
2
).
5.3 Using the confidence score
The expected benefit of using the confidence score is to have an a priori indication on the quality of a
neighbor list for a given word. Such a score may thus be useful for many applications using thesauri
produced by our approach (e.g. for expanding queries in information retrieval tasks). An evaluation
of the confidence score through such applications would certainly be the most suitable, but beyond the
scope of this article. We use default direct assessment towards the MAP: we measure the correlation
between MAP and the confidence score, the idea being that an entry with a neighbor list of low quality
matches an entry with low MAP. We use Spearman?s correlation ? and the Kendall?s rank correlation
coefficient ? , which do not make any assumption about linearity and compare only the order of words
classified according to their MAP with the order according to their confidence score. The results of these
coefficients are given in Table 3, along with p-value of the associated test of significance. A coefficient
715
Correlation coefficient value statistical significance
Kendall ? 0.37 p < 10
?64
Spearman ? 0.51 p < 10
?64
Table 3: Correlation coefficient values between the MAP and the confidence score, and their statistical
significance (p-value).
Figure 3: Average MAP computed on words with a confidence score lower than a threshold q (x-axis,
log-scale), and cumulative proportion of concerned words.
value of 1 indicates a perfect correlation, 0 no correlation and -1 an inverse correlation. A low p-value,
for example < 0.05, indicates a statistically significant result. The confidence scores are obtained with
k = 20. Other experiments, not reported here, show that this parameter k has little influence on the
correlation, for values between 5 and 100.
These measures attest to some statistically significant correlation between our confidence score and the
MAP, however this correlation is imperfect and non-linear. We compute the average MAP on neighbor
lists with a confidence score lower than a threshold q. Figure 3 represents the average MAP (y-axis) in
function of the threshold q (x-axis). It shows that the confidence score is still a good indicator of quality,
as the MAP decreases with the confidence score.
The confidence score can be used to improve the performance of aggregation techniques presented in
Section 4 by integrating it in the final score:
score
w
i
(w
j
) = Q(w
j
) ? Aggreg(1? ?
w
i
(w
j
)/n, 1? ?
w
j
(w
i
)/n) (6)
As shown in Table 4, using this information allows even greater gains than those reported in the previous
section (a Wilcoxon test (p < 0.05) (Hull, 1993) is performed to ensure that the differences are statisti-
cally significant; non-significant ones are shown in italics). In the next section, we propose another use
of the confidence scores to improve results more specifically on the head of the lists, that is to say on the
neighbors judged closest.
Method MAP R-Prec P@1 P@5 P@10 P@50 P@100
S
Hamacher
? = 0.95 9.61 (+7.20) 11.59 (+5.85) 30.86 (-0.53) 19.52 (+5.83) 14.76 (+7.24) 7.03 (+8.88) 4.93 (+8.67)
Table 4: Performance gains (%) by reciprocal rank aggregation using the confidence score, on Word-
Net+Moby reference.
716
Target MAP R-Prec P@1 P@5 P@10
all words 9.16 (+2.17) 11.24 (+2.76) 30.73 (-1.02) 19.30 (+4.64) 14.37 (+4.44)
the third of words with the lowest Q(w) 9.55 (+6.44) 11.81 (+7.99) 31.85 (+2.56) 20.43 (+10.81) 15.46 (+12.37)
Table 5: Performance gains (%) of reranking with the Hungarian algorithm.
6 Local reranking
The previous method gives an overall score to the list, but one can also make use of the individual
ranking probabilities p(?(w
i
, w
j
)), estimated according to the method of Parzen windows. For a given
word w, we have for each of its neighbors w
j
the probability of his current rank: p(?(w,w
j
)) = ?
w
(w
j
).
For a given neighbor w
j
, we can also calculate the probability of any other rank ? : p(?(w,w
j
)) = ?
with ? = 1, 2, ... In this section, we propose to rely on these more local information to improve the
performance by reranking the k-nearest neighbors.
6.1 Reranking by the Hungarian algorithm
A simple approach would be to reorder the list based on this criterion, from the most probable neighbors
to the least ones. But ranking probability estimation for each word is imperfect, and such a reranking
strongly degrades the results. We therefore propose instead a method to rerank the k-nearest neighbors
on a more local and controlled manner: a word that was not originally in the k-nearest neighbors can not
become a k-nearest neighbor, and a word can not be reranked too far from its original rank.
Our problem is expressed by the following matrix M
profit
. The rows correspond to words in their
original ranks (denoted w
1
to w
k
), the columns to new ranks ? at which these words can be assigned,
and matrix values are the probabilities of each word w
j
to appear at rank ? . Given these probabilities,
the goal is to find the most likely permutation of the k-nearest neighbors.
M
profit
=
?
?
?
p(?(w,w
1
) = 1) ? ? ? p(?(w,w
1
) = k)
.
.
.
.
.
.
.
.
.
p(?(w,w
k
) = 1) ? ? ? p(?(w,w
k
) = k)
?
?
?
As pointed out, we want to avoid that an initially very close neighbor was moved far away and vice
versa. This constraint is added by multiplying the matrixM
profit
by a penalty matrixM
penalty
(see below)
with the Hadamard product (element by element matrix product, denoted ?).
M
penalty
=
?
?
?
?
?
1
k?1
k
? ? ? 0
k?1
k
1 ? ? ?
1
k
.
.
.
.
.
.
.
.
.
0
1
k
? ? ? 1
?
?
?
?
?
We then face a combinatorial optimization problem which can be solved in polynomial time by the
Hungarian method (Kuhn and Yaw, 1955, for a description of the algorithm) on the matrix of assignment
costsM
profit
?M
penalty
. This algorithm was originally proposed to optimize the assignment of workers
(in our case, the neighbors) on tasks (in our case, ranks), according to the profit generated by each
worker for each task (in our case, the probability that a neighbor stands at a given rank). The result
of this algorithm therefore indicates a new rank for each word. The algorithm converges to an optimal
solution with a complexity O(k
3
) (for reranking the k-nearest neighbors).
6.2 Results
Table 5 presents the performance achieved by our local reranking method compared to the adjusted
Okapi-BM25 reference using the same experimental conditions as above. As before, the considered
neighborhood is set to k = 20. Precisions beyond this threshold are unchanged and thus not reported.
We test the effectiveness of the local reranking on all neighbor lists and on a third of lists with the lowest
quality scores.
It appears that the reranking on the whole lists does not provide a real gain. However, the gain is
substantial on the lists with low confidence score. Moreover, unlike the experiments of section 4, these
717
gains apply by construction to the heads of lists, which are most likely to be used in practice. This
difference between results on the whole set of words and on those with the lowest confidence scores can
be explained in two ways. First, the lists with the highest confidence scores correspond largely to the
lists with the best MAP, as expected and illustrated in Figure 3. This therefore suggests a priori little
room for improvement. Second, regardless of MAP, we can also assume that these lists already have an
optimum arrangement of individual probabilities that explains the high confidence score. The reranking
thus concerns only few neighbors.
7 Conclusion and future work
The different contributions proposed in this article do not place themselves all at the same level. The
thesaurus construction using tools from the IR is not a major conceptual innovation, but this approach
seems curiously unexplored although it provides very competitive results while requiring minimum im-
plementations through existing tools from IR.
The various propositions exploiting the neighborhood graph to improve the thesaurus are part of a
more original approach where the whole thesaurus is considered. We have specifically examined the
aspects of reciprocity and distance, in terms of rank, between two words to offer several contributions.
The improvements obtained by aggregation over all neighbors or by the local reranking from confidence
scores validate our approach. It should be noted that the gains are small in absolute terms, but, compared
to those observed in the field, correspond to significant improvements.
The various aspects of this work open up many prospects of research. For example, many other
aggregate functions in addition to those tested in section 4 exist in the literature. Some may even offer the
possibility of integrating the confidence score associated with each neighbor, as Choquet?s or Sugeno?s
integrals (Detyniecki, 2000). More generally, it would be interesting to iteratively use improvements
of neighbor lists to update the confidence scores, etc., in the spirit for example of what is proposed
by Pedronette et al. (2014). A detailled analysis of the impact of these techniques according to the type of
semantic relation is still to be performed. Beyond the distributional thesauri construction, the proposed
methods to compute confidence scores or reorder lists of neighbors can be applied to other problems
where the k-nearest neighbor graphs of are built. Also note that we have only considered a small part of
the information carried by the neighborhood graph. We focused on the aspects of reciprocity, but taking
into account other aspects of the graph (in particular the transitivity, or more generally its topology),
could lead to further improvements.
References
Cl?ementine Adam, C?ecile Fabre, and Philippe Muller. 2013.
?
Evaluer et am?eliorer une ressource distributionnelle :
protocole d?annotation de liens s?emantiques en contexte. TAL, 54(1):71?97.
Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz. 2009. Rank-Based Transformation in Measuring Semantic
Relatedness. In 22
nd
Canadian Conference on Artificial Intelligence, pages 187?190.
Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic related-
ness. Computational Linguistics, 32(1):13?47.
James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Workshop of the
ACL Special Interest Group on the Lexicon (SIGLEX), pages 59?66, Philadelphia, USA.
Marcin Detyniecki. 2000. Mathematical aggregation operators and their application to video querying. Ph.D.
thesis, Universit?e de Paris 6.
Dominique Domeng`es and Michel Volle. 1979. Analyse factorielle sph?erique : une exploration. Annales de
l?INSEE, 35:3?83.
Bernard Escoffier. 1978. Analyse factorielle et distances r?epondant au principe d??equivalence distributionnelle.
Revue de statistique appliqu?ee, 26(4):29?37.
Olivier Ferret. 2012. Combining bootstrapping and feature selection for improving a distributional thesaurus. In
20
th
European Conference on Artificial Intelligence (ECAI 2012), pages 336?341, Montpellier, France.
718
Olivier Ferret. 2013. Identifying bad semantic neighbors for improving distributional thesauri. In 51
st
Annual
Meeting of the Association for Computational Linguistics (ACL 2013), pages 561?571, Sofia, Bulgaria.
Olivier Ferret. 2014 (in press). Typing relations in distributional thesauri. In N. Gala, R. Rapp, and G. Bel, editors,
Advances in Language Production, Cognition and the Lexicon. Springer.
John R. Firth, 1957. Studies in Linguistic Analysis, chapter A synopsis of linguistic theory 1930-1955, pages 1?32.
Blackwell, Oxford.
Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer, and Zhiqiang
Wang. 2005. New experiments in distributional representations of synonymy. In Ninth Conference on Compu-
tational Natural Language Learning (CoNLL), pages 25?32, Ann Arbor, Michigan, USA.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In 20
th
International Joint Conference on Artificial Intelligence (IJCAI 2007), pages
6?12.
Gregory Grefenstette. 1994. Explorations in automatic thesaurus discovery. Kluwer Academic Publishers.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama. 2006. Selection of effective contextual informa-
tion for automatic synonym acquisition. In 21
st
International Conference on Computational Linguistics and
44
th
Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 353?360,
Sydney, Australia.
Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word repre-
sentations via global context and multiple word prototypes. In 50th Annual Meeting of the Association for
Computational Linguistics (ACL?12), pages 873?882.
David Hull. 1993. Using Statistical Testing in the Evaluation of Retrieval Experiments. In Proc. of the 16
th
Annual ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR?93, Pittsburgh,
?
Etats-Unis.
Harold W. Kuhn and Bryn Yaw. 1955. The Hungarian method for the assignment problem. Naval Research
Logistic Quarterly, 2:83?97.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato?s problem: the latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211?240.
Dekang Lin. 1998a. Automatic retrieval and clustering of similar words. In 17
th
International Conference on
Computational Linguistics and 36
th
Annual Meeting of the Association for Computational Linguistics (ACL-
COLING?98), pages 768?774, Montr?eal, Canada.
Dekang Lin. 1998b. Automatic retrieval and clustering of similar words. In 17
th
International Conference on
Computational Linguistics and 36
th
Annual Meeting of the Association for Computational Linguistics (ACL-
COLING?98), pages 768?774, Montr?eal, Canada.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word repre-
sentations. In 2013 Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL HLT 2013), pages 746?751, Atlanta, Georgia.
George A. Miller. 1990. WordNet: An On-Line Lexical Database. International Journal of Lexicography, 3(4).
Sebastian Pad?o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199.
Emanuel Parzen. 1962. On estimation of a probability density function and mode. Ann. Math. Stat., 33:1065?
1076.
Daniel Carlos Guimar?aes Pedronette, Ot?avio Augusto Bizetto Penatti, and Ricardo da Silva Torres. 2014. Unsu-
pervised manifold learning using reciprocal knn graphs in image re-ranking and rank aggregation tasks. Image
Vision Computing, 32(2):120?130.
Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu. 1998. Okapi at TREC-7: Automatic Ad
Hoc, Filtering, VLC and Interactive. In Proc. of the 7
th
Text Retrieval Conference, TREC-7, pages 199?210.
719
Magnus Sahlgren. 2001. Vector-based semantic analysis: Representing word meanings based on random labels.
In ESSLLI 2001 Workshop on Semantic Knowledge Acquisition and Categorisation, Helsinki, Finland.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on
New Methods in Language Processing.
Bernard W. Silverman. 1986. Density estimation for statistics and data analysis. Monographs on statistics and
applied probability. Chapman and Hall Boca Raton, London, Glasgow, Weinheim.
Tim Van de Cruys. 2010. Mining for Meaning. The Extraction of Lexico-semantic Knowledge from Text. Ph.D.
thesis, University of Groningen, The Netherlands.
Olga Vechtomova and Stephen E. Robertson. 2012. A domain-independent approach to finding related entities.
Information Processing and Management, 48(4):654?670.
Grady Ward. 1996. Moby thesaurus. Moby Project.
Larry Wasserman. 2005. All of Statistics: A Concise Course in Statistical Inference. Springer Texts in Statistics.
Kazuhide Yamamoto and Takeshi Asakura. 2010. Even unassociated features can improve lexical distributional
similarity. In Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages
32?39, Beijing, China.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping Distributional Feature Vector Quality. Compu-
tational Linguistics, 35(3):435?461.
720
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852?1857,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Event Role Extraction using Domain-Relevant Word Representations
Emanuela Boros?
??
Romaric Besanc?on
?
Olivier Ferret
?
Brigitte Grau
??
?
CEA, LIST, Vision and Content Engineering Laboratory, F-91191, Gif-sur-Yvette, France
?
LIMSI, rue John von Neumann, Campus Universitaire d?Orsay, F-91405 Orsay cedex
?
ENSIIE, 1 square de la r?esistance F-91025
?
Evry cedex
firstname.lastname@cea.fr firstname.lastname@limsi.fr
Abstract
The efficiency of Information Extraction
systems is known to be heavily influenced
by domain-specific knowledge but the cost
of developing such systems is consider-
ably high. In this article, we consider the
problem of event extraction and show that
learning word representations from unla-
beled domain-specific data and using them
for representing event roles enable to out-
perform previous state-of-the-art event ex-
traction models on the MUC-4 data set.
1 Introduction
In the Information Extraction (IE) field, event ex-
traction constitutes a challenging task. An event
is described by a set of participants (i.e. at-
tributes or roles) whose values are text excerpts.
The event extraction task is related to several sub-
tasks: event mention detection, candidate role-
filler extraction, relation extraction and event tem-
plate filling. The problem we address here is the
detection of role-filler candidates and their associ-
ation with specific roles in event templates. For
this task, IE systems adopt various ways of ex-
tracting patterns or generating rules based on the
surrounding context, local context and global con-
text (Patwardhan and Riloff, 2009). Current ap-
proaches for learning such patterns include boot-
strapping techniques (Huang and Riloff, 2012a;
Yangarber et al., 2000), weakly supervised learn-
ing algorithms (Huang and Riloff, 2011; Sudo et
al., 2003; Surdeanu et al., 2006), fully supervised
learning approaches (Chieu et al., 2003; Freitag,
1998; Bunescu and Mooney, 2004; Patwardhan
and Riloff, 2009) and other variations. All these
methods rely on substantial amounts of manually
annotated corpora and use a large body of lin-
guistic knowledge. The performance of these ap-
proaches is related to the amount of knowledge
engineering deployed and a good choice of fea-
tures and classifiers. Furthermore, the efficiency
of the system relies on the a priori knowledge of
the applicative domain (the nature of the events)
and it is generally difficult to apply a system on
a different domain with less annotated data with-
out reconsidering the design of the features used.
An important step forwards is TIER
light
(Huang
and Riloff, 2012a) that targeted the minimization
of human supervision with a bootstrapping tech-
nique for event roles detection. Also, PIPER (Pat-
wardhan and Riloff, 2007; Patwardhan, 2010) dis-
tinguishes between relevant and irrelevant regions
and learns domain-relevant extraction patterns us-
ing a semantic affinity measure. Another possi-
ble approach for dealing with this problem is to
combine the use a restricted set of manually anno-
tated data with a much larger set of data extracted
in an unsupervised way from a corpus. This ap-
proach was experimented for relations in the con-
text of Open Information Extraction (Soderland et
al., 2010) but not for extracting events and their
participants to our knowledge.
In this paper, we propose to approach the task
of labeling text spans with event roles by auto-
matically learning relevant features that requires
limited prior knowledge, using a neural model to
induce semantic word representations (commonly
referred as word embeddings) in an unsupervised
fashion, as in (Bengio et al., 2006; Collobert and
Weston, 2008). We exploit these word embed-
dings as features for a supervised event role (mul-
ticlass) classifier. This type of approach has been
proved efficient for numerous tasks in natural lan-
guage processing, including named entity recog-
nition (Turian et al., 2010), semantic role label-
ing (Collobert et al., 2011), machine translation
(Schwenk and Koehn, 2008; Lambert et al., 2012),
word sense disambiguation (Bordes et al., 2012) or
sentiment analysis (Glorot et al., 2011; Socher et
al., 2011) but has never been used, to our knowl-
1852
edge, for an event extraction task. Our goal is two-
fold: (1) to prove that using as only features word
vector representations makes the approach com-
petitive in the event extraction task; (2) to show
that these word representations are scalable and
robust when varying the size of the training data.
Focusing on the data provided in MUC-4 (Lehnert
et al., 1992), we prove the relevance of our ap-
proach by outperforming state-of-the-art methods,
in the same evaluation environment as in previous
works.
2 Approach
In this work, we approach the event extraction task
by learning word representations from a domain-
specific data set and by using these representa-
tions to identify the event roles. This idea relies
on the assumption that the different words used
for a given event role in the text share some se-
mantic properties, related to their context of use
and that these similarities can be captured by spe-
cific representations that can be automatically in-
duced from the text, in an unsupervised way. We
then propose to rely only on these word repre-
sentations to detect the event roles whereas, in
most works (Riloff, 1996; Patwardhan and Riloff,
2007; Huang and Riloff, 2012a; Huang and Riloff,
2012b), the role fillers are represented by a set
of different features (raw words, their parts-of-
speech, syntactic or semantic roles in the sen-
tence).
Furthermore, we propose two additional contri-
butions to the construction of the word representa-
tions. The first one is to exploit limited knowledge
about the event types (seed words) to improve the
learning procedure by better selecting the dictio-
nary. The second one is to use a max operation
1
on
the word vector representations in order to build
noun phrase representations (since slot fillers are
generally noun phrases), which represents a better
way of aggregating the semantic information born
by the word representations.
2.1 Inducing Domain-Relevant Word
Representations
In order to induce the domain-specific word rep-
resentations, we project the words into a 50-
dimensional word space. We chose a single
1
This max operation consists in taking, for each compo-
nent of the vector, the max value of this component for each
word vector representation.
layer neural network (NN) architecture that avoids
strongly engineered features, assumes little prior
knowledge about the task, but is powerful enough
to capture relevant domain information. Follow-
ing (Collobert et al., 2011), we use an NN which
learns to predict whether a given text sequence
(short word window) exists naturally in the consid-
ered domain. We represent an input sequence of n
words as ?w
i
? = ?w
i?(n/2)
. . . , w
i
, . . . w
i+(n/2)
?.
The main idea is that each sequence of words in
the training set should receive a higher score than
a sequence in which one word is replaced with
a random one. We call the sequence with a ran-
dom word corrupted (
?
?w
i
?) and denote as correct
(?w
i
?) all the sequences of words from the data
set. The goal of the training step is then to min-
imize the following loss function for a word w
i
in the dictionary D: C
w
i
=
?
w
i
?D
max(0, 1 ?
g(?w
i
?)+g(
?
?w
i
?)), where g(?) is the scoring func-
tion given by the neural network. Further details
and evaluations of these embeddings can be found
in (Bengio et al., 2003; Bengio et al., 2006; Col-
lobert and Weston, 2008; Turian et al., 2010). For
efficiency, words are fed to our architecture as in-
dices taken from a finite dictionary. Obviously,
a simple index does not carry much useful infor-
mation about the word. So, the first layer of our
network maps each of these word indices into a
feature vector, by a lookup table operation. Our
first contribution intervenes in the process of the
choosing the proper dictionary. (Bengio, 2009)
has shown that the order of the words in the dic-
tionary of the neural network is not indifferent to
the quality of the achieved representations: he pro-
posed to order the dictionary by frequency and se-
lect the words for the corrupted sequence accord-
ing to this order. In our case, the most frequent
words are not always the most relevant for the task
of event role detection. Since we want to have a
training more focused to the domain specific task,
we chose to order the dictionary by word relevance
to the domain. We accomplish this by considering
a limited number of seed words for each event type
that needs to be discovered in text (e.g. attack,
bombing, kidnapping, arson). We then rate with
higher values the words that are more similar to the
event types words, according to a given semantic
similarity, and we rank them accordingly. We use
the ?Leacock Chodorow? similarity from Word-
net 3.0 (Leacock and Chodorow, 1998). Initial ex-
perimental results proved that using this domain-
1853
oriented order leads to better performance for the
task than the order by frequency.
2.2 Using Word Representations to Identify
Event Roles
After having generated for each word their vec-
tor representation, we use them as features for the
annotated data to classify event roles. However,
event role fillers are not generally single words but
noun phrases that can be, in some cases, identi-
fied as named entities. For identifying the event
roles, we therefore apply a two-step strategy. First,
we extract the noun chunks using SENNA
2
parser
(Collobert et al., 2011; Collobert, 2011) and we
build a representation for these chunks defined as
the maximum, per column, of the vector represen-
tations of the words it contains. Second, we use
a statistical classifier to recognize the slot fillers,
using this representation as features. We chose
the extra-trees ensemble classifier (Geurts et al.,
2006), which is a meta estimator that fits a num-
ber of randomized decision trees (extra-trees) on
various sub-samples of the data set and use averag-
ing to improve the predictive accuracy and control
over-fitting.
3 Experiments and Results
3.1 Task Description
We conducted the experiments on the official
MUC-4 training corpus that consists of 1,700 doc-
uments and instantiated templates for each doc-
ument. The task consists in extracting informa-
tion about terrorist events in Latin America from
news articles. We classically considered the fol-
lowing 4 types of events: attack, bombing, kid-
napping and arson. These are represented by tem-
plates containing various slots for each piece of
information that should be extracted from the doc-
ument (perpetrators, human targets, physical tar-
gets, etc). Following previous works (Huang and
Riloff, 2011; Huang and Riloff, 2012a), we only
consider the ?String Slots? in this work (other slots
need different treatments) and we group certain
slots to finally consider the five slot types PerpInd
(individual perpetrator), PerpOrg (organizational
perpetrator), Target (physical target), Victim (hu-
man target name or description) and Weapon (in-
strument id or type). We used 1,300 documents
(DEV) for training, 200 documents (TST1+TST2)
2
Code and resources can be found at http://ml.
nec-labs.com/senna/
for tuning, and 200 documents (TST3+TST4) as
the blind test set. To compare with similar works,
we do not evaluate the template construction and
only focus on the identification of the slot fillers:
for each answer key in a reference template, we
check if we find it correctly with our extraction
method, using head noun matching (e.g., the vic-
tim her mother Martha Lopez Orozco de Lopez is
considered to match Matha Lopez), and merging
duplicate extractions (so that different extracted
slot fillers sharing the same head noun are counted
only once). We also took into account the answer
keys with multiple values in the reference, deal-
ing with conjunctions (when several victims are
named, we need to find all of them) and disjunc-
tions (when several names for the same organiza-
tion are possible, we need to find any of them).
Our results are reported as Precision/Recall/F1-
score for each event role separately and averaged
on all roles.
3.2 Experiments
In all the experiments involving our model, we es-
tablished the following stable choices of parame-
ters: 50-dimensional vectors obtained by training
on sequences of 5 words, which is consistent with
previous studies (Turian et al., 2010; Collobert
and Weston, 2008). All the hyper-parameters of
our model (e.g. learning rate, size of the hidden
layer, size of the word vectors) have been chosen
by finetuning our event extraction system on the
TST1+TST2 data set. For DRVR-50 and W2V-50,
the embeddings were built from the whole training
corpus (1,300 documents) and the dictionary was
made of all the words of this corpus under their
inflected form.
We used the extra-trees ensemble classifier im-
plemented in (Pedregosa et al., 2011), with hyper-
parameters optimized on the validation data: for-
est of 500 trees and the maximum number of
features to consider when looking for the best
split is
?
number features. We present a 3-
fold evaluation: first, we compare our system with
state-of-the-art systems on the same task, then we
compare our domain-relevant vector representa-
tions (DRVR-50) to more generic word embed-
dings (C&W50, HLBL-50)
3
and finally to another
3
C&W-50 are described in (Collobert and Weston,
2008), HLBL-50 are the Hierarchical log-bilinear embed-
dings (Mnih and Hinton, 2007), provided by (Turian et
al., 2010), available at http://metaoptimize.com/
projects/wordreprs induced from the Reuters-RCV1
1854
State-of-the-art systems
PerpInd PerpOrg Target Victim Weapon Average
(Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46
(Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40
(Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
(Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
(Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50
(Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59
Models based on word embeddings
C&W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65
HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66
W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72
DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73
Table 1: Accuracy of ?String Slots? on the TST3 + TST4 test set P/R/F1 (Precision/Recall/F1-Score)
word representation construction on the domain-
specific data (W2V-50)
4
.
Figure 1: F1-score results for event role labeling
on MUC-4 data, for different size of training data,
of ?String Slots? on the TST3+TST4 with differ-
ent parameters, compared to the learning curve of
TIER (Huang and Riloff, 2012a). The grey points
represent the performances of other IE systems.
Figure 1 presents the average F1-score results,
computed over the slots PerpInd, PerpOrg, Tar-
get, Victim and Weapon. We observe that mod-
els relying on word embeddings globally outper-
form the state-of-the-art results, which demon-
strates that the word embeddings capture enough
semantic information to perform the task of event
newswire corpus
4
W2V-50 are the embeddings induced from the MUC4
data set using the negative sampling training algorithm
(Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et
al., 2013c), available at https://code.google.com/
p/word2vec/
role labeling on ?String Slots? without using any
additional hand-engineered features. Moreover,
our representations (DRVR-50) clearly surpass the
models based on generic embeddings (C&W-50
and HLBL-50) and obtain better results than W2V-
50, based the competitive model of (Mikolov et
al., 2013a), even if the difference is small. We
can also note that the performance of our model
is good even with a small amount of training data,
which makes it a good candidate to easily develop
an event extraction system on a new domain.
Table 1 provides a more detailed analysis of the
comparative results. We can see in this table that
our results surpass those of previous systems (0.73
vs. 0.59) with, particularly, a consistently higher
precision on all roles, whereas recall is smaller for
certain roles (Target and Weapon). To further ex-
plore the impact of these representations, we com-
pared our word embeddings with other word em-
beddings (C&W-50, HLBL-50) and report the re-
sults in Figure 1 and Table 1. The results show
that our model also outperforms the models using
others word embeddings (F1-score of 0.73 against
0.65, 0.66). This proves that a model learned
on a domain-specific data set does indeed pro-
vide better results, even if its size is much smaller
(whereas it is usually considered that neural mod-
els require often important training data). Finally,
we also achieve slightly better results than W2V-50
with other word representations built on the same
corpus, which shows that the choices made for the
word representation construction, such as the use
of domain information for word ordering, tend to
have a positive impact.
1855
4 Conclusions and Perspectives
We presented in this paper a new approach for
event extraction by reducing the features to only
use unsupervised word representations and a small
set of seed words. The word embeddings induced
from a domain-specific corpus bring improvement
over state-of-art models on the standard MUC-
4 corpus and demonstrate a good scalability on
different sizes of training data sets. Therefore,
our proposal offers a promising path towards eas-
ier and faster domain adaptation. We also prove
that using a domain-specific corpus leads to bet-
ter word vector representations for this task than
using other publicly-available word embeddings
(even if they are induced from a larger corpus).
As future work, we will reconsider the archi-
tecture of the neural network and we will refo-
cus on creating a deep learning model while tak-
ing advantage of a larger set of types of infor-
mation such as syntactic information, following
(Levy and Goldberg, 2014), or semantic informa-
tion, following (Yu and Dredze, 2014).
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastian
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
DawnE. Holmes and LakhmiC. Jain, editors, Inno-
vations in Machine Learning, volume 194 of Studies
in Fuzziness and Soft Computing, pages 138?186.
Springer Berlin Heidelberg.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and trends in Machine Learning,
2(1).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In Fifteenth International Conference
on Artificial Intelligence and Statistics (AISTATS
2012), pages 127?135.
Razvan Bunescu and Raymond J Mooney. 2004.
Collective information extraction with relational
markov networks. In 42nd Annual Meeting on As-
sociation for Computational Linguistics (ACL-04),
pages 438?445.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In 41st international Annual Meeting on Association
for Computational Linguistics (ACL-2003), pages
216?223.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In 25th In-
ternational Conference of Machine learning (ICML-
08), pages 160?167. ACM.
Ronan Collobert, Jason Weston, L?eon Battou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In 14th International Con-
ference on Artificial Intelligence and Statistics (AIS-
TATS 2011).
Dayne Freitag. 1998. Information extraction from
HTML: Application of a general machine learning
approach. In AAAI?98, pages 517?523.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale senti-
ment classification: A deep learning approach. In
28th International Conference on Machine Learning
(ICML-11), pages 513?520.
Ruihong Huang and Ellen Riloff. 2011. Peeling back
the layers: Detecting event role fillers in secondary
contexts. In ACL 2011, pages 1137?1147.
Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2012), pages
286?295.
Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In 26th Con-
ference on Artificial Intelligence (AAAI 2012).
Patrik Lambert, Holger Schwenk, and Fr?ed?eric Blain.
2012. Automatic translation of scientific documents
in the hal archive. In LREC 2012, pages 3933?3936.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and Wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, WordNet: An electronic lexical database., pages
265?283. MIT Press.
Wendy Lehnert, Claire Cardie, David Fisher, John Mc-
Carthy, Ellen Riloff, and Stephen Soderland. 1992.
University of Massachusetts: MUC-4 test results
and analysis. In 4th Conference on Message under-
standing, pages 151?158.
1856
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2014), Short Papers, pages 302?308, Bal-
timore, Maryland, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations (ICLR 20013), work-
shop track.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26 (NIPS 2013), pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In NAACL-HLT 2013, pages
746?751.
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical modelling. In
24th International Conference of Machine learning
(ICML 2007), pages 641?648. ACM.
Siddharth Patwardhan and Ellen Riloff. 2007. Ef-
fective information extraction with semantic affinity
patterns and relevant regions. In 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 717?727.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2009), pages 151?160.
Siddharth Patwardhan. 2010. Widening the field of
view of information extraction through sentential
event recognition. Ph.D. thesis, University of Utah.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In AAAI?96, pages
1044?1049.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In IJCNLP 2008, pages 661?666.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
28th International Conference on Machine Learning
(ICML-11), pages 129?136.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AI Magazine, 31(3):93?102.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic ie pattern acquisition. In
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL-03), pages 224?231.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.
2006. A hybrid approach for the acquisition of
information extraction patterns. In EACL-2006
Workshop on Adaptive Text Extraction and Mining
(ATEM 2006), pages 48?55.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In 48th international
Annual Meeting on Association for Computational
Linguistics (ACL 2010), pages 384?394.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
18th Internation Conference on Computational Lin-
guistics (COLING 2000), pages 940?946.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2014), Short Papers, pages 545?550,
Baltimore, Maryland, June.
1857
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561?571,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identifying Bad Semantic Neighbors for Improving Distributional
Thesauri
Olivier Ferret
CEA, LIST, Vision and Content Engineering Laboratory,
Gif-sur-Yvette, F-91191 France.
olivier.ferret@cea.fr
Abstract
Distributional thesauri are now widely
used in a large number of Natural Lan-
guage Processing tasks. However, they
are far from containing only interesting
semantic relations. As a consequence,
improving such thesaurus is an impor-
tant issue that is mainly tackled indirectly
through the improvement of semantic sim-
ilarity measures. In this article, we pro-
pose a more direct approach focusing on
the identification of the neighbors of a
thesaurus entry that are not semantically
linked to this entry. This identification re-
lies on a discriminative classifier trained
from unsupervised selected examples for
building a distributional model of the entry
in texts. Its bad neighbors are found by ap-
plying this classifier to a representative set
of occurrences of each of these neighbors.
We evaluate the interest of this method for
a large set of English nouns with various
frequencies.
1 Introduction
The work we present in this article focuses on the
automatic building of a thesaurus from a corpus.
As illustrated by Table 1, such thesaurus gives for
each of its entries a list of words, called seman-
tic neighbors, that are supposed to be semanti-
cally linked to the entry. Generally, each neigh-
bor is associated with a weight that characterizes
the strength of its link with the entry and all the
neighbors of an entry are sorted according to the
decreasing order of their weight.
The term semantic neighbor is very generic and
can have two main interpretations according to the
kind of semantic relations it is based on: one re-
lies only on paradigmatic relations, such as hy-
pernymy or synonymy, while the other consid-
ers syntagmatic relations, called collocation rela-
tions by (Halliday and Hasan, 1976) in the context
of lexical cohesion or ?non-classical relations? by
(Morris and Hirst, 2004). The distinction between
these two interpretations refers to the distinction
between the notions of semantic similarity and se-
mantic relatedness as it was done in (Budanitsky
and Hirst, 2006) or in (Zesch and Gurevych, 2010)
for instance. However, the limit between these two
notions is sometimes hard to find in existing work
as terms semantic similarity and semantic relat-
edness are often used interchangeably. Moreover,
semantic similarity is frequently considered as in-
cluded into semantic relatedness and the two prob-
lems are often tackled by using the same methods.
In the remainder of this article, we will use the
term semantic similarity with its generic sense and
the term semantic relatedness for referring more
specifically to similarity based on syntagmatic re-
lations.
Following work such as (Grefenstette, 1994), a
widespread way to build a thesaurus from a cor-
pus is to use a semantic similarity measure for ex-
tracting the semantic neighbors of the entries of
the thesaurus. Three main ways of implement-
ing such measures can be distinguished. The first
one relies on handcrafted resources in which se-
mantic relations are clearly identified. Work based
on WordNet-like lexical networks for building se-
mantic similarity measures such as (Budanitsky
and Hirst, 2006) or (Pedersen et al, 2004) falls
into this category. These measures typically ex-
ploit the hierarchical structure of these networks,
based on hypernymy relations. The second ap-
proach makes use of a less structured source of
knowledge about words such as the definitions of
classical dictionaries or the glosses of WordNet.
WordNet?s glosses were used to support Lesk-
like measures in (Banerjee and Pedersen, 2003)
and more recently, measures were also defined
from Wikipedia or Wiktionaries (Gabrilovich and
561
Markovitch, 2007). The last option is the corpus-
based approach, based on the distributional hy-
pothesis (Firth, 1957): each word is characterized
by the set of contexts from a corpus in which it ap-
pears and the semantic similarity of two words is
computed from the contexts they share. This per-
spective was first adopted by (Grefenstette, 1994)
and (Lin, 1998) and then, explored in details in
(Curran and Moens, 2002b), (Weeds, 2003) or
(Heylen et al, 2008).
The problem of improving the results of the
?classical? implementation of the distributional
approach as it can be found in (Curran and Moens,
2002a) for instance was already tackled by some
work. A part of these proposals focus on the
weighting of the elements that are part of the
contexts of words such as (Broda et al, 2009),
in which the weights of context elements are
turned into ranks, or (Zhitomirsky-Geffet and Da-
gan, 2009), followed and extended by (Yamamoto
and Asakura, 2010), that proposes a bootstrap-
ping method for modifying the weights of con-
text elements according to the semantic neighbors
found by an initial distributional similarity mea-
sure. However, another part of these proposals
implies more radical changes. The use of dimen-
sionality reduction techniques, for instance Latent
Semantic Analysis in (Pado? and Lapata, 2007), the
multi-prototype (Reisinger and Mooney, 2010) or
examplar-based models (Erk and Pado, 2010), the
Deep Learning approach of (Huang et al, 2012) or
the redefinition of the distributional approach in a
Bayesian framework (Kazama et al, 2010) can be
classified into this second category.
The work we present in this article takes place
in the framework defined by (Grefenstette, 1994)
for implementing the distributional approach but
proposes a new method for improving a thesaurus
built in this context based on the identification of
its bad semantic neighbors rather than on the adap-
tation of the weight of their features.
2 Principles
Our work shares with (Zhitomirsky-Geffet and
Dagan, 2009) the use of a kind of bootstrapping as
it starts from a distributional thesaurus and to some
extent, exploits it for its improvement. However, it
adopts a more indirect approach: instead of select-
ing the ?best? semantic neighbors of an entry in
the thesaurus for adapting the weights of distribu-
tional context elements, it focuses on the detection
of its bad semantic neighbors, that is to say the
neighbors of the entry that are actually not seman-
tically similar to the entry. In Table 1, waterworks
for the entry cabdriver and hollowness for the en-
try machination are two examples of such kind of
neighbors. By discarding these bad neighbors or
at least by downgrading them, the rank of true se-
mantic neighbors is expected to be lower. This
makes the thesaurus more interesting to use since
the quality of such thesaurus strongly decreases as
the rank of the neighbors of its entries increases
(see Section 4.1 for an illustration), which means
in practice that only the first neighbors of an entry
can be generally exploited.
The approach we propose for identifying the
bad semantic neighbors of a thesaurus entry re-
lies on the distributional hypothesis, as the method
for the initial building of the thesaurus, but im-
plements it in a different way. This hypothesis
roughly specifies that from a semantic viewpoint,
the meaning of a word can be characterized by the
set of contexts in which this word occurs. As a
consequence, two words are considered as seman-
tically similar if they occur in a large enough set
of shared contexts. In work such as (Curran and
Moens, 2002a), this hypothesis is implemented by
collecting for each entry the words it co-occurs
with in a large corpus. This co-occurrence can
be based either on the position of the word in the
text in relation to the entry or on the presence
of a syntactic relation between the entry and the
word. As a result, the distributional representa-
tion of a word takes the unstructured form of a
bag of words or the more structured form of a
set of pairs {syntactic relation, word}. A vari-
ant of this approach was proposed in (Kazama et
al., 2010) where the distributional representation
of a word is modeled as a multinomial distribution
with Dirichlet as prior.
However, this approach globally faces a certain
lack of diversity and complexity of the features of
its models. For instance, features such as ngrams
of words or ngrams of parts of speech are not con-
sidered whereas they are widely used in tasks such
as word sense disambiguation (WSD) for instance,
probably because they would lead to very large
models and because similarity measures such as
the Cosine measure are not necessarily suitable
for heterogeneous representations (Alexandrescu
and Kirchhoff, 2007). Hence, we propose in this
article to build a discriminative model for repre-
562
abnormality defect [0.30], disorder [0.23], deformity [0.22], mutation [0.21], prolapse [0.21], anomaly [0.21] . . .
agreement accord [0.44], deal [0.41], pact [0.38], treaty [0.36], negotiation [0.35], proposal [0.32], arrangement [0.30] . . .
cabdriver waterworks [0.23], toolmaker [0.22], weaponeer [0.17], valkyry [0.17], wang [0.17], amusement-park [0.17] . . .
machination hollowness [0.15], share-price [0.12], clockmaker [0.12], huguenot [0.12], wrangling [0.12], alternation [0.12] . . .
Table 1: First neighbors of some entries of the distributional thesaurus of section 3.2
senting the contexts of a word since this kind of
models are known to integrate easily a wide set
of different types of features. This model aims
more precisely at discriminating from a semantic
viewpoint a word in context, i.e. in a sentence,
from all other words and more particularly, from
those of its neighbors in a distributional thesaurus
that are likely to be actually not semantically sim-
ilar to it. The underlying hypothesis follows the
distributional principles: a word and a synonym
should appear in the same contexts, which means
that they are characterized by the same features.
As a consequence, a model based on these fea-
tures that can identify a word in a sentence is likely
to identify also a synonym of this word in a sen-
tence, and by extension, to identify a word that
is paradigmatically linked to it. More precisely,
we found that such model is specifically effective
for discarding the bad neighbors of the entries of a
distributional thesaurus.
3 Improving a distributional thesaurus
3.1 Overview
The principles presented in the previous section
face one major problem compared to the ?classi-
cal? distributional approach : the semantic similar-
ity of two words can be evaluated directly by com-
puting the similarity of their distributional repre-
sentations. However, in our case, since this rep-
resentation is a discriminative model, the similar-
ity of two words can not be evaluated through the
direct comparison of their models. These models
have to be applied to words in context for being
exploited. As a consequence, for deciding whether
a neighbor of a thesaurus entry is a bad neighbor
or not, the discriminative model of the entry has
to be applied to occurrences of this neighbor in
texts. Hence, the method we propose for improv-
ing a distributional thesaurus applies the following
process to each of its entries:
? building of a classifier for determining
whether a word in a sentence corresponds or
not to the entry;
? selection of a set of examples sentences for
each of the neighbors of the entry in the the-
saurus;
? application of the classifier to these sen-
tences;
? identification of bad neighbors according to
the results of the classifier;
? reranking of entry?s neighbors according to
bad neighbors.
3.2 Building of the initial thesaurus
Before introducing our method for improving dis-
tributional thesauri, we first present the way we
build such a thesaurus. As in (Lin, 1998) or (Cur-
ran and Moens, 2002a), this building is based on
the definition of a semantic similarity measure
from a corpus. The corpus used for defining this
measure was the AQUAINT-2 corpus, a middle-
size corpus made of around 380 million words
coming from news articles. Although our target
language is English, we chose to limit deliber-
ately the level of the tools applied for preprocess-
ing texts to part-of-speech tagging and lemmati-
zation to make possible the transposition of our
method to a large set of languages. This seems
to be a reasonable compromise between the ap-
proach of (Freitag et al, 2005), in which none
normalization of words is done, and the more
widespread use of syntactic parsers in work such
as (Lin, 1998). More precisely, we used TreeTag-
ger (Schmid, 1994) for performing the linguistic
preprocessing of the AQUAINT-2 corpus.
For the extraction of distributional data and the
characteristics of the distributional similarity mea-
sure, we adopted the options of (Ferret, 2010), re-
sulting from a kind of grid search procedure per-
formed with the extended TOEFL test proposed in
(Freitag et al, 2005) as an optimization objective.
More precisely, the following characteristics were
taken:
? distributional contexts made of the co-
occurrents collected in a 3 word window cen-
tered on each occurrence in the corpus of the
target word. These co-occurrents were re-
stricted to nouns, verbs and adjectives;
? soft filtering of contexts: removal of co-
occurrents with only one occurrence;
? weighting function of co-occurrents in con-
563
texts = Pointwise Mutual Information (PMI)
between the target word and the co-occurrent;
? similarity measure between contexts, for
evaluating the semantic similarity of two
words = Cosine measure.
The building of our initial thesaurus from the
similarity measure above was performed classi-
cally by extracting the closest semantic neighbors
of each of its entries. More precisely, the selected
measure was computed between each entry and
its possible neighbors. These neighbors were then
ranked in the decreasing order of the values of this
measure and the first 100 neighbors were kept as
the semantic neighbors of the entry. Both entries
and possible neighbors were AQUAINT-2 nouns
whose frequency was higher than 10.
3.3 Building a discriminative model of words
in context
As mentioned in section 3.1, the starting point of
our reranking process is the definition of a model
for determining to what extent a word in a sen-
tence, which is not supposed to be known in the
context of this task, corresponds or not to a refer-
ence word E. This task can also be viewed as a
tagging task in which the occurrences of a target
word T are labeled with two tags: E and notE.
In the context of our global objective, we are not
of course interested by this task itself but rather by
the fact that such classifier is likely to model the
contexts in which E occurs and as a consequence,
is also likely to model its meaning according to the
distributional hypothesis.
A step further, such classifier can be viewed
as a means for testing whether or not a word
has the same meaning as E. This is a problem
close to WSD as it is performed in the context of
the pseudo-word disambiguation paradigm (Gale
et al, 1992): a pseudo-word is created with two
senses, E and notE, notE corresponding to one
or several words that are supposed to be represen-
tative of a meaning different from the meaning of
E. The objective is then to build a classifier for
distinguishing the pseudo-senses E and notE. As
a consequence of this view, we adopt the same
kind of features as the ones used for WSD for
building our classifier. More precisely, we follow
(Lee and Ng, 2002), a reference work for WSD,
by adopting a Support Vector Machines (SVM)
classifier with a linear kernel and three kinds of
features for characterizing each considered occur-
rence in a text of the reference word E:
? neighboring words;
? Part-of-Speech (POS) of neighboring words;
? local collocations.
Only features based on syntactic relations are
not taken from (Lee and Ng, 2002) since their use
would have not been coherent with the window
based approach of the building of our initial the-
saurus.
For the neighboring words features, we con-
sider all plain words (common and proper nouns,
verbs and adjectives) and adverbs that are present
in the same sentence of an occurrence of E. Each
neighboring word is represented under its lemma
form as a binary feature whose value is equal to 1
when it is present in the same sentence as E.
For the second type of features, we take more
precisely the POS of the three words before E and
those of the three words after E. Each pair {POS,
position} corresponds to a binary feature for the
SVM classifier. A special empty symbol is used
for the POS when the position goes beyond the end
or the beginning of the current sentence. Since we
analyze texts with TreeTagger, the tagset is very
close to the set of Penn Treebank tags.
Finally, the local collocations features corre-
spond to pairs of words, named collocations, in
the neighborhood of E. A collocation is speci-
fied by the notation Ci,j , with i and j referring to
the position of the first and the second word of the
collocation. In our case, i and j take their values
in the interval [?3,+3], similarly to POS. More
precisely, the following 11 types of collocations
are extracted for each occurrence of E: C?1,?1,
C1,1, C?2,?2, C2,2 C?2,?1, C?1,1, C1,2, C?3,?1,
C?2,1, C?1,2 and C1,3. As for POS, a special
empty symbol stands for words beyond the end
or the beginning of the sentence and similarly to
neighboring words features, words in collocations
are given under their lemma form. Each instance
of the 11 types of collocations is represented by a
tuple ?lemma1, position1, lemma2, position2? and
leads to a binary feature for the SVM classifier.
In accordance with the process of section 3.1,
a specific SVM classifier is trained for each entry
of our initial thesaurus, which requires the unsu-
pervised selection of a set of positive and nega-
tive examples. The case of positive examples is
simple: a fixed number of sentences containing at
least one occurrence of the target entry are ran-
domly chosen in the corpus used for building our
564
initial thesaurus and the first occurrence of this en-
try in the sentence is taken as a positive example.
Since we want to characterize words as much as
possible from a semantic viewpoint, the selection
of negative examples is guided by our initial the-
saurus. Choosing a neighbor of the entry with a
high rank would guarantee in principle few false
negative examples, that is to say words1 which are
semantically similar to the entry, since the number
of such neighbors strongly decreases as the rank
of neighbors increases as we will illustrate it in
section 4.1. In practice, taking neighbors with a
rather small rank as negative examples is a bet-
ter option because these examples are more useful
in terms of discrimination as they are close to the
transition zone between negative and positive ex-
amples. Moreover, in order to limit the risk of se-
lecting only false negative examples, three neigh-
bors are taken as negative examples, at ranks 10,
15 and 202. For each of these negative examples,
a fixed number of sentences is selected follow-
ing the same principles as for positive examples,
which means that on average, the number of neg-
ative examples is equal to three times the number
of positive examples. This ratio reflects the fact
that among the neighbors of an entry, the number
of those that are semantically similar to the entry
is far lower than the number of those that are not.
3.4 Identification of bad neighbors and
thesaurus reranking
Once a word-in-context classifier was trained for
an entry, it is used for identifying the bad neigh-
bors of this entry, that is to say the neighbors that
are not semantically similar to it. As this classifier
can only be applied to words in context, a fixed
number of representative occurrences have to be
selected from our reference corpus for each neigh-
bor of the entry. This selection is performed sim-
ilarly to the selection of positive and negative ex-
amples in the previous section. The application of
our word-in-context classifier to each of these oc-
currences determines whether the context of this
occurrence is likely to be compatible with the con-
text of an occurrence of the entry.
In practice, the decision of the classifier is rarely
1More precisely, an example here is an occurrence of a
word in a text but by extension, we also use the term example
for referring to the word itself.
2It should be noted that these ranks come from the eval-
uation of section 4.1 but their choice is not the result of an
optimization process.
positive, which is not surprising: even if two
words are semantically equivalent, each one is
characterized by specific usages, especially in a
given corpus, and some features of our classifier,
such as the collocation features, are more likely
to capture such specificities than the unigrams of
?classical? distributional contexts. As a conse-
quence, we consider that a positive outcome of our
classifier is a significant hint about the presence of
a word that is semantically similar to the entry and
we keep a neighbor as a ?good? neighbor if at least
a fixed number G of its occurrences, among those
selected as reference, are tagged positively by our
word-in-context classifier. Conversely, a neighbor
is defined as ?bad? if the number of its reference
occurrences tagged positively by our classifier is
lower or equal to G.
The neighbors of an entry identified as bad
neighbors are not fully discarded. They are rather
downgraded to the end of the list of neighbors.
Among the downgraded neighbors, their initial or-
der is left unchanged. It should be noted that
the word-in-context classifier is not applied to the
neighbors whose occurrences are used for its train-
ing as it would frequently lead to downgrade these
neighbors, which is not necessarily optimum as we
chose them with a rather low rank.
4 Experiments and evaluation
4.1 Initial thesaurus evaluation
Table 2 shows the results of the evaluation of our
initial thesaurus, achieved by comparing the se-
lected semantic neighbors with two complemen-
tary reference resources: WordNet 3.0 synonyms
(Miller, 1990) [W], which characterize a semantic
similarity based on paradigmatic relations, and the
Moby thesaurus (Ward, 1996) [M], which gathers
a larger set of types of relations and is more rep-
resentative of semantic relatedness3. The fourth
column of Table 2, which gives the average num-
ber of synonyms and similar words in our refer-
ences for the AQUAINT-2 nouns, also illustrates
the difference of these two resources in terms of
richness. A fusion of the two resources is also
considered [WM]. As our objective is to evalu-
ate the extracted semantic neighbors and not the
ability to rebuild the reference resources, these re-
3The Moby thesaurus includes more precisely both
paradigmatic and syntactic relations but we will sometimes
use the term synonym as a shortcut for referring to all the
words associated to one of its entries.
565
freq. ref. #eval.
words
#syn. /
word
recall R-prec. MAP P@1 P@5 P@10 P@100
W 10,473 2.9 24.6 8.2 9.8 11.7 5.1 3.4 0.7
all M 9,216 50.0 9.5 6.7 3.2 24.1 16.4 13.0 4.8
# 14,670 WM 12,243 38.7 9.8 7.7 5.6 22.5 14.1 10.8 3.8
W 3,690 3.7 28.3 11.1 12.5 17.2 7.7 5.1 1.0
high M 3,732 69.4 11.4 10.2 4.9 41.3 28.0 21.9 7.9
# 4,378 WM 4,164 63.2 11.5 11.0 6.5 41.3 26.8 20.8 7.3
W 3,732 2.6 28.6 10.4 12.5 13.6 5.8 3.7 0.7
middle M 3,306 41.3 9.3 6.5 3.1 18.7 13.1 10.4 3.8
# 5,175 WM 4,392 32.0 9.8 9.3 7.4 20.9 12.3 9.3 3.2
W 3,051 2.3 11.9 2.1 3.3 2.6 1.2 0.9 0.3
low M 2,178 30.1 2.8 1.2 0.5 2.5 1.5 1.5 0.9
# 5,117 WM 3,687 18.9 3.5 2.1 2.4 3.3 1.7 1.5 0.7
Table 2: Evaluation of semantic neighbor extraction
sources were filtered to discard entries and syn-
onyms that are not part of the AQUAINT-2 vo-
cabulary (see the difference between the number
of words in the first column and the number of
evaluated words of the third column). Since the
frequency of words is an important factor in dis-
tributional approaches, we give our results glob-
ally but also for three ranges of frequencies that
split our set of nouns into roughly equal parts:
high frequency (frequency > 1000), middle fre-
quency (100 < frequency ? 1000) and low fre-
quency (10 < frequency ? 100). These results
take the form of several measures and start at the
fifth column by the proportion of the synonyms
and similar words of our references that are found
among the first 100 extracted neighbors of each
noun. As these neighbors are ranked according to
their similarity value with their target word, the
evaluation measures are taken from the Informa-
tion Retrieval field by replacing documents with
synonyms and queries with target words (see the
four last columns of Table 2). The R-precision (R-
prec.) is the precision after the first R neighbors
were retrieved, R being the number of reference
synonyms; the Mean Average Precision (MAP) is
the average of the precision value after a reference
synonym is found; precision at different cut-offs is
given for the 1, 5, 10 and 100 first neighbors. All
these values are given as percentages.
The results of Table 2 lead to three main ob-
servations. First, the level of results heavily de-
pends on the frequency range of target words:
the best results are obtained for high frequency
words while evaluation measures significantly de-
crease for words whose frequency is low. Sec-
ond, the characteristics of the reference resources
have a significant impact on results. WordNet
provides a restricted number of synonyms for
each noun while the Moby thesaurus contains for
each entry a large number of synonyms and sim-
ilar words. As a consequence, the precisions
at different cut-offs have a significantly higher
value with Moby as reference than with Word-
Net as reference. Finally, the results of Ta-
ble 2 are compatible with those of (Lin, 1998)
for instance (R-prec. = 11.6 and MAP = 8.1
with WM as reference for all entries of the the-
saurus at http://webdocs.cs.ualberta.
ca/lindek/Downloads/sim.tgz) if we
take into account the fact that the thesaurus of Lin
was built from a much larger corpus and with syn-
tactic co-occurrences.
4.2 Implementation issues
The implementation of the method we have pre-
sented in section 3 raises several issues. One of
these concerns the occurrences to select from texts
of both the entries of the thesaurus and their neigh-
bors. These occurrences are used both for the
training of our word-in-context classifier and for
the identification of bad neighbors. In practice, we
extract randomly from our reference corpus, i.e.
the AQUAINT-2 corpus, a fixed number of sen-
tences, equal to 250, for each word of the vocab-
ulary of our initial thesaurus and exploit them for
the two tasks. This extraction is performed on the
basis of the lemma form of these words. It should
be noted that 250 is the upper limit of the num-
ber of occurrences by word since the frequency
in the corpus of many words is lower than 250.
When this limit is not reached, all the available oc-
566
currences are taken, which may be no more than
11 occurrences for certain low-frequency words.
The upper limit of 250 is halfway between the 385
training examples on average for the Lexical Sam-
ple Task of Senseval 1 and the 118 training exam-
ples on average for the same task of Senseval 2.
The training of our word-in-context classifier
is also an important issue. As mentioned before,
this classifier is a linear SVM. Hence, only its C
regularization parameter can be optimized. Since
we have one specific classifier for each thesaurus
entry, such optimization has globally a high cost,
even for a linear kernel. Hence, we have first eval-
uated through a 5-fold cross-validation method the
results of these classifiers with a default value of
C, equal to 1. Table 3 gives their average accu-
racy value along with their standard deviation for
all the entries of the thesaurus and for the three
frequency ranges of Table 2.
all high middle low
accuracy 86.2 86.1 86.0 86.5
standard deviation 6.1 4.2 5.7 7.6
Table 3: Results of word-in-context classifiers
This table shows a global high level of result
along with similar values for all the frequency
ranges of entries4. Hence, we have decided not to
optimize the C parameter and to adopt the default
value of 1 for all the word-in-context classifiers.
0 5 10 15 2012
13
14
15
16
G threshold
MAP (W)
R?prec. (W)
Figure 1: R-precision and MAP for various values
of the G threshold
The last and the most important implementation
issue is the setting of the threshold G for deter-
mining whether a neighbor is likely to be a bad
4The standard deviation is a little bit higher for the lowest
frequencies but it should be noted that the low number of
examples for low frequency entries does not seem to have
a strong impact on the results of such classifier.
neighbor. For this setting, we have randomly cho-
sen a subset of 859 entries of our initial thesaurus
that corresponds to 10% of the entries with at least
one true neighbor in any of our references. Fig-
ure 1 gives the results of the reranked thesaurus
for these entries in terms of R-precision and MAP
against reference W5 for various values of G. Al-
though the level of these measures does not change
a lot for G > 5, the graph of Figure 1 shows that
G = 15 appears to be an optimal value. Hence,
this is the value used for the detailed evaluation of
the next section.
4.3 Evaluation of the reranked thesaurus
Table 4 gives the evaluation of the application of
our reranking method to the initial thesaurus ac-
cording to the same principles as in section 4.1.
The value of each measure comes with its differ-
ence with the corresponding value for the initial
thesaurus. As the recall measure and the precision
for the last rank do not change in a reranking pro-
cess, they are not given again.
The first thing to notice is that at the global
scale, all measures for all references are signifi-
cantly improved6, which means that our hypothe-
sis about the possibility for a discriminative clas-
sifier to capture the meaning of a word tends to
be validated. It is an interesting result since the
features upon which this classifier was built were
taken from WSD and were not specifically se-
lected for this task. As a consequence, there is
probably some room for improvement.
If we go into details, Table 4 clearly shows two
main trends. First, the improvement of results is
particularly effective for middle frequency entries,
then for low frequency and finally, for high fre-
quency entries. Because of their already high level
in the initial thesaurus, results for high frequency
entries are difficult to improve but it is important
to note that our selection of bad neighbors has a
very low error rate, which at least preserves these
results. This is confirmed by the fact that, with
WordNet as reference, only 744 neighbors were
found wrongly downgraded, spread over 686 en-
tries, which represents only 5% of all downgraded
neighbors. The second main trend of Table 4 con-
5The use of W as reference is justified by the fact that the
number of synonyms for an entry in W is more compatible,
especially for R-precision, with the real use of the resulting
thesaurus in an application.
6The statistical significance of differences with the initial
thesaurus was evaluated by a paired Wilcoxon test with p-
value < 0.05 and < 0.01 (? and ? for non significance).
567
freq. ref. R-prec. MAP P@1 P@5 P@10
W 9.1 (0.9) 10.7 (0.9) 12.8 (1.1) 5.6 (0.5) 3.7 (0.3)
all M 7.2 (0.5) 3.5 (0.3) 26.5 (2.4) 17.9 (1.5) 14.0 (1.0)
WM 8.4 (0.7) 6.1 (0.5) 24.8 (2.3) 15.4 (1.3) 11.7 (0.9)
W 11.3 (0.2) ? 12.6 (0.1) 17.3 (0.1) ? 7.8 (0.1) ? 5.1 (0.0)
high M 10.3 (0.1) 4.9 (0.0) 42.1 (0.8) 28.4 (0.4) 22.1 (0.2)
WM 11.1 (0.1) 6.6 (0.1) 42.0 (0.7) 27.2 (0.4) 20.9 (0.1)
W 11.8 (1.4) 13.8 (1.3) 15.7 (2.1) 6.5 (0.7) 4.1 (0.4)
middle M 7.3 (0.8) 3.6 (0.5) 23.3 (4.6) 16.0 (2.9) 12.4 (2.0)
WM 10.3 (1.0) 8.1 (0.7) 25.1 (4.2) 14.6 (2.3) 10.9 (1.6)
W 3.2 (1.1) 4.6 (1.3) 3.9 (1.3) 1.8 (0.6) 1.3 (0.4)
low M 1.8 (0.6) 0.8 (0.3) 4.4 (1.9) 2.9 (1.4) 2.6 (1.1)
WM 3.1 (1.0) 3.3 (0.9) 5.1 (1.8) 2.9 (1.2) 2.3 (0.8)
Table 4: Results of the reranking of semantic neighbors
cerns the type of semantic relations: results with
Moby as reference are improved in a larger ex-
tent than results with WordNet as reference. This
suggests that our procedure is more effective for
semantically related words than for semantically
similar words, which can be considered as a lit-
tle bit surprising since the notion of context in our
discriminative classifier seems a priori more strict
than in ?classical? distributional contexts. How-
ever, this point must be investigated further as a
significant part of the relations in Moby, even if
they do no represent the largest part of them, are
paradigmatic relations.
WordNet respect, admiration, regard
Moby
admiration, appreciation, accep-
tance, dignity, regard, respect, ac-
count, adherence, consideration,
estimate, estimation, fame, great-
ness, reverence + 79 words more
initial
cordiality, gratitude, admiration,
comradeship, back-scratching,
perplexity, respect, ruination,
appreciation, neighbourliness . . .
reranking
gratitude, admiration, respect,
appreciation, neighborliness, trust,
empathy, goodwill, reciprocity,
half-staff, affection, self-esteem,
reverence, longing, regard . . .
Table 5: Impact of our reranking for the entry es-
teem
Table 5 illustrates more precisely the impact of
our reranking procedure for the middle frequency
entry esteem. Its WordNet row gives all the refer-
ence synonyms for this entry in WordNet while its
Moby row gives the first reference related words
for this entry in Moby. In our initial thesaurus, the
first two neighbors of esteem that are present in our
reference resources are admiration (rank 3) and re-
spect (rank 7). The reranking produces a thesaurus
in which these two words appear as the second
and the third neighbors of the entry because neigh-
bors without clear relation with it such as back-
scratching were downgraded while its third syn-
onym in WordNet is raised from rank 22 to rank
15. Moreover, the number of neighbors among the
first 15 ones that are present in Moby increases
from 3 to 5.
5 Related work
The building of distributional thesaurus is gener-
ally viewed as an application or a mode of eval-
uation of work about semantic similarity or se-
mantic relatedness. As a consequence, the im-
provement of such thesaurus is generally not di-
rectly addressed but is a possible consequence
of the improvement of semantic similarity mea-
sures. However, the extent of this improvement
is rarely evaluated as most of the work about se-
mantic similarity is evaluated on datasets such as
the WordSim-353 test collection (Gabrilovich and
Markovitch, 2007), which are only partially repre-
sentative of the results for thesaurus building.
If we consider more specifically the problem of
improving semantic similarity, and by the way the-
sauri, in a given paradigm, (Broda et al, 2009),
(Zhitomirsky-Geffet and Dagan, 2009) and (Ya-
mamoto and Asakura, 2010), which all take place
in the paradigm defined by (Grefenstette, 1994),
are the closest works to ours. (Broda et al, 2009)
proposes a new weighting scheme of words in
distributional contexts that replaces the weight of
568
word by a function of its rank in the context, which
is a way to be less dependent on the values of a par-
ticular weighting function. (Zhitomirsky-Geffet
and Dagan, 2009) shares with our work the use
of bootstrapping by relying on an initial thesaurus
to derive means of improving it. More specifi-
cally, (Zhitomirsky-Geffet and Dagan, 2009) as-
sumes that the first neighbors of an entry are more
relevant than the others and as a consequence, that
their most significant features are also representa-
tive of the meaning of the entry. The neighbors
of the entry are reranked according to this hypoth-
esis by increasing the weight of these features to
favor their influence in the distributional contexts
that support the evaluation of the similarity be-
tween the entry and its neighbors. (Yamamoto and
Asakura, 2010) is a variant of (Zhitomirsky-Geffet
and Dagan, 2009) that takes into account a larger
number of features for the reranking process. One
main difference between all these works and ours
is that they assume that the initial thesaurus was
built by relying on distributional contexts repre-
sented as bags-of-words. Our method does not
make this assumption as its reranking is based on
a classifier built in an unsupervised way7 from and
applied to the corpus used for building the initial
thesaurus. As a consequence, it could even be ap-
plied to other paradigms than (Grefenstette, 1994).
If we focus more specifically on the improve-
ment of distributional thesauri, (Ferret, 2012) is
the most comparable work to ours, both because
it is specifically focused on this task and it is
based on the same evaluation framework. (Fer-
ret, 2012) selects in an unsupervised way a set
of positive and negative examples of semantically
similar words from the initial thesaurus, uses them
for training a classifier deciding whether or not a
pair of words are semantically similar and finally,
applies this classifier to the neighbors of each en-
try for reranking them. One of the objectives of
(Ferret, 2012) was to rebalance the initial the-
saurus in favor of low frequency entries. Although
this objective was reached, the resulting thesaurus
tends to have a lower performance than the initial
thesaurus for high frequency entries and for syn-
onyms. The problem with high frequency entries
comes from the fact that applying a machine learn-
ing classifier to its training examples does not lead
to a perfect result. The problem with synonyms
7It is a supervised classifier but its training set is selected
in an unsupervised way.
arises from the imbalance between semantic simi-
larity and semantic relatedness among training ex-
amples: most of selected examples were pairs of
words linked by semantic relatedness because this
kind of relations are more frequent among seman-
tic neighbors than relations based on semantic sim-
ilarity.
In both cases, the method proposed in (Ferret,
2012) faces the problem of relying only on the dis-
tributional thesaurus it tries to improve. This is an
important difference with the method presented in
this article, which mainly exploits the context of
the occurrences of words in the corpus used for the
building the initial thesaurus. As a consequence, at
a global scale, our reranked thesaurus outperforms
the final thesaurus of (Ferret, 2012) for nearly all
measures. The only exceptions are the P@1 values
for M and WM as reference. However, it should be
noted that values for both MAP and R-precision,
which are more reliable measures than P@1, are
identical for the two thesauri and the same refer-
ences.
6 Conclusion and perspectives
In this article, we have presented a new approach
for reranking the semantic neighbors of a distribu-
tional thesaurus. This approach relies on the unsu-
pervised building of discriminative classifiers ded-
icated to the identification of its entries in texts,
with the objective to characterize their meaning
according to the distributional hypothesis. The
classifier built for an entry is then applied to a
set of occurrences of its neighbors for identifying
and downgrading those that are not semantically
related to the entry. The proposed method was
tested on a large thesaurus of nouns for English
and led to a significant improvement of this the-
saurus, especially for middle and low frequency
entries and for semantic relatedness. We plan to
extend this work by taking into account the no-
tion of word sense as it is done in (Reisinger and
Mooney, 2010) or (Huang et al, 2012): since we
rely on occurrences of words in texts, this exten-
sion should be quite straightforward by turning our
word-in-context classifiers into true word sense
classifiers.
Acknowledgments
This work was partly supported by the project
ANR ASFALDA ANR-12-CORD-0023.
569
References
Andrei Alexandrescu and Katrin Kirchhoff. 2007.
Data-driven graph construction for semi-supervised
graph-based learning in NLP. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL HLT 2007), pages 204?
211, Rochester, New York.
Satanjeev Bano Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic relat-
edness. In Eighteenth International Conference on
Artificial Intelligence (IJCAI-03), Acapulco, Mex-
ico.
Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-Based Transformation in Measur-
ing Semantic Relatedness. In 22nd Canadian Con-
ference on Artificial Intelligence, pages 187?190.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
James Curran and Marc Moens. 2002a. Scaling con-
text space. In 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-02), pages
231?238, Philadelphia, Pennsylvania, USA.
James R. Curran and Marc Moens. 2002b. Improve-
ments in automatic thesaurus extraction. In Work-
shop of the ACL Special Interest Group on the Lexi-
con (SIGLEX), pages 59?66, Philadelphia, USA.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), short paper, pages 92?97,
Uppsala, Sweden, July.
Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Seventh conference on International Language Re-
sources and Evaluation (LREC?10), Valletta, Malta.
Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In 20th European Conference on Artificial
Intelligence (ECAI 2012), pages 336?341, Montpel-
lier, France.
John R. Firth, 1957. Studies in Linguistic Analysis,
chapter A synopsis of linguistic theory 1930-1955,
pages 1?32. Blackwell, Oxford.
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distribu-
tional representations of synonymy. In Ninth Con-
ference on Computational Natural Language Learn-
ing (CoNLL), pages 25?32, Ann Arbor, Michigan,
USA.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In 20th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI 2007), pages 6?12.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. Work on statistical methods for
word sense disambiguation. In AAAI Fall Sympo-
sium on Probabilistic Approaches to Natural Lan-
guage, pages 54?60.
Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Kris Heylen, Yves Peirsmany, Dirk Geeraerts, and
Dirk Speelman. 2008. Modelling Word Similarity:
An Evaluation of Automatic Synonymy Extraction
Algorithms. In Sixth conference on International
Language Resources and Evaluation (LREC 2008),
Marrakech, Morocco.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?12), pages
873?882.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
bayesian method for robust estimation of distribu-
tional similarities. In 48th Annual Meeting of the
Association for Computational Linguistics, pages
247?256, Uppsala, Sweden.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In 2002
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 41?48.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In 17th International Confer-
ence on Computational Linguistics and 36th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-COLING?98), pages 768?774, Mon-
tral, Canada.
George A. Miller. 1990. WordNet: An On-Line Lex-
ical Database. International Journal of Lexicogra-
phy, 3(4).
Jane Morris and Graeme Hirst. 2004. Non-
classical lexical semantic relations. In Workshop
on Computational Lexical Semantics of Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 46?51, Boston, MA.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
570
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the
relatedness of concepts. In HLT-NAACL 2004,
demonstration papers, pages 38?41, Boston, Mas-
sachusetts, USA.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2010), pages 109?117, Los Angeles,
California, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing.
Grady Ward. 1996. Moby thesaurus. Moby Project.
Julie Weeds. 2003. Measures and Applications of Lex-
ical Distributional Similarity. Ph.D. thesis, Depart-
ment of Informatics, University of Sussex.
Kazuhide Yamamoto and Takeshi Asakura. 2010.
Even unassociated features can improve lexical dis-
tributional similarity. In Second Workshop on
NLP Challenges in the Information Explosion Era
(NLPIX 2010), pages 32?39, Beijing, China.
Torsten Zesch and Iryna Gurevych. 2010. Wisdom
of crowds versus wisdom of linguists - measuring
the semantic relatdness of words. Natural Language
Engineering, 16(1):25?59.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics, 35(3):435?461.
571
