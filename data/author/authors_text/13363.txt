Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 49?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Natural Language Processing at the School of Information Studies for Africa
Bjo?rn Gamba?ck
Userware Laboratory
Swedish Institute of Computer Science
Box 1263, SE?164 29 Kista, Sweden
gamback@sics.se
Gunnar Eriksson
Department of Linguistics
Stockholm University
SE?106 91 Stockholm, Sweden
gunnar@ling.su.se
Athanassia Fourla
Swedish Program for ICT in Developing Regions
Royal Institute of Technology/KTH
Forum 100, SE?164 40 Kista, Sweden
afourla@dsv.su.se
Abstract
The lack of persons trained in computa-
tional linguistic methods is a severe obsta-
cle to making the Internet and computers
accessible to people all over the world in
their own languages. The paper discusses
the experiences of designing and teach-
ing an introductory course in Natural Lan-
guage Processing to graduate computer
science students at Addis Ababa Univer-
sity, Ethiopia, in order to initiate the ed-
ucation of computational linguists in the
Horn of Africa region.
1 Introduction
The development of tools and methods for language
processing has so far concentrated on a fairly small
number of languages and mainly on the ones used
in the industrial part of the world. However, there
is a potentially even larger need for investigating the
application of computational linguistic methods to
the languages of the developing countries: the num-
ber of computer and Internet users of these coun-
tries is growing, while most people do not speak the
European and East-Asian languages that the com-
putational linguistic community has so far mainly
concentrated on. Thus there is an obvious need to
develop a wide range of applications in vernacular
languages, such as translation systems, spelling and
grammar checkers, speech synthesis and recogni-
tion, information retrieval and filtering, and so forth.
But who will develop those systems? A prerequisite
to the creation of NLP applications is the education
and training of computer professionals skilled in lo-
calisation and development of language processing
resources. To this end, the authors were invited to
conduct a course in Natural Language Processing at
the School of Information Studies for Africa, Addis
Ababa University, Ethiopia. As far as we know, this
was the first computational linguistics course given
in Ethiopia and in the entire Horn of Africa region.
There are several obstacles to progress in lan-
guage processing for new languages. Firstly, the par-
ticulars of a language itself might force new strate-
gies to be developed. Secondly, the lack of already
available language processing resources and tools
creates a vicious circle: having resources makes pro-
ducing resources easier, but not having resources
makes the creation and testing of new ones more dif-
ficult and time-consuming.
Thirdly, there is often a disturbing lack of interest
(and understanding) of the needs of people to be able
to use their own language in computer applications
? a lack of interest in the surrounding world, but
also sometimes even in the countries where a lan-
guage is used (?Aren?t those languages going to be
extinct in 50?100 years anyhow?? and ?Our com-
pany language is English? are common comments).
And finally, we have the problem that the course
described in this paper mainly tries to address, the
lack of skilled professionals and researchers with
knowledge both of language processing techniques
and of the domestic language(s) in question.
49
The rest of the paper is laid out as follows: The next
section discusses the language situation in Ethiopia
and some of the challenges facing those trying to in-
troduce NLP methods in the country. Section 3 gives
the background of the students and the university,
before Section 4 goes into the effects these factors
had on the way the course was designed.
The sections thereafter describe the actual course
content, with Section 5 being devoted to the lectures
of the first half of the course, on general linguistics
and word level processing; Section 6 is on the sec-
ond set of lectures, on higher level processing and
applications; while Section 7 is on the hands-on ex-
ercises we developed. The evaluation of the course
and of the students? performance is the topic of Sec-
tion 8, and Section 9 sums up the experiences and
novelties of the course and the effects it has so far
had on introducing NLP in Ethiopia.
2 Languages and NLP in Ethiopia
Ethiopia was the only African country that managed
to avoid being colonised during the big European
power struggles over the continent during the 19th
century. While the languages of the former colonial
powers dominate the higher educational system and
government in many other countries, it would thus
be reasonable to assume that Ethiopia would have
been using a vernacular language for these purposes.
However, this is not the case. After the removal
of the Dergue junta, the Constitution of 1994 di-
vided Ethiopia into nine fairly independent regions,
each with its own ?nationality language?, but with
Amharic being the language for countrywide com-
munication. Until 1994, Amharic was also the prin-
cipal language of literature and medium of instruc-
tion in primary and secondary schools, but higher
education in Ethiopia has all the time been carried
out in English (Bloor and Tamrat, 1996).
The reason for adopting English as the Lingua
Franca of higher education is primarily the linguis-
tic diversity of the country (and partially also an ef-
fect of the fact that British troops liberated Ethiopia
from a brief Italian occupation during the Second
World War). With some 70 million inhabitants,
Ethiopia is the third most populous African country
and harbours more than 80 different languages ?
exactly how many languages there are in a country
is as much a political as a linguistic issue; the count
of languages of Ethiopia and Eritrea together thus
differs from 70 up to 420, depending on the source;
with, for example, the Ethnologue (Gordon, 2005)
listing 89 different ones.
Half-a-dozen languages have more than 1 million
speakers in Ethiopia; three of these are dominant:
the language with most speakers today is probably
Oromo, a Cushitic language spoken in the south and
central parts of the country and written using the
Latin alphabet. However, Oromo has not reached
the same political status as the two large Semitic
languages Tigrinya and Amharic. Tigrinya is spo-
ken in Northern Ethiopia and is the official lan-
guage of neighbouring Eritrea; Amharic is spoken
in most parts of the country, but predominantly in
the Eastern, Western, and Central regions. Oromo
and Amharic are probably two of the five largest lan-
guages on the continent; however, with the dramatic
population size changes in many African coun-
tries in recent years, this is difficult to determine:
Amharic is estimated to be the mother tongue of
more than 17 million people, with at least an addi-
tional 5 million second language speakers.
As Semitic languages, Amharic and Tigrinya are
distantly related to Arabic and Hebrew; the two lan-
guages themselves are probably about as close as
are Spanish and Portuguese (Bloor, 1995). Speak-
ers of Amharic and Tigrinya are mainly Orthodox
Christians and the languages draw common roots
to the ecclesiastic Ge?ez still used by the Coptic
Church. Both languages use the Ge?ez (Ethiopic)
script, written horizontally and left-to-right (in con-
trast to many other Semitic languages). Written
Ge?ez can be traced back to at least the 4th century
A.D. The first versions of the script included con-
sonants only, while the characters in later versions
represent consonant-vowel pairs. Modern Amharic
words have consonantal roots with vowel variation
expressing difference in interpretation.
Several computer fonts have been developed for
the Ethiopic script, but for many years the languages
had no standardised computer representation. An
international standard for the script was agreed on
only in year 1998 and later incorporated into Uni-
code, but nationally there are still about 30 differ-
ent ?standards? for the script, making localisation of
language processing systems and digital resources
50
difficult; and even though much digital information
is now being produced in Ethiopia, no deep-rooted
culture of information exchange and dissemination
has been established. In addition to the digital di-
vide, several other factors have contributed to this
situation, including lack of library facilities and cen-
tral resource sites, inadequate resources for digital
production of journals and books, and poor docu-
mentation and archive collections. The difficulties
of accessing information have led to low expecta-
tions and consequently under-utilisation of existing
information resources (Furzey, 1996).
UNESCO (2001) classifies Ethiopia among the
countries with ?moribund or seriously endangered
tongues?. However, the dominating languages of
the country are not under immediate threat, and seri-
ous efforts have been made in the last years to build
and maintain linguistic resources in Amharic: a lot
of work has been carried out mainly by Ethiopian
Telecom, Ethiopian Science and Technology Com-
mission and Addis Ababa University, as well as by
Ethiopian students abroad, in particular in Germany,
Sweden and the United States. Except for some ini-
tial efforts for the related Tigrinya, work on other
Ethiopian languages has so far been scarce or non-
existent ? see Alemu et al (2003) or Eyassu and
Gamba?ck (2005) for short overviews of the efforts
that have been made to date to develop language pro-
cessing tools for Amharic.
One of the reasons for fostering research in lan-
guage processing in Ethiopia was that the exper-
tise of a pool of researchers in the country would
contribute to maintaining those Ethiopian languages
that are in danger of extinction today. Starting
with Amharic and developing a robust linguistic re-
source base in the country, together with including
the Amharic language in modern language process-
ing tools could create the critical mass of experience,
which is necessary in order to expand to other ver-
nacular languages, too.
Moreover, the development of those conditions
that lay the foundations for language and speech
processing research and development in the country
would prevent potential brain drain from Ethiopia;
instead of most language processing work being
done by Ethiopian students abroad (at present), in
the future it could be done by students, researchers
and professionals inside the country itself.
3 Infrastructure and Student Body
Addis Ababa University (AAU) is Ethiopia?s old-
est, largest and most prestigious university. The De-
partment of Information Science (formerly School
of Information Studies for Africa) at the Faculty of
Informatics conducts a two-year Master?s Program.
The students admitted to the program come from
all over the country and have fairly diverse back-
grounds. All have a four-year undergraduate degree,
but not necessarily in any computer science-related
subject. However, most of the students have been
working with computers for some time after their
under-graduate studies. Those admitted to the pro-
gram are mostly among the top students of Ethiopia,
but some places are reserved for public employees.
The initiative of organising a language process-
ing course as part of the Master?s Program came
from the students themselves: several students ex-
pressed interest in writing theses on speech and lan-
guage subjects, but the faculty acknowledged that
there was a severe lack of staff qulified to teach the
course. In fact, all of the university is under-staffed,
while admittance to the different graduate programs
has been growing at an enormous speed; by 400%
only in the last two years. There was already an
ICT support program in effect between AAU and
SAREC, the Department for Research Cooperation
at the Swedish International Development Coopera-
tion Agency. This cooperation was used to establish
contacts with Stockholm University and the Swedish
Institute of Computer Science, that both had experi-
ence in developing computational linguistic courses.
Information Science is a modern department with
contemporary technology. It has two computer labs
with PCs having Internet access and lecture rooms
with all necessary aids. A library supports the teach-
ing work and is accessible both to students and staff.
The only technical problems encountered arose from
the frequent power failures in the country that cre-
ated difficulties in teaching and/or loss of data. In-
ternet access in the region is also often slow and un-
reliable. However, as a result of the SAREC ICT
support program, AAU is equipped with both an in-
ternal network and with broadband connection to the
outside world. The central computer facilities are
protected from power failures by generators, but the
individual departments have no such back-up.
51
4 Course Design
The main aim of the course plan was to introduce
the students successfully to the main subjects of lan-
guage and speech processing and trigger their inter-
est in further investigation. Several factors were im-
portant when choosing the course materials and de-
ciding on the content and order of the lectures and
exercises, in particular the fact that the students did
not have a solid background in either Computer Sci-
ence or Linguistics, and the time limitations as the
course could only last for ten weeks. As a result, a
curriculum with a holistic view of NLP was built in
the form of a ?crash course? (with many lectures and
labs per week, often having to use Saturdays too)
aiming at giving as much knowledge as possible in
a very short time.
The course was designed before the team travelled
to Ethiopia, but was fine-tuned in the field based on
the day-by-day experience and interaction with the
students: even though the lecturers had some knowl-
edge of the background and competence of the stu-
dents, they obviously would have to be flexible and
able to adjust the course set-up, paying attension
both to the specific background knowledge of the
students and to the students? particular interests and
expectations on the course.
From the outset, it was clear that, for example,
very high programming skills could not be taken for
granted, as given that this is not in itself a require-
ment for being admitted to the Master?s Program.
On the other hand, it was also clear that some such
knowledge could be expected, this course would be
the last of the program, just before the students were
to start working on their theses; and several labora-
tory exercises were developed to give the students
hands-on NLP experience.
Coming to a department as external lecturers is
also in general tricky and makes it more difficult to
know what actual student skill level to expect. The
lecturer team had quite extensive previous experi-
ences of giving external courses this way (in Sweden
and Finland) and thus knew that ?the home depart-
ment? often tends to over-estimate the knowledge of
their students; another good reason for trying to be
as flexible as possible in the course design. and for
listening carefully to the feedback from the students
during the course.
The need for flexibility was, however, somewhat
counter-acted by the long geographical distance and
time constraints. It was necessary to give the course
in about two months time only, and with one of the
lecturers present during the first half of the course
and the other two during the second half, with some
overlap in the middle. Thus the course was split into
two main parts, the first concentrating on general lin-
guistic issues, morphology and lexicology, and the
second on syntax, semantics and application areas.
The choice of reading was influenced by the need
not to assume very elaborated student programming
skills. This ruled out books based mainly on pro-
gramming exercises, such as Pereira and Shieber
(1987) and Gazdar and Mellish (1989), and it was
decided to use Jurafsky and Martin (2000) as the
main text of the course. The extensive web page
provided by those authors was also a factor, since it
could not be assumed that the students would have
full-time access to the actual course book itself. The
costs of buying a regular computer science book is
normally too high for the average Ethiopian student.
To partially ease the financial burden on the stu-
dents, we brought some copies of the book with us
and made those available at the department library.
We also tried to make sure that as much as possible
of the course material was available on the web. In
addition to the course book we used articles on spe-
cific lecture topics particularly material on Amharic,
for which we also created a web page devoted to on-
line Amharic resources and publications.
The following sections briefly describe the differ-
ent parts of the course and the laboratory exercises.
The course web page contains the complete course
materials including the slides from the lectures and
the resources and programs used for the exercises:
www.sics.se/humle/ile/kurser/Addis
5 Linguistics and word level processing
The aim of the first part of the course was to give the
students a brief introduction to Linguistics and hu-
man languages, and to introduce common methods
to access, manipulate, and analyse language data at
the word and phrase levels. In total, this part con-
sisted of seven lectures that were accompanied by
three hands-on exercises in the computer laboratory.
52
5.1 Languages: particularities and structure
The first two lectures presented the concept of a
human language. The lectures focused around five
questions: What is language? What is the ecolog-
ical situation of the world?s languages and of the
main languages of Ethiopia? What differences are
there between languages? What makes spoken and
written modalities of language different? How are
human languages built up?
The second lecture concluded with a discussion of
what information you would need to build a certain
NLP application for a language such as Amharic.
5.2 Phonology and writing systems
Phonology and writing systems were addressed in
a lecture focusing on the differences between writ-
ing systems. The SERA standard for transliterating
Ethiopic script into Latin characters was presented.
These problems were also discussed in a lab class.
5.3 Morphology
After a presentation of general morphological con-
cepts, the students were given an introduction to
the morphology of Amharic. As a means of hand-
ling morphology, regular languages/expressions and
finite-state methods were presented and their limi-
tations when processing non-agglutinative morphol-
ogy were discussed. The corresponding lab exercise
aimed at describing Amharic noun morphology us-
ing regular expressions.
In all, the areas of phonology and morphology
were allotted two lectures and about five lab classes.
5.4 Words, phrases and POS-tagging
Under this heading the students were acquainted
with word level phenomena during two lectures. To-
kenisation problems were discussed and the concept
of dependency relations introduced. This led on
to the introduction of the phrase-level and N-gram
models of syntax. As examples of applications us-
ing this kind of knowledge, different types of part-
of-speech taggers using local syntactic information
were discussed. The corresponding lab exercise,
spanning four lab classes, aimed at building N-gram
models for use in such a system.
The last lecture of the first part of the course
addressed lexical semantics with a quick glance at
word sense ambiguation and information retrieval.
6 Applications and higher level processing
The second part of the course started with an
overview lecture on natural language processing
systems and finished off by a final feedback lecture,
in which the course and the exam were summarised
and students could give overall feedback on the total
course contents and requirements.
The overview lecture addressed the topic of what
makes up present-day language processing systems,
using the metaphor of Douglas Adams? Babel fish
(Adams, 1979): ?What components do we need to
build a language processing system performing the
tasks of the Babel fish?? ? to translate unrestricted
speech in one language to another language ? with
Gamba?ck (1999) as additional reading material.
In all, the second course part consisted of nine
regular lectures, two laboratory exercises, and the
final evaluation lecture.
6.1 Machine Translation
The first main application area introduced was Ma-
chine Translation (MT). The instruction consisted
of two 3-hour lectures during which the following
subjects were presented: definitions and history of
machine translation; different types of MT systems;
paradigms of functional MT systems and translation
memories today; problems, terminology, dictionar-
ies for MT; other kinds of translation aids; a brief
overview of the MT market; MT users, evaluation,
and application of MT systems in real life. Parts of
Arnold et al (1994) complemented the course book.
There was no obligatory assignment in this part
of the course, but the students were able to try out
and experiment with online machine translation sys-
tems. Since there is no MT system for Amharic, they
used their knowledge of other languages (German,
French, English, Italian, etc.) to experience the use
of automatic translation tools.
6.2 Syntax and parsing
Three lectures and one laboratory exercise were de-
voted to parsing and the representation of syntax,
and to some present-day syntactic theories. After in-
troducing basic context-free grammars, Dependency
Grammar was taken as an example of a theory un-
derlying many current shallow processing systems.
Definite Clause Grammar, feature structures, the
53
concept of unification, and subcategorisation were
discussed when moving on to more deeper-level,
unification-based grammars.
In order to give the students an understanding of
the parsing problem, both processing of artificial and
natural languages was discussed, as well as human
language processing, in the view of Kimball (1973).
Several types of parsers were introduced, with in-
creasing complexity: top-down and bottom-up pars-
ing; parsing with well-formed substring tables and
charts; head-first parsing and LR parsing.
6.3 Semantics and discourse
Computational semantics and pragmatics were cov-
ered in two lectures. The first lecture introduced
the basic tools used in current approaches to se-
mantic processing, such as lexicalisation, compo-
sitionality and syntax-driven semantic analysis, to-
gether with different ways of representing meaning:
first-order logic, model-based and lambda-based se-
mantics. Important sources of semantic ambiguity
(quantifiers, for example) were discussed together
with the solutions allowed by using underspecified
semantic representations.
The second lecture continued the semantic repre-
sentation thread by moving on to how a complete
discourse may be displayed in a DRS, a Discourse
Representation Structure, and how this may be used
to solve problems like reference resolution. Dia-
logue and user modelling were introduced, covering
several current conversational systems, with Zue and
Glass (2000) and Wilks and Catizone (2000) as extra
reading material.
6.4 Speech technology
The final lecture before the exam was the only one
devoted to speech technology and spoken language
translation systems. Some problems in current spo-
ken dialogue systems were discussed, while text-to-
speech synthesis and multimodal synthesis were just
briefly touched upon. The bulk of the lecture con-
cerned automatic speech recognition: the parts and
architectures of state-of-the-art speech recognition
systems, Bayes? rule, acoustic modeling, language
modeling, and search strategies, such as Viterbi and
A-star were introduced, as well as attempts to build
recognition systems based on hybrids between Hid-
den Markov Models and Artificial Neural Networks.
7 Laboratory Exercises
Even though we knew before the course that the stu-
dents? actual programming skills were not extensive,
we firmly believe that the best way to learn Compu-
tational Linguistics is by hands-on experience. Thus
a substantial part of the course was devoted to a set
of laboratory exercises, which made up almost half
of the overall grade on the course.
Each exercise was designed so that there was an
(almost obligatory) short introductory lecture on the
topic and the requirements of the exercise, followed
by several opportunities for the students to work on
the exercise in the computer lab under supervision
from the lecturer. To pass, the students both had
to show a working system solving the set problem
and hand in a written solution/explanation. Students
were allowed to work together on solving the prob-
lem, while the textual part had to be handed in by
each student individually, for grading purposes.
7.1 Labs 1?3: Word level processing
The laboratory exercises during the first half of the
course were intended to give the students hands-
on experience of simple language processing using
standard UNIX tools and simple Perl scripts. The
platform was cygwin,1 a freeware UNIX-like envi-
ronment for Windows. The first two labs focused
on regular expressions and the exercises included
searching using ?grep?, simple text preprocessing us-
ing ?sed?, and building a (rather simplistic) model
of Amharic noun morphology using regular expres-
sions in (template) Perl scripts. The third lab exer-
cise was devoted to the construction of probabilis-
tic N-gram data from text corpora. Again standard
UNIX tools were used.
Due to the students? lack of experience with this
type of computer processing, more time than ex-
pected was spent on acquainting them with the
UNIX environment during the first lab excercises.
7.2 Labs 4?5: Higher level processing
The practical exercises during the second half of
the course consisted of a demo and trial of on-line
machine translation systems, and two obligatory as-
signments, on grammars and parsing and on seman-
tics and discourse, respectively. Both these exercises
1www.cygwin.com
54
consisted of two parts and were carried out in the
(freeware) SWI-Prolog framework.2
In the first part of the fourth lab exercise, the
students were to familiarise themselves with basic
grammars by trying out and testing parsing with a
small context-free grammar. The assignments then
consisted in extending this grammar both to add cov-
erage and to restrict it (to stop ?leakage?). The
second part of the lab was related to parsing. The
students received parsers encoding several different
strategies: top-down, bottom-up, well-formed sub-
string tables, head parsing, and link parsing (a link
parser improves a bottom-up parser in a similar way
as a WFST parser improves a top-down parser, by
saving partial parses). The assignments included
creating a test corpus for the parsers, running the
parsers on the corpus, and trying to determine which
of the parsers gave the best performance (and why).
The assignments of the fifth lab were on lambda-
based semantics and the problems arising in a gram-
mar when considering left-recursion and ambiguity.
The lab also had a pure demo part where the students
tried out Johan Bos? ?Discourse Oriented Represen-
tation and Inference System?, DORIS.3
8 Course Evaluation and Grading
The students were encouraged from the beginning
to interact with the lecturers and to give feedback
on teaching and evaluation issues. With the aim of
coming up with the best possible assessment strat-
egy ? in line with suggestions in work reviewed by
Elwood and Klenowski (2002), three meetings with
the students took place at the beginning, the middle,
and end of the course. In these meetings, students
and lecturers together discussed the assessment cri-
teria, the form of the exam, the percentage of the
grade that each part of the exam would bear, and
some examples of possible questions.
This effort to better reflect the objectives of the
course resulted in the following form of evaluation:
the five exercises of the previous section were given,
with the first one carrying 5% of the total course
grade, the other four 10% each, and an additional
written exam (consisting of thirteen questions from
the whole curriculum taught) 55%.
2www.swi-prolog.org
3www.cogsci.ed.ac.uk/?jbos/doris
While correcting the exams, the lecturers tried to
bear in mind that this was the first acquaintance of
the students with NLP. Given the restrictions on the
course, the results were quite positive, as none of the
students taking the exam failed the course. After the
marking of the exams an assessment meeting with
all the students and the lecturers was held, during
which each question of the exam was explained to-
gether with the right answer. The evaluation of the
group did not present particular problems. For grad-
ing, the American system was used according to the
standards of Addis Ababa University (i.e., with the
grades ?A+?, ?A?, ..., ?F?).
9 Results
Except for the contents of the course, the main inno-
vation for the Information Science students was that
the bulk of the course reading list and relevant ma-
terials were available online. The students were able
to access the materials according to their own needs
? in terms of time schedule ? and download and
print it without having to go to the library to copy
books and papers.
Another feature of the on-line availability was that
after the end of the course and as the teaching team
left the country, the supervision of the students? the-
ses was carried out exclusively through the Internet
by e-mail and chat. The final papers with the signa-
tures of the supervisors were even sent electronically
to the department. The main difficulty that had to be
overcome concerned the actual writing of the theses;
the students were not very experienced in producing
academic text and required some distance training,
through comments and suggestions, on the subject.
The main results of the course were that, based
strictly on the course aims, students were success-
fully familiarised with the notion of NLP. This also
led to eight students choosing to write their Mas-
ter theses on speech and language issues: two on
speech technology, on text-to-speech synthesis for
Tigrinya and on speech recognition for Amharic;
three on Amharic information access, on informa-
tion filtering, on information retrieval and text cat-
egorisation, and on automatic text summarisation;
one on customisation of a prototype English-to-
Amharic transfer-based machine translation system;
one on predictive SMS (Short Message Service) text
55
input for Amharic; and one on Amharic part-of-
speech tagging. Most of these were supervised from
Stockholm by the NLP course teaching team, with
support from the teaching staff in Addis Ababa.
As a short-term effect, several scientific papers
were generated by the Master theses efforts. As
a more lasting effect, a previously fairly unknown
field was not only tapped, but also triggered the stu-
dents? interest for further research. Another impor-
tant result was the strengthening of the connections
between Ethiopian and Swedish academia, with on-
going collaboration and supervision, also of students
from later batches. Still, the most important long-
term effect may have been indirect: triggered by the
success of the course, the Addis Ababa Faculty of
Informatics in the spring of 2005 decided to estab-
lish a professorship in Natural Language Processing.
10 Acknowledgments
Thanks to the staff and students at the Department
of Information Science, Addis Ababa University,
in particular Gashaw Kebede, Kinfe Tadesse, Saba
Amsalu, and Mesfin Getachew; and to Lars Asker
and Atelach Alemu at Stockholm University.
This NLP course was funded by the Faculty of
Informatics at Addis Ababa University and the ICT
support program of SAREC, the Department for
Research Cooperation at Sida, the Swedish Inter-
national Development Cooperation Agency.
References
Douglas Adams. 1979. The Hitch-Hiker?s Guide to the
Galaxy. Pan Books, London, England.
Atelach Alemu, Lars Asker, and Mesfin Getachew. 2003.
Natural language processing for Amharic: Overview
and suggestions for a way forward. In Proceedings
of the 10th Conference on Traitement Automatique des
Langues Naturelles, volume 2, pages 173?182, Batz-
sur-Mer, France, June.
Douglas Arnold, Lorna Balkan, Siety Meijer, R. Lee
Humphreys, and Louisa Sadler. 1994. Machine Trans-
lation: An Introductory Guide. Blackwells-NCC,
London, England.
Thomas Bloor and Wondwosen Tamrat. 1996. Issues
in Ethiopian language policy and education. Jour-
nal of Multilingual and Multicultural Development,
17(5):321?337.
Thomas Bloor. 1995. The Ethiopic writing system: a
profile. Journal of the Simplified Spelling Society,
19:30?36.
Jannette Elwood and Val Klenowski. 2002. Creating
communities of shared practice: the challenges of as-
sessment use in learning and teaching. Assessment &
Evaluation in Higher Education, 27(3):243?256.
Samuel Eyassu and Bjo?rn Gamba?ck. 2005. Classifying
Amharic news text using Self-Organizing Maps. In
ACL 2005 Workshop on Computational Approaches to
Semitic Languages, Ann Arbor, Michigan, June. ACL.
Jane Furzey. 1996. Enpowering socio-economic devel-
opment in Africa utilizing information technology. A
country study for the United Nations Economic Com-
mission for Africa (UNECA), African Studies Center,
University of Pennsylvania.
Bjo?rn Gamba?ck. 1999. Human language technology:
The Babel fish. Technical Report T99-09, SICS,
Stockholm, Sweden, November.
Gerald Gazdar and Chris Mellish. 1989. Natural Lan-
guage Processing in Prolog. Addison-Wesley, Wok-
ingham, England.
Raymond G. Gordon, Jr, editor. 2005. Ethnologue: Lan-
guages of the World. SIL International, Dallas, Texas,
15 edition.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall, Upper Saddle
River, New Jersey.
John Kimball. 1973. Seven principles of surface
structure parsing in natural languages. Cognition,
2(1):15?47.
Fernando C. N. Pereira and Stuart M. Shieber. 1987.
Prolog and Natural Language Analysis. Number 10
in Lecture Notes. CSLI, Stanford, California.
Yorick Wilks and Roberta Catizone. 2000. Human-
computer conversation. In Encyclopedia of Microcom-
puters. Dekker, New York, New York.
Stephen Wurm, editor. 2001. Atlas of the World?s Lan-
guages in Danger of Disappearing. The United Na-
tions Educational, Scientific and Cultural Organization
(UNESCO), Paris, France, 2 edition.
Victor Zue and James Glass. 2000. Conversational inter-
faces: Advances and challenges. Proceedings of the
IEEE, 88(8):1166?1180.
56
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 71?78,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Classifying Amharic News Text Using Self-Organizing Maps
Samuel Eyassu
Department of Information Science
Addis Ababa University, Ethiopia
samueleya@yahoo.com
Bjo?rn Gamba?ck?
Swedish Institute of Computer Science
Box 1263, SE?164 29 Kista, Sweden
gamback@sics.se
Abstract
The paper addresses using artificial neu-
ral networks for classification of Amharic
news items. Amharic is the language for
countrywide communication in Ethiopia
and has its own writing system contain-
ing extensive systematic redundancy. It is
quite dialectally diversified and probably
representative of the languages of a conti-
nent that so far has received little attention
within the language processing field.
The experiments investigated document
clustering around user queries using Self-
Organizing Maps, an unsupervised learn-
ing neural network strategy. The best
ANN model showed a precision of 60.0%
when trying to cluster unseen data, and a
69.5% precision when trying to classify it.
1 Introduction
Even though the last years have seen an increasing
trend in investigating applying language processing
methods to other languages than English, most of
the work is still done on very few and mainly Euro-
pean and East-Asian languages; for the vast number
of languages of the African continent there still re-
mains plenty of work to be done. The main obsta-
cles to progress in language processing for these are
two-fold. Firstly, the peculiarities of the languages
themselves might force new strategies to be devel-
oped. Secondly, the lack of already available re-
sources and tools makes the creation and testing of
new ones more difficult and time-consuming.
?Author for correspondence.
Many of the languages of Africa have few speak-
ers, and some lack a standardised written form, both
creating problems for building language process-
ing systems and reducing the need for such sys-
tems. However, this is not true for the major African
languages and as example of one of those this pa-
per takes Amharic, the Semitic language used for
countrywide communication in Ethiopia. With more
than 20 million speakers, Amharic is today probably
one of the five largest on the continent (albeit diffi-
cult to determine, given the dramatic population size
changes in many African countries in recent years).
The Ethiopian culture is ancient, and so are the
written languages of the area, with Amharic using
its own script. Several computer fonts for the script
have been developed, but for many years it had no
standardised computer representation1 which was a
deterrent to electronic publication. An exponentially
increasing amount of digital information is now be-
ing produced in Ethiopia, but no deep-rooted cul-
ture of information exchange and dissemination has
been established. Different factors are attributed to
this, including lack of digital library facilities and
central resource sites, inadequate resources for elec-
tronic publication of journals and books, and poor
documentation and archive collections. The diffi-
culties to access information have led to low expec-
tations and under-utilization of existing information
resources, even though the need for accurate and fast
information access is acknowledged as a major fac-
tor affecting the success and quality of research and
development, trade and industry (Furzey, 1996).
1An international standard for Amharic was agreed on only
in year 1998, following Amendment 10 to ISO?10646?1. The
standard was finally incorporated into Unicode in year 2000:
www.unicode.org/charts/PDF/U1200.pdf
71
In recent years this has lead to an increasing aware-
ness that Amharic language processing resources
and digital information access and storage facili-
ties must be created. To this end, some work has
now been carried out, mainly by Ethiopian Telecom,
the Ethiopian Science and Technology Commission,
Addis Ababa University, the Ge?ez Frontier Foun-
dation, and Ethiopian students abroad. So have, for
example, Sisay and Haller (2003) looked at Amharic
word formation and lexicon building; Nega and Wil-
lett (2002) at stemming; Atelach et al (2003a) at
treebank building; Daniel (Yacob, 2005) at the col-
lection of an (untagged) corpus, tentatively to be
hosted by Oxford University?s Open Archives Ini-
tiative; and Cowell and Hussain (2003) at charac-
ter recognition.2 See Atelach et al (2003b) for an
overview of the efforts that have been made so far to
develop language processing tools for Amharic.
The need for investigating Amharic information
access has been acknowledged by the European
Cross-Language Evaluation Forum, which added an
Amharic?English track in 2004. However, the task
addressed was for accessing an English database
in English, with only the original questions being
posed in Amharic (and then translated into English).
Three groups participated in this track, with Atelach
et al (2004) reporting the best results.
In the present paper we look at the problem of
mapping questions posed in Amharic onto a col-
lection of Amharic news items. We use the Self-
Organizing Map (SOM) model of artificial neural
networks for the task of retrieving the documents
matching a specific query. The SOMs were imple-
mented using the Matlab Neural Network Toolbox.
The rest of the paper is laid out as follows. Sec-
tion 2 discusses artificial neural networks and in par-
ticular the SOM model and its application to infor-
mation access. In Section 3 we describe the Amharic
language and its writing system in more detail to-
gether with the news items corpora used for training
and testing of the networks, while Sections 4 and 5
detail the actual experiments, on text retrieval and
text classification, respectively. Finally, Section 6
sums up the main contents of the paper.
2In the text we follow the Ethiopian practice of referring to
Ethiopians by their given names. However, the reference list
follows Western standard and is ordered according to surnames
(i.e., the father?s name for an Ethiopian).
2 Artificial Neural Networks
Artificial Neural Networks (ANN) is a computa-
tional paradigm inspired by the neurological struc-
ture of the human brain, and ANN terminology bor-
rows from neurology: the brain consists of millions
of neurons connected to each other through long and
thin strands called axons; the connecting points be-
tween neurons are called synapses.
ANNs have proved themselves useful in deriving
meaning from complicated or imprecise data; they
can be used to extract patterns and detect trends that
are too complex to be noticed by either humans or
other computational and statistical techniques. Tra-
ditionally, the most common ANN setup has been
the backpropagation architecture (Rumelhart et al,
1986), a supervised learning strategy where input
data is fed forward in the network to the output
nodes (normally with an intermediate hidden layer
of nodes) while errors in matches are propagated
backwards in the net during training.
2.1 Self-Organizing Maps
Self-Organizing Maps (SOM) is an unsupervised
learning scheme neural network, which was in-
vented by Kohonen (1999). It was originally devel-
oped to project multi-dimensional vectors on a re-
duced dimensional space. Self-organizing systems
can have many kinds of structures, a common one
consists of an input layer and an output layer, with
feed-forward connections from input to output lay-
ers and full connectivity (connections between all
neurons) in the output layer.
A SOM is provided with a set of rules of a lo-
cal nature (a signal affects neurons in the immedi-
ate vicinity of the current neuron), enabling it to
learn to compute an input-output pairing with spe-
cific desirable properties. The learning process con-
sists of repeatedly modifying the synaptic weights
of the connections in the system in response to input
(activation) patterns and in accordance to prescribed
rules, until a final configuration develops. Com-
monly both the weights of the neuron closest match-
ing the inputs and the weights of its neighbourhood
nodes are increased. At the beginning of the training
the neighbourhood (where input patterns cluster de-
pending on their similarity) can be fairly large and
then be allowed to decrease over time.
72
2.2 Neural network-based text classification
Neural networks have been widely used in text clas-
sification, where they can be given terms and hav-
ing the output nodes represent categories. Ruiz
and Srinivasan (1999) utilize an hierarchical array
of backpropagation neural networks for (nonlinear)
classification of MEDLINE records, while Ng et al
(1997) use the simplest (and linear) type of ANN
classifier, the perceptron. Nonlinear methods have
not been shown to add any performance to linear
ones for text categorization (Sebastiani, 2002).
SOMs have been used for information access
since the beginning of the 90s (Lin et al, 1991). A
SOM may show how documents with similar fea-
tures cluster together by projecting the N-dimen-
sional vector space onto a two-dimensional grid.
The radius of neighbouring nodes may be varied to
include documents that are weaker related. The most
elaborate experiments of using SOMs for document
classification have been undertaken using the WEB-
SOM architecture developed at Helsinki University
of Technology (Honkela et al, 1997; Kohonen et al,
2000). WEBSOM is based on a hierarchical two-
level SOM structure, with the first level forming his-
togram clusters of words. The second level is used
to reduce the sensitivity of the histogram to small
variations in document content and performs further
clustering to display the document pattern space.
A Self-Organizing Map is capable of simulating
new data sets without the need of retraining itself
when the database is updated; something which is
not true for Latent Semantic Indexing, LSI (Deer-
wester et al, 1990). Moreover, LSI consumes am-
ple time in calculating similarities of new queries
against all documents, but a SOM only needs to cal-
culate similarities versus some representative subset
of old input data and can then map new input straight
onto the most similar models without having to re-
compute the whole mapping.
The SOM model preparation passes through the
processes undertaken by the LSI model and the clas-
sical vector space model (Salton and McGill, 1983).
Hence those models can be taken as particular cases
of the SOM, when the neighbourhood diameter is
maximized. For instance, one can calculate the
LSI model?s similarity measure of documents versus
queries by varying the SOM?s neighbourhood diam-
eter, if the training set is a singular value decom-
position reduced vector space. Tambouratzis et al
(2003) use SOMs for categorizing texts according to
register and author style and show that the results are
equivalent to those generated by statistical methods.
3 Processing Amharic
Ethiopia with some 70 million inhabitants is the
third most populous African country and harbours
more than 80 different languages.3 Three of these
are dominant: Oromo, a Cushitic language spoken
in the South and Central parts of the country and
written using the Latin alphabet; Tigrinya, spoken in
the North and in neighbouring Eritrea; and Amharic,
spoken in most parts of the country, but predomi-
nantly in the Eastern, Western, and Central regions.
Both Amharic and Tigrinya are Semitic and about as
close as are Spanish and Portuguese (Bloor, 1995),
3.1 The Amharic language and script
Already a census from 19944 estimated Amharic to
be mother tongue of more than 17 million people,
with at least an additional 5 million second language
speakers. It is today probably the second largest lan-
guage in Ethiopia (after Oromo). The Constitution
of 1994 divided Ethiopia into nine fairly indepen-
dent regions, each with its own nationality language.
However, Amharic is the language for countrywide
communication and was also for a long period the
principal literal language and medium of instruction
in primary and secondary schools in the country,
while higher education is carried out in English.
Amharic and Tigrinya speakers are mainly Ortho-
dox Christians, with the languages drawing com-
mon roots to the ecclesiastic Ge?ez still used by the
Coptic Church. Both languages are written using
the Ge?ez script, horizontally and left-to-right (in
contrast to many other Semitic languages). Writ-
ten Ge?ez can be traced back to at least the 4th
century A.D. The first versions of the script in-
cluded consonants only, while the characters in later
versions represent consonant-vowel (CV) phoneme
pairs. In modern written Amharic, each syllable pat-
3How many languages there are in a country is as much a po-
litical as a linguistic issue. The number of languages of Ethiopia
and Eritrea together thus differs from 70 up to 420, depending
on the source; however, 82 (plus 4 extinct) is a common number.
4Published by Ethiopia?s Central Statistal Authority 1998.
73
Order 1 2 3 4 5 6 7
VHHHHC /9/ /u/ /i/ /5/ /e/ /1/ /o/
/s/ ? ? ? ? ? s ?
/m/ ? ? ? ? ? m ?
Table 1: The orders for s (/s/) andm (/m/)
tern comes in seven different forms (called orders),
reflecting the seven vowel sounds. The first order is
the basic form; the other orders are derived from it
by more or less regular modifications indicating the
different vowels. There are 33 basic forms, giving
7*33 syllable patterns, or fidEls.
Two of the base forms represent vowels in isola-
tion (a and ?), but the rest are for consonants (or
semivowels classed as consonants) and thus corre-
spond to CV pairs, with the first order being the base
symbol with no explicit vowel indicator (though a
vowel is pronounced: C+/9/). The sixth order is am-
biguous between being just the consonant or C+/1/.
The writing system also includes 20 symbols for
labialised velars (four five-character orders) and 24
for other labialisation. In total, there are 275 fidEls.
The sequences in Table 1 (for s and m) exemplify
the (partial) symmetry of vowel indicators.
Amharic also has its own numbers (twenty sym-
bols, though not widely used nowadays) and its own
punctuation system with eight symbols, where the
space between words looks like a colon :, while the
full stop, comma and semicolon are ~, , and ;. The
question and exclamation marks have recently been
included in the writing system. For more thorough
discussions of the Ethiopian writing system, see, for
example, Bender et al (1976) and Bloor (1995).
Amharic words have consonantal roots with
vowel variation expressing difference in interpreta-
tion, making stemming a not-so-useful technique in
information retrieval (no full morphological anal-
yser for the language is available yet). There is no
agreed upon spelling standard for compounds and
the writing system uses multitudes of ways to denote
compound words. In addition, not all the letters of
the Amharic script are strictly necessary for the pro-
nunciation patterns of the language; some were sim-
ply inherited from Ge?ez without having any seman-
tic or phonetic distinction in modern Amharic: there
are many cases where numerous symbols are used to
denote a single phoneme, as well as words that have
extremely different orthographic form and slightly
distinct phonetics, but the same meaning. As a re-
sult of this, lexical variation and homophony is very
common, and obviously deteriorates the effective-
ness of Information Access systems based on strict
term matching; hence the basic idea of this research:
to use the approximative matching enabled by self-
organizing map-based artificial neural networks.
3.2 Test data and preprocessing
In our SOM-based experiments, a corpus of news
items was used for text classification. A main ob-
stacle to developing applications for a language like
Amharic is the scarcity of resources. No large cor-
pora for Amharic exist, but we could use a small
corpus of 206 news articles taken from the electronic
news archive of the website of the Walta Information
Center (an Ethiopian news agency). The training
corpus consisted of 101 articles collected by Saba
(Amsalu, 2001), while the test corpus consisted of
the remaining 105 documents collected by Theodros
(GebreMeskel, 2003). The documents were written
using the Amharic software VG2 Main font.
The corpus was matched against 25 queries. The
selection of documents relevant to a given query,
was made by two domain experts (two journal-
ists), one from the Monitor newspaper and the other
from the Walta Information Center. A linguist from
Gonder College participated in making consensus of
the selection of documents made by the two jour-
nalists. Only 16 of the 25 queries were judged to
have a document relevant to them in the 101 docu-
ment training corpus. These 16 queries were found
to be different enough from each other, in the con-
tent they try to address, to help map from document
collection to query contents (which were taken as
class labels). These mappings (assignment) of doc-
uments to 16 distinct classes helped to see retrieval
and classification effectiveness of the ANN model.
The corpus was preprocessed to normalize
spelling and to filter out stopwords. One prepro-
cessing step tried to solve the problems with non-
standardised spelling of compounds, and that the
same sound may be represented with two or more
distinct but redundant written forms. Due to the sys-
tematic redundancy inherited from the Ge?ez, only
about 233 of the 275 fidEls are actually necessary to
74
Sound pattern Matching Amharic characters
/s9/ ?, P
/R9/ ?, ?
/h9/ ?, ?, H, K, p, s
/i9/ ?, ?, a, A
Table 2: Examples of character redundancy
represent Amharic. Some examples of character re-
dundancy are shown in Table 2. The different forms
were reduced to common representations.
A negative dictionary of 745 words was created,
containing both stopwords that are news specific and
the Amharic text stopwords collected by Nega (Ale-
mayehu and Willett, 2002). The news specific com-
mon terms were manually identified by looking at
their frequency. In a second preprocessing step, the
stopwords were removed from the word collection
before indexing. After the preprocessing, the num-
ber of remaining terms in the corpus was 10,363.
4 Text retrieval
In a set of experiments we investigated the devel-
opment of a retrieval system using Self-Organizing
Maps. The term-by-document matrix produced
from the entire collection of 206 documents was
used to measure the retrieval performance of the sys-
tem, of which 101 documents were used for train-
ing and the remaining for testing. After the prepro-
cessing described in the previous section, a weighted
matrix was generated from the original matrix using
the log-entropy weighting formula (Dumais, 1991).
This helps to enhance the occurrence of a term in
representing a particular document and to degrade
the occurrence of the term in the document col-
lection. The weighted matrix can then be dimen-
sionally reduced by Singular Value Decomposition,
SVD (Berry et al, 1995). SVD makes it possible to
map individual terms to the concept space.
A query of variable size is useful for compar-
ison (when similarity measures are used) only if
its size is matrix-multiplication-compatible with the
documents. The pseudo-query must result from the
global weight obtained in weighing the original ma-
trix to be of any use in ranking relevant documents.
The experiment was carried out in two versions, with
the original vector space and with a reduced one.
4.1 Clustering in unreduced vector space
In the first experiment, the selected documents were
indexed using 10,363 dimensional vectors (i.e., one
dimension per term in the corpus) weighted using
log-entropy weighting techniques. These vectors
were fed into an Artificial Neural Network that was
created using a SOM lattice structure for mapping
on a two-dimensional grid. Thereafter a query and
101 documents were fed into the ANN to see how
documents cluster around the query.
For the original, unnormalised (unreduced,
10,363 dimension) vector space we did not try to
train an ANN model for more than 5,000 epochs
(which takes weeks), given that the network perfor-
mance in any case was very bad, and that the net-
work for the reduced vector space had its apex at
that point (as discussed below).
Those documents on the node on which the sin-
gle query lies and those documents in the imme-
diate vicinity of it were taken as being relevant to
the query (the neighbourhood was defined to be six
nodes). Ranking of documents was performed using
the cosine similarity measure, on the single query
versus automatically retrieved relevant documents.
The eleven-point average precision was calculated
over all queries. For this system the average preci-
sion on the test set turned out to be 10.5%, as can be
seen in the second column of Table 3.
The table compares the results on training on the
original vector space to the very much improved
ones obtained by the ANN model trained on the re-
duced vector space, described in the next section.
Recall Original vector Reduced vector
0.00 0.2080 0.8311
0.10 0.1986 0.7621
0.20 0.1896 0.7420
0.30 0.1728 0.7010
0.40 0.0991 0.6888
0.50 0.0790 0.6546
0.60 0.0678 0.5939
0.70 0.0543 0.5300
0.80 0.0403 0.4789
0.90 0.0340 0.3440
1.00 0.0141 0.2710
Average 0.1052 0.5998
Table 3: Eleven-point precision for 16 queries
75
4.2 Clustering in SVD-reduced vector space
In a second experiment, vectors of numerically in-
dexed documents were converted to weighted matri-
ces and further reduced using SVD, to infer the need
for representing co-occurrence of words in identify-
ing a document. The reduced vector space of 101
pseudo-documents was fed into the neural net for
training. Then, a query together with 105 documents
was given to the trained neural net for simulation and
inference purpose.
For the reduced vectors a wider range of values
could be tried. Thus 100, 200, . . . , 1000 epochs
were tried at the beginning of the experiment. The
network performance kept improving and the train-
ing was then allowed to go on for 2000, 3000,
. . . , 10,000, 20,000 epochs thereafter. The average
classification accuracy was at an apex after 5,000
epochs, as can been seen in Figure 1.
The neural net with the highest accuracy was se-
lected for further analysis. As in the previous model,
documents in the vicinity of the query were ranked
using the cosine similarity measure and the precision
on the test set is illustrated in the third column of Ta-
ble 3. As can be seen in the table, this system was
effective with 60.0% eleven-point average precision
on the test set (each of the 16 queries was tested).
Thus, the performance of the reduced vector
space system was very much better than that ob-
tained using the test set of the normal term docu-
ment matrix that resulted in only 10.5% average pre-
cision. In both cases, the precision of the training set
was assessed using the classification accuracy which
shows how documents with similar features cluster
together (occur on the same or neighbouring nodes).
50
55
60
65
70
0 5 10 15 20
%
Epochs (*103)
Figure 1: Average network classification accuracy
5 Document Classification
In a third experiment, the SVD-reduced vector space
of pseudo-documents was assigned a class label
(query content) to which the documents of the train-
ing set were identified to be more similar (by ex-
perts) and the neural net was trained using the
pseudo-documents and their target classes. This was
performed for 100 to 20,000 epochs and the neural
net with best accuracy was considered for testing.
The average precision on the training set was
found to be 72.8%, while the performance of the
neural net on the test set was 69.5%. A matrix of
simple queries merged with the 101 documents (that
had been used for training) was taken as input to
a SOM-model neural net and eventually, the 101-
dimensional document and single query pairs were
mapped and plotted onto a two-dimensional space.
Figure 2 gives a flavour of the document clustering.
The results of this experiment are compatible with
those of Theodros (GebreMeskel, 2003) who used
the standard vector space model and latent semantic
indexing for text categorization. He reports that the
vector space model gave a precision of 69.1% on the
training set. LSI improved the precision to 71.6%,
which still is somewhat lower than the 72.8% ob-
tained by the SOM model in our experiments. Go-
ing outside Amharic, the results can be compared to
the ones reported by Cai and Hofmann (2003) on the
Reuters-21578 corpus5 which contains 21,578 clas-
sified documents (100 times the documents available
for Amharic). Used an LSI approach they obtained
document average precision figures of 88?90%.
In order to locate the error sources in our exper-
iments, the documents missed by the SOM-based
classifier (documents that were supposed to be clus-
tered on a given class label, but were not found un-
der that label), were examined. The documents that
were rejected as irrelevant by the ANN using re-
duced dimension vector space were found to contain
only a line or two of interest to the query (for the
training set as well as for the test set). Also within
the test set as well as in the training set some relevant
documents had been missed for unclear reasons.
Those documents that had been retrieved as rel-
evant to a query without actually having any rele-
vance to that query had some words that co-occur
5Available at www.daviddlewis.com/resources
76
Figure 2: Document clustering at different neuron positions
with the words of the relevant documents. Very im-
portant in this observation was that documents that
could be of some interest to two classes were found
at nodes that are the intersection of the nodes con-
taining the document sets of the two classes.
6 Summary and Conclusions
A set of experiments investigated text retrieval of se-
lected Amharic news items using Self-Organizing
Maps, an unsupervised learning neural network
method. 101 training set items, 25 queries, and 105
test set items were selected. The content of each
news item was taken as the basis for document in-
dexing, and the content of the specific query was
taken for query indexing. A term?document ma-
trix was generated and the occurrence of terms per
document was registered. This original matrix was
changed to a weighted matrix using the log-entropy
scheme. The weighted matrix was further reduced
using SVD. The length of the query vector was also
reduced using the global weight vector obtained in
weighing the original matrix.
The ANN model using unnormalised vector space
had a precision of 10.5%, whereas the best ANN
model using reduced dimensional vector space per-
formed at a 60.0% level for the test set. For this con-
figuration we also tried to classify the data around a
query content, taken that query as class label. The
results obtained then were 72.8% for the training set
and 69.5% for the test set, which is encouraging.
7 Acknowledgments
Thanks to Dr. Gashaw Kebede, Kibur Lisanu, Lars
Asker, Lemma Nigussie, and Mesfin Getachew; and
to Atelach Alemu for spotting some nasty bugs.
The work was partially funded by the Faculty of
Informatics at Addis Ababa University and the ICT
support programme of SAREC, the Department for
Research Cooperation at Sida, the Swedish Inter-
national Development Cooperation Agency.
77
References
Nega Alemayehu and Peter Willett. 2002. Stemming of
Amharic words for information retrieval. Literary and
Linguistic Computing, 17(1):1?17.
Atelach Alemu, Lars Asker, and Gunnar Eriksson.
2003a. An empirical approach to building an Amharic
treebank. In Proc. 2nd Workshop on Treebanks and
Linguistic Theories, Va?xjo? University, Sweden.
Atelach Alemu, Lars Asker, and Mesfin Getachew.
2003b. Natural language processing for Amharic:
Overview and suggestions for a way forward. In Proc.
10th Conf. Traitement Automatique des Langues Na-
turelles, Batz-sur-Mer, France, pp. 173?182.
Atelach Alemu, Lars Asker, Rickard Co?ster, and Jussi
Karlgren. 2004. Dictionary-based Amharic?English
information retrieval. In 5th Workshop of the Cross
Language Evaluation Forum, Bath, England.
Saba Amsalu. 2001. The application of information re-
trieval techniques to Amharic. MSc Thesis, School of
Information Studies for Africa, Addis Ababa Univer-
sity, Ethiopia.
Marvin Bender, Sydney Head, and Roger Cowley. 1976.
The Ethiopian writing system. In Bender et al, eds,
Language in Ethiopia. Oxford University Press.
Michael Berry, Susan Dumais, and Gawin O?Brien.
1995. Using linear algebra for intelligent information
retrieval. SIAM Review, 37(4):573?595.
Thomas Bloor. 1995. The Ethiopic writing system: a
profile. Journal of the Simplified Spelling Society,
19:30?36.
Lijuan Cai and Thomas Hofmann. 2003. Text catego-
rization by boosting automatically extracted concepts.
In Proc. 26th Int. Conf. Research and Development in
Information Retrieval, pp. 182?189, Toronto, Canada.
John Cowell and Fiaz Hussain. 2003. Amharic character
recognition using a fast signature based algorithm. In
Proc. 7th Int. Conf. Image Visualization, pp. 384?389,
London, England.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal
of the American Society for Information Science,
41(6):391?407.
Susan Dumais. 1991. Improving the retrieval of informa-
tion from external sources. Behavior Research Meth-
ods, Instruments and Computers, 23(2):229?236.
Sisay Fissaha and Johann Haller. 2003. Application of
corpus-based techniques to Amharic texts. In Proc.
MT Summit IX Workshop on Machine Translation for
Semitic Languages, New Orleans, Louisana.
Jane Furzey. 1996. Enpowering socio-economic devel-
opment in Africa utilizing information technology. A
country study for the United Nations Economic Com-
mission for Africa, University of Pennsylvania.
Theodros GebreMeskel. 2003. Amharic text retrieval:
An experiment using latent semantic indexing (LSI)
with singular value decomposition (SVD). MSc The-
sis, School of Information Studies for Africa, Addis
Ababa University, Ethiopia.
Timo Honkela, Samuel Kaski, Krista Lagus, and Teuvo
Kohonen. 1997. WEBSOM ? Self-Organizing Maps
of document collections. In Proc. Workshop on Self-
Organizing Maps, pp. 310?315, Espoo, Finland.
Teuvo Kohonen, Samuel Kaski, Krista Lagus, Jarkko
Saloja?rvi, Jukka Honkela, Vesa Paatero, and Antti
Saarela. 2000. Self organization of a massive doc-
ument collection. IEEE Transactions on Neural Net-
works, 11(3):574?585.
Teuvo Kohonen. 1999. Self-Organization and Associa-
tive Memory. Springer, 3 edition.
Xia Lin, Dagobert Soergel, and Gary Marchionini. 1991.
A self-organizing semantic map for information re-
trieval. In Proc. 14th Int. Conf. Research and Develop-
ment in Information Retrieval, pp. 262?269, Chicago,
Illinois.
Hwee Tou Ng, Wei Boon Goh, and Kok Leong Low.
1997. Feature selection, perceptron learning, and a us-
ability case study for text categorization. In Proc. 20th
Int. Conf. Research and Development in Information
Retrieval, pp. 67?73, Philadelphia, Pennsylvania.
Miguel Ruiz and Padmini Srinivasan. 1999. Hierarchical
neural networks for text categorization. In Proc. 22nd
Int. Conf. Research and Development in Information
Retrieval, pp. 281?282, Berkeley, California.
David Rumelhart, Geoffrey Hinton, and Ronald
Williams. 1986. Learning internal representations by
error propagation. In Rumelhart and McClelland, eds,
Parallel Distributed Processing, vol 1. MIT Press.
Gerard Salton and Michael McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys,
34(1):1?47.
George Tambouratzis, N. Hairetakis, S. Markantonatou,
and G. Carayannis. 2003. Applying the SOM model
to text classification according to register and stylistic
content. Int. Journal of Neural Systems, 13(1):1?11.
Daniel Yacob. 2005. Developments towards an elec-
tronic Amharic corpus. In Proc. TALN 12 Workshop
on NLP for Under-Resourced Languages, Dourdan,
France, June (to appear).
78
Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ? AfLaT 2009, pages 104?111,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Methods for Amharic Part-of-Speech Tagging
Bjo?rn Gamba?ck?? Fredrik Olsson? Atelach Alemu Argaw? Lars Asker?
?Userware Laboratory ?Dpt. of Computer & Information Science ?Dpt. of Computer & System Sciences
Swedish Institute of Computer Science Norwegian University of Science & Technology Stockholm University
Kista, Sweden Trondheim, Norway Kista, Sweden
{gamback,fredriko}@sics.se gamback@idi.ntnu.no {atelach,asker}@dsv.su.se
Abstract
The paper describes a set of experiments
involving the application of three state-of-
the-art part-of-speech taggers to Ethiopian
Amharic, using three different tagsets.
The taggers showed worse performance
than previously reported results for Eng-
lish, in particular having problems with
unknown words. The best results were
obtained using a Maximum Entropy ap-
proach, while HMM-based and SVM-
based taggers got comparable results.
1 Introduction
Many languages, especially on the African con-
tinent, are under-resourced in that they have
very few computational linguistic tools or corpora
(such as lexica, taggers, parsers or tree-banks)
available. Here, we will concentrate on the task
of developing part-of-speech taggers for Amharic,
the official working language of the government
of the Federal Democratic Republic of Ethiopia:
Ethiopia is divided into nine regions, each with
its own nationality language; however, Amharic is
the language for country-wide communication.
Amharic is spoken by about 30 million people
as a first or second language, making it the second
most spoken Semitic language in the world (after
Arabic), probably the second largest language in
Ethiopia (after Oromo), and possibly one of the
five largest languages on the African continent.
The actual size of the Amharic speaking popula-
tion must be based on estimates: Hudson (1999)
analysed the Ethiopian census from 1994 and in-
dicated that more than 40% of the population then
understood Amharic, while the current size of the
Ethiopian population is about 80 million.1
182.5 million according to CIA (2009); 76.9 according to
Ethiopian parliament projections in December 2008 based on
the preliminary reports from the census of May 2007.
In spite of the relatively large number of speak-
ers, Amharic is still a language for which very few
computational linguistic resources have been de-
veloped, and previous efforts to create language
processing tools for Amharic?e.g., Alemayehu
and Willett (2002) and Fissaha (2005)?have been
severely hampered by the lack of large-scale lin-
guistic resources for the language. In contrast, the
work detailed in the present paper has been able
to utilize the first publicly available medium-sized
tagged Amharic corpus, described in Section 5.
However, first the Amharic language as such is
introduced (in Section 2), and then the task of part-
of-speech tagging and some previous work in the
field is described (Section 3). Section 4 details the
tagging strategies used in the experiments, the re-
sults of which can be found in Section 6 together
with a short discussion. Finally, Section 7 sums up
the paper and points to ways in which we believe
that the results can be improved in the future.
2 Amharic
Written Amharic (and Tigrinya) uses a unique
script originating from the Ge?ez alphabet (the
liturgical language of the Ethiopian Orthodox
Church). Written Ge?ez can be traced back to at
least the 4th century A.D., with the first versions
including consonants only, while the characters
in later versions represent consonant-vowel (CV)
pairs. In modern Ethiopic script each syllograph
(syllable pattern) comes in seven different forms
(called orders), reflecting the seven vowel sounds.
The first order is the basic form; the others are de-
rived from it by modifications indicating vowels.
There are 33 basic forms, giving 7*33 syllographs,
or fidels (?fidel?, lit. ?alphabet? in Amharic, refers
both to the characters and the entire script). Unlike
Arabic and Hebrew, Amharic is written from left
to right. There is no agreed upon spelling standard
for compound words and the writing system uses
several ways to denote compounds
104
form pattern
root sbr CCC
perfect sa?bba?r CVCCVC
imperfect sa?br CVCC
gerund sa?br CVCC
imperative sba?r CCVC
causative assa?bba?r as-CVCCVC
passive ta?sa?bba?r ta?s-CVCCVC
Table 1: Some forms of the verb sbr (?break?)
2.1 Amharic morphology
A significantly large part of the vocabulary con-
sists of verbs, and like many other Semitic lan-
guages, Amharic has a rich verbal morphology
based on triconsonantal roots with vowel variants
describing modifications to, or supplementary de-
tail and variants of the root form. For example,
the root sbr, meaning ?to break? can have (among
others!) the forms shown in Table 1. Subject, gen-
der, number, etc., are also indicated as bound mor-
phemes on the verb, as well as objects and posses-
sion markers, mood and tense, beneficative, mal-
factive, transitive, dative, negative, etc.
Amharic nouns (and adjectives) can be inflected
for gender, number, definiteness, and case, al-
though gender is usually neutral. The definite ar-
ticle attaches to the end of a noun, as do conjunc-
tions, while prepositions are mostly prefixed.
2.2 Processing Amharic morphology
The first effort on Amharic morphological pro-
cessing was a rule-based system for verbs (and
nouns derived from verbs) which used root pat-
terns and affixes to determine lexical and in-
flectional categories (Bayou, 2000), while Bayu
(2002) used an unsupervised learning approach
based on probabilistic models to extract stems,
prefixes, and suffixes for building a morphological
dictionary. The system was able to successfully
analyse 87% of a small testdata set of 500 words.
The first larger-scale morphological analyser
for Amharic verbs used XFST, the Xerox Finite
State Tools (Fissaha and Haller, 2003). This was
later extended to include all word categories (Am-
salu and Gibbon, 2005). Testing with 1620 words
text from an Amharic bible, 88?94% recall and
54?94% precision (depending on the word-class)
were reported. The lowest precision (54%) was
obtained for verbs; Amsalu and Demeke (2006)
thus describe ways to extend the finite-state sys-
tem to handle 6400 simple verbal stems generated
from 1300 root forms.
Alemayehu and Willett (2002) report on a stem-
mer for Information Retrieval for Amharic, and
testing on a 1221 random word sample stated
?Manual assessment of the resulting stems showed
that 95.5 percent of them were linguistically
meaningful,? but gave no evaluation of the cor-
rectness of the segmentations. Argaw and Asker
(2007) created a rule-based stemmer for a similar
task, and using 65 rules and machine readable dic-
tionaries obtained 60.0% accuracy on fictional text
(testing on 300 unique words) and 76.9% on news
articles (on 1503 words, of which 1000 unique).2
3 Part-of-Speech Tagging
Part-of-speech (POS) tagging is normally treated
as a classification task with the goal to assign lex-
ical categories (word classes) to the words in a
text. Most work on tagging has concentrated on
English and on using supervised methods, in the
sense that the taggers have been trained on an
available, tagged corpus. Both rule-based and sta-
tistical / machine-learning based approaches have
been thoroughly investigated. The Brill Tagger
(Brill, 1995) was fundamental in using a com-
bined rule- and learning-based strategy to achieve
96.6% accuracy on tagging the Penn Treebank
version of the Wall Street Journal corpus. That
is, to a level which is just about what humans
normally achieve when hand-tagging a corpus, in
terms of interannotator agreement?even though
Voutilainen (1999) has shown that humans can get
close to the 100% agreement mark if the annota-
tors are allowed to discuss the problematic cases.
Later taggers have managed to improve Brill?s
figures a little bit, to just above 97% on the Wall
Street Journal corpus using Hidden Markov Mod-
els, HMM and Conditional Random Fields, CRF;
e.g., Collins (2002) and Toutanova et al (2003).
However, most recent work has concentrated on
applying tagging strategies to other languages than
English, on combining taggers, and/or on using
unsupervised methods. In this section we will look
at these issues in more detail, in particular with the
relation to languages similar to Amharic.
3.1 Tagging Semitic languages
Diab et al (2004) used a Support Vector Machine,
SVM-based tagger, trained on the Arabic Penn
2Other knowledge sources for processing Amharic in-
clude, e.g., Gasser?s verb stem finder (available from
nlp.amharic.org) and wordlists as those collected by
Gebremichael (www.cs.ru.nl/?biniam/geez).
105
Treebank 1 to tokenize, POS tag, and annotate
Arabic base phrases. With an accuracy of 95.5%
over a set of 24 tags, the data-driven tagger per-
formed on par with state-of-the-art results for En-
glish when trained on similar-sized data (168k to-
kens). Bar-Haim et al (2008) developed a lexicon-
based HMM tagger for Hebrew. They report
89.6% accuracy using 21 tags and training on 36k
tokens of news text. Mansour (2008) ported this
tagger into Arabic by replacing the morphological
analyzer, achieving an accuracy of 96.3% over 26
tags on a 89k token corpus. His approach modifies
the analyses of sentences receiving a low proba-
bility by adding synthetically constructed analyses
proposed by a model using character information.
A first prototype POS tagger for Amharic used
a stochastic HMM to model contextual dependen-
cies (Getachew, 2001), but was trained and tested
on only one page of text. Getachew suggested a
tagset for Amharic consisting of 25 tags. More
recently, CRFs have been applied to segment and
tag Amharic words (Fissaha, 2005), giving an ac-
curacy of 84% for word segmentation, using char-
acter, morphological and lexical features. The best
result for POS-tagging was 74.8%, when adding a
dictionary and bigrams to lexical and morphologi-
cal features, and 70.0% without dictionary and bi-
grams. The data used in the experiments was also
quite small and consisted of 5 annotated news ar-
ticles (1000 words). The tagset was a reduced ver-
sion (10 tags) of the one used by Getachew (2001),
and will be further discussed in Section 5.2.
3.2 Unsupervised tagging
The desire to use unsupervised machine learning
approaches to tagging essentially originates from
the wish to exploit the vast amounts of unlabelled
data available when constructing taggers. The area
is particularly vivid when it comes to the treatment
of languages for which there exist few, if any, com-
putational resources, and for the case of adapting
an existing tagger to a new language domain.
Banko and Moore (2004) compared unsuper-
vised HMM and transformation-based taggers
trained on the same portions of the Penn Treebank,
and showed that the quality of the lexicon used for
training had a high impact on the tagging results.
Duh and Kirchhoff (2005) presented a minimally-
supervised approach to tagging for dialectal Ara-
bic (Colloquial Egyptian), based on a morpholog-
ical analyzer for Modern Standard Arabic and un-
labeled texts in a number of dialects. Using a tri-
gram HMM tagger, they first produced a baseline
system and then gradually improved on that in an
unsupervised manner by adding features so as to
facilitate the analysis of unknown words, and by
constraining and refining the lexicon.
Unsupervised learning is often casted as the
problem of finding (hidden) structure in unla-
beled data. Goldwater and Griffiths (2007) noted
that most recent approaches to this problem aim
to identify the set of attributes that maximizes
some target function (Maximum Likelihood Esti-
mation), and then to select the values of these at-
tributes based on the representation of the model.
They proposed a different approach, based on
Bayesian principles, which tries to directly max-
imize the probability of the attributes based on
observation in the data. This Bayesian approach
outperformed Maximum Likelihood Estimation
when training a trigram HMM tagger for English.
Toutanova and Johnson (2007) report state-of-the-
art results by extending the work on Bayesian
modelling for unsupervised learning of taggers
both in the way that prior knowledge can be incor-
porated into the model, and in the way that possi-
ble tags for a given word is explicitly modeled.
3.3 Combining taggers
A possible way to improve on POS tagging results
is to combine the output of several different tag-
gers into a committee, forming joint decisions re-
garding the labeling of the input. Roughly, there
are three obvious ways of combining multiple pre-
dicted tags for a word: random decision, voting,
and stacking (Dietterich, 1997), with the first way
suited only for forming a baseline. Voting can
be divided into two subclasses: unweighted votes,
and weighted votes. The weights of the votes, if
any, are usually calculated based on the classifiers?
performance on some initial dataset. Stacking, fi-
nally, is a way of combining the decisions made
by individual taggers in which the predicted tags
for a given word are used as input to a subsequent
tagger which outputs a final label for the word.
Committee-based approaches to POS tagging
have been in focus the last decade: Brill and Wu
(1998) combined four different taggers for English
using unweighted voting and by exploring contex-
tual cues (essentially a variant of stacking). Aires
et al (2000) experimented with 12 different ways
of combining the output from taggers for Brazilian
106
Portuguese, and concluded that some, but not all,
combinations yielded better accuracy than the best
individual tagger. Shacham and Wintner (2007)
contrasted what they refer to as being a na??ve way
of combining taggers with a more elaborate, hi-
erarchical one for Hebrew. In the end, the elabo-
rated method yielded results inferior to the na??ve
approach. De Pauw et al (2006) came to simi-
lar conclusions when using five different ways of
combining four data-driven taggers for Swahili.
The taggers were based on HMM, Memory-based
learning, SVM, and Maximum Entropy, with the
latter proving most accurate. Only in three of
five cases did a combination of classifiers perform
better than the Maximum Entropy-based tagger,
and simpler combination methods mostly outper-
formed more elaborate ones.
Spoustova? et al (2007) report on work on com-
bining a hand-written rule-based tagger with three
statistically induced taggers for Czech. As an ef-
fect of Czech being highly inflectional, the tagsets
are large: 1000?2000 unique tags. Thus the ap-
proach to combining taggers first aims at reducing
the number of plausible tags for a word by using
the rule-based tagger to discard impossible tags.
Precision is then increased by invoking one or all
of the data-driven taggers. Three different ways of
combining the taggers were explored: serial com-
bination, involving one of the statistical taggers;
so called SUBPOS pre-processing, involving two
instances of statistical taggers (possibly the same
tagger); and, parallel combination, in which an ar-
bitrary number of statistical taggers is used. The
combined tagger yielded the best results for Czech
POS tagging reported to date, and as a side-effect
also the best accuracy for English: 97.43%.3
4 The Taggers
This section describes the three taggers used in the
experiments (which are reported on in Section 6).
4.1 Hidden Markov Models: TnT
TnT, ?Trigrams?n?Tags? (Brants, 2000) is a very
fast and easy-to-use HMM-based tagger which
painlessly can be trained on different languages
and tagsets, given a tagged corpus.4 A Markov-
based tagger aims to find a tag sequence which
maximizes P (wordn|tagn) ? P (tagn|tag1...n?1),
where the first factor is the emit (or lexical) prob-
3As reported on ufal.mff.cuni.cz/compost/en
4www.coli.uni-saarland.de/?thorsten/tnt
ability, the likelihood of a word given certain tag,
and the second factor is the state transition (or con-
textual) probability, the likelihood of a tag given a
sequence of preceding tags. TnT uses the Viterbi
algorithm for finding the optimal tag sequence.
Smoothing is implemented by linear interpolation,
the respective weights are determined by deleted
interpolation. Unknown words are handled by a
suffix trie and successive abstraction.
Applying TnT to the Wall Street Journal cor-
pus, Brants (2000) reports 96.7% overall accuracy,
with 97.0% on known and 85.5% on unknown
words (with 2.9% of the words being unknown).
4.2 Support Vector Machines: SVMTool
Support Vector Machines (SVM) is a linear learn-
ing system which builds two class classifiers. It
is a supervised learning method whereby the in-
put data are represented as vectors in a high-
dimensional space and SVM finds a hyperplane (a
decision boundary) separating the input space into
two by maximizing the margin between positive
and negative data points.
SVMTool is an open source tagger based on
SVMs.5 Comparing the accuracy of SVMTool
with TnT on the Wall Street Journal corpus,
Gime?nez and Ma`rquez (2004) report a better per-
formance by SVMTool: 96.9%, with 97.2% on
known words and 83.5% on unknown.
4.3 Maximum Entropy: MALLET
Maximum Entropy is a linear classification
method. In its basic incarnation, linear classifi-
cation combines, by addition, the pre-determined
weights used for representing the importance of
each feature to a given class. Training a Maxi-
mum Entropy classifier involves fitting the weights
of each feature value for a particular class to the
available training data. A good fit of the weights
to the data is obtained by selecting weights to max-
imize the log-likelihood of the learned classifica-
tion model. Using an Maximum Entropy approach
to POS tagging, Ratnaparkhi (1996) reports a tag-
ging accuracy of 96.6% on the Wall Street Journal.
The software of choice for the experiments re-
ported here is MALLET (McCallum, 2002), a
freely available Java implementation of a range of
machine learning methods, such as Na??ve Bayes,
decision trees, CRF, and Maximum Entropy.6
5www.lsi.upc.edu/?nlp/SVMTool
6mallet.cs.umass.edu
107
5 The Dataset
The experiments of this paper utilize the first
medium-sized corpus for Amharic (available at
http://nlp.amharic.org). The corpus consists
of all 1065 news texts (210,000 words) from the
Ethiopian year 1994 (parts of the Gregorian years
2001?2002) from the Walta Information Center, a
private news service based in Addis Ababa. It has
been morphologically analysed and manually part-
of-speech tagged by staff at ELRC, the Ethiopian
Languages Research Center at Addis Ababa Uni-
versity (Demeke and Getachew, 2006).
The corpus is available both in fidel and tran-
scribed into a romanized version known as SERA,
System for Ethiopic Representation in ASCII (Ya-
cob, 1997). We worked with the transliterated
form (202,671 words), to be compatible with the
machine learning tools used in the experiments.
5.1 ?Cleaning? the corpus
Unfortunately, the corpus available on the net con-
tains quite a few errors and tagging inconsisten-
cies: nine persons participated in the manual tag-
ging, writing the tags with pen on hard copies,
which were given to typists for insertion into the
electronic version of the corpus?a procedure ob-
viously introducing several possible error sources.
Before running the experiments the corpus had
to be ?cleaned?: many non-tagged items have been
tagged (the human taggers have, e.g., often tagged
the headlines of the news texts as one item, end-
of-sentence punctuation), while some double tags
have been removed. Reflecting the segmentation
of the original Amharic text, all whitespaces were
removed, merging multiword units with a single
tag into one-word units. Items like ?"? and ?/?
have been treated consistently as punctuation, and
consistent tagging has been added to word-initial
and word-final hyphens. Also, some direct tagging
errors and misspellings have been corrected.
Time expressions and numbers have not been
consistently tagged at all, but those had to be left
as they were. Finally, many words have been tran-
scribed into SERA in several versions, with only
the cases differing. However, this is also difficult
to account for (and in the experiments below we
used the case sensitive version of SERA), since
the SERA notation in general lets upper and lower
cases of the English alphabet represent different
symbols in fidel (the Amharic script).
5.2 Tagsets
For the experiments, three different tagsets were
used. Firstly, the full, original 30-tag set devel-
oped at the Ethiopian Languages Research Center
and described by Demeke and Getachew (2006).
This version of the corpus will be referred to as
?ELRC?. It contains 200, 863 words and differs
from the published corpus in way of the correc-
tions described in the previous section.
Secondly, the corpus was mapped to 11 basic
tags. This set consists of ten word classes: Noun,
Pronoun, Verb, Adjective, Preposition, Conjunc-
tion, Adverb, Numeral, Interjection, and Punctua-
tion, plus one tag for problematic words (unclear:
<UNC>). The main differences between the two
tagsets pertain to the treatment of prepositions and
conjunctions: in ?ELRC? there are specific classes
for, e.g., pronouns attached with preposition, con-
junction, and both preposition and conjunction
(similar classes occur for nouns, verbs, adjectives,
and numerals). In addition, numerals are divided
into cardinals and ordinals, verbal nouns are sepa-
rated from other nouns, while auxiliaries and rela-
tive verbs are distinguished from other verbs. The
full tagset is made up of thirty subclasses of the
basic classes, based on type of word only: the tags
contain no information on grammatical categories
(such as number, gender, tense, and aspect).
Thirdly, for comparison reasons, the full tagset
was mapped to the 10 tags used by Fissaha (2005).
These classes include one for Residual (R) which
was assumed to be equivalent to <UNC>. In addi-
tion, <CONJ> and <PREP> were mapped to Ad-
position (AP), and both <N> and <PRON> to N.
The other mappings were straight-forward, except
that the ?BASIC? tagset groups all verbs together,
while Fissaha kept Auxiliary (AUX) as its own
class. This tagset will be referred to as ?SISAY?.
5.3 Folds
For evaluation of the taggers, the corpus was split
into 10 folds. These folds were created by chop-
ping the corpus into 100 pieces, each of about
2000 words in sequence, while making sure that
each piece contained full sentences (rather than
cutting off the text in the middle of a sentence),
and then merging sets of 10 pieces into a fold.
Thus the folds represent even splits over the cor-
pus, to avoid tagging inconsistencies, but the se-
quences are still large enough to potentially make
knowledge sources such as n-grams useful.
108
Fold TOTAL KNOWN UNKNOWN
fold00 20,027 17,720 2,307
fold01 20,123 17,750 2,373
fold02 20,054 17,645 2,409
fold03 20,169 17,805 2,364
fold04 20,051 17,524 2,527
fold05 20,058 17,882 2,176
fold06 20,111 17,707 2,404
fold07 20,112 17,746 2,366
fold08 20,015 17,765 2,250
fold09 20,143 17,727 2,416
Average 20,086 17,727 2,359
Percent ? 88.26 11.74
Table 2: Statistics for the 10 folds
Table 2 shows the data for each of the folds, in
terms of total number of tokens, as well as split
into known and unknown tokens, where the term
UNKNOWN refers to tokens that are not in any of
the other nine folds. The figures at the bottom
of the table show the average numbers of known
and unknown words, over all folds. Notably, the
average number of unknown words is about four
times higher than in the Wall Street Journal cor-
pus (which, however, is about six times larger).
6 Results
The results obtained by applying the three dif-
ferent tagging strategies to the three tagsets are
shown in Table 3, in terms of average accura-
cies after 10-fold cross validation, over all the
tokens (with standard deviation),7 as well as ac-
curacy divided between the known and unknown
words. Additionally, SVMTool and MALLET in-
clude support for automatically running 10-fold
cross validation on their own folds. Figures for
those runs are also given. The last line of the table
shows the baselines for the tagsets, given as the
number of tokens tagged as regular nouns divided
by the total number of words after correction.
6.1 TnT
As the bold face figures indicate, TnT achieves the
best scores of all three taggers, on all three tagsets,
on known words. However, it has problems with
the unknown words?and since these are so fre-
quent in the corpus, TnT overall performs worse
than the other taggers. The problems with the un-
known words increase as the number of possible
tags increase, and thus TnT does badly on the orig-
inal tagging scheme (?ELRC?), where it only gets
7The standard deviation is given by
?
1
n
?n
i=1(xi ? x)
2
where x is the arithmetic mean ( 1n
?n
i=1 xi).
ELRC BASIC SISAY
TnT 85.56 92.55 92.60
STD DEV 0.42 0.31 0.32
KNOWN 90.00 93.95 93.99
UNKNOWN 52.13 82.06 82.20
SVM 88.30 92.77 92.80
STD DEV 0.41 0.31 0.37
KNOWN 89.58 93.37 93.34
UNKNOWN 78.68 88.23 88.74
Own folds 88.69 92.97 92.99
STD DEV 0.33 0.17 0.26
MaxEnt 87.87 92.56 92.60
STD DEV 0.49 0.38 0.43
KNOWN 89.44 93.26 93.27
UNKNOWN 76.05 87.29 87.61
Own folds 90.83 94.64 94.52
STD DEV 1.37 1.11 0.69
BASELINE 35.50 58.26 59.61
Table 3: Tagging results
a bit over 50% on the unknown words (and 85.6%
overall). For the two reduced tagsets TnT does
better: overall performance goes up to a bit over
92%, with 82% on unknown words.
Table 3 shows the results on the default configu-
ration of TnT, i.e., using 3-grams and interpolated
smoothing. Changing these settings give no sub-
stantial improvement overall: what is gained at
one end (e.g., on unknown words or a particular
tagset) is lost at the other end (on known words or
other tagsets). However, per default TnT uses a
suffix trie of length 10 to handle unknown words.
Extending the suffix to 20 (the maximum value
in TnT) gave a slight performance increase on
?ELCR? (0.13% on unknown words, 0.01% over-
all), while having no effect on the smaller tagsets.
6.2 SVM
The SVM-tagger outperforms TnT on unknown
words, but is a bit worse on known words. Overall,
SVM is slightly better than TnT on the two smaller
tagsets and clearly better on the large tagset, and
somewhat better than MaxEnt on all three tagsets.
These results are based on SVMTool?s default
parameters: a one-pass, left-to-right, greedy tag-
ging scheme with a window size of 5. Previous
experiments with parameter tuning and multiple
pass tagging have indicated that there is room for
performance improvements by ? 2%.
6.3 Maximum Entropy
The MaxEnt tagger gets results comparable to the
other taggers on the predefined folds. Its overall
109
Wordn ; Tag of Wordn
Prefixes of Wordn, length 1-5 characters
Postfixes of Wordn, length 1-5 characters
Is Wordn capitalized?
Is Wordn all digits?
Does Wordn contain digits?
Does Wordn contain a hyphen?
Wordn?1 ; Tag of Wordn?1
Wordn?2 ; Tag of Wordn?2
Wordn+1
Wordn+2
Table 4: Features used in the MaxEnt tagger
performance is equivalent to TnT?s on the smaller
tagsets, but significantly better on ?ELRC?.
As can be seen in Table 3, the MaxEnt tag-
ger clearly outperforms the other taggers on all
tagsets, when MALLET is allowed to create its
own folds: all tagsets achieved classification ac-
curacies higher than 90%, with the two smaller
tagsets over 94.5%. The dramatic increase in the
tagger?s performance on these folds is surprising,
but a clear indication of one of the problems with
n-fold cross validation: even though the results
represent averages after n runs, the choice of the
original folds to suit a particular tagging strategy
is of utmost importance for the final result.
Table 4 shows the 22 features used to represent
an instance (Wordn) in the Maximum Entropy tag-
ger. The features are calculated per token within
sentences: the starting token of a sentence is not
affected by the characteristics of the tokens ending
the previous sentence, nor the other way around.
Thus not all features are calculated for all tokens.
6.4 Discussion
In terms of accuracy, the MaxEnt tagger is by
far the best of the three taggers, and on all three
tagsets, when allowed to select its own folds. Still,
as Table 3 shows, the variation of the results for
each individual fold was then substantially larger.
It should also be noted that TnT is by far the
fastest of the three taggers, in all respects: in terms
of time to set up and learn to use the tagger, in
terms of tagging speed, and in particular in terms
of training time. Training TnT is a matter of sec-
onds, but a matter of hours for MALLET/MaxEnt
and SVMTool. On the practical side, it is worth
adding that TnT is robust, well-documented, and
easy to use, while MALLET and SVMTool are
substantially more demanding in terms of user ef-
fort and also appear to be more sensitive to the
quality and format of the input data.
7 Conclusions and Future Work
The paper has described experiments with apply-
ing three state-of-the-art part-of-speech taggers to
Amharic, using three different tagsets. All tag-
gers showed worse performance than previously
reported results for English. The best accuracy
was obtained using a Maximum Entropy approach
when allowed to create its own folds: 90.1% on a
30 tag tagset, and 94.6 resp. 94.5% on two reduced
sets (11 resp. 10 tags), outperforming an HMM-
based (TnT) and an SVM-based (SVMTool) tag-
ger. On predefined folds all taggers got compa-
rable results (92.5-92.8% on the reduced sets and
4-7% lower on the full tagset). The SVM-tagger
performs slightly better than the others overall,
since it has the best performance on unknown
words, which are four times as frequent in the
200K words Amharic corpus used than in the (six
times larger) English Wall Street Journal corpus.
TnT gave the best results for known words, but
had the worst performance on unknown words.
In order to improve tagging accuracy, we will
investigate including explicit morphological pro-
cessing to treat unknown words, and combining
taggers. Judging from previous efforts on com-
bining taggers (Section 3.3), it is far from certain
that the combination of taggers actually ends up
producing better results than the best individual
tagger. A pre-requisite for successful combination
is that the taggers are sufficiently dissimilar; they
must draw on different characteristics of the train-
ing data and make different types of mistakes.
The taggers described in this paper use no other
knowledge source than a tagged training corpus.
In addition to incorporating (partial) morpholog-
ical processing, performance could be increased
by including knowledge sources such as machine
readable dictionaries or lists of Amharic stem
forms (Section 2.2). Conversely, semi-supervised
or unsupervised learning for tagging clearly are
interesting alternatives to manually annotate and
construct corpora for training taggers. Since
there are few computational resources available
for Amharic, approaches as those briefly outlined
in Section 3.2 deserve to be explored.
Acknowledgements
The work was partially funded by Sida, the Swedish Inter-
national Development Cooperation Agency through SPIDER
(the Swedish Programme for ICT in Developing Regions).
Thanks to Dr. Girma Demeke, Mesfin Getachew, and the
ELRC staff for their efforts on tagging the corpus, and to
Thorsten Brants for providing us with the TnT tagger.
110
References
Rachel V. Xavier Aires, Sandra M. Alu??sio, Denise C. S.
Kuhn, Marcio L. B. Andreeta, and Osvaldo N. Oliveira Jr.
2000. Combining classifiers to improve part of speech
tagging: A case study for Brazilian Portuguese. In 15th
Brazilian Symposium on AI, pp. 227?236, Atibaia, Brazil.
Nega Alemayehu and Peter Willett. 2002. Stemming of
Amharic words for information retrieval. Literary and
Linguistic Computing, 17:1?17.
Saba Amsalu and Dafydd Gibbon. 2005. Finite state mor-
phology of Amharic. In 5th Recent Advances in Natural
Language Processing, pp. 47?51, Borovets, Bulgaria.
Saba Amsalu and Girma A. Demeke. 2006. Non-
concatinative finite-state morphotactics of Amharic sim-
ple verbs. ELRC Working Papers, 2:304-325.
Atelach Alemu Argaw and Lars Asker. 2007. An Amharic
stemmer: Reducing words to their citation forms. Compu-
tational Approaches to Semitic Languages, pp. 104?110,
Prague, Czech Rep.
Michele Banko and Robert C. Moore. 2004. Part of speech
tagging in context. In 20th Int. Conf. on Computational
Linguistics, pp. 556?561, Geneva, Switzerland.
Roy Bar-Haim, Khalil Simaan, and Yoad Winter. 2008. Part-
of-speech tagging of modern Hebrew text. Natural Lan-
guage Engineering, 14:223?251.
Abiyot Bayou. 2000. Design and development of word
parser for Amharic language. MSc Thesis, Addis Ababa
University, Ethiopia.
Tesfaye Bayu. 2002. Automatic morphological analyser:
An experiment using unsupervised and autosegmental ap-
proach. MSc Thesis, Addis Ababa University, Ethiopia.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In 6th Conf. Applied Natural Language Process-
ing, pp. 224?231, Seattle, Wash.
Eric Brill and Jun Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In 17th Int. Conf. on Com-
putational Linguistics, pp. 191?195, Montreal, Canada.
Eric Brill. 1995. Transformation-based error-driven learning
and Natural Language Processing: A case study in part of
speech tagging. Computational Linguistics, 21:543?565.
CIA. 2009. The World Factbook ? Ethiopia. The Central In-
telligence Agency, Washington, DC. [Updated 22/01/09.]
Michael Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with per-
ceptron algorithms. In Empirical Methods in Natural Lan-
guage Processing, pp. 1?8, Philadelphia, Penn.
Girma A. Demeke and Mesfin Getachew. 2006. Manual an-
notation of Amharic news items with part-of-speech tags
and its challenges. ELRC Working Papers, 2:1?17.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004. Au-
tomatic tagging of Arabic text: From raw text to base
phrase chunks. In HLT Conf. North American ACL,
pp. 149?152, Boston, Mass.
Thomas G. Dietterich. 1997. Machine-learning research:
Four current directions. AI magazine, 18:97?136.
Kevin Duh and Katrin Kirchhoff. 2005. POS tagging of di-
alectal Arabic: A minimally supervised approach. Com-
putational Approaches to Semitic Languages, pp. 55?62,
Ann Arbor, Mich.
Sisay Fissaha and Johann Haller. 2003. Amharic verb
lexicon in the context of machine translation. In 10th
Traitement Automatique des Langues Naturelles, vol. 2,
pp. 183?192, Batz-sur-Mer, France.
Sisay Fissaha. 2005. Part of speech tagging for Amharic us-
ing conditional random fields. Computational Approaches
to Semitic Languages, pp. 47?54, Ann Arbor, Mich.
Mesfin Getachew. 2001. Automatic part of speech tag-
ging for Amharic: An experiment using stochastic hid-
den Markov model (HMM) approach. MSc Thesis, Addis
Ababa University, Ethiopia.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector ma-
chines. In 4th Int. Conf. Language Resources and Eval-
uation, pp. 168?176, Lisbon, Portugal.
Sharon Goldwater and Thomas L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In 45th ACL, pp. 744?751, Prague, Czech Rep.
Grover Hudson. 1999. Linguistic analysis of the 1994
Ethiopian census. Northeast African Studies, 6:89?107.
Saib Mansour. 2008. Combining character and morpheme
based models for part-of-speech tagging of Semitic lan-
guages. MSc Thesis, Technion, Haifa, Israel.
Andrew Kachites McCallum. 2002. MALLET: A machine
learning for language toolkit. Webpage.
Guy De Pauw, Gilles-Maurice de Schryver, and Peter W.
Wagacha. 2006. Data-driven part-of-speech tagging of
Kiswahili. In 9th Int. Conf. Text, Speech and Dialogue,
pp. 197?204, Brno, Czech Rep.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Empirical Methods in Natural
Language Processing, pp. 133?142, Philadelphia, Penn.
Danny Shacham and Shuly Wintner. 2007. Morphological
disambiguation of Hebrew: A case study in classifier com-
bination. In Empirical Methods in Natural Language Pro-
cessing, pp. 439?447, Prague, Czech Rep.
Drahomira? Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Krbec,
and Pavel Kve?ton?. 2007. The best of two worlds: Co-
operation of statistical and rule-based taggers for Czech.
Balto-Slavonic Natural Language Processing, pp. 67?74.
Prague, Czech Rep.
Kristina Toutanova and Mark Johnson. 2007. A Bayesian
LDA-based model for semi-supervised part-of-speech tag-
ging. In 21st Int. Conf. Advances in Neural Information
Processing Systems, pp. 1521?1528, Vancover, B.C.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In HLT Conf. North
American ACL, pp. 173?180, Edmonton, Alberta.
Atro Voutilainen. 1999. An experiment on the upper bound
of interjudge agreement: The case of tagging. In 9th Eu-
ropean ACL, pp. 204?208, Bergen, Norway.
Daniel Yacob. 1997. The System for Ethiopic Representa-
tion in ASCII ? 1997 standard. Webpage.
111
Proceedings of the EACL 2009 Demonstrations Session, pages 65?68,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
A Mobile Health and Fitness Companion Demonstrator?
Olov Sta?hl1 Bjo?rn Gamba?ck1,2 Markku Turunen3 Jaakko Hakulinen3
1ICE / Userware 2Dpt. Computer & Information Science 3Dpt. Computer Sciences
Swedish Inst. of Computer Science Norwegian Univ. of Science and Technology Univ. of Tampere
Kista, Sweden Trondheim, Norway Tampere, Finland
{olovs,gamback}@sics.se gamback@idi.ntnu.no {mturunen,jh}@cs.uta.fi
Abstract
Multimodal conversational spoken dia-
logues using physical and virtual agents
provide a potential interface to motivate
and support users in the domain of health
and fitness. The paper presents a multi-
modal conversational Companion system
focused on health and fitness, which has
both a stationary and a mobile component.
1 Introduction
Spoken dialogue systems have traditionally fo-
cused on task-oriented dialogues, such as mak-
ing flight bookings or providing public transport
timetables. In emerging areas, such as domain-
oriented dialogues (Dybkjaer et al, 2004), the in-
teraction with the system, typically modelled as a
conversation with a virtual anthropomorphic char-
acter, can be the main motivation for the interac-
tion. Recent research has coined the term ?Com-
panions? to describe embodied multimodal con-
versational agents having a long lasting interaction
history with their users (Wilks, 2007).
Such a conversational Companion within the
Health and Fitness (H&F) domain helps its users
to a healthier lifestyle. An H&F Companion has
quite different motivations for use than traditional
task-based spoken dialogue systems. Instead of
helping with a single, well-defined task, it truly
aims to be a Companion to the user, providing
social support in everyday activities. The system
should thus be a peer rather than act as an expert
system in health-related issues. It is important to
stress that it is the Companion concept which is
central, rather than the fitness area as such. Thus
it is not of vital importance that the system should
be a first-rate fitness coach, but it is essential that it
?The work was funded by the European Commis-
sion?s IST priority through the project COMPANIONS
(www.companions-project.org).
Figure 1: H&F Companion Architecture
should be able to take a persistent part in the user?s
life, that is, that it should be able to follow the user
in all the user?s activities. This means that the
Companion must have mobile capabilities. Not
necessarily self-mobile (as a robot), but allowing
the user to bring the system with her, like a hand-
bag or a pair of shoes ? or as a mobile phone.
The paper describes such a Health and Fitness
Companion. It has a stationary (?home?) compo-
nent accounting for the main part of the user in-
teraction and a mobile component which follows
the users in actual exercise activities. Section 2
outlines the overall system and its two basic com-
ponents, and Section 3 details the implementation.
Section 4 discusses some related work, while Sec-
tion 5 describes the demonstrator set-up and plans
for future work.
2 The Health and Fitness Companion
The overall system architecture of the Health and
Fitness Companion is shown in Figure 1. The
system components communicate with each other
over a regular mobile phone network. The home
system provides an exercise plan to the mobile part
and in return gets the results of the performed ex-
ercises from the mobile component.
65
Figure 2: Home Companion interface
2.1 The Home H&F Companion
The home part of the H&F Companion gathers in-
formation from the user and makes suggestions
targeted for a healthy lifestyle on a daily basis.
The Companion communicates with the user in
two main dialogue phases: a planning phase where
the system talks about the coming day with the
user and a reporting phase where the user?s actual
activities are assessed with reference to what was
agreed on earlier. The Companion can discuss the
following topics: travelling to work, getting lunch,
activities to be performed before dinner, getting
dinner, and activities to be performed after din-
ner. It knows activities such as playing football,
squash, or badminton; going to the gym or shop-
ping; and watching television or reading a book.
The Home Companion interface, illustrated
in Figure 2, uses a Nabaztag/tag WLAN rabbit
(www.nabaztag.com) as the physical agent in-
terface. The Nabaztag rabbit provides audio out-
put and push-to-talk speech input, moves its ears,
and operates four coloured lights to signal its sta-
tus. In the current implementation, the user needs
to push the button located on top of the Nabaz-
tag/tag device in order to speak to it. As an alter-
native, the system supports external micro-phones
and loudspeakers. The user is also able to pick a
topic by showing RFID tags (embedded in picture
cards or other objects).
The following dialogue example demonstrates
the interaction with the Companion.
C1: Do you have anything arranged for today?
U1: I?m playing Squash
C2: Is that before dinner?
U2: Yes
C3: Okay. Are you going to get the train to work?
U3: Yes
C4: Right. Could you take in a packed lunch to work?
U4: Yes, that would be fine
Figure 3: The Mobile Companion GUI
(Later on in the evening)
C6: Welcome back! How did your game of Squash go?
U5: The game was cancelled, so I went to the gym
C7: Okay. Did you take the train to work?
U6: Yes, I did
C8: Right. Did you take in a packed lunch?
U7: No. I was in a rush so I went to the cafeteria
2.2 The Mobile H&F Companion
The mobile part of the H&F Companion runs on a
mobile handset (e.g., a PDA), and is used during
physical exercise (e.g., while running or walking)
to track the distance, pace, duration, and calories
burned. The data gathered during an exercise is
stored in the device?s record store, and can be used
to compare the results to previous runs.
The user interface of the Mobile Companion
consists of a single screen showing an image of a
Nabaztag rabbit along with some text areas where
various exercise and device status information is
displayed (Figure 3). The rabbit image is intended
to give users a sense of communicating with the
same Companion, no matter if they are using the
home or mobile system. To further the feeling of
persistence, the home and mobile parts of the H&F
Companion also use the same TTS voice.
When the mobile Companion is started, it asks
the user whether it should connect to the home sys-
tem and download the current plan. Such a plan
consists of various tasks (e.g., shopping or exer-
cise tasks) that the user should try to achieve dur-
ing the day, and is generated by the home system
during a session with the user. If the user chooses
to download the plan the Companion summarizes
the content of the plan for the user, excluding all
tasks that do not involve some kind of exercise ac-
tivity. The Companion then suggests a suitable
task based on time of day and the user?s current
location. If the user chooses not to download the
plan, or rejects the suggested exercise(s), the Com-
panion instead asks the user to suggest an exercise.
66
Once an exercise has been agreed upon, the
Companion asks the user to start the exercise and
will then track the progress (distances travelled,
time, pace and calories burned) using a built-in
GPS receiver. While exercising, the user can ask
the Companion to play music or to give reports on
how the user is doing. After the exercise, the Com-
panion will summarize the result and up-load it to
the Home system so it can be referred to later on.
3 H&F Companion Implementation
This section details the actual implementation of
the Health and Fitness Companion, in terms of its
two components (the home and mobile parts).
3.1 Home Companion Implementation
The Home Companion is implemented on top
of Jaspis, a generic agent-based architecture de-
signed for adaptive spoken dialogue systems (Tu-
runen et al, 2005). The base architecture
is extended to support interaction with virtual
and physical Companions, in particular with the
Nabaztag/tag device.
For speech inputs and outputs, the Home Com-
panion uses LoquendoTMASR and TTS compo-
nents. ASR grammars are in ?Speech Recogni-
tion Grammar Specification? (W3C) format and
include semantic tags in ?Semantic Interpreta-
tion for Speech Recognition (SISR) Version 1.0?
(W3C) format. Domain specific grammars were
derived from a WoZ corpus. The grammars are
dynamically selected according to the current di-
alogue state. Grammars can be precompiled for
efficiency or compiled at run-time when dynamic
grammar generation takes place in certain situa-
tions. The current system vocabulary consists of
about 1400 words and a total of 900 CFG grammar
rules in 60 grammars. Statistical language models
for the system are presently being implemented.
Language understanding relies heavily on SISR
information: given the current dialogue state, the
input is parsed into a logical notation compati-
ble with the planning implemented in a Cognitive
Model. Additionally, a reduced set of DAMSL
(Core and Allen, 1997) tags is used to mark func-
tional dialogue acts using rule-based reasoning.
Language generation is implemented as a com-
bination of canned utterances and tree adjoining
grammar-based structures. The starting point for
generation is predicate-form descriptions provided
by the dialogue manager. Further details and
contextual information are retrieved from the di-
alogue history and the user model. Finally, SSML
(Speech Synthesis Markup Language) 1.0 tags are
used for controlling the Loquendo synthesizer.
Dialogue management is based on close-
cooperation of the Dialogue Manager and the Cog-
nitive Manager. The Cognitive Manager models
the domain, i.e., knows what to recommend to the
user, what to ask from the user, and what kind
of feedback to provide on domain level issues.
In contrast, the Dialogue Manager focuses on in-
teraction level phenomena, such as confirmations,
turn taking, and initiative management.
The physical agent interface is implemented
in jNabServer software to handle communication
with Nabaztag/tags, that is, Wi-Fi enabled robotic
rabbits. A Nabaztag/tag device can handle vari-
ous forms of interaction, from voice to touch (but-
ton press), and from RFID ?sniffing? to ear move-
ments. It can respond by moving its ears, or by
displaying or changing the colour of its four LED
lights. The rabbit can also play sounds such as
music, synthesized speech, and other audio.
3.2 Mobile Companion Implementation
The Mobile Companion runs on Windows Mobile-
based devices, such as the Fujitsu Siemens Pocket
LOOX T830. The system is made up of two pro-
grams, both running on the mobile device: a Java
midlet controls the main application logic (exer-
cise tracking, dialogue management, etc.) as well
as the graphical user interface; and a C++-based
speech server that performs TTS and ASR func-
tions on request by the Java midlet, such as load-
ing grammar files or voices.
The midlet is made up of Java manager classes
that provide basic services (event dispatching,
GPS input, audio play-back, TTS and ASR, etc.).
However, the main application logic and the GUI
are implemented using scripts in the Hecl script-
ing language (www.hecl.org). The script files
are read from the device?s file system and evalu-
ated in a script interpreter created by the midlet
when started. The scripts have access to a num-
ber of commands, allowing them to initiate TTS
and ASR operations, etc. Furthermore, events
produced by the Java code are dispatched to the
scripts, such as the user?s current GPS position,
GUI interactions (e.g., stylus interaction and but-
ton presses), and voice input. Scripts are also used
to control the dialogue with the user.
67
The speech server is based on the Loquendo
Embedded ASR (speaker-independent) and TTS
software.1 The Mobile Companion uses SRGS 1.0
grammars that are pre-compiled before being in-
stalled on the mobile device. The current system
vocabulary consists of about 100 words in 10 dy-
namically selected grammars.
4 Related Work
As pointed out in the introduction, it is not the aim
of the Health and Fitness Companion system to be
a full-fledged fitness coach. There are several ex-
amples of commercial systems that aim to do that,
e.g., miCoach (www.micoach.com) from Adi-
das and NIKE+ (www.nike.com/nikeplus).
MOPET (Buttussi and Chittaro, 2008) is a
PDA-based personal trainer system supporting
outdoor fitness activities. MOPET is similar to
a Companion in that it tries to build a relation-
ship with the user, but there is no real dialogue
between the user and the system and it does not
support speech input or output. Neither does
MPTrain/TripleBeat (Oliver and Flores-Mangas,
2006; de Oliveira and Oliver, 2008), a system that
runs on a mobile phone and aims to help users
to more easily achieve their exercise goals. This
is done by selecting music indicating the desired
pace and different ways to enhance user motiva-
tion, but without an agent user interface model.
InCA (Kadous and Sammut, 2004) is a spoken
language-based distributed personal assistant con-
versational character with a 3D avatar and facial
animation. Similar to the Mobile Companion, the
architecture is made up of a GUI client running on
a PDA and a speech server, but the InCA server
runs as a back-end system, while the Companion
utilizes a stand-alone speech server.
5 Demonstration and Future Work
The demonstration will consist of two sequential
interactions with the H&F Companion. First, the
user and the home system will agree on a plan,
consisting of various tasks that the user should try
to achieve during the day. Then the mobile system
will download the plan, and the user will have a
dialogue with the Companion, concerning the se-
lection of a suitable exercise activity, which the
user will pretend to carry out.
1As described in ?Loquendo embedded technologies:
Text to speech and automatic speech recognition.?
www.loquendo.com/en/brochure/Embedded.pdf
Plans for future work include extending the mo-
bile platform with various sensors, for example, a
pulse sensor that gives the Companion informa-
tion about the user?s pulse while exercising, which
can be used to provide feedback such as telling
the user to speed up or slow down. We are also in-
terested in using sensors to allow users to provide
gesture-like input, in addition to the voice and but-
ton/screen click input available today.
Another modification we are considering is to
unify the two dialogue management solutions cur-
rently used by the home and the mobile compo-
nents into one. This would cause the Companion
to ?behave? more consistently in its two shapes,
and make future extensions of the dialogue and the
Companion behaviour easier to manage.
References
Fabio Buttussi and Luca Chittaro. 2008. MOPET:
A context-aware and user-adaptive wearable sys-
tem for fitness training. Artificial Intelligence in
Medicine, 42(2):153?163.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In AAAI
Fall Symposium on Communicative Action in Hu-
mans and Machines, pages 28?35, Cambridge, Mas-
sachusetts.
Laila Dybkjaer, Niels Ole Bernsen, and Wolfgang
Minker. 2004. Evaluation and usability of multi-
modal spoken language dialogue systems. Speech
Communication, 43(1-2):33?54.
Mohammed Waleed Kadous and Claude Sammut.
2004. InCa: A mobile conversational agent. In Pro-
ceedings of the 8th Pacific Rim International Con-
ference on Artificial Intelligence, pages 644?653,
Auckland, New Zealand.
Rodrigo de Oliveira and Nuria Oliver. 2008. Triple-
Beat: Enhancing exercise performance with persua-
sion. In Proceedings of 10th International Con-
ference, on Mobile Human-Computer Interaction,
pages 255?264, Amsterdam, the Netherlands. ACM.
Nuria Oliver and Fernando Flores-Mangas. 2006.
MPTrain: A mobile, music and physiology-based
personal trainer. In Proceedings of 8th International
Conference, on Mobile Human-Computer Interac-
tion, pages 21?28, Espoo, Finland. ACM.
Markku Turunen, Jaakko Hakulinen, Kari-Jouko
Ra?iha?, Esa-Pekka Salonen, Anssi Kainulainen, and
Perttu Prusi. 2005. An architecture and applica-
tions for speech-based accessibility systems. IBM
Systems Journal, 44(3):485?504.
Yorick Wilks. 2007. Is there progress on talking sensi-
bly to machines? Science, 318(9):927?928.
68
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 66?73, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
NTNU-CORE: Combining strong features for semantic similarity
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bjo?rn Gamba?ck, Andre? Lynum
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{emarsi,hansmoe,bungum,sizov,gamback,andrely}@idi.ntnu.no
Abstract
The paper outlines the work carried out at
NTNU as part of the *SEM?13 shared task
on Semantic Textual Similarity, using an ap-
proach which combines shallow textual, dis-
tributional and knowledge-based features by
a support vector regression model. Feature
sets include (1) aggregated similarity based
on named entity recognition with WordNet
and Levenshtein distance through the calcula-
tion of maximum weighted bipartite graphs;
(2) higher order word co-occurrence simi-
larity using a novel method called ?Multi-
sense Random Indexing?; (3) deeper seman-
tic relations based on the RelEx semantic
dependency relationship extraction system;
(4) graph edit-distance on dependency trees;
(5) reused features of the TakeLab and DKPro
systems from the STS?12 shared task. The
NTNU systems obtained 9th place overall (5th
best team) and 1st place on the SMT data set.
1 Introduction
Intuitively, two texts are semantically similar if they
roughly mean the same thing. The task of formally
establishing semantic textual similarity clearly is
more complex. For a start, it implies that we have
a way to formally represent the intended meaning of
all texts in all possible contexts, and furthermore a
way to measure the degree of equivalence between
two such representations. This goes far beyond the
state-of-the-art for arbitrary sentence pairs, and sev-
eral restrictions must be imposed. The Semantic
Textual Similarity (STS) task (Agirre et al, 2012,
2013) limits the comparison to isolated sentences
only (rather than complete texts), and defines sim-
ilarity of a pair of sentences as the one assigned by
human judges on a 0?5 scale (with 0 implying no
relation and 5 complete semantic equivalence). It is
unclear, however, to what extent two judges would
agree on the level of similarity between sentences;
Agirre et al (2012) report figures on the agreement
between the authors themselves of about 87?89%.
As in most language processing tasks, there are
two overall ways to measure sentence similarity, ei-
ther by data-driven (distributional) methods or by
knowledge-driven methods; in the STS?12 task the
two approaches were used nearly equally much.
Distributional models normally measure similarity
in terms of word or word co-occurrence statistics, or
through concept relations extracted from a corpus.
The basic strategy taken by NTNU in the STS?13
task was to use something of a ?feature carpet bomb-
ing approach? in the way of first automatically ex-
tracting as many potentially useful features as possi-
ble, using both knowledge and data-driven methods,
and then evaluating feature combinations on the data
sets provided by the organisers of the shared task.
To this end, four different types of features were
extracted. The first (Section 2) aggregates similar-
ity based on named entity recognition with WordNet
and Levenshtein distance by calculating maximum
weighted bipartite graphs. The second set of features
(Section 3) models higher order co-occurrence sim-
ilarity relations using Random Indexing (Kanerva
et al, 2000), both in the form of a (standard) sliding
window approach and through a novel method called
?Multi-sense Random Indexing? which aims to sep-
arate the representation of different senses of a term
66
from each other. The third feature set (Section 4)
aims to capture deeper semantic relations using ei-
ther the output of the RelEx semantic dependency
relationship extraction system (Fundel et al, 2007)
or an in-house graph edit-distance matching system.
The final set (Section 5) is a straight-forward gath-
ering of features from the systems that fared best in
STS?12: TakeLab from University of Zagreb (S?aric?
et al, 2012) and DKPro from Darmstadt?s Ubiqui-
tous Knowledge Processing Lab (Ba?r et al, 2012).
As described in Section 6, Support Vector Regres-
sion (Vapnik et al, 1997) was used for solving the
multi-dimensional regression problem of combining
all the extracted feature values. Three different sys-
tems were created based on feature performance on
the supplied development data. Section 7 discusses
scores on the STS?12 and STS?13 test data.
2 Compositional Word Matching
Compositional word matching similarity is based
on a one-to-one alignment of words from the two
sentences. The alignment is obtained by maximal
weighted bipartite matching using several word sim-
ilarity measures. In addition, we utilise named entity
recognition and matching tools. In general, the ap-
proach is similar to the one described by Karnick
et al (2012), with a different set of tools used. Our
implementation relies on the ANNIE components in
GATE (Cunningham et al, 2002) and will thus be
referred to as GateWordMatch.
The processing pipeline for GateWordMatch
is: (1) tokenization by ANNIE English Tokeniser,
(2) part-of-speech tagging by ANNIE POS Tagger,
(3) lemmatization by GATE Morphological Anal-
yser, (4) stopword removal, (5) named entity recog-
nition based on lists by ANNIE Gazetteer, (6) named
entity recognition based on the JAPE grammar by
the ANNIE NE Transducer, (7) matching of named
entities by ANNIE Ortho Matcher, (8) computing
WordNet and Levenstein similarity between words,
(9) calculation of a maximum weighted bipartite
graph matching based on similarities from 7 and 8.
Steps 1?4 are standard preprocessing routines.
In step 5, named entities are recognised based on
lists that contain locations, organisations, compa-
nies, newspapers, and person names, as well as date,
time and currency units. In step 6, JAPE grammar
rules are applied to recognise entities such as ad-
dresses, emails, dates, job titles, and person names
based on basic syntactic and morphological features.
Matching of named entities in step 7 is based on
matching rules that check the type of named entity,
and lists with aliases to identify entities as ?US?,
?United State?, and ?USA? as the same entity.
In step 8, similarity is computed for each pair
of words from the two sentences. Words that are
matched as entities in step 7 get a similarity value
of 1.0. For the rest of the entities and non-entity
words we use LCH (Leacock and Chodorow, 1998)
similarity, which is based on a shortest path between
the corresponding senses in WordNet. Since word
sense disambiguation is not used, we take the simi-
larity between the nearest senses of two words. For
cases when the WordNet-based similarity cannot be
obtained, a similarity based on the Levenshtein dis-
tance (Levenshtein, 1966) is used instead. It is nor-
malised by the length of the longest word in the pair.
For the STS?13 test data set, named entity matching
contributed to 4% of all matched word pairs; LCH
similarity to 61%, and Levenshtein distance to 35%.
In step 9, maximum weighted bipartite matching
is computed using the Hungarian Algorithm (Kuhn,
1955). Nodes in the bipartite graph represent words
from the sentences, and edges have weights that cor-
respond to similarities between tokens obtained in
step 8. Weighted bipartite matching finds the one-to-
one alignment that maximizes the sum of similarities
between aligned tokens. Total similarity normalised
by the number of words in both sentences is used as
the final sentence similarity measure.
3 Distributional Similarity
Our distributional similarity features use Random
Indexing (RI; Kanerva et al, 2000; Sahlgren, 2005),
also employed in STS?12 by Tovar et al (2012);
Sokolov (2012); Semeraro et al (2012). It is an
efficient method for modelling higher order co-
occurrence similarities among terms, comparable to
Latent Semantic Analysis (LSA; Deerwester et al,
1990). It incrementally builds a term co-occurrence
matrix of reduced dimensionality through the use of
a sliding window and fixed size index vectors used
for training context vectors, one per unique term.
A novel variant, which we have called ?Multi-
67
sense Random Indexing? (MSRI), inspired by
Reisinger and Mooney (2010), attempts to capture
one or more ?senses? per unique term in an unsu-
pervised manner, each sense represented as an indi-
vidual vector in the model. The method is similar to
classical sliding window RI, but each term can have
multiple context vectors (referred to as ?sense vec-
tors? here) which are updated individually. When
updating a term vector, instead of directly adding the
index vectors of the neighbouring terms in the win-
dow to its context vector, the system first computes a
separate window vector consisting of the sum of the
index vectors. Then cosine similarity is calculated
between the window vector and each of the term?s
sense vectors. Each similarity score is in turn com-
pared to a set similarity threshold: if no score ex-
ceeds the threshold, the sentence vector is added as
a new separate sense vector for the term; if exactly
one score is above the threshold, the window vector
is added to that sense vector; and if multiple scores
are above the threshold, all the involved senses are
merged into one sense vector, together with the win-
dow vector. This accomplishes an incremental clus-
tering of senses in an unsupervised manner while re-
taining the efficiency of classical RI.
As data for training the models we used the
CLEF 2004?2008 English corpus (approx. 130M
words). Our implementation of RI and MSRI is
based on JavaSDM (Hassel, 2004). For classical
RI, we used stopword removal (using a customised
versions of the English stoplist from the Lucene
project), window size of 4+4, dimensionality set to
1800, 4 non-zeros, and unweighted index vector in
the sliding window. For MSRI, we used a simi-
larity threshold of 0.2, a vector dimensionality of
800, a non-zero count of 4, and window size of
5+5. The index vectors in the sliding window were
shifted to create direction vectors (Sahlgren et al,
2008), and weighted by distance to the target term.
Rare senses with a frequency below 10 were ex-
cluded. Other sliding-window schemes, including
unweighted non-shifted vectors and Random Permu-
tation (Sahlgren et al, 2008), were tested, but none
outperformed the sliding-window schemes used.
Similarity between sentence pairs was calcu-
lated as the normalised maximal bipartite similar-
ity between term pairs in each sentence, resulting
in the following features: (1) MSRI-Centroid:
each term is represented as the sum of its sense
vectors; (2) MSRI-MaxSense: for each term
pair, the sense-pair with max similarity is used;
(3) MSRI-Context: for each term, its neigh-
bouring terms within a window of 2+2 is used as
context for picking a single, max similar, sense
from the target term to be used as its represen-
tation; (4) MSRI-HASenses: similarity between
two terms is computed by applying the Hungarian
Algorithm to all their possible sense pair mappings;
(5) RI-Avg: classical RI, each term is represented
as a single context vector; (6) RI-Hungarian:
similarity between two sentences is calculated us-
ing the Hungarian Algorithm. Alternatively, sen-
tence level similarity was computed as the cosine
similarity between sentence vectors composed of
their terms? vectors. The corresponding features
are (1) RI-SentVectors-Norm: sentence vec-
tors are created by summing their constituent terms
(i.e., context vectors), which have first been normal-
ized; (2) RI-SentVectors-TFIDF: same as be-
fore, but TF*IDF weights are added.
4 Deeper Semantic Relations
Two deep strategies were employed to accompany
the shallow-processed feature sets. Two existing
systems were used to provide the basis for these fea-
tures, namely the RelEx system (Fundel et al, 2007)
from the OpenCog initiative (Hart and Goertzel,
2008), and an in-house graph-edit distance system
developed for plagiarism detection (R?kenes, 2013).
RelEx outputs syntactic trees, dependency graphs,
and semantic frames as this one for the sentence
?Indian air force to buy 126 Rafale fighter jets?:
Commerce buy:Goods(buy,jet)
Entity:Entity(jet,jet)
Entity:Name(jet,Rafale)
Entity:Name(jet,fighter)
Possibilities:Event(hyp,buy)
Request:Addressee(air,you)
Request:Message(air,air)
Transitive action:Beneficiary(buy,jet)
Three features were extracted from this: first, if
there was an exact match of the frame found in s1
with s2; second, if there was a partial match until the
first argument (Commerce buy:Goods(buy);
and third if there was a match of the frame category
68
(Commerce buy:Goods).
In STS?12, Singh et al (2012) matched Universal
Networking Language (UNL) graphs against each
other by counting matches of relations and univer-
sal words, while Bhagwani et al (2012) calculated
WordNet-based word-level similarities and created
a weighted bipartite graph (see Section 2). The
method employed here instead looked at the graph
edit distance between dependency graphs obtained
with the Maltparser dependency parser (Nivre et al,
2006). Edit distance is the defined as the minimum
of the sum of the costs of the edit operations (in-
sertion, deletion and substitution of nodes) required
to transform one graph into the other. It is approx-
imated with a fast but suboptimal algorithm based
on bipartite graph matching through the Hungarian
algorithm (Riesen and Bunke, 2009).
5 Reused Features
The TakeLab ?simple? system (S?aric? et al, 2012) ob-
tained 3rd place in overall Pearson correlation and
1st for normalized Pearson in STS?12. The source
code1 was used to generate all its features, that is,
n-gram overlap, WordNet-augmented word overlap,
vector space sentence similarity, normalized differ-
ence, shallow NE similarity, numbers overlap, and
stock index features.2 This required the full LSA
vector space models, which were kindly provided
by the TakeLab team. The word counts required for
computing Information Content were obtained from
Google Books Ngrams.3
The DKPro system (Ba?r et al, 2012) obtained first
place in STS?12 with the second run. We used the
source code4 to generate features for the STS?12
and STS?13 data. Of the string-similarity features,
we reused the Longest Common Substring, Longest
Common Subsequence (with and without normaliza-
tion), and Greedy String Tiling measures. From the
character/word n-grams features, we used Charac-
ter n-grams (n = 2, 3, 4), Word n-grams by Con-
tainment w/o Stopwords (n = 1, 2), Word n-grams
1http://takelab.fer.hr/sts/
2We did not use content n-gram overlap or skip n-grams.
3http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html, version 20120701,
with 468,491,999,592 words
4http://code.google.com/p/
dkpro-similarity-asl/
by Jaccard (n = 1, 3, 4), and Word n-grams by Jac-
card w/o Stopwords (n = 2, 4). Semantic similarity
measures include WordNet Similarity based on the
Resnik measure (two variants) and Explicit Seman-
tic Similarity based on WordNet, Wikipedia or Wik-
tionary. This means that we reused all features from
DKPro run 1 except for Distributional Thesaurus.
6 Systems
Our systems follow previous submissions to the STS
task (e.g., S?aric? et al, 2012; Banea et al, 2012) in
that feature values are extracted for each sentence
pair and combined with a gold standard score in or-
der to train a Support Vector Regressor on the result-
ing regression task. A postprocessing step guaran-
tees that all scores are in the [0, 5] range and equal 5
if the two sentences are identical. SVR has been
shown to be a powerful technique for predictive data
analysis when the primary goal is to approximate a
function, since the learning algorithm is applicable
to continuous classes. Hence support vector regres-
sion differs from support vector machine classifica-
tion where the goal rather is to take a binary deci-
sion. The key idea in SVR is to use a cost function
for building the model which tries to ignore noise in
training data (i.e., data which is too close to the pre-
diction), so that the produced model in essence only
depends on a more robust subset of the extracted fea-
tures.
Three systems were created using the supplied
annotated data based on Microsoft Research Para-
phrase and Video description corpora (MSRpar and
MSvid), statistical machine translation system out-
put (SMTeuroparl and SMTnews), and sense map-
pings between OntoNotes and WordNet (OnWN).
The first system (NTNU1) includes all TakeLab and
DKPro features plus the GateWordMatch feature
with the SVR in its default setting.5 The training
material consisted of all annotated data available,
except for the SMT test set, where it was limited to
SMTeuroparl and SMTnews. The NTNU2 system is
similar to NTNU1, except that the training material
for OnWN and FNWN excluded MSRvid and that
the SVR parameter C was set to 200. NTNU3 is
similar to NTNU1 except that all features available
are included.
5RBF kernel,  = 0.1, C = #samples, ? = 1#features
69
Data NTNU1 NTNU2 NTNU3
MSRpar 0.7262 0.7507 0.7221
MSRvid 0.8660 0.8882 0.8662
SMTeuroparl 0.5843 0.3386 0.5503
SMTnews 0.5840 0.5592 0.5306
OnWN 0.7503 0.6365 0.7200
mean 0.7022 0.6346 0.6779
Table 1: Correlation score on 2012 test data
7 Results
System performance is evaluated using the Pearson
product-moment correlation coefficient (r) between
the system scores and the human scores. Results on
the 2012 test data (i.e., 2013 development data) are
listed in Table 1. This basically shows that except
for the GateWordMatch, adding our other fea-
tures tends to give slightly lower scores (cf. NTNU1
vs NTNU3). In addition, the table illustrates that op-
timizing the SVR according to cross-validated grid
search on 2012 training data (here C = 200), rarely
pays off when testing on unseen data (cf. NTNU1
vs NTNU2).
Table 2 shows the official results on the test data.
These are generally in agreement with the scores on
the development data, although substantially lower.
Our systems did particularly well on SMT, holding
first and second position, reasonably good on head-
lines, but not so well on the ontology alignment data,
resulting in overall 9th (NTNU1) and 12th (NTNU3)
system positions (5th best team). Table 3 lists the
correlation score and rank of the ten best individual
features per STS?13 test data set, and those among
the top-20 overall, resulting from linear regression
on a single feature. Features in boldface are gen-
uinely new (i.e., described in Sections 2?4).
Overall the character n-gram features are the most
informative, particularly for HeadLine and SMT.
The reason may be that these not only capture word
overlap (Ahn, 2011), but also inflectional forms and
spelling variants.
The (weighted) distributional similarity features
based on NYT are important for HeadLine and SMT,
which obviously contain sentence pairs from the
news genre, whereas the Wikipedia based feature is
more important for OnWN and FNWN. WordNet-
based measures are highly relevant too, with variants
NTNU1 NTNU2 NTNU3
Data r n r n r n
Head 0.7279 11 0.5909 59 0.7274 12
OnWN 0.5952 31 0.1634 86 0.5882 32
FNWN 0.3215 45 0.3650 27 0.3115 49
SMT 0.4015 2 0.3786 9 0.4035 1
mean 0.5519 9 0.3946 68 0.5498 12
Table 2: Correlation score and rank on 2013 test data
relying on path length outperforming those based on
Resnik similarity, except for SMT.
As is to be expected, basic word and lemma uni-
gram overlap prove to be informative, with overall
unweighted variants resulting in higher correlation.
Somewhat surprisingly, higher order n-gram over-
laps (n > 1) seem to be less relevant. Longest com-
mon subsequence and substring appear to work par-
ticularly well for OnWN and FNWN, respectively.
GateWordMatch is highly relevant too, in
agreement with earlier results on the development
data. Although treated as a single feature, it is ac-
tually a combination of similarity features where an
appropriate feature is selected for each word pair.
This ?vertical? way of combining features can po-
tentially provide a more fine-grained feature selec-
tion, resulting in less noise. Indeed, if two words are
matching as named entities or as close synonyms,
less precise types of features such as character-based
and data-driven similarity should not dominate the
overall similarity score.
It is interesting to find that MSRI outper-
forms both classical RI and ESA (Gabrilovich and
Markovitch, 2007) on this task. Still, the more ad-
vanced features, such as MSRI-Context, gave in-
ferior results compared to MSRI-Centroid. This
suggests that more research on MSRI is needed
to understand how both training and retrieval can
be optimised. Also, LSA-based features (see
tl.weight-dist-sim-wiki) achieve better
results than both MSRI, RI and ESA. Then again,
larger corpora were used for training the LSA mod-
els. RI has been shown to be comparable to LSA
(Karlgren and Sahlgren, 2001), and since a relatively
small corpus was used for training the RI/MSRI
models, there are reasons to believe that better
scores can be achieved by both RI- and MSRI-based
features by using more training data.
70
HeadLine OnWN FNWN SMT Mean
Features r n r n r n r n r n
CharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1
CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2
CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3
tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4
tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5
GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9 0.50 6
tl.dist-sim-nyt 0.69 5 0.34 28 0.26 23 0.65 8 0.49 7
tl.n-gram-match-lem-1 0.68 6 0.36 16 0.37 8 0.51 14 0.48 8
tl.weight-dist-sim-nyt 0.57 17 0.37 14 0.29 18 0.66 7 0.47 9
tl.n-gram-match-lc-1 0.68 7 0.37 10 0.32 13 0.50 17 0.47 10
MCS06-Resnik-WordNet 0.49 26 0.36 22 0.28 19 0.68 3 0.45 11
TWSI-Resnik-WordNet 0.49 27 0.36 23 0.28 20 0.68 4 0.45 12
tl.weight-word-match-lem 0.56 18 0.37 16 0.37 7 0.50 16 0.45 13
MSRI-Centroid 0.60 13 0.36 17 0.37 9 0.45 19 0.45 14
tl.weight-word-match-olc 0.56 19 0.38 8 0.32 12 0.51 15 0.44 15
MSRI-MaxSense 0.58 15 0.36 15 0.31 14 0.45 20 0.42 16
GreedyStringTiling-3 0.67 9 0.38 6 0.31 15 0.34 29 0.43 17
ESA-Wikipedia 0.50 25 0.30 38 0.32 14 0.54 12 0.42 18
WordNGramJaccard-1 0.64 10 0.37 12 0.25 25 0.33 30 0.40 19
WordNGramContainment-1-stopword 0.64 25 0.38 7 0.25 24 0.32 31 0.40 20
RI-Hungarian 0.58 16 0.33 31 0.10 34 0.42 22 0.36 24
RI-AvgTermTerm 0.56 20 0.33 32 0.11 33 0.37 28 0.34 25
LongestCommonSubstring 0.40 29 0.30 39 0.42 4 0.37 27 0.37 26
ESA-WordNet 0.11 43 0.30 40 0.41 6 0.49 18 0.33 29
LongestCommonSubsequenceNorm 0.53 21 0.39 4 0.19 27 0.18 37 0.32 30
MultisenseRI-ContextTermTerm 0.39 31 0.33 33 0.28 21 0.15 38 0.29 33
MultisenseRI-HASensesTermTerm 0.39 32 0.33 34 0.28 22 0.15 39 0.29 34
RI-SentVectors-Norm 0.34 35 0.35 26 -0.01 51 0.24 35 0.23 39
RelationSimilarity 0.31 39 0.35 27 0.24 26 0.02 41 0.23 40
RI-SentVectors-TFIDF 0.27 40 0.15 50 0.08 40 0.23 36 0.18 41
GraphEditDistance 0.33 38 0.25 46 0.13 31 -0.11 49 0.15 42
Table 3: Correlation score and rank of the best features
8 Conclusion and Future Work
The NTNU system can be regarded as continuation
of the most successful systems from the STS?12
shared task, combining shallow textual, distribu-
tional and knowledge-based features into a support
vector regression model. It reuses features from the
TakeLab and DKPro systems, resulting in a very
strong baseline.
Adding new features to further improve
performance turned out to be hard: only
GateWordMatch yielded improved perfor-
mance. Similarity features based on both classical
and innovative variants of Random Indexing were
shown to correlate with semantic textual similarity,
but did not complement the existing distributional
features. Likewise, features designed to reveal
deeper syntactic (graph edit distance) and semantic
relations (RelEx) did not add to the score.
As future work, we would aim to explore a
vertical feature composition approach similar to
GateWordMatch and contrast it with the ?flat?
composition currently used in our systems.
Acknowledgements
Thanks to TakeLab for source code of their ?simple?
system and the full-scale LSA models. Thanks to the
team from Ubiquitous Knowledge Processing Lab
for source code of their DKPro Similarity system.
71
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). SemEval-2012 Task 6: A pilot on se-
mantic textual similarity. In *SEM (2012), pages
385?393.
Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A.,
and Guo, W. (2013). *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational
Semantics. Association for Computational Lin-
guistics.
Ahn, C. S. (2011). Automatically detecting authors?
native language. PhD thesis, Monterey, Califor-
nia. Naval Postgraduate School.
Banea, C., Hassan, S., Mohler, M., and Mihalcea, R.
(2012). UNT: a supervised synergistic approach
to semantic text similarity. In *SEM (2012),
pages 635?642.
Ba?r, D., Biemann, C., Gurevych, I., and Zesch, T.
(2012). UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity
measures. In *SEM (2012), pages 435?440.
Bhagwani, S., Satapathy, S., and Karnick, H. (2012).
sranjans : Semantic textual similarity using maxi-
mal weighted bipartite graph matching. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceed-
ings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (Sem-
Eval 2012), pages 579?585, Montre?al, Canada.
Association for Computational Linguistics.
Cunningham, H., Maynard, D., Bontcheva, K., and
Tablan, V. (2002). GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Philadel-
phia, Pennsylvania. ACL.
Deerwester, S., Dumais, S., Furnas, G., Landauer,
T., and Harshman, R. (1990). Indexing by latent
semantic analysis. Journal of the American Soci-
ety for Information Science, 41(6):391?407.
Fundel, K., Ku?ffner, R., and Zimmer, R. (2007).
RelEx - Relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Gabrilovich, E. and Markovitch, S. (2007). Comput-
ing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of The
Twentieth International Joint Conference for Ar-
tificial Intelligence., pages 1606?1611.
Hart, D. and Goertzel, B. (2008). Opencog: A soft-
ware framework for integrative artificial general
intelligence. In Proceedings of the 2008 confer-
ence on Artificial General Intelligence 2008: Pro-
ceedings of the First AGI Conference, pages 468?
472, Amsterdam, The Netherlands, The Nether-
lands. IOS Press.
Hassel, M. (2004). JavaSDM package.
Kanerva, P., Kristoferson, J., and Holst, A. (2000).
Random indexing of text samples for latent se-
mantic analysis. In Gleitman, L. and Josh, A.,
editors, Proceedings of the 22nd Annual Confer-
ence of the Cognitive Science Society, page 1036.
Erlbaum.
Karlgren, J. and Sahlgren, M. (2001). From Words
to Understanding. In Uesaka, Y., Kanerva, P., and
Asoh, H., editors, Foundations of real-world in-
telligence, chapter 26, pages 294?311. Stanford:
CSLI Publications.
Karnick, H., Satapathy, S., and Bhagwani, S. (2012).
sranjans: Semantic textual similarity using max-
imal bipartite graph matching. In *SEM (2012),
pages 579?585.
Kuhn, H. (1955). The Hungarian method for the as-
signment problem. Naval research logistics quar-
terly, 2:83?97.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal . . . .
Levenshtein, V. I. (1966). Binary codes capable of
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707?710.
Nivre, J., Hall, J., and Nilsson, J. (2006). Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In In Proc. of LREC-2006, pages
2216?2219.
72
Reisinger, J. and Mooney, R. (2010). Multi-
prototype vector-space models of word meaning.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
number June, pages 109?117.
Riesen, K. and Bunke, H. (2009). Approximate
graph edit distance computation by means of bi-
partite graph matching. Image and Vision Com-
puting, 27(7):950?959.
R?kenes, H. (2013). Graph-Edit Distance Applied to
the Task of Detecting Plagiarism. Master?s the-
sis, Norwegian University of Science and Tech-
nology.
Sahlgren, M. (2005). An introduction to random in-
dexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge En-
gineering, TKE, volume 5.
Sahlgren, M., Holst, A., and Kanerva, P. (2008). Per-
mutations as a Means to Encode Order in Word
Space. Proceedings of the 30th Conference of the
Cognitive Science Society.
S?aric?, F., Glavas?, G., Karan, M., S?najder, J., and
Bas?ic?, B. D. (2012). TakeLab: systems for mea-
suring semantic text similarity. In *SEM (2012),
pages 441?448.
*SEM (2012). Proceedings of the First Joint Con-
ference on Lexical and Computational Seman-
tics (*SEM), volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation,
Montreal, Canada. Association for Computational
Linguistics.
Semeraro, G., Aldo, B., and Orabona, V. E. (2012).
UNIBA: Distributional semantics for textual sim-
ilarity. In *SEM (2012), pages 591?596.
Singh, J., Bhattacharya, A., and Bhattacharyya, P.
(2012). janardhan: Semantic textual similarity us-
ing universal networking language graph match-
ing. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalu-
ation (SemEval 2012), pages 662?666, Montre?al,
Canada. Association for Computational Linguis-
tics.
Sokolov, A. (2012). LIMSI: learning semantic simi-
larity by selecting random word subsets. In *SEM
(2012), pages 543?546.
Tovar, M., Reyes, J., and Montes, A. (2012). BUAP:
a first approximation to relational similarity mea-
suring. In *SEM (2012), pages 502?505.
Vapnik, V., Golowich, S. E., and Smola, A. (1997).
Support vector method for function approxima-
tion, regression estimation, and signal process-
ing. In Mozer, M. C., Jordan, M. I., and Petsche,
T., editors, Advances in Neural Information Pro-
cessing Systems, volume 9, pages 281?287. MIT
Press, Cambridge, Massachusetts.
73
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 430?437, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
NTNU: Domain Semi-Independent Short Message Sentiment Classification
?yvind Selmer Mikael Brevik Bjo?rn Gamba?ck Lars Bungum
Department of Computer and Information Science
Norwegian University of Science and Technology (NTNU)
Sem S?lands vei 7?9, NO?7491 Trondheim, Norway
{oyvinsel,mikaelbr}@stud.ntnu.no {gamback,larsbun}@idi.ntnu.no
Abstract
The paper describes experiments using grid
searches over various combinations of ma-
chine learning algorithms, features and pre-
processing strategies in order to produce the
optimal systems for sentiment classification of
microblog messages. The approach is fairly
domain independent, as demonstrated by the
systems achieving quite competitive results
when applied to short text message data, i.e.,
input they were not originally trained on.
1 Introduction
The informal texts in microblogs such as Twitter
and on other social media represent challenges for
traditional language processing systems. The posts
(?tweets?) are limited to 140 characters and often
contain misspellings, slang and abbreviations. On
the other hand, the posts are often opinionated in
nature as a very result of their informal character,
which has led Twitter to being a gold mine for sen-
timent analysis (SA). SA for longer texts, such as
movie reviews, has been explored since the 1990s;1
however, the limited amount of attributes in tweets
makes the feature vectors shorter than in documents
and the task of analysing them closely related to
phrase- and sentence-level SA (Wilson et al, 2005;
Yu and Hatzivassiloglou, 2003). Hence there are
no guarantees that algorithms that perform well on
document-level SA will do as well on tweets. On
the other hand, it is possible to exploit some of the
special features of the web language, e.g., emoticons
1See Pang and Lee (2008); Feldman (2013) for overviews.
and emotionally loaded abbreviations. Thus the data
will normally go through some preprocessing before
any classification is attempted, e.g., by filtering out
Twitter specific symbols and functions, in particular
retweets (reposting another user?s tweet), mentions
(?@?, tags used to mention another user), hashtags
(?#?, used to tag a tweet to a certain topic), emoti-
cons, and URLs (linking to an external resource,
e.g., a news article or a photo). The first system to re-
ally use Twitter as a corpus was created as a student
course project at Stanford (Go et al, 2009). Pak and
Paroubek (2010) experimented with sentiment clas-
sification of tweets using Support Vector Machines
and Conditional Random Fields, benchmarked with
a Na??ve Bayes Classifier baseline, but were unable
to beat the baseline. Later, and as Twitter has grown
in popularity, many other systems for Twitter Senti-
ment Analysis (TSA) have been developed (see, e.g.,
Maynard and Funk, 2011; Mukherjee et al, 2012;
Saif et al, 2012; Chamlertwat et al, 2012).
Clearly, it is possible to classify the sentiment of
tweets in a single step; however, the approach to
TSA most used so far is a two-step strategy where
the first step is subjectivity classification and the
second step is polarity classification. The goal of
subjectivity classification is to separate subjective
and objective statements. Pak and Paroubek (2010)
counted word frequencies in a subjective vs an ob-
jective set of tweets; the results showed that in-
terjections and personal pronouns are the strongest
indicators of subjectivity. In general, these word
classes, adverbs and (in particular) adjectives (Hatzi-
vassiloglou and Wiebe, 2000) have shown to be
good subjectivity indicators, which has made part-
430
of-speech (POS) tagging a reasonable technique for
filtering out objective tweets. Early research on
TSA showed that the challenging vocabulary made
it harder to accurately tag tweets; however, Gimpel
et al (2011) report on using a POS tagger for mark-
ing tweets, performing with almost 90% accuracy.
Polarity classification is the task of separating the
subjective statements into positives and negatives.
Kouloumpis et al (2011) tried different solutions for
tweet polarity classification, and found that the best
performance came from using n-grams together with
lexicon and microblog features. Interestingly, per-
formance dropped when a POS tagger was included.
They speculate that this can be due to the accuracy
of the POS tagger itself, or that POS tagging just is
less effective for analysing tweet polarity.
In this paper we will explore the application of
a set of machine learning algorithms to the task of
Twitter sentiment classification, comparing one-step
and two-step approaches, and investigate a range of
different preprocessing methods. What we explic-
itly will not do, is to utilise a sentiment lexicon, even
though many methods in TSA rely on lexica with a
sentiment score for each word. Nielsen (2011) man-
ually built a sentiment lexicon specialized for Twit-
ter, while others have tried to induce such lexica
automatically with good results (Velikovich et al,
2010; Mohammad et al, 2013). However, sentiment
lexica ? and in particular specialized Twitter senti-
ment lexica ? make the classification more domain
dependent. Here we will instead aim to exploit do-
main independent approaches as far as possible, and
thus abstain from using sentiment lexica. The rest of
the paper is laid out as follows: Section 2 introduces
the twitter data sets used in the study. Then Section 3
describes the system built for carrying out the twitter
sentiment classification experiments, which in turn
are reported and discussed in Sections 4 and 5.
2 Data
Manually collecting information from Twitter would
be a tedious task, but Twitter offers a well doc-
umented Representational State Transfer Applica-
tion Programming Interface (REST API) which al-
lows users to collect a corpus from the micro-
blogosphere. Most of the data used in TSA re-
search is collected through the Twitter API, either by
Training Dev 1 Dev 2 NTNU
Class Num % Num % Num % Num %
Negative 1288 15 176 21 340 26 86 19
Neutral 4151 48 144 45 739 21 232 50
Positive 3270 37 368 35 575 54 142 31
Total 8709 688 1654 461
Table 1: The data sets used in the experiments
searching for a certain topic/keyword or by stream-
ing realtime data. Four different data sets were used
in the experiments described below. three were sup-
plied by the organisers of the SemEval?13 shared
task on Twitter sentiment analysis (Wilson et al,
2013), in the form of a training set, a smaller initial
development set, and a larger development set. All
sets consist of manually annotated tweets on a range
of topics, including different products and events.
Tweet-level classification (Task 2B) was split into
two subtasks in SemEval?13, one allowing training
only on the data sets supplied by the organisers (con-
strained) and one allowing training also on external
data (unconstrained). To this end, a web applica-
tion2 for manual annotation of tweets was built and
used to annotate a small fourth data set (?NTNU?).
Each of the 461 tweets in the ?NTNU? data set was
annotated by one person only.
The distribution of target classes in the data sets is
shown in Table 1. The data was neither preprocessed
nor filtered, and thus contain hashtags, URLs, emoti-
cons, etc. However, all the data sets provided by
SemEval?13 had more than three target classes (e.g.,
?objective?, ?objective-OR-neutral?), so tweets that
were not annotated as ?positive? or ?negative? were
merged into the ?neutral? target class.
Due to Twitter?s privacy policy, the given data sets
do not contain the tweet text, but only the tweet ID
which in turn can be used to download the text. The
Twitter API has a limit on the number of downloads
per hour, so SemEval?13 provided a Python script
to scrape texts from https://twitter.com. This
script was slow and did not download the texts for all
tweet IDs in the data sets, so a faster and more pre-
cise download script3 for node.js was implemented
and submitted to the shared task organisers.
2http://tinyurl.com/tweetannotator
3http://tinyurl.com/twitscraper
431
3 Experimental Setup
In order to run sentiment classification experiments,
a general system was built. It has a Sentiment Anal-
ysis API Layer which works as a thin extension of
the Twitter API, sending all tweets received in par-
allel to a Sentiment Analysis Classifier server. After
classification, the SA API returns the same JSON
structure as the Twitter API sends out, only with an
additional attribute denoting the tweet?s sentiment.
The Sentiment Analysis Classifier system consists
of preprocessing and classification, described below.
3.1 Preprocessing
As mentioned in the introduction, most approaches
to Twitter sentiment analysis start with a pre-
processing step, filtering out some Twitter specific
symbols and functions. Go et al (2009) used ?:)?
and ?:(? as labels for the polarity, so did not remove
these emoticons, but replaced URLs and user names
with placeholders. Kouloumpis et al (2011) used
both an emoticon set and a hashtagged set. The lat-
ter is a subset of the Edinburgh Twitter corpus which
consists of 97 million tweets (Petrovic? et al, 2010).
Some approaches have also experimented with nor-
malizing the tweets, and removing redundant letters,
e.g., ?loooove? and ?crazyyy?, that are used to ex-
press a stronger sentiment in tweets. Redundant let-
ters are therefore often not deleted, but words rather
trimmed down to one additional redundant letter, so
that the stronger sentiment can be taken into consid-
eration by a score/weight adjustment for that feature.
To find the best features to use, a set of eight dif-
ferent combinations of preprocessing methods was
designed, as detailed in Table 2. These include no
preprocessing (P0, not shown in the table), where
all characters are included as features; full remove
(P4), where all special Twitter features like user
names, URLs, hashtags, retweet (RT ) tags, and
emoticons are stripped; and replacing Twitter fea-
tures with placeholder texts to reduce vocabulary.
The ?hashtag as word? method transforms a hashtag
to a regular word and uses the hashtag as a feature.
?Reduce letter duplicate? removes redundant char-
acters more than three (?happyyyyyyyy!!!!!!? ?
?happyyy!!!?). Some methods, like P1, P2, P4, P5
and P7 remove user names from the text, as they
most likely are just noise for the sentiment. Still,
Method P1 P2 P3 P4 P5 P6 P7
Remove Usernames X X X X X
Username placeholder X
Remove URLs X X X X
URL placeholder X
Remove hashtags X X
Hashtag as word X
Hashtag placeholder X
Remove RT -tags X X X
Remove emoticons X X
Reduce letter duplicate X X X X
Negation attachment X X X
Table 2: Overview of the preprocessing methods
the fact that there are references to URLs and user
names might be relevant for the sentiment. To make
these features more informative for the machine
learning algorithms, a preprocessing method (P3)
was implemented for replacing them with place-
holders. In addition, a very rudimentary treatment
of negation was added, in which the negation is at-
tached to the preceding and following words, so that
they will also reflect the change in sentence polarity.
Even though this preprocessing obviously is
Twitter-specific, the results after it will still be do-
main semi-independent, in as far as the strings pro-
duced after the removal of URLs, user names, etc.,
will be general, and can be used for system training.
3.2 Classification
The classification step currently supports three
machine learning algorithms from the Python
scikit-learn4 package: Na??ve Bayes (NB),
Maximum Entropy (MaxEnt), and Support Vector
Machines (SVM). These are all among the super-
vised learners that previously have been shown to
perform well on TSA, e.g., by Bermingham and
Smeaton (2010) who compared SVM and NB for
microblogs. Interestingly, while the SVM technique
normally beats NB and MaxEnt on longer texts, that
comparison indicated that it has some trouble with
outperforming NB when feature vectors are shorter.
Three different models were implemented:
1. One-step model: a single algorithm classifies
tweets as negative, neutral or positive.
2. Two-step model: the tweets are first classified
as either subjective or neutral. Those that are
4http://scikit-learn.org
432
Figure 1: Performance across all models (red=precision, blue=recall, green=F1-score, brown=accuracy)
classified as subjective are then sent to polarity
classification (i.e., negative or positive).
3. Boosting (Freund and Schapire, 1997): a way
to combine classifiers by generating a set of
sub-models, each of which predicts a sentiment
on its own and then sends it to a voting process
that selects the sentiment with highest score.
In all cases, the final classification is returned to the
API Layer sentiment provider.
4 Experimental Results
Experiments were carried out using the platform in-
troduced in the previous section, with models built
on the training set of Table 1. The testing system
generates and trains different models based on a set
of parameters, such as classification algorithm, pre-
processing methods, whether or not to use inverse
document frequency (IDF) or stop words. A grid
search option can be activated, so that a model is
generated with the best possible parameter set for
the given algorithm, using 10-fold cross validation.
4.1 Selection of Learners and Features
An extensive grid search was conducted. This search
cycled through different algorithms, parameters and
preprocessing techniques. The following param-
eters were included in the search. Three binary
(Yes/No) parameters: Use IDF, Use Smooth
IDF, and Use Sublinear IDF, together with
ngram (unigram/bigram/trigram). SVM
and MaxEnt models in addition included C and
NB models alpha parameters, all with the value
ranges [0.1/0.3/0.5/0.7/0.8/1.0]. SVM
and MaxEnt models also had penalty (L1/L2).
Figure 1 displays the precision, recall, F1-score,
and accuracy for each of the thirteen classifiers with
the Dev 2 data set (see Table 1) used for evaluation.
Note that most classifiers involving the NB algo-
rithm perform badly, both in terms of accuracy and
F-score. This was observed for the other data sets as
well. Further, we can see that one-step classifiers did
better than two-step models, with MaxEnt obtaining
the best accuracy, but SVM a slightly better F-score.
433
Data set Dev 2 Dev 1
Learner SVM MaxEnt SVM MaxEnt
Precision 0.627 0.647 0.700 0.561
Recall 0.592 0.578 0.726 0.589
F1-score 0.598 0.583 0.707 0.556
Accuracy 0.638 0.645 0.728 0.581
Table 3: Best classifier performance (bold=best score;
all classifiers were trained on the training set of Table 1)
A second grid search with the two best classifiers
from the first search was performed instead using the
smaller Dev 1 data set for evaluation. The results
for both the SVM and MaxEnt classifiers are shown
in Table 3. With the Dev 1 data set, SVM performs
much better than MaxEnt. The larger Dev 2 develop-
ment set contains more neutral tweets than the Dev 1
set, which gives us reasons to believe that evaluating
on the Dev 2 set favours the MaxEnt classifier.
A detailed error analysis was conducted by in-
specting the confusion matrices of all classifiers. In
general, classifiers involving SVM tend to give bet-
ter confusion matrices than the others. Using SVM
only in a one-step model works well for positive and
neutral tweets, but a bit poorer for negative. Two-
step models with SVM-based subjectivity classifica-
tion exhibit the same basic behaviour. The one-step
MaxEnt model classifies more tweets as neutral than
the other classifiers. Using MaxEnt for subjectivity
classification and either MaxEnt or SVM for polarity
classification performs well, but is too heavy on the
positive class. Boosting does not improve and be-
haves in a fashion similar to two-step MaxEnt mod-
els. All combinations involving NB tend to heavily
favour positive predictions; only the two-step mod-
els involving another algorithm for polarity classifi-
cation gave some improvement for negative tweets.
The confusion matrices of the two best learners
are shown in Figures 2a-2d, where a learner is shown
to perform better if it has redish colours on the main
diagonal and blueish in the other fields, as is the case
for SVM on the Dev 1 data set (Figure 2c).
As a part of the grid search, all preprocessing
methods were tested for each classifier. Figure 3
shows that P2 (removing user names, URLs, hash-
tags prefixes, retweet tokens, and redundant letters)
is the preprocessing method which performs best
(a) SVM Dev 2 (b) MaxEnt Dev 2
(c) SVM Dev 1 (d) MaxEnt Dev 1
Figure 2: SVM and MaxEnt confusion matrices (out-
put is shown from left-to-right: negative-neutral-positive;
the correct classes are in the same order, top-to-bottom.
?Hotter? colours (red) indicate that more instances were
assigned; ?colder? colours (blue) mean fewer instances.)
P2 P7 P6 P1 P3
0
2
4
6
8
10
10
4
3 3
1
Figure 3: Statistics of preprocessing usage
(gives the best accuracy) and thus used most of-
ten (10 times). Figure 3 also indicates that URLs
are noisy and do not contain much sentiment, while
hashtags and emoticons tend to be more valuable
features (P2 and P7 ? removing URLs ? perform
best, while P4 and P5 ? removing hashtags and
emoticons in addition to URLs ? perform badly).
434
Data set Twitter SMS
System NTNUC NTNUU NTNUC NTNUU
Precision 0.652 0.633 0.659 0.623
Recall 0.579 0.564 0.646 0.623
F1-score 0.590 0.572 0.652 0.623
F1 + /? 0.532 0.507 0.580 0.546
Table 4: The NTNU systems in SemEval?13
4.2 SemEval?13 NTNU Systems and Results
Based on the information from the grid search, two
systems were built for SemEval?13. Since one-step
SVM-based classification showed the best perfor-
mance on the training data, it was chosen for the
system participating in the constrained subtask, NT-
NUC. The preprocessing also was the one with the
best performance on the provided data, P2 which
involves lower-casing all letters; reducing letter du-
plicates; using hashtags as words (removing #); and
removing all URLs, user names and RT -tags.
Given the small size of the in-house (?NTNU?)
data set, no major improvement was expected from
adding it in the unconstrained task. Instead, a rad-
ically different set-up was chosen to create a new
system, and train it on both the in-house and pro-
vided data. NTNUU utilizes a two-step approach,
with SVM for subjectivity and MaxEnt for polarity
classification, a combination intended to capture the
strengths of both algorithms. No preprocessing was
used for the subjectivity step, but user names were
removed before attempting polarity classification.
As further described by Wilson et al (2013), the
SemEval?13 shared task involved testing on a set of
3813 tweets (1572 positive, 601 negative, and 1640
neutral). In order to evaluate classification perfor-
mance on data of roughly the same length and type,
but from a different domain, the evaluation data also
included 2094 Short Message Service texts (SMS;
492 positive, 394 negative, and 1208 neutral).
Table 4 shows the results obtained by the NTNU
systems on the SemEval?13 evaluation data, in terms
of average precision, recall and F-score for all three
classes, as well as average F-score for positive and
negative tweets only (F1+/?; i.e., the measure used
to rank the systems participating in the shared task).
5 Discussion and Conclusion
As can be seen in Table 4, the extra data available
to train the NTNUU system did not really help it:
it gets outperformed by NTNUC on all measures.
Notably, both systems perform well on the out-
of-domain data represented by the SMS messages,
which is encouraging and indicates that the approach
taken really is domain semi-independent. This was
also reflected in the rankings of the two systems in
the shared task: both were on the lower half among
the participating systems on Twitter data (24th/36
resp. 10th/15), but near the top on SMS data, with
NTNUC being ranked 5th of 28 constrained systems
and NTNUU 6th of 15 unconstrained systems.
Comparing the results to those shown in Table 3
and Figure 1, NTNUC?s (SVM) performance is in
line with that on Dev 2, but substantially worse
than on Dev 1; NTNUU (SVM?MaxEnt) performs
slightly worse too. Looking at system output with
and without the ?NTNU? data, both one-step SVM
and MaxEnt models and SVM?MaxEnt classified
more tweets as negative when trained on the ex-
tra data; however, while NTNUC benefited slightly
from this, NTNUU even performed better without it.
An obvious extension to the present work would
be to try other classification algorithms (e.g., Condi-
tional Random Fields or more elaborate ensembles)
or other features (e.g., character n-grams). Rather
than the very simple treatment of negation used
here, an approach to automatic induction of scope
through a negation detector (Councill et al, 2010)
could be used. It would also be possible to relax
the domain-independence further, in particular to
utilize sentiment lexica (including twitter specific),
e.g., by automatic phrase-polarity lexicon extraction
(Velikovich et al, 2010). Since many users tweet
from their smartphones, and a large number of them
use iPhones, several tweets contain iPhone-specific
smilies (?Emoji?). Emoji are implemented as their
own character set (rather than consisting of charac-
ters such as ?:)? and ?:(?, etc.), so a potentially major
improvement could be to convert them to character-
based smilies or to emotion-specific placeholders.
Acknowledgements
Thanks to Amitava Das for initial discussions and to
the human annotators of the ?NTNU? data set.
435
References
Bermingham, A. and Smeaton, A. F. (2010). Clas-
sifying sentiment in microblogs: Is brevity an
advantage? In Proceedings of the 19th Inter-
national Conference on Information and Knowl-
edge Management, pages 1833?1836, Toronto,
Canada. ACM.
Chamlertwat, W., Bhattarakosol, P., Rungkasiri, T.,
and Haruechaiyasak, C. (2012). Discovering con-
sumer insight from Twitter via sentiment anal-
ysis. Journal of Universal Computer Science,
18(8):973?992.
Councill, I. G., McDonald, R., and Velikovich, L.
(2010). What?s great and what?s not: learning
to classify the scope of negation for improved
sentiment analysis. In Proceedings of the 48th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 51?59, Uppsala, Swe-
den. ACL. Workshop on Negation and Specula-
tion in Natural Language Processing.
Feldman, R. (2013). Techniques and applications for
sentiment analysis. Communications of the ACM,
56(4):82?89.
Freund, Y. and Schapire, R. E. (1997). A decision-
theoretic generalization of on-line learning and
application to boosting. Journal of Computer and
System Sciences, 55(1):119?139.
Gimpel, K., Schneider, N., O?Connor, B., Das, D.,
Mills, D., Eisenstein, J., Heilman, M., Yogatama,
D., Flanigan, J., and Smith, N. A. (2011). Part-
of-speech tagging for Twitter: Annotation, fea-
tures, and experiments. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, volume 2: short papers, pages 42?47, Port-
land, Oregon. ACL.
Go, A., Huang, L., and Bhayani, R. (2009). Twit-
ter sentiment analysis. Technical Report CS224N
Project Report, Department of Computer Science,
Stanford University, Stanford, California.
Hatzivassiloglou, V. and Wiebe, J. M. (2000). Ef-
fects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of the 18th
International Conference on Computational Lin-
guistics, pages 299?305, Saarbru?cken, Germany.
ACL.
HLT10 (2010). Proceedings of the 2010 Human
Language Technology Conference of the North
American Chapter of the Association for Com-
putational Linguistics, Los Angeles, California.
ACL.
Kouloumpis, E., Wilson, T., and Moore, J. (2011).
Twitter sentiment analysis: The good the bad and
the OMG! In Proceedings of the 5th International
Conference on Weblogs and Social Media, pages
538?541, Barcelona, Spain. AAAI.
Maynard, D. and Funk, A. (2011). Automatic detec-
tion of political opinions in tweets. In #MSM2011
(2011), pages 81?92.
Mohammad, S., Kiritchenko, S., and Zhu, X.
(2013). NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In SemEval?13
(2013).
#MSM2011 (2011). Proceedings of the 1st
Workshop on Making Sense of Microposts
(#MSM2011), Heraklion, Greece.
Mukherjee, S., Malu, A., Balamurali, A., and Bhat-
tacharyya, P. (2012). TwiSent: A multistage sys-
tem for analyzing sentiment in Twitter. In Pro-
ceedings of the 21st International Conference on
Information and Knowledge Management, pages
2531?2534, Maui, Hawaii. ACM.
Nielsen, F. A?. (2011). A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs.
In #MSM2011 (2011), pages 93?98.
Pak, A. and Paroubek, P. (2010). Twitter as a cor-
pus for sentiment analysis and opinion mining. In
Proceedings of the 7th International Conference
on Language Resources and Evaluation, Valetta,
Malta. ELRA.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Petrovic?, S., Osborne, M., and Lavrenko, V. (2010).
The Edinburgh Twitter corpus. In HLT10 (2010),
pages 25?26. Workshop on Computational Lin-
guistics in a World of Social Media.
Saif, H., He, Y., and Alani, H. (2012). Semantic
sentiment analysis of Twitter. In Proceedings of
the 11th International Semantic Web Conference,
pages 508?524, Boston, Massachusetts. Springer.
436
SemEval?13 (2013). Proceedings of the Interna-
tional Workshop on Semantic Evaluation, Sem-
Eval ?13, Atlanta, Georgia. ACL.
Velikovich, L., Blair-Goldensohn, S., Hannan, K.,
and McDonald, R. (2010). The viability of web-
derived polarity lexicons. In HLT10 (2010), pages
777?785.
Wilson, T., Kozareva, Z., Nakov, P., Ritter, A.,
Rosenthal, S., and Stoyanov, V. (2013). SemEval-
2013 Task 2: Sentiment analysis in Twitter. In
SemEval?13 (2013).
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the 2005 Hu-
man Language Technology Conference and Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Vancouver,
British Columbia, Canada. ACL.
Yu, H. and Hatzivassiloglou, V. (2003). Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion
sentences. In Collins, M. and Steedman, M., edi-
tors, Proceedings of the 2003 Conference on Em-
pirical Methods in Natural Language Processing,
pages 129?136, Sapporo, Japan. ACL.
437
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 21?30,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Improving Word Translation Disambiguation by
Capturing Multiword Expressions with Dictionaries
Lars Bungum, Bjo?rn Gamba?ck, Andre? Lynum, Erwin Marsi
Norwegian University of Science and Technology
Sem S?lands vei 7?9; NO?7491 Trondheim, Norway
{bungum,gamback,andrely,emarsi}@idi.ntnu.no
Abstract
The paper describes a method for identifying
and translating multiword expressions using a
bi-directional dictionary. While a dictionary-
based approach suffers from limited recall,
precision is high; hence it is best employed
alongside an approach with complementing
properties, such as an n-gram language model.
We evaluate the method on data from the
English-German translation part of the cross-
lingual word sense disambiguation task in the
2010 semantic evaluation exercise (SemEval).
The output of a baseline disambiguation sys-
tem based on n-grams was substantially im-
proved by matching the target words and their
immediate contexts against compound and
collocational words in a dictionary.
1 Introduction
Multiword expressions (MWEs) cause particular
lexical choice problems in machine translation
(MT), but can also be seen as an opportunity to both
generalize outside the bilingual corpora often used
as training data in statistical machine translation ap-
proaches and as a method to adapt to specific do-
mains. The identification of MWEs is in general
important for many language processing tasks (Sag
et al, 2002), but can be crucial in MT: since the se-
mantics of many MWEs are non-compositional, a
suitable translation cannot be constructed by trans-
lating the words in isolation. Identifying MWEs
can help to identify idiomatic or otherwise fixed lan-
guage usage, leading to more fluent translations, and
potentially reduce the amount of lexical choice an
MT system faces during target language generation.
In any translation effort, automatic or otherwise,
the selection of target language lexical items to in-
clude in the translation is a crucial part of the fi-
nal translation quality. In rule-based systems lex-
ical choice is derived from the semantics of the
source words, a process which often involves com-
plex semantic composition. Data-driven systems
on the other hand commonly base their translations
nearly exclusively on cooccurrences of bare words
or phrases in bilingual corpora, leaving the respon-
sibility of selecting lexical items in the translation
entirely to the local context found in phrase trans-
lation tables and language models with no explicit
notion of the source or target language semantics.
Still, systems of this type have been shown to pro-
duce reasonable translation quality without explic-
itly considering word translation disambiguation.
Bilingual corpora are scarce, however, and un-
available for most language pairs and target do-
mains. An alternative approach is to build systems
based on large monolingual knowledge sources and
bilingual lexica, as in the hybrid MT system PRE-
SEMT (Sofianopoulos et al, 2012). Since such
a system explicitly uses a translation dictionary, it
must at some point in the translation process decide
which lexical entries to use; thus a separate word
translation disambiguation module needs to be in-
corporated. To research available methods in such a
module we have identified a task where we can use
public datasets for measuring how well a method is
able to select the optimal of many translation choices
from a source language sentence.
21
In phrase-based statistical MT systems, the trans-
lation of multiword expressions can be a notable
source of errors, despite the fact that those systems
explicitly recognize and use alignments of sequen-
tial chunks of words. Several researchers have ap-
proached this problem by adding MWE translation
tables to the systems, either through expanding the
phrase tables (Ren et al, 2009) or by injecting the
MWE translations into the decoder (Bai et al, 2009).
Furthermore, there has been some interest in auto-
matic mining of MWE pairs from bilingual corpora
as a task in itself: Caseli et al (2010) used a dic-
tionary for evaluation of an automatic MWE extrac-
tion procedure using bilingual corpora. They also
argued for the filtering of stopwords, similarly to the
procedure described in the present paper. Sharoff
et al (2006) showed how MWE pairs can be ex-
tracted from comparable monolingual corpora in-
stead of from a parallel bilingual corpus.
The methodology introduced in this paper em-
ploys bilingual dictionaries as a source of multi-
word expressions. Relationships are induced be-
tween the source sentence and candidate transla-
tion lexical items based on their correspondence in
the dictionary. Specifically, we use a determinis-
tic multiword expression disambiguation procedure
based on translation dictionaries in both directions
(from source to target language and vice versa),
and a baseline system that ranks target lexical items
based on their immediate context and an n-gram
language model. The n-gram model represents a
high-coverage, low-precision companion to the dic-
tionary approach (i.e., it has complementary proper-
ties). Results show that the MWE dictionary infor-
mation substantially improves the baseline system.
The 2010 Semantic Evaluation exercise (Sem-
Eval?10) featured a shared task on Cross-Lingual
Word Sense Disambiguation (CL-WSD), where the
focus was on disambiguating the translation of a sin-
gle noun in a sentence. The participating systems
were given an English word in its context and asked
to produce appropriate substitutes in another lan-
guage (Lefever and Hoste, 2010b). The CL-WSD
data covers Dutch, French, Spanish, Italian and Ger-
man; however, since the purpose of the experiments
in this paper just was to assess our method?s abil-
ity to choose the right translation of a word given its
context, we used the English-to-German part only.
The next section details the employed disam-
biguation methodology and describes the data sets
used in the experiments. Section 3 then reports on
the results of experiments applying the methodology
to the SemEval datasets, particularly addressing the
impact of the dictionary MWE correspondences. Fi-
nally, Section 4 sums up the discussion and points to
issues that can be investigated further.
2 Methodology
The core of the disambiguation model introduced
in this paper is dictionary-based multiword extrac-
tion. Multiword extraction is done in both a direct
and indirect manner: Direct extraction uses adjacent
words in the source language in combination with
the word to be translated, if the combination has an
entry in the source-to-target language (SL?TL) dic-
tionary. Indirect extraction works in the reverse di-
rection, by searching the target-to-source (TL?SL)
dictionary and looking up translation candidates for
the combined words. Using a dictionary to identify
multiword expressions after translation has a low re-
call of target language MWEs, since often there ei-
ther are no multiword expressions to be discovered,
or the dictionary method is unable to find a trans-
lation for an MWE. Nevertheless, when an MWE
really is identified by means of the dictionary-based
method, the precision is high.
Due to the low recall, relying on multiword ex-
pressions from dictionaries would, however, not be
sufficient. Hence this method is combined with an
n-gram language model (LM) based on a large tar-
get language corpus. The LM is used to rank trans-
lation candidates according to the probability of the
n-gram best matching the context around the transla-
tion candidate. This is a more robust but less precise
approach, which servers as the foundation for the
high-precision but low-recall dictionary approach.
In the actual implementation, the n-gram method
thus first provides a list of its best suggestions
(currently top-5), and the dictionary method then
prepends its candidates to the top of this list. Con-
sequently, n-gram matching is described before
dictionary-based multiword extraction in the follow-
ing section. First, however, we introduce the data
sets used in the experiments.
22
(a) AGREEMENT in the form of an exchange of letters between
the European Economic Community and the Bank for Interna-
tional Settlements concerning the mobilization of claims held by
the Member States under the medium-term financial assistance
arrangements
{bank 4; bankengesellschaft 1; kreditinstitut 1; zentralbank 1; fi-
nanzinstitut 1}
(b) The Office shall maintain an electronic data bank with the par-
ticulars of applications for registration of trade marks and entries
in the Register. The Office may also make available the contents
of this data bank on CD-ROM or in any other machine-readable
form.
{datenbank 4; bank 3; datenbanksystem 1; daten 1}
(c) established as a band of 1 km in width from the banks of a
river or the shores of a lake or coast for a length of at least 3 km.
{ufer 4; flussufer 3}
Table 1: Examples of contexts for the English word bank
with possible German translations
2.1 The CL-WSD Datasets
The data sets used for the SemEval?10 Cross-
Lingual Word Sense Disambiguation task were con-
structed by making a ?sense inventory? of all pos-
sible target language translations of a given source
language word based on word-alignments in Eu-
roparl (Koehn, 2005), with alignments involving the
relevant source words being manually checked. The
retrieved target words were manually lemmatised
and clustered into translations with a similar sense;
see Lefever and Hoste (2010a) for details.
Trial and test instances were extracted from two
other corpora, JRC-Acquis (Steinberger et al, 2006)
and BNC (Burnard, 2007). The trial data for each
language consists of five nouns (with 20 sentence
contexts per noun), and the test data of twenty nouns
(50 contexts each, so 1000 in total per language,
with the CL-WSD data covering Dutch, French,
Spanish, Italian and German). Table 1 provides ex-
amples from the trial data of contexts for the English
word bank and its possible translations in German.
Gold standard translations were created by hav-
ing four human translators picking the contextually
appropriate sense for each source word, choosing 0?
3 preferred target language translations for it. The
translations are thus restricted to those appearing in
Europarl, probably introducing a slight domain bias.
Each translation has an associated count indicating
how many annotators considered it to be among their
top-3 preferred translations in the given context.
bank, bankanleihe, bankanstalt, bankdarlehen, bankenge-
sellschaft, bankensektor, bankfeiertag, bankgesellschaft, bankin-
stitut, bankkonto, bankkredit, banknote, blutbank, daten, daten-
bank, datenbanksystem, euro-banknote, feiertag, finanzinstitut,
flussufer, geheimkonto, geldschein, gescha?ftsbank, handelsbank,
konto, kredit, kreditinstitut, nationalbank, notenbank, sparkasse,
sparkassenverband, ufer, weltbank, weltbankgeber, west-bank,
westbank, westjordanien, westjordanland, westjordanufer, west-
ufer, zentralbank
Table 2: All German translation candidates for bank as
extracted from the gold standard
In this way, for the English lemma bank, for ex-
ample, the CL-WSD trial gold standard for German
contains the word Bank itself, together with 40 other
translation candidates, as shown in Table 2. Eight
of those are related to river banks (Ufer, but also,
e.g., Westbank and Westjordanland), three concern
databases (Datenbank), and one is for blood banks.
The rest are connected to different types of finan-
cial institutions (such as Handelsbank and Finanz-
institut, but also by association Konto, Weldbank-
geber, Banknote, Geldschein, Kredit, etc.).
2.2 N-Gram Context Matching
N-gram matching is used to produce a ranked list
of translation candidates and their contexts, both in
order to provide robustness and to give a baseline
performance. The n-gram models were built using
the IRSTLM toolkit (Federico et al, 2008; Bungum
and Gamba?ck, 2012) on the DeWaC corpus (Baroni
and Kilgarriff, 2006), using the stopword list from
NLTK (Loper and Bird, 2002). The n-gram match-
ing procedure consists of two steps:
1. An nth order source context is extracted and the
translations for each SL word in this context
are retrieved from the dictionary. This includes
stopword filtering of the context.
2. All relevant n-grams are inspected in order
from left to right and from more specific (5-
grams) to least specific (single words).
For each part of the context with matching n-grams
in the target language model, the appropriate target
translation candidates are extracted and ranked ac-
cording to their language model probability. This
results in an n-best list of translation candidates.
23
Since dictionary entries are lemma-based, lemma-
tization was necessary to use this approach in com-
bination with the dictionary enhancements. The
source context is formed by the lemmata in the sen-
tence surrounding the focus word (the word to be
disambiguated) by a window of up to four words
in each direction, limited by a 5-gram maximum
length. In order to extract the semantically most rel-
evant content, stopwords are removed before con-
structing this source word window. For each of the
1?5 lemmata in the window, the relevant translation
candidates are retrieved from the bilingual dictio-
nary. The candidates form the ordered translation
context for the source word window.
The following example illustrates how the trans-
lation context is created for the focus word ?bank?.
First the relevant part of the source language sen-
tence with the focus word in bold face:
(1) The BIS could conclude stand-by credit
agreements with the creditor countries? cen-
tral bank if they should so request.
For example, using a context of two words in front
and two words after the focus word, the following
source language context is obtained after a prepro-
cessing involving lemmatization, stopword removal,
and insertion of sentence start (<s>) and end mark-
ers (</s>):
(2) country central bank request </s>
From this the possible n-grams in the target side con-
text are generated by assembling all ordered com-
binations of the translations of the source language
words for each context length: the widest contexts
(5-grams) are looked up first before moving on to
narrower contexts, and ending up with looking up
only the translation candidate in isolation.
Each of the n-grams is looked up in the language
model and for each context part the n-grams are or-
dered according to their language model probability.
Table 3 shows a few examples of such generated n-
grams with their corresponding scores from the n-
gram language model.1 The target candidates (ital-
ics) are then extracted from the ordered list of target
language n-grams. This gives an n-best list of trans-
1There are no scores for 4- and 5-grams; as expected when
using direct translation to generate target language n-grams.
n n-gram LM score
5 land mittig bank nachsuchen </s> Not found
4 mittig bank nachsuchen </s> Not found
3 mittig bank nachsuchen Not found
3 kredit anfragen </s> -0.266291
2 mittig bank -3.382560
2 zentral blutbank -5.144870
1 bank -3.673000
Table 3: Target language n-gram examples from look-
ups of stopword-filtered lemmata country central bank
request reported in log scores. The first 3 n-grams were
not found in the language model.
lation candidates from which the top-1 or top-5 can
be taken. Since multiple senses in the dictionary can
render the same literal output, duplicate translation
candidates are filtered out from the n-best list.
2.3 Dictionary-Based Context Matching
After creating the n-gram based list of translation
candidates, additional candidates are produced by
looking at multiword entries in a bilingual dictio-
nary. The existence of multiword entries in the dic-
tionary corresponding to adjacent lemmata in the
source context or translation candidates in the target
context is taken as a clear indicator for the suitability
of a particular translation candidate. Such entries are
added to the top of the n-best list, which represents
a strong preference in the disambiguation system.
Dictionaries are used in all experiments to look up
translation candidates and target language transla-
tions of the words in the context, but this approach is
mining the dictionaries by using lookups of greater
length. Thus is, for example, the dictionary entry
Community Bank translated to the translation candi-
date Commerzbank; this translation candidate would
be put on top of the list of prioritized answers.
Two separate procedures are used to find such in-
dicators, a direct procedure based on the source con-
text and an indirect procedure based on the weaker
target language context. These are detailed in pseu-
docode in Algorithms 1 and 2, and work as follows:
Source Language (SL) Method (Algorithm 1)
If there is a dictionary entry for the source word
and one of its adjacent words, search the set
of translations for any of the translation candi-
dates for the word alone. Specifically, transla-
24
Algorithm 1 SL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: comblemmas? list(previouslemma(b) + b, b + nextlemma(b)) . Find adjacent lemmata
3: for lem ? comblemmas do
4: c? sl-dictionary-lookup(lem) . Look up lemma in SL?TL dict.
5: if c ? tcands then rlist? list(c + rlist) . Push lookup result c onto rlist if in tcands
6: end if
7: end for
8: return rlist . Return new list with lemmata whose translations were in tcands on top
9: end procedure
Algorithm 2 TL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
[The ready-made TL tcands from the dataset are looked up in TL-SL direction. It is necessary to keep a list of the
reverse-translation of the individual tcand as well as the original tcand itself, in order to monitor which tcand it was.
If the SL context is found in either of these reverse lookups the matching tcand is ranked high.]
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: for cand ? tcands do . Assemble list of TL translations
3: translist? list(cand, tl-dictionary-lookup(cand)) + translist
4: . Append TL?SL lookup results of tcands with cand as id
5: end for
6: for cand, trans ? translist do
7: if previouslemma(b)?nextlemma(b) ? trans then . If trans contains either SL lemma
8: rlist? list(cand) + rlist . append this cand onto rlist
9: end if
10: end for
11: return rlist
12: . Return tcands list; top-ranking tcands whose SL-neighbours were found in TL?SL lookup
13: end procedure
tions of the combination of the source word and
an adjacent word in the context are matched
against translation candidates for the word.
Target Language (TL) Method (Algorithm 2)
If a translation candidate looked up in the re-
verse direction matches the source word along
with one or more adjacent words, it is a good
translation candidate. TL candidates are looked
up in a TL?SL dictionary and multiword results
are matched against SL combinations of disam-
biguation words and their immediate contexts.
For both methods the dictionary entry for the tar-
get word or translation candidate is matched against
the immediate context. Thus both methods result
in two different lookups for each focus word, com-
bining it with the previous and next terms, respec-
tively. This is done exhaustively for all combina-
tions of translations of the words in the context win-
dow. Only one adjacent word was used, since very
few of the candidates were able to match the context
even with one word. Hence, virtually none would
be found with more context, making it very unlikely
that larger contexts would contribute to the disam-
biguation procedure, as wider matches would also
match the one-word contexts.
Also for both methods, translation candidates are
only added once, in case the same translation candi-
date generates hits with either (or both) of the meth-
ods. Looking at the running example, stopword fil-
tered and with lemmatized context:
(3) country central bank request
This example generates two source language multi-
word expressions, central bank and bank request. In
the source language method, these word combina-
25
tions are looked up in the dictionary where the zen-
tralbank entry is found for central bank, which is
also found as a translation candidate for bank.
The target language method works in the reverse
order, looking up the translation candidates in the
TL?SL direction and checking if the combined lem-
mata are among the candidates? translations into the
source language. In the example, the entry zentral-
bank:central bank is found in the dictionary, match-
ing the source language context, so zentralbank is
assumed to be a correct translation.
2.4 Dictionaries
Two English-German dictionaries were used in the
experiments, both with close to 1 million entries
(translations). One is a free on-line resource, while
the other was obtained by reversing an existing pro-
prietary German-English dictionary made available
to the authors by its owners:
? The GFAI dictionary (called ?D1? in Section 3
below) is a proprietary and substantially ex-
tended version of the Chemnitz dictionary, with
549k EN entries including 433k MWEs, and
552k DE entries (79k MWEs). The Chem-
nitz electronic German-English dictionary2 it-
self contains over 470,000 word translations
and is available under a GPL license.
? The freely available CC dictionary3 (?D2? be-
low) is an internet-based German-English and
English-German dictionary built through user
generated word definitions. It has 565k/440k
(total/MWE) EN and 548k/210k DE entries.
Note that the actual dictionaries are irrelevant to the
discussion at hand, and that we do not aim to point
out strengths or weaknesses of either dictionary, nor
to indicate a bias towards a specific resource.
3 Results
Experiments were carried out both on the trial and
test data described in Section 2.1 (5 trial and 20 test
words; with 20 resp. 50 instances for each word; in
total 1100 instances in need of disambiguation). The
results show that the dictionaries yield answers with
2http://dict.tu-chemnitz.de/
3http://www.dict.cc/
high precision, although they are robust enough to
solve the SemEval WSD challenge on their own.
For measuring the success rate of the developed
models, we adopt the ?Out-Of-Five? (OOF) score
(Lefever and Hoste, 2010b) from the SemEval?10
Cross-Lingual Word Sense Disambiguation task.
The Out-Of-Five criterion measures how well the
top five candidates from the system match the top
five translations in the gold standard:
OOF (i) =
?
a?Ai
freq i(a)
|Hi|
where Hi denotes the multiset of translations pro-
posed by humans for the focus word in each source
sentence si (1 ? i ? N , N being the number
of test items). Ai is the set of translations produced
by the system for source term i. Since each transla-
tion has an associated count of how many annotators
chose it, there is for each si a function freq i return-
ing this count for each term in Hi (0 for all other
terms), and max freq i gives the maximal count for
any term in Hi. For the first example in Table 1:
?
??????????
??????????
H1 = {bank, bank, bank, bank, zentralbank,
bankengesellschaft, kreditinstitut, finanzinstitut}
freq1(bank) = 4
. . .
freq1(finanzinstitut) = 1
maxfreq1 = 4
and the cardinality of the multiset is: |H1| = 8. This
equates to the sum of all top-3 preferences given to
the translation candidates by all annotators.
For the Out-Of-Five evaluation, the CL-WSD sys-
tems were allowed to submit up to five candidates
of equal rank. OOF is a recall-oriented measure
with no additional penalty for precision errors, so
there is no benefit in outputting less than five can-
didates. With respect to the previous example from
Table 1, the maximum score is obtained by system
output A1 = {bank, bankengesellschaft, kreditinstitut,
zentralbank, finanzinstitut}, which gives OOF (1) =
(4 + 1 + 1 + 1 + 1)/8 = 1, whereas A2 = {bank,
bankengesellschaft, nationalbank, notenbank, sparkasse}
would give OOF (1) = (4 + 1)/8 = 0.625.4
4Note that the maximum OOF score is not always 1 (i.e., it
is not normalized), since the gold standard sometimes contains
more than five translation alternatives.
26
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
Top 8.89 6.99 8.89 22.71 24.43 25.34 24.67
Low 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Mean 2.71 0.99 3.04 8.35 7.10 9.24 10.13
Table 4: F1-score results for individual dictionaries
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
coach 1.00 0.00 1.00 0.21 0.00 0.21 0.21
education 0.83 0.67 0.83 0.47 0.62 0.54 0.53
execution 0.00 0.00 0.00 0.17 0.22 0.17 0.17
figure 1.00 0.00 1.00 0.51 0.57 0.55 0.55
job 0.88 0.80 0.94 0.45 0.78 0.46 0.44
letter 1.00 0.00 1.00 0.66 0.75 0.62 0.66
match 1.00 1.00 1.00 0.80 0.50 0.80 0.80
mission 0.71 0.33 0.71 0.46 0.37 0.36 0.36
mood 0.00 0.00 0.00 0.00 0.00 0.00 0.00
paper 0.68 0.17 0.68 0.53 0.35 0.55 0.55
post 1.00 1.00 1.00 0.39 0.48 0.45 0.48
pot 0.00 0.00 0.00 1.00 1.00 1.00 1.00
range 1.00 1.00 1.00 0.28 0.37 0.30 0.30
rest 1.00 0.67 1.00 0.60 0.56 0.56 0.58
ring 0.09 0.00 0.09 0.37 0.93 0.38 0.38
scene 1.00 0.00 1.00 0.50 0.42 0.44 0.50
side 1.00 0.00 1.00 0.21 0.16 0.23 0.27
soil 1.00 0.00 1.00 0.72 0.58 0.66 0.69
strain 0.00 0.00 0.00 0.51 0.88 0.55 0.55
test 1.00 1.00 1.00 0.62 0.52 0.57 0.61
Mean 0.84 0.74 0.84 0.50 0.56 0.49 0.51
Table 5: Precision scores for all terms filtering out those
instances for which no candidates were suggested
For assessing overall system performance in
the experiments, we take the best (?Top?), worst
(?Low?), and average (?Mean?) of the OOF scores
for all the SL focus words, with F1-score reported
as the harmonic mean of the precision and recall of
the OOF scores. Table 4 shows results for each dic-
tionary approach on the test set, with ?D1? being
the GFAI dictionary, ?D2? the CC dictionary, and
?comb? the combination of both. Target language
look-up contributes more to providing good transla-
tion candidates than the source language methodol-
ogy, and also outperforms a strategy combining all
dictionaries in both directions (?All comb?).
Filtering out the instances for which no candi-
date translation was produced, and taking the aver-
age precision scores only over these, gives the re-
sults shown in Table 5. Markedly different preci-
sion scores can be noticed, but the source language
Source language Target language
Dictionary D1 D2 D1 D2
Mean 3.25 1.5 12.65 11.45
Total 223 256 1,164 880
Table 6: Number of instances with a translation candidate
(?Mean?) and the total number of suggested candidates
Most Most Freq 5-gram 5-gram All Dict VSM
Freq Aligned + Dict Comb Model
Top 51.77 68.71 52.02 52.74 24.67 55.92
Low 1.76 9.93 14.09 15.40 0.00 10.73
Mean 21.18 34.61 30.36 36.38 10.13 30.30
Table 7: Overview of results (F1-scores) on SemEval data
method again has higher precision on the sugges-
tions it makes than the target language counterpart.
As shown in Table 6, this higher precision is offset
by lower coverage, with far fewer instances actually
producing a translation candidate with the dictionary
lookup methods. There is a notable difference in the
precision of the SL and TL approaches, coinciding
with more candidates produced by the latter. Several
words in Table 5 give 100% precision scores for at
least one dictionary, while a few give 0% precision
for some dictionaries. The word ?mood? even has
0% precision for both dictionaries in both directions.
Table 7 gives an overview of different approaches
to word translation disambiguation on the dataset.
For each method, the three lines again give both
the best and worst scoring terms, and the mean
value for all test words. The maximum attainable
score for each of those would be 99.28, 90.48 and
95.47, respectively, but those are perfect scores not
reachable for all items, as described above (OOF-
scoring). Instead the columns Most Freq and Most
Freq aligned give the baseline scores for the Sem-
Eval dataset: the translation most frequently seen
in the corpus and the translation most frequently
aligned in a word-aligned parallel corpus (Europarl),
respectively. Then follows the results when using
only a stopword-filtered 5-gram model built with the
IRSTLM language modeling kit (Federico and Cet-
tolo, 2007), and when combining the 5-gram model
with the dictionary approach (5-gram + Dict).
The next column (All Dict Comb) shows how the
dictionary methods fared on their own. The com-
27
bined dictionary approach has low recall (see Ta-
ble 6) and does not alone provide a good solution to
the overall problem. Due to high precision, however,
the approach is able to enhance the n-gram method
that already produces acceptable results. Finally, the
column VSM Model as comparison gives the results
obtained when using a Vector Space Model for word
translation disambiguation (Marsi et al, 2011).
Comparing the dictionary approach to state-of-
the-art monolingual solutions to the WTD problem
on this dataset shows that the approach performs bet-
ter for the Lowest and Mean scores of the terms, but
not for the Top scores (Lynum et al, 2012). As can
be seen in Table 7, the vector space model produced
the overall best score for a single term. However, the
method combining a 5-gram language model with
the dictionary approach was best both at avoiding
really low scores for any single term and when com-
paring the mean scores for all the terms.
4 Discussion and Conclusion
The paper has presented a method for using dictio-
nary lookups based on the adjacent words in both
the source language text and target language candi-
date translation texts to disambiguate word transla-
tion candidates. By composing lookup words by us-
ing both neighbouring words, improved disambigua-
tion performance was obtained on the data from the
SemEval?10 English-German Cross-Lingual Word
Sense Disambiguation task. The extended use of
dictionaries proves a valuable source of informa-
tion for disambiguation, and can introduce low-cost
phrase-level translation to quantitative Word Sense
Disambiguation approaches such as N-gram or Vec-
tor Space Model methods, often lacking the phrases-
based dimension.
The results show clear differences between the
source and target language methods of using dictio-
nary lookups, where the former has very high preci-
sion (0.84) but low coverage, while the TL method
compensates lower precision (0.51) with markedly
better coverage. The SL dictionary method pro-
vided answers to only between 1.5 and 3.25 of 50
instances per word on average, depending on the dic-
tionary. This owes largely to the differences in algo-
rithms, where the TL method matches any adjacent
lemma to the focus word with the translation of the
pre-defined translation candidates, whereas the SL
method matches dictionaries of the combined lem-
mata of the focus word and its adjacent words to the
same list of translation candidates. False positives
are expected with lower constraints such as these.
On the SemEval data, the contribution of the dictio-
nary methods to the n-grams is mostly in improving
the average score.
The idea of acquiring lexical information from
corpora is of course not new in itself. So did, e.g.,
Rapp (1999) use vector-space models for the pur-
pose of extracting ranked lists of translation can-
didates for extending a dictionary for word trans-
lation disambiguation. Chiao and Zweigenbaum
(2002) tried to identify translational equivalences
by investigating the relations between target and
source language word distributions in a restricted
domain, and also applied reverse-translation filtering
for improved performance, while Sadat et al (2003)
utilised non-aligned, comparable corpora to induce
a bilingual lexicon, using a bidirectional method
(SL?TL, TL?SL, and a combination of both).
Extending the method to use an arbitrary size win-
dow around all words in the context of each focus
word (not just the word itself) could identify more
multiword expressions and generate a more accurate
bag-of-words for a data-driven approach. Differ-
ences between dictionaries could also be explored,
giving more weight to translations found in two or
more dictionaries. Furthermore, the differences be-
tween the SL and TL methods could explored fur-
ther, investigating in detail the consequences of us-
ing a symmetrical dictionary, in order to study the
effect that increased coverage has on results. Test-
ing the idea on more languages will help verify the
validity of these findings.
Acknowledgements
This research has received funding from NTNU and from
the European Community?s 7th Framework Programme
under contract nr 248307 (PRESEMT). Thanks to the
other project participants and the anonymous reviewers
for several very useful comments.
28
References
Bai, M.-H., You, J.-M., Chen, K.-J., and Chang,
J. S. (2009). Acquiring translation equivalences of
multiword expressions by normalized correlation
frequencies. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 478?486, Singapore. ACL.
Baroni, M. and Kilgarriff, A. (2006). Large
linguistically-processed web corpora for multiple
languages. In Proceedings of the 11th Conference
of the European Chapter of the Association for
Computational Linguistics, pages 87?90, Trento,
Italy. ACL.
Bungum, L. and Gamba?ck, B. (2012). Efficient n-
gram language modeling for billion word web-
corpora. In Proceedings of the 8th International
Conference on Language Resources and Evalua-
tion, pages 6?12, Istanbul, Turkey. ELRA. Work-
shop on Challenges in the Management of Large
Corpora.
Burnard, L., editor (2007). Reference Guide for the
British National Corpus (XML Edition). BNC
Consortium, Oxford, England. http://www.
natcorp.ox.ac.uk/XMLedition/URG.
Caseli, H. d. M., Ramisch, C., das Grac?as
Volpe Nunes, M., and Villavicencio, A. (2010).
Alignment-based extraction of multiword expres-
sions. Language Resources and Evaluation, 44(1-
2):59?77. Special Issue on Multiword expression:
hard going or plain sailing.
Chiao, Y.-C. and Zweigenbaum, P. (2002). Look-
ing for candidate translational equivalents in spe-
cialized comparable corpora. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, volume 2, pages 1?5,
Philadelphia, Pennsylvania. ACL. Also published
in AMIA Annual Symposium 2002, pp. 150?154.
Federico, M., Bertoldi, N., and Cettolo, M. (2008).
Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages
1618?1621. ISCA.
Federico, M. and Cettolo, M. (2007). Efficient han-
dling of n-gram language models for statistical
machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 88?95, Prague, Czech
Republic. ACL. 2nd Workshop on Statistical Ma-
chine Translation.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the 10th Machine Translation Summit, pages 79?
86, Phuket, Thailand.
Lefever, E. and Hoste, V. (2010a). Construction
of a benchmark data set for cross-lingual word
sense disambiguation. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation, pages 1584?1590, Valetta, Malta.
ELRA.
Lefever, E. and Hoste, V. (2010b). SemEval-2010
Task 3: Cross-lingual word sense disambiguation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
15?20, Uppsala, Sweden. ACL. 5th International
Workshop on Semantic Evaluation.
Loper, E. and Bird, S. (2002). NLTK: the natu-
ral language toolkit. In Proceedings of the ACL-
02 Workshop on Effective tools and methodolo-
gies for teaching natural language processing and
computational linguistics - Volume 1, ETMTNLP
?02, pages 63?70, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
LREC06 (2006). Proceedings of the 5th Interna-
tional Conference on Language Resources and
Evaluation, Genova, Italy. ELRA.
Lynum, A., Marsi, E., Bungum, L., and Gamba?ck,
B. (2012). Disambiguating word translations with
target language models. In Proceedings of the
15th International Conference on Text, Speech
and Dialogue, pages 378?385, Brno, Czech Re-
public. Springer.
Marsi, E., Lynum, A., Bungum, L., and Gamba?ck,
B. (2011). Word translation disambiguation with-
out parallel texts. In Proceedings of the Inter-
national Workshop on Using Linguistic Informa-
tion for Hybrid Machine Translation, pages 66?
74, Barcelona, Spain.
Rapp, R. (1999). Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, Madrid, Spain. ACL.
29
Ren, Z., Lu?, Y., Cao, J., Liu, Q., and Huang, Y.
(2009). Improving statistical machine translation
using domain bilingual multiword expressions. In
Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics, pages
47?54, Singapore. ACL. Workshop on Multiword
Expressions: Identification, Interpretation, Dis-
ambiguation and Applications.
Sadat, F., Yoshikawa, M., and Uemura, S. (2003).
Learning bilingual translations from comparable
corpora to cross-language information retrieval:
Hybrid statistics-based and linguistics-based ap-
proach. In Proceedings of the 41th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 57?64, Sapporo, Japan. ACL. 6th
International Workshop on Information Retrieval
with Asian languages; a shorter version published
in ACL Annual Meeting 2003, pp. 141?144.
Sag, I., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. (2002). Multiword expressions:
A pain in the neck for NLP. In Gelbukh, A.,
editor, Computational Linguistics and Intelligent
Text Processing: Proceedings of the 3rd Interna-
tional Conference, number 2276 in Lecture Notes
in Computer Science, pages 189?206, Mexico
City, Mexico. Springer-Verlag.
Sharoff, S., Babych, B., and Hartley, A. (2006). Us-
ing collocations from comparable corpora to find
translation equivalents. In LREC06 (2006), pages
465?470.
Sofianopoulos, S., Vassiliou, M., and Tambouratzis,
G. (2012). Implementing a language-independent
MT methodology. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1?10, Jeju, Korea. ACL.
First Workshop on Multilingual Modeling.
Steinberger, R., Pouliquen, B., Widiger, A., Ignat,
C., Erjavec, T., Tufis?, D., and Varga, D. (2006).
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In LREC06 (2006),
pages 2142?2147.
30
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 83?90,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Towards Dynamic Word Sense Discrimination with Random Indexing
Hans Moen, Erwin Marsi, Bjo?rn Gamba?ck
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{hansmoe,emarsi,gamback}@idi.ntnu.no
Abstract
Most distributional models of word sim-
ilarity represent a word type by a single
vector of contextual features, even though,
words commonly have more than one
sense. The multiple senses can be captured
by employing several vectors per word in a
multi-prototype distributional model, pro-
totypes that can be obtained by first con-
structing all the context vectors for the
word and then clustering similar vectors
to create sense vectors. Storing and clus-
tering context vectors can be expensive
though. As an alternative, we introduce
Multi-Sense Random Indexing, which per-
forms on-the-fly (incremental) clustering.
To evaluate the method, a number of mea-
sures for word similarity are proposed,
both contextual and non-contextual, in-
cluding new measures based on optimal
alignment of word senses. Experimental
results on the task of predicting semantic
textual similarity do, however, not show
a systematic difference between single-
prototype and multi-prototype models.
1 Introduction
Many terms have more than one meaning, or
sense. Some of these senses are static and can
be listed in dictionaries and thesauri, while other
senses are dynamic and determined by the con-
texts the terms occur in. Work in Word Sense Dis-
ambiguation often concentrate on the static word
senses, making the task of distinguishing between
them one of classification into a predefined set of
classes (i.e., the given word senses); see, e.g., Erk
et al (2013; Navigli (2009) for overviews of cur-
rent work in the area. The idea of fixed generic
word senses has received a fair amount of criti-
cism in the literature (Kilgarriff, 2000).
This paper instead primarily investigates dy-
namically appearing word senses, word senses that
depend on the actual usage of a term in a cor-
pus or a domain. This task is often referred to as
Word Sense Induction or Word Sense Discrimina-
tion (Schu?tze, 1998). This is, in contrast, essen-
tially a categorisation problem, distinguished by
different senses being more or less similar to each
other at a given time, given some input data. The
dividing line between Word Sense Disambigua-
tion and Discrimination is not necessarily razor
sharp though: also different senses of a term listed
in a dictionary tend to have some level of overlap.
In recent years, distributional models have been
widely used to infer word similarity. Most such
models represent a word type by a single vector of
contextual features obtained from co-occurrence
counts in large textual corpora. By assigning a
single vector to each term in the corpus, the re-
sulting model assumes that each term has a fixed
semantic meaning (relative to all the other terms).
However, due to homonomy and polysemy, word
semantics cannot be adequately represented by a
single-prototype vector.
Multi-prototype distributional models in con-
trast employ different vectors to represent different
senses of a word (Reisinger and Mooney, 2010).
Multiple prototypes can be obtained by first con-
structing context vectors for all words and then
clustering similar context vectors to create a sense
vector. This may be expensive, as vectors need to
stored and clustered. As an alternative, we propose
a new method called Multi-Sense Random Index-
ing (MSRI), which is based on Random Indexing
(Kanerva et al, 2000) and performs an on-the-fly
(incremental) clustering.
MSRI is a method for building a multi-
prototype / multi-sense vector space model, which
attempts to capture one or more senses per unique
term in an unsupervised manner, where each sense
is represented as a separate vector in the model.
83
This differs from the classical Random Indexing
(RI) method which assumes a static sense inven-
tory by restricting each term to have only one vec-
tor (sense) per term, as described in Section 2. The
MSRI method is introduced in Section 3.
Since the induced dynamic senses do not neces-
sarily correspond to the traditional senses distin-
guished by humans, we perform an extrinsic eval-
uation by applying the resulting models to data
from the Semantic Textual Similarity shared task
(Agirre et al, 2013), in order to compare MSRI
to the classical RI method. The experimental set-
up is the topic of Section 4, while the results of
the experiments are given in Section 5. Section 6
then sums up the discussion and points to ways in
which the present work could be continued.
2 Vector Space Models
With the introduction of LSA, Latent Semantic
Analysis (Deerwester et al, 1990), distributed
models of lexical semantics, built from unla-
belled free text data, became a popular sub-field
within the language processing research commu-
nity. Methods for building such semantic mod-
els rely primarily on term co-occurrence infor-
mation, and attempt to capture latent relations
from analysing large amounts of text. Most of
these methods represent semantic models as multi-
dimensional vectors in a vector space model.
After LSA, other methods for building seman-
tic models have been proposed, one of them being
Random Indexing (Kanerva et al, 2000). Com-
mon to these methods is that they generate a con-
text vector for each unique term in the training data
which represents the term?s ?contextual? meaning
in the vector space. By assigning a single con-
text vector to each term in the corpus, the resulting
model assumes that each term has a fixed semantic
meaning (relative to all other terms).
Random Indexing incrementally builds a co-
occurrence matrix of reduced dimensionality, by
first assigning index vectors to each unique term.
The vectors are of a predefined size (typically
around 1000), and consist of a few randomly
placed 1s and -1s. Context vectors of the same size
are also assigned to each term, initially consisting
of only zeros. When traversing a document corpus
using a sliding window of a fixed size, the context
vectors are continuously updated: the term in the
centre of the window (the target term), has the in-
dex vectors of its neighbouring terms (the ones in
the window) added to its context vector using vec-
tor summation. Then the cosine similarity mea-
sure can be used on term pairs to calculate their
similarity (or ?contextual similarity?).
Random Indexing has achieved promising re-
sults in various experiments, for example, on the
TOEFL test (?Test of English as a Foreign Lan-
guage?) (Kanerva et al, 2000). However, it is ev-
ident that many terms have more than one mean-
ing or sense, some being static and some dynamic,
that is, determined by the contexts the terms occur
in. Schu?tze (1998) proposed a method for clus-
tering the contextual occurrences of terms into in-
dividual ?prototype? vectors, where one term can
have multiple prototype vectors representing sep-
arate senses of the term. Others have adopted
the same underlying idea, using alternative meth-
ods and techniques (Reisinger and Mooney, 2010;
Huang et al, 2012; Van de Cruys et al, 2011; Dinu
and Lapata, 2010).
3 Multi-Sense Random Indexing, MSRI
Inspired by the work of Schu?tze (1998) and
Reisinger and Mooney (2010), this paper intro-
duces a novel variant of Random Indexing, which
we have called ?Multi-Sense Random Indexing?.
MSRI attempts to capture one or more senses per
unique term in an unsupervised and incremental
manner, each sense represented as an separate vec-
tor in the model. The method is similar to classical
sliding window RI, but each term can have mul-
tiple context vectors (referred to as sense vectors
here) which are updated separately.
When updating a term vector, instead of directly
adding the index vectors of the neighbouring terms
in the window to its context vector, the system first
computes a separate window vector consisting of
the sum of the index vectors. The similarity be-
tween the window vector and each of the term?s
sense vectors is calculated. Each similarity score
is then compared to a pre-set similarity threshold:
? if no score exceeds the threshold, the window
vector becomes a new separate sense vector
for the term,
? if exactly one score is above the threshold,
the window vector is added to that sense vec-
tor, and
? if multiple scores are above the threshold, all
the involved senses are merged into one sense
vector, together with the window vector.
84
Algorithm 1 MSRI training
for all terms t in a document D do
generate window vector ~win from the neigh-
bouring words? index vectors
for all sense vectors ~si of t do
sim(si) = CosSim( ~win,~si)
end for
if sim(si..k) ? ? then
Merge ~si..k and ~win through summing
else
if sim(si) ? ? then
~si+ = ~win
end if
else
if sim(si..n) < ? then
Assign ~win as new sense vector of t
end if
end if
end for
See Algorithm 1 for a pseudo code version. Here
? represents the similarity threshold.
This accomplishes an incremental (on-line)
clustering of senses in an unsupervised manner,
while retaining the other properties of classical RI.
Even though the algorithm has a slightly higher
complexity than classical RI, this is mainly a mat-
ter of optimisation, which is not the focus of this
paper. The incremental clustering that we apply
is somewhat similar to what is used by Lughofer
(2008), although we are storing in memory only
one element (i.e., vector) for each ?cluster? (i.e.,
sense) at any given time.
When looking up a term in the vector space, a
pre-set sense-frequency threshold is applied to fil-
ter out ?noisy? senses. Hence, senses that have
occurred less than the threshold are not included
when looking up a term and its senses for, for ex-
ample, similarity calculations.
As an example of what the resulting models
contain in terms of senses, Table 1 shows four dif-
ferent senses of the term ?round? produced by the
MSRI model. Note that these senses do not nec-
essarily correspond to human-determined senses.
The idea is only that using multiple prototype
vectors facilitates better modelling of a term?s
meaning than a single prototype (Reisinger and
Mooney, 2010).
round1 round2 round3 round4
finish camping inch launcher
final restricted bundt grenade
match budget dough propel
half fare thick antitank
third adventure cake antiaircraft
Table 1: Top-5 most similar terms for four dif-
ferent senses of ?round? using the Max similarity
measure to the other terms in the model.
3.1 Term Similarity Measures
Unlike classical RI, which only has a single con-
text vector per term and thus calculates similarity
between two terms directly using cosine similarity,
there are multiple ways of calculating the similar-
ity between two terms in MSRI. Some alternatives
are described in Reisinger and Mooney (2010). In
the experiment in this paper, we test four ways of
calculating similarity between two terms t and t?
in isolation, with the Average and Max methods
stemming from Reisinger and Mooney (2010).
Let ~si..n and ~s?j..m be the sets of sense vectors
corresponding to the terms t and t? respectively.
Term similarity measures are then defined as:
Centroid
For term t, compute its centroid vector by
summing its sense vectors ~si..n. The same is
done for t? with its sense vectors ~s?j..m. These
centroids are in turn used to calculate the co-
sine similarity between t and t?.
Average
For all ~si..n in t, find the pair ~si, ~s?j with high-
est cosine similarity:
1
n
n?
i=1
CosSimmax(~si, ~s?j)
Then do the same for all ~s?j..m in t?:
1
m
m?
j=1
CosSimmax(~s?j , ~si)
The similarity between t and t? is computed
as the average of these two similarity scores.
Max
The similarity between ti and t?i equals the
similarity of their most similar sense:
Sim(t, t?) = CosSimmaxij (~si, ~s?i)
85
Hungarian Algorithm
First cosine similarity is computed for each
possible pair of sense vectors ~si..n and ~s?j..m,
resulting in a matrix of similarity scores.
Finding the optimal matching from senses ~si
to ~s?j that maximises the sum of similarities
is known as the assignment problem. This
combinatorial optimisation problem can be
solved in polynomial time through the Hun-
garian Algorithm (Kuhn, 1955). The over-
all similarity between terms t and t? is then
defined as the average of the similarities be-
tween their aligned senses.
All measures defined so far calculate similarity be-
tween terms in isolation. In many applications,
however, terms occur in a particular context that
can be exploited to determine their most likely
sense. Narrowing down their possible meaning to
a subset of senses, or a single sense, can be ex-
pected to yield a more adequate estimation of their
similarity. Hence a context-sensitive measure of
term similarity is defined as:
Contextual similarity
Let ~C and ~C ? be vectors representing the con-
texts of terms t and t? respectively. These
context vectors are constructed by summing
the index vectors of the neighbouring terms
within a window, following the same proce-
dure as used when training the MSRI model.
We then find s? and s? ? as the sense vectors
best matching the context vectors:
s? = argmaxi CosSim(~si, ~C)
s? ? = argmaxj CosSim(~sj , ~C ?)
Finally, contextual similarity is defined as the
similarity between these sense vectors:
Simcontext(t, t
?) = CosSim(s?, s? ?)
3.2 Sentence Similarity Features
In the experiments reported on below, a range of
different ways to represent sentences were tested.
Sentence similarity was generally calculated by
the average of the maximum similarity between
pairs of terms from both sentences, respectively.
The different ways of representing the data in
combination with some sentence similarity mea-
sure will here be referred to as similarity features.
1. MSRI-TermCentroid:
In each sentence, each term is represented as
the sum of its sense vectors. This is similar
to having one context vector, as in classical
RI, but due to the sense-frequency filtering,
potentially ?noisy? senses are not included.
2. MSRI-TermMaxSense:
For each bipartite term pair in the two sen-
tences, their sense-pairs with maximum co-
sine similarity are used, one sense per term.
3. MSRI-TermInContext:
A 5 + 5 window around each (target) term
is used as context for selecting one sense of
the term. A window vector is calculated by
summing the index vectors of the other terms
in the window (i.e., except for the target term
itself). The sense of the target term which is
most similar to the window vector is used as
the representation of the term.
4. MSRI-TermHASenses:
Calculating similarity between two terms is
done by applying the Hungarian Algorithm
to all their bipartite sense pairs.
5. RI-TermAvg:
Classical Random Indexing ? each term is
represented as a single context vector.
6. RI-TermHA:
Similarity between two sentences is calcu-
lated by applying the Hungarian Algorithm to
the context vectors of each constituent term.
The parameters were selected based on a com-
bination of surveying previous work on RI (e.g.,
Sokolov (2012)), and by analysing how sense
counts evolved during training. For MSRI, we
used a similarity threshold of 0.2, a vector dimen-
sionality of 800, a non-zero count of 6, and a win-
dow size of 5 + 5. Sense vectors resulting from
less than 50 observations were removed. For clas-
sical RI, we used the same parameters as for MSRI
(except for a similarity threshold).
4 Experimental Setup
In order to explore the potential of the MSRI
model and the textual similarity measures pro-
posed here, experiments were carried out on data
from the Semantic Textual Similarity (STS) shared
task (Agirre et al, 2012; Agirre et al, 2013).
86
Given a pair of sentences, systems participating
in this task shall compute how semantically sim-
ilar the two sentences are, returning a similar-
ity score between zero (completely unrelated) and
five (completely semantically equivalent). Gold
standard scores are obtained by averaging multi-
ple scores obtained from human annotators. Sys-
tem performance is then evaluated using the Pear-
son product-moment correlation coefficient (?) be-
tween the system scores and the human scores.
The goal of the experiments reported here was
not to build a competitive STS system, but rather
to investigate whether MSRI can outperform clas-
sical Random Indexing on a concrete task such as
computing textual similarity, as well as to identify
which similarity measures and meaning represen-
tations appear to be most suitable for such a task.
The system is therefore quite rudimentary: a sim-
ple linear regression model is fitted on the training
data, using a single sentence similarity measure
as input and the similarity score as the dependent
variable. The implementations of RI and MSRI
are based on JavaSDM (Hassel, 2004).
As data for training random indexing models,
we used the CLEF 2004?2008 English corpus,
consisting of approximately 130M words of news-
paper articles (Peters et al, 2004). All text was
tokenized and lemmatized using the TreeTagger
for English (Schmid, 1994). Stopwords were re-
moved using a customized version of the stoplist
provided by the Lucene project (Apache, 2005).
Data for fitting and evaluating the linear re-
gression models came from the STS development
and test data, consisting of sentence pairs with
a gold standard similarity score. The STS 2012
development data stems from the Microsoft Re-
search Paraphrase corpus (MSRpar, 750 pairs),
the Microsoft Research Video Description cor-
pus (MSvid, 750 pairs), and statistical machine
translation output based on the Europarl corpus
(SMTeuroparl, 734 pairs). Test data for STS
2012 consists of more data from the same sources:
MSRpar (750 pairs), MSRvid (750 pairs) and
SMTeuroparl (459 pairs). In addition, different
test data comes from translation data in the news
domain (SMTnews, 399 pairs) and ontology map-
pings between OntoNotes and WordNet (OnWN,
750 pairs). When testing on the STS 2012 data, we
used the corresponding development data from the
same domain for training, except for OnWN where
we used all development data combined.
The development data for STS 2013 consisted
of all development and test data from STS 2012
combined, whereas test data comprised machine
translation output (SMT, 750 pairs), ontology
mappings both between WordNet and OntoNotes
(OnWN, 561 pairs) and between WordNet and
FrameNet (FNWN, 189 pairs), as well as news ar-
ticle headlines (HeadLine, 750 pairs). For sim-
plicity, all development data combined were used
for fitting the linear regression model, even though
careful matching of development and test data sets
may improve performance.
5 Results and Discussion
Table 2 shows Pearson correlation scores per fea-
ture on the STS 2012 test data using simple linear
regression. The most useful features for each data
set are marked in bold. For reference, the scores of
the best performing STS systems for each data set
are also shown, as well as baseline scores obtained
with a simple normalized token overlap measure.
There is large variation in correlation scores,
ranging from 0.77 down to 0.27. Part of this vari-
ation is due to the different nature of the data sets.
For example, sentence similarity in the SMT do-
main seems harder to predict than in the video
domain. Yet there is no single measure that ob-
tains the highest score on all data sets. There is
also no consistent difference in performance be-
tween the RI and MSRI measures, which seem
to yield about equal scores on average. The
MSRI-TermInContext measure has the low-
est score on average, suggesting that word sense
disambiguation in context is not beneficial in its
current implementation.
The corresponding results on the STS 2013 test
data are shown in Table 3. The same observations
as for the STS 2012 data set can be made: again
there was no consistent difference between the RI
and MSRI features, and no single best measure.
All in all, these results do not provide any ev-
idence that MSRI improves on standard RI for
this particular task (sentence semantic similarity).
Multi-sense distributional models have, however,
been found to outperform single-sense models on
other tasks. For example, Reisinger and Mooney
(2010) report that multi-sense models significantly
increase the correlation with human similarity
judgements. Other multi-prototype distributional
models may yield better results than their single-
prototype counterparts on the STS task.
87
Features: MSRpar MSRvid SMTeuroparl SMTnews OnWN Mean
Best systems 0.73 0.88 0.57 0.61 0.71 0.70
Baseline 0.43 0.30 0.45 0.39 0.59 0.43
RI-TermAvg 0.44 0.71 0.50 0.42 0.65 0.54
RI-TermHA 0.41 0.72 0.44 0.35 0.56 0.49
MSRI-TermCentroid 0.45 0.73 0.50 0.33 0.64 0.53
MSRI-TermHASenses 0.40 0.77 0.47 0.39 0.68 0.54
MSRI-TermInContext 0.33 0.55 0.36 0.27 0.42 0.38
MSRI-TermMaxSense 0.44 0.71 0.50 0.32 0.64 0.52
Table 2: Pearson correlation scores per feature on STS 2012 test data using simple linear regression
Feature Headlines SMT FNWN OnWN Mean
Best systems 0.78 0.40 0.58 0.84 0.65
Baseline 0.54 0.29 0.21 0.28 0.33
RI-TermAvg 0.60 0.37 0.21 0.52 0.42
RI-TermHA 0.65 0.36 0.27 0.52 0.45
MSRI-TermCentroid 0.60 0.35 0.37 0.45 0.44
MSRI-TermHASenses 0.63 0.35 0.33 0.54 0.46
MSRI-TermInContext 0.20 0.29 0.19 0.36 0.26
MSRI-TermMaxSense 0.58 0.35 0.31 0.45 0.42
Table 3: Pearson correlation scores per feature on STS 2013 test data using simple linear regression
Notably, the more advanced features used in our
experiment, such as MSRI-TermInContext,
gave very clearly inferior results when compared
to MSRI-TermHASenses. This suggests that
more research on MSRI is needed to understand
how both training and retrieval can be fully uti-
lized and optimized.
6 Conclusion and Future Work
The paper introduced a new method called Multi-
Sense Random Indexing (MSRI), which is based
on Random Indexing and performs on-the-fly
clustering, as an efficient way to construct multi-
prototype distributional models for word similar-
ity. A number of alternative measures for word
similarity were proposed, both context-dependent
and context-independent, including new measures
based on optimal alignment of word senses us-
ing the Hungarian algorithm. An extrinsic eval-
uation was carried out by applying the resulting
models to the Semantic Textual Similarity task.
Initial experimental results did not show a sys-
tematic difference between single-prototype and
multi-prototype models in this task.
There are many questions left for future work.
One of them is how the number of senses per word
evolves during training and how the distribution
of senses in the final model looks like. So far we
only know that on average the number of senses
keeps growing with more training material, cur-
rently resulting in about 5 senses per word at the
end of training (after removing senses with fre-
quency below the sense-frequency threshold). It
is worth noting that this depends heavily on the
similarity threshold for merging senses, as well as
on the weighting schema used.
In addition there are a number of model para-
meters that have so far only been manually tuned
on the development data, such as window size,
number of non-zeros, vector dimensionality, and
the sense frequency filtering threshold. A system-
atic exploration of the parameter space is clearly
desirable. Another thing that would be worth
looking into, is how to compose sentence vectors
and document vectors from the multi-sense vector
space in a proper way, focusing on how to pick
the right senses and how to weight these. It would
also be interesting to explore the possibilities for
combining the MSRI method with the Reflective
Random Indexing method by Cohen et al (2010)
in an attempt to model higher order co-occurrence
relations on sense level.
The fact that the induced dynamic word senses
do not necessarily correspond to human-created
senses makes evaluation in traditional word sense
disambiguation tasks difficult. However, correla-
88
tion to human word similarity judgement may pro-
vide a way of intrinsic evaluation of the models
(Reisinger and Mooney, 2010). The Usim bench
mark data look promising for evaluation of word
similarity in context (Erk et al, 2013).
It is also worth exploring ways to optimise the
algorithm, as this has not been the focus of our
work so far. This would also allow faster training
and experimentation on larger text corpora, such
as Wikipedia. In addition to the JavaSDM pack-
age (Hassel, 2004), Lucene (Apache, 2005) with
the Semantic Vectors package (Widdows and Fer-
raro, 2008) would be an alternative framework for
implementing the proposed MSRI algorithm.
Acknowledgements
This work was partly supported by the Re-
search Council of Norway through the EviCare
project (NFR project no. 193022) and by the
European Community?s Seventh Framework Pro-
gramme (FP7/20072013) under grant agreement
nr. 248307 (PRESEMT). Part of this work has
been briefly described in our contribution to the
STS shared task (Marsi et al, 2013).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, pages 385?393, Montreal, Canada,
June. Association for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Apache. 2005. Apache Lucene open source package.
http://lucene.apache.org/.
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2010. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43(2):240?256, April.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162?1172,
Cambridge, Massachusetts, October. Association for
Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2013. Measuring word meaning in context. Com-
putational Linguistics, 39(3):501?544.
Martin Hassel. 2004. JavaSDM package. http:
//www.nada.kth.se/?xmartin/java/.
School of Computer Science and Communication;
Royal Institute of Technology (KTH); Stockholm,
Sweden.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 873?
882, Jeju Island, Korea. Association for Computa-
tional Linguistics.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
page 1036, Philadelphia, Pennsylvania. Erlbaum.
Adam Kilgarriff. 2000. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83?97.
Edwin Lughofer. 2008. Extensions of vector quantiza-
tion for incremental clustering. Pattern Recognition,
41(3):995?1011, March.
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,
Bjo?rn Gamba?ck, and Andre? Lynum. 2013. NTNU-
CORE: Combining strong features for semantic sim-
ilarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 66?73, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Roberto Navigli. 2009. Word Sense Disambiguation:
a survey. ACM Computing Surveys, 41(2):1?69.
Carol Peters, Paul Clough, Julio Gonzalo, Gareth J.F.
Jones, Michael Kluck, and Bernardo Magnini, ed-
itors. 2004. Multilingual Information Access
for Text, Speech and Images, 5th Workshop of the
Cross-Language Evaluation Forum, CLEF 2004,
volume 3491 of Lecture Notes in Computer Science.
Springer-Verlag, Bath, England.
89
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109?117, Los Angeles, California, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
1st International Conference on New Methods in
Natural Language Processing, pages 44?49, Univer-
sity of Manchester Institute of Science and Technol-
ogy, Manchester, England, September.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123, March.
Artem Sokolov. 2012. LIMSI: learning semantic
similarity by selecting random word subsets. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, pages 543?546, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1012?1022, Edinburgh, Scotland,
July. Association for Computational Linguistics.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic vectors: a scalable open source package and
online technology management application. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), pages 1183?
1190, Marrakech, Morocco.
90
