Efficient parsing strategies for syntactic analysis of closed captions 
Krzysz to f  Czuba  
kczuba@cs, cmu. edu 
Language Technologies Inst i tute 
Carnegie Mellon University 
P i t tsburgh,  PA 15213, USA 
Abst ract  
We present an efficient multi-level chart parser that 
was designed for syntactic analysis of closed captions 
(subtitles) in a real-time Machine Translation (MT) 
system. In order to achieve high parsing speed, we 
divided an existing English grammar into multiple 
levels. The parser proceeds in stages. At each stage, 
rules corresponding to only one level are used. A 
constituent pruning step is added between levels to 
insure that constituents not likely to be part of the 
final parse are removed. This results in a significant 
parse time and ambiguity reduction. Since the do- 
main is unrestricted, out-of-coverage sentences are to 
be expected and the parser might not produce a sin- 
gle analysis panning the whole input. Despite the 
incomplete parsing strategy and the radical prun- 
ing, the initial evaluation results how that the loss 
of parsing accuracy is acceptable. The parsing time 
favorable compares with a Tomita parser and a chart 
parser parsing time when run on the same grammar 
and lexicon. 
1 In t roduct ion  
In this paper we present on-going research on pars- 
ing closed captions (subtitles) from a news broad- 
cast. The research as been conducted as part of an 
effort to build a prototype of a real-time Machine 
Translation (MT) system translating news captions 
from English into Cantonese (Nyberg and Mita- 
mura, 1997). We describe an efficient multi-level 
chart parser that was designed to handle the kind 
of language used in our domain within a time that 
allows for a real-time automatic translation. In or- 
der to achieve high parsing speed, we divided an 
existing English grammar into multiple levels. The 
parser proceeds in stages. At each stage, rules cor- 
responding to only one level are used. A constituent 
pruning step is added between levels to insure that 
constituents not likely to be part of the final parse 
are removed. This results in a significant parse time 
and ambiguity reduction. Since the domain is un- 
restricted, out-of-coverage s ntences are to be ex- 
pected and the parser might not produce a single 
analysis panning the whole input. Thus, the set of 
final constituents has to be extracted from the chart. 
Despite the incomplete parsing strategy and the rad- 
ical pruning, the initial evaluation results how that 
the loss of parsing accuracy is acceptable. The pars- 
ing time favorable compares with a Tomita parser 
and a chart parser parsing time when run on the 
same grammar and lexicon. 
The outline of the paper is as follows. In Section 2 
we describe the syntactic and semantic haracteris- 
tics of the input domain. Section 3 provides a short 
summary of previous published research. Section 
4 gives an overview of requirements on the parsing 
algorithm posed by our application. Section 5 de- 
scribes how the grammar was partitioned into lev- 
els. Section 6 describes the constituent pruning al- 
gorithm that we used. In Section 7 we present he 
method for extracting final constituents from the 
chart. Section 8 presents the results of an initial 
evaluation. Finally, we close with future research in 
Section 9. 
2 Capt ion ing  domain  
Translation of closed captions has been attempted 
before. (Popowich et al, 1997) describe a sys- 
tem that translates closed captions taken from 
North American prime tlme TV. In their approach, 
(Popowich et al, 1997) assume a shallow parsing 
method that proves effective in achieving broad sys- 
tem coverage required for translation of dosed cap- 
tions from, e.g., movies. As reported by (Popowich 
et al, 1997), the shallow analysis performed by the 
system combined with the transfer-based translation 
strategy results in a number of sentences that are un- 
derstandable only in conjunction with the broadcast 
images. The number of sentencesthat are translated 
incorrectly is also significant. 
The parsing scheme described below was used in 
a pilot Machine Translation system for translation 
of news captions. The following requirements were 
posed: a) the translations should not be mislead- 
ing, b) they can be telegraphic since the input is 
often in a telegraphic style, c) partial translations 
are acceptable, d) if no correct translation can be 
produced then it is preferable to not output any. 
7 
The closed captions were taken from a financial news 
segment. Although the language in this segment is 
not strongly domain-restricted, it is centered around 
the financial aspects of the described events, which 
makes certain tasks such as sense disambiguation 
feasible. 
In order to address the translation quality prob- 
lems found by (Popowich et al, 1997), (Nyberg and 
Mitamura, 1997) propose a multi-engine MT sys- 
tem architechture to handle the task of translating 
closed captions. The parser described in this paper, 
was developed for the knowledge-based module (Ny- 
berg and Mitamura, 1992) in the system and it was 
required to produce "deep" analyses with the level 
of detail sufficient for creating interlingua. 
The stream of closed captions we worked with 
shows many interesting characteristics that influ- 
ence the design of the parsing algorithm and the 
whole MT system. In the paper, we consider only 
the issues that are related to the syntactic analysis 
of closed captions. The stream contaln.q long sen- 
tences taken from interviews, short phrases making 
the program more lively, and telegraphic style sen- 
tences used to report the latest stock market news. 
It has unrestricted syntax in long descriptive sen- 
tences, which resembles written language, with some 
phenomena like omission of function words that are 
characteristic ofspoken language. It is likely to con- 
tain words not present in the lexicon, such as com- 
pany names. Although not considered here directly, 
a caption stream usually also contains ome typos 
and corrections made by the captioner. It is how- 
ever different from, e.g., a speech recognizer output 
since the human captioner usually provides the cor- 
rect transcription and there is no ~mraediate n ed to 
model recognition errors. 
3 Partial~ robust and fast parsing 
The kind of input described above requires robust 
parsing methods. Since our goal was real-time trans- 
lation, the parsing module had to be very efficient. 
We concentrated on reducing the amount of work 
done by the parser at the potential cost of lowering 
the quality of the resulting analysis 1. Similar meth- 
ods have been adopted elsewhere, although in most 
cases the goal was a shallow parse. (Abney, 1996) de- 
scribes a chunking approach based on cascades of fi- 
nite transducers in which the parser finds "islands of 
certainty" at multiple analysis levels. Only maximal 
constituents (in terms of length) found by transduc- 
ers at lower levels are the input to the transducers 
at higher levels, which results in constituent pruning 
and ambiguity containment, and low parsing times. 
(Ciravegna nd Lavelli, 1997) introduce additional 
IDealing with typos, out-of-coverage lexical item, etc, was 
not considered here, although the parser offers some robust- 
hess in skipping unknown words, see Section 7. 
controlling strategies to a chart parser. (Ciravegna 
and Lavelli, 1997) experiment with chunks and add 
a preprocessing step that uses a finite-state machine 
to identify potential chunks in the input. The chart 
parser control strategy is augmented so that con- 
stituents found during parsing that align with chunk 
boundaries are processed with a higher priority in 
the hope that they are more likely to become part 
of the final parse. Constituents that do not align 
with chunk boundaries are assigned lower priorities 
and remain in the agenda. The resulting algorithm 
can avoid pruning of useful constituents that can 
happen in Abney's cascading approach. 
Explicit pruning of constituents i another strat- 
egy that can be used to improve the efficiency of 
a parser as shown, e.g., by (Rayner and Carter, 
1996). (Rayner and Carter, 1996) experimented 
with a bottom-up parser with three levels: lexi- 
cal level, phrasal evel and full parsing level. Con- 
stituents are pruned before the phrasal and full pars- 
ing levels using statistical scores based on how likely 
is a constituent with a certain property to be part 
of the final solution. 
4 Requ i rements  on the algorithm 
Given the characteristics of the input and previously 
published results described above, we decided to de- 
sign a parsing strategy with the following character- 
istics: 
1. bottom-up: this allows for partial parses to be 
identified; 
2. multi-level: combined with pruning provides 
additional control over created constituents; 
3. pruning: constituents not likely to contribute to 
the final parse should be removed early. 
In what follows we will describe some initial re- 
sults of experiments in applying a multi-level chart 
parser with a radical priming strategy to the cap- 
tioning domain. 
5 Introducing levels to the grCmmar 
The parsing proceeds in multiple levels. The gram- 
mar is split into groups of rules corresponding to the 
different levels. At a given level, only rules assigned 
to this level are allowed to fire. They use as input 
all the constituents the parser created so far (only 
inactive edges, all active edges are currently pruned 
between levels). In a sense, this corresponds to pars- 
ing with multiple grammars, although the grammar 
partitioning can be done once the grammar is com- 
plete and fine tuning is performed to achieve the 
right balance between ambiguity, output fragmen- 
tation and parsing speed. This approach makes it 
also possible to test the improvement introduced by 
the special processing scheme in comparison to some 
8 
other parsing scheme since the grammar is kept in a 
general form. 
The grammar that we are currently using has 
been adapted from a general English grammar writ- 
ten for the Tomita parser. The grammar does 
not follow any particular theoretical framework, al- 
though it has been strongly influenced by both 
LFG (Bresnan, 1982) and HPSG (Polard and Sag, 
1994). It consists currently of about 300 gen- 
eral rules with some general attachment prefer- 
ence constraints that have been fine tuned on 
various kinds of text, including news broadcasts 
and written texts. One important characteris- 
tic of the grammar is the use of subcategor'Lza- 
tion information (at the syntactic level, with val- 
ues such as subject+object, subject+complement, 
subj ect+obj ect+oblique ("of''), etc.), and some 
broad semantic classification of adjuncts to help 
prevent ambiguity. The lexicalist character of the 
grammar requires that subcategorization and sim- 
ple semantic information (temporal expressions, lo- 
cation expressions, etc) be present in the lexicon. 
The grammar coverage (Czuba et al, 1998) is quite 
broad, but it is not sufficient, e.g., to cover some 
types of clause attachment that are present in long 
sentences found in the input. 
The changes that were introduced to the gram- 
mar, in comparison to the original version without 
levels, concentrated on a simple addition of levels 
to the rules and duplicating some rules at multiple 
levels. In the current grammar, the following levels 
have been introduced: 
I. lexical level: lexicon lookup, idioms and fixed 
phrases; 
2. nominal phrases without adjuncts; 
3. verb phrases with obligatory complements; 
4. noun and verb phrases with adjuncts; 
5. clausal constituents; noun and verb phrases 
with obligatory clausal complements; 
6. top level with preferences for attachment and 
well-formedness of constituents (e.g., prefer fi- 
nite clauses, prefer ungapped constituents, etc). 
Although some motivation for the above partition- 
ing can be found on X-bar theory, we mostly used 
our intuition for choosing the number of levels and 
deciding how to assign rules to different levels. These 
are parameters of the algorithm and they can be 
tuned in further experiments. See the next section 
for examples of rules that were added to multiple 
levels. 
6 Const i tuent  p run ing  
We will refer to constituents that were created using 
grammar ules with the nonterminal <X> on their 
LHS as <X> constituents. Two constituents will be 
similar if they were created using rules with the same 
nonterminal on the LHS. E.g., two <NP> constituents 
are similar, regardless oftheir positions and spanned 
input. 
The pruning phase was added to the usual chart 
parsing algorithm in a way that makes it invisible 
to the grammar writer. The pruning algorithm is 
called on the chart whenever no more rules are al- 
lowed to fire at the current level and no active arcs 
can be extended. In the initial implementation, the 
pruning algorithm was based on a simple subsump- 
tion relation: only the maximal(i.e., covering the 
longest number of input tokens) constituents from 
a set of similar constituents remain in the chart and 
are added to the agenda for the next level. E.g., if 
the chart contained two <NP> constituents, only the 
one spanning more input would be retained. 
Although the original pruning strategy resulted 
in many reasonable parses, we noticed a few general 
problems. The parser is very sensitive to wrongly 
created long constituents. This means that the 
grammar has to be relatively tight for a successful 
application with the described parser since no global 
optimization is performed. However, this also means 
that if in a particular context the parser builds a con- 
stituent C that is not correct in this context but the 
rules that were used to build the constituent cannot 
be removed from the grammar since they are useful 
in general, the pruning step will wrongly remove all, 
potentially correct, similar constituents subsumed 
byC. 
We observed this kind of behavior in practice and 
at least two cases can be distinguished. Some con- 
stituents are added to the chart early in the analysis 
and they form bigger constituents as the analysis 
progresses. Consequently, if a similar constituent is 
created, the original constituent will be pruned and 
will not be available at higher levels. This behav- 
ior can be observed, e.g., for the string that help, in 
which that is supposed to be a pronoun and help is 
supposed to have a verbal reading. Since that help 
is a well-formed <NP> constituent according to our 
grammar, it will be created and it will subsume the 
pronoun that. This means that the pronoun that will 
be pruned and it will not be available at a higher 
level that tries to create a correct parse incorporat- 
ing that as an object of a preceding verb and help 
as the main verb. In order to solve this problem we 
allowed some rules to be repeated at multiple lev- 
els. The rules introducing pronouns were added at 
two levels. The rules involving verbal complements 
were also introduced twice. Since verbal phrases are 
created relatively late in the analysis, verbal comple- 
ments on, e.g., noun phrases are not available yet. 
Because of that, e.g., the rule that attaches the di- 
rect object o a verb is present twice in the grammar: 
9 
one version of it takes care of nominal objects with- 
out complements, the other one is specific for ob- 
jects with a verbal complements. Since the second 
version of the rule contai~q a check for the presence 
of a verbal complement, no work is repeated. 
The second case that we noticed involved large 
noun phrases created as the result of applying the 
rules for nominal apposition (e.g., the U.S. president, 
Bill Clinton) and coordination. Since the parser 
does not use any semantic information, it is difficult 
to prevent such rules from applying in some wrong 
contexts. Examples include noun phrases at clause 
boundaries as in the following sentence: BTM says it 
will issue new shares to strengthen its capital base, 
BTM plans to raise 300 billion yen via the issue. 
In this case, an apposition its capital base, BTM is 
created and the phrases its capital base and BTM 
are pruned, preventing the parser from finding the 
correct analysis consisting of two finite clauses. 
In order to solve that problem, we introduced an 
option of relaxing the pruning constraint. Currently, 
such relaxing is allowed only for phrases containing 
appositions and coordination. All constituents hat 
are subsumed by similar ones containing apposition 
or coordination are marked and they can never be 
pruned. As a result, both its capital base and BTM 
remain in the chart and can be used to create the 
required clauses at higher levels. 
The pruning algorithm that we implemented can 
be potentially quite costly since it involves many 
comparisons between constituents. Although its 
worst-case cost is quadratic, in practice the equiv- 
alence classes of similar constituents are small and 
they are pruned quickly. As a result, in our experi- 
ments the pr~mlng time was below 0.01% of the total 
parse time. 
In addition to the actual removal of constituent, 
the function implementing the pruning algorithm 
performed local ambiguity packing: it removes con- 
stituents that have the same feature structures as a 
constituent already present in the chart and it cre- 
ates constituents with disjunction of feature struc- 
tures in case similar constituents spanning the same 
chunk of input but having different feature struc- 
tures are found. 
7 Ext rac t ion  f rom the  char t  
After the parser finished creating constituents atthe 
highest level, the final result has to be extracted from 
the chart. Since the parser might not be able to pro- 
duce a single analysis panning the whole input, the 
best sequence of constituents needs to be extracted. 
Currently, asimple best-first beam search through 
the chart is used to find a sequence (path) of con- 
stituents panning the whole input. Paths are al- 
lowed to have gaps, i.e., they do not have to be con- 
tiguous, although we do not allow for overlapping 
constituents. The algorithm prefers horter paths. 
The length of a path is computed as a weighted sum 
of the lengths of constituents in the path. We experi- 
mented with two different ways of assigning weights 
and lengths to constituents. In the first method, 
each constituent was assigned the length of 1 that 
was weighted by a factor depending of the "quality" 
of the constituent. Paths can be extended by a gap 
spanning one input token at a time. Such a gap is 
weighted with the factor of 3. Constituents hat are 
created by rules with the nonterminal <OUTPUT> on 
their LHS are assumed to be of the highest quality 
and they are weighted with the factor of 1. All re- 
maining constituents can also be added to the path 
and are weighted with the factor of 1.5. So a path 
consisting of an <OUTPUT> constituent spanning in- 
put tokens 1 to 3, a gap spanning input tokens 4 and 
5, and a <Vl> constituent spanning input tokens 6 
to 10 would receive the length of 1 + 6 + 1.5 = 8.5. 
This algorithm shows a strong preference for con- 
tiguons paths and assigns lengths depending on the 
number of constituents in the path, ignoring their 
length. 
The second weighting scheme we tried was based 
on the actual ength of constituents. <OUTPUT> con- 
stituents were assigned their actual length multiplied 
by 1. Other constituents had their actual length 
multiplied by 1.5, and gaps were weighted with the 
factor of 2. The path described in the previous para- 
graph was thus assigned the length of 3 + 4 + 7.5 
= 14.5. 
Although the first weighting scheme seems rather 
crude, it resulted in a very good performance both 
in terms of the quality of paths found and the time 
required to find the best path. The best-first search 
was implemented using a binary heap priority queue 
in an array, and the extraction time for the first 
weighting scheme was below 5% of the total time re- 
quired for both parsing and extraction. We also did 
not notice any cases in which the returned path was 
not the optimal one given the constituents found by 
the parser. The second weighting scheme is more 
fine-grained and might turn out to be better on a 
bigger corpus. However, it required about 15 as 
much time to complete the search as the first scheme. 
Finally, in case of ambiguity, the first feature 
structure returned by the parser was chosen. 
8 P re l iminary  resu l t s  
The algorithm has been applied to a sample of 42 
sentences from a real TV broadcast. The sentences 
were picked from a contiguous transcript that was 
cleaned up for captioner mistakes. Since the parser 
was designed for use in a real-time multi-engine MT 
system, we concentrated on sentences which were 
likely to be translated by the knowledge-based part 
of the system. Sentences that were likely to be 
10 
Sentence I, 17 tokens: 
\[\[We\]l\] \[\[Japan\] \[is \[unlikely to adopt \[any more stimulus spending measures\]\] soon \[despite \[that U.S. pressure.\]\]\]\]\] 
algorithm 
chart 
Tomita 
multi-level 
#arcs #constituents 
20410 6680 
891 191 
tim e (s) 
76.6 
28.0 
1.0 
Sentence  2, 49 tokens: 
\[\[Reform legislation\] \[is quite good\]\] \[because \[it \[\[puts up\] \[public money\]\] \[which\] \[\[financial institutions\] \[can get\]\] 
\[to protect \[depositors\]\] \[but only if\], \[\[\[they\] \[\[\[recycle\] \[or if you will,\] \[write off\]\] \[their bad loans\]\]\] \[and\] \[\[clean up\] 
\[their balance sheets\]\]\] [ [so\] \[they\] \[can start \[to loan \[money\]\] again.l\] 
algorithm #arcs 
chart 13530 
Tomita 
multi-level 3101 
#constituents time (s) 
4849 43.7 
- 22.0 
388 3.0 
Figure 1: Sample sentences with bracketing found the parser 
translated by a translation memory look-up, such as 
greetings and short canned phrases, were not added 
to the test corpus. The resulting corpus consisted of 
relatively long sentences, with the average length of 
23.5 tokens (including punctuation). 
The parser was compared to a Tomita parser and 
a chart parser with local ambiguity packing. All the 
parsers were implemented in lisp and used the same 
unification package, which made the parsing results 
easily comparable. They also used the same gram- 
mar and lexicon with about 8000 entries. The gram- 
mar was preprocessed for the Tomita parser and the 
chart parser by removing the rules that were present 
at multiple levels. The chart parser was set up to 
quit after finding the first parse covering the whole 
input. All times given below were obtained on the 
same machine (SPARCStation 5). Care was taken 
to disable lisp garbage collection during parsing. 
As it was expected, the pruning strategy resulted 
in a significant reduction of execution time. In Fig- 
ure 1 we present a few measurements to illustrate 
the time improvement for two example sentences. 
For the first one, all parsers produced a complete 
analysis spanning the whole input. For the second 
one, due to the lack of grammar coverage, no single 
analysis can be found. Figure 1 shows the bracketing 
that the multi-level parser found. It also produced 
correct feature structures for all the chunks that can 
be used by an MT system. 
As can be seen from Table 1, the time improve- 
ment is significant. The improvement in the number 
of elements in the final chart is crucial for good per- 
formance of the extraction algorithm that chooses 
constituents to be output by the parser in case 
there is no single analysis spanning the whole in- 
put. Also, the multilevel parser did not produce am- 
biguous feature-structures (ambiguity containment). 
The Tomita parser produced two packed f-structures 
for the first sentence that would have to be unpacked 
and disambiguated for further processing. For the 
second sentence, the Tomita parser did not find any 
full parse. The multilevel parser produced chunks 
that are usable in an MT system and can be trans- 
lated giving at least partial translation to the user. 
The results on the whole test set were as follows. 
The Tomita parser needed 652 seconds to analyze 
all the sentences. It produced a complete analysis 
for 31 sentences, returning no analysis for 11. The 
chart parser run till the first solution spanning the 
whole input was found needed 937 seconds to ana- 
lyze the same 31 sentences, failing on the rest. In 
the case of the Tomita parser, the average ambigu- 
ity level was 3.7 analyses per sentence. The Tomita 
parser produced an acceptable 2 parse for all the 31 
sentences it could analyze in full. However, the ac- 
ceptable analysis would still have to be distilled from 
all the parses it returned. The time needed by the 
chart parser was prohibitively high for any attempt 
at extracting constituents in the cases when no sin- 
gle parse was found. The total time required by the 
multi-level parser was 60.7 sec, 10.7 times less than 
the Tomita parser and 15.4 times less than the chart 
parser. Figure 2 illustrates the parsing and extrac- 
tion time as function of sentence length. Although 
clearly dependent on the syntactic omplexity of the 
input sentence, the parsing time appears to be lin- 
early related to the input length. 
2An analysis with, e.g., wrong PP~attachment that could 
be potentially repaired using lexical selection rules during 
translation, was marked as acceptable. 
11 
0 . 3 5 ~ ~ ~  
0,3  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  i . . . . . . . .  : . . . . . . . . .  
OA 
5 10 15 ~ ~ ~ ~ ~ 45 50 
Sentenc~Jlen~h 
02 
..~ 
~0.15  
Figure 2: Parse and extraction time as function of sentence l ngth 
Since the parser's ability to attach constituents is 
limited, it produced 114 chunks for all the input sen- 
tences, average 2.71 per sentence. The chunks fully 
covered the input sentences. They were compared to 
a bracketing that was assigned by a human and cor- 
responded to linguistically motivated phrases. Out 
of these, 11 corresponded to a wrong bracketing as 
judged by a human. This affected 8 sentences in 
the corpus. The remaining chunks were acceptable 
(as defined in the footnote on the previous page). 
Although an evaluation i  terms of precision/recall 
would be possible, we have not done it for this paper. 
We believe that an end-to-end evaluation using, e.g., 
an MT or Information Extraction system that would 
be able to handle the parser output would be more 
meaningful, since it would also be a way to evaluate 
the effect of the large number of small chunks the 
parser produced. 
9 Future  research  
There are a number of research issues that we are 
planning to address oon. First, a more thorough 
evaluation is required, as described in the previous 
section. We are currently looking for ways to per- 
form such an evaluation. We are also looking into 
replacing the fixed pruning and constituent extrac- 
tion strategy with one learned from training data. 
We are also investigating ways of learning the num- 
ber of levels and grammar rule partitioning among 
levels. 
10 Acknowledgements  
We would like to thank Eric Nyberg and Teruko Mi- 
tamura for providing support for this research and 
useful discussion, Alon Lavie for helpful comments 
about he research and earlier versions of the paper, 
and the anonymous NAACL Student Workshop re- 
viewers for their constructive comments. 
Re ferences  
S. Abney. 1991. Parsing by chunks. In Berwick, Ab- 
ney, and Tenny, editors, Principle-Based Parsing. 
Kluwer. 
S. Abney. 1996. Partial parsing via finite-state cas- 
cades. In Proceedings of Workshop on Robust 
Parsing, ESSLI-96. 
J. Bresnan, editor. 1982. The mental representation 
of 9rammatieal relations. MIT Press. 
F. Ciravegna and A. LaveUi. 1997. Controlling 
bottom-up chart parser through text chunking. In 
Proceedings of IWPT'97. 
K. Czuba, T. Mitamura, and E. Nyberg. 1998. Can 
practical interlinguas be used for difficult analysis 
problems? In Proceedings of AMTA-98 Workshop 
on Interlinguas. 
E. Nyberg and T. Mitamura. 1992. The kant sys- 
tem: Fast, accurate, high-quality translation in 
practical domains. In Proceedings of COLING-92. 
E. Nyberg and T. Mitamura. 1997. A real-time MT 
system for translating broadcast captions. In Pro- 
ceedings of MT Summit V1. 
C. Polard and I. Sag. 1994. Head-Driven Phrase 
Structure Grammar. University of Chicago Press. 
F. Popowich, D. Turcato, O. Laurens, and 
P. McFetridge. 1997. A lexicalist approach to the 
translation of coUoquial text. In Proceedings of 
TM1-97. 
M. P~yner and D. Carter. 1996. Fast parsing using 
pruning and grammar specialization. In Proceed- 
ings of A CL'96. 
12 
Answering What-Is Questions by Virtual Annotation 
 
John Prager  
IBM T.J. Watson Research Center 
Yorktown Heights, N.Y. 10598 
(914) 784-6809 
jprager@us.ibm.com 
Dragomir Radev 
University of Michigan 
Ann Arbor, MI 48109 
(734) 615-5225 
radev@umich.edu 
Krzysztof Czuba  
Carnegie-Mellon University 
Pittsburgh, PA 15213 
(412) 268 6521 
kczuba@cs.cmu.edu 
 
 
ABSTRACT 
We present the technique of Virtual Annotation as a specialization 
of Predictive Annotation for answering definitional What is 
questions.  These questions generally have the property that the 
type of the answer is not given away by the question, which poses 
problems for a system which has to select answer strings from 
suggested passages.  Virtual Annotation uses a combination of 
knowledge-based techniques using an ontology, and statistical 
techniques using a large corpus to achieve high precision. 
 
Keywords 
Question-Answering, Information Retrieval, Ontologies 
 
1. INTRODUCTION 
Question Answering is gaining increased attention in both the 
commercial and academic arenas.  While algorithms for general 
question answering have already been proposed, we find that such 
algorithms fail to capture certain subtleties of particular types of 
questions.  We propose an approach in which different types of 
questions are processed using different algorithms.  We introduce a 
technique named Virtual Annotation (VA) for answering one such 
type of question, namely the What is question. 
 
We have previously presented the technique of Predictive 
Annotation (PA) [Prager, 2000], which has proven to be an 
effective approach to the problem of Question Answering.  The 
essence of PA is to index the semantic types of all entities in the 
corpus, identify the desired answer type from the question, search 
for passages that contain entities with the desired answer type as 
well as the other query terms, and to extract the answer term or 
phrase.  One of the weaknesses of PA, though, has been in dealing 
with questions for which the system cannot determine the correct 
answer type required.  We introduce here an extension to PA 
which we call Virtual Annotation and show it to be effective for 
those ?What is/are (a/an) X? questions that are seeking hypernyms 
of X.  These are a type of definition question, which other QA 
systems attempt to answer by searching in the document collection 
for textual clues similar to those proposed by [Hearst, 1998], that 
are characteristic of definitions.  Such an approach does not use 
the strengths of PA and is not successful in the cases in which a 
deeper understanding of the text is needed in order to identify 
the defining term in question. 
 
We first give a brief description of PA.  We look at a certain 
class of What is questions and describe our basic algorithm.  
Using this algorithm we develop the Virtual Annotation 
technique, and evaluate its performance with respect to both the 
standard TREC and our own benchmark.  We demonstrate on 
two question sets that the precision improves from .15 and .33 to 
.78 and .83 with the addition of VA. 
2.  BACKGROUND 
For our purposes, a question-answering (QA) system is one 
which takes a well-formed user question and returns an 
appropriate answer phrase found in a body of text.  This 
generally excludes How and Why questions from consideration, 
except in the relatively rare cases when they can be answered by 
simple phrases, such as ?by fermenting grapes? or ?because of 
the scattering of light?.  In general, the response of a QA system 
will be a named entity such as a person, place, time, numerical 
measure or a noun phrase, optionally within the context of a 
sentence or short paragraph.  
 
The core of most QA systems participating in TREC [TREC8, 
2000 & TREC9, 2001] is the identification of the answer type 
desired by analyzing the question.  For example, Who questions 
seek people or organizations, Where questions seek places, 
When questions seek times, and so on.  The goal, then, is to find 
an entity of the right type in the text corpus in a context that 
justifies it as the answer to the question.  To achieve this goal, 
we have been using the technique of PA to annotate the text 
corpus with semantic categories (QA-Tokens) prior to indexing. 
 
Each QA-Token is identified by a set of terms, patterns, or 
finite-state machines defining matching text sequences.  Thus 
?Shakespeare? is annotated with ?PERSON$?, and the text 
string ?PERSON$? is indexed at the same text location as 
?Shakespeare?.  Similarly, ?$123.45? is annotated with 
?MONEY$?.  When a question is processed, the desired QA-
Token is identified and it replaces the Wh-words and their 
auxiliaries.  Thus, ?Who? is replaced by ?PERSON$?, and 
?How much? + ?cost? are replaced by ?MONEY$?.  The 
resulting query is then input to the search engine as a bag of 
words.  The expectation here is that if the initial question were 
?Who wrote Hamlet?, for example, then the modified query of 
?PERSON$ write Hamlet? (after lemmatization) would be a 
 
 
 
 perfect match to text that states ?Shakespeare wrote Hamlet? or 
?Hamlet was written by Shakespeare?. 
 
The modified query is matched by the search engine against 
passages of 1-2 sentences, rather than documents.  The top 10 
passages returned are processed by our Answer Selection module 
which re-annotates the text, identifies all potential answer phrases, 
ranks them using a learned evaluation function and selects the top 
5 answers (see [Radev et al, 2000]). 
 
The problem with ?What is/are (a/an) X? questions is that the 
question usually does not betray the desired answer type.  All the 
system can deduce is that it must find a noun phrase (the QA-
Token THING$).  The trouble with THING$ is that it is too 
general and labels a large percentage of the nouns in the corpus, 
and so does not help much in narrowing down the possibilities.  A 
second problem is that for many such questions the desired answer 
type is not one of the approximately 50 high-level classes (i.e. QA-
Tokens) that we can anticipate at indexing; this phenomenon is 
seen in TREC9, whose 24 definitional What is questions are listed 
in the Appendix.  These all appear to be calling out for a 
hypernym.  To handle such questions we developed the technique 
of Virtual Annotation which is like PA and shares much of the 
same machinery, but does not rely on the appropriate class being 
known at indexing time.  We will illustrate with examples from the 
animal kingdom, including a few from TREC9. 
 
3.  VIRTUAL ANNOTATION 
If we look up a word in a thesaurus such as WordNet [Miller et al, 
1993]), we can discover its hypernym tree, but there is no 
indication which hypernym is the most appropriate to answer a 
What is question.  For example, the hypernym hierarchy for 
?nematode? is shown in Table 1.  The level numbering counts 
levels up from the starting term.  The numbers in parentheses will 
be explained later. 
 
Table 1.  Parentage of ?nematode? according to WordNet. 
 
Level Synset 
0 {nematode, roundworm} 
1 {worm(13)} 
2 {invertebrate} 
3 {animal(2), animate being, beast, brute, creature, 
fauna} 
4 {life form(2), organism(3), being, living thing} 
5 {entity, something} 
 
 
At first sight, the desirability of the hypernyms seems to decrease 
with increasing level number.  However, if we examine ?meerkat? 
we find the hierarchy in Table 2. 
 
We are leaving much unsaid here about the context of the question 
and what is known of the questioner, but it is not unreasonable to 
assert that the ?best? answer to ?What is a meerkat? is either ?a 
mammal? (level 4) or ?an animal? (level 7).  How do we get an 
automatic system to pick the right candidate? 
 
 
 
Table 2.  Parentage of ?meerkat? according to WordNet 
 
Level Synset 
0 {meerkat, mierkat} 
1 {viverrine, viverrine mammal} 
2 {carnivore} 
3 {placental, placental mammal, eutherian, eutherian 
mammal} 
4 {mammal} 
5 {vertebrate, craniate} 
6 {chordate} 
7 {animal(2), animate being, beast, brute, creature, 
fauna} 
8 {life form, organism, being, living thing} 
9 {entity, something} 
 
 
It seems very much that what we would choose intuitively as the 
best answer corresponds to Rosch et al?s basic categories 
[Rosch et al, 1976].  According to psychological testing, these 
are categorization levels of intermediate specificity that people 
tend to use in unconstrained settings.  If that is indeed true, then 
we can use online text as a source of evidence for this tendency.  
For example, we might find sentences such as ??  meerkats and 
other Y ? ?, where Y is one of its hypernyms, indicating that Y 
is in some sense the preferred descriptor. 
 
We count the co-occurrences of the target search term (e.g. 
?meerkat? or ?nematode?) with each of its hypernyms (e.g. 
?animal?) in 2-sentence passages, in the TREC9 corpus.  These 
counts are the parenthetical numbers in Tables 1 and 2.  The 
absence of a numerical label there indicates zero co-occurrences.  
Intuitively, the larger the count, the better the corresponding 
term is as a descriptor. 
 
3.1  Hypernym Scoring and Selection  
Since our ultimate goal is to find passages describing the target 
term, discovering zero co-occurrences allows elimination of 
useless candidates.  Of those remaining, we are drawn to those 
with the highest counts, but we would like to bias our system 
away from the higher levels.  Calling a nematode a life-form is 
correct, but hardly helpful.   
 
The top levels of WordNet (or any ontology) are by definition 
very general, and therefore are unlikely to be of much use for 
purposes of definition.  However, if none of the immediate 
parents of a term we are looking up co-occur in our text corpus, 
we clearly will be forced to use a more general term that does.  
We want to go further, though, in those cases where the 
immediate parents do occur, but in small numbers, and the very 
general parents occur with such high frequencies that our 
algorithm would select them.  In those cases we introduce a 
tentative level ceiling to prevent higher-level terms from being 
chosen if there are suitable lower-level alternatives.   
 
We would like to use a weighting function that decreases 
monotonically with level distance.  Mihalcea and  Moldovan 
[1999], in an analogous context, use the logarithm of the number 
of terms in a given term?s subtree to calculate weights, and they 
claim to have shown that this function is optimal.  Since it is 
approximately true that the level population increases 
 exponentially in an ontology, this suggests that a linear function of 
level number will perform just as well. 
 
Our first step is to generate a level-adapted count (LAC) by 
dividing the co-occurrence counts by the level number (we are 
only interested in levels 1 and greater).  We then select the best 
hypernym(s) by using a fuzzy maximum calculation.  We locate 
the one or more hypernyms with greatest LAC, and then also select 
any others with a LAC within a predefined threshold of it; in our 
experimentation we have found that a threshold value of 20% 
works well.  Thus if, for example, a term has one hypernym at 
level 1 with a count of 30, and another at level 2 with a count of 
50, and all other entries have much smaller counts, then since the 
LAC 25 is within 20% of the LAC 30, both of these hypernyms 
will be proposed.   
 
To prevent the highest levels from being selected if there is any 
alternative, we tentatively exclude them from consideration 
according to the following scheme: 
 
If the top of the tree is at level N, where N <= 3, we set a tentative 
ceiling at N-1, otherwise if N<=5, we set the ceiling at N-2, 
otherwise we set the ceiling at N-3.  If no co-occurrences are found 
at or below this ceiling, then it is raised until a positive value is 
found, and the corresponding term is selected.  
 
If no hypernym at all co-occurs with the target term, then this 
approach is abandoned:  the ?What? in the question is replaced by 
?THING$? and normal procedures of Predictive Annotation are 
followed. 
 
When successful, the algorithm described above discovers one or 
more candidate hypernyms that are known to co-occur with the 
target term.  There is a question, though, of what to do when the 
question term has more than one sense, and hence more than one 
ancestral line in WordNet.  We face a choice of either selecting the 
hypernym(s) with the highest overall score as calculated by the 
algorithm described above, or collecting together the best 
hypernyms in each parental branch.  After some experimentation 
we made the latter choice.  One of the questions that benefitted 
from this was ?What is sake?.  WordNet has three senses for sake: 
good (in the sense of welfare), wine (the Japanese drink) and 
aim/end, with computed scores of 122, 29 and 87/99 respectively.  
It seems likely (from the phrasing of the question) that the ?wine? 
sense is the desired one, but this would be missed entirely if only 
the top-scoring hypernyms were chosen. 
 
We now describe how we arrange for our Predictive Annotation 
system to find these answers.  We do this by using these 
descriptors as virtual QA-Tokens; they are not part of the search 
engine index, but are tagged in the passages that the search engine 
returns at run time. 
 
3.2 Integration 
Let us use H to represent either the single hypernym or a 
disjunction of the several hypernyms found through the WordNet 
analysis.  The original question Q =  
?What is (a/an) X? 
is converted to Q? =  
?DEFINE$ X H? 
where DEFINE$ is a virtual QA-Token that was never seen at 
indexing time, does not annotate any text and does not occur in the 
index.  The processed query Q? then will find passages that 
contain occurrences of both X and H; the token DEFINE$ will 
be ignored by the search engine.  The top passages returned by 
the search engine are then passed to Answer Selection, which re-
annotates the text.  However, this time the virtual QA-Token 
DEFINE$ is introduced and the patterns it matches are defined 
to be the disjuncts in H.  In this way, all occurrences of the 
proposed hypernyms of X in the search engine passages are 
found, and are scored and ranked in the regular fashion.  The 
end result is that the top passages contain the target term and one 
of its most frequently co-occurring hypernyms in close 
proximity, and these hypernyms are selected as answers. 
 
When we use this technique of Virtual Annotation on the 
aforementioned questions, we get answer passages such as 
  
?Such genes have been found in nematode worms 
but not yet in higher animals.? 
and 
?South African golfer Butch Kruger had a good 
round going in the central Orange Free State trials, 
until a mongoose-like animal grabbed his ball with 
its mouth and dropped down its hole. Kruger wrote 
on his card: "Meerkat."? 
 
4 RESULTS 
4.1 Evaluation 
We evaluated Virtual Annotation on two sets of questions ? the 
definitional questions from TREC9 and similar kinds of 
questions from the Excite query log (see 
http://www.excite.com).  In both cases we were looking for 
definitional text in the TREC corpus.  The TREC questions had 
been previously verified (by NIST) to have answers there; the 
Excite questions had no such guarantee.   We started with 174 
Excite questions of the form ?What is X?, where X was a 1- or 
2-word phrase.  We removed those questions that we felt would 
not have been acceptable as TREC9 questions.  These were 
questions where: 
o The query terms did not appear in the TREC corpus, 
and some may not even have been real words (e.g. 
?What is a gigapop?).1  37 questions. 
o The query terms were in the corpus, but there was no 
definition present (e.g ?What is a computer 
monitor?).2  18 questions. 
o The question was not asking about the class of the 
term but how to distinguish it from other members of 
the class (e.g. ?What is a star fruit?).  17 questions. 
o The question was about computer technology that 
emerged after the articles in the TREC corpus were 
written (e.g. ?What is a pci slot?).  19 questions. 
o The question was very likely seeking an example, not 
a definition (e.g. ?What is a powerful adhesive?).  1 
question plus maybe some others ? see the Discussion 
                                               
1 That is, after automatic spelling correction was attempted.   
2 The TREC10 evaluation in August 2001 is expected to contain 
questions for which there is no answer in the corpus 
(deliberately).   While it is important for a system to be able to 
make this distinction, we kept within the TREC9 framework for 
this evaluation. 
 section later.  How to automatically distinguish these 
cases is a matter for further research. 
 
Of the remaining 82 Excite questions, 13 did not have entries in 
WordNet.  We did not disqualify those questions. 
 
For both the TREC and Excite question sets we report two 
evaluation measures.  In the TREC QA track, 5 answers are 
submitted per question, and the score for the question is the 
reciprocal of the rank of the first correct answer in these 5 
candidates, or 0 if the correct answer is not present at all.  A 
submission?s overall score is the mean reciprocal rank (MRR) over 
all questions.  We calculate MRR as well as mean binary score 
(MBS) over the top 5 candidates; the binary score for a question is 
1 if a correct answer was present in the top 5 candidates, 0 
otherwise.  The first sets of MBS and MRR figures are for our base 
system, the second set the system with VA. 
 
Table 3.  Comparison of base system and system with VA on 
both TREC9 and Excite definitional questions. 
 
Source No. of 
Questions 
MBS 
w/o 
VA 
MRR 
w/o 
VA 
MBS 
with 
VA 
MRR 
with 
VA 
TREC9  
(in WN) 
20 .3 .2 .9 .9 
TREC9  
(not in WN) 
4 .5 .375 .5 .5 
TREC9 
Overall 
24 .333 .229 .833 .833 
Excite 
(in WN) 
69 .101 .085 .855 
 
.824 
Excite  
(not in WN) 
13 .384 .295 .384 .295 
Excite 
Overall 
82 .146 .118 .780 .740 
 
 
We see that for the 24 TREC9 definitional questions, our MRR 
score with VA was the same as the MBS score.  This was because 
for each of the 20 questions where the system found a correct 
answer, it was in the top position. 
 
By comparison, our base system achieved an overall MRR score of 
.315 across the 693 questions of TREC9.  Thus we see that with 
VA, the average score of definitional questions improves from 
below our TREC average to considerably higher.  While the 
percentage of definitional questions in TREC9 was quite small, we 
shall explain in a later section how we plan to extend our 
techniques to other question types. 
 
4.2  Errors 
The VA process is not flawless, for a variety of reasons.  One is 
that the hierarchy in WordNet does not always exactly correspond 
to the way people classify the world.  For example, in WordNet a 
dog is not a pet, so ?pet? will never even be a candidate answer to 
?What is a dog?. 
 
When the question term is in WordNet, VA succeeds most of the 
time.  One of the error sources is due to the lack of uniformity of 
the semantic distance between levels.  For example, the parents 
of ?architect? are ?creator? and ?human?, the latter being what 
our system answers to ?What is an architect?.  This is 
technically correct, but not very useful.   
 
Another error source is polysemy.  This does not seem to cause 
problems with VA very often ? indeed the co-occurrence 
calculations that we perform are similar to those done by 
[Mihalcea and Moldovan, 1999] to perform word sense 
disambiguation ? but it can give rise to amusing results.  For 
example, when asked ?What is an ass? the system responded 
with ?Congress?.  Ass has four senses, the last of which in 
WordNet is a slang term for sex.  The parent synset contains the 
archaic synonym congress (uncapitalized!).  In the TREC corpus 
there are several passages containing the words ass and 
Congress, which lead to congress being the hypernym with the 
greatest score.  Clearly this particular problem can be avoided 
by using orthography to indicate word-sense, but the general 
problem remains.  
 
5  DISCUSSION AND FURTHER WORK 
5.1  Discussion 
While we chose not to use Hearst?s approach of key-phrase 
identification as the primary mechanism for answering What is 
questions, we don?t reject the utility of the approach.  Indeed, a 
combination of VA as described here with a key-phrase analysis 
to further filter candidate answer passages might well reduce the 
incidence of errors such as the one with ass mentioned in the 
previous section.  Such an investigation remains to be done. 
 
We have seen that VA gives very high performance scores at 
answering What is questions ? and we suggest it can be 
extended to other types ? but we have not fully addressed the 
issue of automatically selecting the questions to which to apply 
it.  We have used the heuristic of only looking at questions of 
the form ?What is (a/an) X? where X is a phrase of one or two 
words.  By inspection of the Excite questions, almost all of those 
that pass this test are looking for definitions, but some - such as 
?What is a powerful adhesive? - very probably do not.  There 
are also a few questions that are inherently ambiguous 
(understanding that the questioners are not all perfect 
grammarians):  is ?What is an antacid? asking for a definition or 
a brand name?  Even if it is known or assumed that a definition 
is required, there remains the ambiguity of the state of 
knowledge of the questioner.  If the person has no clue what the 
term means, then a parent class, which is what VA finds, is the 
right answer.  If the person knows the class but needs to know 
how to distinguish the object from others in the class, for 
example ?What is a star fruit?, then a very different approach is 
required.  If the question seems very specific, but uses common 
words entirely, such as the Excite question ?What is a yellow 
spotted lizard?, then the only reasonable interpretation seems to 
be a request for a subclass of the head noun that has the given 
property.  Finally, questions such as ?What is a nanometer? and 
?What is rubella? are looking for a value or more common 
synonym.   
 
 5.2 Other Question Types 
The preceding discussion has centered upon What is questions and 
the use of WordNet, but the same principles can be applied to other 
question types and other ontologies.  Consider the question ?Where 
is Chicago?, from the training set NIST supplied for TREC8.  Let 
us assume we can use statistical arguments to decide that, in a 
vanilla context, the question is about the city as opposed to the 
rock group, any of the city?s sports teams or the University.  There 
is still considerable ambiguity regarding the granularity of the 
desired answer.  Is it:  Cook County?  Illinois?  The Mid-West?  
The United States?  North America?  The Western Hemisphere? ?  
 
There are a number of geographical databases available, which 
either alone or with some data massaging can be viewed as 
ontologies with ?located within? as the primary relationship.  Then 
by applying Virtual Annotation to Where questions we can find 
the enclosing region that is most commonly referred to in the 
context of the question term.  By manually applying our algorithm 
to ?Chicago? and the list of geographic regions in the previous 
paragraph we find that ?Illinois? wins, as expected, just beating out 
?The United States?.  However, it should be mentioned that a more 
extensive investigation might find a different weighting scheme 
more appropriate for geographic hierarchies. 
 
The aforementioned answer of ?Illinois? to the question ?Where is 
Chicago?? might be the best answer for an American user, but for 
anyone else, an answer providing the country might be preferred.  
How can we expect Virtual Annotation to take this into account?  
The ?hidden variable? in the operation of VA is the corpus.  It is 
assumed that the user belongs to the intended readership of the 
articles in the corpus, and to the extent that this is true, the results 
of VA will be useful to the user.    
 
Virtual Annotation can also be used to answer questions that are 
seeking examples or instances of a class.  We can use WordNet 
again, but this time look to hyponyms.  These questions are more 
varied in syntax than the What is kind;  they include, for example 
from TREC9 again: 
?Name a flying mammal.? 
?What flower did Vincent Van Gogh paint?? 
and 
?What type of bridge is the Golden Gate Bridge?? 
 
6.  SUMMARY 
We presented Virtual Annotation, a technique to extend the 
capabilities of PA to a class of definition questions in which the 
answer type is not easily identifiable.  Moreover, VA can find text 
snippets that do not contain the regular textual clues for presence 
of definitions.  We have shown that VA can considerably improve 
the performance of answering What is questions, and we indicate 
how other kinds of questions can be tackled by similar techniques. 
 
7.  REFERENCES 
[1] Hearst, M.A. ?Automated Discovery of WordNet Relations? 
in WordNet: an Electronic Lexical Database, Christiane 
Fellbaum Ed, MIT Press, Cambridge MA, 1998. 
[2] Mihalcea, R. and Moldovan, D. ?A Method for Word Sense 
Disambiguation of Unrestricted Text?.  Proceedings of the 
37th Annual Meeting of the Association for Computational 
Linguistics (ACL-99), pp. 152-158, College Park, MD, 1999. 
[3] Miller, G. ?WordNet: A Lexical Database for English?, 
Communications of the ACM 38(11) pp. 39-41, 1995. 
[4] Moldovan, D.I. and Mihalcea, R. ?Using WordNet and 
Lexical Operators to Improve Internet Searches?, IEEE 
Internet Computing, pp. 34-43, Jan-Feb 2000.  
[5] Prager, J.M., Radev, D.R., Brown, E.W. and Coden, A.R. 
?The Use of Predictive Annotation for Question-Answering 
in TREC8?, Proceedings of TREC8, Gaithersburg, MD, 
2000. 
[6] Prager, J.M., Brown, E.W., Coden, A.R., and Radev, D.R. 
"Question-Answering by Predictive Annotation", 
Proceedings of SIGIR 2000, pp. 184-191, Athens, Greece, 
2000. 
[7] Radev, D.R., Prager, J.M. and Samn, V. ?Ranking 
Suspected Answers to Natural Language Questions using 
Predictive Annotation?, Proceedings of ANLP?00, Seattle, 
WA, 2000. 
[8] Rosch, E. et al ?Basic Objects in Natural Categories?, 
Cognitive Psychology 8, pp. 382-439, 1976. 
[9] TREC8 - ?The Eighth Text Retrieval Conference?, E.M. 
Voorhees and D.K. Harman Eds., NIST, Gaithersburg, MD, 
2000. 
[10] TREC9 - ?The Ninth Text Retrieval Conference?, E.M. 
Voorhees and D.K. Harman Eds., NIST, Gaithersburg, MD, 
to appear. 
 
APPENDIX 
What-is questions from TREC9 
617: What are chloroplasts?  (X) 
528: What are geckos? 
544: What are pomegranates?   
241: What is a caldera?  (X) 
358: What is a meerkat? 
434: What is a nanometer?  (X) 
354: What is a nematode? 
463: What is a stratocaster? 
447: What is anise? 
386: What is anorexia nervosa? 
635: What is cribbage? 
300: What is leukemia? 
305: What is molybdenum? 
644: What is ouzo? 
420: What is pandoro?  (X) 
228: What is platinum? 
374: What is porphyria? 
483: What is sake? 
395: What is saltpeter? 
421: What is thalassemia? 
438: What is titanium? 
600: What is typhoid fever? 
468: What is tyvek? 
539: What is witch hazel? 
 
Our system did not correctly answer the questions marked with 
an ?X?.  For all of the others the correct answer was the first of 
the 5 attempts returned. 
 
   
		
	 ffIn Question Answering, Two Heads Are Better Than One
Jennifer Chu-Carroll Krzysztof Czuba John Prager Abraham Ittycheriah
IBM T.J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598, U.S.A.
jencc,kczuba,jprager,abei@us.ibm.com
Abstract
Motivated by the success of ensemble methods
in machine learning and other areas of natu-
ral language processing, we developed a multi-
strategy and multi-source approach to question
answering which is based on combining the re-
sults from different answering agents searching
for answers in multiple corpora. The answer-
ing agents adopt fundamentally different strate-
gies, one utilizing primarily knowledge-based
mechanisms and the other adopting statistical
techniques. We present our multi-level answer
resolution algorithm that combines results from
the answering agents at the question, passage,
and/or answer levels. Experiments evaluating
the effectiveness of our answer resolution algo-
rithm show a 35.0% relative improvement over
our baseline system in the number of questions
correctly answered, and a 32.8% improvement
according to the average precision metric.
1 Introduction
Traditional question answering (QA) systems typically
employ a pipeline approach, consisting roughly of ques-
tion analysis, document/passage retrieval, and answer se-
lection (see e.g., (Prager et al, 2000; Moldovan et al,
2000; Hovy et al, 2001; Clarke et al, 2001)). Although a
typical QA system classifies questions based on expected
answer types, it adopts the same strategy for locating po-
tential answers from the same corpus regardless of the
question classification. In our own earlier work, we de-
veloped a specialized mechanism called Virtual Annota-
tion for handling definition questions (e.g., ?Who was
Galileo?? and ?What are antibiotics??) that consults,
in addition to the standard reference corpus, a structured
knowledge source (WordNet) for answering such ques-
tions (Prager et al, 2001). We have shown that better
performance is achieved by applying Virtual Annotation
and our general purpose QA strategy in parallel. In this
paper, we investigate the impact of adopting such a multi-
strategy and multi-source approach to QA in a more gen-
eral fashion.
Our approach to question answering is additionally
motivated by the success of ensemble methods in ma-
chine learning, where multiple classifiers are employed
and their results are combined to produce the final output
of the ensemble (for an overview, see (Dietterich, 1997)).
Such ensemble methods have recently been adopted in
question answering (Chu-Carroll et al, 2003b; Burger
et al, 2003). In our question answering system, PI-
QUANT, we utilize in parallel multiple answering agents
that adopt different processing strategies and consult dif-
ferent knowledge sources in identifying answers to given
questions, and we employ resolution mechanisms to com-
bine the results produced by the individual answering
agents.
We call our approach multi-strategy since we com-
bine the results from a number of independent agents im-
plementing different answer finding strategies. We also
call it multi-source since the different agents can search
for answers in multiple knowledge sources. In this pa-
per, we focus on two answering agents that adopt fun-
damentally different strategies: one agent uses predomi-
nantly knowledge-based mechanisms, whereas the other
agent is based on statistical methods. Our multi-level
resolution algorithm enables combination of results from
each answering agent at the question, passage, and/or an-
swer levels. Our experiments show that in most cases
our multi-level resolution algorithm outperforms its com-
ponents, supporting a tightly-coupled design for multi-
agent QA systems. Experimental results show signifi-
cant performance improvement over our single-strategy,
single-source baselines, with the best performing multi-
level resolution algorithm achieving a 35.0% relative im-
provement in the number of correct answers and a 32.8%
improvement in average precision, on a previously un-
seen test set.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 24-31
                                                         Proceedings of HLT-NAACL 2003
Answering Agents
KSP
SemanticSearch
KeywordSearch
Question
WordNet
Answer
Cyc
QFrame
QuestionAnalysis QGoals
Knowledge-BasedAnswering Agent
StatisticalAnswering Agent
Aquaintcorpus
TRECcorpus
EB
AnswerResolution
Definition QAnswering Agent
KSP-BasedAnswering Agent
Knowledge Sources
Figure 1: PIQUANT?s Architecture
2 A Multi-Agent QA Architecture
In order to enable a multi-source and multi-strategy ap-
proach to question answering, we developed a modu-
lar and extensible QA architecture as shown in Figure 1
(Chu-Carroll et al, 2003a; Chu-Carroll et al, 2003b).
With a consistent interface defined for each component,
this architecture allows for easy plug-and-play of individ-
ual components for experimental purposes.
In our architecture, a question is first processed by the
question analysis component. The analysis results are
represented as a QFrame, which minimally includes a set
of question features that help activate one or more an-
swering agents. Each answering agent takes the QFrame
and generates its own set of requests to a variety of
knowledge sources. This may include performing search
against a text corpus and extracting answers from the re-
sulting passages, or performing a query against a struc-
tured knowledge source, such as WordNet (Miller, 1995)
or Cyc (Lenat, 1995). The (intermediate) results from
the individual answering agents are then passed on to the
answer resolution component, which combines and re-
solves the set of results, and either produces the system?s
final answers or feeds the intermediate results back to the
answering agents for further processing.
We have developed multiple answering agents, some
general purpose and others tailored for specific ques-
tion types. Figure 1 shows the answering agents cur-
rently available in PIQUANT. The knowledge-based and
statistical answering agents are general-purpose agents
that adopt different processing strategies and consult a
number of different text resources. The definition-Q
agent targets definition questions (e.g., ?What is peni-
cillin?? and ?Who is Picasso??) with a technique called
Virtual Annotation using the external knowledge source
WordNet (Prager et al, 2001). The KSP-based answer-
ing agent focuses on a subset of factoid questions with
specific logical forms, such as capital(?COUNTRY) and
state tree(?STATE). The answering agent sends requests
to the KSP (Knowledge Sources Portal), which returns, if
possible, an answer from a structured knowledge source
(Chu-Carroll et al, 2003a).
In the rest of this paper, we briefly describe our two
general-purpose answering agents. We then focus on a
multi-level answer resolution algorithm, applicable at dif-
ferent points in the QA process of these two answering
agents. Finally, we discuss experiments conducted to dis-
cover effective methods for combining results from mul-
tiple answering agents.
3 Component Answering Agents
We focus on two end-to-end answering agents designed
to answer short, fact-seeking questions from a collection
of text documents, as motivated by the requirements of
the TREC QA track (Voorhees, 2003). Both answer-
ing agents adopt the classic pipeline architecture, con-
sisting roughly of question analysis, passage retrieval,
and answer selection components. Although the answer-
ing agents adopt fundamentally different strategies in
their individual components, they have performed quite
comparably in past TREC QA tracks (Voorhees, 2001;
Voorhees, 2002).
3.1 Knowledge-Based Answering Agent
Our first answering agent utilizes a primarily knowledge-
driven approach, based on Predictive Annotation (Prager
et al, 2000). A key characteristic of this approach is that
potential answers, such as person names, locations, and
dates, in the corpus are predictively annotated. In other
words, the corpus is indexed not only with keywords, as
is typical for most search engines, but also with the se-
mantic classes of these pre-identified potential answers.
During the question analysis phase, a rule-based mech-
anism is employed to select one or more expected an-
swer types, from a set of about 80 classes used in the
predictive annotation process, along with a set of ques-
tion keywords. A weighted search engine query is then
constructed from the keywords, their morphological vari-
ations, synonyms, and the answer type(s). The search en-
gine returns a hit list of typically 10 passages, each con-
sisting of 1-3 sentences. The candidate answers in these
passages are identified and ranked based on three criteria:
1) match in semantic type between candidate answer and
expected answer, 2) match in weighted grammatical rela-
tionships between question and answer passages, and 3)
frequency of answer in candidate passages (redundancy).
The answering agent returns the top n ranked candidate
answers along with a confidence score for each answer.
3.2 Statistical Answering Agent
The second answering agent takes a statistical approach
to question answering (Ittycheriah, 2001; Ittycheriah et
al., 2001). It models the distribution p(c|q, a), which
measures the ?correctness? (c) of an answer (a) to a ques-
tion (q), by introducing a hidden variable representing the
answer type (e) as follows:
p(c|q, a) =
?
e p(c, e|q, a)
=
?
e p(c|e, q, a)p(e|q, a)
p(e|q, a) is the answer type model which predicts, from
the question and a proposed answer, the answer type they
both satisfy. p(c|e, q, a) is the answer selection model.
Given a question, an answer, and the predicted answer
type, it seeks to model the correctness of this configura-
tion. These distributions are modeled using a maximum
entropy formulation (Berger et al, 1996), using training
data which consists of human judgments of question an-
swer pairs. For the answer type model, 13K questions
were annotated with 31 categories. For the answer selec-
tion model, 892 questions from the TREC 8 and TREC 9
QA tracks were used, along with 4K trivia questions.
During runtime, the question is first analyzed by the
answer type model, which selects one out of a set of 31
types for use by the answer selection model. Simultane-
ously, the question is expanded using local context anal-
ysis (Xu and Croft, 1996) with an encyclopedia, and the
top 1000 documents are retrieved by the search engine.
From these documents, the top 100 passages are chosen
that 1) maximize the question word match, 2) have the
desired answer type, 3) minimize the dispersion of ques-
tion words, and 4) have similar syntactic structures as the
question. From these passages, candidate answers are ex-
tracted and ranked using the answer selection model. The
top n candidate answers are then returned, each with an
associated confidence score.
4 Answer Resolution
Given two answering agents with the same pipeline archi-
tecture, there are multiple points in the process at which
(intermediate) results can be combined, as illustrated in
Figure 2. More specifically, it is possible for one answer-
ing agent to provide input to the other after the question
analysis, passage retrieval, and answer selection phases.
In PIQUANT, the knowledge based agent may accept in-
put from the statistical agent after each of these three
phases.1 The contributions from the statistical agent are
taken into consideration by the knowledge based answer-
ing agent in a phase-dependent fashion. The rest of this
section details our combination strategies for each phase.
4.1 Question-Level Combination
One of the key tasks of the question analysis component
is to determine the expected answer type, such as PERSON
for ?Who discovered America?? and DATE for ?When
did World War II end?? This information is taken into ac-
count by most existing QA systems when ranking candi-
date answers, and can also be used in the passage retrieval
process to increase the precision of candidate passages.
We seek to improve the knowledge-based agent?s
performance in passage retrieval and answer selection
through better answer type identification by consulting
the statistical agent?s expected answer type. This task,
however, is complicated by the fact that QA systems em-
ploy different sets of answer types, often with different
granularities and/or with overlapping types. For instance,
while one system may generate ROYALTY for the ques-
tion ?Who was the King of France in 1702??, another
system may produce PERSON as the most specific an-
swer type in its repertoire. This is quite a serious problem
for us as the knowledge based agent uses over 80 answer
types while the statistical agent adopts only 31 categories.
In order to distinguish actual answer type discrepan-
cies from those due to granularity differences, we first
manually created a mapping between the two sets of an-
swer types. This mapping specifies, for each answer type
used by the statistical agent, a set of possible correspond-
ing types used by the knowledge-based agent. For exam-
ple, the GEOLOGICALOBJ class is mapped to a set of finer
grained classes: RIVER, MOUNTAIN, LAKE, and OCEAN.
At processing time, the statistical agent?s answer type
is mapped to the knowledge-based agent?s classes (SA-
1Although it is possible for the statistical agent to receive
input from the knowledge based agent as well, we have not pur-
sued that option because of implementation issues.
Question
Analysis 1
Passage
Retrieval 1
Answer
Selection 1
passages answers
Question
Analysis 2
Passage
Retrieval 2
Answer
Selection 2
Agent 1 (Knowledge-Based)
Agent 2 (Statistical)
question
typeQFrame
Figure 2: Answer Resolution Strategies
types), which are then merged with the answer type(s) se-
lected by the knowledge-based agent itself (KBA-types)
as follows:
1. If the intersection of KBA-types and SA-types is
non-null, i.e., the two agents produced consistent an-
swer types, then the merged set is KBA-types.
2. Otherwise, the two sets of answer types are truly
in disagreement, and the merged set is the union of
KBA-types and SA-types.
The merged answer types are then used by the
knowledge-based agent in further processing.
4.2 Passage-Level Combination
The passage retrieval component selects, from a large text
corpus, a small number of short passages from which an-
swers are identified. Oftentimes, multiple passages that
answer a question are retrieved. Some of these passages
may be better suited than others for the answer selection
algorithm employed downstream. For example, consider
?When was Benjamin Disraeli prime minister??, whose
answer can be found in both passages below:
1. Benjamin Disraeli, who had become prime minister
in 1868, was born into Judaism but was baptized a
Christian at the age of 12.
2. France had a Jewish prime minister in 1936, Eng-
land in 1868, and Spain, of all countries, in 1835,
but none of them, Leon Blum, Benjamin Disraeli or
Juan Alvarez Mendizabel, were devoutly observant,
as Lieberman is.
Although the correct answer, 1868, is present in both
passages, it is substantially easier to identify the answer
from the first passage, where it is directly stated, than
from the second passage, where recognition of parallel
constructs is needed to identify the correct answer.
Because of strategic differences in question analysis
and passage retrieval, our two answering agents often re-
trieve different passages for the same question. Thus, we
perform passage-level combination to make a wider va-
riety of passages available to the answer selection com-
ponent, as shown in Figure 2. The potential advantages
are threefold. First, passages from agent 2 may contain
answers absent in passages retrieved by agent 1. Sec-
ond, agent 2 may have retrieved passages better suited for
the downstream answer selection algorithm than those re-
trieved by agent 1. Third, passages from agent 2 may con-
tain additional occurrences of the correct answer, which
boosts the system?s confidence in the answer through the
redundancy measure.2
Our passage-level combination algorithm adds to the
passages extracted by the knowledge-based agent the top-
ranked passages from the statistical agent that contain
candidate answers of the right type. More specifically,
the statistical agent?s passages are semantically annotated
and the top 10 passages containing at least one candidate
of the expected answer type(s) are selected.3
4.3 Answer-Level Combination
The answer selection component identifies, from a set
of passages, the top n answers for the given question,
with their associated confidence scores. An answer-level
combination algorithm takes the top answer(s) from the
individual answering agents and determines the overall
best answer(s). Of our three combination algorithms, this
most closely resembles traditional ensemble methods, as
voting takes place among the end results of individual an-
2On the other hand, such redundancy may result in error
compounding, as discussed in Section 5.3.
3We selected the top 10 passages so that the same number
of passages are considered from both answering agents.
swering agents to determine the final output of the ensem-
ble.
We developed two answer-level combination algo-
rithms, both utilizing a simple confidence-based voting
mechanism, based on the premise that answers selected
by both agents with high confidence are more likely to
be correct than those identified by only one agent.4 In
both algorithms, named entity normalization is first per-
formed on all candidate answers considered. In the first
algorithm, only the top answer from each agent is taken
into account. If the two top answers are equivalent, the
answer is selected with the combined confidence from
both agents; otherwise, the more confident answer is se-
lected.5 In the second algorithm, the top 5 answers from
each agent are allowed to participate in the voting pro-
cess. Each instance of an answer votes with a weight
equal to its confidence value and the weights of equiv-
alent answers are again summed. The answer with the
highest weight, or confidence value, is selected as the
system?s final answer. Since in our evaluation, the second
algorithm uniformly outperforms the first, it is adopted as
our answer-level combination algorithm in the rest of the
paper.
5 Performance Evaluation
5.1 Experimental Setup
To assess the effectiveness of our multi-level answer res-
olution algorithm, we devised experiments to evaluate the
impact of the question, passage, and answer-level combi-
nation algorithms described in the previous section.
The baseline systems are the knowledge-based and sta-
tistical agents performing individually against a single
reference corpus. In addition, our earlier experiments
showed that when employing a single answer finding
strategy, consulting multiple text corpora yielded better
performance than using a single corpus. We thus con-
figured a version of our knowledge-based agent to make
use of three available text corpora,6 the AQUAINT cor-
pus (news articles from 1998-2000), the TREC corpus
(news articles from 1988-1994),7 and a subset of the En-
cyclopedia Britannica. This multi-source version of the
knowledge-based agent will be used in all answer resolu-
tion experiments in conjunction with the statistical agent.
We configured multiple versions of PIQUANT to eval-
uate our question, passage, and answer-level combination
4In future work we will be investigating weighted voting
schemes based on question features.
5The confidence values from both answering agents are nor-
malized to be between 0 and 1.
6The statistical agent is currently unable to consult multiple
corpora.
7Both the AQUAINT and TREC corpora are available from
the Linguistics Data Consortium, http://www.ldc.org.
algorithms individually and cumulatively. For cumula-
tive effects, we 1) combined the algorithms pair-wise,
and 2) employed all three algorithms together. The two
test sets were selected from the TREC 10 and 11 QA
track questions (Voorhees, 2002; Voorhees, 2003). For
both test sets, we eliminated those questions that did not
have known answers in the reference corpus. Further-
more, from the TREC 10 test set, we discarded all defini-
tion questions,8 since the knowledge-based agent adopts
a specialized strategy for handling definition questions
which greatly reduces potential contributions from other
answering agents. This results in a TREC 10 test set of
313 questions and a TREC 11 test set of 453 questions.
5.2 Experimental Results
We ran each of the baseline and combined systems on the
two test sets. For each run, the system outputs its top
answer and its confidence score for each question. All
answers for a run are then sorted in descending order of
the confidence scores. Two established TREC QA eval-
uation metrics are adopted to assess the results for each
run as follows:
1. % Correct: Percentage of correct answers.
2. Average Precision: A confidence-weighted score
that rewards systems with high confidence in cor-
rect answers as follows, where N is the number of
questions:
1
N
N?
i=1
# correct up to question i/i
Table 1 shows our experimental results. The top sec-
tion shows the comparable baseline results from the sta-
tistical agent (SA-SS) and the single-source knowledge-
based agent (KBA-SS). It also includes results for the
multi-source knowledge-based agent (KBA-MS), which
improve upon those for its single-source counterpart
(KBA-SS).
The middle section of the table shows the answer
resolution results, including applying the question, pas-
sage, and answer-level combination algorithms individu-
ally (Q, P, and A, respectively), applying them pair-wise
(Q+P, P+A, and Q+A), and employing all three algo-
rithms (Q+P+A). Finally, the last row of the table shows
the relative improvement by comparing the best perform-
ing system configuration (highlighted in boldface) with
the better performing single-source, single-strategy base-
line system (SA-SS or KBA-SS, in italics).
Overall, PIQUANT?s multi-strategy and multi-source
approach achieved a 35.0% relative improvement in the
8Definition questions were intentionally excluded by the
track coordinator in the TREC 11 test set.
TREC 10 (313) TREC 11 (453)
% Corr Avg Prec % Corr Avg Prec
SA-SS 36.7% 0.569 32.9% 0.534
KBA-SS 39.6% 0.595 32.5% 0.531
KBA-MS 43.8% 0.641 38.2% 0.622
Q 44.7% 0.647 38.9% 0.632
P 49.5% 0.661 40.0% 0.627
A 49.5% 0.712 43.5% 0.704
Q+P 48.9% 0.656 41.1% 0.640
P+A 51.1% 0.711 44.2% 0.686
Q+A 49.8% 0.716 43.9% 0.709
Q+P+A 50.8% 0.706 44.4% 0.690
rel. improv. 29.0% 20.3% 35.0% 32.8%
Table 1: Experimental Results
number of correct answers and a 32.8% improvement in
average precision on the TREC 11 data set. Of the com-
bined improvement, approximately half was achieved by
the multi-source aspect of PIQUANT, while the other half
was obtained by PIQUANT?s multi-strategy feature. Al-
though the absolute average precision values are com-
parable on both test sets and the absolute percentage of
correct answers is lower on the TREC 11 data, the im-
provement is greater on TREC 11 in both cases. This
is because the TREC 10 questions were taken into ac-
count for manual rule refinement in the knowledge-based
agent, resulting in higher baselines on the TREC 10 test
set. We believe that the larger improvement on the previ-
ously unseen TREC 11 data is a more reliable estimate of
PIQUANT?s performance on future test sets.
We applied an earlier version of our combination algo-
rithms, which performed between our current P and P+A
algorithms, in our submission to the TREC 11 QA track.
Using the average precision metric, that version of PI-
QUANT was among the top 5 best performing systems
out of 67 runs submitted by 34 groups.
5.3 Discussion and Analysis
A cursory examination of the results in Table 1 allows
us to draw two general conclusions about PIQUANT?s
performance. First, all three combination algorithms ap-
plied individually improved upon the baseline using both
evaluation metrics on both test sets. In addition, overall
performance is generally better the later in the process
the combination occurs, i.e., the answer-level combina-
tion algorithm outperformed the passage-level combina-
tion algorithm, which in turn outperformed the question-
level combination algorithm. Second, the cumulative im-
provement from multiple combination algorithms is in
general greater than that from the components. For in-
stance, the Q+A algorithm uniformly outperformed the Q
and A algorithms alone. Note, however, that the Q+P+A
algorithm achieved the highest performance only on the
TREC 11 test set using the % correct metric. We believe
KBA
TREC 10 (313) TREC 11 (453)
+ - + -
SA + 185 43 254 58
- 24 61 41 100
Table 2: Passage Retrieval Analysis
that this is because of compounding errors that occurred
during the multiple combination process.
In ensemble methods, the individual components must
make different mistakes in order for the combined sys-
tem to potentially perform better than the component sys-
tems (Dietterich, 1997). We examined the differences
in results between the two answering agents from their
question analysis, passage retrieval, and answer selection
components. We focused our analysis on the potential
gain/loss from incorporating contributions from the sta-
tistical agent, and how the potential was realized as actual
performance gain/loss in our end-to-end system.
At the question level, we examined those questions
for which the two agents proposed incompatible answer
types. On the TREC 10 test set, the statistical agent in-
troduced correct answer types in 6 cases and incorrect
answer types in 9 cases. As a result, in some cases the
question-level combination algorithm improved system
performance (comparing A and Q+A) and in others it
degraded performance (comparing P and Q+P). On the
other hand, on the TREC 11 test set, the statistical agent
introduced correct and incorrect answer types in 15 and
6 cases, respectively. As a result, in most cases perfor-
mance improved when the question-level combination al-
gorithm was invoked. The difference in question analysis
performance again reflects the fact that TREC 10 ques-
tions were used in question analysis rule refinement in
the knowledge-based agent.
At the passage level, we examined, for each ques-
tion, whether the candidate passages contained the cor-
rect answer. Table 2 shows the distribution of ques-
tions for which correct answers were (+) and were not
(-) present in the passages for both agents. The bold-
faced cells represent questions for which the statistical
agent retrieved passages with correct answers while the
knowledge-based agent did not. There were 43 and 58
such questions in the TREC 10 and TREC 11 test sets, re-
spectively, and employing the passage-level combination
algorithm resulted only in an additional 18 and 8 correct
answers on each test set. This is because the statistical
agent?s proposes in its 10 passages, on average, 29 candi-
date answers, most of which are incorrect, of the proper
semantic type per question. As the downstream answer
selection component takes redundancy into account in an-
swer ranking, incorrect answers may reinforce one an-
other and become top ranked answers. This suggests that
KBA
TREC 10 (313) TREC 11 (453)
1st 2-5th none 1st 2-5th none
SA 1st 66 22 26 93 21 35
2-5th 26 9 13 29 19 22
none 45 14 92 51 21 162
Table 3: Answer Voting Analysis
the relative contributions of our answer selection features
may not be optimally tuned for our multi-agent approach
to QA. We plan to investigate this issue in future work.
At the answer level, we analyzed each agent?s top 5
answers, used in the combination algorithm?s voting pro-
cess. Table 3 shows the distribution of questions for
which an answer was found in 1st place, in 2nd-5th place,
and not found in top 5. Since we employ a linear vot-
ing strategy based on confidence scores, we classify the
cells in Table 3 as follows based on the perceived likeli-
hood that the correct answers for questions in each cell
wins in the voting process. The boldfaced and underlined
cells contain highly likely candidates, since a correct an-
swer was found in 1st place by both agents.9 The bold-
faced cells consist of likely candidates, since a 1st place
correct answer was supported by a 2nd-5th place answer.
The italicized and underlined cells contain possible can-
didates, while the rest of the cells cannot produce correct
1st place answers using our current voting algorithm. On
TREC 10 data, 194 questions fall into the highly likely,
likely, and possible categories, out of which the voting al-
gorithm successfully selected 155 correct answers in 1st
place. On TREC 11 data, 197 correct answers were se-
lected out of 248 questions that fall into these categories.
These results represent success rates of 79.9% and 79.4%
for our answer-level combination algorithm on the two
test sets.
6 Related Work
There has been much work in employing ensemble meth-
ods to increase system performance in machine learning.
In NLP, such methods have been applied to tasks such
as POS tagging (Brill and Wu, 1998), word sense dis-
ambiguation (Pedersen, 2000), parsing (Henderson and
Brill, 1999), and machine translation (Frederking and
Nirenburg, 1994).
In question answering, a number of researchers have
investigated federated systems for identifying answers to
questions. For example, (Clarke et al, 2003) and (Lin et
al., 2003) employ techniques for utilizing both unstruc-
9These cells are not marked as definite because in a small
number of cases, the two answers are not equivalent. For exam-
ple, for the TREC 9 question, ?Who is the emperor of Japan??,
Hirohito, Akihito, and Taisho are all considered correct answers
based on the reference corpus.
tured text and structured databases for question answer-
ing. However, the approaches taken by both these sys-
tems differ from ours in that they enforce an order be-
tween the two strategies by attempting to locate answers
in structured databases first for select question types and
falling back to unstructured text when the former fails,
while we explore both options in parallel and combine
the results from multiple answering agents.
The multi-agent approach to question answering most
similar to ours is that by Burger et al (2003). They
applied ensemble methods to combine the 67 runs sub-
mitted to the TREC 11 QA track, using an unweighted
centroid method for selecting among the 67 proposed an-
swers for each question. However, their combined sys-
tem did not outperform the top scoring system(s). Fur-
thermore, their approach differs from ours in that they fo-
cused on combining the end results of a large number of
systems, while we investigated a tightly-coupled design
for combining two answering agents.
7 Conclusions
In this paper, we introduced a multi-strategy and multi-
source approach to question answering that enables com-
bination of answering agents adopting different strategies
and consulting multiple knowledge sources. In partic-
ular, we focused on two answering agents, one adopt-
ing a knowledge-based approach and one using statistical
methods. We discussed our answer resolution component
which employs a multi-level combination algorithm that
allows for resolution at the question, passage, and answer
levels. Best performance using the % correct metric was
achieved by the three-level algorithm that combines af-
ter each stage, while highest average precision was ob-
tained by a two-level algorithm merging at the question
and answer levels, supporting a tightly-coupled design
for multi-agent question answering. Our experiments
showed that our best performing algorithms achieved a
35.0% relative improvement in the number of correct an-
swers and a 32.8% improvement in average precision on
a previously unseen test set.
Acknowledgments
We would like to thank Dave Ferrucci, Chris Welty, and
Salim Roukos for helpful discussions, Diane Litman and
the anonymous reviewers for their comments on an ear-
lier draft of this paper. This work was supported in
part by the Advanced Research and Development Ac-
tivity (ARDA)?s Advanced Question Answering for In-
telligence (AQUAINT) Program under contract number
MDA904-01-C-0988.
References
Adam L. Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Eric Brill and Jun Wu. 1998. Classifier combination for
improved lexical disambiguation. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics, pages 191?195.
John D. Burger, Lisa Ferro, Warren Greiff, John Hender-
son, Marc Light, and Scott Mardis. 2003. MITRE?s
Qanda at TREC-11. In Proceedings of the Eleventh
Text Retrieval Conference. To appear.
Jennifer Chu-Carroll, David Ferrucci, John Prager, and
Christopher Welty. 2003a. Hybridization in ques-
tion answering systems. In Working Notes of the AAAI
Spring Symposium on New Directions in Question An-
swering, pages 116?121.
Jennifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003b. A
multi-strategy and multi-source approach to question
answering. In Proceedings of the Eleventh Text Re-
trieval Conference. To appear.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
C.L.A. Clarke, G.V. Cormack, G. Kemkes, M. Laszlo,
T.R. Lynam, E.L. Terra, and P.L. Tilker. 2003. Statis-
tical selection of exact answers. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Thomas G. Dietterich. 1997. Machine learning research:
Four current directions. AI Magazine, 18(4):97?136.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Fourth
Conference on Applied Natural Language Processing.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the 4th Conference on Em-
pirical Methods in Natural Language Processing.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2001. Question answering
in Webclopedia. In Proceedings of the Ninth Text RE-
trieval Conference, pages 655?664.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. Question answering using
maximum entropy components. In Proceedings of the
2nd Conference of the North American Chapter of the
Association for Computational Linguistics, pages 33?
39.
Abraham Ittycheriah. 2001. Trainable Question Answer-
ing Systems. Ph.D. thesis, Rutgers - The State Univer-
sity of New Jersey.
Douglas B. Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11).
Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory Mar-
ton, and Stefanie Tellex. 2003. Extracting an-
swers from the web using knowledge annotation and
knowledge mining techniques. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
George Miller. 1995. Wordnet: A lexical database for
English. Communications of the ACM, 38(11).
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 563?570.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive Bayesian classifiers for word sense
disambiguation. In Proceedings of the 1st Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 63?69.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive anno-
tation. In Proceedings of the 23rd SIGIR Conference,
pages 184?191.
John Prager, Dragomir Radev, and Krzysztof Czuba.
2001. Answering what-is questions by virtual anno-
tation. In Proceedings of Human Language Technolo-
gies Conference, pages 26?30.
Ellen M. Voorhees. 2001. Overview of the TREC-9
question answering track. In Proceedings of the 9th
Text Retrieval Conference, pages 71?80.
Ellen M. Voorhees. 2002. Overview of the TREC 2001
question answering track. In Proceedings of the 10th
Text Retrieval Conference, pages 42?51.
Ellen M. Voorhees. 2003. Overview of the TREC
2002 question answering track. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of the 19th SIGIR Conference, pages 4?11.
Question Answering using Constraint Satisfaction:                                       
QA-by-Dossier-with-Constraints  
John Prager  
T.J. Watson Research Ctr. 
Yorktown Heights 
N.Y. 10598 
jprager@us.ibm.com 
Jennifer Chu-Carroll 
T.J. Watson Research Ctr. 
Yorktown Heights  
N.Y. 10598 
jencc@us.ibm.com 
Krzysztof Czuba  
T.J. Watson Research Ctr. 
Yorktown Heights  
N.Y. 10598 
kczuba@us.ibm.com 
 
 
 
Abstract 
QA-by-Dossier-with-Constraints is a new ap-
proach to Question Answering whereby candi-
date answers? confidences are adjusted by 
asking auxiliary questions whose answers con-
strain the original answers.  These constraints 
emerge naturally from the domain of interest, 
and enable application of real-world knowledge 
to QA.  We show that our approach signifi-
cantly improves system performance (75% rela-
tive improvement in F-measure on select 
question types) and can create a ?dossier? of in-
formation about the subject matter in the origi-
nal question. 
1 Introduction 
Traditionally, Question Answering (QA) has 
drawn on the fields of Information Retrieval, Natural 
Language Processing (NLP), Ontologies, Data Bases 
and Logical Inference, although it is at heart a prob-
lem of NLP.  These fields have been used to supply 
the technology with which QA components have 
been built.  We present here a new methodology 
which attempts to use QA holistically, along with 
constraint satisfaction, to better answer questions, 
without requiring any advances in the underlying 
fields. 
Because NLP is still very much an error-prone 
process, QA systems make many mistakes; accord-
ingly, a variety of methods have been developed to 
boost the accuracy of their answers.  Such methods 
include redundancy (getting the same answer from 
multiple documents, sources, or algorithms), deep 
parsing of questions and texts (hence improving the 
accuracy of confidence measures), inferencing 
(proving the answer from information in texts plus 
background knowledge) and sanity-checking (veri-
fying that answers are consistent with known facts).  
To our knowledge, however, no QA system deliber-
ately asks additional questions in order to derive 
constraints on the answers to the original questions.  
We have found empirically that when our own 
QA system?s (Prager et al, 2000; Chu-Carroll et al, 
2003) top answer is wrong, the correct answer is 
often present later in the ranked answer list.  In other 
words, the correct answer is in the passages re-
trieved by the search engine, but the system was un-
able to sufficiently promote the correct answer 
and/or deprecate the incorrect ones.  Our new ap-
proach of QA-by-Dossier-with-Constraints (QDC) 
uses the answers to additional questions to provide 
more information that can be used in ranking candi-
date answers to the original question.  These auxil-
iary questions are selected such that natural 
constraints exist among the set of correct answers.  
After issuing both the original question and auxiliary 
questions, the system evaluates all possible combi-
nations of the candidate answers and scores them by 
a simple function of both the answers? intrinsic con-
fidences, and how well the combination satisfies the 
aforementioned constraints.  Thus we hope to im-
prove the accuracy of an essentially NLP task by 
making an end-run around some of the more diffi-
cult problems in the field. 
We describe QDC and experiments to evaluate its 
effectiveness. Our results show that on our test set, 
substantial improvement is achieved by using con-
straints, compared with our baseline system, using 
standard evaluation metrics. 
2 Related Work 
Logic and inferencing have been a part of Ques-
tion-Answering since its earliest days.  The first 
such systems employed natural-language interfaces 
to expert systems, e.g.  SHRDLU (Winograd, 1972), 
or to databases e.g. LUNAR (Woods, 1973) and 
LIFER/LADDER (Hendrix et al 1977).  CHAT-80 
(Warren & Pereira, 1982) was a DCG-based NL-
query system about world geography, entirely in 
Prolog.  In these systems, the NL question is trans-
formed into a semantic form, which is then proc-
essed further; the overall architecture and system 
operation is very different from today?s systems, 
however, primarily in that there is no text corpus to 
process. 
Inferencing is used in at least two of the more 
visible systems of the present day.  The LCC system 
(Moldovan & Rus, 2001) uses a Logic Prover to 
establish the connection between a candidate answer 
passage and the question.  Text terms are converted 
to logical forms, and the question is treated as a goal 
which is ?proven?, with real-world knowledge being 
provided by Extended WordNet.  The IBM system 
PIQUANT (Chu-Carroll et al, 2003) uses Cyc (Le-
nat, 1995) in answer verification.  Cyc can in some 
cases confirm or reject candidate answers based on 
its own store of instance information; in other cases, 
primarily of a numerical nature, Cyc can confirm 
whether candidates are within a reasonable range 
established for their subtype.   
At a more abstract level, the use of constraints 
discussed in this paper can be viewed as simply an 
example of finding support (or lack of it) for candi-
date answers.  Many current systems (see, e.g. 
(Clarke et al, 2001), (Prager et al, 2004)) employ 
redundancy as a significant feature of operation:  if 
the same answer appears multiple times in an inter-
nal top-n list, whether from multiple sources or mul-
tiple algorithms/agents, it is given a confidence 
boost, which will affect whether and how it gets re-
turned to the end-user. 
 Finally, our approach is somewhat reminiscent of 
the scripts introduced by Schank (Schank et al, 
1975, and see also Lehnert, 1978). In order to gener-
ate meaningful auxiliary questions and constraints, 
we need a model (?script?) of the situation the ques-
tion is about.  Among others, we have identified one 
such script modeling the human life cycle that seems 
common to different question types regarding peo-
ple.   
3 Introducing QDC 
QA-by-Dossier-with-Constraints is an extension 
of on-going work of ours called QA-by-Dossier 
(QbD) (Prager et al, 2004).  In the latter, defini-
tional questions of the form ?Who/What is X? are 
answered by asking a set of specific factoid ques-
tions about properties of X.  So if X is a person, for 
example, these auxiliary questions may be about 
important dates and events in the person?s life-cycle, 
as well as his/her achievement.  Likewise, question 
sets can be developed for other entities such as or-
ganizations, places and things.  
QbD employs the notion of follow-on questions.  
Given an answer to a first-round question, the sys-
tem can ask more specific questions based on that 
knowledge.  For example, on discovering a person?s 
profession, it can ask occupation-specific follow-on 
questions: if it finds that people are musicians, it can 
ask what they have composed, if it finds they are 
explorers, then what they have discovered, and so 
on. 
QA-by-Dossier-with-Constraints extends this ap-
proach by capitalizing on the fact that a set of an-
swers about a subject must be mutually consistent, 
with respect to constraints such as time and geogra-
phy.   The essence of the QDC approach is to ini-
tially return instead of the best answer to 
appropriately selected factoid questions, the top n 
answers (we use n=5), and to choose out of this top 
set the highest confidence answer combination that 
satisfies consistency constraints. 
We illustrate this idea by way of the example, 
?When did Leonardo da Vinci paint the Mona 
Lisa??.  Table 1 shows our system?s top answers to 
this question, with associated scores in the range    
0-1. 
 
 Score Painting Date 
1 .64 2000 
2 .43 1988 
3 .34 1911 
4 .31 1503 
5 .30 1490 
 
Table 1.  Answers for ?When did Leonardo da 
Vinci paint the Mona Lisa?? 
 
The correct answer is ?1503?, which is in 4th 
place, with a low confidence score.  Using QA-by-
Dossier, we ask two related questions ?When was 
Leonardo da Vinci born?? and ?When did Leonardo 
da Vinci die??  The answers to these auxiliary ques-
tions are shown in Table 2.   
Given common knowledge about a person?s life 
expectancy and that a painting must be produced 
while its author is alive, we observe that the best 
dates proposed in Table 2 consistent with one an-
other are that Leonardo da Vinci was born in 1452, 
died in 1519, and painted the Mona Lisa in 1503.  
[The painting date of 1490 also satisfies the con-
straints, but with a lower confidence.]  We will ex-
amine the exact constraints used a little later.  This 
example illustrates how the use of auxiliary ques-
tions helps constrain answers to the original ques-
tion, and promotes correct answers with initial low 
confidence scores.  As a side-effect, a short dossier 
is produced. 
 
 
 Score Born  Score Died 
1 .66 1452  .99 1519 
2 .12 1519  .98 1989 
3 .04 1920  .96 1452 
4 .04 1987  .60 1988 
5 .04 1501  .60 1990 
Table 2.  Answers for auxiliary questions ?When 
was Leonardo da Vinci born?? and ?When did Leo-
nardo da Vinci die??. 
3.1 Reciprocal Questions 
QDC also employs the notion of reciprocal ques-
tions.  These are a type of follow-on question used 
solely to provide constraints, and do not add to the 
dossier.  The idea is simply to double-check the an-
swer to a question by inverting it, substituting the 
first-round answer and hoping to get the original 
subject back.  For example, to double-check ?Sac-
ramento? as the answer to ?What is the capital of 
California?? we would ask ?Of what state is Sacra-
mento the capital??.  The reciprocal question would 
be asked of all of the candidate answers, and the 
confidences of the answers to the reciprocal ques-
tions would contribute to the selection of the opti-
mum answer.  We will discuss later how this 
reciprocation may be done automatically.  In a sepa-
rate study of reciprocal questions (Prager et al, 
2004), we demonstrated an increase in precision 
from .43 to .95, with only a 30% drop in recall. 
Although the reciprocal questions seem to be 
symmetrical and thus redundant, their power stems 
from the differences in the search for answers inher-
ent in our system. The search is primarily based on 
the expected answer type (STATE vs. CAPITAL in 
the above example). This results in different docu-
ment sets being passed to the answer selection mod-
ule. Subsequently, the answer selection module 
works with a different set of syntactic and semantic 
relationships, and the process of asking a reciprocal 
question ends up looking more like the process of 
asking an independent one. The only difference be-
tween this and the ?regular? QDC case is in the type 
of constraint applied to resolve the resulting answer 
set. 
3.2 Applying QDC 
In order to automatically apply QDC during ques-
tion answering, several problems need to be ad-
dressed.  First, criteria must be developed to 
determine when this process should be invoked.  
Second, we must identify the set of question types 
that would potentially benefit from such an ap-
proach, and, for each question type, develop a set of 
auxiliary questions and appropriate constraints 
among the answers.  Third, for each question type, 
we must determine how the results of applying con-
straints should be utilized.  
3.2.1 When to apply QDC 
To address these questions we must distinguish 
between ?planned? and ?ad-hoc? uses of QDC.  For 
answering definitional questions (?Who/what is 
X??) of the sort used in TREC2003, in which collec-
tions of facts can be gathered by QA-by-Dossier, we 
can assume that QDC is always appropriate.  By 
defining broad enough classes of entities for which 
these questions might be asked (e.g. people, places, 
organizations and things, or major subclasses of 
these), we can for each of these classes manually 
establish once and for all a set of auxiliary questions 
for QbD and constraints for QDC.  This is the ap-
proach we have taken in the experiments reported 
here.  We are currently working on automatically 
learning effective auxiliary questions for some of 
these classes. 
In a more ad-hoc situation, we might imagine that 
a simple variety of QDC will be invoked using 
solely reciprocal questions whenever the difference 
between the scores of the first and second answer is 
below a certain threshold.    
3.2.2 How to apply QDC 
We will posit three methods of generating auxil-
iary question sets: 
o By hand 
o Through a structured repository, such as a 
knowledge-base of real-world information 
o Through statistical techniques tied to a machine-
learning algorithm, and a text corpus. 
We think that all three methods are appropriate, 
but we initially concentrate on the first for practical 
reasons.  Most TREC-style factoid questions are 
about people, places, organizations, and things, and 
we can generate generic auxiliary question sets for 
each of these classes.  Moreover, the purpose of this 
paper is to explain the QDC methodology and to 
investigate its value.   
3.2.3 Constraint Networks 
The constraints that apply to a given situation can 
be naturally represented in a network, and we find it 
useful for visualization purposes to depict the con-
straints graphically.  In such a graph the entities and 
values are represented as nodes, and the constraints 
and questions as edges.   
It is not clear how possible, or desirable, it is to 
automatically develop such constraint networks 
(other than the simple one for reciprocal questions), 
since so much real-world knowledge seems to be 
required.  To illustrate, let us look at the constraints 
required for the earlier example.  A more complex 
constraint system is used in our experiments de-
scribed later.  For our Leonardo da Vinci example, 
the set of constraints applied can be expressed as 
follows1: 
 
Date(Died) <= Date(Born) + 100 
Date(Painting) >=  Date(Born) + 7 
Date(Painting) <=  Date(Died) 
 
The corresponding graphical representation is in 
Figure 1.  Although the numerical constants in these 
constraints betray a certain arbitrariness, we found it 
a useful practice to find a middle ground between 
absolute minima or maxima that the values can 
achieve and their likely values.  Furthermore, al-
though these constraints are manually derived for 
our prototype system, they are fairly general for the 
human life-cycle and can be easily reused for other, 
similar questions, or for more complex dossiers, as 
described below. 
 
 
 
Figure 1.  Constraint Network for Leonardo ex-
ample.  Dashed lines represent question-answer 
pairs, solid lines constraints between the answers. 
We also note that even though a constraint net-
work might have been inspired by and centered 
around a particular question, once the network is 
established, any question employed in it could be the 
end-user question that triggers it. 
There exists the (general) problem of when more 
than one set of answers satisfies our constraints.  
Our approach is to combine the first-round scores of 
the individual answers to provide a score for the 
dossier as a whole.  There are several ways to do 
this, and we found experimentally that it does not 
appear critical exactly how this is done.  In the ex-
ample in the evaluation we mention one particular 
combination algorithm. 
3.2.4 Kinds of constraint network 
There are an unlimited number of possible con-
straint networks that can be constructed.  We have 
experimented with the following: 
Timelines.  People and even artifacts have life-
cycles.  The examples in this paper exploit these. 
                                                        
1 Painting is only an example of an activity in these constraints. 
Any other achievement that is usually associated with adulthood 
can be used. 
Geographic (?Where is X?).  Neighboring entities 
are in the same part of the world.   
Kinship (?Who is married to X?).  Most kinship 
relationships have named reciprocals e.g. husband-
wife, parent-child, and cousin-cousin.  Even though 
these are not in practice one-one relationships, we 
can take advantage of sufficiency even if necessity is 
not entailed. 
Definitional (?What is X??, ?What does XYZ stand 
for??)   For good definitions, a term and its defini-
tion are interchangeable. 
Part-whole.  Sizes of parts are no bigger than sizes 
of wholes.  This fact can be used for populations, 
areas, etc. 
3.2.5 QDC potential 
We performed a manual examination of the 500 
TREC2002 questions2 to see for how many of these 
questions the QDC framework would apply.  Being 
a manual process, these numbers provide an upper 
bound on how well we might expect a future auto-
matic process to work.  
We noted that for 92 questions (18%) a non-
trivial constraint network of the above kinds would 
apply.  For a total of 454 questions (91%), a simple 
reciprocal constraint could be generated.  However, 
for 61 of those, the reciprocal question was suffi-
ciently non-specific that the sought reciprocal an-
swer was unlikely to be found in a reasonably-sized 
hit-list.  For example, the reciprocal question to 
?How did Mickey Mantle die?? would be ?Who died 
of cancer??  However, we can imagine using other 
facts in the dossier to craft the question, giving us 
?What famous baseball player (or Yankees player) 
died of cancer??, giving us a much better chance of 
success.  For the simple reciprocation, though, sub-
tracting these doubtful instances leaves 79% of the 
questions appearing to be good candidates for QDC. 
4 Experimental Setup 
4.1 Test set generation 
To evaluate QDC, we had our system develop 
dossiers of people in the creative arts, unseen in pre-
vious TREC questions.  However, we wanted to use 
the personalities in past TREC questions as inde-
pendent indicators of appropriate subject matter.  
Therefore we collected all of the ?creative? people 
in the TREC9 question set, and divided them up into 
classes by profession, so we had, for example, male 
singers Bob Marley, Ray Charles, Billy Joel and 
Alice Cooper; poets William Wordsworth and 
Langston Hughes; painters Picasso, Jackson Pollock 
                                                        
2 This set did not contain definition questions, which, by our 
inspection, lend themselves readily to reciprocation. 
Birthdate 
Deathdate 
Leonardo Painting 
and Vincent Van Gogh, etc. ? twelve such groupings 
in all.  For each set, we entered the individuals in the 
?Google Sets? interface 
(http://labs.google.com/sets), which finds ?similar? 
entities to the ones entered.  For example, from our 
set of male singers it found: Elton John, Sting, Garth 
Brooks, James Taylor, Phil Collins, Melissa 
Etheridge, Alanis Morissette, Annie Lennox, Jack-
son Browne, Bryan Adams, Frank Sinatra and Whit-
ney Houston. 
Altogether, we gathered 276 names of creative 
individuals this way, after removing duplicates, 
items that were not names of individuals, and names 
that did not occur in our test corpus (the AQUAINT 
corpus).  We then used our system manually to help 
us develop ?ground truth? for a randomly selected 
subset of 109 names.  This ground truth served both 
as training material and as an evaluation key.  We 
split the 109 names randomly into a set of 52 for 
training and 57 for testing.  The training process 
used a hill-climbing method to find optimal values 
for three internal rejection thresholds.  In developing 
the ground truth we might have missed some in-
stances of assertions we were looking for, so the 
reported recall (and hence F-measure) figures should 
be considered to be upper bounds, but we believe the 
calculated figures are not far from the truth. 
4.2 QDC Operation 
The system first asked three questions for each 
subject X: 
 
 In what year was X born? 
 In what year did X die? 
 What compositions did X have? 
 
The third of these triggers our named-entity type 
COMPOSITION that is used for all kinds of titled 
works ? books, films, poems, music, plays and so 
on, and also quotations.  Our named-entity recog-
nizer has rules to detect works of art by phrases that 
are in apposition to ?the film ? ? or the ?the book 
? ? etc., and also captures any short phrase in quotes 
beginning with a capital letter.  The particular ques-
tion phrasing we used does not commit us to any 
specific creative verb.  This is of particular impor-
tance since it very frequently happens in text that 
titled works are associated with their creators by 
means of a possessive or parenthetical construction, 
rather than subject-verb-object. 
The top five answers, with confidences, are re-
turned for the born and died questions (subject to 
also passing a confidence threshold test).  The com-
positions question is treated as a list question, mean-
ing that all answers that pass a certain threshold are 
returned.  For each such returned work Wi, two addi-
tional questions are asked: 
 What year did X have Wi? 
 Who had Wi? 
 
The top 5 answers to each of these are returned, 
again as long as they pass a confidence threshold.  
We added a sixth answer ?NIL? to each of the date 
sets, with a confidence equal to the rejection thresh-
old.  (NIL is the code used in TREC ever since 
TREC10 to indicate the assertion that there is no 
answer in the corpus.)  We used a two stage con-
straint-satisfaction process: 
Stage 1:  For each work Wi for subject X, we 
added together its original confidence to the confi-
dence of the answer X in the answer set of the recip-
rocal question (if it existed ? otherwise we added 
zero).  If the total did not exceed a learned threshold 
(.50) the work was rejected. 
Stage 2.  For each subject, with the remaining 
candidate works we generated all possible combina-
tions of the date answers.  We rejected any combina-
tion that did not satisfy the following constraints: 
 
 DIED >= BORN + 7 
 DIED <= BORN + 100 
 WORK >= BORN + 7 
 WORK <= BORN + 100 
 WORK <= DIED 
 DIED <= WORK + 100 
 
The apparent redundancy here is because of the 
potential NIL answers for some of the date slots.  
We also rejected combinations of works whose 
years spanned more than 100 years (in case there 
were no BORN or DIED dates).  In performing these 
constraint calculations, NIL satisfied every test by 
fiat.  The constraint network we used is depicted in 
Figure 2. 
 
 
 
Figure 2.  Constraint Network for evaluation ex-
ample.  Dashed lines represent question-answer 
pairs, solid lines constraints between the answers. 
We used as a test corpus the AQUAINT corpus 
used in TREC-QA since 2002.  Since this was not 
the same corpus from which the test questions were 
generated (the Web), we acknowledged that there 
might be some difference in the most common spell-
ing of certain names, but we made no attempt to cor-
rect for this.  Neither did we attempt to normalize, 
translate or aggregate names of the titled works that 
were returned, so that, for example, ?Well-
Birthdate of X 
Deathdate of X 
Work Wi 
Author X Date of Wi 
Xi = Author of Wi 
Tempered Klavier? and ?Well-Tempered Clavier? 
were treated as different.  Since only individuals 
were used in the question set, we did not have in-
stances of problems we saw in training, such as 
where an ensemble (such as The Beatles) created a 
certain piece, which in turn via the reciprocal ques-
tion was found to have been written by a single per-
son (Paul McCartney).  The reverse situation was 
still possible, but we did not handle it.  We foresee a 
future version of our system having knowledge of 
ensembles and their composition, thus removing this 
restriction.  In general, a variety of ontological rela-
tionships could occur between the original individ-
ual and the discovered performer(s) of the work. 
We generated answer keys by reading the pas-
sages that the system had retrieved and from which 
the answers were generated, to determine ?truth?.  In 
cases of absent information in these passages, we 
did our own corpus searches.  This of course made 
the issue of evaluation of recall only relative, since 
we were not able to guarantee we had found all ex-
isting instances. 
We encountered some grey areas, e.g., if a paint-
ing appeared in an exhibition or if a celebrity en-
dorsed a product, then should the exhibition?s or 
product?s name be considered an appropriate ?work? 
of the artist?  The general perspective adopted was 
that we were not establishing or validating the nature 
of the relationship between an individual and a crea-
tive work, but rather its existence.  We answered 
?yes? if we subjectively felt the association to be 
both very strong and with the individual?s participa-
tion ? for example, Pamela Anderson and Playboy.  
However, books/plays about a person or dates of 
performances of one?s work were considered incor-
rect.  As we shall see, these decisions would not 
have a big impact on the outcome.   
4.3 Effect of Constraints 
The answers collected from these two rounds of 
questions can be regarded as assertions about the 
subject X.  By applying constraints, two possible 
effects can occur to these assertions: 
1. Some works can get thrown out. 
2. An asserted date (which was the top candidate 
from its associated question) can get replaced by 
a candidate date originally in positions 2-6 
(where sixth place is NIL) 
Effect #1 is expected to increase precision at the 
risk of worsening recall; effect #2 can go either way.  
We note that NIL, which is only used for dates, can 
be the correct answer if the desired date assertion is 
absent from the corpus; NIL is considered a ?value? 
in this evaluation. 
By inspection, performances and other indirect 
works (discussed in the previous section) were usu-
ally associated with the correct artist, so our decision 
to remove them from consideration resulted in a de-
crease in both the numerator and denominator of the 
precision and recall calculations, resulting in a 
minimal effect. 
The results of applying QDC to the 57 test indi-
viduals are summarized in Table 3.  The baseline 
assertions for individual X were: 
o Top-ranking birthdate/NIL   
o Top-ranking deathdate/NIL   
o Set of works Wi that passed threshold 
o Top-ranking date for Wi /NIL 
 
The sets of baseline assertions (by individual) are 
in effect the results of QA-by-Dossier WITHOUT 
Constraints (QbD). 
 
  Assertions Micro-Average Macro-Average 
  Total Cor-
rect 
Tru-
th 
Prec Rec F Prec Rec F 
Base-
line 
1671 517 933 .309 .554 .396 .331 .520 .386 
QDC 1417 813 933 .573 .871 .691 .603 .865 .690 
 
Table 3.  Results of Performance Evaluation.  
Two calculations of P/R/F are made, depending on 
whether the averaging is done over the whole set, or 
first by individual; the results are very similar.   
The QDC assertions were the same as those for 
QbD, but reflecting the following effects: 
o Some {Wi, date} pairs were thrown out (3 out of 
14 on average) 
o Some dates in positions 2-6 moved up (applica-
ble to birth, death and work dates) 
The results show improvement in both precision 
and recall, in turn determining a 75-80% relative 
increase in F-measure. 
5 Discussion 
This exposition of QA-by-Dossier-with-
Constraints is very short and undoubtedly leaves 
may questions unanswered.  We have not presented 
a precise method for computing the QDC scores. 
One way to formalize this process would be to treat 
it as evidence gathering and interpret the results in a 
Bayesian-like fashion. The original system confi-
dences would represent prior probabilities reflecting 
the system?s belief that the answers are correct.  As 
more evidence is found, the confidences would be 
updated to reflect the changed likelihood that an an-
swer is correct.  
We do not know a priori how much ?slop? should 
be allowed in enforcing the constraints, since auxil-
iary questions are as likely to be answered incor-
rectly as the original ones.  A further problem is to 
determine the best metric for evaluating such ap-
proaches, which is a question for QA in general.   
The task of generating auxiliary questions and 
constraint sets is a matter of active research.  Even 
for simple questions like the ones considered here, 
the auxiliary questions and constraints we looked at 
were different and manually chosen. Hand-crafting a 
large number of such sets might not be feasible, but 
it is certainly possible to build a few for common 
situations, such as a person?s life-cycle. More gener-
ally, QDC could be applied to situations in which a 
certain structure is induced by natural temporal (our 
Leonardo example) and/or spatial constraints, or by 
properties of the relation mentioned in the question 
(evaluation example). Temporal and spatial con-
straints appear general to all relevant question types, 
and include relations of precedence, inclusion, etc. 
For certain relationships, there are naturally-
occurring reciprocals (if X is married to Y, then Y is 
married to X; if X is a child of Y then Y is a parent 
of X; compound-term to acronym and vice versa).  
Transitive relationships (e.g. greater-than, located-
in, etc.) offer the immediate possibility of con-
straints, but this avenue has not yet been explored. 
5.1 Automatic Generation of Reciprocal Ques-
tions 
While not done in the work reported here, we are 
looking at generating reciprocal questions automati-
cally.  Consider the following transformations: 
 
?What is the capital of California?? -> ?Of what 
state is <candidate> the capital?? 
 
?What is Frank Sinatra?s nickname?? -> 
?Whose (or what person?s) nickname is <can-
didate>?? 
 
?How deep is Crater Lake?? -> ?What (or what 
lake) is <candidate> deep?? 
 
?Who won the Oscar for best actor in 1970??  
-> ?In what year did <candidate> win the 
Oscar for best actor?? (and/or ?What award 
did <candidate> win in 1970??) 
 
These are precisely the transformations necessary 
to generate the auxiliary reciprocal questions from 
the given original questions and candidate answers 
to them.  Such a process requires identifying an en-
tity in the question that belongs to a known class, 
and substituting the class name for the entity.  This 
entity is made the subject of the question, the previ-
ous subject (or trace) being replaced by the candi-
date answer.  We are looking at parse-tree rather 
than string transformations to achieve this.  This 
work will be reported in a future paper.  
5.2 Final Thoughts 
Despite these open questions, initial trials with 
QA-by-Dossier-with-Constraints have been very 
encouraging, whether it is by correctly answering 
previously missed questions, or by improving confi-
dences of correct answers.  An interesting question 
is when it is appropriate to apply QDC.  Clearly, if 
the base QA system is too poor, then the answers to 
the auxiliary questions will be useless; if the base 
system is highly accurate, the increase in accuracy 
will be negligible.  Thus our approach seems most 
beneficial to middle-performance levels, which, by 
inspection of TREC results for the last 5 years, is 
where the leading systems currently lie. 
We had initially thought that use of constraints 
would obviate the need for much of the complexity 
inherent in NLP.  As mentioned earlier, with the 
case of ?The Beatles? being the reciprocal answer to 
the auxiliary composition question to ?Who is Paul 
McCartney??, we see that structured, ontological 
information would benefit QDC.  Identifying alter-
nate spellings and representations of the same name 
(e.g. Clavier/Klavier, but also taking care of varia-
tions in punctuation and completeness) is also nec-
essary.  When we asked ?Who is Ian Anderson??, 
having in mind the singer-flautist for the Jethro Tull 
rock band, we found that he is not only that, but also 
the community investment manager of the English 
conglomerate Whitbread, the executive director of 
the U.S. Figure Skating Association, a writer for 
New Scientist, an Australian medical advisor to the 
WHO, and the general sales manager of Houseman, 
a supplier of water treatment systems.  Thus the 
problem of word sense disambiguation has returned 
in a particularly nasty form.  To be fully effective, 
QDC must be configured not just to find a consistent 
set of properties, but a number of independent sets 
that together cover the highest-confidence returned 
answers3.  Altogether, we see that some of the very 
problems we aimed to skirt are still present and need 
to be addressed.  However, we have shown that even 
disregarding these issues, QDC was able to provide 
substantial improvement in accuracy. 
6 Summary 
We have presented a method to improve the accu-
racy of a QA system by asking auxiliary questions 
for which natural constraints exist.  Using these con-
straints, sets of mutually consistent answers can be 
generated.  We have explored questions in the bio-
graphical areas, and identified other areas of appli-
cability.  We have found that our methodology 
exhibits a double advantage:  not only can it im-
                                                        
3 Possibly the smallest number of sets that provide such cover-
age.   
prove QA accuracy, but it can return a set of mutu-
ally-supporting assertions about the topic of the 
original question.  We have identified many open 
questions and areas of future work, but despite these 
gaps, we have shown an example scenario where 
QA-by-Dossier-with-Constraints can improve the F-
measure by over 75%. 
7 Acknowledgements 
We wish to thank Dave Ferrucci, Elena Filatova 
and Sasha Blair-Goldensohn for helpful discussions.  
This work was supported in part by the Advanced 
Research and Development Activity (ARDA)'s Ad-
vanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 
MDA904-01-C-0988. 
References 
Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci.  ?A Multi-Strategy and Multi-Source 
Approach to Question Answering?, Proceedings 
of the 11th TREC,  2003. 
Clarke, C., Cormack, G., Kisman, D.. and Lynam, T.  
?Question answering by passage selection 
(Multitext experiments for TREC-9)? in Proceed-
ings of the 9th TREC, pp. 673-683, 2001. 
Hendrix, G., E. Sacerdoti, D. Sagalowicz, J. Slocum: 
Developing a Natural Language Interface to Com-
plex Data. VLDB 1977: 292  
Lehnert, W.  The Process of Question Answering. A 
Computer Simulation of Cognition. Lawrence 
Erlbaum Associates, Publishers, 1978.  
Lenat, D. 1995.  "Cyc: A Large-Scale Investment in 
Knowledge Infrastructure." Communications of 
the ACM 38, no. 11. 
Moldovan, D. and V. Rus, ?Logic Form Transfor-
mation of WordNet and its Applicability to Ques-
tion Answering?, Proceedings of the ACL, 2001. 
Prager, J., E. Brown, A. Coden, and D. Radev. 2000. 
"Question-Answering by Predictive Annotation?.  
In Proceedings of SIGIR 2000, pp. 184-191.  
Prager, J., J. Chu-Carroll and K. Czuba, "A Multi-
Agent Approach to using Redundancy and Rein-
forcement in Question Answering" in New Direc-
tions in Question-Answering, Maybury, M. (Ed.),  
to appear in 2004. 
Schank, R. and R. Abelson. ?Scripts, Plans and 
Knowledge?, Proceedings of IJCAI?75. 
Voorhees, E. ?Overview of the TREC 2002 Ques-
tion Answering Track?, Proceedings of the 11th 
TREC, 2003. 
Warren, D., and F. Pereira "An efficient easily 
adaptable system for interpreting natural language 
queries," Computational Linguistics, 8:3-4, 110-
122, 1982.  
Winograd, T. Procedures as a representation for data 
in a computer program for under-standing natural 
language. Cognitive Psychology, 3(1), 1972. 
Woods, W. Progress in natural language understand-
ing --- an application in lunar geology. Proceed-
ings of the 1973 National Computer Conference, 
AFIPS Conference Proceedings, Vol. 42, 441--
450, 1973. 
