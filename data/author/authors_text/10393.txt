Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783?792,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Generative Model for Parsing Natural Language to Meaning
Representations
Wei Lu1, Hwee Tou Ng1,2, Wee Sun Lee1,2
1Singapore-MIT Alliance
2Department of Computer Science
National University of Singapore
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
Luke S. Zettlemoyer
CSAIL
Massachusetts Institute of Technology
lsz@csail.mit.edu
Abstract
In this paper, we present an algorithm for
learning a generative model of natural lan-
guage sentences together with their for-
mal meaning representations with hierarchi-
cal structures. The model is applied to the
task of mapping sentences to hierarchical rep-
resentations of their underlying meaning. We
introduce dynamic programming techniques
for efficient training and decoding. In exper-
iments, we demonstrate that the model, when
coupled with a discriminative reranking tech-
nique, achieves state-of-the-art performance
when tested on two publicly available cor-
pora. The generative model degrades robustly
when presented with instances that are differ-
ent from those seen in training. This allows
a notable improvement in recall compared to
previous models.
1 Introduction
To enable computers to understand natural human
language is one of the classic goals of research in
natural language processing. Recently, researchers
have developed techniques for learning to map sen-
tences to hierarchical representations of their under-
lying meaning (Wong and Mooney, 2006; Kate and
Mooney, 2006).
One common approach is to learn some form of
probabilistic grammar which includes a list of lexi-
cal items that models the meanings of input words
and also includes rules for combining lexical mean-
ings to analyze complete sentences. This approach
performs well but is constrained by the use of a sin-
gle, learned grammar that contains a fixed set of
lexical entries and productions. In practice, such
a grammar may lack the rules required to correctly
parse some of the new test examples.
In this paper, we develop an alternative approach
that learns a model which does not make use of
an explicit grammar but, instead, models the cor-
respondence between sentences and their meanings
with a generative process. This model is defined
over hybrid trees whose nodes include both natu-
ral language words and meaning representation to-
kens. Inspired by the work of Collins (2003), the
generative model builds trees by recursively creating
nodes at each level according to a Markov process.
This implicit grammar representation leads to flexi-
ble learned models that generalize well. In practice,
we observe that it can correctly parse a wider range
of test examples than previous approaches.
The generative model is learned from data that
consists of sentences paired with their meaning rep-
resentations. However, there is no explicit labeling
of the correspondence between words and meaning
tokens that is necessary for building the hybrid trees.
This creates a challenging, hidden-variable learning
problem that we address with the use of an inside-
outside algorithm. Specifically, we develop a dy-
namic programming parsing algorithm that leads to
O(n3m) time complexity for inference, where n is
the sentence length and m is the size of meaning
structure. This approach allows for efficient train-
ing and decoding.
In practice, we observe that the learned generative
models are able to assign a high score to the correct
meaning for input sentences, but that this correct
meaning is not always the highest scoring option.
783
To address this problem, we use a simple rerank-
ing approach to select a parse from a k-best list of
parses. This pipelined approach achieves state-of-
the-art performance on two publicly available cor-
pora. In particular, the flexible generative model
leads to notable improvements in recall, the total
percentage of sentences that are correctly parsed.
2 Related Work
In Section 9, we will compare performance with
the three existing systems that were evaluated on
the same data sets we consider. SILT (Kate et al,
2005) learns deterministic rules to transform either
sentences or their syntactic parse trees to meaning
structures. WASP (Wong and Mooney, 2006) is a
system motivated by statistical machine translation
techniques. It acquires a set of synchronous lexical
entries by running the IBM alignment model (Brown
et al, 1993) and learns a log-linear model to weight
parses. KRISP (Kate and Mooney, 2006) is a dis-
criminative approach where meaning representation
structures are constructed from the natural language
strings hierarchically. It is built on top of SVMstruct
with string kernels.
Additionally, there is substantial related research
that is not directly comparable to our approach.
Some of this work requires different levels of super-
vision, including labeled syntactic parse trees (Ge
and Mooney, 2005; Ge and Mooney, 2006). Others
do not perform lexical learning (Tang and Mooney,
2001). Finally, recent work has explored learning
to map sentences to lambda-calculus meaning rep-
resentations (Wong and Mooney, 2007; Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007).
3 Meaning Representation
We restrict our meaning representation (MR) for-
malism to a variable free version as presented in
(Wong and Mooney, 2006; Kate et al, 2005).
A training instance consists of a natural language
sentence (NL sentence) and its corresponding mean-
ing representation structure (MR structure). Con-
sider the following instance taken from the GEO-
QUERY corpus (Kate et al, 2005):
The NL sentence ?How many states do
not have rivers ?? consists of 8 words, in-
cluding punctuation. The MR is a hierarchical tree
QUERY : answer (NUM)
NUM : count (STATE)
STATE : exclude (STATE STATE)
STATE : state (all) STATE : loc 1 (RIVER)
RIVER : river (all)
Figure 1: An example MR structure
structure, as shown in Figure 1.
Following an inorder traversal of this MR tree, we
can equivalently represent it with the following list
of meaning representation productions (MR produc-
tions):
(0) QUERY : answer (NUM)
(1) NUM : count (STATE)
(2) STATE : exclude (STATE1 STATE2)
(3) STATE : state (all)
(4) STATE : loc 1 (RIVER)
(5) RIVER : river (all)
Each such MR production consists of three com-
ponents: a semantic category, a function symbol
which can be omitted (considered empty), and a list
of arguments. An argument can be either a child se-
mantic category or a constant. Take production (1)
for example: it has a semantic category ?NUM?, a
function symbol ?count?, and a child semantic cate-
gory ?STATE? as its only argument. Production (5)
has ?RIVER? as its semantic category, ?river? as the
function symbol, and ?all? is a constant.
4 The Generative Model
We describe in this section our proposed generative
model, which simultaneously generates a NL sen-
tence and an MR structure.
We denote a single NL word as w, a contiguous
sequence of NL words as w, and a complete NL
sentence as w?. In the MR structure, we denote a
semantic category as M. We denote a single MR
production as ma, or Ma : p?(Mb,Mc), where Ma
is the semantic category for this production, p? is the
function symbol, and Mb,Mc are the child semantic
categories. We denote ma as an MR structure rooted
by an MR production ma, and m?a an MR structure
for a complete sentence rooted by an MR production
ma.
The model generates a hybrid tree that represents
a sentence w? = w1 . . . w2 . . . paired with an MR
structure m?a rooted by ma.
784
Ma
ma
w1 Mb
mb
. . . . . .
w2 Mc
mc
. . . . . .
Figure 2: The generation process
Figure 2 shows part of a hybrid tree that is gen-
erated as follows. Given a semantic category Ma,
we first pick an MR production ma that has the form
Ma : p?(Mb,Mc), which gives us the function sym-
bol p? as well as the child semantic categories Mb
and Mc. Next, we generate the hybrid sequence of
child nodes w1 Mb w2 Mc, which consists of NL
words and semantic categories.
After that, two child MR productions mb and mc
are generated. These two productions will in turn
generate other hybrid sequences and productions, re-
cursively. This process produces a hybrid tree T ,
whose nodes are either NL words or MR produc-
tions. Given this tree, we can recover a NL sentence
w by recording the NL words visited in depth-first
traversal order and can recover an MR structure m
by following a tree-specific traversal order, defined
by the hybrid-patterns we introduce below. Figure 3
gives a partial hybrid tree for the training example
from Section 3. Note that the leaves of a hybrid tree
are always NL tokens.
. . .
STATE
STATE : exclude (STATE STATE)
STATE
STATE : state(all)
states
do not STATE
STATE : loc 1(RIVER)
have RIVER
RIVER : river(all)
rivers
Figure 3: A partial hybrid tree
With several independence assumptions, the
probability of generating
?
w?, m?,T ? is defined as:
P(w?, m?,T ) = P(Ma) ? P(ma|Ma) ? P(w1 Mb w2 Mc|ma)
?P(mb|ma, arg = 1) ? P(. . . |mb)
?P(mc|ma, arg = 2) ? P(. . . |mc) (1)
where ?arg? refers to the position of the child se-
mantic category in the argument list.
Motivated by Collins? syntactic parsing models
(Collins, 2003), we consider the generation process
for a hybrid sequence from an MR production as a
Markov process.
Given the assumption that each MR production
has at most two semantic categories in its arguments
(any production can be transformed into a sequence
of productions of this form), Table 1 includes the list
of all possible hybrid patterns.
# RHS Hybrid Pattern # Patterns
0 m ? w 1
1 m ? [w]Y[w] 4
2 m ? [w]Y[w]Z[w] 8m ? [w]Z[w]Y[w] 8
Table 1: A list of hybrid patterns, [] denotes optional
In this table, m is an MR production, Y and Z
are respectively the first and second child seman-
tic category in m?s argument list. The symbol w
refers to a contiguous sequence of NL words, and
anything inside [] can be optionally omitted. The
last row contains hybrid patterns that reflect reorder-
ing of one production?s child semantic categories
during the generation process. For example, con-
sider the case that the MR production STATE :
exclude (STATE1 STATE2) generates a hybrid se-
quence STATE1 do not STATE2, the hybrid pattern
m ? YwZ is associated with this generation step.
For the example hybrid tree in Figure 2, we can
decompose the probability for generating the hybrid
sequence as follows:
P(w1 Mb w2 Mc|ma) = P(m ? wYwZ|ma) ? P(w1|ma)
?P(Mb|ma, w1) ? P(w2|ma, w1,Mb)
?P(Mc|ma, w1,Mb, w2) ? P(END|ma, w1,Mb, w2,Mc) (2)
Note that unigram, bigram, or trigram assump-
tions can be made here for generating NL words and
semantic categories. For example, under a bigram
assumption, the second to last term can be written
as P(Mc|ma, w1,Mb, w2) ? P(Mc|ma, wk2), where
wk2 is the last word in w2. We call such additional
information that we condition on, the context.
Note that our generative model is different from
the synchronous context free grammars (SCFG) in
a number of ways. A standard SCFG produces a
correspondence between a pair of trees while our
model produces a single hybrid tree that represents
785
the correspondence between a sentence and a tree.
Also, SCFGs use a finite set of context-free rewrite
rules to define the model, where the rules are possi-
bly weighted. In contrast, we make use of the more
flexible Markov models at each level of the genera-
tive process, which allows us to potentially produce
a far wider range of possible trees.
5 Parameter Estimation
There are three categories of parameters used in the
model. The first category of parameters models
the generation of new MR productions from their
parent MR productions: e.g., P(mb|ma, arg = 1);
the second models the generation of a hybrid se-
quence from an MR production: e.g., P(w1|ma),
P(Mb|ma, w1); the last models the selection of a hy-
brid pattern given an MR production, e.g., P(m ?
wY|ma). We will estimate parameters from all cate-
gories, with the following constraints:
1.
?
m? ?(m?|m j, arg=k)=1 for all j and k = 1, 2.
These parameters model the MR structures, and
can be referred to as MR model parameters.
2.
?
t ?(t|m j,?)=1 for all j, where t is a NL word,
the ?END? symbol, or a semantic category. ?
is the context associated with m j and t.
These parameters model the emission of NL
words, the ?END? symbol, and child semantic
categories from an MR production. We call
them emission parameters.
3.
?
r ?(r|m j) = 1 for all j, where r is a hybrid
pattern listed in Table 1.
These parameters model the selection of hybrid
patterns. We name them pattern parameters.
With different context assumptions, we reach dif-
ferent variations of the model. In particular, we con-
sider three assumptions, as follows:
Model I We make the following assumption:
?(tk|m j,?) = P(tk|m j) (3)
where tk is a semantic category or a NL word, and
m j is an MR production.
In other words, generation of the next NL word
depends on its direct parent MR production only.
Such a Unigram Model may help in recall (the num-
ber of correct outputs over the total number of in-
puts), because it requires the least data to estimate.
Model II We make the following assumption:
?(tk|m j,?) = P(tk|m j, tk?1) (4)
where tk?1 is the semantic category or NL word to
the left of tk, i.e., the previous semantic category or
NL word.
In other words, generation of the next NL word
depends on its direct parent MR production as well
as the previously generated NL word or semantic
category only. This model is also referred to as Bi-
gram Model. This model may help in precision (the
number of correct outputs over the total number of
outputs), because it conditions on a larger context.
Model III We make the following assumption:
?(tk|m j,?) =
1
2 ?
(
P(tk|m j) + P(tk|m j, tk?1)
)
(5)
We can view this model, called the Mixgram
Model, as an interpolation between Model I and II.
This model gives us a balanced score for both preci-
sion and recall.
5.1 Modeling Meaning Representation
The MR model parameters can be estimated inde-
pendently from the other two. These parameters can
be viewed as the ?language model? parameters for
the MR structure, and can be estimated directly from
the corpus by simply reading off the counts of occur-
rences of MR productions in MR structures over the
training corpus. To resolve data sparseness problem,
a variant of the bigram Katz Back-Off Model (Katz,
1987) is employed here for smoothing.
5.2 Learning the Generative Parameters
Learning the remaining two categories of parameters
is more challenging. In a conventional PCFG pars-
ing task, during the training phase, the correct cor-
respondence between NL words and syntactic struc-
tures is fully accessible. In other words, there is a
single deterministic derivation associated with each
training instance. Therefore model parameters can
be directly estimated from the training corpus by
counting. However, in our task, the correct corre-
spondence between NL words and MR structures is
unknown. Many possible derivations could reach
the same NL-MR pair, where each such derivation
forms a hybrid tree.
786
The hybrid tree is constructed using hidden vari-
ables and estimated from the training set. An effi-
cient inside-outside style algorithm can be used for
model estimation, similar to that used in (Yamada
and Knight, 2001), as discussed next.
5.2.1 The Inside-Outside Algorithm with EM
In this section, we discuss how to estimate the
emission and pattern parameters with the Expecta-
tion Maximization (EM) algorithm (Dempster et al,
1977), by using an inside-outside (Baker, 1979) dy-
namic programming approach.
Denote ni ? ?mi, wi? as the i-th training instance,
where mi and wi are the MR structure and the NL
sentence of the i-th instance respectively. We also
denote nv ? ?mv, wv? as an aligned pair of MR
substructure and contiguous NL substring, where
the MR substructure rooted by MR production mv
will correspond to (i.e., hierarchically generate) the
NL substring wv. The symbol h is used to de-
note a hybrid sequence, and the function Parent(h)
gives the unique MR substructure-NL subsequence
pair which can be decomposed as h. Parent(nv) re-
turns the set of all possible hybrid sequences un-
der which the pair nv can be generated. Similarly,
Children(h) gives the NL-MR pairs that appear di-
rectly below the hybrid sequence h in a hybrid tree,
and Children(n) returns the set of all possible hybrid
sequences that n can be decomposed as. Figure 4
gives a packed tree structure representing the rela-
tions between the entities.
hp1 ? Parent(nv) . . . . . . hpm ? Parent(nv)
nv? ? ?mv? , wv? ? nv ? ?mv, wv?
hc1 ? Children(nv) . . . . . . hcn ? Children(nv)
Hybrid Sequence Contains
Can be Decomposed As
Figure 4: A packed tree structure representing the relations
between hybrid sequences and NL-MR pairs
The formulas for computing inside and outside
probabilities as well as the equations for updating
parameters are given in Figure 5. We use a CKY-
style parse chart for tracking the probabilities.
5.2.2 Smoothing
It is reasonable to believe that different MR pro-
ductions that share identical function symbols are
likely to generate NL words with similar distribu-
tion, regardless of semantic categories. For example,
The inside (?) probabilities are defined as
? If nv ? ?mv, wv? is leaf
?(nv) = P(wv|mv) (6)
? If nv ? ?mv, wv? is not leaf
?(nv) =
?
h?Children(nv)
(
P(h|mv) ?
?
nv??Children(h)
?(nv? )
)
(7)
The outside (?) probabilities are defined as
? If nv ? ?mv, wv? is root
?(nv) = 1 (8)
? If nv ? ?mv, wv? is not root
?(nv) =
?
h?Parent(nv)
(
?
(
Parent(h)
)
?P
(
h|Parent(h)
)
?
?
nv??Children(h),v?,v
?(nv? )
)
(9)
Parameter Update
? Update the emission parameter
The count ci(t, mv,?k), where t is a NL word
or a semantic category, for an instance pair ni ?
?mi, wi?:
ci(t, mv,?k) =
1
?(ni) ?
?
(t,mv ,?k) in h?Children(mv)
(
?(niv)
?P(h|mv) ?
?
niv??Children(h)
?(niv? )
)
The emission parameter is re-estimated as:
??(t|mv,?k) =
?
i ci(t, mv,?k)?
t?
?
i ci(t?, mv,?k)
(10)
? Update the pattern parameter
The count ci(r, mv), where r is a hybrid pattern,
for an instance pair ni ? ?mi, wi?:
ci(r, mv) =
1
?(ni) ?
?
(r,mv) in h?Children(mv)
(
?(niv)
?P(h|mv) ?
?
niv??Children(h)
?(niv? )
)
The pattern parameter is re-estimated as:
??(r|mv) =
?
i ci(r, mv)?
r?
?
i ci(r?, mv)
(11)
Figure 5: The inside/outside formulas as well as update
equations for EM
RIVER : largest (RIVER) and CITY : largest (CITY)
are both likely to generate the word ?biggest?.
In view of this, a smoothing technique is de-
ployed. We assume half of the time words can
787
be generated from the production?s function symbol
alone if it is not empty. Mathematically, assuming
ma with function symbol pa, for a NL word or se-
mantic category t, we have:
?(t|ma,?) =
{ ?e(t|ma,?) If pa is empty(
?e(t|ma,?) + ?e(t|pa,?)
)
/2 otherwise
where ?e models the generation of t from an MR
production or its function symbol, together with the
context ?.
6 A Dynamic Programming Algorithm for
Inside-Outside Computation
Though the inside-outside approach already em-
ploys packed representations for dynamic program-
ming, a naive implementation of the inference algo-
rithm will still require O(n6m) time for 1 EM iter-
ation, where n and m are the length of the NL sen-
tence and the size of the MR structure respectively.
This is not very practical as in one of the corpora we
look at, n and m can be up to 45 and 20 respectively.
In this section, we develop an efficient dynamic
programming algorithm that enables the inference
to run in O(n3m) time. The idea is as follows. In-
stead of treating each possible hybrid sequence as
a separate rule, we efficiently aggregate the already
computed probability scores for hybrid sequences
that share identical hybrid patterns. Such aggregated
scores can then be used for subsequent computa-
tions. By doing this, we can effectively avoid a large
amount of redundant computations. The algorithm
supports both unigram and bigram context assump-
tions. For clarity and ease of presentation, we pri-
marily make the unigram assumption throughout our
discussion.
We use ? (mv, wv) to denote the inside probabil-
ity for mv-wv pair, br[mv, wv, c] to denote the aggre-
gated probabilities for the MR sub-structure mv to
generate all possible hybrid sequences based on wv
with pattern r that covers its c-th child only. In addi-
tion, we use w(i, j) to denote a subsequence of w with
start index i (inclusive) and end index j (exclusive).
We also use ?r~mv, wv to denote the aggregated in-
side probability for the pair ?mv, wv?, if the hybrid
pattern is restricted to r only. By definition we have:
? (mv, wv) =
?
r
?(r|mv)??r~mv, wv??(END|mv) (12)
Relations between ?r and br can also be estab-
lished. For example, if mv has one child semantic
category, we have:
?m?wY~mv, wv = bm?wY[mv, wv, 1] (13)
For the case when mv has two child semantic cat-
egories as arguments, we have, for example:
?m?wYZw~mv, w(i, j) =
?
i+2?k? j?2
bm?wY[mv, w(i,k), 1]
?bm?Yw[mv, w(k, j), 2] (14)
Note that there also exist relations amongst b
terms for more efficient computation, for example:
bm?wY[mv, w(i, j), c] = ?(wi|mv)
?
(
bm?wY[mv, w(i+1, j), c] + bm?Y[mv, w(i+1, j), c]
)
(15)
Analogous but more complex formulas are used
for computing the outside probabilities. Updating of
parameters can be incorporated into the computation
of outside probabilities efficiently.
7 Decoding
In the decoding phase, we want to find the optimal
MR structure m?? given a new NL sentence w?:
m?
? = arg max
m?
P(m?|w?) = arg max
m?
?
T
P(m?,T |w?) (16)
where T is a possible hybrid tree associated with
the m?-w? pair. However, it is expensive to compute
the summation over all possible hybrid trees. We
therefore find the most likely hybrid tree instead:
m?
?=arg max
m?
max
T
P(m?,T |w?)=arg max
m?
max
T
P(w?, m?,T ) (17)
We have implemented an exact top-k decoding al-
gorithm for this task. Dynamic programming tech-
niques similar to those discussed in Section 6 can
also be applied when retrieving the top candidates.
We also find the Viterbi hybrid tree given a NL-
MR pair, which can be done in an analogous way.
This tree will be useful for reranking.
8 Reranking and Filtering of Predictions
Due to the various independence assumptions we
have made, the model lacks the ability to express
some long range dependencies. We therefore post-
process the best candidate predictions with a dis-
criminative reranking algorithm.
788
Feature Type Description Example
1. Hybrid Rule A MR production and its child hybrid form f1 : STATE : loc 1(RIVER) ? have RIVER
2. Expanded Hybrid Rule A MR production and its child hybrid form expanded f2 : STATE : loc 1(RIVER) ? ?have, RIVER : river(all)?
3. Long-range Unigram A MR production and a NL word appearing below in tree f3 : STATE : exclude(STATE STATE) ? rivers
4. Grandchild Unigram A MR production and its grandchild NL word f4 : STATE : loc 1(RIVER) ? rivers
5. Two Level Unigram A MR production, its parent production, and its child NL word f5 : ?RIVER : river(all), STATE : loc 1(RIVER)? ? rivers
6. Model Log-Probability Logarithm of base model?s joint probability log (P?(w, m,T )).
Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if
the combination is present, and 0 otherwise. Feature 6 takes real values.
8.1 The Averaged Perceptron Algorithm with
Separating Plane
The averaged perceptron algorithm (Collins, 2002)
has previously been applied to various NLP tasks
(Collins, 2002; Collins, 2001) for discriminative
reranking. The detailed algorithm can be found in
(Collins, 2002). In this section, we extend the con-
ventional averaged perceptron by introducing an ex-
plicit separating plane on the feature space.
Our reranking approach requires three compo-
nents during training: a GEN function that defines
for each NL sentence a set of candidate hybrid trees;
a single correct reference hybrid tree for each train-
ing instance; and a feature function ? that defines a
mapping from a hybrid tree to a feature vector. The
algorithm learns a weight vector w that associates a
weight to each feature, such that a score w??(T ) can
be assigned to each candidate hybrid tree T . Given
a new instance, the hybrid tree with the highest score
is then picked by the algorithm as the output.
In this task, the GEN function is defined as the
output hybrid trees of the top-k (k is set to 50 in our
experiments) decoding algorithm, given the learned
model parameters. The correct reference hybrid tree
is determined by running the Viterbi algorithm on
each training NL-MR pair. The feature function is
discussed in section 8.2.
While conventional perceptron algorithms usually
optimize the accuracy measure, we extend it to allow
optimization of the F-measure by introducing an ex-
plicit separating plane on the feature space that re-
jects certain predictions even when they score high-
est. The idea is to find a threshold b after w is
learned, such that a prediction with score below b
gets rejected. We pick the threshold that leads to the
optimal F-measure when applied to the training set.
8.2 Features
We list in Table 2 the set of features we used. Ex-
amples are given based on the hybrid tree in Figure
3. Some of the them are adapted from (Collins and
Koo, 2005) for a natural language parsing task. Fea-
tures 1-5 are indicator functions (i.e., it takes value
1 if a certain combination as the ones listed in Table
2 is present, 0 otherwise), while feature 6 is real val-
ued. Features that do not appear more than once in
the training set are discarded.
9 Evaluation
Our evaluations were performed on two corpora,
GEOQUERY and ROBOCUP. The GEOQUERY cor-
pus contains MR defined by a Prolog-based lan-
guage used in querying a database on U.S. geogra-
phy. The ROBOCUP corpus contains MR defined by
a coaching language used in a robot coaching com-
petition. There are in total 880 and 300 instances for
the two corpora respectively. Standard 10-fold cross
validations were performed and the micro-averaged
results are presented in this section. To make our
system directly comparable to previous systems, all
our experiments were based on identical training and
test data splits of both corpora as reported in the ex-
periments of Wong and Mooney (2006).
9.1 Training Methodology
Given a training set, we first run a variant of IBM
alignment model 1 (Brown et al, 1993) for 100 iter-
ations, and then initialize Model I with the learned
parameter values. This IBM model is a word-to-
word alignment model that does not model word
order, so we do not have to linearize the hierarchi-
cal MR structure. Given this initialization, we train
Model I for 100 EM iterations and use the learned
parameters to initialize Model II which is trained for
another 100 EM iterations. Model III is simply an
interpolation of the above two models. As for the
reranking phase, we initialize the weight vector with
the zero vector 0, and run the averaged perceptron
algorithm for 10 iterations.
789
9.2 Evaluation Methodology
Following Wong (2007) and other previous work,
we report performance in terms of Precision (per-
centage of answered NL sentences that are correct),
Recall (percentage of correctly answered NL sen-
tences, out of all NL sentences) and F-score (har-
monic mean of Precision and Recall).
Again following Wong (2007), we define the cor-
rect output MR structure as follows. For the GEO-
QUERY corpus, an MR structure is considered cor-
rect if and only if it retrieves identical results as
the reference MR structure when both are issued as
queries to the underlying Prolog database. For the
ROBOCUP corpus, an MR structure is considered
correct if and only if it has the same string represen-
tation as the reference MR structure, up to reorder-
ing of children of MR productions whose function
symbols are commutative, such as and, or, etc.
9.3 Comparison over Three Models
Model GEOQUERY (880) ROBOCUP (300)Prec. Rec. F Prec. Rec. F
I 81.3 77.1 79.1 71.1 64.0 67.4
II 89.0 76.0 82.0 82.4 57.7 67.8
III 86.2 81.8 84.0 70.4 63.3 66.7
I+R 87.5 80.5 83.8 79.1 67.0 72.6
II+R 93.2 73.6 82.3 88.4 56.0 68.6
III+R 89.3 81.5 85.2 82.5 67.7 74.4
Table 3: Performance comparison over three models
(Prec.:precision, Rec.:recall, +R: with reranking)
We evaluated the three models, with and with-
out reranking. The results are presented in Table 3.
Comparing Model I and Model II, we noticed that
for both corpora, Model I in general achieves bet-
ter recall while Model II achieves better precision.
This observation conforms to our earlier expecta-
tions. Model III, as an interpolation of the above two
models, achieves a much better F-measure on GEO-
QUERY corpus. However, it is shown to be less ef-
fective on ROBOCUP corpus. We noticed that com-
pared to the GEOQUERY corpus, ROBOCUP corpus
contains longer sentences, larger MR structures, and
a significant amount of non-compositionality. These
factors combine to present a challenging problem for
parsing with the generative model. Interestingly, al-
though Model III fails to produce better best pre-
dictions for this corpus, we found that its top-k list
contains a relatively larger number of correct pre-
dictions than Model I or Model II. This indicates
the possibility of enhancing the performance with
reranking.
The reranking approach is shown to be quite ef-
fective. We observe a consistent improvement in
both precision and F-measure after employing the
reranking phase for each model.
9.4 Comparison with Other Models
Among all the previous models, SILT, WASP, and
KRISP are directly comparable to our model. They
required the same amount of supervision as our sys-
tem and were evaluated on the same corpora.
We compare our model with these models in Ta-
ble 4, where the performance scores for the previous
systems are taken from (Wong, 2007). For GEO-
QUERY corpus, our model performs substantially
better than all the three previous models, with a no-
table improvement in the recall score. In fact, if we
look at the recall scores alone, our best-performing
model achieves a 6.7% and 9.8% absolute improve-
ment over two other state-of-the-art models WASP
and KRISP respectively. This indicates that over-
all, our model is able to handle over 25% of the
inputs that could not be handled by previous sys-
tems. On the other hand, in terms of F-measure,
we gain a 4.1% absolute improvement over KRISP,
which leads to an error reduction rate of 22%. On
the ROBOCUP corpus, our model?s performance is
also ranked the highest1.
System GEOQUERY (880) ROBOCUP (300)Prec. Rec. F Prec. Rec. F
SILT 89.0 54.1 67.3 83.9 50.7 63.2
WASP 87.2 74.8 80.5 88.9 61.9 73.0
KRISP 93.3 71.7 81.1 85.2 61.9 71.7
Model III+R 89.3 81.5 85.2 82.5 67.7 74.4
Table 4: Performance comparison with other directly com-
parable systems
9.5 Performance on Other Languages
As a generic model that requires minimal assump-
tions on the natural language, our model is natural
language independent and is able to handle various
other natural languages than English. To validate
this point, we evaluated our system on a subset of
1We are unable to perform statistical significance tests be-
cause the detailed performance for each fold of previously pub-
lished research work is not available.
790
the GEOQUERY corpus consisting of 250 instances,
with four different NL annotations.
As we can see from Table 5, our model is able
to achieve performance comparable to WASP as re-
ported by Wong (2007).
System English SpanishPrec. Rec. F Prec. Rec. F
WASP 95.42 70.00 80.76 91.99 72.40 81.03
Model III+R 91.46 72.80 81.07 95.19 79.20 86.46
System Japanese TurkishPrec. Rec. F Prec. Rec. F
WASP 91.98 74.40 82.86 96.96 62.40 75.93
Model III+R 87.56 76.00 81.37 93.82 66.80 78.04
Table 5: Performance on different natural languages for
GEOQUERY-250 corpus
Our model is generic, which requires no domain-
dependent knowledge and should be applicable to
a wide range of different domains. Like all re-
search in this area, the ultimate goal is to scale to
more complex, open-domain language understand-
ing problems. In future, we would like to create a
larger corpus in another domain with multiple natu-
ral language annotations to further evaluate the scal-
ability and portability of our approach.
10 Conclusions
We presented a new generative model that simulta-
neously produces both NL sentences and their cor-
responding MR structures. The model can be effec-
tively applied to the task of transforming NL sen-
tences to their MR structures. We also developed
a new dynamic programming algorithm for efficient
training and decoding. We demonstrated that this
approach, augmented with a discriminative rerank-
ing technique, achieves state-of-the-art performance
when tested on standard benchmark corpora.
In future, we would like to extend the current
model to have a wider range of support of MR for-
malisms, such as the one with lambda-calculus sup-
port. We are also interested in investigating ways to
apply the generative model to the inverse task: gen-
eration of a NL sentence that explains a given MR
structure.
Acknowledgments
The authors would like to thank Leslie Pack Kael-
bling for her valuable feedback and comments on
this research. The authors would also like to thank
the anonymous reviewers for their thoughtful com-
ments on this paper. The research is partially sup-
ported by ARF grant R-252-000-240-112.
References
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. Journal of the Acoustical Society of America,
65:S132.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
M. Collins. 2001. Ranking algorithms for named-entity
extraction: boosting and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), pages
489?496.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 1?8.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL 2005), pages 9?
16.
R. Ge and R. J. Mooney. 2006. Discriminative rerank-
ing for semantic parsing. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL 2006),
pages 263?270.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING/ACL
2006), pages 913?920.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the Twentieth National Conference on Ar-
tificial Intelligence (AAAI 2005), pages 1062?1068.
791
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 35(3):400?401.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In Proceedings of the 12th Euro-
pean Conference on Machine Learning (ECML 2001),
pages 466?477.
Y. W. Wong and R. J. Mooney. 2006. Learning for
semantic parsing with statistical machine translation.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT-NAACL
2006), pages 439?446.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics (ACL
2007), pages 960?967.
Y. W. Wong. 2007. Learning for Semantic Parsing and
Natural Language Generation Using Statistical Ma-
chine Translation Techniques. Ph.D. thesis, The Uni-
versity of Texas at Austin.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of the 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the 21st Conference on Uncertainty in Ar-
tificial Intelligence.
L. S. Zettlemoyer and M. Collins. 2007. Online learning
of relaxed CCG grammars for parsing to logical form.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 678?687.
792
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 400?409,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Natural Language Generation with Tree Conditional Random Fields
Wei Lu
1
, Hwee Tou Ng
1,2
, Wee Sun Lee
1,2
1
Singapore-MIT Alliance
2
Department of Computer Science
National University of Singapore
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
Abstract
This paper presents an effective method
for generating natural language sentences
from their underlying meaning represen-
tations. The method is built on top of
a hybrid tree representation that jointly
encodes both the meaning representation
as well as the natural language in a tree
structure. By using a tree conditional
random field on top of the hybrid tree
representation, we are able to explicitly
model phrase-level dependencies amongst
neighboring natural language phrases and
meaning representation components in a
simple and natural way. We show that
the additional dependencies captured by
the tree conditional random field allows it
to perform better than directly inverting a
previously developed hybrid tree semantic
parser. Furthermore, we demonstrate that
the model performs better than a previ-
ous state-of-the-art natural language gen-
eration model. Experiments are performed
on two benchmark corpora with standard
automatic evaluation metrics.
1 Introduction
One of the ultimate goals in the field of natural lan-
guage processing (NLP) is to enable computers to
converse with humans through human languages.
To achieve this goal, two important issues need
to be studied. First, it is important for comput-
ers to capture the meaning of a natural language
sentence in a meaning representation. Second,
computers should be able to produce a human-
understandable natural language sentence from its
meaning representation. These two tasks are re-
ferred to as semantic parsing and natural language
generation (NLG), respectively.
In this paper, we use corpus-based statistical
methods for constructing a natural language gener-
ation system. Given a set of pairs, where each pair
consists of a natural language (NL) sentence and
its formal meaning representation (MR), a learn-
ing method induces an algorithm that can be used
for performing language generation from other
previously unseen meaning representations.
A crucial question in any natural language pro-
cessing system is the representation used. Mean-
ing representations can be in the form of a tree
structure. In Lu et al (2008), we introduced a
hybrid tree framework together with a probabilis-
tic generative model to tackle semantic parsing,
where tree structured meaning representations are
used. The hybrid tree gives a natural joint tree rep-
resentation of a natural language sentence and its
meaning representation.
A joint generative model for natural language
and its meaning representation, such as that used
in Lu et al (2008) has several advantages over var-
ious previous approaches designed for semantic
parsing. First, unlike most previous approaches,
the generative approach models a simultaneous
generation process for both NL and MR. One el-
egant property of such a joint generative model
is that it allows the modeling of both semantic
parsing and natural language generation within the
same process. Second, the generative process pro-
ceeds as a recursive top-down Markov process in
a way that takes advantage of the tree structure
of the MR. The hybrid tree generative model pro-
posed in Lu et al (2008) was shown to give state-
of-the-art accuracy in semantic parsing on bench-
mark corpora.
While semantic parsing with hybrid trees has
been studied in Lu et al (2008), its inverse task
? NLG with hybrid trees ? has not yet been ex-
plored. We believe that the properties that make
the hybrid trees effective for semantic parsing also
make them effective for NLG. In this paper, we de-
velop systems for the generation task by building
400
on top of the generative model introduced in Lu et
al. (2008) (referred to as the LNLZ08 system).
We first present a baseline model by directly
?inverting? the LNLZ08 system, where an NL sen-
tence is generated word by word. We call this
model the direct inversion model. This model is
unable to model some long range global depen-
dencies over the entire NL sentence to be gener-
ated. To tackle several weaknesses exhibited by
the baseline model, we next introduce an alterna-
tive, novel model that performs generation at the
phrase level. Motivated by conditional random
fields (CRF) (Lafferty et al, 2001), a different pa-
rameterization of the conditional probability of the
hybrid tree that enables the model to encode some
longer range dependencies amongst phrases and
MRs is used. This novel model is referred to as
the tree CRF-based model.
Evaluation results for both models are pre-
sented, through which we demonstrate that the tree
CRF-based model performs better than the direct
inversion model. We also compare the tree CRF-
based model against the previous state-of-the-art
model of Wong and Mooney (2007). Further-
more, we evaluate our model on a dataset anno-
tated with several natural languages other than En-
glish (Japanese, Spanish, and Turkish). Evalua-
tion results show that our proposed tree CRF-based
model outperforms the previous model.
2 Related Work
There have been substantial earlier research ef-
forts on investigating methods for transforming
MR to their corresponding NL sentences. Most
of the recent systems tackled the problem through
the architecture of chart generation introduced by
Kay (1996). Examples of such systems include
the chart generator for Head-Driven Phrase Struc-
ture Grammar (HPSG) (Carroll et al, 1999; Car-
roll and Oepen, 2005; Nakanishi et al, 2005), and
more recently for Combinatory Categorial Gram-
mar (CCG) (White and Baldridge, 2003; White,
2004). However, most of these systems only fo-
cused on surface realization (inflection and order-
ing of NL words) and ignored lexical selection
(learning the mappings from MR domain concepts
to NL words).
The recent work by Wong and Mooney (2007)
explored methods for generation by inverting a
system originally designed for semantic pars-
ing. They introduced a system named WASP
?1
that employed techniques from statistical ma-
chine translation using Synchronous Context-Free
Grammar (SCFG) (Aho and Ullman, 1972). The
system took in a linearized MR tree as input, and
translated it into a natural language sentence as
output. Unlike most previous systems, their sys-
tem integrated both lexical selection and surface
realization in a single framework. The perfor-
mance of the system was enhanced by incorpo-
rating models borrowed from PHARAOH (Koehn,
2004). Experiments show that this new hybrid
system named WASP
?1
++ gives state-of-the-art
accuracies and outperforms the direct translation
model obtained from PHARAOH, when evaluated
on two corpora. We will compare our system?s
performance against that of WASP
?1
++ in Sec-
tion 5.
3 The Hybrid Tree Framework and the
LNLZ08 System
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : river(all) RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
what is the longest river that
does not run through texas
Figure 1: An example MR paired with its NL sen-
tence.
Following most previous works in this
area (Kate et al, 2005; Ge and Mooney, 2005;
Kate and Mooney, 2006; Wong and Mooney,
2006; Lu et al, 2008), we consider MRs in the
form of tree structures. An example MR and
its corresponding natural language sentence are
shown in Figure 1. The MR is a tree consisting
of nodes called MR productions. For example,
the node ?QUERY : answer(RIVER)? is one MR
production. Each MR production consists of a
semantic category (?QUERY?), a function symbol
(?answer?) which can be optionally omitted, as
well as an argument list which possibly contains
401
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
texas
run through
that does notRIVER : river(all)
river
the longest
what is
Figure 2: One possible hybrid tree T
1
child semantic categories (?RIVER?).
Now we give a brief overview of the hybrid tree
framework and the LNLZ08 system that was pre-
sented in Lu et al (2008). The training corpus re-
quired by the LNLZ08 system contains example
pairs d
(i)
= (
?
m
(i)
,
?
w
(i)
) for i = 1 . . . N , where
each
?
m
(i)
is an MR, and each
?
w
(i)
is an NL sen-
tence. The system makes the assumption that the
entire training corpus is generated from an under-
lying generative model, which is specified by the
parameter set ?.
The parameter set ? includes the following: the
MR model parameter ?(m
j
|m
i
, arg
k
) which mod-
els the generation of an MR production m
j
from
its parent MR production m
i
as its k-th child, the
emission parameter ?(t|m
i
,?) that is responsible
for generation of an NL word or a semantic cate-
gory t from the MR production m
i
(the parent of
t) under the context ? (such as the token to the left
of the current token), and the pattern parameter
?(r|m
i
), which models the selection of a hybrid
pattern r that defines globally how the NL words
and semantic categories are interleaved given a
parent MR production m
i
. All these parameters
are estimated from the corpus during the training
phase. The list of possible hybrid patterns is given
in Table 1 (at most two child semantic categories
are allowed ? MR productions with more child se-
mantic categories are transformed into those with
two).
In the table, m refers to the MR production, the
symbol w denotes an NL word sequence and is
optional if it appears inside []. The symbol Y and
Z refer to the first and second semantic category
under the MR production m respectively.
# RHS Hybrid Pattern # Patterns
0 m ? w 1
1 m ? [w]Y[w] 4
2
m ? [w]Y[w]Z[w] 8
m ? [w]Z[w]Y[w] 8
Table 1: The list of possible hybrid patterns, [] de-
notes optional
The generative process recursively creates MR
productions as well as NL words at each gen-
eration step in a top-down manner. This pro-
cess results in a hybrid tree for each MR-NL
pair. The list of children under each MR pro-
duction in the hybrid tree forms a hybrid se-
quence. One example hybrid tree for the MR-
NL pair given in Figure 1 is shown in Figure 2.
In this hybrid tree T
1
, the list of children under
the production RIVER : longest(RIVER) forms
the hybrid sequence ?the longest RIVER :
exclude(RIVER
1
RIVER
2
)?. The yield of the hy-
brid tree is exactly the NL sentence. The MR can
also be recovered from the hybrid tree by record-
ing all the internal nodes of the tree, subject to the
reordering operation required by the hybrid pat-
tern.
To illustrate, consider the generation of the hy-
brid tree T
1
shown in Figure 2. The model first
generates an MR production from its parent MR
production (empty as the MR production is the
root in the MR). Next, it selects a hybrid pattern
m ? wY from the predefined list of hybrid pat-
terns, which puts a constraint on the set of all al-
lowable hybrid sequences that can be generated:
the hybrid sequence must be an NL word sequence
402
followed by a semantic category. Finally, actual
NL words and semantic categories are generated
from the parent MR production. Now the genera-
tion for one level is complete, and the above pro-
cess repeats at the newly generated MR produc-
tions, until the complete NL sentence and MR are
both generated.
Mathematically, the above generative process
yields the following formula that models the joint
probability for the MR-NL pair, assuming the con-
text ? for the emission parameter is the preceding
word or semantic category (i.e., the bigram model
is assumed, as discussed in Lu et al (2008)):
p
(
T
1
(
?
w,
?
m)
)
= ?(QUERY : answer(RIVER)|?, arg
1
)
??(m ? wY|QUERY : answer(RIVER))
??(what|QUERY : answer(RIVER),BEGIN)
??(is|QUERY : answer(RIVER),what)
??(RIVER|QUERY : answer(RIVER),is)
??(END|QUERY : answer(RIVER),RIVER)
??(RIVER : longest(RIVER)|
QUERY : answer(RIVER), arg
1
)? . . . (1)
where T
1
(
?
w,
?
m) denotes the hybrid tree T
1
which
contains the NL sentence
?
w and MR
?
m.
For each MR-NL pair in the training set, there
can be potentially many possible hybrid trees asso-
ciated with the pair. However, the correct hybrid
tree is completely unknown during training. The
correct hybrid tree is therefore treated as a hidden
variable. An efficient inside-outside style algo-
rithm (Baker, 1979) coupled with further dynamic
programming techniques is used for efficient pa-
rameter estimation.
During the testing phase, the system makes use
of the learned model parameters to determine the
most probable hybrid tree given a new natural lan-
guage sentence. The MR contained in that hybrid
tree is the output of the system. Dynamic pro-
gramming techniques similar to those of training
are also employed for efficient decoding.
The generative model used in the LNLZ08 sys-
tem has a natural symmetry, allowing for easy
transformation from NL to MR, as well as from
MR to NL. This provides the starting point for our
work in ?inverting? the LNLZ08 system to gener-
ate natural language sentences from the underly-
ing meaning representations.
4 Generation with Hybrid Trees
The task of generating NL sentences from MRs
can be defined as follows. Given a training cor-
pus consisting of MRs paired with their NL sen-
tences, one needs to develop algorithms that learn
how to effectively ?paraphrase? MRs with natu-
ral language sentences. During testing, the sys-
tem should be able to output the most probable NL
?paraphrase? for a given new MR.
The LNLZ08 system models p(T (
?
w,
?
m)), the
joint generative process for the hybrid tree con-
taining both NL and MR. This term can be rewrit-
ten in the following way:
p(T (
?
w,
?
m)) = p(
?
m)? p (T (
?
w,
?
m)|
?
m) (2)
In other words, we reach an alternative view of
the joint generative process as follows. We choose
to generate the complete MR
?
m first. Given
?
m, we
generate hybrid sequences below each of its MR
production, which gives us a complete hybrid tree
T (
?
w,
?
m). The NL sentence
?
w can be constructed
from this hybrid tree exactly.
We define an operation yield(T ) which returns
the NL sentence as the yield of the hybrid tree T .
Given an MR
?
m, we find the most probable NL
sentence
?
w
?
as follows:
?
w
?
= yield
(
argmax
T
p(T |
?
m)
)
(3)
In other words, we first find the most probable
hybrid tree T that contains the provided MR
?
m.
Next we return the yield of T as the most probable
NL sentence.
Different assumptions can be made in the pro-
cess of finding the most probable hybrid tree. We
first describe a simple model which is a direct in-
version of the LNLZ08 system. This model, as a
baseline model, generates a complete NL sentence
word by word. Next, a more sophisticated model
that exploits NL phrase-level dependencies is built
that tackles some weaknesses of the simple base-
line model.
4.1 Direct Inversion Model
Assume that a pre-order traversal of the
MR
?
m gives us the list of MR productions
m
1
,m
2
, . . . ,m
S
, where S is the number of MR
productions in
?
m. Based on the independence
assumption made by the LNLZ08 system, each
MR production independently generates a hybrid
403
sequence. Denote the hybrid sequence gener-
ated under the MR production m
s
as h
s
, for
s = 1, . . . , S. We call the list of hybrid sequences
h = ?h
1
, h
2
, . . . , h
S
? a hybrid sequence list
associated with this particular MR. Thus, our goal
is to find the optimal hybrid sequence list h
?
for
the given MR
?
m, which is formulated as follows:
h
?
= ?h
?
1
, . . . , h
?
S
? = argmax
h
1
,...,h
S
S
?
s=1
p(h
s
|m
s
) (4)
The optimal hybrid sequence list defines the op-
timal hybrid tree whose yield gives the optimal NL
sentence.
Due to the strong independence assumption in-
troduced by the LNLZ08 system, the hybrid tree
generation process is in fact highly decompos-
able. Optimization of the hybrid sequence list
?h
1
, . . . , h
S
? can be performed individually since
they are independent of one another. Thus, math-
ematically, for s = 1, . . . , S, we have:
h
?
s
= argmax
h
s
p(h
s
|m
s
) (5)
The LNLZ08 system presented three models for
the task of transforming NL to MR. In this in-
verse task, for generation of a hybrid sequence,
we choose to use the bigram model (model II). We
choose this model mainly due to its stronger abil-
ity in modeling dependencies between adjacent
NL words, which we believe to be quite important
in this NL generation task. With the bigram model
assumption, the optimal hybrid sequence that can
be generated from each MR production is defined
as follows:
h
?
s
= argmax
h
s
p(h
s
|m
s
)
= argmax
h
s
{
?(r|m
s
)?
|h
s
|+1
?
j=1
?(t
j
|m
s
, t
j?1
)
}
(6)
where t
i
is either an NL word or a semantic cat-
egory with t
0
? BEGIN and t
|h
s
|+1
? END, and
r is the hybrid pattern that matches the hybrid se-
quence h
s
, which is equivalent to t
1
, . . . , t
|h
s
|
.
Equivalently, we can view the problem in the
log-space:
h
?
s
= argmin
h
s
{
? log ?(r|m
s
)
+
|h
s
|+1
?
j=1
? log ?(t
j
|m
s
, t
j?1
)
}
(7)
Note the term ? log ?(r|m
s
) is a constant for
a particular MR production m
s
and a particu-
lar hybrid pattern r. This search problem can
be equivalently cast as the shortest path problem
which can be solved efficiently with Dijkstra?s al-
gorithm (Cormen et al, 2001). We define a set
of states. Each state represents a single NL word
or a semantic category, including the special sym-
bols BEGIN and END. A directed path between
two different states t
u
and t
v
is associated with
a distance measure ? log ?(t
v
|m
s
, t
u
), which is
non-negative. The task now is to find the short-
est path between BEGIN and END
1
. The sequence
of words appearing in this path is simply the most
probable hybrid sequence under this MR produc-
tion m
s
. We build this model by directly inverting
the LNLZ08 system, and this model is therefore
referred to as the direct inversion model.
A major weakness of this baseline model is that
it encodes strong independence assumptions dur-
ing the hybrid tree generation process. Though
shown to be effective in the task of transform-
ing NL to MR, such independence assumptions
may introduce difficulties in this NLG task. For
example, consider the MR shown in Figure 1.
The generation steps of the hybrid sequences
from the two adjacent MR productions QUERY :
answer(RIVER) and RIVER : longest(RIVER)
are completely independent of each other. This
may harm the fluency of the generated NL sen-
tence, especially when a transition from one hy-
brid sequence to another is required. In fact, due
to such an independence assumption, the model
always generates the same hybrid sequence from
the same MR production, regardless of its context
such as parent or child MR productions. Such a
limitation points to the importance of better uti-
lizing the tree structure of the MR for this NLG
task. Furthermore, due to the bigram assumption,
the model is unable to capture longer range depen-
dencies amongst the words or semantic categories
in each hybrid sequence.
To tackle the above issues, we explore ways of
relaxing various assumptions, which leads to an
1
In addition, we should make sure that the generated hy-
brid sequence t
0
. . . t
|h
s
|+1
is a valid hybrid sequence that
comply with the hybrid pattern r. For example, the MR
production STATE : loc 1(RIVER) can generate the follow-
ing hybrid sequence ?BEGIN have RIVER END? but not
this hybrid sequence ?BEGIN have END?. This can be
achieved by finding the shortest path from BEGIN to RIVER,
which then gets concatenated to the shortest path from RIVER
to END.
404
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : river(all) RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
what is RIVER
1
the longest RIVER
1
RIVER
1
that does not RIVER
2
river run through STATE
1
STATENAME
1
texas
Figure 3: An MR (left) and its associated hybrid sequences (right)
alternative model as discussed next.
4.2 Tree CRF-Based Model
Based on the belief that using known phrases usu-
ally leads to better fluency in the NLG task (Wong
and Mooney, 2007), we explore methods for gen-
erating an NL sentence at phrase level rather than
at word level. This is done by generating hybrid
sequences as complete objects, rather than sequen-
tially one word or semantic category at a time,
from MR productions.
We assume that each MR production can gen-
erate a complete hybrid sequence below it from a
finite set of possible hybrid sequences. Each such
hybrid sequence is called a candidate hybrid se-
quence associated with that particular MR produc-
tion. Given a set of candidate hybrid sequences as-
sociated with each MR production, the generation
task is to find the optimal hybrid sequence list h
?
for a given MR
?
m:
h
?
= argmax
h
p(h|
?
m) (8)
Figure 3 shows a complete MR, as well as a
possible tree that contains hybrid sequences as-
sociated with the MR productions. For exam-
ple, in the figure the MR production RIVER :
traverse(STATE) is associated with the hybrid se-
quence run through STATE
1
. Each MR pro-
duction can be associated with potentially many
different hybrid sequences. The task is to deter-
mine the most probable list of hybrid sequences as
the ones appearing on the right of Figure 3, one for
each MR production.
To make better use of the tree structure of MR,
we take the approach of modeling the conditional
distribution using a log-linear model. Following
the conditional random fields (CRF) framework
(Lafferty et al, 2001), we can define the probabil-
ity of the hybrid sequence list given the complete
MR
?
m, as follows:
p(h|
?
m) =
1
Z(
?
m)
exp
(
?
i?V
?
k
?
k
g
k
(h
i
,
?
m, i)
+
?
(i,j)?E
?
k
?
k
f
k
(h
i
, h
j
,
?
m, i, j)
)
(9)
where V is the set of all the vertices in the tree, and
E is the set of the edges in the tree, consisting of
parent-child pairs. The function Z(
?
m) is the nor-
malization function. Note that the dependencies
among the features here form a tree, unlike the se-
quence models used in Lafferty et al (2001). The
function f
k
(h
i
, h
j
,
?
m, i, j) is a feature function of
the entire MR tree
?
m and the hybrid sequences at
vertex i and j. These features are usually referred
to as the edge features in the CRF framework. The
function g
k
(h
i
,
?
m, i) is a feature function of the
hybrid sequence at vertex i and the entire MR tree.
These features are usually referred to as the vertex
features. The parameters ?
k
and ?
k
are learned
from the training data.
In this task, we are given only MR-NL pairs
and do not have the hybrid tree corresponding to
each MR as training data. Now we describe how
the set of candidate hybrid sequences for each MR
production is obtained as well as how the train-
ing data for this model is constructed. After the
joint generative model is learned as done in Lu et
al. (2008), we first use a Viterbi algorithm to find
the optimal hybrid tree for each MR-NL pair in
the training set. From each optimal hybrid tree,
we extract the hybrid sequence h
i
below each MR
production m
i
. Using this process on the train-
ing MR-NL pairs, we can obtain a set of candidate
405
hybrid sequences that can be associated with each
MR production. The optimal hybrid tree generated
by the Viterbi algorithm in this way is considered
the ?correct? hybrid tree for theMR-NL pair and is
used as training data. While this does not provide
hand-labeled training data, we believe the hybrid
trees generated this way form a high quality train-
ing set as both the MR and NL are available when
Viterbi decoding is performed, guaranteeing that
the generated hybrid tree has the correct yield.
There exist several advantages of such a model
over the simple generative model. First, this model
allows features that specifically model the depen-
dencies between neighboring hybrid sequences in
the tree to be used. In addition, the model can effi-
ciently capture long range dependencies between
MR productions and hybrid sequences since each
hybrid sequence is allowed to depend on the entire
MR tree.
For features, we employ four types of simple
features, as presented below. Note that the first
three types of features are vertex features, and the
last are edge features. Examples are given based
on Figure 3. All the features are indicator func-
tions, i.e., a feature takes value 1 if a certain com-
bination is present, and 0 otherwise. The last three
features explicitly encode information from the
tree structure of MR.
Hybrid sequence features : one hybrid sequence
together with the associated MR production.
For example:
g
1
: ?run through STATE
1
,
RIVER : traverse(STATE)? ;
Two-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
and the parent MR production. For example:
g
2
: ?run through STATE
1
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
)? ;
Three-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
the parent MR production, and the grandpar-
ent MR production. For example:
g
3
: ?run through STATE
1
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
),
RIVER : longest(RIVER)? ;
Adjacent hybrid sequence features : two adja-
cent hybrid sequences, together with their as-
sociated MR productions. For example:
f
1
: ?run through STATE
1
,
RIVER
1
that does not RIVER
2
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
)? .
For training, we use the feature forest model
(Miyao and Tsujii, 2008), which was originally
designed as an efficient algorithm for solving max-
imum entropy models for data with complex struc-
tures. The model enables efficient training over
packed trees that potentially represent exponen-
tial number of trees. The tree conditional random
fields model can be effectively represented using
the feature forest model. The model has also been
successfully applied to the HPSG parsing task.
To train the model, we run the Viterbi algorithm
on the trained LNLZ08 model and perform convex
optimization using the feature forest model. The
LNLZ08 model is trained using an EM algorithm
with time complexity O(MN
3
D) per EM itera-
tion, where M and N are respectively the maxi-
mum number of MR productions and NL words
for each MR-NL pair, and D is the number of
training instances. The time complexity of the
Viterbi algorithm is also O(MN
3
D). For training
the feature forest, we use the Amis toolkit (Miyao
and Tsujii, 2002) which utilizes the GIS algorithm.
The time complexity for each iteration of the GIS
algorithm is O(MK
2
D), where K is the maxi-
mum number of candidate hybrid sequences asso-
ciated with each MR production. Finally, the time
complexity for generating a natural language sen-
tence from a particular MR is O(MK
2
).
5 Experiments
In this section, we present the results of our sys-
tems when evaluated on two standard benchmark
corpora. The first corpus is GEOQUERY, which
contains Prolog-based MRs that can be used to
query a US geographic database (Kate et al,
2005). Our task for this domain is to generate
NL sentences from the formal queries. The second
corpus is ROBOCUP. This domain contains MRs
which are instructions written in a formal language
called CLANG. Our task for this domain is to gen-
erate NL sentences from the coaching advice writ-
ten in CLANG.
406
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
Direct inversion model 0.3973 5.5466 0.5468 6.6738
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
Table 2: Results of automatic evaluation of both models (bold type indicates the best performing system).
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
WASP
?1
++ 0.5370 6.4808 0.6022 6.8976
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
Table 3: Results of automatic evaluation of our tree CRF-based model and WASP
?1
++.
English Japanese Spanish Turkish
BLEU NIST BLEU NIST BLEU NIST BLEU NIST
WASP
?1
++ 0.6035 5.7133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283
Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033
Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages.
The GEOQUERY domain contains 880 in-
stances, while the ROBOCUP domain contains 300
instances. The average NL sentence length for the
two corpora are 7.57 and 22.52 respectively. Fol-
lowing the evaluation methodology of Wong and
Mooney (2007), we performed 4 runs of the stan-
dard 10-fold cross validation and report the aver-
aged performance in this section using the stan-
dard automatic evaluation metric BLEU (Papineni
et al, 2002) and NIST (Doddington, 2002)
2
. The
BLEU and NIST scores of the WASP
?1
++ sys-
tem reported in this section are obtained from
the published paper of Wong and Mooney (2007).
Note that to make our experimental results directly
comparable to Wong and Mooney (2007), we used
the identical training and test data splits for the 4
runs of 10-fold cross validation used by Wong and
Mooney (2007) on both corpora.
Our system has the advantage of always pro-
ducing an NL sentence given any input MR, even
if there exist unseen MR productions in the input
MR. We can achieve this by simply skipping those
unseen MR productions during the generation pro-
cess. However, in order to make a fair comparison
against WASP
?1
++, which can only generate NL
sentences for 97% of the input MRs, we also do
not generate any NL sentence in the case of ob-
serving an unseen MR production. All the evalu-
ations discussed in this section follow this evalu-
2
We used the official evaluation script (version 11b) pro-
vided by http://www.nist.gov/.
ation methodology, but we notice that empirically
our system is able to achieve higher BLEU/NIST
scores if we allow generation for those MRs that
include unseen MR productions.
5.1 Comparison between the two models
We compare the performance of our two models
in Table 2. From the table, we observe that the
tree CRF-based model outperforms the direct in-
version model on both domains. This validates
our earlier belief that some long range dependen-
cies are important for the generation task. In ad-
dition, while the direct inversion model performs
reasonably well on the ROBOCUP domain, it per-
forms substantially worse on the GEOQUERY do-
main where the sentence length is shorter. We note
that the evaluation metrics are strongly correlated
with the cumulative matching n-grams between
the output and the reference sentence (n ranges
from 1 to 4 for BLEU, and 1 to 5 for NIST). The
direct inversion model fails to capture the transi-
tional behavior from one phrase to another, which
makes it more vulnerable to n-gram mismatch, es-
pecially when evaluated on the GEOQUERY cor-
pus where phrase-to-phrase transitions are more
frequent. On the other hand, the tree CRF-based
model does not suffer from this problem, mainly
due to its ability to model such dependencies be-
tween neighboring phrases. Sample outputs from
the two models are shown in Figure 4.
407
Reference: what is the largest state bordering texas
Direct inversion model: what the largest states border texas
Tree CRF-based model: what is the largest state that borders texas
Reference: if DR2C7 is true then players 2 , 3 , 7 and 8
should pass to player 4
Direct inversion model: if DR2C7 , then players 2 , 3 7 and 8 should
ball to player 4
Tree CRF-based model: if the condition DR2C7 is true then players 2 ,
3 , 7 and 8 should pass to player 4
Figure 4: Sample outputs from the two models, for GEOQUERY domain (top) and ROBOCUP domain
(bottom) respectively.
5.2 Comparison with previous model
We also compare the performance of our tree CRF-
based model against the previous state-of-the-art
system WASP
?1
++ in Table 3. Our tree CRF-based
model achieves better performance on both cor-
pora. We are unable to carry out statistical sig-
nificance tests since the detailed BLEU and NIST
scores of the cross validation runs of WASP
?1
++
as reported in the published paper of Wong and
Mooney (2007) are not available.
The results confirm our earlier discussions: the
dependencies between the generated NL words
are important and need to be properly modeled.
The WASP
?1
++ system uses a log-linear model
which incorporates two major techniques to at-
tempt to model such dependencies. First, a back-
off language model is used to capture dependen-
cies at adjacent word level. Second, a technique
that merges smaller translation rules into a single
rigid rule is used to capture dependencies at phrase
level (Wong, 2007). In contrast, the proposed tree
CRF-based model is able to explicitly and flexibly
exploit phrase-level features that model dependen-
cies between adjacent phrases. In fact, with the
hybrid tree framework, the better treatment of the
tree structure of MR enables us to model some cru-
cial dependencies between the complete MR tree
and generated NL phrases. We believe that this
property plays an important role in improving the
quality of the generated sentences in terms of flu-
ency, which is assessed by the evaluation metrics.
Furthermore, WASP
?1
++ employs minimum
error rate training (Och, 2003) to directly optimize
the evaluation metrics. We have not done so but
still obtain better performance. In future, we plan
to explore ways to directly optimize the evaluation
metrics in our system.
5.3 Experiments on different languages
Following the work of Wong and Mooney (2007),
we also evaluated our system?s performance on
a subset of the GEOQUERY corpus with 250 in-
stances, where sentences of 4 natural languages
(English, Japanese, Spanish, and Turkish) are
available. The evaluation results are shown in Ta-
ble 4. Our tree CRF-based model achieves better
performance on this task compared to WASP
?1
++.
We are again unable to conduct statistical signifi-
cance tests for the same reason reported earlier.
6 Conclusions
In this paper, we presented two novel models for
the task of generating natural language sentences
from given meaning representations, under a hy-
brid tree framework. We first built a simple di-
rect inversion model as a baseline. Next, to ad-
dress the limitations associated with the direct in-
version model, a tree CRF-based model was in-
troduced. We evaluated both models on standard
benchmark corpora. Evaluation results show that
the tree CRF-based model performs better than the
direct inversion model, and that the tree CRF-based
model also outperforms WASP
?1
++, which was a
previous state-of-the-art system reported in the lit-
erature.
Acknowledgments
The authors would like to thank Seung-Hoon Na
for his suggestions on the presentation of this pa-
per, Yuk Wah Wong for answering various ques-
tions related to the WASP
?1
++ system, and the
anonymous reviewers for their thoughtful com-
ments on this work.
408
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Clis, NJ.
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547?550, Boston, MA, June.
John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP 2005), pages 165?176.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proceedings of
the 7th European Workshop on Natural Language
Generation (EWNLG 1999), pages 86?95.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to
Algorithms (Second Edition). MIT Press.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology Research (HLT 2002), pages 138?145.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the 9th Conference on
Computational Natural Language Learning (CoNLL
2005), pages 9?16.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING/ACL 2006), pages 913?920.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artificial Intelligence (AAAI 2005),
pages 1062?1068.
Martin Kay. 1996. Chart generation. In Proceedings
of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL 1996), pages 200?
204.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas (AMTA 2004), pages 115?124.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 783?792.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the 2nd International Conference on Human
Language Technology Research (HLT 2002), pages
292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technologies
(IWPT 2005), volume 5, pages 93?102.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2002), pages 311?318.
Michael White and Jason Baldridge. 2003. Adapting
chart realization to CCG. In Proceedings of the 9th
European Workshop on Natural Language Genera-
tion (EWNLG 2003), pages 119?126.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proceeding of the 3rd International Confer-
ence on Natural Language Generation (INLG 2004),
pages 182?191.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL 2006), pages 439?446.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Generation by inverting a semantic parser that uses
statistical machine translation. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL/HLT 2007),
pages 172?179.
Yuk Wah Wong. 2007. Learning for Semantic Parsing
and Natural Language Generation Using Statistical
Machine Translation Techniques. Ph.D. thesis, The
University of Texas at Austin.
409
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1291?1301, Dublin, Ireland, August 23-29 2014.
Multilingual Semantic Parsing :
Parsing Multiple Languages into Semantic Representations
Zhanming Jie
University of Electronic Science
& Technology of China
allanmcgrady@gmail.com
Wei Lu
Information Systems Technology and Design
Singapore University of Technology and Design
luwei@sutd.edu.sg
Abstract
We consider multilingual semantic parsing ? the task of simultaneously parsing semantically
equivalent sentences from multiple different languages into their corresponding formal semantic
representations. Our model is built on top of the hybrid tree semantic parsing framework, where
natural language sentences and their corresponding semantics are assumed to be generated jointly
from an underlying generative process. We first introduce a variant of the joint generative pro-
cess, which essentially gives us a new semantic parsing model within the framework. Based on
the different models that can be developed within the framework, we then investigate several ap-
proaches for performing the multilingual semantic parsing task. We present our evaluations on a
standard dataset annotated with sentences in multiple languages coming from different language
families.
1 Introduction
Semantic parsing, the task of parsing natural language sentences into their formal semantic representa-
tions (Mooney, 2007) is one of the most important tasks in the field of natural language processing and
artificial intelligence. This area of research recently has received a significant amount of attention (Zettle-
moyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et
al., 2012b). Consider these example sentence-semantics pairs:
English: Which states have points that are higher than the highest point in Texas ?
Semantics: answer(state(loc
1
(place(higher
2
(highest(place(loc
2
(stateid(
?
TX
?
)))))))))
English: What rivers do not run through Tennessee ?
Semantics: answer(exclude(river(all), traverse
2
(stateid(
?
TN
?
))))
In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs a
model that can parse novel input sentences into their respective semantic representations. Such semantic
representations can then be used to interact with certain downstream components to perform interesting
tasks. For example, retrieving of answers from an underlying database, or performing certain actions
based on the generated executable semantic instructions.
Note that in the training data, although complete sentence-semantics pairs are given, specific word-
level semantic information is not explicitly provided. The model therefore needs to automatically learn
such latent mappings between natural language words/phrases and semantic units.
One natural assumption is that the semantics exhibit certain restricted structures, such as the recursive
tree structures. Under such an assumption, one can convert the second semantics appeared above as the
tree structure illustrated in Figure 1. More details about such tree structured representations will be given
in Section 2.1.
Currently, researchers only focused on the semantic parsing task under a single language setting where
the input is a sentence from one particular language. However, natural language is highly ambiguous,
and identifying the correct semantics associated with words with limited background information is a
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1291
QUERY : answer(RIVER)
RIVER : exclude(RIVER, RIVER)
RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : (
?
tn
?
)
RIVER : river(all)
What rivers do not run through Tennessee ?
????????????
Welche Fl?usse flie?en nicht durch Tennessee ?
Figure 1: An example tree-structured semantic representation (above) and its corresponding natural language sentences (in
English, Chinese and German).
challenging task. Researchers resorted to performing context-dependent semantic parsing to alleviate
such an issue (Zettlemoyer and Collins, 2009).
On the other hand, researchers have successfully exploited parallel texts for improved word-level se-
mantic processing (Chan and Ng, 2005). This is because words from different languages that convey
the same semantics can be used to disambiguate each other?s semantics. In fact, texts from different
languages that convey the same semantic information becomes increasingly available nowadays. Web
crawlers such as Google and Yahoo! are able to rapidly aggregate a large volume of news stories ev-
ery day. One crucial fact is that many such news articles written in different languages are actually all
discussing the same underlying story and therefore convey similar or identical semantic information. To
build better automatic systems for improved natural language understanding, it is therefore helpful to
develop algorithms that can simultaneously process the underlying semantic information associated with
all these documents coming from different language sources together. For example, consider the fol-
lowing example taken from the multilingual version of the dataset, which shows semantically equivalent
sentences from three different languages and their corresponding semantics:
English: What rivers do not run through Tennessee ?
Chinese: ???????????
German: Welche Fl?usse flie?en nicht durch Tennessee ?
Semantics: answer(exclude(river(all), traverse
2
(stateid(
?
TN
?
))))
As a step towards the above-mentioned goal, this work focuses on the development of an automated
system that is capable of simultaneously parsing semantically equivalent natural language texts in differ-
ent languages into their underlying semantics.
Specifically, in this work, we first introduce a new variant of a semantic parsing model under an
existing framework. This new variant can be used together with other models for jointly making semantic
parsing predictions, leading to an improved multilingual semantic parsing system. We demonstrate the
effectiveness of this new variant through experiments. Although bilingual parsing has been extensively
studied in fields such as statistical machine translation (Wu, 1997; Chiang, 2007), to the best of our
knowledge, bilingual or multilingual semantic parsing that focuses on parsing sentences from multiple
different languages into their formal semantic representations has not yet been studied. We present the
very first work on performing multilingual semantic parsing that simultaneously parses semantically
equivalent sentences from multiple different languages into their semantics. We believe this line of work
can potentially lead to further developments and advancements in areas such as multilingual semantic
processing and semantics-based machine translations (Jones et al., 2012a).
2 Background
2.1 Semantics
Researchers have focused on various semantic formalisms for semantic parsing. Popular examples
include the tree-structured semantic representations (Wong and Mooney, 2006; Kate and Mooney,
1292
QUERY : answer(RIVER)
?RIVER : exclude(RIVER, RIVER)
RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : (
?
TN
?
)
Tennessee
run through
do notRIVER : river(all)
rivers
What
Figure 2: An example hybrid tree. Such a hybrid tree is generated from the generative process, and captures the correspon-
dences between natural language words and semantic units.
2006), the lambda calculus expressions (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007),
and dependency-based semantic representations (DCS) (Liang et al., 2013). In this work, we specifically
focus on the tree-structured representations for semantics.
Each semantic representation consists of semantic units as its tree nodes, where each semantic unit is
of the following form:
m
a
? ?
a
: p
?
(?
b
?) (1)
Herem
a
is used to denote a complete semantic unit, which consists of its semantic type ?
a
, its function
symbol p
?
, as well as a list of types for argument semantic units ?
b
? (here ? means 0, 1, or 2; we assume
there are at most two arguments for each semantic unit). In other words, each semantic unit can be
regarded as a function which takes in other semantic representations of specific types as arguments, and
returns a new semantic representation of a particular type. For example, in Figure 1, the semantic unit at
the root has a type QUERY, a function name answer, and a single argument type RIVER.
2.2 Related Work
Substantial research efforts have focused on building monolingual semantic parsing systems. We survey
in this section several of them.
WASP (Wong and Mooney, 2006) is a model motivated by statistical synchronous parsing-based ma-
chine translation (Chiang, 2007), which essentially casts the semantic parsing problem as a phrase-based
translation problem (Koehn et al., 2003). KRISP (Kate and Mooney, 2006) makes use of Support Vector
Machines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequences into
semantic units to construct a tree structure. The SCISSOR model (Ge and Mooney, 2005) performs in-
tegrated semantic and syntactic parsing. The model parses natural language sentences into semantically
augmented parse trees whose nodes consist of both semantic and syntactic labels and then builds seman-
tic representations based on such augmented trees. The hybrid tree model (Lu et al., 2008; Lu et al.,
2009), whose code is publicly available, makes the assumption that there exists an underlying generative
process for jointly producing both the language and semantics. The model employs efficient dynamic
programming algorithms for learning a distribution over the latent hybrid trees which jointly encode both
language and semantics. An example hybrid tree representation is shown in Figure 2. Jones et al. (2012b)
recently proposed a framework that performs semantic parsing with tree transducers. The model learns
representations that are similar to the hybrid tree structures using a generative process under a Bayesian
framework.
Besides these approaches, recently there are also several works that take alternative learning ap-
proaches for semantic parsing which do not require annotated semantic representations (Poon and
Domingos, 2009; Clarke et al., 2010; Goldwasser et al., 2011; Liang et al., 2013; Artzi and Zettle-
moyer, 2013). Most of such approaches rely on either weak supervision or certain forms of indirect
supervision. Some of these works also focus on optimizing specific downstream tasks rather than the
semantic parsing task itself.
1293
ma
w
9
w
10
m
b
w
7
w
8
m
d
w
6
w
3
w
4
w
5
m
c
w
1
w
2
m
a
m
b
w
10
m
d
w
8
w
9
w
7
m
c
w
6
w
4
w
5
w
1
w
2
w
3
Figure 3: Two example hybrid trees. Their leaves are natural language words, and the internal nodes are semantic units. Both
hybrid trees correspond to the same n-m pair ?w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9
w
10
,m
a
(m
b
(m
c
,m
d
))?. Thus they can be
viewed as two different ways of generating such a pair from the joint generative process.
We note there also exist various multilingual or cross-lingual semantic processing works. Most of
such works focus on semantic role labeling(SRL), the task of recovery of shallow meaning. Examples
include multilingual semantic role labeling (Bj?orkelund et al., 2009), multilingual joint syntactic and se-
mantic dependency parsing (Henderson et al., 2013), and cross-lingual transfer of semantic role labeling
models (Kozhevnikov and Titov, 2013). Researchers also looked into exploiting semantic information
for bilingual processing such as machine translations (Chan et al., 2007; Carpuat and Wu, 2007; Jones et
al., 2012a).
In this work, we focus on the task of multilingual semantic parsing under the setting where the in-
put consists of semantically equivalent sentences from multiple different languages, and the outputs
are formal semantic representations. We specifically focus on the hybrid tree model, a state-of-the-art
framework for semantic parsing. We first make an extension to the model, and investigate methods for
performing such a multilingual semantic parsing task by aggregating a few variants of the models under
such a framework.
3 Approach
In this section, we first discuss the hybrid tree model of Lu et al. (2008), and introduce a novel extension.
Next we discuss the approach used for multilingual semantic parsing.
3.1 The Hybrid Tree Model
For a given n-m pair (where n is a complete natural language sentence, and m is a complete semantic
representation), the hybrid tree model assumes that both n and m are generated from an underlying
generative process in a top-down, left-to-right, level-by-level, recursive manner. The joint generative
process for the pair results in a new tree-structured representation called a hybrid tree, which consists of
natural language words as leaves, and semantic units as internal nodes.
There are three types of model parameters involved in the generative process. The meaning repre-
sentation model parameters (?) are used for generating one semantic unit from its parent semantic unit.
The hybrid pattern parameters (?) are used for deciding how natural language words and semantic units
are organized together to form the next level of the nodes of the hybrid tree structure. The emission
parameters (?) are used for generating natural language words from its corresponding semantic unit.
For a given n-m pair, there are multiple possible hybrid trees that can jointly represent such a pair.
See Figure 3 for two possible hybrid trees that contain the same n-m pair. Consider the first example
hybrid tree illustrated there. The probability of generating such a hybrid tree h (i.e., jointly generating
both the natural language sentence n and the semantics m) is:
P (n,m,h) = ?(m
a
)? ?(Xw|m
a
)? ?(X|m
a
,?)? ?(w
9
|m
a
,?)? ?(w
10
|m
a
,?)
??(m
b
|m
a
, arg = 1)? ?(XwYw|m
b
)? ?(X|m
b
,?)? ?(w
3
|m
b
,?)
??(w
4
|m
b
,?)? ?(w
5
|m
b
,?)? ?(Y|m
b
,?)? ?(w
7
|m
b
,?)? ?(w
8
|m
b
,?)
??(m
c
|m
b
, arg = 1)? ?(w|m
c
)? ?(w
1
|m
c
,?)? ?(w
2
|m
c
,?)
??(m
d
|m
b
, arg = 2)? ?(w|m
d
)? ?(w
6
|m
d
,?) (2)
1294
Note that Xw refers to a pattern which says the next level of the hybrid tree is expected to consist
of the first child semantic unit, followed by a contiguous sequence of natural language words. Similar
definitions can be given to the patterns XwYw and w, where X and Y refer to the first and second
child semantic unit, respectively. The symbols X and Y appear in emission parameters are used to
denote placeholders for the first and second child semantic unit, respectively.
The hybrid tree model then focuses on the learning of these model parameters from the training data
using maximum likelihood estimation. In other words, the model tries to maximize:
?
i
logP (n
i
,m
i
; ?, ?, ?) =
?
i
log
?
h
P (n
i
,m
i
,h; ?, ?, ?) (3)
Since the correct hybrid tree associated with n-m pair is unknown, we marginalize over the hidden
variable h. The model parameters will then be estimated using the Expectation-Maximization (EM)
algorithm (Dempster et al., 1977). Specifically, an inside-outside style algorithm (Baker, 2005) is used
where an additional layer of dynamic programming algorithms are used for efficient inference (Lu et
al., 2008). The complexity of the inference algorithm is O(mn
3
), where m is the size of the semantic
representation (number of semantic units), and n is the number of words in the input sentence.
Note that the generation of natural language words involves the context ?. Specifically, if the context
is empty, the model is regarded as the unigram model. If the context is the previously generated word, the
model is called a bigram model. For example, consider the generation of the natural language word w
4
in
the left hybrid tree in Figure 2. The probability for generating this word is ?(w
4
|m
b
) and ?(w
4
|m
b
, w
3
),
under the unigram and the bigram model, respectively. In Lu et al. (2008), the mixgram model (an
interpolation between the unigram model and the bigram model) was also considered when parsing
novel sentences, which yielded a better performance.
Once the model parameters are learned, we will be able to use them to parse novel sentences. Specifi-
cally, for each novel input sentence, we first find the most probable hybrid tree that contains the sentence
n, and then extract its internal nodes to form the semantic representation. Efficient dynamic program-
ming algorithms similar to the ones used for training can also be employed here. In addition, the algo-
rithm can also be extended to support exact top-k decoding, which will be useful later for combining
multiple lists of outputs with rank aggregation (to be discussed in Sec. 3.3).
3.2 The Backward Bigram Model
One assumption associated with the original hybrid tree model is that nodes at each level of the hybrid
tree are generated from the left to the right. An alternative assumption would be that the nodes at each
level are generated in the reverse order ? from the right to the left. While this alternative assumption
will not introduce any difference in the unigram model (since each node is generated from its respective
parent semantic unit only, regardless of its context), such a new assumption will lead to a completely
new generative process under the bigram assumption.
To see this, again consider the emission probability for generating the word w
4
in the hybrid tree on
the left of Figure 3. Under the assumption of our new model, the probability of generating this word is
?(w
4
|m
b
, w
5
), since now the context ? becomes the word to the right of the current word. The parameter
estimation and parsing (decoding) procedures are largely similar to those of the original bigram model,
where similar efficient dynamic programming algorithms can be employed.
3.3 Multilingual Semantic Parsing
In multilingual semantic parsing, the input consists of multiple semantically equivalent sentences, each
of which is from a different language. One approach for building such a multilingual semantic parsing
system is to develop a joint generative process from which both the semantic representations and the
sentences in different languages are generated simultaneously. However, building such a joint model
is non-trivial. Typically, sentences from different languages exhibit very different syntactic structures
and word orderings. It is also non-trivial to design efficient dynamic programming algorithms for this
case where multiple languages are involved in the joint generative process. Furthermore, the difficulty
of building such a joint generative model becomes higher as the number of input languages increases.
1295
Previous research efforts show that it can be beneficial to learn individual models independently, and
then combine the learned models only during the inference stage (Punyakanok et al., 2005; Chang et al.,
2012). Motivated by this, we take the approach that learns a separate semantic parser for each different
language first. Next, we combine these semantic parsers for different languages into a single multilingual
semantic parser only during the inference stage.
One common approach for combining different outputs from different systems is to perform majority
voting based on optimal predictions from each parser. We first obtain the best output semantic represen-
tation from each individual semantic parser, and then count the number of occurrences for each possible
output. The most frequent output semantic representation is returned as the final output of our system.
Naturally, this approach is only applicable when there are at least three systems/models.
An alternative approach is to allow each system to produce a ranked list of k most probable outputs,
each is associated with a score. Our system then aggregates these ranked lists to select the best output.
This problem is known as rank aggregation and has been extensively studied in fields such as data mining
and information retrieval (Dwork et al., 2001; Gleich and Lim, 2011; Li, 2011). For our task, we first
let each semantic parser (for each language) generate a ranked list of the top-k most probable outputs
(hybrid trees) for the given input. Next, based these hybrid trees we find a ranked list of most probable
semantic representations. Each such semantic representation is also associated with a score, which is
the log-likelihood of the hybrid tree, i.e., logP (n,m,h). Note that for each semantic representation,
we only consider the score associated with the most probable hybrid tree that contains such a semantic
representation. We use the standard approach for combining two ranked lists with scores. Consider a
ranked list from the j-th model/system that consists of n distinct items. Let?s use s
(j)
i
to denote the
original score associated with the i-th semantic representation in the j-th ranked list. We normalize the
score s
(j)
i
in the following way to obtain the new score s?
(j)
i
(normalized score, divided by the standard
deviation associated with the sample):
s?
(j)
i
=
s
(j)
i
n?
(j)
?
(j)
where ?
(j)
=
1
n
?
n
k=1
s
(j)
k
, ?
(j)
=
?
?
?
?
1
n? 1
n
?
k=1
(s
(j)
k
? ?
(j)
)
2
Such new scores will then be used for aggregating the results to form a new ranked list. How do we
find the best output from multiple lists? Two useful sources of information that we may use include: 1)
the number of times each output appears in these lists; 2) the combined score
?
j
s?
(j)
for each output s.
We believe the more frequent an output appears in these lists (i.e., more systems/models predict such an
output in their top-k lists), the more likely it can be a good candidate. Therefore we first find the set of
most frequent outputs, next from such a set we select the output with the highest overall score
?
j
s?
(j)
as the final output of our system.
4 Experiments
4.1 Data and Setup
We conducted our experiments on the multilingual GEOQUERY dataset released by Jones et al. (2012b).
This dataset consists of 880 instances of natural language queries related to US geography facts. Each
query is coupled with its corresponding semantic representation originally written in Prolog. The origi-
nal GEOQUERY dataset (Wong and Mooney, 2006; Kate and Mooney, 2006) contains natural language
queries in English only. Additional Chinese annotations were provided by Lu and Ng (2011) when per-
forming a natural language generation task. Jones et al. (2012b) further provided the following three
additional language annotations to this dataset: German, Greek and Thai. Thus, this dataset is now fully
annotated with five different languages, two of which (Chinese, Thai) are Sino-Tibetan languages, and
the rest are all Indo-European languages.
Following previous works on semantic parsing (Kwiatkowski et al., 2010; Jones et al., 2012b), we
split the dataset into two portions. The training set consists of 600 instances, and we report evaluation
results on the portion consisting of the remaining 280 instances. We used the identical split provided by
Jones et al. (2012b) for all the experiments. Following previous works, we used the standard approach for
1296
EN DE EL TH CN
Unigram 70.0 59.6 70.0 68.9 68.9
Bigram 75.4 56.1 65.4 70.7 68.9
Bigram (inv) 74.3 57.1 65.4 71.1 66.8
Mixgram 76.1 62.5 69.3 73.2 70.7
Voting (u,b,m) 76.1 61.1 70.4 73.6 70.0
Voting (u,b,bi) 76.4 61.4 71.8 74.3 72.1
Aggregation 78.6 60.0 72.1 71.4 73.2
Table 1: Monolingual semantic parsing results on all five languages (EN:English, DE:German, EL:Greek, TH:Thai,
CN:Chinese.). We report accuracy percentages in this table.
ENDE ENEL ENTH ENCN DEEL DETH DECN ELTH ELCN THCN
Unigram 74.6 76.1 76.4 75.0 76.8 72.1 74.3 80.4 79.6 74.0
Bigram 80.0 77.9 87.1 78.2 72.1 75.0 76.4 81.4 76.8 79.6
Bigram (inv) 78.2 76.8 86.4 75.7 72.5 75.7 76.1 82.1 75.7 79.3
Mixgram 77.9 76.4 82.5 81.1 76.1 75.7 74.3 81.1 80.7 77.9
Voting (u,b) 80.0 79.6 83.6 82.1 77.1 74.6 74.6 82.1 78.6 79.6
Voting (u,b,bi) 82.1 79.3 86.4 82.1 76.8 77.1 76.4 85.4 78.9 80.7
Aggregation 78.9 82.1 85.7 83.6 76.4 73.6 76.8 83.9 81.4 79.3
Table 2: Semantic parsing results when two different input languages are considered (for example, the column ENDE gives the
results when each input to our system consists of a pair of semantically equivalent sentences written in English and German.).
Scores are accuracy percentages.
evaluation on the multilingual GEOQUERY dataset. Specifically, we first let our semantic parsers produce
semantic representations from multilingual input sentences. The resulting semantic representations are
then converted into Prolog queries in a deterministic manner, which can be used to interact with the
underlying knowledge base to retrieve answers. A predicted semantic representation is considered correct
if and only if it retrieves identical results as the correct reference semantic representation when both are
used for retrieving answers from the underlying database.
4.2 Results and Discussions
We performed experiments on the conventional monolingual semantic parsing task first. We report accu-
racy scores, which are defined as the number of correctly parsed inputs (i.e., the total number of correct
semantic representations) divided by the total number of input sentences. Baseline results for unigram,
bigram, and mixgram models, which are originally introduced in Lu et al. (2008) are reported under
?Unigram?, ?Bigram?, and ?Mixgram? respectively in Table 1. The results for backward bigram models
are reported under ?Bigram(inv)?.
To assess the effectiveness of our methods for combining different outputs, we first conducted ex-
periments on voting over the outputs from the three models originally introduced in the work of Lu et
al. (2008) (Voting(u,b,m)). Next we performed voting over outputs from unigram model, bigram model,
as well as the backward bigram model introduced in this paper (Voting(u,b,bi)). These voting-based
approaches yielded better results over the first voting-based approach. Specifically, we compared this
new voting-based approach against the previous best model reported in Lu et al. (2008) ? mixgram
model, which was also based on a combination of unigram and bigram models. We used the paired
t-test to assess the significance of the overall improvements across different languages when using our
new method. When comparing the approach ?Voting(u,b,m)? over ?Mixgram?, we obtained a one-tailed
p-value of 0.40. When comparing the approach ?Voting(u,b,bi)? over ?Mixgram?, we obtained a one-
tailed p-value of 0.11. We also investigated the effectiveness of the aggregation-based approach. This
approach is based on aggregating the two top-100 lists generated by unigram, bigram and backward
bigram models. When comparing this approach over ?Mixgram?, we obtained a one-tailed p-value of
1297
ENDE ENDE ENDE ENEL ENEL ENTH DEEL DEEL DETH ELTH
EL TH CN TH CN CN TH CN CN CN
Unigram 79.6 78.2 79.3 83.2 83.2 79.3 81.8 79.6 77.1 81.4
Bigram 82.1 85.7 81.8 87.5 81.8 86.4 82.5 80.7 79.6 83.6
Bigram (inv) 82.9 85.4 79.6 86.8 81.1 85.4 82.1 80.4 78.9 83.2
Mixgram 81.4 83.2 81.8 85.0 83.2 84.3 82.9 80.7 79.3 82.9
Voting (u,b) 83.2 85.0 84.3 87.9 84.0 85.0 84.0 83.6 81.1 84.6
Voting (u,b,bi) 84.0 86.1 85.4 89.6 84.3 86.8 85.0 82.5 81.1 84.6
Aggregation 83.6 85.0 85.4 88.9 87.1 85.7 82.5 82.5 80.0 85.4
Table 3: Semantic parsing results when three different input languages are considered (for example, the column ENDEEL gives
the results when each input to our system consists of three semantically equivalent sentences, which are written in English,
German and Greek, respectively.). Scores are accuracy percentages.
ENDE ENDE ENDE ENEL DEEL ENDEEL
ELTH ELCN THCN THCN THCN THCN
Unigram 82.9 82.1 81.1 85.0 82.1 84.0
Bigram 86.1 83.6 84.3 87.1 85.0 86.1
Bigram (inv) 86.4 82.5 84.0 86.8 85.4 85.0
Mixgram 84.0 82.1 83.2 86.4 84.0 85.7
Voting (u,b) 87.5 86.1 86.4 89.6 86.4 89.3
Voting (u,b,bi) 88.6 86.8 87.1 90.0 85.7 89.6
Aggregation 87.1 87.1 86.1 88.9 86.1 88.6
Table 4: Semantic parsing results when four or five different input languages are considered (for example, the column
ENDEELTH gives the results when each input to our system consists of four semantically equivalent sentences, which are
written in English, German, Greek, and Thai respectively.). Scores are accuracy percentages.
0.29 under the paired t-test. These results indicate that the approach based on voting over the unigram,
bigram and backward bigram models gives the most promising results for monolingual semantic parsing,
demonstrating the usefulness of our proposed backward bigram model.
Next we move to the multilingual setting where we would like to simultaneously process more than
two languages. Specifically, we considered multilingual semantic parsing where there are two, three,
four and five input languages. Table 2, Table 3, and Table 4 summarize these results. Table 2 shows the
results for bilingual semantic parsing where we have two different input languages. The results reported
under ?Unigram? are based on the aggregation approach over unigram models. Similarly for ?Bigram?,
?Bigram(inv)?, and ?Mixgram? (we also tried the voting-based approach for combining such baseline
systems, which yielded slightly worse results). From this table we can see that generally speaking by
considering two different languages as the input, our system is able to do better semantic parsing. We
compared the voting-based approaches against the baseline approaches. For the approach ?Voting(u,b)?
(we excluded mixgram models in voting since now we have four models, two from each language, which
are sufficient for voting, and preliminary results show that the inclusion of the mixgram models is not
helpful), it does not outperform the bigram baseline approach (which is the most competitive amongst
all baseline approaches) significantly (p = 0.19). When comparing the aggregation approach against the
bigram baseline approach, we obtain p = 0.04. In contrast, the approach ?Voting(u,b,bi)? outperforms
all the baseline systems significantly (p < 0.005). These results again demonstrate the effectiveness of
our newly proposed backward bigram model.
We can see from the results presented in Table 3 and Table 4 that, in general, the performance of the
multilingual semantic parser tends to improve as the number of input languages increases. However this
is not always the case. For example, consider the final system where we use all five languages as the input
(refer to the results in the column of ENDEELTHCN in Table 4); interestingly, when we remove German
(DE) from the inputs, we are able to build a better system in terms of accuracy (refer to the results in the
column of ENELTHCN). We believe this is partly due to the fact that the monolingual semantic parsing
1298
task with German as the input language (see DE in Table 1) is relatively more challenging. Nevertheless,
when all the languages are considered, the overall system is able to obtain an accuracy of 89.6% with the
voting-based approach where our proposed backward bigram model is incorporated. This is significantly
higher than any other monolingual system?s performance reported in the literature. According to Jones
et al. (2012b), the results of state-of-the-art monolingual semantic parsing systems on four of these five
languages considered here are: 82.1%(EN), 75.0%(DE), 75.4%(EL), and 78.2%(TH). Note that to date,
no single system reported in the literature can dominate all other systems across all these languages on
this dataset in terms of accuracy performance. We hypothesize that this is because semantic information
conveyed by the sentences from a single language tends to be highly ambiguous, and various linguis-
tic phenomenons can be difficult to capture under a monolingual setting for any existing monolingual
semantic parsing system. The multilingual semantic parsing system introduced in this work, in con-
trast, can exploit richer information from multiple languages to successfully disambiguate the semantics
associated with the inputs for improved semantic parsing.
5 Conclusions
In this work, we focused on multilingual semantic parsing, the task of simultaneously parsing sentences
from various different languages into their corresponding formal semantic representations. Our work is
built on top of the hybrid tree framework where different generative process can be developed for jointly
modelling the generation of both language and semantics. We first introduced a variant of the generative
process, leading to a new semantic parsing model. Next we presented methods for combining and ag-
gregating outputs from different models within the framework to build our multilingual semantic parsing
system. Our results demonstrate the effectiveness of our approaches for such a task. To the best of our
knowledge, this is the first work that tackles such a multilingual semantic parsing task which simulta-
neously parses sentences from multiple languages into formal semantic representations. Future work
include explorations on applications of our system in areas such as multilingual semantic processing,
cross-lingual semantic processing, and semantics-based machine translations (Jones et al., 2012a).
Acknowledgements
We would like to thank the anonymous reviewers for comments. This work was conducted during the
first author?s internship at SUTD. This work was supported by SUTD grant SRG ISTD 2013 064.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions
to actions. TACL.
James K Baker. 2005. Trainable grammars for speech recognition. The Journal of the Acoustical Society of
America, 65(S1):S132?S132.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings
of CONLL?09, pages 43?48.
Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation.
In Proceedings of EMNLP-CoNLL ?07, pages 61?72.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In Proceedings
of AAAI ?05, pages 1037?1042.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine
translation. In Proceedings of ACL ?07.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2012. Structured learning with constrained conditional models.
Machine learning, 88(3):399?431.
David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201?228.
1299
James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world?s
response. In Proceedings of CONLL ?10, pages 18?27.
Arthur P Dempster, Nan M Laird, Donald B Rubin, et al. 1977. Maximum likelihood from incomplete data via
the em algorithm. Journal of the Royal statistical Society, 39(1):1?38.
Cynthia Dwork, Ravi Kumar, Moni Naor, and Dandapani Sivakumar. 2001. Rank aggregation methods for the
web. In Proceedings of WWW ?01, pages 613?622.
Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In
Proceedings of CONLL ?05, pages 9?16.
David F Gleich and Lek-Heng Lim. 2011. Rank aggregation via nuclear norm minimization. In Proceedings of
KDD ?11, pages 60?68.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic
parsing. In Proceedings of ACL ?11, pages 1486?1495.
James Henderson, Paola Merlo, Ivan Titov, and Gabriele Musillo. 2013. Multi-lingual joint parsing of syntactic
and semantic dependencies with a latent variable model. Computational Linguistics, 39(4).
Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012a. Semantics-based
machine translation with hyperedge replacement grammars. In Proceedings of COLING ?12, pages 1359?1376.
Bevan Keeley Jones, Mark Johnson, and Sharon Goldwater. 2012b. Semantic parsing with bayesian tree transduc-
ers. In Proceedings of ACL ?12, pages 488?496.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings
of COLING/ACL ?06, pages 913?920.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of
NAACL ?03, pages 48?54.
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-lingual transfer of semantic role labeling models. In Proceed-
ings of ACL ?13.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg
grammars from logical form with higher-order unification. In Proceedings of EMNLP?10, pages 1223?1233.
Hang Li. 2011. Learning to rank for information retrieval and natural language processing. Synthesis Lectures on
Human Language Technologies, 4(1):1?113.
Percy Liang, Michael I Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics.
Computational Linguistics, 39(2):389?446.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification
using string kernels. The Journal of Machine Learning Research, 2:419?444.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed
lambda calculus expressions. In Proceedings of EMNLP ?11, pages 1611?1622.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings of EMNLP ?08, pages 783?792.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Natural language generation with tree conditional random fields.
In Proceedings of EMNLP ?09, pages 400?409.
Raymond J. Mooney. 2007. Learning for semantic parsing. In Computational Linguistics and Intelligent Text
Processing, pages 311?324. Springer.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of EMNLP ?09, pages
1?10.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2005. Learning and inference over constrained
output. In Proceedings of IJCAI ?05, pages 1124?1129.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation.
In Proceedings of HLT/NAACL ?06, pages 439?446.
1300
Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of ACL ?07.
Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403.
Luke S Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Proceedings of UAI ?05.
Luke S Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical
form. In Proceedings of ACL/IJCNLP ?09, pages 976?984.
1301
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 177?186,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Better Punctuation Prediction with Dynamic Conditional Random Fields
Wei Lu and Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive, Singapore 117417
{luwei,nght}@comp.nus.edu.sg
Abstract
This paper focuses on the task of insert-
ing punctuation symbols into transcribed con-
versational speech texts, without relying on
prosodic cues. We investigate limitations as-
sociated with previous methods, and propose a
novel approach based on dynamic conditional
random fields. Different from previous work,
our proposed approach is designed to jointly
perform both sentence boundary and sentence
type prediction, and punctuation prediction on
speech utterances.
We performed evaluations on a transcribed
conversational speech domain consisting of
both English and Chinese texts. Empirical re-
sults show that our method outperforms an ap-
proach based on linear-chain conditional ran-
dom fields and other previous approaches.
1 Introduction
Outputs of standard automatic speech recognition
(ASR) systems typically consist of utterances where
important linguistic and structural information (e.g.,
true case, sentence boundaries, punctuation sym-
bols, etc) is not available. Such information is cru-
cial in improving the readability of the transcribed
speech texts, and plays an important role when fur-
ther processing is required, such as in part-of-speech
(POS) tagging, parsing, information extraction, and
machine translation.
We focus on the punctuation prediction task in
this work. Most previous punctuation prediction
techniques, developed mostly by the speech process-
ing community, exploit both lexical and prosodic
cues. However, in order to fully exploit prosodic fea-
tures such as pitch and pause duration, it is necessary
to have access to the original raw speech waveforms.
In some scenarios where further natural language
processing (NLP) tasks on the transcribed speech
texts become the main concern, speech prosody in-
formation may not be readily available. For exam-
ple, in the recent evaluation campaign of the Inter-
national Workshop on Spoken Language Translation
(IWSLT) (Paul, 2009), only manually transcribed or
automatically recognized speech texts are provided
but the original raw speech waveforms are not avail-
able.
In this paper, we tackle the task of predicting
punctuation symbols from a standard text processing
perspective, where only the speech texts are avail-
able, without relying on additional prosodic fea-
tures such as pitch and pause duration. Specifi-
cally, we perform the punctuation prediction task
on transcribed conversational speech texts, using the
IWSLT corpus (Paul, 2009) as the evaluation data.
Different from many other corpora such as broad-
cast news corpora, a conversational speech corpus
consists of dialogs where informal and short sen-
tences frequently appear. In addition, due to the
nature of conversation, it also contains more ques-
tion sentences compared to other corpora. An ex-
ample English utterance randomly selected from the
IWSLT corpus, along with its punctuated and cased
version, are shown below:
you are quite welcome and by the way we may get
other reservations so could you please call us
as soon as you fix the date
You are quite welcome . And by the way , we may
get other reservations , so could you please call
us as soon as you fix the date ?
177
The rest of this paper is organized as follows.
We start with surveying related work in Section 2.
One class of widely-used previous techniques is then
studied in detail in Section 3. Next, we investigate
methods for improving existing methods in Section
4 and 5. Empirical evaluation results are presented
and discussed in Section 6. We finally conclude in
Section 7.
2 Related Work
Punctuation prediction has been extensively studied
in the speech processing field. It is also sometimes
studied together with a closely related task ? sen-
tence boundary detection.
Much previous work assumes that both lexical
and prosodic cues are available for the task. Kim
and Woodland (2001) performed punctuation inser-
tion during speech recognition. Prosodic features to-
gether with language model probabilities were used
within a decision tree framework. Christensen et
al. (2001) focused on the broadcast news domain
and investigated both finite state and multi-layer per-
ceptron methods for the task, where prosodic and
lexical information was incorporated. Huang and
Zweig (2002) presented a maximum entropy-based
tagging approach to punctuation insertion in spon-
taneous English conversational speech, where both
lexical and prosodic features were exploited. Liu
et al (2005) focused on the sentence boundary de-
tection task, by making use of conditional random
fields (CRF) (Lafferty et al, 2001). Their method
was shown to improve over a previous method based
on hidden Markov model (HMM).
There is relatively less work that exploited lexical
features only. Beeferman et al (1998) focused on
comma prediction with a trigram language model. A
joint language model was learned from punctuated
texts, and commas were inserted so as to maximize
the joint probability score. Recent work by Gravano
et al (2009) presented a purely n-gram based ap-
proach that jointly predicted punctuation and case
information of English.
Stolcke et al (1998) presented a ?hidden event
language model? that treated boundary detection
and punctuation insertion as an interword hidden
event detection task. Their proposed method was
implemented in the handy utility hidden-ngram as
part of the SRILM toolkit (Stolcke, 2002). It was
widely used in many recent spoken language trans-
lation tasks as either a preprocessing (Wang et al,
2008) or postprocessing (Kirchhoff and Yang, 2007)
step. More details about this model will be given in
the next section.
Recently, there are also several research efforts
that try to optimize some downstream application
after punctuation prediction, rather than the predic-
tion task itself. Examples of such downstream ap-
plications include punctuation prediction for part-of-
speech (POS) tagging and name tagging (Hillard et
al., 2006), statistical machine translation (Matusov
et al, 2006), and information extraction (Favre et
al., 2008).
3 Hidden Event Language Model
Many previous research efforts consider the bound-
ary detection and punctuation insertion task as a hid-
den event detection task. One such well-known ap-
proach was introduced by Stolcke et al (1998).
They adopted a HMM to describe a joint distribu-
tion over words and interword events, where the ob-
servations are the words, and the word/event pairs
are encoded as hidden states. Specifically, in this
task word boundaries and punctuation symbols are
encoded as interword events. The training phase
involves training an n-gram language model over
all observed words and events with smoothing tech-
niques. The learned n-gram probability scores are
then used as the HMM state-transition scores. Dur-
ing testing, the posterior probability of an event
at each word is computed with dynamic program-
ming using the forward-backward algorithm. The
sequence of most probable states thus forms the out-
put which gives the punctuated sentence.
Such a HMM-based approach has several draw-
backs. First, the n-gram language model is only
able to capture surrounding contextual information.
However, we argue that in many cases, modeling of
longer range dependencies is required for punctua-
tion insertion. For example, the method is unable
to effectively capture the long range dependency be-
tween the initial phrase ?would you? which strongly
indicates a question sentence, and an ending ques-
tion mark. This hurts the punctuation prediction per-
formance for our task since we are particularly inter-
178
ested in conversational speech texts where question
sentences appear frequently.
Thus, in practice, special techniques are usually
required on top of using a hidden event language
model in order to overcome long range dependen-
cies. Examples include relocating or duplicating
punctuation symbols to different positions of a sen-
tence such that they appear closer to the indicative
words (e.g., ?how much? indicates a question sen-
tence). One such technique was introduced by the
organizers of the IWSLT evaluation campaign, who
suggested duplicating the ending punctuation sym-
bol to the beginning of each sentence before training
the language model1. Empirically, the technique has
demonstrated its effectiveness in predicting question
marks in English, since most of the indicative words
for English question sentences appear at the begin-
ning of a question. However, such a technique is
specially designed and may not be widely applica-
ble in general or to languages other than English.
Furthermore, a direct application of such a method
may fail in the event of multiple sentences per utter-
ance without clearly annotated sentence boundaries
within an utterance.
Another drawback associated with such an ap-
proach is that the method encodes strong depen-
dency assumptions between the punctuation symbol
to be inserted and its surrounding words. Thus, it
lacks the robustness to handle cases where noisy or
out-of-vocabulary (OOV) words frequently appear,
such as in texts automatically recognized by ASR
systems. In this paper, we devise techniques based
on conditional random fields to tackle the difficulties
due to long range dependencies.
4 Linear-Chain Conditional Random
Fields
One natural approach to relax the strong depen-
dency assumptions encoded by the hidden event lan-
guage model is to adopt an undirected graphical
model, where arbitrary overlapping features can be
exploited.
Conditional random fields (CRF) (Lafferty et al,
2001) have been widely used in various sequence
labeling and segmentation tasks (Sha and Pereira,
1http://mastarpj.nict.go.jp/IWSLT2008/downloads/
case+punc tool using SRILM.instructions.txt
2003; Tseng et al, 2005). Unlike a HMM which
models the joint distribution of both the label se-
quence and the observation, a CRF is a discrimi-
native model of the conditional distribution of the
complete label sequence given the observation.
Specifically, a first-order linear-chain CRF which
assumes first-order Markov property is defined by
the following equation:
p?(y|x) =
1
Z(x)
exp
(
?
t
?
k
?kfk(x, yt?1, yt, t)
)
(1)
where x is the observation and y is the label se-
quence. Feature functions fk with time step t are
defined over the entire observation x and two adja-
cent hidden labels. Z(x) is a normalization factor to
ensure a well-formed probability distribution. Fig-
ure 1 gives a simplified graphical representation of
the model, where only the dependencies between la-
bel and observation in the same time step are shown.
y1
x1
y2
x2
y3
x3
. . . yn
xn
Figure 1: A simplified graphical representation for linear-
chain CRF (observations are shaded)
proposed tags
NONE COMMA (,) PERIOD (.)
QMARK (?) EMARK (!)
Table 1: The set of all possible tags for linear-chain CRF
We can model the punctuation prediction task as
the process of assigning a tag to each word, where
the set of possible tags is given in Table 1. That
is, we assume each word can be associated with
an event, which tells us which punctuation sym-
bol (possibly NONE) should be inserted after the
word. The training data consists of a set of utter-
ances where punctuation symbols are encoded as
tags that are assigned to the individual words. The
tag NONE means no punctuation symbol is inserted
after the current word. Any other tag refers to insert-
ing the corresponding punctuation symbol. In the
testing phase, the most probable sequence of tags is
179
Sentence: no , please do not . would you save your questions for the end of my talk , when i ask for them ?
no please do not would you . . . my talk when . . . them
COMMA NONE NONE PERIOD NONE NONE . . . NONE COMMA NONE . . . QMARK
Figure 2: An example tagging of a training sentence for the linear-chain CRF
predicted and the punctuated text can then be con-
structed from such an output. An example tagging
of an utterance is illustrated in Figure 2.
Following (Sutton et al, 2007), we factorize a
feature of conditional random fields as a product
of a binary function on assignment of the set of
cliques at the current time step (in this case an edge),
and a feature function solely defined on the ob-
servation sequence. n-gram occurrences surround-
ing the current word, together with position infor-
mation, are used as binary feature functions, for
n = 1, 2, 3. All words that appear within 5 words
from the current word are considered when build-
ing the features. Special start and end symbols are
used beyond the utterance boundaries. For example,
for the word do shown in Figure 2, example fea-
tures include unigram features do@0, please@-1,
bigram feature would+you@[2,3], and trigram fea-
ture no+please+do@[-2,0].
Such a linear-chain CRF model is capable of mod-
eling dependencies between words and punctuation
symbols with arbitrary overlapping features, thus
avoiding the strong dependency assumptions in the
hidden event language model. However, the linear-
chain CRF model still exhibits several problems for
the punctuation task. In particular, the dependency
between the punctuation symbols and the indicative
words cannot be captured adequately, if they appear
too far away from each other. For example, in the
sample utterance shown in Figure 2, the long range
dependency between the ending question mark and
the indicative words would you which appear very
far away cannot be directly captured. The problem
arises because a linear-chain CRF only learns a se-
quence of tags at the individual word level but is not
fully aware of sentence level information, such as
the start and end of a complete sentence.
Hence, it would be more reasonable to hypothe-
size that the punctuation symbols are annotated at
the sentence level, rather than relying on a limited
window of surrounding words. A model that can
jointly perform sentence segmentation and sentence
type prediction, together with word level punctu-
ation prediction would be more beneficial for our
task. This motivates us to build a joint model for
performing such a task, to be presented in the next
section.
5 Factorial Conditional Random Fields
Extensions to the linear-chain CRF model have been
proposed in previous research efforts to encode long
range dependencies. One such well-known exten-
sion is the semi-Markov CRF (semi-CRF) (Sarawagi
and Cohen, 2005). Motivated by the hidden semi-
Markov model, the semi-CRF is particularly helpful
in text chunking tasks as it allows a state to persist
for a certain interval of time steps. This in practice
often leads to better modeling capability of chunks,
since state transitions within a chunk need not pre-
cisely follow the Markov property as in the case of
linear-chain CRF. However, it is not clear how such
a model can benefit our task, which requires word-
level labeling in addition to sentence boundary de-
tection and sentence type prediction.
The skip-chain CRF (Sutton and McCallum,
2004), another variant of linear-chain CRF, attaches
additional edges on top of a linear-chain CRF for
better modeling of long range dependencies between
states with similar observations. However, such a
model usually requires known long range dependen-
cies in advance and may not be readily applicable to
our task where such clues are not explicit.
As we have discussed above, since we would
like to jointly model both the word-level labeling
task and the sentence-level annotation task (sentence
boundary detection and sentence type prediction),
introducing an additional layer of tags to perform
both tasks together would be desirable. In this sec-
tion, we propose the use of factorial CRF (F-CRF)
(Sutton et al, 2007), which has previously been
shown to be effective for joint labeling of multiple
sequences (McCallum et al, 2003).
180
The F-CRF as a specific case of dynamic condi-
tional random fields was originally motivated from
dynamic Bayesian networks, where an identical
structure repeats over different time steps. Analo-
gous to the linear-chain CRF, one can think of the F-
CRF as a framework that provides the capability of
simultaneously labeling multiple layers of tags for a
given sequence. It learns a joint conditional distri-
bution of the tags given the observation. Formally,
dynamic conditional random fields define the con-
ditional probability of a sequence of label vectors y
given the observation x as:
p?(y|x) =
1
Z(x)
exp
(
?
t
?
c?C
?
k
?kfk(x, y(c,t), t)
)
(2)
where cliques are indexed at each time step, C is a set
of clique indices, and y(c,t) is the set of variables in
the unrolled version of a clique with index c at time
t (Sutton et al, 2007). Figure 3 gives a graphical
representation of a two-layer factorial CRF, where
the cliques include the two within-chain edges (e.g.,
z2 ? z3 and y2 ? y3) and one between-chain edge
(e.g., z3 ? y3) at each time step.
z1
y1
x1
z2
y2
x2
z3
y3
x3
. . .
. . .
zn
yn
xn
Figure 3: A two-layer factorial CRF
layer proposed tags
word NONE,COMMA,PERIOD,
QMARK,EMARK
sentence DEBEG,DEIN,QNBEG,QNIN,
EXBEG,EXIN
Table 2: The set of all possible tags proposed for each
layer
We build two layers of labels for this task, as
listed in Table 2. The word layer tags are respon-
sible for inserting a punctuation symbol (including
NONE) after each word, while the sentence layer
tags are used for annotating sentence boundaries and
identifying the sentence type (declarative, question,
or exclamatory). Tags from the word layer are the
same as those of the linear-chain CRF. The sentence
layer tags are designed for three types of sentences.
DEBEG and DEIN indicate the start and the inner
part of a declarative sentence respectively, likewise
for QNBEG and QNIN (question sentences), as well
as EXBEG and EXIN (exclamatory sentences). The
same example utterance we looked at in the previous
section is now tagged with these two layers of tags,
as shown in Figure 4. Analogous feature factoriza-
tion and the same n-gram feature functions used in
linear-chain CRF are used in F-CRF.
When learning the sentence layer tags together
with the word layer tags, the F-CRF model is capa-
ble of leveraging useful clues learned from the sen-
tence layer about sentence type (e.g., a question sen-
tence, annotated with QNBEG, QNIN, QNIN, . . .,
or a declarative sentence, annotated with DEBEG,
DEIN, DEIN, . . .), which can be used to guide the
prediction of the punctuation symbol at each word,
hence improving the performance at the word layer.
For example, consider jointly labeling the utterance
shown in Figure 4. Intuitively, when evidences
show that the utterance consists of two sentences ?
a declarative sentence followed by a question sen-
tence, the model tends to annotate the second half of
the utterance with the sequence QNBEG QNIN . . ..
This in turn helps to predict the word level tag at
the end of the utterance as QMARK, given the de-
pendencies between the two layers existing at each
time step. In practice, during the learning process,
the two layers of tags are jointly learned, thus pro-
viding evidences that influence each other?s tagging
process.
In this work, we use the GRMM package (Sutton,
2006) for building both the linear-chain CRF (L-
CRF) and factorial CRF (F-CRF). The tree-based
reparameterization (TRP) schedule for belief propa-
gation (Wainwright et al, 2001) is used for approxi-
mate inference.
6 Experiments
We perform experiments on part of the corpus of the
IWSLT09 evaluation campaign (Paul, 2009), where
both Chinese and English conversational speech
181
Sentence: no , please do not . would you save your questions for the end of my talk , when i ask for them ?
no please do not would you . . . my talk when . . . them
COMMA NONE NONE PERIOD NONE NONE . . . NONE COMMA NONE . . . QMARK
DEBEG DEIN DEIN DEIN QNBEG QNIN . . . QNIN QNIN QNIN . . . QNIN
Figure 4: An example tagging of a training sentence for the factorial CRF
texts are used. Two multilingual datasets are consid-
ered, the BTEC (Basic Travel Expression Corpus)
dataset and the CT (Challenge Task) dataset. The
former consists of tourism-related sentences, and the
latter consists of human-mediated cross-lingual di-
alogs in travel domain. The official IWSLT09 BTEC
training set consists of 19,972 Chinese-English ut-
terance pairs, and the CT training set consists of
10,061 such pairs. We randomly split each of the
two datasets into two portions, where 90% of the ut-
terances are used for training the punctuation predic-
tion models, and the remaining 10% for evaluating
the prediction performance. For all the experiments,
we use the default segmentation of Chinese as pro-
vided, and English texts are preprocessed with the
Penn Treebank tokenizer2. We list the statistics of
the two datasets after processing in Table 3. The
proportions of sentence types in the two datasets are
listed. The majority of the sentences are declarative
sentences. However, question sentences are more
frequent in the BTEC dataset compared to the CT
dataset. Exclamatory sentences contribute less than
1% for all datasets and are not listed. We also count
how often each utterance consists of multiple sen-
tences. The utterances from the CT dataset are much
longer (with more words per utterance), and there-
fore more CT utterances actually consist of multiple
sentences.
BTEC CT
CN EN CN EN
declarative sent. 64% 65% 77% 81%
question sent. 36% 35% 22% 19%
multi.sent./uttr. 14% 17% 29% 39%
avg.words./uttr. 8.59 9.46 10.18 14.33
Table 3: Statistics of the BTEC and CT datasets
For the methods based on the hidden event lan-
guage model, we design extensive experiments due
2http://www.cis.upenn.edu/?treebank/tokenization.html
to many possible setups. Specifically, these exper-
iments can be divided into two categories: with or
without duplicating the ending punctuation symbol
to the start of a sentence before training. This set-
ting can be used to assess the impact of the proxim-
ity between the punctuation symbol and the indica-
tive words for the prediction task. Under each cat-
egory, two possible approaches are tried. The sin-
gle pass approach performs prediction in one sin-
gle step, where all the punctuation symbols are pre-
dicted sequentially from left to right. In the cas-
caded approach, we format the training sentences
by replacing all sentence-ending punctuation sym-
bols with special sentence boundary symbols first.
A model for sentence boundary prediction is learned
based on such training data. This step is then fol-
lowed by predicting the actual punctuation symbols.
Both trigram and 5-gram language models are tried
for all combinations of the above settings. This gives
us a total of 8 possible combinations based on the
hidden event language model. When training all the
language models, modified Kneser-Ney smoothing
(Chen and Goodman, 1996) for n-grams is used.
To assess the performance of the punctuation pre-
diction task, we compute precision (prec.), recall
(rec.), and F1-measure (F1), as defined by the fol-
lowing equations:
prec. =
# Correctly predicted punctuation symbols
# predicted punctuation symbols
rec. =
# Correctly predicted punctuation symbols
# expected punctuation symbols
F1 =
2
1/prec.+ 1/rec.
6.1 Performance on Correctly Recognized
Texts
The performance of punctuation prediction on both
Chinese (CN) and English (EN) texts in the correctly
recognized output of the BTEC and CT datasets are
presented in Table 4 and Table 5 respectively. The
182
BTEC
NO DUPLICATION USE DUPLICATION
SINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRF
LM ORDER 3 5 3 5 3 5 3 5
CN
Prec. 87.40 86.44 87.72 87.13 76.74 77.58 77.89 78.50 94.82 94.83
Rec. 83.01 83.58 82.04 83.76 72.62 73.72 73.02 75.53 87.06 87.94
F1 85.15 84.99 84.79 85.41 74.63 75.60 75.37 76.99 90.78 91.25
EN
Prec. 64.72 62.70 62.39 58.10 85.33 85.74 84.44 81.37 88.37 92.76
Rec. 60.76 59.49 58.57 55.28 80.42 80.98 79.43 77.52 80.28 84.73
F1 62.68 61.06 60.42 56.66 82.80 83.29 81.86 79.40 84.13 88.56
Table 4: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the correctly recognized
output of the BTEC dataset. Percentage scores of precision (Prec.), recall (Rec.), and F1 measure (F1) are reported.
CT
NO DUPLICATION USE DUPLICATION
SINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRF
LM ORDER 3 5 3 5 3 5 3 5
CN
Prec. 89.14 87.83 90.97 88.04 74.63 75.42 75.37 76.87 93.14 92.77
Rec. 84.71 84.16 77.78 84.08 70.69 70.84 64.62 73.60 83.45 86.92
F1 86.87 85.96 83.86 86.01 72.60 73.06 69.58 75.20 88.03 89.75
EN
Prec. 73.86 73.42 67.02 65.15 75.87 77.78 74.75 74.44 83.07 86.69
Rec. 68.94 68.79 62.13 61.23 70.33 72.56 69.28 69.93 76.09 79.62
F1 71.31 71.03 64.48 63.13 72.99 75.08 71.91 72.12 79.43 83.01
Table 5: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the correctly recognized
output of the CT dataset. Percentage scores of precision (Prec.), recall (Rec.), and F1 measure (F1) are reported.
performance of the hidden event language model
heavily depends on whether the duplication method
is used and on the actual language under considera-
tion. Specifically, for English, duplicating the end-
ing punctuation symbol to the start of a sentence
before training is shown to be very helpful in im-
proving the overall prediction performance. In con-
trast, applying the same technique to Chinese hurts
the performance.
This observed difference is reasonable and ex-
pected. An English question sentence usually starts
with indicative words such as do you or where that
distinguish it from a declarative sentence. Thus, du-
plicating the ending punctuation symbol to the start
of a sentence so that it is near these indicative words
helps to improve the prediction accuracy. However,
Chinese presents quite different syntactic structures
for question sentences. First, we found that in many
cases, Chinese tends to use semantically vague aux-
iliary words at the end of a sentence to indicate a
question. Such auxiliary words include ? and ?.
Thus, retaining the position of the ending punctu-
ation symbol before training yields better perfor-
mance. Another interesting finding is that, differ-
ent from English, other words that indicate a ques-
tion sentence in Chinese can appear at almost any
position in a Chinese sentence. Examples include
???. . . (where . . . ), . . .??? (what . . . ), or
. . .??. . . (how many/much . . . ). These pose diffi-
culties for the simple hidden event language model,
which only encodes simple dependencies over sur-
rounding words by means of n-gram language mod-
eling.
By adopting a discriminative model which ex-
ploits non-independent, overlapping features, the L-
CRF model generally outperforms the hidden event
language model. By introducing an additional layer
of tags for performing sentence segmentation and
sentence type prediction, the F-CRF model further
boosts the performance over the L-CRF model. We
perform statistical significance tests using bootstrap
resampling (Efron et al, 1993). The improvements
of F-CRF over L-CRF are statistically significant
(p < 0.01) on Chinese and English texts in the CT
183
BTEC
NO DUPLICATION USE DUPLICATION
SINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRF
LM ORDER 3 5 3 5 3 5 3 5
CN
Prec. 85.96 84.80 86.48 85.12 66.86 68.76 68.00 68.75 92.81 93.82
Rec. 81.87 82.78 83.15 82.78 63.92 66.12 65.38 66.48 85.16 89.01
F1 83.86 83.78 84.78 83.94 65.36 67.41 66.67 67.60 88.83 91.35
EN
Prec. 62.38 59.29 56.86 54.22 85.23 87.29 84.49 81.32 90.67 93.72
Rec. 64.17 60.99 58.76 56.21 88.22 89.65 87.58 84.55 88.22 92.68
F1 63.27 60.13 57.79 55.20 86.70 88.45 86.00 82.90 89.43 93.19
Table 6: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the ASR output of IWSLT08
BTEC evaluation dataset. Percentage scores of precision (Prec.), recall (Rec.), and F1 measure (F1) are reported.
dataset, and on English texts in the BTEC dataset.
The improvements of F-CRF over L-CRF on Chi-
nese texts are smaller, probably because L-CRF
is already performing quite well on Chinese. F1
measures on the CT dataset are lower than those
on BTEC, mainly because the CT dataset consists
of longer utterances and fewer question sentences.
Overall, our proposed F-CRF model is robust and
consistently works well regardless of the language
and dataset it is tested on. This indicates that the
approach is general and relies on minimal linguistic
assumptions, and thus can be readily used on other
languages and datasets.
6.2 Performance on Automatically Recognized
Texts
So far we only evaluated punctuation prediction per-
formance on transcribed texts consisting of correctly
recognized words. We now present the evaluation
results on texts produced by ASR systems.
For evaluation, we use the 1-best ASR outputs of
spontaneous speech of the official IWSLT08 BTEC
evaluation dataset, which is released as part of the
IWSLT09 corpus. The dataset consists of 504 utter-
ances in Chinese, and 498 in English. Unlike the
correctly recognized texts described in Section 6.1,
the ASR outputs contain substantial recognition er-
rors (recognition accuracy is 86% for Chinese, and
80% for English (Paul, 2008)). In the dataset re-
leased by the IWSLT organizers, the correct punctu-
ation symbols are not annotated in the ASR outputs.
To conduct our experimental evaluation, we manu-
ally annotated the correct punctuation symbols on
the ASR outputs.
We used all the learned models in Section 6.1, and
applied them to this dataset. The evaluation results
are shown in Table 6. The results show that F-CRF
still gives higher performance than L-CRF and the
hidden event language model, and the improvements
are statistically significant (p < 0.01).
6.3 Performance in Translation
The evaluation process as described in Section 6.2
requires substantial manual efforts to annotate the
correct punctuation symbols. In this section, we in-
stead adopt an indirect approach to automatically
evaluate the performance of punctuation prediction
on ASR output texts by feeding the punctuated ASR
texts to a state-of-the-art machine translation sys-
tem, and evaluate the resulting translation perfor-
mance. The translation performance is in turn mea-
sured by an automatic evaluation metric which cor-
relates well with human judgments. We believe
that such a task-oriented approach for evaluating the
quality of punctuation prediction for ASR output
texts is useful, since it tells us how well the punc-
tuated ASR output texts from each punctuation pre-
diction system can be used for further processing,
such as in statistical machine translation.
In this paper, we use Moses (Koehn et al, 2007),
a state-of-the-art phrase-based statistical machine
translation toolkit, as our translation engine. We
use the entire IWSLT09 BTEC training set for train-
ing the translation system. The state-of-the-art un-
supervised Berkeley aligner3 (Liang et al, 2006) is
used for aligning the training bitext. We use all
the default settings of Moses, except with the lexi-
calized reordering model enabled. This is because
3http://code.google.com/p/berkeleyaligner/
184
NO DUPLICATION USE DUPLICATION
SINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRF
LM ORDER 3 5 3 5 3 5 3 5
CN? EN 30.77 30.71 30.98 30.64 30.16 30.26 30.33 30.42 31.27 31.30
EN? CN 21.21 21.00 21.16 20.76 23.03 24.04 23.61 23.34 23.44 24.18
Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of BLEU)
lexicalized reordering gives better performance than
simple distance-based reordering (Koehn et al,
2005). Specifically, the default lexicalized reorder-
ing model (msd-bidirectional-fe) is used.
For tuning the parameters of Moses, we use the
official IWSLT05 evaluation set where the correct
punctuation symbols are present. Evaluations are
performed on the ASR outputs of the IWSLT08
BTEC evaluation dataset, with punctuation symbols
inserted by each punctuation prediction method.
The tuning set and evaluation set include 7 reference
translations. Following a common practice in statis-
tical machine translation, we report BLEU-4 scores
(Papineni et al, 2002), which were shown to have
good correlation with human judgments, with the
closest reference length as the effective reference
length. The minimum error rate training (MERT)
(Och, 2003) procedure is used for tuning the model
parameters of the translation system. Due to the un-
stable nature of MERT, we perform 10 runs for each
translation task, with a different random initializa-
tion of parameters in each run, and report the BLEU-
4 scores averaged over 10 runs.
The results are reported in Table 7. The best
translation performances for both translation direc-
tions are achieved by applying F-CRF as the punc-
tuation prediction model to the ASR texts. Such im-
provements are observed to be consistent over dif-
ferent runs. The improvement of F-CRF over L-
CRF in translation quality is statistically significant
(p < 0.05) when translating from English to Chi-
nese. In addition, we also assess the translation
performance when the manually annotated punctu-
ation symbols as mentioned in Section 6.2 are used
for translation. The averaged BLEU scores for the
two translation tasks are 31.58 (Chinese to English)
and 24.16 (English to Chinese) respectively, which
show that our punctuation prediction method gives
competitive performance for spoken language trans-
lation.
It is important to note that in this work, we only
focus on optimizing the punctuation prediction per-
formance in the form of F1-measure, without regard
to the subsequent NLP tasks. How to perform punc-
tuation prediction so as to optimize translation per-
formance is an important research topic that is be-
yond the scope of this paper and needs further in-
vestigation in future work.
7 Conclusion
In this paper, we have proposed a novel approach
for predicting punctuation symbols for transcribed
conversational speech texts. Our proposed approach
is built on top of a dynamic conditional random
fields framework, which jointly performs punctua-
tion prediction together with sentence boundary and
sentence type prediction on speech utterances. Un-
like most previous work, it tackles the task from a
purely text processing perspective and does not rely
on prosodic cues.
Experimental results have shown that our pro-
posed approach outperforms the widely used ap-
proach based on the hidden event language model,
and also outperforms a method based on linear-chain
conditional random fields. Our proposed approach
has been shown to be general, working well on both
Chinese and English, and on both correctly recog-
nized and automatically recognized texts. Our pro-
posed approach also results in better translation ac-
curacy when the punctuated automatically recog-
nized texts are used in subsequent translation.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
185
References
D. Beeferman, A. Berger, and J. Lafferty. 1998.
CYBERPUNC: A lightweight punctuation annotation
system for speech. In Proc. of ICASSP?98.
S.F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL?06.
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctu-
ation annotation using statistical prosody models. In
Proc. of ISCA Workshop on Prosody in Speech Recog-
nition and Understanding.
B. Efron, R. Tibshirani, and R.J. Tibshirani. 1993. An
introduction to the bootstrap. Chapman & Hall/CRC.
B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-
Tur, and M. Ostendorf. 2008. Punctuating speech for
information extraction. In Proc. of ICASSP?08.
A. Gravano, M. Jansche, and M. Bacchiani. 2009.
Restoring punctuation and capitalization in transcribed
speech. In Proc. of ICASSP?09.
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tur, M. Harper, M. Ostendorf, and W. Wang. 2006.
Impact of automatic comma prediction on POS/name
tagging of speech. In Proc. of SLT?06.
J. Huang and G. Zweig. 2002. Maximum entropy model
for punctuation annotation from speech. In Proc. of
ICSLP?02.
J.H. Kim and P.C. Woodland. 2001. The use of prosody
in a combined system for punctuation generation and
speech recognition. In Proc. of EuroSpeech?01.
K. Kirchhoff and M. Yang. 2007. The University of
Washington machine translation system for the IWSLT
2007 competition. In Proc. of IWSLT?07.
P. Koehn, A. Axelrod, A.B. Mayne, C. Callison-Burch,
M. Osborne, and D. Talbot. 2005. Edinburgh sys-
tem description for the 2005 IWSLT speech translation
evaluation. In Proc. of IWSLT?05.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL?07 (Demo Session).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML?01.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT/NAACL?06.
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2005.
Using conditional random fields for sentence boundary
detection in speech. In Proc. of ACL?05.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic
sentence segmentation and punctuation prediction for
spoken language translation. In Proc. of IWSLT?06.
A. McCallum, K. Rohanimanesh, and C. Sutton. 2003.
Dynamic conditional random fields for jointly labeling
multiple sequences. In Proc. of NIPS?03 Workshop on
Syntax, Semantics and Statistics.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL?03.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL?02.
M. Paul. 2008. Overview of the IWSLT 2008 evaluation
campaign. In Proc. of IWSLT?08.
M. Paul. 2009. Overview of the IWSLT 2009 evaluation
campaign. In Proc. of IWSLT?09.
S. Sarawagi and W.W. Cohen. 2005. Semi-Markov con-
ditional random fields for information extraction. In
Proc. of NIPS?05.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of HLT-NAACL?03.
A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf,
D. Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998.
Automatic detection of sentence boundaries and dis-
fluencies based on recognized words. In Proc. of IC-
SLP?98.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP?02.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information ex-
traction. In Proc. of ICML?04 workshop on Statistical
Relational Learning.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. Journal of Machine Learning Research, 8.
C. Sutton. 2006. GRMM: GRaphical Models in Mallet.
http://mallet.cs.umass.edu/grmm/.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter for sighan bakeoff 2005. In Proc. of the Fourth
SIGHAN Workshop on Chinese Language Processing.
M. Wainwright, T. Jaakkola, and A. Willsky. 2001. Tree-
based reparameterization for approximate inference on
loopy graphs. In Proc. of NIPS?01.
H. Wang, H. Wu, X. Hu, Z. Liu, J. Li, D. Ren, and
Z. Niu. 2008. The TCH machine translation system
for IWSLT 2008. In Proc. of IWSLT?08.
186
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611?1622,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Probabilistic Forest-to-String Model for Language Generation from
Typed Lambda Calculus Expressions
Wei Lu and Hwee Tou Ng
Department of Computer Science
School of Computing
National University of Singapore
{luwei,nght}@comp.nus.edu.sg
Abstract
This paper describes a novel probabilistic ap-
proach for generating natural language sen-
tences from their underlying semantics in the
form of typed lambda calculus. The approach
is built on top of a novel reduction-based
weighted synchronous context free grammar
formalism, which facilitates the transforma-
tion process from typed lambda calculus into
natural language sentences. Sentences can
then be generated based on such grammar
rules with a log-linear model. To acquire such
grammar rules automatically in an unsuper-
vised manner, we also propose a novel ap-
proach with a generative model, which maps
from sub-expressions of logical forms to word
sequences in natural language sentences. Ex-
periments on benchmark datasets for both En-
glish and Chinese generation tasks yield sig-
nificant improvements over results obtained
by two state-of-the-art machine translation
models, in terms of both automatic metrics
and human evaluation.
1 Introduction
This work focuses on the task of generating natu-
ral language sentences from their underlying mean-
ing representations in the form of formal logical ex-
pressions (typed lambda calculus). Many early ap-
proaches to generation from logical forms make use
of rule-based methods (Wang, 1980; Shieber et al,
1990), which concern surface realization (ordering
and inflecting of words) but largely ignore lexical ac-
quisition. Recent approaches start to employ corpus-
based probabilistic methods, but many of them as-
sume the underlying meaning representations are of
specific forms such as variable-free tree-structured
representations (Wong and Mooney, 2007a; Lu et
al., 2009) or database entries (Angeli et al, 2010).
While these algorithms usually work well on spe-
cific semantic formalisms, it is unclear how well
they could be applied to a different semantic formal-
ism. In this work, we propose a general probabilis-
tic model that performs generation from underlying
formal semantics in the form of typed lambda calcu-
lus expressions (we refer to them as ?-expressions
throughout this paper), where both lexical acquisi-
tion and surface realization are integrated in a single
framework.
One natural proposal is to adopt a state-of-the-art
statistical machine translation approach. However,
unlike text to text translation, which has been ex-
tensively studied in the machine translation commu-
nity, translating from logical forms into text presents
additional challenges. Specifically, logical forms
such as ?-expressions may have complex internal
structures and variable dependencies across sub-
expressions. Problems arise when performing auto-
matic acquisition of a translation lexicon, as well as
performing lexical selection and surface realization
during generation.
In this work, we tackle these challenges by mak-
ing the following contributions:
? A novel forest-to-string generation algorithm:
Inspired by the work of Chiang (2007), we in-
troduce a novel reduction-based weighted bi-
nary synchronous context-free grammar for-
malism for generation from logical forms (?-
expressions), which can then be integrated with
a probabilistic forest-to-string generation algo-
1611
rithm.
? A novel grammar induction algorithm: To au-
tomatically induce such synchronous grammar
rules, we propose a novel generative model
that establishes phrasal correspondences be-
tween logical sub-expressions and natural lan-
guage word sequences, by extending a previ-
ous model proposed for parsing natural language
into meaning representations (Lu et al, 2008).
To our best knowledge, this is the first probabilis-
tic model for generating sentences from the lambda
calculus encodings of their underlying formal mean-
ing representations, that concerns both surface real-
ization and lexical acquisition. We demonstrate the
effectiveness of our model in Section 5.
2 Related Work
The task of language generation from logical forms
has a long history. Many early works do not rely on
probabilistic approaches. Wang (1980) presented an
approach for generation from an extended predicate
logic formalism using hand-written rules. Shieber
et al (1990) presented a semantic head-driven ap-
proach for generation from logical forms based on
rules written in Prolog. Shemtov (1996) presented a
system for generation of multiple paraphrases from
ambiguous logical forms. Langkilde (2000) pre-
sented a probabilistic model for generation from a
packed forest meaning representation, without con-
cerning lexical acquisition. Specifically, we are not
aware of any prior work that handles both automatic
unsupervised lexical acquisition and surface realiza-
tion for generation from logical forms in a single
framework.
Another line of research efforts focused on the
task of language generation from other meaning rep-
resentation formalisms. Wong and Mooney (2007a)
as well as Chen and Mooney (2008) made use
of synchronous grammars to transform a variable-
free tree-structured meaning representation into sen-
tences. Lu et al (2009) presented a language gener-
ation model using the same meaning representation
based on tree conditional random fields. Angeli et
al. (2010) presented a domain-independent proba-
bilistic approach for generation from database en-
tries. All these models are probabilistic models.
Recently there are also substantial research efforts
on the task of mapping natural language to meaning
representations in various formalisms ? the inverse
task of language generation called semantic parsing.
Examples include Zettlemoyer and Collins (2005;
2007; 2009), Kate and Mooney (2006), Wong and
Mooney (2007b), Lu et al (2008), Ge and Mooney
(2009), as well as Kwiatkowski et al (2010).
Of particular interest is our prior work Lu et al
(2008), in which we presented a joint generative pro-
cess that produces a hybrid tree structure containing
words, syntactic structures, and meaning represen-
tations, where the meaning representations are in a
variable-free tree-structured form. One important
property of the model in our prior work is that it
induces a hybrid tree structure automatically in an
unsupervised manner, which reveals the correspon-
dences between natural language word sequences
and semantic elements. We extend our prior model
in the next section, so as to support ?-expressions.
The model in turn serves as the basis for inducing
the synchronous grammar rules later.
3 ?-Hybrid Tree
In Lu et al (2008), a generative model was pre-
sented to model the process that jointly generates
both natural language sentences and their underly-
ing meaning representations of a variable-free tree-
structured form. The model was defined over a
hybrid tree, which consists of meaning representa-
tion tokens as internal nodes and natural language
words as leaves. One limitation of the hybrid tree
model is that it assumes a single fixed tree struc-
ture for the meaning representation. However, ?-
expressions exhibit complex structures and variable
dependencies, and thus it is not obvious how to rep-
resent them in a single tree structure.
In this section, we present a novel ?-hybrid tree
model that provides the following extensions over
the model of Lu et al (2008):
1. The internal nodes of a meaning representation
tree involve ?-expressions which are not neces-
sarily of variable-free form;
2. The meaning representation has a packed forest
representation, rather than a single determinis-
tic tree structure.
3.1 Packed ?-Meaning Forest
We represent a ?-expression with a packed forest of
meaning representation trees (called ?-meaning for-
1612
est). Multiple different meaning representation trees
(called ?-meaning trees) can be extracted from the
same ?-meaning forest, but they all convey equiva-
lent semantics via reductions, as discussed next.
Constructing a ?-meaning forest for a given ?-
expression requires decomposition of a complete ?-
expression into semantically complete and syntacti-
cally correct sub-expressions in a principled man-
ner. This can be achieved with a process called
higher order unification (Huet, 1975). The process
was known to be very complex and was shown to be
undecidable in unrestricted form (Huet, 1973). Re-
cently a restricted form of higher order unification
was applied to a semantic parsing task (Kwiatkowski
et al, 2010). In this work, we employ a similar tech-
nique for building the ?-meaning forest.
For a given ?-expression e, our algorithm finds ei-
ther two expressions h and f such that (h f) ? e, or
three expressions h, f , and g such that ((h f) g) ?
e, where the symbol? is interpreted as ?-equivalent
after reductions1 (Barendregt, 1985). We then build
the ?-meaning forest based on the expressions h, f ,
and g. In practice, we develop a BUILDFOREST(e)
procedure which recursively builds ?-forests by ap-
plying restricted higher-order unification rules on
top of the ?-expression e. Each node of the ?-forest
is called a ?-production, to which we will give more
details in Section 3.2. For example, once a candi-
date triple (h, f, g) as in ((h f) g) ? e has been
identified, the procedure creates a ?-forest with the
root node being a ?-production involving h, and two
sets of child ?-forests given by BUILDFOREST(f)
and BUILDFOREST(g) respectively. For restricted
higher-order unification, besides the similar assump-
tions made by Kwiatkowski et al (2010), we also
impose one additional assumption: limited free vari-
able, which states that the expression hmust contain
no more than one free variable. Note that this pro-
cess provides a semantically equivalent packed for-
est representation of the original ?-expression, with-
out altering its semantics in any way.
For better readability, we introduce the symbol
 as an alternative notation for functional appli-
cation. In other words, h  f refers to (h f) or
h(f), and h  f  g refers to ((h f) g). For ex-
1In this work, for reductions, we consider ?-conversions
(changing bound variables) and ?-conversions (applying func-
tors to their arguments).
ample, the expression ?x.state(x)? loc(boston, x)
can be represented as the functional application form
of [?f.?x.f(x) ? loc(boston, x)] ?x.state(x).2
Such a packed forest representation contains ex-
ponentially many tree structures which all convey
the same semantics. We believe such a semantic
representation is more advantageous than the sin-
gle fixed tree-structured representation. In fact, one
could intuitively regard a different decomposition
path as a different way of interpreting the same se-
mantics. Thus, such a representation could poten-
tially accommodate a wider range of natural lan-
guage expressions, which all share the same seman-
tics but with very different word choices, phrase or-
derings, and syntactic structures (like paraphrases).
It may also alleviate the non-isomorphism issue that
was commonly faced by researchers when mapping
meaning representations and sentences (Wong and
Mooney, 2007b). We will validate our belief later
through experiments.
3.2 The Joint Generative Process
. . .
?a : pia  ?b  ?c
w1 ?b : pib  ?d
. . . w4
w2 ?c : pic
w5
w3
Figure 1: The joint generative process of both ?-meaning tree
and its corresponding natural language sentence, which results
in a ?-hybrid tree.
The generative process for a sentence together
with its corresponding ?-meaning tree is illustrated
in Figure 1, which results in a ?-hybrid tree. Internal
nodes of a ?-hybrid tree are called ?-productions,
which are building blocks of a ?-forest. Each
?-production in turn has at most two child ?-
productions. A ?-production has the form ?a : pia 
?b, where ?a is the expected type3 after type evalu-
ation of the terms to its right, pia is a ?-expression
(serves as the functor), and ?b are types of the child
?-productions (as the arguments). The leave nodes
2Throughout this paper, we abuse this notation a bit by al-
lowing the arguments to be types rather than actual expressions,
such as ?y.?x.loc(y, x))  e, which indicates that the functor
?y.?x.loc(y, x) expects an expression of type e to serve as its
argument.
3This work considers basic types: e (entities) and t (truth
values). It also allows function types, e.g., ?e, t? is the type
assigned to functions that map from entities to truth values.
1613
r : ?e, t? 1
?e, t? 1 : ?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2
?e, t? 2 : ?f.?g.?x.?y.g(y) ? (f(x) y) ?e, ?e, t?? 1  ?e, t? 2
?e, t? 2 : ?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2
?e, t? 1 : ?y.?x.loc(y, x) e 1
runs throughe 1 : miss r
the mississippi
that
?e, t? 2 : ?x.state(x)
states
?e, ?e, t?? 1 : ?y.?x.next to(x, y)
bordering
?e, t? 1 : ?x.state(x)
the states
give me
Figure 2: One example ?-hybrid tree for the sentence ?give me the states bordering states that the mississippi runs through? together
with its logical form ??x0.state(x0) ? ?x1.[loc(miss r, x1) ? state(x1) ? next to(x1, x0)]?.
w are contiguous word sequences. The model re-
peatedly generates ?-hybrid sequences, which con-
sist of words intermixed with ?-productions, from
each ?-production at different levels.
Consider part of the example ?-hybrid tree in Fig-
ure 2. The probability associated with generation of
the subtree that spans the sub-sentence ?that the mis-
sissippi runs through? can be written as:
P
(
?x.loc(miss r, x), that the mississippi runs through
)
= ?(m? wYw|p1)? ?(that e 1 runs through|p1)
??(p2|p1, arg1)? ?(m? w|p2)? ?(the mississippi|p2)
where p1 = ?e, t? : ?y.?x.loc(y, x) e 1 , and p2 =
e : miss r.
Following the work of Lu et al (2008), the gener-
ative process involves three types of parameters ?? =
{?, ?, ?}: 1) pattern parameters ?, which model in
what way the words and child ?-productions are in-
termixed; 2) emission parameters ?, which model
the generation process of words from ?-productions,
where either a unigram or a bigram assumption can
be made (Lu et al, 2008); and 3) meaning repre-
sentation (MR) model parameters ?, which model
the generation process from one ?-production to its
child ?-productions. An analogous inside-outside
algorithm (Baker, 1979) used there is employed
here. Since we allow a packed ?-meaning forest rep-
resentation rather than a fixed tree structure, the MR
model parameters ? in this work should be estimated
with the inside-outside algorithm as well, rather than
being estimated directly from the training data by
simple counting, as was done in Lu et al (2008).
4 The Language Generation Algorithm
Now we present the algorithm for language gener-
ation. We introduce the grammar first, followed by
the features we use. Next, we present the method for
grammar induction, and then discuss the decoder.
4.1 The Grammar
We use a weighted synchronous context free gram-
mar (SCFG) (Aho and Ullman, 1969), which was
previously used in Chiang (2007) for hierarchical
phrase-based machine translation. The grammar is
defined as follows:
? ? ?p? , hw,?? (1)
where ? is the type associated with the ?-production
p?4, and hw is a sequence consisting of natural lan-
guage words intermixed with types. The symbol
? denotes the one-to-one correspondence between
nonterminal occurrences (i.e., in this case types of
?-expressions) in both p? and hw.
We allow a maximum of two nonterminal sym-
bols in each synchronous rule, as was also assumed
in Chiang (2007), which makes the grammar a bi-
nary SCFG. Two example rules are:
?e, t? ?
?
?y.?x.loc(y, x) e 1 , that e 1 runs through
?
e ?
?
miss r, the mississippi
?
where the boxed indices give the correspondences
between nonterminals.
A derivation with the above two synchronous
rules results in the following ?-expression paired
with its natural language counterpart:
4Since type is already indicated by ? , we avoid redundancy
by omitting it when writing p?, without loss of information.
1614
Type 1: ?e, ?e, t?? ?
?
?y.?x.next to(x, y) , bordering
?
?e, t? ?
?
?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2 , ?e, t? 2 ?e, t? 1
?
Type 2: ?e, t? ?
?
?x.loc(miss r, x) ? state(x) , states that the mississippi runs through
?
?e, t? ?
?
?x.loc(miss r, x) , that the mississippi runs through
?
Type 3: ?e, t? ?
?
?f.?x.state(x) ? ?y.[f(y) ? next to(y, x)] ?e, t? 1 , the states bordering ?e, t? 1
?
?e, t? ?
?
?y.?x.loc(y, x) ? state(x) e 1 , states that e 1 runs through
?
Figure 3: Example synchronous rules that can be extracted from the ?-hybrid tree of Figure 2.
?e, t??
?
?x.loc(miss r, x) , that the mississippi runs through
?
where the source side ?-expression is constructed
from the application ?y.?x.loc(y, x) miss r fol-
lowed by a reduction (?-conversion). Assuming the
?-expression to be translated is ?x.loc(miss r, x),
the above rule in fact gives one candidate translation
?that the mississippi runs through?.
4.2 Features
Following the work of Chiang (2007), we assign
scores to derivations with a log-linear model, which
are essentially weighted products of feature values.
For generality, we only consider the following
four simple features in this work:
1. p?(hw|p?): the relative frequency estimate of a
hybrid sequence hw given the ?-production p?;
2. p?(p?|hw, ?): the relative frequency estimate of
a ?-production p? given the phrase hw and the
type ? ;
3. exp(?wc(hw)): the number of words gener-
ated, where wc(hw) refers to the number of
words in hw (i.e., word penalty); and
4. pLM (s?): the language model score of the gen-
erated sentence s?.
The first three features, which are also widely
used in state-of-the-art machine translation models
(Koehn et al, 2003; Chiang, 2007), are rule-specific
and thus can be computed before decoding. The last
feature is computed during the decoding phase in
combination with the sibling rules used.
We score a derivation D with a log-linear model:
w(D) =
(?
r?D
?
i
fi(r)wi
)
? pLM (s?)wLM (2)
where r ? D refers to a rule r that appears in
the derivation D, s? is the target side (sentence) as-
sociated with the derivation D, and fi is a rule-
specific feature (one of features 1?3 above) which
is weighted with wi. The language model feature is
weighted with wLM .
Once the feature values are computed, our goal is
to find the optimal weight vector w?? that maximizes
a certain evaluation metric when used for decoding,
as we will discuss in Section 4.4.
Following popular approaches to learning feature
weights in the machine translation community (Och
and Ney, 2004; Chiang, 2005), we use the minimum
error rate training (MERT) (Och, 2003) algorithm to
learn the feature weights that directly optimize cer-
tain automatic evaluation metric. Specifically, the
Z-MERT (Zaidan, 2009) implementation of the al-
gorithm is used in this work.
4.3 Grammar Induction
Automatic induction of the grammar rules as de-
scribed above from training data (which consists
of pairs of ?-expressions and natural language sen-
tences) is a challenging task. Current state-of-the-
art string-based translation systems (Koehn et al,
2003; Chiang, 2005; Galley and Manning, 2010)
typically begin with a word-aligned corpus to con-
struct phrasal correspondences. Word-alignment in-
formation can be estimated from alignment models,
such as the IBM alignment models (Brown et al,
1993) and HMM-based alignment models (Vogel et
al., 1996; Liang et al, 2006). However, unlike texts,
logical forms have complex internal structures and
variable dependencies across sub-expressions. It is
not obvious how to establish alignments between
logical terms and texts with such alignment models.
Fortunately, the generative model for ?-hybrid
tree introduced in Section 3 explicitly models the
mappings from ?-sub-expressions to (possibly dis-
contiguous) word sequences with a joint genera-
tive process. This motivates us to extract grammar
rules from the ?-hybrid trees. Thus, we first find
the Viterbi ?-hybrid trees for all training instances,
1615
Tree fragment :
?e, t? 2 : ?g.?f.?x.g(x) ? f(x) ?e, t? 1  ?e, t? 2
?e, t? 1 : ?y.?x.loc(y, x) e 1
runs throughe 1 : . . .that
?e, t? 2 : ?x.state(x)
states
Source : (substitution) ?y?.
[
?g.?f.?x.g(x) ? f(x) [?y.?x.loc(y, x) y?] ?x.state(x)
]
 e 1
(two ?-conversions)? ?y?.[?f.?x.loc(y?, x) ? f(x) ?x.state(x)] e 1
(?-conversion)? ?y?.?x.loc(y?, x) ? state(x) e 1
(?-conversion)? ?y.?x.loc(y, x) ? state(x) e 1
Target : ?states that e 1 runs through?
Rule : ?e, t? ?
?
?y.?x.loc(y, x) ? state(x) e 1 , states that e 1 runs through
?
Figure 4: Construction of a two-level ?-hybrid sequence rule via substitution and reductions from a tree fragment. Note that the
subtree rooted by e 1 : miss r gets ?abstracted? by its type e. The auxiliary variable y? of type e is thus introduced to facilitate the
construction process.
based on the learned parameters of the generative ?-
hybrid tree model.
Next, we extract grammar rules on top of these
?-hybrid trees. Specifically, we extract the follow-
ing three types of synchronous grammar rules, with
examples given in Figure 3:
1. ?-hybrid sequence rules: They are the conven-
tional rules constructed from one ?-production
and its corresponding ?-hybrid sequence.
2. Subtree rules: These rules are constructed from
a complete subtree of the ?-hybrid tree. Each
rule provides a mapping between a complete
sub-expression and a contiguous sub-sentence.
3. Two-level ?-hybrid sequence rules: These rules
are constructed from a tree fragment with one
of its grandchild subtrees (the subtree rooted by
one of its grandchild nodes) being abstracted
with its type only. These rules are constructed
via substitution and reductions.
Figure 4 gives an example based on a tree frag-
ment of the ?-hybrid tree in Figure 2. Note that
the first step makes use of the auxiliary vari-
able y? of type e to represent the grandchild
subtree. ?y? is introduced so as to allow any
?-expression of type e serving as this expres-
sion?s argument to replace y?. In fact, if the
semantics conveyed by the grandchild subtree
serves as its argument, we will obtain the exact
complete semantics of the current subtree. As
we can see, the resulting rule is more general,
and is able to capture longer structural depen-
dencies. Such rules are thus potentially more
useful.
The overall algorithm for learning the grammar
rules is sketched in Figure 5.
4.4 Decoding
Our goal in decoding is to find the most probable
sentence s? for a given ?-expression e:
s? = s
(
arg max
D s.t. e(D)?e
w(D)
)
(3)
where e(D) refers to the source side (?-expression)
of the derivation D, and s(D) refers to the target
side (natural language sentence) of D.
A conventional CKY-style decoder as used by
Chiang (2007) is not applicable to this work since
the source side does not exhibit a linear structure.
As discussed in Section 3.1, ?-expressions are rep-
resented as packed ?-meaning forests. Thus, in
this work, we make use of a bottom-up dynamic
programming chart-parsing algorithm that works di-
rectly on translating forest nodes into target natural
language words. The algorithm is similar to that of
Langkilde (2000) for generation from an underly-
ing packed semantic forest. Language models are
incorporated when scoring the n-best candidates at
each forest node, where the cube-pruning algorithm
of Chiang (2007) is used. In order to accommodate
type 2 and type 3 rules as discussed in Section 4.3,
whose source side ?-productions are not present in
the nodes of the original ?-meaning forest, new ?-
productions are created (via substitution and reduc-
tions) and attached to the original ?-meaning forest.
1616
Procedures
? f ? BUILDFOREST(e)
It takes in a ?-expression e and outputs its ?-
meaning forest f . (Sec. 3.1)
? ?? ? TRAINGENMODEL(f, s)
It takes in ?-meaning forest-sentence pairs (f, s),
performs EM training of the generative model, and
outputs the parameters ??. (Sec. 3.2)
? h? FINDHYBRIDTREE(f, s, ??)
It finds the most probable ?-hybrid tree h contain-
ing the given f -s pair, under the generative model
parameters ??. (Sec. 4.3)
? ?h ? EXTRACTRULES(h)
It takes in a ?-hybrid tree h, and extracts a set of
grammar rules ?h out of it. (Sec. 4.3)
Algorithm
1. Inputs and initializations:
? A training set (e, s), an empty rule set ? = ?
2. Learn the grammar:
? For each ei ? e, find its ?-meaning forest:
fi = BUILDFOREST(ei). This gives the set
(f, s).
? Learn the generative model parameter :
??? = TRAINGENMODEL(f, s).
? For each (fi, si) ? (f, s), find the most proba-
ble ?-hybrid tree hi, and then extract the gram-
mar rules from it:
hi = FINDHYBRIDTREE(fi, si, ???)
? = ? ? EXTRACTRULES(hi)
3. Output the learned grammar rule set ?.
Figure 5: The algorithm for learning the grammar rules
5 Experiments
For experiments, we evaluated on the GEOQUERY
dataset, which consists of 880 queries on U.S. geog-
raphy. The dataset was manually labeled with ?-
expressions as their semantics in Zettlemoyer and
Collins (2005). It was used in many previous re-
search efforts on semantic parsing (Zettlemoyer and
Collins, 2005; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2007; Kwiatkowski et al, 2010).
The original dataset was annotated with English sen-
tences only. In order to assess the generation per-
formance across different languages, in our work
the entire dataset was also manually annotated with
Chinese by a native Chinese speaker with linguistics
background5.
For all the experiments we present in this sec-
tion, we use the same split as that of Kwiatkowski
5The annotator created annotations with both ?-expressions
and corresponding English sentences available as references.
et al (2010), where 280 instances are used for test-
ing, and the remaining instances are used for learn-
ing. We further split the learning set into two por-
tions, where 500 instances are used for training the
models, which includes induction of grammar rules,
training a language model, and computing feature
values, and the remaining 100 instances are used for
tuning the feature weights.
As we have mentioned earlier, we are not aware
of any previous work that performs generation from
formal logical forms that concerns both lexical ac-
quisition and surface realization. The recent work
by Angeli et al (2010) presented a generation sys-
tem from database records with an additional focus
on content selection (selection of records and their
subfields for generation). It is not obvious how to
adopt their algorithm in our context where content
selection is not required but the more complex log-
ical semantic representation is used as input. Other
earlier approaches such as the work of Wang (1980)
and Shieber et al (1990) made use of rule-based
approaches without automatic lexical acquisition.
We thus compare our system against two state-
of-the-art machine translation systems: a phrase-
based translation system, implemented in the Moses
toolkit (Koehn et al, 2007)6, and a hierarchical
phrase-based translation system, implemented in the
Joshua toolkit (Li et al, 2009), which is a reim-
plementation of the original Hiero system (Chiang,
2005; Chiang, 2007). The state-of-the-art unsuper-
vised Berkeley aligner (Liang et al, 2006) with de-
fault setting is used to construct word alignments.
We train a trigram language model with modified
Kneser-Ney smoothing (Chen and Goodman, 1996)
from the training dataset using the SRILM toolkit
(Stolcke, 2002), and use the same language model
for all three systems. We use an n-best list of size
100 for all three systems when performing MERT.
5.1 Automatic Evaluation
For automatic evaluation, we measure the original
IBM BLEU score (Papineni et al, 2002) (4-gram
precision with brevity penalty) and the TER score
(Snover et al, 2006) (the amount of edits required
to change a system output into the reference)7. Note
that TER measures the translation error rate, thus a
6We used the default settings, and enabled the default lexi-
calized reordering model, which yielded better performance.
7We used tercom version 0.7.25 with the default settings.
1617
smaller score indicates a better result. For clarity,
we report 1?TER scores. Following the tuning pro-
cedure as conducted in Galley and Manning (2010),
we perform MERT using BLEU as the metric.
We compare our model against state-of-the-art
statistical machine translation systems. As a base-
line, we first conduct an experiment with the fol-
lowing naive approach: we treat the ?-expressions
as plain texts. All the bound variables (e.g., x
in ?x.state(x)) which do not convey semantics
are removed, but free variables (e.g., state in
?x.state(x)) which might convey semantics are left
intact. Quantifiers and logical connectives are also
left intact. While this naive approach might not ap-
pear very sensible, we merely want to treat it as our
simplest baseline.
Alternatively, analogous to the work of Wong
and Mooney (2007a), we could first parse the ?-
expressions into binary tree structures with a deter-
ministic procedure, and then linearize the tree struc-
ture as a sequence. Since there exists different ways
to linearize a binary tree, we consider preorder, in-
order, and postorder traversal of the trees, and lin-
earize them in these three different ways.
As for our system, during the grammar learning
phase, we initialize the generative model parame-
ters with output from the IBM alignment model 1
(Brown et al, 1993)8, and run the ?-hybrid tree gen-
erative model with the unigram emission assumption
for 10 iterations, followed by another 10 iterations
with the bigram assumption. Grammar rules are then
extracted based on the ?-hybrid trees obtained from
such learned generative model parameters.
Since MERT is prone to search errors, we run each
experiment 5 times with randomly initialized fea-
ture weights, and report the averaged scores. Ex-
perimental results for both English and Chinese are
presented in Table 1. As we can observe, the way
that a meaning representation tree is linearized has
a significant impact on the translation performance.
Interestingly, for both Moses and Joshua, the pre-
order setting yields the best performance for En-
glish, whereas it is inorder that yields the best per-
formance for Chinese. This is perhaps due to the
fact that Chinese presents a very different syntactic
structure and word ordering from English.
8We assume word unigrams are generated from free vari-
ables, quantifiers, and logical connectives in IBM model 1.
Our system, on the other hand, employs a packed
forest representation for ?-expressions. Therefore,
it eliminates the ordering constraint by encompass-
ing exponentially many possible tree structures dur-
ing both the alignment and decoding stage. As a
result, our system obtains significant improvements
in both BLEU and 1?TER using the significance
test under the paired bootstrap resampling method
of Koehn (2004). We obtain p < 0.01 for all cases,
except when comparing against Joshua-preorder for
English, where we obtain p < 0.05 for both metrics.
English Chinese
BLEU 1?TER BLEU 1?TER
Moses
text 48.93 61.08 43.23 51.71
preorder 51.13 63.73 42.08 50.43
inorder 46.72 57.59 48.03 55.29
postorder 44.30 55.05 46.36 54.59
Joshua
text 37.40 48.97 36.60 46.20
preorder 51.40 64.69 40.05 49.70
inorder 40.31 50.47 48.32 54.64
postorder 31.10 42.44 41.31 49.71
This work (t) 54.58 67.65 55.11 63.77
(t) w/o type 2 rules 53.77 66.43 54.30 62.49
(t) w/o type 3 rules 53.68 66.17 50.96 60.13
Table 1: Performance on generating English and Chinese from
?-expressions with automatic evaluation metrics (we report per-
centage scores).
5.2 Human Evaluation
We also conducted human evaluation with 5 eval-
uators each on English and Chinese. We randomly
selected about 50% (139) test instances and obtained
output sentences from the three systems. Moses and
Joshua were run with the top-performing settings in
terms of automatic metrics (i.e., preorder for En-
glish and inorder for Chinese). Following Angeli
et al (2010), evaluators are instructed to give scores
based on language fluency and semantic correctness,
on the following scale:
Score Language Fluency Semantic Correctness
5 Flawless Perfect
4 Good Near Perfect
3 Non-native Minor Errors
2 Disfluent Major Errors
1 Gibberish Completely Wrong
For each test instance, we first randomly shuffled
the output sentences of the three systems, and pre-
sented them together with the correct reference to
the evaluators. The evaluators were then asked to
score all the output sentences at once. This eval-
uation process not only ensures that the annotators
have no access to which system generated the out-
1618
English Judge E1 Judge E2 Judge E3 Judge E4 Judge E5 AverageFLU SEM FLU SEM FLU SEM FLU SEM FLU SEM FLU SEM
Moses preorder 4.56 4.57 4.58 4.54 4.52 4.52 4.48 4.14 4.28 4.22 4.48 ? 0.12 4.40 ? 0.20
Joshua preorder 4.50 4.43 4.49 4.29 4.44 4.36 4.46 4.04 4.12 4.06 4.40 ? 0.16 4.24 ? 0.18
This work 4.76 4.73 4.73 4.70 4.68 4.60 4.64 4.37 4.49 4.44 4.66 ? 0.10 4.57 ? 0.16
Chinese Judge C1 Judge C2 Judge C3 Judge C4 Judge C5 AverageFLU SEM FLU SEM FLU SEM FLU SEM FLU SEM FLU SEM
Moses inorder 4.38 4.22 3.95 3.99 4.01 3.80 4.27 4.19 4.09 4.01 4.14 ? 0.18 4.04 ? 0.17
Joshua inorder 4.32 4.04 3.74 3.91 3.76 3.55 4.21 4.04 3.96 3.97 4.00 ? 0.26 3.90 ? 0.21
This work 4.61 4.47 4.53 4.43 4.50 4.31 4.71 4.55 4.57 4.32 4.59 ? 0.08 4.42 ? 0.10
Table 2: Human evaluation results on English and Chinese generation. FLU: language fluency; SEM: semantic correctness.
put, but also minimizes bias associated with scor-
ing different outputs for the same input. The de-
tailed and averaged results (with one standard devi-
ation) for human evaluation are presented in Table 2
for English and Chinese respectively. For both lan-
guages, our system achieves a significant improve-
ment over Moses and Joshua (p < 0.01 with paired
t-tests), in terms of both language fluency and se-
mantic correctness. This set of results is important,
as it demonstrates that our system produces more
fluent texts with more accurate semantics when per-
ceived by real humans.
5.3 Additional Experiments
We also performed the following additional experi-
ments. First, we attempted to increase the number
of EM iterations (to 100) when training the model
with the bigram assumption, so as to assess the ef-
fect of the number of EM iterations on the final gen-
eration performance. We observed similar perfor-
mance. Second, in order to assess the importance of
the two types of novel rules ? subtree rules (type 2)
and two-level ?-hybrid sequence rules (type 3), we
also conducted experiments without these rules for
generation. Experiments show that these two types
of rules are important. Specifically, type 3 rules,
which are able to capture longer structural depen-
dencies, are of particular importance for generating
Chinese. Detailed results for these additional exper-
iments are presented in Table 1.
5.4 Experiments on Variable-free Meaning
Representations
Finally, we also assess the effectiveness of our
model on an alternative meaning representation for-
malism in the form of variable-free tree structures.
Specifically, we tested on the ROBOCUP dataset
(Kuhlmann et al, 2004), which consists of 300
English instructions for coaching robots for soc-
cer games, and a variable-free version of the GEO-
QUERY dataset. These are the standard datasets
used in the generation tasks of Wong and Mooney
(2007a) and Lu et al (2009). Similar to the tech-
nique introduced in Kwiatkowski et al (2010), our
proposed algorithm could still be applied to such
datasets by writing the tree-structured representa-
tions as function-arguments forms. The higher order
unification-based decomposition algorithm could be
applied on top of such forms accordingly. For exam-
ple, midfield(opp) ? ?x.midfield(x)  opp. See
Kwiatkowski et al (2010) for more details. How-
ever, since such forms present monotonous struc-
tures, and thus give less alternative options in the
higher-order unification-based decomposition pro-
cess, it prevents the algorithm from creating many
disjunctive nodes in the packed forest. It is thus hy-
pothesized that the advantages of the packed forest
representation could not be fully exploited with such
a meaning representation formalism.
Following previous works, we performed 4 runs
of 10-fold cross validation based on the same split
as that of Wong and Mooney (2007a) and Lu et
al. (2009), and measured standard BLEU percent-
age and NIST (Doddington, 2002) scores. For ex-
perimentation on each fold, we trained a trigram
language model on the training data of that fold,
and randomly selected 70% of the training data
for grammar induction, with the remaining 30%
for learning of the feature weights using MERT.
Next, we performed grammar induction with the
complete training data of that fold, and used the
learned feature weights for decoding of the test in-
stances. The averaged results are shown in Ta-
ble 3. Our approach outperforms the previous sys-
tem WASP?1++ (Wong and Mooney, 2007a) sig-
nificantly, and achieves comparable or slightly bet-
ter performance as compared to Lu et al (2009).
This set of results is particularly striking. We note
1619
Variable-present dataset
?-expression : argmax(x, river(x) ? ?y.[state(y) ? next to(y, india s) ? loc(x, y)], len(x))
Reference : what is the longest river that flows through a state that borders indiana
Moses : what is the states that border long indiana
Joshua : what is the longest river surrounding states border indiana
This work : what is the longest river in the states that border indiana
?-expression : density(?x.loc(argmax(y, loc(y, usa co) ? river(y), size(y)), x) ? state(x))
Reference : which is the density of the state that the largest river in the united states runs through
Moses : what is the population density in lie on the state with the smallest state in the us
Joshua : what is the population density of states lie on the smallest state in the us
This work : what is the population density of the state with the largest river in the us
Variable-free datasets
?-expression : population(largest one density(state all))
Reference : what is the population of the state with the highest population density
This work : how many people live in the state with the largest population density
?-expression : rule(and(bpos(from goal line(our, jnum(n0.0, n32.0))), not(bpos(left(penalty area(our))))),-
dont(player our(n3), intercept))
Reference : player 3 should not intercept the ball if the ball is within 32 meters of our goal line and not in our left penalty area
This work : if the ball is within 32 meters from our goal line and not on the left side of our penalty area then player 3 should not
intercept it
Figure 6: Sample English outputs for various datasets. For the variable-present dataset, we also show outputs from Moses and
Joshua.
that the algorithm of Lu et al (2009) is capable
of modeling dependencies over phrases, which gives
global optimization over the sentence generated, and
works by building conditional random fields (Laf-
ferty et al, 2001) over trees. But the algorithm of
Lu et al (2009) is also limited to handling tree-
structured meaning representation, and is therefore
unable to accept inputs such as the variable ver-
sion of ?-expressions. Our algorithm works well
by introducing additional new types of synchronous
rules that are able to capture longer range depen-
dencies. WASP?1++, on the other hand, also makes
use of a synchronous parsing-based statistical ma-
chine translation approach. Their system, however,
requires linearization of the tree structure for both
alignment and translation. In contrast, our model
directly performs alignment and translation from a
packed forest representation to a sentence. As a
result, though WASP?1++ made use of additional
features (lexical weights), our system yielded bet-
ter performance. Sample English output sentences
are given in Figure 6.
Robocup Geoquery
BLEU NIST BLEU NIST
WASP?1++ 60.22 6.8976 53.70 6.4808
Lu et al (2009) 62.20 6.9845 57.33 6.7459
This work 62.45 7.0011 57.62 6.6867
Table 3: Performance on variable-free representations
6 Conclusions and Future Work
In this work, we presented a novel algorithm for gen-
erating natural language sentences from their under-
lying semantics in the form of typed lambda calcu-
lus. We tackled the problem by introducing a novel
reduction-based weighted synchronous context-free
grammar formalism, which allows sentence genera-
tion with a log-linear model. In addition, we pro-
posed a novel generative model that jointly gener-
ates lambda calculus expressions and natural lan-
guage sentences. The model is then used for auto-
matic grammar induction. Empirical results show
that our model outperforms state-of-the-art machine
translation models, for both English and Chinese,
in terms of both automatic and human evaluation.
Furthermore, we have demonstrated that the model
can also effectively handle inputs with a variable-
free version of meaning representation.
We believe the algorithm used for inducing the
reduction-based synchronous grammar rules may
find applications in other research problems, such
as statistical machine translation and phrasal syn-
chronous grammar induction. We are interested in
exploring further along such directions in the future.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore. We would like to thank Tom
Kwiatkowski and Luke Zettlemoyer for sharing their
dataset, and Omar F. Zaidan for his help with Z-
MERT.
1620
References
A.V. Aho and J.D. Ullman. 1969. Syntax directed trans-
lations and the pushdown assembler. Journal of Com-
puter and System Sciences, 3(1):37?56.
G. Angeli, P. Liang, and D. Klein. 2010. A simple
domain-independent probabilistic approach to gener-
ation. In Proc. EMNLP, pages 502?512.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. The Journal of the Acoustical Society of Amer-
ica, 65:S132.
H. P. Barendregt. 1985. The Lambda Calculus, Its Syntax
and Semantics (Studies in Logic and the Foundations
of Mathematics, Volume 103). Revised Edition. North-
Holland.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19:263?311.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
ACL, pages 310?318.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: a test of grounded language acquisition. In
Proc. ICML, pages 128?135.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL, pages
263?270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33:201?228.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. HLT, pages 138?145.
M. Galley and C. D. Manning. 2010. Accurate
non-hierarchical phrase-based translation. In Proc.
HLT/NAACL, pages 966?974.
R. Ge and R. J. Mooney. 2009. Learning a compositional
semantic parser using an existing syntactic parser. In
Proc. ACL/IJCNLP, pages 611?619.
G. P. Huet. 1973. The undecidability of unification in
third order logic. Information and Control, 22(3):257?
267.
G. P. Huet. 1975. A unification algorithm for typed ?-
calculus. Theoretical Computer Science, 1(1):27?57.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. COLING/ACL,
pages 913?920.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. NAACL/HLT, pages
48?54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proc. ACL (Demon-
stration Sessions), pages 177?180.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. EMNLP, pages 388?
395.
G. Kuhlmann, P. Stone, R. Mooney, and J. Shavlik. 2004.
Guiding a reinforcement learner with natural language
advice: Initial results in RoboCup soccer. In Proc.
AAAI Workshop on Supervisory Control of Learning
and Adaptive Systems.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Proc. EMNLP, pages 1223?1233.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc.
ICML, pages 282?289.
I. Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proc. NAACL, pages 170?177.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch,
S. Khudanpur, L. Schwartz, W. N. G. Thornton,
J. Weese, and O. F. Zaidan. 2009. Joshua: an open
source toolkit for parsing-based machine translation.
In Proc. WMT, pages 135?139.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. HLT/NAACL, pages 104?111.
W. Lu, H. T. Ng, W. S. Lee, and L. Zettlemoyer. 2008.
A generative model for parsing natural language to
meaning representations. In Proc. EMNLP, pages
783?792.
W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural lan-
guage generation with tree conditional random fields.
In Proc. EMNLP, pages 400?409.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311?318.
H. Shemtov. 1996. Generation of paraphrases from am-
biguous logical forms. In Proc. COLING, pages 919?
924.
S. M. Shieber, G. van Noord, F. C. N. Pereira, and
R. C. Moore. 1990. Semantic-head-driven generation.
Computational Linguistics, 16(1):30?42.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA, pages
223?231.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Proc. ICSLP, pages 901?904.
1621
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc.
COLING, pages 836?841.
J. Wang. 1980. On computational sentence generation
from logical form. In Proc. COLING, pages 405?411.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
Proc. HLT/NAACL, pages 439?446.
Y. W. Wong and R. J. Mooney. 2007a. Generation by in-
verting a semantic parser that uses statistical machine
translation. In Proc. NAACL/HLT, pages 172?179.
Y. W. Wong and R. J. Mooney. 2007b. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proc. ACL, pages 960?967.
O. F. Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. The Prague Bulletin of Mathe-
matical Linguistics, 91:79?88.
L. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Proc.
UAI, pages 658?666.
L. Zettlemoyer and M. Collins. 2007. Online learning of
relaxed CCG grammars for parsing to logical form. In
Proc. EMNLP-CoNLL, pages 678?687.
L. Zettlemoyer and M. Collins. 2009. Learning context-
dependent mappings from sentences to logical form.
In Proc. ACL/IJCNLP, pages 976?984.
1622
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 677?687, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Inference for Event Timeline Construction
Quang Xuan Do Wei Lu Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,luwei,danr}@illinois.edu
Abstract
This paper addresses the task of construct-
ing a timeline of events mentioned in a given
text. To accomplish that, we present a novel
representation of the temporal structure of a
news article based on time intervals. We then
present an algorithmic approach that jointly
optimizes the temporal structure by coupling
local classifiers that predict associations and
temporal relations between pairs of tempo-
ral entities with global constraints. Moreover,
we present ways to leverage knowledge pro-
vided by event coreference to further improve
the system performance. Overall, our experi-
ments show that the joint inference model sig-
nificantly outperformed the local classifiers by
9.2% of relative improvement in F1. The ex-
periments also suggest that good event coref-
erence could make remarkable contribution to
a robust event timeline construction system.
1 Introduction
Inferring temporal relations amongst a collection of
events in a text is a significant step towards vari-
ous important tasks such as automatic information
extraction and document comprehension. Over the
past few years, with the development of the Time-
Bank corpus (Pustejovsky et al2003) , there have
been several works on building automatic systems
for such a task (Mani et al2006; Chambers and
Jurafsky, 2008; Yoshikawa et al2009; Denis and
Muller, 2011).
Most previous works devoted much efforts to the
task of identifying relative temporal relations (such
as before, or overlap) amongst events (Chambers
?
??
|
?
t1
|
?
t2
|
?
t3
?
|
t4
|
?
+?
|
Time
??
I1
I2
I3
e1
e2
e4
e3/e5
e7 ? e6
Figure 1: A graphical illustration of our timeline representation.
The e?s, t?s and I?s are events, time points and time intervals,
respectively.
and Jurafsky, 2008; Denis and Muller, 2011), with-
out addressing the task of identifying correct asso-
ciations between events and their absolute time of
occurrence. Even if this issue is addressed, certain
restrictions are often imposed for efficiency reasons
(Yoshikawa et al2009; Verhagen et al2010). In
practice, however, being able to automatically infer
the correct time of occurrence associated with each
event is crucial. Such information not only leads to
better text comprehension, but also enables fusion
of event structures extracted from multiple articles
or domains.
In this work, we are specifically interested in map-
ping events into an universal timeline representa-
tion. Besides inferring the relative temporal rela-
tions amongst the events, we would also like to au-
tomatically infer a specific absolute time of occur-
rence for each event mentioned in the text. Unlike
previous work, we associate each event with a spe-
cific absolute time interval inferred from the text. An
example timeline representation is illustrated in Fig.
677
1. Further details of our timeline representation are
given in Sec. 2.3.
We perform global inference by combining a col-
lection of local pairwise classifiers through the use
of an Integer Linear Programming (ILP) formula-
tion that promotes global coherence among local de-
cisions. The formulation allows our model to pre-
dict both event-event relations and event-time inter-
val associations simultaneously. We show that, with
the use of time intervals instead of time points, our
approach leads to a more concise ILP formulation
with reduced number of variables and constraints.
Moreover, we observed that event coreference can
reveal important information for such a task. We
propose that different event mentions that refer to
the same event can be grouped together before clas-
sification and performing global inference. This can
reduce the amount of efforts in both classification
and inference stages and can potentially eliminate
mistakes that would be made otherwise without such
coreference information. To the best of our knowl-
edge, our proposal of leveraging event coreference
to support event timeline construction is novel.
Our experiments on a collection of annotated
news articles from the standard ACE dataset demon-
strate that our approach produces robust timelines of
events. We show that our algorithmic approach is
able to combine various local evidences to produce
a global coherent temporal structure, with improved
overall performance. Furthermore, the experiments
show that the overall performance can be further im-
proved by exploiting knowledge from event corefer-
ence.
2 Background
We focus on the task of mapping event mentions in
a news article to a timeline. We first briefly describe
and define several basic concepts.
2.1 Events
Following the annotation guidelines of the ACE
project, we define an event as an action or occur-
rence that happens with associated participants or
arguments. We also distinguish between events and
event mentions, where a unique event can be core-
ferred to by a set of explicit event mentions in an
article. Formally, an event Ei is co-referred to by
a set of event mentions (ei1, e
i
2, . . . , e
i
k). Each event
mention e can be written as p(a1, a2, . . . , al), where
the predicate p is the word that triggers the presence
of e in text, and a1, a2, . . . al are the arguments asso-
ciated with e. In this work we focus on four tempo-
ral relations between two event mentions including
before, after, overlap and no relation.
2.2 Time Intervals
Similar to Denis and Muller (2011), we define time
intervals as pairs of time endpoints. Each time in-
terval I is denoted by [t?, t+], where t? and t+ are
two time endpoints representing the lower and upper
bound of the interval I , respectively, with t? ? t+.
The general form of a time endpoint is written as
?YYYY-MM-DD hh:mm:ss?. An endpoint can be un-
defined, in which case it is set to an infinity value:
??, or +?. There are two types of time intervals:
Explicit intervals are time intervals that can be
extracted directly from a given text. For example,
consider the following snippet of an article in our
data set: The litigation covers buyers in auctions
outside the United States between January 1, 1993
and February 7, 2000. In this example, we can ex-
tract and normalize two time intervals which are ex-
plicitly written, including January 1, 1993? [1993-
01-01 00:00:00, 1993-01-01 23:59:59] and Febru-
ary 7, 2000 ? [2000-02-07 00:00:00, 2000-02-07
23:59:59]. Moreover, an explicit interval can also
be formed by one or more separate explicit temporal
expressions. In the example above, the connective
term between relates the two expressions to form a
single time interval: between January 1, 1993 and
February 7, 2000 ? [1993-01-01 00:00:00, 2000-
02-07 23:59:59]. To extract explicit time intervals
from text, we use the time interval extractor de-
scribed in Zhao et al2012).
Implicit intervals are time intervals that are not
explicitly mentioned in the text. We observed that
there are events that cannot be assigned to any pre-
cise time interval but are roughly known to occur
in the past or in the future relative to the Doc-
ument Creation Time (DCT) of the article. We
introduce two implicit time intervals to represent
the past and the future events as (??, t?DCT ] and
[t+DCT ,+?), respectively. In addition, we also al-
low an event mention to be assigned into the entire
timeline, which is denoted by (??,+?) if we can-
678
not identify its time of occurrence. We also consider
DCT as an implicit interval.
We say that the time interval Ii precedes the time
interval Ij on a timeline if and only if t
+
i ? t
?
j ,
which also implies that Ii succeeds Ij if and only if
t?i ? t
+
j . The two intervals overlap, otherwise.
2.3 Timeline
We define a timeline as a partially ordered set of time
intervals. Fig. 1 gives a graphical illustration of an
example timeline, where events are annotated and
associated with time intervals. Relations amongst
events can be properly reflected in the timeline rep-
resentation. For example, in the figure, the events e1
and e2 are both associated with the interval I1. The
relation between them is no relation, since it is un-
clear which occurs first. On the other hand, e5 and
e3 both happen in the interval I2 but they form an
overlap relation. The events e6 and e7 occur within
the same interval I3, but e7 precedes (i.e. before) e6
on the timeline. The event e4 is associated with the
interval (??,+?), indicating there is no knowl-
edge about its time of occurrence.
We believe that such a timeline representation
for temporally ordering events has several advan-
tages over the temporal graph representations used
in previous works (Chambers and Jurafsky, 2008;
Yoshikawa et al2009; Denis and Muller, 2011).
Unlike previous works, in our model the events are
partially ordered in a single timeline, where each
event is associated with a precise time interval. This
improves human interpretability of the temporal re-
lations amongst events and time. This property of
our timeline representation, thus, facilitates merg-
ing multiple timelines induced from different arti-
cles. Furthermore, as we will show later, the use
of time intervals within the timeline representation
simplifies the global inference formulation and thus
the inference process.
3 A Joint Timeline Model
Our task is to induce a globally coherent timeline
for a given article. We thus adopt a global infer-
ence model for performing the task. The model
consists of two components: (1) two local pairwise
classifiers, one between event mentions and time in-
tervals (the E?T classifier) and one between event
mentions themselves (the E?E classifier), and (2)
a joint inference module that enforces global co-
herency constraints on the final outputs of the two
local classifiers. Fig. 2 shows a simplified temporal
structure of event mentions and time intervals of an
article in our model.
Our E?T classifier is different from previous
work (Chambers and Jurafsky, 2008; Yoshikawa et
al., 2009; Denis and Muller, 2011), where such clas-
sifiers were trained to identify temporal relations be-
tween event mentions and a temporal expression. In
our work, in order to construct absolute timeline of
event mentions, temporal expressions are captured
and normalized as absolute time intervals. The E?T
classifiers are then used to assign event mentions to
their contextually corresponding time intervals.
We also lifted several restrictions imposed in pre-
vious work (Bethard et al2007; Yoshikawa et al
2009; Verhagen et al2010). Specifically, we do
not require that event mentions and time expressions
have to appear in the same sentence, and we do not
require two event mentions have to appear very close
to each other (e.g., main event mentions in adjacent
sentences) in order to be considered as candidate
pairs for classification. Instead, we performed clas-
sifications over all pairs of event mentions and time
intervals as well as over all pairs of event mentions.
We show through experiments that lifting these re-
strictions is indeed important (see Sec. 5).
Another important improvement over previous
work is our global inference model We would like
to highlight that our work is also distinct from most
previous works in the global inference component.
Specifically, our global inference model jointly op-
timizes the E-E relations amongst event mentions
and their associations, E-T, with temporal informa-
tion (intervals in our case). Previous work (Cham-
bers and Jurafsky, 2008; Denis and Muller, 2011),
on the other hand, assumed that the E-T information
is given and only tried to improve E-E.
3.1 The Pairwise Classifiers
We first describe our local classifiers that associate
event mention with time interval and classify tempo-
ral relations between event mentions, respectively.
CE?T : is the E?T classifier that associates an
event mention with a time interval. Given an event
mention and a time interval, the classifier predicts
679
e1
e
2
e
3
e
4
e
n-1
e
n
e
5
? ? ?
I
1
I
2
I
3
I
m? ? ?
Figure 2: A simplified temporal structure of an article. There
are m time intervals I1 ? ? ? Im and n event mentions e1 ? ? ? en.
A solid edge indicates an association between an interval and
an event mention, whereas a dash edge illustrates a temporal
relation between two event mentions.
whether the former associates with the latter.
CE?T (ei, Ij)? {0, 1},
?i, j, 1 ? i ? n, 1 ? j ? m, (1)
where n and m are the number of event mentions
and time intervals in an article, respectively.
CE?E : is the E?E classifier that identifies
the temporal relation between two event mentions.
Given a pair of event mentions, the classifier predicts
one of the four temporal relations between them:
b?efore, a?fter, o?verlap and n?o relation. Specifically:
CE?E(ei, ej)? {b?, a?, o?, n?},
?i, j, 1 ? i, j ? n, i 6= j, (2)
For training of the classifiers, we define a set of
features following some previous work (Bethard et
al., 2007; Chambers and Jurafsky, 2008; Yoshikawa
et al2009), together with some additional features
that we believe to be helpful for the interval-based
representation. We describe the base features below
and use ? and ? to denote the features used for CE?T
and CE?E , respectively. We use the term temporal
entity (or entity, for short) to refer to either an event
mention or a time interval.
Lexical Features: A set of lexical features related
to the temporal entities: (i)?? the word, lemma and
part-of-speech of the input event mentions and the
context surrounding them, where the context is de-
fined as a window of 2 words before and after the
mention; (ii)? the modal verbs to the left and to the
right of the event mention; (iii)? the temporal con-
nectives between the event mentions1.
1We define a list of temporal connectives including before,
after, since, when, meanwhile, lately, etc.
Syntactic Features: (i)?? which entity appears
first in the text; (ii)?? whether the two entities appear
in the same sentence; (iii)?? the quantized number of
sentences between the two entities2; (iv)?? whether
the input event mentions are covered by preposi-
tional phrases and what are the heads of the phrases;
(v)?? if the entities are in the same sentence, what is
their least common constituent on the syntactic parse
tree; (vi)? whether there is any other temporal entity
that is closer to one of the two entities.
Semantic Features?: A set of semantic features,
mostly related to the input event mentions: (i)
whether the input event mentions have a common
synonym from their synsets in WordNet (Fellbaum,
1998); (ii) whether the input event mentions have a
common derivational form derived from WordNet.
Linguistic Features??: The tense and the aspect
of the input event mentions. We use an in-house
rule-based recognizer to extract these features.
Time Interval Features?: A set of features re-
lated to the input time interval: (i) whether the
interval is implicit; (ii) if it is implicit, identify
its interval type: ?dct? = [t?DCT , t
+
DCT ], ?past? =
(??, t?DCT ], ?feature? = [t
+
DCT ,+?), and ?en-
tire? = (??,+?); (iii) the interval is before, after
or overlapping with the DCT.
We note that unlike many previous work (Mani et
al., 2006; Chambers and Jurafsky, 2008; Denis and
Muller, 2011), our classifiers do not use any gold
annotations of event attributes (event class, tense, as-
pect, modal and polarity) provided in the TimeBank
corpus as features.
In our work, we use a regularized averaged Per-
ceptron (Freund and Schapire, 1999) as our classifi-
cation algorithm3. We used the one-vs.-all scheme
to transform a set of binary classifiers into a multi-
class classifier (for CE?E). The raw prediction
scores were converted into probability distribution
using the Softmax function (Bishop 1996). If there
are n classes and the raw score of class i is acti, the
posterior estimation for class i is:
P? (i) =
eacti
?
1?j?n e
actj
2We quantize the number of sentences between two entities
to 0, 1, 2, less than 5 and greater than or equal to 5
3Other algorithm (e.g. SVM) gave comparable or worse re-
sults, so we only show the results from Averaged Perceptron.
680
3.2 Joint Inference for Event Timeline
To exploit the interaction among the temporal enti-
ties in an article, we optimize the predicted tempo-
ral structure, formed by predictions from CE?T and
CE?E , w.r.t. a set of global constraints that enforce
coherency on the final structure. We perform exact
inference using Integer Linear Programming (ILP)
as in (Roth and Yih, 2007; Clarke and Lapata, 2008).
We use the Gurobi Optimizer4 as a solver.
Let I = {I1, I2, . . . , Im} denote the set of time
intervals extracted from an article, and let E =
{e1, e2, . . . , en} denote all event mentions in the
same article. Let EI = {(ei, Ij) ? E ? I|ei ?
E , Ij ? I} denote the set of all pairs of event
mentions and time intervals. We also denote the
set of event mention pairs by EE = {(ei, ej) ?
E ? E|ei ? E , ej ? E , i 6= j}. The prediction prob-
ability of an association of a pair eI ? EI, given
by classifier CE?T , is denoted by p?eI,1?
5. Now, let
R = {b?, a?, o?, n?} be the set of temporal relations be-
tween two event mentions. The prediction proba-
bility of an event mention pair ee ? EE that takes
temporal relation r, given by CE?E , is denoted by
p?ee,r?. Furthermore, we define x?eI,1? to be a binary
indicator variable that takes on the value 1 iff an as-
sociation is predicted between e and I . Similarly,
we define a binary indicator variable y?ee,r? of a pair
of event mentions ee that takes on the value 1 iff ee
is predicted to hold the relation r.
The objective function is then defined as a linear
combination of the prediction probabilities from the
two local classifiers as follows:
arg max
x,y
[
?
?
eI?EI
p?eI,1? ? x?eI,1?
+ (1? ?)
?
ee?EE
?
r?R
p?ee,r? ? y?ee,r?
]
(3)
subject to the following constraints:
x?eI,1? ? {0, 1}, ?eI ? EI (4)
y?ee,r? ? {0, 1}, ?ee ? EE , r ? R (5)
?
r?R
y?ee,r? = 1, ?ee ? EE (6)
4http://gurobi.com/
5This value is complementary to the non-association proba-
bility, denoted by p?eI,0? = 1? p?eI,1?
We use the single parameter ? to balance the over-
all contribution of two components E-T and E-E.
? is determined through cross validation tuning on
a development set. We use (4) and (5) to make sure
x?eI,1? and y?ee,r? are binary values. The equality
constraint (6) ensures that exactly one particular re-
lation can be assigned to each event mention pair.
In addition, we also require that each event is as-
sociated with only one time interval. These con-
straints are encoded as follows:
?
I?I
x?eI,1? = 1, ?e ? E (7)
Our model also enforces reflexivity and transitiv-
ity constraints on the relations among event men-
tions as follows:
y?eiej ,r? ? y?ejei,r?? = 0,
?eiej = (ei, ej) ? EE , i 6= j (8)
y?eiej ,r1? + y?ejek,r2? ? y?eiek,r3? ? 1,
?eiej , ejek, eiek ? EE , i 6= j 6= k (9)
The equality constraints in (8) encode reflexive
property of event-event relations, where the rela-
tion r? denotes the inversion of the relation r. The
set of possible (r, r?) pairs is defined as follows:
{
(b?, a?), (a?, b?), (o?, o?), (n?, n?)
}
. Following the work
of (Bramsen et al2006; Chambers and Jurafsky,
2008), we encode transitive closure of relations be-
tween event mentions with inequality constraints in
(9), which states that if the pair (ei, ej) has a certain
relation r1, and the pair (ej , ek) has the relation r2,
then the relation r3 must be satisfied between ei and
ek. Examples of such triple (r1, r2, r3) include (b?, b?,
b?) and (a?, a?, a?).
Finally, to capture the interactions between our
local pairwise classifiers we add the following con-
straints:
x?eiIk,1? + x?ejIl,1? ? y?eiej ,b?? ? 1,
?eiIk, ejIl ? EI, ?eiej ? EE ,
Ik precedes Il, i 6= j, k 6= l (10)
Intuitively, the inequality constraints in (10) spec-
ify that a temporal relation between two event men-
tions can be inferred from their respective associated
681
time intervals. Specifically, if two event mentions ei
and ej are associated with two time intervals Ik and
Il respectively, and Ik precedes Il in the timeline,
then ei must happen before ej .
It is important to note that our interval-based for-
mulation is more concise in terms of the number of
variables and constraints needed in the ILP relative
to time expression-based (or timepoint-based) for-
mulations used in previous work (Chambers and Ju-
rafsky, 2008). Specifically, in such timepoint-based
formulations, the relation between each event men-
tion and each time expression needs to be inferred,
resulting in |E||T ||RT | variables, where |E|, |T |,
and |RT | are the numbers of event mentions, time
points, and temporal relations respectively. In con-
trast, only |E||I| variables are required in our for-
mulation, where |I| is the number of intervals (since
we extract intervals explicitly, |I| is roughly equal
to |T |). Furthermore, performing inference with the
timepoint-based formulation would require |E||T |
equality constraints to enforce that each event men-
tion can take only one relation inRT for a particular
time point, whereas our interval-based model only
requires |E| constraints, since each event is strictly
associated with one interval (see Eqn. (7)). We jus-
tify the benefits of our formulation later in Sec. 5.4.
4 Incorporating Knowledge from Event
Coreference
One of the key contributions of our work is using
event coreference information to enhance the time-
line construction performance. This is motivated by
the following two principles:
(P1) All mentions of a unique event are associ-
ated with the same time interval, and overlap with
each other.
(P2) All mentions of an event have the same tem-
poral relation with all mentions of another event.
The example below, extracted from an article pub-
lished on 03/11/2003 in the Automatic Content Ex-
traction (ACE), 2005, corpus6 serves to illustrate the
significance of event coreference to our task.
6http://www.itl.nist.gov/iad/mig/tests/ace/2005/
The world?s most powerful fine art auction houses,
Sotheby?s and Christie?s, have agreed to [e11 =
pay] 40 million dollars to settle an international
price-fixing scam, Sotheby?s said. The [e12 = pay-
ment], if approved by the courts, would settle a
slew of [e21 = suits] by clients over auctions held
between 1993 and 2000 outside the US. ... Sotheby?s
and Christie?s will each [e13 = pay] 20 million dol-
lars,? said Sotheby?s, which operates in 34 countries.
In this example, there are 4 event mentions, whose
trigger words are highlighted in bold face. The un-
derlined text gives an explicit time interval: I1 =
[1993-01-01 00:00:00, 2000-12-31 23:59:59] (we
ignore 2 other intervals given by 1993 and 2000
to simplify the illustration). Now if we consider
the event mention e12, it actually belongs to the im-
plicit future interval I2 = [2003-03-11 23:59:59,
+?). Nevertheless, there is a reasonable chance
that CE?T associates it with I1, given that they both
appear in the same sentence, and there is no di-
rect evident feature indicating the event will actu-
ally happen in the future. In such a situation, using
a local classifier to identify the correct temporal as-
sociation could be challenging.
Fortunately, precise knowledge from event coref-
erence may help alleviate such a problem. The
knowledge reveals that the 4 event mentions can be
grouped into 2 distinct events: E1 = {e11, e
1
2, e
1
3},
E2 = {e21}. If CE?T can make a strong prediction
in associating the event mention e11 (or e
1
3) to I2, in-
stead of I1, the system will have a high chance to
re-assign e12 to I2 based on principle (P1). Similarly,
if CE?E is effective in figuring out that some men-
tion of event E1 occurs after some mention of E2,
then all the mentions of E1 would be predicted to
occur after all mentions in E2 according to (P2).
To incorporate knowledge from event coreference
into our classifiers and the joint inference model, we
use the following procedure: (1) performing classi-
fication with CE?T and CE?E on the data, (2) using
the knowledge from event coreference to overwrite
the prediction probabilities obtained by the two lo-
cal classifiers in step (1), and (3) applying the joint
inference model on the new prediction probabilities
obtained from (2). We note that if we stop at step (2),
we get the outputs of the local classifiers enhanced
by event coreference knowledge.
To overwrite the classification probabilities using
682
event coreference knowledge, we propose two ap-
proaches as follows:
MaxScore: We define the probability between
any mention e ? Ei and an interval I as follows:
p?eI,1? = max
e??Ei
P? (e?, I) (11)
where P? (e?, I) is the classifier (CE?T ) probability
for associating event mention e? to the time interval.
On the other hand, the probabilities for associat-
ing the set of temporal relations, R, to each pair of
mentions in Ei?Ej , is given by the following pair:
(ei, ej)? = arg max
(ei? ,ej? )?Ei?Ej ,r?R
P?
(
(ei
?
, ej
?
), r)
)
p?ee,r? = P?
(
(ei, ej)?, r
)
,?r ? R (12)
In other words, over all possible event mention
pairs and relations, we first pick the pair who glob-
ally obtains the highest probability for some rela-
tion. Next, we simply take the probability distri-
bution of that event mention pair as the distribution
over the relations, for the event pair.
SumScore: The probability between any mention
e ? Ei and an interval I is obtained by:
p?eI,1? =
1
|Ei|
?
e??Ei
P? (e?, I) (13)
To obtain the probability distribution over the set
of temporal relations,R, for any pair of mentions in
Ei ? Ej , we used the following procedure:
r? = arg max
r?R
?
ei?Ei
?
ej?Ej
P?
(
(ei, ej), r
)
(ei, ej)? = arg max
(ei? ,ej? )?Ei?Ej
P?
(
(ei
?
, ej
?
), r?
)
p?ee,r? = P?
(
(ei, ej)?, r
)
,?r ? R (14)
In other words, given two groups of event men-
tions, we first compute the total score of each rela-
tion, and select the relation which has the highest
score. Next from the list of pairs of event mentions
from the two groups, we select the pair which has the
relation r* with highest score compared to all other
pairs. The probability distribution of this pair will
be used as the probability distribution of all event
mention pairs between the two events.
In both approaches, we assign the overlap rela-
tions to all pairs of event mentions in the same event
with probability 1.0.
5 Experimental Study
We first describe the experimental data and then
present and discuss the experimental results.
5.1 Data and Setup
Most previous works in temporal reasoning used
the TimeBank corpus as a benchmark. The cor-
pus contains a fairly diverse collection of anno-
tated event mentions, without any specific focus on
certain event types. According to the annotation
guideline of the corpus, most of verbs, nominal-
izations, adjectives, predicative clauses and preposi-
tional phrases can be tagged as events. However, in
practice, when performing temporal reasoning about
events in a given text, one is typically interested in
significant and typed events, such as Killing, Leg-
islation, Election. Furthermore, event mentions in
TimeBank are annotated with neither event argu-
ments nor event coreference information.
We noticed that the ACE 2005 corpus contains the
annotation that we are interested in. The corpus con-
sists of articles annotated with event mentions (with
event triggers and arguments) and event coreference
information. To create an experimental data set for
our work, we selected from the corpus 20 newswire
articles published in March 2003. To extract time
intervals from the articles, we used the time inter-
val extractor described in (Zhao et al2012) with
minimal post-processing. Implicit intervals are also
added according to Sec. 2.2. We then hired an anno-
tator with expertise in the field to annotate the data
with the following information: (i) event mention
and time interval association, and (ii) the temporal
relations between event mentions, including {b?, a?,
o?}. The annotator was not required to annotate all
pairs of event mentions, but as many as possible.
Next, we saturated the relations based on the ini-
tial annotations as follows: (i) event mentions that
had not been associated with any time intervals were
assigned to the entire timeline interval (??,+?),
and (ii) added inferred temporal relations between
event mentions with reflectivity and transitivity. Ta-
ble 1 shows the data statistics before and after sat-
uration. There are totally 8312 event pairs from 20
documents, including no relation pairs. We note that
in a separate experiment, we still evaluated CE?E
on the TimeBank corpus and got better performance
683
Data #Intervals #E-mentions #E-T #E-E
Initial 232 324 305 376
Saturated 232 324 324 5940
Table 1: The statistics of our experimental data set.
than a corresponding classifier in an existing work
(see Sec. 5.4).
We conducted all experiments with 5-fold cross
validation at the instance level on our data set after
saturation. The global inference model was applied
on a whole document. The results of the systems are
reported in averaged precision, recall and F1 score
on the association performance, for CE?T , and the
temporal relations (we excluded the n? relation, for
CE?E). We also measured the overall performance
of the systems by computing the average of the per-
formance of the classifiers.
5.2 A Baseline
We developed a baseline system that works as fol-
lows. It associates an event mention with the closest
time interval found in the same sentence. If such
an interval is not found, the baseline associates the
mention with the closest time interval to the left.
If the interval is again not found, the mention will
be associated with the DCT interval. The baseline
is based on the intuition of natural reading order:
events that are mentioned earlier are likely to pre-
cede those mentioned later. For the temporal rela-
tion between a pair of event mentions, the baseline
treats the event mention that appears earlier in the
text as temporally happening before the other men-
tion. The baseline performance is shown in the first
group of results in Table 2.
5.3 Our Systems
For our systems, we first evaluated the performance
of our local pairwise classifiers and the global in-
ference model. The second group of results in Ta-
ble 2 shows the systems? performance. Overall,
the results show that our global inference model
relatively outperformed the baseline and the local
classifiers by 57.8% and 9.2% in F1, respectively.
We perform a bootstrap resampling significance test
(Koehn, 2004) on the output predictions of the lo-
cal classifiers with and without the inference model.
The test shows that the overall improvement with
the inference model is statistically significant (p <
0.01). This indicates the effectiveness of our joint
inference model with global coherence constraints.
Next, we integrated event coreference knowledge
into our systems (as described in Sec. 4) and eval-
uated their performance. Our experiments showed
that the SumScore approach works better for CE?T ,
while MaxScore is more suitable for CE?E . Our ob-
servations showed that event mentions of an event
may appear in close proximity with multiple time
intervals in the text, making CE?T produce high
prediction scores for many event mention-interval
pairs. This, consequently, confuses MaxScore on
the best association of the event and the time inter-
vals, whereas SumScore overcomes the problem by
averaging out the association scores. On the other
hand, CE?E gets more benefit from MaxScore be-
causeCE?E works better on pairs of event mentions
that appear closely in the text, which activate more
valuable learning features. We will report the results
using the best approach of each classifier.
To evaluate our systems with event coreference
knowledge, we first experimented our systems with
gold event coreference as given by the ACE 2005
corpus. Table 2 shows the contribution of event
coreference to our systems in the third group of the
results. The results show that injecting knowledge
from event coreference remarkably improved both
the local classifiers and the joint inference model.
Overall, the system that combined event corefer-
ence and the global inference model achieved the
best performance, which significantly overtook all
other compared systems. Specifically, it outper-
formed the baseline system, the local classifiers, and
the joint inference model without event coreference
with 80%, 25%, and 14% of relative improvement in
F1, respectively. It also consistently outperformed
the local classifiers enhanced with event corefer-
ence. We note that the precision and recall of CE?T
in the joint inference model are the same because
the inference model enforced each event mention to
be associated with exactly one time interval. This
is also true for the systems integrated with event
coreference because our integration approaches as-
sign only one time interval to an event mention.
We next move to experimenting with automati-
cally learned event coreference systems. In this ex-
684
Model
CE?T CE?E Overall
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
1 Baseline 33.29 33.29 33.29 20.86 32.81 25.03 27.06 33.05 29.16
2
No Event Coref.
Local classifiers 62.70 34.50 43.29 40.46 42.42 40.96 51.58 38.46 42.13
Global inference 47.88 47.88 47.88 41.42 48.04 44.14 44.65 47.96 46.01
3
With Gold Event Coref.
Local classifiers 50.88 50.88 50.88 43.86 52.65 47.46 47.37 51.77 49.17
Global inference 50.88 50.88 50.88 48.04 62.45 54.05 49.46 56.67 52.47
4
With Learned Event Coref.
Local classifiers 46.37 46.37 46.37 40.83 45.28 42.60 43.60 45.83 44.49
Global inference 46.37 46.37 46.37 42.09 52.50 46.47 44.23 49.44 46.42
Table 2: Performance under various evaluation settings. All figures are averaged scores from 5-fold cross-validation experiments.
periment, we re-trained the event coreference sys-
tem described in Chen et al2009) on all arti-
cles in the ACE 2005 corpus, excluding the 20 ar-
ticles used in our data set. The performance of these
systems are shown in the fourth group of the re-
sults in Table 2. The results show that by using a
learned event coreference system, we achieved the
same improvement trends as with gold event coref-
erence. However, we did not obtain significant im-
provement when comparing with global inference
without event coreference information. This result
shows that the performance of an event coreference
system can have a significant impact on the over-
all performance. While this suggests that a better
event coreference system could potentially help the
task more, it also opens the question whether event
coreference can be benefited from our local classi-
fiers through the use of a joint inference framework.
We would like to leave this for future investigations.
5.4 Previous Work-Related Experiments
We also performed experiments using the same set-
ting as in (Yoshikawa et al2009), which followed
the guidelines of the TempEval challenges (Verha-
gen et al2007; Verhagen et al2010), on our sat-
urated data. Several assumptions were made to sim-
plify the task. For example, only main events in
adjacent sentences are considered when identifying
event-event relations. See (Yoshikawa et al2009)
for more details. We performed 5-fold cross valida-
tion without event coreference. Overall, the system
achieved 29.99 F1 for the local classifiers and 34.69
when the global inference is used. These results are
better than the baseline but underperform our full
models where those simplification assumptions are
not imposed, as shown in Table 2, indicating the im-
portance of relaxing their assumptions in practice.
We also evaluated our CE?E on the TimeBank
corpus. We followed the settings of Chambers and
Jurafsky (2008) to extract all event mention pairs
that were annotated with before (or ibefore, ?imme-
diately before?) and after (or iafter) relations in 183
news articles in the corpus. We trained and evalu-
ated ourCE?E on these examples with the same fea-
ture set that we evaluated in our experiments above,
with gold tense and aspect features but without event
type. Following their work, we performed 10-fold
cross validation. Our classifier achieved a micro-
averaged accuracy of 73.45%, whereas Chambers
and Jurafsky (2008) reported 66.8%. We next in-
jected the knowledge of an event coreference sys-
tem trained on the ACE2005 corpus into our CE?E ,
and obtained a micro-averaged accuracy of 73.39%.
It was not surprising that event coreference did not
help in this dataset because: (i) different domains
? the event coreference was trained on ACE 05 but
applied on TimeBank, and (ii) different annotation
guidelines on events in ACE 2005 and TimeBank.
Finally, we conducted an experiment that justi-
fies the advantages of our interval-based inference
model over a time point-based inference. To do this,
we first converted our data in Table 1 from inter-
vals to time points and infer the temporal relations
between the annotated event mentions and the time
points: before, after, overlap, and unknown. We
modified the first component in the objective func-
tion in (3) to accommodate these temporal relations.
We also made several changes to the constraints,
including removing those in (7) since they are no
longer required, and adding constraints that ensure
685
the relation between a time point and an event men-
tion takes exactly one value. Proper changes were
also made to other constraints in (10) to reflect the
fact that time points are considered rather than inter-
vals. We observed that experiment with such a for-
mulation was unable to finish within 5 hours (we ter-
minated the ILP inference after waiting for 5 hours),
whereas our interval-based model finished the ex-
periment with an average of 21 seconds per article.
6 Related Work
Research in temporal reasoning recently received
much attention. Allen (1983) introduced an interval
based temporal logic which has been used widely
in the field. Recent efforts in building an annotated
temporal corpus (Pustejovsky et al2003) has pop-
ularized the use of machine learning techniques for
the task (Mani et al2006; Bethard et al2007).
This corpus was later used (with simplifications) in
two TempEval challenges (Verhagen et al2007;
Verhagen et al2010). In these challenges, several
temporal-related tasks were defined including the
tasks of identifying the temporal relation between an
event mention and a temporal expression in the same
sentence, and recognizing temporal relations of pairs
of event mentions in adjacent sentences. However,
with several restrictions imposed to these tasks, the
developed systems were not practical.
Recently, there has been much work attempting
to leverage Allen?s interval algebra of temporal re-
lations to enforce global constraints on local pre-
dictions. The work of Tatu and Srikanth (2008)
used global relational constraints to not only expand
the training data but also identifies temporal incon-
sistencies to improve local classifiers. They used
greedy search to select the most appropriate config-
uration of temporal relations among events and tem-
poral expressions. For exact inferences, Bramsen et
al. (2006), Chambers and Jurafsky (2008), Denis
and Muller (2011), and Talukdar et al2012) for-
mulated the temporal reasoning problem in an ILP.
However, the inference models in their work were
not a joint model involving multiple local classifiers
but only one local classifier was involved in their ob-
jective functions.
The work of Yoshikawa et al2009) did formu-
late a joint inference model with Markov Logic Net-
work (MLN). They, however, used the same setting
as the TempEval challenges, thus only pairs of tem-
poral entities in the same or adjacent sentences are
considered. Our work, on the other hand, focuses on
constructing an event timeline with time intervals,
taking multiple local pairwise predictions into a joint
inference model and removing the restrictions on the
positions of the temporal entities. Furthermore, we
propose for the first time to use event coreference
and evaluate the importance of its role in the task of
event timeline construction.
7 Conclusions and Future Work
We proposed an interval-based representation of the
timeline of event mentions in an article. Our rep-
resentation allowed us to formalize the joint infer-
ence model that can be solved efficiently, compared
to a time point-based inference model, thus open-
ing up the possibility of building more practical
event temporal inference systems. Our inference
model achieved significant improvement over the lo-
cal classifiers. We also showed that event coref-
erence can naturally support timeline construction,
and good event coreference led to significant im-
provement in the system performance. Specifically,
when such gold event coreference knowledge was
injected into the model, a significant improvement
in the overall performance could be obtained. While
our experiments suggest that the temporal classi-
fiers can potentially help enhance the performance
of event coreference, in future work we would like
to investigate into coupling event coreference with
other components in a global inference framework.
Acknowledgments
The authors gratefully acknowledge the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
No. FA8750-09-C-0181, and the Army Research
Laboratory (ARL) under agreement W911NF-09-2-
0053. The first author also thanks the Vietnam Ed-
ucation Foundation (VEF) for its sponsorship. Any
opinions, findings, and conclusion or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
VEF, DARPA, AFRL, ARL, or the US government.
686
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM.
Steven Bethard, James H. Martin, and Sara Klingenstein.
2007. Timelines from text: Identification of syntactic
temporal relations. In ICSC.
P. Bramsen, P. Deshpande, Y. K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In EMNLP.
N. Chambers and D. Jurafsky. 2008. Jointly combin-
ing implicit constraints improves temporal ordering.
In EMNLP.
Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact and
evaluation for event coreference resolution. In Work-
shop on Events in Emerging Text Types.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research.
Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In IJ-
CAI.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In ACL.
J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The TIMEBANK corpus. In
Corpus Linguistics.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Introduction to Statistical Relational
Learning.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
WSDM.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In SemEval-2007.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In SemEval-2010.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identify-
ing temporal relations with markov logic. In ACL-
IJCNLP.
Ran Zhao, Quang Do, and Dan Roth. 2012. A robust
shallow temporal reasoning system. In NAACL-HLT
Demo.
687
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1308?1318,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semantic Parsing with Relaxed Hybrid Trees
Wei Lu
Information Systems Technology and Design
Singapore University of Technology and Design
luwei@sutd.edu.sg
Abstract
We propose a novel model for parsing
natural language sentences into their for-
mal semantic representations. The model
is able to perform integrated lexicon ac-
quisition and semantic parsing, mapping
each atomic element in a complete seman-
tic representation to a contiguous word
sequence in the input sentence in a re-
cursive manner, where certain overlap-
pings amongst such word sequences are
allowed. It defines distributions over the
novel relaxed hybrid tree structures which
jointly represent both sentences and se-
mantics. Such structures allow tractable
dynamic programming algorithms to be
developed for efficient learning and decod-
ing. Trained under a discriminative set-
ting, our model is able to incorporate a rich
set of features where certain unbounded
long-distance dependencies can be cap-
tured in a principled manner. We demon-
strate through experiments that by exploit-
ing a large collection of simple features,
our model is shown to be competitive to
previous works and achieves state-of-the-
art performance on standard benchmark
data across four different languages. The
system and code can be downloaded from
http://statnlp.org/research/sp/.
1 Introduction
Semantic parsing, the task of transforming natu-
ral language sentences into formal representations
of their underlying semantics, is one of the clas-
sic goals for natural language processing and ar-
tificial intelligence. This area of research recently
has received a significant amount of attention. Var-
ious models have been proposed over the past few
years (Zettlemoyer and Collins, 2005; Kate and
QUERY : answer(RIVER)
RIVER : exclude(RIVER, RIVER)
RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : (
?
tn
?
)
RIVER : river(all)
What rivers do not run through Tennessee ?
Figure 1: An example tree-structured semantic
representation (above) and its corresponding nat-
ural language sentence.
Mooney, 2006; Wong and Mooney, 2006; Lu et
al., 2008; Jones et al., 2012).
Following previous research efforts, we perform
semantic parsing under a setting where the seman-
tics for complete sentences are provided as train-
ing data, but detailed word-level semantic infor-
mation is not explicitly given during the training
phase. As one example, consider the following
natural language sentence paired with its corre-
sponding semantic representation:
What rivers do not run through Tennessee ?
answer(exclude(river(all), traverse(stateid(
?
tn
?
))))
The training data consists of a set of sentences
paired with semantic representations. Our goal is
to learn from such pairs a model, which can be
effectively used for parsing novel sentences into
their semantic representations.
Certain assumptions about the semantics are
typically made. One common assumption is that
the semantics can be represented as certain re-
cursive structures such as trees, which consist of
atomic semantic units as tree nodes. For exam-
ple, the above semantics can be converted into an
equivalent tree structure as illustrated in Figure 1.
We will provide more details about such tree struc-
tured semantic representations in Section 2.1.
1308
Currently, most state-of-the-art approaches that
deal with such tree structured semantic represen-
tations either cast the semantic parsing problem as
a statistical string-to-string transformation prob-
lem (Wong and Mooney, 2006), which ignores
the potentially useful structural information of the
tree, or employ latent-variable models to cap-
ture the correspondences between words and tree
nodes using a generative approach (Lu et al., 2008;
Jones et al., 2012). While generative models can
be used to flexibly model the correspondences be-
tween individual words and semantic nodes of the
tree, such an approach is limited to modeling local
dependencies and is unable to flexibly incorporate
a large set of potentially useful features.
In this work, we propose a novel model for
parsing natural language into tree structured se-
mantic representations. Specifically, we propose
a novel relaxed hybrid tree representation which
jointly encodes both natural language sentences
and semantics; such representations can be effec-
tively learned with a latent-variable discriminative
model where long-distance dependencies can be
captured. We present dynamic programming al-
gorithms for efficient learning and decoding. With
a large collection of simple features, our model
reports state-of-the-art results on benchmark data
annotated with four different languages.
Furthermore, although we focus our discussions
on semantic parsing in this work, our proposed
model is a general. Essentially our model is a dis-
criminative string-to-tree model which recursively
maps overlapping contiguous word sequences to
tree nodes at different levels, where efficient dy-
namic programming algorithms can be used. Such
a model may find applications in other areas of
natural language processing, such as statistical
machine translation and information extraction.
2 Background
2.1 Semantics
Various semantic formalisms have been consid-
ered for semantic parsing. Examples include the
tree-structured semantic representations (Wong
and Mooney, 2006), the lambda calculus expres-
sions (Zettlemoyer and Collins, 2005; Wong and
Mooney, 2007), and dependency-based compo-
sitional semantic representations (Liang et al.,
2013). In this work, we specifically focus on the
tree-structured representations for semantics.
Each semantic representation consists of se-
mantic units as its tree nodes, where each semantic
unit is of the following form:
m
a
? ?
a
: p
?
(?
b
?) (1)
Here m
a
is used to denote a complete seman-
tic unit, which consists of its semantic type ?
a
, its
function symbol p
?
, as well as an argument list ?
b
?
(we assume there are at most two arguments for
each semantic unit). In other words, each seman-
tic unit can be regarded as a function which takes
in other semantics of specific types as arguments,
and returns new semantics of a particular type. For
example, in Figure 1, the semantic unit at the root
has a type QUERY, a function name answer, and
a single argument type RIVER.
2.2 Joint Representations
Semantic parsing models transform sentences into
their corresponding semantics. It is therefore es-
sential to make proper assumptions about joint
representations for language and semantics that
capture how individual words and atomic seman-
tic units connect to each other. Typically, differ-
ent existing models employ different assumptions
for establishing such connections, leading to very
different definitions of joint representations. We
survey in this section various representations pro-
posed by previous works.
The WASP semantic parser (Wong and Mooney,
2006) essentially casts the semantic parsing prob-
lem as a string-to-string transformation problem
by employing a statistical phrase-based machine
translation approach with synchronous gram-
mars (Chiang, 2007). Therefore, one can think of
the joint representation for both language and se-
mantics as a synchronous derivation tree consist-
ing of those derivation steps for transforming sen-
tences into target semantic representation strings.
While this joint representation is flexible, allow-
ing blocks of semantic structures to map to word
sequences, it does not fully exploit the structural
information (tree) as conveyed by the semantics.
The KRISP semantic parser (Kate and Mooney,
2006) makes use of Support Vector Machines with
string kernels (Lodhi et al., 2002) to recursively
map contiguous word sequences into semantic
units to construct a tree structure. Our relaxed
hybrid tree structures also allow input word se-
quences to map to semantic units in a recursive
manner. One key distinction, as we will see, is that
our structure distinguishes words which are imme-
1309
diately associated with a particular semantic unit,
from words which are remotely associated.
The SCISSOR model (Ge and Mooney, 2005)
performs integrated semantic and syntactic pars-
ing. The model parses natural language sentences
into semantically augmented parse trees whose
nodes consist of both semantic and syntactic labels
and then builds semantic representations based on
such augmented trees. Such a joint representation
conveys more information, but requires language-
specific syntactic analysis.
The hybrid tree model (Lu et al., 2008) is based
on the assumption that there exists an underlying
generative process which jointly produces both the
sentence and the semantic tree in a top-down re-
cursive manner. The generative process results in
a hybrid tree structure which consists of words as
leaves and semantic units as nodes. An example
hybrid tree structure is shown in Figure 2 (a). Such
a representation allows each semantic unit to map
to a possibly discontiguous sequence of words.
The model was shown to be effective empirically,
but it implicitly assumes that both the sentence and
semantics exhibit certain degree of structural sim-
ilarity that allows the hybrid tree structures to be
constructed.
UBL (Kwiatkowski et al., 2010) is a semantic
parser based on restricted higher-order unification
with CCG (Steedman, 1996). The model can be
used to handle both tree structured semantic rep-
resentations and lambda calculus expressions, and
assumes there exist CCG derivations as joint rep-
resentations in which each semantic unit is associ-
ated with a contiguous word sequence where over-
lappings amongst word sequences are not allowed.
Jones et al. (2012) recently proposed a frame-
work that performs semantic parsing with tree
transducers. The model learns representations that
are similar to the hybrid tree structures using a
generative process under a Bayesian setting. Thus,
their representations also potentially present simi-
lar issues as the ones mentioned above.
Besides these supervised approaches, recently
there are also several works that take alternative
learning approaches to (mostly task-dependent)
semantic parsing. Poon and Domingos (2009) pro-
posed a model for unsupervised semantic pars-
ing that transforms dependency trees into seman-
tic representations using Markov logic (Richard-
son and Domingos, 2006). Clarke et al. (2010)
proposed a model that learns a semantic parser
Symbol Description
n A complete natural language sentence
m A complete semantic representation
h A complete latent joint representation (or, a
relaxed hybrid tree for our work)
H(n,m) A complete set of latent joint representations
that contain the (n,m) pair exactly
n, n
a
A contiguous sequence of words
w, w
k
A natural language word
m,m
a
A semantic unit
h, h
a
A node in the relaxed hybrid tree
? The feature vector
?
k
The k-th feature
? The weight vector (model parameters)
?
k
The weight for the k-th feature ?
k
Table 1: Notation Table
for answering questions without relying on seman-
tic annotations. Goldwasser et al. (2011) took
an unsupervised approach for semantic parsing
based on self-training driven by confidence esti-
mation. Liang et al. (2013) proposed a model for
learning the dependency-based compositional se-
mantics (DCS) which can be used for optimiz-
ing the end-task performance. Artzi and Zettle-
moyer (2013) proposed a model for mapping in-
structions to actions with weak supervision.
3 Approach
We discuss our approach to semantic parsing in
this section. The notation that we use in this paper
is summarized in Table 1.
3.1 Model
In standard supervised syntactic parsing, one typi-
cally has access to a complete syntactic parse tree
for each sentence in the training phase, which ex-
actly tells the correct associations between words
and syntactic labels. In our problem, however,
each sentence is only paired with a complete se-
mantic representation where the correct associa-
tions between words and semantic units are un-
available. We thus need to model such information
with latent variables.
For a given n-m pair (where n is a complete
natural language sentence, and m is a complete
semantic representation), we assume there exists a
latent joint representation h that consists of both n
andmwhich tells the correct associations between
words and semantic units in such a pair. We use
H(n,m)
1
to denote the set of all such possible
1
We will give a concrete definition of H(n,m) used for
this work, which is the complete set of all possible relaxed
hybrid tree structures for the n-m pair, when we discuss our
own joint representations later in Section 3.2.
1310
ma
m
b
w
10
m
d
w
9
w
8
m
c
w
6
w
7
w
4
w
5
w
1
w
2
w
3
(a)
m
a
(w
1
w
2
w
3
) w
4
w
5
w
6
w
7
w
8
w
9
w
10
m
b
(w
4
w
5
) w
6
w
7
(w
8
) w
9
(w
10
)
m
d
(w
9
)
m
c
(w
6
w
7
)
(b)
Figure 2: Two different ways of jointly representing sentences and their semantics. The hybrid tree
representation of Lu et al. (2008) (left), and our novel relaxed hybrid tree representation (right). In
our representation, a word w can be either immediately associated with its parent m (the words which
appear inside the parenthesis), or remotely associated with m (the words that do not appear inside the
parenthesis, and will also appear under a subtree rooted by one of m?s children).
latent joint representations that contain both n and
m exactly.
Given the joint representations, to model how
the data is generated, one can either take a gener-
ative approach which models the joint probability
distribution over (n,m,h) tuples, or a discrimina-
tive approach which models the distribution over
(m,h) tuples given the observation n. Following
several previous research efforts (Zettlemoyer and
Collins, 2005; Kwiatkowski et al., 2010; Liang et
al., 2013), in this work we define a discriminative
model using a log-linear approach:
P (m,h|n; ?) =
e
???(n,m,h)
?
m
?
,h
?
?H(n,m
?
)
e
???(n,m
?
,h
?
)
(2)
Here ?(n,m,h) is a function defined over the
tuple (n,m,h) that returns a vector consisting of
counts of features associated with the tuple, and ?
is a vector consisting of feature weights, which are
the parameters of the model.
In practice, we are only given the n-m pairs but
the latent structures are not observed. We there-
fore consider the following marginal probability:
P (m|n; ?) =
?
h?H(n,m)
P (m,h|n; ?)
=
?
h?H(n,m)
e
???(n,m,h)
?
m
?
,h
?
?H(n,m
?
)
e
???(n,m
?
,h
?
)
(3)
The above probability is defined for a particular
n-m pair. The complete log-likelihood objective
for the training set is:
L(?) =
?
i
logP (m
i
|n
i
; ?)? ?||?||
2
=
?
i
log
?
h?H(n
i
,m
i
)
P (m
i
,h|n
i
; ?)? ?||?||
2
(4)
where (n
i
,m
i
) refers to the i-th instance in the
training set. Note that here we introduce the ad-
ditional regularization term ?? ? ||?||
2
to control
over-fitting, where ? is a positive scalar.
Our goal is to maximize this objective function
by tuning the model parameters ?. Let?s assume
? = ??
1
, ?
2
, . . . , ?
N
?, where N is the total num-
ber of features (or the total number of parameters).
Differentiating with respect to ?
k
, the weight as-
sociated with the k-th feature ?
k
, yields:
?L(?)
??
k
=
?
i
?
h
E
P (h|n
i
,m
i
;?)
[?
k
(n
i
,m
i
,h)]
?
?
i
?
m,h
E
P (m,h|n
i
;?)
[?
k
(n
i
,m,h)]? 2??
k
(5)
where ?
k
(n,m,h) refers to the number of occur-
rences for the k-th feature in the tuple (n,m,h).
Given the objective value (4) and gradients (5),
standard methods such as stochastic gradient de-
scent or L-BFGS (Liu and Nocedal, 1989) can be
employed to optimize the objective function. We
will discuss the computation of the objective func-
tion and gradients next.
3.2 Relaxed Hybrid Trees
To allow tractable computation of the values for
the objective function (4) and the gradients (5),
1311
. . .
NUM : count(STATE)
STATE : state(STATE)
statesSTATE : next to(CITY)
borders how many. . .
(a)
. . .
NUM : count(STATE)
. . . borders how many states (. . . )
STATE : state(STATE)
. . . borders how many (states)
STATE : next to(CITY)
. . . (borders how many)
. . .
(b)
Figure 3: An example hybrid tree and an example relaxed hybrid tree representation. When the correct
latent structure can not be found, the dependency between the words ?how many? and the semantic unit
?NUM : count(STATE)? can not be captured if the hybrid tree is used, whereas with our relaxed hybrid
tree representation, such a dependency can still be captured.
certain restrictions on the latent structures (h) will
need to be imposed. We define in this section the
set of all valid latent structures H(n,m) for the
(n,m) pair so that some efficient dynamic pro-
gramming algorithms can be deployed.
We introduce our novel relaxed hybrid tree rep-
resentations which jointly encode both natural lan-
guage sentences and the tree-structured semantics.
A relaxed hybrid tree h defined over (n,m) is a
tree whose nodes are (n,m) pairs, where each n is
a contiguous sequence of words from n, and each
m is a semantic unit (a tree node) from m. For
any two nodes h
a
? (n
a
,m
a
) and h
b
? (n
b
,m
b
)
that appear in the relaxed hybrid tree h, if h
a
is the
parent of h
b
in h, then m
a
must also be the parent
of m
b
in m, and n
a
must contain n
b
. If the low-
est common ancestor of h
a
and h
b
in h is neither
h
a
nor h
b
, then n
a
and n
b
do not share any com-
mon word. Note that words that appear at different
positions in n are regarded as different words, re-
gardless of their string forms.
Figure 2 (b) shows an example relaxed hybrid
tree structure that we consider. Assume we would
like to jointly represent both the natural language
sentence n ? w
1
w
2
. . . w
10
and its corresponding
semantic representation m ? m
a
(m
b
(m
c
,m
d
)).
In the given example, the semantic unit m
a
maps
to the complete sentence,m
b
maps to the sequence
w
4
w
5
. . . w
10
, m
c
maps to w
6
w
7
, and m
d
maps to
w
9
. Certain words such as w
4
and w
10
that ap-
pear directly below the semantic unit m
b
but do
not map to any of m
b
?s child semantic units are
highlighted with parentheses ?()?, indicating they
are immediately associated with m
b
. These words
play unique roles in the sub-tree rooted by m
b
and
are expected to be semantically closely related to
m
b
. Note that each word is immediately associ-
ated with exactly one semantic unit.
As a comparison, we also show an example hy-
brid tree representation (Lu et al., 2008) in Fig-
ure 2 (a) that has similar words-semantics cor-
respondences. Different from our representation,
the hybrid tree representation assumes each natu-
ral language word only maps to a single seman-
tic unit (which is its immediate parent), and each
semantic unit maps to a possibly discontiguous
sequence of words. We believe that such a rep-
resentation is overly restrictive, which might ex-
hibit problems in cases where natural language
sentences are highly non-isomorphic to their se-
mantic tree structures. Under our relaxed hybrid
tree representations, words that are immediately
associated with a particular semantic unit now can
also be remotely associated with all its parent se-
mantic units as well. Essentially, our representa-
tion allows us to capture certain unbounded depen-
dencies ? for any word, as long as it appears be-
low a certain semantic unit (in the relaxed hybrid
tree), we can always capture the dependency be-
tween the two, regardless of which actual seman-
tic unit that word is immediately associated with.
Such an important relaxation allows some long-
distance dependencies to be captured, which can
potentially alleviate the sentence-semantics non-
isomorphism issue reported in several earlier se-
mantic parsing works (Kate and Mooney, 2006;
1312
Wong and Mooney, 2007).
To better illustrate the differences, we show a
concrete example in Figure 3, where the correct
latent structure showing the correspondences be-
tween words and semantic units can not be found
with the hybrid tree model. As a result, the hy-
brid tree model will fail to capture the correct de-
pendency between the words ?how many? and the
semantic unit ?NUM : count(STATE)?. On the
other hand, with our relaxed hybrid tree represen-
tation, such a dependency can still be captured,
since these words will still be (remotely) associ-
ated with the semantic unit.
Such a relaxed hybrid tree representation, when
further constrained with the word association pat-
terns that we will introduce next, allows both the
objective function (4) and the gradients of (5) to
be computed through the dynamic programming
algorithms to be presented in Section 4.
3.3 Word Association Patterns
As we have mentioned above, in the relaxed hy-
brid tree structures, each word w under a certain
semantic unit m can either appear directly below
m only (immediately associated with m), or can
also appear in a subtree rooted by one ofm?s child
semantic unit (remotely associated with m).
We allow several different ways for word asso-
ciations and define the allowable patterns for se-
mantic units with different number of arguments
in Table 2. Such patterns are defined so that our
model is amendable to dynamic programming al-
gorithms to be discussed in Sec 4. In this table,
w refers to a contiguous sequence of natural lan-
guage words that are immediately associated with
the current semantic unit, while X and Y refers
to a sequence of natural language words that the
first and second child semantic unit will map to,
respectively.
For example, in Figure 2 (b), the word sequence
directly below the semantic unit m
a
follows the
pattern wX (since the word sequence w
1
w
2
w
3
is
immediately associated with m
a
, and the remain-
ing words are remotely associated with m
a
), and
the word sequence below m
b
follows wXwYw
2
.
The word association patterns are similar to
those hybrid patterns used in hybrid trees. One
key difference is that we disallow the unary pat-
2
This is based on the assumption that m
c
and m
d
are the
first and second child of m
b
in the semantic representation,
respectively. If m
d
is the first child in the semantic represen-
tation andm
c
is the second, the pattern should be wYwXw.
#Args Word Association Patterns
0 w
1 wX, Xw, wXw
2
XY, YX, wXY, wYX, XwY, YwX
XYw, YXw, wXwY, wYwX
wXYw, wYXw, XwYw, YwXw
wXwYw, wYwXw
Table 2: The complete list of word association pat-
terns. Here #Args means the number of arguments
for a semantic unit.
tern X. The reason is, when computing the parti-
tion function in Equation 3, inclusion of pattern X
will result in relaxed hybrid trees consisting of an
infinite number of nodes. However, this issue does
not come up in the original hybrid tree models due
to their generative setting, where the training pro-
cess does not involve such a partition function.
3.4 Features
The features are defined over the (n,m,h) tuples.
In practice, we define features at each level of the
relaxed hybrid tree structure h. In other words,
features are defined over (n,m) tuples where n is
a contiguous sequence of natural language words
(immediately or remotely) associated with the se-
mantic unit m (recall that h contains both n and
m, and each level of h simply consists of a seman-
tic unit and a contiguous sequence of words). Each
feature over (n,m) is then further decomposed
as a product between two indicator feature func-
tions, defined over the natural language words (n)
and semantic unit (m) respectively: ?(n,m) =
?
i
(n) ? ?
o
(m). For each ?
i
(n) we define two
types of features: the local features, which are de-
fined over immediately associated words only, and
the span features, which are defined over all (im-
mediately or remotely) associated words to cap-
ture long range dependencies.
The local features include word unigrams and
bigrams, the word association patterns, as well as
character-level features
3
which perform implicit
morphological analysis. The span features include
word unigrams, bigrams, as well as trigrams. Al-
though our model allows certain more sophisti-
cated features to be exploited, such as word POS
features, word similarity features based on the
WordNet (Pedersen et al., 2004), we deliberately
choose to only include these simple features so
3
For each word, we used all its prefixes (not necessarily
linguistically meaningful ones) whose length are greater than
2 as features, for all languages.
1313
as to make a fair comparison with previous works
which also did not make use of external resources.
For the features defined on m (i.e., ?
o
(m)), we in-
clude only the string form of m, as well as m?s
function name as features.
Finally, we also define features over m only.
Such features are defined over semantic unit pairs
such as (m
a
,m
b
) where m
a
is the parent node of
m
b
as in m. They include: 1) concatenation of the
string forms ofm
a
andm
b
, 2) concatenation of the
string form of m
a
and m
b
?s type, and 3) concate-
nation of the function names of m
a
and m
b
.
4 Algorithms
In this section we describe the efficient algorithms
used for learning and decoding. The algorithms
are inspired by the inside-outside style algorithms
used for the generative hybrid tree models (Lu
et al., 2008), but are different in the following
ways: 1) we need to handle features, including
long-distance features, 2) we need to additionally
handle the computation of the partition function of
Equation (3).
4.1 Learning
The training process involves the computation of
the objective function (4) as well as the gradient
terms (5).
The objective function (4) (excluding the regu-
larization term which can be trivially computed) is
equivalent to the following:
L(?) =
?
i
log
?
h?H(n
i
,m
i
)
e
???(n
i
,m
i
,h)
?
?
i
log
?
m
?
,h
?
?H(n
i
,m
?
)
e
???(n
i
,m
?
,h
?
)
(6)
In the first term,
?
h?H(n
i
,m
i
)
e
???(n
i
,m
i
,h)
is
in fact the sum of the scores (as defined by ? and
?) associated with all such latent structures that
contain both m
i
and n
i
exactly. The second term
is the sum of the scores associated with all the la-
tent structures that contain n
i
exactly. We focus
our discussions on the computation of the first part
first.
We use
m
(p)
w
i
. . . w
j
to denote the combined score
of all such latent relaxed hybrid tree structures that
contain both the semantic tree rooted bym and the
natural language word sequence w
i
. . . w
j
that
forms the word association pattern p with respect
to m. For example, the score of the relaxed hybrid
tree in Figure 2 (b) is contained by
m
a
(wX)
w
1
. . . w
10
(here p = wX because only w
1
w
2
w
3
are imme-
diately associated with m
a
).
We give an illustrative example that shows how
these scores can be computed efficiently using dy-
namic programming. Consider the following case
when m has at least one child semantic unit:
m
(wXw)
w
i
. . . w
j
=
m
(w)
w
i
?
m
(wXw)
w
i+1
. . . w
j
+
m
(w)
w
i
?
m
(Xw)
w
i+1
. . . w
j
Here the symbol ? means extract and compute,
a process that involves 1) extraction of additional
features when the two structures on the right-hand
side are put together (for example, the local bi-
gram feature ?w
i
w
i+1
? can be extracted in the
above case), and 2) computation of the score for
the new structure when the two structures from
both sides of ? are combined, based on the scores
of these structures and newly extracted features.
The above equation holds because for any re-
laxed hybrid tree contained by the left-hand side,
the left-most word w
i
is always immediately as-
sociated with m. The term
m
(wXw)
w
i+1
. . . w
j
is present-
ing a similar but smaller structure to the term on
the left-hand side. The other term
m
(wX)
w
i
. . . w
j
can
also be computed based on similar equations. In
other words, such terms can be computed from
even smaller similar terms in a recursive manner.
A bottom-up dynamic programming algorithm is
used for computing such terms.
When the semantic unit m has two child nodes,
similar equations can also be established. Here we
give an illustrative example:
m
(wXwYw)
w
i
. . . w
j
=
?
j?1
k=i
m
(wX)
w
i
. . . w
k
?
m
(wYw)
w
k+1
. . . w
j
Finally, we have the following equation:
m
w
i
. . . w
j
=
?
p
m
(p)
w
i
. . . w
j
The left-hand side simply means the combined
score for all such relaxed hybrid trees that have
(n,m) as the root, where n ? w
i
. . . w
j
. Once the
computation for a certain (n,m) pair is done, we
1314
can move up to process such pairs that involvem?s
parent node.
The above process essentially computes the in-
side score associated with the (n,m) pair, which
gives the sum of the scores of all such (incomplete)
relaxed hybrid trees that can be constructed with
(n,m) as the root. Similar to (Lu et al., 2008), we
can also define and compute the outside scores for
(n,m) (the combined score of such incomplete re-
laxed hybrid trees that contain (n,m) as one of its
leave nodes) in an analogous manner, where the
computation of the gradient functions can be effi-
ciently integrated in this process.
Computation of the second part of the objective
function (6) involves dynamic programming over
a packed forest representation rather than a single
tree, which requires an extension to the algorithm
described in (Lu et al., 2008). The resulting al-
gorithm is similar to the one used in (Lu and Ng,
2011), which has been used for language gener-
ation from packed forest representations of typed
?-calculus expressions.
4.2 Decoding
The decoding phase involves finding the optimal
semantic tree m
?
given a new input sentence n:
m
?
= arg max
m
P (m|n) (7)
This in fact is equivalent to finding the follow-
ing optimal semantic tree m
?
:
m
?
= arg max
m
?
h?H(n,m)
e
???(n,m,h)
(8)
Unfortunately, the summation operation inside
the arg max prevents us from employing a simi-
lar version of the dynamic programming algorithm
we developed for learning in Section 4.1. To over-
come this difficulty, we instead find the optimal
semantic tree using the following equation:
m
?
= arg max
m,h?H(n,m)
e
???(n,m,h)
(9)
We essentially replace the
?
operation by the
max operation inside the arg max. In other words,
we first find the best latent relaxed hybrid tree h
?
that contains the input sentence n, and next we ex-
tract the optimal semantic tree m
?
from h
?
.
This decoding algorithm is similar to the dy-
namic programming algorithm used for comput-
ing the inside score for a given natural language
sentence n (i.e., the algorithm for computing the
second term of Equation (6)). The difference here
is, at each intermediate step, instead of computing
the combined score for all possible relaxed hybrid
tree structures (i.e., performing sum), we find the
single-best relaxed hybrid tree structure (i.e., per-
forming max).
5 Experiments
We present evaluations on the standard GeoQuery
dataset which is publicly available. This dataset
has been used for evaluations in various seman-
tic parsing works (Wong and Mooney, 2006; Kate
and Mooney, 2006; Lu et al., 2008; Jones et al.,
2012). It consists of 880 natural language sen-
tences paired with their corresponding formal se-
mantic representations. Each semantic represen-
tation is a tree structured representation derived
from a Prolog query that can be used to interact
with a database of U.S. geography facts for retriev-
ing answers. The original dataset was fully anno-
tated in English, and recently Jones et al. (2012)
released a new version of this dataset with three
additional language annotations (German, Greek
and Thai). For all the experiments, we used the
identical experimental setup as described in Jones
et al. (2012). Specifically, we trained on 600 in-
stances, and evaluated on the remaining 280.
We note that there exist two different versions of
the GeoQuery dataset annotated with completely
different semantic representations. Besides the
version that we use in this work, which is an-
notated with tree structured semantic representa-
tions, the other version is annotated with lambda
calculus expressions (Zettlemoyer and Collins,
2005). Results obtained from these two versions
are not comparable.
4
Like many previous works,
we focus on tree structured semantic representa-
tions for evaluations in this work since our model
is designed for handling the class of semantic rep-
resentations with recursive tree structures.
We used the standard evaluation criteria for
judging the correctness of the outputs. Specifi-
cally, our system constructs Prolog queries from
the output parses, and uses such queries to retrieve
answers from the GeoQuery database. An output
is considered correct if and only if it retrieves the
4
Kwiatkowski et al. (2010) showed in Table 3 of their
work that the version with tree-structured representations ap-
peared to be more challenging ? their semantic parser?s per-
formance on this version was substantially lower than that on
the lambda calculus version.
1315
System
English Thai German Greek
Acc. F1 Acc. F1 Acc. F1 Acc. F1
WASP 71.1 77.7 71.4 75.0 65.7 74.9 70.7 78.6
HYBRIDTREE+ 76.8 81.0 73.6 76.7 62.1 68.5 69.3 74.6
UBL-S 82.1 82.1 66.4 66.4 75.0 75.0 73.6 73.7
TREETRANS 79.3 79.3 78.2 78.2 74.6 74.6 75.4 75.4
RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2
Table 3: Performance on the benchmark data, using four different languages as inputs. RHT: relaxed
hybrid tree (this work).
same answers as the gold standard (Jones et al.,
2012). We report accuracy scores ? the percentage
of inputs with correct answers, and F1 measures ?
the harmonic mean of precision (the proportion of
correct answers out of inputs with an answer) and
recall (the proportion of correct answers out of all
inputs). By adopting such an evaluation method
we will be able to directly compare our model?s
performance against those of the previous works.
The evaluations were conducted under such a
setting in order to make comparisons to previous
works. We would like to stress that our model
is designed for general-purpose semantic parsing
that is not only natural language-independent, but
also task-independent. We thus distinguish our
work from several previous works in the literature
which focused on semantic parsing under other as-
sumptions. Specifically, for example, works such
as (Liang et al., 2013; Poon and Domingos, 2009;
Clarke et al., 2010) essentially performed seman-
tic parsing under different settings where the goal
was to optimize the performance of certain down-
stream NLP tasks such as answering questions,
and different semantic formalisms and language-
specific features were usually involved.
For all our experiments, we used the L-BFGS
algorithm for learning the feature weights, where
feature weights were all initialized to zeros and the
regularization hyper-parameter ? was set to 0.01.
We set the maximum number of L-BFGS steps to
100. When all the features are considered, our
model creates over 2 million features for each lan-
guage on the dataset (English: 2.1M, Thai: 2.3M,
German: 2.7M, Greek: 2.6M). Our model re-
quires (on average) a per-instance learning time of
0.428 seconds and a per-instance decoding time of
0.235 seconds, on an Intel machine with a 2.2 GHz
CPU. Our implementation is in Java. Here the
per-instance learning time refers to the time spent
on computing the instance-level log-likelihood as
well as the expected feature counts (needed for the
gradients).
Table 3 shows the evaluation results of our sys-
tem as well as those of several other comparable
previous works which share the same experimen-
tal setup as ours. UBL-S is the system presented
in Kwiatkowski et al. (2010) which performs se-
mantic parsing with the CCG based on mapping
between graphs, and is the only non-tree based
top-performing system. Their system, similar to
ours, also uses a discriminative log-linear model
where two types of features are defined. WASP is
a model based on statistical phrase-based machine
translation as we have described earlier. The hy-
brid tree model (HYBRIDTREE+) performs learn-
ing using a generative process which is augmented
with an additional discriminative-reranking stage,
where certain global features are incorporated (Lu
et al., 2008). The Bayesian tree transducer model
(TREETRANS) learns under a Bayesian genera-
tive framework, using hyper-parameters manually
tuned on the German training data.
We can observe from Table 3 that the semantic
parser based on relaxed hybrid tree gives compet-
itive performance when all the features (described
in Sec 3.4) are used. It significantly outperforms
the hybrid tree model that is augmented with a dis-
criminative reranking step. The model reports the
best accuracy and F1 scores on English and Thai
and best accuracy score on Greek. The scores
on German are lower than those of UBL-S and
TREETRANS, mainly because the span features
appear not to be effective for this language, as we
will discuss next.
We report in Table 4 the test set performance
when certain types of features are excluded from
our system. Such results can help us understand
the effectiveness of features of different types. As
we can see from the table, in general, all fea-
tures play essential roles, though their effective-
1316
System
English Thai German Greek
Acc. F1 Acc. F1 Acc. F1 Acc. F1
RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2
RHT (no local features) 81.4 81.4 78.2 78.2 74.3 74.3 75.7 75.7
RHT (no span features) 81.1 81.1 77.9 77.9 78.2 78.2 78.9 78.9
RHT (no char features) 79.6 79.6 82.1 82.1 73.6 73.6 76.1 76.1
Table 4: Results when certain types of features (local features, span features and character-level features)
are excluded.
ness vary across different languages. The local
features, which capture local dependencies, are
of particular importance. Performance on three
languages (English, Thai, and Greek) will drop
when such features are excluded. Character-level
features are very helpful for the three European
languages (English, German, and Greek), but ap-
pear to be harmful for Thai. This indicates the
character-level features that we propose do not
perform effective morphological analysis for this
Asian language.
5
The span features, which are
able to capture certain long-distance dependen-
cies, also play important roles. Specifically, if
such features are excluded, our model?s perfor-
mance on three languages (Greek, English, Thai)
will drop. Such features do not appear to be help-
ful for Thai and appear to be harmful for Ger-
man. Clearly, such long-distance features are not
contributing useful information to the model when
these two languages are considered. This is espe-
cially the case for German, where we believe such
features are contributing substantial noisy infor-
mation to the model. What underlying language-
specific, syntactic properties are generally caus-
ing these gaps in the performances? We believe
this is an important question that needs to be ad-
dressed in future research. As we have mentioned,
to make an appropriate comparison with previ-
ous works, only simple features are used. We be-
lieve that our system?s performance can be further
improved when additional informative language-
specific features can be extracted from effective
language tools and incorporated into our system.
6 Conclusions
In this work, we present a new discriminative
model for semantic parsing which extends the hy-
5
The character-level features that we introduced are in-
deed very general. We have conducted several additional ex-
periments, which show that our model?s performance for each
language can be further improved when certain language-
specific character-level features are introduced.
brid tree model. Such an extension is similar to the
extension of the generative syntactic parser based
on probabilistic context-free grammars (PCFG) to
the feature-based CRF parser (Finkel et al., 2008),
but is slightly more complex due to latent struc-
tures. Developed on top of our novel relaxed hy-
brid tree representations, our model allows cer-
tain long-distance dependencies to be captured.
We also present efficient algorithms for learn-
ing and decoding. Experiments on benchmark
data show that our model is competitive to previ-
ous works and achieves the state-of-the-art perfor-
mance across several different languages.
Future works include development of efficient
algorithms for feature-based semantic parsing
with alternative loss functions (Zhou et al., 2013),
development of feature-based language generation
models (Lu et al., 2009; Lu and Ng, 2011) and
multilingual semantic parsers (Jie and Lu, 2014),
as well as the development of efficient semantic
parsing algorithms for optimizing the performance
of certain downstream NLP tasks with less super-
vision (Clarke et al., 2010; Liang et al., 2013).
Being able to efficiently exploit features defined
over individual words, our model also opens up the
possibility for us to exploit alternative representa-
tions of words for learning (Turian et al., 2010), or
to perform joint learning of both distributional and
logical semantics (Lewis and Steedman, 2013).
Furthermore, as a general string-to-tree structured
prediction model, this work may find applications
in other areas within NLP.
The system and code can be downloaded from
http://statnlp.org/research/sp/.
Acknoledgments
The author would like to thank the anonymous re-
viewers for their helpful comments. This work
was supported by SUTD grant SRG ISTD 2013
064.
1317
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228, June.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proc. of CONLL ?10, pages
18?27.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proc. of ACL/HLT,
pages 959?967.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proc. of CONLL ?05, pages 9?16.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proc. of ACL ?11, pages 1486?
1495.
Zhanming Jie and Wei Lu. 2014. Multilingual se-
mantic parsing: Parsing multiple languages into se-
mantic representations. In Proc. of COLING, pages
1291?1301.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proc. of ACL ?12, pages 488?496.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proc. of COLING/ACL, pages 913?920.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proc. EMNLP?10, pages 1223?
1233.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179?192.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528, December.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. The Journal of
Machine Learning Research, 2:419?444.
Wei Lu and Hwee Tou Ng. 2011. A probabilis-
tic forest-to-string model for language generation
from typed lambda calculus expressions. In Proc.
of EMNLP ?11, pages 1611?1622.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proc. of EMNLP ?08, pages 783?792.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In Proc. of EMNLP, pages 400?409.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet:: Similarity: measuring the re-
latedness of concepts. In Proc. of HLT-NAACL ?04
(Demonstration), pages 38?41.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proc. of EMNLP ?09,
pages 1?10.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine learning, 62(1-
2):107?136.
Mark Steedman. 1996. Surface structure and interpre-
tation. MIT press.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL ?10,
pages 384?394.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proc. of HLT/NAACL ?06,
pages 439?446.
Yuk Wah Wong and Raymond J Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proc. of ACL ?07.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of UAI ?05.
Junsheng Zhou, Juhong Xu, and Weiguang Qu. 2013.
Efficient latent structural perceptron with hybrid
trees for semantic parsing. In IJCAI, pages 2246?
2252.
1318
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 835?844,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Automatic Event Extraction with Structured Preference Modeling
Wei Lu and Dan Roth
University of Illinois at Urbana-Champaign
{luwei,danr}@illinois.edu
Abstract
This paper presents a novel sequence label-
ing model based on the latent-variable semi-
Markov conditional random fields for jointly
extracting argument roles of events from texts.
The model takes in coarse mention and type
information and predicts argument roles for a
given event template.
This paper addresses the event extraction
problem in a primarily unsupervised setting,
where no labeled training instances are avail-
able. Our key contribution is a novel learning
framework called structured preference mod-
eling (PM), that allows arbitrary preference
to be assigned to certain structures during the
learning procedure. We establish and discuss
connections between this framework and other
existing works. We show empirically that the
structured preferences are crucial to the suc-
cess of our task. Our model, trained with-
out annotated data and with a small number
of structured preferences, yields performance
competitive to some baseline supervised ap-
proaches.
1 Introduction
Automatic template-filling-based event extraction is
an important and challenging task. Consider the fol-
lowing text span that describes an ?Attack? event:
. . . North Korea?s military may have fired a laser
at a U.S. helicopter in March, a U.S. official
said Tuesday, as the communist state ditched its
last legal obligation to keep itself free of nuclear
weapons . . .
A partial event template for the ?Attack? event is
shown on the left of Figure 1. Each row shows an
argument for the event, together with a set of its ac-
ceptable mention types, where the type specifies a
high-level semantic class a mention belongs to.
The task is to automatically fill the template en-
tries with texts extracted from the text span above.
The correct filling of the template for this particular
example is shown on the right of Figure 1.
Performing such a task without any knowledge
about the semantics of the texts is hard. One typi-
cal assumption is that certain coarse mention-level
information, such as mention boundaries and their
semantic class (a.k.a. types), are available. E.g.:
. . . [North Korea?s military]ORG may have fired
[a laser]WEA at [a U.S. helicopter]VEH in
[March]TME, a U.S. official said Tuesday, as the
communist state ditched its last legal obligation
to keep itself free of nuclear weapons . . .
Such mention type information as shown on the
left of Figure 1 can be obtained from various sources
such as dictionaries, gazetteers, rule-based systems
(Stro?tgen and Gertz, 2010), statistically trained clas-
sifiers (Ratinov and Roth, 2009), or some web re-
sources such as Wikipedia (Ratinov et al, 2011).
However, in practice, outputs from existing men-
tion identification and typing systems can be far
from ideal. Instead of obtaining the above ideal an-
notation, one might observe the following noisy and
ambiguous annotation for the given event span:
. . . [[North Korea?s]GPE|LOC military]ORG may have
fired a laser at [a [U.S.]GPE|LOC helicopter]VEH
in [March]TME, [a [U.S.]GPE|LOC official]PER said
[Tuesday]TME, as [the communist state]ORG|FAC|LOC
ditched its last legal obligation to keep [itself ]ORG
free of [nuclear weapons]WEA . . .
Our task is to design a model to effectively select
mentions in an event span and assign them with cor-
responding argument information, given such coarse
835
Argument Possible Types Extracted Text
ATTACKER GPE, ORG, PER N. Korea?s military
INSTRUMENT VEH, WEA a laser
PLACE FAC, GPE, LOC -
TARGET
FAC, GPE, LOC
a U.S. helicopter
ORG, PER, VEH
TIME-WITHIN TME March
Figure 1: The partial event template for the Attack event (left),
and the correct event template annotation for the example event
span given in Sec 1 (right). We primarily follow the ACE stan-
dard in defining arguments and types.
and often noisy mention type annotations.
This work addresses this problem by making the
following contributions:
? Naturally, we are interested in identifying the
active mentions (the mentions that serve as ar-
guments) and their correct boundaries from the
data. This motivates us to build a novel latent-
variable semi-Markov conditional random fields
model (Sarawagi and Cohen, 2004) for such an
event extraction task. The learned model takes
in coarse information as produced by existing
mention identification and typing modules, and
jointly outputs selected mentions and their cor-
responding argument roles.
? We address the problem in a more realistic sce-
nario where annotated training instances are not
available. We propose a novel general learning
framework called structured preference model-
ing (or preference modeling, PM), which en-
compasses both the fully supervised and the
latent-variable conditional models as special
cases. The framework allows arbitrary declar-
ative structured preference knowledge to be in-
troduced to guide the learning procedure in a pri-
marily unsupervised setting.
We present our semi-Markov model and discuss
our preference modeling framework in Section 2 and
3 respectively. We then discuss the model?s relation
with existing constraint-driven learning frameworks
in Section 4. Finally, we demonstrate through ex-
periments that structured preference information is
crucial to model and present empirical results on a
standard dataset in Section 5.
2 The Model
It is not hard to observe from the example presented
in the previous section that dependencies between
A1
T1
C1
B2
C2
A3
T3
C3
B4
C4
. . .
. . .
. . .
An
Tn
Cn
Figure 2: A simplified graphical illustration for the semi-
Markov CRF, under a specific segmentation S ? C1C2 . . . Cn.
In a supervised setting, only correct arguments are observed but
their associated correct mention types are hidden (shaded).
arguments can be important and need to be properly
modeled. This motivates us to build a joint model
for extracting the event structures from the text.
We show a simplified graphical representation of
our model in Figure 2. In the graph, C1, C2 . . . Cn
refer to a particular segmentation of the event
span, where C1, C3 . . . correspond to mentions
(e.g., ?North Korea?s military?, ?a laser?) and C2,
C4 . . . correspond to in-between mention word se-
quences (we call them gaps) (e.g., ?may have
fired?). The symbols T1, T3 . . . refer to mention
types (e.g., GPE, ORG). The symbols A1, A3 . . . re-
fer to event arguments that carry specific roles (e.g.,
ATTACKER). We also introduce symbols B2, B4 . . .
to refer to inter-argument gaps. The event span is
split into segments, where each segment is either
linked to a mention type (Ti; these segments can
be referred to as ?argument segments?), or directly
linked to an inter-argument gap (Bj ; they can be
referred to as ?gap segments?). The two types of
segments appear in the sequence in a strictly alter-
nate manner, where the gaps can be of length zero.
In the figure, for example, the segments C1 and C3
are identified as two argument segments (which are
mentions of types T1 and T3 respectively) and are
mapped to two ?nodes?, and the segment C2 is iden-
tified as a gap segment that connects the two argu-
ments A1 and A3. Note that no overlapping argu-
ments are allowed in this model 1.
We use s to denote an event span and t to denote
a specific realization (filling) of the event template.
Templates consist of a set of arguments. Denote by h
a particular mention boundary and type assignment
for an event span, which gives us a specific segmen-
tation of the given span. Following the conditional
1Extending the model to support certain argument overlap-
ping is possible ? we leave it for future work.
836
random fields model (Lafferty et al, 2001), we pa-
rameterize the conditional probability of the (t, h)
pair given an event span s as follows:
P?(t, h|s) =
ef(s,h,t)??
?
t,h e
f(s,h,t)??
(1)
where f gives the feature functions defined on the
tuple (s, h, t), and ? defines the parameter vector.
Our objective function is the logarithm of the joint
conditional probability of observing the template re-
alization for the observed event span s:
L(?) =
?
i
logP?(ti|si)
=
?
i
log
?
h e
f(si,h,ti)??
?
t,h e
f(si,h,t)??
(2)
This function is not convex due to the summation
over the hidden variable h. To optimize it, we take
its partial derivative with respect to ?j :
?L(?)
??j
=
?
i
Ep?(h|si,ti)[fj(si, h, ti)]
?
?
i
Ep?(t,h|si)[fj(si, h, t)] (3)
which requires computation of expectations terms
under two different distributions. Such statistics
can be collected efficiently with a forward-backward
style algorithm in polynomial time (Okanohara et
al., 2006). We will discuss the time complexity for
our case in the next section.
Given its partial derivatives in Equation 3, one
could optimize the objective function of Equation 2
with stochastic gradient ascent (LeCun et al, 1998)
or L-BFGS (Liu and Nocedal, 1989). We choose to
use L-BFGS for all our experiments in this paper.
Inference involves computing the most probable
template realization t for a given event span:
arg max
t
P?(t|s) = arg max
t
?
h
P?(t, h|s) (4)
where the possible hidden assignments h need to be
marginalized out. In this task, a particular realiza-
tion t already uniquely defines a particular segmen-
tation (mention boundaries) of the event span, thus
the h only contributes type information to t. As we
will discuss in Section 2.3, only a collection of local
features are defined. Thus, a Viterbi-style dynamic
programming algorithm is used to efficiently com-
pute the desired solution.
2.1 Possible Segmentations
According to Equation 3, summing over all possi-
ble h is required. Since one primary assumption is
that we have access to the output of existing mention
identification and typing systems, the set of all possi-
ble mentions defines a lattice representation contain-
ing the set of all possible segmentations that com-
ply with such mention-level information. Assuming
there are A possible arguments for the event and K
annotated mentions, the complexity of the forward-
backward style algorithm is in O(A3K2) under the
?second-order? setting that we will discuss in Sec-
tion 2.2. Typically, K is smaller than the number of
words in the span, and the factor A3 can be regarded
as a constant. Thus, the algorithm is very efficient.
As we have mentioned earlier, such coarse infor-
mation, as produced by existing resources, could be
highly ambiguous and noisy. Also, the output men-
tions can highly overlap with each other. For exam-
ple, the phrase ?North Korea? as in ?North Korea?s
military? can be assigned both type GPE and LOC,
while ?North Korea?s military? can be assigned the
type ORG. Our model will need to disambiguate the
mention boundaries as well as their types.
2.2 The Gap Segments
We believe the gap segments2 are important to
model since they can potentially capture depen-
dencies between two or more adjacent arguments.
For example, the word sequence ?may have fired?
clearly indicates an Attacker-Instrument relation be-
tween the two mentions ?North Korea?s military?
and ?a laser?. Since we are only interested in
modeling dependencies between adjacent argument
segments, we assign hard labels to each gap seg-
ment based on its contextual argument informa-
tion. Specifically, the label of each gap segment
is uniquely determined by its surrounding argu-
ment segments with a list representation. For ex-
ample, in a ?first-order? setting, the gap segment
that appears between its previous argument seg-
ment ?ATTACKER? and its next argument segment
?INSTRUMENT? is annotated as the list consisting
of two elements: [ATTACKER, INSTRUMENT]. To
capture longer-range dependencies, in this work we
use a ?second-order? setting (as shown in Figure 2),
2The length of a gap segment is arbitrary (including zero),
unlike the seminal semi-Markov CRF model of Sarawagi and
Cohen (2004).
837
which means each gap segment is annotated with a
list that consists of its previous two argument seg-
ments as well as its subsequent one.
2.3 Features
Feature functions are factorized as products of two
indicator functions: one defined on the input se-
quence (input features) and the other on the output
labels (output features). In other words, we could
re-write fj(s, h, t) as f ink (s)? f
out
l (h, t).
For gap segments, we consider the following in-
put feature templates:
N-GRAM: Indicator function for n-gram appeared
in the segment (n = 1, 2)
ANCHOR: Indicator function for its relative position
to the event anchor words (to the left, to
the right, overlaps, contains)
and the following output feature templates:
1STORDER: Indicator function for the combination of
its immediate left argument and its imme-
diate right argument.
2NDORDER: Indicator function for the combination of
its immediate two left arguments and its
immediate right argument.
For argument segments, we also define the same
input feature templates as above, with the following
additional ones to capture contextual information:
CWORDS: Indicator function for the previous and
next k (= 1, 2, 3) words.
CPOS: Indicator function for the previous and
next k (= 1, 2, 3) words? POS tags.
and we define the following output feature template:
ARGTYPE: Indicator function for the combination of
the argument and its associated type.
Although the semi-Markov CRF model gives us
the flexibility in introducing features that can not be
exploited in a standard CRF, such as entity name
similarity scores and distance measures, in prac-
tice we found the above simple and general features
work well. This way, the unnormalized score as-
signed to each structure is essentially a linear sum
of the feature weights, each corresponding to an in-
dicator function.
3 Learning without Annotated Data
The supervised model presented in the previous sec-
tion requires substantial human efforts to annotate
the training instances. Human annotations can be
very expensive and sometimes impractical. Even if
annotators are available, getting annotators to agree
with each other is often a difficult task in itself.
Worse still, annotations often can not be reused: ex-
perimenting on a different domain or dataset typi-
cally require annotating new training instances for
that particular domain or dataset.
We investigate inexpensive methods to alleviate
this issue in this section. We introduce a novel gen-
eral learning framework called structured preference
modeling, which allows arbitrary prior knowledge
about structures to be introduced to the learning pro-
cess in a declarative manner.
3.1 Structured Preference Modeling
Denote by X? and Y? the entire input and output
space, respectively. For a particular input x ? X?,
the set x ? Y? gives us all possible structures that
contain x. However, structures are not equally good.
Some structures are generally regarded as better
structures while some are worse.
Let?s asume there is a function ? :
{
x ? Y? ?
[0, 1]
}
that measures the quality of the structures.
This function returns the quality of a certain struc-
ture (x, y), where the value 1 indicates a perfect
structure, and 0 an impossible structure.
Under such an assumption, it is easy to observe
that for a good structure (x, y), we have p?(x, y)?
?(x, y) = p?(x, y), while for a bad structure (x, y),
we have p?(x, y)? ?(x, y) = 0.
This motivates us to optimize the following objec-
tive function:
Lu(?) =
?
i
log
?
y p?(xi, y)? ?(xi, y)
?
y p?(xi, y)
(5)
Intuitively, optimizing such an objective function
is equivalent to pushing the probability mass from
bad structures to good structures corresponding to
the same input.
When the preference function ? is defined as the
indicator function for the correct structure (xi, yi),
the numerator terms of the above formula are simply
of the forms p?(xi, yi), and the model corresponds
to the fully supervised CRF model.
The model also contains the latent-variable CRF
as a special case. In a latent-variable CRF, we have
input-output pairs (xi, yi), but the underlying spe-
cific structure h that contains both xi and yi is hid-
den. The objective function is:
?
i
log
?
h p?(xi, h, yi)?
h,y? p?(xi, h, y
?)
(6)
838
where p?(xi, h, yi) = 0 unless h contains (xi, yi).
We define the following two functions:
q?(xi, h) =
?
y?
p?(xi, h, y
?) (7)
?(xi, h) =
{
1 h contains (xi, yi)
0 otherwise
(8)
Note that this definition of ? models instance-
specific preferences since it relies on yi, which can
be thought of as certain external prior knowledge re-
lated to xi. It is easy to verify that p?(xi, h, yi) =
q?(xi, h)??(xi, h), with q? remains a distribution.
Thus, we could re-write the objective function as:
?
i=1
log
?
h q?(xi, h)? ?(xi, h)?
h q?(xi, h)
(9)
This shows that the latent-variable CRF is a spe-
cial case of our objective function, with the above-
defined ? function. Thus, this new objective func-
tion of Equation 5 is a generalization of both the su-
pervised CRF and the latent-variable CRF.
The preference function ? serves as a source from
which certain prior knowledge about the structure
can be injected into our model in a principled way.
Note that the function is defined at the complete
structure level. This allows us to incorporate both
local and arbitrary global structured information into
the preference function.
Under the log-linear parameterization, we have:
L?(?) =
?
i
log
?
y e
f(xi,y)?? ? ?(xi, y)
?
y e
f(xi,y)??
(10)
This is again a non-convex optimization problem
in general, and to solve it we take its partial deriva-
tive with respect to ?k:
?L?(?)
??k
=
?
i
Ep?(y|xi;?)[fk(xi, y)]
?
?
i
Ep?(y|xi)[fk(xi, y)] (11)
p?(y|xi;?) ? e
f(xi,y)?? ? ?(xi, y)
p?(y|xi) ? e
f(xi,y)??
3.2 Approximate Learning
Computation of the denominator terms of Equation
10 (and the second term of Equation 11) can be done
efficiently and exactly with dynamic programming.
Our main concern is the computation of its numera-
tor terms (and the first term of Equation 11).
The preference function ? is defined at the com-
plete structure level. Unless the function is defined
in specific forms that allow tractable dynamic pro-
gramming (in the supervised case, which gives a
unique term, or in the hidden variable case, which
can define a packed representations of derivations),
the efficient dynamic programming algorithm used
by CRF is no longer generally applicable for arbi-
trary ?. In general, we resort to approximations.
In this work, we exploit a specific form of the
preference function ?. We assume that there exists
a projection from another decomposable function to
?. Specifically, we assume a collection of auxiliary
functions, each of the form ?p : (x, y) ? R, that
scores a property p of the complete structure (x, y).
Each such function measures certain aspect of the
quality of the structure. These functions assign pos-
itive scores to good structural properties and nega-
tive scores to bad ones. We then define ?(x, y) = 1
for all structures that appear at the top-n positions
as ranked by
?
p ?p(x, y) for all possible y?s, and
?(x, y) = 0 otherwise. We show some actual ?p
functions used for a particular event in Section 5.
At each iteration of the training process, to gen-
erate such a n-best list, we first use our model to
produce top n ? b candidate outputs as scored by
the current model parameters, and extract the top n
outputs as scored by
?
p ?p(x, y). In practice we set
n = 10 and b = 1000.
3.3 Event Extraction
Now we can obtain the objective function for our
event extraction task. We replace x by s and y by
(h, t) in Equation 10. This gives us the following
function:
Lu(?) =
?
i
log
?
t,h e
f(si,h,t)?? ? ?(si, h, t)
?
t,h e
f(si,h,t)??
(12)
The partial derivatives are as follows:
?Lu(?)
??k
=
?
i
Ep?(t,h|si;?)[fk(si, h, t)]
?
?
i
Ep?(t,h|si)[fk(si, h, t)] (13)
p?(t, h|si;?) ? e
f(si,h,t)?? ? ?(si, h, t)
p?(t, h|si) ? e
f(si,h,t)??
839
Recall that s is an event span, t is a specfic re-
alization of the event template, and h is the hidden
mention information for the event span.
4 Discussion: Preferences v.s. Constraints
Note that the objective function in Equation 5, if
written in the additive form, leads to a cost func-
tion reminiscent of the one used in constraint-driven
learning algorithm (CoDL) (Chang et al, 2007) (and
similarly, posterior regularization (Ganchev et al,
2010), which we will discuss later at Section 6).
Specifically, in CoDL, the following cost function
is involved in its EM-like inference procedure:
arg max
y
? ? f(x, y)? ?
?
c
d(y,Yc) (14)
where Yc defines the set of y?s that all satisfy a cer-
tain constraint c, and d defines a distance function
from y to that set. The parameter ? controls the de-
gree of the penalty when constraints are violated.
There are some important distinctions between
structured preference modeling (PM) and CoDL.
CoDL primarily concerns constraints, which pe-
nalizes bad structures without explicitly rewarding
good ones. On the other hand, PM concerns prefer-
ences, which can explicitly reward good structures.
Constraints are typically useful when one works
on structured prediction problems for data with cer-
tain (often rigid) regularities, such as citations, ad-
vertisements, or POS tagging for complete sen-
tences. In such tasks, desired structures typically
present certain canonical forms. This allows declar-
ative constraints to be specified as either local struc-
ture prototypes (e.g., in citation extraction, the word
pp. always corresponds to the PAGES field, while
proceedings is always associated with BOOKTITLE
or JOURNAL), or as certain global regulations about
complete structures (e.g., at least one word should
be tagged as verb when performing a sentence-level
POS tagging).
Unfortunately, imposing such (hard or soft) con-
straints for certain tasks such as ours, where the data
tends to be of arbitrary forms without many rigid
regularities, can be difficult and often inappropri-
ate. For example, there is no guarantee that a cer-
tain argument will always be present in the event
span, nor should a particular mention, if appeared,
always be selected and assigned to a specific argu-
ment. For example, in the example event span given
in Section 1, both ?March? and ?Tuesday? are valid
candidate mentions for the TIME-WITHIN argument
given their annotated type TME. One important clue
is that March appears after the word in and is lo-
cated nearer to other mentions that can be poten-
tially useful arguments. However, encoding such
information as a general constraint can be inappro-
priate, as potentially better structures can be found
if one considers other alternatives. On the other
hand, if we believe the structural pattern ?at TAR-
GET in TIME-WITHIN? is in general considered a
better sub-structure than ?said TIME-WITHIN? for
the ?Attack? event, we may want to assign structured
preference to a complete structure that contains the
former, unless there exist other structured evidence
showing the latter turns out to be better.
In this work, our preference function is related
to another function that can be decomposed into a
collection of property functions ?p. Each of them
scores a certain aspect of the complete structure.
This formulation gives us a complete flexibility to
assign arbitrary structured preferences, where posi-
tive scores can be assigned to good properties, and
negative scores to bad ones. Thus, in this way, the
quality of a complete structure is jointly measured
with multiple different property functions.
To summarize, preferences are an effective way to
?define? the event structure to the learner, which is
essential in an unsupervised setting, which may not
be easy to do with other forms of constraints. Prefer-
ences are naturally decomposable, which allows us
to extend their impact without significantly effecting
the complexity of inference.
5 Experiments
In this section, we present our experimental results
on the standard ACE053 dataset (newswire portion).
We choose to perform our evaluations on 4 events
(namely, ?Attack?, ?Meet?, ?Die? and ?Transport?),
which are the only events in this dataset that have
more than 50 instances. For each event, we ran-
domly split the instances into two portions, where
70% are used for learning, and the remaining 30%
for evaluation. We list the corpus statistics in Table
2.
To present general results while making minimal
assumptions, our primary event extraction results
3http://www.itl.nist.gov/iad/mig/tests/ace/2005/doc/
840
Event
Without Annotated Training Data With Annotated Training Data
Random Unsup Rule PM MaxEnt-b MaxEnt-t MaxEnt-p semi-CRF
Attack 20.47 30.12 39.25 42.02 54.03 58.82 65.18 63.11
Meet 35.48 26.09 44.07 63.55 65.42 70.48 75.47 76.64
Die 30.03 13.04 40.58 55.38 51.61 59.65 63.18 67.65
Transport 20.40 6.11 44.34 57.29 53.76 57.63 61.02 64.19
Table 1: Performance for different events under different experimental settings, with gold mention boundaries and types. We report
F1-measure percentages.
Event #A
Learning Set Evaluation Set
#P
#I #M #I #M
Attack 8 188 300/509 78 121/228 7
Meet 7 57 134/244 24 52/98 7
Die 9 41 89/174 19 33/61 6
Transport 13 85 243/426 38 104/159 6
Table 2: Corpus statistics (#A: number of possible arguments
for the event; #I: number of instances; #M: number of ac-
tive/total mentions; #P: number of preference patterns used
for performing our structured preference modeling.)
are independent of mention identification and typing
modules, which are based on the gold mention in-
formation as given by the dataset. Additionally, we
present results obtained by exploiting our in-house
automatic mention identification and typing mod-
ule, which is a hybrid system that combines statis-
tical and rule-based approaches. The module?s sta-
tistical component is trained on the ACE04 dataset
(newswire portion) and overall it achieves a micro-
averaged F1-measure of 71.25% at our dataset.
5.1 With Annotated Training Data
With hand-annotated training data, we are able to
train our model in a fully supervised manner. The
right part of Table 1 shows the performance for
the fully supervised models. For comparison, we
present results from several alternative approaches
based a collection of locally trained maximum en-
tropy (MaxEnt) classifiers. In these approaches, we
treat each argument of the template as one possi-
ble output class, plus a special ?NONE? class for
not selecting it as an argument. We train and apply
the classifiers on argument segments (i.e., mentions)
only. All the models are trained with the same fea-
ture set used in the semi-CRF model.
In the simplest baseline approach MaxEnt-b, type
information for each mention is simply treated as
one special feature. In the approach MaxEnt-t, we
instead use the type information to constrain the
classifier?s predictions based on the acceptable types
associated with each argument. This approach gives
better performance than that of MaxEnt-b. This in-
dicates that such locally trained classifiers are not
robust enough to disambiguate arguments that take
different types. As such, type information serving as
additional constraints at the end does help.
To assess the importance of structured preference,
we also perform experiments where structured pref-
erence information is incorporated at the inference
time of the MaxEnt classifiers. Specifically, for each
event, we first generate n-best lists for output struc-
tures. Next, we re-rank this list based on scores
from our structured preference functions (we used
the same preferences as to be discussed in the next
section). The results for these approaches are given
in the column of MaxEnt-p of Table 1. This simple
approach gives us significant improvements, clos-
ing the gap between locally trained classifiers and
the joint model (in one case the former even out-
performs the latter). Note that no structured pref-
erence information is used when training and eval-
uating our semi-CRF model. This set of results is
not surprising. In fact, similar observations are also
reported in previous works when comparing joint
model against local models with constraints incor-
porated (Roth and Yih, 2005). This clearly indicates
that structured preference information is crucial to
model.
5.2 Without Annotated Training Data
Now we turn to experiments for the more realistic
scenario where human annotations are not available.
We first build our simplest baseline by randomly
assigning arguments to each mention with mention
type information serving as constraints. Averaged
results over 1000 runs are reported in the first col-
umn of Table 1.
Since our model formulation leaves us with com-
plete freedom in designing the preference function,
841
Type Preference pattern (p)
General
{at|in|on} followed by PLACE
{during|at|in|on} followed by TIME-WITHIN
Die
AGENT (immediately) followed by {killed}
{killed} (immediately) followed by VICTIM
VICTIM (immediately) followed by {be killed}
AGENT followed by {killed} (immediately) followed by VICTIM
Transport
X immediately followed by {,|and} immediately followed by X, where X ? {ORIGIN|DESTINATION}
{from|leave} (immediately) followed by ORIGIN
{at|in|to|into} immediately followed by DESTINATION
PERSON followed by {to|visit|arrived}
Figure 3: The complete list of preference patterns used for the ?Die? and ?Transport? event. We simply set ?p = 1.0 for all p?s. In
other words, when a structure contains a pattern, its score is incremented by 1.0. We use {} to refer to a set of possible words or
arguments. For example, {from|leave} means a word which is either from or leave. The symbol () denotes optional. For example,
?{killed} (immediately) followed by VICTIM? is equivalent to the following two preferences: ?{killed} immediately followed by
VICTIM?, and ?{killed} followed by VICTIM?.
one could design arbitrarily good, domain-specific
or even instance-specific preferences. However, to
demonstrate its general effectiveness, in this work
we only choose a minimal amount of general prefer-
ence patterns for evaluations.
We make our preference patterns as general as
possible. As shown in the last column (#P) of Table
2, we use only 7 preference patterns each for the ?At-
tack? and ?Meet? events, and 6 patterns each for the
other two events. In Figure 3, we show the complete
list of the 6 preference patterns for the ?Die? and
?Transport? event used for our experiments. Out of
those 6 patterns, 2 are more general patterns shared
across different events, and 4 are event-specific. In
contrast, for example, for the ?Die? event, the super-
vised approach requires human to select from 174
candidate mentions and annotate 89 of them.
Despite its simplicity, it works very well in prac-
tice. Results are given in the column of ?PM? of
Table 1. It generally gives competitive performance
as compared to the supervised MaxEnt baselines.
On the other hand, a completely unsupervised ap-
proach where structured preferences are not speci-
fied, performs substantially worse. To run such com-
pletely unsupervised models, we essentially follow
the same training procedure as that of the prefer-
ence modeling, except that structured preference in-
formation is not in place when generating the n-best
list. In the absence of proper guidances, such a pro-
cedure can easily converge to bad local minima. The
results are reported in the ?Unsup? column of Ta-
ble 1. In practice, we found that very often, such
a model would prefer short structures where many
mentions are not selected as desired. As a result, the
unsupervised model without preference information
can even perform worse than the random baseline 4.
Finally, we also compare against an approach that
regards the preferences as rules. All such rules are
associated with a same weight and are used to jointly
score each structure. We then output the structure
that is assigned the highest total weight. Such an ap-
proach performs worse than our approach with pref-
erence modeling. The results are presented in the
column of ?Rule? of Table 1. This indicates that
our model is able to learn to generalize with features
through the guidance of our informative preferences.
However, we also note that the performance of pref-
erence modeling depends on the actual quality and
amount of preferences used for learning. In the ex-
treme case, where only few preferences are used, the
performance of preference modeling will be close to
that of the unsupervised approach, while the rule-
based approach will yield performance close to that
of the random baseline.
The results with automatically predicted mention
boundaries and types are given in Table 3. Simi-
lar observations can be made when comparing the
performance of preference modeling with other ap-
proaches. This set of results further confirms the ef-
fectiveness of our approach using preference model-
ing for the event extraction task.
6 Related Work
Structured prediction with limited supervision is a
popular topic in natural language processing.
4For each event, we only performed 1 run with all the initial
feature weights set to zeros.
842
Event Random Unsup PM semi-CRF
Attack 14.26 26.19 32.89 46.92
Meet 26.65 14.08 45.28 58.18
Die 19.17 9.09 44.44 48.57
Transport 15.78 10.14 49.73 52.34
Table 3: Event extraction performance with automatic mention
identifier and typer. We report F1 percentage scores for pref-
erence modeling (PM) as well as two baseline approaches. We
also report performance of the supervised approach trained with
the semi-CRF model for comparison.
Prototype driven learning (Haghighi and Klein,
2006) tackled the sequence labeling problem in a
primarily unsupervised setting. In their work, a
Markov random fields model was used, where some
local constraints are specified via their prototype list.
Constraint-driven learning (CoDL) (Chang et al,
2007) and posterior regularization (PR) (Ganchev et
al., 2010) are both primarily semi-supervised mod-
els. They define a constrained EM framework that
regularizes posterior distribution at the E-step of
each EM iteration, by pushing posterior distributions
towards a constrained posterior set. We have already
discussed CoDL in Section 4 and gave a comparison
to our model. Unlike CoDL, in the PR framework
constraints are relaxed to expectation constraints, in
order to allow tractable dynamic programming. See
also Samdani et al (2012) for more discussions.
Contrastive estimation (CE) (Smith and Eisner,
2005a) is another log-linear framework for primar-
ily unsupervised structured prediction. Their objec-
tive function is related to the pseudolikelihood es-
timator proposed by Besag (1975). One challenge
is that it requires one to design a priori an effective
neighborhood (which also needs to be designed in
certain forms to allow efficient computation of the
normalization terms) in order to obtain optimal per-
formance. The model has been shown to work in un-
supervised tasks such as POS induction (Smith and
Eisner, 2005a), grammar induction (Smith and Eis-
ner, 2005b), and morphological segmentation (Poon
et al, 2009), where good neighborhoods can be
identified. However, it is less intuitive what consti-
tutes a good neighborhood in this task.
The neighborhood assumption of CE is relaxed
in another latent structure approach (Chang et al,
2010a; Chang et al, 2010b) that focuses on semi-
supervised learning with indirect supervisions, in-
spired by the CoDL model described above.
The locally normalized logistic regression (Berg-
Kirkpatrick et al, 2010) is another recently proposed
framework for unsupervised structured prediction.
Their model can be regarded as a generative model
whose component multinomial is replaced with a
miniature logistic regression where a rich set of local
features can be incorporated. Empirically the model
is effective in various unsupervised structured pre-
diction tasks, and outperforms the globally normal-
ized model. Although modeling the semi-Markov
properties of our segments (especially the gap seg-
ments) in our task is potentially challenging, we plan
to investigate in the future the feasibility for our task
with such a framework.
7 Conclusions
In this paper, we present a novel model based on
the semi-Markov conditional random fields for the
challenging event extraction task. The model takes
in coarse mention boundary and type information
and predicts complete structures indicating the cor-
responding argument role for each mention.
To learn the model in an unsupervised manner,
we further develop a novel learning approach called
structured preference modeling that allows struc-
tured knowledge to be incorporated effectively in a
declarative manner.
Empirically, we show that knowledge about struc-
tured preference is crucial to model and the prefer-
ence modeling is an effective way to guide learn-
ing in this setting. Trained in a primarily unsuper-
vised manner, our model incorporating structured
preference information exhibits performance that is
competitive to that of some supervised baseline ap-
proaches. Our event extraction system and code will
be available for download from our group web page.
Acknowledgments
We would like to thank Yee Seng Chan, Mark Sam-
mons, and Quang Xuan Do for their help with the
mention identification and typing system used in
this paper. We gratefully acknowledge the sup-
port of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the view of DARPA,
AFRL, or the US government.
843
References
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of HLT-NAACL?10, pages 582?590.
J. Besag. 1975. Statistical analysis of non-lattice data.
The Statistician, pages 179?195.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of ACL?07, pages 280?287.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained latent
representations. In Proc. of NAACL?10, 6.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010b. Structured output learning with indirect super-
vision. In Proc. ICML?10.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. The Journal of Machine Learning
Research (JMLR), 11:2001?2049.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. of HLT-NAACL?06,
pages 320?327.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. of
ICML?01, pages 282?289.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recogni-
tion. Proc. of the IEEE, pages 2278?2324.
D.C. Liu and J. Nocedal. 1989. On the limited memory
bfgs method for large scale optimization. Mathemati-
cal programming, 45(1):503?528.
D. Okanohara, Y. Miyao, Y. Tsuruoka, and J. Tsujii.
2006. Improving the scalability of semi-markov con-
ditional random fields for named entity recognition. In
Proc. of ACL?06, pages 465?472.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsu-
pervised morphological segmentation with log-linear
models. In Proc. of HLT-NAACL?09, pages 209?217.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of CoNLL?09, pages 147?155.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of ACL-HLT?11, pages 1375?
1384.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In Proc. of
ICML?05, pages 736?743.
R. Samdani, M. Chang, and D. Roth. 2012. Unified ex-
pectation maximization. In Proc. NAACL?12.
S. Sarawagi and W.W. Cohen. 2004. Semi-markov
conditional random fields for information extraction.
NIPS?04, pages 1185?1192.
N.A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL?05, pages 354?362.
N.A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In
Proc. of IJCAI Workshop on Grammatical Inference
Applications, pages 73?82.
J. Stro?tgen and M. Gertz. 2010. Heideltime: High qual-
ity rule-based extraction and normalization of tempo-
ral expressions. In Proc. of SemEval?10, pages 321?
324.
844
