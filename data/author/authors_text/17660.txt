Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1377?1381,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Statistical Machine Translation with Word Class Models
Joern Wuebker, Stephan Peitz, Felix Rietig and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
Automatically clustering words from a mono-
lingual or bilingual training corpus into
classes is a widely used technique in statisti-
cal natural language processing. We present
a very simple and easy to implement method
for using these word classes to improve trans-
lation quality. It can be applied across differ-
ent machine translation paradigms and with
arbitrary types of models. We show its ef-
ficacy on a small German?English and a
larger French?German translation task with
both standard phrase-based and hierarchical
phrase-based translation systems for a com-
mon set of models. Our results show that with
word class models, the baseline can be im-
proved by up to 1.4% BLEU and 1.0% TER
on the French?German task and 0.3% BLEU
and 1.1% TER on the German?English task.
1 Introduction
Data sparsity is one of the major problems for statis-
tical learning methods in natural language process-
ing (NLP) today. Even with the huge training data
sets available in some tasks, for many phenomena
that need to be modeled only few training instances
can be observed. This is partly due to the large vo-
cabularies of natural languages. One possiblity to
reduce the sparsity for model estimation is to re-
duce the vocabulary size. By clustering the vocab-
ulary into a fixed number of word classes, it is pos-
sible to train models that are less prone to sparsity
issues. This work investigates the performance of
standard models used in statistical machine transla-
tion when they are trained on automatically learned
word classes rather than the actual word identities.
In the popular tooklit GIZA++ (Och and Ney,
2003), word classes are an essential ingredient to
model alignment probabilities with the HMM or
IBM translation models. It contains the mkcls tool
(Och, 1999), which can automatically cluster the vo-
cabulary into classes.
Using this tool, we propose to re-parameterize the
standard models used in statistical machine transla-
tion (SMT), which are usually conditioned on word
identities rather than word classes. The idea is that
this should lead to a smoother distribution, which
is more reliable due to less sparsity. Here, we fo-
cus on the phrase-based and lexical channel models
in both directions, simple count models identifying
frequency thresholds, lexicalized reordering models
and an n-gram language model. Although our re-
sults show that it is not a good idea to replace the
original models, we argue that adding them to the
log-linear feature combination can improve transla-
tion quality. They can easily be computed for dif-
ferent translation paradigms and arbitrary models.
Training and decoding is possible without or with
only little change to the code base.
Our experiments are conducted on a medium-
sized French?German task and a small
German?English task and with both phrase-
based and hierarchical phrase-based translation
decoders. By using word class models, we can
improve our respective baselines by 1.4% BLEU and
1.0% TER on the French?German task and 0.3%
BLEU and 1.1% TER on the German?English task.
Training an additional language model for trans-
1377
lation based on word classes has been proposed in
(Wuebker et al, 2012; Mediani et al, 2012; Koehn
and Hoang, 2007). In addition to the reduced spar-
sity, an advantage of the smaller vocabulary is that
longer n-gram context can be modeled efficiently.
Mathematically, our idea is equivalent to a special
case of the Factored Translation Models proposed
by Koehn and Hoang (2007). We will go into more
detail in Section 4. Also related to our work, Cherry
(2013) proposes to parameterize a hierarchical re-
ordering model with sparse features that are condi-
tioned on word classes trained with mkcls. How-
ever, the features are trained with MIRA rather than
estimated by relative frequencies.
2 Word Class Models
2.1 Standard Models
The translation model of most phrase-based and hi-
erarchical phrase-based SMT systems is parameter-
ized by two phrasal and two lexical channel models
(Koehn et al, 2003) which are estimated as relative
frequencies. Their counts are extracted heuristically
from a word aligned bilingual training corpus.
In addition to the four channel models, our base-
line contains binary count features that fire, if the
extraction count of the corresponding phrase pair is
greater or equal to a given threshold ? . We use the
thresholds ? = {2, 3, 4}.
Our phrase-based baseline contains the hierarchi-
cal reordering model (HRM) described by Galley
and Manning (2008). Similar to (Cherry et al,
2012), we apply it in both translation directions
with separate scaling factors for the three orientation
classes, leading to a total of six feature weights.
An n-gram language model (LM) is another im-
portant feature of our translation systems. The
baselines apply 4-gram LMs trained by the SRILM
toolkit (Stolcke, 2002) with interpolated modified
Kneser-Ney smoothing (Chen and Goodman, 1998).
The smaller vocabulary size allows us to efficiently
model larger context, so in addition to the 4-gram
LM, we also train a 7-gram LM based on word
classes. In contrast to an LM of the same size trained
on word identities, the increase in computational re-
sources needed for translation is negligible for the
7-gram word class LM (wcLM).
2.2 Training
By replacing the words on both source and target
side of the training data with their respective word
classes and keeping the word alignment unchanged,
all of the above models can easily be trained con-
ditioned on word classes by using the same training
procedure as usual. We end up with two separate
model files, usually in the form of large tables, one
with word identities and one with classes. Next, we
sort both tables by their word classes. By walking
through both sorted tables simultaneously, we can
then efficiently augment the standard model file with
an additonal feature (or additional features) based on
word classes. The word class LM is directly passed
on to the decoder.
2.3 Decoding
The decoder searches for the best translation given
a set of models hm(eI1, s
K
1 , f
J
1 ) by maximizing the
log-linear feature score (Och and Ney, 2004):
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
, (1)
where fJ1 = f1 . . . fJ is the source sentence, e
I
1 =
e1 . . . eI the target sentence and sK1 = s1 . . . sK the
hidden alignment or derivation.
All the above mentioned models can easily be in-
tegrated into this framework as additional features
hm. The feature weights ?m are tuned with mini-
mum error rate training (MERT) (Och, 2003).
3 Experiments
3.1 Data
Our experiments are performed on a
French?German task. In addition to some
project-internal data, we train the system on the data
provided for the WMT 2012 shared task1. Both the
dev and the test set are composed of a mixture
of broadcast news and broadcast conversations
crawled from the web and have two references.
Table 1 shows the data statistics.
To confirm our results we also run experiments
on the German?English task of the IWSLT 2012
evaluation campaign2.
1http://www.statmt.org/wmt12/
2http://hltc.cs.ust.hk/iwslt/
1378
French German
train Sentences 1.9M
Running Words 57M 50M
dev Sentences 1900
Running Words 61K 55K
test Sentences 2037
Running Words 60K 54K
Table 1: Corpus statistics for the French?German task.
The running word counts for the German side of dev and
test are averaged over both references.
3.2 Setup
In the French?German task, our baseline is a stan-
dard phrase-based system augmented with the hier-
archical reordering model (HRM) described in Sec-
tion 2.1. The language model is a 4-gram LM
trained on all German monolingual sources provided
for WMT 2012. For the class-based models, we
run mkcls on the source and target side of the
bilingual training data to cluster the vocabulary into
100 classes each. This clustering is used to train
the models described above for word classes on the
same training data as their counterparts based on
word identity. This also holds for the wcLM, which
is a 4-gram LM trained on the same data as the base-
line LM. Further, the smaller vocabulary allows us
to build an additional wcLM with a 7-gram context
length. On this task we also run additional experi-
ments with 200 and 500 classes.
On the German?English task, we evaluate our
method for both a standard phrase-based and the hi-
erarchical phrase-based baseline. Again, the phrase-
based baseline contains the HRM model. As bilin-
gual training data we use the TED talks, which we
cluster into 100 classes on both source and target
side. The 4-gram LM is trained on the TED, Eu-
roparl and news-commentary corpora. On this data
set, we directly use a 7-gram wcLM.
In all setups, the feature weights are optimized
with MERT. Results are reported in BLEU (Pap-
ineni et al, 2002) and TER (Snover et al, 2006),
confidence level computation is based on (Koehn,
2004). Our experiments are conducted with the open
source toolkit Jane (Wuebker et al, 2012; Vilar et
al., 2010).
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
-TM +wcTM 21.2 64.2 24.7 59.5
-LM +wcLM 22.2 62.9 25.9 58.9
-HRM +wcHRM 24.6 61.9 27.5 58.1
phrase-based 24.6 61.8 27.8 57.6
+ wcTM 24.7 61.4 28.1 57.1
+ wcLM 24.9 61.2 28.4 57.1
+ wcHRM 25.4? 60.9? 28.9? 56.9?
+ wcLM7 25.5? 60.7? 29.2? 56.6?
+ wcModels200 25.5? 60.8? 29.3? 56.4?
+ wcModels500 25.2? 60.8? 29.0? 56.6?
Table 2: BLEU and TER results on the French?German
task. Results marked with ? are statistically significant
with 95% confidence, results marked with ? with 90%
confidence. -X +wcX denote the systems, where the
model X in the baseline is replaced by its word class
counterpart. The 7-gram word class LM is denoted
as wcLM7. wcModelsX denotes all word class models
trained on X classes.
3.3 Results
Results for the French?German task are given in
Table 2. In a first set of experiments we replaced one
of the standard TM, LM and HRM models by the
same model based on word classes. Unsurprisingly,
this degrades performance with different levels of
severity. The strongest degradation can be seen
when replacing the TM, while replacing the HRM
only leads to a small drop in performance. However,
when the word class models are added as additional
features to the baseline, we observe improvements.
The wcTM yields 0.3% BLEU and 0.5% TER on
test. By adding the 4-gram wcLM, we get another
0.3% BLEU and the wcHRM shows further improve-
ments of 0.5% BLEU and 0.2% TER. Extending the
context length of the wcLM to 7-grams gives an ad-
ditional boost, reaching a total gain over the baseline
of 1.4% BLEU and 1.0% TER. Using 200 classes
instead of 100 seems to perform slightly better on
test, but with 500 classes, translation quality de-
grades again.
On the German?English task, the results shown
in Table 3 are similar in TER, but less pronounced
in BLEU. Here we are able to improve over the
phrase-based baseline by 0.3% BLEU and 1.1% TER
1379
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
phrase-based 30.2 49.6 28.6 51.6
+ wcTM 30.2 49.2 28.9 51.3
+ wcLM7 30.5 48.3? 29.0 50.6?
+ wcHRM 30.8 48.3? 28.9 50.5?
hiero 29.6 50.3 27.9 52.5
+ wcTM 29.8 50.3 28.1 52.3
+ wcLM7 30.0 49.8 28.2 51.7
Table 3: BLEU and TER results on the German?English
task. Results marked with ? are statistically significant
with 95% confidence, results marked with ? with 90%
confidence.
by adding the wcTM, the 7-gram wcLM and the
wcHRM. With the hierarchical decoder we gain
0.3% BLEU and 0.8% TER by adding the wcTM and
the 7-gram wcLM.
4 Equivalence to Factored Translation
Koehn and Hoang (2007) propose to integrate differ-
ent levels of annotation (e.g. morphologial analysis)
as factors into the translation process. Here, the sur-
face form of the source word is analyzed to produce
the factors, which are then translated and finally the
surface form of the target word is generated from the
target factors. Although the translations of the fac-
tors operate on the same phrase segmentation, they
are assumed to be independent. In practice this is
done by phrase expansion, which generates a joint
phrase table as the cross product from the phrase ta-
bles of the individual factors.
In contrast, in this work each word is mapped to
a single class, which means that when we have se-
lected a translation option for the surface form, the
target side on the word class level is predetermined.
Thus, no phrase expansion or generation steps are
necessary to incorporate the word class information.
The phrase table can simply be extended with addi-
tional scores, keeping the set of phrases constant.
Although the implementation is simpler, our ap-
proach is mathematically equivalent to a special
case of the factored translation framework, which is
shown in Figure 1. The generation step from target
word e to its target class c(e) assigns all probability
Input Output
word f word e
class c(f) class c(e)
analysis
translation
translation
generation   
Figure 1: The factored translation model equivalent to
our approach. The generation step assigns all probability
mass to a single event: pgen(c(e)|e) = 1.
mass to a single event:
pgen(c|e) =
{
1, if c = c(e)
0, else
(2)
5 Conclusion
We have presented a simple and very easy to im-
plement method to make use of word clusters for
improving machine translation quality. It is appli-
cable across different paradigms and for arbitrary
types of models. Depending on the model type,
it requires little or no change to the training and
decoding software. We have shown the efficacy
of this method on two translation tasks and with
both the standard phrase-based and the hierarchi-
cal phrase-based translation paradigm. It was ap-
plied to relative frequency translation probabilities,
the n-gram language model and a hierarchical re-
ordering model. In our experiments, the baseline
is improved by 1.4% BLEU and 1.0% TER on the
French?German task and by 0.3% BLEU and 1.1%
TER on the German?English task.
In future work we plan to apply our method to a
wider range of languages. Intuitively, it should be
most effective for morphologically rich languages,
which naturally have stronger sparsity problems.
Acknowledgments
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French State
agency for innovation. The research leading to these
results has also received funding from the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement no 287658.
1380
References
Stanley F. Chen and Joshuo Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, MA,
August.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On Hierarchical Re-ordering and Permutation Parsing
for Phrase-based Decoding. In Proceedings of the 7th
Workshop on Statistical Machine Translation, WMT
?12, pages 200?209, Montral, Canada.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In The 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2013), pages 22?
31, Atlanta, Georgia, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, USA, October.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 868?876, Prague, Czech Republic, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of the 2003
Meeting of the North American chapter of the Associa-
tion for Computational Linguistics (NAACL-03), pages
127?133, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of the Conf.
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunah Cho, Teresa Herrmann, and Alex
Waibel. 2012. The kit translation systems for iwslt
2012. In Proceedings of the International Work-
shop for Spoken Language Translation (IWSLT 2012),
Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
F. J. Och. 1999. An efficient method for determining
bilingual word classes. In Proc. of the Ninth Conf.
of the Europ. Chapter of the Association of Compu-
tational Linguistics, pages 71?76, Bergen, Norway,
June.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf. on
Speech and Language Processing (ICSLP), volume 2,
pages 901?904, Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open source hierarchical transla-
tion, extended with reordering and lexicon models. In
ACL 2010 Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 262?270, Upp-
sala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-
sour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mumbai,
India, December.
1381
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 452?463,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Phrase Orientation Model for Hierarchical Machine Translation
Matthias Huck and Joern Wuebker and Felix Rietig and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{huck,wuebker,rietig,ney}@i6.informatik.rwth-aachen.de
Abstract
We introduce a lexicalized reordering
model for hierarchical phrase-based ma-
chine translation. The model scores mono-
tone, swap, and discontinuous phrase ori-
entations in the manner of the one pre-
sented by Tillmann (2004). While this
type of lexicalized reordering model is a
valuable and widely-used component of
standard phrase-based statistical machine
translation systems (Koehn et al, 2007), it
is however commonly not employed in hi-
erarchical decoders.
We describe how phrase orientation prob-
abilities can be extracted from word-
aligned training data for use with hierar-
chical phrase inventories, and show how
orientations can be scored in hierarchi-
cal decoding. The model is empirically
evaluated on the NIST Chinese?English
translation task. We achieve a signifi-
cant improvement of +1.2 %BLEU over
a typical hierarchical baseline setup and
an improvement of +0.7 %BLEU over a
syntax-augmented hierarchical setup. On
a French?German translation task, we
obtain a gain of up to +0.4 %BLEU.
1 Introduction
In hierarchical phrase-based translation (Chiang,
2005), a probabilistic synchronous context-free
grammar (SCFG) is induced from bilingual train-
ing corpora. In addition to continuous lexical
phrases as in standard phrase-based translation,
hierarchical phrases with usually up to two non-
terminals are extracted from the word-aligned par-
allel training data.
Hierarchical decoding is typically carried out
with a parsing-based procedure. The parsing al-
gorithm is extended to handle translation candi-
dates and to incorporate language model scores
via cube pruning (Chiang, 2007). During decod-
ing, a hierarchical translation rule implicitly spec-
ifies the placement of the target part of a sub-
derivation which is substituting one of its non-
terminals in a partial hypothesis. The hierarchical
phrase-based model thus provides an integrated re-
ordering mechanism. The reorderings which are
being conducted by the hierarchical decoder are
a result of the application of SCFG rules, which
generally means that there must have been some
evidence in the training data for each reordering
operation. At first glance one might be tempted to
believe that any additional designated phrase ori-
entation modeling would be futile in hierarchical
translation as a consequence of this. We argue
that such a conclusion is false, and we will pro-
vide empirical evidence in this work that lexical-
ized phrase orientation scoring can be highly ben-
eficial not only in standard phrase-based systems,
but also in hierarchical ones.
The purpose of a phrase orientation model is
to assess the adequacy of phrase reordering dur-
ing search. In standard phrase-based translation
with continuous phrases only and left-to-right hy-
pothesis generation (Koehn et al, 2003; Zens and
Ney, 2008), phrase reordering is implemented by
jumps within the input sentence. The choice of the
best order for the target sequence is made based
on the language model score of this sequence and
a distortion cost that is computed from the source-
side jump distances. Though the space of admis-
sible reorderings is in most cases contrained by a
maximum jump width or coverage-based restric-
tions (Zens et al, 2004) for efficiency reasons,
the basic approach of arbitrarily jumping to un-
covered positions on source side is still very per-
missive. Lexicalized reordering models assist the
decoder in taking a good decision. Phrase-based
decoding allows for a straightforward integration
of lexicalized reordering models which assign
452
different scores depending on how a currently
translated phrase has been reordered with respect
to its context. Popular lexicalized reordering mod-
els for phrase-based translation distinguish three
orientation classes: monotone, swap, and discon-
tinuous (Tillmann, 2004; Koehn et al, 2007; Gal-
ley and Manning, 2008). To obtain such a model,
scores for the three classes are calculated from the
counts of the respective orientation occurrences in
the word-aligned training data for each extracted
phrase. The left-to-right orientation of phrases
during phrase-based search can be easily deter-
mined from the start and end positions of con-
tinuous phrases. Approximations may need to be
adopted for the right-to-left scoring direction.
The utility of phrase orientation models in stan-
dard phrase-based translation is plausible and has
been empirically established in practice. In hierar-
chical phrase-based translation, some other types
of lexicalized reordering models have been inves-
tigated recently (He et al, 2010a; He et al, 2010b;
Hayashi et al, 2010; Huck et al, 2012a), but
in none of them are the orientation scores condi-
tioned on the lexical identity of each phrase in-
dividually. These models are rather word-based
and applied on block boundaries. Experimental
results obtained with these other types of lexical-
ized reordering models have been very encourag-
ing, though.
There are certain reasons why assessing the ad-
equacy of phrase reordering should be useful in
hierarchical search:
? Albeit phrase reorderings are always a result
of the application of SCFG rules, the decoder
is still able to choose from many different
parses of the input sentence.
? The decoder can furthermore choose from
many translation options for each given
parse, which result in different reorderings
and different phrases being embedded in the
reordering non-terminals.
? All other models only weakly connect an em-
bedded phrase with the hierarchical phrase it
is placed into, in particular as the set of non-
terminals of the hierarchical grammar only
contains two generic non-terminal symbols.
We therefore investigate phrase orientation mod-
eling for hierarchical translation in this work.
2 Outline
The remainder of the paper is structured as fol-
lows: We briefly outline important related pub-
lications in the following section. We subse-
quently give a summary of some essential aspects
of the hierarchical phrase-based translation ap-
proach (Section 4). Phrase orientation modeling
and a way in which a phrase orientation model can
be trained for hierarchical phrase inventories are
explained in Section 5. In Section 6 we introduce
an extension of hierarchical search which enables
the decoder to score phrase orientations. Empiri-
cal results are presented in Section 7. We conclude
the paper in Section 8.
3 Related Work
Hierarchical phrase-based translation was pro-
posed by Chiang (2005). He et al (2010a) inte-
grated a maximum entropy based lexicalized re-
ordering model with word- and class-based fea-
tures. Different classifiers for different rule pat-
terns are trained for their model (He et al,
2010b). A comparable discriminatively trained
model which applies a single classifier for all types
of rules was developed by Huck et al (2012a).
Hayashi et al (2010) explored the word-based re-
ordering model by Tromble and Eisner (2009) in
hierarchical translation.
For standard phrase-based translation, Galley
and Manning (2008) introduced a hierarchical
phrase orientation model. Similar to previous ap-
proaches (Tillmann, 2004; Koehn et al, 2007), it
distinguishes the three orientation classes mono-
tone, swap, and discontinuous. However, it differs
in that it is not limited to model local reordering
phenomena, but allows for phrases to be hierarchi-
cally combined into blocks in order to determine
the orientation class. This has the advantage that
probability mass is shifted from the rather uninfor-
mative default category discontinuous to the other
two orientation classes, which model the location
of a phrase more specifically. In this work, we
transfer this concept to a hierarchical phrase-based
machine translation system.
4 Hierarchical Phrase-Based Translation
The non-terminal set of a standard hierarchical
grammar comprises two symbols which are shared
by source and target: the initial symbol S and one
generic non-terminal symbol X . The generic non-
terminal X is used as a placeholder for the gaps
453
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(a) Monotone phrase orientation.
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(b) Swap phrase orientation.
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(c) Discontinuous phrase orientation.
Figure 1: Extraction of the orientation classes monotone, swap, and discontinuous from word-aligned
training samples. The examples show the left-to-right orientation of the shaded phrases. The dashed
rectangles indicate how the predecessor words are merged into blocks with regard to their word align-
ment.
within the right-hand side of hierarchical transla-
tion rules as well as on all left-hand sides of the
translation rules that are extracted from the paral-
lel training corpus.
Extracted rules of a standard hierarchical gram-
mar are of the form X ? ??, ?,? ? where ??, ??
is a bilingual phrase pair that may contain X , i.e.
? ? ({X } ? VF )+ and ? ? ({X } ? VE)+, where
VF and VE are the source and target vocabulary,
respectively. The non-terminals on the source side
and on the target side of hierarchical rules are
linked in a one-to-one correspondence. The ? re-
lation defines this one-to-one correspondence. In
addition to the extracted rules, a non-lexicalized
initial rule
S ? ?X?0, X?0? (1)
is engrafted into the hierarchical grammar, as well
as a special glue rule
S ? ?S?0X?1, S?0X?1? (2)
that the system can use for serial concatenation
of phrases as in monotonic phrase-based transla-
tion. The initial symbol S is the start symbol of
the grammar.
Hierarchical search is conducted with a cus-
tomized version of the CYK+ parsing algo-
rithm (Chappelier and Rajman, 1998) and cube
pruning (Chiang, 2007). A hypergraph which rep-
resents the whole parsing space is built employing
CYK+. Cube pruning operates in bottom-up topo-
logical order on this hypergraph and expands at
most k derivations at each hypernode.
5 Modeling Phrase Orientation for
Hierarchical Machine Translation
The phrase orientation model we are using was
introduced by Galley and Manning (2008). To
model the sequential order of phrases within the
global translation context, the three orientation
classes monotone (M), swap (S) and discontinu-
ous (D) are distinguished, each in both left-to-
right and right-to-left direction. In order to cap-
ture the global rather than the local context, previ-
ous phrases can be merged into blocks if they are
consistent with respect to the word alignment. A
phrase is in monotone orientation if a consistent
monotone predecessor block exists, and in swap
orientation if a consistent swap predecessor block
exists. Otherwise it is in discontinuous orientation.
Given a sequence of source words fJ1 and a se-
quence of target words eI1, a block ?f j2j1 , ei2i1? (with
1 ? j1 ? j2 ? J and 1 ? i1 ? i2 ? I)
is consistent with respect to the word alignment
A ? {1, ..., I} ? {1, ..., J} iff
?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2
? ?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2.
(3)
Consistency is based upon two conditions in this
definition: (1.) At least one source and target po-
sition within the block must be aligned, and (2.)
words from inside the source interval may only
be aligned to words from inside the target inter-
val and vice versa. These are the same condi-
tions as those that are applied for the extraction of
454
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(a) A monotone orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 1
N(S|f2X?0f4, e2X?0e4) = 0
N(D|f2X?0f4, e2X?0e4) = 0
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(b) Another monotone orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 2
N(S|f2X?0f4, e2X?0e4) = 0
N(D|f2X?0f4, e2X?0e4) = 0
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(c) A swap orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 2
N(S|f2X?0f4, e2X?0e4) = 1
N(D|f2X?0f4, e2X?0e4) = 0
Figure 2: Accumulation of orientation counts for hierarchical phrases during extraction. The hierarchical
phrase ?f2X?0f4, e2X?0e4? (dark shaded) can be extracted from all the three training samples. Its
orientation is identical to the orientation of the continuous phrase (lightly shaded) which the sub-phrase
is cut out of, respectively. Note that the actual lexical content of the sub-phrase may differ. For instance,
the sub-phrase ?f3, e3? is being cut out in Fig. 2a, and the sub-phrase ?f6, e6? is being cut out in Fig. 2b.
standard continuous phrases. The only difference
is that length constraints are applied to phrases, but
not to blocks.
Figure 1 illustrates the extraction of monotone,
swap, and discontinuous orientation classes in
left-to-right direction from word-aligned bilingual
training samples. The right-to-left direction works
analogously.
We found that this concept can be neatly
plugged into the hierarchical phrase-based trans-
lation paradigm, without having to resort to ap-
proximations in decoding, which is necessary to
determine the right-to-left orientation in a standard
phrase-based system (Cherry et al, 2012). To train
the orientations, the extraction procedure from the
standard phrase-based version of the reordering
model can be used with a minor extension. The
model is trained on the same word-aligned data
from which the translation rules are extracted. For
each training sentence, we extract all phrases of
unlimited length that are consistent with the word
alignment, and store their corners in a matrix. The
corners are distinguished by their location: top-
left, top-right, bottom-left, and bottom-right. For
each bilingual phrase, we determine its left-to-
right and right-to-left orientation by checking for
adjacent corners.
The lexicalized orientation probability for the
orientation O ? {M,S,D} and the phrase pair
??, ?? is estimated as its smoothed relative fre-
quency:
p(O) = N(O)?
O??{M,S,D}N(O?)
(4)
p(O|?, ?) = ? ? p(O) +N(O|?, ?)
? +
?
O??{M,S,D}N(O?|f? , e?)
.
(5)
Here, N(O) denotes the global count and
N(O|?, ?) the lexicalized count for the orienta-
tion O. ? is a smoothing constant.
To determine the orientation frequency for a hi-
erarchical phrase with non-terminal symbols, the
orientation counts of all those phrases are accu-
mulated from which a sub-phrase is cut out and
replaced by a non-terminal symbol to obtain this
hierarchical phrase. Figure 2 gives an example.
Negative logarithms of the values are used as
costs in the log-linear model combination (Och
and Ney, 2002). Cost 0 for all orientations is as-
signed to the special rules which are not extracted
from the training data (initial and glue rule).
455
f1
f2
f3
45e
f
	1 	2 	3 45e	
target
so
ur
ce
(a) Monotone non-terminal orientation.
f1
234
f5
fe
f
	1 	5 	e 234	
target
so
ur
ce
(b) Swap non-terminal orientation.
f1
f2
345
fe
f
	1 	2 345 	
target
so
ur
ce
	e
(c) Discontinuous non-terminal orienta-
tion.
Figure 3: Scoring with the orientation classes monotone, swap, and discontinuous. Each picture shows
exactly one hierarchical phrase. The block which replaces the non-terminalX during decoding is embed-
ded with the orientation of this non-terminal X within the hierarchical phrase. The examples show the
left-to-right orientation of the non-terminal. The left-to-right orientation can be detected from the word
alignment of the hierarchical phrase, except for cases where the non-terminal is in boundary position on
target side.
6 Phrase Orientation Scoring in
Hierarchical Decoding
Our implementation of phrase orientation scoring
in hierarchical decoding is based on the observa-
tion that hierarchical rule applications, i.e. the us-
age of rules with non-terminals within their right-
hand sides, settle the target sequence order. Mono-
tone, swap, or discontinuous orientations of blocks
are each due to monotone, swap, or discontinuous
placement of non-terminals which are being sub-
stituted by these blocks.
The problem of phrase orientation scoring can
thus be mostly reduced to three steps which need
to be carried out whenever a hierarchical rule is
applied:
1. Determining the orientations of the non-
terminals in the rule.
2. Retrieving the proper orientation cost of the
topmost rule application in the sub-derivation
which corresponds to the embedded block for
the respective non-terminal.
3. Applying the orientation cost to the log-linear
model combination for the current derivation.
The orientation of a non-terminal in a hierarchi-
cal rule is dependent on the word alignments in
its context. Figure 3 depicts three examples.1 We
however need to deal with special cases where a
non-terminal orientation cannot be established at
the moment when the hierarchical rule is consid-
ered. We first describe the non-degenerate case
(Section 6.1). Afterwards we briefly discuss our
strategy in the special situation of boundary non-
terminals where the non-terminal orientation can-
not be determined from information which is in-
herent to the hierarchical rule under consideration
(Section 6.3).
We focus on left-to-right orientation scoring;
right-to-left scoring is symmetric.
6.1 Determining Orientations
In order to determine the orientation class of a
non-terminal, we rely on the word alignments
within the phrases. With each phrase, we store
the alignment matrix that has been seen most fre-
quently during phrase extraction. Non-terminal
symbols on target side are assumed to be aligned
to the respective non-terminal symbols on source
1Note that even maximal consecutive lexical intervals (ei-
ther on source or target side) are not necessarily aligned in
a way which makes them consistent bilingual blocks. In
Fig. 3a, e4 is for instance aligned both below and above
the non-terminal. In Fig. 3c, neither ?f1f2, e1e2? nor
?f1f2, e3e4? would be valid continuous phrases (the same
holds for ?f3f4, e1e2? and ?f3f4, e3e4?). We actually need
the generalization of the phrase orientation model to hierar-
chical phrases as described in Section 5 for this reason. Oth-
erwise we would be able to just score neighboring consistent
sub-blocks with a model that does not account for hierarchi-
cal phrases with non-terminals.
456
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(a) Last previous aligned target position.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(b) Initial box.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(c) Expansion of the initial box.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(d) The final box is a consistent left-to-right mono-
tone predecessor block of the non-terminal.
Figure 4: Determining the orientation class during decoding. Starting from the last previous aligned
target position, a box is spanned across the relevant alignment links onto the corner of the non-terminal.
The box is then checked for consistency.
side according to the ? relation. In the alignment
matrix, the rows and columns of non-terminals can
obviously contain only exactly this one alignment
link.
Starting from the last previous aligned target po-
sition to the left of the non-terminal, the algorithm
expands a box that spans across the other rele-
vant alignment links onto the corner of the non-
terminal. Afterwards it checks whether the areas
on the opposite sides of the non-terminal position
are non-aligned in the source and target intervals
of this box. The non-terminal is in discontinu-
ous orientation if the box is not a consistent block.
If the box is a consistent block, the non-terminal
is in monotone orientation if its source-side posi-
tion is larger than the maximum of the source-side
interval of the box, and in swap orientation if its
source-side position is smaller than the minimum
of the source-side interval of the box.
Figure 4 illustrates how the procedure operates.
In left-to-right direction, an initial box is spanned
from the last previous aligned target position to
the lower (monotone) or upper (swap) left cor-
ner of the non-terminal. In the example, starting
from ?f3, e5? (Fig. 4a), this initial box is spanned
to the lower left corner by iterating from f3 to
f4 and expanding its target interval to the mini-
mum aligned target position within these two rows
of the alignment matrix. The initial box cov-
ers ?f3f4, e3e4e5? (Fig. 4b). The procedure then
repeatedly checks whether the box needs to be
expanded?alternating to the bottom (monotone)
or top (swap) and to the left?until no alignment
links below or to the left of the box break the
consistency. Two box expansion are conducted
in the example: the first one expands the ini-
tial box below, resulting in a larger box which
covers ?f1f2f3f4, e3e4e5? (Fig. 4c); the second
457
f1
f2
345
e1345
target
so
ur
ce
(a) Left boundary non-
terminal that can be placed
in left-to-right monotone or
discontinuous orientation
when the phrase is embedded
into another one.
f1
f2
345
e1345
target
so
ur
ce
(b) Left boundary non-
terminal that can be placed
in left-to-right discontinuous
or swap orientation when
the phrase is embedded into
another one.
f1
f2345
e1 345
target
so
ur
ce
(c) Left boundary non-
terminal that can be placed in
left-to-right monotone, swap,
or discontinuous orientation
when the phrase is embedded
into another one.
f1
f2345
e1345
target
so
ur
ce
(d) Left boundary non-
terminal that can only be
placed in left-to-right dis-
continuous orientation when
the phrase is embedded into
another one.
Figure 5: Left boundary non-terminal symbols. Orientations the non-terminal can eventually turn out to
get placed in differ depending on existing alignment links in the rest of the phrase. Delayed left-to-right
scoring is not required in cases as in Fig. 5d. Fractional costs for the possible orientations are temporarily
applied in the other cases and recursively corrected as soon as an orientation is constituted in an upper
hypernode.
one expands this new box to the left, resulting in
a final box which covers ?f1f2f3f4, e1e2e3e4e5?
(Fig. 4d) and does not need to be expanded to-
wards the lower left corner any more. Afterwards
the procedure examines whether the final box is
a consistent block by inspecting whether the ar-
eas on the opposite side of the non-terminal po-
sition are non-aligned in the intervals of the box
(areas with waved lines in the Fig. 4d). These ar-
eas do not contain alignment links in the example:
the orientation class of the non-terminal is mono-
tone as it has a consistent left-to-right monotone
predecessor block. (Suppose an alignment link
?f5, e2? would break the consistency: the orienta-
tion class would then be discontinuous as the final
box would not be a consistent block.)
Orientations of non-terminals could basically be
precomputed and stored in the translation table.
We however compute them on demand during de-
coding. The computational overhead did not seem
to be too severe in our experiments.
6.2 Scoring Orientations
Once the orientation is determined, the proper ori-
entation cost of the embedded block needs to be
retrieved. We access the topmost rule application
in the sub-derivation which corresponds to the em-
bedded block for the respective non-terminal and
read the orientation model costs for this rule. The
special case of delayed scoring for boundary non-
terminals as described in the subsequent section is
recursively processed if necessary. The retrieved
orientation costs of the embedded blocks of all
non-terminals are finally added to the log-linear
model combination for the current derivation.
6.3 Boundary Non-Terminals
Cases where a non-terminal orientation cannot be
established at the moment when the hierarchi-
cal rule is considered arise when a non-terminal
symbol is in a boundary position on target side.
We define a non-terminal to be in (left or right)
boundary position iff no symbols are aligned be-
tween the phrase-internal target-side index of the
non-terminal and the (left or right) phrase bound-
ary. Left boundary positions of non-terminals
are critical for left-to-right orientation scoring,
right boundary positions for right-to-left orienta-
tion scoring. We denote non-terminals in bound-
ary position as boundary non-terminals.
The procedure as described in Section 6.1 is not
applicable to boundary non-terminals because a
last previous aligned target position does not ex-
ist. If it is impossible to determine the final non-
terminal orientation in the hypothesis from infor-
mation which is inherent to the phrase, we are
forced to delay the orientation scoring of the em-
bedded block. Our solution in these cases is to
heuristically add fractional costs of all orientations
the non-terminal can still eventually turn out to get
placed in (cf. Figure 5). We do so because not
adding an orientation cost to the derivation would
give it an unjustified advantage over other ones.
As soon as an orientation is constituted in an up-
458
per hypernode, any heuristic and actual orientation
costs can be collected by means of a recursive call.
Note that monotone or swap orientations in upper
hypernodes can top-down transition into discon-
tinuous orientations for boundary non-terminals,
depending on existing phrase-internal alignment
links in the context of the respective boundary
non-terminal. In the derivation at the upper hyper-
node, the heuristic costs are subtracted and the cor-
rect actual costs added. Delayed scoring can lead
to search errors; in order to keep them confined,
the delayed scoring needs to be done separately
for all derivations, not just for the first-best sub-
derivations along the incoming hyperedges.
7 Experiments
We evaluate the effect of phrase orienta-
tion scoring in hierarchical translation on the
Chinese?English 2008 NIST task2 and on the
French?German language pair using the standard
WMT3 newstest sets for development and testing.
7.1 Experimental Setup
We work with a Chinese?English parallel train-
ing corpus of 3.0 M sentence pairs (77.5 M Chi-
nese / 81.0 M English running words). To train the
German?French baseline system, we use 2.0 M
sentence pairs (53.1 M French / 45.8 M German
running words) that are partly taken from the
Europarl corpus (Koehn, 2005) and have partly
been collected within the Quaero project.4
Word alignments are created by aligning the
data in both directions with GIZA++5 and sym-
metrizing the two trained alignments (Och and
Ney, 2003). When extracting phrases, we ap-
ply several restrictions, in particular a maximum
length of ten on source and target side for lexi-
cal phrases, a length limit of five on source and
ten on target side for hierarchical phrases (includ-
ing non-terminal symbols), and no more than two
non-terminals per phrase.
A standard set of models is used in the base-
lines, comprising phrase translation probabilities
and lexical translation probabilities in both direc-
tions, word and phrase penalty, binary features
marking hierarchical rules, glue rule, and rules
2http://www.itl.nist.gov/iad/mig/
tests/mt/2008/
3http://www.statmt.org/wmt13/
translation-task.html
4http://www.quaero.org
5http://code.google.com/p/giza-pp/
with non-terminals at the boundaries, three sim-
ple count-based binary features, phrase length ra-
tios, and a language model. The language models
are 4-grams with modified Kneser-Ney smooth-
ing (Kneser and Ney, 1995; Chen and Goodman,
1998) which have been trained with the SRILM
toolkit (Stolcke, 2002).
Model weights are optimized against BLEU (Pa-
pineni et al, 2002) with MERT (Och, 2003) on
100-best lists. For Chinese?English we employ
MT06 as development set, MT08 is used as unseen
test set. For German?French we employ news-
test2009 as development set, newstest2008, news-
test2010, and newstest2011 are used as unseen test
sets. During decoding, a maximum length con-
straint of ten is applied to all non-terminals except
the initial symbol S . Translation quality is mea-
sured in truecase with BLEU and TER (Snover et
al., 2006). The results on MT08 are checked for
statistical significance over the baseline. Confi-
dence intervals have been computed using boot-
strapping for BLEU and Cochran?s approximate
ratio variance for TER (Leusch and Ney, 2009).
7.2 Chinese?English Experimental Results
Table 1 comprises all results of our empirical eval-
uation on the Chinese?English task.
We first compare the performance of the phrase
orientation model in left-to-right direction only
with the performance of the phrase orientation
model in left-to-right and right-to-left direction
(bidirectional). In all experiments, monotone,
swap, and discontinuous orientation costs are
treated as being from different feature functions
in the log-linear model combination: we assign
a separate scaling factor to each of the orienta-
tions. We have three more scaling factors than in
the baseline for left-to-right direction only, and six
more scaling factors for bidirectional phrase ori-
entation scoring. As can be seen from the results
table, the left-to-right model already yields a gain
of 1.1 %BLEU over the baseline on the unseen test
set (MT08). The bidirectional model performs just
slightly better (+1.2 %BLEU over the baseline).
With both models, the TER is reduced significantly
as well (-1.1 / -1.3 compared to the baseline). We
adopted the discriminative lexicalized reordering
model (discrim. RO) that has been suggested by
Huck et al (2012a) for comparison purposes. The
phrase orientation model provides clearly better
translation quality in our experiments.
459
MT06 (Dev) MT08 (Test)
NIST Chinese?English BLEU [%] TER [%] BLEU [%] TER [%]
HPBT Baseline 32.6 61.2 25.2 66.6
+ discrim. RO 33.0 61.3 25.8 66.0
+ phrase orientation (left-to-right) 33.3 60.7 26.3 65.5
+ phrase orientation (bidirectional) 33.2 60.6 26.4 65.3
+ swap rule 32.8 61.7 25.8 66.6
+ discrim. RO 33.1 61.2 26.0 66.1
+ phrase orientation (bidirectional) 33.3 60.7 26.5 65.3
+ binary swap feature 33.2 61.0 25.9 66.2
+ discrim. RO 33.2 61.3 26.2 66.1
+ phrase orientation (bidirectional) 33.6 60.5 26.6 65.1
+ soft syntactic labels 33.4 60.8 26.1 66.4
+ phrase orientation (bidirectional) 33.7 60.1 26.8 65.1
+ phrase-level s2t+t2s DWL + triplets 34.3 60.1 27.7 65.0
+ discrim. RO 34.8 59.8 27.7 64.7
+ phrase orientation (bidirectional) 35.3 59.0 28.4 63.7
Table 1: Experimental results for the NIST Chinese?English translation task (truecase). On the test set,
bold font indicates results that are significantly better than the baseline (p < .05).
As a next experiment, we bring in more re-
ordering capabilities by augmenting the hierarchi-
cal grammar with a single swap rule
X ? ?X?0X?1,X?1X?0? (6)
supplementary to the initial rule and glue rule.
The swap rule allows adjacent phrases to be trans-
posed. The setup with swap rule and bidirectional
phrase orientation model is about as good as the
setup with just the bidirectional phrase orienta-
tion model and no swap rule. If we furthermore
mark the swap rule with a binary feature (binary
swap feature), we end up at an improvement of
+1.4 %BLEU over the baseline. The phrase ori-
entation model again provides higher translation
quality than the discriminative reordering model.
In a third experiment, we investigate whether
the phrase orientation model also has a positive in-
fluence when integrated into a syntax-augmented
hierarchical system. We configured a hierarchi-
cal setup with soft syntactic labels (Stein et al,
2010), a syntactic enhancement in the manner of
preference grammars (Venugopal et al, 2009). On
MT08, the syntax-augmented system performs 0.9
%BLEU above the baseline setup. We achieve an
additional improvement of +0.7 %BLEU and -1.3
TER by including the bidirectional phrase orien-
tation model. Interestingly, the translation quality
of the setup with soft syntactic labels (but with-
out phrase orientation model) is worse than of the
setup with phrase orientation model (but without
soft syntactic labels) on MT08. The combination
of both extensions provides the best result, though.
In a last experiment, we finally took a very
strong setup which improves over the baseline by
2.5 %BLEU through the integration of phrase-level
discriminative word lexicon (DWL) models and
triplet lexicon models in source-to-target (s2t) and
target-to-source (t2s) direction. The models have
been presented by Hasan et al (2008), Bangalore
et al (2007), and Mauser et al (2009). We apply
them in a similar manner as proposed by Huck et
al. (2011). In this strong setup, the discriminative
reordering model gives gains on the development
set which barely carry over to the test set. Adding
the bidirectional phrase orientation model, in con-
trast, results in a nice gain of +0.7 %BLEU and a
reduction of 1.3 points in TER on the test set, even
on top of the DWL and triplet lexicon models.
7.3 French?German Experimental Results
Table 2 comprises the results of our empirical eval-
uation on the French?German task.
The left-to-right phrase orientation model
boosts the translation quality by up to 0.3 %BLEU.
The reduction in TER is in a similar order of
magnitude. The bidirectional model performs a
bit better again, with an advancement of up to
0.4 %BLEU and a maximal reduction in TER of
0.6 points.
460
newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER
French?German [%] [%] [%] [%] [%] [%] [%] [%]
HPBT Baseline 15.2 71.7 15.0 71.7 15.7 69.5 14.2 72.2
+ phrase orientation (left-to-right) 15.1 71.4 15.3 71.4 15.9 69.2 14.5 71.8
+ phrase orientation (bidirectional) 15.4 71.1 15.4 71.3 15.9 69.1 14.6 71.6
Table 2: Experimental results for the French?German translation task (truecase). newstest2009 is used
as development set.
8 Conclusion
In this paper, we introduced a phrase orientation
model for hierarchical machine translation. The
training of a lexicalized reordering model which
assigns probabilities for monotone, swap, and dis-
continuous orientation of phrases was generalized
from standard continuous phrases to hierarchical
phrases. We explained how phrase orientation
scoring can be implemented in hierarchical decod-
ing and conducted a number of experiments on a
Chinese?English and a French?German transla-
tion task. The results indicate that phrase orienta-
tion modeling is a very suitable enhancement of
the hierarchical paradigm.
Our implementation will be released as part of
Jane (Vilar et al, 2010; Vilar et al, 2012; Huck
et al, 2012b), the RWTH Aachen University open
source statistical machine translation toolkit.6
Acknowledgments
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This material is also
partly based upon work supported by the DARPA
BOLT project under Contract No. HR0011-12-
C-0015. Any opinions, findings and conclu-
sions or recommendations expressed in this ma-
terial are those of the authors and do not neces-
sarily reflect the views of the DARPA. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no 287658.
References
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical Machine Translation through
6http://www.hltpr.rwth-aachen.de/jane/
Global Lexical Selection and Sentence Reconstruc-
tion. In Proc. of the Annual Meeting of the Assoc. for
Computational Linguistics (ACL), pages 152?159,
Prague, Czech Republic, June.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133?137,
Paris, France, April.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, USA, August.
Colin Cherry, Robert C. Moore, and Chris Quirk.
2012. On Hierarchical Re-ordering and Permuta-
tion Parsing for Phrase-based Decoding. In Proc. of
the Workshop on Statistical Machine Translation
(WMT), pages 200?209, Montre?al, Canada, June.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263?270, Ann Ar-
bor, MI, USA, June.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of the Conf. on Empirical Meth-
ods for Natural Language Processing (EMNLP),
pages 847?855, Honolulu, HI, USA, October.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet Lexicon Mod-
els for Statistical Machine Translation. In Proc. of
the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 372?381, Hon-
olulu, HI, USA, October.
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh,
Kevin Duh, and Seiichi Yamamoto. 2010. Hi-
erarchical Phrase-based Machine Translation with
Word-based Reordering Model. In Proc. of the
Int. Conf. on Computational Linguistics (COLING),
pages 439?446, Beijing, China, August.
461
Zhongjun He, Yao Meng, and Hao Yu. 2010a. Extend-
ing the Hierarchical Phrase Based Model with Max-
imum Entropy Based BTG. In Proc. of the Conf. of
the Assoc. for Machine Translation in the Americas
(AMTA), Denver, CO, USA, October/November.
Zhongjun He, Yao Meng, and Hao Yu. 2010b. Max-
imum Entropy Based Phrase Reordering for Hier-
archical Phrase-based Translation. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 555?563, Cambridge,
MA, USA, October.
Matthias Huck, Saab Mansour, Simon Wiesler, and
Hermann Ney. 2011. Lexicon Models for Hierar-
chical Phrase-Based Machine Translation. In Proc.
of the Int. Workshop on Spoken Language Transla-
tion (IWSLT), pages 191?198, San Francisco, CA,
USA, December.
Matthias Huck, Stephan Peitz, Markus Freitag, and
Hermann Ney. 2012a. Discriminative Reordering
Extensions for Hierarchical Phrase-Based Machine
Translation. In Proc. of the Annual Conf. of the
European Assoc. for Machine Translation (EAMT),
pages 313?320, Trento, Italy, May.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012b. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In
Proc. of the Int. Conf. on Acoustics, Speech, and Sig-
nal Processing (ICASSP), volume 1, pages 181?184,
Detroit, MI, USA, May.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Human Language Technology Conf. / North
American Chapter of the Assoc. for Computational
Linguistics (HLT-NAACL), pages 127?133, Edmon-
ton, Canada, May/June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of the Annual Meeting of the Assoc. for
Computational Linguistics (ACL), Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of the MT
Summit X, Phuket, Thailand, September.
Gregor Leusch and Hermann Ney. 2009. Edit dis-
tances with block movements and error rate confi-
dence estimates. Machine Translation, 23(2):129?
140, December.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conf. on Empirical Methods for Natu-
ral Language Processing (EMNLP), pages 210?218,
Singapore, August.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 295?302, Philadelphia, PA, USA, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
USA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proc. of the Conf. of the Assoc. for
Machine Translation in the Americas (AMTA), pages
223?231, Cambridge, MA, USA, August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Proc.
of the Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), Denver, CO, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 2,
pages 901?904, Denver, CO, USA, September.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Boston, MA,
USA.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Proc.
of the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 1007?1016, Sin-
gapore, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
462
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Hu-
man Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics
(HLT-NAACL), pages 236?244, Boulder, CO, USA,
June.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchical
Translation, Extended with Reordering and Lexicon
Models. In Proc. of the Workshop on Statistical Ma-
chine Translation (WMT), pages 262?270, Uppsala,
Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Richard Zens and Hermann Ney. 2008. Improvements
in Dynamic Programming Beam Search for Phrase-
Based Statistical Machine Translation. In Proc. of
the Int. Workshop on Spoken Language Translation
(IWSLT), pages 195?205, Waikiki, HI, USA, Octo-
ber.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering Constraints for
Phrase-Based Statistical Machine Translation. In
Proc. of the Int. Conf. on Computational Linguis-
tics (COLING), pages 205?211, Geneva, Switzer-
land, August.
463
