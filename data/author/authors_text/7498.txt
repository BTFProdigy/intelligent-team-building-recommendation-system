Trend Survey on Japanese Natural Language Processing Studies
over the Last Decade
Masaki Murata?, Koji Ichii?, Qing Ma?,?, Tamotsu Shirado?,
Toshiyuki Kanamaru?,?, and Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{murata,qma,shirado,kanamaru,isahara}@nict.go.jp
?Port and Airport Research Institute
Nagase 3-1-1, Yokosuka, Kanagawa 239-0826, Japan, ichii@pari.go.jp
?Ryukoku University, Otsu 520-2194, Japan, qma@math.ryukoku.ac.jp
?Kyoto University, Yoshida-Nihonmatsu, Sakyo, Kyoto 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Abstract
Using natural language processing, we
carried out a trend survey on Japanese
natural language processing studies that
have been done over the last ten years.
We determined the changes in the num-
ber of papers published for each re-
search organization and on each re-
search area as well as the relationship
between research organizations and re-
search areas. This paper is useful
for both recognizing trends in Japanese
NLP and constructing a method of sup-
porting trend surveys using NLP.
1 Introduction
We conducted a trend survey on Japanese nat-
ural language processing studies that have been
done over the last ten years. We used biblio-
graphic information from journal papers and an-
nual conference papers of the Association for
Natural Language Processing, Japan (The Asso-
ciation for Natural Language Processing, 1995-
2004; The Association for Natural Language Pro-
cessing, 1994-2003). Just ten years have passed
since the association was established. Therefore,
we can use the bibliographic information from the
past ten years. In this study, we investigated what
kinds of studies have been presented in journal
papers and annual conference papers on the Asso-
ciation for Natural Language Processing, Japan.
We first digitized documents listed in the bibli-
ographic information and then extracted various
pieces of useful information for the trend survey.
Figure 1: Change in the number of papers
We also examined the changes in the number of
papers put up by each Japanese research orga-
nization and the changes in the number of pa-
pers written on specific research areas. More-
over, we examined the relationship between each
Japanese research organization and each research
area. This study is useful for trend surveys of
studies performed by members of in the Associa-
tion for Natural Language Processing, Japan.
2 Trend survey on NLP research studies
We show the changes in the number of journal
papers and conference papers in Figure 1. Jour-
nal papers are reviewed, but conference papers are
not reviewed in the association. In comparing the
journal papers and conference papers, we found
that the number of conference papers was much
larger than that of journal papers. We also found
that although both types of papers decreased in
number at some point, they both demonstrate an
upward trend.
Conference papers have a temporal peak in the
fourth year and a temporal drop in the sixth year,
250
Figure 2: Change in the number of journal papers
by each research organization (The two numbers in
the parentheses indicate the total number of papers
and the average value of published years.)
while journal papers have a peak in the sixth year
and a drop in the eighth year. The temporal peak
and drop of the journal papers occurred just two
years after the peak and drop of the conference
papers. We presume this is because journal papers
need more time for reviewing and publishing, and
because journal papers are presented later than
conference papers for studies performed at the
same time.
3 Trend survey on research
organizations
Next, we investigated the change in the number
of papers put out by each research organization.
The results are represented in contour in Figures
2 and 3. The height in contour (the depth of a
black color) indicates the number of papers. We
calculated the average (we call it average value)
of the average, the mode, and the median of the
published years by using the data of the number
of papers performed by each research organiza-
tion. In the figures, each research organization is
listed in ascending order of the average value. We
added the total number of papers and the average
value to each research organization in the figures.
Therefore, research organizations that had many
papers in the earlier years are displayed higher
on the list, while research organizations that had
Figure 3: Change in the number of conference pa-
pers by each research organization
many papers in the later years are displayed lower.
Here, we displayed only research organizations
that had many total papers. If a research orga-
nization?s name was changed during the ten-year
period, we used the name that had the most usage
on published papers for displaying it.1
From these figures, we can see that ATR and
CRL (NICT) put out many journal papers, and
NTT, ATR, Tokyo Institute of Technology, CRL,
and the University of Tokyo put out many confer-
ence papers. We also found that while NTT and
ATR had many papers in the earlier years, CRL
and the Univ. of Tokyo had many papers in the
later years. We can expect that because CRL and
the Univ. of Tokyo demonstrate an upward ten-
dency, their quantity of papers will continue to in-
crease in the future. Using these figures, we can
see very easily in which reference year each re-
search organization put out many papers.
4 Trend survey on research areas
Next, we investigated the change in the number
of papers in each research area. The results are in
Figures 4, 5, and 6. (Because the volume of data
for conference papers was large, it was divided
into two figures.). For journal papers, the height
1When we counted the frequency of a research organiza-
tion whose name was changed, we used all the names of it
including old and new names.
251
Figure 4: Change in the number of journal papers
in each research area
in contour indicates the number of papers. For
conference papers, the height in contour indicates
the base two logarithm of the number of papers
added by one. Using the same method as that de-
scribed above, we calculated the average of the
average, mode, and median of the years papers
were published using the data of the number of
papers in each research area. In the figures, each
research area is displayed in ascending order of
the average value. We added the total number of
papers and the average value to each research area
in the figures. Here, we divided the title of each
paper into words using ChaSen software (Mat-
sumoto et al, 1999), and we treaded each word as
a research area. A paper with a particular word in
Figure 5: Change in the number of conference pa-
pers in each research area (part I)
its title was categorized in the research area indi-
cated by the word. Wemanually eliminated words
that were not indicative of a research area, for ex-
ample, ?teki? (of) and ?kenkyu? (study).
From these figures, it is clear that the research
areas of ?Japanese? and ?analysis? were studied
in an especially large number of papers. We
also found that for journal papers, because the
research areas of ?verb?, ?noun?, ?disambigua-
tion?, ?probability?, ?corpus?, and ?polysemic?
were displayed higher on the list, these areas were
studied thoroughly in the earlier years. Likewise,
we found that the research areas of ?morphol-
ogy?, ?dependency?, ?dialogue?, and ?speech?
were studied thoroughly in the sixth year and the
252
Figure 6: Change in the number of conference pa-
pers in each research area (part II)
research areas of ?summarization?, ?retrieval?,
?translation? and so on were studied well in the
later years. Special journal issues on ?summariza-
tion? were published in the sixth and ninth years,
so the research area of ?summarization? was rep-
resented in many papers in those years. We can
expect that because the research area of ?transla-
tion? demonstrates an upward tendency, the num-
ber of papers on this topic will continue to in-
crease in the future.
In terms of conference papers, we found that
the research areas of ?bilingual?, ?morphology?,
?probability?, ?dictionary?, ?statistics?, and so on
were studied well in the earlier years. In the lower
part of the figures, such research areas as ?re-
Figure 7: Change in the number of conference pa-
pers at each research organization in the research
area of ?translation?
Figure 8: Change in the number of conference pa-
pers in each research area in the research area of
?translation?
trieval?, ?summarization?, ?question? and ?para-
phrase? are found. Thus, we can see that these
research areas were studied thoroughly in recent
years. We can see very easily in which reference
years each research area was studied using these
figures.
5 Trend survey using part of data
Although we have focused on using all the data
in the trend survey so far, we can narrow down
the survey by looking only at a certain part of
the data. For example, when we want to exam-
253
Figure 9: Relationship between research organizations and research areas in journal papers (The name
of each research organization is given a ??? symbol.)
ine a trend survey on translation in more detail,
all we have to do is to extract papers on transla-
tion and use them for a trend survey. We carried
out a trend survey on machine translation in this
manner. We first extracted papers whose titles in-
cluded the word ?translation? and then performed
the same investigations as in Sections 3 and 4.
The results are in Figures 7 and 8. The height in
contour (the depth of a color) indicates the num-
ber of papers. From Figure 7, we can see that
NTT had many papers in the earlier years, and
ATR had many papers in later years. From Figure
7, we can also see that studies on translation of-
ten dealt with specific topics such as ?semantics?,
?knowledge? and ?dictionary? in earlier years and
?support?, ?example?, and ?retrieval? in more re-
cent years.
6 Relationship between research
organizations and research topics
Finally, we investigated the various research ar-
eas that research organizations studied more fre-
quently during the ten-year period. Here, we
show only the results for journal papers. We used
the same method as in the previous sections for
extracting research organizations and research ar-
eas from the data. We counted the cooccurrent
frequency of each research organization and each
research area. We then constructed a cross table
in this manner and then performed the dual scal-
ing method (Weller and Romney, 1990; Ueda et
al., 2003). The result is depicted in Figure 9. The
dual scaling method displays the relationship be-
tween research organizations and research areas.
In Figure 9, ?translation? appears in the lower
left quadrant, ?learning? appears in the lower
right quadrant, ?statistics? and ?retrieval? appear
in the upper right quadrant, and ?noun? and ?sen-
tence? appear in the upper left quadrant. In the
vicinity around these words, the research areas
and organizations relating to them appear. For ex-
ample, in the upper right quadrant, Hitachi and
University of Tokushima appear near ?statistics?
and ?retrieval?, which were frequent study topics
for them. Similarly, ?summarization? appears in
the near upper right area of the source origin and
is surrounded by JAIST, Toyohashi University of
Technology, and Tokyo Institute of Technology.,
indicating it was a frequent topic of study at those
institutions. We can easily see which research
topics were primarily studied by each organiza-
tion using this figure.
Also in Figure 9, research areas on numeri-
cals such as ?probability? and ?learning? appear
254
on the right side. Therefore, we can interpret the
figure as depicting quantitative research topics on
the right side and qualitative research topics on
the left side. Research areas using complicated
processing such as ?learning? and ?translation?
appear in the lower area and research areas deal-
ing with theory such as ?probability?, ?grammar?,
?sentence?, and ?noun? appear in the upper area.
Therefore, we can interpret the figure as depict-
ing theoretical research topics in the upper area
and research topics using complicated processing
in the lower area.
7 Conclusion
In this paper, we described a trend survey carried
out on Japanese natural language processing stud-
ies done over the last ten years. We were able to
investigate trend surveys on research areas very
easily by treating divided words in titles by a mor-
phological analyzer as the indications of research
areas. We displayed the changes in the number of
papers put out by each research organization and
written on specific research topics. We also dis-
played the relationship between research organi-
zations and research areas using the dual scaling
method. The simple methods we used that are de-
scribed here made it possible to show many useful
results.
This paper has the following two significant ef-
fects:
 This paper explained a trend survey on
Japanese natural language processing. By
reading it, we can understand the trends in
research on Japanese natural language pro-
cessing. For example, we can find out
which research areas were studied more of-
ten and we can see which research organiza-
tions were involved in studying natural lan-
guage processing. We can also see which re-
search organization studied a particular re-
search area most often over the ten-year pe-
riod.
 We used natural language processing to
carry out the trend survey described here.
For example, we automatically detected the
indication of a research area from words
used in titles by using a morphological ana-
lyzer. In addition, we displayed words that
were extracted by the morphological ana-
lyzer in several ways to display the results
of the trend survey effectively. The methods
used in this paper would be useful in other
trend surveys.
In short, this paper is useful for recognizing trends
in Japanese NLP and for constructing methods of
supporting trend surveys using NLP.
In the future, we would like to perform an in-
ternational trend survey on natural language pro-
cessing using international conference and jour-
nal papers such as IJCNLP, ACL, and the Journal
of Computational Linguistics. We would also like
to do trend surveys on other topics such as AI, bi-
ology, politics, and sociology.
The kinds of investigations we did can easily be
altered to do many other kinds of investigations
as well. For example, we can use the dual scal-
ing method by investigating the relationship be-
tween the reference years and the research organi-
zations/areas. We can also use the representation
in contour for the relationship between research
organizations and research areas. Although we
showed the data in ascending order of the aver-
age value of the published years, we could show
the data in different order, for example, the or-
der of the total number of papers or the order of
the location, i.e., showing similar research orga-
nizations/areas that are located near each other by
clustering research organizations/areas using their
cooccurrent words. We would like to continue
to study these kinds of support methods for trend
surveys in the future.
References
The Association for Natural Language Processing. 1994-
2003. Journal of Natural Language Processing.
The Association for Natural Language Processing. 1995-
2004. Proceedings of the Annual Meeting of The Associ-
ation for Natural Language Processing.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, and Masayuki Asahara.
1999. Japanese morphological analysis system ChaSen
version 2.0 manual 2nd edition.
Taichiro Ueda, Masao Karita, and Kazue Honda. 2003. Jis-
sen Workshop Excel Tettei Katsuyou Tahenryou Kaiseki.
Shuuwa System. (in Japanese).
Susan C. Weller and A. Kimball Romney. 1990. Metric
Scaling : Correspondence Analysis (Quantitative Appli-
cations in the Social Sciences). SAGE Publications.
255
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 587?594,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine-Learning-Based Transformation of Passive Japanese Sentences
into Active by Separating Training Data into Each Input Particle
Masaki Murata
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
murata@nict.go.jp
Tamotsu Shirado
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
shirado@nict.go.jp
Toshiyuki Kanamaru
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
kanamaru@nict.go.jp
Hitoshi Isahara
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We developed a new method of transform-
ing Japanese case particles when trans-
forming Japanese passive sentences into
active sentences. It separates training data
into each input particle and uses machine
learning for each particle. We also used
numerous rich features for learning. Our
method obtained a high rate of accuracy
(94.30%). In contrast, a method that did
not separate training data for any input
particles obtained a lower rate of accu-
racy (92.00%). In addition, a method
that did not have many rich features for
learning used in a previous study (Mu-
rata and Isahara, 2003) obtained a much
lower accuracy rate (89.77%). We con-
firmed that these improvements were sig-
nificant through a statistical test. We
also conducted experiments utilizing tra-
ditional methods using verb dictionar-
ies and manually prepared heuristic rules
and confirmed that our method obtained
much higher accuracy rates than tradi-
tional methods.
1 Introduction
This paper describes how passive Japanese sen-
tences can be automatically transformed into ac-
tive. There is an example of a passive Japanese
sentence in Figure 1. The Japanese suffix reta
functions as an auxiliary verb indicating the pas-
sive voice. There is a corresponding active-voice
sentence in Figure 2. When the sentence in Fig-
ure 1 is transformed into an active sentence, (i) ni
(by), which is a case postpositional particle with
the meaning of ?by?, is changed into ga, which is
a case postpositional particle indicating the sub-
jective case, and (ii) ga (subject), which is a
case postpositional particle indicating the subjec-
tive case, is changed into wo (object), which is
a case postpositional particle indicating the objec-
tive case. In this paper, we discuss the transfor-
mation of Japanese case particles (i.e., ni ? ga)
through machine learning.1
The transformation of passive sentences into ac-
tive is useful in many research areas including
generation, knowledge extraction from databases
written in natural languages, information extrac-
tion, and answering questions. For example, when
the answer is in the passive voice and the ques-
tion is in the active voice, a question-answering
system cannot match the answer with the question
because the sentence structures are different and
it is thus difficult to find the answer to the ques-
tion. Methods of transforming passive sentences
into active are important in natural language pro-
cessing.
The transformation of case particles in trans-
forming passive sentences into active is not easy
because particles depend on verbs and their use.
We developed a new method of transforming
Japanese case particles when transforming pas-
sive Japanese sentences into active in this study.
Our method separates training data into each in-
put particle and uses machine learning for each in-
put particle. We also used numerous rich features
for learning. Our experiments confirmed that our
method was effective.
1In this study, we did not handle the transformation of
auxiliary verbs and the inflection change of verbs because
these can be transformed based on Japanese grammar.
587
inu ni watashi ga kama- reta.
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 1: Passive sentence
inu ni watashi ga kama- reta.
ga wo
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 3: Example in corpus
inu ga watashi wo kanda.
(dog) subject (I) object (bite)
(Dog bit me.)
Figure 2: Active sentence
2 Tagged corpus as supervised data
We used the Kyoto University corpus (Kurohashi
and Nagao, 1997) to construct a corpus tagged for
the transformation of case particles. It has ap-
proximately 20,000 sentences (16 editions of the
Mainichi Newspaper, from January 1st to 17th,
1995). We extracted case particles in passive-
voice sentences from the Kyoto University cor-
pus. There were 3,576 particles. We assigned a
corresponding case particle for the active voice to
each case particle. There is an example in Figure
3. The two underlined particles, ?ga? and ?wo?
that are given for ?ni? and ?ga? are tags for case
particles in the active voice. We called the given
case particles for the active voice target case par-
ticles, and the original case particles in passive-
voice sentences source case particles. We created
tags for target case particles in the corpus. If we
can determine the target case particles in a given
sentence, we can transform the case particles in
passive-voice sentences into case particles for the
active voice. Therefore, our goal was to determine
the target case particles.
3 Machine learning method (support
vector machine)
We used a support vector machine as the basis
of our machine-learning method. This is because
support vector machines are comparatively better
than other methods in many research areas (Kudoh
and Matsumoto, 2000; Taira and Haruno, 2001;
Small Margin Large Margin
Figure 4: Maximizing margin
Murata et al, 2002).
Data consisting of two categories were classi-
fied by using a hyperplane to divide a space with
the support vector machine. When these two cat-
egories were, positive and negative, for example,
enlarging the margin between them in the train-
ing data (see Figure 42), reduced the possibility of
incorrectly choosing categories in blind data (test
data). A hyperplane that maximized the margin
was thus determined, and classification was done
using that hyperplane. Although the basics of this
method are as described above, the region between
the margins through the training data can include
a small number of examples in extended versions,
and the linearity of the hyperplane can be changed
to non-linear by using kernel functions. Classi-
fication in these extended versions is equivalent
to classification using the following discernment
function, and the two categories can be classified
on the basis of whether the value output by the
function is positive or negative (Cristianini and
Shawe-Taylor, 2000; Kudoh, 2000):
2The open circles in the figure indicate positive examples
and the black circles indicate negative. The solid line indi-
cates the hyperplane dividing the space, and the broken lines
indicate the planes depicting margins.
588
f(x) = sgn
(
l
?
i=1
?iyiK(xi,x) + b
)
(1)
b =
maxi,y
i
=?1
bi + mini,y
i
=1
bi
2
bi = ?
l
?
j=1
?jyjK(xj,xi),
where x is the context (a set of features) of an in-
put example, xi indicates the context of a training
datum, and yi (i = 1, ..., l, yi ? {1,?1}) indicates
its category. Function sgn is:
sgn(x) = 1 (x ? 0), (2)
?1 (otherwise).
Each ?i (i = 1, 2...) is fixed as a value of ?i that
maximizes the value of L(?) in Eq. (3) under the
conditions set by Eqs. (4) and (5).
L(?) =
l
?
i=1
?
i
?
1
2
l
?
i,j=1
?
i
?
j
y
i
y
j
K(x
i
,x
j
) (3)
0 ? ?
i
? C (i = 1, ..., l) (4)
l
?
i=1
?
i
y
i
= 0 (5)
Although function K is called a kernel function
and various functions are used as kernel functions,
we have exclusively used the following polyno-
mial function:
K(x,y) = (x ? y + 1)d (6)
C and d are constants set by experimentation. For
all experiments reported in this paper, C was fixed
as 1 and d was fixed as 2.
A set of xi that satisfies ?i > 0 is called a sup-
port vector, (SVs)3, and the summation portion of
Eq. (1) is only calculated using examples that are
support vectors. Equation 1 is expressed as fol-
lows by using support vectors.
f(x) = sgn
?
?
?
i:x
i
?SV
s
?iyiK(xi,x) + b
?
?(7)
b =
bi:y
i
=?1,x
i
?SV
s
+ bi:y
i
=1,x
i
?SV
s
2
bi = ?
?
i:x
i
?SV
s
?jyjK(xj ,xi),
3The circles on the broken lines in Figure 4 indicate sup-
port vectors.
Table 1: Features
F1 part of speech (POS) of P
F2 main word of P
F3 word of P
F4 first 1, 2, 3, 4, 5, and 7 digits of category number
of P5
F5 auxiliary verb attached to P
F6 word of N
F7 first 1, 2, 3, 4, 5, and 7 digits of category number
of N
F8 case particles and words of nominals that have de-
pendency relationship with P and are other than
N
F9 first 1, 2, 3, 4, 5, and 7 digits of category num-
ber of nominals that have dependency relationship
with P and are other than N
F10 case particles of nominals that have dependency
relationship with P and are other than N
F11 the words appearing in the same sentence
F12 first 3 and 5 digits of category number of words
appearing in same sentence
F13 case particle taken by N (source case particle)
F14 target case particle output by KNP (Kurohashi,
1998)
F15 target case particle output with Kondo?s method
(Kondo et al, 2001)
F16 case patterns defined in IPAL dictionary (IPAL)
(IPA, 1987)
F17 combination of predicate semantic primitives de-
fined in IPAL
F18 predicate semantic primitives defined in IPAL
F19 combination of semantic primitives of N defined
in IPAL
F20 semantic primitives of N defined in IPAL
F21 whether P is defined in IPAL or not
F22 whether P can be in passive form defined in
VDIC6
F23 case particles of P defined in VDIC
F24 type of P defined in VDIC
F25 transformation rule used for P and N in Kondo?s
method
F26 whether P is defined in VDIC or not
F27 pattern of case particles of nominals that have de-
pendency relationship with P
F28 pair of case particles of nominals that have depen-
dency relationship with P
F29 case particles of nominals that have dependency
relationship with P and appear before N
F30 case particles of nominals that have dependency
relationship with P and appear after N
F31 case particles of nominals that have dependency
relationship with P and appear just before N
F32 case particles of nominals that have dependency
relationship with P and appear just after N
589
Table 2: Frequently occurring target case particles in source case particles
Source case particle Occurrence rate Frequent target case Occurrence rate
particles in in
source case particles source case particles
ni (indirect object) 27.57% (493/1788) ni (indirect object) 70.79% (349/493)
ga (subject) 27.38% (135/493)
ga (subject) 26.96% (482/1788) wo (direct object) 96.47% (465/482)
de (with) 17.17% (307/1788) ga (subject) 79.15% (243/307)
de (with) 13.36% (41/307)
to (with) 16.11% (288/1788) to (with) 99.31% (286/288)
wo (direct object) 6.77% (121/1788) wo (direct object) 99.17% (120/121)
kara (from) 4.53% ( 81/1788) ga (subject) 49.38% ( 40/ 81)
kara (from) 44.44% ( 36/ 81)
made (to) 0.78% ( 14/1788) made (to) 100.00% ( 14/ 14)
he (to) 0.06% ( 1/1788) ga (subject) 100.00% ( 1/ 1)
no (subject) 0.06% ( 1/1788) wo (direct object) 100.00% ( 1/ 1)
Support vector machines are capable of han-
dling data consisting of two categories. Data con-
sisting of more than two categories is generally
handled using the pair-wise method (Kudoh and
Matsumoto, 2000).
Pairs of two different categories (N(N-1)/2
pairs) are constructed for data consisting of N cat-
egories with this method. The best category is de-
termined by using a two-category classifier (in this
paper, a support vector machine4 is used as the
two-category classifier), and the correct category
is finally determined on the basis of ?voting? on
the N(N-1)/2 pairs that result from analysis with
the two-category classifier.
The method discussed in this paper is in fact a
combination of the support vector machine and the
pair-wise method described above.
4 Features (information used in
classification)
The features we used in our study are listed in Ta-
ble 1, where N is a noun phrase connected to the
4We used Kudoh?s TinySVM software (Kudoh, 2000) as
the support vector machine.
5The category number indicates a semantic class of
words. A Japanese thesaurus, the Bunrui Goi Hyou (NLRI,
1964), was used to determine the category number of each
word. This thesaurus is ?is-a? hierarchical, in which each
word has a category number. This is a 10-digit number that
indicates seven levels of ?is-a? hierarchy. The top five lev-
els are expressed by the first five digits, the sixth level is ex-
pressed by the next two digits, and the seventh level is ex-
pressed by the last three digits.
6Kondo et al constructed a rich dictionary for Japanese
verbs (Kondo et al, 2001). It defined types and characteris-
tics of verbs. We will refer to it as VDIC.
case particle being analyzed, and P is the phrase?s
predicate. We used the Japanese syntactic parser,
KNP (Kurohashi, 1998), for identifying N, P, parts
of speech and syntactic relations.
In the experiments conducted in this study, we
selected features. We used the following proce-
dure to select them.
? Feature selection
We first used all the features for learning. We
next deleted only one feature from all the fea-
tures for learning. We did this for every fea-
ture. We decided to delete features that would
make the most improvement. We repeated
this until we could not improve the rate of ac-
curacy.
5 Method of separating training data
into each input particle
We developed a new method of separating train-
ing data into each input (source) particle that uses
machine learning for each particle. For example,
when we identify a target particle where the source
particle is ni, we use only the training data where
the source particle is ni. When we identify a tar-
get particle where the source particle is ga, we use
only the training data where the source particle is
ga.
Frequently occurring target case particles are
very different in source case particles. Frequently
occurring target case particles in all source case
particles are listed in Table 2. For example, when
ni is a source case particle, frequently occurring
590
Table 3: Occurrence rates for target case particles
Target case Occurrence rate
particle Closed Open
wo (direct object) 33.05% 29.92%
ni (indirect object) 19.69% 17.79%
to (with) 16.00% 18.90%
de (with) 13.65% 15.27%
ga (subject) 11.07% 10.01%
ga or de 2.40% 2.46%
kara (from) 2.13% 3.47%
Other 2.01% 1.79%
target case particles are ni or ga. In contrast, when
ga is a source case particle, a frequently occurring
target case particle is wo.
In this case, it is better to separate training data
into each source particle and use machine learn-
ing for each particle. We therefore developed this
method and confirmed that it was effective through
experiments (Section 6).
6 Experiments
6.1 Basic experiments
We used the corpus we constructed described in
Section 2 as supervised data. We divided the su-
pervised data into closed and open data (Both the
closed data and open data had 1788 items each.).
The distribution of target case particles in the data
are listed in Table 3. We used the closed data to
determine features that were deleted in feature se-
lection and used the open data as test data (data
for evaluation). We used 10-fold cross validation
for the experiments on closed data and we used
closed data as the training data for the experiments
on open data. The target case particles were deter-
mined by using the machine-learning method ex-
plained in Section 3. When multiple target parti-
cles could have been answers in the training data,
we used pairs of them as answers for machine
learning.
The experimental results are listed in Tables 4
and 5. Baseline 1 outputs a source case particle
as the target case particle. Baseline 2 outputs the
most frequent target case particle (wo (direct ob-
ject)) in the closed data as the target case particle
in every case. Baseline 3 outputs the most fre-
quent target case particle for each source target
case particle in the closed data as the target case
particle. For example, ni (indirect object) is the
most frequent target case particle when the source
case particle is ni, as listed in Table 2. Baseline 3
outputs ni when the source case particle is ni. KNP
indicates the results that the Japanese syntactic
parser, KNP (Kurohashi, 1998), output. Kondo in-
dicates the results that Kondo?s method, (Kondo et
al., 2001), output. KNP and Kondo can only work
when a target predicate is defined in the IPAL dic-
tionary or the VDIC dictionary. Otherwise, KNP
and Kondo output nothing. ?KNP/Kondo + Base-
line X? indicates the use of outputs by Baseline
X when KNP/Kondo have output nothing. KNP
and Kondo are traditional methods using verb dic-
tionaries and manually prepared heuristic rules.
These traditional methods were used in this study
to compare them with ours. ?Murata 2003? indi-
cates results using a method they developed in a
previous study (Murata and Isahara, 2003). This
method uses F1, F2, F5, F6, F7, F10, and F13 as
features and does not have training data for any
source case particles. ?Division? indicates sepa-
rating training data into each source particle. ?No-
division? indicates not separating training data for
any source particles. ?All features? indicates the
use of all features with no features being selected.
?Feature selection? indicates features are selected.
We did two kinds of evaluations: ?Eval. A? and
?Eval. B?. There are some cases where multiple
target case particles can be answers. For example,
ga and de can be answers. We judged the result to
be correct in ?Eval. A? when ga and de could be
answers and the system output the pair of ga and
de as answers. We judged the result to be correct
in ?Eval. B? when ga and de could be answers and
the system output ga, de, or the pair of ga and de
as answers.
Table 4 lists the results using all data. Table 5
lists the results where a target predicate is defined
in the IPAL and VDIC dictionaries. There were
551 items in the closed data and 539 in the open.
We found the following from the results.
Although selection of features obtained higher
rates of accuracy than use of all features in the
closed data, it did not obtain higher rates of accu-
racy in the open data. This indicates that feature
selection was not effective and we should have
used all features in this study.
Our method using all features in the open data
and separating training data into each source parti-
cle obtained the highest rate of accuracy (94.30%
in Eval. B). This indicates that our method is ef-
591
Table 4: Experimental results
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 58.67% 61.41% 62.02% 64.60%
Baseline 2 33.05% 33.56% 29.92% 30.37%
Baseline 3 84.17% 88.20% 84.17% 88.20%
KNP 27.35% 28.69% 27.91% 29.14%
KNP + Baseline 1 64.32% 67.06% 67.79% 70.36%
KNP + Baseline 2 48.10% 48.99% 45.97% 46.48%
KNP + Baseline 3 81.21% 84.84% 80.82% 84.45%
Kondo 39.21% 40.88% 39.32% 41.00%
Kondo + Baseline 1 65.27% 68.57% 67.34% 70.41%
Kondo + Baseline 2 54.87% 56.54% 53.52% 55.26%
Kondo + Baseline 3 78.08% 81.71% 78.30% 81.88%
Murata 2003 86.86% 89.09% 87.86% 89.77%
Our method, no-division + all features 89.99% 92.39% 90.04% 92.00%
Our method, no-division + feature selection 91.28% 93.40% 90.10% 92.00%
Our method, division + all features 91.22% 93.79% 92.28% 94.30%
Our method, division + feature selection 92.06% 94.41% 91.89% 93.85%
Table 5: Experimental results on data that can use IPAL and VDIC dictionaries
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 57.71% 58.98% 58.63% 58.81%
Baseline 2 37.39% 37.39% 37.29% 37.29%
Baseline 3 84.03% 86.57% 86.83% 88.31%
KNP 74.59% 75.86% 75.88% 76.07%
Kondo 76.04% 77.50% 78.66% 78.85%
Our method, no-division + all features 94.19% 95.46% 94.81% 94.81%
Our method, division + all features 95.83% 96.91% 97.03% 97.03%
fective.
Our method that used all the features and did
not separate training data for any source particles
obtained an accuracy rate of 92.00% in Eval. B.
The technique of separating training data into each
source particles made an improvement of 2.30%.
We confirmed that this improvement has a signifi-
cance level of 0.01 by using a two-sided binomial
test (two-sided sign test). This indicates that the
technique of separating training data for all source
particles is effective.
Murata 2003 who used only seven features and
did not separate training data for any source par-
ticles obtained an accuracy rate of 89.77% with
Eval. B. The method (92.00%) of using all fea-
tures (32) made an improvement of 2.23% against
theirs. We confirmed that this improvement had
a significance level of 0.01 by using a two-sided
binomial test (two-sided sign test). This indicates
that our increased features are effective.
KNP and Kondo obtained low accuracy rates
(29.14% and 41.00% in Eval. B for the open data).
We did the evaluation using data and proved that
these methods could work well. A target predicate
in the data is defined in the IPAL and VDIC dictio-
naries. The results are listed in Table 5. KNP and
Kondo obtained relatively higher accuracy rates
(76.07% and 78.85% in Eval. B for the open data).
However, they were lower than that for Baseline 3.
Baseline 3 obtained a relatively high accuracy
rate (84.17% and 88.20% in Eval. B for the open
data). Baseline 3 is similar to our method in terms
of separating the training data into source parti-
cles. Baseline 3 separates the training data into
592
Table 6: Deletion of features
Deleted Closed data Open data
features Eval. A Eval. B Eval. A Eval. B
Acc. Diff. Acc. Diff. Acc. Diff. Acc. Diff.
Not deleted 91.22% ? 93.79% ? 92.28% ? 94.30% ?
F1 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F2 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F3 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F4 91.50% 0.28% 94.13% 0.34% 91.72% -0.56% 93.68% -0.62%
F5 91.22% 0.00% 93.62% -0.17% 91.95% -0.33% 93.96% -0.34%
F6 91.00% -0.22% 93.51% -0.28% 92.23% -0.05% 94.24% -0.06%
F7 90.66% -0.56% 93.18% -0.61% 91.78% -0.50% 93.90% -0.40%
F8 91.22% 0.00% 93.79% 0.00% 92.39% 0.11% 94.24% -0.06%
F9 91.28% 0.06% 93.62% -0.17% 92.45% 0.17% 94.07% -0.23%
F10 91.33% 0.11% 93.85% 0.06% 92.00% -0.28% 94.07% -0.23%
F11 91.50% 0.28% 93.74% -0.05% 92.06% -0.22% 93.79% -0.51%
F12 91.28% 0.06% 93.62% -0.17% 92.56% 0.28% 94.35% 0.05%
F13 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F14 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.41% 0.11%
F15 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F16 91.39% 0.17% 93.90% 0.11% 92.34% 0.06% 94.30% 0.00%
F17 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F18 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.46% 0.16%
F19 91.33% 0.11% 93.90% 0.11% 92.28% 0.00% 94.30% 0.00%
F20 91.11% -0.11% 93.68% -0.11% 92.34% 0.06% 94.35% 0.05%
F21 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F22 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F23 91.28% 0.06% 93.79% 0.00% 92.28% 0.00% 94.24% -0.06%
F24 91.22% 0.00% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F25 89.54% -1.68% 92.11% -1.68% 90.04% -2.24% 92.39% -1.91%
F26 91.16% -0.06% 93.74% -0.05% 92.28% 0.00% 94.30% 0.00%
F27 91.22% 0.00% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F28 90.94% -0.28% 93.51% -0.28% 92.11% -0.17% 94.13% -0.17%
F29 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.30% 0.00%
F30 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F31 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.24% -0.06%
F32 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
source particles and uses the most frequent tar-
get case particle. Our method involves separating
the training data into source particles and using
machine learning for each particle. The fact that
Baseline 3 obtained a relatively high accuracy rate
supports the effectiveness of our method separat-
ing the training data into source particles.
6.2 Experiments confirming importance of
features
We next conducted experiments where we con-
firmed which features were effective. The results
are listed in Table 6. We can see the accuracy rate
for deleting features and the accuracy rate for us-
ing all features. We can see that not using F25
greatly decreased the accuracy rate (about 2%).
This indicates that F25 is particularly effective.
F25 is the transformation rule Kondo used for P
and N in his method. The transformation rules in
Kondo?s method were made precisely for ni (indi-
rect object), which is particularly difficult to han-
dle. F25 is thus effective. We could also see not
using F7 decreased the accuracy rate (about 0.5%).
F7 has the semantic features for N. We found that
the semantic features for N were also effective.
6.3 Experiments changing number of
training data
We finally did experiments changing the number
of training data. The results are plotted in Figure
5. We used our two methods of all features ?Di-
vision? and ?Non-division?. We only plotted the
593
Figure 5: Changing number of training data
accuracy rates for Eval. B in the open data in the
figure. We plotted accuracy rates when 1, 1/2, 1/4,
1/8, and 1/16 of the training data were used. ?Divi-
sion?, which separates training data for all source
particles, obtained a high accuracy rate (88.36%)
even when the number of training data was small.
In contrast, ?Non-division?, which does not sepa-
rate training data for any source particles, obtained
a low accuracy rate (75.57%), when the number of
training data was small. This indicates that our
method of separating training data for all source
particles is effective.
7 Conclusion
We developed a new method of transform-
ing Japanese case particles when transforming
Japanese passive sentences into active sentences.
Our method separates training data for all input
(source) particles and uses machine learning for
each particle. We also used numerous rich features
for learning. Our method obtained a high rate of
accuracy (94.30%). In contrast, a method that did
not separate training data for all source particles
obtained a lower rate of accuracy (92.00%). In ad-
dition, a method that did not have many rich fea-
tures for learning used in a previous study obtained
a much lower accuracy rate (89.77%). We con-
firmed that these improvements were significant
through a statistical test. We also undertook ex-
periments utilizing traditional methods using verb
dictionaries and manually prepared heuristic rules
and confirmed that our method obtained much
higher accuracy rates than traditional methods.
We also conducted experiments on which fea-
tures were the most effective. We found that
Kondo?s transformation rule used as a feature in
our system was particularly effective. We also
found that semantic features for nominal targets
were effective.
We finally did experiments on changing the
number of training data. We found that our
method of separating training data for all source
particles could obtain high accuracy rates even
when there were few training data. This indicates
that our method of separating training data for all
source particles is effective.
The transformation of passive sentences into ac-
tive sentences is useful in many research areas
including generation, knowledge extraction from
databases written in natural languages, informa-
tion extraction, and answering questions. In the
future, we intend to use the results of our study for
these kinds of research projects.
References
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
IPA. 1987. (Information?Technology Promotion Agency,
Japan). IPA Lexicon of the Japanese Language for Com-
puters IPAL (Basic Verbs). (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. Transactions of Infor-
mation Processing Society of Japan, 42(3):465?477. (in
Japanese).
Taku Kudoh and Yuji Matsumoto. 2000. Use of support vec-
tor learning for chunk identification. CoNLL-2000, pages
142?144.
Taku Kudoh. 2000. TinySVM: Support Vector Machines.
http://cl.aist-nara.ac.jp/?taku-ku//software/TinySVM/
index.html.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto Univer-
sity text corpus project. 3rd Annual Meeting of the Asso-
ciation for Natural Language Processing, pages 115?118.
(in Japanese).
Sadao Kurohashi, 1998. Japanese Dependency/Case Struc-
ture Analyzer KNP version 2.0b6. Department of Infor-
matics, Kyoto University. (in Japanese).
Masaki Murata and Hitoshi Isahara, 2003. Conversion of
Japanese Passive/Causative Sentences into Active Sen-
tences Using Machine Learning, pages 115?125. Springer
Publisher.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002. Com-
parison of three machine-learning methods for Thai part-
of-speech tagging. ACM Transactions on Asian Language
Information Processing, 1(2):145?158.
NLRI. 1964. Bunrui Goi Hyou. Shuuei Publishing.
Hirotoshi Taira and Masahiko Haruno. 2001. Feature se-
lection in svm text categorization. In Proceedings of
AAAI2001, pages 480?486.
594
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 1?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Development of an automatic trend exploration system
using the MuST data collection
Masaki Murata1
murata@nict.go.jp
Qing Ma3,1
3qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru1,4
1kanamaru@nict.go.jp
Hitoshi Isahara1
isahara@nict.go.jp
1National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
3Ryukoku University
Otsu, Shiga, 520-2194, Japan
Koji Ichii2
ichiikoji@hiroshima-u.ac.jp
Tamotsu Shirado1
shirado@nict.go.jp
Sachiyo Tsukawaki1
tsuka@nict.go.jp
2Hiroshima University
1-4-1 Kagamiyama, Higashi-hiroshima,
Hiroshima 739-8527, Japan
4Kyoto University
Yoshida-nihonmatsu-cho, Sakyo-ku,
Kyoto, 606-8501, Japan
Abstract
The automatic extraction of trend informa-
tion from text documents such as news-
paper articles would be useful for explor-
ing and examining trends. To enable this,
we used data sets provided by a workshop
on multimodal summarization for trend in-
formation (the MuST Workshop) to con-
struct an automatic trend exploration sys-
tem. This system first extracts units, tem-
porals, and item expressions from news-
paper articles, then it extracts sets of ex-
pressions as trend information, and finally
it arranges the sets and displays them in
graphs. For example, when documents
concerning the politics are given, the sys-
tem extracts ?%? and ?Cabinet approval
rating? as a unit and an item expression in-
cluding temporal expressions. It next ex-
tracts values related to ?%?. Finally, it
makes a graph where temporal expressions
are used for the horizontal axis and the
value of percentage is shown on the ver-
tical axis. This graph indicates the trend
of Cabinet approval rating and is useful
for investigating Cabinet approval rating.
Graphs are obviously easy to recognize
and useful for understanding information
described in documents. In experiments,
when we judged the extraction of a correct
graph as the top output to be correct, the
system accuracy was 0.2500 in evaluation
A and 0.3334 in evaluation B. (In evalua-
tion A, a graph where 75% or more of the
points were correct was judged to be cor-
rect; in evaluation B, a graph where 50%
or more of the points were correct was
judged to be correct.) When we judged
the extraction of a correct graph in the top
five outputs to be correct, accuracy rose to
0.4167 in evaluation A and 0.6250 in eval-
uation B. Our system is convenient and ef-
fective because it can output a graph that
includes trend information at these levels
of accuracy when given only a set of doc-
uments as input.
1 Introduction
We have studied ways to automatically extract
trend information from text documents, such as
newspaper articles, because such a capability will
be useful for exploring and examining trends. In
this work, we used data sets provided by a work-
shop on multimodal summarization for trend in-
formation (the MuST Workshop) to construct an
automatic trend exploration system. This system
firsts extract units, temporals, and item expres-
sions from newspaper articles, then it extract sets
of expressions as trend information, and finally it
arranges the sets and displays them in graphs. For
example, when documents concerning the politics
1
are given, the system extracts ?%? and ?Cabinet
approval rating? as a unit and an item expression
including temporal expressions. It next extracts
values related to ?%?. Finally, it makes a graph
where temporal expressions are used for the hor-
izontal axis and the value of percentage is shown
on the vertical axis. This graph indicates the trend
of Cabinet approval rating and is useful for inves-
tigating Cabinet approval rating. Graphs are obvi-
ously easy to recognize and useful for understand-
ing information described in documents.
2 The MuST Workshop
Kato et al organized the workshop on multimodal
summarization for trend information (the MuST
Workshop) (Kato et al, 2005). In this work-
shop, participants were given data sets consisting
of newspaper documents (editions of the Mainichi
newspaper from 1998 and 1999 (Japanese docu-
ments)) that included trend information for vari-
ous domains. In the data, tags for important ex-
pressions (e.g. temporals, numerical expressions,
and item expressions) were tagged manually.1 The
20 topics of the data sets (e.g., the 1998 home-run
race to break the all-time Major League record,
the approval rating for the Japanese Cabinet, and
news on typhoons) were provided. Trend infor-
mation was defined as information regarding the
change in a value for a certain item. A change in
the number of home runs hit by a certain player or
a change in the approval rating for the Cabinet are
examples of trend information. In the workshop,
participants could freely use the data sets for any
study they chose to do.
3 System
3.1 Structure of the system
Our automatic trend exploration system consists
of the following components.
1. Component to extract important expressions
First, documents related to a certain topic are
given to the system, which then extracts im-
portant expressions that will be used to ex-
tract and merge trend information. The sys-
tem extracts item units, temporal units, and
item expressions as important expressions.
1We do not use manually provided tags for important ex-
pressions because our system automatically extracts impor-
tant expressions.
Here, important expressions are defined as
expressions that play important roles in a
given document set. Item expressions are de-
fined as expressions that are strongly related
to the content of a given document set.
1a. Component to extract important item
units
The system extracts item units that will
be used to extract and merge trend infor-
mation.
For example, when documents concern-
ing the home-run race are given, ?hon?
or ?gou? (the Japanese item units for the
number of home runs) such as in ?54
hon? (54th home run) are extracted.
1b. Component to extract important tempo-
ral units
The system extracts temporal units that
will also be used to extract and merge
trend information.
For example, the system extracts tempo-
ral units such as ?nichi? (day), ?gatsu?
(month), and ?nen? (year). In Japanese,
temporal units are used to express dates,
such as in ?2006 nen, 3 gatsu, 27 nichi?
for March 27th, 2006.
1c. Component to extract important item
expressions
The system extracts item expressions
that will also be used to extract and
merge trend information.
For example, the system extracts expres-
sions that are objects for trend explo-
ration, such as ?McGwire? and ?Sosa?
as item expressions in the case of docu-
ments concerning the home-run race.
2. Component to extract trend information sets
The system identifies the locations in sen-
tences where a temporal unit, an item unit,
and an item expression that was extracted by
the component to extract important expres-
sions appear in similar sentences and extracts
sets of important expressions described by
the sentences as a trend information set. The
system also extracts numerical values appear-
ing with item units or temporal units, and
uses the connection of the numerical values
and the item units or temporal units as nu-
merical expressions or temporal expressions.
2
For example, in the case of documents con-
cerning the home-run race, the system ex-
tracts a set consisting of ?item expression:
McGwire?, ?temporal expression: 11 day?
(the 11th), and ?numerical expression: 47
gou? (47th home run) as a trend information
set.
3. Component to extract and display important
trend information sets
The system gathers the extracted trend infor-
mation sets and displays them as graphs or by
highlighting text displays.
For example, for documents concerning
the home-run race, the system displays as
graphs the extracted trend information sets
for ?McGwire? . In these graphs, temporal
expressions are used for the horizontal axis
and the number of home runs is shown on the
vertical axis.
3.2 Component to extract important
expressions
The system extracts important expressions that
will be used to extract trend information sets. Im-
portant expressions belong to one of the following
categories.
? item units
? temporal units
? item expressions
We use ChaSen (Matsumoto et al, 1999), a
Japanese morphological analyzer, to extract ex-
pressions. Specifically, we use the parts of
speeches in the ChaSen outputs to extract the ex-
pressions.
The system extracts item units, temporal units,
and item expressions by using manually con-
structed rules using the parts of speeches. The
system extracts a sequence of nouns adjacent to
numerical values as item units. It then extracts
expressions from among the item units which in-
clude an expression regarding time or date (e.g.,
?year?, ?month?, ?day?, ?hour?, or ?second?) as
temporal units. The system extracts a sequence of
nouns as item expressions.
The system next extracts important item units,
temporal units, and item expressions that play im-
portant roles in the target documents.
The following three methods can be used to ex-
tract important expressions. The system uses one
of them. The system judges that an expression
producing a high value from the following equa-
tions is an important expression.
? Equation for the TF numerical term in Okapi
(Robertson et al, 1994)
Score =
?
i?Docs
TF
i
TF
i
+
l
i
?
(1)
? Use of total word frequency
Score =
?
i?Docs
TF
i
(2)
? Use of total frequency of documents where a
word appears
Score =
?
i?Docs
1 (3)
In these equations, i is the ID (identification
number) of a document, Docs is a set of document
IDs, TF
i
is the occurrence number of an expres-
sion in document i, l is the length of document i,
and ? is the average length of documents inDocs.
To extract item expressions, we also applied a
method that uses the product of the occurrence
number of an expression in document i and the
length of the expression as TF
i
, so that we could
extract longer expressions.
3.3 Component to extract trend information
sets
The system identifies the locations in sentences
where a temporal unit, an item unit, and an item
expression extracted by the component to extract
important expressions appears in similar sentences
and extracts sets of important expressions de-
scribed by the sentences as a trend information
set. When more than one trend information set
appears in a document, the system extracts the one
that appears first. This is because important and
new things are often described in the beginning of
a document in the case of newspaper articles.
3.4 Component to extract and display
important trend information sets
The system gathers the extracted trend informa-
tion sets and displays them in graphs or as high-
lighted text. In the graphs, temporal expressions
3
are used for the horizontal axis and numerical ex-
pressions are used for the vertical axis. The system
also displays sentences used to extract trend infor-
mation sets and highlights important expressions
in the sentences.
The system extracts multiple item units, tempo-
ral units, and item expressions (through the com-
ponent to extract important expressions) and uses
these to make all possible combinations of the
three kinds of expression. The system extracts
trend information sets for each combination and
calculates the value of one of the following equa-
tions for each combination. The system judges
that the combination producing a higher value rep-
resents more useful trend information. The fol-
lowing four equations can be used for this purpose,
and the system uses one of them.
? Method 1 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? S
1
? S
2
? S
3
(4)
? Method 2 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? (S
1
? S
2
? S
3
)
1
3 (5)
? Method 3 ? Use the frequency of trend in-
formation sets
M = Freq (6)
? Method 4 ? Use the scores of important ex-
pressions
M = S
1
? S
2
? S
3
(7)
In these equations, Freq is the number of trend
information sets extracted as described in Section
3.3, and S1, S2, and S3 are the values of Score as
calculated by the corresponding equation in Sec-
tion 3.2.
The system extracts the top five item units, the
top five item expressions, and the top three tem-
poral units through the component to extract im-
portant expressions and forms all possible combi-
nations of these (75 combinations). The system
then calculates the value of the above equations for
these 75 combinations and judges that a combina-
tion having a larger value represents more useful
trend information.
4 Experiments and Discussion
We describe some examples of the output of our
system in Sections 4.1, 4.2, and 4.3, and the re-
sults from our system evaluation in Section 4.4.
We made experiments using Japanese newspaper
articles.
4.1 Extracting important expressions
To extract important expressions we applied the
equation for the TF numerical term in Okapi and
the method using the product of the occurrence
number for an expression and the length of the
expression as TF
i
for item expressions. We did
experiments using the three document sets for ty-
phoons, the Major Leagues, and political trends.
The results are shown in Table 1.
We found that appropriate important expres-
sions were extracted for each domain. For ex-
ample, in the data set for typhoons, ?typhoon?
was extracted as an important item expression and
an item unit ?gou? (No.), indicating the ID num-
ber of each typhoon, was extracted as an im-
portant item unit. In the data set for the Major
Leagues, the MuST data included documents de-
scribing the home-run race between Mark McG-
wire and Sammy Sosa in 1998. ?McGwire? and
?Sosa? were properly extracted among the higher
ranks. ?gou? (No.) and ?hon? (home run(s)), im-
portant item units for the home-run race, were
properly extracted. In the data set for political
trends, ?naikaku shiji ritsu? (cabinet approval rat-
ing) was properly extracted as an item expression
and ?%? was extracted as an item unit.
4.2 Graphs representing trend information
We next tested how well our system graphed the
trend information obtained from the MuST data
sets. We used the same three document sets as in
the previous section. As important expressions in
the experiments, we used the item unit, the tempo-
ral unit, and the item expression with the highest
scores (the top ranked ones) which were extracted
by the component to extract important expressions
using the method described in the previous sec-
tion. The system made the graphs using the com-
ponent to extract trend information sets and the
component to extract and display important trend
information sets. The graphs thus produced are
shown in Figs. 1, 2, and 3. (We used Excel to draw
these graphs.) Here, we made a temporal axis for
each temporal expression. However, we can also
4
Table 1: Examples of extracting important expressions
Typhoon
item units temporal units item expressions
gou nichi taihuu
(No.) (day) (typhoon)
me-toru ji gogo
(meter(s)) (o?clock) (afternoon)
nin jigoro higai
(people) (around x o?clock) (damage)
kiro fun shashin setsumei
(kilometer(s)) (minute(s)) (photo caption)
miri jisoku chuushin
(millimeter(s)) (per hour) (center)
Major League
item units temporal units item expressions
gou nichi Maguwaia
(No.) (day) (McGwire)
hon nen honruida
(home run(s)) (year) (home run)
kai gatsu Ka-jinarusu
(inning(s)) (month) (Cardinals)
honruida nen buri Ma-ku Maguwaia ichiruishu
(home run(s)) (after x year(s) interval) (Mark McGwire, the first baseman)
shiai fun So-sa
(game(s)) (minute(s)) (Sosa)
Political Trend
item units temporal units item expressions
% gatsu naikaku shiji ritsu
(%) (month) (cabinet approval rating)
pointo gen nichi Obuchi naikaku
(decrease of x point(s)) (day) (Obuchi Cabinet)
pointo zou nen Obuchi shushou
(increase of x point(s)) (year) (Prime Minister Obuchi)
dai kagetu shijiritsu
(generation) (month(s)) (approval rating)
pointo bun no kitai
(point(s)) (divided) (expectation)
5
Figure 1: Trend graph for the typhoon data set
Figure 2: Trend graph for the Major Leagues data
set
display a graph where regular temporal intervals
are used in the temporal axis.
For the typhoon data set, gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as the top ranked item unit, temporal unit, and
item expression. The system extracted trend in-
formation sets using these, and then made a graph
where the temporal expression (day) was used for
the horizontal axis and the ID numbers of the ty-
phoons were shown on the vertical axis. The
MuST data included data for September and Octo-
ber of 1998 and 1999. Figure 1 is useful for seeing
when each typhoon hit Japan during the typhoon
season each year. Comparing the 1998 data with
that of 1999 reveals that the number of typhoons
increased in 1999.
For the Major Leagues data set, gou (No.), nichi
(day), and Maguwaia (McGwire) were extracted
with the top rank. The system used these to make
a graph where the temporal expression (day) was
used for the horizontal axis and the cumulative
number of home runs hit by McGwire was shown
on the vertical axis (Fig. 2). The MuST data
included data beginning in August, 1998. The
graph shows some points where the cumulative
number of home runs decreased (e.g., September
Figure 3: Trend graph for the political trends data
set
4th), which was obviously incorrect. This was be-
cause our system wrongly extracted the number of
home runs hit by Sosa when this was given close
to McGwire?s total.
In the political trends data set, %, gatsu
(month), and naikaku shiji ritsu (cabinet approval
rating) were extracted with the top rankings. The
system used these to make a graph where the
temporal expression (month) was used for the
horizontal axis and the Cabinet approval rating
(Japanese Cabinet) was shown as a percentage on
the vertical axis. The MuST data covered 1998
and 1999. Figure 2 shows the cabinet approval
rating of the Obuchi Cabinet. We found that the
overall approval rating trend was upwards. Again,
there were some errors in the extracted trend infor-
mation sets. For example, although June was han-
dled correctly, the system wrongly extracted May
as a temporal expression from the sentence ?in
comparison to the previous investigation in May?.
4.3 Sentence extraction and highlighting
display
We then tested the sentence extraction and high-
lighting display with respect to trend information
using the MuST data set; in this case, we used
the typhoon data set. As important expressions,
we used the item unit, the temporal unit, and the
item expression extracted with the highest scores
(the top ranked ones) by the component to extract
important expressions using the method described
in the previous section. Gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as an item unit, a temporal unit, and an item ex-
pression. The system extracted sentences includ-
ing the three expressions and highlighted these ex-
pressions in the sentences. The results are shown
in Figure 4. The first trend information sets to ap-
6
Sept. 16, 1998 No. 5
Large-scale and medium-strength Typhoon No. 5 made landfall near Omaezaki in Shizuoka Pre-
fecture before dawn on the 16th, and then moved to the northeast involving the Koshin, Kantou,
and Touhoku areas in the storm.
Sept. 21, 1998 No. 8
Small-scale Typhoon No. 8 made landfall near Tanabe City in Wakayama Prefecture around 4:00
p.m. on the 21st, and weakened while tracking to the northward across Kinki district.
Sept. 22, 1998 No. 7
Typhoon No. 7 made landfall near Wakayama City in the afternoon on the 22nd, and will hit the
Kinki district.
Sept. 21, 1998 No. 8
The two-day consecutive landfall of Typhoon No. 8 on the 21st and Typhoon No. 7 on the 22nd
caused nine deaths and many injuries in a total of six prefectures including Nara, Fukui, Shiga,
and so on.
Oct. 17, 1998 No. 10
Medium-scale and medium-strength Typhoon No. 10 made landfall on Makurazaki City in
Kagoshima Prefecture around 4:30 p.m. on the 17th, and then moved across the West Japan area
after making another landfall near Sukumo City in Kochi Prefecture in the evening.
Aug. 20, 1999 No. 11
The Meteorological Office announced on the 20th that Typhoon No. 11 developed 120 kilometers
off the south-southwest coast of Midway.
Sept. 14, 1999 No. 16
Typhoon No. 16, which developed off the south coast in Miyazaki Prefecture, made landfall near
Kushima City in the prefecture around 5:00 p.m. on the 14th.
Sept. 15, 1999 No. 16
Small-scale and weak Typhoon No. 16 became extratropical in Nagano Prefecture and moved out
to sea off Ibaraki Prefecture on the 15th.
Sept. 24, 1999 No. 18
Medium-scale and strong Typhoon No. 18 made landfall in the north of Kumamoto Prefecture
around 6:00 a.m. on the 24th, and after moving to Suo-Nada made another landfall at Ube City
in Yamaguchi Prefecture before 9:00 p.m., tracked through the Chugoku district, and then moved
into the Japan Sea after 10:00 p.m.
Sept. 25, 1999 No. 18
Typhoon No. 18, which caused significant damage in the Kyushu and Chugoku districts, weakened
and made another landfall before moving into the Sea of Okhotsk around 10:00 a.m. on the 25th.
Figure 4: Sentence extraction and highlighting display for the typhoon data set
7
pear are underlined twice and the other sets are
underlined once. (In the actual system, color is
used to make this distinction.) The extracted tem-
poral expressions and numerical expressions are
presented in the upper part of the extracted sen-
tence. The graphs shown in the previous section
were made by using these temporal expressions
and numerical expressions.
The extracted sentences plainly described the
state of affairs regarding the typhoons and were
important sentences. For the research being done
on summarization techniques, this can be consid-
ered a useful means of extracting important sen-
tences. The extracted sentences typically describe
the places affected by each typhoon and whether
there was any damage. They contain important
descriptions about each typhoon. This confirmed
that a simple method of extracting sentences con-
taining an item unit, a temporal unit, and an item
expression can be used to extract important sen-
tences.
The fourth sentence in the figure includes infor-
mation on both typhoon no.7 and typhoon no.8.
We can see that there is a trend information set
other than the extracted trend information set (un-
derlined twice) from the expressions that are un-
derlined once. Since the system sometimes ex-
tracts incorrect trend information sets, the high-
lighting is useful for identifying such sets.
4.4 Evaluation
We used a closed data set and an open data set
to evaluate our system. The closed data set was
the data set provided by the MuST workshop or-
ganizer and contained 20 domain document sets.
The data sets were separated for each domain.
We made the open data set based on the MuST
data set using newspaper articles (editions of the
Mainichi newspaper from 2000 and 2001). We
made 24 document sets using information retrieval
by term query. We used documents retrieved by
term query as the document set of the domain for
each query term.
We used the closed data set to adjust our system
and used the open data set to calculate the evalua-
tion scores of our system for evaluation.
We judged whether a document set included the
information needed to make trend graphs by con-
sulting the top 30 combinations of three kinds of
important expression having the 30 highest values
as in the method of Section 3.4. There were 19
documents including such information in the open
data. We used these 19 documents for the follow-
ing evaluation.
In the evaluation, we examined how accurately
trend graphs could be output when using the top
ranked expressions. The results are shown in Table
2. The best scores are described using bold fonts
for each evaluation score.
We used five evaluation scores. MRR is the av-
erage of the score where 1/r is given as the score
when the rank of the first correct output is r (Mu-
rata et al, 2005b). TP1 is the average of the pre-
cision in the first output. TP5 is the average of
the precision where the system includes a correct
output in the first five outputs. RP is the average
of the r-precision and AP is the average of the av-
erage precision. (Here, the average means that the
evaluation score is calculated for each domain data
set and the summation of these scores divided by
the number of the domain data sets is the average.)
R-precision is the precision of the r outputs where
r is the number of correct answers. Average pre-
cision is the average of the precision when each
correct answer is output (Murata et al, 2000). The
r-precision indicates the precision where the recall
and the precision have the same value. The preci-
sion is the ratio of correct answers in the system
output. The recall is the ratio of correct answers
in the system output to the total number of correct
answers.
Methods 1 to 4 in Table 2 are the methods used
to extract useful trend information described in
Section 3.4. Use of the expression length means
the product of the occurrence number for an ex-
pression and the length of the expression was used
to calculate the score for an important item ex-
pression. No use of the expression length means
this product was not used and only the occurrence
number was used.
To calculate the r-precision and average preci-
sion, we needed correct answer sets. We made the
correct answer sets by manually examining the top
30 outputs for the 24 (= 4? 6) methods (the com-
binations of methods 1 to 4 and the use of Equa-
tions 1 to 3 with or without the expression length)
and defining the useful trend information among
them as the correct answer sets.
In evaluation A, a graph where 75% or more of
the points were correct was judged to be correct.
In evaluation B, a graph where 50% or more of the
points were correct was judged to be correct.
8
Table 2: Experimental results for the open data
Evaluation A Evaluation B
MRR TP1 TP5 RP AP MRR TP1 TP5 RP AP
Use of Equation 1 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1360 0.1162 0.5522 0.4211 0.7368 0.1968 0.1565
Method 2 0.3847 0.3158 0.4211 0.1360 0.1150 0.5343 0.4211 0.6316 0.1880 0.1559
Method 3 0.3557 0.2632 0.4211 0.1360 0.1131 0.5053 0.3684 0.6316 0.1805 0.1541
Method 4 0.3189 0.2632 0.4211 0.1125 0.0973 0.4492 0.3158 0.6316 0.1645 0.1247
Use of Equation 2 and the expression length
Method 1 0.3904 0.3158 0.4737 0.1422 0.1154 0.5746 0.4211 0.7368 0.2127 0.1674
Method 2 0.3877 0.3158 0.4737 0.1422 0.1196 0.5544 0.4211 0.7368 0.2127 0.1723
Method 3 0.3895 0.3158 0.5263 0.1422 0.1202 0.5491 0.4211 0.7895 0.2127 0.1705
Method 4 0.2216 0.1053 0.3684 0.0846 0.0738 0.3765 0.2105 0.5789 0.1328 0.1043
Use of Equation 3 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
Use of Equation 1 and no use of the expression length
Method 1 0.3789 0.3158 0.4737 0.1294 0.1152 0.5456 0.4211 0.7368 0.2002 0.1627
Method 2 0.3750 0.3158 0.4211 0.1294 0.1137 0.5215 0.4211 0.6842 0.2002 0.1621
Method 3 0.3333 0.2632 0.4211 0.1119 0.1072 0.4798 0.3684 0.6842 0.1763 0.1552
Method 4 0.2588 0.1053 0.4737 0.1269 0.0872 0.3882 0.1579 0.6842 0.1833 0.1189
Use of Equation 2 and no use of the expression length
Method 1 0.3277 0.2105 0.4737 0.1134 0.0952 0.4900 0.2632 0.7895 0.1779 0.1410
Method 2 0.3662 0.2632 0.4737 0.1187 0.1104 0.5417 0.3684 0.7368 0.1831 0.1594
Method 3 0.3504 0.2632 0.4737 0.1187 0.1116 0.5167 0.3684 0.7368 0.1884 0.1647
Method 4 0.1877 0.0526 0.3684 0.0775 0.0510 0.3131 0.1053 0.5263 0.1300 0.0879
Use of Equation 3 and no use of the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
9
From the experimental results, we found that
the method using the total frequency for a word
(Equation 2) and the length of an expression was
best for calculating the scores of important expres-
sions.
Using the length of an expression was impor-
tant. (The way of using the length of an expres-
sion was described in the last part of Section 3.2.)
For example, when ?Cabinet approval rating? ap-
pears in documents, a method without expression
lengths extracts ?rating?. When the system ex-
tracts trend information sets using ?rating?, it ex-
tracts wrong information related to types of ?rat-
ing? other than ?Cabinet approval rating?. This
hinders the extraction of coherent trend informa-
tion. Thus, it is beneficial to use the length of an
expression when extracting important item expres-
sions.
We also found that method 1 (using both the fre-
quency of the trend information sets and the scores
of important expressions) was generally the best.
When we judged the extraction of a correct
graph as the top output in the experiments to be
correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
In terms of the evaluation scores for the 24 original
data sets (these evaluation scores were multiplied
by 19/24), when we judged the extraction of a cor-
rect graph as the top output in the experiments to
be correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
Our system is convenient and effective because it
can output a graph that includes trend information
at these levels of accuracy when given only a set
of documents as input.
As shown in Table 2, the best values for RP
(which indicates the precision where the recall and
the precision have the same value) and AP were
0.2127 and 0.1705, respectively, in evaluation B.
This RP value indicates that our system could
extract about one out of five graphs among the cor-
rect answers when the recall and the precision had
the same value.
5 Related studies
Fujihata et al (Fujihata et al, 2001) developed a
system to extract numerical expressions and their
related item expressions by using syntactic infor-
mation and patterns. However, they did not deal
with the extraction of important expressions or
gather trend information sets. In addition, they did
not make a graph from the extracted expressions.
Nanba et al (Nanba et al, 2005) took an
approach of judging whether the sentence rela-
tionship indicates transition (trend information)
or renovation (revision of information) and used
the judgment results to extract trend information.
They also constructed a system to extract nu-
merical information from input numerical units
and make a graph that includes trend information.
However, they did not consider ways to extract
item numerical units and item expressions auto-
matically.
In contrast to these systems, our system auto-
matically extracts item numerical units and item
expressions that each play an important role in a
given document set. When a document set for
a certain domain is given, our system automati-
cally extracts item numerical units and item ex-
pressions, then extracts numerical expressions re-
lated to these, and finally makes a graph based
on the extracted numerical expressions. When a
document set is given, the system automatically
makes a graph that includes trend information.
Our system also uses an original method of pro-
ducing more than one graphs and selecting an ap-
propriate graph among them using Methods 1 to 4,
which Fujihata et al and Namba et al did not use.
6 Conclusion
We have studied the automatic extraction of trend
information from text documents such as newspa-
per articles. Such extraction will be useful for ex-
ploring and examining trends. We used data sets
provided by a workshop on multimodal summa-
rization for trend information (the MuST Work-
shop) to construct our automatic trend exploration
system. This system first extracts units, tempo-
rals, and item expressions from newspaper arti-
cles, then it extracts sets of expressions as trend
information, and finally it arranges the sets and
displays them in graphs.
In our experiments, when we judged the extrac-
tion of a correct graph as the top output to be cor-
rect, the system accuracy was 0.2500 in evaluation
10
A and 0.3334 in evaluation B. (In evaluation A, a
graph where 75% or more of the points were cor-
rect was judged to be correct; in evaluation B, a
graph where 50% or more of the points were cor-
rect was judged to be correct.) When we judged
the extraction of a correct graph in the top five out-
puts to be correct, we obtained accuracy of 0.4167
in evaluation A and 0.6250 in evaluation B. Our
system is convenient and effective because it can
output a graph that includes trend information at
these levels of accuracy when only a set of docu-
ments is provided as input.
In the future, we plan to continue this line of
study and improve our system. We also hope to
apply the method of using term frequency in doc-
uments to extract trend information as reported by
Murata et al (Murata et al, 2005a).
References
Katsuyuki Fujihata, Masahiro Shiga, and Tatsunori
Mori. 2001. Extracting of numerical expressions
by constraints and default rules of dependency struc-
ture. Information Processing Society of Japan,
WGNL 145.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. MuST: A workshop on multimodal
summarization for trend information. Proceedings
of the Fifth NTCIR WorkshopMeeting on Evaluation
of Information Access Technologies: Information
Retrieval, Question Answering and Cross-Lingual
Information Access.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis
system ChaSen version 2.0 manual 2nd edition.
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara.
2000. Japanese probabilistic information retrieval
using location and category information. The Fifth
International Workshop on Information Retrieval
with Asian Languages, pages 81?88.
Masaki Murata, Koji Ichii, Qing Ma, Tamotsu Shirado,
Toshiyuki Kanamaru, and Hitoshi Isahara. 2005a.
Trend survey on Japanese natural language process-
ing studies over the last decade. In The Second In-
ternational Joint Conference on Natural Language
Processing, Companion Volume to the Proceedings
of Conference including Posters/Demos and Tutorial
Abstracts.
Masaki Murata, Masao Utiyama, and Hitoshi Isahara.
2005b. Use of multiple documents as evidence with
decreased adding in a Japanese question-answering
system. Journal of Natural Language Processing,
12(2).
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
Information Processing Society of Japan, WGNL
168, pages 67?74.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
11
