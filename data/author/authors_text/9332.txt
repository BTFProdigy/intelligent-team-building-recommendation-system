Proceedings of ACL-08: HLT, pages 780?788,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora
Shiqi Zhao1, Haifeng Wang2, Ting Liu1, Sheng Li1
1Harbin Institute of Technology, Harbin, China
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
2Toshiba (China) Research and Development Center, Beijing, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
Paraphrase patterns are useful in paraphrase
recognition and generation. In this paper, we
present a pivot approach for extracting para-
phrase patterns from bilingual parallel cor-
pora, whereby the English paraphrase patterns
are extracted using the sentences in a for-
eign language as pivots. We propose a log-
linear model to compute the paraphrase likeli-
hood of two patterns and exploit feature func-
tions based on maximum likelihood estima-
tion (MLE) and lexical weighting (LW). Us-
ing the presented method, we extract over
1,000,000 pairs of paraphrase patterns from
2M bilingual sentence pairs, the precision
of which exceeds 67%. The evaluation re-
sults show that: (1) The pivot approach is
effective in extracting paraphrase patterns,
which significantly outperforms the conven-
tional method DIRT. Especially, the log-linear
model with the proposed feature functions
achieves high performance. (2) The coverage
of the extracted paraphrase patterns is high,
which is above 84%. (3) The extracted para-
phrase patterns can be classified into 5 types,
which are useful in various applications.
1 Introduction
Paraphrases are different expressions that convey
the same meaning. Paraphrases are important in
plenty of natural language processing (NLP) ap-
plications, such as question answering (QA) (Lin
and Pantel, 2001; Ravichandran and Hovy, 2002),
machine translation (MT) (Kauchak and Barzilay,
2006; Callison-Burch et al, 2006), multi-document
summarization (McKeown et al, 2002), and natural
language generation (Iordanskaja et al, 1991).
Paraphrase patterns are sets of semantically
equivalent patterns, in which a pattern generally
contains two parts, i.e., the pattern words and slots.
For example, in the pattern ?X solves Y?, ?solves? is
the pattern word, while ?X? and ?Y? are slots. One
can generate a text unit (phrase or sentence) by fill-
ing the pattern slots with specific words. Paraphrase
patterns are useful in both paraphrase recognition
and generation. In paraphrase recognition, if two
text units match a pair of paraphrase patterns and the
corresponding slot-fillers are identical, they can be
identified as paraphrases. In paraphrase generation,
a text unit that matches a pattern P can be rewritten
using the paraphrase patterns of P.
A variety of methods have been proposed on para-
phrase patterns extraction (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Ibrahim et al, 2003;
Pang et al, 2003; Szpektor et al, 2004). However,
these methods have some shortcomings. Especially,
the precisions of the paraphrase patterns extracted
with these methods are relatively low.
In this paper, we extract paraphrase patterns from
bilingual parallel corpora based on a pivot approach.
We assume that if two English patterns are aligned
with the same pattern in another language, they are
likely to be paraphrase patterns. This assumption
is an extension of the one presented in (Bannard
and Callison-Burch, 2005), which was used for de-
riving phrasal paraphrases from bilingual corpora.
Our method involves three steps: (1) corpus prepro-
cessing, including English monolingual dependency
780
parsing and English-foreign language word align-
ment, (2) aligned patterns induction, which produces
English patterns along with the aligned pivot pat-
terns in the foreign language, (3) paraphrase pat-
terns extraction, in which paraphrase patterns are ex-
tracted based on a log-linear model.
Our contributions are as follows. Firstly, we are
the first to use a pivot approach to extract paraphrase
patterns from bilingual corpora, though similar
methods have been used for learning phrasal para-
phrases. Our experiments show that the pivot ap-
proach significantly outperforms conventional meth-
ods. Secondly, we propose a log-linear model for
computing the paraphrase likelihood. Besides, we
use feature functions based on maximum likeli-
hood estimation (MLE) and lexical weighting (LW),
which are effective in extracting paraphrase patterns.
Using the proposed approach, we extract over
1,000,000 pairs of paraphrase patterns from 2M
bilingual sentence pairs, the precision of which is
above 67%. Experimental results show that the pivot
approach evidently outperforms DIRT, a well known
method that extracts paraphrase patterns frommono-
lingual corpora (Lin and Pantel, 2001). Besides, the
log-linear model is more effective than the conven-
tional model presented in (Bannard and Callison-
Burch, 2005). In addition, the coverage of the ex-
tracted paraphrase patterns is high, which is above
84%. Further analysis shows that 5 types of para-
phrase patterns can be extracted with our method,
which can by used in multiple NLP applications.
The rest of this paper is structured as follows.
Section 2 reviews related work on paraphrase pat-
terns extraction. Section 3 presents our method in
detail. We evaluate the proposed method in Section
4, and finally conclude this paper in Section 5.
2 Related Work
Paraphrase patterns have been learned and used in
information extraction (IE) and answer extraction of
QA. For example, Lin and Pantel (2001) proposed a
method (DIRT), in which they obtained paraphrase
patterns from a parsed monolingual corpus based on
an extended distributional hypothesis, where if two
paths in dependency trees tend to occur in similar
contexts it is hypothesized that the meanings of the
paths are similar. The examples of obtained para-
(1) X solves Y
Y is solved by X
X finds a solution to Y
......
(2) born in <ANSWER> , <NAME>
<NAME> was born on <ANSWER> ,
<NAME> ( <ANSWER> -
......
(3) ORGANIZATION decides ?
ORGANIZATION confirms ?
......
Table 1: Examples of paraphrase patterns extracted with
the methods of Lin and Pantel (2001), Ravichandran and
Hovy (2002), and Shinyama et al (2002).
phrase patterns are shown in Table 1 (1).
Based on the same hypothesis as above, some
methods extracted paraphrase patterns from the web.
For instance, Ravichandran and Hovy (2002) de-
fined a question taxonomy for their QA system.
They then used hand-crafted examples of each ques-
tion type as queries to retrieve paraphrase patterns
from the web. For instance, for the question type
?BIRTHDAY?, The paraphrase patterns produced by
their method can be seen in Table 1 (2).
Similar methods have also been used by Ibrahim
et al (2003) and Szpektor et al (2004). The main
disadvantage of the above methods is that the pre-
cisions of the learned paraphrase patterns are rela-
tively low. For instance, the precisions of the para-
phrase patterns reported in (Lin and Pantel, 2001),
(Ibrahim et al, 2003), and (Szpektor et al, 2004)
are lower than 50%. Ravichandran and Hovy (2002)
did not directly evaluate the precision of the para-
phrase patterns extracted using their method. How-
ever, the performance of their method is dependent
on the hand-crafted queries for web mining.
Shinyama et al (2002) presented a method that
extracted paraphrase patterns from multiple news ar-
ticles about the same event. Their method was based
on the assumption that NEs are preserved across
paraphrases. Thus the method acquired paraphrase
patterns from sentence pairs that share comparable
NEs. Some examples can be seen in Table 1 (3).
The disadvantage of this method is that it greatly
relies on the number of NEs in sentences. The preci-
781
start Palestinian suicide bomber blew himself up in SLOT1 on SLOT2
killing SLOT3 other people and injuringwounding SLOT4 end
detroit the*e*
a
?s*e* buildingbuilding in detroit
flattened
groundlevelled
to
blastedleveled*e*wasreduced
razedleveled
to downrubble
into ashes*e*
to *e*
(1)
(2)
Figure 1: Examples of paraphrase patterns extracted by
Barzilay and Lee (2003) and Pang et al (2003).
sion of the extracted patterns may sharply decrease
if the sentences do not contain enough NEs.
Barzilay and Lee (2003) applied multi-sequence
alignment (MSA) to parallel news sentences and in-
duced paraphrase patterns for generating new sen-
tences (Figure 1 (1)). Pang et al (2003) built finite
state automata (FSA) from semantically equivalent
translation sets based on syntactic alignment. The
learned FSAs could be used in paraphrase represen-
tation and generation (Figure 1 (2)). Obviously, it
is difficult for a sentence to match such complicated
patterns, especially if the sentence is not from the
same domain in which the patterns are extracted.
Bannard and Callison-Burch (2005) first ex-
ploited bilingual corpora for phrasal paraphrase ex-
traction. They assumed that if two English phrases
e1 and e2 are aligned with the same phrase c in
another language, these two phrases may be para-
phrases. Specifically, they computed the paraphrase
probability in terms of the translation probabilities:
p(e2|e1) =
?
c
pMLE(c|e1)pMLE(e2|c) (1)
In Equation (1), pMLE(c|e1) and pMLE(e2|c) are
the probabilities of translating e1 to c and c to e2,
which are computed based on MLE:
pMLE(c|e1) =
count(c, e1)
?
c? count(c
?, e1)
(2)
where count(c, e1) is the frequency count that
phrases c and e1 are aligned in the corpus.
pMLE(e2|c) is computed in the same way.
This method proved effective in extracting high
quality phrasal paraphrases. As a result, we extend
it to paraphrase pattern extraction in this paper.
S T E (take)
should
We take
market
into
consideration
take
market
into
consideration
take
into
consideration
P S T E (take)
first
T E
demand
demand
Figure 2: Examples of a subtree and a partial subtree.
3 Proposed Method
3.1 Corpus Preprocessing
In this paper, we use English paraphrase patterns ex-
traction as a case study. An English-Chinese (E-
C) bilingual parallel corpus is employed for train-
ing. The Chinese part of the corpus is used as pivots
to extract English paraphrase patterns. We conduct
word alignment with Giza++ (Och and Ney, 2000) in
both directions and then apply the grow-diag heuris-
tic (Koehn et al, 2005) for symmetrization.
Since the paraphrase patterns are extracted from
dependency trees, we parse the English sentences
in the corpus with MaltParser (Nivre et al, 2007).
Let SE be an English sentence, TE the parse tree
of SE , e a word of SE , we define the subtree and
partial subtree following the definitions in (Ouan-
graoua et al, 2007). In detail, a subtree STE(e)
is a particular connected subgraph of the tree TE ,
which is rooted at e and includes all the descendants
of e. A partial subtree PSTE(e) is a connected sub-
graph of the subtree STE(e), which is rooted at e but
does not necessarily include all the descendants of e.
For instance, for the sentence ?We should first take
market demand into consideration?, STE(take) and
PSTE(take) are shown in Figure 21.
3.2 Aligned Patterns Induction
To induce the aligned patterns, we first induce the
English patterns using the subtrees and partial sub-
trees. Then, we extract the pivot Chinese patterns
aligning to the English patterns.
1Note that, a subtree may contain several partial subtrees. In
this paper, all the possible partial subtrees are considered when
extracting paraphrase patterns.
782
Algorithm 1: Inducing an English pattern
1: Input: words in STE(e) : wiwi+1...wj
2: Input: PE(e) = ?
3: For each wk (i ? k ? j)
4: If wk is in PSTE(e)
5: Append wk to the end of PE(e)
6: Else
7: Append POS(wk) to the end of PE(e)
8: End For
Algorithm 2: Inducing an aligned pivot pattern
1: Input: SC = t1t2...tn
2: Input: PC = ?
3: For each tl (1 ? l ? n)
4: If tl is aligned with wk in SE
5: If wk is a word in PE(e)
6: Append tl to the end of PC
7: If POS(wk) is a slot in PE(e)
8: Append POS(wk) to the end of PC
9: End For
Step-1 Inducing English patterns. In this paper, an
English pattern PE(e) is a string comprising words
and part-of-speech (POS) tags. Our intuition for
inducing an English pattern is that a partial sub-
tree PSTE(e) can be viewed as a unit that conveys
a definite meaning, though the words in PSTE(e)
may not be continuous. For example, PSTE(take)
in Figure 2 contains words ?take ... into consid-
eration?. Therefore, we may extract ?take X into
consideration? as a pattern. In addition, the words
that are in STE(e) but not in PSTE(e) (denoted as
STE(e)/PSTE(e)) are also useful for inducing pat-
terns, since they can constrain the pattern slots. In
the example in Figure 2, the word ?demand? indi-
cates that a noun can be filled in the slot X and the
pattern may have the form ?take NN into considera-
tion?. Based on this intuition, we induce an English
pattern PE(e) as in Algorithm 12.
For the example in Figure 2, the generated pat-
tern PE(take) is ?take NN NN into considera-
tion?. Note that the patterns induced in this way
are quite specific, since the POS of each word in
STE(e)/PSTE(e) forms a slot. Such patterns are
difficult to be matched in applications. We there-
2POS(wk) in Algorithm 1 denotes the POS tag of wk.
N N _1 ?? NN _2 NN_1 ?? N N _2
NN_1NN_2 considered byis NN_1 consider NN_2
Figure 3: Aligned patterns with numbered slots.
fore take an additional step to simplify the patterns.
Let ei and ej be two words in STE(e)/PSTE(e),
whose POS posi and posj are slots in PE(e). If ei
is a descendant of ej in the parse tree, we remove
posi from PE(e). For the example above, the POS
of ?market? is removed, since it is the descendant of
?demand?, whose POS also forms a slot. The sim-
plified pattern is ?take NN into consideration?.
Step-2 Extracting pivot patterns. For each En-
glish pattern PE(e), we extract an aligned Chinese
pivot pattern PC . Let a Chinese sentence SC be the
translation of the English sentence SE , PE(e) a pat-
tern induced from SE , we extract the pivot pattern
PC aligning to PE(e) as in Algorithm 2. Note that
the Chinese patterns are not extracted from parse
trees. They are only sequences of Chinese words
and POSes that are aligned with English patterns.
A pattern may contain two or more slots shar-
ing the same POS. To distinguish them, we assign
a number to each slot in the aligned E-C patterns. In
detail, the slots having identical POS in PC are num-
bered incrementally (i.e., 1,2,3...), while each slot in
PE(e) is assigned the same number as its aligned
slot in PC . The examples of the aligned patterns
with numbered slots are illustrated in Figure 3.
3.3 Paraphrase Patterns Extraction
As mentioned above, if patterns e1 and e2 are
aligned with the same pivot pattern c, e1 and e2 may
be paraphrase patterns. The paraphrase likelihood
can be computed using Equation (1). However, we
find that using only the MLE based probabilities can
suffer from data sparseness. In order to exploit more
and richer information to estimate the paraphrase
likelihood, we propose a log-linear model:
score(e2|e1) =
?
c
exp[
N?
i=1
?ihi(e1, e2, c)] (3)
where hi(e1, e2, c) is a feature function and ?i is the
783
weight. In this paper, 4 feature functions are used in
our log-linear model, which include:
h1(e1, e2, c) = scoreMLE(c|e1)
h2(e1, e2, c) = scoreMLE(e2|c)
h3(e1, e2, c) = scoreLW (c|e1)
h4(e1, e2, c) = scoreLW (e2|c)
Feature functions h1(e1, e2, c) and h2(e1, e2, c)
are based on MLE. scoreMLE(c|e) is computed as:
scoreMLE(c|e) = log pMLE(c|e) (4)
scoreMLE(e|c) is computed in the same way.
h3(e1, e2, c) and h4(e1, e2, c) are based on LW.
LW was originally used to validate the quality of a
phrase translation pair in MT (Koehn et al, 2003). It
checks how well the words of the phrases translate
to each other. This paper uses LW to measure the
quality of aligned patterns. We define scoreLW (c|e)
as the logarithm of the lexical weight3:
scoreLW (c|e) =
1
n
n?
i=1
log(
1
|{j|(i, j) ? a}|
?
?(i,j)?a
w(ci|ej)) (5)
where a denotes the word alignment between c and
e. n is the number of words in c. ci and ej are words
of c and e. w(ci|ej) is computed as follows:
w(ci|ej) =
count(ci, ej)
?
c?i
count(c?i, ej)
(6)
where count(ci, ej) is the frequency count of
the aligned word pair (ci, ej) in the corpus.
scoreLW (e|c) is computed in the same manner.
In our experiments, we set a threshold T . If the
score between e1 and e2 based on Equation (3) ex-
ceeds T , e2 is extracted as the paraphrase of e1.
3.4 Parameter Estimation
Five parameters need to be estimated, i.e., ?1, ?2,
?3, ?4 in Equation (3), and the threshold T . To
estimate the parameters, we first construct a devel-
opment set. In detail, we randomly sample 7,086
3The logarithm of the lexical weight is divided by n so as
not to penalize long patterns.
groups of aligned E-C patterns that are obtained as
described in Section 3.2. The English patterns in
each group are all aligned with the same Chinese
pivot pattern. We then extract paraphrase patterns
from the aligned patterns as described in Section 3.3.
In this process, we set ?i = 1 (i = 1, ..., 4) and as-
sign T a minimum value, so as to obtain all possible
paraphrase patterns.
A total of 4,162 pairs of paraphrase patterns have
been extracted and manually labeled as ?1? (correct
paraphrase patterns) or ?0? (incorrect). Here, two
patterns are regarded as paraphrase patterns if they
can generate paraphrase fragments by filling the cor-
responding slots with identical words. We use gra-
dient descent algorithm (Press et al, 1992) to esti-
mate the parameters. For each set of parameters, we
compute the precision P , recall R, and f-measure
F as: P = |set1?set2||set1| , R =
|set1?set2|
|set2| , F =
2PR
P+R ,
where set1 denotes the set of paraphrase patterns ex-
tracted under the current parameters. set2 denotes
the set of manually labeled correct paraphrase pat-
terns. We select the parameters that can maximize
the F-measure on the development set4.
4 Experiments
The E-C parallel corpus in our experiments was con-
structed using several LDC bilingual corpora5. After
filtering sentences that are too long (> 40 words) or
too short (< 5 words), 2,048,009 pairs of parallel
sentences were retained.
We used two constraints in the experiments to im-
prove the efficiency of computation. First, only sub-
trees containing no more than 10 words were used to
induce English patterns. Second, although any POS
tag can form a slot in the induced patterns, we only
focused on three kinds of POSes in the experiments,
i.e., nouns (tags include NN, NNS, NNP, NNPS),
verbs (VB, VBD, VBG, VBN, VBP, VBZ), and ad-
jectives (JJ, JJS, JJR). In addition, we constrained
that a pattern must contain at least one content word
4The parameters are: ?1 = 0.0594137, ?2 = 0.995936,
?3 = ?0.0048954, ?4 = 1.47816, T = ?10.002.
5The corpora include LDC2000T46, LDC2000T47,
LDC2002E18, LDC2002T01, LDC2003E07, LDC2003E14,
LDC2003T17, LDC2004E12, LDC2004T07, LDC2004T08,
LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T04,
LDC2007T02, LDC2007T09.
784
Method #PP (pairs) Precision
LL-Model 1,058,624 67.03%
MLE-Model 1,015,533 60.60%
DIRT top-1 1,179 19.67%
DIRT top-5 5,528 18.73%
Table 2: Comparison of paraphrasing methods.
so as to filter patterns like ?the [NN 1]?.
4.1 Evaluation of the Log-linear Model
As previously mentioned, in the log-linear model of
this paper, we use both MLE based and LW based
feature functions. In this section, we evaluate the
log-linear model (LL-Model) and compare it with
the MLE based model (MLE-Model) presented by
Bannard and Callison-Burch (2005)6.
We extracted paraphrase patterns using two mod-
els, respectively. From the results of each model,
we randomly picked 3,000 pairs of paraphrase pat-
terns to evaluate the precision. The 6,000 pairs of
paraphrase patterns were mixed and presented to the
human judges, so that the judges cannot know by
which model each pair was produced. The sampled
patterns were then manually labeled and the preci-
sion was computed as described in Section 3.4.
The number of the extracted paraphrase patterns
(#PP) and the precision are depicted in the first two
lines of Table 2. We can see that the numbers of
paraphrase patterns extracted using the two mod-
els are comparable. However, the precision of LL-
Model is significantly higher than MLE-Model.
Actually, MLE-Model is a special case of LL-
Model and the enhancement of the precision is
mainly due to the use of LW based features.
It is not surprising, since Bannard and Callison-
Burch (2005) have pointed out that word alignment
error is the major factor that influences the perfor-
mance of the methods learning paraphrases from
bilingual corpora. The LW based features validate
the quality of word alignment and assign low scores
to those aligned E-C pattern pairs with incorrect
alignment. Hence the precision can be enhanced.
6In this experiment, we also estimated a threshold T ? for
MLE-Model using the development set (T ? = ?5.1). The pat-
tern pairs whose score based on Equation (1) exceed T ? were
extracted as paraphrase patterns.
4.2 Comparison with DIRT
It is necessary to compare our method with another
paraphrase patterns extraction method. However, it
is difficult to find methods that are suitable for com-
parison. Some methods only extract paraphrase pat-
terns using news articles on certain topics (Shinyama
et al, 2002; Barzilay and Lee, 2003), while some
others need seeds as initial input (Ravichandran and
Hovy, 2002). In this paper, we compare our method
with DIRT (Lin and Pantel, 2001), which does not
need to specify topics or input seeds.
As mentioned in Section 2, DIRT learns para-
phrase patterns from a parsed monolingual corpus
based on an extended distributional hypothesis. In
our experiment, we implemented DIRT and ex-
tracted paraphrase patterns from the English part of
our bilingual parallel corpus. Our corpus is smaller
than that reported in (Lin and Pantel, 2001). To alle-
viate the data sparseness problem, we only kept pat-
terns appearing more than 10 times in the corpus for
extracting paraphrase patterns. Different from our
method, no threshold was set in DIRT. Instead, the
extracted paraphrase patterns were ranked accord-
ing to their scores. In our experiment, we kept top-5
paraphrase patterns for each target pattern.
From the extracted paraphrase patterns, we sam-
pled 600 groups for evaluation. Each group com-
prises a target pattern and its top-5 paraphrase pat-
terns. The sampled data were manually labeled and
the top-n precision was calculated as
PN
i=1 ni
N?n , where
N is the number of groups and ni is the number of
correct paraphrase patterns in the top-n paraphrase
patterns of the i-th group. The top-1 and top-5 re-
sults are shown in the last two lines of Table 2. Al-
though there are more correct patterns in the top-5
results, the precision drops sequentially from top-1
to top-5 since the denominator of top-5 is 4 times
larger than that of top-1.
Obviously, the number of the extracted para-
phrase patterns is much smaller than that extracted
using our method. Besides, the precision is also
much lower. We believe that there are two reasons.
First, the extended distributional hypothesis is not
strict enough. Patterns sharing similar slot-fillers do
not necessarily have the same meaning. They may
even have the opposite meanings. For example, ?X
worsens Y? and ?X solves Y? were extracted as para-
785
Type Count Example
trivial change 79 (e1) all the members of [NNPS 1] (e2) all members of [NNPS 1]
phrase replacement 267 (e1) [JJ 1] economic losses (e2) [JJ 1] financial losses
phrase reordering 56 (e1) [NN 1] definition (e2) the definition of [NN 1]
structural paraphrase 71 (e1) the admission of [NNP 1] to the wto (e2) the [NNP 1] ?s wto accession
information + or - 27 (e1) [NNS 1] are in fact women (e2) [NNS 1] are women
Table 3: The statistics and examples of each type of paraphrase patterns.
phrase patterns by DIRT. The other reason is that
DIRT can only be effective for patterns appearing
plenty of times in the corpus. In other words, it seri-
ously suffers from data sparseness. We believe that
DIRT can perform better on a larger corpus.
4.3 Pivot Pattern Constraints
As described in Section 3.2, we constrain that the
pattern words of an English pattern e must be ex-
tracted from a partial subtree. However, we do not
have such constraint on the Chinese pivot patterns.
Hence, it is interesting to investigate whether the
performance can be improved if we constrain that
the pattern words of a pivot pattern c must also be
extracted from a partial subtree.
To conduct the evaluation, we parsed the Chinese
sentences of the corpus with a Chinese dependency
parser (Liu et al, 2006). We then induced English
patterns and extracted aligned pivot patterns. For the
aligned patterns (e, c), if c?s pattern words were not
extracted from a partial subtree, the pair was filtered.
After that, we extracted paraphrase patterns, from
which we sampled 3,000 pairs for evaluation.
The results show that 736,161 pairs of paraphrase
patterns were extracted and the precision is 65.77%.
Compared with Table 2, the number of the extracted
paraphrase patterns gets smaller and the precision
also gets lower. The results suggest that the perfor-
mance of the method cannot be improved by con-
straining the extraction of pivot patterns.
4.4 Analysis of the Paraphrase Patterns
We sampled 500 pairs of correct paraphrase pat-
terns extracted using our method and analyzed the
types. We found that there are 5 types of para-
phrase patterns, which include: (1) trivial change,
such as changes of prepositions and articles, etc; (2)
phrase replacement; (3) phrase reordering; (4) struc-
tural paraphrase, which contain both phrase replace-
ments and phrase reordering; (5) adding or reducing
information that does not change the meaning. Some
statistics and examples are shown in Table 3.
The paraphrase patterns are useful in NLP appli-
cations. Firstly, over 50% of the paraphrase patterns
are in the type of phrase replacement, which can
be used in IE pattern reformulation and sentence-
level paraphrase generation. Compared with phrasal
paraphrases, the phrase replacements in patterns are
more accurate due to the constraints of the slots.
The paraphrase patterns in the type of phrase re-
ordering can also be used in IE pattern reformula-
tion and sentence paraphrase generation. Especially,
in sentence paraphrase generation, this type of para-
phrase patterns can reorder the phrases in a sentence,
which can hardly be achieved by the conventional
MT-based generation method (Quirk et al, 2004).
The structural paraphrase patterns have the advan-
tages of both phrase replacement and phrase reorder-
ing. More paraphrase sentences can be generated
using these patterns.
The paraphrase patterns in the type of ?informa-
tion + and -? are useful in sentence compression and
expansion. A sentence matching a long pattern can
be compressed by paraphrasing it using shorter pat-
terns. Similarly, a short sentence can be expanded
by paraphrasing it using longer patterns.
For the 3,000 pairs of test paraphrase patterns, we
also investigate the number and type of the pattern
slots. The results are summarized in Table 4 and 5.
From Table 4, we can see that more than 92%
of the paraphrase patterns contain only one slot,
just like the examples shown in Table 3. In addi-
tion, about 7% of the paraphrase patterns contain
two slots, such as ?give [NN 1] [NN 2]? vs. ?give
[NN 2] to [NN 1]?. This result suggests that our
method tends to extract short paraphrase patterns,
786
Slot No. #PP Percentage Precision
1-slot 2,780 92.67% 66.51%
2-slots 218 7.27% 73.85%
?3-slots 2 <1% 50.00%
Table 4: The statistics of the numbers of pattern slots.
Slot Type #PP Percentage Precision
N-slots 2,376 79.20% 66.71%
V-slots 273 9.10% 70.33%
J-slots 438 14.60% 70.32%
Table 5: The statistics of the type of pattern slots.
which is mainly because the data sparseness prob-
lem is more serious when extracting long patterns.
From Table 5, we can find that near 80% of the
paraphrase patterns contain noun slots, while about
9% and 15% contain verb slots and adjective slots7.
This result implies that nouns are the most typical
variables in paraphrase patterns.
4.5 Evaluation within Context Sentences
In Section 4.1, we have evaluated the precision of
the paraphrase patterns without considering context
information. In this section, we evaluate the para-
phrase patterns within specific context sentences.
The open test set includes 119 English sentences.
We parsed the sentences with MaltParser and in-
duced patterns as described in Section 3.2. For each
pattern e in sentence SE , we searched e?s paraphrase
patterns from the database of the extracted para-
phrase patterns. The result shows that 101 of the
119 sentences contain at least one pattern that can
be paraphrased using the extracted paraphrase pat-
terns, the coverage of which is 84.87%.
Furthermore, since a pattern may have several
paraphrase patterns, we exploited a method to au-
tomatically select the best one in the given context
sentence. In detail, a paraphrase pattern e? of e was
reranked based on a language model (LM):
score(e?|e, SE) =
?scoreLL(e
?|e) + (1 ? ?)scoreLM (e
?|SE) (7)
7Notice that, a pattern may contain more than one type of
slots, thus the sum of the percentages is larger than 1.
Here, scoreLL(e?|e) denotes the score based on
Equation (3). scoreLM (e?|SE) is the LM based
score: scoreLM (e?|SE) = 1n logPLM (S
?
E), where
S?E is the sentence generated by replacing e in SE
with e?. The language model in the experiment was
a tri-gram model trained using the English sentences
in the bilingual corpus. We empirically set ? = 0.7.
The selected best paraphrase patterns in context
sentences were manually labeled. The context infor-
mation was also considered by our judges. The re-
sult shows that the precision of the best paraphrase
patterns is 59.39%. To investigate the contribution
of the LM based score, we ran the experiment again
with ? = 1 (ignoring the LM based score) and found
that the precision is 57.09%. It indicates that the LM
based reranking can improve the precision. How-
ever, the improvement is small. Further analysis
shows that about 70% of the correct paraphrase sub-
stitutes are in the type of phrase replacement.
5 Conclusion
This paper proposes a pivot approach for extracting
paraphrase patterns from bilingual corpora. We use
a log-linear model to compute the paraphrase like-
lihood and exploit feature functions based on MLE
and LW. Experimental results show that the pivot ap-
proach is effective, which extracts over 1,000,000
pairs of paraphrase patterns from 2M bilingual sen-
tence pairs. The precision and coverage of the ex-
tracted paraphrase patterns exceed 67% and 84%,
respectively. In addition, the log-linear model with
the proposed feature functions significantly outper-
forms the conventional models. Analysis shows that
5 types of paraphrase patterns are extracted with our
method, which are useful in various applications.
In the future we wish to exploit more feature func-
tions in the log-linear model. In addition, we will try
to make better use of the context information when
replacing paraphrase patterns in context sentences.
Acknowledgments
This research was supported by National Nat-
ural Science Foundation of China (60503072,
60575042). We thank Lin Zhao, Xiaohang Qu, and
Zhenghua Li for their help in the experiments.
787
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolingual
Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase in a
Meaning-Text Generation Model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann (Eds.):
Natural Language Generation in Artificial Intelligence
and Computational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ting Liu, Jin-Shan Ma, Hui-Jia Zhu, and Sheng Li. 2006.
Dependency Parsing Based on Dynamic Local Opti-
mization. In Proceedings of CoNLL-X, pages 211-215.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Natural Language Engineering 13(2): 95-
135.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
A??da Ouangraoua, Pascal Ferraro, Laurent Tichit, and
Serge Dulucq. 2007. Local Similarity between Quo-
tiented Ordered Trees. In Journal of Discrete Algo-
rithms 5(1): 23-35.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition
of Entailment Relations. In Proceedings of EMNLP,
pages 41-48.
788
Proceedings of ACL-08: HLT, pages 1021?1029,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining Multiple Resources to Improve SMT-based Paraphrasing Model?
Shiqi Zhao1, Cheng Niu2, Ming Zhou2, Ting Liu1, Sheng Li1
1Harbin Institute of Technology, Harbin, China
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
2Microsoft Research Asia, Beijing, China
{chengniu,mingzhou}@microsoft.com
Abstract
This paper proposes a novel method that ex-
ploits multiple resources to improve statisti-
cal machine translation (SMT) based para-
phrasing. In detail, a phrasal paraphrase ta-
ble and a feature function are derived from
each resource, which are then combined in a
log-linear SMT model for sentence-level para-
phrase generation. Experimental results show
that the SMT-based paraphrasing model can
be enhanced using multiple resources. The
phrase-level and sentence-level precision of
the generated paraphrases are above 60% and
55%, respectively. In addition, the contribu-
tion of each resource is evaluated, which indi-
cates that all the exploited resources are useful
for generating paraphrases of high quality.
1 Introduction
Paraphrases are alternative ways of conveying the
same meaning. Paraphrases are important in many
natural language processing (NLP) applications,
such as machine translation (MT), question an-
swering (QA), information extraction (IE), multi-
document summarization (MDS), and natural lan-
guage generation (NLG).
This paper addresses the problem of sentence-
level paraphrase generation, which aims at generat-
ing paraphrases for input sentences. An example of
sentence-level paraphrases can be seen below:
S1: The table was set up in the carriage shed.
S2: The table was laid under the cart-shed.
?This research was finished while the first author worked as
an intern in Microsoft Research Asia.
Paraphrase generation can be viewed as monolin-
gual machine translation (Quirk et al, 2004), which
typically includes a translation model and a lan-
guage model. The translation model can be trained
using monolingual parallel corpora. However, ac-
quiring such corpora is not easy. Hence, data sparse-
ness is a key problem for the SMT-based paraphras-
ing. On the other hand, various methods have been
presented to extract phrasal paraphrases from dif-
ferent resources, which include thesauri, monolin-
gual corpora, bilingual corpora, and the web. How-
ever, little work has been focused on using the ex-
tracted phrasal paraphrases in sentence-level para-
phrase generation.
In this paper, we exploit multiple resources to
improve the SMT-based paraphrase generation. In
detail, six kinds of resources are utilized, includ-
ing: (1) an automatically constructed thesaurus, (2)
a monolingual parallel corpus from novels, (3) a
monolingual comparable corpus from news articles,
(4) a bilingual phrase table, (5) word definitions
from Encarta dictionary, and (6) a corpus of simi-
lar user queries. Among the resources, (1), (2), (3),
and (4) have been investigated by other researchers,
while (5) and (6) are first used in this paper. From
those resources, six phrasal paraphrase tables are ex-
tracted, which are then used in a log-linear SMT-
based paraphrasing model.
Both phrase-level and sentence-level evaluations
were carried out in the experiments. In the former
one, phrase substitutes occurring in the paraphrase
sentences were evaluated. While in the latter one,
the acceptability of the paraphrase sentences was
evaluated. Experimental results show that: (1) The
1021
SMT-based paraphrasing is enhanced using multiple
resources. The phrase-level and sentence-level pre-
cision of the generated paraphrases exceed 60% and
55%, respectively. (2) Although the contributions of
the resources differ a lot, all the resources are useful.
(3) The performance of the method varies greatly on
different test sets and it performs best on the test set
of news sentences, which are from the same source
as most of the training data.
The rest of the paper is organized as follows: Sec-
tion 2 reviews related work. Section 3 introduces the
log-linear model for paraphrase generation. Section
4 describes the phrasal paraphrase extraction from
different resources. Section 5 presents the parameter
estimation method. Section 6 shows the experiments
and results. Section 7 draws the conclusion.
2 Related Work
Paraphrases have been used in many NLP applica-
tions. In MT, Callison-Burch et al (2006) utilized
paraphrases of unseen source phrases to alleviate
data sparseness. Kauchak and Barzilay (2006) used
paraphrases of the reference translations to improve
automatic MT evaluation. In QA, Lin and Pantel
(2001) and Ravichandran and Hovy (2002) para-
phrased the answer patterns to enhance the recall of
answer extraction. In IE, Shinyama et al (2002)
automatically learned paraphrases of IE patterns to
reduce the cost of creating IE patterns by hand. In
MDS, McKeown et al (2002) identified paraphrase
sentences across documents before generating sum-
marizations. In NLG, Iordanskaja et al (1991) used
paraphrases to generate more varied and fluent texts.
Previous work has examined various resources for
acquiring paraphrases, including thesauri, monolin-
gual corpora, bilingual corpora, and the web. The-
sauri, such as WordNet, have been widely used
for extracting paraphrases. Some researchers ex-
tract synonyms as paraphrases (Kauchak and Barzi-
lay, 2006), while some others use looser defini-
tions, such as hypernyms and holonyms (Barzilay
and Elhadad, 1997). Besides, the automatically
constructed thesauri can also be used. Lin (1998)
constructed a thesaurus by automatically clustering
words based on context similarity.
Barzilay andMcKeown (2001) used monolingual
parallel corpora for identifying paraphrases. They
exploited a corpus of multiple English translations
of the same source text written in a foreign language,
from which phrases in aligned sentences that appear
in similar contexts were extracted as paraphrases. In
addition, Finch et al (2005) applied MT evalua-
tion methods (BLEU, NIST,WER and PER) to build
classifiers for paraphrase identification.
Monolingual parallel corpora are difficult to find,
especially in non-literature domains. Alternatively,
some researchers utilized monolingual compara-
ble corpora for paraphrase extraction. Different
news articles reporting on the same event are com-
monly used as monolingual comparable corpora,
from which both paraphrase patterns and phrasal
paraphrases can be derived (Shinyama et al, 2002;
Barzilay and Lee, 2003; Quirk et al, 2004).
Lin and Pantel (2001) learned paraphrases from
a parsed monolingual corpus based on an extended
distributional hypothesis, where if two paths in de-
pendency trees tend to occur in similar contexts it is
hypothesized that the meanings of the paths are simi-
lar. The monolingual corpus used in their work is not
necessarily parallel or comparable. Thus it is easy
to obtain. However, since this resource is used to
extract paraphrase patterns other than phrasal para-
phrases, we do not use it in this paper.
Bannard and Callison-Burch (2005) learned
phrasal paraphrases using bilingual parallel cor-
pora. The basic idea is that if two phrases are
aligned to the same translation in a foreign language,
they may be paraphrases. This method has been
demonstrated effective in extracting large volume of
phrasal paraphrases. Besides, Wu and Zhou (2003)
exploited bilingual corpora and translation informa-
tion in learning synonymous collocations.
In addition, some researchers extracted para-
phrases from the web. For example, Ravichandran
and Hovy (2002) retrieved paraphrase patterns from
the web using hand-crafted queries. Pasca and Di-
enes (2005) extracted sentence fragments occurring
in identical contexts as paraphrases from one bil-
lion web documents. Since web mining is rather
time consuming, we do not exploit the web to ex-
tract paraphrases in this paper.
So far, two kinds of methods have been pro-
posed for sentence-level paraphrase generation, i.e.,
the pattern-based and SMT-based methods. Auto-
matically learned patterns have been used in para-
1022
phrase generation. For example, Barzilay and Lee
(2003) applied multiple-sequence alignment (MSA)
to parallel news sentences and induced paraphras-
ing patterns for generating new sentences. Pang et
al. (2003) built finite state automata (FSA) from se-
mantically equivalent translation sets based on syn-
tactic alignment and used the FSAs in paraphrase
generation. The pattern-based methods can generate
complex paraphrases that usually involve syntactic
variation. However, the methods were demonstrated
to be of limited generality (Quirk et al, 2004).
Quirk et al (2004) first recast paraphrase gener-
ation as monolingual SMT. They generated para-
phrases using a SMT system trained on parallel sen-
tences extracted from clustered news articles. In
addition, Madnani et al (2007) also generated
sentence-level paraphrases based on a SMT model.
The advantage of the SMT-based method is that
it achieves better coverage than the pattern-based
method. The main difference between their methods
and ours is that they only used bilingual parallel cor-
pora as paraphrase resource, while we exploit and
combine multiple resources.
3 SMT-based Paraphrasing Model
The SMT-based paraphrasing model used by Quirk
et al (2004) was the noisy channel model of Brown
et al (1993), which identified the optimal paraphrase
T ? of a sentence S by finding:
T ? = argmax
T
{P (T |S)}
= argmax
T
{P (S|T )P (T )} (1)
In contrast, we adopt a log-linear model (Och
and Ney, 2002) in this work, since multiple para-
phrase tables can be easily combined in the log-
linear model. Specifically, feature functions are de-
rived from each paraphrase resource and then com-
bined with the language model feature1:
T ? = argmax
T
{
N?
i=1
?TM ihTM i(T, S)+
?LMhLM (T, S)} (2)
where N is the number of paraphrase tables.
hTM i(T, S) is the feature function based on the i-
th paraphrase table PTi. hLM (T, S) is the language
1The reordering model is not considered in our model.
model feature. ?TM i and ?LM are the weights of
the feature functions. hTM i(T, S) is defined as:
hTM i(T, S) = log
Ki?
k=1
Scorei(Tk, Sk) (3)
where Ki is the number of phrase substitutes from
S to T based on PTi. Tk in T and Sk in S are
phrasal paraphrases in PTi. Scorei(Tk, Sk) is the
paraphrase likelihood according to PTi2. A 5-gram
language model is used, therefore:
hLM (T, S) = log
J?
j=1
p(tj |tj?4, ..., tj?1) (4)
where J is the length of T , tj is the j-th word of T .
4 Exploiting Multiple Resources
This section describes the extraction of phrasal
paraphrases using various resources. Similar to
Pharaoh (Koehn, 2004), our decoder3 uses top 20
paraphrase options for each input phrase in the de-
fault setting. Therefore, we keep at most 20 para-
phrases for a phrase when extracting phrasal para-
phrases using each resource.
1 - Thesaurus: The thesaurus4 used in this work
was automatically constructed by Lin (1998). The
similarity of two words e1 and e2 was calculated
through the surrounding context words that have de-
pendency relations with the investigated words:
Sim(e1, e2)
=
P
(r,e)?Tr(e1)?Tr(e2)
(I(e1, r, e) + I(e2, r, e))
P
(r,e)?Tr(e1)
I(e1, r, e) +
P
(r,e)?Tr(e2)
I(e2, r, e)
(5)
where Tr(ei) denotes the set of words that have de-
pendency relation r with word ei. I(ei, r, e) is the
mutual information between ei, r and e.
For each word, we keep 20 most similar words as
paraphrases. In this way, we extract 502,305 pairs of
paraphrases. The paraphrasing score Score1(p1, p2)
used in Equation (3) is defined as the similarity
based on Equation (5).
2If none of the phrase substitutes from S to T is from PTi
(i.e., Ki = 0), we cannot compute hTM i(T, S) as in Equation
(3). In this case, we assign hTM i(T, S) a minimum value.
3The decoder used here is a re-implementation of Pharaoh.
4http://www.cs.ualberta.ca/ lindek/downloads.htm.
1023
2 - Monolingual parallel corpus: Following Barzi-
lay and McKeown (2001), we exploit a corpus
of multiple English translations of foreign nov-
els, which contains 25,804 parallel sentence pairs.
We find that most paraphrases extracted using the
method of Barzilay and McKeown (2001) are quite
short. Thus we employ a new approach for para-
phrase extraction. Specifically, we parse the sen-
tences with CollinsParser5 and extract the chunks
from the parsing results. Let S1 and S2 be a pair
of parallel sentences, p1 and p2 two chunks from S1
and S2, we compute the similarity of p1 and p2 as:
Sim(p1, p2) = ?Simcontent(p1, p2)+
(1 ? ?)Simcontext(p1, p2) (6)
where, Simcontent(p1, p2) is the content similarity,
which is the word overlapping rate of p1 and p2.
Simcontext(p1, p2) is the context similarity, which is
the word overlapping rate of the contexts of p1 and
p26. If the similarity of p1 and p2 exceeds a thresh-
old Th1, they are identified as paraphrases. We ex-
tract 18,698 pairs of phrasal paraphrases from this
resource. The paraphrasing score Score2(p1, p2) is
defined as the similarity in Equation (6). For the
paraphrases occurring more than once, we use their
maximum similarity as the paraphrasing score.
3 - Monolingual comparable corpus: Similar to
the methods in (Shinyama et al, 2002; Barzilay and
Lee, 2003), we construct a corpus of comparable
documents from a large corpus D of news articles.
The corpusD contains 612,549 news articles. Given
articles d1 and d2 from D, if their publication date
interval is less than 2 days and their similarity7 ex-
ceeds a threshold Th2, they are recognized as com-
parable documents. In this way, a corpus containing
5,672,864 pairs of comparable documents is con-
structed. From the comparable corpus, parallel sen-
tences are extracted. Let s1 and s2 be two sentences
from comparable documents d1 and d2, if their sim-
ilarity based on word overlapping rate is above a
threshold Th3, s1 and s2 are identified as parallel
sentences. In this way, 872,330 parallel sentence
pairs are extracted.
5http://people.csail.mit.edu/mcollins/code.html
6The context of a chunk is made up of 6 words around the
chunk, 3 to the left and 3 to the right.
7The similarity of two documents is computed using the vec-
tor space model and the word weights are based on tf?idf.
We run Giza++ (Och and Ney, 2000) on the paral-
lel sentences and then extract aligned phrases as de-
scribed in (Koehn, 2004). The generated paraphrase
table is pruned by keeping the top 20 paraphrases for
each phrase. After pruning, 100,621 pairs of para-
phrases are extracted. Given phrase p1 and its para-
phrase p2, we compute Score3(p1, p2) by relative
frequency (Koehn et al, 2003):
Score3(p1, p2) = p(p2|p1) =
count(p2, p1)
P
p? count(p
?, p1)
(7)
People may wonder why we do not use the same
method on the monolingual parallel and comparable
corpora. This is mainly because the volumes of the
two corpora differ a lot. In detail, the monolingual
parallel corpus is fairly small, thus automatical word
alignment tool like Giza++ may not work well on
it. In contrast, the monolingual comparable corpus
is quite large, hence we cannot conduct the time-
consuming syntactic parsing on it as we do on the
monolingual parallel corpus.
4 - Bilingual phrase table: We first construct
a bilingual phrase table that contains 15,352,469
phrase pairs from an English-Chinese parallel cor-
pus. We extract paraphrases from the bilingual
phrase table and compute the paraphrasing score
of phrases p1 and p2 as in (Bannard and Callison-
Burch, 2005):
Score4(p1, p2) =
?
f
p(f |p1)p(p2|f) (8)
where f denotes a Chinese translation of both p1 and
p2. p(f |p1) and p(p2|f) are the translation probabil-
ities provided by the bilingual phrase table. For each
phrase, the top 20 paraphrases are kept according
to the score in Equation (8). As a result, 3,177,600
pairs of phrasal paraphrases are extracted.
5 - Encarta dictionary definitions: Words and their
definitions can be regarded as paraphrases. Here
are some examples from Encarta dictionary: ?hur-
ricane: severe storm?, ?clever: intelligent?, ?travel:
go on journey?. In this work, we extract words? def-
initions from Encarta dictionary web pages8. If a
word has more than one definition, all of them are
extracted. Note that the words and definitions in the
8http://encarta.msn.com/encnet/features/dictionary/diction-
aryhome.aspx
1024
dictionary are lemmatized, but words in sentences
are usually inflected. Hence, we expand the word
- definition pairs by providing the inflected forms.
Here we use an inflection list and some rules for in-
flection. After expanding, 159,456 pairs of phrasal
paraphrases are extracted. Let < p1, p2 > be a word
- definition pair, the paraphrasing score is defined
according to the rank of p2 in all of p1?s definitions:
Score5(p1, p2) = ?
i?1 (9)
where ? is a constant (we empirically set ? = 0.9)
and i is the rank of p2 in p1?s definitions.
6 - Similar user queries: Clusters of similar user
queries have been used for query expansion and sug-
gestion (Gao et al, 2007). Since most queries are at
the phrase level, we exploit similar user queries as
phrasal paraphrases. In our experiment, we use the
corpus of clustered similar MSN queries constructed
by Gao et al (2007). The similarity of two queries
p1 and p2 is computed as:
Sim(p1, p2) = ?Simcontent(p1, p2)+
(1 ? ?)Simclick?through(p1, p2) (10)
where Simcontent(p1, p2) is the content similarity,
which is computed as the word overlapping rate of
p1 and p2. Simclick?through(p1, p2) is the click
through similarity, which is the overlapping rate of
the user clicked documents for p1 and p2. For each
query q, we keep the top 20 similar queries, whose
similarity with q exceeds a threshold Th4. As a re-
sult, 395,284 pairs of paraphrases are extracted. The
score Score6(p1, p2) is defined as the similarity in
Equation (10).
7 - Self-paraphrase: In addition to the six resources
introduced above, a special paraphrase table is used,
which is made up of pairs of identical words. The
reason why this paraphrase table is necessary is that
a word should be allowed to keep unchanged in para-
phrasing. This is a difference between paraphras-
ing and MT, since all words should be translated in
MT. In our experiments, all the words that occur in
the six paraphrase table extracted above are gath-
ered to form the self-paraphrase table, which con-
tains 110,403 word pairs. The score Score7(p1, p2)
is set 1 for each identical word pair.
5 Parameter Estimation
The weights of the feature functions, namely ?TM i
(i = 1, 2, ..., 7) and ?LM , need estimation9. In MT,
the max-BLEU algorithm is widely used to estimate
parameters. However, it may not work in our case,
since it is more difficult to create a reference set of
paraphrases.
We propose a new technique to estimate parame-
ters in paraphrasing. The assumption is that, since a
SMT-based paraphrase is generated through phrase
substitution, we can measure the quality of a gener-
ated paraphrase by measuring its phrase substitutes.
Generally, the paraphrases containing more correct
phrase substitutes are judged as better paraphrases10.
We therefore present the phrase substitution error
rate (PSER) to score a generated paraphrase T :
PSER(T ) = ?PS0(T )?/?PS(T )? (11)
where PS(T ) is the set of phrase substitutes in T
and PS0(T ) is the set of incorrect substitutes.
In practice, we keep top n paraphrases for each
sentence S. Thus we calculate the PSER for each
source sentence S as:
PSER(S) = ?
n[
i=1
PS0(Ti)?/?
n[
i=1
PS(Ti)? (12)
where Ti is the i-th generated paraphrase of S.
Suppose there are N sentences in the develop-
ment set, the overall PSER is computed as:
PSER =
NX
j=1
PSER(Sj) (13)
where Sj is the j-th sentence in the development set.
Our development set contains 75 sentences (de-
scribed in detail in Section 6). For each sentence,
all possible phrase substitutes are extracted from the
six paraphrase tables above. The extracted phrase
substitutes are then manually labeled as ?correct? or
?incorrect?. A phrase substitute is considered as cor-
rect only if the two phrases have the same meaning
in the given sentence and the sentence generated by
9Note that, we also use some other parameters when extract-
ing phrasal paraphrases from different resources, such as the
thresholds Th1, Th2, Th3, Th4, as well as ? and ? in Equa-
tion (6) and (10). These parameters are estimated using differ-
ent development sets from the investigated resources. We do
not describe the estimation of them due to space limitation.
10Paraphrasing a word to itself (based on the 7-th paraphrase
table above) is not regarded as a substitute.
1025
substituting the source phrase with the target phrase
remains grammatical. In decoding, the phrase sub-
stitutes are printed out and then the PSER is com-
puted based on the labeled data.
Using each set of parameters, we generate para-
phrases for the sentences in the development set
based on Equation (2). PSER is then computed as
in Equation (13). We use the gradient descent algo-
rithm (Press et al, 1992) to minimize PSER on the
development set and get the optimal parameters.
6 Experiments
To evaluate the performance of the method on dif-
ferent types of test data, we used three kinds of sen-
tences for testing, which were randomly extracted
from Google news, free online novels, and forums,
respectively. For each type, 50 sentences were ex-
tracted as test data and another 25 were extracted as
development data. For each test sentence, top 10 of
the generated paraphrases were kept for evaluation.
6.1 Phrase-level Evaluation
The phrase-level evaluation was carried out to in-
vestigate the contributions of the paraphrase tables.
For each test sentence, all possible phrase substitutes
were first extracted from the paraphrase tables and
manually labeled as ?correct? or ?incorrect?. Here,
the criterion for identifying paraphrases is the same
as that described in Section 5. Then, in the stage
of decoding, the phrase substitutes were printed out
and evaluated using the labeled data.
Two metrics were used here. The first is the
number of distinct correct substitutes (#DCS). Ob-
viously, the more distinct correct phrase substitutes
a paraphrase table can provide, the more valuable it
is. The second is the accuracy of the phrase substi-
tutes, which is computed as:
Accuracy =
#correct phrase substitutes
#all phrase substitutes
(14)
To evaluate the PTs learned from different re-
sources, we first used each PT (from 1 to 6) along
with PT-7 in decoding. The results are shown in Ta-
ble 1. It can be seen that PT-4 is the most useful, as
it provides the most correct substitutes and the ac-
curacy is the highest. We believe that it is because
PT-4 is much larger than the other PTs. Compared
with PT-4, the accuracies of the other PTs are fairly
PT combination #DCS Accuracy
1+7 178 14.61%
2+7 94 25.06%
3+7 202 18.35%
4+7 553 56.93%
5+7 231 20.48%
6+7 21 14.42%
Table 1: Contributions of the paraphrase tables.
PT-1: from the thesaurus; PT-2: from the monolingual
parallel corpus; PT-3: from the monolingual comparable
corpus; PT-4: from the bilingual parallel corpus; PT-5:
from the Encarta dictionary definitions; PT-6: from the
similar MSN user queries; PT-7: self-paraphrases.
low. This is because those PTs are smaller, thus they
can provide fewer correct phrase substitutes. As a
result, plenty of incorrect substitutes were included
in the top 10 generated paraphrases.
PT-6 provides the least correct phrase substitutes
and the accuracy is the lowest. There are several
reasons. First, many phrases in PT-6 are not real
phrases but only sets of keywords (e.g., ?lottery re-
sults ny?), which may not appear in sentences. Sec-
ond, many words in this table have spelling mis-
takes (e.g., ?widows vista?). Third, some phrase
pairs in PT-6 are not paraphrases but only ?related
queries? (e.g., ?back tattoo? vs. ?butterfly tattoo?).
Fourth, many phrases of PT-6 contain proper names
or out-of-vocabulary words, which are difficult to be
matched. The accuracy based on PT-1 is also quite
low. We found that it is mainly because the phrase
pairs in PT-1 are automatically clustered, many of
which are merely ?similar? words rather than syn-
onyms (e.g., ?borrow? vs. ?buy?).
Next, we try to find out whether it is necessary to
combine all PTs. Thus we conducted several runs,
each of which added the most useful PT from the
left ones. The results are shown in Table 2. We can
see that all the PTs are useful, as each PT provides
some new correct phrase substitutes and the accu-
racy increases when adding each PT except PT-1.
Since the PTs are extracted from different re-
sources, they have different contributions. Here we
only discuss the contributions of PT-5 and PT-6,
which are first used in paraphrasing in this paper.
PT-5 is useful for paraphrasing uncommon concepts
since it can ?explain? concepts with their definitions.
1026
PT combination #DCS Accuracy
4+7 553 56.93%
4+5+7 581 58.97%
4+5+3+7 638 59.42%
4+5+3+2+7 649 60.15%
4+5+3+2+1+7 699 60.14%
4+5+3+2+1+6+7 711 60.16%
Table 2: Performances of different combinations of para-
phrase tables.
For instance, in the following test sentence S1, the
word ?amnesia? is a relatively uncommon word, es-
pecially for the people using English as the second
language. Based on PT-5, S1 can be paraphrased
into T1, which is much easier to understand.
S1: I was suffering from amnesia.
T1: I was suffering from memory loss.
The disadvantage of PT-5 is that substituting
words with the definitions sometimes leads to gram-
matical errors. For instance, substituting ?heat
shield? in the sentence S2 with ?protective barrier
against heat? keeps the meaning unchanged. How-
ever, the paraphrased sentence T2 is ungrammatical.
S2: The U.S. space agency has been cautious
about heat shield damage.
T2: The U.S. space administration has been
cautious about protective barrier against heat
damage.
As previously mentioned, PT-6 is less effective
compared with the other PTs. However, it is use-
ful for paraphrasing some special phrases, such as
digital products, computer software, etc, since these
phrases often appear in user queries. For example,
S3 below can be paraphrased into T3 using PT-6.
S3: I have a canon powershot S230 that uses
CF memory cards.
T3: I have a canon digital camera S230 that
uses CF memory cards.
The phrase ?canon powershot? can hardly be
paraphrased using the other PTs. It suggests that PT-
6 is useful for paraphrasing new emerging concepts
and expressions.
Test sentences Top-1 Top-5 Top-10
All 150 55.33% 45.20% 39.28%
50 from news 70.00% 62.00% 57.03%
50 from novel 56.00% 46.00% 37.42%
50 from forum 40.00% 27.60% 23.34%
Table 3: Top-n accuracy on different test sentences.
6.2 Sentence-level Evaluation
In this section, we evaluated the sentence-level qual-
ity of the generated paraphrases11. In detail, each
generated paraphrase was manually labeled as ?ac-
ceptable? or ?unacceptable?. Here, the criterion for
counting a sentence T as an acceptable paraphrase of
sentence S is that T is understandable and its mean-
ing is not evidently changed compared with S. For
example, for the sentence S4, T4 is an acceptable
paraphrase generated using our method.
S4: The strain on US forces of fighting in Iraq
and Afghanistan was exposed yesterday when
the Pentagon published a report showing that
the number of suicides among US troops is at
its highest level since the 1991 Gulf war.
T4: The pressure on US troops of fighting in
Iraq and Afghanistan was revealed yesterday
when the Pentagon released a report showing
that the amount of suicides among US forces
is at its top since the 1991 Gulf conflict.
We carried out sentence-level evaluation using the
top-1, top-5, and top-10 results of each test sentence.
The accuracy of the top-n results was computed as:
Accuracytop?n =
?N
i=1 ni
N ? n
(15)
where N is the number of test sentences. ni is the
number of acceptable paraphrases in the top-n para-
phrases of the i-th test sentence.
We computed the accuracy on the whole test set
(150 sentences) as well as on the three subsets, i.e.,
the 50 news sentences, 50 novel sentences, and 50
forum sentences. The results are shown in table 3.
It can be seen that the accuracy varies greatly on
different test sets. The accuracy on the news sen-
tences is the highest, while that on the forum sen-
tences is the lowest. There are several reasons. First,
11The evaluation was based on the paraphrasing results using
the combination of all seven PTs.
1027
the largest PT used in the experiments is extracted
using the bilingual parallel data, which are mostly
from news documents. Thus, the test set of news
sentences is more similar to the training data.
Second, the news sentences are formal while the
novel and forum sentences are less formal. Espe-
cially, some of the forum sentences contain spelling
mistakes and grammar mistakes.
Third, we find in the results that, most phrases
paraphrased in the novel and forum sentences are
commonly used phrases or words, such as ?food?,
?good?, ?find?, etc. These phrases are more dif-
ficult to paraphrase than the less common phrases,
since they usually have much more paraphrases in
the PTs. Therefore, it is more difficult to choose the
right paraphrase from all the candidates when con-
ducting sentence-level paraphrase generation.
Fourth, the forum sentences contain plenty of
words such as ?board (means computer board)?,
?site (means web site)?, ?mouse (means computer
mouse)?, etc. These words are polysemous and have
particular meanings in the domains of computer sci-
ence and internet. Our method performs poor when
paraphrasing these words since the domain of a con-
text sentence is hard to identify.
After observing the results, we find that there are
three types of errors: (1) syntactic errors: the gener-
ated sentences are ungrammatical. About 32% of the
unacceptable results are due to syntactic errors. (2)
semantic errors: the generated sentences are incom-
prehensible. Nearly 60% of the unacceptable para-
phrases have semantic errors. (3) non-paraphrase:
the generated sentences are well formed and com-
prehensible but are not paraphrases of the input sen-
tences. 8% of the unacceptable results are of this
type. We believe that many of the errors above can
be avoided by applying syntactic constraints and by
making better use of context information in decod-
ing, which is left as our future work.
7 Conclusion
This paper proposes a method that improves the
SMT-based sentence-level paraphrase generation
using phrasal paraphrases automatically extracted
from different resources. Our contribution is that
we combine multiple resources in the framework of
SMT for paraphrase generation, in which the dic-
tionary definitions and similar user queries are first
used as phrasal paraphrases. In addition, we analyze
and compare the contributions of different resources.
Experimental results indicate that although the
contributions of the exploited resources differ a lot,
they are all useful to sentence-level paraphrase gen-
eration. Especially, the dictionary definitions and
similar user queries are effective for paraphrasing
some certain types of phrases.
In the future work, we will try to use syntactic
and context constraints in paraphrase generation to
enhance the acceptability of the paraphrases. In ad-
dition, we will extract paraphrase patterns that con-
tain more structural variation and try to combine the
SMT-based and pattern-based systems for sentence-
level paraphrase generation.
Acknowledgments
We would like to thank Mu Li for providing us with
the SMT decoder. We are also grateful to Dongdong
Zhang for his help in the experiments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Michael Elhadad. 1997. Using Lex-
ical Chains for Text Summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10-17.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL, pages 50-57.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. In Computational Linguistics 19(2): 263-311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using Machine Translation Evalua-
tion Techniques to Determine Sentence-level Semantic
Equivalence. In Proceedings of IWP, pages 17-24.
1028
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
Lingual Query Suggestion Using Query Logs of Dif-
ferent Languages. In Proceedings of SIGIR, pages
463-470.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase in
a Meaning-Text Generation Model. In Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Version
1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING/ACL,
pages 768-774.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using Paraphrases for Parame-
ter Tuning in Statistical Machine Translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 120-127.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proceedings of ACL,
pages 295-302.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
Marius Pasca and Pe?ter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across the
Web. In Proceedings of IJCNLP, pages 119-130.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Hua Wu and Ming Zhou. 2003. Synonymous Collo-
cation Extraction Using Translation Information. In
Proceedings of ACL, pages 120-127.
1029
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834?842,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Application-driven Statistical Paraphrase Generation
Shiqi Zhao, Xiang Lan, Ting Liu, Sheng Li
Information Retrieval Lab, Harbin Institute of Technology
6F Aoxiao Building, No.27 Jiaohua Street, Nangang District
Harbin, 150001, China
{zhaosq,xlan,tliu,lisheng}@ir.hit.edu.cn
Abstract
Paraphrase generation (PG) is important
in plenty of NLP applications. However,
the research of PG is far from enough. In
this paper, we propose a novel method for
statistical paraphrase generation (SPG),
which can (1) achieve various applications
based on a uniform statistical model, and
(2) naturally combine multiple resources
to enhance the PG performance. In our
experiments, we use the proposed method
to generate paraphrases for three differ-
ent applications. The results show that
the method can be easily transformed from
one application to another and generate
valuable and interesting paraphrases.
1 Introduction
Paraphrases are alternative ways that convey the
same meaning. There are two main threads in the
research of paraphrasing, i.e., paraphrase recogni-
tion and paraphrase generation (PG). Paraphrase
generation aims to generate a paraphrase for a
source sentence in a certain application. PG shows
its importance in many areas, such as question
expansion in question answering (QA) (Duboue
and Chu-Carroll, 2006), text polishing in natu-
ral language generation (NLG) (Iordanskaja et al,
1991), text simplification in computer-aided read-
ing (Carroll et al, 1999), and sentence similarity
computation in the automatic evaluation of ma-
chine translation (MT) (Kauchak and Barzilay,
2006) and summarization (Zhou et al, 2006).
This paper presents a method for statistical
paraphrase generation (SPG). As far as we know,
this is the first statistical model specially designed
for paraphrase generation. It?s distinguishing fea-
ture is that it achieves various applications with a
uniform model. In addition, it exploits multiple
resources, including paraphrase phrases, patterns,
and collocations, to resolve the data shortage prob-
lem and generate more varied paraphrases.
We consider three paraphrase applications in
our experiments, including sentence compression,
sentence simplification, and sentence similarity
computation. The proposed method generates
paraphrases for the input sentences in each appli-
cation. The generated paraphrases are then man-
ually scored based on adequacy, fluency, and us-
ability. The results show that the proposed method
is promising, which generates useful paraphrases
for the given applications. In addition, comparison
experiments show that our method outperforms a
conventional SMT-based PG method.
2 Related Work
Conventional methods for paraphrase generation
can be classified as follows:
Rule-based methods: Rule-based PG methods
build on a set of paraphrase rules or patterns,
which are either hand crafted or automatically
collected. In the early rule-based PG research,
the paraphrase rules are generally manually writ-
ten (McKeown, 1979; Zong et al, 2001), which
is expensive and arduous. Some researchers then
tried to automatically extract paraphrase rules (Lin
and Pantel, 2001; Barzilay and Lee, 2003; Zhao
et al, 2008b), which facilitates the rule-based PG
methods. However, it has been shown that the
coverage of the paraphrase patterns is not high
enough, especially when the used paraphrase pat-
terns are long or complicated (Quirk et al, 2004).
Thesaurus-based methods: The thesaurus-based
methods generate a paraphrase t for a source sen-
tence s by substituting some words in s with
their synonyms (Bolshakov and Gelbukh, 2004;
834
Kauchak and Barzilay, 2006). This kind of method
usually involves two phases, i.e., candidate extrac-
tion and paraphrase validation. In the first phase,
it extracts all synonyms from a thesaurus, such as
WordNet, for the words to be substituted. In the
second phase, it selects an optimal substitute for
each given word from the synonyms according to
the context in s. This kind of method is simple,
since the thesaurus synonyms are easy to access.
However, it cannot generate other types of para-
phrases but only synonym substitution.
NLG-based methods: NLG-based methods (Ko-
zlowski et al, 2003; Power and Scott, 2005) gen-
erally involve two stages. In the first one, the
source sentence s is transformed into its semantic
representation r by undertaking a series of NLP
processing, including morphology analyzing, syn-
tactic parsing, semantic role labeling, etc. In the
second stage, a NLG system is employed to gen-
erate a sentence t from r. s and t are paraphrases
as they are both derived from r. The NLG-based
methods simulate human paraphrasing behavior,
i.e., understanding a sentence and presenting the
meaning in another way. However, deep analysis
of sentences is a big challenge. Moreover, devel-
oping a NLG system is also not trivial.
SMT-based methods: SMT-based methods
viewed PG as monolingual MT, i.e., translating s
into t that are in the same language. Researchers
employ the existing SMT models for PG (Quirk
et al, 2004). Similar to typical SMT, a large
parallel corpus is needed as training data in the
SMT-based PG. However, such data are difficult
to acquire compared with the SMT data. There-
fore, data shortage becomes the major limitation
of the method. To address this problem, we have
tried combining multiple resources to improve the
SMT-based PG model (Zhao et al, 2008a).
There have been researchers trying to propose
uniform PG methods for multiple applications.
But they are either rule-based (Murata and Isa-
hara, 2001; Takahashi et al, 2001) or thesaurus-
based (Bolshakov and Gelbukh, 2004), thus they
have some limitations as stated above. Further-
more, few of them conducted formal experiments
to evaluate the proposed methods.
3 Statistical Paraphrase Generation
3.1 Differences between SPG and SMT
Despite the similarity between PG and MT, the
statistical model used in SMT cannot be directly
applied in SPG, since there are some clear differ-
ences between them:
1. SMT has a unique purpose, i.e., producing
high-quality translations for the inputs. On
the contrary, SPG has distinct purposes in
different applications, such as sentence com-
pression, sentence simplification, etc. The
usability of the paraphrases have to be as-
sessed in each application.
2. In SMT, words of an input sentence should
be totally translated, whereas in SPG, not all
words of an input sentence need to be para-
phrased. Therefore, a SPG model should be
able to decide which part of a sentence needs
to be paraphrased.
3. The bilingual parallel data for SMT are easy
to collect. In contrast, the monolingual paral-
lel data for SPG are not so common (Quirk
et al, 2004). Thus the SPG model should
be able to easily combine different resources
and thereby solve the data shortage problem
(Zhao et al, 2008a).
4. Methods have been proposed for automatic
evaluation in MT (e.g., BLEU (Papineni et
al., 2002)). The basic idea is that a translation
should be scored based on their similarity to
the human references. However, they cannot
be adopted in SPG. The main reason is that it
is more difficult to provide human references
in SPG. Lin and Pantel (2001) have demon-
strated that the overlapping between the au-
tomatically acquired paraphrases and hand-
crafted ones is very small. Thus the human
references cannot properly assess the quality
of the generated paraphrases.
3.2 Method Overview
The SPG method proposed in this work contains
three components, i.e., sentence preprocessing,
paraphrase planning, and paraphrase generation
(Figure 1). Sentence preprocessing mainly in-
cludes POS tagging and dependency parsing for
the input sentences, as POS tags and dependency
information are necessary for matching the para-
phrase pattern and collocation resources in the
following stages. Paraphrase planning (Section
3.3) aims to select the units to be paraphrased
(called source units henceforth) in an input sen-
tence and the candidate paraphrases for the source
835
Multiple Paraphrase Tables
PT1 ??
Paraphrase 
Planning
Paraphrase 
Generation t
Sentence 
Preprocessings
A
PT2 PTn
Figure 1: Overview of the proposed SPG method.
units (called target units) from multiple resources
according to the given application A. Paraphrase
generation (Section 3.4) is designed to generate
paraphrases for the input sentences by selecting
the optimal target units with a statistical model.
3.3 Paraphrase Planning
In this work, the multiple paraphrase resources are
stored in paraphrase tables (PTs). A paraphrase ta-
ble is similar to a phrase table in SMT, which con-
tains fine-grained paraphrases, such as paraphrase
phrases, patterns, or collocations. The PTs used in
this work are constructed using different corpora
and different score functions (Section 3.5).
If the applications are not considered, all units
of an input sentence that can be paraphrased us-
ing the PTs will be extracted as source units. Ac-
cordingly, all paraphrases for the source units will
be extracted as target units. However, when a cer-
tain application is given, only the source and target
units that can achieve the application will be kept.
We call this process paraphrase planning, which is
formally defined as in Figure 2.
An example is depicted in Figure 3. The ap-
plication in this example is sentence compression.
All source and target units are listed below the in-
put sentence, in which the first two source units
are phrases, while the third and fourth are a pattern
and a collocation, respectively. As can be seen, the
first and fourth source units are filtered in para-
phrase planning, since none of their paraphrases
achieve the application (i.e., shorter in bytes than
the source). The second and third source units are
kept, but some of their paraphrases are filtered.
3.4 Paraphrase Generation
Our SPG model contains three sub-models: a
paraphrase model, a language model, and a usabil-
ity model, which control the adequacy, fluency,
Input: source sentence s
Input: paraphrase application A
Input: paraphrase tables PTs
Output: set of source units SU
Output: set of target units TU
Extract source units of s from PTs: SU={su1, ?, sun}
For each source unit sui
Extract its target units TUi={tui1, ?, tuim}
For each target unit tuij
If tuij cannot achieve the application A
Delete tuij from TUi
End If
End For
If TUi is empty
Delete sui from SU
End If
End for
Figure 2: The paraphrase planning algorithm.
and usability of the paraphrases, respectively1.
Paraphrase Model: Paraphrase generation is a
decoding process. The input sentence s is first
segmented into a sequence of I units s?I1, which
are then paraphrased to a sequence of units t?I1.
Let (s?i, t?i) be a pair of paraphrase units, their
paraphrase likelihood is computed using a score
function ?pm(s?i, t?i). Thus the paraphrase score
ppm(s?I1, t?I1) between s and t is decomposed into:
ppm(s?I1, t?I1) =
I?
i=1
?pm(s?i, t?i)?pm (1)
where ?pm is the weight of the paraphrase model.
Actually, it is defined similarly to the translation
model in SMT (Koehn et al, 2003).
In practice, the units of a sentence may be para-
phrased using different PTs. Suppose we have K
PTs, (s?ki , t?ki) is a pair of paraphrase units from
the k-th PT with the score function ?k(s?ki , t?ki),
then Equation (1) can be rewritten as:
ppm(s?I1, t?I1) =
K?
k=1
(
?
ki
?k(s?ki , t?ki)?k) (2)
where ?k is the weight for ?k(s?ki , t?ki).
Equation (2) assumes that a pair of paraphrase
units is from only one paraphrase table. However,
1The SPG model applies monotone decoding, which does
not contain a reordering sub-model that is often used in SMT.
Instead, we use the paraphrase patterns to achieve word re-
ordering in paraphrase generation.
836
The US government should take the overall situation into consideration and actively promote bilateral high-tech trades.
The US government
The US administration
The US government on
overall situation 
overall interest
overall picture
overview
situation as a whole
whole situation
take [NN_1] into consideration  
consider [NN_1]
take into account [NN_1]
take account of [NN_1]
take [NN_1] into account
take into consideration [NN_1]
<promote, OBJ, trades>  
<sanction, OBJ, trades>
<stimulate, OBJ, trades>
<strengthen, OBJ, trades>
<support, OBJ, trades>
<sustain, OBJ, trades>
Paraphrase application: sentence compression
Figure 3: An example of paraphrase planning.
we find that about 2% of the paraphrase units ap-
pear in two or more PTs. In this case, we only
count the PT that provides the largest paraphrase
score, i.e., k? = argmaxk{?k(s?i, t?i)?k}.
In addition, note that there may be some units
that cannot be paraphrased or prefer to keep un-
changed during paraphrasing. Therefore, we have
a self-paraphrase table in the K PTs, which para-
phrases any separate word w into itself with a con-
stant score c: ?self (w,w) = c (we set c = e?1).
Language Model: We use a tri-gram language
model in this work. The language model based
score for the paraphrase t is computed as:
plm(t) =
J?
j=1
p(tj |tj?2tj?1)?lm (3)
where J is the length of t, tj is the j-th word of t,
and ?lm is the weight for the language model.
Usability Model: The usability model prefers
paraphrase units that can better achieve the ap-
plication. The usability of t depends on para-
phrase units it contains. Hence the usability model
pum(s?I1, t?I1) is decomposed into:
pum(s?I1, t?I1) =
I?
i=1
pum(s?i, t?i)?um (4)
where ?um is the weight for the usability model
and pum(s?i, t?i) is defined as follows:
pum(s?i, t?i) = e?(s?i,t?i) (5)
We consider three applications, including sentence
compression, simplification, and similarity com-
putation. ?(s?i, t?i) is defined separately for each:
? Sentence compression: Sentence compres-
sion2 is important for summarization, subti-
tle generation, and displaying texts in small
screens such as cell phones. In this appli-
cation, only the target units shorter than the
sources are kept in paraphrase planning. We
define ?(s?i, t?i) = len(s?i) ? len(t?i), where
len(?) denotes the length of a unit in bytes.
? Sentence simplification: Sentence simplifi-
cation requires using common expressions in
sentences so that readers can easily under-
stand the meaning. Therefore, only the target
units more frequent than the sources are kept
in paraphrase planning. Here, the frequency
of a unit is measured using the language
model mentioned above3. Specifically, the
langauge model assigns a score scorelm(?)
for each unit and the unit with larger score
is viewed as more frequent. We define
?(s?i, t?i) = 1 iff scorelm(t?i) > scorelm(s?i).
? Sentence similarity computation: Given a
reference sentence s?, this application aims to
paraphrase s into t, so that t is more similar
(closer in wording) with s? than s. This ap-
plication is important for the automatic eval-
uation of machine translation and summa-
rization, since we can paraphrase the human
translations/summaries to make them more
similar to the system outputs, which can re-
fine the accuracy of the evaluation (Kauchak
and Barzilay, 2006). For this application,
2This work defines compression as the shortening of sen-
tence length in bytes rather than in words.
3To compute the language model based score, the
matched patterns are instantiated and the matched colloca-
tions are connected with words between them.
837
only the target units that can enhance the sim-
ilarity to the reference sentence are kept in
planning. We define ?(s?i, t?i) = sim(t?i, s?)?
sim(s?i, s?), where sim(?, ?) is simply com-
puted as the count of overlapping words.
We combine the three sub-models based on a
log-linear framework and get the SPG model:
p(t|s) =
K?
k=1
(?k
?
ki
log ?k(s?ki , t?ki))
+ ?lm
J?
j=1
log p(tj |tj?2tj?1)
+ ?um
I?
i=1
?(s?i, t?i) (6)
3.5 Paraphrase Resources
We use five PTs in this work (except the self-
paraphrase table), in which each pair of paraphrase
units has a score assigned by the score function of
the corresponding method.
Paraphrase phrases (PT-1 to PT-3): Para-
phrase phrases are extracted from three corpora:
(1) Corp-1: bilingual parallel corpus, (2) Corp-
2: monolingual comparable corpus (comparable
news articles reporting on the same event), and
(3) Corp-3: monolingual parallel corpus (paral-
lel translations of the same foreign novel). The
details of the corpora, methods, and score func-
tions are presented in (Zhao et al, 2008a). In
our experiments, PT-1 is the largest, which con-
tains 3,041,822 pairs of paraphrase phrases. PT-2
and PT-3 contain 92,358, and 17,668 pairs of para-
phrase phrases, respectively.
Paraphrase patterns (PT-4): Paraphrase patterns
are also extracted from Corp-1. We applied the ap-
proach proposed in (Zhao et al, 2008b). Its basic
assumption is that if two English patterns e1 and e2
are aligned with the same foreign pattern f , then
e1 and e2 are possible paraphrases. One can refer
to (Zhao et al, 2008b) for the details. PT-4 con-
tains 1,018,371 pairs of paraphrase patterns.
Paraphrase collocations (PT-5): Collocations4
can cover long distance dependencies in sen-
tences. Thus paraphrase collocations are useful for
SPG. We extract collocations from a monolingual
4A collocation is a lexically restricted word pair with a
certain syntactic relation. This work only considers verb-
object collocations, e.g., <promote, OBJ, trades>.
corpus and use a binary classifier to recognize if
any two collocations are paraphrases. Due to the
space limit, we cannot introduce the detail of the
approach. We assign the score ?1? for any pair
of paraphrase collocations. PT-5 contains 238,882
pairs of paraphrase collocations.
3.6 Parameter Estimation
To estimate parameters ?k(1 ? k ? K), ?lm,
and ?um, we adopt the approach of minimum error
rate training (MERT) that is popular in SMT (Och,
2003). In SMT, however, the optimization objec-
tive function in MERT is the MT evaluation cri-
teria, such as BLEU. As we analyzed above, the
BLEU-style criteria cannot be adapted in SPG. We
therefore introduce a new optimization objective
function in this paper. The basic assumption is that
a paraphrase should contain as many correct unit
replacements as possible. Accordingly, we design
the following criteria:
Replacement precision (rp): rp assesses the pre-
cision of the unit replacements, which is defined
as rp = cdev(+r)/cdev(r), where cdev(r) is the
total number of unit replacements in the generated
paraphrases on the development set. cdev(+r) is
the number of the correct replacements.
Replacement rate (rr): rr measures the para-
phrase degree on the development set, i.e., the per-
centage of words that are paraphrased. We define
rr as: rr = wdev(r)/wdev(s), where wdev(r) is
the total number of words in the replaced units on
the development set, and wdev(s) is the number of
words of all sentences on the development set.
Replacement f-measure (rf): We use rf as the
optimization objective function in MERT, which
is similar to the conventional f-measure and lever-
ages rp and rr: rf = (2? rp? rr)/(rp+ rr).
We estimate parameters for each paraphrase ap-
plication separately. For each application, we first
ask two raters to manually label all possible unit
replacements on the development set as correct or
incorrect, so that rp, rr, and rf can be automati-
cally computed under each set of parameters. The
parameters that result in the highest rf on the de-
velopment set are finally selected.
4 Experimental Setup
Our SPG decoder is developed by remodeling
Moses that is widely used in SMT (Hoang and
Koehn, 2008). The POS tagger and depen-
dency parser for sentence preprocessing are SVM-
838
Tool (Gimenez and Marquez, 2004) and MST-
Parser (McDonald et al, 2006). The language
model is trained using a 9 GB English corpus.
4.1 Experimental Data
Our method is not restricted in domain or sentence
style. Thus any sentence can be used in develop-
ment and test. However, for the sentence similarity
computation purpose in our experiments, we want
to evaluate if the method can enhance the string-
level similarity between two paraphrase sentences.
Therefore, for each input sentence s, we need a
reference sentence s? for similarity computation.
Based on the above consideration, we acquire
experiment data from the human references of
the MT evaluation, which provide several human
translations for each foreign sentence. In detail,
we use the first translation of a foreign sentence
as the source s and the second translation as the
reference s? for similarity computation. In our ex-
periments, the development set contains 200 sen-
tences and the test set contains 500 sentences, both
of which are randomly selected from the human
translations of 2008 NIST Open Machine Transla-
tion Evaluation: Chinese to English Task.
4.2 Evaluation Metrics
The evaluation metrics for SPG are similar to the
human evaluation for MT (Callison-Burch et al,
2007). The generated paraphrases are manually
evaluated based on three criteria, i.e., adequacy,
fluency, and usability, each of which has three
scales from 1 to 3. Here is a brief description of
the different scales for the criteria:
Adequacy 1: The meaning is evidently changed.
2: The meaning is generally preserved.
3: The meaning is completely preserved.
Fluency 1: The paraphrase t is incomprehensible.
2: t is comprehensible.
3: t is a flawless sentence.
Usability 1: t is opposite to the application purpose.
2: t does not achieve the application.
3: t achieves the application.
5 Results and Analysis
We use our method to generate paraphrases for the
three applications. Results show that the percent-
ages of test sentences that can be paraphrased are
97.2%, 95.4%, and 56.8% for the applications of
sentence compression, simplification, and similar-
ity computation, respectively. The reason why the
last percentage is much lower than the first two
is that, for sentence similarity computation, many
sentences cannot find unit replacements from the
PTs that improve the similarity to the reference
sentences. For the other applications, only some
very short sentences cannot be paraphrased.
Further results show that the average number of
unit replacements in each sentence is 5.36, 4.47,
and 1.87 for sentence compression, simplification,
and similarity computation. It also indicates that
sentence similarity computation is more difficult
than the other two applications.
5.1 Evaluation of the Proposed Method
We ask two raters to label the paraphrases based
on the criteria defined in Section 4.2. The labeling
results are shown in the upper part of Table 1. We
can see that for adequacy and fluency, the para-
phrases in sentence similarity computation get the
highest scores. About 70% of the paraphrases are
labeled ?3?. This is because in sentence similar-
ity computation, only the target units appearing
in the reference sentences are kept in paraphrase
planning. This constraint filters most of the noise.
The adequacy and fluency scores of the other two
applications are not high. The percentages of la-
bel ?3? are around 30%. The main reason is that
the average numbers of unit replacements for these
two applications are much larger than sentence
similarity computation. It is thus more likely to
bring in incorrect unit replacements, which influ-
ence the quality of the generated paraphrases.
The usability is needed to be manually labeled
only for sentence simplification, since it can be
automatically labeled in the other two applica-
tions. As shown in Table 1, for sentence simplifi-
cation, most paraphrases are labeled ?2? in usabil-
ity, while merely less than 20% are labeled ?3?.
We conjecture that it is because the raters are not
sensitive to the slight change of the simplification
degree. Thus they labeled ?2? in most cases.
We compute the kappa statistic between the
raters. Kappa is defined as K = P (A)?P (E)1?P (E) (Car-
letta, 1996), where P (A) is the proportion of times
that the labels agree, and P (E) is the proportion
of times that they may agree by chance. We define
P (E) = 13 , as the labeling is based on three point
scales. The results show that the kappa statistics
for adequacy and fluency are 0.6560 and 0.6500,
which indicates a substantial agreement (K: 0.61-
0.8) according to (Landis and Koch, 1977). The
839
Adequacy (%) Fluency (%) Usability (%)
1 2 3 1 2 3 1 2 3
Sentence rater1 32.92 44.44 22.63 21.60 47.53 30.86 0 0 100
compression rater2 40.54 34.98 24.49 25.51 43.83 30.66 0 0 100
Sentence rater1 29.77 44.03 26.21 22.01 42.77 35.22 25.37 61.84 12.79
simplification rater2 33.33 35.43 31.24 24.32 39.83 35.85 30.19 51.99 17.82
Sentence rater1 7.75 24.30 67.96 7.75 22.54 69.72 0 0 100
similarity rater2 7.75 19.01 73.24 6.69 21.48 71.83 0 0 100
Baseline-1 rater1 47.31 30.75 21.94 43.01 33.12 23.87 - - -
rater2 47.10 30.11 22.80 34.41 41.51 24.09 - - -
Baseline-2 rater1 29.45 52.76 17.79 25.15 52.76 22.09 - - -
rater2 33.95 46.01 20.04 27.61 48.06 24.34 - - -
Table 1: The evaluation results of the proposed method and two baseline methods.
kappa statistic for usability is 0.5849, which is
only moderate (K: 0.41-0.6).
Table 2 shows an example of the generated para-
phrases. A source sentence s is paraphrased in
each application and we can see that: (1) for sen-
tence compression, the paraphrase t is 8 bytes
shorter than s; (2) for sentence simplification, the
words wealth and part in t are easier than their
sources asset and proportion, especially for the
non-native speakers; (3) for sentence similarity
computation, the reference sentence s? is listed be-
low t, in which the words appearing in t but not in
s are highlighted in blue.
5.2 Comparison with Baseline Methods
In our experiments, we implement two baseline
methods for comparison:
Baseline-1: Baseline-1 follows the method pro-
posed in (Quirk et al, 2004), which generates
paraphrases using typical SMT tools. Similar to
Quirk et al?s method, we extract a paraphrase ta-
ble for the SMT model from a monolingual com-
parable corpus (PT-2 described above). The SMT
decoder used in Baseline-1 is Moses.
Baseline-2: Baseline-2 extends Baseline-1 by
combining multiple resources. It exploits all PTs
introduced above in the same way as our pro-
posed method. The difference from our method is
that Baseline-2 does not take different applications
into consideration. Thus it contains no paraphrase
planning stage or the usability sub-model.
We tune the parameters for the two baselines
using the development data as described in Sec-
tion 3.6 and evaluate them with the test data. Since
paraphrase applications are not considered by the
baselines, each baseline method outputs a single
best paraphrase for each test sentence. The gener-
ation results show that 93% and 97.8% of the test
sentences can be paraphrased by Baseline-1 and
Baseline-2. The average number of unit replace-
ments per sentence is 4.23 and 5.95, respectively.
This result suggests that Baseline-1 is less capa-
ble than Baseline-2, which is mainly because its
paraphrase resource is limited.
The generated paraphrases are also labeled by
our two raters and the labeling results can be found
in the lower part of Table 1. As can be seen,
Baseline-1 performs poorly compared with our
method and Baseline-2, as the percentage of la-
bel ?1? is the highest for both adequacy and flu-
ency. This result demonstrates that it is necessary
to combine multiple paraphrase resources to im-
prove the paraphrase generation performance.
Table 1 also shows that Baseline-2 performs
comparably with our method except that it does
not consider paraphrase applications. However,
we are interested how many paraphrases gener-
ated by Baseline-2 can achieve the given applica-
tions by chance. After analyzing the results, we
find that 24.95%, 8.79%, and 7.16% of the para-
phrases achieve sentence compression, simplifi-
cation, and similarity computation, respectively,
which are much lower than our method.
5.3 Informal Comparison with Application
Specific Methods
Previous research regarded sentence compression,
simplification, and similarity computation as to-
tally different problems and proposed distinct
method for each one. Therefore, it is interesting
to compare our method to the application-specific
methods. However, it is really difficult for us to
840
Source
sentence
Liu Lefei says that in the long term, in terms of asset alocation, overseas investment should occupy a
certain proportion of an insurance company?s overall allocation.
Sentence
compression
Liu Lefei says that in [the long run]phr , [in area of [asset alocation][NN 1]]pat, overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
simplification
Liu Lefei says that in [the long run]phr , in terms of [wealth]phr [distribution]phr , overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
similarity
Liu Lefei says that in [the long run]phr , in terms [of capital]phr allocation, overseas investment should
occupy [the [certain][JJ 1] ratio of [an insurance company?s overall allocation][NN 1]]pat.
(reference sentence: Liu Lefei said that in terms of capital allocation, outbound investment should make
up a certain ratio of overall allocations for insurance companies in the long run .)
Table 2: The generated paraphrases of a source sentence for different applications. The target units after
replacement are shown in blue and the pattern slot fillers are in cyan. [?]phr denotes that the unit is a
phrase, while [?]pat denotes that the unit is a pattern. There is no collocation replacement in this example.
reimplement the methods purposely designed for
these applications. Thus here we just conduct an
informal comparison with these methods.
Sentence compression: Sentence compression
is widely studied, which is mostly reviewed as a
word deletion task. Different from prior research,
Cohn and Lapata (2008) achieved sentence com-
pression using a combination of several opera-
tions including word deletion, substitution, inser-
tion, and reordering based on a statistical model,
which is similar to our paraphrase generation pro-
cess. Besides, they also used paraphrase patterns
extracted from bilingual parallel corpora (like our
PT-4) as a kind of rewriting resource. However,
as most other sentence compression methods, their
method allows information loss after compression,
which means that the generated sentences are not
necessarily paraphrases of the source sentences.
Sentence Simplification: Carroll et al (1999)
has proposed an automatic text simplification
method for language-impaired readers. Their
method contains two main parts, namely the lex-
ical simplifier and syntactic simplifier. The for-
mer one focuses on replacing words with simpler
synonyms, while the latter is designed to transfer
complex syntactic structures into easy ones (e.g.,
replacing passive sentences with active forms).
Our method is, to some extent, simpler than Car-
roll et al?s, since our method does not contain syn-
tactic simplification strategies. We will try to ad-
dress sentence restructuring in our future work.
Sentence Similarity computation: Kauchak
and Barzilay (2006) have tried paraphrasing-based
sentence similarity computation. They paraphrase
a sentence s by replacing its words with Word-
Net synonyms, so that s can be more similar in
wording to another sentence s?. A similar method
has also been proposed in (Zhou et al, 2006),
which uses paraphrase phrases like our PT-1 in-
stead of WordNet synonyms. These methods can
be roughly viewed as special cases of ours, which
only focus on the sentence similarity computation
application and only use one kind of paraphrase
resource.
6 Conclusions and Future Work
This paper proposes a method for statistical para-
phrase generation. The contributions are as fol-
lows. (1) It is the first statistical model spe-
cially designed for paraphrase generation, which
is based on the analysis of the differences between
paraphrase generation and other researches, espe-
cially machine translation. (2) It generates para-
phrases for different applications with a uniform
model, rather than presenting distinct methods for
each application. (3) It uses multiple resources,
including paraphrase phrases, patterns, and collo-
cations, to relieve data shortage and generate more
varied and interesting paraphrases.
Our future work will be carried out along two
directions. First, we will improve the components
of the method, especially the paraphrase planning
algorithm. The algorithm currently used is sim-
ple but greedy, which may miss some useful para-
phrase units. Second, we will extend the method to
other applications, We hope it can serve as a uni-
versal framework for most if not all applications.
Acknowledgements
The research was supported by NSFC (60803093,
60675034) and 863 Program (2008AA01Z144).
Special thanks to Wanxiang Che, Ruifang He,
Yanyan Zhao, Yuhang Guo and the anonymous re-
viewers for insightful comments and suggestions.
841
References
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL Workshop on Statistical Machine
Translation, pages 136-158.
Jean Carletta. 1996. Assessing Agreement on Clas-
sification Tasks: The Kappa Statistic. In Computa-
tional Linguistics, 22(2): 249-254.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, John Tait. 1999. Simpli-
fying Text for Language-Impaired Readers. In Pro-
ceedings of EACL, pages 269-270.
Trevor Cohn and Mirella Lapata. 2008. Sentence
Compression Beyond Word Deletion In Proceed-
ings of COLING, pages 137-144.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had Asked:
The impact of paraphrasing for Question Answer-
ing. In Proceedings of HLT-NAACL, pages 33-36.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, pages
43-46.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL, pages 455-462.
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure us-
ing lexico-grammatical resources. In Proceedings
of IWP, pages 1-8.
J. R. Landis and G. G. Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data.
In Biometrics 33(1): 159-174.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Parsing with a
Two-Stage Discriminative Parser. In Proceedings of
CoNLL.
Kathleen R. McKeown. 1979. Paraphrasing Using
Given and New Information in a Question-Answer
System. In Proceedings of ACL, pages 67-72.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal Model for Paraphrasing - Using Transformation
Based on a Defined Criteria. In Proceedings of NL-
PRS, pages 47-54.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic gen-
eration of large-scale paraphrases. In Proceedings of
IWP, pages 73-79.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Tetsuro Takahashi, Tomoyam Iwakura, Ryu Iida, At-
sushi Fujita, Kentaro Inui. 2001. KURA: A
Transfer-based Lexico-structural Paraphrasing En-
gine. In Proceedings of NLPRS, pages 37-46.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. Combining Multiple Resources
to Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. ParaEval: Using Para-
phrases to Evaluate Summaries Automatically. In
Proceedings of HLT-NAACL, pages 447-454.
Chengqing Zong, Yujie Zhang, Kazuhide Yamamoto,
Masashi Sakamoto, Satoshi Shirai. 2001. Approach
to Spoken Chinese Paraphrasing Based on Feature
Extraction. In Proceedings of NLPRS, pages 551-
556.
842
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT: Web based Scoring Method for English Lexical Substitution 
Shiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, Sheng Li 
Information Retrieval Laboratory, School of Computer Science and Technology, 
Box 321, Harbin Institute of Technology 
Harbin, P.R. China, 150001 
{ zhaosq, lzhao, zhangyu, tliu, lisheng }@ir.hit.edu.cn 
 
 
Abstract 
This paper describes the HIT system and its 
participation in SemEval-2007 English 
Lexical Substitution Task. Two main steps 
are included in our method: candidate sub-
stitute extraction and candidate scoring. In 
the first step, candidate substitutes for each 
target word in a given sentence are ex-
tracted from WordNet. In the second step, 
the extracted candidates are scored and 
ranked using a web-based scoring method. 
The substitute ranked first is selected as the 
best substitute. For the multiword subtask, 
a simple WordNet-based approach is em-
ployed. 
1 Introduction 
Lexical substitution aims to find alternative words 
that can occur in given contexts. It is important in 
many applications, such as query reformulation in 
question answering, sentence generation, and 
paraphrasing. There are two key problems in the 
lexical substitution task, the first of which is 
candidate substitute extraction. Generally speaking, 
synonyms can be regarded as candidate substitutes 
of words. However, some looser lexical 
relationships can also be considered, such as 
Hypernyms and Hyponyms defined in WordNet 
(Fellbaum, 1998). In addition, since lexical 
substitution is context dependent, some words 
which do not have similar meanings in general 
may also be substituted in some certain contexts 
(Zhao et al, 2007). As a result, finding a lexical 
knowledge base for substitute extraction is a 
challenging task. 
The other problem is candidate scoring and 
ranking according to given contexts. In the lexical 
substitution task of SemEval-2007, context is con-
strained as a sentence. The system therefore has to 
score the candidate substitutes of each target word 
using the given sentence. The following questions 
should be considered here: (1) What words in the 
given sentence are ?useful? context? (2) How to 
combine the context words and use them in rank-
ing candidate substitutes? For the first question, we 
can use all words of the sentence, words in a win-
dow, or words having syntactic relations with the 
target word. For the second question, we can re-
gard the context words as ?bag of words?, n-grams, 
or syntactic structures. 
In HIT, we extract candidate substitutes from 
WordNet, in which both synonyms and hypernyms 
are investigated (Section 3.1). After that, we score 
the candidates using a web-based scoring method 
(Section 3.2). In this method, we first select frag-
ments containing the target word from the given 
sentence. Then we construct queries by replacing 
the target word in the fragments with the candidate 
substitute. Finally, we search Google using the 
constructed queries and score each candidate based 
on the counts of retrieved snippets. 
The rest of this paper is organized as follows: 
Section 2 reviews some related work on lexical 
substitution. Section 3 describes our system, espe-
cially the web-based scoring method. Section 4 
presents the results and analysis. 
2 Related Work 
Synonyms defined in WordNet have been widely 
used in lexical substitution and expansion (Smea-
ton et al, 1994; Langkilde and Knight, 1998; Bol-
173
shakov and Gelbukh, 2004). In addition, a lot of 
methods have been proposed to automatically con-
struct thesauri of synonyms. For example, Lin 
(1998) clustered words with similar meanings by 
calculating the dependency similarity. Barzilay and 
McKeown (2001) extracted paraphrases using mul-
tiple translations of literature works. Wu and Zhou 
(2003) extracted synonyms with multiple resources, 
including a monolingual dictionary, a bilingual 
corpus, and a monolingual corpus. Besides the 
handcrafted and automatic synonym resources, the 
web has been exploited as a resource for lexical 
substitute extraction (Zhao et al, 2007). 
As for substitute scoring, various methods have 
been investigated, among which the classification 
method is the most widely used (Dagan et al, 2006; 
Kauchak and Barzilay, 2006). In detail, a binary 
classifier is trained for each candidate substitute, 
using the contexts of the substitute as features. 
Then a new contextual sentence containing the tar-
get word can be classified as 1 (the candidate is a 
correct substitute in the given sentence) or 0 (oth-
erwise). The features used in the classification are 
usually similar with that in word sense disam-
biguation (WSD), including bag of word lemmas 
in the sentence, n-grams and parts of speech (POS) 
in a window, etc. There are other models presented 
for candidate substitute scoring. Glickman et al 
(2006) proposed a Bayesian model and a Neural 
Network model, which estimate the probability of 
a word may occur in a given context. 
3 HIT System 
3.1 Candidate Substitute Extraction 
In HIT, candidate substitutes are extracted from 
WordNet. Both synonyms and hypernyms defined 
in WordNet are investigated. Let w be a target 
word, pos the specified POS of w. n the number of 
w?s synsets defined in WordNet. Then the system 
extracts w?s candidate substitutes as follows: 
z Extracts all the synonyms in each synset 
under pos1 as candidate substitutes. 
z If w has no synonym for the i-th synset 
(1?i?n), then extracts the synonyms of its 
nearest hypernym. 
z If pos is r (or a), and no candidate substi-
tute can be extracted as described above, 
                                                 
1 In this task, four kinds of POS are specified: n - noun, v - 
verb, a - adjective, r - adverb.  
then extracts candidate substitutes under the 
POS a (or r). 
3.2 Candidate Substitute Scoring 
As mentioned above, all words in the given sen-
tence can be used as contextual information in the 
scoring of candidate substitutes. However, it is ob-
vious that not all context words are really useful 
when determining a word?s substitutes. An exam-
ple can be seen from Figure 1. 
 
 
She turns eyes <head>bright</head> with 
excitement towards Fiona , still tugging on the 
string of the minitiature airship-cum-dance 
card she has just received at the door . 
Figure 1. An example of a context sentence. 
 
In the example above, words turns, eyes, with, 
and excitement are useful context words, while the 
others are not. The useless contexts may even be 
noise if they are used in the scoring. As a result, it 
is important to select context words carefully. 
In HIT, we select context words based on the 
following assumption: useful context words for 
lexical substitute are those near the target word in 
the given sentence. In other words, the words that 
are far from the target word are not taken into con-
sideration. Obviously, this assumption is not al-
ways true. However, considering only the 
neighboring words can reduce the risk of bringing 
in noise. Besides, Edmonds (1997) has also dem-
onstrated in his paper that short-distance colloca-
tions with neighboring words are more useful in 
lexical choice than long ones. 
Let w be the target word, t a candidate substitute, 
S the context sentence. Our basic idea is that: One 
can substitute w in S with t, which generates a new 
sentence S?. If S? can be found on the web, then the 
substitute is admissible. The more times S? occurs 
on the web, the more probable the substitute is. In 
practice, however, it is difficult to find a whole 
sentence S? on the web due to sparseness. Instead, 
we use fragments of S? which contains t and sev-
eral neighboring context words (based on the as-
sumption above). Then the question is how to ob-
tain one (or more) fragment of S?. 
A window with fixed size can be used here. Su-
ppose p is the position of t in S?, for instance, we 
can construct a fragment using words from posi-
tion p-r to p+r, where r is the radius of window. 
174
However, a fixed r is difficult to set, since it may 
be too large for some sentences, which makes the 
fragments too specific, while too small for some 
other sentences, which makes the fragments too 
loose. An example can be seen in Table 1. 
 
1(a) But when Daniel turned <head>blue</head> 
one time and he totally stopped breathing. 
1(b) Daniel turned t one time 
2(a) We recommend that you <head>check</head> 
with us beforehand. 
2(b) that you t with us 
Table 1. Examples of fragments with fixed size. 
 
In Table1, 1(a) and 2(a) are two sentences from 
the test data of SemEval-2007Task10. 1(b) and 2(b) 
are fragments constructed according to 1(a) and 
2(a), where the window radius is 2 and t denotes 
any candidate substitute of the target word. It is 
obvious that 1(b) is a rather strict fragment, which 
makes it difficult to find sentences containing it on 
the web, while 2(b) is quite loose, which can 
hardly constrain the semantics of t. 
Having considered the problem above, we pro-
pose a rule-based method that constructs fragments 
with varied lengths. Let Ft be a fragment contain-
ing t, the construction rules are as follows: 
Rule-1: Ft must contain at least two words be-
sides t, at least one of which is non-stop word. 
Rule-2: Ft does not cross sub-sentence boundary 
(?,?). 
Rule-3: Ft should be the shortest fragment that 
satisfies Rule-1 and Rule-2. 
According to the rules above, we construct at 
most three fragments for each S?: (1) t occurs at the 
beginning of Ft, (2) t occurs in the middle of Ft, 
and (3) t occurs at the end of Ft. Here we have an-
other constraint: if one constructed fragment F1 is 
the substring of F2, then F2 is removed. Please 
note that the morphology is not taken into account 
when we construct queries. 
For the sentence 1(a) and 2(a) in Table 1, the 
constructed fragments are as follows: 
 
For 1(a): Daniel turned t; t one time; turned t 
one 
For 2(a): recommend that you t; t with us be-
forehand 
Table 2. Examples of the constructed fragments 
 
To score a candidate substitute, we replace ?t? in 
the fragments with each candidate substitute and 
use them as queries, which are then fed to Google. 
The score of t is computed according to the counts 
of retrieved snippets: 
?
=
=
n
i
tWebMining iFSnippetcountn
tScore
1
))((
1
)(     (1) 
where n is the number of constructed fragments, 
Fti is the i-th fragment (query) corresponding to t, 
and count(Snippet(Fti)) is the count of snippets 
retrieved by Fti. 
All candidate substitutes with scores larger than 
0 are ranked and the first 10 substitutes are re-
tained for the oot subtask. If the number of candi-
dates whose scores are larger than 0 is less than 10, 
the system ranks the rest of the candidates by their 
frequencies using a word frequency list. The spare 
capacity is filled with those candidates with largest 
frequencies. For the best subtask, we simply output 
the substitute that ranks first in oot. 
3.3 Detection of Multiwords 
The method used to detect multiword in the HIT 
system is quite similar to that employed in the 
baseline system. We also use WordNet to detect if 
a multiword that includes the target word occurs 
within a window of 2 words before and 2 words 
after the target word.  
A difference from the baseline system lies in 
that our system looks up WordNet using longer 
multiword candidates first. If a longer one is found 
in WordNet, then its substrings will be ignored. 
For example, if we find ?get alng with? in Word-
Net, we will output it as a multiword and will not 
check ?get alng? any more. 
4 Results 
Our system is the only one that participates all the 
three subtasks of Task10, i.e., best, oot, and mw. 
The evaluation results of our system can be found 
in Table 3 to Table 5. Our system ranks the fourth 
in the best subtask and seventh in the oot subtask. 
We have analyzed the results from two aspects, 
i.e., the ability of the system to extract candidate 
substitutes and the ability to rank the correct sub-
stitutes in front. There are a total of 6,873 manual 
substitutes for all the 1,710 items in the gold stan-
dard, only 2,168 (31.54%) of which have been ex-
tracted as candidate substitutes by our system. This 
result suggests that WordNet is not an appropriate 
175
source for lexical substitute extraction. In the fu-
ture work, we will try some other lexical resources, 
such as the Oxford American Writer Thesaurus 
and Encarta. In addition, we will also try the 
method that automatically constructs lexical re-
sources, such as the automatic clustering method. 
Further analysis shows that, 1,388 (64.02%) out 
of the 2,168 extracted correct candidates are 
ranked in the first 10 in the oot output of our sys-
tem. This suggests that there is a big space for our 
system to improve the candidate scoring method. 
In the future work, we will consider more and 
richer features, such as the syntactic features, in 
candidate substitute scoring. Furthermore, A dis-
advantage of this method is that the web mining 
process is quite inefficient. Therefore, we will try 
to use the Web 1T 5-gram Version 1 from Google 
(LDC2006T13) in the future. 
 
 P R ModeP ModeR
OVERALL 11.35 11.35 18.86 18.86 
Further Analysis 
NMWT 11.97 11.97 19.81 19.81 
NMWS 12.55 12.38 19.93 19.65 
RAND 11.81 11.81 20.03 20.03 
MAN 10.81 10.81 17.53 17.53 
Baselines 
WORDNET 9.95 9.95 15.58 15.58 
LIN 8.84 8.53 14.69 14.23 
Table 3. best results. 
 
 P R ModeP ModeR
OVERALL 33.88 33.88 46.91 46.91 
Further Analysis 
NMWT 35.60 35.60 48.48 48.48 
NMWS 36.63 36.63 49.33 49.33 
RAND 33.95 33.95 47.25 47.25 
MAN 33.81 33.81 46.53 46.53 
Baselines 
WORDNET 29.70 29.35 40.57 40.57 
LIN 27.70 26.72 40.47 39.19 
Table 4. oot results. 
 
 Our System WordNet BL 
 P R P R 
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
Table 5. mw results. 
 
Acknowledgements 
This research was supported by National Natural 
Science Foundation of China (60575042, 
60503072, 60675034). 
References 
Barzilay Regina and McKeown Kathleen R. 2001. Ex-
tracting paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL/EACL. 
Bolshakov Igor A. and Gelbukh Alexander. 2004. Syn-
onymous Paraphrasing Using WordNet and Internet. 
In Proceedings of NLDB. 
Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmor-
shtein Efrat, Strapparava Carlo. 2006. Direct Word 
Sense Matching for Lexical Substitution. In Proceed-
ings of ACL. 
Edmonds Philip. 1997. Choosing the Word Most Typi-
cal in Context Using a Lexical Co-occurrence Net-
work. In Proceedings of ACL. 
Fellbaum Christiane. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Cambridge, MA. 
Glickman Oren, Dagan Ido, Keller Mikaela, Bengio 
Samy. 2006. Investigating Lexical Substitution Scor-
ing for Subtitle Generation. In Proceedings of 
CoNLL. 
Kauchak David and Barzilay Regina. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of 
HLT-NAACL. 
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of the COLING-ACL. 
Lin Dekang. 1998. Automatic Retrieval and Clustering 
of Similar Words. In Proceedings of COLING-ACL. 
Smeaton Alan F., Kelledy Fergus, and O?Donell Ruari. 
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding Posting Lists, Query Expansion 
with WordNet and POS Tagging of Spanish. In Pro-
ceedings of TREC-4. 
Wu Hua and Zhou Ming. 2003. Optimizing Synonym 
Extraction Using Monolingual and Bilingual Re-
sources. In Proceedings of IWP. 
Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, and 
Zhang Yu. 2007. Automatic Acquisition of Context-
Specific Lexical Paraphrases. In Proceedings of 
IJCAI-07. 
176
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1317?1325,
Beijing, August 2010
Paraphrasing with Search Engine Query Logs
Shiqi Zhao??, Haifeng Wang?, and Ting Liu?
?Baidu Inc.
?HIT Center for Information Retrieval, Harbin Institute of Technology
{zhaoshiqi, wanghaifeng}@baidu.com, tliu@ir.hit.edu.cn
Abstract
This paper proposes a method that extracts
paraphrases from search engine query
logs. The method first extracts paraphrase
query-title pairs based on an assumption
that a search query and its correspond-
ing clicked document titles may mean the
same thing. It then extracts paraphrase
query-query and title-title pairs from the
query-title paraphrases with a pivot ap-
proach. Paraphrases extracted in each step
are validated with a binary classifier. We
evaluate the method using a query log
from Baidu1, a Chinese search engine.
Experimental results show that the pro-
posed method is effective, which extracts
more than 3.5 million pairs of paraphrases
with a precision of over 70%. The results
also show that the extracted paraphrases
can be used to generate high-quality para-
phrase patterns.
1 Introduction
The use of paraphrases is ubiquitous in hu-
man languages, which also presents a challenge
for natural language processing (NLP). Previous
studies have shown that paraphrasing can play im-
portant roles in plenty of areas, such as machine
translation (MT) (Callison-Burch et al, 2006;
Kauchak and Barzilay, 2006), question answer-
ing (QA) (Duboue and Chu-Carroll, 2006; Riezler
et al, 2007), natural language generation (NLG)
(Iordanskaja et al, 1991), and so on. As a result,
the research on paraphrasing and its applications
have attracted significant interest.
1www.baidu.com
This paper proposes a method that uses search
engine query logs for extracting paraphrases,
which is illustrated in Figure 1. Specifically, three
kinds of paraphrases can be extracted with our
method, which include (1) query-title (Q-T): a
query and a document title that users clicked on;
(2) query-query (Q-Q): two queries, for which
users clicked on the same document title; (3) title-
title (T-T): two titles that users clicked on for the
same query. We train a classifier for each kind to
filter incorrect pairs and refine the paraphrases.
Extracting paraphrases using query logs has
many advantages. First, query logs keep growing,
which have no scale limitation. Second, query
logs reflect web users? real needs, hence the ex-
tracted paraphrases may be more useful than that
from other kinds of corpora. Third, paraphrases
extracted from query logs can be directed applied
in search engines for query suggestion and doc-
ument reranking. In addition, we find that both
queries and titles contain a good many question
sentences, which can be useful in developing QA
systems.
We conduct experiments using a query log of
a commercial Chinese search engine Baidu, from
which we extracted about 2.7 million pairs of
paraphrase Q-T, 0.4 million pairs of paraphrase Q-
Q, and 0.4 million pairs of paraphrase T-T. The
precision of the paraphrases is above 70%. In
addition, we generate paraphrase patterns using
the extracted paraphrases. The results show that
73,484 pairs of paraphrase patterns have been gen-
erated, with a precision of over 78%.
In the rest of the paper, we first review related
work in Section 2. Section 3 describes our method
in detail. Section 4 presents the evaluation and re-
1317
paraphrase Q-T extraction
query title both query and title
paraphrase Q-Q extraction paraphrase T-T extraction
paraphrase relation
Figure 1: Illustration of the proposed method.
sults. Section 5 concludes the paper and discusses
future directions.
2 Related Work
In this section, we briefly review previous studies
on paraphrase extraction and query log mining in
information retrieval (IR).
2.1 Paraphrase Extraction
A variety of data resources have been exploited
for paraphrase extraction. For example, some re-
searchers extract paraphrases from multiple trans-
lations of the same foreign novel (Barzilay and
McKeown, 2001; Ibrahim et al, 2003), while
some others make use of comparable news arti-
cles that report on the same event within a small
time interval (Shinyama et al, 2002; Barzilay and
Lee, 2003; Dolan et al, 2004). Besides the mono-
lingual corpora, bilingual parallel corpora have
also been used for extracting paraphrases (Ban-
nard and Callison-Burch, 2005; Callison-Burch,
2008; Zhao et al, 2008). Their basic assumption
is that phrases that align with the same foreign
phrase may have the same meaning.
The above methods have achieved promising
results. However, their performances are usually
constrained due to the scale and domain limita-
tion. As an alternative, researchers have tried
to acquire paraphrases from large-scale web cor-
pora (Lin and Pantel, 2001; Pas?ca and Dienes,
2005; Bhagat and Ravichandran, 2008) or directly
based on web mining (Ravichandran and Hovy,
2002). These methods are guided by an extended
version of distributional hypothesis, namely, if
two phrases often occur in similar contexts, their
meanings tend to be similar. The disadvantage
of these methods is that the underlying assump-
tion does not always hold. Phrases with opposite
meanings can also occur in similar contexts, such
as ?X solves Y? and ?X worsens Y? (Lin and Pan-
tel, 2001). In addition, the extracted paraphrases
are generally short fragments with two slots (vari-
ables) at both ends.
2.2 Query Log Mining in IR
Query logs are widely used in the IR commu-
nity, especially for mining similar queries. For ex-
ample, Wen et al (2002) clustered queries based
on user click information. Their basic idea is
that if some queries result in similar user clicks,
the meanings of these queries should be similar.
Such methods have also been investigated in (Gao
et al, 2007) for cross-lingual query suggestion
and (Zhao et al, 2007) for synonymous questions
identification. This paper is partly inspired by
their studies. However, we do not simply use click
information as clues for mining similar queries.
Instead, we mine paraphrases across queries and
clicked document titles.
In addition, query logs can be used for query
expansion. For instance, Cui et al (2002)
extract probabilistic correlations between query
terms and document terms by analyzing query
logs, which are then used to select high-quality
1318
H1: If a query q hits a title t, then q and
t are likely to be paraphrases.
H2: If queries q1 and q2 hit the same title t,
q1 and q2 are likely to be paraphrases.
H3: If a query q hits titles t1 and t2, then
t1 and t2 are likely to be paraphrases.
Table 1: Hypotheses for extracting paraphrases.
expansion terms for new queries. Note that the
expansion terms are merely related terms of the
queries, not necessarily paraphrases.
There are other studies that use query logs
for constructing ontologies (Sekine and Suzuki,
2007), learning named entities (Pas?ca, 2007),
building user profiles (Richardson, 2008), correct-
ing spelling errors (Ahmad and Kondrak, 2005),
and so forth.
3 The Proposed Method
3.1 Basic Idea
Nowadays, more and more users tend to search
long queries with search engines. Many users
even directly search questions to get exact an-
swers. By analyzing our query log that records
rich information including user queries, clicked
urls, titles, etc., we find that most titles of clicked
documents are highly related with search queries.
Especially, paraphrases can be easily found from
long queries and the corresponding clicked ti-
tles. This motivates us to extract paraphrases from
query-title pairs. Here we introduce a concept hit
that will be frequently used: given a query q, a
web document d, and d?s title t, if there exist some
users that click on d when searching q, then we
say q hits t.
The hypothesis for extracting paraphrase Q-T
is shown in Table 1 (H1). In addition, we find
that when several queries hit the same title, the
queries are likely to be paraphrases of each other.
The other way round, when a query hits several
titles, paraphrases can also be found among the ti-
tles. We therefore further extract paraphrase Q-Q
and T-T from the paraphrase Q-T. The underly-
ing hypotheses can be found in Table 1 (H2 and
INPUT: Q: query space, T : title space
OUTPUT: Pqt: the set of paraphrase Q-T,
Pqq: the set of paraphrase Q-Q,
Ptt: the set of paraphrase T-T,
ParaSet: the set of paraphrases
1. FOR any q ? Q and t ? T
2. IF q hits t
3. IF IsParaphrase(q, t)
4. Add ?q, t? to Pqt
5. END IF
6. END IF
7. END FOR
8. FOR any q1, q2 ? Q and t ? T
9. IF ?q1, t? ? Pqt and ?q2, t? ? Pqt
10. IF IsParaphrase(q1, q2)
11. Add ?q1, q2? to Pqq
12. END IF
13. END IF
14. END FOR
15. FOR any t1, t2 ? T and q ? Q
16. IF ?q, t1? ? Pqt and ?q, t2? ? Pqt
17. IF IsParaphrase(t1, t2)
18. Add ?t1, t2? to Ptt
19. END IF
20. END IF
21. END FOR
22. RETURN ParaSet = Pqt ? Pqq ? Ptt
Table 2: Algorithm for extracting paraphrases.
H3). Note that, based on H2 and H3, paraphrase
Q-Q and T-T can be directly extracted from raw
Q-T pairs. However, in consideration of preci-
sion, we extract them from paraphrase Q-T. We
call our paraphrase Q-Q and T-T extraction ap-
proach as a pivot approach, since we use titles as
pivots (queries as targets) when extracting para-
phrase Q-Q and use queries as pivots (titles as tar-
gets) when extracting paraphrase T-T.
3.2 Algorithm
Our paraphrase extraction algorithm is shown in
Table 2. In particular, lines 1?7 extract para-
1319
phrase Q-T from the query log. Lines 8?14 and
15?21 extract paraphrase Q-Q and T-T, respec-
tively. Line 22 combines the paraphrase Q-T, Q-
Q, and T-T together. To filter noise, the extracted
Q-T, Q-Q, and T-T pairs are all validated using
a function IsParaphrase(s1, s2). In this work,
we recast paraphrase validation as a binary clas-
sification problem. Any pair of ?s1, s2? is classi-
fied as 1 (paraphrase) or 0 (non-paraphrase) with
a support vector machine (SVM) classifier. The
features used for classification will be detailed in
Section 3.3.
In practice, we exploit a query log that contains
287 million Q-T pairs, which are then filtered us-
ing the following constraints: (1) exclude Q-T
pairs that are too short, i.e., either query q or tittle
t contains less than three terms; (2) exclude Q-T
pairs where q subsumes t or vice versa, e.g., ??
? (beef)? and ?????? (cooking method of
beef)?; (3) exclude Q-T pairs in which the similar-
ity between q and t is below a predefined threshold
T 2; (4) exclude Q-T pairs whose t contains fre-
quent internet terms, such as ??? (home page)?,
??? (web site)?, ??? (online)?, since such ti-
tles are mostly organization home pages, online
videos, downloadable resources, etc., which are
useless for our purpose of paraphrase extraction.
3.3 Features for Paraphrase Validation
Given a pair of candidate paraphrases ?s1, s2?, in
which s1 and s2 can be either a query or a title, we
exploit the following features in the classification-
based paraphrase validation.
? Frequency Feature FF . FF is defined based
on each ?s1, s2??s frequency. We expect that more
frequent ?s1, s2? should be more reliable.
FF (s1, s2) = {
c(s1,s2)
C if c(s1, s2) < C
1 if c(s1, s2) ? C
(1)
where c(s1, s2) denotes the number of times that
the ?s1, s2? pair occurs in the corpus. C is a nor-
malizing factor (C = 10 in our experiments).
2The similarity is computed based on word overlap rate,
which will be described in detail in section 3.3. We set T =
0.6 in the experiments.
? Length Rate Feature FLR:
FLR(s1, s2) =
min{cw(s1), cw(s2)}
max{cw(s1), cw(s2)}
(2)
where cw(s) denotes the number of words in s.
? Word Overlap Rate Feature FWOR:
FWOR(s1, s2) =
cw(s1 ? s2)
max{cw(s1), cw(s2)}
(3)
where ?s1 ? s2? is the intersection of s1 and s2.
? Character Overlap Rate Feature FCOR. Chi-
nese words are composed of characters. It is quite
often that words with similar characters share
similar meanings, such as ??? (comfortable)?
and ??? (comfortable)?, ??? (sell)? and ??
? (sell)?. Here we use FCOR to measure the sim-
ilarity between s1 and s2 at the character level.
Detailedly, we segment s1 and s2 into sets of
characters and compute the overlap rate based on
Equation (3)3.
? Cosine Similarity Feature FCS . In FCS , both
s1 and s2 are represented as vectors and their co-
sine similarity is computed as:
FCS(s1, s2) =
vecw(s1) ? vecw(s2)
?vecw(s1)? ? ?vecw(s2)?
(4)
where vecw(s) is the vector of words in s, ??? de-
notes the dot product of two vectors, ?vecw(s)?
is the norm of a vector. Here, the weight of each
word w in a vector is computed using a heuristic
similar to tf-idf:
W (w) = tf(w)? log( Nc(w) + 0.1) (5)
where tf(w) is the frequency of w in the given s,
c(w) is the number of times that w occurs in the
corpus, N = maxw c(w).
? Edit Distance Feature FED. Let ED(s1, s2)
be the edit distance at the word level between s1
and s2, we compute FED as follows:
FED(s1, s2) = 1?
ED(s1, s2)
max{cw(s1), cw(s2)}
(6)
3In FCOR, cw(s) of Equation (3) denotes the number of
characters in s.
1320
? Named Entity (NE) Similarity Feature FNE .
NE information is critical in paraphrase identifica-
tion (Shinyama et al, 2002). We therefore com-
pute the NE similarity between s1 and s2 and take
it as a feature. We employ a Chinese NE recog-
nition tool that can recognize person names, loca-
tions, organizations, and numerals. The NE simi-
larity is computed as:
FNE(s1, s2) =
cne(s1 ? s2) + 1
max{cne(s1), cne(s2)}+ 1
(7)
where cne(s) denotes the number of NEs in s.
Equation (7) guarantees FNE = 1 if there are no
NEs in either s1 or s2.
? Pivot Fertility Feature FPF : FPF is a fea-
ture specially designed for paraphrase Q-Q and
T-T extraction, which are based on the pivot ap-
proach4. Specifically, we define fertility of a pivot
as the number of targets it corresponds to. Our ob-
servation indicates that the larger the fertility of a
pivot is, the more noisy the targets are. Hence we
define FPF as:
FPF (s1, s2) = maxp
1
f(p) (8)
where s1 = q1, s2 = q2, p = t when classifying
Q-Q, while s1 = t1, s2 = t2, p = q when classi-
fying T-T. f(p) denotes the fertility of the pivot p.
The value is maximized over p if s1 and s2 can be
extracted with multiple pivots.
3.4 Generating Paraphrase Patterns
A key feature of our method is that the extracted
paraphrases are particularly suitable for generat-
ing paraphrase patterns, especially for the hot do-
mains that are frequently searched. For example,
there are quite a few paraphrases concerning the
therapy of various diseases, from which we can
easily induce patterns expressing the meaning of
?How to treat [X] disease?, such as ?[X] ? ?
? ???, ??? ?? [X] ??, and ?[X] ? ?
?? ???. Therefore, in this work, we try to
generate paraphrase patterns using the extracted
paraphrases.
In our preliminary experiments, we only induce
paraphrase patterns from paraphrases that contain
4FPF is not used in paraphrase Q-T validation.
SAME RELA DIFF
percent (%) 55.92 44.08 -
Table 3: Human labeling of candidate Q-T.
no more than 6 words. In addition, only one slot
is allowed in each pair of paraphrase patterns. Let
s1 and s2 be a pair of paraphrases extracted above.
If there exist words w ? s1 and v ? s2 that satisfy
(1) w = v, (2) w and v are not stop words, then
we can induce a pair of paraphrase patterns by re-
placing w in s1 and v in s2 with a slot ?[X]?. It is
obvious that several pairs of paraphrase patterns
may be induced from one pair of paraphrases.
4 Experiments
We experiment with a query log that contains a
total of 284,316,659 queries. Statistics reveal that
170,315,807 queries (59.90%) lead to at least one
user click, each having 1.69 clicks on average. We
extract 287,129,850 raw Q-T pairs using the query
log, from which 4,448,347 pairs of candidate Q-
T are left after filtering as described in Section
3.2. Almost all queries and titles are written in
Chinese, though some of them contain English or
Japanese words. The preprocessing of candidate
Q-T includes Chinese word segmentation (WSeg)
and NE recognition (NER). Our WSeg tool is im-
plemented based on forward maximum matching,
while the NER tool is based on a NE dictionary
mined from the web.
4.1 Evaluation of Candidate Q-T
We first evaluate candidate Q-T without valida-
tion. To this end, we randomly sampled 5000
pairs of candidate Q-T and labeled them manu-
ally. Each pair is labeled into one of the 3 classes:
SAME - q and t have the same meaning; RELA - q
and t have related meanings; DIFF - q and t have
clearly different meanings. The labeling results
are listed in Table 3. We can see that no candidate
Q-T is in the DIFF class. This is not surprising,
since users are unlikely to click on web pages un-
related to their queries.
To gain a better insight into the data, we ana-
lyzed the subtle types of candidate Q-T in both
SAME and RELA classes. In detail, we sampled
1321
1000 pairs of candidate Q-T from the 5000 pairs
labeled above, in which 563 are in the SAME
class, while the other 437 are in the RELA class.
Our analysis suggests that candidate Q-T in the
SAME class can be divided into 4 subtle types:
? Trivial change (12.61%): changes of punctu-
ation or stop words, such as ??? ?? ?
??? and ??????????.
? Word or phrase replacement (68.38%): re-
placements of synonymous words or phrases,
such as ??? ? ? ?? ?? ? (how
mach is ...)? and ??? ? ? ?? ??
??? (what is the price of ...)?.
? Structure change (7.10%): changes of both
words and word orders, such as ?????
? ?? ? ?? (what fruit can I eat on a
diet)? and ?? ?? ?? ?? ?? (what
fruit can help loss weight)?.
? Others (11.90%): candidate Q-T that cannot
be classified into the 3 types above.
The above analysis reveals that more than two
thirds of candidate Q-T in the SAME class are in
the ?word or phrase replacement? type, while the
ones with structure changes are slightly more than
7%. We believe this is mainly because queries
and titles are relatively short and their structures
are simple. Thus structure rewriting can hardly be
conducted. This distribution is in line with that
reported in (Zhao et al, 2008).
As for the RELA class, we find that 42.33% of
such candidate Q-T share a problem of named en-
tity mismatch, such as ??? (US) ?? ??
??? and ??? (China) ?? ?? ?? ?
??. This indicates that the NE similarity feature
is necessary in paraphrase validation.
4.2 Evaluation of Paraphrase Q-T
The candidate Q-T extracted above are classified
with a SVM classifier5 under its default setting.
To evaluate the classifier, we run 5-fold cross val-
idation with the 5000 human annotated data, in
which we use 4000 for training and the rest 1000
for testing in each run. The evaluation criteria are
5We use libsvm-2.82 toolkit, which can be downloaded
from http://www.csie.ntu.edu.tw/ cjlin/libsvm/
precision (P), recall (R), and f-measure (F), which
are defined as follows:
P = ?Sa ? Sm??Sa?
(9)
R = ?Sa ? Sm??Sm?
(10)
F = 2? P ?RP +R (11)
where Sa is the set of paraphrases automatically
recognized with the classifier, Sm is the set of
paraphrases manually annotated. Precision, re-
call, and f-measure are averaged over 5 runs in
the 5-fold cross validation.
Figure 2 (a) shows the classification results
(dark bars). For comparison, we also show the
precision, recall6, and f-measure of the candidate
Q-T (light bars). As can be seen, the precision is
improved from 0.5592 to 0.7444 after classifica-
tion. F-measure is also evidently enhanced. This
result indicates that the classification-based para-
phrase validation is effective. We then use all of
the 5000 annotated data to train a classifier and
classify all the candidate Q-T. Results show that
2,762,291 out of 4,448,347 pairs of candidate Q-
T are classified as paraphrases.
4.3 Evaluation of Paraphrase Q-Q and T-T
From the paraphrase Q-T, we further extracted
934,758 pairs of candidate Q-Q and 438,954 pairs
of candidate T-T (without validation). We ran-
domly sampled 5000 from each for human an-
notation. The results show that the precisions of
candidate Q-Q and T-T are 0.4672 and 0.6860, re-
spectively. As can be seen, the precision of can-
didate Q-Q is much lower than that of candidate
T-T. Our analysis reveals that it is mainly because
candidate Q-Q are more noisy, since user queries
contain quite a lot of spelling mistakes and infor-
mal expressions.
The candidate Q-Q and T-T are also refined
based on classification. We first evaluate the clas-
sification performance using the 5000 human la-
beled data. The experimental setups for Q-Q and
6We assume all possible paraphrases are included in the
candidates, thus its recall is 100%.
1322
(a) Q-T classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.5592 1 0.7173
para. 0.7444 0.8391 0.7887
P R F
(b) Q-Q classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.4672 1 0.6369
para. 0.7345 0.6575 0.6938
P R F
(c) T-T classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.686 1 0.8138
para. 0.7056 0.9776 0.8196
P R F
Figure 2: Classification precision (P), recall (R), and f-measure (F).
T-T classification are the same as that of Q-T clas-
sification, in which we run 5-fold cross validation
with a SVM classifier using its default parameters.
Figure 2 (b) and (c) give the classification results
(dark bars) as well as the precision, recall, and f-
measure of the candidates (light bars).
We can see that the precision of Q-Q is signifi-
cantly enhanced from 0.4672 to 0.7345 after clas-
sification, which suggests that a substantial part
of errors and noise are removed. The increase of
f-measure demonstrates the effectiveness of clas-
sification despite the decrease of recall. Mean-
while, the quality of candidate T-T is not clearly
improved after classification. The reason should
be that the precision of candidate T-T is already
pretty high. We then use all 5000 human labeled
data to train a classifier for Q-Q and T-T respec-
tively and classify all candidate Q-Q and T-T. Re-
sults show that 390,920 pairs of paraphrase Q-Q
and 415,539 pairs of paraphrase T-T are extracted
after classification.
4.4 Evaluation of Paraphrase Patterns
Using the method introduced in Section 3.4, we
have generated 73,484 pairs of paraphrase pat-
terns that appear at least two times in the cor-
pus. We randomly selected 500 pairs and labeled
them manually. The results show that the preci-
sion is 78.4%. Two examples are shown in Ta-
ble 4, in which p1 and p2 are paraphrase patterns.
Some slot fillers are also listed below. We real-
p1 [X]??????
p2 ???? [X]??
(how to open [X] file)
slot 7z; ashx; aspx; bib; cda; cdfs; cmp;
cpi; csf; csv; cur; dat; dek...
p1 ?? [X]???
p2 ?? [X]???
(poems about [X])
slot ?? (prairies);?? (Yangtze River);
?? (Mount Tai);?? (nostalgia)...
Table 4: Examples of paraphrase patterns.
ize that the method currently used for inducing
paraphrase patterns is simple. Hence we will im-
prove the method in our following experiments.
Specifically, multiple slots will be allowed in a
pair of patterns. In addition, we will try to ap-
ply the alignment techniques in the generation of
paraphrase patterns, as Zhao et al (2008) did.
4.5 Analysis
Feature Contribution. To investigate the contri-
butions of different features used in classification,
we tried different feature combinations for each of
our three classifiers. The results are shown in Ta-
ble 5, in which ?+? means the feature has contri-
bution to the corresponding classifier. As can be
seen, the character overlap rate feature (FCOR),
cosine similarity feature (FCS), and NE similarity
1323
Feature Q-T Q-Q T-T
FF +
FLR +
FWOR
FCOR + + +
FCS + + +
FED +
FNE + + +
FPF +
Table 5: Feature contribution.
feature (FNE) are the most useful, which play im-
portant roles in all the three classifiers. The other
features are useful in some of the classifiers ex-
cept the word overlap rate feature (FWOR). The
classification results reported in prior sections are
all achieved with the optimal feature combination.
Analysis of the Paraphrases. We combine the
extracted paraphrase Q-T, Q-Q and T-T and get
a total of 3,560,257 pairs of unique paraphrases.
Statistics show that only 8380 pairs (0.24%) are
from more than one source, which indicates that
the intersection among the three sets is very small.
Further statistics show that the average length of
the queries and titles in the paraphrases is 6.69
(words).
To have a detailed analysis of the extracted
paraphrases, we randomly selected 1000 pairs and
manually labeled the precision, types, and do-
mains. It is found that more than 43% of the para-
phrases are paraphrase questions, in which how
(36%), what (19%), and yes/no (14%) questions
are the most common. In addition, we find that
the precision of paraphrase questions (84.26%)
is evidently higher than non-question paraphrases
(65.14%). Those paraphrase questions are useful
in question analysis and expansion in QA, which
can hardly be extracted from other kinds of cor-
pora.
As expected, the paraphrases we extract cover
a variety of domains. However, around 50% of
them are in the 7 most popular domains7, includ-
ing: (1) health and medicine, (2) documentary
download, (3) entertainment, (4) software, (5) ed-
7Note that pornographic queries have been filtered from
the query log beforehand.
ucation and study, (6) computer game, (7) econ-
omy and finance. This analysis reflects what web
users are most concerned about. These domains,
especially (4) and (6), are not well covered by the
parallel and comparable corpora previously used
for paraphrase extraction.
5 Conclusions and Future Directions
In this paper, we put forward a novel method that
extracts paraphrases from search engine query
logs. Our contribution is that we, for the first
time, propose to extract paraphrases from user
queries and the corresponding clicked document
titles. Specifically, three kinds of paraphrases
are extracted, which can be (1) a query and a
hit title, (2) two queries that hit the same title,
and (3) two titles hit by the same query. The
extracted paraphrases are refined based on clas-
sification. Using the proposed method, we ex-
tracted over 3.5 million pairs of paraphrases from
a query log of Baidu. Human evaluation results
show that the precision of the paraphrases is above
70%. The results also show that we can gener-
ate high-quality paraphrase patterns from the ex-
tracted paraphrases.
Our future research will be conducted along the
following directions. Firstly, we will use a much
larger query log for paraphrase extraction, so as to
enhance the coverage of paraphrases. Secondly,
we plan to have a deeper study of the transitivity
of paraphrasing. Simply speaking, we want to find
out whether we can extract ?s1, s3? as paraphrases
given that ?s1, s2? and ?s2, s3? are paraphrases.
6 Acknowledgments
We would like to thank Wanxiang Che, Hua Wu,
and the anonymous reviewers for their useful
comments on this paper.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a Spelling Error Model from Search Query
Logs. In Proceedings of HLT/EMNLP, pages 955-
962.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, pages 597-604.
1324
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of ACL/EACL, pages 50-57.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Sur-
face Patterns. In Proceedings of ACL-08: HLT,
pages 674-682.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP, pages 196-205.
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, Wei-Ying Ma.
2002. Probabilistic Query Expansion Using Query
Logs In Proceedings of WWW, pages 325-332.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News
Sources. In Proceedings of COLING, pages 350-
356.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian
Hu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007.
Cross-Lingual Query Suggestion Using Query Logs
of Different Languages. In Proceedings of SIGIR,
pages 463-470.
Ali Ibrahim, Boris Katz, Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolin-
gual Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natu-
ral Language Engineering 7(4): 343-360.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across
the Web. In Proceedings of IJCNLP, pages 119-
130.
Marius Pas?ca. 2007. Weakly-supervised Discovery
of Named Entities using Web Search Queries. In
Proceedings of CIKM, pages 683-690.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Matthew Richardson. 2008. Learning about the World
through Long-Term Query Logs. In ACM Transac-
tions on the Web 2(4): 1-27.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring
Ontological Knowledge from Query Logs. In Pro-
ceedings of WWW, pages 1223-1224.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from
News Articles. In Proceedings of HLT, pages 40-
46.
Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query Clustering Using User Logs. In ACM
Transactions on Information Systems 20(1): 59-81,
2002.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
Question Paraphrases for QA from Encarta Logs. In
Proceedings of IJCAI, pages 1795-1800.
1325
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326?1334,
Beijing, August 2010
Leveraging Multiple MT Engines for Paraphrase Generation
Shiqi Zhao??, Haifeng Wang?, Xiang Lan?, and Ting Liu?
?Baidu Inc.
?HIT Center for Information Retrieval, Harbin Institute of Technology
{zhaoshiqi, wanghaifeng}@baidu.com,
{xlan, tliu}@ir.hit.edu.cn
Abstract
This paper proposes a method that lever-
ages multiple machine translation (MT)
engines for paraphrase generation (PG).
The method includes two stages. Firstly,
we use a multi-pivot approach to acquire
a set of candidate paraphrases for a source
sentence S. Then, we employ two kinds
of techniques, namely the selection-based
technique and the decoding-based tech-
nique, to produce a best paraphrase T for
S using the candidates acquired in the first
stage. Experimental results show that:
(1) The multi-pivot approach is effective
for obtaining plenty of valuable candi-
date paraphrases. (2) Both the selection-
based and decoding-based techniques can
make good use of the candidates and pro-
duce high-quality paraphrases. Moreover,
these two techniques are complementary.
(3) The proposed method outperforms a
state-of-the-art paraphrase generation ap-
proach.
1 Introduction
This paper addresses the problem of paraphrase
generation (PG), which seeks to generate para-
phrases for sentences. PG is important in many
natural language processing (NLP) applications.
For example, in machine translation (MT), a
sentence can be paraphrased so as to make it
more translatable (Zhang and Yamamoto, 2002;
Callison-Burch et al, 2006). In question answer-
ing (QA), a question can be paraphrased to im-
prove the coverage of answer extraction (Duboue
and Chu-Carroll, 2006; Riezler et al, 2007). In
natural language generation (NLG), paraphrasing
can help to increase the expressive power of the
NLG systems (Iordanskaja et al, 1991).
In this paper, we propose a novel PG method.
For an English sentence S, the method first ac-
quires a set of candidate paraphrases with a multi-
pivot approach, which uses MT engines to auto-
matically translate S into multiple pivot languages
and then translate them back into English. Fur-
thermore, the method employs two kinds of tech-
niques to produce a best paraphrase T for S us-
ing the candidates, i.e., the selection-based and
decoding-based techniques. The former selects
a best paraphrase from the candidates based on
Minimum Bayes Risk (MBR), while the latter
trains a MT model using the candidates and gen-
erates paraphrases with a MT decoder.
We evaluate our method on a set of 1182 En-
glish sentences. The results show that: (1) al-
though the candidate paraphrases acquired by MT
engines are noisy, they provide good raw ma-
terials for further paraphrase generation; (2) the
selection-based technique is effective, which re-
sults in the best performance; (3) the decoding-
based technique is promising, which can generate
paraphrases that are different from the candidates;
(4) both the selection-based and decoding-based
techniques outperform a state-of-the-art approach
SPG (Zhao et al, 2009).
2 Related Work
2.1 Methods for Paraphrase Generation
MT-based method is the mainstream method on
PG. It regards PG as a monolingual machine trans-
lation problem, i.e., ?translating? a sentence S
into another sentence T in the same language.
1326
Quirk et al (2004) first presented the MT-based
method. They trained a statistical MT (SMT)
model on a monolingual parallel corpus extracted
from comparable news articles and applied the
model to generate paraphrases. Their work shows
that SMT techniques can be extended to PG. How-
ever, its usefulness is limited by the scarcity of
monolingual parallel data.
To overcome the data sparseness problem, Zhao
et al (2008a) improved the MT-based PG method
by training the paraphrase model using multi-
ple resources, including monolingual parallel cor-
pora, monolingual comparable corpora, bilingual
parallel corpora, etc. Their results show that bilin-
gual parallel corpora are the most useful among
the exploited resources. Zhao et al (2009) further
improved the method by introducing a usability
sub-model into the paraphrase model so as to gen-
erate varied paraphrases for different applications.
The main disadvantage of the MT-based
method is that its performance heavily depends on
the fine-grained paraphrases, such as paraphrase
phrases and patterns, which provide paraphrase
options in decoding. Hence one has to first ex-
tract fine-grained paraphrases from various cor-
pora with different methods (Zhao et al, 2008a;
Zhao et al, 2009), which is difficult and time-
consuming.
In addition to the MT-based method, re-
searchers have also investigated other methods for
paraphrase generation, such as the pattern-based
methods (Barzilay and Lee, 2003; Pang et al,
2003), thesaurus-based methods (Bolshakov and
Gelbukh, 2004; Kauchak and Barzilay, 2006),
and NLG-based methods (Kozlowski et al, 2003;
Power and Scott, 2005).
2.2 Pivot Approach for Paraphrasing
Bannard and Callison-Burch (2005) introduced
the pivot approach to extracting paraphrase
phrases from bilingual parallel corpora. Their ba-
sic assumption is that two English phrases aligned
with the same phrase in a foreign language (also
called a pivot language) are potential paraphrases.
Zhao et al (2008b) extended the approach and
used it to extract paraphrase patterns. Both of the
above works have proved the effectiveness of the
pivot approach in paraphrase extraction.
Pivot approach can also be used in paraphrase
generation. It generates paraphrases by translating
sentences from a source language to one (single-
pivot) or more (multi-pivot) pivot languages and
then translating them back to the source language.
Duboue et al (2006) first proposed the multi-
pivot approach for paraphrase generation, which
was specially designed for question expansion in
QA. In addition, Max (2009) presented a single-
pivot approach for generating sub-sentential para-
phrases. A clear difference between our method
and the above works is that we propose selection-
based and decoding-based techniques to gener-
ate high-quality paraphrases using the candidates
yielded from the pivot approach.
3 Multi-pivot Approach for Acquiring
Candidate Paraphrases
A single-pivot PG approach paraphrases a sen-
tence S by translating it into a pivot language
PL with a MT engine MT1 and then translat-
ing it back into the source language with MT2.
In this paper, a single-pivot PG system is repre-
sented as a triple (MT1, PL, MT2). A multi-
pivot PG system is made up of a set of single-pivot
systems with various pivot languages and MT en-
gines. Given m pivot languages and n MT en-
gines, we can build a multi-pivot PG system con-
sisting of N (N ? n ? m ? n) single-pivot ones,
where N = n ? m ? n iff all the n MT engines
can perform bidirectional translation between the
source and each pivot language.
In this work, we experiment with 6 pivot lan-
guages (Table 1) and 3 MT engines (Table 2) in
the multi-pivot approach. All the 3 MT engines
are off-the-shelf systems, in which Google and
Microsoft translators are SMT engines, while Sys-
tran translator is a rule-based MT engine. Each
MT engine can translate English to all the 6 pivot
languages and back to English. We thereby con-
struct a multi-pivot PG system consisting of 54
(3*6*3) single-pivot systems.
The advantages of the multi-pivot PG approach
lie in two aspects. First, it effectively makes use
of the vast bilingual data and translation rules un-
derlying the MT engines. Second, the approach is
simple, which just sends sentences to the online
MT engines and gets the translations back.
1327
Source Sentence he said there will be major cuts in the salaries of high-level civil servants .
(GG, G, MS) he said there are significant cuts in the salaries of high-level officials .
(GG, F , GG) he said there will be significant cuts in the salaries of top civil level .
(MS, C, MS) he said that there will be a major senior civil service pay cut .
(MS, F , ST ) he said there will be great cuts in the wages of the high level civils servant .
(ST , G, GG) he said that there are major cuts in the salaries of senior government officials .
Table 3: Examples of candidate paraphrases obtained using the multi-pivot approach.
1 French (F) 4 Italian (I)
2 German (G) 5 Portuguese (P)
3 Spanish (S) 6 Chinese (C)
Table 1: Pivot languages used in the approach.
1 Google Translate (GG)
(translate.google.com)
2 Microsoft Translator (MS)
(www.microsofttranslator.com)
3 Systran Online Translation (ST)
(www.systransoft.com)
Table 2: MT engines utilized in the approach.
4 Producing High-quality Paraphrases
using the Candidates
Table 3 shows some examples of candidate para-
phrases for a sentence. As can be seen, the can-
didates do provide some correct and useful para-
phrase substitutes (in bold) for the source sen-
tence. However, they also contain quite a few er-
rors (in italic) due to the limited translation qual-
ity of the MT engines. The problem is even
worse when the source sentences get longer and
more complicated. Therefore, we need to com-
bine the outputs of the multiple single-pivot PG
systems and produce high-quality paraphrases out
of them. To this end, we investigate two tech-
niques, namely, the selection-based and decoding-
based techniques.
4.1 Selection-based Technique
Given a source sentence S along with a set D of
candidate paraphrases {T1, T2, ..., Ti, ...TN}, the
goal of the selection-based technique is to select
the best paraphrase T?i for S from D. The para-
phrase selection technique we propose is based on
Minimum Bayes Risk (MBR). In detail, the MBR
based technique first measures the quality of each
candidate paraphrase Ti ? D in terms of Bayes
risk (BR), and then selects the one with the min-
imum BR as the best paraphrase. In detail, given
S, a candidate Ti ? D, a reference paraphrase
T 1, and a loss function L(T, Ti) that measures the
quality of Ti relative to T , we define the Bayes
risk as follows:
BR(Ti) = EP (T,S)[L(T, Ti)], (1)
where the expectation is taken under the true dis-
tribution P (T, S) of the paraphrases. According
to (Bickel and Doksum, 1977), the candidate para-
phrase that minimizes the Bayes risk can be found
as follows:
T?i = arg minTi?D
?
T?T
L(T, Ti)P (T |S), (2)
where T represents the space of reference para-
phrases. In practice, however, the collection of
reference paraphrases is not available. We thus
construct a set D? = D?{S} to approximate T 2.
In addition, we cannot estimate P (T |S) in Equa-
tion (2), either. Therefore, we make a simplifica-
tion by assigning a constant c to P (T |S) for each
T ? D?, which can then be removed:
T?i = arg minTi?D
?
T?D?
L(T, Ti). (3)
Equation (3) can be further rewritten using a gain
function G(T, Ti) instead of the loss function:
1Here we assume that we have the collection of all possi-
ble paraphrases of S, which are used as references.
2The source sentence S is included in D? based on the
consideration that a sentence is allowed to keep unchanged
during paraphrasing.
1328
T?i = arg maxTi?D
?
T?D?
G(T, Ti). (4)
We define the gain function based on BLEU:
G(T, Ti) = BLEU(T, Ti). BLEU is a
widely used metric in the automatic evaluation of
MT (Papineni et al, 2002). It measures the sim-
ilarity of two sentences by counting the overlap-
ping n-grams (n=1,2,3,4 in our experiments):
BLEU(T, Ti) = BP ?exp(
4?
n=1
wn log pn(T, Ti)),
where pn(T, Ti) is the n-gram precision of Ti and
wn = 1/4. BP (? 1) is a brevity penalty that
penalizes Ti if it is shorter than T .
In summary, for each sentence S, the MBR
based technique selects a paraphrase that is the
most similar to all candidates and the source sen-
tence. The underlying assumption is that correct
paraphrase substitutes should be common among
the candidates, while errors committed by the
single-pivot PG systems should be all different.
We denote this approach as S-1 hereafter.
Approaches for comparison. In the experiments,
we also design another two paraphrase selection
approaches S-2 and S-3 for comparison with S-1.
S-2: S-2 selects the best single-pivot PG
system from all the 54 ones. The selection
is also based on MBR and BLEU. For each
single-pivot PG system, we sum up its gain
function values over a set of source sentences
(i.e., ?S
?
TS?D?S G(TS , TSi)). Then we se-lect the one with the maximum gain value as
the best single-pivot system. In our experi-
ments, the selected best single-pivot PG system is
(ST, P,GG), the candidate paraphrases acquired
by which are then returned as the best paraphrases
in S-2.
S-3: S-3 is a simple baseline, which just ran-
domly selects a paraphrase from the 54 candidates
for each source sentence S.
4.2 Decoding-based Technique
The selection-based technique introduced above
has an inherent limitation that it can only select
a paraphrase from the candidates. That is to say, it
major cuts high-level civil servants
significant cuts senior officials
major cuts* high-level officials
important cuts senior civil servants
big cuts
great cuts
Table 4: Extracted phrase pairs. (*This is called
a self-paraphrase of the source phrase, which
is generated when a phrase keeps unchanged in
some of the candidate paraphrases.)
can never produce a perfect paraphrase if all the
candidates have some tiny flaws. To solve this
problem, we propose the decoding-based tech-
nique, which trains a MT model using the can-
didate paraphrases of each source sentence S and
generates a new paraphrase T for S with a MT
decoder.
In this work, we implement the decoding-based
technique using Giza++ (Och and Ney, 2000) and
Moses (Hoang and Koehn, 2008), both of which
are commonly used SMT tools. For a sentence
S, we first construct a set of parallel sentences
by pairing S with each of its candidate para-
phrases: {(S,T1),(S,T2),...,(S,TN )} (N = 54).
We then run word alignment on the set using
Giza++ and extract aligned phrase pairs as de-
scribed in (Koehn, 2004). Here we only keep the
phrase pairs that are aligned ?3 times on the set,
so as to filter errors brought by the noisy sentence
pairs. The extracted phrase pairs are stored in a
phrase table. Table 4 shows some extracted phrase
pairs.
Note that Giza++ is sensitive to the data size.
Hence it is interesting to examine if the alignment
can be improved by augmenting the parallel sen-
tence pairs. To this end, we have tried augmenting
the parallel set for each sentence S by pairing any
two candidate paraphrases. In this manner, C2N
sentence pairs are augmented for each S. We con-
duct word alignment using the (N+C2N ) sentence
pairs and extract aligned phrases from the original
N pairs. However, we have not found clear im-
provement after observing the results. Therefore,
we do not adopt the augmentation strategy in our
experiments.
1329
Using the extracted phrasal paraphrases, we
conduct decoding for the sentence S with Moses,
which is based on a log-linear model. The default
setting of Moses is used, except that the distortion
model for phrase reordering is turned off3. The
language model in Moses is trained using a 9 GB
English corpus. We denote the above approach as
D-1 in what follows.
Approach for comparison. The main advantage
of the decoding-based technique is that it allows
us to customize the paraphrases for different re-
quirements through tailoring the phrase table or
tuning the model parameters. As a case study,
this paper shows how to generate paraphrases with
varied paraphrase rates4.
D-2: The extracted phrasal paraphrases (in-
cluding self-paraphrases) are stored in a phrase ta-
ble, in which each phrase pair has 4 scores mea-
suring their alignment confidence (Koehn et al,
2003). Our basic idea is to control the paraphrase
rate by tuning the scores of the self-paraphrases.
We thus extend D-1 to D-2, which assigns a
weight ? (? > 0) to the scores of the self-
paraphrase pairs. Obviously, if we set ? < 1,
the self-paraphrases will be penalized and the de-
coder will prefer to generate a paraphrase with
more changes. If we set ? > 1, the decoder will
tend to generate a paraphrase that is more similar
to the source sentence. In our experiments, we set
? = 0.1 in D-2.
5 Experimental Setup
Our test sentences are extracted from the paral-
lel reference translations of a Chinese-to-English
MT evaluation5, in which each Chinese sentence
c has 4 English reference translations, namely e1,
e2, e3, and e4. We use e1 as a test sentence to para-
phrase and e2, e3, e4 as human paraphrases of e1
for comparison with the automatically generated
paraphrases. We process the test set by manually
filtering ill-formed sentences, such as the ungram-
matical or incomplete ones. 1182 out of 1357
3We conduct monotone decoding as previous work
(Quirk et al, 2004; Zhao et al, 2008a, Zhao et al, 2009).
4The paraphrase rate reflects how different a paraphrase
is from the source sentence.
52008 NIST Open Machine Translation Evaluation: Chi-
nese to English Task.
Score Adequacy Fluency
5 All Flawless English
4 Most Good English
3 Much Non-native English
2 Little Disfluent English
1 None Incomprehensible
Table 5: Five point scale for human evaluation.
test sentences are retained after filtering. Statistics
show that about half of the test sentences are from
news and the other half are from essays. The aver-
age length of the test sentences is 34.12 (words).
Manual evaluation is used in this work. A para-
phrase T of a sentence S is manually scored based
on a five point scale, which measures both the ?ad-
equacy? (i.e., how much of the meaning of S is
preserved in T ) and ?fluency? of T (See Table 5).
The five point scale used here is similar to that in
the human evaluation of MT (Callison-Burch et
al., 2007). In MT, adequacy and fluency are eval-
uated separately. However, we find that there is a
high correlation between the two aspects, which
makes it difficult to separate them. Thus we com-
bine them in this paper.
We compare our method with a state-of-the-
art approach SPG6 (Zhao et al, 2009), which
is a statistical approach specially designed for
PG. The approach first collects a large volume of
fine-grained paraphrase resources, including para-
phrase phrases, patterns, and collocations, from
various corpora using different methods. Then it
generates paraphrases using these resources with
a statistical model7.
6 Experimental Results
We evaluate six approaches, i.e., S-1, S-2, S-3, D-
1, D-2 and SPG, in the experiments. Each ap-
proach generates a 1-best paraphrase for a test
sentence S. We randomize the order of the 6 para-
phrases of each S to avoid bias of the raters.
6SPG: Statistical Paraphrase Generation.
7We ran SPG under the setting of baseline-2 as described
in (Zhao et al, 2009).
1330
00.5
1
1.52
2.53
3.54
4.5
score 3.92 3.52 2.78 3.62 3.36 3.47
S-1 S-2 S-3 D-1 D-2 SPG
Figure 1: Evaluation results of the approaches.
6.1 Human Evaluation Results
We have 6 raters in the evaluation, all of whom
are postgraduate students. In particular, 3 raters
major in English, while the other 3 major in com-
puter science. Each rater scores the paraphrases
of 1/6 test sentences, whose results are then com-
bined to form the final scoring result. The av-
erage scores of the six approaches are shown in
Figure 1. We can find that among the selection-
based approaches, the performance of S-3 is the
worst, which indicates that randomly selecting a
paraphrase from the candidates works badly. S-
2 performs much better than S-3, suggesting that
the quality of the paraphrases acquired with the
best single-pivot PG system are much higher than
the randomly selected ones. S-1 performs the best
in all the six approaches, which demonstrates the
effectiveness of the MBR-based selection tech-
nique. Additionally, the fact that S-1 evidently
outperforms S-2 suggests that it is necessary to ex-
tend a single-pivot approach to a multi-pivot one.
To get a deeper insight of S-1, we randomly
sample 100 test sentences and manually score all
of their candidates. We find that S-1 successfully
picks out a paraphrase with the highest score for
72 test sentences. We further analyze the remain-
ing 28 sentences for which S-1 fails and find that
the failures are mainly due to the BLEU-based
gain function. For example, S-1 sometimes se-
lects paraphrases that have correct phrases but in-
correct phrase orders, since BLEU is weak in eval-
uating phrase orders and sentence structures. In
the next step we shall improve the gain function
by investigating other features besides BLEU.
In the decoding-based approaches, D-1 ranks
the second in the six approaches only behind S-1.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
S-1 S-2 S-3 D-1 D-2 SPG
r1 r2 r3 r4 r5 r6
Figure 2: Evaluation results from each rater.
We will further improve D-1 in the future rather
than simply use Moses in decoding with the de-
fault setting. However, the value of D-1 lies in
that it enables us to break down the candidates
and generate new paraphrases flexibly. The per-
formance decreases when we extend D-1 to D-2
to achieve a larger paraphrase rate. This is mainly
because more errors are brought in when more
parts of a sentence are paraphrased.
We can also find from Figure 1 that S-1, S-2,
and D-1 all get higher scores than SPG, which
shows that our method outperforms this state-of-
the-art approach. This is more important if we
consider that our method is lightweight, which
makes no effort to collect fine-grained paraphrase
resources beforehand. After observing the results,
we believe that the outperformance of our method
can be mainly ascribed to the selection-based and
decoding-based techniques, since we avoid many
errors by voting among the candidates. For in-
stance, an ambiguous phrase may be incorrectly
paraphrased by some of the single-pivot PG sys-
tems or the SPG approach. However, our method
may obtain the correct paraphrase through statis-
tics over all candidates and selecting the most
credible one.
The human evaluation of paraphrases is subjec-
tive. Hence it is necessary to examine the coher-
ence among the raters. The scoring results from
the six raters are depicted in Figure 2. As it can be
seen, they show similar trends though the raters
have different degrees of strictness.
1331
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
PR1 0.116 0.138 0.232 0.149 0.206 0.139 0.366 0.379 0.386PR2 0.211 0.272 0.427 0.22 0.3 0.234 0.607 0.602 0.694
S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3
Figure 3: Paraphrase rates of the approaches.
6.2 Paraphrase Rate
Human evaluation assesses the quality of para-
phrases. However, the paraphrase rates cannot be
reflected. A paraphrase that is totally transformed
from the source sentence and another that is al-
most unchanged may get the same score. There-
fore, we propose two strategies, i.e., PR1 and PR2,
to compute the paraphrase rate:
PR1(T ) = 1? OL(S, T )L(S) ; PR2(T ) =
ED(S, T )
L(S) .
Here, PR1 is defined based on word overlapping
rate, in which OL(S, T ) denotes the number of
overlapping words between a paraphrase T and its
source sentence S, L(S) denotes the number of
words in S. PR2 is defined based on edit distance,
in which ED(S, T ) denotes the edit distance be-
tween T and S. Obviously, PR1 only measures
the percentage of words that are changed from
S to T , whereas PR2 further takes word order
changes into consideration. It should be noted that
PR1 and PR2 not only count the correct changes
between S and T , but also count the incorrect
ones. We compute the paraphrase rate for each
of the six approaches by averaging the paraphrase
rates over the whole test set. The results are shown
in the left part of Figure 3.
On the whole, the paraphrase rates of the ap-
proaches are not high. In particular, we can see
that the paraphrase rate of D-2 is clearly higher
than D-1, which is in line with our intention of de-
signing D-2. We can also see that the paraphrase
rate of S-3 is the highest among the approaches.
We find it is mainly because the paraphrases gen-
erated with S-3 contain quite a lot of errors, which
contribute most of the changes.
7 Analysis
7.1 Effectiveness of the Proposed Method
Our analysis starts from the candidate paraphrases
acquired with the multi-pivot approach. Actu-
ally, the results of S-3 reflect the average qual-
ity of the candidate paraphrases. A score of 2.78
(See Figure 1) indicates that the candidates are
unacceptable according to the human evaluation
metrics. This is in line with our expectation that
the automatically acquired paraphrases through a
two-way translation are noisy. However, the re-
sults of S-1 and D-1 demonstrate that, using the
selection-based and decoding-based techniques,
we can produce paraphrases of good quality. Es-
pecially, S-1 gets a score of nearly 4, which sug-
gests that the paraphrases are pretty good accord-
ing to our metrics. Moreover, our method out-
performs SPG built on pre-extracted fine-grained
paraphrases. It shows that our method makes good
use of the paraphrase knowledge from the large
volume of bilingual data underlying the multiple
MT engines.
7.2 How to Choose Pivot Languages and MT
Engines in the Multi-pivot Approach
In our experiments, besides the six pivot lan-
guages used in the multi-pivot system, we have
also tried another five pivot languages, including
Arabic, Japanese, Korean, Russian, and Dutch.
They are finally abandoned since we find that they
perform badly. Our experience on choosing pivot
languages is that: (1) a pivot language should be
a language whose translation quality can be well
guaranteed by the MT engines; (2) it is better to
choose a pivot language similar to the source lan-
guage (e.g., French - English), which is easier to
translate; (3) the translation quality of a pivot lan-
guage should not vary a lot among the MT en-
gines. On the other hand, it is better to choose
MT engines built on diverse models and corpora,
which can provide different paraphrase options.
We plan to employ a syntax-based MT engine in
our further experiments besides the currently used
phrase-based SMT and rule-based MT engines.
1332
S he said there will be major cuts in the salaries of high-level civil servants .
S-1 he said that there will be significant cuts in the salaries of senior officials .
S-2 he said there will be major cuts in salaries of civil servants high level .
S-3 he said that there will be significant cuts in the salaries of senior officials .
D-1 he said , there will be significant cuts in salaries of senior civil servants .
D-2 he said , there will be significant cuts in salaries of senior officials .
SPG he said that there will be the main cuts in the wages of high-level civil servants .
HP1 he said there will be a big salary cut for high-level government employees .
HP2 he said salaries of senior public servants would be slashed .
HP3 he claimed to implement huge salary cut to senior civil servants .
Table 6: Comparing the automatically generated paraphrases with the human paraphrases.
7.3 Comparing the Selection-based and
Decoding-based Techniques
It is necessary to compare the paraphrases gener-
ated via the selection-based and decoding-based
techniques. As stated above, the selection-based
technique can only select a paraphrase from the
candidates, while the decoding-based technique
can generate a paraphrase different from all can-
didates. In our experiments, we find that for
about 90% test sentences, the paraphrases gener-
ated by the decoding-based approach D-1 are out-
side the candidates. In particular, we compare the
paraphrases generated by S-1 and D-1 and find
that, for about 40% test sentences, S-1 gets higher
scores than D-1, while for another 21% test sen-
tences, D-1 gets higher scores than S-18. This
indicates that the selection-based and decoding-
based techniques are complementary. In addition,
we find examples in which the decoding-based
technique can generate a perfect paraphrase for
the source sentence, even if all the candidate para-
phrases have obvious errors. This also shows that
the decoding-based technique is promising.
7.4 Comparing Automatically Generated
Paraphrases with Human Paraphrases
We also analyze the characteristics of the gener-
ated paraphrases and compare them with the hu-
man paraphrases (i.e., the other 3 reference trans-
lations in the MT evaluation, see Section 5, which
are denoted as HP1, HP2, and HP3). We find that,
compared with the automatically generated para-
phrases, the human paraphrases are more com-
8For the rest 39%, S-1 and D-1 get identical scores.
plicated, which involve not only phrase replace-
ments, but also structure reformulations and even
inferences. Their paraphrase rates are also much
higher, which can be seen in the right part of Fig-
ure 3. We show the automatic and human para-
phrases for the example sentence of this paper in
Table 6. To narrow the gap between the automatic
and human paraphrases, it is necessary to learn
structural paraphrase knowledge from the candi-
dates in the future work.
8 Conclusions and Future Work
We put forward an effective method for para-
phrase generation, which has the following con-
tributions. First, it acquires a rich fund of para-
phrase knowledge through the use of multiple MT
engines and pivot languages. Second, it presents
a MBR-based technique that effectively selects
high-quality paraphrases from the noisy candi-
dates. Third, it proposes a decoding-based tech-
nique, which can generate paraphrases that are
different from the candidates. Experimental re-
sults show that the proposed method outperforms
a state-of-the-art approach SPG.
In the future work, we plan to improve the
selection-based and decoding-based techniques.
We will try some standard system combination
strategies, like confusion networks and consensus
decoding. In addition, we will refine our evalu-
ation metrics. In the current experiments, para-
phrase correctness (adequacy and fluency) and
paraphrase rate are evaluated separately, which
seem to be incompatible. We plan to combine
them together and propose a uniform metric.
1333
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics: Basic Ideas and Selected Topics.
Holden-Day Inc., Oakland, CA, USA.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL-2007 Workshop on Statistical Ma-
chine Translation, pages 136-158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Ver-
sion 1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-
sentence paraphrases from predicate/argument
structure using lexico-grammatical resources. In
Proceedings of IWP, pages 1-8.
Aure?lien Max. 2009. Sub-sentential Paraphrasing by
Contextual Pivot Translation. In Proceedings of the
2009 Workshop on Applied Textual Inference, ACL-
IJCNLP 2009, pages 18-26.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
ACL, pages 440-447.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations:
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of HLT-NAACL, pages 102-
109.
Kishore Papineni, Salim Roukos, ToddWard, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic
generation of large-scale paraphrases. In Proceed-
ings of IWP, pages 73-79.
Chris Quirk, Chris Brockett, andWilliamDolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.
Yujie Zhang and Kazuhide Yamamoto. 2002. Para-
phrasing of Chinese Utterances. In Proceedings of
COLING, pages 1163-1169.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven Statistical Paraphrase Genera-
tion. In Proceedings of ACL-IJCNLP 2009, pages
834-842.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. CombiningMultiple Resources to
Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
1334
Coling 2008: Paraphrases and Applications?Tutorial notes, pages 1?87,
Beijing, August 2010
Paraphrases and Applications
Shiqi Zhao
Baidu, Inc.
Haifeng Wang
Baidu, Inc.
Outline
? Part I
? Introduction
?Paraphrase Identification
?Paraphrase Extraction
? Part II
?Paraphrase Generation
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
1
? Paraphrase 
?Noun
Definition
? Alternative expressions of the same meaning
?Verb 
? Generate paraphrases for the input expression
? ?same meaning??
?Quite subjective
?Different degrees of strictness
?Depend on applications
Paraphrase (noun): Alternative expressions of the same meaning
Korean Kim Yuna won gold
with a world-record score in 
women's figure skating at the 
Vancouver Olympics Thursday.
Korean figure skater Kim 
Yuna has won the gold 
medal of women?s figure 
skating at the Winter 
Olympics in Vancouver
Kim Yu-Na (19) is a South 
Korean ice skater who took 
the gold medal at the 
Vancouver Olympics.
Kim Yuna, a South Korean 
figure skater has won the 
gold medal at the on-
going Winter Olympics 
2010.
Yuna Kim of South Korea 
won the women's figure 
skating gold medal at the 
Vancouver Olympics in 
record fashion.
2
Paraphrase (verb): Generate paraphrases for an input S.
Automatic 
S
paraphrase 
generation
T1 T2 T3 T4
Classification of Paraphrases
? According to granularity
?Surface paraphrases 
? Lexical level
? Phrase level
? Sentence level
? Discourse level
?Structural paraphrases
? Pattern level 
? Collocation level
3
Examples
? Lexical paraphrases (generally synonyms)
? solve and resolve
? Paraphrase phrases
? look after and take care of
? Paraphrase sentences
? The table was set up in the carriage shed.
? The table was laid under the cart-shed.
? Paraphrase patterns
[X] considers [Y]?   
? [X] takes [Y] into consideration
? Paraphrase collocations
? (turn on, OBJ, light)
? (switch on, OBJ, light)
? According to paraphrase style
?Trivial change
Classification of Paraphrases
 
?Phrase replacement
?Phrase reordering
?Sentence split & merge
?Complex paraphrases
4
Examples
? Trivial change
? all the members of and all members of 
? Phrase replacement
? He said there will be major cuts in the salaries of high-level civil servants.
? He said there will be major cuts in the salaries of senior officials.
? Phrase reordering
? Last night, I saw Tom in the shopping mall.
? I saw Tom in the shopping mall last night.
? Sentence split & merge   
? He bought a computer, which is very expensive.
? (1) He bought a computer. (2) The computer is very expensive.
? Complex paraphrases
? He said there will be major cuts in the salaries of high-level civil servants.
? He claimed to implement huge salary cut to senior civil servants.
Applications of Paraphrases
? Machine Translation (MT)
? Simplify input sentences
? Summarization 
? Sentence clustering
? Alleviate data sparseness 
? Parameter tuning
? Automatic evaluation
? Question Answering (QA)
? Question reformulation
? Information Extraction (IE)
? IE pattern expansion
? Automatic evaluation
? Natural Language Generation 
(NLG)
? Sentence rewriting 
? Others
? Changing writing style
? Text simplification  
? Information Retrieval (IR)
? Query reformulation 
? Identifying plagiarism 
? Text steganography
? ??
5
Research on Paraphrasing
? Paraphrase identification
?Identify (sentential) paraphrases  
? Paraphrase extraction
?Extract paraphrase instances (different granularities)
? Paraphrase generation
?Generate (sentential) paraphrases
? P h li tiarap rase app ca ons
?Apply paraphrases in other areas
Textual Entailment ? A Similar Direction
? Textual entailment:
?A directional relation between two text fragments      
? T: the entailing text
? H: the entailed hypothesis
?T entails H if, typically, a human reading T would infer 
that H is most likely true.
?Compare entailment with paraphrase
? P h i bidi i l ilarap rase s rect ona  enta ment 
6
Text Entailment ? A Similar Direction
? Recognizing Textual Entailment Track (RTE)
?RTE 1 (2004) to RTE 5 (2009)-    -  
?RTE-6 (2010) is in progress
? Example:
?T: A shootout at the Guadalajara airport in May, 1993, 
killed Cardinal Juan Jesus Posadas Ocampo.
?H: Juan Jesus Posadas Ocampo died in 1993      .
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? Part II
?Paraphrase Generation
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
7
Paraphrase Identification
? Specially refers to sentential paraphrase 
identification
?Given any pair of sentences, automatically identifies 
whether these two sentences are paraphrases
? Paraphrase identification is not trivial
Susan often goes to see movies with her boyfriend.       
Susan never goes to see movies with her boyfriend.
He said there will be major cuts in the salaries of high-level civil servants.
He claimed to implement huge salary cut to senior civil servants.
Overview 
? Classification based methods
?Reviewed as a binary classification problem i e     , . ., 
input s1 and s2 to a classifier and output 0/1
?Compute the similarities between s1 and s2 at different  
levels, which are then used as classification features
? Alignment based methods
?Align s1 and s2 first, and score the sentence pair 
based on the alignment results
? Alignment based on ITG
? Alignment based on quasi-synchronous dependency 
grammars 
8
Classification based Methods
? Brockett and Dolan, 2005
?Features:
? String similarity features
?Sentence length, word overlap, edit distance, ?
? Morphological variants
?Word pairs with the same stem
? WordNet lexical mappings
?Synonym pairs / word-hypernym pairs from WordNet
orbit | orbital 
operation | procedure 
? Word association pairs
?Automatically learned synonym pairs
?Classifier
? SVM classifier
vendors | suppliers 
Classification based Methods (cont?)
? Finch et al, 2005
?Using MT evaluation techniques to compute sentence       
similarities, which are then used as classification 
features
? WER, PER, BLEU, NIST
? Feature vector vec(s1, s2)
? vec1(s1, s2): s1 as reference, s2 as MT system output;
? vec2(s1, s2): s2 as reference, s1 as MT system output;
? vec(s1, s2): average of vec1(s1, s2) and vec2(s1, s2): 
?Classifier
? SVM classifier
9
Classification based Methods (cont?)
? Malakasiotis, 2009
?Combining multiple classification features   
? String similarity (various levels)
?Tokens, stems, POS tags, nouns only, verbs only, ?
? Different measures
?Edit distance, Jaro-Winkler distance, Manhattan distance?
? Synonym similarity
?Treat synonyms in two sentences as identical words
? Syntax similarity
?Dependency parsing of two sentences and compute the 
overlap of dependencies
?Classifier
? Maximum Entropy classifier
Alignment based Methods
? Wu, 2005
?Conduct alignment based on Inversion Transduction      
Grammars (ITG)
? Sensitive to the differences in sentence structures
? Without using any thesaurus to deal with lexical variation
?Performance is comparable to the classification 
based methods
Al f ll i i i t t l t il t? so per orms we  n recogn z ng ex ua  en a men
10
Alignment based Methods (cont?)
? Das and Smith, 2009
?Conduct alignment based on Quasi-Synchronous     
Dependency Grammar (QG)
? Alignment between two dependency trees
? Assumption: the dependency trees of two paraphrase 
sentences should be aligned closely
?Why does it work?
About 120 potential jurors were being asked to complete a lengthy questionnaire .
Align words that 
are not identical
?Performs competitively with classification based 
methods
The jurors were taken into the courtroom in groups of 40 and asked to fill out a questionnaire .
A Summary
? Classification based method is still the 
mainstream method since: , 
?Binary classification problem is well defined;
?Classification algorithms and tools are readily 
available;
?It can combine various features in a simple way;
?It achieves state-of-the-art performance.
11
References 
? Brockett and Dolan. 2005. Support Vector Machines for Paraphrase 
Identification and Corpus Construction.
? Fi h t l 2005 U i M hi T l ti E l ti T h i tnc  e  a . . s ng ac ne rans a on va ua on ec n ques o 
Determine Sentence-level Semantic Equivalence.
? Wu. 2005. Recognizing Paraphrases and Textual Entailment using 
Inversion Transduction Grammars.
? Malakasiotis. 2009. Paraphrase Recognition Using Machine Learning to 
Combine Similarity Measures.
? Das and Smith. 2009. Paraphrase Identification as Probabilistic Quasi-
Synchronous Recognition.
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? Part II
?Paraphrase Generation
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
12
Corpora Assumption Algorithm
Three Elements for Paraphrase 
Extraction
? thesauri
? monolingual parallel 
corpora
? monolingual compar-
able corpora
? bilingual parallel 
? Different translation 
versions preserve 
the meaning of the
original source
? Comparable news 
articles may contain 
distinct descriptions 
? co-training 
? classification 
? logistic regression
? clustering
? word alignment
???
corpora
? large web corpora
? search engine query
logs
? dictionary glosses
???
of the same facts
? Multiple phrases that 
align with the same 
foreign phrase may 
have the same mean-
ing
? Distributional hypoth-
esis
???
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
13
Method Overview
? Extract words with specific semantic relations as 
paraphrases
?Most common: synonyms
?Other relations: hypernyms, hyponyms?
? Widely used thesauri
?In English
? WordNet
?In other languages
? E.g., HowNet, Tongyici Cilin in Chinese
Pros and Cons
? Pros
?Existing resources 
?High quality
? Thesauri are hand crafted 
? Cons
?Language limitation
? Thesauri are not available in many languages
?Difficult to update
14
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
Method Overview
? Corpus
?Multiple translations of the same foreign literary work       
? Assumption
?Different translation versions preserve the meaning of 
the original source, but may use different expressions
15
Vingt mille lieues sous les mers
(in French)
Example
20000 Leagues Under the Sea
(different English translation versions)
??
Sentence Alignment and Preprocessing
? Barzilay and McKeown, 2001
?Collected 11 English translations for 5 foreign novels       
? E.g., Madame Bovary, Fairy Tale, Twenty Thousand 
Leagues under the sea?
?Sentence alignment
? A dynamic programming algorithm 
? Produced 44,562 pairs of parallel sentences
? Precision is 94 5%  .
?Other preprocessing 
? POS tagging and chunking
? Phrases are the atomic units in paraphrase extraction
16
Paraphrase Phrase Extraction
? Barzilay and McKeown, 2001 (cont?)
?Extracting paraphrase phrases  
? Assumption: phrases in aligned sentences which appear in 
similar contexts are paraphrases
? Method: co-training
? Iteratively learn contexts and paraphrases
Left context right contextparaphrases
My imagination melted into hazy drowsiness , and I soon fell into an uneasy slumber .
My imagination wandered  into vague unconsciousness , and I soon fell into a deep sleep .
Pros and Cons
? Pros
?Easy to align monolingual parallel sentences     
? Cons
?Domain limitation
? Limited in literary works
?Scale limitation
? Th i f th i l ti l lle s ze o  e corpus s re a ve y sma
?Context dependence
? E.g., ?John said? and ?he said?
17
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
Method Overview
? Corpus
?News articles that report the same event within a brief          
period of time
? Produced by different news agencies
? Assumption
?Comparable news articles may contain distinct 
descriptions of the same facts
18
Example
Comparable documents
d1 d2
Procedure
News 
corpus
Paraphrase
phrases
Paraphrase
patterns
Paraphrase 
generation
Identify
comparable 
documents
Extract
paraphrase
phrases
Extract
paraphrase
patterns
MT-based
paraphrase
generation
model
Extract
parallel
sentences
Comparable 
documents
Parallel
corpus
19
Identify Comparable Documents
? Input
?News articles from different news agencies     
? E.g., CNN, New York Times, Washington Post?
? Processing
?Method-1: Retrieve documents on a given topic or event
? Needs predefined topics or events
?Method-2: Cluster documents
? Content similarity; time interval
? Output
?Corpus of comparable documents
Extract Parallel (Paraphrase) Sentences
? Input
?Corpus of comparable documents   
? Processing
?Sentence clustering
? Method-1: based on an assumption: first sentences of a 
news article usually summarize its content
? Method-2: based on computing the content similarity
? Output
?Corpus of parallel (paraphrase) sentences
20
Extract Paraphrase Patterns
? Using NEs as anchors
? Shinyama et al, 2002
? Basic idea: paraphrase sentences should contain comparable NEs
Comparable NEs
Slots of the same type
paraphrases
Extract Paraphrase Patterns
? Multiple-sequence alignment
? Barzilay and Lee, 2003
backbone slot
21
Pros and Cons
? Pros
?Language independent-
? Comparable news can be found in many languages
? Cons
?Domain-dependent
? Paraphrases are extracted from specific domains or topics
?Sentence clustering
? Either too strict or too loose
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
22
Method Overview
? Corpus
?A parallel corpus of the source language and a         
foreign language
? Assumption
?Multiple phrases that align with the same foreign 
phrase may have the same meaning
? The method is also termed as ?pivot approach?       
Example 
source language
foreign language
(pivot language) 
Alignment
??
different parts
??
different places
??
????
??
????
ei
ej
cm
cn
??
various locations
??
??
ek
23
A Simple Version
? Takao et al, 2002 
?Basic idea: 
? Generating lexical paraphrases using 2-way dictionaries
? English word e1 can be translated to a Japanese word j with 
an E-J dic. D1, and then j can be translated back to an 
English word e2 with a J-E dictionary D2. e1 and e2 are 
extracted as paraphrases
Extracting Paraphrase Phrases
? Bannard and Callison-Burch, 2005 
?Word alignment and phrase extraction    
?Basic assumption:
? If two English phrases e1 and e2 can be aligned with the 
same foreign phrase f, e1 and e2 are likely to be paraphrases. 
?Paraphrase probability:
2 1
2 2 1? arg max ( | )e ee p e e?= Pivot in a foreign language
2 1
1 2arg max ( | ) ( | )e e
f
p f e p e f
?
= ?
Translation probability 
 
24
?should take the matter into consideration?
??????????
take the matter into consideration
Bannard & Callison-Burch (2005) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
take the matter into account
take the matter into consideration
the consideration of this matter
the consideration of this matter
take the matter into account
He?ll take the matter into consideration
????????
We need to consider this matter
??????????
consider this matter
take the matter into consideration
Add Syntactic Constraints
? Callison-Burch, 2008
?Basic idea: 
? Two paraphrase phrases should have the same syntactic 
type.
?Paraphrase probability:
2 2 1 2 1
2 2 1 1
: ( ) ( )
? arg max ( | , ( ))
arg max ( | ( )) ( | ( ))
e e e s e s e
e p e e s e
p f e s e p e f s e
? ? =
=
= ?
given the syntactic type
?Syntactic constraints are also used when substituting 
paraphrases in sentences
2 2 1 2 1
1 1 2 1
: ( ) ( )
, ,
e e e s e s e f? ? =
25
?should take the matter into consideration?
??????????
take the matter into consideration
take the matter into account
Callison-Burch (2008) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
    
take the matter into consideration
the consideration of this matter
the consideration of this matter
take the matter into account
He?ll take the matter into consideration
????????
We need to consider this matter
??????????
consider this matter
take the matter into consideration
Learning Paraphrases from Graphs
? Kok and Brockett, 2010 
?Basic idea: 
? Convert aligned phrases into a graph, extract paraphrases 
based on random walks and hitting times
26
?should take the matter into consideration?
??????????
take the matter into consideration
Kok and Brockett (2010) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
take the matter into account
consider this matter
take the matter into account
He?ll take the matter into consideration
????????
We need to consider this matter
??????????
consider this matter
take the matter into consideration
Extracting Paraphrase Patterns
? Zhao et al, 2008
?Basic idea: 
? Generate paraphrase patterns that include part-of-speech 
slots.
?Paraphrase probability:
2 1 1 2
1
( | ) exp[ ( , , )]
N
i i
c i
score e e h e e c?
=
=? ?
1 1 2 1
2 1 2 2
3 1 2 1
4 1 2 2
( , , ) ( | )
( , , ) ( | )
( , , ) ( | )
( , , ) ( | )
MLE
MLE
LW
LW
h e e c score c e
h e e c score e c
h e e c score c e
h e e c score e c
=
=
=
=
Based on maximum 
likelihood estimation
Based on lexical weighting
27
take
demand into
take market   demand into       consideration
Inducing English patterns Inducing Chinese patterns
Example
market consideration
take into considerationNN NN
take into considerationNN
?
?? ?? ??
take NN into        consideration
??
?
? NN
consider NN?
?? ? NN
Extract paraphrase patterns
take NN into consideration & consider NN
?should take the matter into consideration?
??????????
take [NN] into consideration
take [NN] into account
Zhao et al(2008) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
H ?ll t k th tt i t id ti
take [NN] into consideration
the consideration of [NN]
the consideration of [NN]
take [NN] into account
e  a e e ma er n o cons era on
????????
We need to consider this matter
??????????
consider [NN]
take [NN] into consideration
28
Pros and Cons
? Pros
?The method proves effective hence it?s widely used   ,    
? High precision
? Large scale
? Cons
?Language limitation
? Cannot work where the large-scale bilingual parallel corpora 
are not available
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
29
Method Overview
? Corpus
?Large corpus of web documents    
?Or directly based on web mining
? Assumption
?Distributional hypothesis
? If two words / phrases / patterns often occur in similar 
contexts, their meanings tend to be similar
Example
Shakespeare
Chekhov Merchant of Venice 
War and Peace
X wrote    Y
Maupassant
Hugo Gorky
Tagore
Murakami Tolstoy
Yasunari
Notre Dame de Paris 
   
Romeo and Juliet 
Madame Bovary 
Madame Bovary 
similar similarparaphrases
X is the author of    Y
Shakespeare
Maupassant
Hugo
Gorky
Hemingway
Balzac
Merchant of Venice 
Notre Dame de Paris 
The Old Man and Sea 
Romeo and Juliet 
30
Extracting Lexical Paraphrases (Word Clustering)
? Lin, 1998
?Basic idea 
? Measure words? similarity based on the distributional pattern 
of words
?Corpus
? A (dependency) parsed corpus
?Word similarity
Mutual 
information
1 2
1 2
1 2( , ) ( ) ( )
1 2
1 2( , ) ( ) ( , ) ( )
( ( , , ) ( , , ))
( , )
( , , ) ( , , )
r r
r r
r w T w T w
r w T w r w T w
I w r w I w r w
sim w w
I w r w I w r w
? ?
? ?
+= +
?
? ?
Extracting Syntactic Paraphrase Patterns
? Lin and Pantel, 2001
? Basic idea: extended distributional hypothesis
? Corpus: a large corpus of parsed monolingual sentences
? pattern pairs
X
solves
Y
X
finds
solution
toa
? Pattern similarity
1 2 1 2 1 2( , ) ( , ) ( , )sim p p sim SlotX SlotX sim SlotY SlotY= ?
Y
Similarity of the slot fillers
31
Extracting Surface Paraphrases
? Bhagat and Ravichandran, 2008
?Basic idea is the same as the above work        
?Corpus:
? a large corpus of monolingual sentences without parsing 
?150GB, 25 billion words
?Surface paraphrases
? Pairs of n-grams
E g ?X acquired Y? and ?X completed the acquisition of Y?? . .,        
?Techniques
? Apply locality sensitive hashing (LSH) to speed up the 
computation
Learning Unary Paraphrase Patterns
? Szpector and Dagan, 2008
?Binary paraphrase patterns (most of the previous work)       
? Each pattern has two slots at both ends
?E.g., ?X solves Y? and ?X found a solution to Y?
?Unary paraphrase patterns
? Each pattern has a single slot
?E.g., ?X take a nap? and ?X sleep?
?Method
sleep
kids
in
room
? The same with the above works
?Based on distributional hypothesis
the
X sleep
32
Extracting Paraphrases based on Web Mining
? Ravichandran and Hovy, 2002 
?Basic idea
? Learn paraphrase patterns with search engines
?Corpus
? The whole internet
?Method
? Extract paraphrase patterns for each type, e.g., ?BIRTHDAY?
? Provide hand-crafted seeds, e.g., ?Mozart, 1756? 
? Retrieve sentences containing the seeds from the web with a 
search engine
? Extract patterns, e.g.,
?born in <ANSWER> , <NAME>
?<NAME> was born on <ANSWER> ,
???
Pros and Cons
? Pros
?Language independent 
? Cons
?For methods based on large web corpora
? Computation complexity is high
?Needs to process an extremely large corpus
?Needs to compute pairwise similarity for all candidates
?For methods based on web mining
? Extract paraphrase patterns type by type
? Needs to prepare seeds beforehand
33
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
Paraphrasing with Search Engine Query Logs
? Zhao et al, 2010
?Corpus
? Query logs (queries and titles) of a search engine
?Assumption
? If a query q hits a title t, then q and t are likely to be 
paraphrases
? If queries q1 and q2 hit the same title t, then q1 and q2 are 
likely to be paraphrases
? If a query q hits titles t1 and t2, then t1 and t2 are likely to be 
paraphrases
34
Example
???????
???????
q1
t1
???????q2
Paraphrases:
<q1, t1>
<q1, t2>
< 2 t1>
query-title
???????
??
??
t2
q , 
<q1,q2>
<t1,t2>
query-query
title-title
Method
? Step-1: extracting <q, t> paraphrases
? Extracting candidate <q, t> pairs from query logs
? Paraphrase validation based on binary classification
? Combining multiple features
? Step-2: extracting <q, q> paraphrases
? Extracting candidate <q, q> from <q, t> paraphrases
? Paraphrase validation based on binary classification
? Step-3: extracting <t, t> paraphrases 
? Extracting candidate <t, t> from <q, t> paraphrases
? Paraphrase validation based on binary classification
35
Pros and Cons
? Pros
?No scale limitation  
? Query logs keep growing
? A large volume of paraphrases can be extracted
?Query logs reflect web users? real needs
? Cons
?Query logs data are only available in IR companies
?User queries are noisy
? Spelling mistakes, grammatical errors?
Extracting Paraphrases from Dictionary Glosses
? Corpus
?Glosses of dictionaries  
? Assumption 
?A word and its definition (gloss) in the dictionary have 
the same meaning
36
Example (Encarta Dictionary) 
hurricane
severe storm
high wind
fast and force person or thing
Method 
? Prune and reformulate the definitions
?For a verb v extracts the head of the definition (h)   ,        
and h?s adverb modifier m as v?s paraphrase
? Kaji et al, 2002
?Rule based method for extracting the appropriate part 
from the definition
? Higashinaka and Nagao, 2002
? E g w should not be in def; ignore contents in parentheses. .,          
in def; avoid double negation?
37
Pros and Cons
? Pros
?Explain unfamiliar words with simpler definitions     
? Cons
?Transformation of person, number, tense
president head of company
presidents
heads of company
head of companies
E.g.,
  
heads of companies
References 
? From monolingual parallel corpora
? Barzilay and McKeown. 2001. Extracting Paraphrases from a Parallel 
Corpus.
? From monolingual comparable corpora
? Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles.
? Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An 
Unsupervised Approach Using Multiple-Sequence Alignment.
? Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised 
Construction of Large Paraphrase Corpora: Exploiting Massively       
Parallel News Sources.
38
References (cont?)
? From bilingual parallel corpora
? Takao et al 2002. Comparing and Extracting Paraphrasing Words with 
2-Way Bilingual Dictionaries.
? Bannard and Callison-Burch. 2005. Paraphrasing with Bilingual Parallel 
Corpora.
? Callison-Burch. 2008. Syntactic Constraints on Paraphrases Extracted 
from Parallel Corpora.
? Kok and Brockett. 2010. Hitting the Right Paraphrases in Good Time.
? Zhao et al 2008. Pivot Approach for Extracting Paraphrase Patterns 
from bilingual corpora  .
References (cont?)
? From large web corpora
? Lin. 1998. Automatic Retrieval and Clustering of Similar Words.
? Lin and Pantel. 2001. Discovery of Inference Rules for Question 
Answering.
? Bhagat and Ravichandran. 2008. Large Scale Acquisition of 
Paraphrases for Learning Surface Patterns. 
? Szpector and Dagan. 2008. Learning Entailment Rules for Unary 
Templates. 
? Ravichandran and Hovy. 2002. Learning Surface Text Patterns for a 
Question Answering System  .
39
References (cont?)
? From other resources
? Zhao et al 2010. Paraphrasing with Search Engine Query Logs. 
? Kaji et al 2002. Verb Paraphrase based on Case Frame Alignment. 
? Higashinaka and Nagao. 2002. Interactive Paraphrasing Based on 
Linguistic Annotation.
C ffee B eak!o r
40
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
Rule based Method
? Two types:
?Based on hand crafted rules  -  
? Widely used in early studies of paraphrase generation
? McKeown, 1979; Zong et al, 2001; Tetsuro et al, 2001; 
Zhang and Yamamoto, 2002?? 
?Based on automatically extracted rules
? Extract paraphrase patterns from corpora
? Barzilay and Lee 2003 Zhao et al2009  , ,   ., ??
41
Based on Hand-crafted Rules
Sentence analysis
- morphological
- syntactic 
- semantic
- ?
Rule matching 
&
Paraphrase 
generation
S T
Paraphrase 
rule base
Compile 
paraphrase 
rules
? Examples of paraphrase rules
? Change the positions of adverbials
Based on Hand-crafted Rules
    
? He booked a single room in Beijing yesterday. =>
? Yesterday, he booked a single room in Beijing.
? Split a compound sentence into a group of simple sentences
? He booked a single room in Beijing yesterday =>
? He booked a single room in Beijing.
? He booked a single room yesterday.
? He booked a room.
? Rewrite a sentence using hand-crafted patterns
? Can I have a cup of tea? =>
? May I have a cup of tea?
? I would like a cup of tea, please.
? Give me a cup of tea.
42
Based on Automatically Extracted Rules
? Studies on paraphrase patterns extraction has been 
introduced above
? Some of them have tried to apply the extracted 
paraphrase patterns in paraphrase generation
? Complex paraphrase patterns
? Barzilay and Lee, 2003
? E.g., 
? Short and simple paraphrase patterns
? Zhao et al, 2009
? E.g., consider [NN] and take [NN] into consideration
Pros and Cons
? Methods based on hand-crafted rules
? Pros
? Can design paraphrase rules for specific applications and 
requirements
? Cons
? It is time-consuming to construct paraphrase rules
? Problem of rules conflict 
? Coverage of paraphrase rules is limited
? Methods based on automatically extracted rules
P? ros
? Can generate paraphrases with structural changes
? Cons
? Coverage of paraphrase rules is limited
43
References 
? McKeown. 1979. Paraphrasing Using Given and New Information in a 
Question-Answer System.
? Z t l 2001 A h t S k Chi P h i B dong e  a . . pproac  o po en nese arap ras ng ase  on 
Feature Extraction.
? Tetsuro et al. 2001. KURA: A Transfer-Based Lexico-Structural 
Paraphrasing Engine.
? Zhang and Yamamoto. 2002. Paraphrasing of Chinese Utterances.
? Barzilay and Lee. 2003. Learning to Paraphrase - An Unsupervised 
Approach Using Multiple-Sequence Alignment.
? Zhao et al 2009. Application-driven Statistic Paraphrase Generation.       
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method 
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
44
Thesaurus based Method
? Also known as lexical substitution
?Substitute words in a sentence with their synonyms        
that fit in the given context
?SemEval-2007: English lexical substitution task
?SemEval-2010: Cross-lingual lexical substitution
?Example:
? There will be major cuts in the salaries of high level civil        -   
servants. 
? There will be major cuts in the wages of high-level civil 
servants.
Thesaurus based Method
? Include two stages
?Stage 1: extract candidate substitutes from-     
predefined inventories.
? E.g., WordNet
?Stage-2: find substitutes that fit in the given context
? Using language model or web data (e.g., Google 5-gram) for 
evaluating the fitness in the context
? Disambiguation may also be useful    
45
Stage-1: Candidate Extraction
? Various thesauri have been tried
?WordNet: 
? the most commonly used
?Others: 
? Encarta, Roget, Oxford American Writer?s Thesaurus?
? Extracting different information as candidates
?Synsets (all synsets vs. best synset)
?Hypernyms, similar-to, also-see?
?Words in glosses
Example:
WordNet 
different
synsets
46
Example:
Encarta
definition of the synset   
synset
Stage-2: Substitute Selection
? Rank the candidates and select the one fits best 
in the given context   
? Context constraints
?Semantic constraints
? Select substitutes with the correct meaning wrt the given 
context
?Syntactic constraints
? The sentence generated after substitution should keep 
grammatical
47
SubFinder: A Lexical Substitution System
? SubFinder
?University of North Texas    
?Performs well in SemEval-2007 English lexical 
substitution task 
? Candidate extraction
?WordNet
?Encarta
?Others
? Prove to be useless 
SubFinder: A Lexical Substitution System
? Substitute selection (5 ranking methods R1~R5)
?Language model 
? Google 1T 5-gram (R1)
? Query search engine (R2)
?Latent semantic analysis (LSA) (R3)
? Rank a candidate by its relatedness to the context sentence
?Word sense disambiguation (WSD) (R4)
? Disambiguate the target word and select the synset of the 
right sense
?Pivot approach (R5)
? Check whether a candidate substitute can be generated via a 
2-way translation
48
SubFinder: A Lexical Substitution System
? Combine R1~R5:
?Voting mechanism 
?Contribution of each ranking method is not analyzed/
1
( )
i
i m m
m rankings c
score c
r
?
?
= ?
Ranks according 
to R1-R5
       
Pros and Cons
? Pros
?Based on existing inventories   
? Cons
?Cannot generate structural paraphrases
?Language limitation
? Question
H t diff t th i?? ow o merge eren  esaur
? Thesauri have different forms of synset clustering 
49
References 
? McCarthy and Navigli. 2007. SemEval-2007 Task 10: English Lexical 
Substitution Task.
? H t l 2007 UNT S bFi d C bi i K l d S fassan e  a . . : u n er: om n ng now e ge ources or 
Automatic Lexical Substitution.
? Yuret. 2007. KU: Word Sense Disambiguation by Substitution.
? Giuliano et al 2007. FBK-irst: Lexical Substitution Task Exploiting Domain 
and Syntagmatic Coherence.
? Martinez et al 2007. MELB-MKB: Lexical Substitution System based on 
Relatives in Context.
? Kauchak and Barzilay. 2006. Paraphrasing for Automatic Evaluation.       
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method 
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
50
Overview 
? Two steps
?(1) analysis and (2) generation     
NLU NLG
R
s t
paraphrases
NLG based Methods
? Kozlowski et al, 2003
?Generate single sentence paraphrases -  
?Input: predicate/argument structure
? Not natural language sentences/
?Based on lexico-grammatical resources
? Map elementary semantic structures with syntactic realization
51
NLG based Methods (cont?)
? Power and Scott, 2005
? Concerning larger-scale
Rhetorical 
structure tree 
paraphrases
? Paraphrases of multiple 
sentences or even the 
whole text
? Paraphrases vary not only 
at lexical and syntactic 
levels, but also in 
document structure and
generator
Different realizations
   
layout
? Problem:
? The input is not natural 
language texts/
t1 t2 t3 tn
NLG based Methods (cont?)
? Power and Scott, 2005 (cont?)
?Example: 
reason
NUCLEUS: recommend(doctors, elixir)
SATELLITE: conjunction
1: quick-results(elixir)
2: few-side-effects(elixir)
Rhetorical 
structure tree
Doctors recommend Elixir since it 
gives quick results and it has few 
side effects.
solution1
(1) Elixir gives quick results.
(2) Elixir has few side effects.
(3) Therefore, it is recommended 
by doctors.
solution2
52
NLG based Methods (cont?)
? Fujita et al, 2005
?Paraphrase light verb constructions (LVC) in -    
sentences
? LVC: consists of a light-verb that syntactically governs a 
deverbal noun
?Semantic representation
? LCS: Lexical Conceptual Structure
?Procedure
? Semantic analysis
? Semantic transformation
? Surface generation
Pros and Cons
? Pros 
?It simulates human being?s behavior when generating       
paraphrases:
? Step-1: understand the meaning of a sentence
? Step-2: generate a new sentence expressing the meaning 
? Cons 
?Both deep analysis of sentences and NLG are difficult 
to realize
53
References 
? Kozlowski et al 2003. Generation of single-sentence paraphrases from 
predicate/argument structure using lexico-grammatical resources. 
? P d S tt 2005 A t ti ti f l l hower an  co . . u oma c genera on o  arge-sca e parap rases. 
? Fujita et al 2005. Exploiting Lexical Conceptual Structure for Paraphrase 
Generation.
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method 
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
54
Machine Translation vs. Paraphrase Generation
Translations t
Language L1 Language L2
Paraphrasings
Language L1
t
For both machine translation and paraphrase generation:
(1) t should preserve the meaning of s
(2) t should be a fluent sentence 
Paraphrase Generation as Machine Translation
? Quirk et al, 2004
?First recast paraphrase generation as a monolingual       
machine translation task
Paraphrase 
generations t
A typical MT model 
(source channel model)
PT
paraphrase table 
From comparable 
news articles
55
Paraphrase Generation as Machine Translation 
(cont?)
? Model
?Source channel model  
* arg max ( | )
arg max ( | ) ( )
t
t
t p t s
p s t p t
=
=
Language model
?Translation? model
(based on a phrasal 
paraphrase table)
? Paraphrase table
?Monolingual parallel sentences
Paraphrase Generation as Machine Translation 
(cont?)
  
? Extracted from comparable news articles
? 139K pairs
?Word alignment & phrase pair extraction
? With Giza++
? Limitation
?Lack of monolingual parallel corpora to train the 
paraphrase table!!!
56
? Zhao et al, 2008
?Combine multiple resources to train the paraphrase
Paraphrase Generation as Machine Translation 
(cont?)
       
table
Paraphrase 
generations t
Log-linear model
PT1
Multiple paraphrase tables 
PT2 PTn?
From various resources
Paraphrase Generation as Machine Translation 
(cont?)
? Model
?Log linear model-  
_ _
1
* arg max{ ( , ) ( , )}
N
TM i TM i LM LMt
i
t h t s h t s? ?
=
= +?
N paraphrase tables, each feature 
corresponds to a paraphrase table
Language model
_
1
( , ) log ( , )
iK
TM i i k k
k
h t s score t s
=
= ?
57
Paraphrase Generation as Machine Translation 
(cont?)
? Paraphrase tables
? PT1: from word clusters
? Volumes of the PTs:
    
(Lin, 1998)
? PT2: from monolingual 
parallel corpora
? PT3: from monolingual 
comparable corpora
? PT4: from bilingual parallel 
corpora
? PT5: from Encarta 
dictionary glosses
? PT6: from clusters of 
similar user queries
Proves most useful!
? Differences between machine translation and 
paraphrase generation (Zhao et al2009):
Paraphrase Generation vs. Machine Translation
    ., 
MT has a unique purpose PG has distinct purposes in different applications
Machine Translation (MT) Paraphrase Generation (PG)
In MT, all words in a sentence 
h ld b t l t d
In PG, not all words need to be 
h ds ou  e rans a e parap rase
In MT, the bilingual parallel data
are easy to collect
In PG, multiple resources need 
to be combined
In MT, automatic evaluation 
metrics (e.g., BLEU) are available
In PG, automatic evaluation 
metrics are not available
58
Application-driven Statistical Paraphrase 
Generation 
? Zhao et al, 2009
?Propose a statistical model for paraphrase generation      
?Generate different paraphrases in different applications
Paraphrase 
plannings t
Sentence 
preprocessing
Paraphrase 
generation
A The given application
PT1
Multiple paraphrase tables 
PT2 PTn?
Also combine 
multiple resources
Self-paraphrase PT: 
allows words to 
keep unchanged in  
paraphrasing
? Paraphrase planning
?When an application A is given only the paraphrase
Application-driven Statistical Paraphrase 
Generation (cont?)
    ,    
pairs that can achieve A are kept
Paraphrase application: sentence compression
The US government should take the overall situation into consideration and actively promote bilateral high-tech trades.
Example:
The US government
The US administration
The US government on
overall situation 
overall interest
overall picture
overview
situation as a whole
whole situation
??
take [NN_1] into consideration  
consider [NN_1]
take into account [NN_1]
take account of [NN_1]
take [NN_1] into account
take into consideration [NN_1] 
??
<promote, OBJ, trades>  
<sanction, OBJ, trades>
<stimulate, OBJ, trades>
<strengthen, OBJ, trades>
<support, OBJ, trades>
<sustain, OBJ, trades>
59
? Model:
?Log linear model
Application-driven Statistical Paraphrase 
Generation (cont?)
-  
1
2 1
1
( | ) ( log ( , ))
log ( | )
i i
i
K
k k k k
k k
J
lm j j j
j
p s t
p t t t
? ?
?
=
? ?
=
=
+
? ?
?
t s Paraphrase model
Language model
1
( , )
I
um i i
i
s t? ?
=
+ ? Usability model 
(defined for each 
application)
References 
? Lin. 1998. Automatic Retrieval and Clustering of Similar Words.
? Quirk et al 2004. Monolingual Machine Translation for Paraphrase 
G tienera on.
? Finch et al 2004. Paraphrasing as Machine Translation.
? Zhao et al 2008. Combining Multiple Resources to Improve SMT-based 
Paraphrasing Model.
? Zhao et al 2009. Application-driven Statistical Paraphrase Generation.
60
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method
A li ti f P h? pp ca ons o  arap rases
?Evaluation of Paraphrases
?Conclusions and Future work
Overview 
? Basic idea
?We can generate a paraphrase t for a sentence s by         
translating s into  a foreign language, and then 
translating it back into the source language.
s t Source language 
MT1
p
MT2
Pivot language 
MT engines
61
? Example:
Overview (cont?)
What toxins are most hazardous to expectant mothers?English
? Single-pivot
      
Che tossine sono pi? pericolose alle donne incinte?
 
Italian 
What toxins are more dangerous to pregnant women?English 
?Using a single pivot language
? Multi-pivot
?Using multiple pivot languages 
Pivot based Methods
? Duboue and Chu-Carroll, 2006
?Applied in QA systems   
? Paraphrase the input questions so as to improve the 
coverage in answer extraction
?Pivot languages
? 11
?MT engines
? 2: Babelfish (B) and Google MT (G)      
? 4 combinations: B+B, B+G, G+G, G+B
62
Pivot based Methods (cont?)
? Duboue and Chu-Carroll, 2006 (cont?)
?Given a list of automatically generated paraphrases      , 
we need to select a best one.
? For QA, we need to select the paraphrase that can find the 
answer more easily than the original question.
Features for paraphrase selection (in a classification framework)
SUM IDF The sum of the IDF scores for all terms in the original question and the 
h ( f h ith i f ti t )parap rase. pre er parap rases w  more n orma ve erms
Lengths Number of query terms for each of the paraphrase and the original 
question. (prefer shorter paraphrases)
Cosine 
Distance
The distance between the vectors of both questions, IDF-weighted. 
(filter paraphrases that diverge too much from the original)
Answer 
Types
Whether answer types, as predicted by the question analyzer, are the 
same or overlap. (the answer type should be the same)
Pivot based Methods (cont?)
? Max, 2009
?Paraphrasing sub sentential fragments -  
? Allows the exploitation of context during both source-pivot 
translation and pivot-source back-translation
context constraints context 
constraints
paraphrase
63
Pivot based Methods (cont?)
? Max, 2009 (cont?)
?Application
? Text revision
?Pivot language
? English 
?Paraphrases are acquired for French sub-sentences
?MT engine
? S t t SMT (St t l 2007)ource con ex  aware  roppa e  a ., 
Pivot based Methods (cont?)
? Zhao et al, 2010
3 MT engines: (1) Google 
translator (GG), (2) Microsoft 
translator (MS), (3) Systran 
translator (ST)
6 pivot languages: (1) 
French (F) (2) German (G) ,   , 
(3) Spanish (S), (4) Italian (I), 
(5) Portuguese (P), (6) 
Chinese (C)
54 combinations
64
Pivot based Methods (cont?)
? Zhao et al, 2010 (cont?)
?Produce a high quality paraphrase using the list of  -       
candidates
Source he said there will be major cuts in the salaries of high-level civil servants
(GG, G, MS) he said there are significant cuts in the salaries of high-level officials
(GG, F, GG) he said there will be significant cuts in the salaries of top civil level
(GG, P, GG) he said there will be big cuts in salaries of high-level civil
(MS, C, MS) he said that there will be a major senior civil service pay cut             
(MS, S, GG) he said there will be significant cuts in the salaries of senior officials
(MS, F, ST) he said there will be great cuts in the wages of the high level civils servant
(ST, G, GG) he said that there are major cuts in the salaries of senior government officials
?? ??
Good paraphrases Bad paraphrases
? Zhao et al, 2010 (cont?)
?Two techniques for producing high quality
Pivot based Methods (cont?)
    -  
paraphrases using the candidates
? Selection-based technique
?Select a best paraphrase from the 54 candidates based on 
Minimum Bayes Risk (MBR)
? Decoding-based technique
?Train a MT model using the 54 candidates, and generates a 
h i h inew parap rase w t  t
65
References 
? Duboue and Chu-Carroll. 2006. Answering the Question You Wish They 
Had Asked: The Impact of Paraphrasing for Question Answering.
? St t l 2007 E l iti S Si il it f SMT i C t troppa e  a . . xp o ng ource m ar y or  us ng on ex -
informed Features.
? Max. 2009. Sub-sentential Paraphrasing by Contextual Pivot Translation.
? Zhao et al 2010. Leveraging Multiple MT Engines for Paraphrase 
Generation.
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
66
Paraphrasing for MT
? Applications:
?Translate unknown terms (phrases)   
?Expand training data
?Rewrite input sentences
?Improve automatic evaluation
?Tune parameters
Translate Unknown Terms (Phrases)
? Basic idea:
?In SMT when encountering an unknown source term ,       
(phrase), we can substitute a paraphrase for it and 
then proceed using the translation of that paraphrase
f1 -> f1?
f2 -> f2?
paraphrase table
f1 -> e1
f2 -> e2
SMT phrase table
new phrase pair  
?
fi -> fj
?
fm -> fm?
  
?
fj -> ej
?
fn -> en
unknown phrase
fi
fi -> ej
67
Translate Unknown Terms (Phrases) (cont?)
? Callison-Burch et al, 2006
?Paraphrases are extracted from bilingual parallel      
corpora using the pivot approach
?New phrase pairs generated through paraphrasing 
are incorporated into the phrase table
? The paraphrase probability is added as a new feature 
function:
paraphrase 
2 1 1
1 2
( | ) If phrase table entry ( ,  )
( , ) is generated from ( ,  )
1 Otherwise
p f f e f
h e f e f
??= ???
probability
Translate Unknown Terms (Phrases) (cont?)
? Marton et al, 2009
?Paraphrases are extracted from monolingual corpora     , 
based on distributional hypothesis
f
Unknown phrase
L1__R1
L2__R2
?
contexts paraphrase phrases
f1
f2
?
?Combine the new phrase pairs in the phrase table
1 2 1
1 2
( , ) If phrase table entry ( ,  )
( , ) is generated from ( ,  )
1 Otherwise
f fpsim DP DP e f
h e f e f
??= ???
Context 
similarity
68
? Mirkin et al, 2009
?Use not only paraphrases but also entailment rules
Translate Unknown Terms (Phrases) (cont?)
       
? From WordNet
?Paraphrases: synonyms in WordNet
?Entailment rules: hypernyms in WordNet
paraphrase 
generation
paraphrase 
selection
s
generated 
para. list top-k para.
SMT t
top-n 
tran. translation 
selection
WordNet
synonyms
hypernyms
context 
model
language 
model
? Onishi et al, 2010
?Using paraphrase lattices for SMT
Translate Unknown Terms (Phrases) (cont?)
    
? Step-1: Paraphrase the input sentence, and generate a 
paraphrase lattice
?Paraphrases are extracted from bilingual parallel corpora based 
on the pivot approach
? Step-2: Give the paraphrase lattice as the input to the lattice 
decoder
69
? Effectiveness 
?When the training data of SMT is small
Translate Unknown Terms (Phrases) (cont?)
       
? Effective?
?Problem of unknown terms is more serious when the training 
data is small
?When the training data of SMT is large
? Ineffective/
?Unknown terms can be covered by adding more training data
Expand Training Data
? Enlarge training data via paraphrasing the 
source-side sentences in the parallel corpus     
Original training data
e1
e2
?
en
f1
f2
?
fn
English Foreign 
expanded training data
e1
e2
?
f1
f2
?
f
English Foreign 
e1?
e2?
?
en?
paraphrasing
en
e1?
e2?
?
en?
n
f1
f2
?
fn
70
Rewrite Input Sentences
? Paraphrase the sentence to be translated, so as 
to make it more translatable    
?Yamamoto, 2002; Zhang and Yamamoto, 2002 
? Rule-based Paraphraser for simplifying the source sentences 
?Shimohata et al, 2004
? Shorten long sentences and sentences with redundant 
information in a speech translation system
Improve Automatic Evaluation
? Automatic evaluation of MT
?Based on counting the overlaps between the       
references and machine outputs
? E.g., BLEU, NIST?
?Only computing the surface similarity is limited
? A meaning may be expressed in a way that is not included in 
the references 
?Human references are expensive to produce     
?Solution: paraphrase the references so as to include 
as many correct expressions as possible!
71
Improve Automatic Evaluation (cont?)
? Kauchak and Barzilay, 2006
?Find a paraphrase of the reference that is closer in          
wording to the system output 
? Extract candidates from WordNet synonyms
It is hard to believe that such tremendous changes have taken place for those people 
and lands that I have never stopped missing while living abroad.
For someone born here but has been sentimentally attached to a foreign country far 
Correct Wrong 
Reference
System 
output
?Filter the invalid substitution given the context
? Binary classification
?Features: context n-grams and local collocations
from home, it is difficult to believe this kind of changes.
Improve Automatic Evaluation (cont?)
? Zhou et al, 2006
?ParaEval: Compute the similarity of reference and       
system output using paraphrases
? Paraphrases are learned from bilingual parallel corpora with 
a pivot approach
?Two-tier matching strategy for SMT evaluation
? First tier: paraphrase match
? Second tier: unigram match for words not matched by         
paraphrases
72
Tune Parameters
? Madnani et al 2007
?Similar to the studies using paraphrases to improve        
automatic evaluation of MT
?Parameter tuning in SMT also needs references
? Parameter estimation of SMT: 
?optimize BLEU on a development set
?Expand the references automatically via paraphrasing
? P h tiarap rase genera on 
?Paraphrase resources are acquired based on a pivot approach
?Recast paraphrase generation as a monolingual MT problem 
and decode with a typical SMT decoder
References 
? Translate unknown terms (phrases)
? Callison-Burch et al 2006. Improved Statistical Machine Translation 
Using Paraphrases.
? Marton et al 2009. Improved Statistical Machine Translation Using 
Monolingually-Derived Paraphrases.
? Mirkin et al 2009. Source-Language Entailment Modeling for 
Translating Unknown Terms.
? Onishi et al 2010. Paraphrase Lattice for Statistical Machine 
Translation.
? Expand training data  
? Nakov. 2008. Improved Statistical Machine Translation Using 
Monolingual Paraphrases.
? Bond et al 2008. Improving Statistical Machine Translation by 
Paraphrasing the Training Data.
73
References (cont?)
? Rewrite input sentences
? Yamamoto. 2002. Machine Translation by Interaction between 
Paraphraser and Transfer.
? Zhang and Yamamoto. 2002. Paraphrasing of Chinese Utterances.
? Shimohata et al 2004. Building a Paraphrase Corpus for Speech 
Translation.
? Improve automatic evaluation
? Kauchak and Barzilay. 2006. Paraphrasing for Automatic Evaluation.
? Zhou et al 2006. Re-evaluating Machine Translation Results with 
P h S tarap rase uppor . 
? Tune parameters
? Madnani et al 2007. Using Paraphrases for Parameter Tuning in 
Statistical Machine Translation.
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
74
Paraphrasing for QA
? Goal:
?Alleviate the problem of word mismatch between      
questions and answers
? Two directions:
?Paraphrase questions 
? Rewrite a question into a group of paraphrases, so as to 
improve the coverage in answer extraction
?Paraphrase answer extraction patterns
? Generate answer extraction patterns as many as possible 
Paraphrasing for QA
? Ravichandran and Hovy, 2002.
?Mining paraphrase patterns from the web     
? Using hand-crafted seeds (e.g., (Mozart, 1756) for BIRTHDAY)
? Mining patterns containing the seeds
Question taxonomy 
BIRTHDAY
1.00  <NAME> ( <ANSWER> - )
0.85  <NAME> was born on <ANSWER>,
0.60 <NAME> was born in <ANSWER>
scores Paraphrase
patterns
    
0.59  <NAME> was born <ANSWER>
0.53  <ANSWER> <NAME> was born
0.50  ? <NAME> ( <ANSWER>
0.36  <NAME> ( <ANSWER> -
Given seed (Mozart, 1756)
75
Paraphrasing for Summarization
? Improve automatic evaluation of summaries
?Zhou et al2006  ., 
?Similar to the automatic evaluation of MT
? Measure the similarity between references and system 
outputs using paraphrase match as well as exact match
? Improve sentence clustering
?Barzilay et al, 1999
?Considering paraphrase match when Computing 
sentence similarity
Other Applications
? Paraphrasing for NLG
?Text revision and transformation   
? Dras, 1997
?Text transformation in order to meet external constraints, such 
as length and readability
? Paraphrasing for IR
?Query rewriting 
? Z k d R k tti 2002u erman an  as u . .
?Paraphrase user queries with WordNet synonyms
76
Other Applications (cont?)
? Writing style transformation
?Kaji et al2004  ., 
? Paraphrasing predicates from written language to spoken 
language
? Text simplification
?Carroll et al 1999
? Simplifying texts for language-impaired readers or non-native 
kspea ers
? Identify plagiarism
?Uzuner et al 2005
? Using paraphrases to better identify plagiarism
References 
? Paraphrasing for QA
? Ravichandran and Hovy. 2002. Learning Surface Text Patterns for a 
Question Answering System.
? Duboue and Chu-Carroll. 2006. Answering the Question You Wish They 
Had Asked: The Impact of Paraphrasing for Question Answering.
? Paraphrasing for summarization
? Barzilay et al 1999. Information Fusion in the Context of Multi-
Document Summarization.
? Zhou et al 2006. ParaEval: Using Paraphrases to Evaluate Summaries 
Automatically.
? Paraphrasing for NLG
? Dras. 1997. Reluctant Paraphrase: Textual Restructuring under an 
Optimisation Model.
77
References (cont?) 
? Paraphrasing for IR
? Zukerman and Raskutti. 2002. Lexical Query Paraphrasing for 
Document Retrieval.
? Writing style transformation 
? Kaji et al 2004. Paraphrasing Predicates from Written Language to 
Spoken Language Using the Web.
? Text simplification
? Carroll et al 1999. Simplifying Text for Language-Impaired Readers.
? Identify plagiarism 
? Uzuner et al 2005. Using Syntactic Information to Identify Plagiarism.
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
78
Evaluation of Paraphrases 
? No widely accepted evaluation criteria/
?Problem 1: Researchers define various evaluation-     
methods in their studies
? Difficult to make a direct comparison among different works
?Problem-2: Human evaluation is commonly used
? Human evaluation is rather subjective
? Difficult to replicate 
Evaluation of Paraphrase Identification
? Human evaluation
? A tomatic e al ationu  v u
?Brockett and Dolan, 2005
?Alignment Error Rate (AER)
? AER is indicative of how far the corpus is from providing a 
solution under a standard SMT tool
| | | |A P A S? + ?
| |
AER
A S
= +
Automatic 
alignment
POSSIBLE + SURE 
alignment in the gold 
standard
SURE alignment in 
the gold standard
79
Evaluation of Lexical Substitution
? Automatic evaluation
?McCarthy and Navigli 2007  , 
?Construction of gold standard data
? Five annotators, who are native speakers
? For each test word, each annotator provides up to three 
substitutes
?Evaluation:
? Precision and Recall  
Evaluation of Paraphrase Phrases
? Human evaluation
?Ask judges: 
? Whether paraphrases were approximately conceptual 
equivalent
? Whether the paraphrases were roughly interchangeable 
given the genre 
? Whether the substitutions preserved the meaning and 
remained grammatical 
? ??
?The criteria above are vaguely defined and not easy 
to reproduce
80
Evaluation of Paraphrase Phrases (cont?)
? Automatic evaluation
?Callison Burch et al2008-   ., 
?Data:
? Parallel sentences, in which paraphrases are annotated 
through manual alignment (gold standard)
?Two fashions of evaluation
? Calculate how well an automatic paraphrasing technique can 
align the paraphrases in a sentence pair     
? Calculate the lower-bound precision and relative recall of 
a paraphrasing technique (which extracts paraphrases from 
other resources)
Evaluation of Paraphrase 
Phrases (cont?)
? Alignment precision and recall ? Lower-bound precision and 
relative recall
Manual alignment
1 2
1 2
Pr
1 2 1 2,
1 2,
| ( , , ) ( , , ) |
| ( , , ) |
ec
e e C
e e C
Align
PP e e S PP e e M
PP e e S
< >?
< >?
=
??
?
System alignment
 
R 1
,
Pr
| ( , ) ( , , ) |
| ( , ) |
MET EF
s G C p s MET
LB ecision
para p s para p s G
para p s< >? ?
? =
?? ?
Paraphrase acquired with 
a method MET
Paraphrase in the gold 
standard set
1 2
1 2
Re
1 2 1 2,
1 2,
| ( , , ) ( , , ) |
| ( , , ) |
call
e e C
e e C
Align
PP e e S PP e e M
PP e e M
< >?
< >?
=
??
? R 1, 1
Re Re
| ( , ) ( , , ) |
| ( , , ) |
MET EF
s G C p s REF
l call
para p s para p s G
para p s G< >? ?
? =
?? ?
81
Evaluation of Paraphrase Patterns
? Human evaluation 
?Paraphrase patterns cannot be evaluated without      
context information
? E.g., X acquire Y, X buy Y
?Correct or not? It depends on what fill in slots X and Y
? Common view:
?A pair of paraphrase patterns is considered correct if the judge 
could think of contexts under which it holds
? Problem:
?Different judges may think of totally distinct contexts, thus the 
agreement among the judges could be low
Evaluation of Paraphrase Patterns (cont?)
? Szpektor et al, 2007
?Evaluate paraphrase patterns (and entailment rules)      
with instances rather than directly evaluate patterns
? Judges are presented not only with a pair of patterns, but 
also a sample of sentences that match its left-hand side
? Judges assess whether two patterns are paraphrases under 
each specific example
? A pair of paraphrase patterns is considered as correct only 
when the percentage of correct examples is high enough
82
Evaluation of Paraphrase Generation
? Human evaluation 
?Similar to human evaluation of SMT     
?Criteria (Zhao et al, 2009, 2010)
? Adequacy: If the meaning of the source sentence is 
preserved in the paraphrase?
? Fluency: if the generated paraphrase is well-formed?
? Usability (Zhao et al, 2009): If the paraphrase meets the 
requirement of the given application?
? Paraphrase rate (Zhao et al, 2009): How different the 
paraphrase is from the source sentence?
Evaluation of Paraphrase Generation (cont?)
? Three scales for adequacy, fluency, and usability (Zhao 
et al, 2009)
Adequacy
1 The meaning is evidently changed.
2 The meaning is generally preserved.
3 The meaning is completely preserved.
Fluency
1 The paraphrase t is incomprehensible.
2 t is comprehensible.
3 t is a flawless sentence.
1 t is opposite to the application p rpose
? Five scales for adequacy and fluency (Zhao et al, 2010)
Usability
      u .
2 t does not achieve the application.
3 t achieves the application.
83
Evaluation of Paraphrase Generation (cont?)
? Paraphrase rate (Zhao et al, 2010):
?PR 1: based on word overlap rate-      
?PR-2: based on edit distance 
( , )
1( ) 1
( )
OL S T
PR T
L S
= ? Word overlap rate
Number of words 
in the source sen.
( , )
2( )
( )
ED S T
PR T
L S
= Edit distance
Evaluation of Paraphrase Generation (cont?)
? Two questions:
?Q1: Why not adopt automatic MT methods here e g      , . ., 
BLEU, NIST, TER??
? Reason-1: It is much more difficult to construct human 
references in paraphrase generation than MT
? Reason-2: Paraphrases that change less will get larger 
scores in criteria like BLEU
?Q2: How to combine the evaluation of paraphrase 
quality and paraphrase rate?
? They seem to be incompatible
84
Evaluation within Applications
? Evaluate the role of a paraphrasing module within 
a certain application system   
?E.g., in MT, examine whether a paraphrasing module 
helps to alleviate the unknown term problem
?E.g., in QA, whether paraphrasing the answer patterns 
can improve the coverage of answer extraction
? Problems:
?Whether the result can hold for a different application?
?How to evaluate the role of the paraphrase module 
independently (not influenced by other modules)?
References 
? Brockett and Dolan. 2005. Support Vector Machines for Paraphrase 
Identification.
? S kt t l 2007 I t b d E l ti f E t il t R lzpe or e  a . . ns ance- ase  va ua on o  n a men  u e 
Acquisition.
? McCarthy and Navigli. 2007. SemEval-2007 Task 10: English Lexical 
Substitution Task.
? Callison-Burch et al 2008. ParaMetric: An Automatic Evaluation Metric for 
Paraphrasing.
? Zhao et al 2009. Application-driven Statistical Paraphrase Generation.
? Zhao et al 2010. Leveraging Multiple MT Engines for Paraphrase          
Generation.
85
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
Conclusions and Future Work
? Conclusions
?Paraphrasing is important in various research areas      
?Many different kinds of corpora and data resources 
have been investigated for paraphrase extraction
?Paraphrase generation is a task similar to MT, but not 
the same
?Paraphrase evaluation is problematic. Automatic 
evaluation methods are in need
86
Conclusions and Future Work (cont?)
? Future work
?Paraphrase extraction 
? Improve the quality of the extracted paraphrases
?Paraphrase generation
? Application-driven paraphrase generation
?Paraphrase application
? Apply paraphrasing techniques in commercial NLP systems, 
rather than merely in labs    
?Paraphrase evaluation
? Come up with evaluation methods that can be widely 
accepted
Thanks!
QA
87
