Proceedings of the Second Workshop on Statistical Machine Translation, pages 193?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multi-Engine Machine Translation
with an Open-Source Decoder for Statistical Machine Translation
Yu Chen1, Andreas Eisele1,2, Christian Federmann2,
Eva Hasler3, Michael Jellinghaus1, Silke Theison1
(authors listed in alphabetical order)
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
3: University of Cologne, Germany
Abstract
We describe an architecture that allows
to combine statistical machine translation
(SMT) with rule-based machine translation
(RBMT) in a multi-engine setup. We use a
variant of standard SMT technology to align
translations from one or more RBMT sys-
tems with the source text. We incorporate
phrases extracted from these alignments into
the phrase table of the SMT system and use
the open-source decoder Moses to find good
combinations of phrases from SMT training
data with the phrases derived from RBMT.
First experiments based on this hybrid archi-
tecture achieve promising results.
1 Introduction
Recent work on statistical machine translation has
led to significant progress in coverage and quality of
translation technology, but so far, most of this work
focuses on translation into English, where relatively
simple morphological structure and abundance of
monolingual training data helped to compensate for
the relative lack of linguistic sophistication of the
underlying models. As SMT systems are trained on
massive amounts of data, they are typically quite
good at capturing implicit knowledge contained in
co-occurrence statistics, which can serve as a shal-
low replacement for the world knowledge that would
be required for the resolution of ambiguities and the
insertion of information that happens to be missing
in the source text but is required to generate well-
formed text in the target language.
Already before, decades of work went into the im-
plementation of MT systems (typically rule-based)
for frequently used language pairs1, and these sys-
tems quite often contain a wealth of linguistic
knowledge about the languages involved, such as
fairly complete mechanisms for morphological and
syntactic analysis and generation, as well as a large
number of bilingual lexical entries spanning many
application domains.
It is an interesting challenge to combine the differ-
ent types of knowledge into integrated systems that
could then exploit both explicit linguistic knowledge
contained in the rules of one or several conventional
MT system(s) and implicit knowledge that can be
extracted from large amounts of text.
The recently started EuroMatrix2 project will ex-
plore this integration of rule-based and statistical
knowledge sources, and one of the approaches to
be investigated is the combination of existing rule-
based MT systems into a multi-engine architecture.
The work described in this paper is one of the
first incarnations of such a multi-engine architec-
ture within the project, and a careful analysis of the
results will guide us in the choice of further steps
within the project.
2 Architectures for multi-engine MT
Combinations of MT systems into multi-engine ar-
chitectures have a long tradition, starting perhaps
with (Frederking and Nirenburg, 1994). Multi-
engine systems can be roughly divided into simple
1See (Hutchins et al, 2006) for a list of commercial MT
systems
2See http://www.euromatrix.net
193
Figure 1: Architecture for multi-engine MT driven
by a SMT decoder
architectures that try to select the best output from a
number of systems, but leave the individual hypothe-
ses as is (Tidhar and Ku?ssner, 2000; Akiba et al,
2001; Callison-Burch and Flournoy, 2001; Akiba et
al., 2002; Nomoto, 2004; Eisele, 2005) and more so-
phisticated setups that try to recombine the best parts
from multiple hypotheses into a new utterance that
can be better than the best of the given candidates,
as described in (Rayner and Carter, 1997; Hogan and
Frederking, 1998; Bangalore et al, 2001; Jayaraman
and Lavie, 2005; Matusov et al, 2006; Rosti et al,
2007).
Recombining multiple MT results requires find-
ing the correspondences between alternative render-
ings of a source-language expression proposed by
different MT systems. This is generally not straight-
forward, as different word order and errors in the
output can make it hard to identify the alignment.
Still, we assume that a good way to combine the var-
ious MT outcomes will need to involve word align-
ment between the MT output and the given source
text, and hence a specialized module for word align-
ment is a central component of our setup.
Additionally, a recombination system needs a way
to pick the best combination of alternative building
blocks; and when judging the quality of a particu-
lar configuration, both the plausibility of the build-
ing blocks as such and their relation to the context
need to be taken into account. The required opti-
mization process is very similar to the search in a
SMT decoder that looks for naturally sounding com-
binations of highly probable partial translations. In-
stead of implementing a special-purpose search pro-
cedure from scratch, we transform the information
contained in the MT output into a form that is suit-
able as input for an existing SMT decoder. This has
the additional advantage that resources used in stan-
dard phrase-based SMT can be flexibly combined
with the material extracted from the rule-based MT
results; the optimal combination can essentially be
reduced to the task of finding good relative weights
for the various phrase table entries.
A sketch of the overall architecture is given in
Fig. 1, where the blue (light) parts represent the
modules and data sets used in purely statistical MT,
and the red (dark) parts are the additional modules
and data sets derived from the rule-based engines. It
should be noted that this is by far not the only way
to combine systems. In particular, as this proposed
setup gives the last word to the SMT decoder, we
risk that linguistically well-formed constructs from
one of the rule-based engines will be deteriorated in
the final decoding step. Alternative architectures are
under exploration and will be described elsewhere.
3 MT systems and other knowledge
sources
For the experiments, we used a set of six rule-based
MT engines that are partly available via web inter-
faces and partly installed locally. The web based
systems are provided by Google (based on Systran
for the relevant language pairs), SDL, and ProMT
which all deliver significantly different output. Lo-
cally installed systems are OpenLogos, Lucy (a re-
cent offspring of METAL), and translate pro by lin-
genio (only for German? English). In addition to
these engines, we also used the scripts included in
the Moses toolkit (Koehn et al, 2006)3 to generate
phrase tables from the training data. We enhanced
the phrase tables with information on whether a
given pair of phrases can also be derived via a third,
intermediate language. We assume that this can be
useful to distinguish different degrees of reliability,
but due to lack of time for fine-tuning we could not
yet show that it indeed helps in increasing the overall
quality of the output.
3see http://www.statmt.org/moses/
194
4 Implementation Details
4.1 Alignment of MT output
The input text and the output text of the MT systems
was aligned by means of GIZA++ (Och and Ney,
2003), a tool with which statistical models for align-
ment of parallel texts can be trained. Since training
new models on merely short texts does not yield very
accurate results, we applied a method where text can
be aligned based on existing models that have been
trained on the Europarl Corpus (Koehn, 2005) be-
forehand. This was achieved by using a modified
version of GIZA++ that is able to load given mod-
els.
The modified version of GIZA++ is embedded
into a client-server setup. The user can send two
corresponding files to the server, and specify two
models for both translation directions from which
alignments should be generated. After generating
alignments in both directions (by running GIZA++
twice), the system also delivers a combination of
these alignments which then serves as input to the
following steps described below.
4.2 Phrase tables from MT output
We then concatenated the phrase tables from the
SMT baseline system and the phrase tables obtained
from the rule-based MT systems and augmented
them by additional columns, one for each system
used. With this additional information it is clear
which of the MT systems a phrase pair stems from,
enabling us to assign relative weights to the con-
tributions of the different systems. The optimal
weights for the different columns can then be as-
signed with the help of minimum error rate training
(Och, 2003).
5 Results
We compared the hybrid system to a purely statis-
tical baseline system as well as two rule-based sys-
tems. The only differences between the baseline sys-
tem and our hybrid system are the phrase table ? the
hybrid system includes more lexical entries than the
baseline ? and the weights obtained from minimum
error rate training.
For a statistical system, lexical coverage becomes
an obstacle ? especially when the bilingual lexical
entries are trained on documents from different do-
mains. However, due to the distinct mechanisms
used to generate these entries, rule-based systems
and statistical systems usually differ in coverage.
Our system managed to utilize lexical entries from
various sources by integrating the phrase tables de-
rived from rule-based systems into the phrase table
trained on a large parallel corpus. Table 1 shows
Systems Token #
Ref. 2091 (4.21%)
R-I 3886 (7.02%)
R-II 3508 (6.30%)
SMT 3976 (7.91%)
Hybrid 2425 (5.59%)
Table 1: Untranslated tokens (excl. numbers and
punctuations) in output for news commentary task
(de-en) from different systems
a rough estimation of the number of untranslated
words in the respective output of different systems.
The estimation was done by counting ?words? (i.e.
tokens excluding numbers and punctuations) that ap-
pear in both the source document and the outputs.
Note that, as we are investigating translations from
German to English, where the languages share a lot
of vocabulary, e.g. named entities such as ?USA?,
there are around 4.21% of words that should stay the
same throughout the translation process. In the hy-
brid system, 5.59% of the words remain unchanged,
which is is the lowest percentage among all systems.
Our baseline system (SMT in Table 1), not compris-
ing additional phrase tables, was the one to produce
the highest number of such untranslated words.
Baseline Hybrid
test 18.07 21.39
nc-test 21.17 22.86
Table 2: Performance comparison (BLEU scores)
between baseline and hybrid systems, on in-domain
(test) and out-of-domain (nc-test) test data
Higher lexical coverage leads to better perfor-
mance as can be seen in Table 2, which compares
BLEU scores of the baseline and hybrid systems,
both measured on in-domain and out-of-domain test
data. Due to time constraints these numbers reflect
195
results from using a single RBMT system (Lucy);
using more systems would potentially further im-
prove results.
6 Outlook
Due to lack of time for fine-tuning the parameters
and technical difficulties in the last days before de-
livery, the results submitted for the shared task do
not yet show the full potential of our architecture.
The architecture described here places a strong
emphasis on the statistical models and can be seen
as a variant of SMT where lexical information from
rule-based engines is used to increase lexical cover-
age. We are currently also exploring setups where
statistical alignments are fed into a rule-based sys-
tem, which has the advantage that well-formed syn-
tactic structures generated via linguistic rules can-
not be broken apart by the SMT components. But
as rule-based systems typically lack mechanisms for
ruling out implausible results, they cannot easily
cope with errors that creep into the lexicon due to
misalignments and similar problems.
7 Acknowledgements
This research has been supported by the European
Commission in the FP6-IST project EuroMatrix. We
also want to thank Teresa Herrmann for helping us
with the Lucy system.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of MT
Summit VIII, Santiago de Compostela, Spain.
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to select
the best among outputs from multiple mt systems. In
COLING.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In ASRU, Italy.
Chris Callison-Burch and Raymond S. Flournoy. 2001.
A program for automatically selecting the best output
from multiple machine translation engines. In Proc. of
MT Summit VIII, Santiago de Compostela, Spain.
Andreas Eisele. 2005. First steps towards multi-engine
machine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, June.
Robert E. Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In ANLP, pages 95?100.
Christopher Hogan and Robert E. Frederking. 1998. An
evaluation of the multi-engine MT architecture. In
Proceedings of AMTA, pages 113?123.
John Hutchins, Walter Hartmann, and Etsuo Ito. 2006.
IAMT compendium of translation software. Twelfth
Edition, January.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. of EAMT, Budapest, Hungary.
P. Koehn, M. Federico, W. Shen, N. Bertoldi, O. Bo-
jar, C. Callison-Burch, B. Cowan, C. Dyer, H. Hoang,
R. Zens, A. Constantin, C. C. Moran, and E. Herbst.
2006. Open source toolkit for statistical machine trans-
lation: Factored translation models and confusion net-
work decoding. Final Report of the 2006 JHU Summer
Workshop.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the MT
Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In In Proc. EACL, pages 33?40.
Tadashi Nomoto. 2004. Multi-engine machine translation
with voted language model. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Manny Rayner and David M. Carter. 1997. Hybrid lan-
guage processing in the spoken language translator. In
Proc. ICASSP ?97, pages 107?110, Munich, Germany.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Dan Tidhar and Uwe Ku?ssner. 2000. Learning to select a
good translation. In COLING, pages 843?849.
196
Proceedings of the Third Workshop on Statistical Machine Translation, pages 179?182,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Moses to Integrate Multiple Rule-Based Machine Translation Engines
into a Hybrid System
Andreas Eisele1,2, Christian Federmann2, Herve? Saint-Amand1,
Michael Jellinghaus1, Teresa Herrmann1, Yu Chen1
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
Abstract
Based on an architecture that allows to com-
bine statistical machine translation (SMT)
with rule-based machine translation (RBMT)
in a multi-engine setup, we present new results
that show that this type of system combination
can actually increase the lexical coverage of
the resulting hybrid system, at least as far as
this can be measured via BLEU score.
1 Introduction
(Chen et al, 2007) describes an architecture that
allows to combine statistical machine translation
(SMT) with one or multiple rule-based machine
translation (RBMT) systems in a multi-engine setup.
It uses a variant of standard SMT technology to align
translations from one or more RBMT systems with
the source text and incorporated phrases extracted
from these alignments into the phrase table of the
SMT system. Using this approach it is possible to
employ a vanilla installation of the open-source de-
coder Moses1 (Koehn et al, 2007) to find good com-
binations of phrases from SMT training data with
the phrases derived from RBMT. A similar method
was presented in (Rosti et al, 2007).
This setup provides an elegant solution to the
fairly complex task of integrating multiple MT re-
sults that may differ in word order using only stan-
dard software modules, in particular GIZA++ (Och
and Ney, 2003) for the identification of building
blocks and Moses for the recombination, but the
authors were not able to observe improvements in
1see http://www.statmt.org/moses/
terms of BLEU score. A closer investigation re-
vealed that the experiments had suffered from a cou-
ple of technical difficulties, such as mismatches in
character encodings generated by different MT en-
gines and similar problems. This motivated us to
re-do these experiments in a somewhat more sys-
tematic way for this year?s shared translation task,
paying the required attention to all the technical de-
tails and also to try it out on more language pairs.
2 System Architecture
For conducting the translations, we use a multi-
engine MT approach based on a ?vanilla? Moses
SMT system with a modified phrase table as a cen-
tral element. This modification is performed by aug-
menting the standard phrase table with entries ob-
tained from translating the data with several rule-
based MT systems. The resulting phrase table thus
combines statistically gathered phrase pairs with
phrase pairs generated by linguistic rules.
Basing its decision about the final translation on
the obtained ?combined? phrase table, the SMT de-
coder searches for the best translation by recombin-
ing the building blocks that have been contributed by
the different RBMT systems and the original SMT
system trained on Europarl data.
A sketch of the overall architecture is given in
Fig. 1, where the lighter parts represent the mod-
ules and data sets used in purely statistical MT,
and the darker parts are the additional modules and
data sets derived from the rule-based engines. The
last word in the proposed setup is thus given to the
SMT decoder, which can recombine (and potentially
also tear apart) linguistically well-formed constructs
179
ModelLanguagePhrasetable
Combined
Alignment,PhraseExtraction
DecoderSMT
Rule?basedMT engines
ParallelCorpus
SourceText
TargetText
MonolingualCorpus
Hypotheses
CountingSmoothing
Figure 1: Hybrid architecture of the system
from the rule-based engines? output.
2.1 The Combined Phrase Table
The combined phrase table is built from the orig-
inal Moses phrase table and separate phrase tables
for each of the RBMT systems that are used in our
setup. Since the original phrase table is created
during the training process of the Moses decoder
with the Europarl bilingual corpus as training ma-
terial, it comprises general knowledge about typical
constructions and vocabulary from the Europarl do-
main. Therefore, a standard Moses SMT system is,
in principle, well adapted for input from this do-
main. However, it will have problems in dealing
with vocabulary and structures that did not occur in
the training data. The additional phrase tables are
generated separately for each RBMT system from
the source text and its translation by the respective
system. By using a combined phrase table that in-
cludes the original Moses phrase table as well as the
phrase tables from the RBMT systems, the hybrid
system can both handle a wider range of syntactic
constructions and exploit knowledge that the RBMT
systems possess about the particular vocabulary of
the source text.
3 Implementation
3.1 MT Systems and Knowledge Sources
Apart from the Moses SMT system, we used a
set of six rule-based MT engines that are partly
available via web interfaces and partly installed lo-
cally. The web interfaces are provided by Al-
tavista Babelfish (based on Systran), SDL, ProMT
and Lucy (a recent offspring of METAL). All of
them deliver significantly different output trans-
lations. Locally installed systems are OpenLo-
gos (for German?English, English?Spanish and
English?French) and translatePro by lingenio (for
German?English). The language model for our pri-
mary setup is based on the Europarl corpus whereas
the English Gigaword corpus served as training data
for a contrastive setup that was created for the trans-
lation direction German?English only.
3.2 Alignment of RBMT output
As already mentioned above, the construction of the
RBMT system specific phrase tables is a major part
of the overall system architecture. Such an RBMT
phrase table is generated from a bilingual corpus
consisting of the input text and its translation by
the respective RBMT system. Because this corpus
has the mere size of the text to be translated, it usu-
ally is not big enough to ensure the statistical meth-
ods for phrase table building of the Moses system to
work. Therefore, we create the alignments between
the RBMT input and output with help of another tool
(Theison, 2007) that is based on knowledge learned
in a previously conducted training phase with an ap-
propriately bigger corpus. On the basis of the align-
ments created in this manner, the Moses training
script provides a phrase table that consists of the
source text vocabulary. These steps are carried out
for each one of the six RBMT systems leading to
six source text specific phrase tables which are then
combined with the original Moses phrase table.
3.3 Combination of Phrase Tables
The combination process basically consists of the
concatenation of the Moses phrase table and the pre-
viously created RBMT phrase tables with one mi-
nor adjustment: The phrase table resulting from this
combination now also features additional columns
indicating which system each phrase table entry
originated from. For each new source text, the
RBMT phrase tables have to be created from scratch
and incorporated into a new combined phrase table.
3.4 Tuning
The typical process for creating an SMT system with
the Moses toolkit includes a tuning step in which
180
Europarl NewsCommentary
de-en en-de fr-en en-fr es-en en-es de-en en-de fr-en en-fr es-en en-es
SMT 22.81 19.78 24.18 21.62 31.68 24.46 14.24 9.75 11.60 12.24 17.27 14.48
Hybrid 27.85 20.75 28.12 28.82 33.15 32.31 17.36 13.57 17.66 20.71 22.16 22.55
RBMT1? 13.34 11.09 ?? 17.19 ?? 18.63 14.90 12.34 ?? 15.11 ?? 17.13
RBMT2 16.19 12.06 ?? ?? ?? ?? 16.66 13.64 ?? ?? ?? ??
RBMT3 16.32 10.88 18.18 20.38 19.32 20.89 16.88 12.53 17.20 18.82 19.00 19.98
RBMT4 15.58 12.09 19.00 22.20 18.99 21.69 17.41 13.93 17.73 20.85 19.14 21.70
RBMT5 15.58 9.54 21.36 12.98 18.47 20.59 15.99 11.05 18.65 19.49 20.50 20.02
RBMT6 13.96 9.44 17.16 18.91 18.01 19.18 15.08 10.41 16.86 17.82 18.70 19.97
Table 1: Performance of baseline SMT system, our system and RBMT systems (BLEU scores)
the system searches for the best weight configura-
tion for the columns in the phrase table while given
a development set to be translated, and correspond-
ing reference translations. In our hybrid setup, it is
equally essential to conduct tuning since the com-
bined phrase table we use contains 7 more columns
than the original Moses phrase table. All these
columns are given the same default weight initially
and thus still need be to be tuned to more meaning-
ful values. From this year?s Europarl development
data the first 200 sentences of each of the data sets
dev2006, test2006, test2007 and devtest2006 were
concatenated to build our development set. This set
of 800 sentences was used for Minimum Error Rate
Training (Och, 2003) to tune the weights of our sys-
tem with respect to BLEU score.
4 Results
In order to be able to evaluate our hybrid approaches
in contrast to stand-alone rule-based approaches, we
also calculated BLEU scores for the translations
conducted by the RBMT systems used in the hy-
brid setup. Our hybrid system is compared to a SMT
baseline and all the 6 RBMT systems that we used.
Table 1 shows the evaluation of all the systems in
terms of BLEU score (Papineni et al, 2002) with the
best score highlighted. The empty cells in the table
indicate the language pairs which are not available
in the corresponding systems2. The SMT system is
the one upon which we build the hybrid system. Ac-
cording to the scores, the hybrid system produces
better results than the baseline SMT system in all
2The identities of respective RBMT systems are not revealed
in this paper. RBMT1 is evaluated on the partial results pro-
duced due to some technical problems.
cases. The difference between our system and the
baseline is more significant for out-of-domain tests,
where gaps in the lexicon tend to be more severe.
Figure 2 illustrates an example of how the hy-
brid system differs from the baseline SMT system
and how it benefits from the RBMT systems. The
example lists the English translations of the same
German sentence (from News Commentary test set)
from different systems involved in our experiment.
Neither the word ?Pentecost? nor its German trans-
lation ?Pfingsten? has appeared in the training cor-
pus. Therefore, the SMT baseline system cannot
translate the word and chooses to leave the word
as it is whereas all the RBMT systems translate the
word correctly. The hybrid system appears to have
the corresponding lexicon gap covered by the ex-
tra entries produced by the RBMT systems. On the
other side, these additional entries may not always
be helpful. The errors in RBMT outputs can be sig-
nificant noise that destroys the correct information
in the SMT system. In the example translation pro-
duced by the hybrid system, there is a comma miss-
ing after ?in addition?, which appears to be frequent
in the RBMT outputs.
5 Outlook
The results reported in this paper are still somewhat
preliminary in the sense that many possible (includ-
ing some desirable) variants of the setup could not
be tried out due to lack of time. In particular, we
think that the full power of our approach on out-
of-domain test data can only be exploited with the
help of large language models trained on out-of-
domain text, but could not yet try this systematically.
Furthermore, the presence of multiple instances of
181
Source Daru?ber hinaus gibt es je zwei Feiertage zu Ostern, Pfingsten, und Weihnachten.
Reference In addition, Easter, Pentecost, and Christmas are each two-day holidays.
Moses In addition, there are two holidays, pfingsten to Easter, and Christmas.
Hybrid In addition there are the two holidays to Easter, Pentecost and Christmas.
RBMT1 Furthermore there are two holidays to Easter, Pentecost and Christmas .
RBMT2 Furthermore there are two holidays each at Easter, Pentecost and Christmas.
RBMT3 In addition there are each two holidays to Easters, Whitsun, and Christmas.
RBMT4 In addition, there is two holidays to Easter, Pentecost, and Christmas.
RBMT5 Beyond that there are ever two holidays to Easter, Whitsuntide, and Christmas.
RBMT6 In addition it gives two holidays apiece to easter, Pentecost, and Christmas.
Figure 2: German-English translation examples
the same phrase pair (with different weight) in the
combined phrase table causes the decoder to gen-
erate many instances of identical results in differ-
ent ways, which increases computational effort and
significantly decreases the number of distinct cases
that are considered during MERT. We suspect that a
modification of our scheme that avoids this problem
will be able to achieve better results, but experiments
in this direction are still ongoing.
The approach presented here combines the
strengths of multiple systems and is different from
recent work on post-correction of RBMT output as
presented in (Simard et al, 2007; Dugast et al,
2007), which focuses on the improvement of a sin-
gle RBMT system by correcting typical errors via
SMT techniques. These ideas are independent and a
suitable combination of them could give rise to even
better results.
Acknowledgments
This work was supported by the EuroMatrix project
funded by the European Commission (6th Frame-
work Programme). We thank Martin Kay, Hans
Uszkoreit, and Silke Theison for interesting discus-
sions and practical help, and two anonymous re-
viewers for hints to improve the paper.
References
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
SMT decoder. In Proceedings of WMT07, pages 193?
196, Prague, Czech Republic, June. Association for
Computational Linguistics.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of WMT07, pages
220?223, Prague, Czech Republic, June. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL Demo and Poster Sessions, pages 177?180, Jun.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, Mar.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with
statistical phrase-based post-editing. In Proceedings
of WMT07, pages 203?206, Prague, Czech Republic,
June. Association for Computational Linguistics.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical methods.
Diploma thesis, Saarland University.
182
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 42?46,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Combining Multi-Engine Translations with Moses
Yu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2,
Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
{yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de
{eisele,cfedermann,uszkoreit}@dfki.de
Abstract
We present a simple method for generating
translations with the Moses toolkit (Koehn
et al, 2007) from existing hypotheses pro-
duced by other translation engines. As
the structures underlying these translation
engines are not known, an evaluation-
based strategy is applied to select sys-
tems for combination. The experiments
show promising improvements in terms of
BLEU.
1 Introduction
With the wealth of machine translation systems
available nowadays (many of them online and
for free), it makes increasing sense to investigate
clever ways of combining them. Obviously, the
main objective lies in finding out how to integrate
the respective advantages of different approaches:
Statistical machine translation (SMT) and rule-
based machine translation (RBMT) systems of-
ten have complementary characteristics. Previous
work on building hybrid systems includes, among
others, approaches using reranking, regeneration
with an SMT decoder (Eisele et al, 2008; Chen
et al, 2007), and confusion networks (Matusov et
al., 2006; Rosti et al, 2007; He et al, 2008).
The approach by (Eisele et al, 2008) aimed
specifically at filling lexical gaps in an SMT sys-
tem with information from a number of RBMT
systems. The output of the RBMT engines was
word-aligned with the input, yielding a total of
seven phrase tables which where simply concate-
nated to expand the phrase table constructed from
the training corpus. This approach differs from the
confusion network approaches mainly in that the
final hypotheses do not necessarily follow any of
the input translations as the skeleton. On the other
hand, it emphasizes that the additional translations
should be produced by RBMT systems with lexi-
cons that cannot be learned from the data.
The present work continues on the same track
as the paper mentioned above but implements a
number of important changes, most prominently
a relaxation of the restrictions on the number and
type of input systems. These differences are de-
scribed in more detail in Section 2. Section 3 ex-
plains the implementation of our system and Sec-
tion 4 its application in a number of experiments.
Finally, Section 5 concludes this paper with a sum-
mary and some thoughts on future work.
2 Integrating Multiple Systems of
Unknown Type and Quality
When comparing (Eisele et al, 2008) to the
present work, our proposal is more general in a
way that the requirement for knowledge about the
systems is minimum. The types and the identities
of the participated systems are assumed unknown.
Accordingly, we are not able to restrict ourselves
to a certain class of systems as (Eisele et al, 2008)
did. We rely on a standard phrase-based SMT
framework to extract the valuable pieces from the
system outputs. These extracted segments are also
used to improve an existing SMT system that we
have access to.
While (Eisele et al, 2008) included translations
from all of a fixed number of RBMT systems
and added one feature to the translation model for
each system, integrating all given system outputs
in this way in our case could expand the search
space tremendously. Meanwhile, we cannot rely
on the assumption that all candidate systems ac-
tually have the potential to improve our baseline.
This implies the need for a first step of system se-
lection where the best candidate systems are iden-
tified and a limited number of them is chosen to be
included in the combination. Our approach would
not work without a small set of tuning data being
available so that we can evaluate the systems for
later selection and adjust the weights of our sys-
tems. Such tuning data is included in this year?s
42
task.
In this paper, we use the Moses decoder to con-
struct translations from the given system outputs.
We mainly propose two slightly different ways:
One is to construct translation models solely from
the given translations and the other is to extend
an existing translation model with these additional
translations.
3 Implementation
Despite the fact that the output of current MT sys-
tems is usually not comparable in quality to hu-
man translations, the machine-generated transla-
tions are nevertheless ?parallel? to the input so
that it is straightforward to construct a translation
model from data of this kind. This is the spirit
behind our method for combining multiple trans-
lations.
3.1 Direct combination
Clearly, for the same source sentence, we expect
to have different translations from different trans-
lation systems, just like we would expect from hu-
man translators. Also, every system may have its
own advantages. We break these translations into
smaller units and hope to be able to select the best
ones and form them into a better translation.
One single translation of a few thousand sen-
tences is normally inadequate for building a re-
liable general-purpose SMT system (data sparse-
ness problem). However, in the system combina-
tion task, this is no longer an issue as the system
only needs to translate sentences within the data
set.
When more translation engines are available,
the size of this set becomes larger. Hence,
we collect translations from all available systems
and pair them with the corresponding input text,
thus forming a medium-sized ?hypothesis? cor-
pus. Our system starts processing this corpus
with a standard phrase-based SMT setup, using the
Moses toolkit (Koehn et al, 2007).
The hypothesis corpus is first tokenized and
lowercased. Then, we run GIZA++ (Och and
Ney, 2003) on the corpus to obtain word align-
ments in both directions. The phrases are extracted
from the intersection of the alignments with the
?grow? heuristics. In addition, we also generate
a reordering model with the default configuration
as included in the Moses toolkit. This ?hypothe-
sis? translation model can already be used by the
Moses decoder together with a language model to
perform translations over the corresponding sen-
tence set.
3.2 Integration into existing SMT system
Sometimes, the goal of system combination is not
only to produce a translation but also to improve
one of the systems. In this paper, we aim at incor-
porating the additional system outputs to improve
an out-of-domain SMT system trained on the Eu-
roparl corpus (Koehn, 2005). Our hope is that the
additional translation hypotheses could bring in
new phrases or, more generally, new information
that was not contained in the Europarl model. In
order to facilitate comparisons, we use in-domain
LMs for all setups.
We investigate two alternative ways of integrat-
ing the additional phrases into the existing SMT
system: One is to take the hypothesis translation
model described in Section 3.1, the other is to
construct system-specific models constructed with
only translations from one system at a time.
Although the Moses decoder is able to work
with two phrase tables at once (Koehn and
Schroeder, 2007), it is difficult to use this method
when there is more than one additional model.
The method requires tuning on at least six more
features, which expands the search space for the
translation task unnecessarily. We instead inte-
grate the translation models from multiple sources
by extending the phrase table. In contrast to the
prior approach presented in (Chen et al, 2007) and
(Eisele et al, 2008) which concatenates the phrase
tables and adds new features as system markers,
our extension method avoids duplicate entries in
the final combined table.
Given a set of hypothesis translation models
(derived from an arbitrary number of system out-
puts) and an original large translation model to be
improved, we first sort the models by quality (see
Section 3.3), always assigning the highest priority
to the original model. The additional phrase tables
are appended to the large model in sorted order
such that only phrase pairs that were never seen
before are included. Lastly, we add new features
(in the form of additional columns in the phrase ta-
ble) to the translation model to indicate each pair?s
origin.
3.3 System evaluation
Since both the system translations and the ref-
erence translations are available for the tuning
43
set, we first compare each output to the reference
translation using BLEU (Papineni et al, 2001)
and METEOR (Banerjee and Lavie, 2005) and a
combined scoring scheme provided by the ULC
toolkit (Gimenez and Marquez, 2008). In our ex-
periments, we selected a subset of 5 systems for
the combination, in most cases, based on BLEU.
On the other hand, some systems may be de-
signed in a way that they deliver interesting unique
translation segments. Therefore, we also measure
the similarity among system outputs as shown in
Table 2 in a given collection by calculating aver-
age similarity scores across every pair of outputs.
de-en fr-en es-en en-de en-fr en-es
Num. 20 23 28 15 16 9
Median 19.87 26.55 22.50 13.78 24.76 23.70
Range 16.37 17.06 9.74 4.75 11.05 13.94
Top 5 de-en fr-en es-en en-de en-fr en-es
Median 22.26 27.93 26.43 15.21 26.62 26.61
Range 4.31 4.76 5.71 1.71 0.68 5.56
Table 1: Statistics of system outputs? BLEU scores
The range of BLEU scores cannot indicate the
similarity of the systems. The direction with the
most systems submitted is Spanish-English but
their respective performances are very close to
each other. As for the selected subset, the English-
French systems have the most similar performance
in terms of BLEU scores. The French-English
translations have the largest range in BLEU but the
similarity in this group is not the lowest.
de-en fr-en es-en en-de en-fr en-es
All 34.09 46.48 61.83 31.74 44.95 38.11
Selected 36.65 56.16 56.06 33.92 52.78 57.25
Table 2: Similarity of the system outputs
Ideally, we should select systems with highest
quality scores and lowest similarity scores. For
German-English, we selected the three with the
highest METEOR scores and another two with
high METEOR scores but low similarity scores to
the first three. For the other language directions,
we chose five systems from different institutions
with the highest scores.
3.4 Language models
We use a standard n-gram language model for
each target language using the monolingual train-
ing data provided in the translation task. These
LMs are thus specific to the same domain as the
input texts. Moreover, we also generate ?hypoth-
esis? LMs solely based on the given system out-
puts, that is, LMs that model how the candidate
systems convey information in the target language.
These LMs do not require any additional training
data. Therefore, we do not require any training
data other than the given system outputs by using
the ?hypothesis? language model and the ?hypoth-
esis? translation model.
3.5 Tuning
After building the models, it is essential to tune
the SMT system to optimize the feature weights.
We use Minimal Error Rate Training (Och, 2003)
to maximize BLEU on the complete development
data. Unlike the standard tuning procedure, we do
not tune the final system directly. Instead, we ob-
tain the weights using models built from the tuning
portion of the system outputs.
For each combination variant, we first train
models on the provided outputs corresponding to
the tuning set. This system, called the tuning sys-
tem, is also tuned on the tuning set. The initial
weights of any additional features not included in
the standard setting are set to 0. We then adapt the
weights to the system built with translations cor-
responding to the test set. The procedure and the
settings for building this system must be identical
to that of the tuning system.
4 Experiments
The purpose of this exercise is to understand the
nature of the system combination task in prac-
tice. Therefore, we restrict ourselves to the train-
ing data and system translations provided by the
shared task. The types of the systems that pro-
duced the translations are assumed to be unknown.
We report results for six translation directions be-
tween four languages.
4.1 Data and baseline
We build an SMT system from release v4 of the
Europarl corpus (Koehn, 2005), following a stan-
dard routine using the Moses toolkit. The sys-
tem also includes 5-gram language models trained
on in-domain corpora of the respective target lan-
guages using SRILM (Stolcke, 2002).
The systems in this paper, including the base-
line, are all tuned on the same 501-sentence tuning
set. Note also that the provided n-best outputs are
excluded in our experiments.
44
4.2 Results
The experiments include three different setups for
direct system combination, involving only hypoth-
esis translation models. System S0, the baseline
for this group, uses a hypothesis translation model
built with all available system translations and a
hypothesis LM (also from the machine-generated
outputs). S1 differs from S0 in that the LM in S1 is
generated from a large news corpus. S2 consists of
translation models built with only the five selected
systems. The BLEU scores of these systems are
shown in Table 3.
de-en fr-en es-en en-de en-fr en-es
Top 1 21.16 30.91 28.54 14.96 26.55 27.84
Mean 17.29 23.78 21.39 12.76 22.96 21.43
S0 20.46 27.50 23.35 13.95 27.29 25.59
S1 21.76 28.05 25.49 15.16 27.70 26.09
S2 21.71 24.98 27.26 15.62 24.28 25.22
Table 3: BLEU scores of direct system combina-
tion
When all outputs are included, the combined
system can always produce translations better than
most of the systems. When only a hypothesis LM
is used, the BLEU scores are always higher than
the average BLEU scores of the outputs. It even
outperforms the top system for English-French.
This simple setup (S0) is certainly a feasible so-
lution when no additional data is available and no
system evaluation is possible. This approach ap-
pears to be more effective on typically difficult
language pairs that involve German.
As for the systems with normal language mod-
els, neither of the systems ensure better transla-
tions. The translation quality is not completely
determined by the number of included translations
and their quality. On the other hand, the output
set with higher diversity (Table 2) usually leads
to better combination results. This observation is
consistent with the results from the system inte-
gration experiments shown in Table 4.
de-en fr-en es-en en-de en-fr en-es
Bas 19.13 25.07 24.55 13.59 23.67 23.67
Med 17.99 24.56 20.70 13.19 24.19 22.12
All 21.40 28.00 27.75 15.21 27.20 26.41
Top5 21.70 26.01 28.53 15.52 27.87 27.92
Table 4: BLEU scores of integrated SMT systems
(Bas: Baseline, Med: Median)
There are two variants in our experiments on
system integration. All in Table 4 represents the
system that integrates the complete hypothesis
translation model with the Europarl model, while
Top 5 refers to the system that incorporates the five
system-specific models separately. Both setups re-
sult in an improvement over the baseline Europarl-
based SMT system. BLEU scores increase by up
to 4.25 points. The integrated SMT system some-
times produces translations better than the best
system (7 out of 12 cases).
5 Conclusion
This work uses the Moses toolkit to combine
translations from multiple engines in a simple way.
The experiments on six translation directions show
interesting results: The final translations are al-
ways better than the majority of the given systems,
while the combination performs better than the
best system in half the cases. A similar approach
was applied to improve an existing SMT system
which was built in a domain different from the test
task. We achieved improvements in all cases.
There are many possible future directions to
continue this work. As we have shown, the qual-
ity of the combined system is more related to the
diversity of the involved systems than to the num-
ber of the systems or their quality. Hand-picked
systems lead to better combinations than those se-
lected by BLEU scores. It would be interesting
to develop a more comprehensive system selection
strategy.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an
open-source SMT decoder. In Proceedings of
45
WMT07, pages 193?196, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to integrate mul-
tiple rule-based machine translation engines into a
hybrid system. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 179?
182, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Jesus Gimenez and Lluis Marquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 195?198, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbs. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of Annual meet-
ing of the Association for Computation Linguis-
tics (acl), demonstration session, pages 177?180,
Prague, Czech, June.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Antti-Veikko I. Rosti, Spyridon Matsoukas, and
Richard M. Schwartz. 2007. Improved word-level
system combination for machine translation. In
ACL.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
46
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 70?74,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Translation Combination using Factored Word Substitution
Christian Federmann1, Silke Theison2, Andreas Eisele1,2, Hans Uszkoreit1,2,
Yu Chen2, Michael Jellinghaus2, Sabine Hunsicker2
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de
Abstract
We present a word substitution approach
to combine the output of different machine
translation systems. Using part of speech
information, candidate words are deter-
mined among possible translation options,
which in turn are estimated through a pre-
computed word alignment. Automatic
substitution is guided by several decision
factors, including part of speech, local
context, and language model probabili-
ties. The combination of these factors
is defined after careful manual analysis
of their respective impact. The approach
is tested for the language pair German-
English, however the general technique it-
self is language independent.
1 Introduction
Despite remarkable progress in machine transla-
tion (MT) in the last decade, automatic translation
is still far away from satisfactory quality. Even the
most advanced MT technology as summarized by
(Lopez, 2008), including the best statistical, rule-
based and example-based systems, produces out-
put rife with errors. Those systems may employ
different algorithms or vary in the linguistic re-
sources they use which in turn leads to different
characteristic errors.
Besides continued research on improving MT
techniques, one line of research is dedicated to bet-
ter exploitation of existing methods for the com-
bination of their respective advantages (Macherey
and Och, 2007; Rosti et al, 2007a).
Current approaches for system combination in-
volve post-editing methods (Dugast et al, 2007;
Theison, 2007), re-ranking strategies, or shal-
low phrase substitution. The combination pro-
cedure applied for this pape tries to optimize
word-level translations within a ?trusted? sentence
frame selected due to the high quality of its syntac-
tic structure. The underlying idea of the approach
is the improvement of a given (original) translation
through the exploitation of additional translations
of the same text. This can be seen as a simplified
version of (Rosti et al, 2007b).
Considering our submission from the shared
translation task as the ?trusted? frame, we add
translations from four additional MT systems that
have been chosen based on their performance in
terms of automatic evaluation metrics. In total, the
combination system performs 1,691 substitutions,
i.e., an average of 0.67 substitutions per sentence.
2 Architecture
Our system combination approach computes a
combined translation from a given set of machine
translations. Below, we present a short overview
by describing the different steps in the derivation
of a combined translation.
Compute POS tags for translations. We apply
part-of-speech (POS) tagging to prepare the
selection of possible substitution candidates.
For the determination of POS tags we use the
Stuttgart TreeTagger (Schmid, 1994).
Create word alignment. The alignment between
source text and translations is needed to
identify translation options within the differ-
ent systems? translations. Word alignment
is computed using the GIZA++ toolkit (Och
and Ney, 2003), only one-to-one word align-
ments are employed.
Select substitution candidates. For the shared
task, we decide to substitute nouns, verbs
and adjectives based on the available POS
tags. Initially, any such source word is con-
sidered as a possible substitution candidate.
As we do not want to require substitution can-
70
didates to have exactly the same POS tag as
the source, we use groups of ?similar? tags.
Compute decision factors for candidates. We
define several decision factors to enable an
automatic ranking of translation options.
Details on these can be found in section 4.
Evaluate the decision factors and substitute.
Using the available decision factors we
compute the best translation and substitute.
The general combination approach is language
independent as it only requires a (statistical) POS
tagger and GIZA++ to compute the word align-
ments. More advanced linguistic resources are not
required. The addition of lexical resources to im-
prove the extracted word alignments has been con-
sidered, however the idea was then dropped as we
did not expect any short-term improvements.
3 System selection
Our system combination engine takes any given
number of translations and enables us to compute
a combined translation out of these. One of the
given system translations is chosen to provide the
?sentence skeleton?, i.e. the global structure of the
translation, thus representing the reference system.
All other systems can only contribute single words
for substitution to the combined translation, hence
serve as substitution sources.
3.1 Reference system
Following our research on hybrid translation try-
ing to combine the strengths of rule-based MT
with the virtues of statistical MT, we choose our
own (usaar) submission from the shared task to
provide the sentence frame for our combination
system. As this translation is based upon a rule-
based MT system, we expect the overall sentence
structure to be of a sufficiently high quality.
3.2 Substitution sources
For the implementation of our combination sys-
tem, we need resources of potential substitution
candidates. As sources for possible substitution,
we thus include the translation results of the fol-
lowing four systems:
? Google (google)1
1The Google submission was translated by the Google
MT production system offered within the Google Language
Tools as opposed to the qualitatively superior Google MT
research system.
? University of Karlsruhe (uka)
? University of Maryland (umd)
? University of Stuttgart (stuttgart)
The decision to select the output of these par-
ticular MT systems is based on their performance
in terms of different automatic evaluation metrics
obtained with the IQMT Framework by (Gime?nez
and Amigo?, 2006). This includes BLEU, BLEU1,
TER, NIST, METEOR, RG, MT06, and WMT08.
The results, listing only the three best systems per
metric, are given in table 1.
metric best three systems
BLEU1 google uka systran
0.599 0.593 0.582
BLEU google uka umd
0.232 0.231 0.223
TER umd rwth.c3 uka
0.350 0.335 0.332
NIST google umd uka
6.353 6.302 6.270
METEOR google uka stuttgart
0.558 0.555 0.548
RG umd uka google
0.527 0.525 0.520
MT06 umd google stuttgart
0.415 0.413 0.410
WMT08 stuttgart rbmt3 google
0.344 0.341 0.336
Table 1: Automatic evaluation results.
On grounds of these results we anticipate the
four above named translation engines to perform
best when being combined with our hybrid ma-
chine translation system. We restrict the substi-
tution sources to the four potentially best systems
in order to omit bad substitutions and to reduce
the computational complexity of the substitution
problem. It is possible to choose any other num-
ber of substitution sources.
4 Substitution
As mentioned above, we consider nouns, verbs
and adjectives as possible substitution candidates.
In order to allow for automatic decision making
amongst several translation options we define a set
of factors, detailed in the following. Furthermore,
we present some examples in order to illustrate the
use of the factors within the decision process.
71
4.1 Decision factors
The set of factors underlying the decision proce-
dure consists of the following:
A: Matching POS. This Boolean factor checks
whether the target word POS tag matches the
source word?s POS category. The factor com-
pares the source text to the reference trans-
lation as we want to preserve the sentential
structure of the latter.
B: Majority vote. For this factor, we compute
an ordered list of the different translation op-
tions, sorted by decreasing frequency. A con-
sensus between several systems may help to
identify the best translation.
Both the reference system and the Google
submission receive a +1 bonus, as they ap-
peared to offer better candidates in more
cases within the small data sample of our
manual analysis.
C: POS context. Further filtering is applied de-
termining the words? POS context. This is
especially important as we do not want to de-
grade the sentence structure maintained by
the translation output of the reference system.
In order to optimize this factor, we conduct
trials with the single word, the ?1 left, and
the +1 right context. To reduce complex-
ity, we shorten POS tags to a single character,
e.g. NN ? N or NPS ? N .
D: Language Model. We use an English lan-
guage model to score the different translation
options. As the combination system only re-
places single words within a bi-gram context,
we employ the bi-gram portion of the English
Gigaword language model.
The language model had been estimated us-
ing the SRILM toolkit (Stolcke, 2002).
4.2 Factor configurations
To determine the best possible combination of our
different factors, we define four potential factor
configurations and evaluate them manually on a
small set of sentences. The configurations differ
in the consideration of the POS context for factor
C (strict including ?1 left context versus relaxed
including no context) and in the usage of factor A
Matching POS (+A). Table 2 shows the settings of
factors A and C for the different configurations.
configuration Matching POS POS context
strict disabled ?1 left
strict+A enabled ?1 left
relaxed disabled single word
relaxed+A enabled single word
Table 2: Factor configurations for combination.
Our manual evaluation of the respective substi-
tution decisions taken by different factor combi-
nation is suggestive of the ?relaxed+A? configura-
tion to produce the best combination result. Thus,
this configuration is utilized to produce sound
combined translations for the complete data set.
4.3 Factored substitution
Having determined the configuration of the dif-
ferent factors, we compute those for the complete
data set, in order to apply the final substitution step
which will create the combined translation.
The factored substitution algorithm chooses
among the different translation options in the fol-
lowing way:
(a) Matching POS? If factor A is activated for
the current factor configuration (+A), sub-
stitution of the given translation options can
only be possible if the factor evaluates to
True. Otherwise the substitution candidate is
skipped.
(b) Majority vote winner? If the majority vote
yields a unique winner, this translation option
is taken as the final translation.
Using the +1 bonuses for both the reference
system and the Google submission we intro-
duce a slight bias that was motivated by man-
ual evaluation of the different systems? trans-
lation results.
(c) Language model. If several majority vote
winners can be determined, the one with the
best language model score is chosen.
Due to the nature of real numbers this step
always chooses a winning translation option
and thus the termination of the substitution
algorithm is well-defined.
Please note that, while factors A, B, and D are
explicitly used within the substitution algorithm,
factor C POS context is implicitly used only when
computing the possible translation options for a
given substitution candidate.
72
configuration substitutions ratio
strict 1,690 5.714%
strict+A 1,347 4.554%
relaxed 2,228 7.532%
relaxed+A 1,691 5.717%
Table 3: Substitutions for 29,579 candidates.
Interestingly we are able to obtain best results
without considering the ?1 left POS context, i.e.
only checking the POS tag of the single word
translation option for factor C.
4.4 Combination results
We compute system combinations for each of the
four factor configurations defined above. Table
3 displays how many substitutions are conducted
within each of these configurations.
The following examples illustrate the perfor-
mance of the substitution algorithm used to pro-
duce the combined translations.
?Einbruch?: the reference translation for ?Ein-
bruch? is ?collapse?, the substitution sources
propose ?slump? and ?drop?, but also ?col-
lapse?, all three, considering the context,
forming good translations. The majority vote
rules out the suggestions different to the ref-
erence translation due to the fact that 2 more
systems recommend ?collapse? as the correct
translation.
?Ru?ckgang?: the reference system translates this
word as ?drop? while all of the substitution
sources choose ?decline? as the correct trans-
lation. Since factor A evaluates to True, i.e.
the POS tags are of the same nature, ?de-
cline? is clearly selected as the best transla-
tion by factor B Majority vote and thus re-
places ?drop? in the final combined transla-
tion result.
?Tagesgescha?fte?: our reference system trans-
lates ?Tagesgescha?fte? with ?requirements?,
while two of the substitution systems indi-
cate ?business? to be a better translation. Due
to the +1 bonus for our reference translation
a tie between the two possible translations
emerges, leaving the decision to the language
model score, which is higher for ?business?.
4.5 Evaluation results
Table 4 shows the results of the manual evaluation
campaign carried out as part of the WMT09 shared
task. Randomly chosen sentences are presented
to the annotator, who then has to put them into
relative order. Note that each annotator is shown a
random subset of the sentences to be evaluated.
system relative rank data points
google -2.74 174
uka -3.00 217
umd -3.03 170
stuttgart -2.89 163
usaar -2.78 186
usaar-combo -2.91 164
Table 4: Relative ranking results from the WMT09
manual evalution campaign.
Interestingly, our combined system is not able
to outperform the baseline, i.e., additional data
did not improve translation results. However the
evaluation is rather intransparent since it does not
allow for a strict comparison between sentences.
5 Conclusion
Within the system described in this paper, we ap-
proach a hybrid translation technique combining
the output of different MT systems. Substituting
particular words within a well-structured transla-
tion frame equips us with considerably enhanced
translation output. We obtain promising results
providing substantiated proof that our approach is
going in the right direction.
Further steps in the future will include machine
learning methods to optimize the factor selection.
This was, due to limited amount of time and data,
not feasible thus far. We will also investigate the
potential of phrase-based substitution taking into
account multi-word alignments instead of just sin-
gle word mappings. Additionally, we would like
to continue work on the integration of lexical re-
sources to post-correct the word alignments ob-
tained by GIZA++ as this will directly improve the
overall system performance.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
73
References
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
220?223, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
framework for automatic machine translation eval-
uation. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC?06).
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Wolfgang Macherey and Franz J. Och. 2007. An em-
pirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical meth-
ods. Master?s thesis, Saarland University, Computa-
tional Linguistics department.
74
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 128?136,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Intersecting multilingual data for faster and better statistical translations
Yu Chen1,2, Martin Kay1,3, Andreas Eisele1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
3: Stanford University, CA, USA
{yuchen,kay,eisele}@coli.uni-saarland.de
Abstract
In current phrase-based SMT systems, more
training data is generally better than less.
However, a larger data set eventually intro-
duces a larger model that enlarges the search
space for the translation problem, and con-
sequently requires more time and more re-
sources to translate. We argue redundant in-
formation in a SMT system may not only de-
lay the computations but also affect the qual-
ity of the outputs. This paper proposes an ap-
proach to reduce the model size by filtering
out the less probable entries based on com-
patible data in an intermediate language, a
novel use of triangulation, without sacrificing
the translation quality. Comprehensive exper-
iments were conducted on standard data sets.
We achieved significant quality improvements
(up to 2.3 BLEU points) while translating with
reduced models. In addition, we demon-
strate a straightforward combination method
for more progressive filtering. The reduction
of the model size can be up to 94% with the
translation quality being preserved.
1 Introduction
Statistical machine translation (SMT) applies ma-
chine learning techniques to a bilingual corpus to
produce a translation system entirely automatically.
Such a scheme has many potential advantages over
earlier systems which relied on carefully crafted
rules. The most obvious is that it at dramatically
reduces cost in human labor and it is able to reach
many critical translation rules that are easily over-
looked by human being.
SMT systems generally assemble translations by
selecting phrases from a large candidate set. Un-
supervised learning often introduces a considerable
amount of noise into this set as a result of which the
selection process becomes more longer and less ef-
fective. This paper provides one approach to these
problems.
Various filtering techniques, such as (Johnson et
al., 2007) and (Chen et al, 2008), have been ap-
plied to eliminate a large portion of the translation
rules that were judged unlikely to be of value for
the current translation. However, these approaches
were only able to improve the translation quality
slightly. In this paper, we describe a triangulation
approach (Kay, 1997) that incorporates multilingual
data to improve system efficiency and translation
quality at the same time. Most of the previous tri-
angulation approaches (Kumar et al, 2007; Cohn
and Lapata, 2007; Filali and Bilmes, 2005; Simard,
1999; Och and Ney, 2001) add information obtained
from a third language. In other words, they work
with the union of the data from the different lan-
guages. In contrast, we work with the intersection of
information acquired through a third language. The
hope is that the intersection will be more precise and
more compact than the union, so that a better result
will be obtained more efficiently.
2 Noise in a phrase-based SMT system
The phrases in a translation model are extracted
heuristically from a word alignment between the
parallel texts in two languages using machine learn-
ing techniques. The translation model feature values
are stored in the form of a so-called phrase-table,
128
while the distortion model is in the reordering-table.
As we have said models built in this way tend to con-
tain a contains a considerable amount of noise. The
phrase-table entries are far less reliable than the lex-
icons and grammar rules handcrafted for rule-based
systems.
The main source of noise in the phrase table is
errors from the word alignment process. For exam-
ple, many function words occur so frequently that
they are incorrectly mapped to translations of many
function words in the other language to which they
are, in fact, unrelated. On the other hand, many
words remain unaligned on account of their very low
frequency. Another source noise comes from the
phrase extraction algorithm itself. The unaligned
words are usually attached to aligned sequences In
order to achieve longer phrase pairs.
The final selection of entries from the phrase ta-
ble is based not only on the values assigned to them
there, but also to values coming from the language
and reordering models, so that entries that receive an
initially high value may end up not being preferred.
(1) Sie
they
lieben
love
ihre
their
Kinder
children
nicht.
not
They don?t love their children.
The frequently occurring German negative ?nicht?
in (1). is sometimes difficult for SMT systems
to translate into English because it may appear in
many positions of a sentence. For instance, it oc-
curs at the end of the sentence in (1). The phrase
pairs ?ihre kinder nicht ? their children are not?
and ?ihre kinder nicht ? their children? are both
likely also to appear in the phrase table and the for-
mer has greater estimated probability. However, the
language model would preferred the latter in this ex-
ample because the sentence ?They love their children
are not.? is unlikely to be attested. Accordingly,
SMT system may therefore produce the misleading
translation in (2).
(2) They love their children.
The system would not produce translations with the
opposite meanings if the noisy entries like ?ihre
kinder nicht ? their children? were excluded from
the translation candidates. Eliminating the noise
should help to improve the system?s performance,
for both efficiency and translation quality.
3 Triangulated filtering
While direct translation and pivot translation
through a bridge language presumably introduce
noise, in substantially similar amounts, there is no
reason to expect the noise in the two systems to cor-
relate strongly. In fact, the noise from such differ-
ent sources, tends to be quite distinct, whereas the
more useful information is often retained. This en-
courages us to hope that information gathered from
various sources will be more reliable overall.
Our plan is to ameliorate the noise problem by
constructing a smaller phrase-table by taking the
intersection of a number of sources. We reason that a
target phrase is will appear as a candidate translation
of a given source phrase, only if it also appears as a
candidate translation for some word or phrase in the
bridge language mapping to the source phrase. We
refer to this triangulation approach as triangulated
phrase-table filtering.
TargetTextSourceText
ModelFiltered
ParallelCorpus
Extraction
Alignment,Phrase
SMTDecoder
TranslationModel
LanguageModelMonolingualCorpus
CountingSmoothing
Filtering
ModelTarget?Bridge
ModelSource?Bridge
Figure 1: Triangulated filtering in SMT systems
Figure 1 illustrates our triangulation approach.
Two bridge models are first constructed: one from
the source language to the bridge language, and an-
other from the target language to the bridge lan-
guage. Then, we use these two models to filter the
original source-target model. For each phrase pair
in the original table, we try to find a common link
in these bridge models to connect both phrases. If
such links do not exist, we remove the entry from
the table. The probability values in the table remain
129
unchanged. The reduced table can be used in place
of the original one in the SMT system.
There are various forms of links that can be used
as our evidence for the filtering process. One obvi-
ous form is complete phrases in the bridge language,
which means, for each phrase pair in the model to
be filtered, we should look for a third phrase in the
bridge language that can relate the two phrases in the
pair.
This approach to filtering examines each phrase
pair presented in the phrase-table one by one. For
each phrase pair, we collect the corresponding trans-
lations using the models for translation into a third
language. If both phrases can be mapped to some
phrases in the bridge language, but to different ones,
we should remove it from the model. It is also possi-
ble that neither of the phrases appear in correspond-
ing bridge models. In this case, we consider the
bridge models insufficient for making the filtering
decision and prefer to keep the pair in the table.
The way a decoder constructs translation hypothe-
ses is directly related to the weights for different
model features in a SMT system, which are usually
optimized for a given set of models with minimum
error rate training (MERT) (Och, 2003) to achieve
better translation performance. In other words, the
weights obtained for a model do not necessarily ap-
ply to another model. Since the triangulated filter-
ing method removes a part of the model, it is impor-
tant to readjust the feature weights for the reduced
phrase-table.
4 Experimental design
All the text data used in our experiments are
from Release v3 of ?European Parliament Proceed-
ings Parallel Corpus 1996-2006? (Europarl) cor-
pus (Koehn, 2005). We mainly investigated trans-
lations from Spanish to English. There are enough
structural differences in these two language to in-
troduce some noise in the phrase table. French,
Portuguese, Danish, German and Finnish were used
as bridge languages. Portuguese is very similar to
Spanish and French somewhat less so. Finnish is un-
related and fairly different typologically with Danish
and German occupying the middle ground. In addi-
tion, we also present briefly the results on German-
English translations with Dutch, Spanish and Danish
as bridges.
For the Spanish-English pair, three translation
models were constructed over the same parallel cor-
pora. We acquired comparable data sets by draw-
ing several subsets from the same corpus according
to various maximal sentence lengths. The subsets
Tokens
Model Sentences Spanish English
EP-20 410,487 5,220,142 5,181,452
EP-40 964,687 20,820,067 20,229,833
EP-50 1,100,813 26,731,269 25,867,370
Europarl 1,304,116 37,870,751 36,429,274
Table 1: Europarl subsets for building the Spanish-
English SMT system
we used in the experiments are presented by ?EP-
20?, ?EP-40? and ?EP-50?, in which the numbers
indicate the maximal sentence length in respective
Europarl subsets. Table 1 lists the characteristics
of the Spanish-English subsets. Although the max-
imal sentence length in these sets is far less than
that of the whole corpus (880 tokens), EP-50 al-
ready includes nearly 85% of Spanish-English sen-
tence pairs from Europarl.
The translations models, both the models to be
filtered and the bridge models, were generated
from compatible Europarl subsets using the Moses
toolkit (Koehn et al, 2007) with the most basic con-
figurations. The feature weights for the Spanish-
English translation models were optimized over a
development set of 500 sentences using MERT to
maximize BLEU (Papineni et al, 2001).
The triangulated filtering algorithm was applied
to each combination of a translation model and a
third language. The reordering models were also
filtered according to the phrase-table. Only those
phrase pairs that appeared in the phrase-table re-
mained in the reordering table. We rerun the MERT
process solely based on the remaining entries in the
filtered tables. Each table is used to translate a set of
2,000 sentences of test data (from the shared task of
the third Workshop on Statistical Machine Transla-
tion, 2008 1). Both the test set and the development
data set have been excluded from the training data.
We evaluated the proposed phrase-table filtering
1For details, see
http://www.statmt.org/wmt08/shared-task.html
130
method mainly from two points of view: the effi-
ciency of systems with filtered tables and the quality
of output translations produced by the systems.
5 Results
5.1 System efficiency
Often the question of machine translation is not only
how to produce a good translation, but also how
to produce it quickly. To evaluate the system ef-
ficiency, we measured both storage space and time
consumption. For recording the computation time,
we run an identical of installation of the decoder
with different models and then measure the average
execution time for the given translation task.
In Table 2, we give the number of entries in each
phrase table (N ), and the physical file size of the
phrase table (SPT ) and the reordering table (SRT )
(without any compression or binarization), Tl, the
time for the program to load phrase tables and Tt the
time to translate the complete test set. We also high-
lighted the largest and the smallest reduction from
each group.
All filtered models showed significant reductions
in size. The greatest reduction of model sizes, taking
both phrase-table and reordering table into account,
is nearly 11 gigabytes for filtering the largest model
(EP-50) with a Finnish bridge, which leads to the
maximal time saving of 939 seconds, or almost 16
minutes, for translating two thousand sentences.
The reduction rates from two larger models are
very close to each other whereas the filtered table
scaled down the most significantly on the smallest
model (EP-20), which was in fact constructed over a
much smaller subset of Europarl corpus, consisting
of less than half of the sentences pairs in the other
two Europarl subsets. Compared to the larger Eu-
roparl subsets, the small data set is expected to pro-
duce more errors through training as there is much
less relevant data for the machine learning algorithm
to correctly extract useful information from. Conse-
quently, there are more noisy entries in this small
model, and therefore more entries to be removed. In
addition, the filtering is done by exact matching of
complete phrases, which presumably happens much
less frequently even for correctly paired phrase pairs
in the very limited data supplied by the smallest
training set. For the same reason, the distinction be-
tween different bridge languages was less clear for
this small model.
Due to hardware limitation, we are not able to
fit the unfiltered phrase tables completely into the
memory. Every table was filtered based on the given
input so only a small portion of each table was
loaded into memory. This may diminish the differ-
ence between the original and the filtered table to a
certain degree. The relative time consumptionnev-
ertheless agrees with the reduction in size: phrase
tables from the smallest model showed the most re-
duction for both loading the models and processing
the translations.
For loading time, we count the time it takes to
start and to load the bilingual phrase-tables plus re-
ordering tables and the monolingual language model
into the memory. The majority of the loading time
for the smallest model, even before filtering, has
been used for loading language models and other
start-up processes, could not be reduced as much as
the reduction on table size.
5.2 Translation quality
Bridge EP-20 EP-40 EP-50
? 26.62 31.43 31.68
pt 28.40 32.90 33.93
fr 28.28 32.69 33.47
da 28.48 32.47 33.88
de 28.05 32.65 33.13
fi 28.02 31.91 33.04
Table 3: BLEU scores of translations using filtered phrase
tables
Efficiency aside, a translation system should be
able to produce useful translation. It is important
to verify that the filtering approach does not affect
the translation quality of the system. Table 3 show
the BLEU scores of each translation acquired in the
experiments.
Between translation models of different sizes,
there are obvious performance gaps. Different
bridge languages can cause different effects on per-
formance. However, the translation qualities from
a single model are fairly close to each other. We
therefore take it that the effect of the triangulation
approach is rather robust across translation models
of different sizes.
131
Time Table Size
Model+Bridge Tl (s) Tt (s) N SPT (byte) SRT (byte)
EP-20+ ? 55 3529 7,599,271 953M 717M
EP-20+ pt 53 2826 1,712,508 (22.54%) 198M 149M
EP-20+ fr 48 2702 1,536,056 (20.21%) 172M 131M
EP-20+ da 52 2786 1,659,067 (21.83%) 186M 141M
EP-20+ de 43 2732 1,260,524 (16.59%) 132M 101M
EP-20+ fi 47 2670 1,331,323 (17.52%) 147M 111M
EP-40+ ? 65 3673 19,199,807 2.5G 1.9G
EP-40+ pt 50 3091 8,378,517 (43.64%) 1.1G 1.8G
EP-40+ fr 46 3129 8,599,708 (44.79%) 1.1G 741M
EP-40+ da 42 3050 6,716,304 (34.98%) 842M 568M
EP-40+ de 46 3069 6,113,769 (31.84%) 725M 492M
EP-40+ fi 40 2889 4,473,483 (23.30%) 533M 353M
EP-50+ ? 140 4130 54,382,715 7.1G 5.4G
EP-50+ pt 78 3410 13,225,654 (24.32%) 1.6G 1.3G
EP-50+ fr 97 3616 24,057,849 (44.24%) 3.0G 2.3G
EP-50+ da 81 3418 12,547,839 (23.07%) 1.5G 1.2G
EP-50+ de 95 3488 15,938,151 (29.31%) 1.9G 1.5G
EP-50+ fi 71 3191 7,691,904 (17.75%) 895M 677M
Table 2: System efficiency: time consumption and phrase-table size
It is obvious that the best systems are usually
NOT from the filtered tables that preserved the most
entries from the original. All the filtered models
showed some improvement in quality with updated
model weights. Mostly around 1.5 BLEU points, the
increases ranged from 0.36 to 2.25. Table 4 gives a
set of translations from the experiments. The unfil-
tered baseline system inserted the negative by mis-
take while all the filtered systems are able to avoid
this. It indicates that there are indeed noisy entries
affecting translation quality in the original table. We
were able to achieve better translations by eliminat-
ing noisy entries.
The filtering methods indeed tend to remove en-
tries composed of long phrases. Table 5 lists the
average length of phrases in several models. Both
source phrases and target phrases are taken into ac-
count. The best models have shortest phrases on av-
erage. Discarding such entries seems to be neces-
sary. This is consistent with the findings in (Koehn,
2003) that phrases longer than three words improve
performance little for training corpora of up to 20
million words.
Quality gains appeared to converge in the results
across different bridge languages while the original
models became larger. Translations generated us-
ing large models filtered with different bridge lan-
Bridge EP-20 EP-40 EP-50
? 3.776 4.242 4.335
pt 3.195 3.943 3.740
fr 3.003 3.809 3.947
da 3.005 3.74 3.453
de 2.535 3.501 3.617
fi 2.893 3.521 3.262
Table 5: Average phrase length
guages are less diverse. Meanwhile, the degradation
is less for a larger model. It is reasonable to expect
improvements for extremely large models with arbi-
trary bridge languages. For relatively small models,
the selection of bridge languages would be critical
for the effect of our approach.
5.3 Language clustering
To further understand how the triangulated filter-
ing approach worked and why it could work as it
did, we examined a randomly selected phrase table
fragment through the experiments. The segment in-
cluded 10 potential English translations of the same
Spanish word ?fabricantes?, the plural form of the
word ?fabricante? (manufacturer).
Table 6 shows the filtering results on a randomly
selected segment from the original ?EP-40? model,
including 10 English translations of the same source
132
source As??, se van modificando poco a poco los principios habituales del Estado de derecho por influencia de una
concepcin extremista de la lucha con tra las discriminaciones..
ref thus , the usual principles of the rule of law are being gradually altered under the influence of an extremist
approach to combating discrimination.
baseline we are not changing the usual principles of the rule of law from the influence of an extremist approach in
the fight against discrimination.
pt so , are gradually changing normal principles of the rule of law by influence of an extremist conception of
the fight against discrimination.
fr so , we are gradually changing the usual principles of the rule of law by influence of an extremist conception
of the fight against discrimination.
da so , are gradually changing the usual principles of the rule of law by influence of an extremist conception
of the fight against discrimination.
de thus , we are gradually altering the usual principles of the rule of law by influence of an extremist concep-
tion of the fight against discrimination.
fi so , are gradually changing normal principles of the rule of law by influence of an extremist conception of
the fight against discrimination.
Table 4: Examples
fabricantes pt fr da de fi
a manufacturer X X X X 4
battalions X X X 3
car manufacturers have 0
car manufacturers X X X X X 5
makers X X X 3
manufacturer X X X X X 5
manufacturers X X X X X 5
producers are X X X 3
producers need 0
producers X X X X X 5
Table 6: Phrase-table entries before and after filtering a
model with different bridges
word ?fabricantes?. X indicates that the corre-
sponding English phrase remained in the table after
triangulated filtering with the corresponding bridge
language. We also counted the number of tables that
included each phrase pair.
Regardless of the bridge language, the triangu-
lated filtering approach had removed those entries
that are clearly noise. Meanwhile, entries which
are surely correct were always preserved in the fil-
tered tables. The results of using different bridge
languages turned out to be consistent on these ex-
treme cases. The 5 filtering processes agreed on six
out of ten pairs.
As for the other 4 pairs, the decisions were differ-
ent using different bridge languages. The remaining
entries were always different when the bridge was
changed. None of the languages led to the identi-
cal eliminations. None of the cases excludes all er-
rors. Apparently, the selection of bridge languages
had immediate effects on the filtering results.
 31
 31.2
 31.4
 31.6
 31.8
 32
 32.2
 32.4
 32.6
 32.8
 33
 4  6  8  10  12  14  16  18  20
BL
EU
 (%
)
Phrase-table Entries (Mil.)
Portugese
French
Danish
German
Finnish
Baseline
Figure 2: Clustering of bridge languages
We compared two factors of these filtered tables:
their sizes and the corresponding BLEU scores. Fig-
ure 2 shows interesting signs of language similar-
ity/dissimilarity. There are apparently two groups
of languages having extremely close performance,
which happen to fall in two language groups: Ger-
manic (German and Danish) and Romance (French
and Portuguese). The Romance group was as-
sociated with larger filtered tables that produced
slightly better translations. The filtered tables cre-
ated with Germanic bridge languages contained ap-
133
proximately 2 million entries less than Romance
groups. The translation quality difference between
these two groups was within 1 point of BLEU.
Observed from this figure, it seems that the trans-
lation quality was connected to the similarity be-
tween the bridge language and the source language.
The closer the bridge is to the source language, the
better translations it may produce. For instance, Por-
tuguese led to a filtered table that produced the best
translations. On the other hand, the more different
the bridge languages compared to the source, the
larger portion of the phrase-table the filtering algo-
rithm will remove. The table filtered with German
was the smallest in the four cases.
Finnish, a language that is unrelated to others, was
associated with distinctive results. The size of the
table filtered with Finnish is only 23% of the orig-
inal, almost half of the table generated with Por-
tuguese. Finnish has extremely rich morphology,
hence a great many word-forms, which would make
exact matching in bridge models less likely to hap-
pen. Many more phrase pairs in the original table
were removed for this reason even though some of
these entries were beneficial for translations. Even
though the improvement on translation quality due
to the Finnish bridge was less significant than the
others, it is clear that triangulated filtering retained
the useful information from the original model.
5.4 Further filtering
The filtering decision with a bridge language on a
particular phrase pair is fixed: either to keep the en-
try or to discard it. It is difficult to adjust the system
to work differently. However, as the triangulated fil-
tering procedure does not consider probability distri-
butions in the models, it is possible to further filter
the tables according to the probabilities.
The phrase pairs are associated with values com-
puted from the given set of feature weights and
sorted, so that we can remove any portions of the
remain entries based on the values. Each generated
table is used to translate the test set again. Fig-
ure 3 shows BLEU scores of the translation out-
puts produced with tables derived from the ?EP-50?
model with respect to their sizes. We also included
the curve of probability-based filtering alone as the
baseline.
The difference between filtered tables at the same
 24
 26
 28
 30
 32
 0  10  20  30  40  50
BL
EU
 (%
)
Phrase-table Entries (Mil.)
BaselinePortugeseFrenchDanishGermanFinnish
Figure 3: Combining probability-based filtering
size can be over 6 BLEU points, which is a re-
markable advantage for the triangulated filtering ap-
proach always producing better translations. The
curves of the triangulated filtered models are clearly
much steeper than that of the naive pruned ones.
Data in these filtered models are more compact than
the original model before any filtering. The triangu-
lated filtered phrase-tables contain more useful in-
formation than a normal phrase-table of the same
size. The curves representing the triangulated filter-
ing performance are always on the left of the original
curves.
We are able to use less than 6% of the original
phrase table (40% of the table filtered with Finnish)
to obtain translations with the same quality as the
original. The extreme case, using only 1.4% of the
original table, leads to a reasonable BLEU score, in-
dicating that most of the output sentences should
still be understandable. In this case, the overall size
of the phrase table and the reordering table was less
than 100 megabytes, potentially feasible for mobile
devices, whereas the original models took nearly
12.5 gigabytes of disk space.
5.5 Different source language
Bridge EP-40 EP-50
? 5.1G 26.92 6.5G 27.23
Dutch 562M 27.11 1.3G 28.14
Spanish 3.0G 27.28 3.6G 28.09
Danish 505M 28.04 780M 28.21
Table 7: Filtered German-English systems (Size and
BLEU)
134
In addition to Spanish-English translation, we
also conducted experiments on German-English
translation. The results, shown in Table 7, appear
consistent with the results of Spanish-English trans-
lation. Translations in most cases have performance
close to the original unfiltered models, whereas the
reduction in phrase-table size ranged from 40% to
85%. Meanwhile, translation speed has been in-
creased up to 17%.
Due to German?s rich morphology, the unfil-
tered German-English models contain many more
entries than the Spanish-English ones constructed
from similar data sets. Unlike the Spanish-English
models, the difference between ?EP-40? and ?EP-
50? was not significant. Neither was the difference
between the impacts of the filtering in terms of trans-
lation quality. In addition, German and English are
so dissimilar that none of the three bridge languages
we chose turned out to be significantly superior.
6 Conclusions
We highlighted one problem of the state-of-the-art
SMT systems that was generally neglected: the
noise in the translation models. Accordingly, we
proposed triangulated filtering methods to deal with
this problem. We used data in a third language as ev-
idence to locate the less probable items in the trans-
lation models so as to obtain the intersection of in-
formation extracted from multilingual data. Only
the occurrences of complete phrases were taken into
account. The probability distributions of the phrases
have not been considered so far.
Although the approach was fairly naive, our ex-
periments showed it to be effective. The approaches
were applied to SMT systems built with the Moses
toolkit. The translation quality was improved at least
1 BLEU for all 15 cases (filtering 3 different models
with 5 bridge languages). The improvement can be
as much as 2.25 BLEU. It is also clear that the best
translations were not linked to the largest translation
models. We also sketched a simple extension to the
triangulated filtering approach to further reduce the
model size, which allows us to generate reasonable
results with only 1.4% of the entries from the origi-
nal table.
The results varied for different bridge languages
as well as different models. For translation from
Spanish to English, Finnish, the most distinctive
bridge language, appeared to be a more effective
intermediate language which could remove more
phrase pair entries while still improving the transla-
tion quality. Portuguese, the most close to the source
language, always leads to a filtered model that pro-
duces the best translations. The selection of bridge
languages has more obvious impact on the perfor-
mance of our approach when the size of the model
to filter was larger.
The work gave one instance of the general ap-
proach described in Section 3. There are several
potential directions for continuing this work. The
most straightforward one is to use our approaches
with more different languages, such as Chinese and
Arabic, and incompatible corpora, for example, dif-
ferent segments of Europarl. The main focus of such
experiments should be verifying the conclusions we
had in this paper.
Acknowledgments
This work was supported by European Community
through the EuroMatrix project funded under the
Sixth Framework Programme and the EuroMatrix
Plus project funded under the Seventh Framework
Programme for Research and Technological Devel-
opment.
References
Yu Chen, Andreas Eisele, and Martin Kay. 2008. Im-
proving Statistical Machine Translation Efficiency by
Triangulation. In the 6th International Conference
on Language Resources and Evaluation (LREC ?08),
May.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In the 45th Annual Meet-
ing of the Association for Computational Linguistics,
Prague, Czech, June.
Karim Filali and Jeff Bilmes. 2005. Leveraging Multi-
ple Languages to Improve Statistical MT Word Align-
ments. In IEEE Automatic Speech Recognition and
Understanding (ASRU), Cancun, Mexico, November.
J. Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving Translation Qual-
ity by Discarding Most of the Phrasetable. In the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
135
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic, June.
Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Translation,
12(1-2):3?23.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit 2005.
Shankar Kumar, Franz Josef Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 42?50, Prague, Czech.
Franz Josef Och and Hermann Ney. 2001. Statistical
multi-source translation. In MT Summit VIII, Santiago
de Compostela, Spain.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Michel Simard. 1999. Text-translation alignment: Three
languages are better than two. In EMNLP/VLC-99,
College Park, MD, June.
136
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 77?81,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Further Experiments with Shallow Hybrid MT Systems
Christian Federmann1, Andreas Eisele1, Hans Uszkoreit1,2,
Yu Chen1, Sabine Hunsicker1, Jia Xu1
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit,yuchen,sabine.hunsicker,jia.xu}@dfki.de
Abstract
We describe our hybrid machine trans-
lation system which has been developed
for and used in the WMT10 shared task.
We compute translations from a rule-
based MT system and combine the re-
sulting translation ?templates? with par-
tial phrases from a state-of-the-art phrase-
based, statistical MT engine. Phrase sub-
stitution is guided by several decision
factors, a continuation of previous work
within our group. For the shared task,
we have computed translations for six lan-
guage pairs including English, German,
French and Spanish. Our experiments
have shown that our shallow substitu-
tion approach can effectively improve the
translation result from the RBMT system;
however it has also become clear that a
deeper integration is needed to further im-
prove translation quality.
1 Introduction
In recent years the quality of machine translation
(MT) output has improved greatly, although each
paradigm suffers from its own particular kind of
errors: statistical machine translation (SMT) of-
ten shows poor syntax, while rule-based engines
(RBMT) experience a lack in vocabulary. Hybrid
systems try to avoid these typical errors by com-
bining techniques from both paradigms in a most
useful manner.
In this paper we present the improved version of
the hybrid system we developed last year?s shared
task (Federmann et al, 2009). We take the out-
put from an RBMT engine as basis for our hybrid
translations and substitute noun phrases by trans-
lations from an SMT engine. Even though a gen-
eral increase in quality could be observed, our sys-
tem introduced errors of its own during the substi-
tution process. In an internal error analysis, these
degradations were classified as follows:
- the translation by the SMT engine is incorrect
- the structure degrades through substitution
(because of e.g. capitalization errors, double
prepositions, etc.)
- the phrase substitution goes astray (caused by
alignment problems, etc.)
Errors of the first class cannot be corrected, as
we have no way of knowing when the translation
by the SMT engine is incorrect. The other two
classes could be eliminated, however, by introduc-
ing additional steps for pre- and post-processing
as well as improving the hybrid algorithm itself.
Our current error analysis based on the results of
this year?s shared task does not show these types
of errors anymore.
Additionally, we extended our coverage to also
include the language pairs English?French and
English?Spanish in both directions as well as
English?German, compared to last year?s initial
experiments for German?English only. We were
able to achieve an increase in translation quality
for this language set, which shows that the substi-
tution method works for different language config-
urations.
2 Architecture
Our hybrid translation system takes translation
output from a) the Lucy RBMT system (Alonso
and Thurmair, 2003) and b) a Moses-based SMT
system (Koehn et al, 2007). We then identify
noun phrases inside the rule-based translation and
compute the most likely correspondences in the
statistical translation output. For these, we apply a
factored substitution method that decides whether
the original RBMT phrase should be kept or rather
be replaced by the Moses phrase. As this shallow
substitution process may introduce problems at
77
phrase boundaries, we afterwards perform several
post-processing steps to cleanup and finalize the
hybrid translation result. A schematic overview
of our hybrid system and its main components is
given in figure 1.
Figure 1: Schematic overview of the hybrid MT
system architecture.
2.1 Input to the Hybrid System
Lucy RBMT System We obtain the translation
as well as linguistic structures from the RBMT
system. An internal evaluation has shown that
these structures are usually of a high quality which
supports our initial decision to consider the RBMT
output as an appropriate ?template? for our hybrid
translation approach. The Lucy translation output
can include additional markup that allows to iden-
tify unknown words or other, local phenomena.
The Lucy system is a transfer-based MT system
that performs translation in three phases, namely
analysis, transfer, and generation. Intermediate
tree structures for each of the translation phases
can be extracted from the Lucy system to guide
the hybrid system. Sadly, only the 1-best path
through these three phases is given, so no alterna-
tive translation possibilities can be extracted from
the given data; a fact that clearly limits the poten-
tial for more deeply integrated hybrid translation
approaches. Nevertheless, the availability of the
1-best trees already allows to improve the transla-
tion quality of the RBMT system as we will show
in this paper.
Moses SMT System We used a state-of-the-art
Moses SMT system to create statistical phrase-
based translations of our input text. Moses has
been modified so that it returns the translation re-
sults together with the bidirectional word align-
ments between the source texts and the transla-
tions. Again, we make use of markup which helps
to identify unknown words as these will later guide
the factored substitution method. Both of the
translation models and the language models within
our SMT systems were only trained with lower-
cased and tokenized Europarl training data. The
system used sets of feature weights determined us-
ing data sets also from Europarl (test2008). In
addition, we used LDC gigaword corpus to train
large scale n-gram language models to be used in
our hybrid system. We tokenized the source texts
using the standard tokenizers available from the
shared task website. The SMT translations are re-
cased before being fed into the hybrid system to-
gether with the word alignment information.The
hybrid system can easily be adapted to support
other statistical translation engines. If the align-
ment information is not available, a suitable align-
ment tool would be necessary to compute it as the
alignment is a key requirement for the hybrid sys-
tem.
2.2 Aligning RBMT and SMT Output
We compute alignment in several components of
the hybrid system, namely:
source-text-to-tree: we first find an alignment
between the source text and the correspond-
ing analysis tree(s). As Lucy tends to sub-
divide large sentences into several smaller
units, it sometimes becomes necessary to
align more than one tree structure to a given
source sentence.
analysis-transfer-generation: for each of the
analysis trees, we re-construct the path from
its tree nodes, via the transfer tree, and their
corresponding generation tree nodes.
tree-to-target-text: similarly to the first align-
ment process, we find a mapping between
generation tree nodes and the actual transla-
tion output of the RBMT system.
source-text-to-tokenized: as the Lucy RBMT
system works on non-tokenized input text
and our Moses system takes tokenized input,
78
we need to align the source text to its tok-
enized form.
Given the aforementioned alignments, we can then
correlate phrases from the rule-based translation
with their counterparts from the statistical trans-
lation, both on source or target side. As our
hybrid approach relies on the identification of
such phrase pairs, the computation of the different
alignments is critical to obtain good combination
performance.
Please note that all these tree-based alignments
can be computed with a very high accuracy. How-
ever, due to the nature of statistical word align-
ment, the same does not hold for the alignment
obtained from the Moses system. If the alignment
process has produced erroneous phrase tables, it is
very likely that Lucy phrases and their ?aligned?
SMT matches simply will not fit. Or put the other
way round: the better the underlying SMT word
alignment, the greater the potential of the hybrid
substitution approach.
2.3 Factored Substitution
Given the results of the alignment process, we can
then identify ?interesting? phrases for substitution.
Following our experimental setup from last year?s
shared task, we again decided to focus on noun
phrases as these seem to be best-suited for in-place
swapping of phrases. Our initial assumption is that
SMT phrases are better on a lexical level, hence
we aim to replace Lucy?s noun phrases by their
Moses counterparts.
Still, we want to perform the substitution in a
controlled manner in order to avoid problems or
non-matching insertions. For this, we have (man-
ually) derived a set of factors that are checked for
each of the phrase pairs that are processed. The
factors are described briefly below:
identical? simply checks whether two candidate
phrases are identical.
too complex? a Lucy phrase is ?too complex?
to substitute if it contains more than 2
embedded noun phrases.
many-to-one? this factor checks if a Lucy phrase
containing more than one word is mapped to
a Moses phrase with only one token.
contains pronoun? checks if the Lucy phrase
contains a pronoun.
contains verb? checks if the Lucy phrase con-
tains a verb.
unknown? checks whether one of the phrases is
marked as ?unknown?.
length mismatch computes the number of words
for both phrases and checks if the absolute
difference is too large.
language model computes language model
scores for both phrases and checks which is
more likely according to the LM.
All of these factors have been designed and ad-
justed during an internal development phase using
data from previous shared tasks.
2.4 Post-processing Steps
After the hybrid translation has been computed,
we perform several post-processing steps to clean
up and finalize the result:
cleanup first, we perform basic cleanup opera-
tions such as whitespace normalization, cap-
italizing the first word in each sentence, etc.
multi-words then, we take care of proper han-
dling of multi-word expressions. Using the
tree structures from the RBMT system we
eliminate superfluous whitespace and join
multi-words, even if they were separated in
the SMT phrase.
prepositions finally, we give prepositions a spe-
cial treatment. Experience from last year?s
shared task had shown that things like double
prepositions contributed to a large extent to
the amount of avoidable errors. We tried to
circumvent this class of error by identifying
the correct prepositions; erroneous preposi-
tions are removed.
3 Hybrid Translation Analysis
We evaluated the intermediate outputs using
BLEU (Papineni et al, 2001) against human refer-
ences as in table 3. The BLEU score is calculated
in lower case after the text tokenization. The trans-
lation systems compared are Moses, Lucy, Google
and our hybrid system with different configura-
tions:
Hybrid: we use the language model with case
information and substitute some NPs in Lucy
outputs by Moses outputs.
Hybrid LLM: same as Hybrid but we use a
larger language model.
79
Table 1: Intermediate results of BLEU[%] scores for WMT10 shared task.
System de?en en?de fr?en en?fr es?en en?es
Moses 18.32 12.66 22.26 20.06 24.28 24.72
Lucy 16.85 12.38 18.49 17.61 21.09 20.85
Google 25.64 18.51 28.53 28.70 32.77 32.20
Hybrid 17.29 13.05 18.92 19.58 22.53 23.55
Hybrid LLM 17.37 13.73 18.93 19.76 22.61 23.66
Hybrid SG 17.43 14.40 19.67 20.55 24.37 24.99
Hybrid NCLM 17.38 14.42 19.56 20.55 24.41 24.92
Hybrid SG: same as Hybrid but the NP substitu-
tions are based on Google output instead of
Moses translations.
Hybrid NCLM: same as Hybrid but we use the
language model without case information.
We participated in the translation evaluation in
six language pairs: German to English (de?en),
English to German (en?de), French to English
(fr?en), English to French (en?fr), Spanish to
English (es?en) and English to Spanish (en?es).
As shown in table 3, the Moses translation sys-
tem achieves better results overall than the Lucy
system does. Google?s system outperforms other
systems in all language pairs. The hybrid transla-
tion as described in section 2 improves the Lucy
translation quality with a BLEU score up to 2.7%
absolutely.
As we apply a larger language model or a lan-
guage model without case information, the trans-
lation performance can be improved further. One
major problem in the hybrid translation is that the
Moses outputs are still not good enough to replace
the Lucy outputs, therefore we experimented on
a hybrid translation of Google and Lucy systems
and substitute some unrelaible NP translations by
the Google?s translations. The results in the line
of ?Hybrid SG? shows that the hybrid translation
quality can be enhanced if the translation system
where we select substitutions is better.
4 Internal Evaluation of Results
In the analysis of the remaining issues, the fol-
lowing main sources of problems can be distin-
guished:
- Lucy?s output contains structural errors that
cannot be fixed by the chosen approach.
- Lucy results contain errors that could have
been corrected by alternative expressions
from SMT, but the constraints in our system
were too restrictive to let that happen.
- The SMT engine we use generates subopti-
mal results that find their way into the hybrid
result.
- SMT results that are good are incorporated
into the hybrid results in a wrong way.
We have inspected a part of the results and classi-
fied the problems according to these criteria. As
this work is still ongoing, it is too early to report
numerical results for the relative frequencies of the
different causes of the error. However, we can
already see that three of these four cases appear
frequently enough to justify further attention. We
observed several cases in which the parser in the
Lucy system was confused by unknown expres-
sions and delivered results that could have been
significantly improved by a more robust parsing
approach. We also encountered several cases in
which an expression from SMT was used although
the original Lucy output would have been better.
Also we still observe problems finding to correct
correspondences between Lucy output and SMT
output, which leads to situations where material is
inserted in the wrong place, which can lead to the
loss of content words in the output.
5 Conclusion and Outlook
In our contribution to the shared task we have ap-
plied the hybrid architecture from (Federmann et
al., 2009) to six language pairs. We have identi-
fied and fixed many of the problems we had ob-
served last year, and we think that, in addition to
the increased coverage in laguage pairs, the overall
quality has been significantly increased.
However, in the last section we characterized
three main sources of problems that will require
further attention. We will address these problems
in the near future in the following way:
80
1. We will investigate in more detail the align-
ment issue that leads to occasional loss of
content words, and we expect that a careful
inspection and correction of the code will in
all likelihood give us a good remedy.
2. The problem of picking expressions from the
SMT output that appear more probable to the
language model although they are inferior to
the original expression from the RBMT sys-
tem is more difficult to fix. We will try to find
better thresholds and biases that can at least
reduce the number of cases in which this type
of degradation happen.
3. Finally, we will also address the robustness
issue that leads to suboptimal structures from
the RBMT engine caused by parsing failures.
Our close collaboration with Lucy enables us to
address these issues in a very effective way via the
inspection and classification of intermediate struc-
tures and, if these structures indicate parsing prob-
lems, the generation of variants of the input sen-
tence that facilitate correct parsing.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme. The authors want to thank Michael
Jellinghaus and Bastian Simon for help with the
inspection of intermediate results and classifica-
tion of errors.
References
Juan A. Alonso and Gregor Thurmair. 2003. The Com-
prendium Translator system. In Proc. of the Ninth
MT Summit.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combination
using factored word substitution. In Proceedings of
the Fourth Workshop on Statistical Machine Transla
tion, pages 70?74, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL Demo and Poster Sessions, pages 177?
180, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
81
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 102?109,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Using Deep Belief Nets for Chinese Named Entity Categorization 
Yu Chen1, You Ouyang2, Wenjie Li2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{csyouyang, cswjli}@comp.polyu.edu.hk
Abstract 
Identifying named entities is essential in 
understanding plain texts. Moreover, the 
categories of the named entities are indicative 
of their roles in the texts. In this paper, we 
propose a novel approach, Deep Belief Nets 
(DBN), for the Chinese entity mention 
categorization problem. DBN has very strong 
representation power and it is able to 
elaborately self-train for discovering 
complicated feature combinations. The 
experiments conducted on the Automatic 
Context Extraction (ACE) 2004 data set 
demonstrate the effectiveness of DBN. It 
outperforms the state-of-the-art learning 
models such as SVM or BP neural network. 
1 Introduction 
Named entities (NE) are defined as the names of 
existing objects, such as persons, organizations 
and etc. Identifying NEs in plain texts provides 
structured information for semantic analysis. 
Hence the named entity recognition (NER) task 
is a fundamental task for a wide variety of 
natural language processing applications, such as 
question answering, information retrieval and etc. 
In a text, an entity may either be referred to by a 
common noun, a noun phrase, or a pronoun. 
Each reference of the entity is called a mention. 
NER indeed requires the systems to identify 
these entity mentions from plain texts. The task 
can be decomposed into two sub-tasks, i.e., the 
identification of the entities in the text and the 
classification of the entities into a set of pre-
defined categories. In the study of this paper, we 
focus on the second sub-task and assume that the 
boundaries of all the entity mentions to be 
categorized are already correctly identified. 
In early times, NER systems are mainly based 
on handcrafted rule-based approaches. Although 
rule-based approaches achieved reasonably good 
results, they have some obvious flaws. First, they 
require exhausted handcraft work to construct a 
proper and complete rule set, which partially 
expressing the meaning of entity. Moreover, 
once the interest of task is transferred to a 
different domain or language, rules have to be 
revised or even rewritten. The discovered rules 
are indeed heavily dependent on the task 
interests and the particular corpus. Finally, the 
manually-formatted rules are usually incomplete 
and their qualities are not guaranteed. 
Recently, more attentions are switched to the 
applications of machine learning models with 
statistic information. In this camp, entity 
categorization is typically cast as a multi-class 
classification process, where the named entities 
are represented by feature vectors. Usually, the 
vectors are abstracted by some lexical and 
syntactic features instead of semantic feature. 
Many learning models, such as Support Vector 
Machine (SVM) and Neural Network (NN), are 
then used to classify the entities by their feature 
vectors. 
Entity categorization in Chinese attracted less 
attention when compared to English or other 
western languages. This is mainly because the 
unique characteristics of Chinese. One of the 
most common problems is the lack of boundary 
information in Chinese texts. For this problem, 
character-based methods are reported to be a 
possible substitution of word-based methods. As 
to character-based methods, it is important to 
study the implicit combination of characters.  
In our study, we explore the use of Deep 
Belief Net (DBN) in character-based entity 
categorization. DBN is a neural network model 
which is developed under the deep learning 
architecture. It is claimed to be able to 
automatically learn a deep hierarchy of the input 
features with increasing levels of abstraction for 
the complex problem. In our problem, DBN is 
used to automatically discover the complicated 
composite effects of the characters to the NE 
categories from the input data. With DBN, we 
need not to manually construct the character 
combination features for expressing the semantic 
relationship among characters in entities. 
Moreover, the deep structure of DBN enables the 
possibility of discovering very sophisticated 
102
combinations of the characters, which may even 
be hard to discover by human. 
The rest of this paper is organized as follow. 
Section 2 reviews the related work on name 
entity categorization. Section 3 introduces the 
methodology of the proposed approach. Section 
4 provides the experimental results. Finally, 
section 5 concludes the whole paper. 
2 Related work 
Over the past decades, NER has evolved from 
simple rule-based approaches to adapted self-
training machine learning approaches. 
As early rule-based approaches, MacDonald 
(1993) utilized local context, which implicate 
internal and external evidence, to aid on 
categorization. Wacholder (1997) employed an 
aggregation of classification method to capture 
internal rules. Both used hand-written rules and 
knowledge bases. Later, Collins (1999) adopted 
the AdaBoost algorithm to find a weighted 
combination of simple classifiers. They reported 
that the combination of simple classifiers can 
yield some powerful systems with much better 
performances. As a matter of fact, these methods 
all need manual studies on the construction of the 
rule set or the simple classifiers. 
Machine learning models attract more 
attentions recently. Usually, they train 
classification models based on context features. 
Various lexical and syntactic features are 
considered, such as N-grams, Part-Of-Speech 
(POS), and etc. Zhou and Su (2002) integrated 
four different kinds of features, which convey 
different semantic information, for a 
classification model based on the Hidden 
Markov Model (HMM). Koen (2006) built a 
classifier with the Conditional Random Field 
(CRF) model to classify noun phrases in a text 
with the WordNet SynSet. Isozaki and Kazawa 
(2002) studied the use of SVM instead. 
There were fewer studies in Chinese entity 
categorization. Guo and Jiang (2005) applied 
Robust Risk Minimization to classify the named 
entities. The features include seven traditional 
lexical features and two external-NE-hints based 
features. An important result they reported is that 
character-based features can be as good as word-
based features since they avoid the Chinese word 
segmentation errors. In (Jing et al, 2003), it was 
further reported that pure character-based models 
can even outperform word-based models with 
character combination features.  
Deep Belief Net is introduced in (Hinton et al, 
2006). According to their definition, DBN is a 
deep neural network that consists of one or more 
Restricted Boltzmann Machine (RBM) layers 
and a Back Propagation (BP) layer. This multi-
layer structure leads to a strong representation 
power of DBN. Moreover, DBN is quite efficient 
by using RBM to implement the middle layers, 
since RBM can be learned very quickly by the 
Contrastive Divergence (CD) approach. 
Therefore, we believe that DBN is very suitable 
for the character-level Chinese entity mention 
categorization approach. It can be used to solve 
the multi-class categorization problem with just 
simple binary features as the input. 
3 Deep Belief Network for Chinese 
Entity Categorization 
3.1 Problem Formalization 
An Entity mention categorization is a process of 
classifying the entity mentions into different 
categories. In this paper, we assume that the 
entity mentions are already correctly detected 
from the texts. Moreover, an entity mention 
should belong to one and only one predefined 
category. Formally, the categorization function 
of the name entities is 
( ( ))if V e C?            (1) 
where 
ie  is an entity mention from all the 
mention set E, ( )iV e  is the binary feature 
vector of 
ie , C={C1, C2, ?, CM} is the pre-
defined categories. Now the question is to find a 
classification function : Df R C?  which maps 
the feature vector V(ei) of an entity mention to its 
category. Generally, this classification function 
is learned from training data consisting of entity 
mentions with labeled categories. The learned 
function is then used to predict the category of 
new entity mentions by their feature vectors. 
3.2 Character-based Features 
As mentioned in the introduction, we intend to 
use character-level features for the purpose of 
avoiding the impact of the Chinese word 
segmentation errors. Denote the character 
dictionary as D={d1, d2, ?, dN}. To an e, it?s 
feature vector is V(e)={ v1, v2, ?, vN }. Each unit 
vi can be valued as Equation 2. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (2) 
103
For example, there is an entity mention ??
? ?Clinton?. So its feature vector is a vector 
with the same length as the character dictionary, 
in which all the dimensions are 0 except the three 
dimensions standing for ?, ?, and ?. The 
representation is clearly illustrated in Figure 1 
below. Since our objective is to test the 
effectiveness of DBN for this task. Therefore, we 
do not involve any other feature. 
 
Fig. 1. Generating the character-level features 
Characters compose the named entity and 
express its meaning. As a matter of fact, the 
composite effect of the characters to the 
mention category is quite complicated. For 
example, ?? ?Mr. Li? and ?? ?Laos? both 
have character ?, but ?? ?Mr. Li? indicates 
a person but ?? ?Laos? indicates a country. 
These are totally different NEs. Another 
example is ????? ?Capital of Paraguay? 
and ??? ?Asuncion?. They are two entity 
mentions point to the same entity despite that 
the two entities do not have any common 
characters. In such case, independent character 
features are not sufficient to determine the 
categories of the entity mentions. So we should 
also introduce some features which are able to 
represent the combinational effects of the 
characters. However, such kind of features is 
very hard to discover. Meanwhile, a complete 
set of combinations is nearly impossible to be 
found manually due to the exponential number 
of all the possible combinations. As in our 
study, we adopt DBN to automatically find the 
character combinations.  
3.3 Deep Belief Nets 
Deep Belief Network (DBN) is a complicated 
model which combines a set of simple models 
that are sequentially connected (Ackley, 1985). 
This deep architecture can be viewed as multiple 
layers. In DBN, upper layers are supposed to 
represent more ?abstract? concepts that explain 
the input data whereas lower layers extract ?low-
level features? from the data. DBN often consists 
of many layers, including multiple Restricted 
Boltzmann Machine (RBM) layers and a Back 
Propagation (BP) layer.  
 
Fig. 2.  The structure of a DBN. 
As illustrated in Figure 2, when DBN receives 
a feature vector, the feature vector is processed 
from the bottom to the top through several RBM 
layers in order to get the weights in each RBM 
layer, maintaining as many features as possible 
when they are transferred to the next layer. RBM 
deals with feature vectors only and omits the la-
bel information. It is unsupervised. In addition, 
each RBM layer learns its parameters indepen-
dently. This makes the parameters optimal for 
the relevant RBM layer but not optimal for the 
whole model. To solve this problem, there is a 
supervised BP layer on top of the model which 
fine-tunes the whole model in the learning 
process and generates the output in the inference 
process. After the processing of all these layers, 
the final feature vector consists of some sophisti-
cated features, which reflect the structured in-
formation among the original features. With this 
new feature vector, the classification perfor-
mance is better than directly using the original 
feature vector. 
None of the RBM is capable of guaranteeing 
that all the information conveyed to the output is 
accurate or important enough. However the 
learned information produced by preceding RBM 
layer will be continuously refined through the 
next RBM layer to weaken the wrong or insigni-
ficant information in the input. Each layer can 
detect feature in the relevant spaces. Multiple 
layers help to detect more features in different 
spaces. Lower layers could support object detec-
tion by spotting low-level features indicative of 
object parts. Conversely, information about ob-
jects in the higher layers could resolve lower-
level ambiguities. The units in the final layer 
share more information from the data. This in-
creases the representation power of the whole 
model. It is certain that more layers mean more 
computation time. 
104
DBN has some attractive features which make 
it very suitable for our problem. 
1) The unsupervised process can detect the 
structures in the input and automatically ob-
tain better feature vectors for classification. 
2) The supervised BP layer can modify the 
whole network by back-propagation to im-
prove both the feature vectors and the classi-
fication results. 
3) The generative model makes it easy to in-
terpret the distributed representations in the 
deep hidden layers. 
4) This is a fast learning algorithm that can 
find a fairly good set of parameters quickly 
and can ensure the efficiency of DBN. 
3.3.1 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which is 
the core component of DBN. RBM is Boltzmann 
Machine with no connection within the same 
layer. An RBM is constructed with one visible 
layer and one hidden layer. Each visible unit in 
the visible layer V  is an observed variable iv  
while each hidden unit in the hidden layer H  is 
a hidden variable 
jh
. Its joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (3) 
In RBM, the parameters that need to be esti-
mated are ( , , )W b c? ?  and 2( , ) {0,1}v h ? . 
To learn RBM, the optimum parameters are 
obtained by maximizing the above probability on 
the training data (Hinton, 1999). However, the 
probability is indeed very difficult in practical 
calculation. A traditional way is to find the gra-
dient between the initial parameters and the re-
spect parameters. By modifying the previous pa-
rameters with the gradient, the expected parame-
ters can gradually approximate the target para-
meters as 
0
( 1) ( ) ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (4) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging to 
the target. 
Traditionally, the Markov chain Monte Carlo 
method (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(5) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the av-
erage over the data states and its relevant sample 
in hidden unit. 
h v? ?
 denotes the multiplication 
of the average over the model states in visible 
unit and its relevant sample in hidden unit. 
However, MCMC requires estimating an ex-
ponential number of terms. Therefore, it typically 
takes a long time to converge to 
h v? ?
. Hinton 
(2002) introduced an alternative algorithm, i.e., 
the contrastive divergence (CD) algorithm, as a 
substitution. It is reported that CD can train the 
model much more efficiently than MCMC. To 
estimate the distribution ( )p x , CD considers a 
series of distributions { ( )np x } which indicate the 
distributions in n steps. It approximates the gap 
of two different Kullback-Leiler divergences 
(Kullback, 1987) as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (6) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution p?  
defined by the model. In each step, the gap is 
approximately minimized so that we can obtain 
the final distribution which has the smallest 
Kullback-Leiler divergence with the fantasy dis-
tribution.  
After n steps, the gradient can be estimated 
and used in Equation 4 to adjust the weights of 
RBM. In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust the 
weight of RBM. In this case, the estimate of the 
gradient is just the gap between the products of 
the visual layer and the hidden layer, i.e., 
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (7) 
Figure 3 below illustrates the process of learning 
RBM with CD-based gradient estimation. 
 
105
Fig. 3.  Learning RBM with CD-based gradient 
estimation 
3.3.2 Back-propagation (BP) 
The RBM layers provide an unsupervised analy-
sis on the structures of data set. They automati-
cally detect sophisticated feature vectors. The 
last layer in DBN is the BP layer. It takes the 
output from the last RBM layer and applies it in 
the final supervised learning process. In DBN, 
not only is the supervised BP layer used to gen-
erate the final categories, but it is also used to 
fine-tune the whole network. Specifically speak-
ing, when the parameters in BP layer are 
changed during its iterating process, the changes 
are passed to the other RBM layers in a top-to-
bottom sequence. 
The BP algorithm has a feed-forward step and 
a back-propagation step. In the feed-forward step, 
the input values are propagated to obtain the out-
put values. In the back-propagation step, the out-
put values are compared to the real category la-
bels and used them to modify the parameters of 
the model. We consider the weight
ijw  
which 
indicates the edge pointing from the i-th node in 
one RBM layer to the j-th node in its upper layer. 
The computation in feed-forward is 
i ijo w , 
where 
io  is the stored output for the unit i. In 
the back-propagation step, we compute the error 
E in the upper layers and also the gradient with 
respect to this error, i.e., 
i ijE o w? ?
. Then the 
weight
ijw  
will be adjusted by the gradient des-
cent. 
ij i i j
i ij
Ew o oo w? ? ?
?? ? ? ? ??
 (8) 
where ??  is used to control the length of the 
moving step. 
3.3.3 DBN-based Entity Mention Categori-
zation 
For each entity mention, it is represented by the 
character feature vector as introduced in section 
3.2 and then fed to DBN. The training procedure 
can be divided into two phases. The first phase is 
the parameter estimation process of the RBMs on 
all the inputted feature vectors. When a feature 
vector is fed to DBN, the first RBM layer is 
adjusted automatically according to this vector. 
After the first RBM layer is ready, its output 
becomes the input of the second RBM layer. The 
weights of the second RBM layer are also 
adjusted. The similar procedure is carried out on 
all the RBM layers. Then DBN will operates in 
the second phase, the back-propagation 
algorithm. The labeled categories of the entity 
mention are used to tune the parameters of the 
BP layer. Moreover, the changes of the BP layer 
are also fed back to the RBM layers. The 
procedure will iterate until the terminating 
condition is met. It can be a fixed number of 
iterations or a pre-given precision threshold. 
Once the weights of all the layers in DBN are 
obtained, the estimated model could be used to 
prediction. 
 
Fig. 4.  The mention categorization process 
of DBN 
Figure 4 illustrates the classification process of 
DBN. In prediction, for an entity mention e, we 
first calculate its feature vector V(e) and used as 
the input of DBN. V(e) is passed through all the 
layers to get the outputs for all RBM layers and 
last back-propagation layer. In the ith RBM layer, 
the dimensions in the input vector Vinput_i(e) are 
combined to yield the dimensions of the next 
feature vector Voutput_i(e) as input of the next layer. 
After the feature vector V(e) goes through all the 
RBM layers, it is indeed transformed to another 
feature vector V?(e) which consists of 
complicated combinations of the original 
character features and contains rich structured 
information between the characters. This feature 
vector is then fed into the BP layer to get the 
final category c(e). 
4 Experiments 
4.1 Experiment Setup 
In our experiment, we use the ACE 2004 corpus 
to evaluate our approach. The objective of this 
study is that the correctly detected Chinese entity 
mentions categorization using DBN from the text 
and figure out the suitability of DBN on this task. 
Moreover, an entity mention should belong to 
one and only one category. 
106
According to the guideline of the ACE04 task, 
there are five categories for consideration in total, 
i.e., Person, Organization, Geo-political entity, 
Location, and Facility. Moreover, each entity 
mention is expressed in two forms, i.e., the head 
and the extent. For example, ??????? 
?President Clinton of USA? is the extent of an 
entity mention and ???  ?Clinton? is the 
corresponding head. The two phrases both point 
to a named entity whose name is Clinton and he 
is the president of USA.  Here we make the 
?breakdown? strategy mentioned in Li et al 
(2007) that only the entity head is considered to 
generate the feature vector, considering that the 
information from the entity head refines the 
name entity. Although the entity extent includes 
more information, it also brings many noises 
which may make the learning process much 
more difficult. 
   In our experiments, we test the machine 
learning models under a 4-flod cross-validation. 
All entity mentions are divided into four parts 
randomly where three parts are used for training 
and one for test. In total, 7746 mentions are used 
for training and 2482 mentions are used for 
testing at each round. Precision is chosen as the 
evaluation criterion, calculated by the proportion 
of the number of correctly categorized instances 
and the number of total instances. Since all the 
instances should be classified, the recall value is 
equal to the precision value. 
4.2 Evaluation on Named Entity categoriza-
tion 
First of all, we provide some statistics of the data 
set. The distribution of entity mentions in each 
category is given in table 1. The size of the 
character dictionary in the corpus is 1185, so 
does the dimension of each feature vector. 
Type Quantity 
Person 4197 
Organization 1783 
Geo-political entity 287 
Location 3263 
Facility 399 
Table 1.  Number of entity mentions in each 
category 
In the first experiment, we compare the 
performance of DBN with some popular 
classification algorithms, including Support 
Vector Machine (labeled by SVM) and a 
traditional BP neutral network (labeled by NN 
(BP)). To implement the models, we use the 
LibSVM toolkit1 for SVM and the neural neutral 
network toolbox in Matlab2 for BP. The DBN in 
this experiment includes two RBM layers and 
one BP layer. Results of the first experiment are 
given in Table 2.  
Learning Model Precision 
DBN 91.45% 
SVM 90.29% 
NN(BP) 87.23% 
Table 2.  Performances of the systems with 
different classification models 
In this experiment, the DBN has three RBM 
layers and one BP layer. And the numbers of 
units in each RBM layer are 900, 600 and 300 
respectively. NN (BP) has the same structure as 
DBN. As for SVM, we choose the linear kernel 
with the penalty parameter C=1 and set the other 
parameters as default after comparing different 
kernels and parameters. 
In the results, DBN achieved better 
performance than both SVM and BP neural 
network. This clearly proved the advantages of 
DBN. The deep architecture of DBN yields 
stronger representation power which makes it 
able to detect more complicated and efficient 
features, thus better performance is achieved.  
In the second experiment, we intend to 
examine the performance of DBN with different 
number of RBM layers, from one RBM layer 
plus one BP layer to three RBM layers plus one 
BP layer. The amount of the units in the first 
RBM layer is set 900 and the amount in the 
second RBM layer is 600, if the second layer 
exists. As for the third RBM layers, the amount 
of units is set to 300. 
Construction of Neural Network Precision 
Three RBMs and One BP 91.45% 
Two RBMs and One BP 91.42% 
One RBM and one BP 91.05% 
Table 3.  Performance of DBNs with different 
number s of RBM layers 
Results in Table 3 show that the performance 
tends to be better when more RBM layers are 
incorporated. More RBM layers do enhance the 
representation power of DBN. However, it is 
also noted that the improvement is not significant 
from two layers to three layers. The reason may 
                                                 
1 available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
2 available at 
http://www.mathworks.com/access/helpdesk/help/toolbox
/nnet/backprop.html 
107
be that two-RBM DBN already has enough 
representation power for modeling this data set 
and thus one more RBM layer brings 
insignificant improvement. It is also mentioned 
in Hinton (2006) that more than three RBM 
layers are indeed not necessary. Another 
important result in Table 3 is that the DBN with 
One RBM and one BP performs much better than 
the neutral network with only BP in Table 1. 
This clearly showed the effectiveness of feature 
combination by the RBM layer again. 
As to the amount of units in each RBM layer, 
it is manually fixed in upper experiments. This 
number certainly affects the representation 
power of an RBM layer, consequently the 
representation power of the whole DBN. In this 
set of experiment, we intend to study the 
effectiveness of the unit size to the performance 
of DBN. A series of DBNs with only one RBM 
layer and different unit numbers for this RBM 
layer is evaluated. The results are provided in 
Table 4 below. 
Construction of Neural Network Precision 
one RBM(300 units) + one BP 90.61% 
one RBM(600 units) + one BP 90.69% 
one RBM(900 units) + one BP 91.05% 
one RBM(1200 units) + one BP 90.98% 
one RBM(1500 units) + one BP 90.61% 
one RBM(1800 units) + one BP 90.57% 
Table 4.  Performance of One-RBM DBNs 
with different number of units 
Based on the results, we can see that the 
performance is quite stable with different unit 
numbers. But the numbers that are closer to the 
original feature size seem to be some better. This 
could suggest that we should not decrease or 
increase the dimension of the vector feature too 
much when casting the vector transformation by 
RBM layers. 
Finally, we show the results of the individual 
categories. For each category, the Precision-
Recall-F values are provided in table 5, in which 
the F-measure is calculated by 
2*Precision*Recall-measure= Precision+RecallF
    (9) 
Type P R F 
Person 91.26% 96.26% 93.70% 
Organization 89.86% 89.04% 89.45% 
Location 77.58% 59.21% 76.17% 
Geo-political 
entity 
93.60% 91.89% 92.74% 
Facility 77.43% 63.72% 69.91% 
Table 5.  Performances of the system on each 
category 
5 Conclusions 
In this paper we presented our recent work on 
applying a novel machine learning model, the 
Deep Belief Nets, on Chinese entity mention 
categorization. It is demonstrated that DBN is 
very suitable for character-level mention 
categorization approaches due to its strong 
representation power and the ability on 
discovering complicated feature combinations. 
We conducted a series of experiments to prove 
the benefits of DBN. Experimental results 
clearly showed the advantages of DBN that it 
obtained better performance than existing 
approaches such as SVM and traditional BP 
neutral network. 
References  
David Ackley, Geoffrey Hinton, and Terrence 
Sejnowski. 1985. A learning algorithm for 
Boltzmann machines. Cognitive Science. 9. 
David MacDonald. 1993. Internal and external 
evidence in the identification and semantic 
categorization of proper names. Corpus 
Processing for Lexical Acquisition, MIT Press, 61-
76. 
Geoffrey Hinton. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Geoffrey Hinton. 2002. Training products of experts 
by minimizing contrastive divergence. Neural 
Computation, 14, 1771?1800. 
Geoffrey Hinton, Simon Osindero, and Yee-Whey 
Teh. 2006. A fast learning algorithm for deep 
belief nets. Neural Computation. 18, 1527?1554 . 
GuoDong Zhou and Jian Su. 2002. Named entity 
recognition using an hmm-based chunk tagger. In 
proceedings of ACL. 473-480. 
Hideki Isozaki and Hideto Kazawa. 2002. Efficient 
support vector classifiers for named entity 
recognition. In proceedings of IJCNLP. 1-7. 
Honglei Guo, Jianmin Jiang, Guang Hu and Tong 
Zhang. 2005. Chinese named entity recognition 
based on multilevel linguistics features. In 
pr ceedings of IJCNLP. 90-99. 
Jing, Hongyan, Radu Florian, Xiaoqiang Luo, Tong 
Zhang and Abraham Ittycheriah. 2003. How to get 
a Chinese name (entity): Segmentation and 
combination issues. In proceedings of EMNLP. 
200-207. 
Koen Deschacht and Marie-Francine Moens. 2006, 
Efficient Hierarchical Entity Classifier Using 
Conditional Random Field. In Proceedings of the 
108
2nd Workshop on Ontology Learning and 
Population. 33-40. 
Michael Collins and Yoram Singer. 1999. 
Unsupervised models for named entity 
classification. In Proceedings of EMNLP'99. 
Nina Wacholder, Yael Ravin and Misook Choi. 1997. 
Disambiguation of Proper Names in Text. In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing. 
Solomon Kullback. 1987. Letter to the Editor: The 
Kullback-Leibler distance. The American 
Statistician 41 (4): 340?341. 
Wenjie Li and Donglei Qian. 2007. Detecting, 
Categorizing and Clustering Entity Mentions in 
Chinese Text, in Proceedings of the 30th Annual 
International ACM SIGIR Conference (SIGIR?07). 
647-654. 
Yoshua Bengio and Yann LeCun. 2007. Scaling 
learning algorithms towards ai. Large-Scale Ker-
nel Machines. MIT Press. 
 
109
Exploring Deep Belief Network for Chinese Relation Extraction 
Yu Chen1, Wenjie Li2, Yan Liu2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{cswjli, csyliu}@comp.polyu.edu.hk 
Abstract 
Relation extraction is a fundamental 
task in information extraction that 
identifies the semantic relationships 
between two entities in the text. In this 
paper, a novel model based on Deep 
Belief Network (DBN) is first 
presented to detect and classify the 
relations among Chinese entities. The 
experiments conducted on the 
Automatic Content Extraction (ACE) 
2004 dataset demonstrate that the 
proposed approach is effective in 
handling high dimensional feature 
space including character N-grams, 
entity types and the position 
information. It outperforms the state-
of-the-art learning models such as 
SVM or BP neutral network. 
1 Introduction 
Information Extraction (IE) is to automatically 
pull out the structured information required by 
the users from a large volume of plain text. It 
normally includes three sequential tasks, i.e., 
entity extraction, relation extraction and event 
extraction. In this paper, we limit our focus on 
relation extraction.  
In early time, pattern-based approaches were 
the main focus of most research studies in 
relation extraction. Although pattern-based 
approaches achieved reasonably good results, 
they have some obvious flaws. It requires 
expensive handcraft work to assemble patterns 
and not all relations can be identified by a set 
of reliable patterns (Willy Yap, 2009). Also, 
once the interest of task is transferred to a 
different domain or a different language, 
patterns have to be revised or even rewritten. 
That is to say, the discovered patterns are 
heavily dependent on the task in a specific 
domain or on a particular corpus. 
Naturally, a vast amount of work was spent 
on feature-based machine learning approaches 
in later years. In this camp, relation extraction 
is typically cast as a classification problem, 
where the most important issue is to train a 
model to scale and measure the similarity of 
features reflecting relation instances. The 
entity semantic information expressing relation 
was often formulated as the lexical and 
syntactic features, which are identical to a 
certain linear vector in high dimensions. Many 
learning models are capable of self-training 
and classifying these vectors according to 
similarity, such as Support Vector Machine 
(SVM) and Neural Network (NN).  
Recently, kernel-based approaches have 
been developing rapidly. These approaches 
involved kernels of structure representations, 
like parse tree or dependency tree, in similarity 
calculation. In fact, feature-based approaches 
can be viewed as the special and simplified 
kinds of kernel-based approaches. They used 
dot-product as the kernel function and did not 
range over the intricate structure information 
(Ji, et al 2009). 
Relation extraction in Chinese received 
quite limited attention as compared to English 
and other western languages. The main reason 
is the unique characteristic of Chinese, such as 
more flexible grammar, lack of boundary 
information and morphological variations etc 
(Sun and Dong, 2009). Especially, the existing 
Chinese syntactic analysis tools at current 
stage are not yet reliable to capture the 
valuable structured information. It is urgent to 
develop approaches that are in particular 
suitable for Chinese relation extraction. 
In this paper, we explore the use of Deep 
Belief Network (DBN), a new feature-based 
machine learning model for Chinese relation 
extraction. It is a neural network model 
developed under the deep learning architecture 
that is claimed by Hinton (2006) to be able to 
automatically learn a deep hierarchy of 
features with increasing levels of abstraction 
for the complex problems like natural language 
processing (NLP). It avoids assembling 
patterns that express the semantic relation 
information and meanwhile it succeeds to 
produce accurate model that is not confined to 
the parsing results.  
The rest of this paper is structured in the 
following manner. Section 2 reviews the 
previous work on relation extraction. Section 3 
presents task definition, briefly introduces the 
DBN model and the feature construction. 
Section 4 provides the experimental results. 
Finally, Section 5 concludes the paper.  
2 Related Work 
Over the past decades, relation extraction had 
come to a significant progress from simple 
pattern-based approaches to adapted self-
training machine learning approaches. 
Brin (1998) used Dual Iterative Pattern 
Relation Expansion, a bootstrapping-based 
system, to find the largest common substrings 
as patterns. It had the ability of searching 
patterns automatically and was good for large 
quantity of uniform contexts. Chen (2006) 
proposed graph algorithm called label 
propagation, which transferred the pattern 
similarity to probability of propagating the 
label information from any vertex to its nearby 
vertices. The label matrix indicated the relation 
type. 
Feature-based approaches utilized the linear 
vector of carefully chosen lexical and syntactic 
features derived from different levels of text 
analysis and ranging from part-of-speech (POS) 
tagging to full parsing and dependency parsing 
(Zhang 2009). Jing and Zhai (2007) defined a 
unified graphic representation of features that 
served as a general framework in order to 
systematically explore the information at 
diverse levels in three subspaces and finally 
estimated the effectiveness of these features. 
They reported that the basic unit feature was 
generally sufficient to achieve state-of-art 
performance. Meanwhile, over-inclusion 
complex features were harmful. 
Kernel-based approaches utilize kernel 
functions on structures between two entities, 
such as sequences and trees, to measure the 
similarity between two relation instances. 
Zelenok (2003) applied parsing tree kernel 
function to distinguish whether there was an 
existing relationship between two entities. 
However, they limited their task on Person-
affiliation and organization-location.  
The previous work mainly concentrated on 
relation extraction in English. Relatively, less 
attention was drawn on Chinese relation 
extraction. However, its importance is being 
gradually recognized. For instance, Zhang et al 
(2008) combined position information, entity 
type and context features in a feature-based 
approach and Che (2005) introduced the edit 
distance kernel over the original Chinese string 
representation.  
DBN is a new feature-based approach for 
NLP tasks. According to the work by Hinton 
(2006), DBN consisted of several layers 
including multiple Restricted Boltzmann 
Machine (RBM) layers and a Back 
Propagation (BP) layer. It was reported to 
perform very well in many classification 
problems (Ackley, 1985), which is from the 
origin of its ability to scale gracefully and be 
computationally tractable when applied to high 
dimensional feature vectors. Furthermore, to 
against the combinations of feature were 
intricate, it detected invariant representations 
from local translations of the input by deep 
architecture.  
3 Deep Belief Network for Chinese 
Relation Extraction 
3.1 Task Definition 
Relation extraction, promoted by the 
Automatic Content Extraction (ACE) program, 
is a task of finding predefined semantic 
relations between pairs of entities from the 
texts. According to the ACE program, an entity 
is an object or a set of objects in the world 
while a relation is an explicitly or implicitly 
stated relationship between entities. The task 
can be formalized as:  
1 2( , , )e e s r?       (1) 
where 
1e  and 2e  are the two entities in a 
sentence s  under concern and r  is the relation 
between them. We call the triple 
1 2( , , )e e s  the 
relation candidate. According to the ACE 2004 
guideline 1 , five relation types are defined. 
They are: 
Role: it represents an affiliation between a 
Person entity and an Organization, Facility, 
or GPE (a Geo-political entity) entities. 
Part: it represents the part-whole relationship 
between Organization, Facility and GPE 
entities. 
At: it represents that a Person, Organization, 
GPE, or Facility entity is location at a 
Location entities. 
Near: it represents the fact that a Person, 
Organization, GPE or Facility entity is near 
(but not necessarily ?At?) a Location or 
GPE entities. 
Social: it represents personal and professional 
affiliations between Person entities. 
3.2 Deep Belief Networks (DBN) 
DBN often consists of several layers, 
including multiple RBM layers and a BP layer. 
As illustrated in Figure 1, each RBM layer 
learns its parameters independently and 
unsupervisedly. RBM makes the parameters 
optimal for the relevant RBM layer and detect 
complicated features, but not optimal for the 
whole model. There is a supervised BP layer 
on top of the model which fine-tunes the whole 
model in the learning process and generates the 
output in the inference process. RBM keeps 
information as more as possible when it 
transfers vectors to next layer. It makes 
networks to avoid local optimum. RBM is also 
adopted to ensure the efficiency of the DBN 
model. 
 
Fig. 1.  The structure of a DBN. 
                                                 
1 available at http://www.nist.gov/speech/tests/ace/. 
Deep architecture of DBN represents many 
functions compactly. It is expressible by 
integrating different levels of simple functions 
(Y. Bengio and Y. LeCun). Upper layers are 
supposed to represent more ?abstract? concepts 
that explain the input data whereas lower 
layers extract ?low-level features? from the 
data. In addition, none of the RBM guarantees 
that all the information conveyed to the output 
is accurate or important enough. The learned 
information produced by preceding RBM layer 
will be continuously refined through the next 
RBM layer to weaken the wrong or 
insignificant information in the input. Multiple 
layers filter valuable features. The units in the 
final layer share more information from the 
data. This increases the representation power 
of the whole model. The final feature vectors 
used for classification consist of sophisticated 
features which reflect the structured 
information, promote better classification 
performance than direct original feature vector.  
3.3 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which 
is the core component of DBN. RBM is 
Boltzmann Machine with no connection within 
the same layer. An RBM is constructed with 
one visible layer and one hidden layer. Each 
visible unit in the visible layer V  is an 
observed variable 
iv  while each hidden unit in 
the hidden layer H  is a hidden variable 
jh
. Its 
joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (2) 
In RBM, 2( , ) {0,1}v h ? and ( , , )W b c? ? are 
the parameters that need to be estimated?W  
is the weight tying visible layer and hidden 
layer. b is the bias of units v and c is the bias of 
units h. 
To learn RBM, the optimum parameters are 
obtained by maximizing the joint distribution 
( , )p v h  on the training data (Hinton, 1999). A 
traditional way is to find the gradient between 
the initial parameters and the expected 
parameters. By modifying the previous 
parameters with the gradient, the expected 
parameters can gradually approximate the 
target parameters as 
0
( 1) ( ) log ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (3) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging 
to the target. 
Traditionally, the Monte Carlo Markov 
chain (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(4) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the 
average over the data states and its relevant 
sample in hidden unit. 
h v? ?
 denotes the 
multiplication of the average over the model 
states in visible units and its relevant sample in 
hidden units. 
  
Fig. 2.  Learning RBM with CD-based 
gradient estimation 
However, MCMC requires estimating an 
exponential number of terms. Therefore, it 
typically takes a long time to converge to 
h v? ?
. Hinton (2002) introduced an alternative 
algorithm, i.e., the contrastive divergence (CD) 
algorithm, as a substitution. It is reported that 
CD can train the model much more efficiently 
than MCMC. To estimate the distribution ( )p x , 
CD considers a series of distributions { ( )np x } 
which indicate the distributions in n steps. It 
approximates the gap of two different 
Kullback-Leiler divergences as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (5) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution 
p?  defined by the model.  
In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust 
the weight of RBM as Equation 6.  
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (6) 
Figure 2 below illustrates the process of 
learning RBM with CD-based gradient 
estimation. 
3.4 Back-Propagation (BP) 
The RBM layers provide an unsupervised 
analysis on the structures of data set. They 
automatically detect sophisticated feature 
vectors. The last layer in DBN is the BP layer. 
It takes the output from the last RBM layer and 
applies it in the final supervised learning 
process. In DBN, not only is the supervised BP 
layer used to generate the final categories, but 
it is also used to fine-tune the whole network. 
Specifically speaking, when the BP layer is 
changed during its iterating process, the 
changes are passed to the other RBM layers in 
a top-to-bottom sequence. 
3.5 The Feature Set 
DBN is able to detect high level hidden 
features from lexical, syntactic and/or position 
characteristic. As mentioned in related work, 
over-inclusion complex features are harmful. 
We therefore involve only three kinds of low 
level features in this study. They are described 
below. 
3.5.1 Character-based Features 
Since Chinese text is written without word 
boundaries, the word-level features are limited 
by the efficiency of word segmentation results. 
In the paper presented by H. Jing (2003) and 
some others, they observed that pure character-
based models can even outperform word-based 
models. Li et al?s (2008) work relying on 
character-based features also achieved 
significant performance in relation extraction. 
We denote the character dictionary as D={d1, 
d2, ?, dN}. In our experiment, N is 1500. To 
an e, it?s character-based feature vector is 
V(e)={ v1, v2, ?, vN }. Each unit vi can be 
valued as Equation 8. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (7) 
3.5.2 Entity Type Features 
According to the ACE 2004 guideline, there 
are five entity types in total, including Person, 
Organization, GPE, Location, and Facility. We 
recognize and classify the relation between the 
recognized entities. The entities in ACE 2004 
corpus were labeled with these five types. 
Type features are distinctive for classification. 
For example, the entities of Location cannot 
appear in the Role relation.  
3.5.3 Relative Position Features 
We define three types of position features 
which depict the relative structures between 
the two entities, including Nested, Adjacent 
and Separated. For each relation candidate 
triple 
1 2( , , )e e s , let .starte  and .ende  denote 
the starting and end positions of e  in a 
document. Table 1 summarizes the conditions 
for each type, where }2,1{, ?ji  and ji ? .  
Type Condition 
Nested ( .start, .end) ( .start, .end)i i j je e e e?
 
Adjacent .end= .start-1i je e
 
Separated ( .start< .start)&( .end+1< .start)i j i je e e e
 
Table 1. The internal postion structure features 
between two named entities 
We combine the character-based features of 
two entities, their type information and 
position information as the feature vector of 
relation candidate.  
3.6 Order of Entity Pair 
A relation is basically an order pair. For 
example, ?Bank of China in Hong Kong? 
conveys the ACE-style relation ?At? between 
two entities ?Bank of China (Organization)? 
and ?Hong Kong (Location)?. We can say that 
Bank of China can be found in Hong Kong, 
but not vice verse. The identified relation is 
said to be correct only when both its type and 
the order of the entity pair are correct. We 
don?t explicitly incorporate such order 
restriction as an individual feature but use the 
specified rules to sort the two entities in a 
relation once the relation type is recognized. 
As for those symmetric relation types, the 
order needs not to be concerned. Either order is 
considered correct in the ACE standard. As for 
those asymmetric relation types, we simply 
select the first (in adjacent and separated 
structure) or outer (in nested structures) as the 
first entity. In most cases, this treatment leads 
to the correct order. We also make use of 
entity types to verify (and rectify if necessary) 
this default order. For example, considering 
?At? is a relation between a Person, 
Organization, GPE, or Facility entity and a 
Location entity, the Location entity must be 
placed after the Person, Organization, GPE, or 
Facility entity in a relation. 
4 Experiments and Evaluations 
4.1 Experiment Setup 
The experiments are conducted on the ACE 
2004 Chinese relation extraction dataset, 
which consists of 221 documents selected from 
broadcast news and newswire reports. There 
are 2620 relation instances and 11800 pairs of 
entities have no relationship in the dataset. The 
size of the feature space is 3017.  
We examine the proposed DBN model 
using 4-fold cross-validation. The performance 
is measured by precision, recall, and F-
measure. 
2*Precision*Recall-measure= Precision+RecallF
    (8) 
In the following experiments, we plan to test 
the effectiveness of the DBN model in three 
ways: 
Detection Only: For each relation candidate, 
we only recognize whether there is a certain 
relationship between the two entities, no 
matter what type of relation they hold.  
Detection and Classification in Sequence: 
For each relation candidate, when it is 
detected to be an instance of relation, it 
proceeds to detect the type of the relation 
the two entities hold. 
Detection and Classification in Combination: 
We define N+1 relation label, N for relation 
types defined by ACE and one for NULL 
indicating there is no relationship between 
the two entities. In this way, the processes 
of detection and classification are combined. 
We will compare DBN with a well-known 
Support Vector Machine model (labeled as 
SVM in the tables) and a traditional BP neutral 
network model (labeled as NN (BP only)). 
Among them, SVM has been successfully 
applied in many classification applications. We 
use the LibSVM toolkit 2  to implement the 
SVM model. 
4.2 Evaluation on Detection Only 
We first evaluate relation detection, where 
only two output classes are concerned, i.e. 
NULL (which means no relation recognized) 
and RELATION. The parameters used in DBN, 
SVM and NN (BP only) are tuned 
experimentally and the results with the best 
parameter settings are presented in Table 2. In 
each of our experiments, we test many 
parameters of SVM and chose the best set of 
that to show below. 
Regarding the structure of DBN, we 
experiment with different combinations of unit 
numbers in the RBM layers. Finally we choose 
DBN with three RBM layers and one BP layer. 
And the numbers of units in each RBM layer 
are 2400, 1800 and 1200 respectively, which is 
the best size of each layer in our experiment. 
Our empirical results showed that the numbers 
of units in adjoining layers should not decrease 
the dimension of feature vector too much when 
casting the vector transformation. NN has the 
same structure as DBN. As for SVM, we 
choose the linear kernel with the penalty 
parameter C=0.3, which is the best penalty 
coefficient, and set the other parameters as 
default after comparing different kernels and 
parameter values.  
Model Precision Recall F-measure 
DBN 67.8% 70.58% 69.16% 
SVM 73.06% 52.42% 61.04% 
NN (BP 
only) 
51.51% 61.77% 56.18% 
Table 2. Performances of DBN, SVM and NN 
models for detection only 
As showed in Table 2, with their best 
parameter settings, DBN performs much better 
                                                 
2 http://www.csie.ntu.edu.tw/~cjlin/libsvm/  
than both SVM and NN (BP only) in terms of 
F-measure. It tells that DBN is quite good in 
this binary classification task. Since RBM is a 
fast approach to approximate global optimum 
of networks, its advantage over NN (BP only) 
is clearly demonstrated in their results.  
4.3 Evaluation on Detection and 
Classification in Sequence 
In the next experiment, we go one step further. 
If a relation is detected, we classified it into 
one of the 5 pre-defined relation types. For 
relation type classification, DBN and NN (BP 
only) have the same structures as they are in 
the first experiment. We adopt SVM linear 
kernel again and set C to 0.09 and other 
parameters as default. The overall performance 
of detection and classification of three models 
are illustrated in Table 3 below. DBN again is 
more effective than SVM and NN. 
Model Precision Recall F-measure 
DBN 63.67% 59% 61.25% 
SVM 67.78% 47.43% 55.81% 
NN  61% 45.62% 52.2% 
Table 3. Performances of DBN and other 
classification models for detection and 
classification in sequence 
4.4 Evaluation on Detection and 
Classification in Combination 
In the third experiment, we unify relation 
detection and relation type classification into 
one classification task. All the candidates are 
directly classified into one of the 6 classes, 
including 5 relation types and a NULL class. 
Parameter settings of the three models in this 
experiment are identical to those in the second 
experiment, except that C in SVM is set to 0.1. 
Model Precision Recall F-measure 
DBN 65.8% 59.15% 62.3% 
SVM 75.25% 44.07% 55.59% 
NN (BP 
only) 
63.2% 45.7% 53.05% 
Table 4. Performances of DBN, SVM and NN 
models for detection and classification in 
combination 
As demonstrated, DBN outperforms both 
SVM and NN (BP only) in all these three 
experiments consistently. In this regard, the 
advantages of DBN over the other two models 
are apparent. RBM approximates expected 
parameters rapidly and the deep DBN 
architecture yields stronger representativeness 
of complicated, efficient features.  
Comparing the results of the second and the 
third experiments, SVM perform better 
(although not quite significantly) when 
detection and classification are in sequence 
than in combination. This finding is consistent 
with our previous work (to be added later). It 
can possibly be that preceding detection helps 
to deal with the severe unbalance problem, i.e. 
there are much more relation candidates that 
don?t hold pre-defined relations. However, 
DBN obtaining the opposite result cause by 
that the amount of examples we have is not 
sufficient for DBN to self-train itself well for 
type classification. We will further exam this 
issue in our feature work. 
4.5 Evaluation on DBN Structure 
Next, we compare the performance of DBN 
with different structures by changing the 
number of RBM layers. All the candidates are 
directly classified into 6 types in this 
experiment.  
DBN  Precision Recall F-measure 
3 RBMs + 
BP 
65.8% 59.15% 62.3% 
2 RBMs + 
BP 
65.22% 57.1% 60.09% 
1 RBM + 
BP 
64.35% 55.5% 59.6% 
Table 5. Performance RBM with different 
layers 
The results provided in Table 5 show that 
the performance can be improved when more 
RBM layers are incorporated. Multiple RBM 
layers enhance representation power. Since it 
was reported by Hinton (2006) that three RBM 
layer is enough to detect the complex features 
and more RBM layer are of less help, we do 
not try to go beyond the three layers in this 
experiment. Note that the improvement is more 
obvious from two layers to three layers than 
from one layer to two layers. 
4.6 Error Analysis 
Finally, we provide the test results for 
individual relation types in Table 6. We can 
see that the proposed model performs better on 
?Role? and ?Part? relations. When taking a 
closer look at their relation instance 
distributions, the instances of these two types 
comprise over 63% percents of all the relation 
instances in the dataset. Clearly their better 
results benefit from the amount of training data. 
It further implies that if we have more training 
data, we should be able to train a more 
powerful DBN. The same characteristic is also 
observed in Table 7 which shows the 
distributions of the identified relations against 
the gold standard.  However, the sizes of ?At? 
relation instances and ?Role? relation instances 
are similar, its result is much worse. We 
believe it is from the origin of that the position 
feature is not distinctive for ?At? relation, as 
shown in Table 8. ?Near? and ?Social? are two 
symmetric relation types. Ideally, they should 
have better results. But due to quite small 
number of training examples, you can see that 
they are actually the types with the worst F-
measure. 
Type Precision Recall F-measure 
Role 65.19% 69.2% 67.14% 
Part 67.86% 71.43% 69.59% 
At 51.15% 60% 55.22% 
Near 15.38% 33.33% 20.05% 
Social 25% 35.71% 29.41% 
Table 6. Performance of DBN for each 
relation type 
 R P A N S Null 
Role (R) 191 1 5 0 0 96 
Part (P) 1 95 12 0 0 32 
At (A) 4 8 111 2 1 91 
Near (N) 0 1 0 2 0 10 
Social (S) 1 0 0 0 5 14 
Table 7. Distribution of the identified relations 
Type Adjacent  Separated Nested  
Role 7 63 223 
Part 1 17 122 
At 21 98 98 
Near 0 8 5 
Social 10 10 10 
           Identified 
Standard 
Table 8.  Statistic of position feature 
The main mistakes observed in Table 7 are 
wrongly classifying a ?Part? relation as a ?At? 
relations. We further inspect these 12 mistakes 
and find that it is indeed difficult to distinct the 
two types for the given entity pairs. Here is a 
typical example: entity 1: ?????  (the 
Democratic Party of the United States, defined 
as an organization entity), entity 2: ?? (the 
United States, defined as a GPE entity). 
Therefore, the major problem we have to face 
is how to effectively recall more relations. 
Given the limited training resources, it is 
needed to well explore the appropriate external 
knowledge or the Web resources. 
5 Conclusions 
In this paper we present our recent work on 
applying a novel machine learning model, 
namely Deep Belief Network, to Chinese 
relation extraction. DBN is demonstrated to 
be effective for Chinese relation extraction 
because of its strong representativeness. We 
conduct a series of experiments to prove the 
benefits of DBN. Experimental results clearly 
show the strength of DBN which obtains 
better performance than other existing models 
such as SVM and the traditional BP neutral 
network. In the future, we will explore if it is 
possible to incorporate the appropriate 
external knowledge in order to recall more 
relation instances, given the limited training 
resource. 
References 
Ackley D., Hinton G. and Sejnowski T. 1985. A 
learning algorithm for Boltzmann machines, 
Cognitive Science, 9. 
Brin Sergey. 1998. Extracting patterns and relations 
from world wide web, In Proceedings of 
WebDB Workshop at 6th International 
Conference on Extending Database 
Technology (WebDB?98), 172-183. 
Che W.X. Improved-Edit-Distance Kernel for 
Chinese Relation Extraction, In Dale, R.,Wong, 
K.-F., Su, J., Kwong, O.Y. (eds.) IJCNLP 
2005.LNCS(LNAI). vol. 2651. 
H. Jing, R. Florian, X. Luo, T. Zhang, A. 
Ittycheriah. 2003. How to get a Chinese name 
(entity): Segmentation and combination issues. 
In proceedings of EMNLP. 200-207. 
Hinton, G.. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Hinton, G. E. 2002. Training products of experts by 
minimizing contrastive divergence, Neural 
Computation, 14(8), 1711?1800. 
Hinton G. E., Osindero S. and Teh Y. 2006. A fast 
learning algorithm for deep belief nets, Neural 
Computation, 18. 1527?1554. 
Ji Zhang, You Ouyang, Wenjie Li and Yuexian 
Hou. 2009. A Novel Composite Kernel 
Approach to Chinese Entity Relation 
Extraction. in Proceedings of the 22nd 
International Conference on the Computer 
Processing of Oriental Languages, Hong Kong, 
pp240-251. 
Ji Zhang, You Ouyang, Wenjie Li, and Yuexian 
Hou. 2009. Proceedings of the 22nd 
International Conference on Computer 
Processing of Oriental Languages. 236-247.  
Jiang J. and Zhai C. 2007. A Systematic 
Exploration of the Feature Space for Relation 
Extraction, In Proceedings of NAACL/HLT, 
113?120. 
Jinxiu Chen, Donghong Ji, Chew L., Tan and 
Zhengyu Niu. 2006. Relation extraction using 
label propagation based semi-supervised 
learning, In Proceedings of ACL?06, 129?136. 
Li W.J., Zhang P., Wei F.R., Hou Y.X. and Lu, Q. 
2008. A Novel Feature-based Approach to 
Chinese Entity Relation Extraction, In 
Proceeding of ACL 2008 (Companion Volume), 
89?92 
Sun Xia and Dong Lehong, 2009. Feature-based 
Approach to Chinese Term Relation Extraction. 
International Conference on Signal Processing 
Systems. 
Willy Yap and Timothy Baldwin. 2009. 
Experiments on Pattern-based Relation 
Learning. Proceeding of the 18th ACM 
conference on Information and knowledge 
management. 1657-1660.  
Y. Bengio and Y. LeCun. 2007. Scaling learning 
algorithms towards ai. Large-Scale Kernel 
Machines. MIT Press. 
Zelenko D. Aone C and Richardella A. 2003. 
Kernel Methods for Relation Extraction, 
Journal of Machine Learning Research 
2003(2), 1083?1106. 
Zhang P., Li W.J., Wei F.R., Lu Q. and Hou Y.X. 
2008. Exploiting the Role of Position Feature 
in Chinese Relation Extraction, In Proceedings 
of the 6th International Conference on 
Language Resources and Evaluation (LREC). 
 
