Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
In: Proceedings of CoNLL-2000 and LLL-2000, pages 160-162, Lisbon, Portugal, 2000. 
Phrase Parsing with Rule Sequence Processors: 
an Appl icat ion to the Shared CoNLL Task 
Marc  V i la in  and Dav id  Day  
The MITRE Corporation 
Bedford, MA 01730, USA 
{mbv, day}Omitre,  org 
For several years, chunking has been an inte- 
gral part of MITRE's approach to information 
extraction. Our work exploits chunking in two 
principal ways. First, as part of our extraction 
system (Alembic) (Aberdeen et al, 1995), the 
chunker delineates descriptor phrases for entity 
extraction. Second, as part of our ongoing re- 
search in parsing, chunks provide the first level 
of a stratified approach to syntax - the second 
level is defined by grammatical relations, much 
as in the SPARKLE effort (Carroll et al, 1997). 
Because of our ongoing work with chunking, 
we were naturally interested in evaluating our 
approach on the common CoNLL task. In this 
note, we thus present three different evaluations 
of our work on phrase-level parsing. The first 
is a baseline of sorts, our own version of the 
"chunking as tagging" approach introduced by 
Ramshaw and Marcus (Ramshaw and Marcus, 
1995). The second set of results reports the 
performance of a trainable rule-based system, 
the Alembic phrase rule parser. As a point of 
comparison, we also include a third set of mea- 
sures produced by running the standard Alem- 
bic chunker on the common task with little or 
no adaptation. 
1 Chunking as Tagging 
For this first experiment, we coerced our part- 
of-speech tagger to generate chunk labels. We 
did so in what can only count as the most rudi- 
mentary way: by training the tagger to map 
part-of-speech labels to the chunk labels of the 
common task. The learning procedure is a re- 
implementation f Brill's transformation-based 
approach (Brill, 1993), extended to cover ap- 
proximately an order of magnitude more rule 
schemata. As input, the training corpus was 
tagged with the parts-of-speech from the corn- 
N 
1000 
2000 
4000 
accuracy precision 
86 77 
89 82 
89 81 
recall FB= 1 
77 77 
82 82 
81 81 
Table 1: Performance of the brute-force re- 
tagging approach 
mon data set: these provided an initial labeling 
of the data which was then directly converted 
to chunk labels through the action of transfor- 
mation rules (Brill's so-called contextual rules). 
Because the learning procedure is none the 
swiftest, we restricted ourselves to subsets of 
the training data, acquiring rules from the first 
1000, 2000, and 4000 sentences of the training 
set. In each case, we acquired 500 transforma- 
tion rules. We measured the following perfor- 
mance of these rules on the test set. 
These results are hardly stellar, falling some 
10 points of F below the performance of previ- 
ous approaches to noun group detection. To be 
sure, the chunking task is more demanding than 
the simple identification of noun group bound- 
aries, so one would expect lower performance on 
the harder problem. But the rudimentary way 
in which we implemented the approach is likely 
also to blame. 
There are a number of clear-cut ways in which 
we could attempt o improve our performance 
using this approach. In particular, we would ex- 
pect to obtain better esults if we did not oblit- 
erate the part-of-speech of a lexeme in the pro- 
cess of tagging it with a chunk label. Indeed, 
in our experiments, the learning procedure ac- 
quired transformations that simply replaced the 
part-of-speech tag with a chunking tag, thereby 
inhibiting potentially useful downstream rules 
for accessing the part-of-speech information of 
160 
a chunk-tagged word. 
2 Chunk ing  w i th  the  Phrase  Ru le  
Parser  
Our main interest in this common evaluation, 
however, was not to set new high-water marks 
with the approach of Ramshaw and Marcus, but 
to exercise our phrase rule parser. 
The Alembic phrase rule parser (Vilain and 
Day, 1996) provides the core of the system's 
syntactic processing. In our extraction appli- 
cations, the phraser (as we call it) initially tags 
named entities and other fixed-class constructs 
(like titles). The phraser also treats as atomic 
units the stereotypical combinations of named 
entities that one finds in newswire text, e.g., 
the person-title-organization apposition "U.N. 
secretary general Kofi Anan". The three com- 
ponents of the apposition axe initially parsed 
as fixed-class entities, and are then combined 
to form a single person-denoting phrase. These 
preliminary parsing steps provide part of the in- 
put to the chunker, which is itself implemented 
as a phrase rule parser. 
The architecture of the parser is based on 
Brill's approach. The parser follows a sequence 
of rules in order to build phrases out of parse is- 
lands. These islands are initially introduced by 
instantiating partial phrases around individual 
lexemes (useful for name tagging), or around 
runs of certain parts of speech (useful for both 
name tagging and chunking). It is the job of the 
phrase parsing rules to grow the boundaries of 
these phrases to the left or right, and to assign 
them a type, e.g., a name tag or a chunk la- 
bel. As with other rule sequence processors, the 
phraser proceeds in sequence through its cata- 
logue of rules, applying each in turn wherever it
matches, and then discarding it to proceed on 
to the next rule in the sequence. 
For example, in name tagging, we seed 
initial phrases around runs of capitalized 
words. A phrase such as "meetings in Paris 
and Rome" would produce an initial phrase 
analysis of "meetings in <?>Par is</?> and 
<?>Rome</?>",  where the "?" on the phrases 
are initial labels that indicate the phrase has 
not received a type. 
The patterns that are implemented byphrase 
parsing rules are similar to those in Brill's 
transformation-based p-o-s tagger. A rule can 
test for the presence of a given part of speech, of 
a lexeme, of a list of lexemes, and so on. These 
tests are themselves anchored to a specific locus 
(a phrase or lexeme) and are performed relative 
to that locus. As actions, the rules can grow the 
boundaries of a phrase, and set or modify its la- 
bel. For example, a typical name tagging rule 
would assign a LOCATION tag to any phrase 
preceded by the preposition "in". And indeed, 
this very rule tends to emerge as the very first 
rule acquired in training a phraser-based name 
tagger. We show it here with no further com- 
ment, trusting that its syntax is self-evident. 
(def-phraser-rule 
:conditions (:left-i :lex "in") 
:actions (:set-label :LOCATION)) 
In our particular example ("meetings in 
<?>Par is</?> and <?>Rome</?>") ,  this 
rule would re-label the <?> phrase around Paris 
with the LOCATION tag. A subsequent rule 
might then exploit the coordination to infer 
that "Rome" is a location as well, implement- 
ing the transformation "LOCATION and <?>" 
--+ "LOCATION and LOCATION". This incre- 
mental patching of errors is the hallmark of 
Brill's approach. 
An interesting property of this rule language 
is that the phraser can be operated either as 
a trainable procedure, using standard error- 
driven transformation learning, or as a hand- 
engineered system. For the purpose of the com- 
mon CoNLL task, let us first present our results 
for the trainable case. 
We again approached the task in a relatively 
rudimentary way, in this case by applying the 
phrase rule learning procedure with no partic- 
ular adaptation to the task. Indeed, the proce- 
dure can be parameterized by word lists which 
it can then exploit to improve its performance. 
Since our main interest here was to see our base- 
line performance on the task, we did not harvest 
such word lists from the training data (there is 
an automated way to do this). We ran a num- 
ber of training runs based on different partitions 
of the training data, with the following overall 
performance on test data, averaged across runs. 
accuracy 89 \]precision 89 I recall 
161 
test data precision The constituents hat were most accurately 
recognized were noun groups (F=88), with verb 
groups a close second (F=87). These were 
followed by the ostensibly easy cases of PP's 
(F=86), SBAR's (F=79), and ADVP's (F=75). 
Our lowest performing constituent for which 
the learning procedure actually generated rules 
was ADJP's (F=37), with no rules generated 
to identify CONJP's, INTJ's, LST's, or PRT's 
(F=0 in all these cases). 
In general, precision, tended to be several 
points of F higher than recall, and in the case 
of ADJP's average precision was 76 compared 
to average recall of 24! 
3 Chunk ing  w i th  the  
Hand-Eng ineered  System 
As a point of comparison, we also applied our 
hand-engineered chunker to the CoNLL task. 
We expected that it would not perfbrm at its 
best on this task, since it was designed with 
a significantly different model of chunking in 
mind, and indeed, unmodified, it produced is- 
appointing results: 
accuracy precision recall\[ Ffl- 1 
84 80 75 \[ 77 
The magnitude of our error term was some- 
thing of a surprise. With production runs 
on standard newswire stories (several hundred 
words in lengths) the chunker typically produces 
fewer errors per story than one can count on one 
hand. The discrepancy with the results mea- 
sured on the CoNLL task is of course due to 
the fact that our manually engineered parser 
was designed to produce chunks to a different 
standard. 
The standard was carefully defined so as to 
be maximally informative to downstream pro- 
cessing. Generally speaking, this means that it 
tends to make distinctions that are not made in 
the CoNLL data, e.g., splitting verbal runs such 
as "failed to realize" into individual verb groups 
when more than one event is denoted. 
Our curiosity about these discrepancies i
now piqued. As a point of further investiga- 
tion, we intend to apply the phraser's training 
procedure to adapt the manual chunker to the 
CoNLL task. With transformation-based rule 
sequences, this is easy to do: one merely trains 
the procedure to transform the output required 
ADJP  
ADVP 
CONJP  
INT J  
LST  
NP  
PP  
PRT  
SBAR 
VP  
all 
75.89% 
80.64% 
0.00% 
0.00% 
0.00% 
87.85% 
91.77% 
0.00% 
91.36% 
90.34% 
88.82% 
recall Fp=l 
24.43% 36.96 
70.21% 75.06 
0.00% 0.00 
0.00% 0.00 
0.0O% 0.00 
87.77% 87.81 
80.42% 85.72 
0.00% 0.00 
69.16% 78.72 
84.13% 87.13 
82.91% 85.76 
Table 2: The results of the phrase rule parser. 
for the one task into that required for the other. 
The rules acquired in this way are then sim- 
ply tacked on to the end of the original rule 
sequence (a half dozen such rules written by 
hand bring the performance of the chunker up 
to F=82, for example). 
A more interesting point of investigation, 
however, would be to analyze the discrepan- 
cies between current chunk standards from the 
standpoint of syntactic and semantic riteria. 
We look forward to reporting on this at some 
future point. 
Re ferences  
J. Aberdeen, J. Burger, D. Day, L. Hirschman, 
P. Robinson, and M. Vilain. 1995. Mitre: De- 
scription of the alembic system used for muc-6. 
In Proc. 6th Message Understanding Conference 
(MUC-6). Defense Advanced Research Projects 
Agency, November. 
E. Brill. 1993. A Corpus-based Approach to Lan- 
guage Learning. Ph.D. thesis, U. Pennsylvania. 
J. Carroll, T. Briscoe, N. Calzolari, S. Fed- 
erici, S. Montemagni, V. Pirrelli, G. Grefen- 
stette, A. Sanfilippo, G. Carroll, and M. Rooth. 
1997. SPARKLE work package 1, specifica- 
tion of phrasal parsing, final report. Avail- 
able at http://www, i lc .  pi. cnr. i t /spark le / -  
sparkle, htm, November. 
L. Ramshaw and M. Marcus. 1995. Text chunking 
using transformation-based learning. In Proc. of 
the 3rd Workshop on Very Large Corpora, pages 
82-94, Cambridge, MA, USA. 
M. Vilain and D. Day. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
16th Intl. Conference on Computational Linguis- 
tics (COLING-96). 
162 
