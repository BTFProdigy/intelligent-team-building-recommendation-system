Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 762?771, Dublin, Ireland, August 23-29 2014.
docrep: A lightweight and efficient document representation framework
Tim Dawborn and James R. Curran
e
-lab, School of Information Technologies
University of Sydney
NSW 2006, Australia
{tim.dawborn,james.r.curran}@sydney.edu.au
Abstract
Modelling linguistic phenomena requires highly structured and complex data representations.
Document representation frameworks (DRFs) provide an interface to store and retrieve multiple
annotation layers over a document. Researchers face a difficult choice: using a heavy-weight
DRF or implement a custom DRF. The cost is substantial, either learning a new complex system,
or continually adding features to a home-grown system that risks overrunning its original scope.
We introduce DOCREP, a lightweight and efficient DRF, and compare it against existing DRFs.
We discuss our design goals and implementations in C
++
, Python, and Java. We transform the
OntoNotes 5 corpus using DOCREP and UIMA, providing a quantitative comparison, as well as
discussing modelling trade-offs. We conclude with qualitative feedback from researchers who
have used DOCREP for their own projects. Ultimately, we hope DOCREP is useful for the busy
researcher who wants the benefits of a DRF, but has better things to do than to write one.
1 Introduction
Computational Linguistics (CL) is increasingly a data-driven research discipline with researchers us-
ing diverse collections of large-scale corpora (Parker et al., 2011). Representing linguistic phenomena
can require modelling intricate data structures, both flat and hierarchical, layered over the original text;
e.g. tokens, sentences, parts-of-speech, named entities, coreference relations, and trees. The scale and
complexity of the data demands efficient representations. A document representation framework (DRF)
should support the creation, storage, and retrieval of different annotation layers over collections of hetero-
geneous documents. DRFs typically store their annotations as stand-off annotations, treating the source
document as immutable and annotations ?stand-off? with offsets back into the document.
Researchers may choose to use a heavy-weight DRF, for example GATE (Cunningham et al., 2002)
or UIMA (G?otz and Suhre, 2004), but this can require substantial investment to learn and apply the
framework. Alternatively, researchers may ?roll-their-own? framework for a particular project. While
this is not inherently bad, our experience is that the scope of such smaller DRFs often creeps, without the
benefits of the features and stability present in mature DRFs. Moreover, some DRFs are based on object
serialisation, restricting the user to a specific language. In sum, while DRFs provide substantial benefits,
they can come at an opportunity cost to valuable research time.
DOCREP aims to solve this problem by proving a light-weight DRF that does not get in the way. Using
a language-agnostic storage layer enables reuse across different tasks in whatever tools and programming
languages are most appropriate. Efficiency is our primary goal, and we emphasise compact serialisation
and lazy loading. Our streaming design is informed by the pipeline operation of UNIX commands.
Section 2 compares existing DRFs and annotation schemes. We describe and introduce DOCREP in
Section 3, outlining the design goals and the problems it aims to solve. We compare DOCREP to UIMA
through a case study in Section 4, converting OntoNotes to both DRFs. Section 5 discusses real world uses
of DOCREP within our research group and outlines experiences of its use by NLP researchers. DOCREP
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
762
will be useful for any researcher who wants rapid development with multi-layered annotation that per-
forms well at scale, but at minimal technical cost.
2 Background
Easily and efficiently storing and retrieving linguistic annotations over corpora is a core issue for data-
driven linguistics. A number of attempts to formalise linguistic annotation formats have emerged over the
years, including Annotation Graphs (AG) (Bird and Liberman, 1999), the Linguistic Annotation Format
(LAF) (Ide and Romary, 2004, 2006), and more recently, the Graph Annotation Framework (GRAF)
(Ide and Suderman, 2007). GRAF is a serialisation of the LAF model, using XML stand-off annotations
to store layers of annotation. The GRAF representation is sufficiently abstract as to be used as a pivot
format between other annotation schemes. Ide and Suderman (2009) use GRAF as an intermediate format
to convert annotations between GATE and UIMA. The MASC corpus (Ide et al., 2010) has multiple layers
of annotation which are distributed in GRAF. Neumann et al. (2013) provide insight into the effectiveness
of GRAF as a format for corpus distribution when they import MASC into an annotation database. These
linguistic annotation formalisations provide a useful set of requirements for DRFs. While these abstract
formalisations are constructive from a theoretical perspective, they do not take into account the runtime
performance of abstract representations, nor their ease of use for programmers.
Several DRFs have been developed and used within the CL community. GATE (Cunningham et al.,
2002; Cunningham, 2002) has a focus on the human annotation of textual documents. While it has a
large collection of extensions and plugins, it was not designed in a matter than suits web-scale corpus
processing. Additionally, GATE is limited to Java, making integration with CL tools written in other
languages difficult. UIMA (G?otz and Suhre, 2004; Lally et al., 2008) is a Java framework for providing
annotations over the abstract definition of documents, providing functionality to link between different
views of the same document (e.g. translations of a document). UIMA calls these different views different
?subjects of analysis? (SOFA). When UIMA was adopted into the Apache Software Foundation, a C
++
version of the UIMA API was developed. However, it appears to lag behind behind the Java API in devel-
opment effort and usefulness, with many undocumented components, numerous external dependencies,
and with substantial missing functionality provided by the Java API. Additionally, the C
++
API is written
in an non-idiomatic manner, making it harder for developers to use.
Publicly available CL pipelining tools have emerged in recent years, providing a way to perform a wide
range of CL processes over documents. The Stanford NLP pipeline
1
is one such example, but is Java only
and must be run on a single machine. CURATOR (Clarke et al., 2012) provides a cross-language NLP
pipeline using Thrift to provide cross-language communication and RPC. CURATOR requires a server to
coordinate the components within the pipeline. Using pipelining functionality within a framework often
the inspection of per-component contributions more difficult. We are not aware of any DRFs which use a
streaming model to utilise UNIX pipelines, a paradigm CL researchers are already familiar with.
3 The docrep document representation framework
DOCREP (/d6krEp/), a portmanteau of document representation, is a lightweight, efficient, and modern
document representation framework for NLP systems that is designed to be simple to use and intuitive to
work with. We use the term lightweight to compare it to the existing document representation systems
used within the CL community, the main one being UIMA. The overhead of using DOCREP instead of a
flat-file format is minimal, especially in comparison to large bulky frameworks.
Our research group has used DOCREP as its primary data storage format in both research projects and
commercial projects since mid-2012. DOCREP has undergone an iterative design process during this time
as limitations and issues arose, allowing modelling issues to be ironed out and a set of best practices to be
established. These two years of solid use by CL researchers has resulted in a easy to use DRF we believe
is suitable for most CL applications and researchers.
DOCREP was designed with streaming in mind, facilitating from the data storage layer upwards the
ability for CL applications to utilise parallel processing. This streaming model is a model that many
1
http://nlp.stanford.edu/software/corenlp.shtml
763
CL researchers are already familiar with from writing UNIX pipelines (Church, 1994; Brew and Moens,
2002), again reducing the overhead required to use DOCREP.
DOCREP is not a new language that researchers need to learn. Instead, it is a serialisation protocol
and set of APIs to interact with annotations and documents. Using DOCREP is as simple as importing the
package in ones favourite programming language and annotating class definitions appropriately. Neither
a separate compilation step nor an external annotation definition file are required.
3.1 Idiomatic APIs
One of the motivations for constructing DOCREP was the lack of a good document representation frame-
work in programming languages other than Java. We have implemented DOCREP APIs in three com-
monly used programming languages in the CL community: C
++
, Python, and Java. All of these APIs are
open source and publicly available on GitHub,
2
released under the MIT licence. The C
++
API is written
in C
++
11, the Python API supports version 2.7 as well as versions ? 3.3, and the Java API supports
versions ? 6. All three APIs are setup to use the standard build tools for the language.
When implementing these APIs, we aimed to make the interface as similar as possible between the
three languages, while still feeling idiomatic within that language. Using the API should feel natural
for that language. Figure 1 shows an example set of identical model definitions in C
++
, Python, and
Java. This example defines a Token type, a Sent type spanning over a series of sequential Token
annotations, and a Doc type. The Token and Sent types include some annotation attributes. Annota-
tion instances are stored on the document in Stores. Apart from the missing implementations of the
Schema constructors in the C
++
example, these are complete and runnable definitions of annotation
types in DOCREP. The Schema classes in the C
++
example are automatically induced via runtime class
introspection in the Python and Java APIs; functionality which C
++
does not possess.
3.2 Serialisation protocol
We chose to reuse an existing serialisation format for DOCREP. This allows developers to use existing
serialisation libraries for processing DOCREP streams in languages we do not provide a DOCREP API for.
One of our design considerations when creating DOCREP was a desire for the protocol to be self-
describing. With a self-describing protocol, no external files need to be associated with a serialised
stream in order to know how to interpret the serialised data. This requires an efficient serialisation
protocol because including the definition of the type system with each document comes at a cost. This is
different to UIMA which requires its XML type definition files in order to deserialise the serialised data.
The four main competitors in the web-scale binary serialisation format space are BSON,
3
Mes-
sagePack,
4
Protocol Buffers,
5
and Thrift.
6
BSON and MessagePack are similar in their design. They both
aim to provide a general purpose data serialisation format for common data types and data structures.
BSON is used as the primary data representation within the MongoDB database. Protocol Buffers and
Thrift work in a similar manner to one another. Their serialisation protocols are not self describing and
require an external file which defines how to interpret the messages on the stream. In this external file,
users define the structure of the messages they wish to serialise and deserialise, and use a provided tool
to convert this external file into source code for their programming language of choice. Protocol Buffers
and Thrift also provide RPC functionality, however this was not needed for our situation. Thrift is used
by the CURATOR NLP pipeline (Clarke et al., 2012) to provide both serialisation and RPC functionality
between cross-language disjoint components in the pipeline.
After designing the serialisation protocol for DOCREP, we implemented it on top of these binary se-
rialisation formats in order to compare the size of the serialised data and the speed at which it could be
compressed. As a simple stand-off annotation task, we chose to use the CoNLL 2003 NER shared task
2
https://github.com/schwa-lab/libschwa
3
http://bsonspec.org/
4
http://msgpack.org/
5
http://code.google.com/p/protobuf/
6
http://thrift.apache.org/
764
struct Token : public dr::Ann {
dr::Slice<uint64_t> span;
std::string raw;
std::string norm;
class Schema;
};
struct Sent : public dr::Ann {
dr::Slice<Token
*
> span;
bool is_headline;
class Schema;
};
struct Doc : public dr::Doc {
dr::Store<Token> tokens;
dr::Store<Sent> sents;
class Schema;
};
struct Token::Schema : public dr::Ann::Schema<Token> {
DR_FIELD(&Token::span) span;
DR_FIELD(&Token::raw) raw;
DR_FIELD(&Token::norm) norm;
Schema(void);
};
struct Sent::Schema : public dr::Ann::Schema<Sent> {
DR_POINTER(&Sent::span, &Doc::tokens) tokens;
DR_FIELD(&Sent::is_headline) is_headline;
Schema(void);
};
struct Doc::Schema : public dr::Doc::Schema<Doc> {
DR_STORE(&Doc::tokens) tokens;
DR_STORE(&Doc::sents) sents;
Schema(void);
};
(a) C
++
example
class Token(dr.Ann):
span = dr.Slice()
raw = dr.Text()
norm = dr.Text()
class Sent(dr.Ann):
span = dr.Slice(Token)
is_headline = dr.Field()
class Doc(dr.Doc):
tokens = dr.Store(Token)
sents = dr.Store(Sent)
(b) Python example
@dr.Ann
public class Token extends AbstractAnn {
@dr.Field public ByteSlice span;
@dr.Field public String raw;
@dr.Field public String norm;
}
@dr.Ann
public class Sent extends AbstractAnn {
@dr.Pointer public Slice<Token> span;
@dr.Field public bool isHeadline;
}
@dr.Doc
public class Doc extends AbstractDoc {
@dr.Store public Store<Token> tokens;
@dr.Store public Store<Sent> sents;
}
(c) Java example
Figure 1: Examples of identical type definitions using the DOCREP API in C
++
, Python, and Java.
Self- Uncompressed DEFLATE Snappy LZMA
describing Time Size Time Size Time Size Time Size
Original data ? ? 31.30 1.0 5.95 0.1 9.81 39 0.39
BSON X 2.5 188.42 5.3 30.32 0.6 56.36 441 16.22
MessagePack X 1.6 52.15 3.2 16.61 0.3 24.82 61 4.36
Protocol Buffers ? 1.4 51.51 3.5 18.52 0.3 29.31 67 5.13
Thrift ? 1.0 126.12 3.5 20.64 0.4 33.69 224 10.99
Table 1: A comparison of binary serialisation libraries being used as the DOCREP serialisation format.
Times are reported in seconds and sizes in MB. MessagePack and BSON include the full type system
definition on the stream for each document whereas Protocol Buffers and Thrift do not.
765
data, randomly sampling around 50 MB worth of sentences from the English training data. The seriali-
sation stores the documents, sentences, and tokens, along with the POS and NER tags for the tokens. The
appropriate message specification files were written for Protocol Buffers and Thrift, and the type system
was serialised as a header for BSON and MessagePack.
Table 1 shows the results of this experiment. The reported size of the original data is smaller than the
sample size as we chose to output it in a more concise textual representation than the data was originally
distributed in. BSON performs noticeably worse than the others, in terms of both size and speed. While
serialising slightly faster, the size of the serialised data produced by Thrift is more then double the size of
both MessagePack and Protocol Buffers, and does not compress quite as well. MessagePack compressed
slightly better than Protocol Buffers and was on par in terms of speed, while being self-describing on the
stream. The result of this experiment and some similar others lead us to conclude that MessagePack was
the best serialisation format for DOCREP to use.
At the time of writing, the Python and Java DOCREP APIs use the official MessagePack libraries for
those languages. We implemented our own C
++
MessagePack library to facilitate laziness.
3.3 Laziness
The serialisation protocol was designed such that we could make the streaming aspect of DOCREP as effi-
cient as possible. Before each collection of annotation objects appears in the serialised data, the number
of bytes used to store the serialised annotations is stored. If the current application is not interested in
the particular annotation types that are about to be read in, it can simply skip over the correct number of
bytes without having to deserialise the internal MessagePack structure.
All three of our APIs implement this laziness. Only the types of annotations that the application
specifies interest in will be deserialised at runtime. The other types of annotations will simply be kept in
their serialised format and written back out to the output stream unmodified. This is also true for attributes
on annotations that the current application is not interested in. The Python API provides an option to
fully instantiate each of the types at runtime, even if you have not defined classes for them. Unknown
annotation types will have classes created at runtime based on the schema of the types described in the
serialisation protocol.
3.4 Processing tools
We trade-off performance against easy inspection of files. We provide a set of command-line tools for
manipulating, filtering, and distributing DOCREP streams. The command-line tools mimic the standard
set of UNIX tools used to process textual files as well as some other stream introspection and statistics
gathering tools. All of these tools and their uses are documented on the DOCREP website.
7
Our provided
toolbox for processing DOCREP streams contains tools for counting, visualising, filtering, ordering, par-
titioning, and exporting DOCREP streams. Due to space limitations in this paper, we are unable to go into
these tools in detail.
Below are two examples of some of the tools in action. The first example filters the documents by a
regular expression comparison against their ID attribute, and then outputs the ID of the document with
the most number of tokens. The second randomly chooses 10 documents from a stream, passing them to
another tool, and then opens the first returned document in the stream visualiser.
$ dr grep 'doc.id ? /x-\d+/' corpus.dr | dr count -s tokens | sort -rn | head -n 1
$ dr sample -n 10 corpus.dr | ./my-tool | dr head -n 1 | dr less
3.5 Streaming model
Emphasising the fact that the DOCREP protocol is a streaming protocol, combining multiple DOCREP
files together is as simple as concatenating the files together. The DOCREP deserialisers expect an input
stream to contain zero or more serialised documents. Being able to easily distribute all documents in a
corpus along with their annotation layers as a single file is very attractive.
7
https://github.com/schwa-lab/libschwa
766
This kind of streaming model makes distributed processing very easy using a typical work queue
model. A distributed pipeline ?source? can serve the documents from the DOCREP stream by reading
them off the input stream without having to deserialise them (subsection 3.3) and a ?sink? can simply
concatenate the received documents together to the output stream, again without having to deserialise
them. We provide a DOCREP source and sink distributed processing tool along with APIs for easily
writing worker clients. The distribution is achieved through ?MQ
8
which allows for both scale-up and
scale-out distributed processing out of the box without the need for a separate controller process to
manage communication between client processes.
4 Case study: OntoNotes 5
The OntoNotes 5 corpus (Pradhan et al., 2013) is a large corpus of linguistically annotated documents
from multiple genres in three different languages. This 5th release covers newswire, broadcast news,
broadcast conversation, and web data in English and Chinese, a pivot corpus in English, and newswire
data in Arabic. Roughly half of the broadcast conversation data is parallel data, with some of the docu-
ments providing tree-to-tree alignments. Of the 15 710 documents in the corpus, 13 109 are in English,
2002 are in Chinese, and 599 are in Arabic.
Each of the documents in the OntoNotes 5 corpus contain multiple layers of syntactic and semantic
annotations. It builds upon the Penn Treebank for syntax and PropBank for predicate-argument structure,
adding named entities, coreference, and word sense disambiguation layers to some documents.
The annotations in the OntoNotes 5 corpus are provided in two different formats: as a series of flat
files (340 MB) per document with each file containing one annotation layer, and as a relational database
in the form of a SQL file (5812 MB). Both of these data formats have usability issues. Working with
the flat files requires parsing each of the different file formats and aligning the data between the files for
the same document. Working with the database requires working out how the tables are related to one
another, as well as knowledge of SQL, or having access to an efficient API for querying the database.
To outline the effectiveness of document representation frameworks, and in particular the efficiency
of DOCREP, we provide code to convert the OntoNotes 5 corpus into both DOCREP and UIMA represen-
tations, comparing the conversion time, resultant size on disk, and ease of doing this conversion. We
provide conversion scripts in all three languages for DOCREP and in Java and C
++
for UIMA. Addi-
tionally, we also provide a verification script, reproducing the original OntoNotes 5 flat files from the
document representation form, ensuring that no data was lost in the conversion.
4.1 Modelling decisions
The choices made on how to model the different annotation layers were almost identical in UIMA and
DOCREP. The main difference occurs when you have an annotation over a sequential span of other
annotations. UIMA has no way to model this directly. The most common way users choose to model
this is as a normal Annotation subtype with its begin offset set to the begin offset of the first
covered annotation and its end offset set to the end offset of the last covered annotation. An example of
this situation is named entity annotations. In OntoNotes, named entities are represented as annotations
over a sequence of token annotations. How this is represented in UIMA is shown in the XML snippet in
Figure 2. The main disadvantage in this modelling approach is that there is then no direct representation
that the named entity annotation is an annotation over a sequence of token annotations. In DOCREP,
named entity annotation is directly modelled as a sequence of token annotations. The DOCREP definition
for the named entity type is shown on the right hand side of Figure 2.
DOCREP does not allow for the direct modelling of cross-document information. This occurs in the
OntoNotes 5 corpus in the form of the parallel document and parallel tree information. Because DOCREP
is a streaming protocol, the documents are thought of as independent from one another and as such, no
formal relationships between the documents can be made at the framework level. This parallel document
information can still be be stored as metadata on the documents. This situation is dealt with in UIMA by
the SOFA.
8
http://www.zeromq.org/
767
<typeDescription>
<name>
ontonotes5.to_uima.types.NamedEntity
</name>
<description/>
<supertypeName>
uima.tcas.Annotation
</supertypeName>
<features>
<featureDescription>
<name>tag</name>
<description>The NE tag.</description>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
<featureDescription>
<name>startOffset</name>
<description>Character offset into the start token.</description>
<rangeTypeName>uima.cas.Integer</rangeTypeName>
</featureDescription>
<featureDescription>
<name>endOffset</name>
<description>Character offset into the end token.</description>
<rangeTypeName>uima.cas.Integer</rangeTypeName>
</featureDescription>
</features>
</typeDescription>
@dr.Ann
public class NamedEntity extends AbstractAnn {
@dr.Pointer public Slice<Token> span;
@dr.Field public String tag;
@dr.Field public int startOffset;
@dr.Field public int endOffset;
}
Figure 2: Defining the named entity annotation type in UIMA (left) and the DOCREP Java API (top-right).
UIMA DOCREP
Java Java Java Java C
++
C
++
C
++
Java C
++
Python
XMI XCAS bin cbin XMI XCAS bin ? ? ?
Conversion time 25 25 25 25 77 77 77 12 12 27
Serialisation time 131 122 2103 76 630 611 695 61 23 32
Size on disk 1894 3252 1257 99 2141 3252 2135 371 371 371
Table 2: A comparison of the resources required to represent the OntoNotes 5 corpus in UIMA and
DOCREP. Times are reported in seconds and sizes are reported in MB.
4.2 Empirical results
In these experiments, we first load all of the data into memory from the database for the current document
we are processing. This data is stored in an object structure which knows nothing about document
representation frameworks. We then convert this object representation into the appropriate UIMA and
DOCREP annotations, recording how long the conversion took. The UIMA and DOCREP versions of the
documents are then serialised to disk, recording how long the serialisation took and the resultant size
on disk. All of these performance experiments were run on the same isolated machine, running 64-bit
Ubuntu 12.04, using OpenJDK 1.7, CPython 2.7, and gcc 4.8.
In order to provide a fair comparison between UIMA and DOCREP, we perform the conversion using
both the Java and C
++
UIMA APIs, as well as using all three DOCREP APIs (Java, C
++
, and Python).
The code to load the data from the database and construct the in-memory object structure was common
between the UIMA and DOCREP conversions. For UIMA, we serialise in all available output formats: both
the XMI and XCAS XML formats, the binary format (bin), and the compressed binary (cbin) format. The
UIMA C
++
API does not appear to support output in the compressed binary format.
The result of this conversion process can be seen in Table 2. The first row shows the accumulated
time taken to convert all of the documents from their in-memory representation into UIMA and DOCREP
annotations. As visible in the table, DOCREP performs this conversion twice as fast as UIMA in Java
and six times as fast as UIMA in C
++
?
The second row shows the accumulated time taken to serialise
768
Flat DOCREP UIMA UIMA UIMA UIMA SQL MySQL MySQL
files XMI XCAS bin cbin -indices +indices
Uncompressed 340 371 1894 3252 1257 99 4560 4303 5812
gzip (DEFLATE) 52 115 268 330 375 66 646 ? ?
xz (LZMA) 30 69 144 185 150 65 262 ? ?
Table 3: A comparison of the how well each of the annotation serialisation formats compress using
standard compression libraries. All sizes are reported in MB.
all of the documents to disk. DOCREP serialises up to 34 times faster than UIMA in Java, depending
on the UIMA output format, and up to 30 times faster in C
++
?
The third row in this table shows the
accumulated serialisation size on disk. Apart from the compressed binary output format in UIMA (cbin),
DOCREP serialisation requires up to nine times less space than UIMA
?
We are unsure why the sizes for the
different output formats in UIMA do not match up between the Java and C
++
APIs
?
We are also unsure
why the UIMA Java binary serialisation is so slow, especially in comparison to the compressed binary
serialisation.
Table 3 shows how well each of the serialisation formats compress using three standard compression
libraries. Each of these compression libraries were run with their default settings. The files generated
by UIMA as well as the ?flat file? files were first placed into a tarball so that the compression algorithms
could be run over the whole corpus instead of per document. The ?flat files? used were the original
OntoNotes 5 flat files containing the annotation layers that were converted. The SQL numbers are using
the original OntoNotes 5 SQL file. The MySQL numbers are obtained after loading the original SQL into
a MySQL database and obtaining table and index sizes from the information_schema.tables
table. The MySQL database was not altered from the initial import. Unsurprisingly, the DOCREP binary
representation does not compress as well as textual serialisation formats with lots of repetition, such as
XML or the original stand-off annotation files. However, under all of these reported situations, apart
from the UIMA compressed binary format, our DOCREP representation is two to five times smaller than
its UIMA counterpart, and 15 times smaller than the representation in MySQL. The UIMA compressed
binary (cbinary) format has already been compressed so it is unsurprising that compressing it further
makes little difference.
5 Usability
We have primarily evaluated the usefulness of DOCREP from an efficiency perspective, reporting time
and space requirements for a complex corpus conversion. In this section, we provide feedback from NLP
researchers in our lab who have been using DOCREP over the past two years for a variety of NLP tasks.
As researchers ourselves, we are aware of how valuable research time is. We provide these real-world
examples of DOCREP?s use to solidifying that DOCREP is a valuable tool for researchers.
Coreference DOCREP is a great tool for this project as all we want to do is develop a good coreference
system; we do not want to have to worry about the storage of data. Having an API in Python is
super convenient, allowing us to write code that changes frequently as we try new ideas. Related
publication: Webster and Curran (2014)
Event Linking Some work on Event Linking sought to work with gold annotations on one hand, and
knowledge from web-based hyperlinks on the other. For some processes these data sources were
to be treated identically, and for some differently. DOCREP?s extensibility easily supported this
use-case, while providing a consistent polymorphic abstraction that made development straightfor-
ward, while incorporating many other layers of annotation such as extracted temporal relations.
Separately, describing the relationship between a pair of documents in DOCREP was a challenging
use-case that required more engineering and fore-thought than most DOCREP applications so far.
Related publication: Nothman et al. (2012).
769
Named Entity Linking Our approach to NEL uses a pipeline of components and we initially wrote
our own DRF using Python?s object serialisation. While this worked well initially, we accrued
technical debt as we added features with minimal refactoring. Before too long, a substantial part
of our experiment runtime was devoted to dataset loading and storage. DOCREP made this easier
and using UNIX pipelines over structured document objects is a productive workflow. Related
publications: Radford et al. (2012); Pink et al. (2013).
Quote Extraction and Attribution For this task we performed experiments over four corpora, all with
distinct data formats and assumptions. Our early software loaded each format into memory, which
was a slow, error-prone, and hard-to-debug process. This approach became completely unusable
when we decided to experiment with coreference systems, as it introduced even more unique data
formats. Converting everything to DOCREP greatly simplified the task, as we could represent ev-
erything we needed efficiently, and within one representation system. We also gained a nice speed
boost, and were able to write a simple set of tests that examined a given DOCREP file for validity,
which greatly improved our code quality. Related publication: O?Keefe et al. (2013).
Slot Filling Being one of the last stages in an NLP pipeline, slot filling utilises all of the document
information it can get its hands on. Being able to easily accept annotation layers from prior NLP
components allows us to focus on slot filling instead of component integration engineering. Having
access to a multi-language API means we are able to write efficiency-critical code in C
++
and the
more experimental and dynamic components in Python.
6 Conclusion
We present a light-weight and easy-to-use document representation framework for the busy NLP re-
searcher who wants to model document structure, but does not want to use a heavy-weight DRF. We
provide empirical evidence of the efficiency of DOCREP, and provide insights into its use within our
research group over the past two years. We believe NLP other researchers will benefit from DOCREP as
they are now able to utilise the usefulness of a DRF without it getting in the way of their research time.
Acknowledgments
We would like to thank the anonymous reviewers for their useful feedback. We would also like to thank
Will Radford and Joel Nothman for their contributions to this paper as well as to DOCREP itself over the
past years. This work was supported by ARC Discovery grant DP1097291 and the Capital Markets CRC
Computable News project.
References
Steven Bird and Mark Liberman. 1999. A formal framework for linguistic annotation. Speech Commu-
nication, 33:23?60.
Chris Brew and Marc Moens. 2002. Data-intensive linguistics. HCRC Language Technology Group,
University of Edinburgh.
Kenneth Ward Church. 1994. Unix? for poets. Notes of a course from the European Summer School on
Language and Speech Communication, Corpus Based Methods.
James Clarke, Vivek Srikumar, Mark Sammons, and Dan Roth. 2012. An NLP curator (or: How I learned
to stop worrying and love NLP pipelines). In Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12). Istanbul, Turkey.
Hamish Cunningham. 2002. GATE, a general architecture for text engineering. Computers and the
Humanities, 36:223?254.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: an archi-
tecture for development of robust HLT applications. In Proceedings of 40th Annual Meeting of the
Association for Computational Linguistics, pages 168?175. Association for Computational Linguis-
tics, Philadelphia, Pennsylvania, USA.
770
T. G?otz and O. Suhre. 2004. Design and implementation of the UIMA common analysis system. IBM
Systems Journal, 43(3):476?489.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Rebecca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the people. In Proceedings of the ACL 2010 Conference
Short Papers, pages 68?73. Uppsala, Sweden.
Nancy Ide and Laurent Romary. 2004. International standard for a linguistic annotation framework.
Natural Language Engineering, 10(3-4):211?225.
Nancy Ide and Laurent Romary. 2006. Representing linguistic corpora and their annotations. In Pro-
ceedings of the Fifth Language Resources and Evaluation Conference LREC.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-based format for linguistic annotations. In
Proceedings of the Linguistic Annotation Workshop, pages 1?8. Association for Computational Lin-
guistics, Prague, Czech Republic.
Nancy Ide and Keith Suderman. 2009. Bridging the Gaps: Interoperability for GrAF, GATE, and UIMA.
In Proceedings of the Third Linguistic Annotation Workshop, pages 27?34. Association for Computa-
tional Linguistics, Suntec, Singapore.
Adam Lally, Karin Verspoor, and Eoric Nyberg. 2008. Unstructured Information Management Architec-
ture (UIMA) Version 1.0. Standards Specification 5, OASIS.
Arne Neumann, Nancy Ide, and Manfred Stede. 2013. Importing MASC into the ANNIS linguistic
database: A case study of mapping GrAF. In Proceedings of the 7th Linguistic Annotation Workshop
and Interoperability with Discourse, pages 98?102. Sofia, Bulgaria.
Joel Nothman, Matthew Honnibal, Ben Hachey, and James R. Curran. 2012. Event linking: grounding
event reference in a news archive. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pages 228?232. Jeju, Korea.
Tim O?Keefe, James R. Curran, Peter Ashwell, and Irena Koprinska. 2013. An annotated corpus of
quoted opinions in news articles. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pages 516?520. Association for Computational
Linguistics, Sofia, Bulgaria.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth
Edition. Technical report, Linguistic Data Consortium, Philadelphia.
Glen Pink, Will Radford, Will Cannings, Andrew Naoum, Joel Nothman, Daniel Tse, and James R.
Curran. 2013. SYDNEY CMCRC at TAC 2013. In Proceedings of the Text Analysis Conference.
National Institute of Standards and Technology, Gaithersburg, MD USA.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj?orkelund, Olga
Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards Robust Linguistic Analysis using
OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language
Learning, pages 143?152. Sofia, Bulgaria.
Will Radford, Will Cannings, Andrew Naoum, Joel Nothman, Glen Pink, Daniel Tse, and James R.
Curran. 2012. (Almost) Total Recall ? SYDNEY CMCRC at TAC 2012. In Proceedings of the Text
Analysis Conference. National Institute of Standards and Technology, Gaithersburg, MD USA.
Kellie Webster and James R. Curran. 2014. Low memory incremental coreference resolution. In Pro-
ceedings of COLING 2014. The COLING 2014 Organizing Committee, Dublin, Ireland. To appear.
771
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345?355,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Faster Parsing by Supertagger Adaptation
Jonathan K. Kummerfelda Jessika Roesner b Tim Dawborna James Haggertya
James R. Currana? Stephen Clark c?
School of Information Technologiesa Department of Computer Scienceb Computer Laboratoryc
University of Sydney University of Texas at Austin University of Cambridge
NSW 2006, Australia Austin, TX, USA Cambridge CB3 0FD, UK
james@it.usyd.edu.aua? stephen.clark@cl.cam.ac.uk c?
Abstract
We propose a novel self-training method
for a parser which uses a lexicalised gram-
mar and supertagger, focusing on increas-
ing the speed of the parser rather than
its accuracy. The idea is to train the su-
pertagger on large amounts of parser out-
put, so that the supertagger can learn to
supply the supertags that the parser will
eventually choose as part of the highest-
scoring derivation. Since the supertag-
ger supplies fewer supertags overall, the
parsing speed is increased. We demon-
strate the effectiveness of the method us-
ing a CCG supertagger and parser, obtain-
ing significant speed increases on newspa-
per text with no loss in accuracy. We also
show that the method can be used to adapt
the CCG parser to new domains, obtain-
ing accuracy and speed improvements for
Wikipedia and biomedical text.
1 Introduction
In many NLP tasks and applications, e.g. distribu-
tional similarity (Curran, 2004) and question an-
swering (Dumais et al, 2002), large volumes of
text and detailed syntactic information are both
critical for high performance. To avoid a trade-
off between these two, we need to increase parsing
speed, but without losing accuracy.
Parsing with lexicalised grammar formalisms,
such as Lexicalised Tree Adjoining Grammar and
Combinatory Categorial Grammar (CCG; Steed-
man, 2000), can be made more efficient using a
supertagger. Bangalore and Joshi (1999) call su-
pertagging almost parsing because of the signifi-
cant reduction in ambiguity which occurs once the
supertags have been assigned.
In this paper, we focus on the CCG parser and
supertagger described in Clark and Curran (2007).
Since the CCG lexical category set used by the su-
pertagger is much larger than the Penn Treebank
POS tag set, the accuracy of supertagging is much
lower than POS tagging; hence the CCG supertag-
ger assigns multiple supertags1 to a word, when
the local context does not provide enough infor-
mation to decide on the correct supertag.
The supertagger feeds lexical categories to the
parser, and the two interact, sometimes using mul-
tiple passes over a sentence. If a spanning analy-
sis cannot be found by the parser, the number of
lexical categories supplied by the supertagger is
increased. The supertagger-parser interaction in-
fluences speed in two ways: first, the larger the
lexical ambiguity, the more derivations the parser
must consider; second, each further pass is as
costly as parsing a whole extra sentence.
Our goal is to increase parsing speed without
loss of accuracy. The technique we use is a form
of self-training, in which the output of the parser is
used to train the supertagger component. The ex-
isting literature on self-training reports mixed re-
sults. Clark et al (2003) were unable to improve
the accuracy of POS tagging using self-training.
In contrast, McClosky et al (2006a) report im-
proved accuracy through self-training for a two-
stage parser and re-ranker.
Here our goal is not to improve accuracy, only
to maintain it, which we achieve through an adap-
tive supertagger. The adaptive supertagger pro-
duces lexical categories that the parser would have
used in the final derivation when using the base-
line model. However, it does so with much lower
ambiguity levels, and potentially during an ear-
lier pass, which means sentences are parsed faster.
By increasing the ambiguity level of the adaptive
models to match the baseline system, we can also
slightly increase supertagging accuracy, which can
lead to higher parsing accuracy.
1We use supertag and lexical category interchangeably.
345
Using the parser to generate training data also
has the advantage that it is not a domain specific
process. Previous work has shown that parsers
typically perform poorly outside of their train-
ing domain (Gildea, 2001). Using a newspaper-
trained parser, we constructed new training sets for
Wikipedia and biomedical text. These were used
to create new supertagging models adapted to the
different domains.
The self-training method of adapting the su-
pertagger to suit the parser increased parsing speed
by more than 50% across all three domains, with-
out loss of accuracy. Using an adapted supertagger
with ambiguity levels tuned to match the baseline
system, we were also able to increase F-score on
labelled grammatical relations by 0.75%.
2 Background
Many statistical parsers use two stages: a tag-
ging stage that labels each word with its gram-
matical role, and a parsing stage that uses the tags
to form a parse tree. Lexicalised grammars typ-
ically contain a much smaller set of rules than
phrase-structure grammars, relying on tags (su-
pertags) that contain a more detailed description
of each word?s role in the sentence. This leads to
much larger tag sets, and shifts a large proportion
of the search for an optimal derivation to the tag-
ging component of the parser.
Figure 1 gives two sentences and their CCG
derivations, showing how some of the syntactic
ambiguity is transferred to the supertagging com-
ponent in a lexicalised grammar. Note that the
lexical category assigned to with is different in
each case, reflecting the fact that the prepositional
phrase attaches differently. Either we need a tag-
ging model that can resolve this ambiguity, or both
lexical categories must be supplied to the parser
which can then attempt to resolve the ambiguity
by eventually selecting between them.
2.1 Supertagging
Supertaggers typically use standard linear-time
tagging algorithms, and only consider words in the
local context when assigning a supertag. The C&C
supertagger is similar to the Ratnaparkhi (1996)
tagger, using features based on words and POS
tags in a five-word window surrounding the target
word, and defining a local probability distribution
over supertags for each word in the sentence, given
the previous two supertags. The Viterbi algorithm
I ate pizza with cutlery
NP (S\NP)/NP NP ((S\NP)\(S\NP))/NP NP
> >
S\NP (S\NP)\(S\NP)
<
S\NP
<
S
I ate pizza with anchovies
NP (S\NP)/NP NP (NP\NP)/NP NP
>
NP\NP
<
NP
>
S\NP
<
S
Figure 1: Two CCG derivations with PP ambiguity.
can be used to find the most probable supertag se-
quence. Alternatively the Forward-Backward al-
gorithm can be used to efficiently sum over all se-
quences, giving a probability distribution over su-
pertags for each word which is conditional only on
the input sentence.
Supertaggers can be made accurate enough for
wide coverage parsing using multi-tagging (Chen
et al, 1999), in which more than one supertag
can be assigned to a word; however, as more su-
pertags are supplied by the supertagger, parsing
efficiency decreases (Chen et al, 2002), demon-
strating the influence of lexical ambiguity on pars-
ing complexity (Sarkar et al, 2000).
Clark and Curran (2004) applied supertagging
to CCG, using a flexible multi-tagging approach.
The supertagger assigns to a word all lexical cate-
gories whose probabilities are within some factor,
?, of the most probable category for that word.
When the supertagger is integrated with the C&C
parser, several progressively lower ? values are
considered. If a sentence is not parsed on one
pass then the parser attempts to parse the sentence
again with a lower ? value, using a larger set of
categories from the supertagger. Since most sen-
tences are parsed at the first level (in which the av-
erage number of supertags assigned to each word
is only slightly greater than one), this provides
some of the speed benefit of single tagging, but
without loss of coverage (Clark and Curran, 2004).
Supertagging has since been effectively applied
to other formalisms, such as HPSG (Blunsom and
Baldwin, 2006; Zhang et al, 2009), and as an in-
formation source for tasks such as Statistical Ma-
chine Translation (Hassan et al, 2007). The use
of parser output for supertagger training has been
explored for LTAG by Sarkar (2007). However, the
focus of that work was on improving parser and
supertagger accuracy rather than speed.
346
Previously , watch imports were denied such duty-free treatment
S/S , N /N N (S [dcl ]\NP)/(S [pss]\NP) (S [pss]\NP)/NP NP/NP N/N N
N N (S[dcl]\NP)/NP S [pss]\NP (N /N )/(N /N )
S [adj ]\NP (S [dcl ]\NP)/(S [adj ]\NP) (S [pss]\NP)/NP N /N
(S [pt ]\NP)/NP
(S[dcl]\NP)/NP
Figure 2: An example sentence and the sets of categories assigned by the supertagger. The first category
in each column is correct and the categories used by the parser are marked in bold. The correct category
for watch is included here, for expository purposes, but in fact was not provided by the supertagger.
2.2 Semi-supervised training
Previous exploration of semi-supervised training
in NLP has focused on improving accuracy, often
for the case where only small amounts of manually
labelled training data are available. One approach
is co-training, in which two models with indepen-
dent views of the data iteratively inform each other
by labelling extra training data. Sarkar (2001) ap-
plied co-training to LTAG parsing, in which the su-
pertagger and parser provide the two views. Steed-
man et al (2003) extended the method to a variety
of parser pairs.
Another method is to use a re-ranker (Collins
and Koo, 2002) on the output of a system to gener-
ate new training data. Like co-training, this takes
advantage of a different view of the data, but the
two views are not independent as the re-ranker is
limited to the set of options produced by the sys-
tem. This method has been used effectively to
improve parsing performance on newspaper text
(McClosky et al, 2006a), as well as adapting a
Penn Treebank parser to a new domain (McClosky
et al, 2006b).
As well as using independent views of data to
generate extra training data, multiple views can be
used to provide constraints at test time. Holling-
shead and Roark (2007) improved the accuracy
of a parsing pipeline by using the output of later
stages to constrain earlier stages.
The only work we are aware of that uses self-
training to improve the efficiency of parsers is van
Noord (2009), who adopts a similar idea to the
one in this paper for improving the efficiency of
a Dutch parser based on a manually constructed
HPSG grammar.
3 Adaptive Supertagging
The purpose of the supertagger is to cut down the
search space for the parser by reducing the set of
categories that must be considered for each word.
A perfect supertagger would assign the correct cat-
egory to every word. CCG supertaggers are about
92% accurate when assigning a single lexical cate-
gory to each word (Clark and Curran, 2004). This
is not accurate enough for wide coverage parsing
and so a multi-tagging approach is used instead.
In the final derivation, the parser uses one category
from each set, and it is important to note that hav-
ing the correct category in the set does not guaran-
tee that the parser will use it.
Figure 2 gives an example sentence and the sets
of lexical categories supplied by the supertagger,
for a particular value of ?.2 The usual target of
the supertagging task is to produce the top row of
categories in Figure 2, the correct categories. We
propose a new task that instead aims for the cat-
egories the parser will use, which are marked in
bold for this case. The purpose of this new task is
to improve speed.
The reason speed will be improved is that we
can construct models that will constrain the set of
possible derivations more than the baseline model.
We can construct these models because we can
obtain much more of our target output, parser-
annotated sentences, than we could for the gold-
standard supertagging task.
The new target data will contain tagging errors,
and so supertagging accuracy measured against
the correct categories may decrease. If we ob-
tained perfect accuracy on our new task then we
would be removing all of the categories not cho-
sen by the parser. However, parsing accuracy will
not decrease since the parser will still receive the
categories it would have used, and will therefore
be able to form the same highest-scoring deriva-
tion (and hence will choose it).
To test this idea we parsed millions of sentences
2Two of the categories for such have been left out for
reasons of space, and the correct category for watch has been
included for expository reasons. The fact that the supertagger
does not supply this category is the reason that the parser does
not analyse the sentence correctly.
347
in three domains, producing new data annotated
with the categories that the parser used with the
baseline model. We constructed new supertagging
models that are adapted to suit the parser by train-
ing on the combination of these sets and the stan-
dard training corpora. We applied standard evalu-
ation metrics for speed and accuracy, and explored
the source of the changes in parsing performance.
4 Data
In this work, we consider three domains: news-
wire, Wikipedia text and biomedical text.
4.1 Training and accuracy evaluation
We have used Sections 02-21 of CCGbank (Hock-
enmaier and Steedman, 2007), the CCG version of
the Penn Treebank (Marcus et al, 1993), as train-
ing data for the newspaper domain. Sections 00
and 23 were used for development and test eval-
uation. A further 113,346,430 tokens (4,566,241
sentences) of raw data from the Wall Street Jour-
nal section of the North American News Corpus
(Graff, 1995) were parsed to produce the training
data for adaptation. This text was tokenised us-
ing the C&C tools tokeniser and parsed using our
baseline models. For the smaller training sets, sen-
tences from 1988 were used as they would be most
similar in style to the evaluation corpus. In all ex-
periments the sentences from 1989 were excluded
to ensure no overlap occurred with CCGbank.
As Wikipedia text we have used 794,024,397
tokens (51,673,069 sentences) from Wikipedia ar-
ticles. This text was processed in the same way as
the NANC data to produce parser-annotated train-
ing data. For supertagger evaluation, one thousand
sentences were manually annotated with CCG lex-
ical categories and POS tags. For parser evalua-
tion, three hundred of these sentences were man-
ually annotated with DepBank grammatical rela-
tions (King et al, 2003) in the style of Briscoe
and Carroll (2006). Both sets of annotations were
produced by manually correcting the output of the
baseline system. The annotation was performed
by Stephen Clark and Laura Rimell.
For the biomedical domain we have used sev-
eral different resources. As gold standard data for
supertagger evaluation we have used supertagged
GENIA data (Kim et al, 2003), annotated by
Rimell and Clark (2008). For parsing evalua-
tion, grammatical relations from the BioInfer cor-
pus were used (Pyysalo et al, 2007), with the
Source Sentence Length Corpus %
Range Average Variance
0-4 3.26 0.64 1.2
5-20 14.04 17.41 39.2
News 21-40 28.76 29.27 49.4
41-250 49.73 86.73 10.2
All 24.83 152.15 100.0
0-4 2.81 0.60 22.4
5-20 11.64 21.56 48.9
Wiki 21-40 28.02 28.48 24.3
41-250 49.69 77.70 4.5
All 15.33 154.57 100.0
0-4 2.98 0.75 0.9
5-20 14.54 15.14 41.3
Bio 21-40 28.49 29.34 48.0
41-250 49.17 68.34 9.8
All 24.53 139.35 100.0
Table 1: Statistics for sentences in the supertagger
training data. Sentences containing more than 250
tokens were not included in our data sets.
same post-processing process as Rimell and Clark
(2009) to convert the C&C parser output to Stan-
ford format grammatical relations (de Marneffe
et al, 2006). For adaptive training we have
used 1,900,618,859 tokens (76,739,723 sentences)
from the MEDLINE abstracts tokenised by McIn-
tosh and Curran (2008). These sentences were
POS-tagged and parsed twice, once as for the
newswire and Wikipedia data, and then again, us-
ing the bio-specific models developed by Rimell
and Clark (2009). Statistics for the sentences in
the training sets are given in Table 1.
4.2 Speed evaluation data
For speed evaluation we held out three sets of sen-
tences from each domain-specific corpus. Specif-
ically, we used 30,000, 4,000 and 2,000 unique
sentences of length 5-20, 21-40 and 41-250 tokens
respectively. Speeds on these length controlled
sets were combined to calculate an overall pars-
ing speed for the text in each domain. Note that
more than 20% of the Wikipedia sentences were
less than five words in length and the overall dis-
tribution is skewed towards shorter sentences com-
pared to the other corpora.
5 Evaluation
We used the hybrid parsing model described in
Clark and Curran (2007), and the Viterbi decoder
to find the highest-scoring derivation. The multi-
pass supertagger-parser interaction was also used.
The test data was excluded from training data
for the supertagger for all of the newswire and
Wikipedia models. For the biomedical models ten-
348
fold cross validation was used. The accuracy of
supertagging is measured by multi-tagging at the
first ? level and considering a word correct if the
correct tag is amongst any of the assigned tags.
For the biomedical parser evaluation we have
used the parsing model and grammatical relation
conversion script from Rimell and Clark (2009).
Our timing measurements are calculated in two
ways. Overall times were measured using the C&C
parser?s timers. Individual sentence measurements
were made using the Intel timing registers, since
standard methods are not accurate enough for the
short time it takes to parse a single sentence.
To check whether changes were statistically sig-
nificant we applied the test described by Chinchor
(1995). This measures the probability that two sets
of responses are drawn from the same distribution,
where a score below 0.05 is considered significant.
Models were trained on an Intel Core2Duo
3GHz with 4GB of RAM. The evaluation was per-
formed on a dual quad-core Intel Xeon 2.27GHz
with 16GB of RAM.
5.1 Tagging ambiguity optimisation
The number of lexical categories assigned to a
word by the CCG supertagger depends on the prob-
abilities calculated for each category and the ?
level being used. Each lexical category with a
probability within a factor of ? of the most prob-
able category is included. This means that the
choice of ? level determines the tagging ambigu-
ity, and so has great influence on parsing speed, ac-
curacy and coverage. Also, the tagging ambiguity
produced by a ? level will vary between models.
A more confident model will have a more peaked
distribution of category probabilities for a word,
and therefore need a smaller ? value to assign the
same number of categories.
Additionally, the C&C parser uses multiple ?
levels. The first pass over a sentence is at a high ?
level, resulting in a low tagging ambiguity. If the
categories assigned are too restrictive to enable a
spanning analysis, the system makes another pass
with a lower ? level, resulting in a higher tagging
ambiguity. A maximum of five passes are made,
with the ? levels varying from 0.075 to 0.001.
We have taken two approaches to choosing ?
levels. When the aim of an experiment is to im-
prove speed, we use the system?s default ? levels.
While this choice means a more confident model
will assign fewer tags, this simply reflects the fact
that the model is more confident. It should pro-
duce similar accuracy results, but with lower am-
biguity, which will lead to higher speed.
For accuracy optimisation experiments we tune
the ? levels to produce the same average tagging
ambiguity as the baseline model on Section 00 of
CCGbank. Accuracy depends heavily on the num-
ber of categories supplied, so the new models are
at an accuracy disadvantage if they propose fewer
categories. By matching the ambiguity of the de-
fault model, we can increase accuracy at the cost
of some of the speed improvements the new mod-
els obtain.
6 Results
We have performed four primary sets of exper-
iments to explore the ability of an adaptive su-
pertagger to improve parsing speed or accuracy. In
the first two experiments, we explore performance
on the newswire domain, which is the source of
training data for the parsing model and the base-
line supertagging model. In the second set of ex-
periments, we train on a mixture of gold standard
newswire data and parser-annotated data from the
target domain.
In both cases we perform two experiments. The
first aimed to improve speed, keeping the ? levels
the same. This should lead to an increase in speed
as the extra training data means the models are
more confident and so have lower ambiguity than
the baseline model for a given ? value. The second
experiment aimed to improve accuracy, tuning the
? levels as described in the previous section.
6.1 Newswire speed improvement
In our first experiment, we trained supertagger
models using Generalised Iterative Scaling (GIS)
(Darroch and Ratcliff, 1972), the limited mem-
ory BFGS method (BFGS) (Nocedal and Wright,
1999), the averaged perceptron (Collins, 2002),
and the margin infused relaxed algorithm (MIRA)
(Crammer and Singer, 2003). Note that these
are all alternative methods for estimating the lo-
cal log-linear probability distributions used by the
Ratnaparkhi-style tagger. We do not use global
tagging models as in Lafferty et al (2001) or
Collins (2002). The training data consisted of Sec-
tions 02?21 of CCGbank and progressively larger
quantities of parser-annotated NANC data ? from
zero to four million extra sentences. The results of
these tests are presented in Table 2.
349
Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec)
Data 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 1.27 96.34 85.46 39.6
BFGS 1.27 1.23 1.19 1.18 96.33 96.18 95.95 95.93 85.45 85.51 85.57 85.68 39.8 49.6 71.8 60.0
GIS 1.28 1.24 1.21 1.20 96.44 96.27 96.09 96.11 85.44 85.46 85.58 85.62 37.4 44.1 51.3 54.1
MIRA 1.30 1.24 1.17 1.13 96.44 96.14 95.56 95.18 85.44 85.40 85.38 85.42 34.1 44.8 60.2 73.3
Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.
Sentences Av. Time Change (ms) Total Time Change (s)
Sentence length 5-20 21-40 41-250 5-20 21-40 41-250 5-20 21-40 41-250
Lower tag amb. 1166 333 281 -7.54 -71.42 -183.23 -1.1 -29 -26
Earlier pass Same tag amb. 248 38 8 -2.94 -27.08 -108.28 -0.095 -1.3 -0.44
Higher tag amb. 530 33 14 -5.84 -32.25 -44.10 -0.40 -1.3 -0.31
Lower tag amb. 19288 3120 1533 -1.13 -5.18 -38.05 -2.8 -20 -30
Same pass Same tag amb. 7285 259 35 -0.29 0.94 24.57 -0.28 0.30 0.44
Higher tag amb. 1133 101 24 -0.25 2.70 8.09 -0.037 0.34 0.099
Lower tag amb. 334 114 104 0.90 7.60 -46.34 0.039 1.1 -2.5
Later pass Same tag amb. 14 1 0 1.06 4.26 n/a 0.0019 0.0053 0.0
Higher tag amb. 2 1 1 -0.13 26.43 308.03 -3.4e-05 0.033 0.16
Table 3: Breakdown of the source of changes in speed. The test sentences are divided into nine sets
based on the change in parsing behaviour between the baseline model and a model trained using MIRA,
Sections 02-21 of CCGbank and 4,000,000 NANC sentences.
Using the default ? levels we found that the
perceptron-trained models lost accuracy, disqual-
ifying them from this test. The BFGS, GIS and
MIRA models produced mixed results, but no
statistically significant decrease in accuracy, and
as the amount of parser-annotated data was in-
creased, parsing speed increased by up to 85%.
To determine the source of the speed improve-
ment we considered the times recorded by the tim-
ing registers. In Table 3, we have aggregated these
measurements based on the change in the pass at
which the sentence is parsed, and how the tag-
ging ambiguity changes on that pass. For sen-
tences parsed on two different passes the ambigu-
ity comparison is at the earlier pass. The ?Total
Time Change? section of the table is the change in
parsing time for sentences of that type when pars-
ing ten thousand sentences from the corpus. This
takes into consideration the actual distribution of
sentence lengths in the corpus.
Several effects can be observed in these re-
sults. 72% of sentences are parsed on the same
pass, but with lower tag ambiguity (5th row in Ta-
ble 3). This provides 44% of the speed improve-
ment. Three to six times as many sentences are
parsed on an earlier pass than are parsed on a later
pass. This means the sentences parsed later have
very little effect on the overall speed. At the same
time, the average gain for sentences parsed earlier
is almost always larger than the average cost for
sentences parsed later. These effects combine to
produce a particularly large improvement for the
sentences parsed at an earlier pass. In fact, despite
making up only 7% of sentences in the set, those
parsed earlier with lower ambiguity provide 50%
of the speed improvement.
It is also interesting to note the changes for sen-
tences parsed on the same pass, with the same
ambiguity. We may expect these sentences to be
parsed in approximately the same amount of time,
and this is the case for the short set, but not for the
two larger sets, where we see an increase in pars-
ing time. This suggests that the categories being
supplied are more productive, leading to a larger
set of possible derivations.
6.2 Newswire accuracy optimised
Any decrease in tagging ambiguity will generally
lead to a decrease in accuracy. The parser uses a
more sophisticated algorithm with global knowl-
edge of the sentence and so we would expect it
to be better at choosing categories than the su-
pertagger. Unlike the supertagger it will exclude
categories that cannot be used in a derivation. In
the previous section, we saw that training the su-
pertagger on parser output allowed us to develop
models that produced the same categories, despite
lower tagging ambiguity. Since they were trained
on the categories the parser was able to use in
derivations, these models should also now be pro-
viding categories that are more likely to be useful.
This leads us to our second experiment, opti-
350
Tagging Accuracy (%) F-score Speed (sents / sec)
NANC sents 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 96.34 85.46 39.6
BFGS 96.33 96.42 96.42 96.66 85.45 85.55 85.64 85.98 39.5 43.7 43.9 42.7
GIS 96.34 96.43 96.53 96.62 85.36 85.47 85.84 85.87 39.1 41.4 41.7 42.6
Perceptron 95.82 95.99 96.30 - 85.28 85.39 85.64 - 45.9 48.0 45.2 -
MIRA 96.23 96.29 96.46 96.63 85.47 85.45 85.55 85.84 37.7 41.4 41.4 42.9
Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
News Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.267 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.126 1.151 1.130 95.18 93.56 90.07 85.42 81.2 75.2 73.3 83.9 60.3
Wiki 1.147 1.154 1.129 95.06 93.52 90.03 84.70 81.4 75.5 62.4 73.9 58.7
Bio 1.134 1.146 1.114 94.66 93.15 89.88 84.23 80.7 75.9 66.2 90.4 59.3
Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The
highlighted values are the top speed for each evaluation set and results that are statistically indistinguish-
able from it.
mising accuracy on newswire. We used the same
models as in the previous experiment, but tuned
the ? levels as described in Section 5.1.
Comparing Tables 2 and 4 we can see the in-
fluence of ? level choice, and therefore tagging
ambiguity. When the default ? values were used
ambiguity dropped consistently as more parser-
annotated data was used, and category accuracy
dropped in the same way. Tuning the ? levels to
match ambiguity produces the opposite trend.
Interestingly, while the decrease in supertag ac-
curacy in the previous experiment did not translate
into a decrease in F-score, the increase in tag accu-
racy here does translate into an increase in F-score.
This indicates that the supertagger is adapting to
suit the parser. In the previous experiment, the
supertagger was still providing the categories the
parser would have used with the baseline supertag-
ging model, but it provided fewer other categories.
Since the parser is not a perfect supertagger these
other categories may in fact have been incorrect,
and so supertagger accuracy goes down, without
changing parsing results. Here we have allowed
the supertagger to assign extra categories, which
will only increase its accuracy.
The increase in F-score has two sources. First,
our supertagger is more accurate, and so the parser
is more likely to receive category sets that can be
combined into the correct derivation. Also, the su-
pertagger has been trained on categories that the
parser is able to use in derivations, which means
they are more productive.
As Table 6 shows, this change translates into an
improvement of up to 0.75% in F-score on Section
Model Tag. Acc. F-score Speed
(%) (%) (sents/sec)
Baseline 96.51 85.20 39.6
GIS, 4,000k NANC 96.83 85.95 42.6
BFGS, 4,000k NANC 96.91 85.90 42.7
MIRA, 4,000k NANC 96.84 85.79 42.9
Table 6: Evaluation of top models on Section 23 of
CCGbank. All changes in F-score are statistically
significant.
23 of CCGbank. All of the new models in the table
make a statistically significant improvement over
the baseline.
It is also interesting to note that the results in
Tables 2, 4 and 6, are similar for all of the train-
ing algorithms. However, the training times differ
considerably. For all four algorithms the training
time is proportional to the amount of data, but the
GIS and BFGS models trained on only CCGbank
took 4,500 and 4,200 seconds to train, while the
equivalent perceptron and MIRA models took 90
and 95 seconds to train.
6.3 Annotation method comparison
To determine whether these improvements were
dependent on the annotations being produced
by the parser we performed a set of tests with
supertagger, rather than parser, annotated data.
Three extra training sets were created by annotat-
ing newswire sentences with supertags using the
baseline supertagging model. One set used the
one-best tagger, and two were produced using the
most probable tag for each word out of the set sup-
plied by the multi-tagger, with variations in the ?
value and dictionary cutoff for the two sets.
351
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.331 1.322 96.53 94.86 91.32 85.84 80.1 75.2 41.8 32.6 31.4
Wiki 1.293 1.251 96.28 94.79 91.08 85.02 81.7 75.8 40.4 37.2 37.2
Bio 1.287 1.195 96.15 94.28 91.03 84.95 80.6 76.1 39.2 52.9 26.2
Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
Annotation method Tag. Acc. F-score
Baseline 96.34 85.46
Parser 96.46 85.55
One-best super 95.94 85.24
Multi-tagger a 95.91 84.98
Multi-tagger b 96.00 84.99
Table 8: Comparison of annotation methods for
extra data. The multi-taggers used ? values 0.075
and 0.001, and dictionary cutoffs 20 and 150, for
taggers a and b respectively.
Corpus Speed (sents / sec)
Sent length 5-20 21-40 41-250
News 242 44.8 8.24
Wiki 224 42.0 6.10
Bio 268 41.5 6.48
Table 9: Cross-corpus speed for the baseline
model on data sets balanced on sentence length.
As Table 8 shows, in all cases the use of
supertagger-annotated data led to poorer perfor-
mance than the baseline system, while the use of
parser-annotated data led to an improvement in F-
score. The parser has access to a range of infor-
mation that the supertagger does not, producing a
different view of the data that the supertagger can
productively learn from.
6.4 Cross-domain speed improvement
When applying parsers out of domain they are typ-
ically slower and less accurate (Gildea, 2001). In
this experiment, we attempt to increase speed on
out-of-domain data. Note that for some of the re-
sults presented here it may appear that the C&C
parser does not lose speed when out of domain,
since the Wikipedia and biomedical corpora con-
tain shorter sentences on average than the news
corpus. However, by testing on balanced sets it
is clear that speed does decrease, particularly for
longer sentences, as shown in Table 9.
For our domain adaptation development ex-
periments, we considered a collection of differ-
ent models; here we only present results for the
best set of models. For speed improvement these
were MIRA models trained on 4,000,000 parser-
annotated sentences from the target domain.
As Table 5 shows, this training method pro-
duces models adapted to the new domain. In par-
ticular, note that models trained on Wikipedia or
the biomedical data produce lower F-scores3 than
the baseline on newswire. Meanwhile, on the
target domain they are adapted to, these models
achieve a higher F-score and parse sentences at
least 45% faster than the baseline.
The changes in tagging ambiguity and accuracy
also show that adaptation has occurred. In all
cases, the new models have lower tagging ambi-
guity, and lower supertag accuracy. However, on
the corpus of the extra data, the performance of
the adapted models is comparable to the baseline
model, which means the parser is probably still be
receiving the same categories that it used from the
sets provided by the baseline system.
6.5 Cross-domain accuracy optimised
The ambiguity tuning method used to improve ac-
curacy on the newspaper domain can also be ap-
plied to the models trained on other domains. In
Table 7, we have tested models trained using GIS
and 400,000 sentences of parsed target-domain
text, with ? levels tuned to match ambiguity with
the baseline.
As for the newspaper domain, we observe in-
creased supertag accuracy and F-score. Also, in
almost every case the new models perform worse
than the baseline on domains other than the one
they were trained on.
In some cases the models in Table 7 are less ac-
curate than those in Table 5. This is because as
well as optimising the ? levels we have changed
training methods. All of the training methods were
tried, but only the method with the best results in
newswire is included here, which for F-score when
trained on 400,000 sentences was GIS.
The accuracy presented so far for the biomedi-
3Note that the F-scores for Wikipedia and biomedical text
are reported to only three significant figures as only 300 and
500 sentences respectively were available for parser evalua-
tion.
352
Train Corpus F-score
Rimell and Clark (2009) 81.5
Baseline 80.7
CCGbank + Genia 81.5
+ Newswire 81.9
+ Wikipedia 82.2
+ Biomedical 81.7
+ R&C annotated Bio 82.3
Table 10: Performance comparison for models us-
ing extra gold standard biomedical data. Models
were trained with GIS and 4,000,000 extra sen-
tences, and are tested using a POS-tagger trained
on biomedical data.
cal model is considerably lower than that reported
by Rimell and Clark (2009). This is because no
gold standard biomedical training data was used
in our experiments. Table 10 shows the results of
adding Rimell and Clark?s gold standard biomedi-
cal supertag data and using their biomedical POS-
tagger. The table also shows how accuracy can be
further improved by adding our parser-annotated
data from the biomedical domain as well as the
additional gold standard data.
7 Conclusion
This work has demonstrated that an adapted su-
pertagger can improve parsing speed and accu-
racy. The purpose of the supertagger is to re-
duce the search space for the parser. By train-
ing the supertagger on parser output, we allow the
parser to reach the derivation it would have found,
sooner. This approach also enables domain adap-
tation, improving speed and accuracy outside the
original domain of the parser.
The perceptron-based algorithms used in this
work are also able to function online, modifying
the model weights after each sentence is parsed.
This could be used to construct a system that con-
tinuously adapts to the domain it is parsing.
By training on parser-annotated NANC data
we constructed models that were adapted to the
newspaper-trained parser. The fastest model
parsed sentences 1.85 times as fast and was as
accurate as the baseline system. Adaptive train-
ing is also an effective method of improving per-
formance on other domains. Models trained on
parser-annotated Wikipedia text and MEDLINE
text had improved performance on these target do-
mains, in terms of both speed and accuracy. Op-
timising for speed or accuracy can be achieved by
modifying the ? levels used by the supertagger,
which controls the lexical category ambiguity at
each level used by the parser.
The result is an accurate and efficient wide-
coverage CCG parser that can be easily adapted for
NLP applications in new domains without manu-
ally annotating data.
Acknowledgements
We thank the reviewers for their helpful feed-
back. This work was supported by Australian Re-
search Council Discovery grants DP0665973 and
DP1097291, the Capital Markets Cooperative Re-
search Centre, and a University of Sydney Merit
Scholarship. Part of the work was completed at the
Johns Hopkins University Summer Workshop and
(partially) supported by National Science Founda-
tion Grant Number IIS-0833652.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Phil Blunsom and Timothy Baldwin. 2006. Multi-
lingual deep lexical acquisition for HPSGs via su-
pertagging. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 164?171, Sydney, Australia.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the Poster Ses-
sion of the 21st International Conference on Com-
putational Linguistics, Sydney, Australia.
John Chen, Srinivas Bangalore, and Vijay K. Shanker.
1999. New models for improving supertag disam-
biguation. In Proceedings of the Ninth Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 188?195, Bergen, Nor-
way.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram su-
pertagger. In Proceedings of the 6th International
Workshop on Tree Adjoining Grammars and Related
Frameworks, pages 259?268, Venice, Italy.
Nancy Chinchor. 1995. Statistical significance
of MUC-6 results. In Proceedings of the Sixth
Message Understanding Conference, pages 39?43,
Columbia, MD, USA.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Con-
ference on Computational Linguistics, pages 282?
288, Geneva, Switzerland.
353
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Proceedings of the seventh Conference on
Natural Language Learning, pages 49?55, Edmon-
ton, Canada.
Michael Collins and Terry Koo. 2002. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
John N. Darroch and David Ratcliff. 1972. General-
ized iterative scaling for log-linear models. The An-
nals of Mathematical Statistics, 43(5):1470?1480.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?54,
Genoa, Italy.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering: Is
more always better? In Proceedings of the 25th In-
ternational ACMSIGIR Conference on Research and
Development, Tampere, Finland.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing, Pittsburgh, PA, USA.
David Graff. 1995. North American News Text Cor-
pus. LDC95T21. Linguistic Data Consortium.
Philadelphia, PA, USA.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
288?295, Prague, Czech Republic.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Kristy Hollingshead and Brian Roark. 2007. Pipeline
iteration. In Proceedings of the 45th Meeting of the
Association for Computational Linguistics, pages
952?959, Prague, Czech Republic.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(1):180?182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of
the 4th International Workshop on Linguistically In-
terpreted Corpora, Budapest, Hungary.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289, San Francisco, CA, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, Brook-
lyn, NY, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 337?344, Sydney, Aus-
tralia.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain inde-
pendent lexicon and template acquisition. In Pro-
ceedings of the Australasian Language Technology
Workshop, Hobart, Australia.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal Optimization. Springer.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007. On the unification of syntactic annotations
under the Stanford dependency scheme: a case study
on bioinfer and GENIA. In Proceedings of the ACL
workshop on biological, translational, and clinical
language processing, pages 25?32, Prague, Czech
Republic.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the 1996 Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142, Philadelphia, PA, USA.
354
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
475?484, Honolulu, HI, USA.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Anoop Sarkar, Fel Xia, and Aravind K. Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proceedings of
the COLING Workshop on Efficiency in Large-scale
Parsing Systems, pages 37?42, Luxembourg.
Anoop Sarkar. 2001. Applying co-training methods
to statistical parsing. In Proceedings of the Second
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 1?8,
Pittsburgh, PA, USA.
Anoop Sarkar. 2007. Combining supertagging and
lexicalized tree-adjoining grammar parsing. In
Srinivas Bangalore and Aravind Joshi, editors, Com-
plexity of Lexical Descriptions and its Relevance to
Natural Language Processing: A Supertagging Ap-
proach. MIT Press, Boston, MA, USA.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Stephen Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 331?338, Budapest, Hun-
gary.
Geertjan van Noord. 2009. Learning efficient parsing.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 817?825. Association for Com-
putational Linguistics.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi
Tsujii. 2009. HPSG supertagging: A sequence la-
beling view. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages
210?213, Paris, France.
355
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 60?65,
Dublin, Ireland, August 23rd 2014.
Command-line utilities for managing and exploring annotated corpora
Joel Nothman and Tim Dawborn and James R. Curran
e
-lab, School of Information Technologies
University of Sydney
NSW 2006, Australia
{joel.nothman,tim.dawborn,james.r.curran}@sydney.edu.au
Abstract
Users of annotated corpora frequently perform basic operations such as inspecting the available
annotations, filtering documents, formatting data, and aggregating basic statistics over a corpus.
While these may be easily performed over flat text files with stream-processing UNIX tools,
similar tools for structured annotation require custom design. Dawborn and Curran (2014) have
developed a declarative description and storage for structured annotation, on top of which we
have built generic command-line utilities. We describe the most useful utilities ? some for quick
data exploration, others for high-level corpus management ? with reference to comparable UNIX
utilities. We suggest that such tools are universally valuable for working with structured corpora;
in turn, their utility promotes common storage and distribution formats for annotated text.
1 Introduction
Annotated corpora are a mainstay of language technology, but are often stored or transmitted in a variety
of representations. This lack of standardisation means that data is often manipulated in an ad hoc manner,
and custom software may be needed for even basic exploration of the data, which creates a bottleneck
in the engineer or researcher?s workflow. Using a consistent storage representation avoids this problem,
since generic utilities for rapid, high-level data manipulation can be developed and reused.
Annotated text processing frameworks such as GATE (Cunningham et al., 2002) and UIMA (Lally
et al., 2009) provide a means of implementing and combining processors over collections of annotated
documents, for which each framework defines a serialisation format. Developers using these frameworks
are aided by utilities for basic tasks such as searching among annotated documents, profiling processing
costs, and generic processing like splitting each document into many. Such utilities provide a means of
quality assurance and corpus management, as well as enabling rapid prototyping of complex processors.
The present work describes a suite of command-line utilities ? summarised in Table 1 ? designed for
similar ends in a recently released document representation and processing framework, DOCREP (Daw-
born and Curran, 2014). DOCREP represents annotation layers in a binary, streaming format, such that
process pipelining can be achieved through UNIX pipes. The utilities have been developed organically
as needed over the past two years, and are akin to UNIX utilities (grep, head, wc, etc.) which instead
operate over flat file formats. The framework and utilities are free and open source (MIT Licence) and
are available at https://github.com/schwa-lab/libschwa.
Some of our tools are comparable to utilities in UIMA and GATE, while others are novel. A number of
our tools exploit the evaluation of user-supplied Python functions over each document, providing great
expressiveness while avoiding engineering overhead when exploring data or prototyping.
Our utilities make DOCREP a good choice for UNIX-style developers who would prefer a quick script-
ing language over an IDE, but such modalities should also be on offer in other frameworks. We believe
that a number of these utilities are applicable across frameworks and would be valuable to researchers
and engineers working with manually and automatically annotated corpora. Moreover, we argue, the
availability of tools for rapid corpus management and exploration is an important factor in encouraging
users to adopt common document representation and processing frameworks.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
60
2 Utilities for managing structured data
Data-oriented computing tends to couple data primitives with a declarative language or generic tools
that operate over those primitives. UNIX provides tools for operating over paths, processes, streams and
textual data. Among them are wc to count lines and words, and grep to extract passages matched by
a regular expression; piped together, such utilities accomplish diverse tasks with minimal development
cost. Windows PowerShell extends these notions to structured .NET objects (Oakley, 2006); Yahoo!
Pipes (Yahoo!, 2007) provides equivalent operations over RSS feeds; SQL transforms relational data; and
XSLT (Clark, 1999) or XQuery (Chamberlin, 2002) make XML more than mere markup.
Textual data with multiple layers of structured annotation, and processors over these, are primitives
of natural language processing. Such nested and networked structures are not well represented as flat
text files, limiting the utility of familiar UNIX utilities. By standardising formats for these primitives,
and providing means to operate over them, frameworks such as GATE and UIMA promise savings in
development and data management costs. These frameworks store annotated corpora with XML, so users
may exploit standard infrastructure (e.g. XQuery) for basic transformation and aggregation over the data.
Generic XML tools are limited in their ability to exploit the semantics of a particular XML language,
such that expressing queries over annotations (which include pointers, spatial relations, etc.) can be
cumbersome. LT-XML (Thompson et al., 1997) implements annotators using standard XML tools, while
Rehm et al. (2008) present extensions to an XQuery implementation specialised to annotated text.
Beyond generic XML transformation, UIMA and GATE and their users provide utilities with broad
application for data inspection and management, ultimately leading to quality assurance and rapid de-
velopment within those frameworks. Both GATE and UIMA provide sophisticated graphical tools for
viewing and modifying annotations; for comparing parallel annotations; and for displaying a concor-
dance of contexts for a term across a document collection (Cunningham et al., 2002; Lally et al., 2009).
Both also provide means of profiling the efficiency of processing pipelines. The Eclipse IDE serves as
a platform for tool delivery and is comparable to the UNIX command-line, albeit more graphical, while
providing further opportunities for integration. For example, UIMA employs Java Logical Structures to
yield corpus inspection within the Eclipse debugger (Lally et al., 2009).
Generic processors in these frameworks include those for combining or splitting documents, or copy-
ing annotations from one document to another. The community has further built tools to export corpora
to familiar query environments, such as a relational database or Lucene search engine (Hahn et al., 2008).
The uimaFIT library (Ogren and Bethard, 2009) simplifies the creation and deployment of UIMA proces-
sors, but to produce and execute a processor for mere data exploration still has some overhead.
Other related work includes utilities for querying or editing treebanks (e.g. Kloosterman, 2009), among
specialised annotation formats; and for working with binary-encoded structured data such as Protocol
Buffers (e.g. protostuff
1
). Like these tools and those within UIMA and GATE, the utilities presented in
this work reduce development effort, with an orientation towards data management and evaluation of
arbitrary functions from the command-line.
3 Common utility structure
For users familiar with UNIX tools to process textual streams, implementing similar tools for working
with structured DOCREP files seemed natural. Core utilities are developed in C
++
, while others are able
to exploit the expressiveness of interpreted evaluation of Python. We describe a number of the tools
according to their application in the next section; here we first outline common features of their design.
Invocation and API Command-line invocation of utilities is managed by a dispatcher program, dr,
whose operation may be familiar from the git versioning tool: an invocation of dr cmd delegates to a
command named dr-cmd where found on the user?s PATH. Together with utility development APIs in
both C
++
and Python, this makes it easy to extend the base set of commands.
2
1
https://code.google.com/p/protostuff/
2
Note that DOCREP processing is not limited in general to these languages. As detailed in Dawborn and Curran (2014) APIs
are currently available in C
++
, Java and Python.
61
Command Description Required input Output Similar in UNIX
dr count Aggregate stream or many tabular wc
dr dump View raw annotations stream JSON-like less / hexdump
dr format Excerpt stream + expression text printf / awk
dr grep Select documents stream + expression stream grep
dr head Select prefix stream stream head
dr sample Random documents stream + proportion stream shuf -n
dr shell Interactive exploration stream + commands mixed python
dr sort Reorder documents stream + expression stream sort
dr split Partition stream + expression files split
dr tail Select suffix stream stream tail
Table 1: Some useful commands and comparable UNIX tools, including required input and output types.
Streaming I/O As shown in Table 1, most of our DOCREP utility commands take a single stream of
documents as input (defaulting to stdin), and will generally output either plain text or another DOCREP
stream (to stdout by default). This parallels the UNIX workflow, and intends to exploit its constructs
such as pipes, together with their familiarity to a UNIX user. This paradigm harnesses a fundamental
design decision in DOCREP: the utilisation of a document stream, rather than storing a corpus across
multiple files.
Self-description for inspection Generic command-line tools require access to the schema as well as
the data of an annotated corpus. DOCREP includes a description of the data schema along with the data
itself, making such tools possible with minimal user input. Thus by reading from a stream, fields and
annotation layers can be referenced by name, and pointers across annotation layers can be dereferenced.
3
Extensions to this basic schema may also be useful. For many tools, a Python class (on file) can be
referenced that provides decorations over the document: in-memory fields that are not transmitted on
the stream, but are derived at runtime. For example, a document with pointers from dependent to gover-
nor may be decorated with a list of dependents on each governor. Arbitrary Python accessor functions
(descriptors) may similarly be added. Still, for many purposes, the self-description is sufficient.
Custom expression evaluation A few of our utilities rely on the ability to evaluate a function given a
document. The suite currently supports evaluating such functions in Python, providing great flexibility
and power.
4
Their input is an object representing the document, and its offset (0 for the first document
in a stream, 1 for the second, etc.). Its output depends on the purpose of the expression, which may be
for displaying, filtering, splitting or sorting a corpus, depending on the utility in use, of which examples
appear below. Often it is convenient to specify an anonymous function on the command-line, a simple
Python expression such as len(doc.tokens) > 1000, into which local variables doc (document
object) and ind (offset index) are injected as well as built-in names like len. (Python code is typeset in
blue.) In some cases, the user may want to predefine a library of such functions in a Python module, and
may then specify the path to that function on the command-line instead of an expression.
4 Working with DOCREP on the command-line
Having described their shared structure, this section presents examples of the utilities available for work-
ing with DOCREP streams. We consider three broad application areas: quality assurance, managing
corpora, and more specialised operations.
3
This should be compared to UIMA?s Type System Descriptions, and uimaFIT?s automatic detection thereof.
4
dr grep was recently ported to C
++
with a custom recursive descent parser and evaluator, which limits expressiveness
but promises faster evaluation. In terms of efficiency, we note that some of the Python utilities have performed more than twice
as fast using the JIT compilation of PyPy, rather than the standard CPython interpreter.
62
4.1 Debugging and quality assurance
Validating the input and output of a process is an essential part of pipeline development in terms of
quality assurance and as part of a debugging methodology. Basic quality assurance may require viewing
the raw content contained on a stream: schema, data, or both. This could ensure, for instance, that a user
has received the correct version of a corpus from a correspondent, or that a particular field was used as
expected. Since DOCREP centres on a binary wire format, dr dump (cf. UNIX?s hexdump) provides a
decoded textual view of the raw content on a stream. Optionally, it can provide minimal interpretation of
schema semantics to improve readability (e.g. labelling fields by name rather than number), or can show
schema details to the exclusion of data.
For an aggregate summary of the contents of a stream, dr count is a versatile tool. It mirrors UNIX?s
wc in providing the basic statistics over a stream (or multiple files) at different granularities. Without
any arguments, dr count outputs the total number of documents on standard input, but the number of
annotations in each store (total number of tokens, sentences, named entities, etc.) can be printed with flag
-a, or specific stores with -s. The same tool can produce per-document (as distinct from per-stream)
statistics with -e, allowing for quick detection of anomalies, such as an empty store where annotations
were expected. dr count also doubles as a progress meter, where its input is the output of a concurrent
corpus processor, as in the following example:
my-proc < /path/to/input | tee /path/to/output | dr count -tacv 1000
Here, the options -acvn output cumulative totals over all stores every n documents, while -t prefixes
each row of output with a timestamp.
Problems can often be identified from only a small sample of documents. dr head (cf. UNIX?s
head) extracts a specified number of documents from the beginning of a stream, defaulting to 1.
dr sample provides a stochastic alternative, employing reservoir sampling (Vitter, 1985) to efficiently
draw a specified fraction of the entire stream. Its output can be piped to processing software for smoke
testing, for instance. Such tools are obviously useful for a binary format; yet it is not trivial to split on
document boundaries even for simple text representations like the CONLL shared task format.
4.2 Corpus management
Corpora often need to be restructured: they may be heterogeneous, or need to be divided or sampled
from, such as when apportioning documents to manual annotators. In other cases, corpora or annotations
from many sources should be combined.
As with the UNIX utility of the same name, dr grep has many uses. Here it might be used to extract
a particular document, or to remove problematic documents. The user provides a function that evaluates
to a Boolean value; where true, an input document is reproduced on the output stream. Thus it might
extract a particular document by its identifier, all documents with a minimum number of words, or those
referring to a particular entity. Note, however, that like its namesake, it performs a linear search, while
a non-streaming data structure could provide fast indexed access; each traversed document is (at least
partially) deserialised, adding to the computational overhead.
5
dr grep is often piped into dr count,
to print the number of documents (or sub-document annotations) in a subcorpus.
dr split moves beyond such binary filtering. Like UNIX?s split, it partitions a file into multiple,
using a templated filename. In dr split, an arbitrary function may determine the particular output
paths, such as to split a corpus whose documents have one or more category label into a separate file
for each category label. Thus dr split -t /path/to/{key}.dr py 'doc.categories'
evaluates each document?s categories field via a Python expression, and for each key in the returned
list will write to a path built from that key. In a more common usage, dr split k 10 will assign
documents by round robin to files named fold000.dr through fold009.dr, which is useful to
derive a k-fold cross-validation strategy for machine learning. In order to stratify a particular field across
5
Two commands that are not featured in this paper may allow for faster access: dr subset extends upon dr head to
extract any number of documents with minimal deserialisation overhead, given their offset indices in a stream. dr offsets
outputs the byte offset b of each document in a stream, such that C?s fseek(b) or UNIX?s tail -c+(b+1) can be used to
skip to a particular document. An index may thus be compiled in conjunction with dr format described below. Making fast
random access more user friendly is among future work.
63
the partitions for cross-validation, it is sufficient to first sort the corpus by that field using dr split k.
This is one motivation for dr sort, which may similarly accept a Python expression as the sort key, e.g.
dr sort py doc.label. As a special case, dr sort random will shuffle the input documents,
which may be useful before manual annotation or order-sensitive machine learning algorithms.
The inverse of the partitioning performed by dr split is to concatenate multiple streams. Given
DOCREP?s streaming design, UNIX?s cat suffices; for other corpus representations, a specialised tool
may be necessary to merge corpora.
While the above management tools partition over documents, one may also operate on portions of the
annotation on each document. Deleting annotation layers, merging annotations from different streams
(cf. UNIX?s cut and paste), or renaming fields would thus be useful operations, but are not currently
available as a DOCREP command-line utility. Related tasks may be more diagnostic, such as identifying
annotation layers that consume undue space on disk; dr count --bytes shows the number of bytes
consumed by each store (cf. UNIX?s du), rather than its cardinality.
4.3 Exploration and transformation
Other tools allow for more arbitrary querying of a corpus, such as summarising each document.
dr format facilitates this by printing a string evaluated for each document. The tool could be used
to extract a concordance, or enumerate features for machine learning. The following would print each
document?s id field and its first thirty tokens, given a stream on standard input:
dr format py '"{}\t{}".format(doc.id, " ".join(tok.norm for tok in
doc.tokens[:30]))'
We have also experimented with specialised tools for more particular, but common, formats, such as
the tabular format employed in CoNLL shared tasks. dr conll makes assumptions about the schema
to print one token per line with sentences separated by a blank line, and documents by a specified delim-
iter. Additional fields of each token (e.g. part of speech), or fields derived from annotations over tokens
(e.g. IOB-encoded named entity recognition tags) can be added as additional columns using command-
line flags. However, the specification of such details on the command-line becomes verbose and may not
easily express all required fields, such that developing an ad hoc script to undertake this transformation
may often prove a more maintainable solution.
Our most versatile utility makes it easy to explore or modify a corpus in an interactive Python shell.
This functionality is inspired by server development frameworks (such as Django) that provide a shell
specially populated with data accessors and other application-specific objects. dr shell reads docu-
ments from an input stream and provides a Python iterator over them named docs. If an output path is
specified on the command-line, a function write_doc is also provided, which serialises its argument
to disk. The user would otherwise have the overhead of opening input and output streams and initialising
the de/serialisation process; the time saved is small but very useful in practice, since it makes interactive
corpus exploration cheap. This in turn lowers costs when developing complex processors, as interactive
exploration may validate a particular technique. The following shows an interactive session in which the
user prints the 100 lemmas with highest document frequency.
$ dr shell /path/to/input.dr
>>> from collections import Counter
>>> df = Counter()
>>> for doc in docs:
... doc_lemmas = {tok.lemma for tok in doc.tokens}
... df.update(doc_lemmas)
...
>>> for lemma, count in df.most_common(100):
... print("{:5d}\t{}".format(count, lemma))
Finally, dr shell -c can execute arbitrary Python code specified on the command-line, rather than
interactively. This enables rapid development of ad hoc tools employing the common read-process-write
paradigm (whether writing serialised documents or plain text), in the vein of awk or sed.
64
5 Discussion
DOCREP?s streaming model allows the reuse of existing UNIX components such as cat, tee and pipes.
This is similar to the way in which choosing an XML data representation means users can exploit standard
XML tools. The specialised tools described above are designed to mirror the functionality if not names
of familiar UNIX counterparts, making it simple for UNIX users to adopt the tool suite.
No doubt, many users of Java/XML/Eclipse find command-line tools unappealing, just as a ?UNIX
hacker? might be put off by monolithic graphical interfaces and unfamiliar XML processing tools. Ideally
a framework should appeal to a broad user base; providing tools in many modalities may increase user
adoption of a document processing framework, without which it may seem cumbersome and confining.
In this vein, a substantial area for future work within DOCREP is to provide more graphical tools, and
utilities such as concordancing or database export that are popular within other frameworks. Further
utilities might remove existing fields or layers from annotations; select sub-documents; set attributes on
the basis of evaluated expressions; merge annotations; or compare annotations.
We have described utilities developed in response to a new document representation and processing
framework, DOCREP. Similar suites deserve attention as an essential adjunct to representation frame-
works, and should encompass expressive tools and various paradigms of user interaction. When per-
forming basic corpus manipulation, these utilities save substantial user effort, particularly by adopting a
familiar interface. Such boons highlight the benefit of using a consistent annotated corpus representation.
References
Donald D. Chamberlin. 2002. XQuery: An XML query language. IBM Systems Journal, 41(4):597?615.
James Clark. 1999. XSL transformations (XSLT). W3C Recommendation, 16 November.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A frame-
work and graphical development environment for robust NLP tools and applications. In Proceedings
of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 168?175.
Tim Dawborn and James R. Curran. 2014. docrep: A lightweight and efficient document representation
framework. In Proceedings of the 25th International Conference on Computational Linguistics.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias M?hlhausen, Michael Poprat, Katrin Tomanek,
and Joachim Wermter. 2008. An overview of JCoRe, the JULIE lab UIMA component repository.
In Proceedings of LREC?08 Workshop ?Towards Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7.
Geert Kloosterman. 2009. An overview of the Alpino Treebank tools. http://odur.let.rug.nl/
vannoord/alp/Alpino/TreebankTools.html. Last updated 19 December.
Adam Lally, Karin Verspoor, and Eoric Nyberg. 2009. Unstructured Information Management Architec-
ture (UIMA) Version 1.0. OASIS Standard, 2 March.
Andy Oakley. 2006. Monad (AKA PowerShell): Introducing the MSH Command Shell and Language.
O?Reilly Media.
Philip Ogren and Steven Bethard. 2009. Building test suites for UIMA components. In Proceedings of the
Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing
(SETQA-NLP 2009), pages 1?4.
Georg Rehm, Richard Eckart, Christian Chiarcos, and Johannes Dellert. 2008. Ontology-based
XQuery?ing of XML-encoded language resources on multiple annotation layers. In Proceedings of
the Sixth International Conference on Language Resources and Evaluation (LREC?08).
Henry S. Thompson, Richard Tobin, David McKelvie, and Chris Brew. 1997. LT XML: Software API
and toolkit for XML processing. http://www.ltg.ed.ac.uk/software/.
Jeffrey S. Vitter. 1985. Random sampling with a reservoir. ACM Transactions on Mathematical Software
(TOMS), 11(1):37?57.
Yahoo! 2007. Yahoo! Pipes. http://pipes.yahoo.com/pipes/. Launched 7 February.
65
