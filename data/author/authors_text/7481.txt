Coling 2008: Companion volume ? Posters and Demonstrations, pages 165?168
Manchester, August 2008
Multilingual Mobile-Phone Translation Services for World Travelers
Michael Paul, Hideo Okuma, Hirofumi Yamamoto, Eiichiro Sumita,
Shigeki Matsuda, Tohru Shimizu, Satoshi Nakamura
? NICT Spoken Language Communication Group
? ATR Spoken Language Communication Research Labs
Hikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto, Japan
Michael.Paul@nict.go.jp
Abstract
This demonstration introduces two new
multilingual translation services for mo-
bile phones. The first translation service
provides state-of-the-art text-to-text trans-
lations of Japanese as well as English con-
versational spoken language in the travel
domain into 17 languages using statistical
machine translation technologies trained
automatically from a large-scale multilin-
gual corpus. The second demonstration
is a speech translation service between
Japanese and English for real environ-
ments. It is based on distributed speech
recognition with noise suppression. Flexi-
ble interfaces between internal and exter-
nal speech translation resources ease the
portability of the system to other languages
and enable real-time location-free commu-
nication world-wide.
1 Introduction
Spoken language translation technologies attempt
to bridge the language barriers between people
with different native languages who each want
to engage in conversation by using their mother-
tongue. The importance of these technologies is
increasing due to increases in the number of op-
portunities for cross-language communication in
face-to-face conversation, especially in the domain
of tourism.
We demonstrate two multilingual translation
services for mobile phones that are built on corpus-
based speech recognition and translation technolo-
gies. These services enable smooth and location-
free communication in real environments covering
the major languages of most nations (see Figure 1).
c
?NICT/ATR, 2008. Licensed under the Creative
Commons Attribution-Noncommercial-Share Alike 3.0 Un-
ported license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
Figure 1: Global Language Coverage
The first multilingual translation service de-
scribed in this paper is a text-to-text translation
service that enables users to translate Japanese
and English conversational spoken language sen-
tences in the travel domain into 17 other languages.
The system?s core components consist of a mul-
tilingual, sentence-aligned spoken language cor-
pus covering 18 of the major world languages
and state-of-the-art statistical machine translation
(SMT) engines that are trained automatically from
this corpus covering 306 (=18x17) translation di-
rections. A graphical user-interface (GUI) allows
24x7 world-wide access to the translation service
(see Section 2).
The second multilingual translation service is an
extension of the text-based translation service that
additionally provides speech recognition capabil-
ities. This is the first commercial speech transla-
tion service in the world. The system is based on
distributed speech recognition and operates as fol-
lows: (1) front-end processing (noise suppression,
feature extraction, and feature parameter compres-
sion) is carried out on the mobile phone, (2) back-
end processing (recognition, translation) is done
on a server and (3) translation results are sent back
and displayed on the mobile phone (see Section 3).
2 Multilingual Text Translation Service
(MTTS)
The multilingual text translation service for mo-
bile phones can be accessed via ?http://atr-
langue.jp/smlt? or by using the QR code in Figure 2
that also illustrates the graphical user interface of
165
Figure 2: QR Code and GUI of MTTS
the translation service. Two different modes are
distinguished: (1) the multilingual mode where the
input is translated into all 17 languages simultane-
ously and the translation results are displayed side-
by-side and (2) the bilingual mode where a single
language out of 17 languages can be selected as the
target language of the Japanese or English input
text translation. The bilingual mode also features
back-translation functionality, i.e., a reverse trans-
lation of the generated translation output into the
source language, that enables immediate feedback
on the quality of the translation output. In order to
solve font problems of mobile phones, the trans-
lated sentences are rendered on the server side and
an image is sent and displayed in the mobile phone.
2.1 Multilingual Travel Conversation Corpus
The translation engines used for the translation ser-
vice are trained on the Basic Travel Expressions
Corpus (ATR-BTEC) which is a collection of sen-
tences that travel experts consider useful for people
Table 1: Language Characteristics
Language Order Segments Morphology
Arabic (ar) SVO phrase rich
Danish (da) SVO words medium
German (de) SVO words medium
English (en) SVO words poor
Spanish (es) SVO words medium
French (fr) SVO words medium
Indonesian (id) SVO words rich
Italian (it) SVO words medium
Japanese (ja) SOV none poor
Korean (ko) SOV phrase poor
Malay (ms) SVO words rich
Dutch (nl) SVO words medium
Portuguese (pt) SVO words medium
Brazilian (pt-b) SVO words medium
Portuguese
Russian (ru) SVO words rich
Thai (th) SVO none none
Vietnamese (vi) SVO phrase none
Chinese (zh) SVO none none
going abroad and cover a large variety of topics in
travel situations like shopping or stay (Kikui et al,
2006). The multilingual corpus consists of 160K
sentences for each of the 18 languages, aligned
at the sentence-level. The characteristics of all
ATR-BTEC corpus languages are summarized in
Table 1. These languages differ largely in word or-
der (SVO, SOV), segmentation unit (phrase, word,
none), and morphology (poor, medium, rich). Con-
cerning word segmentation, the corpora were pre-
processed using simple tokenization tools for all
European languages and language-specific word-
segmentation tools for languages like Chinese,
Japanese, Korean, or Thai that do not use white-
space to separate word/phrase tokens. All data sets
were lower-cased and punctuation marks were re-
moved.
2.2 Statistical Machine Translation Engines
Phrase-based statistical machine translation ap-
proaches continue to dominate the field of machine
translation. The translation service makes use of
state-of-the-art phrase-based SMT systems within
the framework of feature-based exponential mod-
els containing the following features:
? Phrase translation probability
? Inverse phrase translation probability
? Lexical weighting probability
? Inverse lexical weighting probability
? Phrase penalty
? Language model probability
? Simple distance-based distortion model
? Word penalty
166
Table 2: Language Model Perplexity
Lang Entropy Total Eval Data
uage Entropy Words Vocab
ar 5.73 21,663 3,780 1,067
da 5.66 17,411 3,077 884
de 5.58 16,698 2,995 910
en 4.53 14,370 3,169 807
es 5.35 15,622 2,919 943
fr 4.77 16,793 3,521 929
id 6.09 18,145 2,977 908
it 5.52 16,078 2,914 956
ja 4.03 15,080 3,745 929
ko 4.21 15,011 3,567 943
ms 6.43 19,144 2,977 909
nl 5.66 17,609 3,110 909
pt-b 5.73 16,981 2,962 932
pt 5.54 16,064 2,900 946
ru 6.20 16,040 2,587 1,143
th 5.12 20,230 3,953 738
vi 4.84 19,531 4,034 792
zh 5.11 14,748 2,887 944
The basic framework within which all the MT
systems were constructed is shown in Figure 3.
SourceL a n g ua g eI n p ut
T a rg etL a n g ua g eO ut p ut
D ecod i n g  A l g ori t h margmax P ( s rc | t rg) *  P ( t rg)
T ra n s l a t i onM od el s L a n g ua g eM od el s
st a t i st i c a la n a l y si s
P a ra l l elT ex t  C orp ora M on ol i n g ua lT ex t  C orp ora
Figure 3: SMT Framework
Translation examples from the respective bilin-
gual text corpus are aligned in order to extract
phrasal equivalences and to calculate the bilingual
feature probabilities. Monolingual features like the
language model probability are trained on mono-
lingual text corpora of the target language whereby
standard word alignment and language modeling
tools were used. For decoding, the CleopATRa
decoder (Finch et al, 2007), a multi-stack phrase-
based SMT decoder is used.
2.3 Evaluation
In order to get an idea of how difficult the trans-
lation tasks are, we trained standard 5gram lan-
guage models on 160K sentence pairs and eval-
uated the entropy and total entropy, i.e., the en-
tropy multiplied by word counts, of each language
on an evaluation data set of 510 sentences each.
Table 2 shows that the total entropy of European
Table 3: Automatic Evaluation Results
BLEU (%) METEOR (%)
en-* *-en ja-* *-ja en-* *-en ja-* *-ja
ar 18.21 51.01 13.03 46.09 40.90 69.01 37.52 58.02
da 59.70 70.90 45.94 55.34 75.08 82.56 64.41 65.83
de 56.48 69.25 41.99 59.20 74.01 81.48 63.69 69.61
en ? ? 61.56 68.53 ? ? 78.19 75.39
es 65.22 73.82 51.77 63.24 78.15 85.28 68.30 72.17
fr 64.69 71.04 52.36 63.16 79.28 83.05 71.14 72.82
id 48.35 59.69 40.59 57.24 66.82 75.83 62.33 69.00
it 56.80 70.43 43.45 60.77 72.41 82.96 62.35 70.70
ja 68.53 61.56 ? ? 75.39 78.19 ? ?
ko 37.00 58.82 69.96 85.10 57.89 75.92 83.25 89.73
ms 40.99 57.63 36.13 55.84 61.08 74.75 58.73 67.33
nl 57.46 72.85 41.43 59.70 75.88 84.52 63.42 72.19
pt-b 59.99 69.41 46.50 58.07 72.77 80.70 64.68 69.14
pt 62.81 70.25 48.24 59.20 75.65 83.32 67.38 68.32
ru 44.46 61.23 36.08 55.13 66.41 73.75 60.59 64.55
th 46.49 51.35 43.75 50.85 62.47 73.12 60.25 62.91
vi 55.18 57.42 50.86 55.07 71.04 73.98 68.67 70.81
zh 53.08 59.33 51.68 69.43 69.85 74.68 65.88 77.62
languages like Danish, German, English, Span-
ish, etc. does not differ much. Moreover, lan-
guages with phrasal segments and/or rich morphol-
ogy like Arabic, Malay, Russian or Vietnamese
have a high total entropy and thus can be expected
to be more difficult to translate. This is confirmed
by the translation experiments in which the eval-
uation data sets were translated using the servers
translation engines and the translation quality was
evaluated using the standard automatic evaluation
metrics BLEU (Papineni et al, 2002) and ME-
TEOR (Banerjee and Lavie, 2005) where scores
range between 0 (worst) and 1 (best). Besides Ko-
rean (single references only), all languages were
evaluated using 16 reference translations. The
evaluation results in Table 3 show that closely
related language pairs like Japanese-Korean or
Portuguese-Brazilian can be translated very accu-
rately, whereas translations into languages with
high total entropy are of lower quality.
3 Multilingual Speech Translation
Service (MSTS)
The speech translation service1 can be accessed via
?http://www.atr-trek.co.jp/contents html? or using
the QR code in Figure 4 that also illustrates the
graphical user interface of the translation service.
After connecting to the top page, the translation
service is activated by selecting the ?Translation?
option. In order to achieve robust speech recogni-
1The speech translation service for Japanese?English on
Docomo 905i mobile phones started November 2007.
167
Speak!
Figure 4: QR Code and GUI of MSTS
tion, the service features a push-to-talk function-
ality, i.e., the user (1) presses the key to start the
service (2) speaks freely into the integrated micro-
phone of the mobile phone, and (3) presses the key
again after the speech input is finished. Fast and
accurate front-end and back-end processing algo-
rithms enable high-speed speech translation of the
input. Both, the speech recognition results as well
as the translation results are sent back to and dis-
played on the mobile phone.
3.1 Multilingual Speech Corpus
Similar to the statistical machine translation ap-
proach introduced in Section 2.2, the speech recog-
nition components are based on large-sized mul-
tilingual speech corpora. For Japanese, speech
recordings of 4000 speakers were collected result-
ing on a total of 200 hours of speech. For English,
almost the same amount of speech data were col-
lected from 500 speakers in North America (300
speakers), the UK (100 speakers), and Australia
(100 speakers).
3.2 Distributed Speech Recognition
The speech interface is based on distributed speech
recognition (DSR) that is integrated as a client-
server architecture compatible with the ETSI ES
202 050 standards. The usage of Speech Trans-
lation Markup Language (STML) enables flexible
connections between internal and external speech
translation resources like speech recognition and
translation servers via a network. Figure 5 illus-
trates the architecture of the utilized DSR system.
The front-end processing includes noise suppres-
sion, feature extraction and feature parameter com-
pression and is carried out on the mobile phone.
The data stream is then sent via internet to the ap-
plication service provider (ASP) for back-end pro-
cessing, i.e. speech recognition and statistical ma-
chine translation. The recognition and translation
results are sent back to the mobile phone for dis-
play to the user.
Back-e n dN e t w o r kF r o n t -e n d
A p p l i cat i o n  S e r v i ce  P r o v i d e r  ( A S P )  A p p l i t i o n  S e r v i e  P r o v i d e r  ( A S P )  
ETSI ES 202  050c o m p a t i b l e  B i t -s t r e a mD a t a  ( 4 . 8 k b i t s / s )
Sp e e c h  R e c o g n i t i o n  a n d  Tr a n s l a t i o n  R e s u l t s
Speechr eco g n i t i o n L a n g u a g et r a n s l a t i o n
Figure 5: MSTS Architecture
4 Conclusion
This paper introduced the first commercial speech
translation service in the world. State-of-the-
art spoken language translation technologies (dis-
tributed speech recognition with noise suppres-
sion, multilingual statistical machine translation)
are implemented into a flexible client-server archi-
tecture that covers the major languages of most
countries and enables users to communicate in real
environments all over the world using their own
mobile phones.
5 Acknowledgments
This work is partly supported by the Grant-in-Aid
for Scientific Research (C) and the Special Coordi-
nation Funds for Promoting Science and Technol-
ogy of the Ministry of Education, Culture, Sports,
Science and Technology, Japan.
References
Banerjee, S. and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 65?72, Ann Arbor, Michigan.
Finch, A., E. Denoual, H. Okuma, M. Paul, H. Ya-
mamoto, K. Yasuda, R. Zhang, and E. Sumita.
2007. The NICT/ATR Speech Translation System
for IWSLT 2007. In Proc. of the IWSLT, pages 103?
110, Trento, Italy.
Kikui, G., S. Yamamoto, T. Takezawa, and E. Sumita.
2006. Comparative study on corpora for speech
translation. IEEE Transactions on Audio, Speech
and Language Processing, 14(5):1674?1682.
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th ACL, pages
311?318, Philadelphia, USA.
168
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 25?28,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
NICT-ATR Speech-to-Speech Translation System 
Eiichiro Sumita Tohru Shimizu Satoshi Nakamura 
 
National Institute of Information and Communications Technology  
& 
ATR Spoken Language Communication Research Laboratories 
2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, Japan 
eiichiro.sumita, tohru.shimizu & satoshi.nakamura@atr.jp 
 
Abstract 
This paper describes the latest version of 
speech-to-speech translation systems de-
veloped by the team of NICT-ATR for over 
twenty years. The system is now ready to 
be deployed for the travel domain. A new 
noise-suppression technique notably im-
proves speech recognition performance. 
Corpus-based approaches of recognition, 
translation, and synthesis enable coverage 
of a wide variety of topics and portability 
to other languages.  
1 Introduction 
Speech recognition, speech synthesis, and machine 
translation research started about half a century 
ago. They have developed independently for a long 
time until speech-to-speech translation research 
was proposed in the 1980?s. The feasibility of 
speech-to-speech translation was the focus of re-
search at the beginning because each component 
was difficult to build and their integration seemed 
more difficult. After groundbreaking work for two 
decades, corpus-based speech and language proc-
essing technology have recently enabled the 
achievement of speech-to-speech translation that is 
usable in the real world.  
This paper introduces (at ACL 2007) the state-
of-the-art speech-to-speech translation system de-
veloped by NICT-ATR, Japan. 
2 SPEECH-TO-SPEECH TRANSLA-
TION SYSTEM 
A speech-to-speech translation system is very large 
and complex. In this paper, we prefer to describe 
recent progress. Detailed information can be found 
in [1, 2, 3] and their references. 
2.1 Speech recognition 
To obtain a compact, accurate model from corpora 
with a limited size, we use MDL-SSS [4] and 
composite multi-class N-gram models [5] for 
acoustic and language modeling, respectively. 
MDL-SSS is an algorithm that automatically de-
termines the appropriate number of parameters ac-
cording to the size of the training data based on the 
Maximum Description Length (MDL) criterion. 
Japanese, English, and Chinese acoustic models 
were trained using the data from 4,200, 532, and 
536 speakers, respectively. Furthermore, these 
models were adapted to several accents, e.g., US 
(the United States), AUS (Australia), and BRT 
(Britain) for English. A statistical language model 
was trained by using large-scale corpora (852 k 
sentences of Japanese, 710 k sentences of English, 
510 k sentences of Chinese) drawn from the travel 
domain. 
Robust speech recognition technology in noisy 
situations is an important issue for speech transla-
tion in real-world environments. An MMSE 
(Minimum mean square error) estimator for log 
Mel-spectral energy coefficients using a GMM 
(Gaussian Mixture Model) [6] is introduced for 
suppressing interference and noise and for attenu-
ating reverberation. 
Even when the acoustic and language models 
are trained well, environmental conditions such as 
variability of speakers, mismatches between the 
training and testing channels, and interference 
from environmental noise may cause recognition 
errors. These utterance recognition errors can be 
rejected by tagging them with a low confidence 
value. To do this we introduce generalized word 
25
posterior probability (GWPP)-based recognition 
error rejection for the post processing of the speech 
recognition [7, 8]. 
2.2 Machine translation 
The translation modules are automatically con-
structed from large-scale corpora: (1) TATR, a 
phrase-based SMT module and (2) EM, a simple 
memory-based translation module. EM matches a 
given source sentence against the source language 
parts of translation examples. If an exact match is 
achieved, the corresponding target language sen-
tence will be output. Otherwise, TATR is called up. 
In TATR, which is built within the framework of 
feature-based exponential models, we used the fol-
lowing five features: phrase translation probability 
from source to target; inverse phrase translation 
probability; lexical weighting probability from 
source to target; inverse lexical weighting prob-
ability; and phrase penalty. 
Here, we touch on two approaches of TATR: 
novel word segmentation for Chinese, and lan-
guage model adaptation.  
We used a subword-based approach for word 
segmentation of Chinese [9]. This word segmenta-
tion is composed of three steps. The first is a dic-
tionary-based step, similar to the word segmenta-
tion provided by LDC. The second is a subword-
based IOB tagging step implemented by a CRF  
tagging model. The subword-based IOB tagging 
achieves a better segmentation than character-
based IOB tagging. The third step is confidence-
dependent disambiguation to combine the previous 
two results. The subword-based segmentation was 
evaluated with two different data from the Sighan 
Bakeoff and the NIST machine translation evalua-
tion workshop. With the data of the second Sighan 
Bakeoff1, our segmentation gave a higher F-score 
than the best published results. We also evaluated 
this segmentation in a translation scenario using 
the data of NIST translation evaluation 2  2005, 
where its BLEU score3 was 1.1% higher than that 
using the LDC-provided word segmentation. 
The language model that is used plays an impor-
tant role in SMT. The effectiveness of the language 
                                                 
                                                
1 http://sighan.cs.uchicago.edu/bakeoff2005/ 
2http://www.nist.gov/speech/tests/mt/mt05eval_official_
results_release_20050801_v3.html 
3http://www.nist.gov/speech/tests/mt/resources/scoring.
htm 
model is significant if the test data happen to have 
the same characteristics as those of the training 
data for the language models. However, this coin-
cidence is rare in practice. To avoid this perform-
ance reduction, a topic adaptation technique is of-
ten used. We applied this adaptation technique to 
machine translation. For this purpose, a ?topic? is 
defined as clusters of bilingual sentence pairs. In 
the decoding, for a source input sentence, f, a topic 
T is determined by maximizing P(f|T). To maxi-
mize P(f|T) we select cluster T that gives the high-
est probability for a given translation source sen-
tence f. After the topic is found, a topic-dependent 
language model P(e|T) is used instead of P(e), the 
topic-independent language model. The topic-
dependent language models were tested using 
IWSLT06 data 4 . Our approach improved the 
BLEU score between 1.1% and 1.4%. The paper of 
[10] presents a detailed description of this work.  
2.3 Speech synthesis 
An ATR speech synthesis engine called XIMERA 
was developed using large corpora (a 110-hour 
corpus of a Japanese male, a 60-hour corpus of a 
Japanese female, and a 20-hour corpus of a Chi-
nese female). This corpus-based approach makes it 
possible to preserve the naturalness and personality 
of the speech without introducing signal processing 
to the speech segment [11]. XIMERA?s HMM 
(Hidden Markov Model)-based statistical prosody 
model is automatically trained, so it can generate a 
highly natural F0 pattern [12]. In addition, the cost 
function for segment selection has been optimized 
based on perceptual experiments, thereby improv-
ing the naturalness of the selected segments [13]. 
3 EVALUATION 
3.1 Speech and language corpora 
We have collected three kinds of speech and lan-
guage corpora: BTEC (Basic Travel Expression 
Corpus), MAD (Machine Aided Dialog), and FED 
(Field Experiment Data) [14, 15, 16, and 17]. The 
BTEC Corpus includes parallel sentences in two 
languages composed of the kind of sentences one 
might find in a travel phrasebook. MAD is a dialog 
corpus collected using a speech-to-speech transla-
tion system. While the size of this corpus is rela-
tively limited, the corpus is used for adaptation and 
 
4 http://www.slt.atr.jp/IWSLT2006/ 
26
evaluation. FED is a corpus collected in Kansai 
International Airport uttered by travelers using the 
airport. 
3.2 Speech recognition system 
The size of the vocabulary was about 35 k in ca-
nonical form and 50 k with pronunciation varia-
tions. Recognition results are shown in Table 1 for 
Japanese, English, and Chinese with a real-time 
factor5 of 5. Although the speech recognition per-
formance for dialog speech is worse than that for 
read speech, the utterance correctness excluding 
erroneous recognition output using GWPP [8] was 
greater than 83% in all cases. 
 
 BTEC MAD FED 
Characteristics Read speech 
Dialog 
speech 
(Office) 
Dialog 
speech 
(Airport)
# of speakers 20 12 6
# of utterances 510 502 155
# of word tokens 4,035 5,682 1,108
Average length 7.9 11.3 7.1
Perplexity 18.9 23.2 36.2
Japanese 94.9 92.9 91.0
English 92.3 90.5 81.0Word ac-curacy 
Chinese 90.7 78.3 76.5
All 82.4 62.2 69.0Utterance 
correct-
ness 
Not re-
jected 87.1 83.9 91.4
Table 1 Evaluation of speech recognition 
3.3 Machine Translation 
The mechanical evaluation is shown, where there 
are sixteen reference translations. The performance 
is very high except for English-to-Chinese (Table 
2). 
 
 BLEU
Japanese-to-English 0.6998
English-to-Japanese 0.7496
Japanese-to-Chinese 0.6584
Chinese-to-Japanese 0.7400
English-to-Chinese 0.5520
Chinese-to-English 0.6581
Table 2 Mechanical evaluation of translation 
                                                 
5 The real time factor is the ratio to an utterance time. 
The translation outputs were ranked A (perfect), 
B (good), C (fair), or D (nonsense) by professional 
translators. The percentage of ranks is shown in 
Table 3. This is in accordance with the above 
BLEU score. 
 
 A AB ABC
Japanese-to-English 78.4 86.3 92.2
English-to-Japanese 74.3 85.7 93.9
Japanese-to-Chinese 68.0 78.0 88.8
Chinese-to-Japanese 68.6 80.4 89.0
English-to-Chinese 52.5 67.1 79.4
Chinese-to-English 68.0 77.3 86.3
Table 3 Human Evaluation of translation 
4 System presented at ACL 2007 
The system works well in a noisy environment and 
translation can be performed for any combination 
of Japanese, English, and Chinese languages. The 
display of the current speech-to-speech translation 
system is shown below.  
 
 
 
Figure 1  Japanese-to-English Display of NICT-
ATR Speech-to-Speech Translation System 
 
5 CONCLUSION 
This paper presented a speech-to-speech transla-
tion system that has been developed by NICT-ATR 
for two decades. Various techniques, such as noise 
suppression and corpus-based modeling for both 
speech processing and machine translation achieve 
robustness and portability.  
The evaluation has demonstrated that our system 
is both effective and useful in a real-world envi-
ronment. 
27
References  
[1] S. Nakamura, K. Markov, H. Nakaiwa, G. Kikui, H. 
Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E. 
Sumita, and S. Yamamoto. The ATR multilingual 
speech-to-speech translation system. IEEE Trans. on 
Audio, Speech, and Language Processing, 14, No. 
2:365?376, 2006. 
[2] T. Shimizu, Y. Ashikari, E. Sumita, H. Kashioka, 
and S. Nakamura, ?Development of client-server 
speech translation system on a multi-lingual speech 
communication platform,? Proc. of the International 
Workshop on Spoken Language Translation, pp. 213-
216, Kyoto, Japan, 2006. 
[3] R. Zhang, H. Yamamoto, M. Paul, H. Okuma, K. 
Yasuda, Y. Lepage, E. Denoual, D. Mochihashi, A. 
Finch, and E. Sumita, ?The NiCT-ATR Statistical 
Machine Translation System for the IWSLT 2006 
Evaluation,? Proc. of the International Workshop on 
Spoken Language Translation, pp. 83-90, Kyoto, Ja-
pan , 2006. 
[4] T. Jitsuhiro, T. Matsui, and S. Nakamura. Automatic 
generation of non-uniform context-dependent HMM 
topologies based on the MDL criterion. In Proc. of 
Eurospeech, pages 2721?2724, 2003. 
[5] H. Yamamoto, S. Isogai, and Y. Sagisaka. Multi-
class composite N-gram language model. Speech 
Communication, 41:369?379, 2003. 
[6] M. Fujimoto and Y. Ariki. Combination of temporal 
domain SVD based speech enhancement and GMM 
based speech estimation for ASR in noise - evalua-
tion on the AURORA II database and tasks. In Proc. 
of Eurospeech, pages 1781?1784, 2003. 
[7] F. K. Soong, W. K. Lo, and S. Nakamura. Optimal 
acoustic and language model weight for minimizing 
word verification errors. In Proc. of ICSLP, pages 
441?444, 2004 
[8] W. K. Lo and F. K. Soong. Generalized posterior 
probability for minimum error verification of recog-
nized sentences. In Proc. of ICASSP, pages 85?88, 
2005. 
[9] R. Zhang, G. Kikui, and E. Sumita, ?Subword-based 
tagging by conditional random fields for Chinese 
word segmentation,? in Companion volume to the 
proceedings of the North American chapter of the 
Association for Computational Linguistics (NAACL), 
2006, pp. 193?196. 
[10] H. Yamamoto and E. Sumita, ?Online language 
model task adaptation for statistical machine transla-
tion (in Japanese),? in FIT2006, Fukuoka, Japan, 
2006, pp. 131?134. 
[11] H. Kawai, T. Toda, J. Ni, and M. Tsuzaki. XI-
MERA: A new TTS from ATR based on corpus-
based technologies. In Proc. of 5th ISCA Speech 
Synthesis Workshop, 2004. 
[12] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobaya-
shi, and T. Kitamura. Speech parameter generation 
algorithms for HMM-based speech synthesis. In Proc. 
of ICASSP, pages 1215?1218, 2000. 
[13] T. Toda, H. Kawai, and M. Tsuzaki. Optimizing 
sub-cost functions for segment selection based on 
perceptual evaluation in concatenative speech syn-
thesis. In Proc. of ICASSP, pages 657?660, 2004. 
[14] T. Takezawa and G. Kikui. Collecting machine ?
translation-aided bilingual dialogs for corpus-based 
speech translation. In Proc. of Eurospeech, pages 
2757?2760, 2003. 
[15] G. Kikui, E. Sumita, T. Takezawa, and S. Yama-
moto. Creating corpora for speech-to-speech transla-
tion. In Proc. Of Eurospeech, pages 381?384, 2003. 
[16] T. Takezawa and G. Kikui. A comparative study on 
human communication behaviors and linguistic char-
acteristics for speech-to-speech translation. In Proc. 
of LREC, pages 1589?1592, 2004. 
[17] G. Kikui, T. Takezawa, M. Mizushima, S. Yama-
moto, Y. Sasaki, H. Kawai, and S. Nakamura. Moni-
tor experiments of ATR speech-to-speech translation 
system. In Proc. of Autumn Meeting of the Acousti-
cal Society of Japan, pages 1?7?10, 2005, in Japa-
nese. 
28
