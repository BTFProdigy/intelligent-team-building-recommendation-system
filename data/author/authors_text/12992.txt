Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 130?134,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
SMT and SPE Machine Translation Systems for WMT?09
Holger Schwenk and Sadaf Abdul-Rauf and Lo??c Barrault
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
schwenk,abdul,barrault@lium.univ-lemans.fr
Jean Senellart
SYSTRAN SA
92044 Paris La De?fense cedex, FRANCE
senellart@systran.fr
Abstract
This paper describes the development of
several machine translation systems for
the 2009 WMT shared task evaluation.
We only consider the translation between
French and English. We describe a sta-
tistical system based on the Moses de-
coder and a statistical post-editing sys-
tem using SYSTRAN?s rule-based system.
We also investigated techniques to auto-
matically extract additional bilingual texts
from comparable corpora.
1 Introduction
This paper describes the machine translation sys-
tems developed by the Computer Science labo-
ratory at the University of Le Mans (LIUM) for
the 2009 WMT shared task evaluation. This work
was performed in cooperation with the company
SYSTRAN. We only consider the translation be-
tween French and English (in both directions).
The main differences to the previous year?s system
(Schwenk et al, 2008) are as follows: better us-
age of SYSTRAN?s bilingual dictionary in the sta-
tistical system, less bilingual training data, addi-
tional language model training data (news-train08
as distributed by the organizers), usage of com-
parable corpora to improve the translation model,
and development of a statistical post-editing sys-
tem (SPE). These different components are de-
scribed in the following.
2 Used Resources
In the frame work of the 2009 WMT shared trans-
lation task many resources were made available.
The following sections describe how they were
used to train the translation and language models
of the systems.
2.1 Bilingual data
The latest version of the French/English Europarl
and news-commentary corpus were used. We re-
alized that the first corpus contains parts with for-
eign languages. About 1200 such lines were ex-
cluded.1 Additional bilingual corpora were avail-
able, namely the Canadian Hansard corpus (about
68M English words) and an UN corpus (about
198M English words). In several initial exper-
iments, we found no evidence that adding this
data improves the overall system and they were
not used in the final system, in order to keep
the phrase-table small. We also performed ex-
periments with the provided so-called bilingual
French/English Gigaword corpus (575M English
words in release 3). Again, we were not able
to achieve any improvement by adding this data
to the training material of the translation model.
These findings are somehow surprising since it
was eventually believed by the community that
adding large amounts of bitexts should improve
the translation model, as it is usually observed for
the language model (Brants et al, 2007).
In addition to these human generated bitexts,
we also integrated a high quality bilingual dictio-
nary from SYSTRAN. The entries of the dictio-
nary were directly added to the bitexts. This tech-
nique has the potential advantage that the dictio-
nary words could improve the alignments of these
words when they also appear in the other bitexts.
However, it is not guaranteed that multi-word ex-
pressions will be correctly aligned by GIZA++
and that only meaningful translations will actually
appear in the phrase-table. A typical example is
fire engine ? camion de pompiers, for which the
individual constituent words are not good trans-
lations of each other. The use of a dictionary to
improve an SMT system was also investigated by
1Lines 580934?581316 and 599839?600662.
130
EN
SMT
FR
used as queries
per day articles
candidate sentence pairs parallel 
sentences
+?5 day articles
from English Gigaword
English
translations Gigaword
French
174M words
133M words
tail
removal
sentences with
extra words at ends
+
9.3M words
parallel 
number / table
comparison
      length  
removing
WER
10.3M words
Figure 1: Architecture of the parallel sentence extraction system (Rauf and Schwenk, 2009).
(Brown et al, 1993).
In comparison to our previous work (Schwenk
et al, 2008), we also included all verbs in the
French subjonctif and passe? simple tense. In fact,
those tenses seem to be frequently used in news
material. In total about 10,000 verbs, 1,500 adjec-
tives/adverbs and more than 100,000 noun forms
were added.
2.2 Use of Comparable corpora
Available human translated bitexts such as the UN
and the Hansard corpus seem to be out-of domain
for this task, as mentioned above. Therefore, we
investigated a new method to automatically extract
and align parallel sentences from comparable in-
domain corpora. In this work we used the AFP
news texts since there are available in the French
and English LDC Gigaword corpora.
The general architecture of our parallel sentence
extraction system is shown in figure 1. We first
translate 174M words from French into English
using an SMT system. These English sentences
are then used to search for translations in the En-
glish AFP texts of the Gigaword corpus using in-
formation retrieval techniques. The Lemur toolkit
(Ogilvie and Callan, 2001) was used for this pur-
pose. Search was limited to a window of ?5 days
of the date of the French news text. The retrieved
candidate sentences were then filtered using the
word error rate with respect to the automatic trans-
lations. In this study, sentences with an error rate
below 32% were kept. Sentences with a large
length difference (French versus English) or con-
taining a large fraction of numbers were also dis-
carded. By these means, about 9M words of ad-
ditional bitexts were obtained. An improved ver-
sion of this algorithm using TER instead of the
word error rate is described in detail in (Rauf and
Schwenk, 2009).
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. We
realized that the news-train08 corpora contained
some foreign texts, in particular in German. We
tried to filter those lines using simple regular ex-
pressions. We also discarded lines with a large
fraction of numerical expressions. In addition,
LDC?s Gigaword collection, the Hansard corpus
and the UN corpus were used for both languages.
Finally, about 30M words crawled from the WEB
were used for the French LM. All this data pre-
dated the evaluation period.
2.4 Development data
All development was done on news-dev2009a and
news-dev2009b was used as internal test set. The
default Moses tokenization was used. All our
models are case sensitive and include punctuation.
The BLEU scores reported in this paper were cal-
culated with the NIST tool and are case sensitive.
3 Language Modeling
Language modeling plays an important role in
SMT systems. 4-gram back-off language models
(LM) were used in all our systems. The word list
contains all the words of the bitext used to train
the translation model and all words that appear at
least ten times in the news-train08 corpus. Sep-
arate LMs were build on each data source with
the SRI LM toolkit (Stolcke, 2002) and then lin-
early interpolated, optimizing the coefficients with
an EM procedure. The perplexities of these LMs
131
Corpus # Fr words Dev09a Dev09b Test09
SMT system
Eparl+NC 46.5M 22.44 22.38 25.60
Eparl+NC+dict 48.5M 22.60 22.55 26.01
Eparl+NC+dict+AFP 57.8M 22.82 22.63? 26.18
SPE system
SYSTRAN - 17.76 18.13 19.98
Eparl+NC 45.5M 22.84 22.59# 25.59
Eparl+NC+AFP 54.4M 22.72 21.96 25.40
Table 1: Case sensitive NIST BLEU scores for the French-English systems. ?NC? denotes the news-
commentary bitexts, ?dict? SYSTRAN?s bilingual dictionary and ?AFP? the automatically aligned news
texts (?=primary, #=contrastive system)
are given in Table 2. Adding the new news-train08
monolingual data had an important impact on the
quality of the LM, even when the Gigaword data
is already included.
Data French English
Vocabulary size 407k 299k
Eparl+news 248.8 416.7
+ LDC Gigaword 142.2 194.9
+ Hansard and UN 137.5 187.5
news-train08 alone 165.0 245.9
all 120.6 174.8
Table 2: Perplexities on the development data of
various language models.
4 Architecture of the SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source
sentence f . It is today common practice to use
phrases as translation units (Koehn et al, 2003;
Och and Ney, 2003) and a log linear framework in
order to introduce several models explaining the
translation process:
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to max-
imize a scoring function on a development set
(Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).2 This speeds
up the process and corrects an error of GIZA++
that can appear with rare words. This previously
caused problems when adding the entries of the
bilingual dictionary to the bitexts.
Phrases and lexical reorderings are extracted us-
ing the default settings of the Moses toolkit. The
parameters of Moses are tuned on news-dev2009a,
using the cmert tool. The basic architecture of
the system is identical to the one used in the
2008 WMT evaluation (Schwenk et al, 2008),
but we did not use two pass decoding and n-best
list rescoring with a continuous space language
model.
The results of the SMT systems are summarized
in the upper part of Table 1 and 3. The dictionary
and the additional automatically produced AFP bi-
texts achieved small improvements when translat-
ing from French to English. In the opposite trans-
lation direction, the systems that include the addi-
tional AFP texts exhibit a bad generalisation be-
havior. We provide also the performance of the
different systems on the official test set, calculated
after the evaluation. In most of the cases, the ob-
served improvements carry over on the test set.
5 Architecture of the SPE system
During the last years statistical post-editing sys-
tems have shown to achieve very competitive per-
formance (Simard et al, 2007; Dugast et al,
2007). The main idea of this techniques is to use
2The source is available at http://www.cs.cmu.
edu/
?
qing/
132
Corpus # En words Dev09a Dev09b Test09
SMT system
Eparl+NC 41.6M 21.89 21.78 23.80
Eparl+NC+dict 44.0M 22.28 22.35# 24.13
Eparl+NC+dict+AFP 51.7M 22.21 21.43 23.88
SPE system
SYSTRAN - 18.68 18.84 20.29
Eparl+NC 44.2M 23.03 23.15 24.36
Eparl+NC+AFP 53.3M 22.95 23.15? 24.62
Table 3: Case sensitive NIST BLEU scores for the English-French systems. ?NC? denotes the news-
commentary bitexts, ?dict? denotes SYSTRAN?s bilingual dictionary and ?AFP? the automatically
aligned news texts (?=primary, #=contrastive system)
an SMT system to correct the errors of a rule-
based translation system. In this work, SYSTRAN
server version 6, followed by an SMT system
based on Moses were used. The post-editing sys-
tems uses exactly the same language models than
the above described stand-alone SMT systems.
The translation model was trained on the Europarl,
the news-commentary and the extracted AFP bi-
texts. The results of these SPE systems are sum-
marized in the lower part of Table 1 and 3. SYS-
TRAN?s rule-based system alone already achieves
remarkable BLEU scores although it was not op-
timized or adapted to this task. This could be sig-
nificantly improved using statistical post-editing.
The additional AFP texts were not useful when
translating form French to English, but helped to
improve the generalisation behavior for the En-
glish/French systems.
When translating from English to French (Ta-
ble 3), the SPE system is clearly better than the
carefully optimized SMT system. Consequently,
it was submitted as primary system and the SMT
system as contrastive one.
6 Conclusion and discussion
We described the development of two comple-
mentary machine translation systems for the 2009
WMT shared translation task: an SMT and an SPE
system. The last one is based on SYSTRAN?s
rule-based system. Interesting findings of this re-
search include the fact that the SPE system out-
performs the SMT system when translating into
French. This system has also obtained the best
scores in the human evaluation.
With respect to the SMT system, we were
not able to improve the translation model by
adding large amounts of bitexts, although different
sources were available (Canadian Hansard, UN
or WEB data). Eventually these corpora are too
noisy or out-of-domain. On the other hand, the
integration of a high quality bilingual dictionary
was helpful, as well as the automatic alignment of
news texts from comparable corpora.
Future work will concentrate on the integration
of previously successful techniques, in particu-
lar continuous space language models and lightly-
supervised training (Schwenk, 2008). We also be-
lieve that the tokenization could be improved, in
particular for the French sources texts. Numbers,
dates and other numerical expressions could be
translated by a rule-based system.
System combination has recently shown to pro-
vide important improvements of translation qual-
ity. We are currently working on a combination of
the SMT and SPE system. It may be also interest-
ing to add a third (hierarchical) MT system.
7 Acknowledgments
This work has been partially funded by the French
Government under the project INSTAR (ANR
JCJC06 143038) and the by the Higher Education
Commission, Pakistan through the HEC Overseas
Scholarship 2005.
133
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In EMNLP, pages 858?
867.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the
workshop on Human Language Technology, pages
202?205, Princeton, New Jersey.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Second Workshop on SMT,
pages 179?182.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In EACL, page to be published.
Holger Schwenk, Jean-Baptiste Fouet, and Jean Senel-
lart. 2008. First steps towards a general purpose
French/English statistical machine translation sys-
tem. In Third Workshop on SMT, pages 119?122.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT, pages 182?189.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with
statistical phrase-based post-editing. In Second
Workshop on SMT, pages 203?206.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP, pages II: 901?
904.
134
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 271?275,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MANY : Open Source MT System Combination at WMT?10
Lo??c Barrault
LIUM, University of Le Mans
Le Mans, France.
FirstName.LastName@lium.univ-lemans.fr
Abstract
LIUM participated in the System Combi-
nation task of the Fifth Workshop on Sta-
tistical Machine Translation (WMT 2010).
Hypotheses from 5 French/English MT
systems were combined with MANY, an
open source system combination software
based on confusion networks currently de-
veloped at LIUM.
The system combination yielded signifi-
cant improvements in BLEU score when
applied on WMT?09 data. The same be-
havior has been observed when tuning is
performed on development data of this
year evaluation.
1 Introduction
This year, the LIUM computer science labora-
tory has participated in the French-English sys-
tem combination task at WMT?10 evaluation cam-
paign. The system used for this task is MANY1
(Barrault, 2010), an open source system combina-
tion software based on Confusion Networks (CN).
Several improvements have been made in order to
being able to combine many systems outputs in a
decent time.
The focus has been put on the tuning step, and
more precisely how to perform system parameter
tuning. Two methods have been experimented cor-
responding to two different representations of sys-
tem combination. In the first one, system combi-
nation is considered as a whole : fed by system
hypotheses as input and generating a new hypoth-
esis as output. The second method considers that
the alignment module is independent from the de-
coder, so that the parameters from each module
can be tuned separately.
1MANY is available at the following address http://
www-lium.univ-lemans.fr/?barrault/MANY
Those tuning approaches are described in sec-
tion 3. Before that, a quick description of MANY,
including recent developments, can be found in
section 2. Results on WMT?09 data are pre-
sented in section 4 along results of tuning on
newssyscombtune2010.
2 System description
MANY is a system combination software (Bar-
rault, 2010) based on the decoding of a lattice
made of several Confusion Networks (CN). This is
a widespread approach in MT system combination
(Rosti et al, 2007); (Shen et al, 2008); (Karakos
et al, 2008). MANY can be decomposed in two
main modules. The first one is the alignment mod-
ule which actually is a modified version of TERp
(Snover et al, 2009). Its role is to incrementally
align the hypotheses against a backbone in order to
create a confusion network. Those confusion net-
works are then connected together to create a lat-
tice. This module uses different costs (which cor-
responds to a match, an insertion, a deletion, a sub-
stitution, a shift, a synonym and a stem) to com-
pute the best alignment and incrementally build
a confusion network. In the case of confusion
network, the match (substitution, synonyms, and
stems) costs are considered when the word in the
hypothesis matches (is a substitution, a synonyms
or a stems of) at least one word of the considered
confusion sets in the CN, as shown in Figure 1.
The second module is the decoder. This decoder
is based on the token pass algorithm and it accepts
as input the lattice previously created. The proba-
bilities computed in the decoder can be expressed
as follow :
log(PW ) =
Len(W )?
n=0
{
?1logPws(n) + ?2logPlm(n)
+?3Lpen(n) + ?4Npen(n)
}
(1)
where Len(W ) is the length of the hypothesis,
271
Is
the dinner
included ?
Do you dinnercalculated ?have
Is the dinner included ?
isSupper ?included
Paraphrase
{
Match
Is the dinner included
?
NULLsupper
Match
Match
Match
Sub
Sub
Ins
Sub
Do you
NULL
supper
calculated
have
NULL
Match
Figure 1: Incremental alignment with TERp re-
sulting in a confusion network.
Pws(n) is the score of the nth word in the lattice,
Plm(n) is its LM probability, Lpen(n) is the length
penalty (which apply when Wn is not a null-arc),
Npen(n) is the penalty applied when crossing a
null-arc, and the ?i are the features weights.
Multithreading
One major issue with system combination con-
cerns scaling. Indeed, in order to not lose infor-
mation about word order, all system hypotheses
are considered as backbone and all other hypothe-
ses are aligned to it to create a CN. Consequently,
if we consider N system outputs, then to build N
confusion networks, N ? (N ? 1) alignments with
modified TERp have to be performed. Moreover,
in order to get better results, the TERp costs have
to be optimized, which requires a lot of iterations,
all of which calculate N ? (N ? 1) alignments.
However, the building of a CN with system i as
backbone does not depend on the building of CN
with other system as backbone. Therefore multi-
threading has been integrated into MANY so that
multiple CNs can be created in parallel. From now
on, the number of thread can be specified in the
configuration file.
3 Tuning
As mentioned before, MANY is made of two main
modules : the alignment module based on a modi-
fied version of TERp and the decoder. Considering
10 systems, 19 parameters in total have to be op-
timized in order to get better results. By default,
TERp costs are set to 0.0 for match and 1.0 for
everything else. These costs are not correct, since
a shift in that case will hardly be possible. TERp
costs, system priors, fudge factor, null-arc penalty,
length penalty are tuned with Condor (a global op-
timizer based on the Powell?s algorithm, (Berghen
and Bersini, 2005)).
Two ways of tuning have been experimented.
The first one consists in optimizing the whole set
of parameters together (see section 3.1). The sec-
ond one rely on the (maybe likely) independence
of the TERp parameters towards those of the de-
coder and consists in tuning TERp parameters in
a first step and then using the optimized TERp
costs when tuning the decoder parameters (see
section 3.2).
3.1 Tuning all parameters together
Condor is an optimizer which aims at minimizing
a certain objective function. In our case, the ob-
jective function is the whole system combination.
As input, it takes the whole set of parameters (i.e.
TERp costs except match costs (which is always
set to 0), system priors, the fudge factor, and null-
arc and length penalty) and outputs -BLEU score.
The BLEU score is one of the most robust met-
rics as presented in (Leusch et al, 2009), which is
consequently an obvious target for optimization.
Such a tuning protocol has the disadvantage
to be slower as all the confusion networks have
to be regenerated at each step because the TERp
costs provided by the optimizer will hardly be the
same for two iterations (thus, confusion networks
computed during previous iterations can hardly be
reused). Another issue with this approach is that it
is hard to converge when the parameter set is that
large. This is mainly due to the fact that we can-
not guarantee the convexity of the problem. How-
ever, one advantage is that the possible correlation
between all parameters are taken into account dur-
ing the optimization process, which is not the case
when optimizing in several steps.
3.2 Two-step tuning
Tuning TERp parameters : In order to opti-
mize TERp parameters (i.e. del, ins, sub, shift,
stem and syn costs), we have to determine which
measure to use to evaluate a certain configuration.
We naturally considered the minimization of the
TERp score. To do so, the confusion networks are
built using the set of parameters given by the op-
timizer. TERp scores are then calculated between
the reference and each CN, and summed up.
The goal of this step is to guide the confusion
networks generation process to produce sentences
272
similar to the reference. Consequently, if the con-
fusion networks generated at this step have a lower
TERp score, then this means that the decoder is
more likely to find a better hypothesis inside.
Tuning decoder parameters : Based on the
TERp configuration determined at the previous
step, this step aims at finding good parameter val-
ues. Those parameters control the final hypothe-
sis size and the importance given to the language
model probabilities compared to the translation
scores (occurring on words). The metric which is
minimized is -BLEU for the same reasons men-
tioned in section 3.1.
4 Experiments and Results
During experiments, data from last year evaluation
campaign are used for testing the tuning approach.
news-dev2009a is used as development set, and
news-dev2009b as internal test, these corpora are
described in Table 1.
NAME #sent. #words #tok
news-dev2009a 1025 21583 24595
news-dev2009b 1026 21837 24940
Table 1: WMT?09 corpora : number of sentences,
words and tokens calculated on the reference.
For the sake of speed and simplicity, the five
best systems (ranking given by score on dev) are
considered only. Baseline systems performances
on dev and test are presented in Table 2.
Corpus Sys0 Sys1 Sys2 Sys3 Sys4
Dev 18.20 17.83 20.14 21.06 17.72
Test 18.53 18.33 20.43 21.35 18.15
Table 2: Baseline systems performance on
WMT?09 data (%BLEU).
When tuning all parameters together, the set ob-
tained is presented in Table 3. The 2-step tuning
Costs : Del Stem Syn Ins Sub Shift
0.89 0.94 1.04 0.98 0.94 0.94
Dec. : Fudge Nullpen Lenpen
0.01 0.25 1.46
Weights : Sys0 Sys1 Sys2 Sys3 Sys4
0.04 0.04 0.16 0.26 0.04
Table 3: Parameters obtained with 1-step tuning.
protocol applied on news-dev2009a provides the
set of parameters presented in Table 4.
Costs : Del Stem Syn Ins Sub Shift
9e-6 0.89 1.22 0.26 0.44 1.76
Dec. : Fudge Nullpen Lenpen
0.1 0.27 2.1
Weights : Sys0 Sys1 Sys2 Sys3 Sys4
0.07 0.09 0.09 0.09 0.11
Table 4: Parameters obtained with 2-step tuning.
Results on development corpus of WMT?09
(used as test set) are presented in Table 5. We
System Dev Test
Best single 21.06 21.35
MANY 22.08 22.28
MANY-2steps 21.94 22.09
Table 5: System Combination results on WMT?09
data.
can observe that 2-step tuning provides almost 0.9
BLEU point improvement on development corpus
which is well reflected on test set with a gain of
more than 0.7 BLEU. The best results are obtain
when tuning all parameters together, which give
more than 1 BLEU point improvement on dev and
more than 0.9 on test.
4.1 Discussion
Choosing a measure to optimize the TERp costs is
not something easy. One important remark is that
default (equal) costs are not suitable to get good
confusion networks. The goal of the confusion
networks is to make possible the generation of a
new hypothesis which can be different from those
provided by each individual system.
In these experiments, TERp calculated between
the CNs and the reference is used as the distance
to be minimized by the optimizer. We can no-
tice that for the 2-step optimization, the deletion
cost is very small. This is probably not a value
which is expected, because in this case, this means
that deletions can occur in an hypothesis without
penalizing it a lot. However, this parameter set
has a beneficial impact on the system combination
performance. Another comment is that the sys-
tem weights are not directly proportional to the re-
sults. This suggests that some phrases proposed
by weaker systems can have a higher importance
for system combination.
By contrast, optimizing parameters all together
provides more fair weights, according to the re-
273
sults of the single systems.
4.2 2010 evaluation campaign
For this year system combination tasks, a de-
velopment corpus (syscombtune) and the test
(syscombtest), described in Table 6, were pro-
vided to participants.
NAME #sentences #words #words tok
syscombtune 455 9348 10755
syscombtest 2034 - -
Table 6: Description of WMT?10 corpora.
Language model : The English target language
models has been trained on all monolingual data
provided for the translation tasks. In addition,
LDC?s Gigaword collection was used for both lan-
guages. Data corresponding to the development
and test periods were removed from the Gigaword
collections.
Tuning on syscombdev2010 corpus produced
the parameter set presented in Table 7
Costs : Del Stem Syn Ins Sub Shift
Dec. : Fudge Nullpen Lenpen
0.01 0.33 1.6
Weights : Sys0 Sys1 Sys2 Sys3 Sys4
0.11 0.21 0.04 0.15 0.15
Table 7: Parameters obtained with tuning.
The result provided by the system with this con-
figuration can be compared to the single systems
in Table 8.
System newssyscombtune2010
Sys0 27.74
Sys1 27.26
Sys2 27.15
Sys3 27.06
Sys4 27.04
MANY 28.63
Table 8: Baseline systems performance on
WMT?10 development data (%BLEU).
A behavior comparable to WMT?09 evaluation
campaign is observed, which suggests that the ap-
proach is correct.
5 Conclusion and future work
We have shown that tuning all parameters together
is better than 2-step tuning. However, the second
method has not been fully explored. Tuning TERp
parameters targeting minimum TERp score is not
satisfying. Therefore, an alternative measure, like
ngram agreement which would be more related to
BLEU, can be considered in order to obtain better
parameters.
Further improvement for MANY will be con-
sidered like case insensitive combination then re-
casing the output using majority vote on the con-
fusion networks. This is currently a work in
progress.
6 Acknowledgement
This work has been partially funded by the Eu-
ropean Union under the EuroMatrix Plus project
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720)
References
Barrault, L. (2010). MANY : Open source ma-
chine translation system combination. Prague
Bulletin of Mathematical Linguistics, Special
Issue on Open Source Tools for Machine Trans-
lation, 93:147?155.
Berghen, F. V. and Bersini, H. (2005). CON-
DOR, a new parallel, constrained extension of
Powell?s UOBYQA algorithm: Experimental
results and comparison with the DFO algo-
rithm. Journal of Computational and Applied
Mathematics, 181:157?175.
Karakos, D., Eisner, J., Khudanpur, S., and
Dreyer, M. (2008). Machine translation sys-
tem combination using ITG-based alignments.
In 46th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies., pages 81?84, Columbus, Ohio,
USA.
Leusch, G., Matusov, E., and Ney, H. (2009).
The RWTH system combination system for
WMT 2009. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 61?65, Athens, Greece.
Rosti, A.-V., Matsoukas, S., and Schwartz, R.
(2007). Improved word-level system combina-
tion for machine translation. In Association for
Computational Linguistics, pages 312?319.
274
Shen, W., Delaney, B., Anderson, T., and Slyh,
R. (2008). The MIT-LL/AFRL IWSLT-2008
MT System. In International Workshop on Spo-
ken Language Translation, Hawaii, U.S.A.
Snover, M., Madnani, N., Dorr, B., and
Schwartz, R. (2009). TER-Plus: Para-
phrase, semantic, and alignment enhancements
to translation edit rate. Machine Translation
Journal.
275
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392?399,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Translation Model Adaptation by Resampling
Kashif Shah, Lo??c Barrault, Holger Schwenk
LIUM, University of Le Mans
Le Mans, France.
FirstName.LastName@lium.univ-lemans.fr
Abstract
The translation model of statistical ma-
chine translation systems is trained on par-
allel data coming from various sources and
domains. These corpora are usually con-
catenated, word alignments are calculated
and phrases are extracted. This means
that the corpora are not weighted accord-
ing to their importance to the domain of
the translation task. This is in contrast
to the training of the language model for
which well known techniques are used to
weight the various sources of texts. On
a smaller granularity, the automatic cal-
culated word alignments differ in quality.
This is usually not considered when ex-
tracting phrases either.
In this paper we propose a method to auto-
matically weight the different corpora and
alignments. This is achieved with a resam-
pling technique. We report experimen-
tal results for a small (IWSLT) and large
(NIST) Arabic/English translation tasks.
In both cases, significant improvements in
the BLEU score were observed.
1 Introduction
Two types of resources are needed to train statis-
tical machine translation (SMT) systems: parallel
corpora to train the translation model and mono-
lingual texts in the target language to build the
language model. The performance of both mod-
els depends of course on the quality and quantity
of the available resources.
Today, most SMT systems are generic, i.e. the
same system is used to translate texts of all kinds.
Therefore, it is the domain of the training re-
sources that influences the translations that are se-
lected among several choices. While monolingual
texts are in general easily available in many do-
mains, the freely available parallel texts mainly
come from international organisations, like the
European Union or the United Nations. These
texts, written in particular jargon, are usually
much larger than in-domain bitexts. As an exam-
ple we can cite the development of an NIST Ara-
bic/English phrase-based translation system. The
current NIST test sets are composed of a news
wire part and a second part of web-style texts.
For both domains, there is only a small number
of in-domain bitexts available, in comparison to
almost 200 millions words of out-of-domain UN
texts. The later corpus is therefore likely to domi-
nate the estimation of the probability distributions
of the translation model.
It is common practice to use a mixture language
model with coefficients that are optimized on the
development data, i.e. by these means on the do-
main of the translation task. Domain adaptation
seems to be more tricky for the translation model
and it seems that very little research has been done
that seeks to apply similar ideas to the translation
model. To the best of our knowledge, there is no
commonly accepted method to weight the bitexts
coming from different sources so that the transla-
tion model is best optimized to the domain of the
task. Mixture models are possible when only two
different bitexts are available, but are rarely used
for more corpora (see discussion in the next sec-
tion).
In this work we propose a new method to adapt
the translation model of an SMT system. We only
perform experiments with phrase-based systems,
but the method is generic and could be easily ap-
plied to an hierarchical or syntax-based system.
We first associate a weighting coefficient to each
bitext. The main idea is to use resampling to pro-
duce a new collection of weighted alignment files,
followed by the standard procedure to extract the
phrases. In a second step, we also consider the
392
alignment score of each parallel sentence pair, em-
phasizing by these means good alignments and
down-weighting less reliable ones. All the param-
eters of our procedure are automatically tuned by
optimizing the BLEU score on the development
data.
The paper is organized as follows. The next
section describes related work on weighting the
corpora and model adaptation. Section 3 de-
scribes the architecture allowing to resample and
to weight the bitexts. Experimental results are pre-
sented in section 4 and the paper concludes with a
discussion.
2 Related Work
Adaptation of SMT systems is a topic of in-
creasing interest since few years. In previous
work, adaptation is done by using mixture mod-
els, by exploiting comparable corpora and by self-
enhancement of translation models.
Mixture models were used to optimize the co-
efficients to the adaptation domain. (Civera and
Juan, 2007) proposed a model that can be used
to generate topic-dependent alignments by exten-
sion of the HMM alignment model and derivation
of Viterbi alignments. (Zhao et al, 2004) con-
structed specific language models by using ma-
chine translation output as queries to extract sim-
ilar sentences from large monolingual corpora.
(Foster and Kuhn, 2007) applied a mixture model
approach to adapt the system to a new domain by
using weights that depend on text distances to mix-
ture components. The training corpus was divided
into different components, a model was trained on
each part and then weighted appropriately for the
given context. (Koehn and Schroeder, 2007) used
two language models and two translation models:
one in-domain and other out-of-domain to adapt
the system. Two decoding paths were used to
translate the text.
Comparable corpora are exploited to find addi-
tional parallel texts. Information retrieval tech-
niques are used to identify candidate sentences
(Hildebrand et al, 2005). (Snover et al, 2008)
used cross-lingual information retrieval to find
texts in the target language that are related to the
domain of the source texts.
A self-enhancing approach was applied by
(Ueffing, 2006) to filter the translations of the
test set with the help of a confidence score and
to use reliable alignments to train an additional
phrase table. This additional table was used with
the existing generic phrase table. (Ueffing, 2007)
further refined this approach by using transduc-
tive semi-supervised methods for effective use of
monolingual data from the source text. (Chen et
al., 2008) performed domain adaptation simulta-
neously for the translation, language and reorder-
ing model by learning posterior knowledge from
N-best hypothesis. A related approach was in-
vestigated in (Schwenk, 2008) and (Schwenk and
Senellart, 2009) in which lightly supervised train-
ing was used. An SMT system was used to trans-
late large collections of monolingual texts, which
were then filtered and added to the training data.
(Matsoukas et al, 2009) propose to weight each
sentence in the training bitext by optimizing a dis-
criminative function on a given tuning set. Sen-
tence level features were extracted to estimate the
weights that are relevant to the given task. Then
certain parts of the training bitexts were down-
weighted to optimize an objective function on the
development data. This can lead to parameter
over-fitting if the function that maps sentence fea-
tures to weights is complex.
The technique proposed in this paper is some-
how related to the above approach of weighting
the texts. Our method does not require an ex-
plicit specification of the in-domain and out-of-
domain training data. The weights of the corpora
are directly optimized on the development data us-
ing a numerical method, similar to the techniques
used in the standard minimum error training of the
weights of the feature functions in the log-linear
criterion. All the alignments of the bitexts are re-
sampled and given equal chance to be selected and
therefore, influence the translation model in a dif-
ferent way. Our proposed technique does not re-
quire the calculation of extra sentence level fea-
tures, however, it may use the alignments score as-
sociated with each aligned sentence pair as a con-
fidence score.
3 Description of the algorithm
The architecture of the algorithm is summarized in
figure 1. The starting point is an (arbitrary) num-
ber of parallel corpora. We first concatenate these
bitexts and perform word alignments in both direc-
tions using GIZA++. This is done on the concate-
nated bitexts since GIZA++ may perform badly
if some of the individual bitexts are rather small.
Next, the alignments are separated in parts corre-
393
Figure 1: Architecture of SMT Weighting System
sponding to the individual bitexts and a weighting
coefficient is associated to each one. We are not
aware of a procedure to calculate these coefficients
in an easy and fast way without building an actual
SMT system. Note that there is an EM procedure
to do this for language modeling.
In the next section, we will experimentally com-
pare equal coefficients, coefficients set to the same
values than those obtained when building an inter-
polated language model on the source language,
and a new method to determine the coefficients by
optimizing the BLEU score on the development
data.
One could imagine to directly use these coef-
ficients when calculating the various probabilities
of the extracted phrases. In this work, we propose
a different procedure that makes no assumptions
on how the phrases are extracted and probabilities
are calculated. The idea is to resample alignments
from the alignment file corresponding to the indi-
vidual bitexts according to their weighting coeffi-
cients. By these means, we create a new, poten-
tially larger alignment file, which then in turn will
be used by the standard phrase extraction proce-
dure.
3.1 Resampling the alignments
In statistics, resampling is based upon repeated
sampling within the same sample until a sample
is obtained which better represents a given data
set (Yu, 2003). Resampling is used for validating
models on given data set by using random subsets.
It overcomes the limitations to make assumptions
about the distribution of the data. Usually resam-
pling is done several times to better estimate and
select the samples which better represents the tar-
get data set. The more often we resample, the
closer we get to the true probability distribution.
In our case we performed resampling with re-
placement according to the following algorithm:
Algorithm 1 Resampling
1: for i = 0 to required size do
2: Select any alignment randomly
3: Alscore ? normalized alignment score
4: Threshold? rand[0, 1]
5: if Alscore > Threshold then
6: keep it
7: end if
8: end for
Let us call resampling factor, the number of
times resampling should be done. An interesting
question is to determine the optimal value of this
resampling factor.
It actually depends upon the task or data we are
experimenting on. We may start with one time
resampling and could stop when results becomes
stable. Figure 2 plots a typical curve of the BLEU
score as a function of the number of times we re-
sample. It can be observed that the curve is grow-
ing proportionally to the resampling factor until it
becomes stable after a certain point.
3.2 Weighting Schemes
We concentrated on translation model adaptation
when the bitexts are heterogeneous, e.g. in-
domain and out-of-domain or of different sizes. In
this case, weighting these bitexts seems interest-
ing and can be used in order to select data which
better represent the target domain. Secondly when
sentences are aligned, some alignments are reli-
able and some are less. Using unreliable align-
ments can put negative effect on the translation
quality. So we need to exclude or down-weight
394
 52
 52.5
 53
 53.5
 54
 54.5
 55
 55.5
 56
 0  5  10  15  20
BL
EU
Resampling factor
dev
test
baseline(test)
Figure 2: The curve shows that by increasing the
resampling factor we get better and stable results
on Dev and Test.
unreliable alignments and keep or up-weight the
good ones. We conceptually divided the weight-
ing in two parts that is (i) weighting the corpora
and (ii) weighting the alignments
3.2.1 Weighting Corpora
We started to resample the bitexts with equal
weights to see the effect of resampling. This gives
equal importance to each bitext without taking into
account the domain of the text to be translated.
However, it should be better to give appropriate
weights according to a given domain as shown in
equation 1
?1bitext1 + ?2bitext2 + ..+ ?nbitextn (1)
where the ?n are the coefficients to optimize.
One important question is how to find out the ap-
propriate coefficient for each corpus. We investi-
gated a technique similar to the algorithm used to
minimize the perplexity of an interpolated target
LM. Alternatively, it is also possible to construct a
interpolated language model on the source side of
bitexts. This approach was implemented and these
coefficients were used as the weights for each bi-
text. One can certainly ask the question whether
the perplexity is a good criterion for weighting bi-
texts. Therefore, we worked on direct optimiza-
tion of these coefficients by CONDOR (Berghen
and Bersini, 2005). This freely available tool is a
numerical optimizer based on Powell?s UOBYQA
algorithm (Powell, 1994). The aim of CONDOR
is to minimize a objective function using the least
number of function evaluations. Formally, it is
used to find x? ? Rn with given constraints which
satisfies
F (x?) = min
x
F (x) (2)
where n is the dimension of search space and x?
is the optimum of x. The following algorithm was
used to weight the bitexts.
Algorithm 2 WeightingCorpora
1: Determine word to word alignment with
GIZA++ on concatenated bitext.
2: while Not converged do
3: Run Condor initialized with LM weights.
4: Create new alignment file by resampling
according to weights given by Condor.
5: Use the alignment file to extract phrases
and build the translation table (phrase table)
6: Tune the system with MERT (this step can
be skipped until weights are optimized to
save time)
7: Calculate the BLEU score
8: end while
3.2.2 Weighting Alignments
Alignments produced by GIZA++ have alignment
scores associated with each sentence pair in both
direction, i.e. source to target and target to source.
We used these alignment scores as confidence
measurement for each sentence pair. Alignment
scores depend upon the length of each sentence,
therefore, they must be normalized regarding the
size of the sentence. Alignment scores have a very
large dynamic range and we have applied a loga-
rithmic mapping in order to flatten the probability
distribution :
log(? ?
( ntrg
?
asrc trg + nsrc
?
atrg src)
2
) (3)
where a is the alignment score, n the size of a
sentence and ? a coefficient to optimize. This is
also done by Condor.
Of course, some alignments will appear several
times, but this will increase the probability of cer-
tain phrase-pairs which are supposed to be more
related to the target domain. We have observed
that the weights of an interpolated LM build on
the source side of the bitext are good initial val-
ues for CONDOR. Moreover, weights optimized
by Condor are in the same order than these ?LM
weights?. Therefore, we do not perform MERT
of the SMT systems build at each step of the op-
timization of the weights ?i and ? by CONDOR,
395
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) Dev (NIST06) Test (NIST08)
Baseline 53.98 53.37 43.16 42.21
With equal weights 53.71 53.20 43.10 42.11
With LM weights 54.20 53.71 43.42 42.22
Condor weights 54.80 53.98 43.49 42.28
Table 1: BLEU scores when weighting corpora (one time resampling)
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) Dev (NIST06) Test (NIST08)
Baseline 53.98 53.37 43.16 42.21
With equal weights 53.80 53.30 43.13 42.15
With LM weights 54.32 53.91 43.54 42.37
Condor weights 55.10 54.13 43.80 42.40
Table 2: BLEU scores when weighting corpora (optimum number of resampling)
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) TER(Test) Dev (NIST06) Test (NIST08) TER(Test)
Baseline 53.98 53.37 32.75 43.16 42.21 51.69
With equal weights 53.85 53.33 32.80 43.28 42.21 51.72
With LM weights 54.80 54.10 31.50 43.42 42.41 51.50
Condor weights 55.48 54.58 31.31 43.95 42.54 51.35
Table 3: BLEU and TER scores when weighting corpora and alignments (optimum number of resam-
pling)
but use the values obtained by running MERT on
a system obtained by using the ?LM weights? to
weight the alignments. Once CONDOR has con-
verged to optimal weights, we can then tune our
system by MERT. This saves lot of time taken by
the tuning process and it had no impact on the re-
sults.
4 Experimental evaluation
The baseline system is a standard phrase-based
SMT system based on the Moses SMT toolkit
(Koehn and et al, 2007). In our system we
used fourteen features functions. These features
functions include phrase and lexical translation
probabilities in both directions, seven features for
lexicalized distortion model, a word and phrase
penalty, and a target language model. The MERT
tool is used to tune the coefficients of these fea-
ture functions. We considered Arabic to English
translation. Tokenization of the Arabic source
texts is done by a tool provided by SYSTRAN
which also performs a morphological decompo-
sition. We considered two well known official
evaluation tasks to evaluate our approach, namely
NIST and IWSLT.
For IWSLT, we used the BTEC bitexts (194M
words), Dev1, Dev2, Dev3 (60M words each) as
training data, Dev6 as development set and Dev7
as test set. From previous experiments, we have
evidence that the various development corpora are
not equally important and weighting them cor-
rectly should improve the SMT system. We an-
alyze the translation quality as measured by the
BLEU score for the three methods: equal weights,
LM weights and Condor weights and considering
one time resampling. Further experiments were
performed using the optimized number of resam-
pling with and without weighting the alignments.
We have realized that it is beneficial to always in-
clude the original alignments. Even if we resample
many times there is a chance that some alignments
might never be selected but we do not want to
loose any information. By keeping original align-
ments, all alignments are given a chance to be se-
396
lected at least once. All these results are summa-
rized in tables 1, 2 and 3.
One time resampling along with equal weights
gave worse results than the baseline system while
improvements in the BLEU score were observed
with LM and Condor weights for the IWSLT task,
as shown in table 1. Resampling many times al-
ways gave more stable results, as already shown
in figure 2 and as theoretically expected. For this
task, we resampled 15 times. The improvements
in the BLEU score are shown in table 2. Fur-
thermore, using the alignment scores resulted in
additional improvements in the BLEU score. For
the IWSLT task, we achieved and overall improve-
ment of 1.5 BLEU points on the development set
and 1.2 BLEU points on the test set as shown in
table 3
To validate our approach we further experi-
mented with the NIST evaluation task. Most of
the training data used in our experiments for the
NIST task is made available through the LDC. The
bitexts consist of texts from the GALE project1
(1.6M words), various news wire translations2
(8.0M words) on development data from pre-
vious years (1.6M words), LDC treebank data
(0.4M words) and the ISI extracted bitexts (43.7M
words). The official NIST06 evaluation data was
used as development set and the NIST08 evalua-
tion data was used as test set. The same procedure
was adapted for the NIST task as for the IWSLT
task. Results are shown in table 1 by using differ-
ent weights and one time resampling. Further im-
provements in the results are shown in table 2 with
the optimum number of resampling which is 10
for this task. Finally, results by weighting align-
ments along with weighting corpora are shown in
table 3. Our final system achieved an improve-
ment of 0.79 BLEU points on the development set
and 0.33 BLEU points on the test set. TER scores
are also shown on test set of our final system in
table 3. Note that these results are state-of-the-art
when compared to the official results of the 2008
NIST evaluation3.
The weights of the different corpora are shown
in table 4 for the IWSLT and NIST task. In both
cases, the weights optimized by CONDOR are
substantially different form those obtained when
1LDC2005E83, 2006E24, E34, E85 and E92
2LDC2003T07, 2004E72, T17, T18, 2005E46 and
2006E25.
3http://www.nist.gov/speech/tests/mt/
2008/
creating an interpolated LM on the source side of
the bitexts. In any case, the weights are clearly
non uniform, showing that our algorithm has fo-
cused on in-domain data. This can be nicely seen
for the NIST task. The Gale texts were explictely
created to contain in-domain news wire and WEB
texts and actually get a high weight despite their
small size, in comparison to the more general news
wire collection from LDC.
5 Conclusion and future work
We have proposed a new technique to adapt the
translation model by resampling the alignments,
giving a weight to each corpus and using the
alignment score as confidence measurement of
each aligned phrase pair. Our technique does not
change the phrase pairs that are extracted,4 but
only the corresponding probability distributions.
By these means we hope to adapt the translation
model in order to increase the weight of transla-
tions that are important to the task, and to down-
weight the phrase pairs which result from unreli-
able alignments.
We experimentally verified the new method on
the low-resource IWSLT and the resource-rich
NIST?08 tasks. We observed significant improve-
ment on both tasks over state-of-the-art baseline
systems. This weighting scheme is generic and
it can be applied to any language pair and target
domain. We made no assumptions on how the
phrases are extracted and it should be possible to
apply the same technique to other SMT systems
which rely on word-to-word alignments.
On the other hand, our method is computation-
ally expensive since the optimisation of the coef-
ficients requires the creation of a new phrase table
and the evaluation of the resulting system in the
tuning loop. Note however, that we run GIZA++
only once.
In future work, we will try to directly use the
weights of the corpora and the alignments in the
algorithm that extracts the phrase pairs and cal-
culates their probabilities. This would answer
the interesting question whether resampling itself
is needed or whether weighting the corpora and
alignments is the key to the observed improve-
ments in the BLEU score.
Finally, it is straight forward to consider more
feature functions when resampling the alignments.
This may be a way to integrate linguistic knowl-
4when also including the original alignments
397
IWSLT Task BTEC Dev1 Dev2 Dev3
# of Words 194K 60K 60K 60K
LM Coeffs 0.7233 0.1030 0.0743 0.0994
Condor Coeffs 0.6572 0.1058 0.1118 0.1253
NIST TASK Gale NewsWire TreeBank Dev ISI
# of words 1.6M 8.1M 0.4M 1.7M 43.7M
LM Coeffs 0.3215 0.1634 0.0323 0.1102 0.3726
Condor Coeffs 0.4278 0.1053 0.0489 0.1763 0.2417
Table 4: Weights of the different bitexts.
edge into the SMT system, e.g. giving low scores
to word alignments that are ?grammatically not
reasonable?.
Acknowledgments
This work has been partially funded by the Eu-
ropean Commission under the project Euromatrix
and by the Higher Education Commission(HEC)
Pakistan as Overseas scholarship. We are very
thankful to SYSTRAN who provided support for
the Arabic tokenization.
References
Frank Vanden Berghen and Hugues Bersini.
2005. CONDOR, a new parallel, constrained
extension of Powell?s UOBYQA algorithm:
Experimental results and comparison with the
DFO algorithm. Journal of Computational and
Applied Mathematics, 181:157?175, Septem-
ber.
Boxing Chen, Min Zhang, Aiti Aw, and
Haizhou Li. 2008. Exploiting n-best hypothe-
ses for SMT self- enhancement. In Association
for Computational Linguistics, pages 157?160.
Jorge Civera and Alfons Juan. 2007. Do-
main adaptation in statistical machine transla-
tion with mixture modelling. In Second Work-
shop on SMT, pages 177?180.
George Foster and Roland Kuhn. 2007.
Mixture-model adaptation for SMT. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, pages 128?135. Associa-
tion for Computational Linguistics.
Almut Silja Hildebrand, Matthias Eck, Stephan
Vogel, and Alex Waibel. 2005. Adaptation
of the translation model for statistical machine
translation based on information retrieval. In
EAMT, pages 133?142.
Philipp Koehn and et al 2007. Moses: Open
source toolkit for statistical machine transla-
tion. In Association for Computational Linguis-
tics, demonstration session., pages 224?227.
Philipp Koehn and Josh Schroeder. 2007. Ex-
periments in domain adaptation for statistical
machine translation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 224?227. Association for Computa-
tional Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and
Bing Zhang. 2009. Discriminative corpus
weight estimation for machine translation. In
Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing,
pages 708?717.
M.J.D. Powell. 1994. A direct search opti-
mization method that models the objective and
constraint functions by linar interpolation. In
In Advances in Optimization and Numerical
Analysis, Proceedings of the sixth Workshop
on Optimization and Numerical Analysis, Oax-
aca, Mexico, volume 275, pages 51?67. Kluwer
Academic Publishers.
Holger Schwenk and Jean Senellart. 2009.
Translation model adaptation for an Ara-
bic/French news translation system by lightly-
supervised training. In MT Summit.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical
machine translation. In IWSLT, pages 182?189.
Matthew Snover, Bonnie Dorr, and Richard
Schwartz. 2008. Language and translation
398
model adaptation using comparalble corpora.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 857?866.
Nicola Ueffing. 2006. Using monolingual sour-
cce language data to improve MT performance.
In IWSLT, pages 174?181.
Nicola Ueffing. 2007. Transductive learning for
statistical machine translation. In Association
for Computational Linguistics, pages 25?32.
Chong Ho Yu. 2003. Resampling methods:
Concepts, applications, and justification. In
Practical Assessment Research and Evaluation.
Bing Zhao, Matthias Ech, and Stephen Vogal.
2004. Language model adaptation for statistical
machine translation with structured query mod-
els. In Proceedings of the 20th international
conference on Computational Linguistics. As-
sociation for Computational Linguistics.
399
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 135?139,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
MANY improvements for WMT?11
Lo??c Barrault
LIUM, University of Le Mans
Le Mans, France.
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development op-
erated into MANY for the 2011 WMT
system combination evaluation campaign.
Hypotheses from French/English and En-
glish/French MT systems were combined
with a new version of MANY, an open
source system combination software based
on confusion networks decoding currently
developed at LIUM. MANY has been up-
dated in order to optimize decoder pa-
rameters with MERT, which proves to
find better weights. The system combi-
nation yielded significant improvements in
BLEU score when applied on system com-
bination data from two languages.
1 Introduction
This year, the LIUM computer science laboratory
participated in the French-English system combi-
nation task at WMT?11 evaluation campaign. The
system used for this task is MANY1 (Barrault,
2010), an open source system combination soft-
ware based on Confusion Networks (CN).
For this year evaluation, rather more technical
than scientific improvements have been added to
MANY. The tuning process has been improved
by using MERT (Och, 2003) as a replacement
of the numerical optimizer Condor (Berghen and
Bersini, 2005). The impact of such change is de-
tailed in section 3.
After the evaluation period, some experiments
have been performed on the English-French sys-
tem combination task. The results are presented
in the section 5. Before that, a quick description
of MANY, including recent developments, can be
found in section 2.
1MANY is available at the following address http://
www-lium.univ-lemans.fr/?barrault/MANY
2 System description
MANY is a system combination software (Bar-
rault, 2010) based on the decoding of a lattice
made of several Confusion Networks (CN). This
is a widespread approach in MT system combina-
tion (Rosti et al, 2007; Shen et al, 2008; Karakos
et al, 2008; Rosti et al, 2009). MANY can be
decomposed in two main modules. The first one
is the alignment module which actually is a modi-
fied version of TERp (Snover et al, 2009). Its role
is to incrementally align the hypotheses against a
backbone in order to create a confusion network.
Those confusion networks are then connected to-
gether to create a lattice. This module uses dif-
ferent costs (which corresponds to a match, an in-
sertion, a deletion, a substitution, a shift, a syn-
onym and a stem) to compute the best alignment
and incrementally build a confusion network. In
the case of confusion network, the match (substi-
tution, synonyms, and stems) costs are considered
when the word in the hypothesis matches (is a sub-
stitution, a synonyms or a stems of) at least one
word of the considered confusion sets in the CN.
System 0
System 1
TERp 
alignment
LM
output
1-best 
output
1-best 
output
TERp 
alignment
DECODEMerge
System M
1-best 
output
TERp 
alignment
{best hyponbest list
Lattice
CN
CN
CN
Figure 1: System combination based on confusion
network decoding.
The second module is the decoder. This decoder
is based on the token pass algorithm and it accepts
as input the lattice previously created. The proba-
bilities computed in the decoder can be expressed
as follow :
135
log(PW ) =
?
i
?i log
(
hi(t)
)
(1)
where t is the hypothesis, the ?i are the weights
of the feature functions hi. The following features
are considered for decoding:
? The language model probability: the proba-
bility given by a 4-gram language model.
? The word penalty: penalty depending on the
size (in words) of the hypothesis.
? The null-arc penalty: penalty depending on
the number of null-arcs crossed in the lattice
to obtain the hypothesis.
? System weights: each word receive a weight
corresponding to the sum of the weights of all
systems which proposed it.
3 Tuning
As mentioned before, MANY is made of two main
modules: the alignment module based on a modi-
fied version of TERp and the decoder. Considering
a maximum of 24 systems for this year evaluation,
33 parameters in total have to be optimized. By
default, TERp costs are set to 0.0 for match and
1.0 for everything else. These costs are not correct,
since a shift in that case will hardly be possible.
TERp costs are tuned with Condor (a numerical
optimizer based on Powell?s algorithm, (Berghen
and Bersini, 2005)). Decoder feature functions
weights are optimized with MERT (Och, 2003).
The 300-best list created at each MERT iteration
is appended to the n-best lists created at previous
iterations. This proves to be a more reliable tuning
as shown in the following experiments.
During experiments, data from WMT?09 eval-
uation campaign are used for testing the tuning
approach. news-dev2009a is used as development
set, and news-dev2009b as internal test, these cor-
pora are described in Table 1.
NAME #sent. #words #tok
news-dev2009a 1025 21583 24595
news-dev2009b 1026 21837 24940
Table 1: WMT?09 corpora : number of sentences,
words and tokens calculated on the reference.
For the sake of simplicity, the five best systems
(ranking given by score on dev) are considered
only. Baseline systems performances on dev and
test are presented in Table 2.
Corpus Sys0 Sys1 Sys2 Sys3 Sys4
Dev 18.20 17.83 20.14 21.06 17.72
Test 18.53 18.33 20.43 21.35 18.15
Table 2: Baseline systems performance on
WMT?09 data (%BLEU).
The 2-step tuning protocol applied on news-
dev2009a, when using MERT to optimize decoder
feature functions weights provides the set of pa-
rameters presented in Table 3.
Costs: Del Stem Syn Ins Sub Shift
0.87 0.91 0.94 0.90 0.98 1.21
Dec.: LM weight Word pen. Null pen.
0.056 0.146 0.042
Wghts.: Sys0 Sys1 Sys2 Sys3 Sys4
-0.03 -0.21 -0.23 -0.28 -0.02
Table 3: Parameters obtained with tuning decoder
parameters with MERT.
Results on development corpus of WMT?09
(used as test set) are presented in Table 4. We can
System Dev Test
Best single 21.06 21.35
MANY (2010) 22.08 22.28
MANY-2steps (2010) 21.94 22.09
MANY-2steps/MERT (2011) 23.05 23.07
Table 4: System Combination results on WMT?09
data (%BLEU-cased).
observe that 2-step tuning provides almost +0.9
BLEU point improvement on development corpus
which is well reflected on test set with a gain of
more than 0.8 BLEU. By using MERT, this im-
provement is increased to reach almost +2 BLEU
point on dev corpus and +1.7 BLEU on test.
There are two main reasons for this improve-
ment. The first one is the use of MERT which
make use of specific heuristics to better opti-
mize toward BLEU score. The second one is the
fully log-linear interpolation of features functions
scores operated into the decoder (previously, the
word and null penalties were applied linearly).
136
4 2011 evaluation campaign
A development corpus, newssyscombtune2011,
and a test set, newssyscombtest2011, described in
Table 5, were provided to participants.
NAME #sent. #words #tok
newssyscombtune2011 1003 23108 26248
newssyscombtest2011 2000 42719 48502
Table 5: Description of WMT?11 corpora.
Language model: The English target language
models has been trained on all monolingual data
provided for the translation tasks. In addition,
LDC?s Gigaword collection was used for both lan-
guages. Data corresponding to the development
and test periods were removed from the Gigaword
collections.
Sys. # BLEU TER Sys. # BLEU TER
Sys0 29.86 52.46 Sys11 27.23 53.48
Sys1 29.74 51.74 Sys12* 26.82 54.23
Sys2 29.73 52.90 Sys13 26.25 55.60
Sys3 29.58 52.73 Sys14* 26.13 55.65
Sys4* 29.39 52.91 Sys15 25.90 55.69
Sys5 28.89 53.74 Sys16 25.45 56.92
Sys6 28.53 53.27 Sys17 25.23 56.09
Sys7* 28.31 54.22 Sys18 23.63 60.25
Sys8* 28.08 54.47 Sys19 21.90 63.65
Sys9* 27.98 53.92 Sys20 21.77 60.78
Sys10 27.46 54.60 Sys21 20.97 64.00
Sys22 16.63 65.83
MANY-5sys 31.83 51.27
MANY-10sys 31.75 51.91
MANY-allsys 30.75 54.33
Table 6: Systems performance on newssyscomb-
tune2011 development data (%BLEU-cased). (*
indicate a contrastive run)
Choosing the right number of systems to com-
bine: Table 6 shows the performance of the in-
put systems (ordered by BLEU score computed on
newssyscombtune2011) and the result of 3 system
combination setups. The difference in these se-
tups only reside on the number of inputs to use for
combination (5, 10 and all system outputs). Notice
that the contrastive runs have not been used when
combining 5 and 10 systems. The motivation for
this is to benefit from the multi-site systems de-
velopment which more likely provide varied out-
puts (i.e. different ngrams and word choice). The
results show that combining 5 systems is slightly
better than 10, but give more than 1 BLEU point
improvement compared to combining all systems.
Still, the combination always provide an improve-
ment, which was not the case in last year evalua-
tion.
The results obtained by combining 5 and 10 sys-
tems are presented in Table 7.
Sys. # BLEU TER Sys. # BLEU TER
Sys0 29.43 52.01 Sys6 28.08 53.19
Sys1 29.15 51.30 Sys11 27.24 53.74
Sys2 28.87 52.82 Sys13 26.74 52.92
Sys3 28.82 52.57 Sys15 26.31 54.61
Sys5 28.08 53.19 Sys16 25.23 55.38
MANY (5sys) 30.74 51.17
MANY (10sys) 30.60 51.39
Table 7: Baseline systems performance on
WMT?11 syscomb test data (%BLEU-cased).
Optimizing MANY on newssyscombtune2011
corpus produced the parameter set presented in Ta-
ble 8. We can see that the weights of all system are
not proportional to the BLEU score obtained on
the development corpus. This suggest that a bet-
ter system selection could be found. This is even
more probable since the weight of system Sys2 is
positive (which imply a negative impact on each
word proposed by this system), which means that
when an hypothesis contains a word coming from
this system, then its score is decreased.
Costs: Del Stem Syn Ins Sub Shift
0.90 0.88 0.96 0.97 1.01 1.19
Dec.: LM weight Null pen. Len pen.
0.0204 0.26 0.005
Wghts.:Sys0 Sys1 Sys2 Sys3 Sys5
-0.16 -0.30 0.008 -0.16 -0.09
Table 8: Parameters obtained after tuning the sys-
tem parameter using 5 hypotheses.
Table 9 contains the BLEU scores computed be-
tween the outputs of the five systems used during
combination. An interesting observation is that the
system which receive the bigger weight is the one
which ?distance?2 against all other system outputs
2This ?distance? is expressed in terms of ngrams agree-
ment
137
Sys0 Sys1 Sys2 Sys3 Sys5 mean
Sys0 - 53.59 62.67 64.60 62.50 60.84
Sys1 53.51 - 54.19 52.42 51.69 52.95
Sys2 62.72 54.28 - 65.49 63.09 61.40
Sys3 64.63 52.51 65.47 - 61.35 60.99
Sys5 62.55 51.78 63.10 61.37 - 59.70
mean 60.85 53.04 61.36 60.97 59.66
Table 9: Cross-system BLEU scores computed
on WMT?11 French-English test corpus outputs
(%BLEU-cased).
is the highest, whereas the ?closest? system get the
smallest weight. This suggests that systems closer
to other systems tends to be less useful for sys-
tem combination. This is an interesting behaviour
which has to be explored deeper and validated on
other tasks and corpora.
5 MANY for french outputs
After the evaluation period, some experiments
have been conducted in order to combine french
outputs. The main difference lie in the fact that
linguistic resources are not easily or freely avail-
able for that kind of language. Therefore, instead
of using TERp with relax3 shift constraint, the
strict constraint was used (shifts occur only when
a match is found).
The available data are detailed in the Table 10.
NAME #sent. #words #tok
syscombtune 1003 24659 29171
syscombtest 2000 45372 53970
Table 10: Description of WMT?11 corpora for
system combination in french.
The results obtained are presented in Table 11.
The BLEU score increase by more than 0.8 point
but the TER score decrease by 0.58. The metric
targeted during tuning is BLEU, which can ex-
plain the improvement in that metric. When deal-
ing with english text, the only case where such be-
haviour is observed is when combining all systems
(see Table 6.
6 MANY technical news
Several improvements have been performed on
MANY. The decoder is now based on a fully log-
3Shifts can occur when a match, a stem, a synonym or a
paraphrase is found.
Corpus syscombtune2011 syscombtest2011
BLEU TER BLEU TER
Sys0 35.99 49.16 34.36 49.78
Sys1 32.99 51.90 30.73 52.52
Sys2 32.41 52.77 29.85 53.61
Sys3 32.40 51.26 30.48 52.20
Sys4 32.30 52.21 31.02 52.49
MANY 36.81 49.74 34.51 50.54
Table 11: Systems and combination performance
on WMT?11 french data (%BLEU-cased).
linear model (whereas before, the word and null
penalties were applied linearly). Using MERT to
tune the decoder parameters is therefore possible
and allows to reach bigger improvement compared
to using Condor. This is probably due to the fact
that MERT uses several heuristics useful for tun-
ing on BLEU score.
In order to facilitate the use of MANY, it has
been integrated in the Experiment Management
System, EMS - (Koehn, 2010). An experiment can
now be setup/modified/re-run easily by modifying
a single configuration file. The default behavior of
this framework is to perform 3 runs of MERT in
parallel (using torque) and take the best optimiza-
tion run. Apart from avoiding local maximum, the
procedure allows to see the variability of the opti-
mization process and report more realistic results
(for example, by taking the average).
7 Conclusion and future work
For WMT?11 system combination evaluation cam-
paign, several rather technical improvements have
been performed into MANY. By homogenizing
the log-linear model used by the decoder and uti-
lizing MERT for tuning, MANY achieves im-
provements of more than 2 BLEU points on
WMT?09 data and about 1.3 BLEU point on
newssyscombtest2011 relatively to the best single
system. Moreover, a dry-run operated on french
data shows a promising result with an improve-
ment of more than 0.8 BLEU points. This will be
further explored in the future.
MANY can benefit from various information.
At the moment, the decision taken by the decoder
mainly depends on a target language model. This
is clearly not enough to achieve greater perfor-
mances. The next issues which will be addressed
within the MANY framework is to estimate good
confidence measure to use in place of the systems
138
priors. These confidences measures have to be re-
lated to the system performances, but also to the
complementarity of the systems considered.
8 Acknowledgement
This work has been partially funded by the Eu-
ropean Union under the EuroMatrix Plus project
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720)
References
[Barrault, 2010] Barrault, L. (2010). MANY :
Open source machine translation system com-
bination. Prague Bulletin of Mathematical Lin-
guistics, Special Issue on Open Source Tools for
Machine Translation, 93:147?155.
[Berghen and Bersini, 2005] Berghen, F. V. and
Bersini, H. (2005). CONDOR, a new parallel,
constrained extension of Powell?s UOBYQA
algorithm: Experimental results and compari-
son with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?
175.
[Karakos et al, 2008] Karakos, D., Eisner, J.,
Khudanpur, S., and Dreyer, M. (2008). Ma-
chine translation system combination using
ITG-based alignments. In 46th Annual Meeting
of the Association for Computational Linguis-
tics: Human Language Technologies., pages
81?84, Columbus, Ohio, USA.
[Koehn, 2010] Koehn, P. (2010). An experimental
management system. The Prague Bulletin of
Mathematical Linguistics, 94:87?96.
[Och, 2003] Och, F. (2003). Minimum error rate
training in statistical machine translation. In
ACL, Sapporo, Japan.
[Rosti et al, 2007] Rosti, A.-V., Matsoukas, S.,
and Schwartz, R. (2007). Improved word-level
system combination for machine translation.
In Association for Computational Linguistics,
pages 312?319.
[Rosti et al, 2009] Rosti, A.-V., Zhang, B., Mat-
soukas, S., , and Schwartz, R. (2009). In-
cremental hypothesis alignment with flexi-
ble matching for building confusion networks:
BBN system description for WMT09 system
combination task. In EACL/WMT, pages 61?
65.
[Shen et al, 2008] Shen, W., Delaney, B., An-
derson, T., and Slyh, R. (2008). The MIT-
LL/AFRL IWSLT-2008 MT System. In Inter-
national Workshop on Spoken Language Trans-
lation, Hawaii, U.S.A.
[Snover et al, 2009] Snover, M., Madnani, N.,
Dorr, B., and Schwartz, R. (2009). TER-Plus:
Paraphrase, semantic, and alignment enhance-
ments to translation edit rate. Machine Trans-
lation Journal.
139
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 464?469,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2011
WMT shared task evaluation. Our main sys-
tems were standard phrase-based statistical
systems based on the Moses decoder, trained
on the provided data only, but we also per-
formed initial experiments with hierarchical
systems. Additional, new features this year in-
clude improved translation model adaptation
using monolingual data, a continuous space
language model and the treatment of unknown
words.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2011 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Lambert et
al., 2010) are as follows: use of more training data
as provided by the organizers, improved translation
model adaptation by unsupervised training, a con-
tinuous space language model for the translation
into French, some attempts to automatically induce
translations of unknown words and first experiments
with hierarchical systems. These different points are
described in the rest of the paper, together with a
summary of the experimental results showing the
impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations were then used
directly with the source texts to create additional bi-
texts. In a second stage, these additional bilingual
data were incorporated into the system (see Sec-
tion 5 and Tables 4 and 5).
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
6) were used. We also took as training data a sub-
set of the French?English Gigaword (109) corpus.
We applied the same filters as last year to select this
subset. The first one is a lexical filter based on the
IBM model 1 cost (Brown et al, 1993) of each side
of a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter was
trained on a corpus composed of Eparl, NC, and UN
data. The other filter is an n-gram language model
(LM) cost of the target sentence (see Section 3), nor-
malised with respect to its length. This filter was
trained with all monolingual resources available ex-
cept the 109 data. We generated two subsets, both
by selecting sentence pairs with a lexical cost infe-
rior to 4, and an LM cost respectively inferior to 2.3
(1091, 115 million English words) and 2.6 (10
9
2, 232
million English words).
464
2.2 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the task do-
main.
First, we generated automatic translations of the
provided monolingual News corpus and selected the
sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitext contain no new translations, since
all words of the translation output come from the
translation model, but it contains new combinations
(phrases) of known words, and reinforces the prob-
ability of some phrase pairs (Schwenk, 2008). This
year, we improved this method in the following way.
In the original approach, the automatic translations
are added to the human translated bitexts and a com-
plete new system is build, including time consuming
word alignment with GIZA++. For WMT?11, we
directly used the word-to-word alignments produced
by the decoder at the output instead of GIZA?s align-
ments. This speeds-up the procedure and yields the
same results in our experiments. A detailed compar-
ison is given in (Lambert et al, 2011).
Second, as in last year?s evaluation, we automat-
ically extracted and aligned parallel sentences from
comparable in-domain corpora. We used the AFP
and APW news texts since there are available in the
French and English LDC Gigaword corpora. The
general architecture of our parallel sentence extrac-
tion system is described in detail by Abdul-Rauf and
Schwenk (2009). We first translated 91M words
from French into English using our first stage SMT
system. These English sentences were then used to
search for translations in the English AFP and APW
texts of the Gigaword corpus using information re-
trieval techniques. The Lemur toolkit (Ogilvie and
Callan, 2001) was used for this purpose. Search
was limited to a window of ?5 days of the date of
the French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER below
75% were kept. Sentences with a large length differ-
ence (French versus English) or containing a large
fraction of numbers were also discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the develop-
ment and test periods were removed from the Giga-
word collections.
2.4 Development data
All development was done on newstest2009, and
newstest2010 was used as internal test set. The de-
fault Moses tokenization was used. However, we
added abbreviations for the French tokenizer. All
our models are case sensitive and include punctua-
tion. The BLEU scores reported in this paper were
calculated with the tool multi-bleu.perl and are case
sensitive.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Our main system is a phrase-based system
(Koehn et al, 2003; Och and Ney, 2003), but we
have also performed some experiments with a hier-
archical system (Chiang, 2007). Both use a log lin-
ear framework in order to introduce several models
explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och
and Ney, 2002). The phrase-based system uses four-
teen features functions, namely phrase and lexical
translation probabilities in both directions, seven
features for the lexicalized distortion model, a word
and a phrase penalty and a target language model
(LM). The hierarchical system uses only 8 features:
a LM weight, a word penalty and six weights for the
translation model.
Both systems are based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
465
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases, lexical reorderings or hierarchical rules
are extracted using the default settings of the Moses
toolkit. The parameters of Moses were tuned on
newstest2009, using the ?new? MERT tool. We re-
peated the training process three times, each with a
different seed value for the optimisation algorithm.
In this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build on
each data source with the SRI LM toolkit (Stolcke,
2002) and then linearly interpolated, optimizing the
coefficients with an EM procedure. The perplexities
of these LMs were 99.4 for French and 129.7 for
English. In addition, we build a 5-gram continuous
space language model for French (Schwenk, 2007).
This model was trained on all the available French
texts using a resampling technique. The continu-
ous space language model is interpolated with the
4-gram back-off model and used to rescore n-best
lists. This reduces the perplexity by about 8% rela-
tive.
4 Treatment of unknown words
Finally, we propose a method to actually add new
translations to the system inspired from (Habash,
2008). For this, we propose to identity unknown
words and propose possible translations.
Moses has two options when encountering an un-
known word in the source language: keep it as it is
or drop it. The first option may be a good choice
for languages that use the same writing system since
the unknown word may be a proper name. The sec-
ond option is usually used when translating between
language based on different scripts, e.g. translating
1The source is available at http://www.cs.cmu.edu/
?qing/
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
from Arabic to English. Alternatively, we propose to
infer automatically possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case, we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
target segment works are finis
stemmed word found fini
translations found finished, ended
segment proposed works are finished
works are ended
segment kept works are finished
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
comparing the source and the target segment in order
to detect identical words. Once the unknown word
is selected, we are looking for its stemmed form in
the dictionary and propose some translations for the
unknown word based on lexical score of the phrase
table (see Table 2 for some examples). The snowball
466
Bitext #Fr Words PT size newstest2009 newstest2010
(M) (M) BLEU BLEU TER METEOR
Eparl+NC 56 7.1 26.74 27.36 (0.19) 55.11 (0.14) 60.13 (0.05)
Eparl+NC+1091 186 16.3 27.96 28.20 (0.04) 54.46 (0.10) 60.88 (0.05)
Eparl+NC+1092 323 25.4 28.20 28.57 (0.10) 54.12 (0.13) 61.20 (0.05)
Eparl+NC+news 140 8.4 27.31 28.41 (0.13) 54.15 (0.14) 61.13 (0.04)
Eparl+NC+1092+news 406 25.5 27.93 28.70 (0.24) 54.12 (0.16) 61.30 (0.20)
Eparl+NC+1092+IR 351 25.3 28.07 28.51 (0.18) 54.07 (0.06) 61.18 (0.07)
Eparl+NC+1092+news+IR 435 26.1 27.99 28.93 (0.02) 53.84 (0.07) 61.46 (0.07)
+larger beam+pruned PT 435 8.2 28.44 29.05 (0.14) 53.74 (0.16) 61.68 (0.09)
Table 4: French?English results: number of French words (in million), number of entries in the filtered phrase-table
(in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different
systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3
values (see Section 3)
corpus newstest2010 subtest2010
number of sentences 2489 109
number of words 70522 3586
number of UNK detected 118 118
nbr of sentences containing UNK 109 109
BLEU Score without UNK process 29.43 24.31
BLEU Score with UNK process 29.43 24.33
TER Score without UNK process 53.08 58.54
TER Score with UNK process 53.08 58.59
Table 3: Statistics of the unknown word (UNK) process-
ing algorithm on our internal test (newstest2010) and its
sub-part containing only the processed sentences (sub-
test2010).
stemmer2 was used. Then the different hypothesis
are evaluated with the target language model.
We processed the produced translations with this
method. It can happen that some words are transla-
tions of themselves, e.g. the French word ?duel? can
be translated by the English word ?duel?. If theses
words are present into the extracted dictionary, we
keep them. If we do not find any translation in our
dictionary, we keep the translation. By these means
we hope to keep named entities.
Several statistics made on our internal test (new-
stest2010) are shown in Table 3. Its shows that the
influence of the detected unknown words is minimal.
Only 0.16% of the words in the corpus are actually
unknown. However, the main goal of this process
is to increase the human readability and usefulness
without degrading automatic metrics. We also ex-
pect a larger impact in other tasks for which we have
2http://snowball.tartarus.org/
smaller amounts of parallel training data. In future
versions of this detection process, we will try to de-
tect unknown words before the translation process
and propose alternatives hypothesis to the Moses de-
coder.
5 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 4 and 5, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between two
average scores is less than the sum of the standard
deviations, we can say that this difference is not sig-
nificant. The reverse is not true. Note that most of
the improvements shown in the tables are small and
not significant. However many of the gains are cu-
mulative and the sum of several small gains makes a
significant difference.
Baseline French?English System
The first section of Table 4 shows results of the de-
velopment of the baseline SMT system, used to gen-
erate automatic translations.
Although no French translations were generated,
we did similar experiments in the English?French
direction (first section of Table 5).
467
Bitext #En Words newstest2009 newstest2010
(M) BLEU BLEU TER
Eparl+NC 52 26.20 28.06 (0.22) 56.85 (0.08)
Eparl+NC+1091 167 26.84 29.08 (0.12) 55.83 (0.14)
Eparl+NC+1092 284 26.95 29.29 (0.03) 55.77 (0.19)
Eparl+NC+1092+news 299 27.34 29.56 (0.14) 55.44 (0.18)
Eparl+NC+1092+IR 311 27.14 29.43 (0.12) 55.48 (0.06)
Eparl+NC+1092+news+IR 371 27.32 29.73 (0.21) 55.16 (0.20)
+rescoring with CSLM 371 27.46 30.04 54.79
Table 5: English?French results: number of English words (in million) and BLEU scores in the development (new-
stest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number
in parentheses are the average and standard deviation over 3 values (see Section 3.)
In both cases the best system is the one trained
on the Europarl, News-commentary and 1092 cor-
pora. This system was used to generate the auto-
matic translations. We did not observe any gain
when adding the United Nations data, so we dis-
carded this data.
Impact of the Additional Bitexts
With the baseline French?English SMT system (see
above), we translated the French News corpus to
generate an additional bitext (News). We also trans-
lated some parts of the French LDC Gigaword cor-
pus, to serve as queries to our IR system (see section
2.2). The resulting additional bitext is referred to as
IR. The second section of Tables 4 and 5 summarize
the system development including the additional bi-
texts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate the
automatic translations, but with less than half of
the data. Adding the News corpus to a larger cor-
pus, such as Eparl+NC+1092, has less impact but
still yields some improvement: 0.1 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. This
effect is studied in detail in a separate paper (Lam-
bert et al, 2011). With the IR additional bitext added
to Eparl+NC+1092, we observe no improvement in
French to English, and a very small improvement
in English to French. However, added to the base-
line system (Eparl+NC+1092) adapted with the News
data, the IR additional bitexts yield a small (0.2
BLEU) improvement in both translation directions.
Final System
In both translation directions our best system was the
one trained on Eparl+NC+1092+News+IR. We fur-
ther achieved small improvements by pruning the
phrase-table and by increasing the beam size. To
prune the phrase-table, we used the ?sigtest-filter?
available in Moses (Johnson et al, 2007), more pre-
cisely the ??  filter3.
We also build hierarchical systems on the various
human translated corpora, using up to 323M words
(corpora Eparl+NC+1092). The systems yielded sim-
ilar results than the phrase-based approach, but re-
quired much more computational resources, in par-
ticular large amounts of main memory to perform
the translations. Running the decoder was actually
only possible with binarized rule-tables. Therefore,
the hierarchical system was not used in the evalua-
tion system.
3The p-value of two-by-two contingency tables (describing
the degree of association between a source and a target phrase)
is calculated with Fisher exact test. This probability is inter-
preted as the probability of observing by chance an association
that is at least as strong as the given one, and hence as its sig-
nificance. An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and each of the
component phrases occurs exactly once in its side of the paral-
lel corpus (1-1-1 phrase pairs). In this case the negative log of
the p-value is ? = logN (N is number of sentence pairs in the
corpus). ? ?  is the largest threshold that results in all of the
1-1-1 phrase pairs being included.
468
6 Conclusions and Further Work
We presented the development of our statistical ma-
chine translation systems for the French?English
and English?French 2011 WMT shared task. In the
official evaluation the English?French system was
ranked first according to the BLEU score and the
French?English system second.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 121?126, Uppsala, Sweden, July.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
model adaptation using monolingual data. In Sixth
Workshop on SMT, page this volume.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
469
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 369?373,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2012
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk and Lo??c Barrault
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2012
WMT shared task evaluation. We developed
phrase-based systems based on the Moses de-
coder, trained on the provided data only. Ad-
ditionally, new features this year included im-
proved language and translation model adap-
tation using the cross-entropy score for the
corpus selection.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2012 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences with
respect to previous year?s system (Schwenk et al,
2011) are as follows: (i) use of more training data as
provided by the organizers and (ii) better selection
of the monolingual and parallel data according to
the domain, using the cross-entropy difference with
respect to in-domain and out-of-domain language
models (Moore and Lewis, 2010). We kept some
previous features: the improvement of the transla-
tion model adaptation by unsupervised training, a
parallel corpus retrieved by Information Retrieval
(IR) techniques and finally, the rescoring with a con-
tinuous space target language model for the trans-
lation into French. These different points are de-
scribed in the rest of the paper, together with a sum-
mary of the experimental results showing the impact
of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
7) were used. We also took as training data a subset
of the French?English Gigaword (109) corpus. This
year we changed the filters applied to select this sub-
set (see Sect. 2.4). We also included in the training
data the test sets from previous shared tasks, that we
called the ntsXX corpus and which was composed
of newstest2008, newstest2009, newssyscomb2009.
2.2 Development data
Development was initially done on newstest2010,
and newstest2011 was used as internal test set (Sec-
tion 3.1). The development and internal test sets
were then (Section 4) switched (tuning was done
on newstest2011 and internal evaluation on new-
stest2010). The default Moses tokenization was
used. However, we added abbreviations for the
French tokenizer. All our models are case sensitive
and include punctuation. The BLEU scores reported
in this paper were calculated with the mteval-v13
tool and are case insensitive.
2.3 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the domain.
369
First, we generated automatic translations of the
provided monolingual News corpus in French and
English, for years 2009, 2010 and 2011, and selected
the sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitexts contain no new translations, since
all words of the translation output come from the
translation model, but they contain new combina-
tions (phrases) of known words, and reinforce the
probability of some phrase pairs (Schwenk, 2008).
Like last year, we directly used the word-to-word
alignments produced by the decoder at the output
instead of GIZA?s alignments. This speeds-up the
procedure and yields the same results in our experi-
ments. A detailed comparison is given in (Lambert
et al, 2011).
Second, as in last year?s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. We used the
AFP (Agence France Presse) and APW (Associated
Press Worldstream Service) news texts since there
are available in the French and English LDC Giga-
word corpora. The general architecture of our par-
allel sentence extraction system is described in de-
tail by Abdul-Rauf and Schwenk (2009). We first
translated 91M words from French into English us-
ing our first stage SMT system. These English sen-
tences were then used to search for translations in
the English AFP and APW texts of the Gigaword
corpus using information retrieval techniques. The
Lemur toolkit (Ogilvie and Callan, 2001) was used
for this purpose. Search was limited to a window of
?5 days of the date of the French news text. The re-
trieved candidate sentences were then filtered using
the Translation Error Rate (TER) with respect to the
automatic translations. In this study, sentences with
a TER below 75% were kept. Sentences containing
a large fraction of numbers were discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.4 Domain-based Data selection
Before training the target language models, a text se-
lection has been made using the cross-entropy differ-
ence method (Moore and Lewis, 2010). This tech-
nique works by computing the difference between
two cross-entropy values.
We first score an out-of-domain corpus against
a language model trained on a set of in-domain
data and compute the cross-entropy for each sen-
tence. Then, we score the same out-of-domain cor-
pus against a language model trained on a random
sample of itself, with a size roughly equal to the in-
domain corpus. From this point, the difference be-
tween in-domain cross-entropy and out-of-domain
cross-entropy is computed for each sentence, and
these sentences are sorted regarding this score.
By estimating and minimizing on a development
set the perplexity of several percentages of the sorted
out-of-domain corpus, we can then estimate the the-
oretical best point of data size for this specific cor-
pus. According the original paper and given our re-
sults, this leads to better selection than the simple
perplexity sorting (Gao et al, 2002). This way, we
can be assured to discard the vast majority of noise
in the corpora and to select data well-related to the
task.
In this task, the French and English target lan-
guage models were trained on data selected from all
provided monolingual corpora. In addition, LDC?s
Gigaword collection was used for both languages.
Data corresponding to the development and test pe-
riods were removed from the Gigaword collections.
We had time to apply the domain-based data selec-
tion only for French. Thus all data were used for
English.
We used this method to filter the French?English
109 parallel corpus as well, based on the differ-
ence between in-domain cross-entropy and out-of-
domain cross-entropy calculated for each sentence
of the English side of the corpus. We kept 49 mil-
lion words (in the English side) to train our models,
called 109f .
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sentence
f . We have build phrase-based systems (Koehn et
al., 2003; Och and Ney, 2003), using the standard
log linear framework in order to introduce several
models explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
370
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och,
2003). The phrase-based system uses fourteen fea-
tures functions, namely phrase and lexical transla-
tion probabilities in both directions, seven features
for the lexicalized distortion model, a word and a
phrase penalty and a target language model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and is constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases and lexical reorderings are extracted us-
ing the default settings of the Moses toolkit. The
parameters of Moses were tuned using the MERT
tool. We repeated the training process three times,
each with a different seed value for the optimisation
algorithm. In this way we have a rough idea of the
error introduced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build
on each data source with the SRI LM toolkit (Stol-
cke, 2002) and then linearly interpolated, optimizing
the coefficients with an EM procedure. The perplex-
ities of these LMs on newstest2011 were 119.1 for
French and 174.8 for English. In addition, we build a
5-gram continuous space language model for French
(Schwenk, 2007). These models were trained on
all the available texts using a resampling technique.
The continuous space language model is interpo-
lated with the 4-gram back-off model and used to
rescore n-best lists. This reduces the perplexity by
about 13% relative.
3.1 Number translation
We have also performed some experiments with
number translation. English and French do not use
1The source is available at http://www.cs.cmu.edu/
?qing/
the same conventions for integer and decimal num-
bers. For example, the English decimal number 0.99
is translated in French by 0,99. In the same way,
the English integer 32,000 is translated in French by
32 000. It should be possible to perform these mod-
ifications by rules.
In this study, we first replaced the numbers by a
tag @@NUM for integer and @@DEC for decimal num-
bers. Integers in the range 1 to 31 were not replaced
since they appear in dates. Then, we created the tar-
get language model using the tagged corpora. Ta-
ble 1 shows results of experiments performed with
and without rule-based number translation.
Corpus NT BLEU TER
NC no 26.57 (0.07) 58.13 (0.06)
NC yes 26.84 (0.15) 57.71 (0.34)
Eparl+NC no 29.28 (0.11) 55.28 (0.13)
Eparl+NC yes 29.26 (0.10) 55.44 (0.29)
Table 1: Results of the study on number translation (NT)
from English to French
We did observe small gains in the translation
quality when only the news-commentary bitexts are
used, but there were no differences when more train-
ing data is available. Due to time constraints, this
procedure was not used in the submitted system.
4 Results and Discussion
The results of our SMT systems are summarized in
Table 2. The MT metric scores for the development
set are the average of three optimisations performed
with different seeds (see Section 3). For the test set,
they are the average of four values: the three val-
ues corresponding to these different optimisations,
plus a fourth value obtained by taking as weight for
each model, the average of the weights obtained in
the three optimisations (Cettolo et al, 2011). The
numbers in parentheses are the standard deviation of
these three or four values. The standard deviation
gives a lower bound of the significance of the differ-
ence between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is not
significant. The reverse is not true.
The results of Table 2 show that adding several
adapted corpora (the filtered 109 corpus, the syn-
371
Bitext #Source newstest2011 newstest2010
Words (M) BLEU TER BLEU TER
Translation : En?Fr
Eparl+NC 57 30.91 (0.05) 53.61 (0.12) 28.45 (0.08) 56.29 (0.20)
Eparl+NC+ntsXX 58 31.12 (0.08) 53.67 (0.08) 28.49 (0.04) 56.45 (0.12)
Eparl+NC+ntsXX+109f 107 31.67 (0.06) 53.29 (0.03) 29.38 (0.12) 55.45 (0.15)
Eparl+NC+ntsXX+109f+IR 133 32.41 (0.02) 52.20 (0.02) 29.48 (0.11) 55.33 (0.20)
Eparl+NC+ntsXX+109f+news+IR 162 32.26 (0.04) 52.24 (0.12) 29.79 (0.12) 55.04 (0.20)
Translation : Fr?En
Eparl+NC 64 29.59 (0.12) 51.86 (0.06) 28.12 (0.05) 53.19 (0.06)
Eparl+NC+ntsXX 64 29.59 (0.04) 51.89 (0.14) 28.32 (0.08) 53.22 (0.08)
Eparl+NC+ntsXX+109f 120 30.69 (0.06) 50.77 (0.04) 28.95 (0.14) 52.62 (0.14)
Eparl+NC+ntsXX+109f+IR 149 30.56 (0.02) 50.94 (0.15) 28.67 (0.11) 52.78 (0.06)
Eparl+NC+ntsXX+109f+news+IR 179 30.85 (0.07) 50.72 (0.03) 28.94 (0.05) 52.57 (0.02)
Table 2: English?French and French?English results: number of source words (in million) and scores on the develop-
ment (newstest2011) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and
the number in parentheses are the average and standard deviation over 3 or 4 values when available (see Section 4.)
thetic corpus and the corpus retrieved via IR meth-
ods) to the Eparl+NC+ntsXX baseline, a gain of 1.1
BLEU points and 1.4 TER points was achieved for
the English?French system.
On the other hand, adding the bitexts extracted
from the comparable corpus (IR) does actually hurt
the performance of the French?English system: the
BLEU score decreases from 28.95 to 28.67 on our
internal test set. During the evaluation period, we
added all the corpora at once and we observed this
only in our analysis after the evaluation.
In both translation directions our
best system was the one trained on
Eparl+NC+ntsXX+109f+News+IR. Finally, we
applied a continuous space language model for the
system translating into French.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.
2011. Methods for smoothing the optimizer instability
in SMT. In Proc. of Machine Translation Summit XIII,
Xiamen, China.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statistical
language modeling for chinese. In ACM Transactions
on Asian Language Information Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
372
model adaptation using monolingual data. In Sixth
Workshop on SMT, pages 284?293.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the ACL 2010 Conference Short Papers.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. Lium?s smt machine trans-
lation systems for WMT 2011. In Proceedings of
the Sixth Workshop on Statistical Machine Translation,
pages 464?469, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
373
