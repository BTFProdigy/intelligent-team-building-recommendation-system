Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 61?72,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Log-Linear Model for Unsupervised Text Normalization
Yi Yang
School of Interactive Computing
Georgia Institute of Technology
yiyang@gatech.edu
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Abstract
We present a unified unsupervised statistical
model for text normalization. The relation-
ship between standard and non-standard to-
kens is characterized by a log-linear model,
permitting arbitrary features. The weights
of these features are trained in a maximum-
likelihood framework, employing a novel se-
quential Monte Carlo training algorithm to
overcome the large label space, which would
be impractical for traditional dynamic pro-
gramming solutions. This model is im-
plemented in a normalization system called
UNLOL, which achieves the best known re-
sults on two normalization datasets, outper-
forming more complex systems. We use the
output of UNLOL to automatically normalize
a large corpus of social media text, revealing a
set of coherent orthographic styles that under-
lie online language variation.
1 Introduction
Social media language can differ substantially from
other written text. Many of the attempts to character-
ize and overcome this variation have focused on nor-
malization: transforming social media language into
text that better matches standard datasets (Sproat et
al., 2001; Liu et al, 2011). Because there is lit-
tle available training data, and because social me-
dia language changes rapidly (Eisenstein, 2013b),
fully supervised training is generally not considered
appropriate for this task. However, due to the ex-
tremely high-dimensional output space ? arbitrary
sequences of words across the vocabulary ? it is
a very challenging problem for unsupervised learn-
ing. Perhaps it is for these reasons that the most suc-
cessful systems are pipeline architectures that cob-
ble together a diverse array of techniques and re-
sources, including statistical language models, de-
pendency parsers, string edit distances, off-the-shelf
spellcheckers, and curated slang dictionaries (Liu et
al., 2011; Han and Baldwin, 2011; Han et al, 2013).
We propose a different approach, performing nor-
malization in a maximum-likelihood framework.
There are two main sources of information to be
exploited: local context, and surface similarity be-
tween the observed strings and normalization can-
didates. We treat the local context using standard
language modeling techniques; we treat string simi-
larity with a log-linear model that includes features
for both surface similarity and word-word pairs.
Because labeled examples of normalized text
are not available, this model cannot be trained
in the standard supervised fashion. Nor can we
apply dynamic programming techniques for unsu-
pervised training of locally-normalized conditional
models (Berg-Kirkpatrick et al, 2010), as their com-
plexity is quadratic in the size of label space; in
normalization, the label space is the vocabulary it-
self, with at least 104 elements. Instead, we present
a new training approach using Monte Carlo tech-
niques to compute an approximate gradient on the
feature weights. This training method may be appli-
cable in other unsupervised learning problems with
a large label space.
This model is implemented in a normalization
system called UNLOL (unsupervised normalization
in a LOg-Linear model). It is a lightweight proba-
61
bilistic approach, relying only on a language model
for the target domain; it can be adapted to new
corpora text or new domains easily and quickly.
Our evaluations show that UNLOL outperforms the
state-of-the-art on standard normalization datasets.
In addition, we demonstrate the linguistic insights
that can be obtained from normalization, using
UNLOL to identify classes of orthographic transfor-
mations that form coherent linguistic styles.
2 Background
The text normalization task was introduced
by Sproat et al (2001), and attained popularity in
the context of SMS messages (Choudhury et al,
2007b). It has become still more salient in the era of
widespread social media, particularly Twitter. Han
and Baldwin (2011) formally define a normalization
task for Twitter, focusing on normalizations between
single tokens, and excluding multi-word tokens like
lol (laugh out loud). The normalization task has
been criticized by Eisenstein (2013b), who argues
that it strips away important social meanings. In
recent work, normalization has been shown to yield
improvements for part-of-speech tagging (Han
et al, 2013), parsing (Zhang et al, 2013), and
machine translation (Hassan and Menezes, 2013).
As we will show in Section 7, accurate automated
normalization can also improve our understanding
of the nature of social media language.
Supervised methods Early work on normaliza-
tion focused on labeled SMS datasets, using ap-
proaches such as noisy-channel modeling (Choud-
hury et al, 2007a) and machine translation (Aw
et al, 2006), as well as hybrid combinations of
spelling correction and speech recognition (Kobus
et al, 2008; Beaufort et al, 2010). This work
sought to balance language models (favoring words
that fit in context) with transformation models (fa-
voring words that are similar to the observed text).
Our approach can also be seen as a noisy channel
model, but unlike this prior work, no labeled data is
required.
Unsupervised methods Cook and Stevenson
(2009) manually identify several word formation
types within a noisy channel framework. They
parametrize each formation type with a small num-
ber of scalar values, so that all legal transformations
of a given type are equally likely. The scalar pa-
rameters are then estimated using expectation max-
imization. This work stands apart from most of the
other unsupervised models, which are pipelines.
Contractor et al (2010) use string edit distance
to identify closely-related candidate orthographic
forms and then decode the message using a language
model. Gouws et al (2011) refine this approach
by mining an ?exception dictionary? of strongly-
associated word pairs such as you/u. Like Con-
tractor et al (2010), we apply string edit distance,
and like Gouws et al (2011), we capture strongly
related word pairs. However, rather than applying
these properties as filtering steps in a pipeline, we
add them as features in a unified log-linear model.
Recent approaches have sought to improve accu-
racy by bringing more external resources and com-
plex architectures to bear. Han and Baldwin (2011)
begin with a set of string similarity metrics, and then
apply dependency parsing to identify contextually-
similar words. Liu et al (2011) extract noisy train-
ing pairs from the search snippets that result from
carefully designed queries to Google, and then train
a conditional random field (Lafferty et al, 2001) to
estimate a character-based translation model. They
later extend this work by adding a model of vi-
sual priming, an off-the-shelf spell-checker, and lo-
cal context (Liu et al, 2012a). Hassan and Menezes
(2013) use a random walk framework to capture
contextual similarity, which they then interpolate
with an edit distance metric. Rather than seek-
ing additional external resources or designing more
complex metrics of context and similarity, we pro-
pose a unified statistical model, which learns feature
weights in a maximum-likelihood framework.
3 Approach
Our approach is motivated by the following criteria:
? Unsupervised. We want to be able to train
a model without labeled data. At present, la-
beled data for Twitter normalization is avail-
able only in small quantities. Moreover, as
social media language is undergoing rapid
change (Eisenstein, 2013b), labeled datasets
may become stale and increasingly ill-suited to
new spellings and words.
62
? Low-resource. Other unsupervised ap-
proaches take advantage of resources such as
slang dictionaries and spell checkers (Han and
Baldwin, 2011; Liu et al, 2011). Resources
that characterize the current state of internet
language risk becoming outdated; in this paper
we investigate whether high-quality normaliza-
tion is possible without any such resources.
? Featurized. The relationship between any pair
of words can be characterized in a number of
different ways, ranging from simple character-
level rules (e.g., going/goin) to larger substi-
tutions (e.g., someone/sum1), and even to pat-
terns that are lexically restricted (e.g., you/u,
to/2). For these reasons, we seek a model that
permits many overlapping features to describe
candidate word pairs. These features may in-
clude simple string edit distance metrics, as
well as lexical features that memorize specific
pairs of standard and nonstandard words.
? Context-driven. Learning potentially arbitrary
word-to-word transformations without supervi-
sion would be impossible without the strong
additional cue of local context. For example,
in the phrase
give me suttin to believe in,
even a reader who has never before seen the
word suttin may recognize it as a phonetic
transcription of something. The relatively high
string edit distance is overcome by the strong
contextual preference for the word something
over orthographically closer alternatives such
as button or suiting. We can apply an arbi-
trary target language model, leveraging large
amounts of unlabeled data and catering to the
desired linguistic characteristics of the normal-
ized content.
? Holistic. While several prior approaches ?
such as normalization dictionaries ? operate at
the token level, our approach reasons over the
scope of the entire message. The necessity for
such holistic, joint inference and learning can
be seen by changing the example above to:
gimme suttin 2 beleive innnn.
None of these tokens are standard (except 2,
which appears in a nonstandard sense here), so
without joint inference, it would not be possi-
ble to use context to help normalize suttin.
Only by jointly reasoning over the entire mes-
sage can we obtain the correct normalization.
These desiderata point towards a featurized se-
quence model, which must be trained without la-
beled examples. While there is prior work on train-
ing sequence models without supervision (Smith
and Eisner, 2005; Berg-Kirkpatrick et al, 2010),
there is an additional complication not faced by
models for tasks such as part-of-speech tagging
and named entity recognition: the potential label
space of standard words is large, on the order of
at least 104. Naive application of Viterbi decod-
ing ? which is a component of training for both
Contrastive Estimation (Smith and Eisner, 2005)
and the locally-normalized sequence labeling model
of Berg-Kirkpatrick et al (2010) ? will be stymied
by Viterbi?s quadratic complexity in the dimension
of the label space. While various pruning heuris-
tics may be applied, we instead look to Sequen-
tial Monte Carlo (SMC), a randomized algorithm
which approximates the necessary feature expecta-
tions through weighted samples.
4 Model
Given a set of source-language sentences S =
{s1, s2, . . .} (e.g., Tweets), our goal is to trans-
duce them into target-language sentences T =
{t1, t2, . . .} (standard English). We are given a tar-
get language model P (t), which can be estimated
from some large set of unlabeled target-language
sentences. We denote the vocabularies of source lan-
guage and target language as ?S and ?T respectively.
We define a log-linear model that scores source
and target strings, with the form
P (s|t; ?) ? exp
(
?Tf(s, t)
)
. (1)
The desired conditional probability P (t|s) can be
obtained by combining this model with the target
language model, P (t|s) ? P (s|t; ?)P (t). Since no
labeled data is available, the parameters ? must be
estimated by maximizing the log-likelihood of the
source-language data. We define the log-likelihood
63
`?(s) for a source-language sentence s as follows:
`?(s) = logP (s) = log
?
t
P (s|t; ?)P (t)
We would like to maximize this objective by mak-
ing gradient-based updates.
?`?(s)
??
=
1
P (s)
?
t
P (t)
?
??
P (s|t; ?)
=
?
t
P (t|s)
(
f(s, t)?
?
s?
P (s?|t)f(s?, t)
)
= Et|s[f(s, t)? Es?|t[f(s
?, t)]]
(2)
We are left with a difference in expected feature
counts, as is typical in log-linear models. However,
unlike the supervised case, here both terms are ex-
pectations: the outer expectation is over all target se-
quences (given the observed source sequence), and
the nested expectation is over all source sequences,
given the target sequence. As the space of possible
target sequences t grows exponentially in the length
of the source sequence, it will not be practical to
compute this expectation directly.
Dynamic programming is the typical solution for
computing feature expectations, and can be applied
to sequence models when the feature function de-
composes locally. There are two reasons this will
not work in our case. First, while the forward-
backward algorithm would enable us to compute
Et|s, it would not give us the nested expectation
Et|s[Es?|t]; this is the classic challenge in training
globally-normalized log-linear models without la-
beled data (Smith and Eisner, 2005). Second, both
forward-backward and the Viterbi algorithm have
time complexity that is quadratic in the dimension of
the label space, at least 104 or 105. As we will show,
Sequential Monte Carlo (SMC) algorithms have a
number of advantages in this setting: they permit
the efficient computation of both the outer and inner
expectations, they are trivially parallelizable, and
the number of samples provides an intuitive tuning
tradeoff between accuracy and speed.
4.1 Sequential Monte Carlo approximation
Sequential Monte Carlo algorithms are a class of
sampling-based algorithms in which latent vari-
ables are sampled sequentially (Cappe et al, 2007).
They are particularly well-suited to sequence mod-
els, though they can be applied more broadly. SMC
algorithms maintain a set of weighted hypotheses;
the weights correspond to probabilities, and in our
case, the hypotheses correspond to target language
word sequences. Specifically, we approximate the
conditional probability,
P (t1:n|s1:n) ?
K?
k=1
?kn?tk1:n(t1:n),
where ?kn is the normalized weight of sample k at
word n (??kn is the unnormalized weight), and ?tk1:n is
a delta function centered at tk1:n.
At each step, and for each hypothesis k, a new
target word is sampled from a proposal distribution,
and the weight of the hypothesis is then updated. We
maintain feature counts for each hypothesis, and ap-
proximate the expectation by taking a weighted av-
erage using the hypothesis weights. The proposal
distribution will be described in detail later.
We make a Markov assumption, so that the emis-
sion probability P (s|t) decomposes across the ele-
ments of the sentence P (s|t) =
?N
n P (sn|tn). This
means that the feature functions f(s, t) must decom-
pose on each ?sn, tn? pair. We can then rewrite (1)
as
P (s|t; ?) =
N?
n
exp
(
?Tf(sn, tn)
)
Z(tn)
(3)
Z(tn) =
?
s
exp
(
?Tf(s, tn)
)
. (4)
In addition, we assume that the target language
model P (t) can be written as an N-gram language
model, P (t) =
?
n P (tn|tn?1, . . . tn?k+1). With
these assumptions, we can view normalization as
a finite state-space model in which the target lan-
guage model defines the prior distribution of the pro-
cess and Equation 3 defines the likelihood function.
We are able to compute the the posterior probabil-
ity P (t|s) using sequential importance sampling, a
member of the SMC family.
The crucial idea in sequential importance sam-
pling is to update the hypotheses tk1:n and their
weights ?kn so that they approximate the posterior
distribution at the next time step, P (t1:n+1|s1:n+1).
64
Assuming the proposal distribution has the form
Q(tk1:n|s1:n), the importance weights are given by
?kn ?
P (tk1:n|s1:n)
Q(tk1:n|s1:n)
(5)
In order to update the hypotheses recursively, we
rewrite P (t1:n|s1:n) as:
P (t1:n|s1:n) =
P (sn|t1:n, s1:n?1)P (t1:n|s1:n?1)
P (sn|s1:n?1)
=
P (sn|tn)P (tn|t1:n?1, s1:n?1)P (t1:n?1|s1:n?1)
P (sn|s1:n?1)
?P (sn|tn)P (tn|tn?1)P (t1:n?1|s1:n?1),
assuming a bigram language model. We further as-
sume the proposal distribution Q can be factored as:
Q(t1:n|s1:n) =Q(tn|t1:n?1, s1:n)Q(t1:n?1|s1:n?1)
=Q(tn|tn?1, sn)Q(t1:n?1|s1:n?1).
(6)
Then the unnormalized importance weights sim-
plify to a recurrence:
??kn =
P (sn|tkn)P (t
k
n|t
k
n?1)P (t
k
1:n?1|s1:n?1)
Q(tn|tn?1, sn)Q(tk1:n?1|s1:n?1)
(7)
=?kn?1
P (sn|tkn)P (t
k
n|t
k
n?1)
Q(tn|tn?1, sn)
(8)
Therefore, we can approximate the posterior dis-
tribution P (tn|s1:n) ?
?K
k=1 ?
k
n?tkn(tn), and com-
pute the outer expectation as follows:
Et|s[f(s, t)] =
K?
k=1
?kN
N?
n=1
f(sn, tkn) (9)
We compute the nested expectation using a non-
sequential Monte Carlo approximation, assuming
we can draw s`,k ? P (s|tkn).
Es|tk [f(s, t
k)] =
1
L
N?
n=1
L?
`=1
f(s`,kn , t
k
n)
This gives the overall gradient computation:
Et|s[f(s, t)? Es?|t[f(s
?, t)]] =
1
?K
k=1 ??
k
N
K?
k=1
??kN
?
N?
n=1
(
f(sn, tkn)?
1
L
L?
`=1
f(s`,kn , t
k
n)
)
(10)
where we sample tkn and update ?
k
n while mov-
ing from left-to-right, and sample s`,kn at each n.
Note that although the sequential importance sam-
pler moves left-to-right like a filter, we use only the
final weights ?N to compute the expectation. Thus,
the resulting expectation is based on the distribu-
tion P (s1:N |t1:N ), so that no backwards ?smooth-
ing? pass (Godsill et al, 2004) is needed to elim-
inate bias. Other applications of sequential Monte
Carlo make use of resampling (Cappe et al, 2007) to
avoid degeneration of the hypothesis weights, but we
found this to be unnecessary due to the short length
of Twitter messages.
4.2 Proposal distribution
The major computational challenge for dynamic
programming approaches to normalization is the
large label space, equal to the size of the target vo-
cabulary. It may appear that all we have gained
by applying sequential Monte Carlo is to convert
a computational problem into a statistical one: a
naive sampling approach will have little hope of
finding the small high-probability region of the high-
dimensional label space. However, sequential im-
portance sampling allows us to address this issue
through the proposal distribution, from which we
sample the candidate words tn. Careful design of the
proposal distribution can guide sampling towards
the high-probability space. In the asymptotic limit of
an infinite number of samples, any non-pathological
proposal distribution will ultimately arrive at the de-
sired estimate, but a good proposal distribution can
greatly reduce the number of samples needed.
Doucet et al (2001) note that the optimal pro-
posal ? which minimizes the variance of the im-
portance weights conditional on t1:n?1 and s1:n ?
has the following form:
Q(tkn|sn, t
k
n?1) =
P (sn|tkn)P (t
k
n|t
k
n?1)
?
t? P (sn|t
?)P (t?|tkn?1)
(11)
65
Sampling from this proposal requires computing
the normalized distribution P (sn|tkn); similarly, the
update of the hypothesis weights (Equation 8) re-
quires the calculation ofQ in its normalized form. In
each case, the total cost is the product of the vocabu-
lary sizes, O(#|?T |#|?S |), which is not tractable as
the vocabularies become large.
In low-dimensional settings, a convenient so-
lution is to set the proposal distribution equal
to the transition distribution, Q(tkn|sn, t
k
n?1) =
P (tkn|t
k
n?1, . . . , t
k
n?k+1). This choice is called the
?bootstrap filter,? and it has the advantage that the
weights ?(k) are exactly identical to the product
of emission likelihoods
?
n P (sn|t
k
n). The com-
plexity of computing the hypothesis weights is thus
O(#|?S |). However, because this proposal ignores
the emission likelihood, the bootstrap filter has very
little hope of finding a high-probability sample in
high-entropy contexts.
We strike a middle ground between efficiency and
accuracy, using a proposal distribution that is closely
related to the overall likelihood, yet is tractable to
sample and compute:
Q(tkn|sn, t
k
n?1)
def
=
P (sn|tkn)Z(t
k
n)P (t
k
n|t
k
n?1)
?
t? P (sn|t
?)Z(t?)P (t?|tkn?1)
=
exp
(
?Tf(sn, tn)
)
P (tkn|t
k
n?1)
?
t? exp
(
?Tf(sn, t?)
)
P (t?|tkn?1)
(12)
Here, we simply replace the likelihood distribu-
tion in (11) by its unnormalized version.
To update the unnormalized hypothesis weights
??kn, we have
??kn =?
k
n?1
?
t? exp
(
?Tf(sn, t?)
)
P (t?|tkn?1)
Z(tkn)
(13)
The numerator requires summing over all ele-
ments in ?T and the denominator Z(tkn) requires
summing over all elements in ?S , for a total cost of
O(#|?T |+ #|?S |).
4.3 Decoding
Given an input source sentence s, the decoding prob-
lem is to find a target sentence t that maximizes
P (t|s) ? P (s|t)P (t) =
?N
n P (sn|tn)P (tn|tn?1).
Feature name Description
word-word pair A set of binary features for each
source/target word pair ?s, t?
string similarity A set of binary features in-
dicating whether s is one of
the top N string similar non-
standard words of t, for N ?
{5, 10, 25, 50, 100, 250, 500, 1000}
Table 1: The feature set for our log-linear model
As with learning, we cannot apply the usual dy-
namic programming algorithm (Viterbi), because
of its quadratic cost in the size of the target lan-
guage vocabulary. This must be multiplied by
the cost of computing the normalized probability
P (sn|tn), resulting in a prohibitive time complexity
of O(#|?S |#|?T |2N).
We consider two approximate decoding algo-
rithms. The first is to simply apply the proposal dis-
tribution, with linear complexity in the size of the
two vocabularies. However, this decoder is not iden-
tical to P (t|s), because of the extra factor of Z(t)
in the numerator. Alternatively, we can apply the
proposal distribution for selecting target word can-
didates, then apply the Viterbi algorithm only within
these candidates. The total cost is O(#|?S |T 2N),
where T is the number of target word candidates we
consider; this will asymptotically approach P (t|s)
as T ? #|?T |. Our evaluations use the more expen-
sive proposal+Viterbi decoding, but accuracy with
the more efficient proposal-based decoding is very
similar.
4.4 Features
Our system uses the feature types described in Ta-
ble 1. The word pair features are designed to cap-
ture lexical conventions, e.g. you/u. We only con-
sider word pair features that fired during training.
The string similarity features rely on the similarity
function proposed by Contractor et al (2010), which
has proven effective for normalization in prior work.
We bin this similarity to create binary features indi-
cating whether a string s is in the top-N most similar
strings to t; this binning yields substantial speed im-
provements without negatively impacting accuracy.
66
5 Implementation and data
The model and inference described in the pre-
vious section are implemented in a software
system for normalizing text on twitter, called
UNLOL: unsupervised normalization in a LOg-
Linear model. The final system can process roughly
10,000 Tweets per hour. We now describe some im-
plementation details.
5.1 Normalization candidates
Most tokens in tweets do not require normalization.
The question of how to identify which words are
to be normalized is still an open problem. Follow-
ing Han and Baldwin (2011), we build a dictionary
of words which are permissible in the target domain,
and make no attempt to normalize source strings
that match these words. As with other comparable
approaches, we are therefore unable to normalize
strings like ill into I?ll. Our set of ?in-vocabulary?
(IV) words is based on the GNU aspell dictionary
(v0.60.6), containing 97,070 words. From this dic-
tionary, we follow Liu et al (2012a) and remove all
the words with a count of less than 20 in the Edin-
burgh Twitter corpus (Petrovic? et al, 2010) ? re-
sulting in a total of 52,449 target words. All sin-
gle characters except a and i are excluded, and rt
is treated as in-vocabulary. For all in-vocabulary
words, we define P (sn|tn) = ?(sn, tn), taking the
value of zero when sn 6= tn. This effectively pre-
vents our model from attempting to normalize these
words.
In addition to words that are in the target vocabu-
lary, there are many other strings that should not be
normalized, such as names and multiword shorten-
ings (e.g. going to/gonna).1 We follow prior work
and assume that the set of normalization candidates
is known in advance during test set decoding (Han et
al., 2013). However, the unlabeled training data has
no such information. Thus, during training we at-
tempt to normalize all tokens that (1) are not in our
lexicon of IV words, and (2) are composed of letters,
numbers and the apostrophe. This set includes con-
tractions like "gonna" and "gotta", which would not
appear in the test set, but are nonetheless normalized
1Whether multiword shortenings should be normalized is ar-
guable, but they are outside the scope of current normalization
datasets (Han and Baldwin, 2011).
during training. For each OOV token, we conduct a
pre-normalization step by reducing any repetitions
of more than two letters in the nonstandard words to
exactly two letters (e.g., cooool? cool).
5.2 Language modeling
The Kneser-Ney smoothed trigram target language
model is estimated with the SRILM toolkit Stolcke
(2002), using Tweets from the Edinburgh Twitter
corpus that contain no OOV words besides hash-
tags and username mentions (following (Han et al,
2013)). We use this language model for both training
and decoding. We occasionally find training con-
texts in which the trigram ?tn, tn?1, tn?2? is unob-
served in the language model data; features resulting
from such trigrams are not considered when comput-
ing the weight gradients.
5.3 Parameters
The Monte Carlo approximations require two pa-
rameters: the number of samples for sequential
Monte Carlo (K), and the number of samples for the
non-sequential sampler of the nested expectation (L,
from Equation 10). The theory of Monte Carlo ap-
proximation states that the quality of the approxima-
tion should only improve as the number of samples
increases; we obtained good results with K = 10
and L = 1, and found relatively little improvement
by increasing these values. The number of hypothe-
ses considered by the decoder is set to T = 10;
again, the performance should only improve with T ,
as we more closely approximate full Viterbi decod-
ing.
6 Experiments
Datasets We use two existing labeled Twitter
datasets to evaluate our approach. The first dataset
? which we call LWWL11, based on the names of
its authors Liu et al (2011) ? contains 3,802 indi-
vidual ?nonstandard? words (i.e., words that are not
in the target vocabulary) and their normalized forms.
The rest of the message in which the words is appear
is not available. As this corpus does not provide lin-
guistic context, its decoding must use a unigram tar-
get language model. The second dataset ? which
is called LexNorm1.1 by its authors Han and Bald-
win (2011) ? contains 549 complete tweets with
1,184 nonstandard tokens (558 unique word types).
67
Method Dataset Precision Recall F-measure
(Liu et al 2011)
LMML11
68.88 68.88 68.88
(Liu et al 2012) 69.81 69.81 69.81
UNLOL 73.04 73.04 73.04
(Han and Baldwin, 2011)
LexNorm 1.1
75.30 75.30 75.30
(Liu et al 2012) 84.13 78.38 81.15
(Hassan et al 2013) 85.37 56.4 69.93
UNLOL 82.09 82.09 82.09
UNLOL LexNorm 1.2 82.06 82.06 82.06
Table 2: Empirical results
In this corpus, we can decode with a trigram lan-
guage model.
Close analysis of LexNorm1.1 revealed some in-
consistencies in annotation (for example, y?all
and 2 are sometimes normalized to you and to,
but are left unnormalized in other cases). In ad-
dition, several annotations disagree with existing
resources on internet language and dialectal En-
glish. For example, smh is normalized to some-
how in LexNorm1.1, but internetslang.com
and urbandictionary.com assert that it stands
for shake my head, and this is evident from examples
such as smh at this girl. Similarly, finna
is normalized to finally in LexNorm1.1, but from
the literature on African American English (Green,
2002), it corresponds to fixing to (e.g., i?m finna
go home). To address these issues, we have pro-
duced a new version of this dataset, which we call
LexNorm1.2 (after consulting with the creators of
LexNorm1.1). LexNorm1.2 differs from version 1.1
in the annotations for 172 of the 2140 OOV words.
We evaluate on LexNorm1.1 to compare with prior
work, but we also present results on LexNorm1.2
in the hope that it will become standard in future
work on normalization in English. The dataset
is available at http://www.cc.gatech.edu/
~jeisenst/lexnorm.v1.2.tgz.
To obtain unlabeled training data, we randomly
sample 50 tweets from the Edinburgh Twitter cor-
pus Petrovic? et al (2010) for each OOV word. Some
OOV words appear less than 50 times in the cor-
pus, so we obtained more training tweets for them
through the Twitter search API.
Metrics Prior work on these datasets has assumed
perfect detection of words requiring normalization,
and has focused on finding the correct normalization
for these words (Han and Baldwin, 2011; Han et al,
2013). Recall has been defined as the proportion of
words requiring normalization which are normalized
correctly; precision is defined as the proportion of
normalizations which are correct.
Results We run our training algorithm for two it-
erations (pass the training data twice). The results
are presented in Table 2. Our system, UNLOL,
achieves the highest published F-measure on both
datasets. Performance on LexNorm1.2 is very simi-
lar to LexNorm1.1, despite the fact that roughly 8%
of the examples were relabeled.
In the normalization task that we consider, the to-
kens to be normalized are specified in advance. This
is the same task specification as in the prior work
against which we compare. At test time, our system
attempts normalizes all such tokens; every error is
thus both a false positive and false negative, so pre-
cision equals to recall for this task; this is also true
for Han and Baldwin (2011) and Liu et al (2011).
It is possible to trade recall for precision by re-
fusing to normalize words when the system?s confi-
dence falls below a threshold. A good setting of this
threshold can improve the F-measure, but we did not
report these results because we have no development
set for parameter tuning.
Regularization One potential concern is that the
number of non-zero feature weights will continually
increase until the memory cost becomes overwhelm-
ing. Although we did not run up against mem-
68
0 100000 200000 300000 400000number of features
79
80
81
82
83
F-m
eas
ure
?=1e?04
?=5e?05
?=1e?05
?=5e?06 ?=1e?06 ?=0e+00 ? Dataset F-measure # of features
10?4
LexNorm 1.1
79.05 9,281
5? 10?5 80.32 11,794
10?5 81.00 42,466
5? 10?6 82.52 74,744
10?6 82.35 241,820
0 82.26 369,366
5? 10?6 LexNorm 1.2 82.23 74,607
Figure 1: Effect of L1 regularization on the F-measure and the number of features with non-zero weights
ory limitations in the experiments producing the re-
sults in Table 2, this issue can be addressed through
the application of L1 regularization, which produces
sparse weight vectors by adding a penalty of ?||?||1
to the log-likelihood. We perform online optimiza-
tion of the L1-regularized log-likelihood by apply-
ing the truncated gradient method (Langford et al,
2009). We use an exponential decreasing learning
rate ?k = ?0?k/N , where k is the iteration counter
andN is the size of training data. We set ?0 = 1 and
? = 0.5. Experiments were run until 300,000 train-
ing instances were observed, with a final learning
rate of less than 1/32. As shown in Figure 1, a small
amount of regularization can dramatically decrease
the number of active features without harming per-
formance.
7 Analysis
We apply our normalization system to investi-
gate the orthographic processes underlying language
variation in social media. Using a dataset of 400,000
English language tweets, sampled from the month
of August in each year from 2009 to 2012, we ap-
ply UNLOL to automatically normalize each token.
We then treat these normalizations as labeled train-
ing data, and examine the Levenshtein alignment be-
tween the source and target tokens. This alignment
gives approximate character-level transduction rules
to explain each OOV token. We then examine which
rules are used by each author, constructing a matrix
of authors and rules.2
Factorization of the author-rule matrix reveals sets
of rules that tend to be used together; we might
call these rulesets ?orthographic styles.? We apply
non-negative matrix factorization (Lee and Seung,
2001), which characterizes each author by a vector
of k style loadings, and simultaneously constructs
k style dictionaries, which each put weight on dif-
ferent orthographic rules. Because the loadings are
constrained to be non-negative, the factorization can
be seen as sparsely assigning varying amounts of
each style to each author. We choose the factoriza-
tion that minimizes the Frobenius norm of the recon-
struction error, using the NIMFA software package
(http://nimfa.biolab.si/).
The resulting styles are shown in Table 3, for
k = 10; other values of k give similar overall re-
sults with more or less detail. The styles incor-
porate a number of linguistic phenomena, includ-
ing: expressive lengthening (styles 7-9; see Brody
and Diakopoulos, 2011); g- and t-dropping (style 5,
see Eisenstein 2013a) ; th-stopping (style 6); and
the dropping of several word-final vowels (styles
1-3). Some of these styles, such as t-dropping
and th-stopping, have direct analogues in spoken
language varieties (Tagliamonte and Temple, 2005;
Green, 2002), while others, like expressive length-
ening, seem more unique to social media. The re-
lationships between these orthographic styles and
social variables such as geography and demograph-
2We tried adding these rules as features and retraining the
normalization system, but this hurt performance.
69
style rules examples
1. you; o-dropping y/_ ou/_u *y/*_ o/_ u, yu, 2day, knw, gud, yur, wud, yuh, u?ve, toda,
everthing, everwhere, ourself
2. e-dropping, u/o be/b_ e/_ o/u e*/_* b, r, luv, cum, hav, mayb, bn, remembr, btween,
gunna, gud
3. a-dropping a/_ *a/*_ re/r_ ar/_r r, tht, wht, yrs, bck, strt, gurantee,
elementry, wr, rlly, wher, rdy, preciate,
neway
4. g-dropping g*/_* ng/n_ g/_ goin, talkin, watchin, feelin, makin
5. t-dropping t*/_* st/s_ t/_ jus, bc, shh, wha, gota, wea, mus, firts, jes,
subsistutes
6. th-stopping h/_ *t/*d th/d_ t/d dat, de, skool, fone, dese, dha, shid, dhat,
dat?s
7. (kd)-lengthening i_/id _/k _/d _*/k* idk, fuckk, okk, backk, workk, badd, andd,
goodd, bedd, elidgible, pidgeon
8. o-lengthening o_/oo _*/o* _/o soo, noo, doo, oohh, loove, thoo, helloo
9. e-lengthening _/i e_/ee _/e _*/e* mee, ive, retweet, bestie, lovee, nicee, heey,
likee, iphone, homie, ii, damnit
10. a-adding _/a __/ma _/m _*/a* ima, outta, needa, shoulda, woulda, mm,
comming, tomm, boutt, ppreciate
Table 3: Orthographic styles induced from automatically normalized Twitter text
ics must be left to future research, but they offer a
promising generalization of prior work that has fo-
cused almost exclusively on exclusively on lexical
variation (Argamon et al, 2007; Eisenstein et al,
2010; Eisenstein et al, 2011), with a few exceptions
for character-level features (Brody and Diakopoulos,
2011; Burger et al, 2011).
Note that style 10 is largely the result of mis-
taken normalizations. The tokens ima, outta, and
needa all refer to multi-word expressions in stan-
dard English, and are thus outside the scope of the
normalization task as defined by Han et al (2013).
UNLOL has produced incorrect single-token nor-
malizations for these terms: i/ima, out/outta, and
need/needa. But while these normalizations are
wrong, the resulting style nonetheless captures a co-
herent orthographic phenomenon.
8 Conclusion
We have presented a unified, unsupervised statistical
model for normalizing social media text, attaining
the best reported performance on the two standard
normalization datasets. The power of our approach
comes from flexible modeling of word-to-word re-
lationships through features, while exploiting con-
textual regularity to train the corresponding feature
weights without labeled data. The primary techni-
cal challenge was overcoming the large label space
of the normalization task; we accomplish this us-
ing sequential Monte Carlo. Future work may con-
sider whether sequential Monte Carlo can offer sim-
ilar advantages in other unsupervised NLP tasks. An
additional benefit of our joint statistical approach is
that it may be combined with other downstream lan-
guage processing tasks, such as part-of-speech tag-
ging (Gimpel et al, 2011) and named entity resolu-
tion (Liu et al, 2012b).
Acknowledgments
We thank the reviewers for thoughtful comments
on our submission. This work also benefitted
from discussions with Timothy Baldwin, Paul Cook,
Frank Dellaert, Arnoud Doucet, Micha Elsner, and
Sharon Goldwater. It was supported by NSF SOCS-
1111142.
References
S. Argamon, M. Koppel, J. Pennebaker, and J. Schler.
2007. Mining the blogosphere: age, gender, and the
varieties of self-expression. First Monday, 12(9).
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of ACL, pages 33?40.
70
Richard Beaufort, Sophie Roekhaut, Louise-Am?lie
Cougnon, and C?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of ACL, pages 770?
779.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL, pages 582?590.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on twit-
ter. In Proceedings of EMNLP.
Olivier Cappe, Simon J. Godsill, and Eric Moulines.
2007. An overview of existing methods and recent ad-
vances in sequential monte carlo. Proceedings of the
IEEE, 95(5):899?924, May.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007a. Investigation and model-
ing of the structure of texting language. Interna-
tional Journal on Document Analysis and Recognition,
10(3):157?174.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007b. Investigation and modeling of the structure of
texting language. International Journal of Document
Analysis and Recognition (IJDAR), 10(3-4):157?174.
Danish Contractor, Tanveer A. Faruquie, and L. Venkata
Subramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceedings of COLING, pages 189?196.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity, CALC ?09, pages
71?78, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
A. Doucet, N.J. Gordon, and V. Krishnamurthy. 2001.
Particle filters for state estimation of jump markov lin-
ear systems. Trans. Sig. Proc., 49(3):613?624, March.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of ACL.
Jacob Eisenstein. 2013a. Phonological factors in social
media writing. In Proceedings of the NAACL Work-
shop on Language Analysis in Social Media.
Jacob Eisenstein. 2013b. What to do about bad language
on the internet. In Proceedings of NAACL, pages 359?
369.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In Pro-
ceedings of ACL.
Simon J. Godsill, Arnaud Doucet, and Mike West. 2004.
Monte carlo smoothing for non-linear time series. In
Journal of the American Statistical Association, pages
156?168.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First Workshop on Unsu-
pervised Learning in NLP, EMNLP ?11.
Lisa J. Green. 2002. African American English: A
Linguistic Introduction. Cambridge University Press,
September.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: makn sens a #twitter. In
Proceedings of ACL, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lex-
ical normalization for social media text. ACM Trans-
actions on Intelligent Systems and Technology, 4(1):5.
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proceedings of ACL.
Catherine Kobus, Fran?ois Yvon, and G?raldine
Damnati. 2008. Normalizing sms: are two metaphors
better than one? In Proceedings of COLING, pages
441?448.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of ICML, pages 282?289.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. The
Journal of Machine Learning Research, 10:777?801.
D. D. Lee and H. S. Seung. 2001. Algorithms for Non-
Negative Matrix Factorization. In Advances in Neural
Information Processing Systems (NIPS), volume 13,
pages 556?562.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normaliz-
ing text messages without pre-categorization nor su-
pervision. In Proceedings of ACL, pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of ACL, pages 1035?1044.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang
Fu, and Furu Wei. 2012b. Joint inference of named
entity recognition and normalization for tweets. In
Proceedings of ACL.
71
Sa?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 354?362, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. Sproat, A.W. Black, S. Chen, S. Kumar, M. Os-
tendorf, and C. Richards. 2001. Normalization of
non-standard words. Computer Speech & Language,
15(3):287?333.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904.
Sali Tagliamonte and Rosalind Temple. 2005. New
perspectives on an ol? variable: (t,d) in british en-
glish. Language Variation and Change, 17:281?302,
September.
Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of ACL,
pages 1159?1168.
72
Learning Representations for Weakly
Supervised Natural Language
Processing Tasks
Fei Huang?
Temple University
Arun Ahuja??
Northwestern University
Doug Downey?
Northwestern University
Yi Yang?
Northwestern University
Yuhong Guo?
Temple University
Alexander Yates?
Temple University
Finding the right representations for words is critical for building accurate NLP systems when
domain-specific labeled data for the task is scarce. This article investigates novel techniques for
extracting features from n-gram models, Hidden Markov Models, and other statistical language
models, including a novel Partial Lattice Markov Random Field model. Experiments on part-
of-speech tagging and information extraction, among other tasks, indicate that features taken
from statistical language models, in combination with more traditional features, outperform
traditional representations alone, and that graphical model representations outperform n-gram
models, especially on sparse and polysemous words.
? 1805 N. Broad St., Wachman Hall 324, Philadelphia, PA 19122, USA.
E-mail: {fei.huang,yuhong,yates}@temple.edu.
?? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ahuja@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ddowney@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: yya518@eecs.northwestern.edu.
Submission received: 13 June 2012; revised submission received: 25 November 2012, accepted for publication:
15 January 2013.
doi:10.1162/COLI a 00167
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve
strong performance. Thus, a part-of-speech (POS) tagger would traditionally use a
feature like, ?the previous token is the? to help classify a given token as a noun
or adjective. For supervised NLP tasks with sufficient domain-specific training data,
these traditional features yield state-of-the-art results. However, NLP systems are in-
creasingly being applied to the Web, scientific domains, personal communications like
e-mails and tweets, among many other kinds of linguistic communication. These texts
have very different characteristics from traditional training corpora in NLP. Evidence
from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing
(Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan,
Ward, and Martin 2007), among other NLP tasks (Daume? III and Marcu 2006; Chelba
and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer,
Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades
significantly when tested on domains different from those used for training. Collecting
labeled training data for each new target domain is typically prohibitively expensive.
In this article, we investigate representations that can be applied to weakly supervised
learning, that is, learning when domain-specific labeled training data are scarce.
A growing body of theoretical and empirical evidence suggests that traditional,
manually crafted features for a variety of NLP tasks limit systems? performance in this
weakly supervised learning for two reasons. First, feature sparsity prevents systems
from generalizing accurately, because many words and features are not observed in
training. Also because word frequencies are Zipf-distributed, this often means that there
is little relevant training data for a substantial fraction of parameters (Bikel 2004b), espe-
cially in new domains (Huang and Yates 2009). For example, word-type features form
the backbone of most POS-tagging systems, but types like ?gene? and ?pathway? show
up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical data will have seen few training
examples related to sentences with features ?gene? and ?pathway? (Blitzer, McDonald,
and Pereira 2006; Ben-David et al. 2010).
Further, because words are polysemous, word-type features prevent systems from
generalizing to situations in which words have different meanings. For instance, the
word type ?signaling? appears primarily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, signaling that . . . ? (Marcus, Marcinkiewicz,
and Santorini 1993). In biomedical text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered a noun (NN) (PennBioIE 2005); this
phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010).
Our response to the sparsity and polysemy challenges with traditional NLP repre-
sentations is to seek new representations that allow systems to generalize to previously
unseen examples. That is, we seek representations that permit classifiers to have close
to the same accuracy on examples from other domains as they do on the domain of the
training data. Our approach depends on the well-known distributional hypothesis,
which states that a word?s meaning is identified with the contexts in which it appears
(Harris 1954; Hindle 1990). Our goal is to develop probabilistic statistical language
models that describe the contexts of individual words accurately. We then construct
representations, or mappings from word tokens and types to real-valued vectors,
from statistical language models. Because statistical language models are designed to
model words? contexts, the features they produce can be used to combat problems
with polysemy. And by careful design of the statistical language models, we can limit
86
Huang et al. Computational Linguistics
the number of features that they produce, controlling how sparse those features are in
training data.
Our specific contributions are as follows:
1. We show how to generate representations from a variety of language
models, including n-gram models, Brown clusters, and Hidden Markov
Models (HMMs). We also introduce a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of a Factorial Hidden
Markov Model (Ghahramani and Jordan 1997) for language modeling,
and we show how to produce representations from it.
2. We quantify the performance of these representations in experiments
on POS tagging in a domain adaptation setting, and weakly supervised
information extraction (IE). We show that the graphical models outperform
n-gram representations, even when the n-gram models leverage larger
corpora for training. The PL-MRF representation achieves a state-of-the-art
93.8% accuracy on a biomedical POS tagging task, which represents a
5.5 percentage point absolute improvement over more traditional POS
tagging representations, a 4.8 percentage point improvement over a tagger
using an n-gram representation, and a 0.7 percentage point improvement
over a tagger with an n-gram representation using several orders of
magnitude more training data. The HMM representation improves
over the n-gram model by 7 percentage points on our IE task.
3. We analyze how sparsity, polysemy, and differences between domains
affects the performance of a classifier using different representations.
Results indicate that statistical language model representations, and
especially graphical model representations, provide the best features
for sparse and polysemous words.
The next section describes background material and related work on representation
learning for NLP. Section 3 presents novel representations based on statistical language
models. Sections 4 and 5 discuss evaluations of the representations, first on sequence-
labeling tasks in a domain adaptation setting, and second on a weakly supervised set-
expansion task. Section 6 concludes and outlines directions for future work.
2. Background and Previous Work on Representation Learning
2.1 Terminology and Notation
In a traditional machine learning task, the goal is to make predictions on test data using
a hypothesis that is optimized on labeled training data. In order to do so, practitioners
predefine a set of features and try to estimate classifier parameters from the observed
features in the training data. We call these feature sets representations of the data.
Formally, let X be an instance space for a learning problem. Let Z be the space of
possible labels for an instance, and let f : X ? Z be the target function to be learned.
A representation is a function R: X ? Y , for some suitable feature space Y (such as Rd).
We refer to dimensions of Y as features, and for an instance x ? X we refer to values
for particular dimensions of R(x) as features of x. Given a set of training examples, a
learning machine?s task is to select a hypothesis h from the hypothesis space H, a subset
of ZR(X ). Errors by the hypothesis are measured using a loss function L(x, R, f, h) that
87
Computational Linguistics Volume 40, Number 1
measures the cost of the mismatch between the target function f (x) and the hypothesis
h(R(x)).
As an example, the instance set for POS tagging in English is the set of all English
sentences, and Z is the space of POS sequences containing labels like NN (for noun) and
VBG (for present participle). The target function f is the mapping between sentences
and their correct POS labels. A traditional representation in NLP converts sentences
into sequences of vectors, one for each word position. Each vector contains values for
features like, ?+1 if the word at this position ends with -tion, and 0 otherwise.? A
typical loss function would count the number of words that are tagged differently by
f (x) and h(R(x)).
2.2 Representation-Learning Problem Formulation
Machine learning theory assumes that there is a distribution D over X from which
data is sampled. Given a training set S = {(x1, z1), . . . , (xN, zN )} ? (D(X ),Z )N, a fixed
representation R, a hypothesis space H, and a loss function L, a machine learning
algorithm seeks to identify the hypothesis in H that will minimize the expected loss
over samples from distribution D:
h? = argmin
h?H
Ex?D(X )L(x, R, f, h) (1)
The representation-learning paradigm breaks the traditional notion of a fixed rep-
resentation R. Instead, we allow a space of possible representations R. The full learning
problem can then be formulated as the task of identifying the best R ? R and h ? H
simultaneously:
R?, h? = argmin
R?R,h?H
Ex?D(X )L(x, R, f, h) (2)
The representation-learning problem formulation in Equation (2) can in fact be
reduced to the general learning formulation in Equation (1) by setting the fixed rep-
resentation R to be the identity function, and setting the hypothesis space to be R?H
from the representation-learning task. We introduce the new formulation primarily as
a way of changing the perspective on the learning task: most NLP systems consider
a fixed, manually crafted transformation of the original data to some new space, and
investigate hypothesis classes over that space. In the new formulation, systems learn
the transformation to the feature space, and then apply traditional classification or
regression algorithms.
2.3 Theory on Domain Adaptation
We refer to the distribution D over the instance space X as a domain. For example,
the newswire domain is a distribution over sentences that gives high probability to
sentences about governments and current events; the biomedical literature domain
gives high probability to sentences about proteins and regulatory pathways. In domain
adaptation, a system observes a set of training examples (R(x), f (x)), where instances
x ? X are drawn from a source domain DS, to learn a hypothesis for classifying ex-
amples drawn from a separate target domain DT. We assume that large quantities of
unlabeled data are available for the source domain and target domain, and call these
88
Huang et al. Computational Linguistics
samples US and UT, respectively. For any domain D, let R(D) represent the induced
distribution over the feature space Y given by PrR(D)[y] = PrD[{x such that R(x) = y}].
Previous work by Ben-David et al. (2007, 2010) proves theoretical bounds on an
open-domain learning machine?s performance. Their analysis shows that the choice of
representation is crucial to domain adaptation. A good choice of representation must
allow a learning machine to achieve low error rates on the source domain. Just as
important, however, is that the representation must simultaneously make the source
and target domains look as similar to one another as possible. That is, if the labeling
function f is the same on the source and target domains, then for every h ? H, we can
provably bound the error of h on the target domain by its error on the source domain
plus a measure of the distance between DS and DT:
Ex?DTL(x, R, f, h) ? Ex?DSL(x, R, f, h) + d1(R(DS), R(DT )) (3)
where the variation divergence d1 is given by
d1(D,D?) = 2 sup
B?B
|PrD[B] ? PrD? [B]| (4)
where B is the set of measurable sets under D and D? (Ben-David et al. 2007, 2010).
Crucially, the distance between domains depends on the features in the representa-
tion. The more that features appear with different frequencies in different domains, the
worse this bound becomes. In fact, one lower bound for the d1 distance is the accuracy
of the best classifier for predicting whether an unlabeled instance y = R(x) belongs to
domain S or T (Ben-David et al. 2010). Thus, if R provides one set of common features for
examples from S, and another set of common features for examples from T, the domain
of an instance becomes easy to predict, meaning the distance between the domains
grows, and the bound on our classifier?s performance grows worse.
In light of Ben-David et al.?s theoretical findings, traditional representations in
NLP are inadequate for domain adaptation because they contribute to the d1 distance
between domains. Although many previous studies have shown that lexical features
allow learning systems to achieve impressively low error rates during training, they also
make texts from different domains look very dissimilar. For instance, a feature based on
the word ?bank? or ?CEO? may be common in a domain of newswire text, but scarce
or nonexistent in, say, biomedical literature. Ben David et al.?s theory predicts greater
variance in the error rate of the target domain classifier as the distance grows.
At the same time, traditional representations contribute to data sparsity, a lack of
sufficient training data for the relevant parameters of the system. In traditional super-
vised NLP systems, there are parameters for each word type in the data, or perhaps
even combinations of word types. Because vocabularies can be extremely large, this
leads to an explosion in the number of parameters. As a consequence, for many of their
parameters, supervised NLP systems have zero or only a handful of relevant labeled
examples (Bikel 2004a, 2004b). No matter how sophisticated the learning technique, it
is difficult to estimate parameters without relevant data. Because vocabularies differ
across domains, domain adaptation greatly exacerbates this issue of data sparsity.
2.4 Problem Formulation for the Domain Adaptation Setting
Formally, we define the task of representation learning for domain adaptation as the
following optimization problem: Given a set of unlabeled instances US drawn from the
89
Computational Linguistics Volume 40, Number 1
source domain and unlabeled instances UT from the target domain, as well as a set of
labeled instances LS drawn from the source domain, identify a function R? from the
space of possible representations R that minimizes
R?, h? = argmin
R?R,h?H
(
Ex?DSL(x, R, f, h)
)
+ ?d1(R(DS), R(DT )) (5)
where ? is a free parameter.
Note that there is an underlying tension between the two terms of the objec-
tive function: The best representation for the source domain would naturally include
domain-specific features, and allow a hypothesis to learn domain-specific patterns.
We are aiming, however, for the best general classifier, which happens to be trained
on training data from one domain (or a few domains). The domain-specific features
contribute to distance between domains, and to classifier errors on data taken from
domains not seen in training. By optimizing for this combined objective function, we
allow the optimization method to trade off between features that are best for classifying
source-domain data and features that allow generalization to new domains.
Unlike the representation-learning problem-formulation in Equation (2), Equa-
tion (5) does not reduce to the standard machine-learning problem (Equation (1)). In
a sense, the d1 term acts as a regularizer on R, which also affects H. Representation
learning for domain adaptation is a fundamentally novel learning task.
2.5 Tractable Representation Learning: Statistical Language Models
as Representations
For most hypothesis classes and any interesting space of representations, Equations (2)
and (5) are completely intractable to optimize exactly. Even given a fixed representation,
it is intractable to compute the best hypothesis for many hypothesis classes. And the d1
metric is intractable to compute from samples of a distribution, although Ben-David
et al. (2007, 2010) propose some tractable bounds. We view these problem formulations
as high-level goals rather than as computable objectives.
As a tractable objective, in this work we describe an investigation into the use of
statistical language models as a way to represent the meanings of words. This approach
depends on the well-known distributional hypothesis, which states that a word?s
meaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990).
From this hypothesis, we can formulate the following testable prediction, which we call
the statistical language model representation hypothesis, or LMRH:
To the extent that a model accurately describes a word?s possible contexts, parameters
of that model are highly informative descriptors of the word?s meaning, and are
therefore useful as features in NLP tasks like POS tagging, chunking, NER, and
information extraction.
The LMRH says, essentially, that for NLP tasks, we can decouple the task of optimiz-
ing a representation from the task of optimizing a hypothesis. To learn a representation,
we can train a statistical language model on unlabeled text, and then use parameters
or latent states from the statistical language model to create a representation function.
Optimizing a hypothesis then follows the standard learning framework, using the
representation from the statistical language model.
90
Huang et al. Computational Linguistics
The LMRH is similar to the manifold and cluster assumptions behind other semi-
supervised approaches to machine learning, such as Alternating Structure Optimization
(ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer,
McDonald, and Pereira 2006). All three of these techniques use predictors built on
unlabeled data as a way to harness the manifold and cluster assumptions. However,
the LMRH is distinct from at least ASO and SCL in important ways. Both ASO and SCL
create multiple ?synthetic? or ?pivot? prediction tasks using unlabeled data, and find
transformations of the input feature space that perform well on these tasks. The LMRH,
on the other hand, is more specific ? it asserts that for language problems, if we opti-
mize word representations on a single task (the language modeling task), this will lead
to strong performance on weakly supervised tasks. In reported experiments on NLP
tasks, both ASO and SCL use certain synthetic predictors that are essentially language
modeling tasks, such as the task of predicting whether the next token is of word type w.
To the extent that these techniques? performance relies on language-modeling tasks as
their ?synthetic predictors,? they can be viewed as evidence in support of the LMRH.
One significant consequence of the LMRH is that it allows us to leverage well-
developed techniques and models from statistical language modeling. Section 3
presents a series of statistical language models that we investigate for learning repre-
sentations for NLP.
2.6 Previous Work
There is a long tradition of NLP research on representations, mostly falling into one of
four categories: 1) vector space models of meaning based on document-level lexical co-
occurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010);
2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990;
Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and
Jordan 2003; Va?yrynen and Honkela 2004, 2005; Va?yrynen, Honkela, and Lindqvist
2007); 3) using clusters that are induced from distributional similarity (Brown et al.
1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse
features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu
2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao
et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008;
Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih
and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert
and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering
for representations, but where previous work has used bigram and trigram statistics to
form clusters, we build sophisticated models that attempt to capture the context of a
word, and hence its similarity to other words, more precisely. Our experiments show
that the new graphical models provide representations that outperform those from
previous work on several tasks.
Neural network statistical language models have recently achieved state-of-the-art
perplexity results (Mnih and Hinton 2009), and representations based on them have im-
proved in-domain chunking, NER, and SRL (Weston, Ratle, and Collobert 2008; Turian,
Bergstra, and Bengio 2009; Turian, Ratinov, and Bengio 2010). As far as we are aware,
Turian, Ratinov, and Bengio (2010) is the only other work to test a learned representation
on a domain adaptation task, and they show improvement on out-of-domain NER
with their neural net representations. Though promising, the neural network models
are computationally expensive to train, and these statistical language models work
only on fixed-length histories (n-grams) rather than full observation sequences. Turian,
91
Computational Linguistics Volume 40, Number 1
Ratinov, and Bengio?s (2010) tests also show that Brown clusters perform as well or
better than neural net models on all of their chunking and NER tests. We concentrate on
probabilistic graphical models with discrete latent states instead. We show that HMM-
based and other representations significantly outperform the more commonly used
Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings
of sequence-labeling tasks.
Most previous work on domain adaptation has focused on the case where some
labeled data are available in both the source and target domains (Chan and Ng 2006;
Daume? III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daume? III 2007; Jiang
and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze,
Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are
known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this
problem setting have focused on appropriately weighting examples from the source and
target domains so that the learning algorithm can balance the greater relevance of the
target-domain data with the larger source-domain data set. In some cases, researchers
combine this approach with semi-supervised learning to include unlabeled examples
from the target domain as well (Daume? III, Kumar, and Saha 2010). These techniques
do not handle open-domain corpora like the Web, where they require expert input to
acquire labels for each new single-domain corpus, and it is difficult to come up with
a representative set of labeled training data for each domain. Our technique requires
only unlabeled data from each new domain, which is significantly easier and cheaper to
acquire. Where target-domain labeled data is available, however, these techniques can
in principle be combined with ours to improve performance, although this has not yet
been demonstrated empirically.
A few researchers have considered the more general case of domain adaptation
without labeled data in the target domain. Perhaps the best known is Blitzer, McDonald,
and Pereira?s (2006) Structural Correspondence Learning (SCL). SCL uses ?pivot? words
common to both source and target domains, and trains linear classifiers to predict these
pivot words from their context. After an SVD reduction of the weight vectors for these
linear classifiers, SCL projects the original features through these weight vectors to
obtain new features that are added to the original feature space. Like SCL, our language
modeling techniques attempt to predict words from their context, and then use the
output of these predictions as new features. Unlike SCL, we attempt to predict all words
from their context, and we rely on traditional probabilistic methods for language mod-
eling. Our best learned representations, which involve significantly different techniques
from SCL, especially latent-variable probabilistic models, significantly outperform SCL
in POS tagging experiments.
Other approaches to domain adaptation without labeled data from the target do-
main include Satpal and Sarawagi (2007), who show that by changing the optimization
function during conditional random field (CRF) training, they can learn classifiers that
port well to new domains. Their technique selects feature subsets that minimize the
distance between training text and unlabeled test text, but unlike our techniques, theirs
cannot learn representations with features that do not appear in the original feature set.
In contrast, we learn hidden features through statistical language models. McClosky,
Charniak, and Johnson (2010) use classifiers from multiple source domains and features
that describe how much a target document diverges from each source domain to deter-
mine an optimal weighting of the source-domain classifiers for parsing the target text.
However, it is unclear if this ?source-combination? technique works well on domains
that are not mixtures of the various source domains. Dai et al. (2007) use KL-divergence
between domains to directly modify the parameters of their naive Bayes model for a
92
Huang et al. Computational Linguistics
text classification task trained purely on the source domain. These last two techniques
are not representation learning, and are complementary to our techniques.
Our representation-learning approach to domain adaptation is an instance of
semi-supervised learning. Of the vast number of semi-supervised approaches to
sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki?s
(2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text
to achieve the current best performance on in-domain chunking, and semi-supervised
approaches to improving in-domain SRL with large quantities of unlabeled text
(Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Fu?rstenau and
Lapata 2009). Ando and Zhang?s (2005) semi-supervised sequence labeling technique
has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and
Pereira 2006); our representation-learning approaches outperform it. Unlike most semi-
supervised techniques, we concentrate on a particularly simple task decomposition: un-
supervised learning for new representations, followed by standard supervised learning.
In addition to our task decomposition being simple, our learned representations are also
task-independent, so we can learn the representation once, and then apply it to any task.
One of the best-performing representations that we consider for domain adaptation
is based on the HMM (Rabiner 1989). HMMs have of course also been used for super-
vised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and
Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised
POS tagging have focused on incorporating prior knowledge into the POS induction
model (Grac?a et al. 2009; Toutanova and Johnson 2007), or on new training techniques
like contrastive estimation (Smith and Eisner 2005) for alternative sequence models.
Despite the fact that completely connected, standard HMMs perform poorly at the POS
induction task (Johnson 2007), we show that they still provide very useful features
for a supervised POS tagger. Experiments in information extraction have previously
also shown that HMMs provide informative features for this quite different, semantic
processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010).
This article extends our previous work on learning representations for do-
main adaptation (Huang and Yates 2009, 2010) by investigating new language
representations?the naive Bayes representation and PL-MRF representation (Huang
et al. 2011)?by analyzing results in terms of polysemy, sparsity, and domain diver-
gence; by testing on new data sets including a Chinese POS tagging task; and by pro-
viding an empirical comparison with Brown clusters as representations.
3. Learning Representations of Distributional Similarity
In this section, we will introduce several representation learning models.
3.1 Traditional POS-Tagging Representations
As an example of our terminology, we begin by describing a representation used in
traditional POS taggers (this representation will later form a baseline for our POS
tagging experiments). The instance set X is the set of English sentences, and Z is the set
of POS tag sequences. A traditional representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per word xi in the sentence. Dimensions
for each latent vector include indicators for the word type of xi and various orthographic
features. Table 1 presents the full list of features in TRAD-R. Because our IE task classifies
word types rather than tokens, this baseline is not appropriate for that task. Herein, we
93
Computational Linguistics Volume 40, Number 1
Table 1
Summary of features provided by our representations. ?a1[g(a)] represents a set of boolean
features, one for each value of a, where the feature is true iff g(a) is true. xi represents a token at
position i in sentence x, w represents a word type, Suffixes = {-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity},
k (and k) represents a value for a latent state (set of latent states) in a latent-variable model, y?
represents the maximum a posteriori sequence of states y for x, yi is the latent variable for xi, and
yi,j is the latent variable for xi at layer j. prefix(y,p) is the p-length prefix of the Brown cluster y.
Representation Features
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
n-GRAM-R ?w? ,w??P(w?ww??)/P(w)
LSA-R ?w,j{v?left(w)}j
?w,j{v?right(w)}j
NB-R ?k1[y?i = k]
HMM-TOKEN-R ?k1[y?i = k]
HMM-TYPE-R ?kP(y = k|x = w)
I-HMM-TOKEN-R ?j,k1[y?i,j = k]
I-HMM-TYPE-R ?j,kP(y.,j = k|x = w)
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j, p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[y?i,j = k]
LATTICE-TYPE-R ?kP(y = k|x = w)
describe how we can learn representations R by using a variety of statistical language
models, for use in both our IE and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all representations for IE do not.
3.2 n-gram Representations
n-gram representations, which we call n-GRAM-R, model a word type w in terms of the
n-gram contexts in which w appears in a corpus. Specifically, for word w we generate
the vector P(w?ww??)/P(w), the conditional probability of observing the word sequence
w? to the left and w?? to the right of w. Each dimension in this vector represents a com-
bination of the left and right words. The experimental section describes the particular
corpora and statistical language modeling methods used for estimating probabilities.
Note that these features depend only on the word type w, and so for every token xi = w,
n-GRAM-R provides the same set of features regardless of local context.
One drawback of n-GRAM-R is that it does not handle sparsity well?the features
are as sparsely observed as the lexical features in TRAD-R, except that n-GRAM-R fea-
tures can be obtained from larger corpora. As an alternative, we apply latent semantic
analysis (LSA) (Deerwester et al. 1990) to compute a reduced-rank representation. For
word w, let vright(w) represent the right context vector of w, which in each dimension
contains the value of P(ww??)/P(w) for some word w??, as observed in the n-gram
model. Similarly, let vleft(w) be the left context vector of w. We apply LSA to the set
94
Huang et al. Computational Linguistics
   
 


	 	 	 	
 	


 
Figure 1
A graphical representation of the naive Bayes statistical language model. The B and E are special
dummy words for the beginning and end of the sentence.
of right context vectors and the set of left context vectors separately,1 to find reduced-
rank versions v?right(w) and v
?
left(w), where each dimension represents a combination
of several context word types. We then use each component of v?right(w) and v
?
left(w)
as features. After experimenting with different choices for the number of dimensions to
reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes
the performance of our supervised sequence labelers on held-out data. We call this
model LSA-R.
3.3 A Context-Dependent Representation Using Naive Bayes
The n-GRAM-R and LSA-R representations always produce the same features F for a
given word type w, regardless of the local context of a particular token xi = w. Our
remaining representations are all context-dependent, in the sense that the features
provided for token xi depend on the local context around xi. We begin with a statis-
tical language model based on the Naive Bayes model with categorical latent states
S = {1, . . . , K}. First, we form trigrams from our sentences. For each trigram, we form a
separate Bayes net in which each token from the trigram is conditionally independent
given the latent state. For tokens xi?1, xi, and xi+1, the probability of this trigram given
latent state Yi = y is given by:
P(xi?1, xi, xi+1|yi) = Pleft(xi?1|yi)Pmid(xi|yi)Pright(xi+1|yi) (6)
where Pleft, Pmid, and Pright are multinomial distributions conditioned on the latent state.
The probability of a whole sentence is then given by the product of the probabilities
of its trigrams. Figure 1 shows a graphical representation of this model. We train our
models using standard expectation-maximization (Dempster, Laird, and Rubin 1977)
with random initialization of the parameters.
Because our factorization of the sentence does not take into account the fact that the
trigrams overlap, the resulting statistical language model is mass-deficient. Worse still,
it is throwing away information from the dependencies among trigrams which might
help make better clustering decisions. Nevertheless, this model closely mirrors many
of the clustering algorithms used in previous approaches to representation learning for
sequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras,
1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find a
simultaneous reduction of the left and right context vectors, a significantly more complex undertaking.
95
Computational Linguistics Volume 40, Number 1
and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as an
important benchmark.
Given a naive Bayes statistical language model, we construct an NB-R representa-
tion that produces |S| boolean features Fs(xi) for each token xi and each possible latent
state s ? S:
Fs(xi) =
{
true if s = arg maxs??SP(xi?1, xi, xi+1|yi = s?),
false otherwise.
For a reasonable choice of S (i.e., |S|  |V|), each feature should be observed often
in a sufficiently large training data set. Therefore, compared with n-GRAM-R, NB-R
produces far fewer features. On the other hand, its features for xi depend not just on
the contexts in which xi has appeared in the statistical language model?s training data,
but also on xi?1 and xi+1 in the current sentence. Furthermore, because the range of
the features is much more restrictive than real-valued features, it is less prone to data
sparsity or variations across domains than real-valued features.
3.4 Context-Dependent, Structured Representations: The Hidden Markov Model
In previous work, we have implemented several representations based on hidden
Markov models (Rabiner 1989), which we used for both sequential labeling (like POS
tagging [Huang et al. 2011] and NP chunking [Huang and Yates 2009]) and IE (Downey,
Schoenmackers, and Etzioni 2007). Figure 2 shows a graphical model of an HMM. An
HMM is a generative probabilistic model that generates each word xi in the corpus
conditioned on a latent variable yi. Each yi in the model takes on integral values from 1
to K, and each one is generated by the latent variable for the preceding word, yi?1. The
joint distribution for a corpus x = (x1, . . . , xN ) and a set of state vectors y = (y1, . . . , yN )
is given by: P(x, y) =
?
i P(xi|yi)P(yi|yi?1). Using expectation-maximization (EM)
(Dempster, Laird, and Rubin 1977), it is possible to estimate the distributions for
P(xi|yi) and P(yi|yi?1) from unlabeled data.
We construct two different representations from HMMs, one for sequence-labeling
tasks and one for IE. For sequence labeling, we use the Viterbi algorithm to produce the
optimal setting y? of the latent states for a given sentence x, or y? = argmax
y
P(x, y). We
use the value of y?i as a new feature for xi that represents a cluster of distributionally
similar words. For IE, we require features for word types w, rather than tokens xi.
Applying Bayes? rule to the HMM parameters, we compute a distribution P(Y|x = w),
where Y is a single latent node, x is a single token, and w is its word type. We then use
each of the K values for P(Y = k|x = w), where k ranges from 1 to K, as features. This set
   
 

	 	 	 	
 	

Figure 2
The Hidden Markov Model.
96
Huang et al. Computational Linguistics
of features represents a ?soft clustering? of w into K different clusters. We refer to these
representations as HMM-TOKEN-R and HMM-TYPE-R, respectively.
We also compare against a multi-layer variation of the HMM from our previous
work (Huang and Yates 2010). This model trains an ensemble of M independent HMM
models on the same corpus, initializing each one randomly. We can then use the Viterbi-
optimal decoded latent state of each independent HMM model as a separate feature for
a token, or the posterior distribution for P(Y|x = w) from each HMM as a separate set
of features for each word type. We refer to this statistical language model as an I-HMM,
and the representations as I-HMM-TOKEN-R and I-HMM-TYPE-R, respectively.
Finally, we compare against Brown clusters (Brown et al. 1992) as learned features.
Although not traditionally described as such, Brown clustering involves constructing
an HMM model in which each word type is restricted to having exactly one latent state
that may generate it. Brown et al. describe a greedy agglomerative clustering algorithm
for training this model on unlabeled text. Following Turian, Ratinov, and Bengio (2010),
we use Percy Liang?s implementation of this algorithm for our comparison, and we test
runs with 100, 320, 1,000 and 3,200 clusters. We use features from these clusters identical
to Turian et al.?s.2 Turian et al. have shown that Brown clusters match or exceed the
performance of neural network-based statistical language models in domain adaptation
experiments for named-entity recognition, as well as in-domain experiments for NER
and chunking.
Because HMM-based representations offer a small number of discrete states as
features, they have a much greater potential to combat sparsity than do n-gram mod-
els. Furthermore, for token-based representations, these models can potentially handle
polysemy better than n-gram statistical language models by providing different features
in different contexts.
3.5 A Novel Lattice Statistical Language Model Representation
Our final statistical language model is a novel latent-variable statistical language model,
called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The
model contains a lattice of M ? N latent states, where N is the number of words in a
sentence and M is the number of layers in the model. The dotted and solid lines in the
figure together form a complete lattice of edges between these nodes; the PL-MRF uses
only the solid edges. Formally, let c = 	N2 
, where N is the length of the sentence; let i
denote a position in the sentence, and let j denote a layer in the lattice. If i < c and j is
odd, or if j is even and i > c, we delete edges between yi,j and yi,j+1 from the complete
lattice. The same set of nodes remains, but the partial lattice contains fewer edges and
paths between the nodes. A central ?trunk? at i = c connects all layers of the lattice, and
branches from this trunk connect either to the branches in the layer above or the layer
below (but not both).
The result is a model that retains most of the edges of the complete lattice, but
unlike the complete lattice, it supports tractable inference. As M, N ? ?, five out of
every six edges from the complete lattice appear in the PL-MRF. However, the PL-MRF
makes the branches conditionally independent from one another, except through the
trunk. For instance, the left branch between layers 1 and 2 ((y1,1, y1,2) and (y2,1, y2,2)) in
Figure 3 are disconnected; similarly, the right branch between layers 2 and 3 ((y4,2, y4,3)
and (y5,2, y5,3)) are disconnected, except through the trunk and the observed nodes. As
2 Percy Liang?s implementation is available at http://metaoptimize.com/projects/wordreprs/.
97
Computational Linguistics Volume 40, Number 1
y4,1
y3,1
y4,2
y3,2
y4,3
y3,3
y4,4
y3,4
y4,5
y3,5
x1
y2,1
y1,1
x2
y2,2
y1,2
x3
y2,3
y1,3
x4
y2,4
y1,4
x5
y2,5
y1,5
Figure 3
The PL-MRF model for a five-word sentence and a four-layer lattice. Dashed gray edges are part
of a complete lattice, but not part of the PL-MRF.
a result, excluding the observed nodes, this model has a low tree-width of 2 (excluding
observed nodes), and a variety of efficient dynamic programming and message-passing
algorithms for training and inference can be readily applied (Bodlaender 1988). Our
inference algorithm passes information from the branches inwards to the trunk, and
then upward along the trunk, in time O(K4MN). In contrast, a fully connected lattice
model has tree-width = min(M, N), making inference and learning intractable (Sutton,
McCallum, and Rohanimanesh 2007), partly because of the difficulty in enumerating
and summing over the exponentially-many configurations y for a given x.
We can justify the choice of this model from a linguistic perspective as a way to
capture the multi-dimensional nature of words. Linguists have long argued that words
have many different features in a high dimensional space: They can be separately
described by part of speech, gender, number, case, person, tense, voice, aspect, mass
vs. count, and a host of semantic categories (agency, animate vs. inanimate, physical vs.
abstract, etc.), to name a few (Sag, Wasow, and Bender 2003). In the PL-MRF, each layer
of nodes is intended to represent some latent dimension of words.
We represent the probability distribution for PL-MRFs as log-linear models that
decompose over cliques in the MRF graph. Let Cliq(x, y) represent the set of all maximal
cliques in the graph of the MRF model for x and y. Expressing the lattice model in log-
linear form, we can write the marginal probability P(x) of a given sentence x as:
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x?,y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
where score(c, x, y) = exp(?c ? fc(xc, yc)). Our model includes parameters for transitions
between two adjacent latent variables on layer j: ?transi,s,i+1,s?,j for yi,j = s and yi+1,j = s
?. It
also includes observation parameters for latent variables and tokens, as well as for pairs
of adjacent latent variables in different layers and their tokens: ?obsi,j,s,w and ?
obs
i,j,s,j+1,s?,w for
yi,j = s, yi,j+1 = s?, and xi = w.
98
Huang et al. Computational Linguistics
As with our HMM models, we create two representations from PL-MRFs, one for
tokens and one for types. For tokens, we decode the model to compute y?, the matrix of
optimal latent state values for sentence x. For each layer j and and each possible latent
state value k, we add a boolean feature for token xi that is true iff y?i,j = k. For word
types, we compute distributions over the latent state space. Let y be a column vector of
latent variables for word type w. For a PL-MRF model with M layers of binary variables,
there are 2M possible values for y. Our type representation computes a probability
distribution over these 2M possible values, and uses each probability as a feature for
w.3 We refer to these two representations as LATTICE-TOKEN-R and LATTICE-TYPE-R,
respectively.
We train the PL-MRF using contrastive estimation (Smith and Eisner 2005), which
iteratively optimizes the following objective function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x??N (x),y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
(7)
where N (x), the neighborhood of x, indicates a set of perturbed variations of the original
sentence x. Contrastive estimation seeks to move probability mass away from the per-
turbed neighborhood sentences and onto the original sentence. We use a neighborhood
function that includes all sentences which can be obtained from the original sentence by
swapping the order of a consecutive pair of words. Training uses gradient descent over
this non-convex objective function with a standard software package (Liu and Nocedal
1989) and converges to a local maximum or saddle point.
For tractability, we modify the training procedure to train the PL-MRF one layer
at a time. Let ?i represent the set of parameters relating to features of layer i, and let
??i represent all other parameters. We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and optimize ?1, and so on. For training each
layer, we use a convergence threshold of 10?6 on the objective function in Equation (7),
and each layer typically converges in under 100 iterations.
4. Domain Adaptation with Learned Representations
We evaluate the representations described earlier on POS tagging and NP chunking
tasks in a domain adaptation setting.
4.1 A Rich Problem Setting for Representation Learning
Existing supervised NLP systems are domain-dependent: There is a substantial drop in
their performance when tested on data from a new domain. Domain adaptation is the
task of overcoming this domain dependence. The aim is to build an accurate system for
3 This representation is only feasible for small numbers of layers, and in our experiments that require type
representations, we used M = 10. For larger values of M, other representations are also possible. We also
experimented with a representation which included only M possible values: For each layer l, we included
P(yl = 0|w) as a feature. We used the less-compact representation in our experiments because results
were better.
99
Computational Linguistics Volume 40, Number 1
a target domain by training on labeled examples from a separate source domain. This
problem is sometimes also called transfer learning (Raina et al. 2007).
Two of the challenges for NLP representations, sparsity and polysemy, are exacer-
bated by domain adaptation. New domains come with new words and phrases that
appear rarely (or even not at all) in the training domain, thus increasing problems
with data sparsity. And even for words that do appear commonly in both domains, the
contexts around the words will change from the training domain to the target domain.
As a result, domain adaptation adds to the challenge of handling polysemous words,
whose meaning depends on context.
In short, domain adaptation is a challenging setting for testing NLP representations.
We now present several experiments testing our representations against state-of-the-
art POS taggers in a variety of domain adaptation settings, showing that the learned
representations surpass the previous state-of-the-art, without requiring any labeled data
from the target domain.
4.2 Experimental Set-up
For domain adaptation, we test our representations on two sequence labeling tasks:
POS tagging and chunking. To incorporate learned representation into our models, we
follow this general procedure, although the details vary by experiment and are given in
the following sections. First, we collect a set of unannotated text from both the training
domain and test domain. Second, we learn representations on the unannotated text.
We then automatically annotate both the training and test data with features from the
learned representation. Finally, we train a supervised linear-chain CRF model on the
annotated training set and apply it to the test set.
A linear-chain CRF is a Markov random field (Darroch, Lauritzen, and Speed 1980)
in which the latent variables form a path with edges only between consecutive nodes in
the path, and all latent variables are globally conditioned on the observations. Let X be a
random variable over data sequences, and Z be a random variable over corresponding
label sequences. The conditional distribution over the label sequence Z given X has the
form
p?(Z = z|X = x) ? exp
?
?
?
i
?
j
?j fj(zi?1, zi, x, i)
?
? (8)
where fj(zi?1, zi, x, i) is a real-valued feature function of the entire observation sequence
and the labels at positions i and i ? 1 in the label sequence, and ?j is a parameter to be
estimated from training data.
We use an open source CRF software package designed by Sunita Sarawagi to train
and apply our CRF models.4 As is standard, we use two kinds of feature functions:
transition and observation. Transition feature functions indicate, for each pair of labels
l and l?, whether zi = l and zi?1 = l?. Boolean observation feature functions indicate, for
each label l and each feature f provided by a representation, whether zi = l and xi has
feature f . For each label l and each real-valued feature f in representation R, real-valued
observation feature functions have value f (x) if zi = l, and are zero otherwise.
4 Available from http://sourceforge.net/projects/crf/.
100
Huang et al. Computational Linguistics
4.3 Domain Adaptation for POS Tagging
Our first experiment tests the performance of all the representations we introduced
earlier on an English POS tagging task, trained on newswire text, to tag biomedical re-
search literature. We follow Blitzer et al.?s experimental set-up. The labeled data consists
of the WSJ portion of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993)
as source domain data, and 561 labeled sentences (9,576 tokens) from the biomedical
research literature database MEDLINE as target domain data (PennBioIE 2005). Fully
23% of the tokens in the labeled test text are never seen in the WSJ training data. The
unlabeled data consists of the WSJ text plus 71,306 additional sentences of MEDLINE
text (Blitzer, McDonald, and Pereira 2006). As a preprocessing step, we replace hapax
legomena (defined as words that appear once in our unlabeled training data) with
the special symbol *UNKNOWN*, and do the same for words in the labeled test sets that
never appeared in any of our unlabeled training text.
For representations, we tested TRAD-R, n-GRAM-R, LSA-R, NB-R, HMM-TOKEN-
R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE-TOKEN-R (8, 12, 16,
and 20 layers). Each latent node in the I-HMMs had 80 possible values, creating
808 ? 1015 possible configurations of the eight-layer I-HMM for a single word. Each
node in our PL-MRF is binary, creating a much smaller number (220 ? 106) of possible
configurations for each word in a 20-layer representation. To give the n-gram model
the largest training data set available, we trained it on the Web 1Tgram corpus (Brants
and Franz 2006). We included the top 500 most common n-grams for each word type,
and then used mutual information on the training data to select the top 10,000 most
relevant n-gram features for all word types, in order to keep the number of features
manageable. We incorporated n-gram features as binary values indicating whether xi
appeared with the n-gram or not. For comparison, we also report on the performance of
Brown clusters (100, 320, 1,000, and 3,200 possible clusters), following Turian, Ratinov,
and Bengio (2010). Finally, we compare against Blitzer, McDonald, and Pereira (2006)
SCL technique, described in Section 2.6, and the standard semi-supervised learning
algorithm ASO (Ando and Zhang 2005), whose results on this task were previously
reported by Blitzer, McDonald, and Pereira (2006).
Table 2 shows the results for the best variation of each kind of model?20 layers for
the PL-MRF, 7 layers for the I-HMM, and 3,200 clusters for the Brown clustering. All
statistical language model representations outperform the TRAD-R baseline.
In nearly all cases, learned representations significantly outperformed TRAD-R. The
best representation, the 20-layer LATTICE-TOKEN-R, reduces error by 47% (35% on
OOV) relative to the baseline TRAD-R, and by 44% (24% on out-of-vocabulary words
(OOV)) relative to the benchmark SCL system. For comparison, this model achieved a
96.8% in-domain accuracy on Sections 22?24 of the Penn Treebank, about 0.5 percentage
point shy of a state-of-the-art in-domain system with more sophisticated supervised
learning (Shen, Satta, and Joshi 2007). The BROWN-TOKEN-R representation, which
Turian, Ratinov, and Bengio (2010) demonstrated performed as well or better than
a variety of neural network statistical language models as representations, achieved
accuracies between the SCL system and the HMM-TOKEN-R. The WEB1T-n-GRAM-R,
I-HMM-TOKEN-R, and LATTICE-TOKEN-R all performed quite close to one another,
but the I-HMM-TOKEN-R and LATTICE-TOKEN-R were trained on many orders of
magnitude less text. The LSA-R and NB-R outperformed the TRAD-R baseline but
not the SCL system. The n-GRAM-R, which was trained on the same text as the
other representations except the WEB1T-n-GRAM-R, performed far worse than the
WEB1T-n-GRAM-R.
101
Computational Linguistics Volume 40, Number 1
Table 2
Learned representations, and especially latent-variable statistical language model
representations, significantly outperform a traditional CRF system on domain adaptation for
POS tagging. Percent error is shown for all words and out-of-vocabulary (OOV) words. The
SCL+500bio system was given 500 labeled training sentences from the biomedical domain.
1.8% of tokens in the biomedical test set had POS tags like ?HYPHENATED?, which are not
part of the tagset for the training data, and were labeled incorrectly by all systems without
access to labeled data from the biomedical domain. As a result, an error rate of 1.8 + 3.9 = 5.7
serves as a reasonable lower bound for a system that has never seen labeled examples from
the biomedical domain.
Model All words OOV words
TRAD-R 11.7 32.7
n-GRAM-R 11.7 32.2
LSA-R 11.6 31.1
NB-R 11.6 30.7
ASO 11.6 29.1
SCL 11.1 28
BROWN-TOKEN-R 10.0 25.2
HMM-TOKEN-R 9.5 24.8
WEB1T-n-GRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 ?
The amount of unlabeled training data has a significant impact on the performance
of these representations. This is apparent in the difference between WEB1T-n-GRAM-
R and n-GRAM-R, but it is also true for our other representations. Figure 4 shows the
accuracy of a representative subset of our taggers on words not seen in labeled training
data, as we vary the amount of unlabeled training data available to the language
Figure 4
Learning curve for representations: target domain accuracy of our taggers on OOV words
(not seen in labeled training data), as a function of the number of unlabeled examples given
to the language models.
102
Huang et al. Computational Linguistics
models. Performance grows steadily for all representations we measured, and none
of the learning curves appears to have peaked. Furthermore, the margin between the
more complex graphical models and the simpler n-gram models grows with increasing
amounts of training data.
4.3.1 Sparsity and Polysemy. We expected that statistical language model represen-
tations would perform well in part because they provide meaningful features for
sparse and polysemous words. For sparse tokens, these trends are already evident
in the results in Table 2: Models that provide a constrained number of features, like
HMM-based models, tend to outperform models that provide huge numbers of fea-
tures (each of which, on average, is only sparsely observed in training data), like
TRAD-R.
As for polysemy, HMM models significantly outperform naive Bayes models and
the n-GRAM-R. The n-GRAM-R?s features do not depend on a token type?s context at all,
and the NB-R?s features depend only on the tokens immediately to the right and left of
the current word. In contrast, the HMM takes into account all tokens in the surrounding
sentence (although the strength of the dependence on more distant words decreases
rapidly). Thus the performance of the HMM compared with n-GRAM-R and NB-R,
as well as the performance of the LATTICE-TOKEN-R compared with the WEB1T-n-
GRAM-R, suggests that representations that are sensitive to the context of a word
produce better features.
To test these effects more rigorously, we selected 109 polysemous word types from
our test data, along with 296 non-polysemous word types. The set of polysemous word
types was selected by filtering for words in our labeled data that had at least two
POS tags that began with distinct letters (e.g., VBZ and NNS). An initial set of non-
polysemous word types was selected by filtering for types that appeared with just
one POS tag. We then manually inspected these initial selections to remove obvious
cases of word types that were in fact polysemous within a single part-of-speech, such
as ?bank.? We further define sparse word types as those that appear five times or
fewer in all of our unlabeled data, and we define non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table 3 shows our POS tagging results
on the tokens of our labeled biomedical data with word types matching these four
categories.
As expected, all of our statistical language models outperform the baseline by
a larger margin on polysemous words than on non-polysemous words. The margin
between graphical model representations and the WEB1T-n-GRAM-R model also in-
creases on polysemous words, except for the NB-R. The WEB1T-n-GRAM-R uses none
of the local context to decide which features to provide, and the NB-R uses only the
immediate left and right context, so both models ignore most of the context. In contrast,
the remaining graphical models use Viterbi decoding to take into account all tokens
in the surrounding sentence, which helps to explain their relative improvement over
WEB1T-n-GRAM-R on polysemous words.
The same behavior is evident for sparse words, as compared with non-sparse
words: All of the statistical language model representations outperform the baseline
by a larger margin on sparse words than not-sparse words, and all of the graphical
models perform better relative to the WEB1T-n-GRAM-R on sparse words than not-
sparse words. By reducing the feature space from millions of possible n-gram fea-
tures to L categorical features, these models ensure that each of their features will
be observed often in a reasonably sized training data set. Thus representations based
103
Computational Linguistics Volume 40, Number 1
Table 3
Graphical models consistently outperform n-gram models by a larger margin on sparse words
than not-sparse words, and by a larger margin on polysemous words than not-polysemous
words. One exception is the NB-R, which performs worse relative to WEB1T-n-GRAM-R on
polysemous words than non-polysemous words. For each graphical model representation,
we show the difference in performance between that representation and WEB1T-n-GRAM-R
in parentheses. For each representation, differences in accuracy on polysemous and
non-polysemous subsets were statistically significant at p < 0.01 using a two-tailed
Fisher?s exact test. Likewise for performance on sparse vs. non-sparse categories.
polysemous not polysemous sparse not sparse
tokens 159 4,321 463 12,194
TRAD-R 59.5 78.5 52.5 89.6
WEB1T-n-GRAM-R 68.2 85.3 61.8 94.0
NB-R 64.5 88.7 57.8 89.4
(-WEB1T-n-GRAM-R) (?3.7) (+3.4) (?4.0) (?4.6)
HMM-TOKEN-R 67.9 83.4 60.2 91.6
(-WEB1T-n-GRAM-R) (?0.3) (?1.9) (?1.6) (?2.4)
I-HMM-TOKEN-R 75.6 85.2 62.9 94.5
(-WEB1T-n-GRAM-R) (+7.4) (?0.1) (+1.1) (+0.5)
LATTICE-TOKEN-R 70.5 86.9 65.2 94.6
(-WEB1T-n-GRAM-R) (+2.3) (+1.6) (+3.4) (+0.6)
on graphical models help address two key issues in building representations for POS
tagging.
4.3.2 Domain Divergence. Besides sparsity and polysemy, Ben-David et al.?s (2007, 2010)
theoretical analysis of domain adaptation shows that the distance between two domains
under a representation R of the data is crucial for a good representation. We test their
predictions using learned representations.
Ben-David et al.?s (2007, 2010) analysis depends on a particular notion of distance,
the d1 divergence, that is computationally intractable to calculate. For our analysis, we
resort instead to two different computationally efficient approximations of this measure.
The first uses a more standard notion of distance: the Jensen-Shannon Divergence (dJS),
a distance metric for probability distributions:
dJS(p||q) = 12
?
i
[
pilog
( pi
mi
)
+ qilog
( qi
mi
)]
where mi =
pi+qi
2 .
Intuitively, we aim to measure the distance between two domains by measuring
whether features appear more commonly in one domain than in the other. For instance,
the biomedical domain is far from the newswire domain under the TRAD-R repre-
sentation because word-based features like protein, gene, and pathway appear far more
commonly in the biomedical domain than the newswire domain. Likewise, bank and
president appear far more commonly in newswire text. Since the d1 distance is related
to the optimal classifier for distinguishing two domains, it makes sense to measure the
distance by comparing the frequencies of these features: a classifier can easily use the
occurrence of words like bank and protein to accurately predict whether a given sentence
belongs to the newswire or biomedical domain.
104
Huang et al. Computational Linguistics
More formally, let S and T be two domains, and let f be a feature5 in representation
R?that is, a dimension of the image space of R. Let V be the set of possible values
that f can take on. Let US be an unlabeled sample drawn from S, and likewise for
UT. We first compute the relative frequencies of the different values of f in R(US) and
R(UT ), and then compute dJS between these empirical distributions. Let pf represent the
empirical distribution over V estimated from observations of feature f in R(US), and let
qf represent the same distribution estimated from R(UT ).
Definition 1
JS domain divergence for a feature or df (US, UT ) is the domain divergence between
domains S and T under feature f from representation R, and is given by
df (US, UT ) = dJS(pf ||qf )
For a multidimensional representation, we compute the full domain divergence as a
weighted sum over the domain divergences for its features. Because individual features
may vary in their relevance to a sequence-labeling task, we use weights to indicate
their importance to the overall distance between the domains. We set the weight wf
for feature f proportional to the L1 norm of CRF parameters related to f in the trained
POS tagger. That is, let ? be the CRF parameters for our trained POS tagger, and let
?f = {?l,v|l be the state for zi and v be the value for f}. We set wf =
||?f ||1
||?||1 .
Definition 2
JS Domain Divergence or dR(US, UT ), is the distance between domains S and T under
representation R, and is given by
dR(US, UT ) =
?
f
wf df (US, UT )
Blitzer (2008) uses a different notion of domain divergence to approximate the d1
divergence, which we also experimented with. He trains a CRF classifier on examples
labeled with a tag indicating which domain the example was drawn from. We refer to
this type of classifier as a domain classifier. Note that these should not be confused
with our CRFs used for POS tagging, which take as input examples which are labeled
with POS sequences. For the domain classifier, we tag every token from the WSJ domain
as 0, and every token from the biomedical domain as 1. Blitzer then uses the accuracy
of his domain classifier on a held-out test set as his measure of domain divergence. A
high accuracy for the domain classifier indicates that the representation makes the two
domains easy to separate, and thus high accuracy signifies a high domain divergence. To
measure domain divergence using a domain classifier, we trained our representations
on all of the unlabeled data for this task, as before. We then used 500 randomly sampled
sentences from the WSJ domain, and 500 randomly sampled biomedical sentences, and
labeled these with 0 for the WSJ data and 1 for the biomedical data. We measured
the error rate of our domain-classifier CRF as the average error rate across folds when
performing three-fold cross-validation on these 1,000 sentences.
5 For simplicity, the definition we provide here works only for discrete features, although it is possible to
extend this definition to continuous-valued features.
105
Computational Linguistics Volume 40, Number 1
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.32 0.37 0.42 0.47
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Divergence under the Representation 
LATTICE-R
I-HMM-R
Trad-R
Ngram-R
1 HMM 
7 HMMs 
8 layer LATTICE 
20 layer LATTICE 
Figure 5
Target-domain POS tagging accuracy for a model developed using a representation R correlates
strongly with lower JS domain divergence between WSJ and biomedical text under each
representation R. The correlation coefficients r2 for the linear regressions drawn in the
figure are both greater than 0.97.
Figure 5 plots the accuracies and JS domain divergences for our POS taggers.
Figure 6 shows the difference between target-domain error and source-domain error
as a function of JS domain divergence. Figures 7 and 8 show the same information,
except that the x axis plots the accuracy of a domain classifier as the way of mea-
suring domain divergence. These results give empirical support to Ben-David et al.?s
(2007, 2010) theoretical analysis: Smaller domain divergence?whether measured by
JS domain divergence or by the accuracy of a domain classifier?correlates strongly
with better target-domain accuracy. Furthermore, smaller domain divergence correlates
strongly with a smaller difference in the accuracy of the taggers on the source and
target domains.
Figure 6
Smaller JS domain divergence correlates with a smaller difference between target-domain error
and source-domain error.
106
Huang et al. Computational Linguistics
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Classification Accuracy 
Trad Rep
I-HMM
Ngram
PL-MRF
1 HMM 
7 HMMs 
20 layer PL-MRF 
8 layer PL-MRF 
Figure 7
Target-domain tagging accuracy decreases with the accuracy of a CRF domain classifier.
Intuitively, this means that training data from a source domain is less helpful for tagging in
a target domain when source-domain data is easy to distinguish from target-domain data.
Figure 8
Better domain classification correlates with a larger difference between target-domain error and
source-domain error.
Although both the JS domain divergence and the domain classifier provide only
approximations of the d1 metric for domain divergence, they agree very strongly:
In both cases, the LATTICE-TOKEN-R representations had the lowest domain diver-
gence, followed by the I-HMM-TOKEN-R representations, followed by TRAD-R, with
n-GRAM-R somewhere between LATTICE-TOKEN-R and I-HMM-TOKEN-R. The main
difference between the two metrics appears to be that the JS domain divergence gives
a greater domain divergence to the eight-layer LATTICE-TOKEN-R model and the
n-GRAM-R, placing them past the four- through eight-layer I-HMM-TOKEN-R represen-
tations. The domain classifier places these models closer to the other LATTICE-TOKEN-R
representations, just past the seven-layer I-HMM-TOKEN-R representation.
107
Computational Linguistics Volume 40, Number 1
The domain divergences of all models, using both techniques for measuring diver-
gence, remain significantly far from zero, even under the best representation. As a result,
there is ample room to experiment with even less-divergent representations of the two
domains, to see if they might yield ever-increasing target-domain accuracies. Note that
this is not simply a matter of adding more layers to the layered models. The I-HMM-
TOKEN-R model performed best with seven layers, and the eight-layer representation
had about the same accuracy and domain divergence as the five-layer model. This
may be explained by the fact that the I-HMM layers are trained independently, and so
additional layers may be duplicating other ones, and causing the supervised classifier
to overfit. But it also shows that our current methodology has no built-in technique
for constraining the domain divergence in our representations?the decrease in domain
divergence from our more sophisticated representations is a coincidental byproduct of
our training methodology, but there is no guarantee that our current mechanisms will
continue to decrease domain divergence simply by increasing the number of layers. An
important consideration for future research is to devise explicit learning mechanisms
that guide representations towards smaller domain divergences.
4.4 Domain Adaptation for Noun-Phrase Chunking and Chinese POS Tagging
We test the generality of our representations by using them for other tasks, domains, and
languages. Here, we report on further sequence-labeling tasks in a domain adaptation
setting: noun phrase chunking for adaptation from news text to biochemistry journals,
and POS tagging in Mandarin for a variety of domains. In the next section, we describe
the use of our representations in a weakly supervised information extraction task.
For chunking, the training set consists of the CoNLL 2000 shared task data for
source-domain labeled data (Sections 15?18 of the WSJ portion of the Penn Treebank,
labeled with chunk tags) (Tjong, Sang, and Buchholz 2000). For test data, we used
biochemistry journal data from the Open American National Corpus6 (OANC). One
of the authors manually labeled 198 randomly selected sentences (5,361 tokens) from
the OANC biochemistry text with noun-phrase chunk information.7 We focus on noun
phrase chunks because they are relatively easy to annotate manually, but contain a large
variety of open-class words that vary from domain to domain. The labeled training set
consists of 8,936 sentences and 211,726 tokens. Twenty-three percent of chunks in the
test set begin with an OOV word (especially adjective-noun constructions like ?aqueous
formation? and ?angular recess?), and 29% begin with a word seen at most twice in
training data; we refer to these as OOV chunks and rare chunks. For our unlabeled
data, we use 15,000 sentences (358,000 tokens; Sections 13?19) of the Penn Treebank
and 45,000 sentences (1,083,000 tokens) from the OANC?s biochemistry section. We
tested TRAD-R (augmented with features for automatically generated POS tags), LSA-R,
n-GRAM-R, NB-R, HMM-TOKEN-R, I-HMM-TOKEN-R (7 layers, which performed best
for POS tagging) and LATTICE-TOKEN-R (20 layers) representations.
Figure 9 shows our NP chunking results for this domain adaptation task. The
performance improvements for the HMM-based chunkers are impressive: LATTICE-
TOKEN-R reduces error by 57% with respect to TRAD-R, and comes close to state-of-the-
art results for chunking on newswire text. The results suggest that this representation
allows the CRF to generalize almost as well to out-of-domain text as in-domain text.
6 Available from http://www.anc.org/OANC/.
7 The labeled data for this experiment are available from the first author?s Web site.
108
Huang et al. Computational Linguistics
F1
on
B
io
ch
em
is
tr
y
Te
xt
0.72 0.74 
0.75 0.76 
0.84 
0.87 0.89 0.86 0.87 0.87 0.88 
0.91 
0.94 0.94 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Trad-R Ngram-R LSA-R NB-R HMM-R I-HMM-R LATTICE-R
OOV ALL
Freq: 0 1 2 all
Chunks: 284 39 39 1,258
R P R P R P R P
TRAD-R .74 .70 .85 .87 .79 .86 .86 .87
n-GRAM-R .74 .74 .85 .85 .79 .86 .87 .87
LSA-R .76 .74 .82 .83 .78 .85 .87 .88
NB-R .73 .78 .86 .73 .86 .75 .88 .88
HMM-TOKEN-R .80 .89 .92 .88 .92 .90 .91 .90
I-HMM-TOKEN-R .90 .86 .92 .95 .87 .97 .95 .92
LATTICE-TOKEN-R .92 .85 .94 .95 .87 .97 .95 .93
Figure 9
On biomedical journal data from the OANC, our best NP chunker outperforms the baseline
CRF chunker by 0.17 F1 on chunks that begin with OOV words, and by 0.08 on all chunks. The
table shows performance breakdowns (recall and precision) for chunks whose first word has
frequency 0, 1, and 2 in training data, and the number of chunks in test data that fall into each
of these categories.
Improvements are greatest on OOV and rare chunks, where LATTICE-TOKEN-R made
absolute improvements over TRAD-R by 0.17 and 0.09 F1, respectively. Improvements
for the single-layer HMM-TOKEN-R were smaller but still significant: 36% relative re-
duction in error overall, and 32% for OOV chunks.
The improved performance from our HMM-based chunker caused us to wonder
how well the chunker could work without some of its other features. We removed all
tag features and orthographic features and all features for word types that appear fewer
than 20 times in training. This chunker still achieves 0.91 F1 on OANC data, and 0.93
F1 on WSJ data (Section 20), outperforming the TRAD-R system in both cases. It has
only 20% as many features as the baseline chunker, greatly improving its training time.
Thus these features are more valuable to the chunker than features from automatically
produced tags and features for all but the most common words.
For Chinese POS tagging, we use text from the UCLA Corpus of Written Chinese
(Tao and Xiao 2007), which is part of the Lancaster Corpus of Mandarin Chinese
(LCMC). The UCLA Corpus consists of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres (see Table 4). We use gold-standard word segmentation labels
for training and testing. The LCMC tagset consists of 50 Chinese POS tags. On average,
each genre contains 5,284 word tokens, for a total of 68,695 tokens among all genres. We
use the ?news? genre as our source domain, which we use for training and development
109
Computational Linguistics Volume 40, Number 1
Table 4
POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations
outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target
domains, * indicates the performance is statistically significantly better than the Stanford and
TRAD-R baselines at p < 0.05, using a two-tailed ?2 test; ** indicates significance at p < 0.01.
On the news domain, the Stanford tagger is significantly different from all other systems
using a two-tailed ?2 test with p < 0.01.
Domain Stanford TRAD NGR LSA NB HMM I-H LAT
lore 88.4 84.0 84.2 85.3 85.3 89.7 89.9 90.1*
religion 83.5 79.1 79.4 79.8 80.0 85.2 85.6 85.9*
humour 89.0 84.2 84.5 86.2 86.8 89.6 89.6 89.9*
general-fic 87.5 84.5 85.0 85.3 85.7 89.4 89.7 89.9*
essay 88.4 83.2 83.7 84.0 84.3 89.0 89.1 90.1*
mystery 87.4 82.4 83.4 84.3 85.3 90.1 91.1 91.3**
romance 87.5 84.2 84.5 85.3 86.1 89.0 89.5 89.8**
science-fic 88.6 82.1 82.5 83.0 83.0 87.0 88.3 88.6
skills 82.7 77.3 77.7 78.2 78.4 84.9 85.0 85.1**
science 86.0 82.0 82.3 82.4 82.4 87.8 87.8 87.9*
adventure-fic 82.1 74.3 75.2 76.1 77.8 81.7 82.0 82.2
report 91.7 84.2 85.1 85.3 86.1 91.9 91.9 91.9
news 98.8** 96.9 92.3 93.4 94.3 94.2 97.0 97.1
all but news 87.0 81.2 82.0 82.8 83.6 88.1 88.4 88.8**
all domains 88.7 83.2 83.6 84.4 85.5 89.5 89.7 90.0**
data. For test data, we randomly select 20% of every other genre. For our unlabeled
data, we use all of the ?news? text, plus the remaining 80% of the texts from the other
genres. As before, we replace hapax legomena in the unlabeled data with the special
symbol *UNKNOWN*, and do the same for word types in the labeled test sets that never
appear in our unlabeled training texts. We compare against a state-of-the-art Chinese
POS tagger for in-domain text, the CRF-based Stanford tagger (Tseng, Jurafsky, and
Manning 2005). We obtained the code for this tagger,8 and retrained it on our training
data set.
The Chinese POS tagging results are shown in Table 4. The LATTICE-TOKEN-R
outperforms the state-of-the-art Stanford tagger on all target domains. Overall, on all
out-of-domain tests, LATTICE-TOKEN-R provides a relative reduction in error of 13.8%
compared with the Stanford tagger. The best performance is on the ?mystery? domain,
where the LATTICE-TOKEN-R model reaches 91.3% accuracy, a 3.9 percentage points
improvement over the Stanford tagger. Its performance on the in-domain ?news? test set
is significantly worse (1.7 percentage points) than the Stanford tagger, suggesting that
the Stanford tagger relies on domain-dependent features that are helpful for tagging
news, but not for tagging in general. The LATTICE-TOKEN-R?s accuracy is still signifi-
cantly worse on out-of-domain text than in-domain text, but the gap between the two
(8.3 percentage points) is better than the gap for the Stanford tagger (11.8 percentage
points). We believe that the lower out-of-domain performance of our Chinese POS
tagger, compared with our English POS tagger and our chunker, was at least in part
due to having far less unlabeled text available for this task.
8 Available at http://nlp.stanford.edu/software/tagger.shtml.
110
Huang et al. Computational Linguistics
5. Information Extraction Experiments
In this section, we evaluate our learned representations on their ability to capture
semantic, rather than syntactic, information. Specifically, we investigate a set-expansion
task in which we?re given a corpus and a few ?seed? noun phrases from a semantic
category (e.g., Superheroes), and our goal is to identify other examples of the category
in the corpus. This is a different type of weakly supervised task from the earlier domain
adaptation tasks because we are given only a handful of positive examples from a cate-
gory, rather than a large sample of positively and negatively labeled training examples
from a separate domain.
Existing set-expansion techniques utilize the distributional hypothesis: Candidate
noun phrases for a given semantic class are ranked based on how similar their contex-
tual distributions are to those of the seeds. Here, we measure how performance on the
set-expansion task varies when we employ different representations for the contextual
distributions.
5.1 Methods
The set-expansion task we address is formalized as follows. Given a corpus, a set of
seeds from some semantic category C, and a separate set of candidate phrases P, output
a ranking of the phrases in P in decreasing order of likelihood of membership in the
semantic category C.
For any given representation R, the set-expansion algorithm we investigate is
straightforward: We rank candidate phrases in increasing order of the distance between
their feature vectors and those of the seeds. The particular distance metrics utilized are
detailed subsequently.
Because set expansion is performed at the level of word types rather than to-
kens, it requires type-based representations. We compare HMM-TYPE-R, n-GRAM-R,
LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment. We used a 25-state HMM,
and the LATTICE-TYPE-R as described in the previous section. Following previous set-
expansion experiments with n-grams (Ahuja and Downey 2010), we use a trigram
model with Kneser-Ney smoothing for n-GRAM-R.
The distances between the candidate phrases and the seeds for HMM-TYPE-R,
n-GRAM-R, and LATTICE-TYPE-R representations are calculated by first creating a
prototypical ?seed feature vector? equal to the mean of the feature vectors for each
of the seeds in the given representation. Then, we rank candidate phrases in order
of increasing distance between their feature vector and the seed feature vector. As a
distance measure between vectors (in this case, probability distributions), we compute
the average of five standard distance measures, including KL and JS divergence, and
cosine, Euclidean, and L1 distance. In experiments, we found that improving upon
this simple averaging was not easy?in fact, tuning a weighted average of the distance
measures for each representation did not improve results significantly on held-out data.
For Brown clusters, we use prefixes of all possible lengths as features. We define
the similarity between two Brown representation feature vectors to be the number of
features they share in common (this is equivalent to the length of the longest common
prefix between the two original Brown cluster labels). The candidate phrases are then
ranked in decreasing order of the sum of their similarity scores to each of the seeds. We
experimented with normalizing the similarity scores by the longer of the two vector
lengths, and found this to decrease results slightly. We use unnormalized (integer)
similarity scores for Brown clusters in our experiments.
111
Computational Linguistics Volume 40, Number 1
5.2 Data Sets
We utilized a set of approximately 100,000 sentences of Web text, joining multi-word
named entities in the corpus into single tokens using the Lex algorithm (Downey,
Broadhead, and Etzioni 2007). This process enables each named entity (the focus of the
set-expansion experiments) to be treated as a single token, with a single representation
vector for comparison. We developed all word type representations using this corpus.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia
?listOf? pages from Pantel et al. (2009) and augmented these with our own manually
defined categories, such that each list contained at least ten distinct examples occurring
in our corpus. In all, we had 432 examples across 16 distinct categories such as Coun-
tries, Greek Islands, and Police TV Dramas.
5.3 Results
For each semantic category, we tested five different random selections of five seed
examples, treating the unselected members of the category as positive examples, and
all other candidate phrases as negative examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 5. All representations improve performance over
a random baseline, equal to the average AUC over five random orderings for each
category, and the graphical models outperform the n-gram representation. I-HMM-
TYPE-R and Brown clustering in the particular case of 1,000 clusters perform best, with
HMM-TYPE-R performing nearly as well. Brown clusters give somewhat lower results
as the number of clusters varies.
As with POS tagging, we expect that language model representations improve
performance on the IE task by providing informative features for sparse word types.
However, because the IE task classifies word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous word types. To test these hypotheses,
we measured how IE performance changed in sparse or polysemous settings. We identi-
fied polysemous categories as those for which fewer than 90% of the category members
had the category as a clear dominant sense (estimated manually); other categories were
considered non-polysemous. Categories whose members had a median number of
occurrences in the corpus of less than 30 were deemed sparse, and others non-sparse.
Table 5
I-HMM-TYPE-R outperforms the other methods, improving performance over a random
baseline by twice as much as either n-GRAM-R or LATTICE-TYPE-R.
model AUC
I-HMM-TYPE-R 0.18
HMM-TYPE-R 0.17
BROWN-TYPE-R-3200 0.16
BROWN-TYPE-R-1000 0.18
BROWN-TYPE-R-320 0.15
BROWN-TYPE-R-100 0.13
LATTICE-TYPE-R 0.11
n-GRAM-R baseline 0.10
Random baseline 0.10
112
Huang et al. Computational Linguistics
Table 6
Graphical models as representations for IE consistently perform better relative to n-gram models
on sparse words, but not necessarily polysemous words.
polysemous not-polysemous sparse not-sparse
types 222 210 266 166
categs. 12 4 13 3
n-GRAM-R 0.07 0.17 0.06 0.25
LATTICE-TYPE-R 0.09 0.15 0.1 0.19
-n-GRAM-R +0.02 ?0.02 +0.04 ?0.06
HMM-TYPE-R 0.14 0.26 0.15 0.32
-n-GRAM-R +0.07 +0.09 +0.09 +0.07
IE performance on these subsets of the data are shown in Table 6. Both graphical
model representations outperform the n-gram representation more on sparse words, as
expected. For polysemy, the picture is mixed: The LATTICE-TYPE-R outperforms
n-GRAM-R on polysemous categories, whereas HMM-TYPE-R?s performance advan-
tage over n-GRAM-R decreases.
One surprise on the IE task is that the LATTICE-TYPE-R performs significantly less
well than the HMM-TYPE-R, whereas the reverse is true on POS tagging. We suspect
that the difference is due to the issue of classifying types vs. tokens. Because of their
more complex structure, PL-MRFs tend to depend more on transition parameters than
do HMMs. Furthermore, our decision to train the PL-MRFs using contrastive estimation
with a neighborhood that swaps consecutive pairs of words also tends to emphasize
transition parameters. As a result, we believe the posterior distribution over latent states
given a word type is more informative in our HMM model than the PL-MRF model.
We measured the entropy of these distributions for the two models, and found that
H(PPL-MRF(y|x = w)) = 9.95 bits, compared with H(PHMM(y|x = w)) = 2.74 bits, which
supports the hypothesis that the drop in the PL-MRF?s performance on IE is due to its
dependence on transition parameters. Further experiments are warranted to investigate
this issue.
5.4 Testing the Language Model Representation Hypothesis in IE
The language model representation hypothesis (Section 2) suggests that all else being
equal, more accurate language models will provide features that lead to better perfor-
mance on NLP tasks. Here, we test this hypothesis on the set expansion IE task.
Figures 10 and 11 show how the performance of the HMM-TYPE-R varies with the
language modeling accuracy of the underlying HMM. Language modeling accuracy
is measured in terms of perplexity on held-out text. Here, we use set expansion data
sets from previous work (Ahuja and Downey 2010). The first two are composed of
extractions from the TextRunner information extraction system (Banko et al. 2007) and
are denoted as Unary (361 examples) and Binary (265 examples). The second, Wikipedia
(2,264 examples), is a sample of Wikipedia concept names. We evaluate the performance
of several different trained HMMs with numbers of latent states K ranging from 5 to
1,600 (to help illustrate how IE and LM performance varies even when model capacity
is fixed, we include three distinct models with K = 100 states trained separately over
the full corpus). We used a distributed implementation of HMM training and corpus
113
Computational Linguistics Volume 40, Number 1
K = 5
K = 10
K = 25 K = 50
K = 100
K = 100
K = 100
K = 200
K = 400
Figure 10
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy of
the HMM varies, on TextRunner data sets. IE accuracy (in terms of area under the precision-recall
curve) tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
5 10
25 50
100
100
100
200 400
5
5
25
2550
50
100
200
100
200
800
1600 400
Figure 11
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy
of the HMM varies on the Wikipedia data set. Number labels indicate the number of latent
states K, and performance is shown for three training corpus sizes (the full corpus consists of
approximately 60 million tokens). IE accuracy (in terms of area under the precision-recall curve)
tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
partitioning techniques (Yang, Yates, and Downey 2013) to enable training of our larger
capacity HMM models on large data sets.
The results provide support for the language model representation hypothesis,
showing that IE performance does tend to improve as language model perplexity
decreases. On the smaller Unary and Binary sets (Figure 10), although IE accuracy
114
Huang et al. Computational Linguistics
does decrease for the lowest-perplexity models, overall language model perplexity
exhibits a negative correlation with IE area under the precision-recall curve (the Pearson
correlation coefficient is ?0.18 for Unary, and ?0.28 for Binary). For Wikipedia (Fig-
ure 11), the trend is more consistent, with IE performance increasing monotonically
as perplexity decreases for models trained on the full training corpus (the Pearson
correlation coefficient is ?0.90).
Figure 11 also illustrates how LM and IE performance changes as the amount
of training text varies. In general, increasing the training corpus size increases IE
performance and decreases perplexity. Over all data points in the figure, IE perfor-
mance correlates most strongly with model perplexity (?0.68 Pearson correlation, ?0.88
Spearman correlation), followed by corpus size (0.66, 0.71) and model capacity (?0.05,
0.38). The small negative Pearson correlation between model capacity and IE perfor-
mance is primarily due to the model with 1,600 states trained on 4% of the corpus.
This model has a large parameter space and sparse training data, and thus suffers from
overfitting in terms of both model perplexity and IE performance. If we ignore this
overfit model, the Pearson correlation between model capacity and IE performance for
the other models in the Figure is 0.24.
Our results show that IE based on distributional similarity tends to improve as the
quality of the latent variable model used to measure distributional similarity improves.
A similar trend was exhibited in our previous work (Ahuja and Downey 2010); here, we
extend the previous results to models with more latent states and a larger, more reliable
test set (Wikipedia). The results suggest that scaling up the training of latent variable
models to utilize larger training corpora and more latent states may be a promising
direction for improving IE capabilities.
6. Conclusion and Future Work
Our study of representation learning demonstrates that by using statistical language
models to aggregate information across many unannotated examples, it is possible to
find accurate distributional representations that can provide highly informative features
to weakly supervised sequence labelers and named-entity classifiers. For both domain
adaptation and weakly supervised set expansion, our results indicate that graphical
models outperform n-gram models as representations, in part for their greater ability to
handle sparsity and polysemy. Our IE task provides important evidence to support the
Language Model Representation Hypothesis, showing that the AUC of the IE system
correlates more with language model perplexity than the size of the training data or
the capacity of the language model. Finally, our sequence labeling experiments provide
empirical evidence in support of theoretical work on domain adaptation, showing that
target-domain tagging accuracy is highly correlated with two different measures of
domain divergence.
Representation learning remains a promising area for finding further improve-
ments in various NLP tasks. The representations we have described are trained in
an unsupervised fashion, so a natural extension is to investigate supervised or semi-
supervised representation-learning techniques. As mentioned previously, our current
techniques have no built-in methods for enforcing that they provide similar features in
different domains; devising a mechanism that enforces this could allow for less domain-
divergent and potentially more accurate representations. We have considered sequence
labeling, but another promising direction is to apply these techniques to more complex
structured prediction tasks, like parsing or relation extraction. Our current approach
to sequence labeling requires retraining of a CRF for every new domain; incremental
115
Computational Linguistics Volume 40, Number 1
retraining techniques for new domains would speed up the process. Finally, models
that combine our representation learning approach with instance weighting and other
forms of supervised domain adaptation may take better advantage of labeled data in
target domains, when it is available.
Acknowledgments
This material is based on work supported
by the National Science Foundation under
grant no. IIS-1065397.
References
Ahuja, Arun and Doug Downey. 2010.
Improved extraction assessment through
better language models. In Proceedings of
the Annual Meeting of the North American
Chapter of the Association of Computational
Linguistics (NAACL-HLT), pages 225?228,
Los Angeles, CA.
Ando, Rie Kubota and Tong Zhang. 2005.
A high-performance semi-supervised
learning method for text chunking.
In Proceedings of the ACL, pages 1?9,
Ann Arbor, MI.
Banko, Michele, Michael J. Cafarella,
Stephen Soderland, Matt Broadhead, and
Oren Etzioni. 2007. Open information
extraction from the web. In Proceedings of
the IJCAI, pages 2670?2676, Hyderabad.
Banko, Michele and Robert C. Moore.
2004. Part of speech tagging in context.
In Proceedings of the COLING, pages
556?561, Geneva.
Ben-David, Shai, John Blitzer, Koby
Crammer, Alex Kulesza, Fernando Pereira,
and Jenn Wortman. 2010. A theory of
learning from different domains. Machine
Learning, 79:151?175.
Ben-David, Shai, John Blitzer, Koby
Crammer, and Fernando Pereira. 2007.
Analysis of representations for domain
adaptation. In Advances in Neural
Information Processing Systems 20,
pages 127?144, Vancouver.
Bengio, Yoshua. 2008. Neural net language
models. Scholarpedia, 3(1):3,881.
Bengio, Yoshua, Re?jean Ducharme, Pascal
Vincent, and Christian Janvin. 2003.
A neural probabilistic language model.
Journal of Machine Learning Research,
3:1,137?1,155.
Bengio, Yoshua, Jerome Louradour,
Ronan Collobert, and Jason Weston.
2009. Curriculum learning. In Proceedings
of the International Conference on Machine
Learning (ICML), pages 41?48,
Montreal.
Bikel, Daniel M. 2004a. A distributional
analysis of a lexicalized statistical
parsing model. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 182?189,
Barcelona.
Bikel, Daniel M. 2004b. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993?1,022.
Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
Blitzer, John, Koby Crammer, Alex Kulesza,
Fernando Pereira, and Jenn Wortman.
2007. Learning bounds for domain
adaptation. In Advances in Neural
Information Processing Systems,
pages 129?136, Vancouver.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Association for Computational Linguistics
(ACL), pages 40?47, Prague.
Blitzer, John, Ryan McDonald, and
Fernando Pereira. 2006. Domain
adaptation with structural correspondence
learning. In Proceedings of the EMNLP,
pages 120?128, Sydney.
Bodlaender, Hans L. 1988. Dynamic
programming on graphs with bounded
treewidth. In Proceedings of the 15th
International Colloquium on Automata,
Languages and Programming,
pages 105?118, Tampere.
Brants, Thorsten and Alex Franz. 2006.
Web 1t 5-gram version 1. www.ldc.
upenn.edu/catalog/.
Brown, Peter F., Vincent J. Della Pietra,
Peter V. deSouza, Jenifer C. Lai, and
Robert L. Mercer. 1992. Class-based
n-gram models of natural language.
Computational Linguistics, 18:467?479.
Candito, Marie and Benoit Crabbe. 2009.
Improving generative statistical parsing
with semi-supervised word clustering.
In Proceedings of the IWPT, pages 138?141,
Paris.
116
Huang et al. Computational Linguistics
Chan, Yee Seng and Hwee Tou Ng. 2006.
Estimating class priors in domain
adaptation for word sense disambiguation.
In Proceedings of the Association for
Computational Linguistics (ACL),
pages 89?96, Sydney.
Chelba, Ciprian and Alex Acero. 2004.
Adaptation of maximum entropy
classifier: Little data can help a lot.
In Proceedings of the EMNLP,
pages 285?292, Barcelona.
Collobert, Robert and Jason Weston. 2008. A
unified architecture for natural language
processing: Deep neural networks with
multitask learning. In Proceedings of the
International Conference on Machine Learning
(ICML), pages 160?167, Helsinki.
Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,
and Yong Yu. 2007. Transferring naive
Bayes classifiers for text classification.
In Proceedings of the National Conference
on Artificial Intelligence (AAAI),
pages 540?545, Vancouver.
Darroch, J. N., S. L. Lauritzen, and
T. P. Speed. 1980. Markov fields and
log-linear interaction models for
contingency tables. The Annals of
Statistics, 8(3):522?539.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
ACL, pages 256?263, Prague.
Daume? III, Hal, Abhishek Kumar, and
Avishek Saha. 2010. Frustratingly easy
semi-supervised domain adaptation.
In Proceedings of the ACL Workshop
on Domain Adaptation (DANLP),
pages 53?59, Uppsala.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society of Information Science,
41(6):391?407.
Dempster, Arthur, Nan Laird, and Donald
Rubin. 1977. Likelihood from incomplete
data via the EM algorithm. Journal of
the Royal Statistical Society, Series B,
39(1):1?38.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the latent words language
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 21?29,
Singapore.
Dhillon, Paramveer S., Dean Foster, and
Lyle Ungar. 2011. Multi-View Learning of
Word Embeddings via CCA. In Proceedings
of the Advances in Neural Information
Processing Systems (NIPS), volume 24,
pages 886?874, Granada.
Downey, Doug, Matthew Broadhead, and
Oren Etzioni. 2007. Locating complex
named entities in web text. In Proceedings
of the 20th International Joint Conference
on Artificial Intelligence (IJCAI 2007),
pages 2,733?2,739, Hyderabad.
Downey, Doug, Stefan Schoenmackers, and
Oren Etzioni. 2007. Sparse information
extraction: Unsupervised language models
to the rescue. In Proceedings of the ACL,
pages 696?703, Prague.
Dredze, Mark and Koby Crammer. 2008.
Online methods for multi-domain learning
and adaptation. In Proceedings of EMNLP,
pages 689?697, Honolulu, HI.
Dredze, Mark, Alex Kulesza, and Koby
Crammer. 2010. Multi-domain learning
by confidence weighted parameter
combination. Machine Learning,
79:123?149.
Finkel, Jenny Rose and Christopher D.
Manning. 2009. Hierarchical Bayesian
domain adaptation. In Proceedings of
HLT-NAACL, pages 602?610, Boulder, CO.
Fu?rstenau, Hagen and Mirella Lapata. 2009.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Ghahramani, Zoubin and Michael I. Jordan.
1997. Factorial hidden Markov models.
Machine Learning, 29(2-3):245?273.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Conference on
Empirical Methods in Natural Language
Processing, pages 167?202, Pittsburgh, PA.
Goldwater, Sharon and Thomas L. Griffiths.
2007. A fully Bayesian approach to
unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages 744?751,
Prague.
Grac?a, Joa?o V., Kuzman Ganchev, Ben Taskar,
and Fernando Pereira. 2009. Posterior vs.
parameter sparsity in latent variable
models. In Proceedings of the Neural
Information Processing Systems Conference
(NIPS), pages 664?672, Vancouver.
Harris, Z. 1954. Distributional structure.
Word, 10(23):146?162.
Hindle, Donald. 1990. Noun classification
from predicage-argument structures.
In Proceedings of the ACL, pages 268?275,
Pittsburgh, PA.
117
Computational Linguistics Volume 40, Number 1
Honkela, Timo. 1997. Self-organizing
maps of words for natural language
processing applications. In Proceedings
of the International ICSC Symposium on
Soft Computing, pages 401?407, Millet,
Alberta.
Huang, Fei and Alexander Yates. 2009.
Distributional representations for
handling sparsity in supervised sequence
labeling. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 495?503,
Singapore.
Huang, Fei and Alexander Yates. 2010.
Exploring representation-learning
approaches to domain adaptation. In
Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language
Processing (DANLP), pages 23?30, Uppsala.
Huang, Fei, Alexander Yates, Arun Ahuja,
and Doug Downey. 2011. Language
models as representations for weakly
supervised NLP tasks. In Proceedings
of the Conference on Natural Language
Learning (CoNLL), pages 125?134,
Portland, OR.
Jiang, Jing and ChengXiang Zhai. 2007a.
Instance weighting for domain
adaptation in NLP. In Proceedings
of ACL, pages 264?271, Prague.
Jiang, Jing and ChengXiang Zhai. 2007b. A
two-stage approach to domain adaptation
for statistical classifiers. In Proceedings of
the Conference on Information and Knowledge
Management (CIKM), pages 401?410, Lisbon.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers. In Proceedings of
the EMNLP, pages 296?305, Prague.
Kaski, S. 1998. Dimensionality reduction
by random mapping: Fast similarity
computation for clustering. In
Proceedings of the IJCNN, pages 413?418,
Washington, DC.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings of
the Annual Meeting of the Association of
Computational Linguistics (ACL),
pages 595?603, Columbus, OH.
Lin, Dekang and Xiaoyun Wu. 2009. Phrase
clustering for discriminative learning.
In Proceedings of the ACL-IJCNLP,
pages 1,030?1,038, Singapore.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory method for large scale
optimization. Mathematical Programming B,
45(3):503?528.
Mansour, Y., M. Mohri, and
A. Rostamizadeh. 2009. Domain
adaptation with multiple sources.
In Proceedings of the Advances in Neural
Information Processing Systems,
pages 1,041?1,048, Vancouver.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Martin, Sven, Jorg Liermann, and Hermann
Ney. 1998. Algorithms for bigram and
trigram word clustering. Speech
Communication, 24:19?37.
McClosky, David. 2010. Any Domain Parsing:
Automatic Domain Adaptation for Parsing.
Ph.D. thesis, Brown University,
Providence, RI.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2010. Automatic domain
adaptation for parsing. In North American
Chapter of the Association for Computational
Linguistics - Human Language Technologies
2010 Conference (NAACL-HLT 2010),
pages 28?36, Los Angeles, CA.
Miller, Scott, Jethran Guinness, and
Alex Zamanian. 2004. Name tagging with
word clusters and discriminative training.
In Proceedings of the Annual Meeting of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL),
pages 337?342, Boston, MA.
Mnih, Andriy and Geoffrey Hinton. 2007.
Three new graphical models for statistical
language modelling. In Proceedings of
the 24th International Conference on
Machine Learning, pages 641?648,
Corvallis, OR.
Mnih, Andriy and Geoffrey Hinton. 2009.
A scalable hierarchical distributed
language model. In Proceedings of the
Neural Information Processing Systems
(NIPS), pages 1,081?1,088, Vancouver.
Mnih, Andriy, Zhang Yuecheng, and
Geoffrey Hinton. 2009. Improving a
statistical language model through
non-linear prediction. Neurocomputing,
72(7-9):1414?1418.
Morin, Frederic and Yoshua Bengio. 2005.
Hierarchical probabilistic neural network
language model. In Proceedings of the
International Workshop on Artificial
Intelligence and Statistics, pages 246?252,
Barbados.
Pantel, Patrick, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu,
and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set
expansion. In Proceedings of the EMNLP,
pages 938?947, Singapore.
118
Huang et al. Computational Linguistics
PennBioIE. 2005. Mining the bibliome
project. http://bioie.ldc.upenn.edu/.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 183?190, Columbus, OH.
Pradhan, Sameer, Wayne Ward, and James H.
Martin. 2007. Towards robust semantic role
labeling. In Proceedings of NAACL-HLT,
pages 556?563, Rochester, NY.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285.
Raina, Rajat, Alexis Battle, Honglak Lee,
Benjamin Packer, and Andrew Y. Ng.
2007. Self-taught learning: Transfer
learning from unlabeled data.
In Proceedings of the 24th International
Conference on Machine Learning,
pages 759?766, Corvallis, OR.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Conference on Natural Language Learning
(CoNLL), pages 147?155, Boulder, CO.
Ritter, H. and T. Kohonen. 1989.
Self-organizing semantic maps.
Biological Cybernetics, 61(4):241?254.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Synactic Theory: A Formal
Introduction. CSLI Publications, Stanford,
CA, second edition.
Sahlgren, Magnus. 2001. Vector-based
semantic analysis: Representing word
meanings based on random labels.
In Proceedings of the Semantic Knowledge
Acquisition and Categorization Workshop,
pages 1?12, Helsinki.
Sahlgren, Magnus. 2005. An introduction
to random indexing. In Methods and
Applications of Semantic Indexing Workshop
at the 7th International Conference on
Terminology and Knowledge Engineering
(TKE), 87:1?9.
Sahlgren, Magnus. 2006. The word-space
model: Using distributional analysis to
represent syntagmatic and paradigmatic
relations between words in high-dimensional
vector spaces. Ph.D. thesis, Stockholm
University.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill.
Satpal, Sandeep and Sunita Sarawagi.
2007. Domain adaptation of conditional
probability models via feature subsetting.
In Proceedings of ECML/PKDD,
pages 224?235, Warsaw.
Sekine, Satoshi. 1997. The domain
dependence of parsing. In Proceedings of
Applied Natural Language Processing
(ANLP), pages 96?102, Washington, DC.
Shen, Libin, Giorgio Satta, and Aravind K.
Joshi. 2007. Guided learning for
bidirectional sequence classification.
In Proceedings of the ACL, pages 760?767,
Prague.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 354?362,
Ann Arbor, MI.
Sutton, Charles, Andrew McCallum, and
Khashayar Rohanimanesh. 2007. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. Journal of
Machine Learning Research, 8:693?723.
Suzuki, Jun and Hideki Isozaki. 2008.
Semi-supervised sequential labeling and
segmentation using giga-word scale
unlabeled data. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-HLT),
pages 665?673, Columbus, OH.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras,
and Michael Collins. 2009. An empirical
study of semi-supervised structured
conditional models for dependency
parsing. In Proceedings of the EMNLP,
pages 551?560, Singapore.
Tao, Hongyin and Richard Xiao. 2007.
The UCLA Chinese corpus. UCREL.
www.lancaster.ac.uk/fass/projects/
corpus/UCLA/.
Tjong, Erik F., Kim Sang, and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference
on Computational Natural Language
Learning, pages 127?132, Lisbon.
Toutanova, Kristina and Mark Johnson.
2007. A Bayesian LDA-based model for
semi-supervised part-of-speech
tagging. In Proceedings of the NIPS,
pages 1,521?1,528, Vancouver.
Tseng, Huihsin, Daniel Jurafsky, and
Christopher Manning. 2005.
Morphological features help POS
tagging of unknown words across
language varieties. In Proceedings
of the Fourth SIGHAN Workshop,
pages 32?39, Jeju Island.
119
Computational Linguistics Volume 40, Number 1
Turian, Joseph, James Bergstra, and
Yoshua Bengio. 2009. Quadratic
features and deep architectures for
chunking. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human
Language Technologies (NAACL HLT),
pages 245?248, Boulder, CO.
Turian, Joseph, Lev Ratinov, and Yoshua
Bengio. 2010. Word representations:
A simple and general method for
semi-supervised learning. In Proceedings
of the Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 384?394, Uppsala.
Turney, Peter D. and Patrick Pantel. 2010.
From frequency to meaning: Vector
space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Ushioda, Akira. 1996. Hierarchical clustering
of words. In Proceedings of the International
Conference on Computational Linguistics
(COLING), pages 1,159?1,162, Copenhagen.
Va?yrynen, Jaakko and Timo Honkela. 2004.
Word category maps based on emergent
features created by ICA. In Proceedings of
the STePs 2004 Cognition + Cybernetics
Symposium, pages 173?185, Tikkurila.
Va?yrynen, Jaakko and Timo Honkela. 2005.
Comparison of independent component
analysis and singular value decomposition
in word context analysis. In Proceedings
of the International and Interdisciplinary
Conference on Adaptive Knowledge
Representation and Reasoning (AKRR),
pages 135?140, Espoo.
Va?yrynen, Jaakko, Timo Honkela, and
Lasse Lindqvist. 2007. Towards explicit
semantic features using independent
component analysis. In Proceedings of the
Workshop Semantic Content Acquisition
and Representation (SCAR), pages 20?27,
Stockholm.
Weston, Jason, Frederic Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Yang, Yi, Alexander Yates, and Doug
Downey. 2013. Overcoming the memory
bottleneck in distributed training
of latent variable models of text.
In Proceedings of the NAACL-HLT,
pages 579?584, Atlanta, GA.
Zhao, Hai, Wenliang Chen, Chunyu Kit,
and Guodong Zhou. 2009. Multilingual
dependency learning: A huge feature
engineering method to semantic
dependency parsing. In Proceedings of the
CoNLL 2009 Shared Task, pages 55?60,
Boulder, CO.
120
Proceedings of NAACL-HLT 2013, pages 579?584,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Overcoming the Memory Bottleneck in Distributed Training of
Latent Variable Models of Text
Yi Yang
Northwestern University
Evanston, IL
yiyang@eecs.northwestern.edu
Alexander Yates
Temple University
Philadelphia, PA
yates@temple.edu
Doug Downey
Northwestern University
Evanston, IL
ddowney@eecs.northwestern.edu
Abstract
Large unsupervised latent variable models
(LVMs) of text, such as Latent Dirichlet Al-
location models or Hidden Markov Models
(HMMs), are constructed using parallel train-
ing algorithms on computational clusters. The
memory required to hold LVM parameters
forms a bottleneck in training more powerful
models. In this paper, we show how the mem-
ory required for parallel LVM training can
be reduced by partitioning the training corpus
to minimize the number of unique words on
any computational node. We present a greedy
document partitioning technique for the task.
For large corpora, our approach reduces mem-
ory consumption by over 50%, and trains the
same models up to three times faster, when
compared with existing approaches for paral-
lel LVM training.
1 Introduction
Unsupervised latent variable models (LVMs) of text
are utilized extensively in natural language process-
ing (Griffiths and Steyvers, 2004; Ritter et al, 2010;
Downey et al, 2007; Huang and Yates, 2009; Li and
McCallum, 2005). LVM techniques include Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), Hid-
den Markov Models (HMMs) (Rabiner, 1989), and
Probabilistic Latent Semantic Analysis (Hofmann,
1999), among others.
LVMs become more predictive as they are trained
on more text. However, training LVMs on mas-
sive corpora introduces computational challenges, in
terms of both time and space complexity. The time
complexity of LVM training has been addressed
through parallel training algorithms (Wolfe et al,
2008; Chu et al, 2006; Das et al, 2007; Newman
et al, 2009; Ahmed et al, 2012; Asuncion et al,
2011), which reduce LVM training time through the
use of large computational clusters.
However, the memory cost for training LVMs re-
mains a bottleneck. While LVM training makes se-
quential scans of the corpus (which can be stored on
disk), it requires consistent random access to model
parameters. Thus, the model parameters must be
stored in memory on each node. Because LVMs in-
clude a multinomial distribution over words for each
latent variable value, the model parameter space in-
creases with the number of latent variable values
times the vocabulary size. For large models (i.e.,
with many latent variable values) and large cor-
pora (with large vocabularies), the memory required
for training can exceed the limits of the commod-
ity servers comprising modern computational clus-
ters. Because model accuracy tends to increase with
both corpus size and model size (Ahuja and Downey,
2010; Huang and Yates, 2010), training accurate lan-
guage models requires that we overcome the mem-
ory bottleneck.
We present a simple technique for mitigating the
memory bottleneck in parallel LVM training. Ex-
isting parallelization schemes begin by partitioning
the training corpus arbitrarily across computational
nodes. In this paper, we show how to reduce mem-
ory footprint by instead partitioning the corpus to
minimize the number of unique words on each node
(and thereby minimize the number of parameters the
node must store). Because corpus partitioning is
a pre-processing step in parallel LVM training, our
579
technique can be applied to reduce the memory foot-
print of essentially any existing LVM or training ap-
proach. The accuracy of LVM training for a fixed
model size and corpus remains unchanged, but in-
telligent corpus partitioning allows us to train larger
and typically more accurate models using the same
memory capacity.
While the general minimization problem we en-
counter is NP-hard, we develop greedy approxima-
tions that work well. In experiments with both
HMM and LDA models, we show that our technique
offers large advantages over existing approaches in
terms of both memory footprint and execution time.
On a large corpus using 50 nodes in parallel, our best
partitioning method can reduce the memory required
per node to less than 1/10th that when training with-
out corpus partitioning, and to half that of a random
partitioning. Further, our approach reduces the train-
ing time of an existing parallel HMM codebase by
3x. Our work includes the release of our partitioning
codebase, and an associated codebase for the paral-
lel training of HMMs.1
2 Problem Formulation
In a distributed LVM system, a training corpus D =
{d1, d2, . . . , dN} of documents is distributed across
T computational nodes. We first formalize the mem-
ory footprint on each node nt, where t = {1, ..., T}.
Let Dt ? D denote the document collection on node
nt, and Vt be the number of word types (i.e., the
number of unique words) in Dt. Let K be the num-
ber of latent variable values in the LVM.
With these quantities, we can express how many
parameters must be held in memory on each com-
putational node for training LVMs in a distributed
environment. In practice, the LVM parameter space
is dominated by an observation model: a condi-
tional distribution over words given the latent vari-
able value. Thus, the observation model includes
K(Vt? 1) parameters. Different LVMs include var-
ious other parameters to specify the complete model.
For example, a first-order HMM includes additional
distributions for the initial latent variable and latent
variable transitions, for a total of K(Vt ? 1) + K2
parameters. LDA, on the other hand, includes just a
1https://code.google.com/p/
corpus-partition/
single multinomial over the latent variables, making
a total of K(Vt ? 1) + K ? 1 parameters.
The LVM parameters comprise almost all of the
memory footprint for LVM training. Further, as the
examples above illustrate, the number of parame-
ters on each node tends to vary almost linearly with
Vt (in practice, Vt is typically larger than K by an
order of magnitude or more). Thus, in this paper
we attempt to minimize memory footprint by lim-
iting Vt on each computational node. We assume
the typical case in a distributed environment where
nodes are homogeneous, and thus our goal is to par-
tition the corpus such that the maximum vocabulary
size Vmax = maxTt=1Vt on any single node is mini-
mized. We define this task formally as follows.
Definition CORPUSPART : Given a corpus of
N documents D = {d1, d2, . . . , dN}, and T nodes,
partition D into T subsets D1, D2, . . . , DT , such
that Vmax is minimized.
For illustration, consider the following small ex-
ample. Let corpus C contain three short docu-
ments {c1=?I live in Chicago?, c2=?I am studying
physics?, c3=?Chicago is a city in Illinois?}, and
consider partitioning C into 2 non-empty subsets,
i.e., T = 2. There are a total of three possibilities:
? {{c1, c2}, {c3}}. Vmax = 7
? {{c1, c3}, {c2}}. Vmax = 8
? {{c2, c3}, {c1}}. Vmax = 10
The decision problem version of
CORPUSPART is NP-Complete, by a re-
duction from independent task scheduling (Zhu and
Ibarra, 1999). In this paper, we develop greedy
algorithms for the task that are effective in practice.
We note that CORPUSPART has a submodu-
lar problem structure, where greedy algorithms are
often effective. Specifically, let |S| denote the vo-
cabulary size of a set of documents S, and let S? ?
S. Then for any document c the following inequality
holds.
|S? ? c| ? |S?| ? |S ? c| ? |S|
That is, adding a document c to the subset S? in-
creases vocabulary size at least as much as adding
c to S; the vocabulary size function is submodular.
The CORPUSPART task thus seeks a partition
of the data that minimizes the maximum of a set of
submodular functions. While formal approximation
580
guarantees exist for similar problems, to our knowl-
edge none apply directly in our case. For example,
(Krause et al, 2007) considers maximizing the mini-
mum over a set of monotonic submodular functions,
which is the opposite of our problem. The distinct
task of minimizing a single submodular function has
been investigated in e.g. (Iwata et al, 2001).
It is important to emphasize that data partition-
ing is a pre-processing step, after which we can em-
ploy precisely the same Expectation-Maximization
(EM), sampling, or variational parameter learning
techniques as utilized in previous work. In fact,
for popular learning techniques including EM for
HMMs (Rabiner, 1989) and variational EM for LDA
(Wolfe et al, 2008), it can be shown that the param-
eter updates are independent of how the corpus is
partitioned. Thus, for those approaches our parti-
tioning is guaranteed to produce the same models as
any other partitioning method; i.e., model accuracy
is unchanged.
Lastly, we note that we target synchronized LVM
training, in which all nodes must finish each train-
ing iteration before any node can proceed to the
next iteration. Thus, we desire balanced partitions to
help ensure iterations have similar durations across
nodes. We achieve this in practice by constraining
each node to hold at most 3% more than Z/T to-
kens, where Z is the corpus size in tokens.
3 Corpus Partitioning Methods
Our high-level greedy partitioning framework is
given in Algorithm 1. The algorithm requires an-
swering two key questions: How do we select which
document to allocate next? And, given a document,
on which node should it be placed? We present al-
ternative approaches to each question below.
Algorithm 1 Greedy Partitioning Framework
INPUT: {D, T}
OUTPUT: {D1, . . . , DT }
Objective: Minimize Vmax
Initialize each subset Dt = ? for T nodes
repeat
document selection:Select document d from D
node selection: Select node nt, and add d to Dt
Remove d from D
until all documents are allocated
A baseline partitioning method commonly used
in practice simply distributes documents across
nodes randomly. As our experiments show, this
baseline approach can be improved significantly.
In the following, set operations are interpreted as
applying to the set of unique words in a document.
For example, |d?Dt| indicates the number of unique
word types in node nt after document d is added to
its document collection Dt.
3.1 Document Selection
For document selection, previous work (Zhu and
Ibarra, 1999) proposed a heuristic DISSIMILARITY
method that selects the document d that is least sim-
ilar to any of the node document collections Dt,
where the similarity of d and Dt is calculated as:
Sim(d,DT ) = |d ? Dt|. The intuition behind the
heuristic is that dissimilar documents are more likely
to impact future node selection decisions. Assigning
the dissimilar documents earlier helps ensure that
more greedy node selections are informed by these
impactful assignments.
However, DISSIMILARITY has a prohibitive time
complexity of O(TN2), because we must compare
T nodes to an order of N documents for a total of
N iterations. To scale to large corpora, we propose
a novel BATCH DISSIMILARITY method. In BATCH
DISSIMILARITY, we select the top L most dissim-
ilar documents in each iteration, instead of just the
most dissimilar. Importantly, L is altered dynami-
cally: we begin with L = 1, and then increase L by
one for iteration i+1 iff using a batch size of L+1 in
iteration i would not have altered the algorithm?s ul-
timate selections (that is, if the most dissimilar doc-
ument in iteration i + 1 is in fact the L + 1st most
dissimilar in iteration i). In the ideal case where L
is incremented each iteration, BATCH DISSIMILAR
will have a reduced time complexity of O(TN3/2).
Our experiments revealed two key findings re-
garding document selection. First, BATCH DISSIM-
ILARITY provides a memory reduction within 0.1%
of that of DISSIMILARITY (on small corpora where
running DISSIMILARITY is tractable), but partitions
an estimated 2,600 times faster on our largest eval-
uation corpus. Second, we found that document se-
lection has relatively minor impact on memory foot-
print, providing a roughly 5% incremental benefit
over random document selection. Thus, although
581
we utilize BATCH DISSIMILARITY in the final sys-
tem we evaluate, simple random document selection
may be preferable in some practical settings.
3.2 Node Selection
Given a selected document d, the MINIMUM
method proposed in previous work selects node nt
having the minimum number of word types after al-
location of d to nt (Zhu and Ibarra, 1999). That is,
MINIMUM minimizes |d ?Dt|. Here, we introduce
an alternative node selection method JACCARD that
selects node nt maximizing the Jaccard index, de-
fined here as |d ?Dt|/|d ?Dt|.
Our experiments showed that our JACCARD node
selection method outperforms the MINIMUM selec-
tion method. In fact, for the largest corpora used
in our experiments, JACCARD offered an 12.9%
larger reduction in Vmax than MINIMUM. Our
proposed system, referred to as BJAC, utilizes
our best-performing strategies for document selec-
tion (BATCH DISSIMILARITY) and node selection
(JACCARD).
4 Evaluation of Partitioning Methods
We evaluate our partitioning method against the
baseline and Z&I, the best performing scalable
method from previous work, which uses random
document selection and MINIMUM node selection
(Zhu and Ibarra, 1999). We evaluate on three cor-
pora (Table 1): the Brown corpus of newswire text
(Kucera and Francis, 1967), the Reuters Corpus Vol-
ume1 (RCV1) (Lewis et al, 2004), and a larger Web-
Sent corpus of sentences gathered from the Web
(Downey et al, 2007).
Corpus N V Z
Brown 57339 56058 1161183
RCV1 804414 288062 99702278
Web-Sent 2747282 214588 58666983
Table 1: Characteristics of the three corpora. N = #
of documents, V = # of word types, Z = # of tokens.
We treat each sentence as a document in the Brown
and Web-Sent corpora.
Table 2 shows how the maximum word type size
Vmax varies for each method and corpus, for T = 50
nodes. BJAC significantly decreases Vmax over the
Corpus baseline Z&I BJAC
Brown 6368 5714 4369
RCV1 49344 32136 24923
Web-Sent 72626 45989 34754
Table 2: Maximum word type size Vmax for each
partitioning method, for each corpus. For the larger
corpora, BJAC reduces Vmax by over 50% compared
to the baseline, and by 23% compared to Z&I.
random partitioning baseline typically employed in
practice. Furthermore, the advantage of BJAC over
the baseline is maintained as more computational
nodes are utilized, as illustrated in Figure 1. BJac
reduces Vmax by a larger factor over the baseline as
more computational nodes are employed.
  0
  20,000
  40,000
  60,000
  80,000
  100,000
  120,000
  140,000
10 20 30 40 50 60 70 80 90 100
nu
m
ber
 of 
wo
rd t
ype
s
 number of nodes
Vmax by baselineVmax by BJac
Figure 1: Effects of partitioning as the number of
computational nodes increases (Web-Sent corpus).
With 100 nodes, BJac?s Vmax is half that of the base-
line, and 1/10th of the full corpus vocabulary size.
5 Evaluation in Parallel LVM Systems
We now turn to an evaluation of our corpus parti-
tioning within parallel LVM training systems.
Table 3 shows the memory footprint required for
HMM and LDA training for three different partition-
ing methods. We compare BJAC with the random
partitioning baseline, Zhu?s method, and with all-
words, the straightforward approach of simply stor-
ing parameters for the entire corpus vocabulary on
every node (Ahuja and Downey, 2010; Asuncion et
al., 2011). All-words has the same memory footprint
as when training on a single node.
For large corpora, BJAC reduces memory size
per node by approximately a factor of two over the
random baseline, and by a factor of 8-11 over all-
582
LVM Corpus all-words baseline BJAC
HMM
Brown 435.3 56.2 40.9
RCV1 2205.4 384.1 197.8
Web-Sent 1644.8 561.7 269.7
LDA
Brown 427.7 48.6 33.3
RCV1 2197.7 376.5 190.1
Web-Sent 1637.2 554.1 262.1
Table 3: Memory footprint of computational nodes
in megabytes(MB), using 50 computational nodes.
Both models utilize 1000 latent variable values.
words. The results demonstrate that in addition to
the well-known savings in computation time offered
by parallel LVM training, distributed computation
also significantly reduces the memory footprint on
each node. In fact, for the RCV1 corpus, BJAC re-
duces memory footprint to less than 1/10th that of
training with all words on each computational node.
We next evaluate the execution time for an itera-
tion of model training. Here, we use a parallel im-
plementation of HMMs, and measure iteration time
for training on the Web-sent corpus with 50 hidden
states as the number of computational nodes varies.
We compare against the random baseline and against
the all-words approach utilized in an existing paral-
lel HMM codebase (Ahuja and Downey, 2010). The
results are shown in Table 4. Moving beyond the all-
words method to exploit corpus partitioning reduces
training iteration time, by a factor of two to three.
However, differences in partitioning methods have
only small effects in iteration time: BJAC has essen-
tially the same iteration time as the random baseline
in this experiment.
It is also important to consider the additional time
required to execute the partitioning methods them-
selves. However, in practice this additional time
is negligible. For example, BJAC can partition the
Web-sent corpus in 368 seconds, using a single com-
putational node. By contrast, training a 200-state
HMM on the same corpus requires over a hundred
CPU-days. Thus, BJAC?s time to partition has a neg-
ligible impact on total training time.
6 Related Work
The CORPUSPART task has some similarities
to the graph partitioning task investigated in other
T all-words baseline BJAC
25 4510 1295 1289
50 2248 740 735
100 1104 365 364
200 394 196 192
Table 4: Average iteration time(sec) for training an
HMM with 50 hidden states on Web-Sent. Partition-
ing with BJAC outperforms all-words, which stores
parameters for all word types on each node.
parallelization research (Hendrickson and Kolda,
2000). However, our LVM training task differs sig-
nificantly from those in which graph partitioning is
typically employed. Specifically, graph partitioning
tends to be used for scientific computing applica-
tions where communication is the bottleneck. The
graph algorithms focus on creating balanced parti-
tions that minimize the cut edge weight, because
edge weights represent communication costs to be
minimized. By contrast, in our LVM training task,
memory consumption is the bottleneck and commu-
nication costs are less significant.
Zhu & Ibarra (1999) present theoretical results
and propose techniques for the general partitioning
task we address. In contrast to that work, we fo-
cus on the case where the data to be partitioned is a
large corpus of text. In this setting, we show that our
heuristics partition faster and provide smaller mem-
ory footprint than those of (Zhu and Ibarra, 1999).
7 Conclusion
We presented a general corpus partitioning tech-
nique which can be exploited in LVM training to re-
duce memory footprint and training time. We eval-
uated the partitioning method?s performance, and
showed that for large corpora, our approach reduces
memory consumption by over 50% and learns mod-
els up to three times faster when compared with ex-
isting implementations for parallel LVM training.
Acknowledgments
This work was supported in part by NSF Grants
IIS-101675 and IIS-1065397, and DARPA contract
D11AP00268.
583
References
Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shra-
van Narayanamurthy, and Alexander J. Smola. 2012.
Scalable inference in latent variable models. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ?12, pages 123?
132, New York, NY, USA. ACM.
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Human Language Technologies: Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL HLT).
Arthur U. Asuncion, Padhraic Smyth, and Max Welling.
2011. Asynchronous distributed estimation of topic
models for document analysis. Statistical Methodol-
ogy, 8(1):3 ? 17. Advances in Data Mining and Statis-
tical Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun.
2006. Map-Reduce for machine learning on multicore.
In Bernhard Scho?lkopf, John C. Platt, and Thomas
Hoffman, editors, NIPS, pages 281?288. MIT Press.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ?07, pages 271?280, New York, NY,
USA. ACM.
D. Downey, S. Schoenmackers, and O. Etzioni. 2007.
Sparse information extraction: Unsupervised language
models to the rescue. In Proc. of ACL.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
Bruce Hendrickson and Tamara G Kolda. 2000. Graph
partitioning models for parallel computing. Parallel
computing, 26(12):1519?1534.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?99, pages
50?57, New York, NY, USA. ACM.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. 2001.
A combinatorial strongly polynomial algorithm for
minimizing submodular functions. J. ACM, 48:761?
777.
Andreas Krause, H. Brendan Mcmahan, Google Inc, Car-
los Guestrin, and Anupam Gupta. 2007. Selecting
observations against adversarial objectives. Technical
report, In NIPS, 2007a.
H. Kucera and W. N. Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.
David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,
G. Dietterich, and Fan Li. 2004. Rcv1: A new bench-
mark collection for text categorization research. Jour-
nal of Machine Learning Research, 5:361?397.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Artifi-
cial intelligence - Volume 2, AAAI?05, pages 813?818.
AAAI Press.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully
distributed EM for very large datasets. In Proceed-
ings of the 25th international conference on Machine
learning, ICML ?08, pages 1184?1191, New York,
NY, USA. ACM.
Huican Zhu and Oscar H. Ibarra. 1999. On some ap-
proximation algorithms for the set partition problem.
In Proceedings of the 15th Triennial Conf. of Int. Fed-
eration of Operations Research Society.
584
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538?544,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Fast Easy Unsupervised Domain Adaptation
with Marginalized Structured Dropout
Yi Yang and Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
{yiyang, jacobe}@gatech.edu
Abstract
Unsupervised domain adaptation often re-
lies on transforming the instance represen-
tation. However, most such approaches
are designed for bag-of-words models, and
ignore the structured features present in
many problems in NLP. We propose a
new technique called marginalized struc-
tured dropout, which exploits feature
structure to obtain a remarkably simple
and efficient feature projection. Applied
to the task of fine-grained part-of-speech
tagging on a dataset of historical Por-
tuguese, marginalized structured dropout
yields state-of-the-art accuracy while in-
creasing speed by more than an order-of-
magnitude over previous work.
1 Introduction
Unsupervised domain adaptation is a fundamen-
tal problem for natural language processing, as
we hope to apply our systems to datasets unlike
those for which we have annotations. This is par-
ticularly relevant as labeled datasets become stale
in comparison with rapidly evolving social media
writing styles (Eisenstein, 2013), and as there is
increasing interest in natural language processing
for historical texts (Piotrowski, 2012). While a
number of different approaches for domain adap-
tation have been proposed (Pan and Yang, 2010;
S?gaard, 2013), they tend to emphasize bag-of-
words features for classification tasks such as sen-
timent analysis. Consequently, many approaches
rely on each instance having a relatively large
number of active features, and fail to exploit the
structured feature spaces that characterize syn-
tactic tasks such as sequence labeling and pars-
ing (Smith, 2011).
As we will show, substantial efficiency im-
provements can be obtained by designing domain
adaptation methods for learning in structured fea-
ture spaces. We build on work from the deep
learning community, in which denoising autoen-
coders are trained to remove synthetic noise from
the observed instances (Glorot et al, 2011a). By
using the autoencoder to transform the original
feature space, one may obtain a representation
that is less dependent on any individual feature,
and therefore more robust across domains. Chen
et al (2012) showed that such autoencoders can
be learned even as the noising process is analyt-
ically marginalized; the idea is similar in spirit
to feature noising (Wang et al, 2013). While
the marginalized denoising autoencoder (mDA) is
considerably faster than the original denoising au-
toencoder, it requires solving a system of equa-
tions that can grow very large, as realistic NLP
tasks can involve 10
5
or more features.
In this paper we investigate noising functions
that are explicitly designed for structured feature
spaces, which are common in NLP. For example,
in part-of-speech tagging, Toutanova et al (2003)
define several feature ?templates?: the current
word, the previous word, the suffix of the current
word, and so on. For each feature template, there
are thousands of binary features. To exploit this
structure, we propose two alternative noising tech-
niques: (1) feature scrambling, which randomly
chooses a feature template and randomly selects
an alternative value within the template, and (2)
structured dropout, which randomly eliminates
all but a single feature template. We show how it
is possible to marginalize over both types of noise,
and find that the solution for structured dropout is
substantially simpler and more efficient than the
mDA approach of Chen et al (2012), which does
not consider feature structure.
We apply these ideas to fine-grained part-of-
speech tagging on a dataset of Portuguese texts
from the years 1502 to 1836 (Galves and Faria,
2010), training on recent texts and evaluating
538
on older documents. Both structure-aware do-
main adaptation algorithms perform as well as
standard dropout ? and better than the well-
known structural correspondence learning (SCL)
algorithm (Blitzer et al, 2007) ? but structured
dropout is more than an order-of-magnitude faster.
As a secondary contribution of this paper, we
demonstrate the applicability of unsupervised do-
main adaptation to the syntactic analysis of histor-
ical texts.
2 Model
In this section we first briefly describe the de-
noising autoencoder (Glorot et al, 2011b), its ap-
plication to domain adaptation, and the analytic
marginalization of noise (Chen et al, 2012). Then
we present three versions of marginalized denois-
ing autoencoders (mDA) by incorporating differ-
ent types of noise, including two new noising pro-
cesses that are designed for structured features.
2.1 Denoising Autoencoders
Assume instances x
1
, . . . ,x
n
, which are drawn
from both the source and target domains. We will
?corrupt? these instances by adding different types
of noise, and denote the corrupted version of x
i
by
?
x
i
. Single-layer denoising autoencoders recon-
struct the corrupted inputs with a projection matrix
W : R
d
? R
d
, which is estimated by minimizing
the squared reconstruction loss
L =
1
2
n
?
i=1
||x
i
?W
?
x
i
||
2
. (1)
If we write X = [x
1
, . . . ,x
n
] ? R
d?n
, and we
write its corrupted version
?
X, then the loss in (1)
can be written as
L(W) =
1
2n
tr
[
(
X?W
?
X
)
>
(
X?W
?
X
)
]
.
(2)
In this case, we have the well-known closed-
form solution for this ordinary least square prob-
lem:
W = PQ
?1
, (3)
where Q =
?
X
?
X
>
and P = X
?
X
>
. After ob-
taining the weight matrix W, we can insert non-
linearity into the output of the denoiser, such as
tanh(WX). It is also possible to apply stack-
ing, by passing this vector through another autoen-
coder (Chen et al, 2012). In pilot experiments,
this slowed down estimation and had little effect
on accuracy, so we did not include it.
High-dimensional setting Structured predic-
tion tasks often have much more features than
simple bag-of-words representation, and perfor-
mance relies on the rare features. In a naive im-
plementation of the denoising approach, both P
and Q will be dense matrices with dimension-
ality d ? d, which would be roughly 10
11
ele-
ments in our experiments. To solve this problem,
Chen et al (2012) propose to use a set of pivot
features, and train the autoencoder to reconstruct
the pivots from the full set of features. Specifi-
cally, the corrupted input is divided to S subsets
?
x
i
=
[
(
?
x)
1
i
>
, . . . , (
?
x)
S
i
>
]
>
. We obtain a projec-
tion matrix W
s
for each subset by reconstructing
the pivot features from the features in this subset;
we can then use the sum of all reconstructions as
the new features, tanh(
?
S
s=1
W
s
X
s
).
Marginalized Denoising Autoencoders In the
standard denoising autoencoder, we need to gen-
erate multiple versions of the corrupted data
?
X
to reduce the variance of the solution (Glorot et
al., 2011b). But Chen et al (2012) show that it
is possible to marginalize over the noise, analyt-
ically computing expectations of both P and Q,
and computing
W = E[P]E[Q]
?1
, (4)
where E[P] =
?
n
i=1
E[x
i
?
x
>
i
] and E[Q] =
?
n
i=1
E[
?
x
i
?
x
>
i
]. This is equivalent to corrupting
the data m?? times. The computation of these
expectations depends on the type of noise.
2.2 Noise distributions
Chen et al (2012) used dropout noise for domain
adaptation, which we briefly review. We then de-
scribe two novel types of noise that are designed
for structured feature spaces, and explain how they
can be marginalized to efficiently compute W.
Dropout noise In dropout noise, each feature is
set to zero with probability p > 0. If we define
the scatter matrix of the uncorrupted input as S =
XX
>
, the solutions under dropout noise are
E[Q]
?,?
=
{
(1? p)
2
S
?,?
if ? 6= ?
(1? p)S
?,?
if ? = ?
, (5)
and
E[P]
?,?
= (1? p)S
?,?
, (6)
539
where ? and ? index two features. The form of
these solutions means that computing W requires
solving a system of equations equal to the num-
ber of features (in the naive implementation), or
several smaller systems of equations (in the high-
dimensional version). Note also that p is a tunable
parameter for this type of noise.
Structured dropout noise In many NLP set-
tings, we have several feature templates, such as
previous-word, middle-word, next-word, etc, with
only one feature per template firing on any token.
We can exploit this structure by using an alterna-
tive dropout scheme: for each token, choose ex-
actly one feature template to keep, and zero out all
other features that consider this token (transition
feature templates such as ?y
t
, y
t?1
? are not con-
sidered for dropout). Assuming we haveK feature
templates, this noise leads to very simple solutions
for the marginalized matrices E[P] and E[Q],
E[Q]
?,?
=
{
0 if ? 6= ?
1
K
S
?,?
if ? = ?
(7)
E[P]
?,?
=
1
K
S
?,?
, (8)
ForE[P], we obtain a scaled version of the scat-
ter matrix, because in each instance
?
x, there is ex-
actly a 1/K chance that each individual feature
survives dropout. E[Q] is diagonal, because for
any off-diagonal entry E[Q]
?,?
, at least one of ?
and ? will drop out for every instance. We can
therefore view the projection matrix W as a row-
normalized version of the scatter matrix S. Put
another way, the contribution of ? to the recon-
struction for ? is equal to the co-occurence count
of ? and ?, divided by the count of ?.
Unlike standard dropout, there are no free
hyper-parameters to tune for structured dropout.
Since E[Q] is a diagonal matrix, we eliminate the
cost of matrix inversion (or of solving a system of
linear equations). Moreover, to extend mDA for
high dimensional data, we no longer need to di-
vide the corrupted input
?
x to several subsets.
1
For intuition, consider standard feature dropout
with p =
K?1
K
. This will look very similar to
structured dropout: the matrix E[P] is identical,
and E[Q] has off-diagonal elements which are
scaled by (1 ? p)
2
, which goes to zero as K is
1
E[P] is an r by d matrix, where r is the number of pivots.
large. However, by including these elements, stan-
dard dropout is considerably slower, as we show in
our experiments.
Scrambling noise A third alternative is to
?scramble? the features by randomly selecting al-
ternative features within each template. For a fea-
ture ? belonging to a template F , with probability
p we will draw a noise feature ? also belonging
to F , according to some distribution q. In this
work, we use an uniform distribution, in which
q
?
=
1
|F |
. However, the below solutions will also
hold for other scrambling distributions, such as
mean-preserving distributions.
Again, it is possible to analytically marginal-
ize over this noise. Recall that E[Q] =
?
n
i=1
E[
?
x
i
?
x
>
i
]. An off-diagonal entry in the ma-
trix
?
x
?
x
>
which involves features ? and ? belong-
ing to different templates (F
?
6= F
?
) can take four
different values (x
i,?
denotes feature ? in x
i
):
? x
i,?
x
i,?
if both features are unchanged,
which happens with probability (1? p)
2
.
? 1 if both features are chosen as noise features,
which happens with probability p
2
q
?
q
?
.
? x
i,?
or x
i,?
if one feature is unchanged and
the other one is chosen as the noise feature,
which happens with probability p(1 ? p)q
?
or p(1? p)q
?
.
The diagonal entries take the first two values
above, with probability 1 ? p and pq
?
respec-
tively. Other entries will be all zero (only one
feature belonging to the same template will fire
in x
i
). We can use similar reasoning to compute
the expectation of P. With probability (1 ? p),
the original features are preserved, and we add the
outer-product x
i
x
>
i
; with probability p, we add the
outer-product x
i
q
>
. Therefore E[P] can be com-
puted as the sum of these terms.
3 Experiments
We compare these methods on historical Por-
tuguese part-of-speech tagging, creating domains
over historical epochs.
3.1 Experiment setup
Datasets We use the Tycho Brahe corpus to
evaluate our methods. The corpus contains a total
of 1,480,528 manually tagged words. It uses a set
of 383 tags and is composed of various texts from
540
historical Portuguese, from 1502 to 1836. We di-
vide the texts into fifty-year periods to create dif-
ferent domains. Table 1 presents some statistics of
the datasets. We hold out 5% of data as develop-
ment data to tune parameters. The two most recent
domains (1800-1849 and 1750-1849) are treated
as source domains, and the other domains are tar-
get domains. This scenario is motivated by train-
ing a tagger on a modern newstext corpus and ap-
plying it to historical documents.
Dataset
# of Tokens
Total Narrative Letters Dissertation Theatre
1800-1849 125719 91582 34137 0 0
1750-1799 202346 57477 84465 0 60404
1700-1749 278846 0 130327 148519 0
1650-1699 248194 83938 115062 49194 0
1600-1649 295154 117515 115252 62387 0
1550-1599 148061 148061 0 0 0
1500-1549 182208 126516 0 55692 0
Overall 1480528 625089 479243 315792 60404
Table 1: Statistics of the Tycho Brahe Corpus
CRF tagger We use a conditional random field
tagger, choosing CRFsuite because it supports
arbitrary real valued features (Okazaki, 2007),
with SGD optimization. Following the work of
Nogueira Dos Santos et al (2008) on this dataset,
we apply the feature set of Ratnaparkhi (1996).
There are 16 feature templates and 372, 902 fea-
tures in total. Following Blitzer et al (2006), we
consider pivot features that appear more than 50
times in all the domains. This leads to a total of
1572 pivot features in our experiments.
Methods We compare mDA with three alterna-
tive approaches. We refer to baseline as training
a CRF tagger on the source domain and testing on
the target domain with only base features. We also
include PCA to project the entire dataset onto a
low-dimensional sub-space (while still including
the original features). Finally, we compare against
Structural Correspondence Learning (SCL; Blitzer
et al, 2006), another feature learning algorithm.
In all cases, we include the entire dataset to com-
pute the feature projections; we also conducted ex-
periments using only the test and training data for
feature projections, with very similar results.
Parameters All the hyper-parameters are de-
cided with our development data on the training
set. We try different low dimension K from 10 to
2000 for PCA. Following Blitzer (2008) we per-
form feature centering/normalization, as well as
rescaling for SCL. The best parameters for SCL
are dimensionality K = 25 and rescale factor
? = 5, which are the same as in the original pa-
per. For mDA, the best corruption level is p = 0.9
for dropout noise, and p = 0.1 for scrambling
noise. Structured dropout noise has no free hyper-
parameters.
3.2 Results
Table 2 presents results for different domain adap-
tation tasks. We also compute the transfer ra-
tio, which is defined as
adaptation accuracy
baseline accuracy
, shown in
Figure 1. The generally positive trend of these
graphs indicates that adaptation becomes progres-
sively more important as we select test sets that are
more temporally remote from the training data.
In general, mDA outperforms SCL and PCA,
the latter of which shows little improvement over
the base features. The various noising approaches
for mDA give very similar results. However, struc-
tured dropout is orders of magnitude faster than
the alternatives, as shown in Table 3. The scram-
bling noise is most time-consuming, with cost
dominated by a matrix multiplication.
Method PCA SCL
mDA
dropout structured scambling
Time 7,779 38,849 8,939 339 327,075
Table 3: Time, in seconds, to compute the feature
transformation
4 Related Work
Domain adaptation Most previous work on do-
main adaptation focused on the supervised setting,
in which some labeled data is available in the tar-
get domain (Jiang and Zhai, 2007; Daum?e III,
2007; Finkel and Manning, 2009). Our work fo-
cuses on unsupervised domain adaptation, where
no labeled data is available in the target domain.
Several representation learning methods have been
proposed to solve this problem. In structural corre-
spondence learning (SCL), the induced represen-
tation is based on the task of predicting the pres-
ence of pivot features. Autoencoders apply a sim-
ilar idea, but use the denoised instances as the la-
tent representation (Vincent et al, 2008; Glorot et
al., 2011b; Chen et al, 2012). Within the con-
text of denoising autoencoders, we have focused
541
Task baseline PCA SCL
mDA
dropout structured scrambling
from 1800-1849
? 1750 89.12 89.09 89.69 90.08 90.08 90.01
? 1700 90.43 90.43 91.06 91.56 91.57 91.55
? 1650 88.45 88.52 87.09 88.69 88.70 88.57
? 1600 87.56 87.58 88.47 89.60 89.61 89.54
? 1550 89.66 89.61 90.57 91.39 91.39 91.36
? 1500 85.58 85.63 86.99 88.96 88.95 88.91
from 1750-1849
? 1700 94.64 94.62 94.81 95.08 95.08 95.02
? 1650 91.98 90.97 90.37 90.83 90.84 90.80
? 1600 92.95 92.91 93.17 93.78 93.78 93.71
? 1550 93.27 93.21 93.75 94.06 94.05 94.02
? 1500 89.80 89.75 90.59 91.71 91.71 91.68
Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849.
Figure 1: Transfer ratio for adaptation to historical text
on dropout noise, which has also been applied as
a general technique for improving the robustness
of machine learning, particularly in neural net-
works (Hinton et al, 2012; Wang et al, 2013).
On the specific problem of sequence labeling,
Xiao and Guo (2013) proposed a supervised do-
main adaptation method by using a log-bilinear
language adaptation model. Dhillon et al (2011)
presented a spectral method to estimate low di-
mensional context-specific word representations
for sequence labeling. Huang and Yates (2009;
2012) used an HMM model to learn latent rep-
resentations, and then leverage the Posterior Reg-
ularization framework to incorporate specific bi-
ases. Unlike these methods, our approach uses a
standard CRF, but with transformed features.
Historical text Our evaluation concerns syntac-
tic analysis of historical text, which is a topic of in-
creasing interest for NLP (Piotrowski, 2012). Pen-
nacchiotti and Zanzotto (2008) find that part-of-
speech tagging degrades considerably when ap-
plied to a corpus of historical Italian. Moon and
Baldridge (2007) tackle the challenging problem
of tagging Middle English, using techniques for
projecting syntactic annotations across languages.
Prior work on the Tycho Brahe corpus applied su-
pervised learning to a random split of test and
training data (Kepler and Finger, 2006; Dos San-
tos et al, 2008); they did not consider the domain
adaptation problem of training on recent data and
testing on older historical text.
5 Conclusion and Future Work
Denoising autoencoders provide an intuitive so-
lution for domain adaptation: transform the fea-
tures into a representation that is resistant to the
noise that may characterize the domain adaptation
process. The original implementation of this idea
produced this noise directly (Glorot et al, 2011b);
later work showed that dropout noise could be an-
alytically marginalized (Chen et al, 2012). We
take another step towards simplicity by showing
that structured dropout can make marginalization
even easier, obtaining dramatic speedups without
sacrificing accuracy.
Acknowledgments : We thank the reviewers for
useful feedback. This research was supported by
National Science Foundation award 1349837.
542
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?06, pages 120?128, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.
John Blitzer. 2008. Domain Adaptation of Natural
Language Processing Systems. Ph.D. thesis, Uni-
versity of Pennsylvania.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In John Langford and
Joelle Pineau, editors, Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML-
12), ICML ?12, pages 767?774. ACM, New York,
NY, USA, July.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In ACL, volume 1785, page 1787.
Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In NIPS, volume 24, pages 199?207.
C??cero Nogueira Dos Santos, Ruy L Milidi?u, and
Ra?ul P Renter??a. 2008. Portuguese part-of-speech
tagging using entropy guided transformation learn-
ing. In Computational Processing of the Portuguese
Language, pages 143?152. Springer.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In Proceedings of NAACL,
Atlanta, GA.
Jenny Rose Finkel and Christopher D Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 602?610. Association for Computa-
tional Linguistics.
Charlotte Galves and Pablo Faria. 2010. Ty-
cho Brahe Parsed Corpus of Historical Por-
tuguese. http://www.tycho.iel.unicamp.br/ ty-
cho/corpus/en/index.html.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011a. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011b. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 513?520.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, pages 495?503. Association for Compu-
tational Linguistics.
Fei Huang and Alexander Yates. 2012. Biased rep-
resentation learning for domain adaptation. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1313?1323. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In ACL,
volume 2007, page 22.
F?abio N Kepler and Marcelo Finger. 2006. Comparing
two markov methods for part-of-speech tagging of
portuguese. In Advances in Artificial Intelligence-
IBERAMIA-SBIA 2006, pages 482?491. Springer.
Taesun Moon and Jason Baldridge. 2007. Part-of-
speech tagging for middle english through align-
ment and projection of parallel diachronic texts. In
EMNLP-CoNLL, pages 390?399.
C??cero Nogueira Dos Santos, Ruy L. Milidi?u, and
Ra?ul P. Renter??a. 2008. Portuguese part-of-speech
tagging using entropy guided transformation learn-
ing. In Proceedings of the 8th international con-
ference on Computational Processing of the Por-
tuguese Language, PROPOR ?08, pages 143?152,
Berlin, Heidelberg. Springer-Verlag.
Naoaki Okazaki. 2007. Crfsuite: a fast implementa-
tion of conditional random fields (crfs).
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345?1359.
Marco Pennacchiotti and Fabio Massimo Zanzotto.
2008. Natural language processing across time:
An empirical investigation on italian. In Advances
in Natural Language Processing, pages 371?382.
Springer.
Michael Piotrowski. 2012. Natural language process-
ing for historical texts. Synthesis Lectures on Hu-
man Language Technologies, 5(2):1?157.
543
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, April 16.
Noah A Smith. 2011. Linguistic structure prediction.
Synthesis Lectures on Human Language Technolo-
gies, 4(2):1?274.
Anders S?gaard. 2013. Semi-supervised learning and
domain adaptation in natural language processing.
Synthesis Lectures on Human Language Technolo-
gies, 6(2):1?103.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international
conference on Machine learning, pages 1096?1103.
ACM.
Sida I. Wang, Mengqiu Wang, Stefan Wager, Percy
Liang, and Christopher D. Manning. 2013. Fea-
ture noising for log-linear structured prediction. In
Empirical Methods in Natural Language Processing
(EMNLP).
Min Xiao and Yuhong Guo. 2013. Domain adapta-
tion for sequence labeling tasks with a probabilis-
tic language adaptation model. In Sanjoy Dasgupta
and David Mcallester, editors, Proceedings of the
30th International Conference on Machine Learn-
ing (ICML-13), volume 28, pages 293?301. JMLR
Workshop and Conference Proceedings.
544
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 30?33,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Active Learning with Constrained Topic Model
Yi Yang
Northwestern University
yiyang@u.northwestern.edu
Shimei Pan
IBM T. J. Watson Research Center
shimei@us.ibm.com
Doug Downey
Northwestern University
ddowney@eecs.northwestern.edu
Kunpeng Zhang
University of Illinois at Chicago
kzhang6@uic.edu
Abstract
Latent Dirichlet Allocation (LDA) is a topic
modeling tool that automatically discovers
topics from a large collection of documents.
It is one of the most popular text analysis
tools currently in use. In practice however,
the topics discovered by LDA do not al-
ways make sense to end users. In this ex-
tended abstract, we propose an active learn-
ing framework that interactively and itera-
tively acquires user feedback to improve the
quality of learned topics. We conduct exper-
iments to demonstrate its effectiveness with
simulated user input on a benchmark dataset.
1 Introduction
Statistical topic models such as Latent Dirichlet Al-
location (LDA) (Blei et al., 2003) provide powerful
tools for uncovering hidden thematic patterns in text
and are useful for representing and summarizing the
contents of large document collections. However,
when using topic models in practice, users often face
one critical problem: topics discovered by the model
do not always make sense. A topic may contain the-
matically unrelated words. Moreover, two thematic
related words may appear in different topics. This
is mainly because the objective function optimized
by LDA may not reflect human judgments of topic
quality (Boyd-Graber et al., 2009).
Potentially, we can solve these problems by incor-
porating additional user guidance or domain knowl-
edge in topic modeling. With standard LDA how-
ever, it is impossible for users to interact with the
model and provide feedback. (Hu et al., 2011) pro-
posed an interactive topic modeling framework that
allows users to add word must-links. However, it
has several limitations. Since the vocabulary size of
a large document collection can be very large, users
may need to annotate a large number of word con-
straints for this method to be effective. Thus, this
process can be very tedious. More importantly, it
cannot handle polysemes. For example, the word
?pound? can refer to either a currency or a unit of
mass. If a user adds a must-link between ?pound?
and another financial term, then he/she cannot add
a must-link between ?pound? and any measurement
terms. Since word must-links are added without
context, there is no way to disambiguate them. As a
result, word constraints frequently are not as effec-
tive as document constraints.
Active learning (Settles, 2010) provides a use-
ful framework which allows users to iteratively give
feedback to the model to improve its quality. In gen-
eral, with the same amount of human labeling, ac-
tive learning often results in a better model than that
learned by an off-line method.
In this extended abstract, we propose an active
learning framework for LDA. It is based on a new
constrained topic modeling framework which is ca-
pable of handling pairwise document constraints.
We present several design choices and the pros and
cons of each choice. We also conduct simulated ex-
periments to demonstrate the effectiveness of the ap-
proach.
2 Active Learning With Constrained Topic
Modeling
In this section, we first summarize our work on con-
strained topic modeling. Then, we introduce an
active topic learning framework that employs con-
strained topic modeling.
In LDA, a document?s topic distribution
~
? is
drawn from a Dirichlet distribution with prior ~?.
A simple and commonly used Dirichlet distribution
uses a symmetric ~? prior. However, (Wallach et al.,
2009) has shown that an asymmetric Dirichlet prior
over the document-topic distributions
~
? and a sym-
metric Dirichlet prior over the topic-word distribu-
tions
~
? yield significant improvements in model per-
formance. Our constrained topic model uses asym-
metric priors to encode constraints.
To incorporate user feedback, we focus on two
1
30
Figure 1: Diagram illustrating the topic model active learning framework.
types of document constraints. A must-link be-
tween two documents indicates that they belong to
the same topics, while a cannot-link indicates that
they belong to different topics.
Previously, we proposed a constrained LDA
framework called cLDA,
1
which is capable of incor-
porating pairwise document constraints. Given pair-
wise document constraints, the topic distribution of
a document cannot be assumed to be independently
sampled. More specifically, we denote the collection
of documents as D = {d
1
, d
2
, ..., d
N
}. We also de-
noteM
i
? D as the set of documents sharing must-
links with document d
i
, and C
i
? D as the set of
documents sharing cannot-links with document d
i
.
~
?
i
is the topic distribution of d
i
, and ~? is the global
document-topic hyper-parameter shared by all doc-
uments.
Given the documents inM
i
, we introduce an aux-
iliary variable ~?
M
i
:
~?
i
M
= T ?
1
|M
i
|
?
j?M
i
~
?
j
, (1)
where T controls the concentration parameters. The
larger the value of T is, the closer
~
?
i
is to the average
of
~
?
j
?s.
Given the documents in C
i
, we introduce another
auxiliary variable:
~?
i
C
= T ? arg
~
?
i
maxmin
j?C
i
KL(
~
?
i
,
~
?
j
), (2)
whereKL(
~
?
i
,
~
?
j
) is the KL-divergence between two
distributions
~
?
i
and
~
?
j
. This means we choose a vec-
tor that is maximally far away from C
i
, in terms of
KL divergence to its nearest neighbor in C
i
.
In such a way, we force documents sharing must-
links to have similar topic distributions while docu-
ments sharing cannot-links to have dissimilar topic
distributions. Note that it also encodes constraint as
soft preference rather than hard constraint. We use
Collapsed Gibbs Sampling for LDA inference. Dur-
ing Gibbs Sampling, instead of always drawing
~
?
i
1
currently in submission.
from Dirichlet(~?), we draw
~
?
i
based on the fol-
lowing distribution:
~
?
i
? Dir(?~?+?
M
~?
i
M
+?
C
~?
i
C
) = Dir(~?
i
). (3)
Here, ?
g
, ?
M
and ?
C
are the weights to control the
trade-off among the three terms. In our experiment,
we choose T = 100, ?
g
= ?
M
= ?
C
= 1.
Our evaluation has shown that cLDA is effective
in improving topic model quality. For example, it
achieved a significant topic classification error re-
duction on the 20 Newsgroup dataset. Also, top-
ics learned by cLDA are more coherent than those
learned by standard LDA.
2.1 Active Learning with User Interaction
In this subsection, we present an active learning
framework to iteratively acquire constraints from
users. As shown in Figure 1, given a document col-
lection, the framework first runs standard LDA with
a burnin component. Since it uses a Gibbs sampler
(Griffiths and Steyvers, 2004) to infer topic samples
for each word token, it usually takes hundreds of it-
erations for the sampler to converge to a stable state.
Based on the results of the burnt-in model, the sys-
tem generates a target document and a set of anchor
documents for a user to annotate. Target document is
a document on which the active learner solicits user
feedback, and anchor documents are representatives
of a topic model?s latent topics. If a large portion of
the word tokens in a document belongs to topic i, we
say the document is an anchor document for topic i.
A user judges the content of the target and the
anchor documents and then informs the system
whether the target document is similar to any of the
anchor documents. The user interface is designed
so that the user can drag the target document near
an anchor document if she considers both to be the
same topic. Currently, one target document can be
must-linked to only one anchor document. Since
it is possbile to have multiple topics in one docu-
ment, in the future, we will allow user to add must
links between one target and mulitple anchor doc-
uments. After adding one or more must-links, the
31
system automatically adds cannot-links between the
target document and the rest anchor documents.
Given this input, the system adds them to a con-
straint pool. It then uses cLDA to incorporate these
constraints and generates an updated topic model.
Based on the new topic model, the system chooses a
new target document and several new anchor docu-
ments for the user to annotate. This process contin-
ues until the user is satisfied with the resulting topic
model.
How to choose the target and anchor documents
are the key questions that we consider in the next
subsections.
2.2 Target Document Selection
A target document is defined as a document on
which the active learner solicits user feedback. We
have investigated several strategies for selecting a
target document.
Random: The active learner randomly selects a doc-
ument from the corpus. Although this strategy is
the simplest, it may not be efficient since the model
may have enough information about the document
already.
MaxEntropy: The entropy of a document d is com-
puted as H
d
= ?
?
K
i=1
?
dk
log ?
dk
, where K is the
number of topics, and ? is model?s document-topic
distribution. Therefore, the system will select a doc-
ument about which it is most confused. A uniform
? implies that the model has no topic information
about the document and thus assigns equal probabil-
ity to all topics.
MinLikelihood: The likelihood of a document d is
computed as L
d
= (
?
N
i=1
?
K
k=1
?
ki
?
dk
)/N , where
N is the number of tokens in d, and ? is model?s
topic-word distribution. Since the overall likeli-
hood of the input documents is the objective func-
tion LDA aims to maximize, using this criteria, the
system will choose a document that is most difficult
for which the current model achieves the lowest ob-
jective score.
2.3 Anchor Documents Selection
Given a target document d, the active learner then
generates one or more anchor documents based on
the target document?s topic distribution ?
d
. It filters
out topics with trivial value in ?
d
and extracts an an-
chor topic set T
anc
which only contains topics with
non-trivial value in ?
d
. A trivial ?
di
means that the
mass of ith component in ?
d
is neglectable, which
indicates that the model rarely assign topic i to doc-
ument d. For each topic t in T
anc
, the active learner
selects an anchor document who has minimum Eu-
clidean distance with an ideal anchor ?
?
t
. In the ideal
anchor ?
?
t
, all the components are zero except the
value of the t
th
component is 1. For example, if a
target document d?s ?
d
is {0.5, 0.3, 0.03, 0.02, 0.15}
in a K = 5 topic model, the active learner would
generate T
anc
= {0, 1, 4} and for each t in T
anc
, an
anchor document.
However, it is possible that some topics learned
by LDA are only ?background? topics which have
significant non-trivial probabilities over many doc-
uments (Song et al., 2009). Since background top-
ics are often uninteresting ones, we use a weighted
anchor topic selection method to filter them. A
weighted k
th
component of ?
?
dk
for document d is
defined as follows: ?
?
dk
= ?
dk
/
?
D
i=0
?
ik
. There-
fore, instead of keeping the topics with non-trivial
values, we keep those whose weighted values are
non-trivial.
3 Evaluation
In this section, we evaluate our active learning
framework. Topic models are often evaluated us-
ing perplexity on held-out test data. However, re-
cent work (Boyd-Graber et al., 2009; Chuang et al.,
2013) has shown that human judgment sometimes
is contrary to the perplexity measure. Following
(Mimno et al., 2011), we employ Topic Coherence,
a metric which was shown to be highly consistent
with human judgment, to measure a topic model?s
quality. It relies upon word co-occurrence statistics
within documents, and does not depend on external
resources or human labeling.
We followed (Basu et al., 2004) to create a Mix3
sub-dataset from the 20 Newsgroups data
2
, which
consists of two newsgroups with similar topics
(rec.sport.hockey, rec.sport.baseball) and one with
a distinctive topic (sci.space). We use this dataset
to evaluate the effectiveness of the proposed frame-
work.
3.1 Simulated Experiments
We first burn-in LDA for 500 iterations. Then for
each additional iteration, the active learner generates
one query which consists of one target document and
one or more anchor documents. We simulate user
feedback using the documents? ground truth labels.
If a target document has the same label as one of
the anchor documents, we add a must-link between
them. We also add cannot-links between the target
document and the rest of the anchor documents. All
these constraints are added into a constraint pool.
We also augment the constraint pool with derived
constraints. For example, due to transitivity, if there
is a must-link between (a, b) and (b, c), then we add
2
Available at http://people.csail.mit.edu/
jrennie/20Newsgroups
32
Topic Words
1 writes, like, think, good, know, better, even, people, run, hit
2 space, nasa, system, gov, launch, orbit, moon, earth, access, data
3 game, play, hockey, season, league, fun, wing, cup, shot, score
1 baseball, hit, won, shot, hitter, base, pitching, cub, ball, yankee
2 space, nasa, system, gov, launch, obit, moon, earth, mission, shuttle
3 hockey, nhl, playoff, star, wing, cup, king, detroit, ranger
Table 1: Ten most probable words of each topic before (above) and after active learning (below).
a must link between (a, c). We simulate the process
for 100 iterations to acquire constraints. After that,
we keep cLDA running for 400 more iterations with
the acquired constraints until it converges.
Figure 2: Topic coherence with different number of
iterations.
Figure 2 shows the topic coherence scores for dif-
ferent target document selection strategies. This re-
sult indicates 1). MaxEntropy has the best topic co-
herence score. 2). All active learning strategies out-
perform standard LDA, and the results are statisti-
cally significant at p = 0.05. With standard LDA,
500 more iterations without any constraints does not
improve the topic coherence. However, by active
learning with cLDA for 500 iterations, the topic co-
herences are significantly improved.
Using MaxEntropy target document selection
method, we demonstrate the improvement of the
most probable topic keywords before and after ac-
tive learning. Table 1 shows that before active learn-
ing, topic 1?s most probable words are incoherent
and thus it is difficult to determine the meaning of
the topic . After active learning, in contrast, topic 1?s
most probable words become more consistent with
a ?baseball? topic. This example suggests that the
active learning framework that interactively and it-
eratively acquires pairwise document constraints is
effective in improving the topic model?s quality.
4 Conclusion
We presented a novel active learning framework for
LDA that employs constrained topic modeling to
actively incorporate user feedback encoded as pair-
wise document constraints. With simulated user in-
put, our preliminary results demonstrate the effec-
tiveness of the framework on a benchmark dataset.
In the future, we will perform a formal user study
in which real users will interact with the system to
iteratively refine topic models.
Acknowledgments
This work was supported in part by DARPA contract
D11AP00268.
References
Sugato Basu, A. Banjeree, ER. Mooney, Arindam Baner-
jee, and Raymond J. Mooney. 2004. Active semi-
supervision for pairwise constrained clustering. In
SDM, pages 333?344.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Jordan Boyd-Graber, Jonathan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In NIPS.
Jason Chuang, Sonal Gupta, Christopher D. Manning,
and Jeffrey Heer. 2013. Topic model diagnostics:
Assessing domain relevance via topical alignment. In
ICML.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235.
Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.
2011. Interactive topic modeling. In ACL, pages 248?
257.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
EMNLP, pages 262?272.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin Madison.
Yangqiu Song, Shimei Pan, Shixia Liu, Michelle X.
Zhou, and Weihong Qian. 2009. Topic and keyword
re-ranking for lda-based topic modeling. In CIKM,
pages 1757?1760.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter. In
NIPS, pages 1973?1981.
33
