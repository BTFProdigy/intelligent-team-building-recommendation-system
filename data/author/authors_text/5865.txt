Coling 2008: Companion volume ? Posters and Demonstrations, pages 87?90
Manchester, August 2008
Easily Identifiable Discourse Relations
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, Aravind Joshi
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
Abstract
We present a corpus study of local dis-
course relations based on the Penn Dis-
course Tree Bank, a large manually anno-
tated corpus of explicitly or implicitly re-
alized relations. We show that while there
is a large degree of ambiguity in temporal
explicit discourse connectives, overall con-
nectives are mostly unambiguous and al-
low high-accuracy prediction of discourse
relation type. We achieve 93.09% accu-
racy in classifying the explicit relations
and 74.74% accuracy overall. In addition,
we show that some pairs of relations oc-
cur together in text more often than ex-
pected by chance. This finding suggests
that global sequence classification of the
relations in text can lead to better results,
especially for implicit relations.
1 Introduction
Discourse relations between textual units are con-
sidered key for the ability to properly interpret
or produce discourse. Various theories of dis-
course have been developed (Moore and Wiemer-
Hastings, 2003) and different relation taxonomies
have been proposed (Hobbs, 1979; McKeown,
1985; Mann and Thompson, 1988; Knott and
Sanders, 1998). Among the most cognitively
salient relations are causal (contingency), contrast
(comparison), and temporal.
Very often, the discourse relations are explicit,
signaled directly by the use of appropriate dis-
course connectives:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(E1) He is very tired because he played tennis all morning.
(E2) He is not very strong, but he can run amazingly fast.
(E3) We had some tea in the afternoon and later went to a
restaurant for a big dinner.
Discourse relations can also be implicit, inferred
by the context of the utterance and general world
knowledge.
(I1) I took my umbrella this morning. [because] The forecast
was rain in the afternoon.
(I2) She is never late for meetings. [but] He always arrives
10 minutes late.
(I3) She woke up early. [afterward] She had breakfast and
went for a walk in the park.
An additional complication for automatic clas-
sification of discourse relations is that even in the
presence of an explicit discourse connective, the
connective might be ambiguous between several
senses. For example, since can be used to signal
either a temporal or a contingency relation.
They have not spoken to each other since they argued last
fall. (Temporal)
I assumed you were not coming since you never replied to
the invitation. (Causal)
Several questions directly related to efforts in
automatic recognition of discourse relations arise:
In a general text, what is the proportion of ex-
plicit versus implicit relations? Since implicit rela-
tions are presumably harder to recognize automati-
cally, the larger their proportion, the more difficult
the overall prediction of discourse relation will be.
How ambiguous are discourse connectives?
The degree of ambiguity would give an upper
bound on the accuracy with which explicit rela-
tions can be identified. The more ambiguous dis-
course connectives are, the more difficult it would
be to automatically decide which discourse rela-
tion is expressed in a given sentence, even in the
presence of a connective.
87
In a text, are adjacent discourse relations inde-
pendent of each other or are certain sequences of
relations more likely? In the latter case, a ?dis-
course grammar? of text can be used and easy to
identify relations such as unambiguous explicit re-
lations can help determine the class of implicit re-
lations that immediately follow or precede them.
In this study, we address the above questions us-
ing the largest existing corpus manually annotated
with discourse relations?the Penn Discourse Tree
Bank (Prasad et al, 2008).
2 The Penn Discourse Tree Bank
The Penn Discourse Treebank (PDTB) is a new re-
source (Prasad et al, 2008) of annotated discourse
relations. The annotation covers the same 1 mil-
lion word Wall Street Journal (WSJ) corpus used
for the Penn Treebank (Marcus et al, 1994).
he PDTB is the first corpus to systematically
identify and distinguish explicit and implicit dis-
course relations. By definition, an explicit relation
is triggered by the presence of a discourse con-
nective which occurs overtly in the text. The dis-
course connective can essentially be viewed as a
discourse-level predicate which takes two clausal
arguments. For example, sentence E1 above could
be represented as BECAUSE(?He is very tired?,
?he played tennis all morning?). The corpus rec-
ognizes 100 such explicit connectives and contains
annotations for 19,458 explicit relations
1
.
The PDTB also contains provisions for the an-
notation of implicit discourse relations between
adjacent sentences which are inferred by the reader
but are not overtly marked by a discourse connec-
tive. In this case, the annotator was asked to pro-
vide a connective that best captured the inferred
relation. There are a total of 16,584 implicit rela-
tions annotated in the corpus.
2
In addition to discourse relations and their ar-
guments, the PDTB also provides the senses of
each relation(Miltsakaki et al, 2008). The tagset
of senses is organized hierarchically into three lev-
els - class, type, and subtype. The top class level
contains the four major semantic classes: Expan-
sion, Comparison, Contingency and Temporal.
1
The PDTB allows annotators to tag a relation with multi-
ple senses. In this work we count both of the annotated senses.
So even though there are only 18,459 explicit relations, there
are 19,458 explicit senses.
2
Again, because of multiple senses per relation, the 16,584
senses are part of 16,224 relations.
Class Explicit (%) Implicit (%) Total
Comparison 5590 (69.05%) 2505 (30.95%) 8095
Contingency 3741 (46.75%) 4261 (53.25%) 8002
Temporal 3696 (79.55%) 950 (20.45%) 4646
Expansion 6431 (42.04%) 8868 (57.96%) 15299
Table 1: Discourse relation distribution in seman-
tic and explicit/implicit classes in the PDTB
3 Distribution and ambiguity of
connectives
Table 1 shows the distribution of discourse rela-
tions between the four main relation classes and
their type of realization (implicit or explicit). In-
terestingly, temporal and comparison relations are
predominantly explicit. About 80% and 70%, re-
spectively, of their occurrences are marked by a
discourse connective. The contingency relations
are almost evenly distributed between explicit and
implicit. The expansion relations, the overall
largest class of discourse relations, is in most cases
implicit and not marked by a discourse connective.
Given the figures in Table 1, we would expect
that overall temporal and comparison relations will
be more easily identified since they are overtly
marked. Of course this would only be the case if
discourse markers are mostly unambiguous.
Here we show all connectives that appear more
than 50 times in the PDTB, their predominant
sense (comparison, contingency, temporal or ex-
pansion), as well as the percentage of occurrences
of the connective in its predominant sense. For
example the connective but has comparison as its
predominant sense and 97.19% of the 3,308 occur-
rences of this connective were comparisons.
Comparison but (3308; 97.19%), while (781; 66.07%),
however (485; 99.59%), although (328; 99.70%),
though (320; 100.00%), still (190; 98.42%), yet (101;
97.03%)
Expansion and (3000; 96.83%), also (1746; 99.94%), for
example (196; 100.00%), in addition (165; 100.00%),
instead (112; 97.32%), indeed (104; 95.19%), more-
over (101; 100.00%), for instance (98, 100.00%), or
(98; 96.94%), unless (95; 98.95%), in fact (82; 92.68%)
separately (74; 100.00%)
Contingency if (1223; 95.99%), because (858, 100.00%),
so (263; 100.00%), since (184; 52.17%), thus (112;
100.00%), as a result (78; 100.00%)
Temporal when (989; 80.18%), as (743; 70.26%), af-
ter (577; 99.65%), then (340; 93.24%), before (326;
100.00%), meanwhile (193; 48.70%), until (162;
87.04%), later (91; 98.90%), once (84; 95.24%)
The connectives that signal comparison and
contingency are mostly unambiguous. Obvious
exceptions are two of the connectives that are often
used to signal temporal relations: while and since.
88
The predominant senses of these connectives are
comparison (66.07%) and contingency (52.17%)
respectively. Disambiguating these problematic
connectives has already been addressed in previ-
ous work (Miltsakaki et al, 2005), but even the
predominantly temporal connectives are rather am-
biguous. For example less than 95% of the occur-
rances of meanwhile, as, when, until, and then are
temporal relaions.
While some connectives such as ?since? are am-
biguous, most are not. The discourse connec-
tives in the corpus appear in their predominant
sense 93.43% (for comparsion), 94.72% (for con-
tingency), 84.10% (for temporal), and 97.63% (for
expansion) of the time. Temporal connectives are
most ambiguous and connectives signaling expan-
sion are least ambiguous.
4 Automatic classification
The analyses in the previous section show two very
positive trends: many of the discourse relations are
explicitly marked by the use of a discourse connec-
tive, especially comparison and temporal relations,
and discourse connectives are overall mostly un-
ambiguous. These facts would suggest that even
based only on the connective, classification of dis-
course relations could be done well for all data (in-
cluding both implicit and explicit examples) and
particularly well for explicit examples alone. In-
deed, Table 2 shows the performance of a decision
tree classifier for discourse relations, on all data
and on the explicit subset in the second and third
column respectively.
We use the natural distribution of relation
classes found in theWall Street Journal texts, with-
out downsampling to get balanced distribution of
classes. There are four task settings, distinguishing
each type of relation from all others. For example,
comparison relations can be distinguished from all
other relations in the corpus with overall accuracy
of 91.28%, based only on the discourse connective
(first line in Table 2). The recall for recognizing
comparison relations is 0.66, directly reflecting the
fact that 31% of all comparison relations are im-
plicit (Table 1) and the connective feature did not
help at all in those cases. Over explicit data only,
the classification accuracy for comparison relation
versus any other relation is 97.23%, and precision
and recall is 0.95 and above.
As expected, the overall accuracy of identify-
ing contingency and expansion relations is lower,
Task All relations Explicit relations only
Comparison 91.28% (76.54%) 97.23% (69.72%)
Contingency 84.44% (76.81%) 93.99% (79.73%)
Temporal 94.79% (86.54%) 95.4% (79.98%)
Expansion 77.51% (55.67%) 97.61% (65.16%)
Table 2: Decision tree classification accuracy us-
ing only the presence of connectives as binary fea-
tures. The majority class is given in brackets.
Class Precision Recall
Temporal 0.841 [0.841] 0.729 [0.903]
Expansion 0.658 [0.973] 0.982 [0.957]
Contingency 0.948 [0.947] 0.369 [0.844]
Comparison 0.935 [0.935] 0.671 [0.971]
Table 3: Four-way classification. The first number
is for all data, thesecond for explicit relations only.
84.44% and 77.51% on all data respectively, re-
flecting the fact that these relations are often im-
plicit. But by themselves these accuracy numbers
are actually reasonable, setting a rather high base-
line for any more sophisticated method of classify-
ing discourse relations. On explicit data only, the
two-way classification accuracy for the four main
types of relations is 94% and higher.
In four-way classification, disambiguating be-
tween the four main semantic types of discourse
relations leads to 74.74% classification accuracy.
The accuracy for four-way classification of explicit
relations is 93.09%. The precision and recall for
each class is shown in Table 4. The worst per-
formance on the explicit portion of the data is the
precision for temporal relations and the recall for
contingency relations, both of which are 0.84.
5 N-gram discourse relation models
We have shown above that some relations, such as
comparison, can be easily identified because they
are often explicit and are expressed by an unam-
biguous connective. However, one must build a
more subtle automatic classifier to find the implicit
relations. We now look at the frequencies in which
various relations are adjacent in the PDTB. Results
from previous studies of discourse relations sug-
gest that the context of a relation can be helpful in
disambiguating the relation (Wellner et al, 2006).
Here we identify specific dependencies that exist
between sequences of relations.
We computed ?
2
statistics to test the indepen-
dence of each pair of relations. The question is:
do relations A and B occur adjacent to each other
more than they would simply due to chance? The
89
First Relation Second Relation ?
2
p-value
E. Comparison I. Contingency 20.1 .000007
E. Comparison E. Comparison 17.4 .000030
E. Comparison I. Expansion 9.91 .00161
I. Temporal E. Temporal 9.42 .00214
I. Contingency E. Contingency 9.29 .00230
I. Expansion E. Expansion 6.34 .0118
E. Expansion I. Expansion 5.50 .0191
I. Contingency E. Comparison 4.95 .0260
Table 4: ?
2
results for pairs of relations
pairs of implicit and explicit relations which have
significant associations with each other (pval <
0.05) are shown in Table 4. For example, ex-
plicit comparison and implicit contingency co-
occur much more often than would be expected if
they were independent. As explicit comparisons
are generally fairly easy to identify, knowing that
they tend to co-occur may be helpful when search-
ing for implicit contingency relations in a text.
6 Conclusion
We have tried to summarize the difficulty of find-
ing discourse relations using the Penn Discourse
Treebank. We noted that explicit and implicit rela-
tions are approximately evenly distributed overall,
making the task easier than many researchers have
feared. We have found that some relations, such as
temporal and comparison, are more likely to be ex-
plicit than implicit, making them relatively easier
to find, while contingency and expansion are more
often implicit. Among the discourse connectives,
the majority are not very ambiguous between the
different types of relations, with some notable ex-
ceptions such as since and meanwhile.
We have carried out a novel quantitative study
of the patterns of dependencies between discourse
relations. We found that while there does not ap-
pear to be a clear template for the sequence of
relations, there are individual relation pairs that
tend to co-occur. Specifically, we found that even
though contingency relations are likely to be im-
plicit and thus difficult to find, they are likely to
be found near an explicit comparison. We plan to
exploit these findings in future work, addressing
discourse relation labeling in text as a sequence la-
beling problem and using the explicit cue words
of surrounding relations as features for finding the
?hidden? implicit relations.
7 Acknowledgments
This work was partially supported by an Integra-
tive Graduate Education and Research Traineeship
grant from National Science Foundation (NSF-
IGERT 0504487) and by NSF Grant IIS -07-
05671. We would like to thank Nikhil Dinesh for
help with the PDTB, and Rashmi Prasad, Bonnie
Webber and Eleni Miltsakaki for insightful discus-
sions.
References
Hobbs, J. 1979. Coherence and coreference. Cognitive
Science, 3:67?90.
Knott, A. and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmat-
ics, 30(2):135?175.
Mann, W. and S. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text orga-
nization. Text, 8:243?281.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McKeown, Kathleen R. 1985. Text generation: us-
ing discourse strategies and focus constraints to gen-
erate natural language text. Cambridge University
Press, New York, NY, USA.
Miltsakaki, E., N. Dinesh, R. Prasad, A. Joshi, and
B. Webber. 2005. Experiments on sense annotations
and sense disambiguation of discourse connectives.
In Proceedings of the Fourth Workshop on Treebanks
and Linguistic Theories (TLT2005).
Miltsakaki, Eleni, Livio Robaldo, Alan Lee, and Ar-
avind Joshi. 2008. Sense annotation in the penn dis-
course treebank. Computational Linguistics and In-
telligent Text Processing, Lecture Notes in Computer
Science, 4919:275?286.
Moore, J. and P. Wiemer-Hastings. 2003. Discourse in
computational linguistics and artificial intelligence.
Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC).
Wellner, B., J. Pustejovsky, C. Havasi, R. Sauri, and
A. Rumshisky. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGDIAL Workshop on Discourse and Dialogue.
90
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186?195,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Revisiting Readability: A Unified Framework for Predicting Text Quality
Emily Pitler
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler@seas.upenn.edu
Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Abstract
We combine lexical, syntactic, and discourse
features to produce a highly predictive model
of human readers? judgments of text readabil-
ity. This is the first study to take into ac-
count such a variety of linguistic factors and
the first to empirically demonstrate that dis-
course relations are strongly associated with
the perceived quality of text. We show that
various surface metrics generally expected to
be related to readability are not very good pre-
dictors of readability judgments in our Wall
Street Journal corpus. We also establish that
readability predictors behave differently de-
pending on the task: predicting text readabil-
ity or ranking the readability. Our experi-
ments indicate that discourse relations are the
one class of features that exhibits robustness
across these two tasks.
1 Introduction
The quest for a precise definition of text quality?
pinpointing the factors that make text flow and easy
to read?has a long history and tradition. Way back
in 1944 Robert Gunning Associates was set up, of-
fering newspapers, magazines and business firms
consultations on clear writing (Gunning, 1952).
In education, teaching good writing technique and
grading student writing has always been of key
importance (Spandel, 2004; Attali and Burstein,
2006). Linguists have also studied various aspects of
text flow, with cohesion-building devices in English
(Halliday and Hasan, 1976), rhetorical structure the-
ory (Mann and Thompson, 1988) and centering the-
ory (Grosz et al, 1995) among the most influential
contributions.
Still, we do not have unified computational mod-
els that capture the interplay between various as-
pects of readability. Most studies focus on a sin-
gle factor contributing to readability for a given in-
tended audience. The use of rare words or technical
terminology for example can make text difficult to
read for certain audience types (Collins-Thompson
and Callan, 2004; Schwarm and Ostendorf, 2005;
Elhadad and Sutaria, 2007). Syntactic complexity
is associated with delayed processing time in un-
derstanding (Gibson, 1998) and is another factor
that can decrease readability. Text organization (dis-
course structure), topic development (entity coher-
ence) and the form of referring expressions also de-
termine readability. But we know little about the rel-
ative importance of each factor and how they com-
bine in determining perceived text quality.
In our work we use texts from the Wall Street
Journal intended for an educated adult audience
to analyze readability factors including vocabulary,
syntax, cohesion, entity coherence and discourse.
We study the association between these features and
reader assigned readability ratings, showing that dis-
course and vocabulary are the factors most strongly
linked to text quality. In the easier task of text qual-
ity ranking, entity coherence and syntax features
also become significant and the combination of fea-
tures allows for ranking prediction accuracy of 88%.
Our study is novel in the use of gold-standard dis-
course features for predicting readability and the si-
multaneous analysis of various readability factors.
186
2 Related work
2.1 Readability with respect to intended
readers
The definition of what one might consider to be
a well-written and readable text heavily depends
on the intended audience (Schriver, 1989). Obvi-
ously, even a superbly written scientific paper will
not be perceived as very readable by a lay person
and a great novel might not be appreciated by a
third grader. As a result, the vast majority of prior
work on readability deals with labeling texts with
the appropriate school grade level. A key observa-
tion in even the oldest work in this area is that the
vocabulary used in a text largely determines its read-
ability. More common words are easier, so some
metrics measured text readability by the percent-
age of words that were not among the N most fre-
quent in the language. It was also observed that fre-
quently occurring words are often short, so word
length was used to approximate readability more
robustly than using a predefined word frequency
list. Standard indices were developed based on the
link between word frequency/length and readabil-
ity, such as Flesch-Kincaid (Kincaid, 1975), Auto-
mated Readability Index (Kincaid, 1975), Gunning
Fog (Gunning, 1952), SMOG (McLaughlin, 1969),
and Coleman-Liau (Coleman and Liau, 1975). They
use only a few simple factors that are designed to
be easy to calculate and are rough approximations
to the linguistic factors that determine readability.
For example, Flesch-Kincaid uses the average num-
ber of syllables per word to approximate vocabulary
difficulty and the average number of words per sen-
tence to approximate syntactic difficulty.
In recent work, the idea of linking word frequency
and text readability has been explored for making
medical information more accessible to the general
public. (Elhadad and Sutaria, 2007) classified words
in medical texts as familiar or unfamiliar to a gen-
eral audience based on their frequencies in corpora.
When a description of the unfamiliar terms was pro-
vided, the perceived readability of the texts almost
doubled.
A more general and principled approach to using
vocabulary information for readability decisions has
been the use of language models. For any given text,
it is easy to compute its likelihood under a given lan-
guage model, i.e. one for text meant for children,
or for text meant for adults, or for a given grade
level. (Si and Callan, 2001), (Collins-Thompson and
Callan, 2004), (Schwarm and Ostendorf, 2005), and
(Heilman et al, 2007) used language models to pre-
dict the suitability of texts for a given school grade
level. But even for this type of task other factors
besides vocabulary use are at play in determining
readability. Syntactic complexity is an obvious fac-
tor: indeed (Heilman et al, 2007) and (Schwarm and
Ostendorf, 2005) also used syntactic features, such
as parse tree height or the number of passive sen-
tences, to predict reading grade levels. For the task
of deciding whether a text is written for an adult or
child reader, (Barzilay and Lapata, 2008) found that
adding entity coherence to (Schwarm and Ostendorf,
2005)?s list of features improves classification accu-
racy by 10%.
2.2 Readability as coherence for competent
language users
In linguistics and natural language processing, the
text properties rather than those of the reader are em-
phasized. Text coherence is defined as the ease with
which a person (tacitly assumed to be a competent
language user) understands a text. Coherent text is
characterized by various types of cohesive links that
facilitate text comprehension (Halliday and Hasan,
1976).
In recent work, considerable attention has been
devoted to entity coherence in text quality, espe-
cially in relation to information ordering. In many
applications such as text generation and summariza-
tion, systems need to decide the order in which se-
lected sentences or generated clauses should be pre-
sented to the user. Most models attempting to cap-
ture local coherence between sentences were based
on or inspired by centering theory (Grosz et al,
1995), which postulated strong links between the
center of attention in comprehension of adjacent
sentences and syntactic position and form of refer-
ence. In a detailed study of information ordering
in three very different corpora, (Karamanis et al, to
appear) assessed the performance of various formu-
lations of centering. Their results were somewhat
unexpected, showing that while centering transition
preferences were useful, the most successful strat-
egy for information ordering was based on avoid-
187
ing rough shifts, that is, sequences of sentences that
share no entities in common. This supports previous
findings that such types of transitions are associated
with poorly written text and can be used to improve
the accuracy of automatic grading of essays based
on various non-discourse features (Miltsakaki and
Kukich, 2000). In a more powerful generalization
of centering, Barzilay and Lapata (2008) developed
a novel approach which doesn?t postulate a prefer-
ence for any type of transition but rather computes
a set of features that capture transitions of all kinds
in the text and their relative proportion. Their en-
tity coherence features prove to be very suitable for
various tasks, notably for information ordering and
reading difficulty level.
Form of reference is also important in well-
written text and appropriate choices lead to im-
proved readability. Use of pronouns for reference
to highly salient entities is perceived as more de-
sirable than the use of definite noun phrases (Gor-
don et al, 1993; Krahmer and Theune, 2002). The
syntactic forms of first mention?when an entity is
first introduced in a text?differ from those of subse-
quent mentions (Poesio and Vieira, 1998; Nenkova
and McKeown, 2003) and can be exploited for im-
proving and predicting text coherence (Siddharthan,
2003; Nenkova and McKeown, 2003; Elsner and
Charniak, 2008).
3 Data
The objective of our study is to analyze various
readability factors, including discourse relations, be-
cause few empirical studies exist that directly link
discourse structure with text quality. In the past,
subsections of the Penn Treebank (Marcus et al,
1994) have been annotated for discourse relations
(Carlson et al, 2001; Wolf and Gibson, 2005). For
our study we chose to work with the newly released
Penn Discourse Treebank which is the largest anno-
tated resource which focuses exclusively on implicit
local relations between adjacent sentences and ex-
plicit discourse connectives.
3.1 Discourse annotation
The Penn Discourse Treebank (Prasad et al, 2008)
is a new resource with annotations of discourse con-
nectives and their senses in the Wall Street Journal
portion of the Penn Treebank (Marcus et al, 1994).
All explicit relations (those marked with a discourse
connective) are annotated. In addition, each adjacent
pair of sentences within a paragraph is annotated. If
there is a discourse relation, then it is marked im-
plicit and annotated with one or more connectives. If
there is a relation between the sentences but adding a
connective would be inappropriate, it is marked Al-
tLex. If the consecutive sentences are only related
by entity-based coherence (Knott et al, 2001) they
are annotated with EntRel. Otherwise, they are an-
notated with NoRel.
Besides labeling the connective, the PDTB also
annotates the sense of each relation. The relations
are organized into a hierarchy. The top level rela-
tions are Expansion, Comparison, Contingency, and
Temporal. Briefly, an expansion relation means that
the second clause continues the theme of the first
clause, a comparison relation indicates that some-
thing in the two clauses is being compared, contin-
gency means that there is a causal relation between
the clauses, and temporal means they occur either at
the same time or sequentially.
3.2 Readability ratings
We randomly selected thirty articles from the Wall
Street Journal corpus that was used in both the Penn
Treebank and the Penn Discourse Treebank.1 Each
article was read by at least three college students,
each of whom was given unlimited time to read the
texts and perform the ratings.2 Subjects were asked
the following questions:
? How well-written is this article?
? How well does the text fit together?
? How easy was it to understand?
? How interesting is this article?
For each question, they provided a rating between 1
and 5, with 5 being the best and 1 being the worst.
1One of the selected articles was missing from the Penn
Treebank. Thus, results that do not require syntactic informa-
tion (Tables 1, 2, 4, and 6) are over all thirty articles, while
Tables 3, 5, and 7 report results for the twenty-nine articles with
Treebank parse trees.
2(Lapata, 2006) found that human ratings are significantly
correlated with self-paced reading times, a more direct measure
of processing effort which we plan to explore in future work.
188
After collecting the data, it turned out that most of
the time subjects gave the same rating to all ques-
tions. For competent language users, we view text
readability and text coherence as equivalent prop-
erties, measuring the extent to which a text is well
written. Thus for all subsequent analysis, we will
use only the first question (?On a scale of 1 to 5,
how well written is this text??). The score of an arti-
cle was then the average of all the ratings it received.
The article scores ranged from 1.5 to 4.33, with a
mean of 3.2008 and a standard deviation of .7242.
The median score was 3.286.
We define our task as predicting this average rat-
ing for each article. Note that this task may be
more difficult than predicting reading level, as each
of these articles appeared in the Wall Street Journal
and thus is aimed at the same target audience. We
suspected that in classifying adult text, more subtle
features might be necessary.
4 Identifying correlates of text quality
4.1 Baseline measures
We first computed the Pearson correlation coeffi-
cients between the simple metrics that most tradi-
tional readability formulas use and the average hu-
man ratings. These results are shown in Table 1. We
tested the average number of characters per word,
average number of words per sentence, maximum
number of words per sentence, and article length
(F7).3 Article length (F7) was the only significant
baseline factor, with correlation of -0.37. Longer ar-
ticles are perceived as less well-written and harder
to read than shorter ones. None of the other baseline
metrics were close to being significant predictors of
readability.
Average Characters/Word r = -.0859, p = .6519
Average Words/Sentence r = .1637, p = .3874
Max Words/Sentence r = .0866, p = .6489
F7 text length r = -.3713, p = .0434
Table 1: Baseline readability features
3For ease of reference, we number each non-baseline feature
in the text and tables.
4.2 Vocabulary
We use a unigram language model, where the prob-
ability of an article is:
?
w
P (w|M)C(w) (1)
P (w|M) is the probability of word-type w accord-
ing to a background corpus M , and C(w) is the
number of times w appears in the article.
The log likelihood of an article is then:
?
w
C(w) log(P (w|M)) (2)
Note that this model will be biased in favor of
shorter articles. Since each word has probability less
than 1, the log probability of each word is less than
0, and hence including additional words decreases
the log likelihood. We compensate for this by per-
forming linear regressions with the unigram log like-
lihood and with the number of words in the article as
an additional variable.
The question then arises as to what to use as a
background corpus. We chose to experiment with
two corpora: the entire Wall Street Journal corpus
and a collection of general AP news, which is gen-
erally more diverse than the financial news found in
the WSJ. We predicted that the NEWS vocabulary
would be more representative of the types of words
our readers would be familiar with. In both cases we
used Laplace smoothing over the word frequencies
and a stoplist.
The vocabulary features we used are article like-
lihood estimated from a language model from WSJ
(F5), and article likelihood according to a unigram
language model from NEWS (F6). We also combine
the two likelihood features with article length, in or-
der to get a better estimate of the language model?s
influence on readability independent of the length of
the article.
F5 Log likelihood, WSJ r = .3723, p = .0428
F6 Log likelihood, NEWS r= .4497, p = .0127
LL with length, WSJ r = .3732, p = .0422
LL with length, NEWS r = .6359, p = .0002
Table 2: Vocabulary features
Both vocabulary-based features (F5 and F6) are
significantly correlated with the readability judg-
ments, with p-values smaller than 0.05 (see Table 2).
189
The correlations are positive: the more probable an
article was based on its vocabulary, the higher it was
generally rated. As expected, the NEWS model that
included more general news stories had a higher cor-
relation with people?s judgments. When combined
with the length of the article, the unigram language
model from the NEWS corpus becomes very predic-
tive of readability, with the correlation between the
two as high as 0.63.
4.3 Syntactic features
Syntactic constructions affect processing difficulty
and so might also affect readability judgments.
We examined the four syntactic features used in
(Schwarm and Ostendorf, 2005): average parse tree
height (F1), average number of noun phrases per
sentence (F2), average number of verb phrases per
sentence (F3), and average number of subordinate
clauses per sentence(SBARs in the Penn Treebank
tagset) (F4). The sentence ?We?re talking about
years ago [SBAR before anyone heard of asbestos
having any questionable properties].? contains an
example of an SBAR clause.
Having multiple noun phrases (entities) in each
sentence requires the reader to remember more
items, but may make the article more interesting.
(Barzilay and Lapata, 2008) found that articles writ-
ten for adults tended to contain many more entities
than articles written for children. While including
more verb phrases in each sentence increases the
sentence complexity, adults might prefer to have re-
lated clauses explicitly grouped together.
F1 Average Parse Tree Height r = -.0634, p = .7439
F2 Average Noun Phrases r = .2189, p = .2539
F3 Average Verb Phrases r = .4213, p = .0228
F4 Average SBARs r = .3405, p = .0707
Table 3: Syntax-related features
The correlations between readability and syntac-
tic features is shown in Table 3. The strongest corre-
lation is that between readability and number of verb
phrases (0.42). This finding is in line with prescrip-
tive clear writing advice (Gunning, 1952; Spandel,
2004), but is to our knowledge novel in the compu-
tational linguistics literature. As (Bailin and Graf-
stein, 2001) point out, the sentences in (1) are eas-
ier to comprehend than the sentences in (2), even
though they are longer.
(1) It was late at night, but it was clear. The stars
were out and the moon was bright.
(2) It was late at night. It was clear. The stars were
out. The moon was bright.
Multiple verb phrases in one sentence may be in-
dicative of explicit discourse relations, which we
will discuss further in section 4.6.
Surprisingly, the use of clauses introduced
by a (possibly empty) subordinating conjunction
(SBAR), are actually positively correlated (and al-
most approaching significance) with readability. So
while for children or less educated adults these con-
structions might pose difficulties, they were favored
by our assessors. On the other hand, the average
parse tree height negatively correlated with readabil-
ity as expected, but surprisingly the correlation is
very weak (-0.06).
4.4 Elements of lexical cohesion
In their classic study of cohesion in English, (Hal-
liday and Hasan, 1976) discuss the various aspects
of well written discourse, including the use of cohe-
sive devices such as pronouns, definite descriptions
and topic continuity from sentence to sentence.4 To
measure the association between these features and
readability rankings, we compute the number of pro-
nouns per sentence (F11) and the number of defi-
nite articles per sentence (F12). In order to qual-
ify topic continuity from sentence to sentence in
the articles, we compute average cosine similarity
(F8), word overlap (F9) and word overlap over just
nouns and pronouns (F10) between pairs of adjacent
sentences5. Each sentence is turned into a vector
of word-types, where each type?s value is its tf-idf
(where document frequency is computed over all the
articles in the WSJ corpus). The cosine similarity
metric is then:
cos (s, t) =
s ? t
|s| |t|
(3)
4Other cohesion building devises discussed by Halliday
and Hansan include lexical reiteration and discourse relations,
which we address next.
5Similar features have been used for automatic essay grad-
ing as well (Higgins et al, 2004).
190
F8 Avr. Cosine Overlap r = -.1012, p = .5947
F9 Avr. Word Overlap r = -.0531, p = .7806
F10 Avr. Noun+Pronoun Overlap r = .0905, p = .6345
F11 Avr. # Pronouns/Sent r = .2381, p = .2051
F12 Avr # Definite Articles r = .2309, p = .2196
Table 4: Superficial measures of topic continuity and pro-
noun and definite description use
None of these features correlate significantly with
readability as can be seen from the results in Ta-
ble 4. The overlap features are particularly bad
predictors of readability, with average word/cosine
overlap in fact being negatively correlated with read-
ability. The form of reference?use of pronouns
and definite descriptions?exhibit a higher correla-
tion with readability (0.23), but these values are not
significant for the size of our corpus.
4.5 Entity coherence
We use the Brown Coherence Toolkit6 to compute
entity grids (Barzilay and Lapata, 2008) for each ar-
ticle. In each sentence, an entity is identified as the
subject (S), object (O), other (X) (for example, part
of a prepositional phrase), or not present (N). The
probability of each transition type is computed. For
example, an S-O transition occurs when an entity
is the subject in one sentence then an object in the
next; X-N transition occurs when an entity appears
in non-subject or object position in one sentence and
not present in the next, etc.7 The entity coherence
features are the probability of each of these pairs of
transitions, for a total of 16 features (F17?32; see
complete results in Table 5).
None of the entity grid features are significantly
correlated with the readability ratings. One very in-
teresting result is that the proportion of S-S transi-
tions in which the same entity was mentioned in sub-
ject position in two adjacent sentences, is negatively
correlated with readability. In centering theory, this
is considered the most coherent type of transition,
keeping the same center of attention. Moreover, the
feature most strongly correlated with readability is
the S-N transition (0.31) in which the subject of one
sentence does not appear at all in the following sen-
6http://www.cs.brown.edu/ melsner/manual.html
7The Brown Coherence Toolkit identifies NPs as the same
entity if they have identical head nouns.
F17 Prob. of S-S transition r = -.1287, p = .5059
F18 Prob. of S-O transition r = -.0427, p = .8261
F19 Prob. of S-X transition r = -.1450, p = .4529
F20 Prob. of S-N transition r = .3116, p = .0999
F21 Prob. of O-S transition r = .1131, p = .5591
F22 Prob. of O-O transition r = .0825, p = .6706
F23 Prob. of O-X transition r = .0744, p = .7014
F24 Prob. of O-N transition r = .2590, p = .1749
F25 Prob. of X-S transition r = .1732, p = .3688
F26 Prob. of X-O transition r = .0098, p = .9598
F27 Prob. of X-X transition r = -.0655, p = .7357
F28 Prob. of X-N transition r = .1319, p = .4953
F29 Prob. of N-S transition r = .1898, p = .3242
F30 Prob. of N-O transition r = .2577, p = .1772
F31 Prob. of N-X transition r = .1854, p = .3355
F32 Prob. of N-N transition r = -.2349, p = .2200
Table 5: Linear correlation between human readability
ratings and entity coherence.
tence. Of course, it is difficult to interpret the en-
tity grid features one by one, since they are inter-
dependent and probably it is the interaction of fea-
tures (relative proportions of transitions) that capture
overall readability patterns.
4.6 Discourse relations
Discourse relations are believed to be a major factor
in text coherence. We computed another language
model which is over discourse relations instead of
words. We treat each text as a bag of relations rather
than a bag of words. Each relation is annotated
for both its sense and how it is realized (implicit
or explicit). For example, one text might contain
{Implicit Comparison, Explicit Temporal, NoRel}.
We computed the probability of each of our articles
according to a multinomial model, where the proba-
bility of a text with n relation tokens and k relation
types is:
P (n)
n!
x1!...xk!
px11 ...p
xk
k (4)
P (n) is the probability of an article having length
n, xi is the number of times relation i appeared, and
pi is the probability of relation i based on the Penn
Discourse Treebank. P (n) is the maximum likeli-
hood estimation of an article having n discourse re-
lations based on the entire Penn Discourse Treebank
(the number of articles with exactly n discourse re-
lations, divided by the total number of articles).
191
The log likelihood of an article based on its dis-
course relations (F13) feature is defined as:
log(P (n)) + log(n!) +
k?
i=1
(xi log(pi)? log(xi!))
(5)
The multinomial distribution is particularly suit-
able, because it directly incorporates length, which
significantly affects readability as we discussed ear-
lier. It also captures patterns of relative frequency of
relations, unlike the simpler unigram model. Note
also that this equation has an advantage over the un-
igram model that was not present for vocabulary.
While every article contains at least one word, some
articles do not contain any discourse relations. Since
the PDTB annotated all explicit relations and re-
lations between adjacent sentences in a paragraph,
an article with no discourse connectives and only
single sentence paragraphs would not contain any
annotated discourse relations. Under the unigram
model, these articles? probabilities cannot be com-
puted. Under the multinomial model, the probabil-
ity of an article with zero relations is estimated as
Pr(N = 0), which can be calculated from the cor-
pus.
As in the case of vocabulary features, the presence
of more relations will lead to overall lower probabil-
ities so we also consider the number of discourse
relations (F14) and the log likelihood combined with
the number of relations as features. In order to iso-
late the effect of the type of discourse relation (ex-
plicitly expressed by a discourse connective such as
?because? or ?however? versus implicitly expressed
by adjacency), we also compute multinomial model
features for the explicit discourse relations (F15) and
over just the implicit discourse relations (F16).
F13 LogL of discourse rels r = .4835, p = .0068
F14 # of discourse relations r = -.2729, p = .1445
LogL of rels with # of rels r = .5409, p = .0020
# of relations with # of words r = .3819, p = .0373
F15 Explicit relations only r = .1528, p = .4203
F16 Implicit relations only r = .2403, p = .2009
Table 6: Discourse features
The likelihood of discourse relations in the text
under a multinomial model is very highly and sig-
nificantly correlated with readability ratings, espe-
cially after text length is taken into account. Cor-
relations are 0.48 and 0.54 respectively. The prob-
ability of the explicit relations alone is not a suffi-
ciently strong indicator of readability. This fact is
disappointing as the explicit relations can be iden-
tified much more easily in unannotated text (Pitler
et al, 2008). Note that the sequence of just the im-
plicit relations is also not sufficient. This observa-
tion implies that the proportion of explicit and im-
plicit relations may be meaningful but we leave the
exploration of this issue for later work.
4.7 Summary of findings
So far, we introduced six classes of factors that have
been discussed in the literature as readability cor-
relates. Through statistical tests of associations we
identified the individual factors significantly corre-
lated with readability ratings. These are, in decreas-
ing order of association strength:
LogL of Discourse Relations (r = .4835)
LogL, NEWS (r= .4497)
Average Verb Phrases (.4213)
LogL, WSJ (r = .3723)
Number of words (r = -.3713)
Vocabulary and discourse relations are the
strongest predictors of readability, followed by aver-
age number of verb phrases and length of the text.
This empirical confirmation of the significance of
discourse relations as a readability factor is novel for
the computational linguistics literature. Note though
that for our work we use oracle discourse annota-
tions directly from the PDTB and no robust systems
for automatic discourse annotation exist today.
The significance of the average number of verb
phrases as a readability predictor is somewhat sur-
prising but intriguing. It would lead to reexamina-
tion of the role of verbs/predicates in written text,
which we also plan to address in future work. None
of the other factors showed significant association
with readability ratings, even though some correla-
tions had relatively large positive values.
5 Combining readability factors
In this section, we turn to the question of how the
combination of various factors improves the predic-
tion of readability. We use the leaps package in R
to find the best subset of features for linear regres-
sion, for subsets of size one to eight. We use the
192
squared multiple correlation coefficient (R2) to as-
sess the effectiveness of predictions. R2 is the pro-
portion of variance in readability ratings explained
by the model. If the model predicts readability per-
fectly, R2 = 1, and if the model has no predictive
capability, R2 = 0.
F13, R2 = 0.2662
F6 + F7, R2 = 0.4351
F6 + F7 + F13, R2 = 0.5029
F6 + F7 + F13 + F14, R2 = 0.6308
F1 + F6 + F7 + F10 + F13, R2 = 0.6939
F1 + F6 + F7 + F10 + F13 + F23, R2 = 0.7316
F1 + F6 + F7 + F10 + F13 + F22 + F23, R2 = 0.7557
F1+F6+F7+F10+F11+F13+F19+F30, R2 = 0.776.
The linear regression results confirm the expec-
tation that the combination of different factors is a
rather complex issue. As expected, discourse, vo-
cabulary and length which were the significant in-
dividual factors appear in the best model for each
feature set size. Their combination gives the best
result for regression with three predictors, and they
explain half of the variance in readability ratings,
R2 = 0.5029.
But the other individually significant feature, av-
erage number of verb phrases per sentence (F3)
never appears in the best models. Instead, F1?the
depth of the parse tree?appears in the best model
with more than four features.
Also unexpectedly, two of the superficial cohe-
sion features appear in the larger models: F10 is
the average word overlap over nouns and pronouns
and F11 is the average number of pronouns per sen-
tence. Entity grid features also make their way into
the best models when more features are used for pre-
diction: S-X, O-O, O-X, N-O transitions (F19, F22,
F23, F30).
6 Readability as ranking
In this section we consider the problem of pairwise
ranking of text readability. That is, rather than try-
ing to predict the readability of a single document,
we consider pairs of documents and predict which
one is better. This task may in fact be the more natu-
ral one, since in most applications the main concern
is with the relative quality of articles rather than their
absolute scores. This setting is also beneficial in
terms of data use, because each pair of articles with
different average readability scores now becomes a
data point for the classification task.
We thus create a classification problem: given two
articles, is article 1 more readable than article 2?
For each pair of texts whose readability ratings on
the 1 to 5 scale differed by at least 0.5, we form
one data point for the ranking problem, resulting in
243 examples. The predictors are the differences be-
tween the two articles? features. For classification,
we used WEKA?s linear support vector implemen-
tation (SMO) and performance was evaluated using
10-fold cross-validation.
Features Accuracy
None (Majority Class) 50.21%
ALL 88.88%
log l discourse rels 77.77%
number discourse rels 74.07%
N-O transition 70.78%
O-N transition 69.95%
Avg VPs sen 69.54%
log l NEWS 66.25%
number of words 65.84%
Grid only 79.42%
Discourse only 77.36%
Syntax only 74.07%
Vocab only 66.66%
Length only 65.84%
Cohesion only 64.60%
no cohesion 89.30%
no vocab 88.88%
no length 88.47%
no discourse 88.06%
no grid 84.36%
no syntax 82.71%
Table 7: SVM prediction accuracy, linear kernel
The classification results are shown in Table 7.
When all features are used for prediction, the ac-
curacy is high, 88.88%. The length of the article
can serve as a baseline feature?longer articles are
ranked lower by the assessors, so this feature can
be taken as baseline indicator of readability. Only
six features used by themselves lead to accuracies
higher than the length baseline. These results indi-
cate that the most important individual factors in the
readability ranking task, in decreasing order of im-
portance, are log likelihood of discourse relations,
number of discourse relations, N-O transitions, O-N
193
transitions, average number of VPs per sentence and
text probability under a general language model.
In terms of classes of features, the 16 entity
grid features perform the best, leading to an accu-
racy of 79.41%, followed by the combination of
the four discourse features (77.36%), and syntax
features (74.07%). This is evidence for the fact
that there is a complex interplay between readabil-
ity factors: the entity grid factors which individ-
ually have very weak correlation with readability
combine well, while adding the three additional dis-
course features to the likelihood of discourses rela-
tions actually worsens performance slightly. Simi-
lar indication for interplay between features is pro-
vided by the class ablation classification results, in
which classes of features are removed. Surprisingly,
removing syntactic features causes the biggest dete-
rioration in performance, a drop in accuracy from
88.88% to 82.71%. The removal of vocabulary,
length, or discourse features has a minimal negative
impact on performance, while removing the cohe-
sion features actually boosts performance.
7 Conclusion
We have investigated which linguistic features cor-
relate best with readability judgments. While sur-
face measures such as the average number of words
per sentence or the average number of characters
per word are not good predictors, there exist syn-
tactic, semantic, and discourse features that do cor-
relate highly. The average number of verb phrases
in each sentence, the number of words in the article,
the likelihood of the vocabulary, and the likelihood
of the discourse relations all are highly correlated
with humans? judgments of how well an article is
written.
While using any one out of syntactic, lexical, co-
herence, or discourse features is substantally better
than the baseline surface features on the discrim-
ination task, using a combination of entity coher-
ence and discourse relations produces the best per-
formance.
8 Acknowledgments
This work was partially supported by an Inte-
grative Graduate Education and Research Trainee-
ship grant from National Science Foundation (NS-
FIGERT 0504487) and by NSF Grant IIS-07-05671.
We thank Aravind Joshi, Bonnie Webber, and the
anonymous reviewers for their many helpful com-
ments.
References
Y. Attali and J. Burstein. 2006. Automated essay scoring
with e-rater v.2. The Journal of Technology, Learning
and Assessment, 4(3).
A. Bailin and A. Grafstein. 2001. The linguistic assump-
tions underlying readability formulae a critique. Lan-
guage and Communication, 21(3):285?301.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Lin-
guistics, 34(1):1?34.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proceedings of the
Second SIGdial Workshop, pages 1?10.
M. Coleman and TL Liau. 1975. A computer readabil-
ity formula designed for machine scoring. Journal of
Applied Psychology, 60(2):283?284.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT/NAACL?04.
Noemie Elhadad and Komal Sutaria. 2007. Mining a lex-
icon of technical terms and lay equivalents. In Biolog-
ical, translational, and clinical language processing,
pages 49?56, Prague, Czech Republic. Association for
Computational Linguistics.
M. Elsner and E. Charniak. 2008. Coreference-inspired
coherence modeling. In Proceedings of ACL-HLT?08,
(short paper).
E. Gibson. 1998. Linguistic complexity: locality of syn-
tactic dependencies. Cognition, 68:1?76.
P. Gordon, B. Grosz, and L. Gilliom. 1993. Pronouns,
names, and the centering of attention in discourse.
Cognitive Science, 17:311?347.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of dis-
course. Computational Linguistics, 21(2):203?226.
Robert Gunning. 1952. The technique of clear writing.
McGraw-Hill; Fouth Printing edition.
Michael A.K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman Group Ltd, London, U.K.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining Lexical and Grammatical
Features to Improve Readability Measures for First
and Second Language Texts. Proceedings of NAACL
HLT, pages 460?467.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student es-
says. In Proceedings of HLT/NAACL?04.
194
N. Karamanis, M. Poesio, C. Mellish, and J. Oberlander.
(to appear). Evaluating centering for information or-
dering using corpora. Computational Linguistics.
JP Kincaid. 1975. Derivation of New Readability For-
mulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Per-
sonnel.
A. Knott, J. Oberlander, M. ODonnell, and C. Mellish.
2001. Beyond elaboration: The interaction of relations
and focus in coherent text. Text representation: lin-
guistic and psycholinguistic aspects, pages 181?196.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In K. van
Deemter and R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language Genera-
tion and Interpretation, pages 223?264. CSLI Publi-
cations.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendalls tau. Computational Linguistics,
32(4):471?484.
W. Mann and S. Thompson. 1988. Rhetorical structure
theory: Towards a functional theory of text organiza-
tion. Text, 8.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
G.H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading, 12(8):639?646.
E. Miltsakaki and K. Kukich. 2000. The role of centering
theory?s rough-shift in the teaching and evaluation of
writing skills. In Proceedings of ACL?00, pages 408?
415.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. In Proceedings of
HLT/NAACL 2003 (short paper).
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse re-
lations. In Coling 2008: Companion volume: Posters
and Demonstrations, pages 85?88, Manchester, UK,
August.
M. Poesio and R. Vieira. 1998. A corpus-based investi-
gation of definite description use. Computational Lin-
guistics, 24(2):183?216.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The penn discourse
treebank 2.0. In Proceedings of LREC?08.
KA Schriver. 1989. Evaluating text quality: the con-
tinuum from text-focused toreader-focused methods.
Professional Communication, IEEE Transactions on,
32(4):238?255.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of ACL?05, pages
523?530.
L. Si and J. Callan. 2001. A statistical model for sci-
entific readability. Proceedings of the tenth interna-
tional conference on Information and knowledge man-
agement, pages 574?576.
A. Siddharthan. 2003. Syntactic simplification and Text
Cohesion. Ph.D. thesis, University of Cambridge, UK.
V. Spandel. 2004. Creating writers through 6-trait writ-
ing assessment and instruction. Allyn & Bacon.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational Lin-
guistics, 31(2):249?288.
195
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 306?314,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Automatically Evaluating Content Selection in Summarization without
Human Models
Annie Louis
University of Pennsylvania
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
We present a fully automatic method for
content selection evaluation in summariza-
tion that does not require the creation of
human model summaries. Our work capi-
talizes on the assumption that the distribu-
tion of words in the input and an informa-
tive summary of that input should be sim-
ilar to each other. Results on a large scale
evaluation from the Text Analysis Con-
ference show that input-summary compar-
isons are very effective for the evaluation
of content selection. Our automatic meth-
ods rank participating systems similarly
to manual model-based pyramid evalua-
tion and to manual human judgments of
responsiveness. The best feature, Jensen-
Shannon divergence, leads to a correlation
as high as 0.88 with manual pyramid and
0.73 with responsiveness evaluations.
1 Introduction
The most commonly used evaluation method for
summarization during system development and
for reporting results in publications is the auto-
matic evaluation metric ROUGE (Lin, 2004; Lin
and Hovy, 2003). ROUGE compares system sum-
maries against one or more model summaries
by computing n-gram word overlaps between the
two. The wide adoption of such automatic mea-
sures is understandable because they are conve-
nient and greatly reduce the complexity of eval-
uations. ROUGE scores also correlate well with
manual evaluations of content based on compar-
ison with a single model summary, as used in
the early editions of the Document Understanding
Conferences (Over et al, 2007).
In our work, we take the idea of automatic
evaluation to an extreme and explore the feasi-
bility of developing a fully automatic evaluation
method for content selection that does not make
use of human model summaries at all. To this end,
we show that evaluating summaries by comparing
them with the input obtains good correlations with
manual evaluations for both query focused and up-
date summarization tasks.
Our results have important implications for fu-
ture development of summarization systems and
their evaluation.
High correlations between system ranking pro-
duced with the fully automatic method and
manual evaluations show that the new eval-
uation measures can be used during system
development when human model summaries
are not available.
Our results provide validation of several features
that can be optimized in the development of
new summarization systems when the objec-
tive is to improve content selection on aver-
age, over a collection of test inputs. However,
none of the features is consistently predictive
of good summary content for individual in-
puts.
We find that content selection performance on
standard test collections can be approximated
well by the proposed fully automatic method.
This result greatly underlines the need to re-
quire linguistic quality evaluations alongside
content selection ones in future evaluations
and research.
2 Model-free methods for evaluation
Proposals for developing fully automatic methods
for summary evaluation have been put forward
in the past. Their attractiveness is obvious for
large scale evaluations, or for evaluation on non-
standard test sets for which human models are not
available.
306
For example in Radev et al (2003), a large
scale fully automatic evaluation of eight summa-
rization systems on 18,000 documents was per-
formed without any human effort. A search engine
was used to rank documents according to their rel-
evance to a given query. The summaries for each
document were also ranked for relevance with re-
spect to the same query. For good summariza-
tion systems, the relevance ranking of summaries
is expected to be similar to that of the full docu-
ments. Based on this intuition, the correlation be-
tween relevance rankings of summaries and orig-
inal documents was used to compare the different
systems. The approach was motivated by the as-
sumption that the distribution of terms in a good
summary is similar to the distribution of terms in
the original document.
Even earlier, Donaway et al (2000) suggested
that there are considerable benefits to be had in
adopting model-free methods of evaluation involv-
ing direct comparisons between the original docu-
ment and its summary. The motivation for their
work was the considerable variation in content se-
lection choices in model summaries (Rath et al,
1961). The identity of the model writer signifi-
cantly affects summary evaluations (also noted by
McKeown et al (2001), Jing et al (1998)) and
evaluations of the same systems can be rather dif-
ferent when different models are used. In their
experiments, Donaway et al (2000) demonstrated
that the correlations between manual evaluation
using a model summary and
a) manual evaluation using a different model
summary
b) automatic evaluation by directly comparing
input and summary1,
are the same. Their conclusion was that such au-
tomatic methods should be seriously considered as
an alternative to model based evaluation.
In this paper, we present a comprehensive study
of fully automatic summary evaluation without
any human models. A summary?s content is
judged for quality by directly estimating its close-
ness to the input. We compare several probabilistic
and information-theoretic approaches for charac-
terizing the similarity and differences between in-
put and summary content. A simple information-
theoretic measure, Jensen Shannon divergence be-
tween input and summary, emerges as the best fea-
1They used cosine similarity to perform the input-
summary comparison.
ture. System rankings produced using this mea-
sure lead to correlations as high as 0.88 with hu-
man judgements.
3 TAC summarization track
3.1 Query-focused and Update Summaries
Two types of summaries, query-focused and up-
date summaries, were evaluated in the summariza-
tion track of the 2008 Text Analysis Conference
(TAC)2. Query-focused summaries were produced
from input documents in response to a stated user
information need. The update summaries require
more sophistication: two sets of articles on the
same topic are provided. The first set of articles
represents the background of a story and users are
assumed to be already familiar with the informa-
tion contained in them. The update task is to pro-
duce a multi-document summary from the second
set of articles that can serve as an update to the
user. This task is reminiscent of the novelty de-
tection task explored at TREC (Soboroff and Har-
man, 2005).
3.2 Data
The test set for the TAC 2008 summarization task
contains 48 inputs. Each input consists of two sets
of 10 documents each, called docsets A and B.
Both A and B are on the same general topic but
B contains documents published later than those
in A. In addition, the user?s information need as-
sociated with each input is given by a query state-
ment consisting of a title and narrative. An exam-
ple query statement is shown below.
Title: Airbus A380
Narrative: Describe developments in the pro-
duction and launch of the Airbus A380.
A system must produce two summaries: (1) a
query-focused summary of docset A, (2) a compi-
lation of updates from docset B, assuming that the
user has read all the documents in A. The max-
imum length for both types of summaries is 100
words.
There were 57 participating systems in TAC
2008. We use the summaries and evaluations of
these systems for the experiments reported in the
paper.
3.3 Evaluation metrics
Both manual and automatic evaluations were con-
ducted at NIST to assess the quality of summaries
2http://www.nist.gov/tac
307
manual score R-1 recall R-2 recall
Query Focused summaries
pyramid score 0.859 0.905
responsiveness 0.806 0.873
Update summaries
pyramid score 0.912 0.941
responsiveness 0.865 0.884
Table 1: Spearman correlation between manual
scores and ROUGE-1 and ROUGE-2 recall. All
correlations are highly significant with p-value <
0.00001.
produced by the systems.
Pyramid evaluation: The pyramid evaluation
method (Nenkova and Passonneau, 2004) has been
developed for reliable and diagnostic assessment
of content selection quality in summarization and
has been used in several large scale evaluations
(Nenkova et al, 2007). It uses multiple human
models from which annotators identify seman-
tically defined Summary Content Units (SCU).
Each SCU is assigned a weight equal to the
number of human model summaries that express
that SCU. An ideal maximally informative sum-
mary would express a subset of the most highly
weighted SCUs, with multiple maximally infor-
mative summaries being possible. The pyramid
score for a system summary is equal to the ratio
between the sum of weights of SCUs expressed
in a summary (again identified manually) and the
sum of weights of an ideal summary with the same
number of SCUs.
Four human summaries provided by NIST for
each input and task were used for the pyramid
evaluation at TAC.
Responsiveness evaluation: Responsiveness of a
summary is a measure of overall quality combin-
ing both content selection and linguistic quality:
summaries must present useful content in a struc-
tured fashion in order to better satisfy the user?s
need. Assessors directly assigned scores on a
scale of 1 (poor summary) to 5 (very good sum-
mary) to each summary. These assessments are
done without reference to any model summaries.
The (Spearman) correlation between the pyramid
and responsiveness metrics is high but not perfect:
0.88 and 0.92 respectively for query focused and
update summarization.
ROUGE evaluation: NIST also evaluated the
summaries automatically using ROUGE (Lin,
2004; Lin and Hovy, 2003). Comparison between
a summary and the set of four model summaries
is computed using unigram (R1) and bigram over-
laps (R2)3. The correlations between ROUGE and
manual evaluations is shown in Table 1 and varies
between 0.80 and 0.94.
Linguistic quality evaluation: Assessors scored
summaries on a scale from 1 (very poor) to 5 (very
good) for five factors of linguistic quality: gram-
maticality, non-redundancy, referential clarity, fo-
cus, structure and coherence.
We do not make use of any of the linguistic
quality evaluations. Our work focuses on fully au-
tomatic evaluation of content selection, so man-
ual pyramid and responsiveness scores are used
for comparison with our automatic method. The
pyramid metric measures content selection exclu-
sively, while responsiveness incorporates at least
some aspects of linguistic quality.
4 Features for content evaluation
We describe three classes of features to compare
input and summary content: distributional simi-
larity, summary likelihood and use of topic signa-
tures. Both input and summary words were stop-
word filtered and stemmed before computing the
features.
4.1 Distributional Similarity
Measures of similarity between two probability
distributions are a natural choice for the task at
hand. One would expect good summaries to be
characterized by low divergence between proba-
bility distributions of words in the input and sum-
mary, and by high similarity with the input.
We experimented with three common measures:
KL and Jensen Shannon divergence and cosine
similarity. These three metrics have already been
applied for summary evaluation, albeit in differ-
ent contexts. In Lin et al (2006), KL and JS di-
vergences between human and machine summary
distributions were used to evaluate content selec-
tion. The study found that JS divergence always
outperformed KL divergence. Moreover, the per-
formance of JS divergence was better than stan-
dard ROUGE scores for multi-document summa-
rization when multiple human models were used
for the comparison.
The use of cosine similarity in Donaway et
al. (2000) is more directly related to our work.
They show that the difference between evaluations
3The scores were computed after stemming but stop
words were retained in the summaries.
308
based on two different human models is about the
same as the difference between system ranking
based on one model summary and the ranking pro-
duced using input-summary similarity. Inputs and
summaries were compared using only one metric:
cosine similarity.
Kullback Leibler (KL) divergence: The KL di-
vergence between two probability distributions P
and Q is given by
D(P ||Q) =
?
w
p
P
(w) log
2
p
P
(w)
p
Q
(w)
(1)
It is defined as the average number of bits wasted
by coding samples belonging to P using another
distribution Q, an approximate of P . In our case,
the two distributions are those for words in the
input and summary respectively. Since KL di-
vergence is not symmetric, both input-summary
and summary-input divergences are used as fea-
tures. In addition, the divergence is undefined
when p
P
(w) > 0 but p
Q
(w) = 0. We perform
simple smoothing to overcome the problem.
p(w) =
C + ?
N + ? ? B
(2)
Here C is the count of word w and N is the
number of tokens; B = 1.5|V |, where V is the
input vocabulary and ? was set to a small value
of 0.0005 to avoid shifting too much probability
mass to unseen events.
Jensen Shannon (JS) divergence: The JS diver-
gence incorporates the idea that the distance be-
tween two distributions cannot be very different
from the average of distances from their mean dis-
tribution. It is formally defined as
J(P ||Q) =
1
2
[D(P ||A) + D(Q||A)], (3)
where A = P+Q
2
is the mean distribution of P
and Q. In contrast to KL divergence, the JS dis-
tance is symmetric and always defined. We use
both smoothed and unsmoothed versions of the di-
vergence as features.
Similarity between input and summary: The
third metric is cosine overlap between the tf ? idf
vector representations (with max-tf normalization)
of input and summary contents.
cos? =
v
inp
.v
summ
||v
inp
||||v
summ
||
(4)
We compute two variants:
1. Vectors contain all words from input and
summary
2. Vectors contain only topic signatures from
the input and all words of the summary
Topic signatures are words highly descriptive of
the input, as determined by the application of log-
likelihood test (Lin and Hovy, 2000). Using only
topic signatures from the input to represent text is
expected to be more accurate because the reduced
vector has fewer dimensions compared with using
all the words from the input.
4.2 Summary likelihood
The likelihood of a word appearing in the sum-
mary is approximated as being equal to its proba-
bility in the input. We compute both a summary?s
unigram probability as well as its probability un-
der a multinomial model.
Unigram summary probability:
(p
inp
w
1
)
n
1
(p
inp
w
2
)
n
2
...(p
inp
w
r
)
n
r (5)
where p
inp
w
i
is the probability in the input of
word w
i
, n
i
is the number of times w
i
appears
in the summary, and w
1
...w
r
are all words in the
summary vocabulary.
Multinomial summary probability:
N !
n
1
!n
2
!...n
r
!
(p
inp
w
1
)
n
1
(p
inp
w
2
)
n
2
...(p
inp
w
r
)
n
r (6)
where N = n
1
+ n
2
+ ...+ n
r
is the total number
of words in the summary.
4.3 Use of topic words in the summary
Summarization systems that directly optimize for
more topic signatures during content selection
have fared very well in evaluations (Conroy et al,
2006). Hence the number of topic signatures from
the input present in a summary might be a good
indicator of summary content quality. We experi-
ment with two features that quantify the presence
of topic signatures in a summary:
1. Fraction of the summary composed of input?s
topic signatures.
2. Percentage of topic signatures from the input
that also appear in the summary.
While both features will obtain higher values
for summaries containing many topic words, the
first is guided simply by the presence of any topic
word while the second measures the diversity of
topic words used in the summary.
309
4.4 Feature combination using linear
regression
We also evaluated the performance of a linear re-
gression metric combining all of the above fea-
tures. The value of the regression-based score for
each summary was obtained using a leave-one-
out approach. For a particular input and system-
summary combination, the training set consisted
only of examples which included neither the same
input nor the same system. Hence during training,
no examples of either the test input or system were
seen.
5 Correlations with manual evaluations
In this section, we report the correlations between
system ranking using our automatic features and
the manual evaluations. We studied the predictive
power of features in two scenarios.
MACRO LEVEL; PER SYSTEM: The values of fea-
tures were computed for each summary submitted
for evaluation. For each system, the feature values
were averaged across all inputs. All participating
systems were ranked based on the average value.
Similarly, the average manual score, pyramid or
responsiveness, was also computed for each sys-
tem. The correlations between the two rankings
are shown in Tables 2 and 4.
MICRO LEVEL; PER INPUT: The systems were
ranked for each input separately, and correlations
between the summary rankings for each input
were computed (Table 3).
The two levels of analysis address different
questions: Can we automatically identify sys-
tem performance across all test inputs (macro
level) and can we identify which summaries for a
given input were good and which were bad (mi-
cro level)? For the first task, the answer is a defi-
nite ?yes? while for the second task the results are
mixed.
In addition, we compare our results to model-
based evaluations using ROUGE and analyze the
effects of stemming the input and summary vo-
cabularies. In order to allow for in-depth discus-
sion, we will analyze our findings only for query
focused summaries. Similar results were obtained
for the evaluation of update summaries and are de-
scribed in Section 7.
5.1 Performance at macro level
Table 2 shows the Spearman correlation between
manual and automatic scores averaged across the
Features pyramid respons.
JS div -0.880 -0.736
JS div smoothed -0.874 -0.737
% of input topic words 0.795 0.627
KL div summ-inp -0.763 -0.694
cosine overlap 0.712 0.647
% of summ = topic wd 0.712 0.602
topic overlap 0.699 0.629
KL div inp-summ -0.688 -0.585
mult. summary prob. 0.222 0.235
unigram summary prob. -0.188 -0.101
regression 0.867 0.705
ROUGE-1 recall 0.859 0.806
ROUGE-2 recall 0.905 0.873
Table 2: Spearman correlation on macro level for
the query focused task. All results are highly sig-
nificant with p-values < 0.000001 except unigram
and multinomial summary probability, which are
not significant even at the 0.05 level.
48 inputs. We find that both distributional simi-
larity and the topic signature features produce sys-
tem rankings very similar to those produced by hu-
mans. Summary probabilities, on the other hand,
turn out to be unpredictive of content selection
performance. The linear regression combination
of features obtains high correlations with manual
scores but does not lead to better results than the
single best feature: JS divergence.
JS divergence outperforms other features in-
cluding the regression metric and obtains the best
correlations with both types of manual scores, 0.88
with pyramid score and 0.74 with responsiveness.
The regression metric performs comparably with
correlations of 0.86 and 0.70. The correlations ob-
tained by both JS divergence and the regression
metric with pyramid evaluations are in fact better
than that obtained by ROUGE-1 recall (0.85).
The best topic signature based feature?
percentage of input?s topic signatures that are
present in the summary?ranks next only to JS di-
vergence and regression. The correlation between
this feature and pyramid and responsiveness eval-
uations is 0.79 and 0.62 respectively. The propor-
tion of summary content composed of topic words
performs worse as an evaluation metric with cor-
relations 0.71 and 0.60. This result indicates that
summaries that cover more topics from the input
are judged to have better content than those in
which fewer topics are mentioned.
Cosine overlaps and KL divergences obtain
good correlations but still lower than JS diver-
gence or percentage of input topic words. Further,
rankings based on unigram and multinomial sum-
310
mary probabilities do not correlate significantly
with manual scores.
5.2 Performance on micro level
On a per input basis, the proposed metrics are not
that effective in distinguishing which summaries
have better content. The minimum and maximum
correlations with manual evaluations across the 48
inputs are given in Table 3. The number and per-
centage of inputs for which correlations were sig-
nificant are also reported.
Now, JS divergence obtains significant correla-
tions with pyramid scores for 73% of the inputs
but for particular inputs, the correlation can be
as low as 0.27. The results are worse for other
features and for comparison with responsiveness
scores.
At the micro level, combining features with re-
gression gives the best result overall, in contrast to
the findings for the macro level setting. This re-
sult has implications for system development; no
single feature can reliably predict good content for
a particular input. Even a regression combination
of all features is a significant predictor of content
selection quality in only 77% of the cases.
We should note however, that our features are
based only on the distribution of terms in the in-
put and therefore less likely to inform good con-
tent for all input types. For example, a set of
documents each describing different opinion on a
given issue will likely have less repetition on both
lexical and content unit level. The predictiveness
of features like ours will be limited for such in-
puts4. However, model summaries written for the
specific input would give better indication of what
information in the input was important and inter-
esting. This indeed is the case as we shall see in
Section 6.
Overall, the micro level results suggest that the
fully automatic measures we examined will not be
useful for providing information about summary
quality for an individual input. For averages over
many test sets, the fully automatic evaluations give
more reliable and useful results, highly correlated
with rankings produced by manual evaluations.
4In fact, it would be surprising to find an automatically
computable feature or feature combination which would be
able to consistently predict good content for all individual in-
puts. If such features existed, an ideal summarization system
would already exist.
5.3 Effects of stemming
The analysis presented so far is on features com-
puted after stemming the input and summary
words. We also computed the values of the same
features without stemming and found that diver-
gence metrics benefit greatly when stemming is
done. The biggest improvements in correlations
are for JS and KL divergences with respect to re-
sponsiveness. For JS divergence, the correlation
increases from 0.57 to 0.73 and for KL divergence
(summary-input), from 0.52 to 0.69.
Before stemming, the topic signature and bag
of words overlap features are the best predictors
of responsiveness (correlations are 0.63 and 0.64
respectively) but do not change much after stem-
ming (topic overlap?0.62, bag of words?0.64).
Divergences emerge as better metrics only after
stemming.
Stemming also proves beneficial for the likeli-
hood features. Before stemming, their correlations
are directed in the wrong direction, but they im-
prove after stemming to being either positive or
closer to zero. However, even after stemming,
summary probabilities are not good predictors of
content quality.
5.4 Difference in correlations: pyramid and
responsiveness scores
Overall, we find that correlations with pyramid
scores are higher than correlations with respon-
siveness. Clearly our features are designed to
compare input-summary content only. Since re-
sponsiveness judgements were based on both con-
tent and linguistic quality of summaries, it is not
surprising that these rankings are harder to repli-
cate using our content based features. Neverthe-
less, responsiveness scores are dominated by con-
tent quality and the correlation between respon-
siveness and JS divergence is high, 0.73.
Clearly, metrics of linguistic quality should be
integrated with content evaluations to allow for
better predictions of responsiveness. To date, few
attempts have been made to automatically eval-
uate linguistic quality in summarization. Lapata
and Barzilay (2005) proposed a method for co-
herence evaluation which holds promise but has
not been validated so far on large datasets such
as those used in TAC and DUC. In a simpler ap-
proach, Conroy and Dang (2008) use higher order
ROUGE scores to approximate both content and
linguistic quality.
311
pyramid responsiveness
features max min no. significant (%) max min no. significant (%)
JS div -0.714 -0.271 35 (72.9) -0.654 -0.262 35 (72.9)
JS div smoothed -0.712 -0.269 35 (72.9) -0.649 -0.279 33 (68.8)
KL div summ-inp -0.736 -0.276 35 (72.9) -0.628 -0.261 35 (72.9)
% of input topic words 0.701 0.286 31 (64.6) 0.693 0.279 29 (60.4)
cosine overlap 0.622 0.276 31 (64.6) 0.618 0.265 28 (58.3)
KL div inp-summ -0.628 -0.262 28 (58.3) -0.577 -0.267 22 (45.8)
topic overlap 0.597 0.265 30 (62.5) 0.689 0.277 26 (54.2)
% summary = topic wd 0.607 0.269 23 (47.9) 0.534 0.272 23 (47.9)
mult. summary prob. 0.434 0.268 8 (16.7) 0.459 0.272 10 (20.8)
unigram summary prob. 0.292 0.261 2 ( 4.2) 0.466 0.287 2 (4.2)
regression 0.736 0.281 37 (77.1) 0.642 0.262 32 (66.7)
ROUGE-1 recall 0.833 0.264 47 (97.9) 0.754 0.266 46 (95.8)
ROUGE-2 recall 0.875 0.316 48 (100) 0.742 0.299 44 (91.7)
Table 3: Spearman correlations at micro level (query focused task). Only the minimum, maximum
values of the significant correlations are reported together with the number and percentage of significant
correlations.
update input only avg. update & background
features pyramid respons. pyramid respons.
JS div -0.827 -0.764 -0.716 -0.669
JS div smoothed -0.825 -0.764 -0.713 -0.670
% of input topic words 0.770 0.709 0.677 0.616
KL div summ-inp -0.749 -0.709 -0.651 -0.624
KL div inp-summ -0.741 -0.717 -0.644 -0.638
cosine overlap 0.727 0.691 0.649 0.631
% of summary = topic wd 0.721 0.707 0.647 0.636
topic overlap 0.707 0.674 0.645 0.619
mult. summmary prob. 0.284 0.355 0.152 0.224
unigram summary prob. -0.093 0.038 -0.151 -0.053
regression 0.789 0.605 0.699 0.522
ROUGE-1 recall 0.912 0.865 . .
ROUGE-2 recall 0.941 0.884 . .
regression combining features comparing with background and update inputs (without averaging)
correlations = 0.8058 with pyramid, 0.6729 with responsiveness
Table 4: Spearman correlations at macro level for update summarization. Results are reported separately
for features comparing update summaries with the update input only or with both update and background
inputs and averaging the two.
6 Comparison with ROUGE
For manual pyramid scores, the best correlation,
0.88, we observed in our experiments was with
JS divergence. This result is unexpectedly high
for a fully automatic evaluation metric. Note that
the best correlation between pyramid scores and
ROUGE (for R2) is 0.90, practically identical with
JS divergence. For ROUGE-1, the correlation is
0.85.
In the case of manual responsiveness, which
combines aspects of linguistic quality along with
content selection evaluation, the correlation with
JS divergence is 0.73. For ROUGE, it is 0.80
for R1 and 0.87 for R2. Using higher order n-
grams is obviously beneficial as observed from the
differences between unigram and bigram ROUGE
scores. So a natural extension of our features
would be to use distance between bigram distri-
butions. At the same time, for responsiveness,
ROUGE-1 outperforms all the fully automatic fea-
tures. This is evidence that the model summaries
provide information that is unlikely to ever be ap-
proximated by information from the input alone,
regardless of feature sophistication.
At the micro level, ROUGE does clearly better
than all the automatic measures. The results are
shown in the last two rows of Table 3. ROUGE-1
recall obtains significant correlations for over 95%
of inputs for responsiveness and 98% of inputs for
pyramid evaluation compared to 73% (JS diver-
gence) and 77% (regression). Undoubtedly, at the
input level, comparison with model summaries is
substantially more informative.
When reference summaries are available,
ROUGE provides scores that agree best with hu-
man judgements. However, when model sum-
312
maries are not available, our features can provide
reliable estimates of system quality when averaged
over a set of test inputs. For predictions at the level
of individual inputs, our fully automatic features
are less useful.
7 Update Summarization
In Table 4, we report the performance of our fea-
tures for system evaluation on the update task. The
column, ?update input only? summarizes the cor-
relations obtained by features comparing the sum-
maries with only the update inputs (set B). We
also compared the summaries individually to the
update and background (set A) inputs. The two
sets of features were then combined by a) averag-
ing (?avg. update and background?) and b) linear
regression (last line of Table 4).
As in the case of query focused summarization,
JS divergence and percentage of input topic sig-
natures in summary are the best features for the
update task as well. The overall best feature is
JS divergence between the update input and the
summaries?correlations of 0.82 and 0.76 with
pyramid and responsiveness.
Interestingly, the features combining both up-
date and background inputs do not lead to better
correlations than those obtained using the update
input only. The best performance from combined
features is given by the linear regression metric.
Although the correlation of this regression feature
with pyramid scores (0.80) is comparable to JS di-
vergence with update inputs, its correlation with
responsiveness (0.67) is clearly lower. These re-
sults show that the term distributions in the update
input are sufficiently good predictors of content
for update summaries. The role of the background
input appears to be negligable.
8 Discussion
We have presented a successful framework for
model-free evaluations of content which uses the
input as reference. The power of model-free eval-
uations generalizes across at least two summariza-
tion tasks: query focused and update summariza-
tion.
We have analyzed a variety of features for input-
summary comparison and demonstrated that the
strength of different features varies considerably.
Similar term distributions in the input and the sum-
mary and diverse use of topic signatures in the
summary are highly indicative of good content.
We also find that preprocessing like stemming im-
proves the performance of KL and JS divergence
features.
Very good results were obtained from a corre-
lation analysis with human judgements, showing
that input can indeed substitute for model sum-
maries and manual efforts in summary evaluation.
The best correlations were obtained by a single
feature, JS divergence (0.88 with pyramid scores
and 0.73 with responsiveness at system level).
Our best features can therefore be used to eval-
uate the content selection performance of systems
in a new domain where model summaries are un-
available. However, like all other content evalua-
tion metrics, our features must be accompanied by
judgements of linguistic quality to obtain whole-
some indicators of summary quality and system
performance. Evidence for this need is provided
by the lower correlations with responsiveness than
the content-only pyramid evaluations.
The results of our analysis zero in on JS diver-
gence and topic signature as desirable objectives to
optimize during content selection. On the macro
level, they are powerful predictors of content qual-
ity. These findings again emphasize the need for
always including linguistic quality as a component
of evaluation.
Observations from our input-based evaluation
also have important implications for the design of
novel summarization tasks. We find that high cor-
relations with manual evaluations are obtained by
comparing query-focused summaries with the en-
tire input and making no use of the query at all.
Similarly in the update summarization task, the
best predictions of content for update summaries
were obtained using only the update input. The
uncertain role of background inputs and queries
expose possible problems with the task designs.
Under such conditions, it is not clear if query-
focused content selection or ability to compile up-
dates are appropriately captured by any evaluation.
References
J. Conroy and H. Dang. 2008. Mind the gap: Dangers
of divorcing evaluations of summary content from
linguistic quality. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 145?152.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
ACL, short paper.
313
R. Donaway, K. Drummey, and L. Mather. 2000. A
comparison of rankings produced by summarization
evaluation measures. In NAACL-ANLP Workshop
on Automatic Summarization.
H. Jing, R. Barzilay, K. Mckeown, and M. Elhadad.
1998. Summarization evaluation methods: Experi-
ments and analysis. In In AAAI Symposium on Intel-
ligent Summarization, pages 60?68.
M. Lapata and R. Barzilay. 2005. Automatic evalua-
tion of text coherence: Models and representations.
In IJCAI?05.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of the 18th conference on Computational
linguistics, pages 495?501.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurance statistics. In
Proceedings of HLT-NAACL 2003.
C. Lin, G. Cao, J. Gao, and J. Nie. 2006. An
information-theoretic approach to automatic evalu-
ation of summaries. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 463?470.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In ACL Text Summarization
Workshop.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivas-
siloglou, B. Schiffman, and S. Teufel. 2001.
Columbia multi-document summarization: Ap-
proach and evaluation. In DUC?01.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarization: The pyramid
method. In HLT/NAACL.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The pyramid method: Incorporating human con-
tent selection variation in summarization evaluation.
ACM Trans. Speech Lang. Process., 4(2):4.
P. Over, H. Dang, and D. Harman. 2007. Duc in con-
text. Inf. Process. Manage., 43(6):1506?1520.
D. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer,
H. Qi, A. C?elebi, D. Liu, and E. Drabek. 2003.
Evaluation challenges in large-scale multi-document
summarization: the mead project. In Proceedings of
ACL 2003, Sapporo, Japan.
G. J. Rath, A. Resnick, and R. Savage. 1961. The
formation of abstracts by the selection of sentences:
Part 1: sentence selection by man and machines.
American Documentation, 2(12):139?208.
I. Soboroff and D. Harman. 2005. Novelty detec-
tion: the trec experience. In HLT ?05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 105?112.
314
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 139?147,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Predicting the fluency of text with shallow structural features: case studies
of machine translation and human-written text
Jieun Chae
University of Pennsylvania
chaeji@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
Sentence fluency is an important compo-
nent of overall text readability but few
studies in natural language processing
have sought to understand the factors that
define it. We report the results of an ini-
tial study into the predictive power of sur-
face syntactic statistics for the task; we use
fluency assessments done for the purpose
of evaluating machine translation. We
find that these features are weakly but sig-
nificantly correlated with fluency. Ma-
chine and human translations can be dis-
tinguished with accuracy over 80%. The
performance of pairwise comparison of
fluency is also very high?over 90% for a
multi-layer perceptron classifier. We also
test the hypothesis that the learned models
capture general fluency properties applica-
ble to human-written text. The results do
not support this hypothesis: prediction ac-
curacy on the new data is only 57%. This
finding suggests that developing a dedi-
cated, task-independent corpus of fluency
judgments will be beneficial for further in-
vestigations of the problem.
1 Introduction
Numerous natural language applications involve
the task of producing fluent text. This is a core
problem for surface realization in natural language
generation (Langkilde and Knight, 1998; Banga-
lore and Rambow, 2000), as well as an impor-
tant step in machine translation. Considerations
of sentence fluency are also key in sentence sim-
plification (Siddharthan, 2003), sentence compres-
sion (Jing, 2000; Knight and Marcu, 2002; Clarke
and Lapata, 2006; McDonald, 2006; Turner and
Charniak, 2005; Galley and McKeown, 2007), text
re-generation for summarization (Daume? III and
Marcu, 2004; Barzilay and McKeown, 2005; Wan
et al, 2005) and headline generation (Banko et al,
2000; Zajic et al, 2007; Soricut and Marcu, 2007).
Despite its importance for these popular appli-
cations, the factors contributing to sentence level
fluency have not been researched indepth. Much
more attention has been devoted to discourse-level
constraints on adjacent sentences indicative of co-
herence and good text flow (Lapata, 2003; Barzi-
lay and Lapata, 2008; Karamanis et al, to appear).
In many applications fluency is assessed in
combination with other qualities. For example, in
machine translation evaluation, approaches such
as BLEU (Papineni et al, 2002) use n-gram over-
lap comparisons with a model to judge overall
?goodness?, with higher n-grams meant to capture
fluency considerations. More sophisticated ways
to compare a system production and a model in-
volve the use of syntax, but even in these cases flu-
ency is only indirectly assessed and the main ad-
vantage of the use of syntax is better estimation of
the semantic overlap between a model and an out-
put. Similarly, the metrics proposed for text gener-
ation by (Bangalore et al, 2000) (simple accuracy,
generation accuracy) are based on string-edit dis-
tance from an ideal output.
In contrast, the work of (Wan et al, 2005)
and (Mutton et al, 2007) directly sets as a goal
the assessment of sentence-level fluency, regard-
less of content. In (Wan et al, 2005) the main
premise is that syntactic information from a parser
can more robustly capture fluency than language
models, giving more direct indications of the de-
gree of ungrammaticality. The idea is extended in
(Mutton et al, 2007), where four parsers are used
139
and artificially generated sentences with varying
level of fluency are evaluated with impressive suc-
cess. The fluency models hold promise for ac-
tual improvements in machine translation output
quality (Zwarts and Dras, 2008). In that work,
only simple parser features are used for the pre-
diction of fluency, but no actual syntactic prop-
erties of the sentences. But certainly, problems
with sentence fluency are expected to be mani-
fested in syntax. We would expect for example
that syntactic tree features that capture common
parse configurations and that are used in discrim-
inative parsing (Collins and Koo, 2005; Charniak
and Johnson, 2005; Huang, 2008) should be use-
ful for predicting sentence fluency as well. In-
deed, early work has demonstrated that syntac-
tic features, and branching properties in particular,
are helpful features for automatically distinguish-
ing human translations from machine translations
(Corston-Oliver et al, 2001). The exploration of
branching properties of human and machine trans-
lations was motivated by the observations during
failure analysis that MT system output tends to
favor right-branching structures over noun com-
pounding. Branching preference mismatch man-
ifest themselves in the English output when trans-
lating from languages whose branching properties
are radically different from English. Accuracy
close to 80% was achieved for distinguishing hu-
man translations from machine translations.
In our work we continue the investigation of
sentence level fluency based on features that cap-
ture surface statistics of the syntactic structure in
a sentence. We revisit the task of distinguishing
machine translations from human translations, but
also further our understanding of fluency by pro-
viding comprehensive analysis of the association
between fluency assessments of translations and
surface syntactic features. We also demonstrate
that based on the same class of features, it is possi-
ble to distinguish fluent machine translations from
disfluent machine translations. Finally, we test the
models on human written text in order to verify
if the classifiers trained on data coming from ma-
chine translation evaluations can be used for gen-
eral predictions of fluency and readability.
For our experiments we use the evaluations
of Chinese to English translations distributed by
LDC (catalog number LDC2003T17), for which
both machine and human translations are avail-
able. Machine translations have been assessed
by evaluators for fluency on a five point scale (5:
flawless English; 4: good English; 3: non-native
English; 2: disfluent English; 1: incomprehen-
sible). Assessments by different annotators were
averaged to assign overall fluency assessment for
each machine-translated sentence. For each seg-
ment (sentence), there are four human and three
machine translations.
In this setting we address four tasks with in-
creasing difficulty:
? Distinguish human and machine translations.
? Distinguish fluent machine translations from
poor machine translations.
? Distinguish the better (in terms of fluency)
translation among two translations of the
same input segment.
? Use the models trained on data from MT
evaluations to predict potential fluency prob-
lems of human-written texts (from the Wall
Street Journal).
Even for the last most challenging task results
are promising, with prediction accuracy almost
10% better than a random baseline. For the other
tasks accuracies are high, exceeding 80%.
It is important to note that the purpose of our
study is not evaluation of machine translation per
se. Our goal is more general and the interest is in
finding predictors of sentence fluency. No general
corpora exist with fluency assessments, so it seems
advantageous to use the assessments done in the
context of machine translation for preliminary in-
vestigations of fluency. Nevertheless, our findings
are also potentially beneficial for sentence-level
evaluation of machine translation.
2 Features
Perceived sentence fluency is influenced by many
factors. The way the sentence fits in the con-
text of surrounding sentences is one obvious factor
(Barzilay and Lapata, 2008). Another well-known
factor is vocabulary use: the presence of uncom-
mon difficult words are known to pose problems
to readers and to render text less readable (Collins-
Thompson and Callan, 2004; Schwarm and Osten-
dorf, 2005). But these discourse- and vocabulary-
level features measure properties at granularities
different from the sentence level.
Syntactic sentence level features have not been
investigated as a stand-alone class, as has been
140
done for the other types of features. This is why
we constrain our study to syntactic features alone,
and do not discuss discourse and language model
features that have been extensively studied in prior
work on coherence and readability.
In our work, instead of looking at the syntac-
tic structures present in the sentences, e.g. the
syntactic rules used, we use surface statistics of
phrase length and types of modification. The sen-
tences were parsed with Charniak?s parser (Char-
niak, 2000) in order to calculate these features.
Sentence length is the number of words in a sen-
tence. Evaluation metrics such as BLEU (Papineni
et al, 2002) have a built-in preference for shorter
translations. In general one would expect that
shorter sentences are easier to read and thus are
perceived as more fluent. We added this feature
in order to test directly the hypothesis for brevity
preference.
Parse tree depth is considered to be a measure
of sentence complexity. Generally, longer sen-
tences are syntactically more complex but when
sentences are approximately the same length the
larger parse tree depth can be indicative of in-
creased complexity that can slow processing and
lead to lower perceived fluency of the sentence.
Number of fragment tags in the sentence parse
Out of the 2634 total sentences, only 165 con-
tained a fragment tag in their parse, indicating
the presence of ungrammaticality in the sentence.
Fragments occur in headlines (e.g. ?Cheney will-
ing to hold bilateral talks if Arafat observes U.S.
cease-fire arrangement?) but in machine transla-
tion the presence of fragments can signal a more
serious problem.
Phrase type proportion was computed for
prepositional phrases (PP), noun phrases (NP)
and verb phrases (VP). The length in number of
words of each phrase type was counted, then di-
vided by the sentence length. Embedded phrases
were also included in the calculation: for ex-
ample a noun phrase (NP1 ... (NP2)) would
contribute length(NP1) + length(NP2) to the
phrase length count.
Average phrase length is the number of words
comprising a given type of phrase, divided by the
number of phrases of this type. It was computed
for PP, NP, VP, ADJP, ADVP. Two versions of
the features were computed?one with embedded
phrases included in the calculation and one just for
the largest phrases of a given type. Normalized av-
erage phrase length is computed for PP, NP and
VP and is equal to the average phrase length of
given type divided by the sentence length. These
were computed only for the largest phrases.
Phrase type rate was also computed for PPs,
VPs and NPs and is equal to the number of phrases
of the given type that appeared in the sentence, di-
vided by the sentence length. For example, the
sentence ?The boy caught a huge fish this morn-
ing? will have NP phrase number equal to 3/8 and
VP phrase number equal to 1/8.
Phrase length The number of words in a PP,
NP, VP, without any normalization; it is computed
only for the largest phrases. Normalized phrase
length is the average phrase length (for VPs, NPs,
PPs) divided by the sentence length. This was
computed both for longest phrase (where embed-
ded phrases of the same type were counted only
once) and for each phrase regardless of embed-
ding.
Length of NPs/PPs contained in a VP The aver-
age number of words that constitute an NP or PP
within a verb phrase, divided by the length of the
verb phrase. Similarly, the length of PP in NP was
computed.
Head noun modifiers Noun phrases can be very
complex, and the head noun can be modified in va-
riety of ways?pre-modifiers, prepositional phrase
modifiers, apposition. The length in words of
these modifiers was calculated. Each feature also
had a variant in which the modifier length was di-
vided by the sentence length. Finally, two more
features on total modification were computed: one
was the sum of all modifier lengths, the other the
sum of normalized modifier length.
3 Feature analysis
In this section, we analyze the association of the
features that we described above and fluency. Note
that the purpose of the analysis is not feature
selection?all features will be used in the later ex-
periments. Rather, the analysis is performed in or-
der to better understand which factors are predic-
tive of good fluency.
The distribution of fluency scores in the dataset
is rather skewed, with the majority of the sen-
tences rated as being of average fluency 3 as can
be seen in Table 1.
Pearson?s correlation between the fluency rat-
ings and features are shown in Table 2. First of all,
fluency and adequacy as given by MT evaluators
141
Fluency score The number of sentences
1 ? fluency < 2 7
1 ? fluency < 2 295
2 ? fluency < 3 1789
3 ? fluency < 4 521
4 ? fluency < 5 22
Table 1: Distribution of fluency scores.
are highly correlated (0.7). This is surprisingly
high, given that separate fluency and adequacy as-
sessments were elicited with the idea that these
are qualities of the translations that are indepen-
dent of each other. Fluency was judged directly by
the assessors, while adequacy was meant to assess
the content of the sentence compared to a human
gold-standard. Yet, the assessments of the two
aspects were often the same?readability/fluency
of the sentence is important for understanding the
sentence. Only after the assessor has understood
the sentence can (s)he judge how it compares to
the human model. One can conclude then that a
model of fluency/readability that will allow sys-
tems to produce fluent text is key for developing a
successful machine translation system.
The next feature most strongly associated with
fluency is sentence length. Shorter sentences are
easier and perceived as more fluent than longer
ones, which is not surprising. Note though that the
correlation is actually rather weak. It is only one
of various fluency factors and has to be accommo-
dated alongside the possibly conflicting require-
ments shown by the other features. Still, length
considerations reappear at sub-sentential (phrasal)
levels as well.
Noun phrase length for example has almost the
same correlation with fluency as sentence length
does. The longer the noun phrases, the less fluent
the sentence is. Long noun phrases take longer to
interpret and reduce sentence fluency/readability.
Consider the following example:
? [The dog] jumped over the fence and fetched the ball.
? [The big dog in the corner] fetched the ball.
The long noun phrase is more difficult to read,
especially in subject position. Similarly the length
of the verb phrases signal potential fluency prob-
lems:
? Most of the US allies in Europe publicly [object to in-
vading Iraq]V P .
? But this [is dealing against some recent remarks of
Japanese financial minister, Masajuro Shiokawa]V P .
VP distance (the average number of words sep-
arating two verb phrases) is also negatively corre-
lated with sentence fluency. In machine transla-
tions there is the obvious problem that they might
not include a verb for long stretches of text. But
even in human written text, the presence of more
verbs can make a difference in fluency (Bailin and
Grafstein, 2001). Consider the following two sen-
tences:
? In his state of the Union address, Putin also talked
about the national development plan for this fiscal year
and the domestic and foreign policies.
? Inside the courtyard of the television station, a recep-
tion team of 25 people was formed to attend to those
who came to make donations in person.
The next strongest correlation is with unnormal-
ized verb phrase length. In fact in terms of correla-
tions, in turned out that it was best not to normal-
ize the phrase length features at all. The normal-
ized versions were also correlated with fluency,
but the association was lower than for the direct
count without normalization.
Parse tree depth is the final feature correlated
with fluency with correlation above 0.1.
4 Experiments with machine translation
data
4.1 Distinguishing human from machine
translations
In this section we use all the features discussed in
Section 2 for several classification tasks. Note that
while we discussed the high correlation between
fluency and adequacy, we do not use adequacy in
the experiments that we report from here on.
For all experiments we used four of the classi-
fiers in Weka?decision tree (J48), logistic regres-
sion, support vector machines (SMO), and multi-
layer perceptron. All results are for 10-fold cross
validation.
We extracted the 300 sentences with highest flu-
ency scores, 300 sentences with lowest fluency
scores among machine translations and 300 ran-
domly chosen human translations. We then tried
the classification task of distinguishing human and
machine translations with different fluency quality
(highest fluency scores vs. lowest fluency score).
We expect that low fluency MT will be more easily
142
adequacy sentence length unnormalized NP length VP distance
0.701(0.00) -0.132(0.00) -0.124(0.00) -0.116(0.00)
unnormalized VP length Max Tree depth phrase length avr. NP length (embedded)
-0.109(0.00) -0.106(0.00) -0.105(0.00) -0.097(0.00)
avr. VP length (embedded) SBAR length avr. largest NP length Unnormalized PP
-0.094(0.00) -0.086(0.00) -0.084(0.00) -0.082(0.00)
avr PP length (embedded) SBAR count PP length in VP Normalized PP1
-0.070(0.00) -0.069(0.001) -0.066(0.001) 0.065(0.001)
NP length in VP PP length normalized VP length PP length in NP
-0.058(0.003) -0.054(0.006) 0.054(0.005) 0.053(0.006)
Fragment avr. ADJP length (embedded) avr. largest VP length
-0.049(0.011) -0.046(0.019) -0.038(0.052)
Table 2: Pearson?s correlation coefficient between fluency and syntactic phrasing features. P-values are
given in parenthesis.
worst 300 MT best 300 MT total MT (5920)
SMO 86.00% 78.33% 82.68%
Logistic reg. 77.16% 79.33% 82.68%
MLP 78.00% 82% 86.99%
Decision Tree(J48) 71.67 % 81.33% 86.11%
Table 3: Accuracy for the task of distinguishing machine and human translations.
distinguished from human translation in compari-
son with machine translations rated as having high
fluency.
Results are shown in Table 3. Overall the
best classifier is the multi-layer perceptron. On
the task using all available data of machine and
human translations, the classification accuracy is
86.99%. We expected that distinguishing the ma-
chine translations from the human ones will be
harder when the best translations are used, com-
pared to the worse translations, but this expecta-
tion is fulfilled only for the support vector machine
classifier.
The results in Table 3 give convincing evi-
dence that the surface structural statistics can dis-
tinguish very well between fluent and non-fluent
sentences when the examples come from human
and machine-produced text respectively. If this is
the case, will it be possible to distinguish between
good and bad machine translations as well? In or-
der to answer this question, we ran one more bi-
nary classification task. The two classes were the
300 machine translations with highest and lowest
fluency respectively. The results are not as good as
those for distinguishing machine and human trans-
lation, but still significantly outperform a random
baseline. All classifiers performed similarly on the
task, and achieved accuracy close to 61%.
4.2 Pairwise fluency comparisons
We also considered the possibility of pairwise
comparisons for fluency: given two sentences,
can we distinguish which is the one scored more
highly for fluency. For every two sentences, the
feature for the pair is the difference of features of
the individual sentences.
There are two ways this task can be set up. First,
we can use all assessed translations and make pair-
ings for every two sentences with different fluency
assessment. In this setting, the question being ad-
dressed is Can sentences with differing fluency be
distinguished?, without regard to the sources of
the sentence. The harder question is Can a more
fluent translation be distinguished from a less flu-
ent translation of the same sentence?
The results from these experiments can be seen
in Table 4. When any two sentences with differ-
ent fluency assessments are paired, the prediction
accuracy is very high: 91.34% for the multi-layer
perceptron classifier. In fact all classifiers have ac-
curacy higher than 80% for this task. The surface
statistics of syntactic form are powerful enough to
distinguishing sentences of varying fluency.
The task of pairwise comparison for translations
of the same input is more difficult: doing well on
this task would be equivalent to having a reliable
measure for ranking different possible translation
variants.
In fact, the problem is much more difficult as
143
Task J48 Logistic Regression SMO MLP
Any pair 89.73% 82.35% 82.38% 91.34%
Same Sentence 67.11% 70.91% 71.23% 69.18%
Table 4: Accuracy for pairwise fluency comparison. ?Same sentence? are comparisons constrained
between different translations of the same sentences, ?any pair? contains comparisons of sentences with
different fluency over the entire data set.
can be seen in the second row of Table 4. Lo-
gistic regression, support vector machines and
multi-layer perceptron perform similarly, with
support vector machine giving the best accuracy
of 71.23%. This number is impressively high, and
significantly higher than baseline performance.
The results are about 20% lower than for predic-
tion of a more fluent sentence when the task is not
constrained to translation of the same sentence.
4.3 Feature analysis: differences among tasks
In the previous sections we presented three varia-
tions involving fluency predictions based on syn-
tactic phrasing features: distinguishing human
from machine translations, distinguishing good
machine translations from bad machine transla-
tions, and pairwise ranking of sentences with dif-
ferent fluency. The results differ considerably and
it is interesting to know whether the same kind
of features are useful in making the three distinc-
tions.
In Table 5 we show the five features with largest
weight in the support vector machine model for
each task. In many cases, certain features appear
to be important only for particular tasks. For ex-
ample the number of prepositional phrases is an
important feature only for ranking different ver-
sions of the same sentence but is not important for
other distinctions. The number of appositions is
helpful in distinguishing human translations from
machine translations, but is not that useful in the
other tasks. So the predictive power of the features
is very directly related to the variant of fluency dis-
tinctions one is interested in making.
5 Applications to human written text
5.1 Identifying hard-to-read sentences in
Wall Street Journal texts
The goal we set out in the beginning of this pa-
per was to derive a predictive model of sentence
fluency from data coming from MT evaluations.
In the previous sections, we demonstrated that
indeed structural features can enable us to per-
form this task very accurately in the context of
machine translation. But will the models conve-
niently trained on data from MT evaluation be at
all capable to identify sentences in human-written
text that are not fluent and are difficult to under-
stand?
To answer this question, we performed an ad-
ditional experiment on 30 Wall Street Journal ar-
ticles from the Penn Treebank that were previ-
ously used in experiments for assessing overall
text quality (Pitler and Nenkova, 2008). The arti-
cles were chosen at random and comprised a to-
tal of 290 sentences. One human assessor was
asked to read each sentence and mark the ones that
seemed disfluent because they were hard to com-
prehend. These were sentences that needed to be
read more than once in order to fully understand
the information conveyed in them. There were 52
such sentences. The assessments served as a gold-
standard against which the predictions of the flu-
ency models were compared.
Two models trained on machine translation data
were used to predict the status of each sentence in
the WSJ articles. One of the models was that for
distinguishing human translations from machine
translations (human vs machine MT), the other
was the model for distinguishing the 300 best from
the 300 worst machine translations (good vs bad
MT). The classifiers used were decision trees for
human vs machine distinction and support vector
machines for good vs bad MT. For the first model
sentences predicted to belong to the ?human trans-
lation? class are considered fluent; for the second
model fluent sentences are the ones predicted to be
in the ?best MT? class.
The results are shown in Table 6. The two
models vastly differ in performance. The model
for distinguishing machine translations from hu-
man translations is the better one, with accuracy
of 57%. For both, prediction accuracy is much
lower than when tested on data from MT evalu-
ations. These findings indicate that building a new
144
MT vs HT good MT vs Bad MT Ranking Same sentence Ranking
unnormalized PP SBAR count avr. NP lengt normalized NP length
PP length in VP Unnormalized VP length normalized PP length PP count
avr. NP length post attribute length NP count normalized NP length
# apposition VP count normalized NP length max tree depth
SBAR length sentence length normalized VP length avr. phrase length
Table 5: The five features with highest weights in the support vector machine model for the different
tasks.
Model Acc P R
human vs machine trans. 57% 0.79 0.58
good MT vs bad MT 44% 0.57 0.44
Table 6: Accuracy, precision and recall (for fluent
class) for each model when test on WSJ sentences.
The gold-standard is assessment by a single reader
of the text.
corpus for the finer fluency distinctions present in
human-written text is likely to be more beneficial
than trying to leverage data from existing MT eval-
uations.
Below, we show several example sentences on
which the assessor and the model for distinguish-
ing human and machine translations (dis)agreed.
Model and assessor agree that sentence is prob-
lematic:
(1.1) The Soviet legislature approved a 1990 budget yes-
terday that halves its huge deficit with cuts in defense spend-
ing and capital outlays while striving to improve supplies to
frustrated consumers.
(1.2) Officials proposed a cut in the defense budget this
year to 70.9 billion rubles (US$114.3 billion) from 77.3 bil-
lion rubles (US$125 billion) as well as large cuts in outlays
for new factories and equipment.
(1.3) Rather, the two closely linked exchanges have been
drifting apart for some years, with a nearly five-year-old
moratorium on new dual listings, separate and different list-
ing requirements, differing trading and settlement guidelines
and diverging national-policy aims.
The model predicts the sentence is good, but the
assessor finds it problematic:
(2.1) Moody?s Investors Service Inc. said it lowered the
ratings of some $145 million of Pinnacle debt because of
?accelerating deficiency in liquidity,? which it said was ev-
idenced by Pinnacle?s elimination of dividend payments.
(2.2) Sales were higher in all of the company?s business
categories, with the biggest growth coming in sales of food-
stuffs such as margarine, coffee and frozen food, which rose
6.3%.
(2.3) Ajinomoto predicted sales in the current fiscal year
ending next March 31 of 480 billion yen, compared with
460.05 billion yen in fiscal 1989.
The model predicts the sentences are bad, but
the assessor considered them fluent:
(3.1) The sense grows that modern public bureaucracies
simply don?t perform their assigned functions well.
(3.2) Amstrad PLC, a British maker of computer hardware
and communications equipment, posted a 52% plunge in pre-
tax profit for the latest year.
(3.3) At current allocations, that means EPA will be spend-
ing $300 billion on itself.
5.2 Correlation with overall text quality
In our final experiment we focus on the relation-
ship between sentence fluency and overall text
quality. We would expect that the presence of dis-
fluent sentences in text will make it appear less
well written. Five annotators had previously as-
sess the overall text quality of each article on a
scale from 1 to 5 (Pitler and Nenkova, 2008). The
average of the assessments was taken as a single
number describing the article. The correlation be-
tween this number and the percentage of fluent
sentences in the article according to the different
models is shown in Table 7.
The correlation between the percentage of flu-
ent sentences in the article as given by the human
assessor and the overall text quality is rather low,
0.127. The positive correlation would suggest that
the more hard to read sentence appear in a text,
the higher the text would be rated overall, which
is surprising. The predictions from the model for
distinguishing good and bad machine translations
very close to zero, but negative which corresponds
better to the intuitive relationship between the two.
Note that none of the correlations are actually
significant for the small dataset of 30 points.
6 Conclusion
We presented a study of sentence fluency based on
data from machine translation evaluations. These
data allow for two types of comparisons: human
(fluent) text and (not so good) machine-generated
145
Fluency given by Correlation
human 0.127
human vs machine trans. model -0.055
good MT vs bad MT model 0.076
Table 7: Correlations between text quality assess-
ment of the articles and the percentage of fluent
sentences according to different models.
text, and levels of fluency in the automatically pro-
duced text. The distinctions were possible even
when based solely on features describing syntac-
tic phrasing in the sentences.
Correlation analysis reveals that the structural
features are significant but weakly correlated with
fluency. Interestingly, the features correlated with
fluency levels in machine-produced text are not the
same as those that distinguish between human and
machine translations. Such results raise the need
for caution when using assessments for machine
produced text to build a general model of fluency.
The captured phenomena in this case might be
different than these from comparing human texts
with differing fluency. For future research it will
be beneficial to build a dedicated corpus in which
human-produced sentences are assessed for flu-
ency.
Our experiments show that basic fluency dis-
tinctions can be made with high accuracy. Ma-
chine translations can be distinguished from hu-
man translations with accuracy of 87%; machine
translations with low fluency can be distinguished
from machine translations with high fluency with
accuracy of 61%. In pairwise comparison of sen-
tences with different fluency, accuracy of predict-
ing which of the two is better is 90%. Results are
not as high but still promising for comparisons in
fluency of translations of the same text. The pre-
diction becomes better when the texts being com-
pared exhibit larger difference in fluency quality.
Admittedly, our pilot experiments with human
assessment of text quality and sentence level flu-
ency are small, so no big generalizations can be
made. Still, they allow some useful observations
that can guide future work. They do show that for
further research in automatic recognition of flu-
ency, new annotated corpora developed specially
for the task will be necessary. They also give
some evidence that sentence-level fluency is only
weakly correlated with overall text quality. Dis-
course apects and language model features that
have been extensively studied in prior work are in-
deed much more indicative of overall text quality
(Pitler and Nenkova, 2008). We leave direct com-
parison for future work.
References
A. Bailin and A. Grafstein. 2001. The linguistic as-
sumptions underlying readability formulae: a cri-
tique. Language and Communication, 21:285?301.
S. Bangalore and O. Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In
COLING, pages 42?48.
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In INLG?00:
Proceedings of the first international conference on
Natural language generation, pages 1?8.
M. Banko, V. Mittal, and M. Witbrock. 2000. Head-
line generation based on statistical translation. In
Proceedings of the 38th Annual Meeting of the As-
sociation for Co mputational Linguistics.
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
R. Barzilay and K. McKeown. 2005. Sentence fusion
for multidocument news summarization. Computa-
tional Linguistics, 31(3).
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In NAACL-2000.
J. Clarke and M. Lapata. 2006. Models for sen-
tence compression: A comparison across domains,
training requirements and evaluation measures. In
ACL:COLING?06, pages 377?384.
M. Collins and T. Koo. 2005. Discriminative rerank-
ing for natural language parsing. Comput. Linguist.,
31(1):25?70.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty.
In Proceedings of HLT/NAACL?04.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001.
A machine learning approach to the automatic eval-
uation of machine translation. In Proceedings of
39th Annual Meeting of the Association for Compu-
tational Linguistics, pages 148?155.
H. Daume? III and D. Marcu. 2004. Generic sentence
fusion is an ill-defined summarization task. In Pro-
ceedings of the Text Summarization Branches Out
Workshop at ACL.
146
M. Galley and K. McKeown. 2007. Lexicalized
markov grammars for sentence compression. In
Proceedings of Human Language Technologies: The
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT).
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594.
H. Jing. 2000. Sentence simplification in automatic
text summarization. In Proceedings of the 6th Ap-
plied NLP Conference, ANLP?2000.
N. Karamanis, M. Poesio, C. Mellish, and J. Oberlan-
der. (to appear). Evaluating centering for infor-
mation ordering using corpora. Computational Lin-
guistics.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1).
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of ACL?03.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL?06.
A. Mutton, M. Dras, S. Wan, and R. Dale. 2007. Gleu:
Automatic evaluation of sentence-level fluency. In
ACL?07, pages 344?351.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
186?195.
S. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and sta-
tistical language models. In Proceedings of ACL?05,
pages 523?530.
A. Siddharthan. 2003. Syntactic simplification and
Text Cohesion. Ph.D. thesis, University of Cam-
bridge, UK.
R. Soricut and D. Marcu. 2007. Abstractive head-
line generation using widl-expressions. Inf. Process.
Manage., 43(6):1536?1548.
J. Turner and E. Charniak. 2005. Supervised and un-
supervised learning for sentence compression. In
ACL?05.
S. Wan, R. Dale, and M. Dras. 2005. Searching
for grammaticality: Propagating dependencies in the
viterbi algorithm. In Proceedings of the Tenth Eu-
ropean Workshop on Natural Language Generation
(ENLG-05).
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2007.
Multi-candidate reduction: Sentence compression as
a tool for document summarization tasks. Inf. Pro-
cess. Manage., 43(6):1549?1570.
S. Zwarts and M. Dras. 2008. Choosing the right
translation: A syntactically informed classification
approach. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 1153?1160.
147
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 541?548,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Performance Confidence Estimation for Automatic Summarization
Annie Louis
University of Pennsylvania
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
We address the task of automatically pre-
dicting if summarization system perfor-
mance will be good or bad based on fea-
tures derived directly from either single- or
multi-document inputs. Our labelled cor-
pus for the task is composed of data from
large scale evaluations completed over the
span of several years. The variation of data
between years allows for a comprehensive
analysis of the robustness of features, but
poses a challenge for building a combined
corpus which can be used for training and
testing. Still, we find that the problem can
be mitigated by appropriately normalizing
for differences within each year. We ex-
amine different formulations of the classi-
fication task which considerably influence
performance. The best results are 84%
prediction accuracy for single- and 74%
for multi-document summarization.
1 Introduction
The input to a summarization system significantly
affects the quality of the summary that can be pro-
duced for it, by either a person or an automatic
method. Some inputs are difficult and summaries
produced by any approach will tend to be poor,
while other inputs are easy and systems will ex-
hibit good performance. User satisfaction with the
summaries can be improved, for example by auto-
matically flagging summaries for which a system
expects to perform poorly. In such cases the user
can ignore the summary and avoid the frustration
of reading poor quality text.
(Brandow et al, 1995) describes an intelligent
summarizer system that could identify documents
which would be difficult to summarize based on
structural properties. Documents containing ques-
tion/answer sessions, speeches, tables and embed-
ded lists were identified based on patterns and
these features were used to determine whether an
acceptable summary can be produced. If not, the
inputs were flagged as unsuitable for automatic
summarization. In our work, we provide deeper
insight into how other characteristics of the text
itself and properties of document clusters can be
used to identify difficult inputs.
The task of predicting the confidence in system
performance for a given input is in fact relevant not
only for summarization, but in general for all ap-
plications aimed at facilitating information access.
In question answering for example, a system may
be configured not to answer questions for which
the confidence of producing a correct answer is
low, and in this way increase the overall accuracy
of the system whenever it does produce an answer
(Brill et al, 2002; Dredze and Czuba, 2007).
Similarly in machine translation, some sen-
tences might contain difficult to translate phrases,
that is, portions of the input are likely to lead
to garbled output if automatic translation is at-
tempted. Automatically identifying such phrases
has the potential of improving MT as shown by
an oracle study (Mohit and Hwa, 2007). More re-
cent work (Birch et al, 2008) has shown that prop-
erties of reordering, source and target language
complexity and relatedness can be used to pre-
dict translation quality. In information retrieval,
the problem of predicting system performance has
generated considerable interest and has led to no-
tably good results (Cronen-Townsend et al, 2002;
Yom-Tov et al, 2005; Carmel et al, 2006).
541
2 Task definition
In summarization, researchers have recognized
that some inputs might be more successfully han-
dled by a particular subsystem (McKeown et al,
2001), but little work has been done to qualify the
general characteristics of inputs that lead to subop-
timal performance of systems. Only recently the
issue has drawn attention: (Nenkova and Louis,
2008) present an initial analysis of the factors that
influence system performance in content selection.
This study was based on results from the Doc-
ument Understanding Conference (DUC) evalua-
tions (Over et al, 2007) of multi-document sum-
marization of news. They showed that input, sys-
tem identity and length of the target summary were
all significant factors affecting summary quality.
Longer summaries were consistently better than
shorter ones for the same input, so improvements
can be easy in applications where varying target
size is possible. Indeed, varying summary size is
desirable in many situations (Kaisser et al, 2008).
The most predictive factor of summary quality
was input identity, prompting a closer investiga-
tion of input properties that are indicative of dete-
rioration in performance. For example, summaries
of articles describing different opinions about an
issue or of articles describing multiple distinct
events of the same type were of overall poor qual-
ity, while summaries of more focused inputs, deal-
ing with descriptions of a single event, subject or
person (biographical), were on average better.
A number of features were defined, capturing
aspects of how focused on a single topic a given
input is. Analysis of the predictive power of the
features was done using only one year of DUC
evaluations. Data from later evaluations was used
to train and test a logistic regression classifier for
prediction of expected system performance. The
task could be performed with accuracy of 61.45%,
significantly above chance levels.
The results also indicated that special care needs
to be taken when pooling data from different eval-
uations into a single dataset. Feature selection per-
formed on data from one year was not useful for
prediction on data from other years, and actually
led to worse performance than using all features.
Moreover, directly indicating which evaluation the
data came from was the most predictive feature
when testing on data from more than one year.
In the work described here, we show how the
approach for predicting performance confidence
can be improved considerably by paying special
attention to the way data from different years is
combined, as well as by adopting alternative task
formulations (pairwise comparisons of inputs in-
stead of binary class prediction), and utilizing
more representative examples for good and bad
performance. We also extend the analysis to sin-
gle document summarization, for which predict-
ing system performance turns out to be much more
accurate than for multi-document summarization.
We address three key questions.
What features are predictive of performance on
a given input? In Section 4, we discuss four
classes of features capturing properties of the in-
put, related to input size, information-theoretic
properties of the distribution of words in the input,
presence of descriptive (topic) words and similar-
ity between the documents in multi-document in-
puts. Rather than using a single year of evaluations
for the analysis, we report correlation with ex-
pected system performance for all years and tasks,
showing that in fact the power of these features
varies considerably across years (Section 5).
How to combine data from different years? The
available data spans several years of summariza-
tion evaluations. Between years, systems change,
as well as number of systems and average input
difficulty. All of these changes impact system per-
formance and make data from different years dif-
ficult to analyze when taken together. Still, one
would want to combine all of the available eval-
uations in order to have more data for developing
machine learning models. In Section 6 we demon-
strate that this indeed can be achieved, by normal-
izing within each year by the highest observed per-
formance and only then combining the data.
How to define input difficulty? There are several
possible definitions of ?input difficulty? or ?good
performance?. All the data can be split in two
binary classes of ?good? and ?bad? performance
respectively, or only representative examples in
which there is a clear difference in performance
can be used. In Section 7 we show that these alter-
natives can dramatically influence prediction ac-
curacy: using representative examples improves
accuracy by more than 10%. Formulating the task
as ranking of two inputs, predicting which one is
more difficult, also turns out to be helpful, offering
more data even within the same year of evaluation.
542
3 Data
We use the data from single- and multi-document
evaluations performed as part of the Document
Understanding Conferences (Over et al, 2007)
from 2001 to 2004.1 Generic multi-document
summarization was evaluated in all of these years,
single document summaries were evaluated only
in 2001 and 2002. We use the 100-word sum-
maries from both tasks.
In the years 2002-2004, systems were eval-
uated respectively on 59, 37 and 100 (50
for generic summarization and 50 biographical)
multi-document inputs. There were 149 inputs for
single document summarization in 2001 and 283
inputs in 2002. Combining the datasets from the
different years yields a collection of 432 observa-
tions for single-document summarization, and 196
for multi-document summarization.
Input difficulty, or equivalently expected con-
fidence of system performance, was defined em-
pirically, based on actual content selection evalua-
tions of system summaries. More specifically, ex-
pected performance for each input was defined as
the average coverage score of all participating sys-
tems evaluated on that input. In this way, the per-
formance confidence is not specific to any given
system, but instead reflects what can be expected
from automatic summarizers in general.
The coverage score was manually computed by
NIST evaluators. It measures content selection by
estimating overlap between a human model and a
system summary. The scale for the coverage score
was different in 2001 compared to other years: 0
to 4 scale, switching to a 0 to 1 scale later.
4 Features
For our experiments we use the features proposed,
motivated and described in detail by (Nenkova and
Louis, 2008). Four broad classes of easily com-
putable features were used to capture aspects of
the input predictive of system performance.
Input size-related Number of sentences in the
input, number of tokens, vocabulary size, percent-
age of words used only once, type-token ratio.
Information-theoretic measures Entropy of
the input word distribution and KL divergence be-
tween the input and a large document collection.
1Evaluations from later years did not include generic sum-
marization, but introduced new tasks such as topic-focused
and update summarization.
Log-likelihood ratio for words in the input
Number of topic signature words (Lin and Hovy,
2000; Conroy et al, 2006) and percentage of sig-
nature words in the vocabulary.
Document similarity in the input set These
features apply to multi-document summarization
only. Pairwise similarity of documents within an
input were computed using tf.idf weighted vector
representations of the documents, either using all
words or using only topic signature words. In both
settings, minimum, maximum and average cosine
similarity was computed, resulting in six similar-
ity features.
Multi-document summaries from DUC 2001
were used for feature selection. The 29 sets for
that year were divided according to the average
coverage score of the evaluated systems. Sets with
coverage below the average were deemed to be the
ones that will elicit poor performance and the rest
were considered examples of sets for which sys-
tems perform well. T-tests were used to select fea-
tures that were significantly different between the
two classes. Six features were selected: vocabu-
lary size, entropy, KL divergence, percentage of
topic signatures in the vocabulary, and average co-
sine and topic signature similarity.
5 Correlations with performance
The Pearson correlations between features of the
input and average system performance for each
year is shown in Tables 1 and 2 for multi- and
single-document summarization respectively. The
last two columns show correlations for the com-
bined data from different evaluation years. For
the last column in both tables, the scores in each
year were first normalized by the highest score that
year. Features that were significantly correlated
with expected performance at confidence level of
0.95 are marked with (*). Overall, better perfor-
mance is associated with smaller inputs, lower en-
tropy, higher KL divergence and more signature
terms, as well as with higher document similarity
for multi-document summarization.
Several important observations can be made
from the correlation numbers in the two tables.
Cross-year variation There is a large variation in
the strength of correlation between performance
and various features. For example, KL diver-
gence is significantly correlated with performance
for most years, with correlation of 0.4618 for the
generic summaries in 2004, but the correlation was
543
features 2001 2002 2003 2004G 2004B All(UN) All(N)
tokens -0.2813 -0.2235 -0.3834* -0.4286* -0.1596 -0.2415* -0.2610*
sentences -0.2511 -0.1906 -0.3474* -0.4197* -0.1489 -0.2311* -0.2753*
vocabulary -0.3611* -0.3026* -0.3257* -0.4286* -0.2239 -0.2568* -0.3171*
per-once -0.0026 -0.0375 0.1925 0.2687 0.2081 0.2175* 0.1813*
type/token -0.0276 -0.0160 0.1324 0.0389 -0.1537 -0.0327 -0.0993
entropy -0.4256* -0.2936* -0.1865 -0.3776* -0.1954 -0.2283* -0.2761*
KL divergence 0.3663* 0.1809 0.3220* 0.4618* 0.2359 0.2296* 0.2879*
avg cosine 0.2244 0.2351 0.1409 0.1635 0.2602 0.1894* 0.2483*
min cosine 0.0308 0.2085 -0.5330* -0.1766 0.1839 -0.0337 -0.0494
max cosine 0.1337 0.0305 0.2499 0.1044 -0.0882 0.0918 0.1982*
num sign -0.1880 -0.0773 -0.1799 -0.0149 0.1412 -0.0248 0.0084
% sign. terms 0.3277 0.1645 0.1429 0.3174* 0.3071* 0.1952* 0.2609*
avg topic 0.2860 0.3678* 0.0826 0.0321 0.1215 0.1745* 0.2021*
min topic 0.0414 0.0673 -0.0167 -0.0025 -0.0405 -0.0177 -0.0469
max topic 0.2416 0.0489 0.1815 0.0134 0.0965 0.1252 0.2082*
Table 1: Correlations between input features and average system performance for multi-document inputs
of DUC 2001-2003, 2004G (generic task), 2004B (biographical task), All data (2002-2004) - UNnor-
malized and Normalized coverage scores. P-values smaller than 0.05 are marked by *.
not significant (0.1809) for 2002 data. Similarly,
the average similarity of topic signature vectors is
significant in 2002, but has correlations close to
zero in the following two years. This shows that
no feature exhibits robust predictive power, espe-
cially when there are relatively few datapoints. In
light of this finding, developing additional features
and combining data to obtain a larger collection of
samples are important for future progress.
Normalization Because of the variation from year
to year, normalizing performance scores is benefi-
cial and leads to higher correlation for almost all
features. On average, correlations increase by 0.05
for all features. Two of the features, maximum co-
sine similarity and max topic word similarity, be-
come significant only in the normalized data. As
we will see in the next section, prediction accu-
racy is also considerably improved when scores
are normalized before pooling the data from dif-
ferent years together.
Single- vs. multi-document task The correla-
tions between performance and input features are
higher in single-document summarization than in
multi-document. For example, in the normalized
data KL divergence has correlation of 0.28 for
multi-document summarization but 0.40 for sin-
gle document. The number of signature terms
is highly correlated with performance in single-
document summarization (-0.25) but there is prac-
tically no correlation for multi-document sum-
maries. Consequently, we can expect that the
performance prediction will be more accurate for
single-document summarization.
features 2001 2002 All(N)
tokens -0.3784* -0.2434* -0.3819*
sentences -0.3999* -0.2262* -0.3705*
vocabulary -0.4410* -0.2706* -0.4196*
per-once -0.0718 0.0087 0.0496
type/token 0.1006 0.0952 0.1785
entropy -0.5326* -0.2329* -0.3789*
KL divergence 0.5332* 0.2676* 0.4035*
num sign -0.2212* -0.1127 -0.2519*
% sign 0.3278* 0.1573* 0.2042*
Table 2: Correlations between input features and
average system performance for single doc. inputs
of DUC?01, ?02, All (?01+?02) N-normalized. P-
values smaller than 0.05 are marked by *.
6 Classification experiments
In this section we explore how the alternative task
formulations influence success of predicting sys-
tem performance. Obviously, the two classes of
interest for the prediction will be ?good perfor-
mance? and ?poor performance?. But separat-
ing the real valued coverage scores for inputs into
these two classes can be done in different ways.
All the data can be used and the definition of
?good? or ?bad? can be determined in relation to
the average performance on all inputs. Or only the
best and worst sets can be used as representative
examples. We explore the consequences of adopt-
ing either of these options.
For the first set of experiments, we divide all
inputs based on the mean value of the average sys-
tem scores as in (Nenkova and Louis, 2008). All
multi-document results reported in this paper are
based on the use of the six significant features dis-
cussed in Section 4. DUC 2002, 2003 and 2004
data was used for 10-fold cross validation. We ex-
544
perimented with three classifiers available in R?
logistic regression (LogR), decision tree (DTree)
and support vector machines (SVM). SVM and
decision tree classifiers are libraries under CRAN
packages e1071 and rpart.2 Since our develop-
ment set was very small (only 29 inputs), we did
not perform any parameter tuning.
There is nearly equal number of inputs on either
side of the average system performance and the
random baseline performance in this case would
give 50% accuracy.
6.1 Multi-document task
The classification accuracy for the multi-
document inputs is reported in Table 3. The
partitioning into classes was done based on
the average performance (87 easy sets and 109
difficult sets).
As expected, normalization considerably im-
proves results. The absolute largest improvement
of 10% is for the logistic regression classifier. For
this classifier, prediction accuracy for the non-
normalized data is 54% while for the normalized
data, it is 64%. Logistic regression gives the best
overall classification accuracy on the normalized
data compared to SVM classifier that does best on
the unnormalized data (56% accuracy). Normal-
ization also improves precision and recall for the
SVM and logistic regression classifiers.
The differences in accuracies obtained by the
classifiers is also noticable and we discuss these
further in Section 7.
6.2 Single document task
We now turn to the task of predicting summa-
rization performance for single document inputs.
As we saw in section 5, the features are stronger
predictors for summarization performance in the
single-document task. In addition, there is more
data from evaluations of single document summa-
rizers. Stronger features and more training data
can both help achieve higher prediction accura-
cies. In this section, we separate out the two fac-
tors and demonstrate that indeed the features are
much more predictive for single document sum-
marization than for multidocument.
In order to understand the effect of having more
training data, we did not divide the single doc-
ument inputs into a separate development set to
use for feature selection. Instead, all the features
2http://cran.r-project.org/web/packages/
classifier accuracy P R F
DTree 66.744 66.846 67.382 67.113
LogR 67.907 67.089 69.806 68.421
SVM 69.069 66.277 80.317 72.625
Table 4: Single document input classification Pre-
cision (P), Recall (R),and F score (F) for difficult
inputs on DUC?01 and ?02 (total 432 examples)
divided into 2 classes based on the average cover-
age score (217 difficult and 215 easy inputs).
discussed in Section 4 except the six cosine and
topic signature similarity measures are used. The
coverage score ranges in DUC 2001 and 2002 are
different. They are normalized by the maximum
score within the year, then combined and parti-
tioned in two classes with respect to the average
coverage score. In this way, the 432 observations
are split into almost equal halves, 215 good perfor-
mance examples and 217 bad performance. Table
4 shows the accuracy, precision and recall of the
classifiers on single-document inputs.
From the results in Table 4 it is evident that
all three classifiers achieve accuracies higher than
those for multi-document summarization. The im-
provement is largest for decision tree classifica-
tion, nearly 15%. The SVM classifier has the high-
est accuracy for single document summarization
inputs, (69%), which is 7% absolute improvement
over the performance of the SVM classifier for
the multi-document task. The smallest improve-
ment of 4% is for the logistic regression classi-
fier which is the one with highest accuracy for the
multi-document task
Improved accuracy could be attributed to the
fact that almost double the amount of data is avail-
able for the single-document summarization ex-
periments. To test if this was the main reason for
improvement, we repeated the single-document
experiments using a random sample of 196 inputs,
the same amount of data as for the multi-document
case. Even with reduced data, single-document
inputs are more easily classifiable as difficult or
easy compared to multi-document, as shown in Ta-
bles 3 and 5. The SVM classifier is still the best
for single-document summarization and its accu-
racy is the same with reduced data as with all
data. With less data, the performance of the lo-
gistic regression and decision tree classifiers de-
grades more and is closer to the numbers for multi-
document inputs.
545
Classifier N/UN Acc Pdiff Rdiff Peasy Reasy Fdiff Feasy
DTree UN 51.579 56.580 56.999 46.790 45.591 55.383 44.199N 52.105 56.474 57.786 46.909 45.440 55.709 44.298
LogR UN 54.211 56.877 71.273 50.135 34.074 62.145 39.159N 63.684 63.974 79.536 63.714 45.980 69.815 51.652
SVM UN 55.789 57.416 73.943 50.206 32.753 63.784 38.407N 62.632 61.905 81.714 61.286 38.829 69.873 47.063
Table 3: Multi-document input classification results on UNnormalized and Normalized data from DUC
2002 to 2004. Both Normalized and UNormalized data contain 109 difficult and 87 easy inputs. Since
the split is not balanced, the accuracy of classification as well as the Precision (P), Recall (R) and F score
(F) are reported for both classes of easy and diff(icult) inputs.
classifier accuracy P R F
DTree 53.684 54.613 53.662 51.661
LogR 61.579 63.335 60.400 60.155
SVM 69.474 66.339 85.835 73.551
Table 5: Single-document-input classification Pre-
cision (P), Recall (R), and F score (F) for difficult
inputs on a random sample of 196 observations (99
difficult/97 easy) from DUC?01 and ?02.
7 Learning with representative examples
In the experiments in the previous section, we used
the average coverage score to split inputs into two
classes of expected performance. Poor perfor-
mance was assigned to the inputs for which the
average system coverage score was lower than the
average for all inputs. Good performance was as-
signed to those with higher than average cover-
age score. The best results for this formulation
of the prediction task is 64% accuracy for multi-
document classification (logistic regression classi-
fier; 196 datapoints) and 69% for single-document
(SVM classifier; 432 and 196 datapoints).
However, inputs with coverage scores close to
the average may not be representative of either
class. Moreover, inputs for which performance
was very similar would end up in different classes.
We can refine the dataset by using only those ob-
servations that are highly representative of the cat-
egory they belong to, removing inputs for which
system performance was close to the average. It
is desirable to be able to classify mediocre inputs
as a separate category. Further studies are neces-
sary to come up with better categorization of in-
puts rather than two strict classes of difficult and
easy. For now, we examine the strength of our fea-
tures in distinguishing the extreme types by train-
ing and testing only on inputs that are representa-
tive of these classes.
We test this hypothesis by starting with 196
multi-document inputs and performing the 10-fold
cross validation using only 80%, 60% and 50%
of the data, incrementally throwing away obser-
vations around the mean. For example, the 80%
model was learnt on 156 observations, taking the
extreme 78 observations on each side into the dif-
ficult and easy categories. For the single document
case, we performed the same tests starting with
a random sample of 196 observations as 100%
data.3 All classifiers were trained and tested on
the same division of folds during cross validation
and compared using a paired t-test to determine
the significance of differences if any. Results are
shown in Table 6. In parentheses after the accu-
racy of a given classifier, we indicate the classifiers
that are significantly better than it.
Classifiers trained and tested using only repre-
sentative examples perform more reliably. The
SVM classifier is the best one for the single-
document setting and in most cases significantly
outperforms logistic regression and decision tree
classifiers on accuracy and recall. In the multi-
document setting, SVM provides better overall re-
call than logistic regression. However, with re-
spect to accuracy, SVM and logistic regression
classifiers are indistinguishable. The decision tree
classifier performs worse.
For multi-document classification, the F score
drops initially when data is reduced to only 80%.
But when using only half of the data, accuracy
of prediction reaches 74%, amounting to 10% ab-
solute improvement compared to the scenario in
which all available data is used. In the single-
document case, accuracy for the SVM classifier
increases consistently, reaching accuracy of 84%.
8 Pairwise ranking approach
The task we addressed in previous sections was to
classify inputs into ones for which we expect good
3We use the same amount of data as is available for multi-
document so that the results can be directly comparable.
546
Single document classification Multi-document classification
Data CL Acc P R F Acc P R F
100%
DTree 53.684 (S) 54.613 53.662 (S) 51.661 52.105 (S,L) 56.474 57.786 (S,L) 55.709
LogR 61.579 (S) 63.335 60.400 (S) 60.155 63.684 63.974 79.536 69.815
SVM 69.474 66.339 85.835 73.551 62.632 61.905 81.714 69.873
80%
DTree 62.000 (S) 62.917 (S) 67.089 (S) 62.969 53.333 57.517 55.004 (S) 51.817
LogR 68.000 68.829 69.324 (S) 67.686 58.667 60.401 59.298 (S) 57.988
SVM 71.333 70.009 86.551 75.577 62.000 61.492 71.075 63.905
60%
DTree 68.182 (S) 72.750 60.607 (S) 64.025 57.273 (S) 63.000 58.262 (S) 54.882
LogR 70.909 73.381 69.250 69.861 67.273 68.357 70.167 65.973
SVM 76.364 73.365 82.857 76.959 66.364 68.619 75.738 67.726
50%
DTree 70.000 (S) 69.238 67.905 (S) 66.299 65.000 60.381 (L) 70.809 64.479
LogR 76.000 (S) 76.083 72.500 (S) 72.919 74.000 72.905 70.381 (S) 70.965
SVM 84.000 83.476 89.000 84.379 72.000 67.667 79.143 71.963
Table 6: Performance of multiple classifiers on extreme observations from single and multi-document
data (100% data = 196 data points in both cases divided into 2 classes on the basis of average coverge
score). Reported precision (P), recall (R) and F score (F) are for difficult inputs. Experiments on ex-
tremes use equal number of examples from each class - baseline performance is 50%. Systems whose
performance is significantly better than the specified numbers are shown in brackets (S-SVM, D-Decision
Tree, L-Logistic Regression).
performance and ones for which poor system per-
formance is expected. In this section, we evaluate
a different approach to input difficulty classifica-
tion. Given a pair of inputs, can we identify the
one on which systems will perform better? This
ranking task is easier than requiring a strict deci-
sion on whether performance will be good or not.
Ranking approaches are widely used in text
planning and sentence ordering (Walker et al,
2001; Karamanis, 2003) to select the text with best
structure among a set of possible candidates. Un-
der the summarization framework, (Barzilay and
Lapata, 2008) ranked different summaries for the
same input according to their coherence. Simi-
larly, ranking alternative document clusters on the
same topic to choose the best input will prove an
added advantage to summarizer systems. When
summarization is used as part of an information
access interface, the clustering of related docu-
ments that form the input to a system is done
automatically. Currently, the clustering of docu-
ments is completely independent of the need for
subsequent summarization of the resulting clus-
ters. Techniques for predicting summarizer per-
formance can be used to inform clustering so that
the clusters most suitable for summarization can
be chosen. Also, when sample inputs for which
summaries were deemed to be good are available,
these can be used as a standard with which new
inputs can be compared.
For the pairwise comparison task, the features
are the difference in feature values between the
two inputs A and B that form a pair. The dif-
ference in average system scores of inputs A and
B in the pair is used to determine the input for
which performance was better. Every pair could
give two training examples, one positive and one
negative depending on the direction in which the
differences are computed. We choose one exam-
ple from every pair, maintaining an equal number
of positive and negative instances.
The idea of using representative examples can
be applied for the pairwise formulation of the task
as well?the larger the difference in system perfor-
mance is, the better example the pair represents.
Very small score differences are not as indicative
of performance on one input being better than the
other. Hence the experiments were duplicated on
80%, 60% and 40% of the data where the retained
examples were the ones with biggest difference
between the system performance on the two sets
(as indicated by the average coverage score). The
range of score differences in each year are indi-
cated in the Table 7.
All scores are normalized by the maximum
score within the year. Therefore the smallest and
largest possible differences are 0 and 1 respec-
tively. The entries corresponding to the years
2002, 2003 and 2004 show the SVM classification
results when inputs were paired only with those
within the same year. Next inputs of all years were
paired with no restrictions. We report the classifi-
cation accuracies on a random sample of these ex-
amples equal in size to the number of datapoints
in the 2004 examples.
Using only representative examples leads to
547
Amt Data Min score diff Points Acc.
All
2002 0.00028 1710 65.79
2003 0.00037 666 73.94
2004 0.00023 4948 70.71
2002-2004 0.00005 4948 68.85
80%
2002 0.05037 1368 68.39
2003 0.08771 532 78.87
2004 0.05226 3958 73.36
2002-2004 0.02376 3958 70.68
60%
2002 0.10518 1026 73.04
2003 0.17431 400 82.50
2004 0.11244 2968 77.41
2002-2004 0.04844 2968 71.39
40%
2002 0.16662 684 76.03
2003 0.27083 266 87.31
2004 0.18258 1980 79.34
2002-2004 0.07489 1980 74.95
Maximum score difference 2002 (0.8768), 2003 (0.8969),
2004 (0.8482), 2002-2004 (0.8768)
Table 7: Accuracy of SVM classification of mul-
tidocument input pairs. When inputs are paired
irrespective of year (2002-2004), datapoints equal
in number to that in 2004 were chosen at random.
consistently better results than using all the data.
The best classification accuracy is 76%, 87% and
79% for comparisons within the same year and
74% for comparisons across years. It is important
to observe that when inputs are compared with-
out any regard to the year, the classifier perfor-
mance is worse than when both inputs in the pair
are taken from the same evaluation year, present-
ing additional evidence of the cross-year variation
discussed in Section 5. A possible explanation
is that system improvements in later years might
cause better scores to be obtained on inputs which
were difficult previously.
9 Conclusions
We presented a study of predicting expected sum-
marization performance on a given input. We
demonstrated that prediction of summarization
system performance can be done with high ac-
curacy. Normalization and use of representative
examples of difficult and easy inputs both prove
beneficial for the task. We also find that per-
formance predictions for single-document sum-
marization can be done more accurately than for
multi-document summarization. The best classi-
fier for single-document classification are SVMs,
and the best for multi-document?logistic regres-
sion and SVM. We also record good prediction
performance on pairwise comparisons which can
prove useful in a variety of situations.
References
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. CL, 34(1):1?34.
A. Birch, M. Osborne, and P. Koehn. 2008. Predicting
success in machine translation. In Proceedings of
EMNLP, pages 745?754.
R. Brandow, K. Mitze, and L. F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Inf. Process. Manage., 31(5):675?685.
E. Brill, S. Dumais, and M. Banko. 2002. An analysis
of the askmsr question-answering system. In Pro-
ceedings of EMNLP.
D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.
2006. What makes a query difficult? In Proceed-
ings of SIGIR, pages 390?397.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
ACL.
S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2002.
Predicting query performance. In Proceedings of SI-
GIR, pages 299?306.
M. Dredze and K. Czuba. 2007. Learning to admit
you?re wrong: Statistical tools for evaluating web
qa. In NIPS Workshop on Machine Learning for Web
Search.
M. Kaisser, M. A. Hearst, and J. B. Lowe. 2008. Im-
proving search results quality by customizing sum-
mary lengths. In Proceedings of ACL: HLT, pages
701?709.
N. Karamanis. 2003. Entity Coherence for Descriptive
Text Structuring. Ph.D. thesis, University of Edin-
burgh.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING, pages 495?501.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivas-
siloglou, B. Schiffman, and S. Teufel. 2001.
Columbia multi-document summarization: Ap-
proach and evaluation. In Proceedings of DUC.
B. Mohit and R. Hwa. 2007. Localization of difficult-
to-translate phrases. In Proceedings of ACL Work-
shop on Statistical Machine Translations.
A. Nenkova and A. Louis. 2008. Can you summa-
rize this? identifying correlates of input difficulty
for multi-document summarization. In Proceedings
of ACL: HLT, pages 825?833.
P. Over, H. Dang, and D. Harman. 2007. Duc in con-
text. Inf. Process. Manage., 43(6):1506?1520.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proceedings of
NAACL, pages 1?8.
E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.
2005. Learning to estimate query difficulty: includ-
ing applications to missing content detection and
distributed information retrieval. In Proceedings of
SIGIR, pages 512?519.
548
Entity-driven Rewrite for Multi-document Summarization
Ani Nenkova
University of Pennsylvania
Department of Computer and Information Science
nenkova@seas.upenn.edu
Abstract
In this paper we explore the benefits from
and shortcomings of entity-driven noun
phrase rewriting for multi-document sum-
marization of news. The approach leads to
20% to 50% different content in the sum-
mary in comparison to an extractive sum-
mary produced using the same underlying
approach, showing the promise the tech-
nique has to offer. In addition, summaries
produced using entity-driven rewrite have
higher linguistic quality than a comparison
non-extractive system. Some improvement
is also seen in content selection over extrac-
tive summarization as measured by pyramid
method evaluation.
1 Introduction
Two of the key components of effective summariza-
tions are the ability to identify important points in
the text and to adequately reword the original text
in order to convey these points. Automatic text
summarization approaches have offered reasonably
well-performing approximations for identifiying im-
portant sentences (Lin and Hovy, 2002; Schiffman et
al., 2002; Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Daume? III and Marcu, 2006) but, not sur-
prisingly, text (re)generation has been a major chal-
lange despite some work on sub-sentential modifica-
tion (Jing and McKeown, 2000; Knight and Marcu,
2000; Barzilay and McKeown, 2005). An addi-
tional drawback of extractive approaches is that es-
timates for the importance of larger text units such
as sentences depend on the length of the sentence
(Nenkova et al, 2006).
Sentence simplification or compaction algorithms
are driven mainly by grammaticality considerations.
Whether approaches for estimating importance can
be applied to units smaller than sentences and used
in text rewrite in the summary production is a ques-
tion that remains unanswered. The option to operate
on smaller units, which can be mixed and matched
from the input to give novel combinations in the
summary, offers several possible advantages.
Improve content Sometimes sentences in the in-
put can contain both information that is very appro-
priate to include in a summary and information that
should not appear in a summary. Being able to re-
move unnecessary parts can free up space for better
content. Similarly, a sentence might be good over-
all, but could be further improved if more details
about an entity or event are added in. Overall, a sum-
marizer capable of operating on subsentential units
would in principle be better at content selection.
Improve readability Linguistic quality evalua-
tion of automatic summaries in the Document Un-
derstanding Conference reveals that summarizers
perform rather poorly on several readability aspects,
including referential clarity. The gap between hu-
man and automatic performance is much larger for
linguistic quality aspects than for content selection.
In more than half of the automatic summaries there
were entities for which it was not clear what/who
they were and how they were related to the story.
The ability to add in descriptions for entities in the
summaries could improve the referential clarity of
summaries and can be achieved through text rewrite
118
of subsentential units.
IP issues Another very practical reason to be in-
terested in altering the original wording of sentences
in summaries in a news browsing system involves in-
tellectual property issues. Newspapers are not will-
ing to allow verbatim usage of long passages of
their articles on commercial websites. Being able to
change the original wording can thus allow compa-
nies to include longer than one sentence summaries,
which would increase user satisfaction (McKeown
et al, 2005).
These considerations serve as direct motivation
for exploring how a simple but effective summarizer
framework can accommodate noun phrase rewrite in
multi-document summarization of news. The idea
is for each sentence in a summary to automatically
examine the noun phrases in it and decide if a dif-
ferent noun phrase is more informative and should
be included in the sentence in place of the original.
Consider the following example:
Sentence 1 The arrest caused an international con-
troversy.
Sentence 2 The arrest in London of former Chilean
dictator Augusto Pinochet caused an interna-
tional controversy.
Now, consider the situation where we need to ex-
press in a summary that the arrest was controversial
and this is the first sentence in the summary, and sen-
tence 1 is available in the input (?The arrest caused
an international controversy?), as well as an unre-
lated sentence such as ?The arrest in London of for-
mer Chilean dictator Augusto Pinochet was widely
discussed in the British press?. NP rewrite can allow
us to form the rewritten sentence 2, which would be
a much more informative first sentence for the sum-
mary: ?The arrest in London of former Chilean dic-
tator Augusto Pinochet caused an international con-
troversy?. Similarly, if sentence 2 is available in
the input and it is selected in the summary after a
sentence that expresses the fact that the arrest took
place, it will be more appropriate to rewrite sentence
2 into sentence 1 for inclusion in the summary.
This example shows the potential power of noun
phrase rewrite. It also suggests that context will play
a role in the rewrite process, since different noun
phrase realizations will be most appropriate depend-
ing on what has been said in the summary up to the
point at which rewrite takes place.
2 NP-rewrite enhanced frequency
summarizer
Frequency and frequency-related measures of im-
portance have been traditionally used in text sum-
marization as indicators of importance (Luhn, 1958;
Lin and Hovy, 2000; Conroy et al, 2006). No-
tably, a greedy frequency-driven approach leads to
very good results in content selection (Nenkova et
al., 2006). In this approach sentence importance is
measured as a function of the frequency in the in-
put of the content words in that sentence. The most
important sentence is selected, the weight of words
in it are adjusted, and sentence weights are recom-
puted for the new weights beofre selecting the next
sentence.
This conceptually simple summarization ap-
proach can readily be extended to include NP rewrite
and allow us to examine the effect of rewrite capa-
bilities on overall content selection and readability.
The specific algorithm for frequency-driven summa-
rization and rewrite is as follows:
Step 1 Estimate the importance of each content
word wi based on its frequency in the input ni,
p(wi) = niN .
Step 2 For each sentence Sj in the input, estimate
its importance based on the words in the sen-
tence wi ? Sj : the weight of the sentence is
equal to the average weight of content words
appearing in it.
Weight(Sj) =
?
wi?Sj
p(wi)
|wi?Sj |
Step 3 Select the sentence with the highest weight.
Step 4 For each maximum noun phrase NPk in the
selected sentence
4.1 For each coreferring noun phrase NPi,
such that NPi ? NPk from all
input documents, compute a weight
Weight(NPi) = FRW (wr ? NPi).
4.2 Select the noun phrase with the highest
weight and insert it in the sentence in
119
place of the original NP. In case of ties,
select the shorter noun phrase.
Step 5 For each content word in the rewritten sen-
tence, update its weight by setting it to 0.
Step 6 If the desired summary length has not been
reached, go to step 2.
Step 4 is the NP rewriting step. The function
FRW is the rewrite composition function that as-
signs weights to noun phrases based on the impor-
tance of words that appear in the noun phrase. The
two options that we explore here are FRW ? Avr
and FRW ? Sum; the weight of an NP equals
the average weight or sum of weights of content
words in the NP respectively. The two selections
lead to different behavior in rewrite. FRW ? Avr
will generally prefer the shorter noun phrases, typ-
ically consisting of just the noun phrase head and
it will overall tend to reduce the selected sentence.
FRW ? Sum will behave quite differently: it will
insert relevant information that has not been con-
veyed by the summary so far (add a longer noun
phrase) and will reduce the NP if the words in it
already appear in the summary. This means that
FRW ? Sum will have the behavior close to what
we expect for entity-centric rewrite: inluding more
descriptive information at the first mention of the en-
tity, and using shorter references at subsequent men-
tions.
Maximum noun phrases are the unit on which
NP rewrite operates. They are defined in a depen-
dency parse tree as the subtree that has as a root
a noun such that there is no other noun on the
path between it and the root of the tree. For ex-
ample , there are two maximum NPs, with heads
?police? and ?Augusto Pinochet? in the sentence
?British police arrested former Chilean dictator Au-
gusto Pinochet?. The noun phrase ?former chilean
dictator? is not a maximum NP, since there is a noun
(augusto pinochet) on the path in the dependency
tree between the noun ?dictator? and the root of the
tree. By definition a maximum NP includes all nom-
inal and adjectival premodifiers of the head, as well
as postmodifiers such as prepositional phrases, ap-
positions, and relative clauses. This means that max-
imum NPs can be rather complex, covering a wide
range of production rules in a context-free grammar.
The dependency tree definition of maximum noun
phrase makes it easy to see why these are a good
unit for subsentential rewrite: the subtree that has
the head of the NP as a root contains only modifiers
of the head, and by rewriting the noun phrase, the
amount of information expressed about the head en-
tity can be varied.
In our implementation, a context free grammar
probabilistic parser (Charniak, 2000) was used to
parse the input. The maximum noun phrases were
identified by finding sequences of <np>...</np>
tags in the parse such that the number of opening and
closing tags is equal. Each NP identified by such tag
spans was considered as a candidate for rewrite.
Coreference classes A coreference class CRm is
the class of all maximum noun phrases in the input
that refer to the same entity Em. The general prob-
lem of coreference resolution is hard, and is even
more complicated for the multi-document summa-
rization case, in which cross-document resolution
needs to be performed. Here we make a simplify-
ing assumption, stating that all noun phrases that
have the same noun as a head belong to the same
coreference class. While we expected that this as-
sumption would lead to some wrong decisions, we
also suspected that in most common summarization
scenarios, even if there are more than one entities ex-
pressed with the same noun, only one of them would
be the main focus for the news story and will ap-
pear more often across input sentences. References
to such main entities will be likely to be picked in
a sentence for inclusion in the summary by chance
more often than other competeing entities. We thus
used the head noun equivalance to form the classes.
A post-evaluation inspection of the summaries con-
firmed that our assumption was correct and there
were only a small number of errors in the rewrit-
ten summaries that were due to coreference errors,
which were greatly outnumbered by parsing errors
for example. In a future evaluation, we will evalu-
ate the rewrite module assuming perfect coreference
and parsing, in order to see the impact of the core
NP-rewrite approach itself.
3 NP rewrite evaluation
The NP rewrite summarization algorithm was ap-
plied to the 50 test sets for generic multi-document
120
summarization from the 2004 Document Under-
standing Conference. Two examples of its operation
with FRW ? Avr are shown below.
Original.1 While the British government defended
the arrest, it took no stand on extradition of Pinochet
to Spain.
NP-Rewite.1 While the British government de-
fended the arrest in London of former Chilean dicta-
tor Augusto Pinochet, it took no stand on extradition
of Pinochet to Spain.
Original.2 Duisenberg has said growth in the euro
area countries next year will be about 2.5 percent,
lower than the 3 percent predicted earlier.
NP-Rewrite.2 Wim Duisenberg, the head of the new
European Central Bank, has said growth in the euro
area will be about 2.5 percent, lower than just 1 per-
cent in the euro-zone unemployment predicted ear-
lier.
We can see that in both cases, the NP rewrite
pasted into the sentence important additional infor-
mation. But in the second example we also see an
error that was caused by the simplifying assumption
for the creation of the coreference classes accord-
ing to which the percentage of unemployment and
growth have been put in the same class.
In order to estimate how much the summary is
changed because of the use of the NP rewrite, we
computed the unigram overlap between the original
extractive summary and the NP-rewrite summary.
As expected, FFW ? Sum leads to bigger changes
and on average the rewritten summaries contained
only 54% of the unigrams from the extractive sum-
maries; for FRW ? Avr, there was a smaller change
between the extractive and the rewritten summary,
with 79% of the unigrams being the same between
the two summaries.
3.1 Linguistic quality evaluation
Noun phrase rewrite has the potential to improve
the referential clarity of summaries, by inserting in
the sentences more information about entities when
such is available. It is of interest to see how the
rewrite version of the summarizer would compare
to the extractive version, as well as how its linguis-
tic quality compares to that of other summarizers
that participated in DUC. Four summarizers were
evaluated: peer 117, which was a system that used
generation techniques to produce the summary and
SYSTEM Q
1
Q
2
Q
3
Q
4
Q
5
SUMId 4.06 4.12 3.80 3.80 3.20
SUMAvr 3.40 3.90 3.36 3.52 2.80
SUMSum 2.96 3.34 3.30 3.48 2.80
peer 117 2.06 3.08 2.42 3.12 2.10
Table 1: Linguistic quality evaluation. Peer 117 was
the only non-extractive system entry in DUC 2004;
SUMId is the frequency summarizer with no NP
rewrite; and the two versions of rewrite with sum
and average as combination functions.
was the only real non-extractive summarizer partic-
ipant at DUC 2004 (Vanderwende et al, 2004); the
extractive frequency summarizer, and the two ver-
sions of the rewrite algorithm (Sum and Avr). The
evaluated rewritten summaries had potential errors
coming from different sources, such as coreference
resolution, parsing errors, sentence splitting errors,
as well as errors coming directly from rewrite, in
which an unsuitable NP is chosen to be included in
the summary. Improvements in parsing for exam-
ple could lead to better overall rewrite results, but
we evaluated the output as is, in order to see what
is the performance that can be expected in a realistic
setting for fully automatic rewrite.
The evaluation was done by five native English
speakers, using the five DUC linguistic quality ques-
tions on grammaticality (Q
1
), repetition (Q
2
), refer-
ential clarity (Q
3
), focus (Q
4
) and coherence (Q
5
).
Five evaluators were used so that possible idiosyn-
cratic preference of a single evaluator could be
avoided. Each evaluator evaluated all five sum-
maries for each test set, presented in a random order.
The results are shown in table 3.1. Each summary
was evaluated for each of the properties on a scale
from 1 to 5, with 5 being very good with respect to
the quality and 1, very bad.
Comparing NP rewrite to extraction Here we
would be interested in comparing the extractive fre-
quency summarizer (SUMId), and the two version of
systems that rewrite noun phrases: SUMAvr (which
changes about 20% of the text) and SUMSum (which
changes about 50% of the text). The general trend
that we see for all five dimensions of linguistic qual-
ity is that the more the text is automatically altered,
the worse the linguistic quality of the summary
121
gets. In particular, the grammaticality of the sum-
maries drops significantly for the rewrite systems.
The increase of repetition is also significant between
SUMId and SUMSum. Error analysis showed that
sometimes increased repetition occurred in the pro-
cess of rewrite for the following reason: the context
weight update for words is done only after each noun
phrase in the sentence has been rewritten. Occasion-
ally, this led to a situation in which a noun phrase
was augmented with information that was expressed
later in the original sentence. The referential clar-
ity of rewritten summaries also drops significantly,
which is a rather disappointing result, since one of
the motivations for doing noun phrase rewrite was
the desire to improve referential clarity by adding in-
formation where such is necessary. One of the prob-
lems here is that it is almost impossible for human
evaluators to ignore grammatical errors when judg-
ing referential clarity. Grammatical errors decrease
the overall readability and a summary that is given
a lower grammaticality score tends to also receive
lower referential clarity score. This fact of quality
perception is a real challenge for summarizeration
systems that move towards abstraction and alter the
original wording of sentences since certainly auto-
matic approaches are likely to introduce ingrammat-
icalities.
Comparing SUMSum and peer 117 We now turn
to the comparison of between SUMSum and the gen-
eration based system 117. This system is unique
among the DUC 2004 systems, and the only one
that year that experimented with generation tech-
niques for summarization. System 117 is verb-
driven: it analizes the input in terms of predicate-
argument triples and identifies the most important
triples. These are then verbalized by a generation
system originally developed as a realization compo-
nent in a machine translation engine. As a result,
peer 117 possibly made even more changes to the
original text then the NP-rewrite system. The results
of the comparison are consistent with the observa-
tion that the more changes are made to the original
sentences, the more the readability of summaries de-
creases. SUMSum is significantly better than peer
117 on all five readability aspects, with notable dif-
ference in the grammaticality and referential quality,
for which SUMSum outperforms peer 117 by a full
point. This indicates that NPs are a good candidate
granularity for sentence changes and it can lead to
substantial altering of the text while preserving sig-
nificantly better overall readability.
3.2 Content selection evaluation
We now examine the question of how the content in
the summaries changed due to the NP-rewrite, since
improving content selection was the other motiva-
tion for exploring rewrite. In particular, we are in-
terested in the change in content selection between
SUMSum and SUMId (the extractive version of the
summarizer). We use SUMSum for the compari-
son because it led to bigger changes in the sum-
mary text compared to the purely extractive version.
We used the pyramid evaluation method: four hu-
man summaries for each input were manually ana-
lyzed to identify shared content units. The weight of
each content unit is equal to the number of model
summaries that express it. The pyramid score of
an automatic summary is equal to the weight of the
content units expressed in the summary divided by
the weight of an ideally informative summary of the
same length (the content unit identification is again
done manually by an annotator).
Of the 50 test sets, there were 22 sets in which
the NP-rewritten version had lower pyramid scores
than the extractive version of the summary, 23 sets
in which the rewritten summaries had better scores,
and 5 sets in which the rewritten and extractive sum-
maries had exactly the same scores. So we see that
in half of the cases the NP-rewrite actually improved
the content of the summary. The summarizer version
that uses NP-rewrite has overall better content selec-
tion performance than the purely extractive system.
The original pyramid score increased from 0.4039 to
0.4169 for the version with rewrite. This improve-
ment is not significant, but shows a trend in the ex-
pected direction of improvement.
The lack of significance in the improvement is due
to large variation in performance: when np rewrite
worked as expected, content selection improved.
But on occasions when errors occurred, both read-
ability and content selection were noticeably com-
promised. Here is an example of summaries for
the same input in which the NP-rewritten version
had better content. After each summary, we list the
content units from the pyramid content analysis that
were expressed in the summary. The weight of each
122
content unit is given in brackets before the label of
the unit and content units that differ between the ex-
tractive and rewritten version are displayed in italic.
The rewritten version conveys high weight content
units that do not appear in the extractive version,
with weights 4 (maximum weight here) and 3 re-
spectively.
Extractive summary Italy?s Communist Re-
founding Party rejected Prime Minister Prodi?s pro-
posed 1999 budget. By one vote, Premier Romano
Prodi?s center-left coalition lost a confidence vote
in the Chamber of Deputies Friday, and he went to
the presidential palace to rsign. Three days after the
collapse of Premier Romano Prodi?s center-left gov-
ernment, Italy?s president began calling in political
leaders Monday to try to reach a consensus on a new
government. Prodi has said he would call a confi-
dence vote if he lost the Communists? support.? I
have always acted with coherence,? Prodi said be-
fore a morning meeting with President Oscar Luigi.
(4) Prodi lost a confidence vote
(4) The Refounding Party is Italy?s Communist
Party
(4) The Refounding Party rejected the govern-
ment?s budget
(3) The dispute is over the 1999 budget
(2) Prodi?s coalition was center-left coalition
(2) The confidence vote was lost by only 1 vote
(1) Prodi is the Italian Prime Minister
(1) Prodi wants a confidence vote from Parliament
NP-rewrite version Communist Refounding, a
fringe group of hard-line leftists who broke with the
minstream Communists after they overhauled the
party following the collapse of Communism in East-
ern Europe rejected Prime Minister Prodi?s proposed
1999 budget. By only one vote, the center-left prime
minister of Italy, Romano Prodi, lost The vote in the
lower chamber of Parliament 313 against the con-
fidence motion brought by the government to 312
in favor in Parliament Friday and was toppled from
power. President Oscar Luigi Scalfaro, who asked
him to stay on as caretaker premier while the head
of state decides whether to call elections.
(4) Prodi lost a confidence vote
(4) Prodi will stay as caretaker until a new gov-
ernment is formed
(4) The Refounding Party is Italy?s Communist
Party
(4) The Refounding Party rejected the govern-
ment?s budget
(3) Scalfaro must decide whether to hold new
elections
(3) The dispute is over the 1999 budget
(2) Prodi?s coalition was center-left coalition
(2) The confidence vote was lost by only 1 vote
(1) Prodi is the Italian Prime Minister
Below is another example, showing the worse de-
terioration of the rewritten summary compared to
the extractive one, both in terms of grammatical-
ity and content. Here, the problem with repetition
during rewrite arises: the same person is mentioned
twice in the sentence and at both places the same
overly long description is selected during rewrie,
rendering the sentence practically unreadable.
Extractive summary Police said Henderson and
McKinney lured Shepard from the bar by saying
they too were gay and one of their girlfriends said
Shepard had embarrassed one of the men by mak-
ing a pass at him. 1,000 people mourned Matthew
Shepherd, the gay University of Wyoming student
who was severely beaten and left to die tied to a
fence. With passersby spontaneously joining the
protest group, two women held another sign that
read,? No Hate Crimes in Wyoming.? Two candle-
light vigils were held Sunday night. Russell An-
derson, 21, and Aaron McKinney, 21, were charged
with attempted murder.
(4) The victim was a student at the University of
Wyoming
(4) The victim was brutally beaten
(4) The victim was openly gay
(3) The crime was widely denounced
(3) The nearly lifeless body was tied to a fence
(3) The victim died
(3) The victim was left to die
(2) The men were arrested on charges of kidnap-
ping and attempted first degree murder
123
(2) There were candlelight vigils in support for
the victim
(1) Russell Henderson and Aaron McKinney are
the names of the people responsible for the death
NP-rewrite version Police said Henderson and
McKinney lured the The slight, soft-spoken 21-
year-old Shepard, a freshman at the University of
Wyoming, who became an overnight symbol of anti-
gay violence after he was found dangling from the
fence by a passerby from a bar by saying they too
were gay and one of their girlfriends said the The
slight, soft-spoken 21-year-old Shepard, a fresh-
man at the University of Wyoming, who became an
overnight symbol of anti-gay violence after he was
found dangling from the fence by a passerby had
embarrassed one of the new ads in that supposedly
hate-free crusade.
(4) The victim was a student at the University of
Wyoming
(3)The nearly lifeless body was tied to a fence (1)
A passerby found the victim
(1) Russell Henderson and Aaron McKinney are
the names of the people responsible for the death
(1) The victim was 22-year old
Even from this unsuccessful attempt for rewrite
we can see how changes of the original text can be
desirable, since some of the newly introduced infor-
mation is in fact suitable for the summary.
4 Conclusions
We have demonstrated that an entity-driven ap-
proach to rewrite in multi-document summarization
can lead to considerably different summary, in terms
of content, compared to the extractive version of
the same system. Indeed, the difference leads to
some improvement measurable in terms of pyramid
method evaluation. The approach also significantly
outperforms in linguistic quality a non-extractive
event-centric system.
Results also show that in terms of linguistic qual-
ity, extractive systems will be curently superior to
systems that alter the original wording from the in-
put. Sadly, extractive and abstractive systems are
evaluated together and compared against each other,
putting pressure on system developers and prevent-
ing them from fully exploring the strengths of gen-
eration techniques. It seems that if researchers
in the field are to explore non-extractive methods,
they would need to compare their systems sepa-
rately from extractive systems, at least in the begin-
ning exploration stages. The development of non-
extractive approaches in absolutely necessary if au-
tomatic summarization were to achieve levels of per-
formance close to human, given the highly abstrac-
tive form of summaries written by people.
Results also indicate that both extractive and non-
extractive systems perform rather poorly in terms of
the focus and coherence of the summaries that they
produce, identifying macro content planning as an
important area for summarization.
References
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3).
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In NAACL-2000.
John Conroy, Judith Schlesinger, and Dianne O?Leary.
2006. Topic-focused multi-document summarization
using an approximate oracle score. In Proceedings of
ACL, companion volume.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL), Sydney, Australia.
Gunes Erkan and Dragomir Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Hongyan Jing and Kathleen McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of the 1st Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?00).
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? step one: Sentence compression. In
Proceeding of The American Association for Artificial
Intelligence Conference (AAAI-2000), pages 703?710.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics, pages 495?501.
124
Chin-Yew Lin and Eduard Hovy. 2002. Automated
multi-document summarization in neats. In Proceed-
ings of the Human Language Technology Conference
(HLT2002 ).
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Development,
2(2):159?165.
K. McKeown, R. Passonneau, D. Elson, A. Nenkova,
and J. Hirschberg. 2005. Do summaries help? a
task-based evaluation of multi-document summariza-
tion. In SIGIR.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing or-
der into texts. In Proceedings of EMNLP 2004, pages
404?411.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that influ-
ence summarization. In Proceedings of SIGIR.
Barry Schiffman, Ani Nenkova, and Kathleen McKeown.
2002. Experiments in multidocument summarization.
In Proceedings of the Human Language Technology
Conference.
Lucy Vanderwende, Michele Banko, and Arul Menezes.
2004. Event-centric summary generation. In Pro-
ceedings of the Document Understanding Conference
(DUC?04).
125
Proceedings of ACL-08: HLT, pages 825?833,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Can you summarize this? Identifying correlates of input difficulty for
generic multi-document summarization
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Abstract
Different summarization requirements could
make the writing of a good summary more dif-
ficult, or easier. Summary length and the char-
acteristics of the input are such constraints in-
fluencing the quality of a potential summary.
In this paper we report the results of a quanti-
tative analysis on data from large-scale evalu-
ations of multi-document summarization, em-
pirically confirming this hypothesis. We fur-
ther show that features measuring the cohe-
siveness of the input are highly correlated with
eventual summary quality and that it is possi-
ble to use these as features to predict the diffi-
culty of new, unseen, summarization inputs.
1 Introduction
In certain situations even the best automatic sum-
marizers or professional writers can find it hard to
write a good summary of a set of articles. If there
is no clear topic shared across the input articles, or
if they follow the development of the same event in
time for a longer period, it could become difficult
to decide what information is most representative
and should be conveyed in a summary. Similarly,
length requirements could pre-determine summary
quality?a short outline of a story might be confus-
ing and unclear but a page long discussion might
give an excellent overview of the same issue.
Even systems that perform well on average pro-
duce summaries of poor quality for some inputs. For
this reason, understanding what aspects of the in-
put make it difficult for summarization becomes an
interesting and important issue that has not been ad-
dressed in the summarization community untill now.
In information retrieval, for example, the variable
system performance has been recognized as a re-
search challenge and numerous studies on identify-
ing query difficulty have been carried out (most re-
cently (Cronen-Townsend et al, 2002; Yom-Tov et
al., 2005; Carmel et al, 2006)).
In this paper we present results supporting the hy-
potheses that input topicality cohesiveness and sum-
mary length are among the factors that determine
summary quality regardless of the choice of summa-
rization strategy (Section 2). The data used for the
analyses comes from the annual Document Under-
standing Conference (DUC) in which various sum-
marization approaches are evaluated on common
data, with new test sets provided each year.
In later sections we define a suite of features cap-
turing aspects of the topicality cohesiveness of the
input (Section 3) and relate these to system perfor-
mance, identifying reliable correlates of input diffi-
culty (Section 4). Finally, in Section 5, we demon-
strate that the features can be used to build a clas-
sifier predicting summarization input difficulty with
accuracy considerably above chance level.
2 Preliminary analysis and distinctions:
DUC 2001
Generic multi-document summarization was fea-
tured as a task at the Document Understanding Con-
ference (DUC) in four years, 2001 through 2004.
In our study we use the DUC 2001 multi-document
task submissions as development data for in-depth
analysis and feature selection. There were 29 in-
put sets and 12 automatic summarizers participating
in the evaluation that year. Summaries of different
825
lengths were produced by each system: 50, 100, 200
and 400 words. Each summary was manually eval-
uated to determine the extent to which its content
overlaped with that of a human model, giving a cov-
erage score. The content comparison was performed
on a subsentence level and was based on elementary
discourse units in the model summary.1
The coverage scores are taken as an indicator of
difficultly of the input: systems achieve low cover-
age for difficult sets and higher coverage for easy
sets. Since we are interested in identifying charac-
teristics of generally difficult inputs rather than in
discovering what types of inputs might be difficult
for one given system, we use the average system
score per set as indicator of general difficulty.
2.1 Analysis of variance
Before attempting to derive characteristics of inputs
difficult for summarization, we first confirm that in-
deed expected performance is influenced by the in-
put itself. We performed analysis of variance for
DUC 2001 data, with automatic system coverage
score as the dependent variable, to gain some insight
into the factors related to summarization difficulty.
The results of the ANOVA with input set, summa-
rizer identity and summary length as factors, as well
as the interaction between these, are shown in Ta-
ble 1.
As expected, summarizer identity is a significant
factor: some summarization strategies/systems are
more effective than others and produce summaries
with higher coverage score. More interestingly, the
input set and summary length factors are also highly
significant and explain more of the variability in
coverage scores than summarizer identity does, as
indicated by the larger values of the F statistic.
Length The average automatic summarizer cov-
erage scores increase steadily as length requirements
are relaxed, going up from 0.50 for 50-word sum-
maries to 0.76 for 400-word summaries as shown in
Table 2 (second row). The general trend we observe
is that on average systems are better at producing
summaries when more space is available. The dif-
1The routinely used tool for automatic evaluation ROUGE
was adopted exactly because it was demonstrated it is highly
correlated with the manual DUC coverage scores (Lin and
Hovy, 2003a; Lin, 2004).
Type 50 100 200 400
Human 1.00 1.17 1.38 1.29
Automatic 0.50 0.55 0.70 0.76
Baseline 0.41 0.46 0.52 0.57
Table 2: Average human, system and baseline coverage
scores for different summary lengths of N words. N =
50, 100, 200, and 400.
ferences are statistically significant2 only between
50-word and 200- and 400-word summaries and be-
tween 100-word and 400-word summaries. The fact
that summary quality improves with increasing sum-
mary length has been observed in prior studies as
well (Radev and Tam, 2003; Lin and Hovy, 2003b;
Kolluru and Gotoh, 2005) but generally little atten-
tion has been paid to this fact in system development
and no specific user studies are available to show
what summary length might be most suitable for
specific applications. In later editions of the DUC
conference, only summaries of 100 words were pro-
duced, focusing development efforts on one of the
more demanding length restrictions. The interaction
between summary length and summarizer is small
but significant (Table 1), with certain summariza-
tion strategies more successful at particular sum-
mary lengths than at others.
Improved performance as measured by increase
in coverage scores is observed for human summa-
rizers as well (shown in the first row of Table 2).
Even the baseline systems (first n words of the most
recent article in the input or first sentences from
different input articles) show improvement when
longer summaries are allowed (performance shown
in the third row of the table). It is important to
notice that the difference between automatic sys-
tem and baseline performance increases as the sum-
mary length increases?the difference between sys-
tems and baselines coverage scores is around 0.1
for the shorter 50- and 100-word summaries but 0.2
for the longer summaries. This fact has favorable
implications for practical system developments be-
cause it indicates that in applications where some-
what longer summaries are appropriate, automati-
cally produced summaries will be much more infor-
mative than a baseline summary.
2One-sided t-test, 95% level of significance.
826
Factor DF Sum of squares Expected mean squares F stat Pr(> F )
input 28 150.702 5.382 59.4227 0
summarizer 11 34.316 3.120 34.4429 0
length 3 16.082 5.361 59.1852 0
input:summarizer 306 65.492 0.214 2.3630 0
input:length 84 36.276 0.432 4.7680 0
summarizer:length 33 6.810 0.206 2.2784 0
Table 1: Analysis of variance for coverage scores of automatic systems with input, summarizer, and length as factors.
Input The input set itself is a highly significant
factor that influences the coverage scores that sys-
tems obtain: some inputs are handled by the systems
better than others. Moreover, the input interacts both
with the summarizers and the summary length.
This is an important finding for several reasons.
First, in system evaluations such as DUC the inputs
for summarization are manually selected by anno-
tators. There is no specific attempt to ensure that
the inputs across different years have on average the
same difficulty. Simply assuming this to be the case
could be misleading: it is possible in a given year to
have ?easier? input test set compared to a previous
year. Then system performance across years can-
not be meaningfully compared, and higher system
scores would not be indicative of system improve-
ment between the evaluations.
Second, in summarization applications there is
some control over the input for summarization. For
example, related documents that need to summa-
rized could be split into smaller subsets that are more
amenable to summarization or routed to an appropri-
ate summarization system than can handle this kind
of input using a different strategy, as done for in-
stance in (McKeown et al, 2002).
Because of these important implications we inves-
tigate input characteristics and define various fea-
tures distinguishing easy inputs from difficult ones.
2.2 Difficulty for people and machines
Before proceeding to the analysis of input difficulty
in multi-document summarization, it is worth men-
tioning that our study is primarily motivated by sys-
tem development needs and consequently the focus
is on finding out what inputs are easy or difficult
for automatic systems. Different factors might make
summarization difficult for people. In order to see to
what extent the notion of summarization input dif-
summary length correlation
50 0.50
100 0.57*
200 0.77**
400 0.70**
Table 3: Pearson correlation between average human and
system coverage scores on the DUC 2001 dataset. Sig-
nificance levels: *p < 0.05 and **p < 0.00001.
ficulty is shared between machines and people, we
computed the correlation between the average sys-
tem and average human coverage score at a given
summary length for all DUC 2001 test sets (shown
in Table 3). The correlation is highest for 200-word
summaries, 0.77, which is also highly significant.
For shorter summaries the correlation between hu-
man and system performance is not significant.
In the remaining part of the paper we deal ex-
clusively with difficulty as defined by system per-
formance, which differs from difficulty for people
summarizing the same material as evidenced by the
correlations in Table 3. We do not attempt to draw
conclusions about any cognitively relevant factors
involved in summarizing.
2.3 Type of summary and difficulty
In DUC 2001, annotators prepared test sets from five
possible predefined input categories:3 .
Single event (3 sets) Documents describing a single
event over a timeline (e.g. The Exxon Valdez
oil spill).
3Participants in the evaluation were aware of the different
categories of input and indeed some groups developed systems
that handled different types of input employing different strate-
gies (McKeown et al, 2001). In later years, the idea of multi-
strategy summarization has been further explored by (Lacatusu
et al, 2006)
827
Subject (6 sets) Documents discussing a single
topic (e.g. Mad cow disease)
Biographical (2 sets) All documents in the input
provide information about the same person
(e.g. Elizabeth Taylor)
Multiple distinct events (12 sets) The documents
discuss different events of the same type (e.g.
different occasions of police misconduct).
Opinion (6 sets) Each document describes a differ-
ent perspective to a common topic (e.g. views
of the senate, congress, public, lawyers etc on
the decision by the senate to count illegal aliens
in the 1990 census).
Figure 1 shows the average system coverage score
for the different input types. The more topically co-
hesive input types such as biographical, single event
and subject, which are more focused on a single en-
tity or news item and narrower in scope, are eas-
ier for systems. The average system coverage score
for them is higher than for the non-cohesive sets
such as multiple distinct events and opinion sets, re-
gardless of summary length. The difference is even
more apparently clear when the scores are plotted af-
ter grouping input types into cohesive (biographical,
single event and subject) and non-cohesive (multi-
ple events and opinion). Such grouping also gives
the necessary power to perform statistical test for
significance, confirming the difference in coverage
scores for the two groups. This is not surprising: a
summary of documents describing multiple distinct
events of the same type is likely to require higher
degree of generalization and abstraction. Summa-
rizing opinions would in addition be highly subjec-
tive. A summary of a cohesive set meanwhile would
contain facts directly from the input and it would be
easier to determine which information is important.
The example human summaries for set D32 (single
event) and set D19 (opinions) shown below give an
idea of the potential difficulties automatic summa-
rizers have to deal with. set D32 On 24 March 1989,
the oil tanker Exxon Valdez ran aground on a reef near
Valdez, Alaska, spilling 8.4 million gallons of crude oil
into Prince William Sound. In two days, the oil spread
over 100 miles with a heavy toll on wildlife. Cleanup
proceeded at a slow pace, and a plan for cleaning 364
miles of Alaskan coastline was released. In June, the
tanker was refloated. By early 1990, only 5 to 9 percent of
spilled oil was recovered. A federal jury indicted Exxon
on five criminal charges and the Valdez skipper was guilty
of negligent discharge of oil.
set D19 Congress is debating whether or not to count ille-
gal aliens in the 1990 census. Congressional House seats
are apportioned to the states and huge sums of federal
money are allocated based on census population. Cali-
fornia, with an estimated half of all illegal aliens, will be
greatly affected. Those arguing for inclusion say that the
Constitution does not mention ?citizens?, but rather, in-
structs that House apportionment be based on the ?whole
number of persons? residing in the various states. Those
opposed say that the framers were unaware of this issue.
?Illegal aliens? did not exist in the U.S. until restrictive
immigration laws were passed in 1875.
The manual set-type labels give an intuitive idea
of what factors might be at play but it is desirable to
devise more specific measures to predict difficulty.
Do such measures exist? Is there a way to automati-
cally distinguish cohesive (easy) from non-cohesive
(difficult) sets? In the next section we define a num-
ber of features that aim to capture the cohesiveness
of an input set and show that some of them are in-
deed significantly related to set difficulty.
3 Features
We implemented 14 features for our analysis of in-
put set difficulty. The working hypothesis is that co-
hesive sets with clear topics are easier to summarize
and the features we define are designed to capture
aspects of input cohesiveness.
Number of sentences in the input, calculated
over all articles in the input set. Shorter inputs
should be easier as there will be less information loss
between the summary and the original material.
Vocabulary size of the input set, equal to the
number of unique words in the input. Smaller vo-
cabularies would be characteristic of easier sets.
Percentage of words used only once in the input.
The rationale behind this feature is that cohesive in-
put sets contain news articles dealing with a clearly
defined topic, so words will be reused across docu-
ments. Sets that cover disparate events and opinions
are likely to contain more words that appear in the
input only once.
Type-token ratio is a measure of the lexical vari-
ation in an input set and is equal to the input vo-
cabulary size divided by the number of words in the
828
Figure 1: Average system coverage scores for summaries in a category
input. A high type-token ratio indicates there is little
(lexical) repetition in the input, a possible side-effect
of non-cohesiveness.
Entropy of the input set. Let X be a discrete ran-
dom variable taking values from the finite set V =
{w1, ..., wn} where V is the vocabulary of the in-
put set and wi are the words that appear in the input.
The probability distribution p(w) = Pr(X = w)
can be easily calculated using frequency counts from
the input. The entropy of the input set is equal to the
entropy of X:
H(X) = ?
i=n
?
i=1
p(wi) log2 p(wi) (1)
Average, minimum and maximum cosine over-
lap between the news articles in the input. Repeti-
tion in the input is often exploited as an indicator of
importance by different summarization approaches
(Luhn, 1958; Barzilay et al, 1999; Radev et al,
2004; Nenkova et al, 2006). The more similar the
different documents in the input are to each other,
the more likely there is repetition across documents
at various granularities.
Cosine similarity between the document vector
representations is probably the easiest and most
commonly used among the various similarity mea-
sures. We use tf*idf weights in the vector represen-
tations, with term frequency (tf) normalized by the
total number of words in the document in order to re-
move bias resulting from high frequencies by virtue
of higher document length alone.
The cosine similarity between two (document
representation) vectors v1 and v2 is given by cos? =
v1.v2
||v1||||v2|| . A value of 0 indicates that the vectors are
orthogonal and dissimilar, a value of 1 indicates per-
fectly similar documents in terms of the words con-
tained in them.
To compute the cosine overlap features, we find
the pairwise cosine similarity between each two
documents in an input set and compute their aver-
age. The minimum and maximum overlap features
are also computed as an indication of the overlap
bounds. We expect cohesive inputs to be composed
of similar documents, hence the cosine overlaps in
these sets of documents must be higher than those in
non-cohesive inputs.
KL divergence Another measure of relatedness
of the documents comprising an input set is the dif-
ference in word distributions in the input compared
to the word distribution in a large collection of di-
verse texts. If the input is found to be largely dif-
ferent from a generic collection, it is plausible to as-
sume that the input is not a random collection of ar-
ticles but rather is defined by a clear topic discussed
within and across the articles. It is reasonable to ex-
pect that the higher the divergence is, the easier it is
to define what is important in the article and hence
the easier it is to produce a good summary.
For computing the distribution of words in a gen-
eral background corpus, we used all the inputs sets
from DUC years 2001 to 2006. The divergence mea-
sure we used is the Kullback Leibler divergence, or
829
relative entropy, between the input (I) and collection
language models. Let pinp(w) be the probability of
the word w in the input and pcoll(w) be the proba-
bility of the word occurring in the large background
collection. Then the relative entropy between the in-
put and the collection is given by
KL divergence =
?
w?I
pinp(w) log2
pinp(w)
pcoll(w)
(2)
Low KL divergence from a random background
collection may be characteristic of highly non-
cohesive inputs consisting of unrelated documents.
Number of topic signature terms for the input
set. The idea of topic signature terms was intro-
duced by Lin and Hovy (Lin and Hovy, 2000) in the
context of single document summarization, and was
later used in several multi-document summarization
systems (Conroy et al, 2006; Lacatusu et al, 2004;
Gupta et al, 2007).
Lin and Hovy?s idea was to automatically iden-
tify words that are descriptive for a cluster of docu-
ments on the same topic, such as the input to a multi-
document summarizer. We will call this cluster T .
Since the goal is to find descriptive terms for the
cluster, a comparison collection of documents not
on the topic is also necessary (we will call this back-
ground collection NT ).
Given T and NT , the likelihood ratio statistic
(Dunning, 1994) is used to identify the topic signa-
ture terms. The probabilistic model of the data al-
lows for statistical inference in order to decide which
terms t are associated with T more strongly than
with NT than one would expect by chance.
More specifically, there are two possibilities for
the distribution of a term t: either it is very indicative
of the topic of cluster T , and appears more often in
T than in documents from NT , or the term t is not
topical and appears with equal frequency across both
T and NT . These two alternatives can be formally
written as the following hypotheses:
H1: P (t|T ) = P (t|NT ) = p (t is not a descrip-
tive term for the input)
H2: P (t|T ) = p1 and P (t|NT ) = p2 and p1 >
p2 (t is a descriptive term)
In order to compute the likelihood of each hypoth-
esis given the collection of the background docu-
ments and the topic cluster, we view them as a se-
quence of words wi: w1w2 . . . wN . The occurrence
of a given word t, wi = t, can thus be viewed a
Bernoulli trial with probability p of success, with
success occurring when wi = t and failure other-
wise.
The probability of observing the term t appearing
k times in N trials is given by the binomial distribu-
tion
b(k,N, p) =
(
N
k
)
pk(1 ? p)N?k (3)
We can now compute
? = Likelihood of the data given H1
Likelihood of the data given H2 (4)
which is equal to
? = b(ct,N, p)b(cT ,NT , p1) ? b(cNT ,NNT , p2)
(5)
The maximum likelihood estimates for the proba-
bilities can be computed directly. p = ctN , where ct is
equal to the number of times term t appeared in the
entire corpus T+NT, and N is the number of words
in the entire corpus. Similarly, p1 = cTNT , where cT
is the number of times term t occurred in T and NT
is the number of all words in T . p2 = cNTNNT , where
cNT is the number of times term t occurred in NT
and NNT is the total number of words in NT.
?2log? has a well-know distribution: ?2. Bigger
values of ?2log? indicate that the likelihood of the
data under H2 is higher, and the ?2 distribution can
be used to determine when it is significantly higher
(?2log? exceeding 10 gives a significance level of
0.001 and is the cut-off we used).
For terms for which the computed ?2log? is
higher than 10, we can infer that they occur more
often with the topic T than in a general corpus NT ,
and we can dub them ?topic signature terms?.
Percentage of signature terms in vocabulary
The number of signature terms gives the total count
of topic signatures over all the documents in the in-
put. However, the number of documents in an input
set and the size of the individual documents across
different sets are not the same. It is therefore possi-
ble that the mere count feature is biased to the length
830
and number of documents in the input set. To ac-
count for this, we add the percentage of topic words
in the vocabulary as a feature.
Average, minimum and maximum topic sig-
nature overlap between the documents in the in-
put. Cosine similarity measures the overlap between
two documents based on all the words appearing in
them. A more refined document representation can
be defined by assuming the document vectors con-
tain only the topic signature words rather than all
words. A high overlap of topic words across two
documents is indicative of shared topicality. The
average, minimum and maximum pairwise cosine
overlap between the tf*idf weighted topic signature
vectors of the two documents are used as features
for predicting input cohesiveness. If the overlap is
large, then the topic is similar across the two docu-
ments and hence their combination will yield a co-
hesive input.
4 Feature selection
Table 4 shows the results from a one-sided t-test
comparing the values of the various features for
the easy and difficult input set classes. The com-
parisons are for summary length of 100 words be-
cause in later years only such summaries were evalu-
ated. The binary easy/difficult classes were assigned
based on the average system coverage score for the
given set, with half of the sets assigned to each class.
In addition to the t-tests we also calculated Pear-
son?s correlation (shown in Table 5) between the fea-
tures and the average system coverage score for each
set. In the correlation analysis the input sets are not
classified into easy or difficult but rather the real val-
ued coverage scores are used directly. Overall, the
features that were identified by the t-test as most de-
scriptive of the differences between easy and diffi-
cult inputs were also the ones with higher correla-
tions with real-valued coverage scores.
Our expectations in defining the features are con-
firmed by the correlation results. For example, sys-
tems have low coverage scores for sets with high-
entropy vocabularies as indicated by the negative
and high by absolute value correlation (-0.4256).
Sets with high entropy are those in which there is
little repetition within and across different articles,
and for which it is subsequently difficult to deter-
feature t-stat p-value
KL divergence* -2.4725 0.01
% of sig. terms in vocab* -2.0956 0.02
average cosine overlap* -2.1227 0.02
vocabulary size* 1.9378 0.03
set entropy* 2.0288 0.03
average sig. term overlap* -1.8803 0.04
max cosine overlap -1.6968 0.05
max topic signature overlap -1.6380 0.06
number of sentences 1.4780 0.08
min topic signature overlap -0.9540 0.17
number of signature terms 0.8057 0.21
min cosine overlap -0.2654 0.39
% of words used only once 0.2497 0.40
type-token ratio 0.2343 0.41
?Significant at a 95% confidence level(p < 0.05)
Table 4: Comparison of non-cohesive (average system
coverage score < median average system score) vs cohe-
sive sets for summary length of 100 words
mine what is the most important content. On the
other hand, sets characterized by bigger KL diver-
gence are easier?there the distribution of words is
skewed compared to a general collection of articles,
with important topic words occurring more often.
Easy to summarize sets are characterized by low
entropy, small vocabulary, high average cosine and
average topic signature overlaps, high KL diver-
gence and a high percentage of the vocabulary con-
sists of topic signature terms.
5 Classification results
We used the 192 sets from multi-document summa-
rization DUC evaluations in 2002 (55 generic sets),
2003 (30 generic summary sets and 7 viewpoint sets)
and 2004 (50 generic and 50 biography sets) to train
and test a logistic regression classifier. The sets from
all years were pooled together and evenly divided
into easy and difficult inputs based on the average
system coverage score for each set.
Table 6 shows the results from 10-fold cross val-
idation. SIG is a classifier based on the six features
identified as significant in distinguishing easy from
difficult inputs based on a t-test comparison (Ta-
ble 4). SIG+yt has two additional features: the year
and the type of summarization input (generic, view-
point and biographical). ALL is a classifier based on
all 14 features defined in the previous section, and
831
feature correlation
set entropy -0.4256
KL divergence 0.3663
vocabulary size -0.3610
% of sig. terms in vocab 0.3277
average sig. term overlap 0.2860
number of sentences -0.2511
max topic signature overlap 0.2416
average cosine overlap 0.2244
number of signature terms -0.1880
max cosine overlap 0.1337
min topic signature overlap 0.0401
min cosine overlap 0.0308
type-token ratio -0.0276
% of words used only once -0.0025
Table 5: Correlation between coverage score and feature
values for the 29 DUC?01 100-word summaries.
features accuracy P R F
SIG 56.25% 0.553 0.600 0.576
SIG+yt 69.27% 0.696 0.674 0.684
ALL 61.45% 0.615 0.589 0.600
ALL+yt 65.10% 0.643 0.663 0.653
Table 6: Logistic regression classification results (accu-
racy, precision, recall and f-measure) for balanced data of
100-word summaries from DUC?02 through DUC?04.
ALL+yt also includes the year and task features.
Classification accuracy is considerably higher
than the 50% random baseline. Using all features
yields better accuracy (61%) than using solely the
6 significant features (accuracy of 56%). In both
cases, adding the year and task leads to extra 3%
net improvement. The best overall results are for
the SIG+yt classifier with net improvement over the
baseline equal to 20%. At the same time, it should
be taken into consideration that the amount of train-
ing data for our experiments is small: a total of 192
sets. Despite this, the measures of input cohesive-
ness capture enough information to result in a clas-
sifier with above-baseline performance.
6 Conclusions
We have addressed the question of what makes the
writing of a summary for a multi-document input
difficult. Summary length is a significant factor,
with all summarizers (people, machines and base-
lines) performing better at longer summary lengths.
An exploratory analysis of DUC 2001 indicated that
systems produce better summaries for cohesive in-
puts dealing with a clear topic (single event, subject
and biographical sets) while non-cohesive sets about
multiple events and opposing opinions are consis-
tently of lower quality. We defined a number of fea-
tures aimed at capturing input cohesiveness, ranging
from simple features such as input length and size
to more sophisticated measures such as input set en-
tropy, KL divergence from a background corpus and
topic signature terms based on log-likelihood ratio.
Generally, easy to summarize sets are character-
ized by low entropy, small vocabulary, high average
cosine and average topic signature overlaps, high
KL divergence and a high percentage of the vocab-
ulary consists of topic signature terms. Experiments
with a logistic regression classifier based on the fea-
tures further confirms that input cohesiveness is pre-
dictive of the difficulty it will pose to automatic sum-
marizers.
Several important notes can be made. First, it is
important to develop strategies that can better handle
non-cohesive inputs, reducing fluctuations in sys-
tem performance. Most current systems are devel-
oped with the expectation they can handle any input
but this is evidently not the case and more attention
should be paid to the issue. Second, the interpre-
tations of year to year evaluations can be affected.
As demonstrated, the properties of the input have a
considerable influence on summarization quality. If
special care is not taken to ensure that the difficulty
of inputs in different evaluations is kept more or less
the same, results from the evaluations are not com-
parable and we cannot make general claims about
progress and system improvements between evalua-
tions. Finally, the presented results are clearly just a
beginning in understanding of summarization diffi-
culty. A more complete characterization of summa-
rization input will be necessary in the future.
References
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan
832
Pelleg. 2006. What makes a query difficult? In SI-
GIR ?06: Proceedings of the 29th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 390?397.
John Conroy, Judith Schlesinger, and Dianne O?Leary.
2006. Topic-focused multi-document summarization
using an approximate oracle score. In Proceedings of
ACL, companion volume.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
2002. Predicting query performance. In Proceedings
of the 25th Annual International ACM SIGIR confer-
ence on Research and Development in Information Re-
trieval (SIGIR 2002), pages 299?306.
Ted Dunning. 1994. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 2007.
Measuring importance and query relevance in topic-
focused multi-document summarization. In ACL?07,
companion volume.
BalaKrishna Kolluru and Yoshihiko Gotoh. 2005. On
the subjectivity of human authored short summaries.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
Finley Lacatusu, Andrew Hickl, Sanda Harabagiu, and
Luke Nezda. 2004. Lite gistexter at duc2004. In Pro-
ceedings of the 4th Document Understanding Confer-
ence (DUC?04).
F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley,
B. Rink, P. Wang, and L. Taylor. 2006. Lcc?s gistexter
at duc 2006: Multi-strategy multi-document summa-
rization. In DUC?06.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics, pages 495?501.
Chin-Yew Lin and Eduard Hovy. 2003a. Automatic eval-
uation of summaries using n-gram co-occurance statis-
tics. In Proceedings of HLT-NAACL 2003.
Chin-Yew Lin and Eduard Hovy. 2003b. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73?80.
Chin-Yew Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In ACL Text Summarization
Workshop.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Development,
2(2):159?165.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivassiloglou,
B. Schiffman, and S. Teufel. 2001. Columbia multi-
document summarization: Approach and evaluation.
In DUC?01.
Kathleen McKeown, Regina Barzilay, David Evans,
Vasleios Hatzivassiloglou, Judith Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with columbia?s newsblaster. In Pro-
ceedings of the 2nd Human Language Technologies
Conference HLT-02.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that influ-
ence summarization. In Proceedings of SIGIR.
Dragomir Radev and Daniel Tam. 2003. Single-
document and multi-document summary evaluation
via relative utility. In Poster session, International
Conference on Information and Knowledge Manage-
ment (CIKM?03).
Dragomir Radev, Hongyan Jing, Malgorzata Sty, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40:919?938.
Elad Yom-Tov, Shai Fine, David Carmel, and Adam Dar-
low. 2005. Learning to estimate query difficulty: in-
cluding applications to missing content detection and
distributed information retrieval. In SIGIR ?05: Pro-
ceedings of the 28th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 512?519.
833
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 169?172,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
High Frequency Word Entrainment in Spoken Dialogue
Ani Nenkova
Dept. of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Agust??n Gravano
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
agus@cs.columbia.edu
Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
julia@cs.columbia.edu
Abstract
Cognitive theories of dialogue hold that en-
trainment, the automatic alignment between
dialogue partners at many levels of linguistic
representation, is key to facilitating both pro-
duction and comprehension in dialogue. In
this paper we examine novel types of entrain-
ment in two corpora?Switchboard and the
Columbia Games corpus. We examine en-
trainment in use of high-frequency words (the
most common words in the corpus), and its as-
sociation with dialogue naturalness and flow,
as well as with task success. Our results show
that such entrainment is predictive of the per-
ceived naturalness of dialogues and is signifi-
cantly correlated with task success; in overall
interaction flow, higher degrees of entrainment
are associated with more overlaps and fewer
interruptions.
1 Introduction
When people engage in conversation, they adapt the
way they speak to their conversational partner. For
example, they often adopt a certain way of describ-
ing something based upon the way their conversa-
tional partner describes it, negotiating a common
description, particularly for items that may be un-
familiar to them (Brennan, 1996). They also alter
their amplitude, if the person they are speaking with
speaks louder than they do (Coulston et al, 2002),
or reuse syntactic constructions employed earlier in
the conversation (Reitter et al, 2006). This phe-
nomenon is known in the literature as entrainment,
accommodation, adaptation, or alignment.
There is a considerable body of literature which
posits that entrainment may be crucial to human per-
ception of dialogue success and overall quality, as
well as to participants? evaluation of their conversa-
tional partners. Pickering and Garrod (2004) pro-
pose that the automatic alignment at many levels of
linguistic representation (lexical, syntactic and se-
mantic) is key for both production and comprehen-
sion in dialogue, and facilitates interaction. Gole-
man (2006) also claims that a key to successful com-
munication is human ability to synchronize their
communicative behavior with that of their conver-
sational partner. For example, in laboratory stud-
ies of non-verbal entrainment (mimicry of manner-
isms and facial expressions between subjects and
a confederate), Chartrand and Bargh (1999) found
not only that subjects displayed a strong uninten-
tional entrainment, but also that greater entrain-
ment/mimicry led subjects to feel that they liked the
confederate more and that the overall interaction was
progressing more smoothly. People who had a high
inclination for empathy (understanding the point of
view of the other) entrained to a greater extent than
others. Reitter et al (2007) also found that degree of
entrainment in lexical and syntactic repetitions that
occurred in only the first five minutes of each dia-
logue significantly predicted task success in studies
of the HCRC Map Task Corpus.
In this paper we examine a novel dimension of
entrainment between conversation partners: the use
of high-frequency words, the most frequent words in
the dialogue or corpus. In Section 2 we describe ex-
periments on high-frequency word entrainment and
perceived dialogue naturalness in Switchboard dia-
169
logues. The degree of high-frequency word entrain-
ment predicts naturalness with an accuracy of 67%
over a 50% baseline. In Section 3 we discuss experi-
ments on the association of high-frequency word en-
trainment with task success and turn-taking. Results
show that degree of high-frequency word entrain-
ment is positively and significantly correlated with
task success and proportion of overlaps in these di-
alogues, and negatively and significantly correlated
with proportion of interruptions.
2 Predicting perceived naturalness
2.1 The Switchboard Corpus
The Switchboard Corpus (Godfrey et al, 1992) is
a collection of recordings of spontaneous telephone
conversations between speakers of many varieties of
American English who were asked to discuss a pre-
assigned topic from a set including favorite types of
music or the new roles of women in society. The
corpus consists of 2430 conversations with an aver-
age duration of 6 minutes, for a total of 240 hours
and three million words. The corpus has been ortho-
graphically transcribed and annotated for degree of
naturalness on Likert scales from 1 (very natural) to
5 (not natural at all).
2.2 Entrainment and perceived naturalness
Previous studies (Niederhoffer and Pennebaker,
2002) have suggested that adaptation in overall word
count as well as words of particular parts of speech,
or words associated with emotion or with various
cognitive states, can predict the degree of coordi-
nation and engagement of conversational partners.
Here, we examine conversational partners? similar-
ity in high-frequency word usage in the Switchboard
corpus as a predictor of the hand-annotated natural-
ness scores for their conversation. Using entrain-
ment over the most frequent words in the entire cor-
pus has the advantage of avoiding sparsity problems;
we hypothesize that it will be more general and ro-
bust than attempting to measure lexical entrainment
over the high-frequency words that occur in a partic-
ular conversation.
Our measure of entrainment entr(w) is defined as
the negated absolute value of the difference between
the fraction of times a particular word w is used by
the two speakers S1 and S2. More formally,
entr(w) = ?
?
?
?
?
countS1(w)
ALLS1
? countS2(w)ALLS2
?
?
?
?
Here, ALLSi is the number of all words ut-
tered by speaker Si in the given conversation, and
countSi(w) is the number of times Si used word w.
The entr(w) statistic was computed for the 100
most common words in the entire Switchboard cor-
pus and feature selection was used to determine the
25 most predictive words used for later classifica-
tion: um, how, okay, go, I?ve, all, very, as, or, up, a,
no, more, something, from, this, what, too, got, can,
he, in, things, you, and.
The data for the experiments was a balanced set of
250 conversations rated ?1? (very natural) and 250
examples of problematic conversations with ratings
of 3, 4 or 5. The accuracy of predicting the binary
naturalness (ratings of 1 or 3-5) of each conversa-
tion from a logistic regression model is 63.76%, sig-
nificantly over a 50% random baseline. This result
confirms the hypothesis that entrainment in high-
frequency word usage is a good indicator of the per-
ceived naturalness of a conversation.
Some of our 25 high-frequency words are in fact
cue phrases, which are important indicators of dia-
logue structure. This suggests that a more focused
examination of this class of words might be useful.
3 Association with task success and
dialogue flow
3.1 The Columbia Games Corpus
The Columbia Games Corpus (Benus et al, 2007) is
a collection of 12 spontaneous task-oriented dyadic
conversations elicited from native speakers of Stan-
dard American English. Subjects played a series
of computer games requiring verbal communication
between partners to achieve a common goal, ei-
ther identifying matching cards appearing on each
of their screens, or moving an object on one screen
to the same location in which it appeared on the
other, where each subject could see only their own
screen. The games were designed to encourage fre-
quent and natural conversation by engaging the sub-
jects in competitive yet collaborative tasks. For ex-
ample, players could receive points in the games in a
variety of ways and had to negotiate the best strategy
170
for matching cards; in other games, they received
more points if they could place objects in exactly
the same location. Subjects were scored on each
game and their overall score determined the addi-
tional monetary compensation they would receive.
A total of 9h 8m (?73,800 words) of dialogue were
recorded. All files in the corpus were orthograph-
ically transcribed and words were hand-aligned by
trained annotators. A subset of the corpus was also
labeled for different types of turn-taking behavior.
These include (i) smooth turn exchanges?speaker
S2 takes the floor after speaker S1 has completed her
turn, with no overlap; (ii) overlaps?S2 starts his
turn before S1 has completely finished her turn, but
S1 does complete her turn; (iii) interruptions?S2
starts talking before S1 completes her turn, and as a
result S1 does not complete her utterance. We used
these annotations to study the association between
entrainment and turn-taking behavior.
3.2 Entrainment and task success
In the Columbia Games Corpus, we hypothesize that
the game score achieved by the participants is a good
measure of the effectiveness of the dialogue. To de-
termine the extent to which task success is related
to the degree of entrainment in high-frequency word
usage, we examined 48 dialogues. We computed the
correlation coefficient between the game score (nor-
malized by the highest achieved score for the game
type) and two different ways of quantifying the de-
gree of entrainment between the speakers (S1 and
S2) in several word classes. In addition to overall
high-frequency words, we looked at two subclasses
of words often used in dialogue:
25MF-G The 25 most frequent words in the game.
25MF-C The 25 most frequent words over the entire
corpus: the, a, okay, and, of, I, on, right, is, it, that, have,
yeah, like, in, left, it?s, uh, so, top, um, bottom, with, you, to.
ACW Affirmative cue words: alright, gotcha, huh,
mm-hm, okay, right, uh-huh, yeah, yep, yes, yup. There
are 5831 instances in the corpus (7.9% of all words).
FP Filled pauses: uh, um, mm. The corpus contains
1845 instances of filled pauses (2.5% of all tokens).
We generalize our measure of word entrainment
entr(w) to each of these classes of words c:
ENTR1(c) =
?
w?c
entr(w)
ENTR1 ranges from 0 to ??, with 0 meaning per-
fect match on usage of lexical items in class c. An
alternative measure of entrainment that we experi-
mented with is defined as
ENTR2(c) = ?
?
w?c
|countS1(w)? countS2(w)|
?
w?c
(countS1(w) + countS2(w))
The entrainment score defined in this way ranges
from 0 to ?1, with 0 meaning perfect match on lex-
ical usage and ?1 meaning perfect mismatch.
The correlations between the normalized game
score and these measures of entrainment are shown
in Table 1. ENTR1 for the 25 most frequent words,
both corpus-wide and game-specific, is highly and
significantly correlated with task success, with
stronger results for game-specific words. For the
ENTR1 ENTR2
Word class cor p cor p
25MF-C 0.341 0.018 0.187 0.202
25MF-G 0.376 0.008 0.260 0.074
ACW 0.230 0.116 0.372 0.009
FP ?0.080 0.591 ?0.007 0.964
Table 1: Pearson?s correlation with game score.
filled pauses class, there is essentially no correlation
between entrainment and task success, while for af-
firmative cue words there is association only under
the ENTR2 definition of entrainment. The differ-
ence in results between ENTR1 and ENTR2 sug-
gests that the two measures of entrainment capture
different aspects of dialogue coordination and that
exploring various formulations of entrainment de-
serves future attention.
3.3 Dialogue coordination
The coordination of turn-taking in dialogue is espe-
cially important for successful interaction. Speech
overlaps (O), might indicate a lively, highly coor-
dinated conversation, with participants anticipating
the end of their interlocutor?s speaking turn. Smooth
switches of turns (S) with no overlapping speech
are also characteristic of good coordination, in cases
where these are not accompanied by long pauses be-
tween turns. On the other hand, interruptions (I)
and long inter-turn latency (L)?long simultaneous
pauses by the speakers? are generally perceived as
a sign of poorly coordinated dialogues.
171
To determine the relationship between entrain-
ment and dialogue coordination, we examined the
correlation between entrainment types and the pro-
portion of interruptions, smooth switches and over-
laps, for which we have manual annotations for a
subset of 12 dialogues. We also looked at the cor-
relation of entrainment with mean latency in each
dialogue. Table 2 summarizes our major findings.
cor p
ENTR1(25MF-C) I ?0.612 0.035
ENTR1(25MF-G) I ?0.514 0.087
ENTR1(ACW) O 0.636 0.026
ENTR2(ACW) O 0.606 0.037
ENTR1(FP) O 0.750 0.005
ENTR2(25MF-G) O 0.605 0.037
ENTR2(25MF-G) S ?0.663 0.019
ENTR2(ACW) L ?0.757 0.004
ENTR2(25MF-G) L ?0.523 0.081
Table 2: Pearson?s correlation with proportion of over-
laps, interruptions, smooth switches, and mean latency.
The two measures that were significantly cor-
related with task success?ENTR1(25MF-C) and
ENTR1(25MF-G)?also correlated negatively with
the proportion of interruptions in the dialogue. This
finding could have important implications for the de-
velopment of spoken dialog systems (SDS). For ex-
ample, a measure of entrainment might be used to
anticipate the user?s propensity to interrupt the sys-
tem, signalling the need to change dialogue strategy.
It also suggests that if the system entrains to users it
might help to reduce such interruptions. While our
study is of association, not causality, this suggests
future areas of investigation.
Our other correlations reveal that turn exchanges
characterized by overlaps are reliably associated
with entrainment in usage of affirmative cue word,
filled pauses and game-specific most frequent
words. Long latency is negatively associated with
entrainment in affirmative cue words and game-
specific most frequent words. Overall, the more
entrainment, the more engaged the participants and
the better coordination there is between them, with
shorter latencies and more overlaps.
Unexpectedly, smooth switches correlate nega-
tively with entrainment in game-specific most fre-
quent words. This result might be confounded by the
presence of long latencies in some switches. While
smooth switches are desirable, especially in SDS,
long latencies between turns can indicate lack of co-
ordination.
4 Conclusion
We present a corpus study relating dialogue natural-
ness, success and coordination with speaker entrain-
ment on common words: most frequent words over-
all, most frequent words in a dialogue, filled pauses,
and affirmative cue words. We find that degree of
entrainment with respect to most frequent words can
distinguish dialogues rated most natural from those
rated less natural. Entrainment over classes of com-
mon words also strongly correlates with task success
and highly engaged and coordinated turn-taking be-
havior. Entrainment over corpus-wide most frequent
words significantly correlates with task success and
minimal interruptions?important goals of SDS. In
future work we will explore the consequences of
system entrainment to SDS users in helping systems
achieve these goals, and the use of simple measures
of entrainment to modify dialogue strategies in order
to decrease the occurrence of user interruptions.
Acknowledgments
This work was funded in part by NSF IIS-0307905.
References
S. Benus, A. Gravano, and J. Hirschberg. 2007.
The prosody of backchannels in American English.
ICPhS?07.
S.E. Brennan. 1996. Lexical entrainment in spontaneous
dialog. ISSD?96.
T. Chartrand and J. Bargh. 1999. The chameleon ef-
fect: the perception-behavior link and social interac-
tion. J. of Personality & Social Psych., 76(6):893?910.
R. Coulston, S. Oviatt, and C. Darves. 2002. Amplitude
convergence in children?s conversational speech with
animated personas. ICSLP?02.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. ICASSP?92.
Daniel Goleman. 2006. Social Intelligence. Bantam.
K. Niederhoffer and J. Pennebaker. 2002. Linguistic
style matching in social interaction.
M. J. Pickering and S. Garrod. 2004. Toward a mecha-
nistic psychology of dialogue. Behavioral and Brain
Sciences, 27:169?226.
D. Reitter and J. Moore. 2007. Predicting success in
dialogue. ACL?07.
D. Reitter, F. Keller, and J.D. Moore. 2006. Compu-
tational Modelling of Structural Priming in Dialogue.
HLT-NAACL?06.
172
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683?691,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic sense prediction for implicit discourse relations in text
Emily Pitler, Annie Louis, Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,lannie,nenkova@seas.upenn.edu
Abstract
We present a series of experiments on au-
tomatically identifying the sense of im-
plicit discourse relations, i.e. relations
that are not marked with a discourse con-
nective such as ?but? or ?because?. We
work with a corpus of implicit relations
present in newspaper text and report re-
sults on a test set that is representative
of the naturally occurring distribution of
senses. We use several linguistically in-
formed features, including polarity tags,
Levin verb classes, length of verb phrases,
modality, context, and lexical features. In
addition, we revisit past approaches using
lexical pairs from unannotated text as fea-
tures, explain some of their shortcomings
and propose modifications. Our best com-
bination of features outperforms the base-
line from data intensive approaches by 4%
for comparison and 16% for contingency.
1 Introduction
Implicit discourse relations abound in text and
readers easily recover the sense of such relations
during semantic interpretation. But automatic
sense prediction for implicit relations is an out-
standing challenge in discourse processing.
Discourse relations, such as causal and contrast
relations, are often marked by explicit discourse
connectives (also called cue words) such as ?be-
cause? or ?but?. It is not uncommon, though, for a
discourse relation to hold between two text spans
without an explicit discourse connective, as the ex-
ample below demonstrates:
(1) The 101-year-old magazine has never had to woo ad-
vertisers with quite so much fervor before.
[because] It largely rested on its hard-to-fault demo-
graphics.
In this paper we address the problem of au-
tomatic sense prediction for discourse relations
in newspaper text. For our experiments, we use
the Penn Discourse Treebank, the largest exist-
ing corpus of discourse annotations for both im-
plicit and explicit relations. Our work is also
informed by the long tradition of data intensive
methods that rely on huge amounts of unanno-
tated text rather than on manually tagged corpora
(Marcu and Echihabi, 2001; Blair-Goldensohn et
al., 2007).
In our analysis, we focus only on implicit dis-
course relations and clearly separate these from
explicits. Explicit relations are easy to iden-
tify. The most general senses (comparison, con-
tingency, temporal and expansion) can be disam-
biguated in explicit relations with 93% accuracy
based solely on the discourse connective used to
signal the relation (Pitler et al, 2008). So report-
ing results on explicit and implicit relations sepa-
rately will allow for clearer tracking of progress.
In this paper we investigate the effectiveness of
various features designed to capture lexical and
semantic regularities for identifying the sense of
implicit relations. Given two text spans, previous
work has used the cross-product of the words in
the spans as features. We examine the most infor-
mative word pair features and find that they are not
the semantically-related pairs that researchers had
hoped. We then introduce several other methods
capturing the semantics of the spans (polarity fea-
tures, semantic classes, tense, etc.) and evaluate
their effectiveness. This is the first study which
reports results on classifying naturally occurring
implicit relations in text and uses the natural dis-
tribution of the various senses.
2 Related Work
Experiments on implicit and explicit relations
Previous work has dealt with the prediction of dis-
course relation sense, but often for explicits and at
the sentence level.
Soricut and Marcu (2003) address the task of
683
parsing discourse structures within the same sen-
tence. They use the RST corpus (Carlson et al,
2001), which contains 385 Wall Street Journal ar-
ticles annotated following the Rhetorical Structure
Theory (Mann and Thompson, 1988). Many of
the useful features, syntax in particular, exploit
the fact that both arguments of the connective are
found in the same sentence. Such features would
not be applicable to the analysis of implicit rela-
tions that occur intersententially.
Wellner et al (2006) used the GraphBank (Wolf
and Gibson, 2005), which contains 105 Associated
Press and 30 Wall Street Journal articles annotated
with discourse relations. They achieve 81% accu-
racy in sense disambiguation on this corpus. How-
ever, GraphBank annotations do not differentiate
between implicits and explicits, so it is difficult to
verify success for implicit relations.
Experiments on artificial implicits Marcu and
Echihabi (2001) proposed a method for cheap ac-
quisition of training data for discourse relation
sense prediction. Their idea is to use unambiguous
patterns such as [Arg1, but Arg2.] to create syn-
thetic examples of implicit relations. They delete
the connective and use [Arg1, Arg2] as an example
of an implicit relation.
The approach is tested using binary classifica-
tion between relations on balanced data, a setting
very different from that of any realistic applica-
tion. For example, a question-answering appli-
cation that needs to identify causal relations (i.e.
as in Girju (2003)), must not only differentiate
causal relations from comparison relations, but
also from expansions, temporal relations, and pos-
sibly no relation at all. In addition, using equal
numbers of examples of each type can be mislead-
ing because the distribution of relations is known
to be skewed, with expansions occurring most fre-
quently. Causal and comparison relations, which
are most useful for applications, are less frequent.
Because of this, the recall of the classification
should be the primary metric of success, while
the Marcu and Echihabi (2001) experiments report
only accuracy.
Later work (Blair-Goldensohn et al, 2007;
Sporleder and Lascarides, 2008) has discovered
that the models learned do not perform as well on
implicit relations as one might expect from the test
accuracies on synthetic data.
3 Penn Discourse Treebank
For our experiments, we use the Penn Discourse
Treebank (PDTB; Prasad et al, 2008), the largest
available annotated corpora of discourse relations.
The PDTB contains discourse annotations over the
same 2,312 Wall Street Journal (WSJ) articles as
the Penn Treebank.
For each explicit discourse connective (such as
?but? or ?so?), annotators identified the two text
spans between which the relation holds and the
sense of the relation.
The PDTB also provides information about lo-
cal implicit relations. For each pair of adjacent
sentences within the same paragraph, annotators
selected the explicit discourse connective which
best expressed the relation between the sentences
and then assigned a sense to the relation. In Exam-
ple (1) above, the annotators identified ?because?
as the most appropriate connective between the
sentences, and then labeled the implicit discourse
relation Contingency.
In the PDTB, explicit and implicit relations are
clearly distinguished, allowing us to concentrate
solely on the implicit relations.
As mentioned above, each implicit and explicit
relation is annotated with a sense. The senses
are arranged in a hierarchy, allowing for annota-
tions as specific as Contingency.Cause.reason. In
our experiments, we use only the top level of the
sense annotations: Comparison, Contingency, Ex-
pansion, and Temporal. Using just these four rela-
tions allows us to be theory-neutral; while differ-
ent frameworks (Hobbs, 1979; McKeown, 1985;
Mann and Thompson, 1988; Knott and Sanders,
1998; Asher and Lascarides, 2003) include differ-
ent relations of varying specificities, all of them
include these four core relations, sometimes under
different names.
Each relation in the PDTB takes two arguments.
Example (1) can be seen as the predicate Con-
tingency which takes the two sentences as argu-
ments. For implicits, the span in the first sentence
is called Arg1 and the span in the following sen-
tence is called Arg2.
4 Word pair features in prior work
Cross product of words Discourse connectives
are the most reliable predictors of the semantic
sense of the relation (Marcu, 2000; Pitler et al,
2008). However, in the absence of explicit mark-
ers, the most easily accessible features are the
684
words in the two text spans of the relation. In-
tuitively, one would expect that there is some rela-
tionship that holds between the words in the two
arguments. Consider for example the following
sentences:
The recent explosion of country funds mirrors the ?closed-
end fund mania? of the 1920s, Mr. Foot says, when narrowly
focused funds grew wildly popular. They fell into oblivion
after the 1929 crash.
The words ?popular? and ?oblivion? are almost
antonyms, and one might hypothesize that their
occurrence in the two text spans is what triggers
the contrast relation between the sentences. Sim-
ilarly, a pair of words such as (rain, rot) might be
indicative of a causal relation. If this hypothesis is
correct, pairs of words (w1, w2) such that w1 ap-
pears in the first sentence and w2 appears in the
second sentence would be good features for iden-
tifying contrast relations.
Indeed, word pairs form the basic feature
of most previous work on classifying implicit
relations (Marcu and Echihabi, 2001; Blair-
Goldensohn et al, 2007; Sporleder and Las-
carides, 2008) or the simpler task of predicting
which connective should be used to express a rela-
tion (Lapata and Lascarides, 2004).
Semantic relations vs. function word pairs If
the hypothesis for word pair triggers of discourse
relations were true, the analysis of unambiguous
relations can be used to discover pairs of words
with causal or contrastive relations holding be-
tween them. Yet, feature analysis has not been per-
formed in prior studies to establish or refute this
possibility.
At the same time, feature selection is always
necessary for word pairs, which are numerous and
lead to data sparsity problems. Here, we present a
meta analysis of the feature selection work in three
prior studies.
One approach for reducing the number of fea-
tures follows the hypothesis of semantic rela-
tions between words. Marcu and Echihabi (2001)
considered only nouns, verbs and and other cue
phrases in word pairs. They found that even
with millions of training examples, prediction re-
sults using all words were superior to those based
on only pairs of non-function words. However,
since the learning curve is steeper when function
words were removed, they hypothesize that using
only non-function words will outperform using all
words once enough training data is available.
In a similar vein, Lapata and Lascarides (2004)
used pairings of only verbs, nouns and adjectives
for predicting which temporal connective is most
suitable to express the relation between two given
text spans. Verb pairs turned out to be one of the
best features, but no useful information was ob-
tained using nouns and adjectives.
Blair-Goldensohn et al (2007) proposed sev-
eral refinements of the word pair model. They
show that (i) stemming, (ii) using a small fixed
vocabulary size consisting of only the most fre-
quent stems (which would tend to be dominated
by function words) and (iii) a cutoff on the mini-
mum frequency of a feature, all result in improved
performance. They also report that filtering stop-
words has a negative impact on the results.
Given these findings, we expect that pairs of
function words are informative features helpful in
predicting discourse relation sense. In our work
that we describe next, we use feature selection to
investigate the word pairs in detail.
5 Analysis of word pair features
For the analysis of word pair features, we use
a large collection of automatically extracted ex-
plicit examples from the experiments in Blair-
Goldensohn et al (2007). The data, from now on
referred to as TextRels, has explicit contrast and
causal relations which were extracted from the En-
glish Gigaword Corpus (Graff, 2003) which con-
tains over four million newswire articles.
The explicit cue phrase is removed from each
example and the spans are treated as belonging to
an implicit relation. Besides cause and contrast,
the TextRels data include a no-relation category
which consists of sentences from the same text that
are separated by at least three other sentences.
To identify features useful for classifying com-
parison vs other relations, we chose a random sam-
ple of 5000 examples for Contrast and 5000 Other
relations (2500 each of Cause and No-relation).
For the complete set of 10,000 examples, word
pair features were computed. After removing
word pairs that appear less than 5 times, the re-
maining features were ranked by information gain
using the MALLET toolkit1.
Table 1 lists the word pairs with highest infor-
mation gain for the Contrast vs. Other and Cause
vs. Other classification tasks. All contain very fre-
quent stop words, and interestingly for the Con-
1mallet.cs.umass.edu
685
trast vs. Other task, most of the word pairs contain
discourse connectives.
This is certainly unexpected, given that word
pairs were formed by deleting the discourse con-
nectives from the sentences expressing Contrast.
Word pairs containing ?but? as one of their ele-
ments in fact signal the presence of a relation that
is not Contrast.
Consider the example shown below:
The government says it has reached most isolated townships
by now, but because roads are blocked, getting anything but
basic food supplies to people remains difficult.
Following Marcu and Echihabi (2001), the pair
[The government says it has reached most isolated
townships by now, but] and [roads are blocked,
getting anything but basic food supplies to peo-
ple remains difficult.] is created as an example of
the Cause relation. Because of examples like this,
?but-but? is a very useful word pair feature indi-
cating Cause, as the but would have been removed
for the artifical Contrast examples. In fact, the top
17 features for classifying Contrast versus Other
all contain the word ?but?, and are indications that
the relation is Other.
These findings indicate an unexpected anoma-
lous effect in the use of synthetic data. Since re-
lations are created by removing connectives, if an
unambiguous connective remains, its presence is a
reliable indicator that the example should be clas-
sified as Other. Such features might work well and
lead to high accuracy results in identifying syn-
thetic implicit relations, but are unlikely to be use-
ful in a realistic setting of actual implicits.
Comparison vs. Other Contingency vs. Other
the-but s-but the-in the-and in-the the-of
of-but for-but but-but said-said to-of the-a
in-but was-but it-but a-and a-the of-the
to-but that-but the-it* to-and to-to the-in
and-but but-the to-it* and-and the-the in-in
a-but he-but said-in to-the of-and a-of
said-but they-but of-in in-and in-of s-and
Table 1: Word pairs with highest information gain.
Also note that the only two features predic-
tive of the comparison class (indicated by * in
Table 1): the-it and to-it, contain only func-
tion words rather than semantically related non-
function words. This ranking explains the obser-
vations reported in Blair-Goldensohn et al (2007)
where removing stopwords degraded classifier
performance and why using only nouns, verbs or
adjectives (Marcu and Echihabi, 2001; Lapata and
Lascarides, 2004) is not the best option2.
6 Features for sense prediction of
implicit discourse relations
The contrast between the ?popular?/?oblivion? ex-
ample we started with above can be analyzed in
terms of lexical relations (near antonyms), but also
could be explained by different polarities of the
two words: ?popular? is generally a positive word,
while ?oblivion? has negative connotations.
While we agree that the actual words in the ar-
guments are quite useful, we also define several
higher-level features corresponding to various se-
mantic properties of the words. The words in the
two text spans of a relation are taken from the
gold-standard annotations in the PDTB.
Polarity Tags: We define features that represent
the sentiment of the words in the two spans. Each
word?s polarity was assigned according to its en-
try in the Multi-perspective Question Answering
Opinion Corpus (Wilson et al, 2005). In this re-
source, each sentiment word is annotated as posi-
tive, negative, both, or neutral. We use the number
of negated and non-negated positive, negative, and
neutral sentiment words in the two text spans as
features. If a writer refers to something as ?nice?
in Arg1, that counts towards the positive sentiment
count (Arg1Positive); ?not nice? would count to-
wards Arg1NegatePositive. A sentiment word is
negated if a word with a General Inquirer (Stone
et al, 1966) Negate tag precedes it. We also have
features for the cross products of these polarities
between Arg1 and Arg2.
We expected that these features could help
Comparison examples especially. Consider the
following example:
Executives at Time Inc. Magazine Co., a subsidiary of
Time Warner, have said the joint venture with Mr. Lang
wasn?t a good one. The venture, formed in 1986, was sup-
posed to be Time?s low-cost, safe entry into women?s maga-
zines.
The word good is annotated with positive po-
larity, however it is negated. Safe is tagged as
having positive polarity, so this opposition could
indicate the Comparison relation between the two
sentences.
Inquirer Tags: To get at the meanings of the
spans, we look up what semantic categories each
2In addition, an informal inspection of 100 word pairs
with high information gain for Contrast vs. Other (the longest
word pairs were chosen, as those are more likely to be content
words) found only six semantically opposed pairs.
686
word falls into according to the General Inquirer
lexicon (Stone et al, 1966). The General In-
quirer has classes for positive and negative polar-
ity, as well as more fine-grained categories such as
words related to virtue or vice. The Inquirer even
contains a category called ?Comp? that includes
words that tend to indicate Comparison, such as
?optimal?, ?other?, ?supreme?, or ?ultimate?.
Several of the categories are complementary:
Understatement versus Overstatement, Rise ver-
sus Fall, or Pleasure versus Pain. Pairs where one
argument contains words that indicate Rise and the
other argument indicates Fall might be good evi-
dence for a Comparison relation.
The benefit of using these tags instead of just
the word pairs is that we see more observations for
each semantic class than for any particular word,
reducing the data sparsity problem. For example,
the pair rose:fell often indicates a Comparison re-
lation when speaking about stocks. However, oc-
casionally authors refer to stock prices as ?jump-
ing? rather than ?rising?. Since both jump and rise
are members of the Rise class, new jump examples
can be classified using past rise examples.
Development testing showed that including fea-
tures for all words? tags was not useful, so we in-
clude the Inquirer tags of only the verbs in the two
arguments and their cross-product. Just as for the
polarity features, we include features for both each
tag and its negation.
Money/Percent/Num: If two adjacent sen-
tences both contain numbers, dollar amounts, or
percentages, it is likely that a comparison rela-
tion might hold between the sentences. We in-
cluded a feature for the count of numbers, percent-
ages, and dollar amounts in Arg1 and Arg2. We
also included the number of times each combina-
tion of number/percent/dollar occurs in Arg1 and
Arg2. For example, if Arg1 mentions a percent-
age and Arg2 has two dollar amounts, the feature
Arg1Percent-Arg2Money would have a count of 2.
This feature is probably genre-dependent. Num-
bers and percentages often appear in financial texts
but would be less frequent in other genres.
WSJ-LM: This feature represents the extent to
which the words in the text spans are typical of
each relation. For each sense, we created uni-
gram and bigram language models over the im-
plicit examples in the training set. We compute
each example?s probability according to each of
these language models. The features are the ranks
of the spans? likelihoods according to the vari-
ous language models. For example, if of the un-
igram models, the most likely relation to generate
this example was Contingency, then the example
would include the feature ContingencyUnigram1.
If the third most likely relation according to the
bigram models was Expansion, then it would in-
clude the feature ExpansionBigram3.
Expl-LM: This feature ranks the text spans ac-
cording to language models derived from the ex-
plicit examples in the TextRels corpus. However,
the corpus contains only Cause, Contrast and No-
relation, hence we expect the WSJ language mod-
els to be more helpful.
Verbs: These features include the number of
pairs of verbs in Arg1 and Arg2 from the same
verb class. Two verbs are from the same verb class
if each of their highest Levin verb class (Levin,
1993) levels (in the LCS Database (Dorr, 2001))
are the same. The intuition behind this feature is
that the more related the verbs, the more likely the
relation is an Expansion.
The verb features also include the average
length of verb phrases in each argument, as well
as the cross product of this feature for the two ar-
guments. We hypothesized that verb chunks that
contain more words, such as ?They [are allowed to
proceed]? often contain rationales afterwards (sig-
nifying Contingency relations), while short verb
phrases like ?They proceed? might occur more of-
ten in Expansion or Temporal relations.
Our final verb features were the part of speech
tags (gold-standard from the Penn Treebank) of
the main verb. One would expect that Expansion
would link sentences with the same tense, whereas
Contingency and Temporal relations would con-
tain verbs with different tenses.
First-Last, First3: The first and last words of
a relation?s arguments have been found to be par-
ticularly useful for predicting its sense (Wellner et
al., 2006). Wellner et al (2006) suggest that these
words are such predictive features because they
are often explicit discourse connectives. In our
experiments on implicits, the first and last words
are not connectives. However, some implicits have
been found to be related by connective-like ex-
pressions which often appear in the beginning of
the second argument. In the PDTB, these are an-
notated as alternatively lexicalized relations (Al-
tLexes). To capture such effects, we included the
first and last words of Arg1 as features, the first
687
and last words of Arg2, the pair of the first words
of Arg1 and Arg2, and the pair of the last words.
We also add two additional features which indicate
the first three words of each argument.
Modality: Modal words, such as ?can?,
?should?, and ?may?, are often used to express
conditional statements (i.e. ?If I were a wealthy
man, I wouldn?t have to work hard.?) thus signal-
ing a Contingency relation. We include a feature
for the presence or absence of modals in Arg1 and
Arg2, features for specific modal words, and their
cross-products.
Context: Some implicit relations appear imme-
diately before or immediately after certain explicit
relations far more often than one would expect due
to chance (Pitler et al, 2008). We define a feature
indicating if the immediately preceding (or follow-
ing) relation was an explicit. If it was, we include
the connective trigger of the relation and its sense
as features. We use oracle annotations of the con-
nective sense, however, most of the connectives
are unambiguous.
One might expect a different distribution of re-
lation types in the beginning versus further in the
middle of a paragraph. We capture paragraph-
position information using a feature which indi-
cates if Arg1 begins a paragraph.
Word pairs Four variants of word pair mod-
els were used in our experiments. All the models
were eventually tested on implicit examples from
the PDTB, but the training set-up was varied.
Wordpairs-TextRels In this setting, we trained
a model on word pairs derived from unannotated
text (TextRels corpus).
Wordpairs-PDTBImpl Word pairs for training
were formed from the cross product of words in
the textual spans (Arg1 x Arg2) of the PDTB im-
plicit relations.
Wordpairs-selected Here, only word pairs from
Wordpairs-PDTBImpl with non-zero information
gain on the TextRels corpus were retained.
Wordpairs-PDTBExpl In this case, the model
was formed by using the word pairs from the ex-
plicit relations in the sections of the PDTB used
for training.
7 Classification Results
For all experiments, we used sections 2-20 of the
PDTB for training and sections 21-22 for testing.
Sections 0-1 were used as a development set for
feature design.
We ran four binary classification tasks to iden-
tify each of the main relations from the rest. As
each of the relations besides Expansion are infre-
quent, we train using equal numbers of positive
and negative examples of the target relation. The
negative examples were chosen at random. We
used all of sections 21 and 22 for testing, so the
test set is representative of the natural distribution.
The training sets contained: Comparison (1927
positive, 1927 negative), Contingency (3500
each), Expansion3 (6356 each), and Temporal
(730 each).
The test set contained: 151 examples of Com-
parison, 291 examples of Contingency, 986 exam-
ples of Expansion, 82 examples of Temporal, and
13 examples of No-relation.
We used Naive Bayes, Maximum Entropy
(MaxEnt), and AdaBoost (Freund and Schapire,
1996) classifiers implemented in MALLET.
7.1 Non-Wordpair Features
The performance using only our semantically in-
formed features is shown in Table 7. Only the
Naive Bayes classification results are given, as
space is limited and MaxEnt and AdaBoost gave
slightly lower accuracies overall.
The table lists the f-score for each of the target
relations, with overall accuracy shown in brack-
ets. Given that the experiments are run on natural
distribution of the data, which are skewed towards
Expansion relations, the f-score is the more impor-
tant measure to track.
Our random baseline is the f-score one would
achieve by randomly assigning classes in propor-
tion to its true distribution in the test set. The best
results for all four tasks are considerably higher
than random prediction, but still low overall. Our
features provide 6% to 18% absolute improve-
ments in f-score over the baseline for each of the
four tasks. The largest gain was in the Contin-
gency versus Other prediction task. The least im-
provement was for distinguishing Expansion ver-
sus Other. However, since Expansion forms the
largest class of relations, its f-score is still the
highest overall. We discuss the results per relation
class next.
Comparison We expected that polarity features
would be especially helpful for identifying Com-
3The PDTB also contains annotations of entity relations,
which most frameworks consider a subset of Expansion.
Thus, we include relations annotated as EntRel as positive
examples of Expansion.
688
Features Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Four-way
Money/Percent/Num 19.04 (43.60) 18.78 (56.27) 22.01 (41.37) 10.40 (23.05) (63.38)
Polarity Tags 16.63 (55.22) 19.82 (76.63) 71.29 (59.23) 11.12 (18.12) (65.19)
WSJ-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Expl-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Verbs 18.55 (26.19) 36.59 (62.44) 59.36 (52.53) 12.61 (41.63) (65.33)
First-Last, First3 21.01 (52.59) 36.75 (59.09) 63.22 (56.99) 15.93 (61.20) (65.40)
Inquirer tags 17.37 (43.8) 15.76 (77.54) 70.21 (58.04) 11.56 (37.69) (62.21)
Modality 17.70 (17.6) 21.83 (76.95) 15.38 (37.89) 11.17 (27.91) (65.33)
Context 19.32 (56.66) 29.55 (67.42) 67.77 (57.85) 12.34 (55.22) (64.01)
Random 9.91 19.11 64.74 5.38
Table 2: f-score (accuracy) using different features; Naive Bayes.
parison relations. Surprisingly, polarity was actu-
ally one of the worst classes of features for Com-
parison, achieving an f-score of 16.33 (in contrast
to using the first, last and first three words of the
sentences as features, which leads to an f-score of
21.01). We examined the prevalence of positive-
negative or negative-positive polarity pairs in our
training set. 30% of the Comparison examples
contain one of these opposite polarity pairs, while
31% of the Other examples contain an opposite
polarity pair. To our knowledge, this is the first
study to examine the prevalence of polarity words
in the arguments of discourse relations in their
natural distributions. Contrary to popular belief,
Comparisons do not tend to have more opposite
polarity pairs.
The two most useful classes of features for rec-
ognizing Comparison relations were the first, last
and first three words in the sentence and the con-
text features that indicate the presence of a para-
graph boundary or of an explicit relation just be-
fore or just after the location of the hypothesized
implicit relation (19.32 f-score).
Contingency The two best features for the Con-
tingency vs. Other distinction were verb informa-
tion (36.59 f-score) and first, last and first three
words in the sentence (36.75 f-score). Context
again was one of the features that led to improve-
ment. This makes sense, as Pitler et al (2008)
found that implicit contingencies are often found
immediately following explicit comparisons.
We were surprised that the polarity features
were helpful for Contingency but not Comparison.
Again we looked at the prevalence of opposite po-
larity pairs. While for Comparison versus Other
there was not a significant difference, for Contin-
gency there are quite a few more opposite polarity
pairs (52%) than for not Contingency (41%).
The language model features were completely
useless for distinguishing contingencies from
other relations.
Expansion As Expansion is the majority class
in the natural distribution, recall is less of a prob-
lem than precision. The features that help achieve
the best f-score are all features that were found to
be useful in identifying other relations.
Polarity tags, Inquirer tags and context were
the best features for identifying expansions with
f-scores around 70%.
Temporal Implicit temporal relations are rela-
tively rare, making up only about 5% of our test
set. Most temporal relations are explicitly marked
with a connective like ?when? or ?after?.
Yet again, the first and last words of the sen-
tence turned out to be useful indicators for tem-
poral relations (15.93 f-score). The importance of
the first and last words for this distinction is clear.
It derives from the fact that temporal implicits of-
ten contain words like ?yesterday? or ?Monday? at
the end of the sentence. Context is the next most
helpful feature for temporal relations.
7.2 Which word pairs help?
For Comparison and Contingency, we analyze the
behavior of word pair features under several differ-
ent settings. Specifically we want to address two
important related questions raised in recent work
by others: (i) is unannotated data from explicits
useful for training models that disambiguate im-
plicit discourse relations and (ii) are explicit and
implicit relations intrinsically different from each
other.
Wordpairs-TextRels is the worst approach. The
best use of word pair features is Wordpairs-
selected. This model gives 4% better absolute f-
score for Comparison and 14% for Contingency
over Wordpairs-TextRels. In this setting the Tex-
tRels data was used to choose the word pair fea-
tures, but the probabilities for each feature were
estimated using the training portion of the PDTB
689
Comp. vs. Other
Wordpairs-TextRels 17.13 (46.62)
Wordpairs-PDTBExpl 19.39 (51.41)
Wordpairs-PDTBImpl 20.96 (42.55)
First-last, first3 (best-non-wp) 21.01 (52.59)
Best-non-wp + Wordpairs-selected 21.88 (56.40)
Wordpairs-selected 21.96 (56.59)
Cont. vs. Other
Wordpairs-TextRels 31.10 (41.83)
Wordpairs-PDTBExpl 37.77 (56.73)
Wordpairs-PDTBImpl 43.79 (61.92)
Polarity, verbs, first-last, first3,
modality, context (best-non-wp)
42.14 (66.64)
Wordpairs-selected 45.60 (67.10)
Best-non-wp + Wordpairs-selected 47.13 (67.30)
Expn. vs. Other
Best-non-wp + wordpairs 62.39 (59.55)
Wordpairs-PDTBImpl 63.84 (60.28)
Polarity, inquirer tags, context (best-
non-wp)
76.42 (63.62)
Temp. vs. Other
First-last, first3 (best-non-wp) 15.93 (61.20)
Wordpairs-PDTBImpl 16.21 (61.98)
Best-non-wp + Wordpairs-PDTBImpl 16.76 (63.49)
Table 3: f-score (accuracy) of various feature sets;
Naive Bayes.
implicit examples.
We also confirm that even within the PDTB,
information from annotated explicit relations
(Wordpairs-PDTBExpl) is not as helpful as
information from annotated implicit relations
(Wordpairs-PDTBImpl). The absolute difference
in f-score between the two models is close to 2%
for Comparison, and 6% for Contingency.
7.3 Best results
Adding other features to word pairs leads to im-
proved performance for Contingency, Expansion
and Temporal relations, but not for Comparison.
For contingency detection, the best combina-
tion of our features included polarity, verb in-
formation, first and last words, modality, context
with Wordpairs-selected. This combination led
to a definite improvement, reaching an f-score of
47.13 (16% absolute improvement in f-score over
Wordpairs-TextRels).
For detecting expansions, the best combination
of our features (polarity+Inquirer tags+context)
outperformed Wordpairs-PDTBImpl by a wide
margin, close to 13% absolute improvement (f-
scores of 76.42 and 63.84 respectively).
7.4 Sequence Model of Discourse Relations
Our results from the previous section show that
classification of implicits benefits from informa-
tion about nearby relations, and so we expected
improvements using a sequence model, rather than
classifying each relation independently.
We trained a CRF classifier (Lafferty et al,
2001) over the sequence of implicit examples from
all documents in sections 02 to 20. The test set
is the same as used for the 2-way classifiers. We
compare against a 6-way4 Naive Bayes classifier.
Only word pairs were used as features for both.
Overall 6-way prediction accuracy is 43.27% for
the Naive Bayes model and 44.58% for the CRF
model.
8 Conclusion
We have presented the first study that predicts im-
plicit discourse relations in a realistic setting (dis-
tinguishing a relation of interest from all others,
where the relations occur in their natural distri-
butions). Also unlike prior work, we separate the
task from the easier task of explicit discourse pre-
diction. Our experiments demonstrate that fea-
tures developed to capture word polarity, verb
classes and orientation, as well as some lexical
features are strong indicators of the type of dis-
course relation.
We analyze word pair features used in prior
work that were intended to capture such semantic
oppositions. We show that the features in fact do
not capture semantic relation but rather give infor-
mation about function word co-occurrences. How-
ever, they are still a useful source of information
for discourse relation prediction. The most bene-
ficial application of such features is when they are
selected from a large unannotated corpus of ex-
plicit relations, but then trained on manually an-
notated implicit relations.
Context, in terms of paragraph boundaries and
nearby explicit relations, also proves to be useful
for the prediction of implicit discourse relations.
It is helpful when added as a feature in a standard,
instance-by-instance learning model. A sequence
model also leads to over 1% absolute improvement
for the task.
9 Acknowledgments
This work was partially supported by NSF grants
IIS-0803159, IIS-0705671 and IGERT 0504487.
We would like to thank Sasha Blair-Goldensohn
for providing us with the TextRels data and for
the insightful discussion in the early stages of our
work.
4the four main relations, EntRel, NoRel
690
References
N. Asher and A. Lascarides. 2003. Logics of conver-
sation. Cambridge University Press.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and Refining Rhetorical-
Semantic Relation Models. In Proceedings of
NAACL HLT, pages 428?435.
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of the Second SIGdial Workshop on Discourse and
Dialogue, pages 1?10.
B.J. Dorr. 2001. LCS Verb Database. Technical Re-
port Online Software Database, University of Mary-
land, College Park, MD.
Y. Freund and R.E. Schapire. 1996. Experiments with
a New Boosting Algorithm. In Machine Learning:
Proceedings of the Thirteenth International Confer-
ence, pages 148?156.
R. Girju. 2003. Automatic detection of causal relations
for Question Answering. In Proceedings of the ACL
2003 workshop on Multilingual summarization and
question answering-Volume 12, pages 76?83.
D. Graff. 2003. English gigaword corpus. Corpus
number LDC2003T05, Linguistic Data Consortium,
Philadelphia.
J. Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67?90.
A. Knott and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmat-
ics, 30(2):135?175.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Interna-
tional Conference on Machine Learning 2001, pages
282?289.
M. Lapata and A. Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT-
NAACL 2004: Main Proceedings.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. Chicago, IL.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. Text, 8.
D. Marcu and A. Echihabi. 2001. An unsupervised
approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 368?375.
D. Marcu. 2000. The Theory and Practice of Dis-
course and Summarization. The MIT Press.
K. McKeown. 1985. Text Generation: Using Dis-
course strategies and Focus Constraints to Gener-
ate Natural Language Text. Cambridge University
Press, Cambridge, England.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING08), short paper.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347?354.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational
Linguistics, 31(2):249?288.
691
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13?16,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Using Syntax to Disambiguate Explicit Discourse Connectives in Text
?
Emily Pitler and Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,nenkova@seas.upenn.edu
Abstract
Discourse connectives are words or
phrases such as once, since, and on
the contrary that explicitly signal the
presence of a discourse relation. There
are two types of ambiguity that need to
be resolved during discourse processing.
First, a word can be ambiguous between
discourse or non-discourse usage. For
example, once can be either a temporal
discourse connective or a simply a word
meaning ?formerly?. Secondly, some
connectives are ambiguous in terms of the
relation they mark. For example since
can serve as either a temporal or causal
connective. We demonstrate that syntactic
features improve performance in both
disambiguation tasks. We report state-of-
the-art results for identifying discourse
vs. non-discourse usage and human-level
performance on sense disambiguation.
1 Introduction
Discourse connectives are often used to explicitly
mark the presence of a discourse relation between
two textual units. Some connectives are largely
unambiguous, such as although and additionally,
which are almost always used as discourse con-
nectives and the relations they signal are unam-
biguously identified as comparison and expansion,
respectively. However, not all words and phrases
that can serve as discourse connectives have these
desirable properties.
Some linguistic expressions are ambiguous be-
tween DISCOURSE AND NON-DISCOURSE US-
AGE. Consider for example the following sen-
tences containing and and once.
?
This work was partially supported by NSF grants IIS-
0803159, IIS-0705671 and IGERT 0504487.
(1a) Selling picked up as previous buyers bailed out of their
positions and aggressive short sellers? anticipating fur-
ther declines?moved in.
(1b) My favorite colors are blue and green.
(2a) The asbestos fiber, crocidolite, is unusually resilient
once it enters the lungs, with even brief exposures to
it causing symptoms that show up decades later, re-
searchers said.
(2b) A form of asbestos once used to make Kent cigarette
filters has caused a high percentage of cancer deaths
among a group of workers exposed to it more than 30
years ago, researchers reported.
In sentence (1a), and is a discourse connec-
tive between the two clauses linked by an elabo-
ration/expansion relation; in sentence (1b), the oc-
currence of and is non-discourse. Similarly in sen-
tence (2a), once is a discourse connective marking
the temporal relation between the clauses ?The as-
bestos fiber, crocidolite is unusually resilient? and
?it enters the lungs?. In contrast, in sentence (2b),
once occurs with a non-discourse sense, meaning
?formerly? and modifying ?used?.
The only comprehensive study of discourse vs.
non-discourse usage in written text
1
was done in
the context of developing a complete discourse
parser for unrestricted text using surface features
(Marcu, 2000). Based on the findings from a
corpus study, Marcu?s parser ?ignored both cue
phrases that had a sentential role in a majority of
the instances in the corpus and those that were
too ambiguous to be explored in the context of a
surface-based approach?.
The other ambiguity that arises during dis-
course processing involves DISCOURSE RELA-
TION SENSE. The discourse connective since for
1
The discourse vs. non-discourse usage ambiguity is even
more problematic in spoken dialogues because there the num-
ber of potential discourse markers is greater than that in writ-
ten text, including common words such as now, well and
okay. Prosodic and acoustic features are the most powerful
indicators of discourse vs. non-discourse usage in that genre
(Hirschberg and Litman, 1993; Gravano et al, 2007)
13
instance can signal either a temporal or a causal
relation as shown in the following examples from
Miltsakaki et al (2005):
(3a) There have been more than 100 mergers and acquisi-
tions within the European paper industry since the most
recent wave of friendly takeovers was completed in the
U.S. in 1986.
(3b) It was a far safer deal for lenders since NWA had a
healthier cash flow and more collateral on hand.
Most prior work on relation sense identifica-
tion reports results obtained on data consisting of
both explicit and implicit relations (Wellner et al,
2006; Soricut and Marcu, 2003). Implicit relations
are those inferred by the reader in the absence of
a discourse connective and so are hard to identify
automatically. Explicit relations are much easier
(Pitler et al, 2008).
In this paper, we explore the predictive power of
syntactic features for both the discourse vs. non-
discourse usage (Section 3) and discourse relation
sense (Section 4) prediction tasks for explicit con-
nectives in written text. For both tasks we report
high classification accuracies close to 95%.
2 Corpus and features
2.1 Penn Discourse Treebank
In our work we use the Penn Discourse Treebank
(PDTB) (Prasad et al, 2008), the largest public
resource containing discourse annotations. The
corpus contains annotations of 18,459 instances
of 100 explicit discourse connectives. Each dis-
course connective is assigned a sense from a three-
level hierarchy of senses. In our experiments
we consider only the top level categories: Ex-
pansion (one clause is elaborating information in
the other), Comparison (information in the two
clauses is compared or contrasted), Contingency
(one clause expresses the cause of the other), and
Temporal (information in two clauses are related
because of their timing). These top-level discourse
relation senses are general enough to be annotated
with high inter-annotator agreement and are com-
mon to most theories of discourse.
2.2 Syntactic features
Syntactic features have been extensively used
for tasks such as argument identification: di-
viding sentences into elementary discourse units
among which discourse relations hold (Soricut
and Marcu, 2003; Wellner and Pustejovsky, 2007;
Fisher and Roark, 2007; Elwell and Baldridge,
2008). Syntax has not been used for discourse vs.
non-discourse disambiguation, but it is clear from
the examples above that discourse connectives ap-
pear in specific syntactic contexts.
The syntactic features we used were extracted
from the gold standard Penn Treebank (Marcus et
al., 1994) parses of the PDTB articles:
Self Category The highest node in the tree
which dominates the words in the connective but
nothing else. For single word connectives, this
might correspond to the POS tag of the word, how-
ever for multi-word connectives it will not. For
example, the cue phrase in addition is parsed as
(PP (IN In) (NP (NN addition) )). While the POS
tags of ?in? and ?addition? are preposition and
noun, respectively, together the Self Category of
the phrase is prepositional phrase.
Parent Category The category of the immedi-
ate parent of the Self Category. This feature is
especially helpful for disambiguating cases simi-
lar to example (1b) above in which the parent of
and would be an NP (the noun phrase ?blue and
green?), which will rarely be the case when and
has a discourse function.
Left Sibling Category The syntactic category
of the sibling immediately to the left of the Self
Category. If the left sibling does not exist, this fea-
tures takes the value ?NONE?. Note that having no
left sibling implies that the connective is the first
substring inside its Parent Category. In example
(1a), this feature would be ?NONE?, while in ex-
ample (1b), the left sibling of and is ?NP?.
Right Sibling Category The syntactic category
of the sibling immediately to the right of the Self
Category. English is a right-branching language,
and so dependents tend to occur after their heads.
Thus, the right sibling is particularly important as
it is often the dependent of the potential discourse
connective under investigation. If the connective
string has a discourse function, then this depen-
dent will often be a clause (SBAR). For example,
the discourse usage in ?After I went to the store,
I went home? can be distinguished from the non-
discourse usage in ?After May, I will go on vaca-
tion? based on the categories of their right siblings.
Just knowing the syntactic category of the right
sibling is sometimes not enough; experiments on
the development set showed improvements by in-
cluding more features about the right sibling.
Consider the example below:
(4) NASA won?t attempt a rescue; instead, it will try to pre-
dict whether any of the rubble will smash to the ground
14
and where.
The syntactic category of ?where? is SBAR, so the
set of features above could not distinguish the sin-
gle word ?where? from a full embedded clause
like ?I went to the store?. In order to address
this deficiency, we include two additional features
about the contents of the right sibling, Right Sib-
ling Contains a VP and Right Sibling Contains
a Trace.
3 Discourse vs. non-discourse usage
Of the 100 connectives annotated in the PDTB,
only 11 appear as a discourse connective more
than 90% of the time: although, in turn, af-
terward, consequently, additionally, alternatively,
whereas, on the contrary, if and when, lest, and on
the one hand...on the other hand. There is quite
a range among the most frequent connectives: al-
though appears as a discourse connective 91.4% of
the time, while or only serves a discourse function
2.8% of the times it appears.
For training and testing, we used explicit dis-
course connectives annotated in the PDTB as pos-
itive examples and occurrences of the same strings
in the PDTB texts that were not annotated as ex-
plicit connectives as negative examples.
Sections 0 and 1 of the PDTB were used for de-
velopment of the features described in the previous
section. Here we report results using a maximum
entropy classifier
2
using ten-fold cross-validation
over sections 2-22.
The results are shown in Table 3. Using the
string of the connective as the only feature sets
a reasonably high baseline, with an f-score of
75.33% and an accuracy of 85.86%. Interest-
ingly, using only the syntactic features, ignoring
the identity of the connective, is even better, re-
sulting in an f-score of 88.19% and accuracy of
92.25%. Using both the connective and syntactic
features is better than either individually, with an
f-score of 92.28% and accuracy of 95.04%.
We also experimented with combinations of
features. It is possible that different con-
nectives have different syntactic contexts for
discourse usage. Including pair-wise interac-
tion features between the connective and each
syntactic feature (features like connective=also-
RightSibling=SBAR) raised the f-score about
1.5%, to 93.63%. Adding interaction terms be-
tween pairs of syntactic features raises the f-score
2
http://mallet.cs.umass.edu
Features Accuracy f-score
(1) Connective Only 85.86 75.33
(2) Syntax Only 92.25 88.19
(3) Connective+Syntax 95.04 92.28
(3)+Conn-Syn Interaction 95.99 93.63
(3)+Conn-Syn+Syn-Syn Interaction 96.26 94.19
Table 1: Discourse versus Non-discourse Usage
slightly more, to 94.19%. These results amount
to a 10% absolute improvement over those ob-
tained by Marcu (2000) in his corpus-based ap-
proach which achieves an f-score of 84.9%
3
for
identifying discourse connectives in text. While
bearing in mind that the evaluations were done on
different corpora and so are not directly compara-
ble, as well as that our results would likely drop
slightly if an automatic parser was used instead of
the gold-standard parses, syntactic features prove
highly beneficial for discourse vs. non-discourse
usage prediction, as expected.
4 Sense classification
While most connectives almost always occur with
just one of the senses (for example, because is al-
most always a Contingency), a few are quite am-
biguous. For example since is often a Temporal
relation, but also often indicates Contingency.
After developing syntactic features for the dis-
course versus non-discourse usage task, we inves-
tigated whether these same features would be use-
ful for sense disambiguation.
Experiments and results We do classification be-
tween the four senses for each explicit relation
and report results on ten-fold cross-validation over
sections 2-22 of the PDTB using a Naive Bayes
classifier
4
.
Annotators were allowed to provide two senses
for a given connective; in these cases, we consider
either sense to be correct
5
. Contingency and Tem-
poral are the senses most often annotated together.
The connectives most often doubly annotated in
the PDTB are when (205/989), and (183/2999),
and as (180/743).
Results are shown in Table 4. The sense clas-
sification accuracy using just the connective is al-
ready quite high, 93.67%. Incorporating the syn-
tactic features raises performance to 94.15% accu-
3
From the reported precision of 89.5% and recall of
80.8%
4
We also ran a MaxEnt classifier and achieved quite sim-
ilar but slightly lower results.
5
Counting only the first sense as correct leads to about 1%
lower accuracy.
15
Features Accuracy
Connective Only 93.67
Connective+Syntax+Conn-Syn 94.15
Interannotator agreement 94
on sense class (Prasad et al, 2008)
Table 2: Four-way sense classification of explicits
racy. While the improvement is not huge, note that
we seem to be approaching a performance ceiling.
The human inter-annotator agreement on the top
level sense class was also 94%, suggesting further
improvements may not be possible. We provide
some examples to give a sense of the type of er-
rors that still occur.
Error Analysis While Temporal relations are the
least frequent of the four senses, making up only
19% of the explicit relations, more than half of
the errors involve the Temporal class. By far
the most commonly confused pairing was Contin-
gency relations being classified as Temporal rela-
tions, making up 29% of our errors.
A random example of each of the most common
types of errors is given below.
(5) Builders get away with using sand and financiers junk
[when] society decides it?s okay, necessary even, to
look the other way. Predicted: Temporal Correct:
Contingency
(6) You get a rain at the wrong time [and] the crop is ruined.
Predicted: Expansion Correct: Contingency
(7) In the nine months, imports rose 20% to 155.039 trillion
lire [and] exports grew 18% to 140.106 trillion lire.
Predicted: Expansion Correct: Comparison
(8) [The biotechnology concern said] Spanish authorities
must still clear the price for the treatment [but] that
it expects to receive such approval by year end. Pre-
dicted: Comparison Correct: Expansion
Examples (6) and (7) show the relatively rare
scenario when and does not signal expansion, and
Example (8) shows but indicating a sense besides
comparison. In these cases where the connective
itself is not helpful in classifying the sense of the
relation, it may be useful to incorporate features
that were developed for classifying implicit rela-
tions (Sporleder and Lascarides, 2008).
5 Conclusion
We have shown that using a few syntactic features
leads to state-of-the-art accuracy for discourse vs.
non-discourse usage classification. Including syn-
tactic features also helps sense class identification,
and we have already attained results at the level of
human annotator agreement. These results taken
together show that explicit discourse connectives
can be identified automatically with high accuracy.
References
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In Proceedings of the International Confer-
ence on Semantic Computing, Santa Clara, CA.
S. Fisher and B. Roark. 2007. The utility of parse-
derived features for automatic discourse segmenta-
tion. In Proceedings of ACL, pages 488?495.
A. Gravano, S. Benus, H. Chavez, J. Hirschberg, and
L. Wilcox. 2007. On the role of context and prosody
in the interpretation of ?okay?. In Proceedings of
ACL, pages 800?807.
J. Hirschberg and D. Litman. 1993. Empirical stud-
ies on the disambiguation of cue phrases. Computa-
tional linguistics, 19(3):501?530.
D. Marcu. 2000. The rhetorical parsing of unrestricted
texts: A surface-based approach. Computational
Linguistics, 26(3):395?448.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
E. Miltsakaki, N. Dinesh, R. Prasad, A. Joshi, and
B. Webber. 2005. Experiments on sense annota-
tion and sense disambiguation of discourse connec-
tives. In Proceedings of the Fourth Workshop on
Treebanks and Linguistic Theories (TLT 2005).
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In COLING, short paper.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
LREC?08.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
B. Wellner and J. Pustejovsky. 2007. Automatically
identifying the arguments of discourse connectives.
In Proceedings of EMNLP-CoNLL, pages 92?101.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue.
16
Proceedings of NAACL HLT 2007, pages 9?16,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
To Memorize or to Predict: Prominence Labeling in Conversational Speech
A. Nenkova, J. Brenier, A. Kothari, S. Calhoun?, L. Whitton, D. Beaver, D. Jurafsky
Stanford University
{anenkova,jbrenier,anubha,lwhitton,dib,jurafsky}@stanford.edu
?University of Edinburgh
Sasha.Calhoun@ed.ac.uk
Abstract
The immense prosodic variation of natural con-
versational speech makes it challenging to pre-
dict which words are prosodically prominent in
this genre. In this paper, we examine a new fea-
ture, accent ratio, which captures how likely it is
that a word will be realized as prominent or not.
We compare this feature with traditional accent-
prediction features (based on part of speech and
N -grams) as well as with several linguistically mo-
tivated and manually labeled information structure
features, such as whether a word is given, new, or
contrastive. Our results show that the linguistic fea-
tures do not lead to significant improvements, while
accent ratio alone can yield prediction performance
almost as good as the combination of any other sub-
set of features. Moreover, this feature is useful even
across genres; an accent-ratio classifier trained only
on conversational speech predicts prominence with
high accuracy in broadcast news. Our results sug-
gest that carefully chosen lexicalized features can
outperform less fine-grained features.
1 Introduction
Being able to predict the prominence or pitch accent
status of a word in conversational speech is impor-
tant for implementing text-to-speech in dialog sys-
tems, as well as in detection of prosody in conversa-
tional speech recognition.
Previous investigations of prominence prediction
from text have primarily relied on robust surface fea-
tures with some deeper information structure fea-
tures. Surface features like a word?s part-of-speech
(POS) (Hirschberg, 1993) and its unigram and bi-
gram probability (Pan and McKeown, 1999; Pan and
0Thanks to the Edinburgh-Stanford Link and ONR (MURI
award N000140510388) for generous support.
Hirschberg, 2000) are quite useful; content words
are much more likely to be accented than function
words, and words with higher probability are less
likely to be prominent. More sophisticated linguis-
tic features have also been used, generally based on
information-structural notions of contrast, focus, or
given-new. (Hirschberg, 1993).
For example, in the Switchboard utterance be-
low, there is an intrinsic contrast between the words
?women? and ?men?, making both terms more
salient (words in all capital letters represent promi-
nent tokens):
you SEE WOMENc GOING off to WARS as WELL as
MENc.
Similarly the givenness of a word may help deter-
mine its prominence. The speaker needs to focus the
hearer?s attention on new entities in the discourse, so
these are likely to be realized as prominent. Old en-
tities, on the other had, need not be prominent; these
tendencies can be seen in the following example.
theyold have all the WATERnew theyold WANT. theyold
can ACTUALLY PUMP waterold.
While previous models have attempted to cap-
ture global properties of words (via POS or unigram
probability), they have not in general used word
identity as a predictive feature, assuming either that
current supervised training sets would be too small
or that word identity would not be robust across gen-
res (Pan et al, 2002). In this paper, we show a way
to capture word identity in a feature, accent ratio,
that works well with current small supervised train-
ing sets, and is robust to genre differences.
We also use a corpus which has been hand-
labeled for information structure features (including
given/new and contrast information) to investigate
the relative usefulness of both linguistic and shallow
features, as well as how well different features com-
bine with each other.
9
2 Data and features
For our experiments we use 12 Switchboard (God-
frey et al, 1992) conversations, 14,555 tokens in to-
tal. Each word was manually labeled for presence
or absence of pitch accent1 , as well as additional
features including information status (or givenness),
contrast and animacy distinctions, (Nissim et al,
2004; Calhoun et al, 2005; Zaenen et al, 2004), fea-
tures that linguistic literature suggests are predictive
of prominence (Bolinger, 1961; Chafe, 1976).
All of the features described in detail below have
been shown to have statistically significant correla-
tion with prominence (Brenier et al, 2006).
Information status The information status (IS),
or givenness, of discourse entities is important for
choosing appropriate reference form (Prince, 1992;
Gundel et al, 1993) and possibly plays a role in
prominence decisions as well (Brown, 1983). No
previous studies have examined the usefulness of
information status in naturally occurring conversa-
tional speech. The annotation in our corpus is based
on the givenness hierarchy of Prince: first mentions
of entities were marked as new and subsequent men-
tions as old. Entities that are not previously men-
tioned, but that are generally known or semantically
related to other entities in the preceding context are
marked as mediated. Obviously, the givenness an-
notation applies only to referring expressions, i.e.
noun phrases the semantic interpretation of which is
a discourse entity. This restriction inherently limits
the power of the feature for prominence prediction,
which has to be performed for all classes of words.
Complete details of the IS annotation can be found
in (Nissim et al, 2004).
Kontrast One reason speakers make entities in
an utterance prominence is because of information
structure considerations (Rooth, 1992; Vallduv?? and
Vilkuna, 1998). That is, parts of an utterance which
distinguish the information the speaker actually says
from the information they could have said, are made
salient, e.g. because that information answers a
question, or contrasts with a similar entity in the
context. Several possible triggers of this sort of
salience were marked in the corpus, with words that
were not kontrastive (in this sense) being marked as
background:
1Of all tokens, 8,429 (or 58%) were not accented.
? contrastive if the word is directly differentiated
from a previous topical or semantically-related
word;
? subset if it refers to a member of a more general
set mentioned in the surrounding context;
? adverbial if a focus-sensitive adverb such as
?only? or ?even? is associated with the word
being annotated;
? correction if the speaker intended to correct or
clarify a previous word or phrase;
? answer if the word completes a question by the
other speaker;
? nonapplic for filler phrases such as ?in fact?, ?I
mean?, etc.
Note that only content words in full sentences
were marked for kontrast, and filler phrases such
as ?in fact? and ?I mean? were excluded. A com-
plete description of the annotation guidelines can be
found in (Calhoun et al, 2005).
Animacy Each noun and pronoun is labeled for the
animacy of its referent (Zaenen et al, 2004). The
categories include concrete, non-concrete, human,
organizations, place, and time.
Dialog act Specifies the function of the utterance
such as statement, opinion, agree, reject, abandon;
or type of question (yes/no, who, rhetoric)
In addition to the above theoretically motivated
features, we used several automatically derivable
word measures.
Part-of-speech Two such features were used, the
full Penn Treebank tagset (called POS) , and a col-
lapsed tagset (called BroadPOS) with six broad cat-
egories (nouns, verbs, function words, pronouns, ad-
jectives and adverbs).
Unigram and bigram probability These features
are defined as log(pw) and log(pwi |pwi?1) respec-
tively and their values were calculated from the
Fisher corpus (Cieri et al, 2004). High probability
words are less likely to be prominent.
TF.IDF This measure captures how central a word is
for a particular conversation. It is a function of the
frequency of occurrence of the word in the conver-
sation (nw), the number of conversations that con-
tain the word in a background corpus (k) and the
number of all conversations in the background cor-
pus (N ). Formally, TF.IDF1 = nw ? log(Nk ). We
10
also used a variant, TF.IDF2, computed by normal-
izing TF.IDF1 by the number of occurrences of the
most frequent word in the conversation. TF.IDF2 =
TF.IDF1/max(nw?conv). Words with high TF.IDF
values are important in the conversation and are
more likely to be prominent.
Stopword This is a binary feature indicating if the
word appears in a high-frequency stopword list from
the Bow toolkit (McCallum, 1996). The list spans
both function and content word classes, though nu-
merals and some nouns and verbs were removed.
Utterance length The number of words.
Length The number of characters in the words. This
feature is correlated with phonetic features that have
been shown to be useful for the task, such as the
number of vowels or phones in the word.
Position from end/beginning The position of the
word in the utterance divided by the number of
words that precede the current word.
Accent ratio This final (new) feature takes the
?memorization? of previous productions of a given
word to the extreme, measuring how likely it is that
a word belongs to a prominence class or not. Our
feature extends an earlier feature proposed by (Yuan
et al, 2005), which was a direct estimate of how
likely it is for the word to be accented as observed
in some corpus. (Yuan et al, 2005) showed that the
original accent ratio feature was not included in the
best set of features for accent prediction. We believe
the reason for this is the fact that the original ac-
cent ratio feature was computed for all words, even
words in which the value was indistinguishable from
chance (.50). Our new feature incorporates the sig-
nificance of the prominence probability, assuming a
default value of 0.5 for those words for which there
is insufficient evidence in the training data. More
specifically,
AccentRatio(w) =
{
k
n if B(k, n, 0.5) ? 0.05
0.5 otherwise
where k is the number of times word w appeared
accented in the corpus, n is the total number of
times the word w appeared, and B(k, n, 0.5) is
the probability (under a binomial distribution) that
there are k successes in n trials if the probabil-
ity of success and failure is equal. Simply put,
the accent ratio of a word is equal to the esti-
mated probability of the word being accented if this
probability is significantly different from 0.5, and
equal to 0.5 otherwise. For example, AccentRa-
tio(you)=0.3407, AccentRatio(education)=0.8666,
and AccentRatio(probably)=0.5.
Many of our features for accent prediction are
based only on the 12 training conversations. Other
features, such as the unigram, bigram, and TF*IDF
features, are computed from larger data sources. Ac-
cent ratio is also computed over a larger corpus,
since the binomial test requires a minimum of six
occurrences of a word in the corpus in order to get
significance and assign an accent ratio value differ-
ent from 0.5. We thus used 60 Switchboard conver-
sations (Ostendorf et al, 2001), annotated for pitch
accent, to compute k and n for each word.
3 Results
For our experiments we used the J48 decision trees
in WEKA (Witten and Frank, 2005). All the results
that we report are from 10-fold cross-validation on
the 12 Switchboard conversations.
Some previous studies have reported results on
prominence prediction in conversational speech with
the Switchboard corpus. Unfortunately these studies
used different parts of the corpus or different label-
ings (Gregory and Altun, 2004; Yuan et al, 2005),
so our results are not directly comparable. Bear-
ing this difference in mind, the best reported results
to our knowledge are those in (Gregory and Altun,
2004), where conditional random fields were used
with both textual, acoustic, and oracle boundary fea-
tures to yield 76.36% accuracy.
Table 1 shows the performance of decision tree
classifiers using a single feature. The majority class
baseline (not accented) has accuracy of 58%. Accent
ratio is the most predictive feature: the accent ratio
classifier has accuracy of 75.59%, which is two per-
cent net improvement above the previously known
best feature (unigram). The accent ratio classifier
assigns a ?no accent? class to all words with accent
ratio lower than 0.38 and ?accent? to all other words.
In Section 4 we discuss in detail the accent ratio dic-
tionary, but it is worth noting that it does correctly
classify even some high-frequency function words
like ?she?, ?he?, ?do? or ?up? as accented.
11
3.1 Combining features
We would expect that a combination of features
would lead to better prediction when compared to
a classifier based on a single feature. Several past
studies have examined classes of features. In order
to quantify the utility of different specific features,
we ran exhaustive experiments producing classifiers
with all possible combinations of two, three, four
and five features.
As we can see from figure 1 and table 2, the clas-
sifiers using accent ratio as a feature perform best,
for all sizes of feature sets. Moreover, the increase
of performance compared to a single-feature classi-
fier is very slight when accent ratio is used as fea-
ture. Kontrast seems to combine well with accent
ratio and all of the best classifiers with more than
one feature use kontrast in addition to accent ratio.
This indicates that automatic detection of kontrast
can potentially help in prominence prediction. But
the gains are small, the best classifiers without kon-
trast but still including accent ratio perform within
0.2 percent of the classifiers that use both.
On the other hand, classifiers that do not use ac-
cent ratio perform poorly compared to those that do,
and even a classifier using five features (unigram,
broad POS, token length, position from beginning
and bigram) performs about as well as a classifier
using solely accent ratio as a feature. Also, when
accent ratio is not used, the overall improvement of
the classifier grows faster with the addition of new
features. This suggest that accent ratio provides rich
information about words beyond that of POS class
and general informativeness.2
Table 2 gives the specific features in (n + 1)-
feature classifiers that lead to better results than the
best n-classifier. The figures are for the classifiers
performing best overall. Interestingly, none of these
best classifiers for all feature set sizes uses POS or
unigram as a feature. We assume that accent ratio
captures all the relevant information that is present
in the unigram and POS features. The best classifier
with five features uses, in addition to accent ratio,
kontrast, tf.idf, information status and distance from
the beginning of the utterance. All of these features
convey somewhat orthogonal information: seman-
2To verify this we will examine the accent ratio dictionary
in closer detail in the next section.
Accent Ratio (AR) 75.59%
AR + Kontrast 76.15%
AR + END/BEG 75.91%
AR + tf.idf2 75.82%
AR + Info Status 75.82%
AR + Length 75.77%
AR + tf.idf1 75.74%
AR + unigram 75.71%
AR + stopword 75.70%
AR + kontrast + length 76.45%
AR + kontrast + BEG 76.24%
AR + kontrast + unigram 76.24%
AR + kontrast + tf.idf1 76.24%
AR + kontrast + length + tfidf1 76.56%
AR + kontrast + length + stopword 76.54%
AR + kontrast + length +tf.idf2 76.52%
AR + kontrast + Status + BEG 76.47%
AR + kontrast + tf.idf1 + Status + BEG 76.65%
AR + kontrast + tf.idf2 + Status + BEG 76.58%
Table 2: Performance increase augmenting the ac-
cent ratio classifier.
tic, topicality, discourse and phrasing information
respectively. Still, all of them in combination im-
prove the performance over accent ratio as a single
feature only by one percent.
Figure 1 shows the overall improvement of clas-
sifiers with the addition of new features in three sce-
narios: overall best, best when kontrast is not used
as a feature and best with neither kontrast nor ac-
cent ratio. The best classifier with five features that
do not include kontrast has accent ratio, broad POS,
word length, stopword and bigram as features and
has accuracy of 76.28%, or just 0.27% worse than
the overall best classifier that uses kontrast and in-
formation status. This indicates that while there is
some benefit to using the two features, they do not
lead to any substantial boost in performance. Strik-
ingly, the best classifier that uses neither accent ra-
tio nor kontrast performs very similarly to a classi-
fier using accent ratio as the only feature: 75.82%
for the classifier using unigram, POS, tf.idf1, word
length and position from end of the utterance.
3.2 The power of linguistic features
One of the objectives of our study was to assess how
useful gold-standard annotations for complex lin-
guistic features are for the task of prominence pre-
diction. The results in this section indicate that an-
imacy distinctions (concrete/non-concrete, person,
time, etc) and dialog act did not have much power
12
AccentRatio unigram stopword POS tf.idf2 tf.idf1 BroadPos Length Kontrast bigram Info Stat
75.59 73.77 70.77 70.28 70.14 69.50 68.64 67.64 67.57 65.87 64.13
Table 1: Single feature classifier performance. Features not in the table (position from end, animacy, utter-
ance length and dialog act) all achieve lower accuracy of around 60%
1 2 3 4 5
73
74
75
76
77
78
Classifier performance
Number of features
Pr
ed
ict
io
n 
ac
cu
ra
cy
Overall best
Without kontrast
Without accent
+ ratio or kontrast
Figure 1: Performance increase with the addition of
new features.
as individual features (table 1) and were never in-
cluded in a model that was best for a given feature
set size (table 2).
Information status is somewhat useful and ap-
pears in the overall best classifier with five features
(table 2). But when compared with other classifiers
with the same number of features, the benefits from
adding information status to the model are small.
For example, the accent ratio + information status
classifier performs 0.23% better than accent ratio
alone, but so does the classifier using accent ratio
and tf.idf. There are two reasons that can explain
why the givenness of the referent is not as helpful
as we might have hoped. First of all, the informa-
tion status distinction applies only to referring ex-
pressions and has undefined values for words such
as verbs, adjectives or function words. Second, in-
formation status of an entity influences the form of
referring expression that is used, with old items be-
ing more likely to be pronominalized. In the numer-
ous cases where pronominalization of old informa-
tion does occur, features such as POS, unigram or
accent ratio will be sensitive to the change of infor-
mation status simply based on the lexical item.
Kontrast is by far the most useful linguistic fea-
ture. It is used in all of the best classifiers for any
feature set size (table 2). It applies to more words
than givenness does, since salience distinctions can
be made for any part-of-speech class. Still, not all
words were annotated for kontrast either, and more-
over kontrast only captures one kind of semantic
salience. This is particularly true of discourse mark-
ers like ?especially? or ?definitely?: these would ei-
ther be in sentence fragments that weren?t marked
for kontrast, or would probably be marked as ?back-
ground? since they are not salience triggers in a se-
mantic sense. As we can see from figure 1, clas-
sifiers that use kontrast perform only slightly better
than others that use only ?cheaper? features.
4 The accent ratio dictionary
Contrary to our initial expectations, both classes in
the accent ratio dictionary (for both low and high
probability of being prominent) cover the full set of
possible POS categories. Tables 3 and 4 list words in
both classes (with words sorted by increasing accent
ratio in each column). The ?no accent? class is dom-
inated by function words, but also includes nouns
and verbs. One of the drawbacks of POS as a fea-
ture for prominence prediction is that normally aux-
iliary verbs will be tagged as ?VB?, the same class
as other more contentful verbs. The informativeness
(unigram probability) of a word would distinguish
between these types of verbs, but so does the accent
ratio measure as well.
Furthermore, some relatively frequent words such
as ?too?, ?now?, ?both?, ?no?, ?yes?, ?else?, ?wow?
have high accent ratio, that is, a high probability for
accenting. Such distinctions within the class of func-
tion words would not be possible on the basis of in-
13
.00?.08 .09?.16 .17?.24 .25?.32 .33?.42
a could you?d being me
uh in because take i?ve
um minutes oh said we?re
uh-huh and since wanna went
the by says been over
an who us those you
of grew where into thing
to cause they?ve little what
were gonna am until some
as about sort they?re out
than their you?re I had
with but didn?t that make
at on her don?t way
for be going this did
from through i?ll should anything
or which will type i?m
you?ve are our we kind
was we?ll just so go
would during though have stuff
it huh like got then
when is your new she
them bit needs mean he
it?s there?s my much do
if any many i?d up
can has they know
him stayed get doesn?t
these supposed there even
Table 3: Accent ratio entries with low prominence
probability.
formativeness, POS, or even information structure
features. Another class like that is words like ?yes?,
?okay?, ?sure? that are mostly accented by virtue of
being the only word in the phrase.
Some rather common words, ?not? for example,
are not included in the accent ratio dictionary be-
cause they do not exhibit a statistically strong pref-
erence for a prominence class. The accent ratio clas-
sifier would thus assign class ?accented? to the word
?not?, which is indeed the class this word occurs in
more often.
Another fact that becomes apparent with the in-
spection of the accent ratio dictionary is that while
certain words have a statistically significant prefer-
ence for deaccenting, there is also a lot of variation
in their observed realization. For example, personal
pronouns such as ?I? and ?you? have accent ratios
near 0.33. This means that every third such pronoun
was actually realized as prominent by the speaker.
In a conversational setting there is an implicit con-
trast between the two speakers, which could partly
explain the phenomenon, but the situations which
prompt the speaker to realize the distinction in their
.58?.74 .75?.79 .80?.86 .87?1.0
lot both sometimes half
time no change topic
now seems child else
kids life young obviously
old tell Texas themselves
too ready town wow
really easy room gosh
three heard pay anyway
work isn?t interesting Dallas
nice again true outside
yeah first mother mostly
two right problems yes
person children agree great
day married war exactly
working may needed especially
job happen told definitely
talking business finally lately
usually still neat thirty
rather daughter sure higher
places gone house forty
government guess okay hey
ten news seven Iowa
parents major best poor
paper fact also glad
actually five older basic
Table 4: Accent ratio values for words with high
probability for being accented.
speech will be the focus of a future linguistic inves-
tigation.
Kontrast is helpful in predicting ?accented? class
for some generally low ratio words. However, even
with its help, production variation in the conversa-
tions cannot be fully explained. The following ex-
amples from our corpus show low accent ratio words
(that, did, and, have, had) that were produced as
prominent.
so i did THAT. and then i, you know, i DID that for SIX
years. AND then i stayed HOME with my SON.
i HAVE NOT, to be honest, HAD much EXPERIENCE
with CHILDREN in that SITUATION.
they?re going to HAVE to WORK it OUT to WORKING
part TIME.
The examples attest to the presence of variation
in production: in the first utterance, for example, we
see the words ?did?, ?and? and ?that? produced both
as prominent and not prominent. Intonational phras-
ing most probably accounts for some of this varia-
tion since it is likely that even words that are typ-
ically not prominent will be accented if they occur
just before or after a longer pause. We come back to
this point in the closing section.
14
5 Robustness of accent ratio
While accent ratio works well for our data (Table
2), a feature based so strongly on memorizing the
status of each word in the training data might lead
to problems. One potential problem, suggested by
Pan et al (2002) for lexicalized features in general,
is whether a lexical feature like accent ratio might
be less robust across genres. Another question is
whether our definition of accent ratio is better than
one that does not use the binomial test: we need to
investigate whether these statistical tests indeed im-
prove performance. We focus on these two issues in
the next two subsections.
Binomial test cut-off
As discussed above, the original accent ratio feature
(Yuan et al, 2005) was based directly on the frac-
tion of accented occurrences in the training set. We
might expect such a use of raw frequencies to be
problematic. Given what we know about word dis-
tributions in text (Baayen, 2001), we would expect
about half of the words in a big corpus to appear only
once. In an accent ratio dictionary without binomial
test cut-off, all such words will have accent ratio of
either exactly 1 or 0, but one or even few occurrences
of a word would not be enough to determine statis-
tical significance. By contrast, our modified accent
ratio feature uses binomial test cut-off to make the
accent ratio more robust to small training sets.
To test if the binomial test cut-off really improved
the accent ratio feature, we compared the perfor-
mance on Switchboard of classifiers using accent
ratio with and without cut-off. The binominal test
improved the performance of the accent ratio fea-
ture from 73.49% (Yuan et al original version) to
75.59% (our version).
Moreover, Yuan et al report that their version of
the feature did not combine well with other features,
while in our experiments best performance was al-
ways achieved by the classifiers that made use of the
accent ratio feature in addition to others.
A cross-genre experiment: broadcast news
In a systematic analysis of the usefulness of differ-
ent informativeness, syntactic and semantic features
for prominence prediction, Pan et al (2002) showed
that word identity is a powerful feature. But they hy-
pothesized that this would not be a useful feature in
a domain independent pitch accent prediction task.
Their hypothesis that word identity cannot be a ro-
bust across genres would obviously carry over to ac-
cent ratio. In order to test the hypothesis, we used
the accent ratio dictionary derived from the Switch-
board corpus to predict prominence in the Boston
University Radio corpus of broadcast news. Using
an accent ratio dictionary from Switchboard and as-
signing class ?not accented? to words with accent ra-
tio less than 0.38 and ?accented? otherwise leads to
82% accuracy of prediction for this broadcast news
corpus. If the accent ratio dictionary is built from
the BU corpus itself, the performance is 83.67%.3
These results indicate that accent ratio is a robust
enough feature and is applicable across genres.
6 Conclusions and future work
In this paper we introduced a new feature for promi-
nence prediction, accent ratio. The accent ratio of
a word is the (maximum likelihood estimate) prob-
ability that a word is accented if there is a signifi-
cant preference for a class, and 0.5 otherwise. Our
experiments demonstrate that the feature is power-
ful both by itself and in combination with other fea-
tures. Moreover, the feature is robust to genre, and
accent ratio dictionaries can be used for prediction
of prominence in read news with very good results.
Of the linguistic features we examined, kontrast
is the only one that is helpful beyond what can be
gained using shallow features such as n-gram prob-
ability, POS or tf.idf. While the improvements from
kontrast are relatively small, the consistency of these
small improvements suggest that developing auto-
matic methods for approximating the gold-standard
annotation we used here, similar to what has been
done for information status in (Nissim, 2006), may
be worthwhile. An automatic predictor for kontrast
may also be helpful in other applications such as
question answering or textual entailment.
All of the features in our study were text-based.
There is a wide variety of research investigating
phonological or acoustic features as well. For exam-
ple Gregory and Altun (2004) used acoustic features
3This result is comparable with the result of (Yuan et al,
2005) who in their experiment with the same corpus report the
best result as 83.9% using three features: unigram, bigram and
backwards bigram probability.
15
such as duration and energy, and phonological fea-
tures such as oracle (hand-labeled) intonation phrase
boundaries, and the number of phones and sylla-
bles in a word. Although acoustic features are not
available in a text-to-speech scenario, we hypothe-
size that in a task where such features are available
(such as in speech recognition applications), acous-
tic or phonological features could improve the per-
formance of our text-only features. To test this hy-
pothesis, we augmented our best 5-feature classifier
which did not include kontrast with hand-labeled in-
tonation phrase boundary information. The resulting
classifier reached an accuracy of 77.45%, more than
one percent net improvement over 76.28% accuracy
of the model based solely on text features and not in-
cluding kontrast. Thus in future work we plan to in-
corporate more acoustic and phonological features.
Finally, prominence prediction classifiers need to
be incorporated in a speech synthesis system and
their performance should be gauged via listening
experiments that test whether the incorporation of
prominence leads to improvement in synthesis.
References
R. H. Baayen. 2001. Word Frequency Distributions.
Kluwer Academic Publishers.
D.L. Bolinger. 1961. Contrastive Accent and Contrastive
Stress. Language, 37(1):83?96.
J. Brenier, A. Nenkova, A. Kothari, L. Whitton,
D. Beaver, and D. Jurafsky. 2006. The (non)utility of
linguistic features for predicting prominence in spon-
taneous speech. In IEEE/ACL 2006 Workshop on Spo-
ken Language Technology.
G. Brown. 1983. Prosodic structure and the given/new
distinction. Prosody: Models and Measurements,
pages 67?77.
S. Calhoun, M. Nissim, M. Steedman, and J.M. Brenier.
2005. A framework for annotating information struc-
ture in discourse. Pie in the Sky: Proceedings of the
workshop, ACL, pages 45?52.
W. Chafe. 1976. Givenness, contrastiveness, definite-
ness, subjects, topics, and point of view. Subject and
Topic, pages 25?55.
C. Cieri, D. Graff, O. Kimball, D. Miller, and Kevin
Walker. 2004. Fisher English training speech part 1
transcripts. LDC.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In IEEE ICASSP-92.
M. Gregory and Y. Altun. 2004. Using conditional ran-
dom fields to predict pitch accents in conversational
speech. Proceedings of ACL, 2004.
J. Gundel, N. Hedberg, and R. Zacharski. 1993. Cog-
nitive status and the form of referring expressions in
discourse. Language, 69:274?307.
J. Hirschberg. 1993. Pitch Accent in Context: Predicting
Intonational Prominence from Text. Artificial Intelli-
gence, 63(1-2):305?340.
A. McCallum. 1996. Bow: A toolkit for statistical lan-
guage modeling, text retrieval, classification and clus-
tering. http://www.cs.cmu.edu/ mccallum/bow.
M. Nissim, S. Dingare, J. Carletta, and M. Steedman.
2004. An annotation scheme for information status in
dialogue. In LREC 2004.
M. Nissim. 2006. Learning information status of dis-
course entities. In Proceedings of EMNLP 2006.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labeled database of spontaneous speech. Proc. of the
ISCA Workshop on Prosody in Speech Recognition and
Understanding, pages 119?121.
S. Pan and J. Hirschberg. 2000. Modeling local context
for pitch accent prediction. In Proceedings of ACL-00.
S. Pan and K. McKeown. 1999. Word informativeness
and automatic pitch accent modeling. In Proceedings
of EMNLP/VLC-99.
S. Pan, K. McKeown, and J. Hirschberg. 2002. Ex-
ploring features from natural language generation in
prosody modeling. Computer speech and language,
16:457?490.
E. Prince. 1992. The ZPG letter: subject, definiteness,
and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a
fund raising text, pages 295?325. John Benjamins.
Mats Rooth. 1992. A theory of focus interpretation. Nat-
ural Language Semantics, 1(1):75?116.
E. Vallduv?? and M. Vilkuna. 1998. On rheme and kon-
trast. Syntax and Semantics, 29:79?108.
I. H. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. 2nd Edition,
Morgan Kaufmann, San Francisco.
J. Yuan, J. Brenier, and D. Jurafsky. 2005. Pitch Accent
Prediction: Effects of Genre and Speaker. Proceed-
ings of Interspeech.
A. Zaenen, J. Carletta, G. Garretson, J. Bresnan,
A. Koontz-Garboden, T. Nikitina, M.C. O?Connor, and
T. Wasow. 2004. Animacy Encoding in English: why
and how. ACL Workshop on Discourse Annotation.
16
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Measuring Importance and Query Relevance in Topic-focused
Multi-document Summarization
Surabhi Gupta and Ani Nenkova and Dan Jurafsky
Stanford University
Stanford, CA 94305
surabhi@cs.stanford.edu, {anenkova,jurafsky}@stanford.edu
Abstract
The increasing complexity of summarization systems
makes it difficult to analyze exactly which mod-
ules make a difference in performance. We carried
out a principled comparison between the two most
commonly used schemes for assigning importance to
words in the context of query focused multi-document
summarization: raw frequency (word probability) and
log-likelihood ratio. We demonstrate that the advan-
tages of log-likelihood ratio come from its known dis-
tributional properties which allow for the identifica-
tion of a set of words that in its entirety defines the
aboutness of the input. We also find that LLR is more
suitable for query-focused summarization since, un-
like raw frequency, it is more sensitive to the integra-
tion of the information need defined by the user.
1 Introduction
Recently the task of multi-document summarization
in response to a complex user query has received
considerable attention. In generic summarization,
the summary is meant to give an overview of the
information in the documents. By contrast, when
the summary is produced in response to a user query
or topic (query-focused, topic-focused, or generally
focused summary), the topic/query determines what
information is appropriate for inclusion in the sum-
mary, making the task potentially more challenging.
In this paper we present an analytical study of two
questions regarding aspects of the topic-focused sce-
nario. First, two estimates of importance on words
have been used very successfully both in generic and
query-focused summarization: frequency (Luhn,
1958; Nenkova et al, 2006; Vanderwende et al,
2006) and loglikelihood ratio (Lin and Hovy, 2000;
Conroy et al, 2006; Lacatusu et al, 2006). While
both schemes have proved to be suitable for sum-
marization, with generally better results from log-
likelihood ratio, no study has investigated in what
respects and by how much they differ. Second, there
are many little-understood aspects of the differences
between generic and query-focused summarization.
For example, we?d like to know if a particular word
weighting scheme is more suitable for focused sum-
marization than others. More significantly, previous
studies show that generic and focused systems per-
form very similarly to each other in query-focused
summarization (Nenkova, 2005) and it is of interest
to find out why.
To address these questions we examine the two
weighting schemes: raw frequency (or word proba-
bility estimated from the input), and log-likelihood
ratio (LLR) and two of its variants. These metrics
are used to assign importance to individual content
words in the input, as we discuss below.
Word probability R(w) = nN , where n is the num-
ber of times the word w appeared in the input and N
is the total number of words in the input.
Log-likelihood ratio (LLR) The likelihood ratio ?
(Manning and Schutze, 1999) uses a background
corpus to estimate the importance of a word and it
is proportional to the mutual information between
a word w and the input to be summarized; ?(w) is
defined as the ratio between the probability (under
a binomial distribution) of observing w in the input
and the background corpus assuming equal proba-
bility of occurrence of w in both and the probability
of the data assuming different probabilities for w in
the input and the background corpus.
LLR with cut-off (LLR(C)) A useful property
of the log-likelihood ratio is that the quantity
193
?2 log(?) is asymptotically well approximated by
?2 distribution. A word appears in the input sig-
nificantly more often than in the background corpus
when ?2 log(?) > 10. Such words are called signa-
ture terms in Lin and Hovy (2000) who were the first
to introduce the log-likelihood weighting scheme for
summarization. Each descriptive word is assigned
an equal weight and the rest of the words have a
weight of zero:
R(w) = 1 if (?2 log(?(w)) > 10), 0 otherwise.
This weighting scheme has been adopted in several
recent generic and topic-focused summarizers (Con-
roy et al, 2006; Lacatusu et al, 2006).
LLR(CQ) The above three weighting schemes as-
sign a weight to words regardless of the user query
and are most appropriate for generic summarization.
When a user query is available, it should inform
the summarizer to make the summary more focused.
In Conroy et al (2006) such query sensititivity is
achieved by augmenting LLR(C) with all content
words from the user query, each assigned a weight
of 1 equal to the weight of words defined by LLR(C)
as topic words from the input to the summarizer.
2 Data
We used the data from the 2005 Document Under-
standing Conference (DUC) for our experiments.
The task is to produce a 250-word summary in re-
sponse to a topic defined by a user for a total of 50
topics with approximately 25 documents for each
marked as relevant by the topic creator. In com-
puting LLR, the remaining 49 topics were used as a
background corpus as is often done by DUC partic-
ipants. A sample topic (d301) shows the complexity
of the queries:
Identify and describe types of organized crime that
crosses borders or involves more than one country. Name
the countries involved. Also identify the perpetrators in-
volved with each type of crime, including both individuals
and organizations if possible.
3 The Experiment
In the summarizers we compare here, the various
weighting methods we describe above are used to
assign importance to individual content words in the
input. The weight or importance of a sentence S in
GENERIC FOCUSED
Frequency 0.11972 0.11795
(0.11168?0.12735) (0.11010?0.12521)
LLR 0.11223 0.11600
(0.10627?0.11873) (0.10915?0.12281)
LLR(C) 0.11949 0.12201
(0.11249?0.12724) (0.11507?0.12950)
LLR(CQ) not app 0.12546
(.11884?.13247)
Table 1: SU4 ROUGE recall (and 95% confidence
intervals) for runs on the entire input (GENERIC) and
on relevant sentences (FOCUSED).
the input is defined as
WeightR(S) =
?
w?S
R(w) (1)
where R(w) assigns a weight for each word w.
For GENERIC summarization, the top scoring sen-
tences in the input are taken to form a generic extrac-
tive summary. In the computation of sentence im-
portance, only nouns, verbs, adjectives and adverbs
are considered and a short list of light verbs are ex-
cluded: ?has, was, have, are, will, were, do, been,
say, said, says?. For FOCUSED summarization, we
modify this algorithm merely by running the sen-
tence selection algorithm on only those sentences
in the input that are relevent to the user query. In
some previous DUC evaluations, relevant sentences
are explicitly marked by annotators and given to sys-
tems. In our version here, a sentence in the input is
considered relevant if it contains at least one word
from the user query.
For evaluation we use ROUGE (Lin, 2004) SU4
recall metric1, which was among the official auto-
matic evaluation metrics for DUC.
4 Results
The results are shown in Table 1. The focused sum-
marizer using LLR(CQ) is the best, and it signif-
icantly outperforms the focused summarizer based
on frequency. Also, LLR (using log-likelihood ra-
tio to assign weights to all words) perfroms signif-
icantly worse than LLR(C). We can observe some
trends even from the results for which there is no
significance. Both LLR and LLR(C) are sensitive to
the introduction of topic relevance, producing some-
what better summaries in the FOCUSED scenario
1
-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d
194
compared to the GENERIC scenario. This is not the
case for the frequency summarizer, where using only
the relevant sentences has a negative impact.
4.1 Focused summarization: do we need query
expansion?
In the FOCUSED condition there was little (for LLR
weighting) or no (for frequency) improvement over
GENERIC. One possible explanation for the lack of
clear improvement in the FOCUSED setting is that
there are not enough relevant sentences, making it
impossible to get stable estimates of word impor-
tance. Alternatively, it could be the case that many
of the sentences are relevant, so estimates from the
relevant portion of the input are about the same as
those from the entire input.
To distinguish between these two hypotheses, we
conducted an oracle experiment. We modified the
FOCUSED condition by expanding the topic words
from the user query with all content words from any
of the human-written summaries for the topic. This
increases the number of relevant sentences for each
topic. No automatic method for query expansion can
be expected to give more accurate results, since the
content of the human summaries is a direct indica-
tion of what information in the input was important
and relevant and, moreover, the ROUGE evaluation
metric is based on direct n-gram comparison with
these human summaries.
Even under these conditions there was no signif-
icant improvement for the summarizers, each get-
ting better by 0.002: the frequency summarizer gets
R-SU4 of 0.12048 and the LLR(CQ) summarizer
achieves R-SU4 of 0.12717.
These results seem to suggest that considering the
content words in the user topic results in enough rel-
evant sentences. Indeed, Table 2 shows the mini-
mum, maximum and average percentage of relevant
sentences in the input (containing at least one con-
tent words from the user the query), both as defined
by the original query and by the oracle query ex-
pansion. It is clear from the table that, on aver-
age, over half of the input comprises sentences that
are relevant to the user topic. Oracle query expan-
sion makes the number of relevant sentences almost
equivalent to the input size and it is thus not sur-
prising that the corresponding results for content se-
lection are nearly identical to the query independent
Original query Oracle query expansion
Min 13% 52%
Average 57% 86%
Max 82% 98%
Table 2: Percentage of relevant sentences (contain-
ing words from the user query) in the input. The
oracle query expansion considers all content words
form human summaries of the input as query words.
runs of generic summaries for the entire input.
These numbers indictate that rather than finding
ways for query expansion, it might instead be more
important to find techniques for constraining the
query, determining which parts of the input are di-
rectly related to the user questions. Such techniques
have been described in the recent multi-strategy ap-
proach of Lacatusu et al (2006) for example, where
one of the strategies breaks down the user topic
into smaller questions that are answered using ro-
bust question-answering techniques.
4.2 Why is log-likelihood ratio better than
frequency?
Frequency and log-likelihood ratio weighting for
content words produce similar results when applied
to rank all words in the input, while the cut-off
for topicality in LLR(C) does have a positive im-
pact on content selection. A closer look at the
two weighting schemes confirms that when cut-off
is not used, similar weighting of content words is
produced. The Spearman correlation coefficient be-
tween the weights for words assigned by the two
schemes is on average 0.64. At the same time, it is
likely that the weights of sentences are dominated
by only the top most highly weighted words. In
order to see to what extent the two schemes iden-
tify the same or different words as the most impor-
tant ones, we computed the overlap between the 250
most highly weighted words according to LLR and
frequency. The average overlap across the 50 sets
was quite large, 70%.
To illustrate the degree of overlap, we list below
are the most highly weighted words according to
each weighting scheme for our sample topic con-
cerning crimes across borders.
LLR drug, cocaine, traffickers, cartel, police, crime, en-
forcement, u.s., smuggling, trafficking, arrested, government,
seized, year, drugs, organised, heroin, criminal, cartels, last,
195
official, country, law, border, kilos, arrest, more, mexican, laun-
dering, officials, money, accounts, charges, authorities, cor-
ruption, anti-drug, international, banks, operations, seizures,
federal, italian, smugglers, dealers, narcotics, criminals, tons,
most, planes, customs
Frequency drug, cocaine, officials, police, more, last, gov-
ernment, year, cartel, traffickers, u.s., other, drugs, enforce-
ment, crime, money, country, arrested, federal, most, now, traf-
ficking, seized, law, years, new, charges, smuggling, being, of-
ficial, organised, international, former, authorities, only, crimi-
nal, border, people, countries, state, world, trade, first, mexican,
many, accounts, according, bank, heroin, cartels
It becomes clear that the advantage of likelihood
ratio as a weighting scheme does not come from
major differences in overall weights it assigns to
words compared to frequency. It is the signifi-
cance cut-off for the likelihood ratio that leads to
noticeable improvement (see Table 1). When this
weighting scheme is augmented by adding a score
of 1 for content words that appear in the user topic,
the summaries improve even further (LLR(CQ)).
Half of the improvement can be attributed to the
cut-off (LLR(C)), and the other half to focusing
the summary using the information from the user
query (LLR(CQ)). The advantage of likelihood ra-
tio comes from its providing a principled criterion
for deciding which words are truly descriptive of the
input and which are not. Raw frequency provides no
such cut-off.
5 Conclusions
In this paper we examined two weighting schemes
for estimating word importance that have been suc-
cessfully used in current systems but have not to-
date been directly compared. Our analysis con-
firmed that log-likelihood ratio leads to better re-
sults, but not because it defines a more accurate as-
signment of importance than raw frequency. Rather,
its power comes from the use of a known distribution
that makes it possible to determine which words are
truly descriptive of the input. Only when such words
are viewed as equally important in defining the topic
does this weighting scheme show improved perfor-
mance. Using the significance cut-off and consider-
ing all words above it equally important is key.
Log-likelihood ratio summarizer is more sensitive
to topicality or relevance and produces summaries
that are better when it take the user request into ac-
count than when it does not. This is not the case for
a summarizer based on frequency.
At the same time it is noteworthy that the generic
summarizers perform about as well as their focused
counterparts. This may be related to our discovery
that on average 57% of the sentences in the doc-
ument are relevant and that ideal query expansion
leads to a situation in which almost all sentences
in the input become relevant. These facts could
be an unplanned side-effect from the way the test
topics were produced: annotators might have been
influenced by information in the input to be sum-
marizied when defining their topic. Such observa-
tions also suggest that a competitive generic summa-
rizer would be an appropriate baseline for the topic-
focused task in future DUCs. In addition, including
some irrelavant documents in the input might make
the task more challenging and allow more room for
advances in query expansion and other summary fo-
cusing techniques.
References
J. Conroy, J. Schlesinger, and D. O?Leary. 2006. Topic-focused
multi-document summarization using an approximate oracle
score. In Proceedings of the COLING/ACL?06 (Poster Ses-
sion).
F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley, B. Rink,
P. Wang, and L. Taylor. 2006. Lcc?s gistexter at duc 2006:
Multi-strategy multi-document summarization. In Proceed-
ings of DUC?06.
C. Lin and E. Hovy. 2000. The automated acquisition of topic
signatures for text summarization. In Proceedings of COL-
ING?00.
C. Lin. 2004. Rouge: a package for automatic evaluation of
summaries. In Proceedings of the Workshop on Text Sum-
marization Branches Out (WAS 2004).
H. P. Luhn. 1958. The automatic creation of literature abstracts.
IBM Journal of Research and Development, 2(2):159?165.
C. Manning and H. Schutze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A compositional context sensitive multi-document summa-
rizer: Exploring the factors that influence summarization. In
Proceedings of ACM SIGIR?06.
A. Nenkova. 2005. Automatic text summarization of newswire:
lessons learned from the document understanding confer-
ence. In Proceedings of AAAI?05.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft
research at duc 2006: Task-focused summarization with sen-
tence simplification and lexical expansion. In Proceedings of
DUC?06.
196
Syntactic Simplication for Improving Content Selection in Multi-Document
Summarization
Advaith Siddharthan, Ani Nenkova and Kathleen McKeown
Columbia University
Computer Science Department
 	
 

@ Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 241?248, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Automatically Learning Cognitive Status for Multi-Document
Summarization of Newswire
Ani Nenkova and Advaith Siddharthan and Kathleen McKeown
Department of Computer Science
Columbia University
 
ani,advaith,kathy  @cs.columbia.edu
Abstract
Machine summaries can be improved by
using knowledge about the cognitive sta-
tus of news article referents. In this paper,
we present an approach to automatically
acquiring distinctions in cognitive status
using machine learning over the forms of
referring expressions appearing in the in-
put. We focus on modeling references to
people, both because news often revolve
around people and because existing natu-
ral language tools for named entity iden-
tification are reliable. We examine two
specific distinctions?whether a person in
the news can be assumed to be known to a
target audience (hearer-old vs hearer-new)
and whether a person is a major charac-
ter in the news story. We report on ma-
chine learning experiments that show that
these distinctions can be learned with high
accuracy, and validate our approach using
human subjects.
1 Introduction
Multi-document summarization has been an active
area of research over the past decade (Mani and
Maybury, 1999) and yet, barring a few exceptions
(Daume? III et al, 2002; Radev and McKeown,
1998), most systems still use shallow features to pro-
duce an extractive summary, an age-old technique
(Luhn, 1958) that has well-known problems. Ex-
tractive summaries contain phrases that the reader
cannot understand out of context (Paice, 1990) and
irrelevant phrases that happen to occur in a relevant
sentence (Knight and Marcu, 2000; Barzilay, 2003).
Referring expressions in extractive summaries illus-
trate this problem, as sentences compiled from dif-
ferent documents might contain too little, too much
or repeated information about the referent.
Whether a referring expression is appropriate de-
pends on the location of the referent in the hearer?s
mental model of the discourse?the referent?s cog-
nitive status (Gundel et al, 1993). If, for example,
the referent is unknown to the reader at the point of
mention in the discourse, the reference should in-
clude a description, while if the referent was known
to the reader, no descriptive details are necessary.
Determining a referent?s cognitive status, how-
ever, implies the need to model the intended audi-
ence of the summary. Can such a cognitive status
model be inferred automatically for a general read-
ership? In this paper, we address this question by
performing a study with human subjects to confirm
that reasonable agreement on the distinctions can be
achieved between different humans (cf.  5). We
present an automatic approach for inferring what the
typical reader is likely to know about people in the
news. Our approach uses machine learning, exploit-
ing features based on the form of references to peo-
ple in the input news articles (cf.  4). Learning
cognitive status of referents is necessary if we want
to ultimately generate new, more appropriate refer-
ences for news summaries.
1.1 Cognitive status
In human communication, the wording used by
speakers to refer to a discourse entity depends on
their communicative goal and their beliefs about
what listeners already know. The speaker?s goals
and beliefs about the listener?s knowledge are both a
part of a cognitive/mental model of the discourse.
241
Cognitive status distinctions depend on two pa-
rameters related to the referent?a) whether it al-
ready exists in the hearer?s model of the discourse,
and b) its degree of salience. The influence of these
distinctions on the form of referring expressions has
been investigated in the past. For example, center-
ing theory (Grosz et al, 1995) deals predominantly
with local salience (local attentional status), and the
givenness hierarchy (information status) of Prince
(1992) focuses on how a referent got in the discourse
model (e.g. through a direct mention in the current
discourse, through previous knowledge, or through
inference), leading to distinctions such as discourse-
old, discourse-new, hearer-old, hearer-new, infer-
able and containing inferable. Gundel et al (1993)
attempt to merge salience and givenness in a single
hierarchy consisting of six distinctions in cognitive
status (in focus, activated, familiar, uniquely identi-
fiable, referential, type-identifiable).
Among the distinctions that have an impact on the
form of references in a summary are the familiarity
of the referent:
D. Discourse-old vs discourse-new
H. Hearer-old vs hearer-new
and its global salience1:
M. Major vs minor
In general, initial (discourse-new) references to en-
tities are longer and more descriptive, while sub-
sequent (discourse-old) references are shorter and
have a purely referential function. Nenkova and
McKeown (2003) have studied this distinction for
references to people in summaries and how it can be
used to automatically rewrite summaries to achieve
better fluency and readability.
The other two cognitive status distinctions,
whether an entity is central to the summary or not
(major or minor) and whether the hearer can be as-
sumed to be already familiar with the entity (hearer-
old vs hearer-new status), have not been previously
studied in the context of summarization. There is
a tradeoff, particularly important for a short sum-
mary, between what the speaker wants to convey
1The notion of global salience is very important to summa-
rization, both during content selection and during generation on
initial references to entities. On the other hand, in focus or local
attentional state are relevant to anaphoric usage during subse-
quent mentions.
and how much the listener needs to know. The
hearer-old/new distinction can be used to determine
whether a description for a character is required
from the listener?s perspective. The major/minor
distinction plays a role in defining the communica-
tive goal, such as what the summary should be about
and which characters are important enough to refer
to by name.
1.2 Hearer-Old vs Hearer-New
Hearer-new entities in a summary should be de-
scribed in necessary detail, while hearer-old enti-
ties do not require an introductory description. This
distinction can have a significant impact on over-
all length and intelligibility of the produced sum-
maries. Usually, summaries are very short, 100 or
200 words, for input articles totaling 5,000 words
or more. Several people might be involved in a
story, which means that if all participants are fully
described, little space will be devoted to actual
news. In addition, introducing already familiar en-
tities might distract the reader from the main story
(Grice, 1975). It is thus a good strategy to refer
to an entity that can be assumed hearer-old by just
a title + last name, e.g. President Bush, or by full
name only, with no accompanying description, e.g.
Michael Jackson.
1.3 Major vs Minor
Another distinction that human summarizers make
is whether a character in a story is a major or a
minor one and this distinction can be conveyed by
using different forms of referring expressions. It is
common to see in human summaries references such
as the dissident?s father. Usually, discourse-initial
references solely by common noun, without the in-
clusion of the person?s name, are employed when
the person is not the main focus of a story (San-
ford et al, 1988). By detecting the cognitive sta-
tus of a character, we can decide whether to name
the character in the summary. Furthermore, many
summarization systems use the presence of named
entities as a feature for computing the importance
of a sentence (Saggion and Gaizaukas, 2004; Guo
et al, 2003). The ability to identify the major story
characters and use only them for sentence weighting
can benefit such systems since only 5% of all peo-
ple mentioned in the input are also mentioned in the
summaries.
242
2 Why care about people in the news?
News reports (and consequently, news summaries)
tend to have frequent references to people (in DUC
data - see  3 for description - from 2003 and 2004,
there were on average 3.85 references to people per
100-word human summary); hence it is important
for news summarization systems to have a way of
modeling the cognitive status of such referents and
a theory for referring to people.
It is also important to note that there are differ-
ences in references to people between news reports
and human summaries of news. Journalistic con-
ventions for many mainstream newspapers dictate
that initial mentions to people include a minimum
description such as their role or title and affilia-
tion. However, in human summaries, where there
are greater space constraints, the nature of initial ref-
erences changes. Siddharthan et al (2004) observed
that in DUC?04 and DUC?03 data2, news reports
contain on average one appositive phrase or relative
clause every 3.9 sentences, while the human sum-
maries contain only one per 8.9 sentences on aver-
age. In addition to this, we observe from the same
data that the average length of a first reference to a
named entity is 4.5 words in the news reports and
only 3.6 words in human summaries. These statis-
tics imply that human summarizers do compress ref-
erences, and thus can save space in the summary for
presenting information about the events. Cognitive
status models can inform a system when such refer-
ence compression is appropriate.
3 Data preparation: the DUC corpus
The data we used to train classifiers for these two
distinctions is the Document Understanding Confer-
ence collection (2001?2004) of 170 pairs of doc-
ument input sets and the corresponding human-
written multi-document summaries (2 or 4 per set).
Our aim is to identify every person mentioned in
the 10 news reports and the associated human sum-
maries for each set, and assign labels for their cog-
nitive status (hearer old/new and major/minor). To
do this, we first preprocess the data (  3.1) and then
perform the labeling (  3.2).
2The data provided under DUC for these years includes sets
of about 10 news reports, 4 human summaries for each set, and
the summaries by participating machine summarizers.
3.1 Automatic preprocessing
All documents and summaries were tagged with
BBN?s IDENTIFINDER (Bikel et al, 1999) for
named entities, and with a part-of-speech tagger and
simplex noun-phrase chunker (Grover et al, 2000).
In addition, for each named entity, relative clauses,
appositional phrases and copula constructs, as well
as pronominal co-reference were also automatically
annotated (Siddharthan, 2003). We thus obtained
coreference information (cf. Figure 1) for each per-
son in each set, across documents and summaries.
Andrei Sakharov
Doc 1:
[IR] laureate Andrei D. Sakharov [CO] Sakharov
[CO] Sakharov [CO] Sakharov [CO] Sakharov [PR]
his [CO] Sakharov [PR] his [CO] Sakharov [RC] who
acted as an unofficial Kremlin envoy to the troubled
Transcaucasian region last month [PR] he [PR] He
[CO] Sakharov
Doc 1:
[IR] Andrei Sakharov [AP] , 68 , a Nobel Peace Prize
winner and a human rights activist , [CO] Sakharov
[IS] a physicist [PR] his [CO] Sakharov
Figure 1: Example information collected for Andrei
Sakharov from two news report. ?IR? stands for ?ini-
tial reference?, ?CO? for noun co-reference, ?PR? for
pronoun reference, ?AP? for apposition, ?RC? for rel-
ative clause and ?IS? for copula constructs.
The tools that we used were originally devel-
oped for processing single documents and we had
to adapt them for use in a multi-document setting.
The goal was to find, for each person mentioned
in an input set, the list of all references to the per-
son in both input documents and human summaries.
For this purpose, all input documents were concate-
nated and processed with IDENTIFINDER. This was
then automatically post-processed to mark-up core-
ferring names and to assign a unique canonical name
(unique id) for each name coreference chain. For the
coreference, a simple rule of matching the last name
was used, and the canonical name was the ?First-
Name LastName? string where the two parts of the
name could be identified 3. Concatenating all docu-
ments assures that the same canonical name will be
assigned to all named references to the same person.
3Occasionally, two or more different people with the same
last name are discussed in the same set and this algorithm would
lead to errors in such cases. We did keep a list of first names
associated with the entity, so a more refined matching model
could be developed, but this was not the focus of this work.
243
The tools for pronoun coreference and clause and
apposition identification and attachment were run
separately on each document. Then the last name of
each of the canonical names derived from the IDEN-
TIFINDER output was matched with the initial ref-
erence in the generic coreference list for the doc-
ument with the last name. The tools that we used
have been evaluated separately when used in nor-
mal single document setting. In our cross-document
matching processes, we could incur more errors, for
example when the general coreference chain is not
accurate. On average, out of 27 unique people per
cluster identified by IDENTIFINDER, 4 people and
the information about them are lost in the matching
step for a variety of reasons such as errors in the
clause identifier, or the coreference.
3.2 Data labeling
Entities were automatically labeled as hearer-old or
new by analyzing the syntactic form that human
summarizers used for initial references to them. The
labeling rests on the assumption that the people who
produced the summaries used their own model of the
reader when choosing appropriate references for the
summary. The following instructions had been given
to the human summarizers, who were not profes-
sional journalists: ?To write this summary, assume
you have been given a set of stories on a news topic
and that your job is to summarize them for the gen-
eral news sections of the Washington Post. Your au-
dience is the educated adult American reader with
varied interests and background in current and re-
cent events.? Thus, the human summarizers were
given the freedom to use their assumptions about
what entities would be generally hearer-old and they
could refer to these entities using short forms such as
(1) title or role+ last name or (2) full name only with
no pre- or post-modification. Entities that the major-
ity of human summarizers for the set referred to us-
ing form (1) or (2) were labeled as hearer-old. From
the people mentioned in human summaries, we ob-
tained 118 examples of hearer-old and 140 examples
of hearer-new persons - 258 examples in total - for
supervised machine learning.
In order to label an entity as major or minor, we
again used the human summaries?entities that were
mentioned by name in at least one summary were la-
beled major, while those not mentioned by name in
any summary were labeled minor. The underlying
assumption is that people who are not mentioned in
any human summary, or are mentioned without be-
ing named, are not important. There were 258 major
characters who made it to a human summary and
3926 minor ones that only appeared in the news re-
ports. Such distribution between the two classes is
intuitively plausible, since many people in news ar-
ticles express opinions, make statements or are in
some other way indirectly related to the story, while
there are only a few main characters.
4 Machine learning experiments
The distinction between hearer-old and hearer-new
entities depends on the readers. In other words, we
are attempting to automatically infer which charac-
ters would be hearer-old for the intended readership
of the original reports, which is also expected to be
the intended readership of the summaries. For our
experiments, we used the WEKA (Witten and Frank,
2005) machine learning toolkit and obtained the best
results for hearer-old/new using a support vector ma-
chine (SMO algorithm) and for major/minor, a tree-
based classifier (J48). We used WEKA?s default set-
tings for both algorithms.
We now discuss what features we used for our
two classification tasks (cf. list of features in table
1). Our hypothesis is that features capturing the fre-
quency and syntactic and lexical forms of references
are sufficient to infer the desired cognitive model.
Intuitively, pronominalization indicates that an
entity was particularly salient at a specific point of
the discourse, as has been widely discussed in at-
tentional status and centering literature (Grosz and
Sidner, 1986; Gordon et al, 1993). Modified noun
phrases (with apposition, relative clauses or premod-
ification) can also signal different status.
In addition to the syntactic form features, we used
two months worth of news articles collected over the
web (and independent of the DUC collection we use
in our experiments here) to collect unigram and bi-
gram lexical models of first mentions of people. The
names themselves were removed from the first men-
tion noun phrase and the counts were collected over
the premodifiers only. One of the lexical features
we used is whether a person?s description contains
any of the 20 most frequent description words from
our web corpus. We reasoned that these frequent de-
244
0,1: Number of references to the person, including pro-
nouns (total and normalized by feature 16)
2,3: Number of times apposition was used to describe
the person(total and normalized by feature 16)
4,5: Number of times a relative clause was used to de-
scribe the person (total and normalized by 16)
6: Number of times the entity was referred to by
name after the first reference
7,8: Number of copula constructions involving the per-
son (total and normalized by feature 16)
9,10: Number of apposition, relative clause or copula
descriptions (total and normalized by feature 16)
11,12,13: Probability of an initial reference according to the
bigram model (av.,max and min of all initial refer-
ences)
14: Number of top 20 high frequency description
words (from references to people in large news
corpus) present in initial references
15: Proportion of first references containing full name 16: Total number of documents containing the person
17,18: Number of appositives or relative clause attaching
to initial references (total and normalized by fea-
ture 16)
Table 1: List of Features provided to WEKA.
scriptors may signal importance; the full list is:
president, former, spokesman, sen, dr, chief, coach,
attorney, minister, director, gov, rep, leader, secre-
tary, rev, judge, US, general, manager, chairman.
Another lexical feature was the overall likelihood
of a person?s description using the bigram model
from our web corpus. This indicates whether a per-
son has a role or affiliation that is frequently men-
tioned. We performed 20-fold cross validation for
both classification tasks. The results are shown in
Table 2 (accuracy) and Table 3 (precision/recall).
4.1 Major vs Minor results
For major/minor classification, the majority class
prediction has 94% accuracy, but is not a useful
baseline as it predicts that no person should be men-
tioned by name and all are minor characters. J48
correctly predicts 114 major characters out of 258
in the 170 document sets. As recall appeared low,
we further analyzed the 148 persons from DUC?03
and DUC?04 sets, for which DUC provides four hu-
man summaries. Table 4 presents the distribution of
recall taking into account how many humans men-
tioned the person by name in their summary (origi-
nally, entities were labeled as main if any summary
had a reference to them, cf.  3.2). It can be seen that
recall is high (0.84) when all four humans consider
a character to be major, and falls to 0.2 when only
one out of four humans does. These observations re-
flect the well-known fact that humans differ in their
choices for content selection, and indicate that in the
automatic learning is more successful when there is
more human agreement.
In our data there were 258 people mentioned by
name in at least one human summary. In addition,
there were 103 people who were mentioned in at
least one human summary using only a common
noun reference (these were identified by hand, as
common noun coreference cannot be performed re-
liably enough by automatic means), indicating that
29% of people mentioned in human summaries are
not actually named. Examples of such references
include an off duty black policeman, a Nigerian
born Roman catholic priest, Kuwait?s US ambas-
sador. For the purpose of generating references in
a summary, it is important to evaluate how many of
these people are correctly classified as minor char-
acters. We removed these people from the training
data and kept them as a test set. WEKA achieved
a testing accuracy of 74% on these 103 test exam-
ples. But as discussed before, different human sum-
marizers sometimes made different decisions on the
form of reference to use. Out of the 103 referent
for which a non-named reference was used by a
summarizer, there were 40 where other summariz-
ers used named reference. Only 22 of these 40 were
labeled as minor characters in our automatic proce-
dure. Out of the 63 people who were not named in
any summary, but mentioned in at least one by com-
mon noun reference, WEKA correctly predicted 58
(92%) as minor characters. As before, we observe
that when human summarizers generate references
of the same form (reflecting consensus on convey-
ing the perceived importance of the character), the
machine predictions are accurate.
We performed feature selection to identify which
are the most important features for the classification
task. For the major/minor classification, the impor-
tant features used by the classifier were the number
of documents the person was mentioned in (feature
16), number of mentions within the document set
(features 1,6), number of relative clauses (feature
245
4,5) and copula (feature 8) constructs, total number
of apposition, relative clauses and copula (feature
9), number of high frequency premodifiers (feature
14) and the maximum bigram probability (feature
12). It was interesting that presence of apposition
did not select for either major or minor class. It is
not surprising that the frequency of mention within
and across documents were significant features?a
frequently mentioned entity will naturally be consid-
ered important for the news report. Interestingly, the
syntactic form of the references was also a signifi-
cant indicator, suggesting that the centrality of the
character was signaled by the journalists by using
specific syntactic constructs in the references.
Major/Minor Hearer New/Old
WEKA 0.96 (J48) 0.76 (SMO)
Majority class prediction 0.94 0.54
Table 2: Cross validation testing accuracy results.
Class Precision Recall F-measure
SMO hearer-new 0.84 0.68 0.75
hearer-old 0.69 0.85 0.76
J48 major-character 0.85 0.44 0.58
minor-character 0.96 0.99 0.98
Table 3: Cross validation testing P/R/F results.
Number of summaries Number of Number and %
containing the person examples recalled by J48
1 out of 4 59 15 (20%)
2 out of 4 35 20 (57%)
3 out of 4 29 23 (79%)
4 out of 4 25 21 (84%)
Table 4: J48 Recall results and human agreement.
4.2 Hearer Old vs New Results
The majority class prediction for the hearer-old/new
classification task is that no one is known to the
reader and it leads to overall classification accu-
racy of 54%. Using this prediction in a summarizer
would result in excessive detail in referring expres-
sions and a consequent reduction in space available
to summarize the news events. The SMO prediction
outperformed the baseline accuracy by 22% and is
more meaningful for real tasks.
For the hearer-old/new classification, the feature
selection step chose the following features: the num-
ber of appositions (features 2,3) and relative clauses
(feature 5), number of mentions within the docu-
ment set (features 0,1), total number of apposition,
relative clauses and copula (feature 10), number of
high frequency premodifiers (feature 14) and the
minimum bigram probability (feature 13). As in the
minor-major classification, the syntactic choices for
reference realization were useful features.
We conducted an additional experiment to see
how the hearer old/new status impacts the use of ap-
position or relative clauses for elaboration in refer-
ences produced in human summaries. It has been
observed (Siddharthan et al, 2004) that on average
these constructs occur 2.3 times less frequently in
human summaries than in machine summaries. As
we show, the use of postmodification to elaborate re-
lates to the hearer-old/new distinction.
To determine when an appositive or relative
clause can be used to modify a reference, we con-
sidered the 151 examples out of 258 where there was
at least one relative clause or apposition describing
the person in the input. We labeled an example as
positive if at least one human summary contained
an apposition or relative clause for that person and
negative otherwise. There were 66 positive and 85
negative examples. This data was interesting be-
cause while for the majority of examples (56%) all
the human summarizers agreed not to use postmod-
ification, there were very few examples (under 5%)
where all the humans agreed to postmodify. Thus it
appears that for around half the cases, it should be
obvious that no postmodification is required, but for
the other half, human decisions go either way.
Notably, none of the hearer-old persons (using test
predictions of SMO) were postmodified. Our cogni-
tive status predictions cleanly partition the examples
into those where postmodification is not required,
and those where it might be. Since no intuitive rule
handled the remaining examples, we added the test-
ing predictions of hearer-old/new and major/minor
as features to the list in Table 1, and tried to learn
this task using the tree-based learner J48. We report
a testing accuracy of 71.5% (majority class baseline
is 56%). There were only three useful features?
the predicted hearer-new/old status, the number of
high frequency premodifiers for that person in the
input (feature 14 in table 1) and the average number
of postmodified initial references in the input docu-
ments (feature 17).
5 Validating the results on current news
We tested the classifiers on data different from that
provided by DUC, and also tested human consen-
246
sus on the hearer-new/old distinction. For these pur-
poses, we downloaded 45 clusters from one day?s
output from Newsblaster4. We then automatically
compiled the list of people mentioned in the ma-
chine summaries for these clusters. There were 107
unique people that appeared in the machine sum-
maries, out of 1075 people in the input clusters.
5.1 Human agreement on hearer-old/new
A question arises when attempting to infer hearer-
new/old status: Is it meaningful to generalize this
across readers, seeing how dependent it is on the
world knowledge of individual readers?
To address this question, we gave 4 Ameri-
can graduate students a list of the names of peo-
ple in the DUC human summaries (cf.  3), and
asked them to write down for each person, their
country/state/organization affiliation and their role
(writer/president/attorney-general etc.). We consid-
ered a person hearer-old to a subject if they correctly
identified both role and affiliation for that person.
For the 258 people in the DUC summaries, the four
subjects demonstrated 87% agreement ( 	
 5.
Similarly, they were asked to perform the same
task for the Newsblaster data, which dealt with con-
temporary news6, in contrast with the DUC data
that contained news from the the late 80s and early
90s. On this data, the human agreement was 91%
( 	
 ). This is a high enough agreement to
suggest that the classification of national and inter-
national figures as hearer old/new across the edu-
cated adult American reader with varied interests
and background in current and recent events is a
well defined task. This is not necessarily true for
the full range of cognitive status distinctions; for
example Poesio and Vieira (1998) report lower hu-
man agreement on more fine-grained classifications
of definite descriptions.
5.2 Results on the Newsblaster data
We measured how well the models trained on DUC
data perform with current news labeled using human
4http://newsblaster.cs.columbia.edu
5  (kappa) is a measure of inter-annotator agreement over
and above what might be expected by pure chance (See Carletta
(1996) for discussion of its use in NLP).  if there is perfect
agreement between annotators and ffReferences to Named Entities: a Corpus Study
Ani Nenkova and Kathleen McKeown
Columbia University
Computer Science Department
New York, NY 10027
 
ani,kathy  @cs.columbia.edu
Abstract
References included in multi-document sum-
maries are often problematic. In this paper, we
present a corpus study performed to derive a
statistical model for the syntactic realization of
referential expressions. The interpretation of
the probabilistic data helps us gain insight on
how extractive summaries can be rewritten in
an efficient manner to produce more fluent and
easy-to-read text.
1 Introduction
Automatically generated summaries, and particularly
multi-document summaries, suffer from lack of coher-
ence One explanation is that the most widespread sum-
marization strategy is still sentence extraction, where sen-
tences are extracted word for word from the original doc-
uments and are strung together to form a summary. Syn-
tactic form and its influence on summary coherence have
not been taken into account in the implementation of a
full-fledged summarizer, except in the preliminary work
of (Schiffman et al, 2002).
Here we conduct a corpus study focusing on identify-
ing the syntactic properties of first and subsequent men-
tions of people in newswire text (e.g., ?Chief Petty Of-
ficer Luis Diaz of the U.S. Coast Guard in Miami? fol-
lowed by ?Diaz?). The resulting statistical model of the
flow of referential expressions suggest a set of rewrite
rules that can transform the summary back to a more co-
herent and readable text.
In the following sections, we first describe the corpus
that we used and then the statistical model that we de-
veloped. It is based on Markov chains and captures how
subsequent mentions are conditioned by earlier mentions.
We close with discussion of our evaluation, which mea-
sures how well the highest probability path in the model
can be used to regenerate the sequence of references.
2 The Corpus
We used a corpus of news stories, containing 651,000
words drawn from six different newswire agencies, in or-
der to study the syntactic form of noun phrases in which
references to people have been realized. We were inter-
ested in the occurrence of features such as type and num-
ber of premodifiers, presence and type of postmodifiers,
and form of name reference for people.
We constructed a large, automatically annotated cor-
pus by merging the output of Charniak?s statistical
parser (Charniak, 2000) with that of the IBM named
entity recognition system Nominator (Wacholder et al,
1997). The corpus contains 6240 references. In this sec-
tion, we describe the features that were annotated.
Given our focus on references to mentions of peo-
ple, there are two distinct types of premodifiers, ?titles?
and ?name-external modifiers?. The titles are capital-
ized noun premodifiers that conventionally are recog-
nized as part of the name, such as ?president? in ?Presi-
dent George W. Bush?. Name-external premodifiers are
modifiers that do not constitute part of the name, such as
?Irish flutist? in ?Irish flutist James Galway?.
The three major categories of postmodification that we
distinguish are apposition, prepositional phrase modifica-
tion and relative clause. All other postmodifications, such
as remarks in parenthesis and verb-initial modifications
are lumped in a category ?others?.
There are three categories of names corresponding
to the general European and American name structure.
They include full name (first+(middle initial)+last), last
name only, and nickname (first or nickname).
In sum, the target NP features that we examined were:
 Is the target named entity the head of the phrase or
not? Is it in a possessive construction or not?
 If it is the head, what kind of pre- and post- modifi-
cation does it have?
 How was the name itself realized in the NP?
In order to identify the appropriate sequences of syn-
tactic forms in coreferring noun phrases, we analyze
the coreference chains for each entity mentioned in the
text. A coreference chain consists of all the mentions
of an entity within a document. In a manually built
corpus, a coreference chain can include pronouns and
common nouns that refer to the person. However, these
forms could not be automatically identified, so corefer-
ence chains in our corpus only include noun phrases that
contain at least one word from the name. There were
3548 coreference chains in the corpus.
3 A Markov Chain Model
The initial examination of the data showed that syntactic
forms in coreference chains can be effectively modeled
by Markov chains.
Let   be random variables taking values in I. We say
that   	
 is a Markov chain with initial distribution

and transition matrix  if

 
 has distribution

 for  , conditional on   ,  ffColumbia?s Newsblaster: New Features and Future Directions
Kathleen McKeown, Regina Barzilay, John Chen, David Elson, David Evans,
Judith Klavans, Ani Nenkova, Barry Schiffman and Sergey Sigelman
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, New York, N.Y. 10027
kathy@cs.columbia.edu
Abstract
Columbia?s Newsblaster tracking and summa-
rization system is a robust system that clus-
ters news into events, categorizes events into
broad topics and summarizes multiple articles
on each event. Here we outline our most cur-
rent work on tracking events over days, produc-
ing summaries that update a user on new infor-
mation about an event, outlining the perspec-
tives of news coming from different countries
and clustering and summarizing non-English
sources.
1 Introduction
Columbia?s Newsblaster1 provide news updates on a
daily basis from news published on the Internet; it crawls
news sites, categorizes stories into six broad areas, groups
news into stories on the same event, and generates a sum-
mary of the multiple articles describing each event. In ad-
dition to demonstrating the robustness of current summa-
rization and tracking technology, Newsblaster also serves
as a research environment in which we explore new di-
rections and problems. Currently, we are exploring the
tasks of multilingual summarization where input sources
are drawn frommultiple languages and a summary is gen-
erated in English on the same event (Figure 1), tracking
events across days and generating summaries that update
the user on what is new, and editing generated summaries
to improve fluency and accuracy. Our focus here is on
editing references to people, improving coherency of the
summary and ensuring that references are accurate. Edit-
ing is particularly important as we add multilingual capa-
bilities, given the errors inherent in machine translation.
1http://newsblaster.cs.columbia.edu
2 Multilingual Tracking and
Summarization
The multilingual version of Columbia Newsblaster is
built upon the English version of Columbia Newsblaster,
sharing the same structure and components. To add mul-
tilingual capability, the system first crawls web sites in
foreign languages, and stores both the language and en-
coding for the files. To extract the article text from the
HTML pages, we use a new article extraction component
using language-independent statistical features computed
over text blocks along with a machine learning compo-
nent to classify text blocks as one of ?Article Text?, ?Ti-
tle?, ?Image?, ?Image Caption?, or ?Other?. The article
extraction component has been trained and tested on En-
glish, Japanese, and Russian data, but is also being suc-
cessfully applied to French, Spanish, German, and Ital-
ian data. We plan to train the article extractor on other
languages (Chinese, Arabic, Korean, Spanish, German,
French, etc.) in the near future.
To cluster multilingual documents with English doc-
uments, we use the existing Newsblaster English doc-
ument clustering module. Non-English documents are
translated for clustering after the article extraction phase.
We use simple and fast document translation techniques
for clustering if available, since we potentially process
thousands of documents for a language for each run. We
have developed simple dictionary lookup techniques for
translation for clustering for Japanese and Russian; for
other languages we use an interface to the Systran trans-
lation system via Babelfish. We plan on adding Arabic
translation to the system in the near future.
Summarization is performed using the same summa-
rization strategies in Newsblaster. We are experimenting
with different methods for improving summary quality
when translation of text is noisy. For example, when an
input cluster contains both English and foreign sources,
we weight the English higher in cases where we deter-
mine it is representative of both the English and foreign
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 15-16
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Multilingual Version
input documents. We are also experimenting with meth-
ods for determining similarity across documents using
different levels of translation.
3 Different Perspectives
When news media report on international issues, they re-
flect the perspectives of their own countries. In the past,
Newsblaster has included all international sources as in-
put to its summaries. Recently, we have added a feature
of ?international perspectives? to the system. In addition
to the universal summary for a particular event, which
includes all international sources, Newsblaster now gen-
erates separate summaries for each country, which may
illustrate unique biases or disagree on facts. The News-
blaster interface allows users to view any pair of sum-
maries side by side to compare different perspectives.
4 Summary Rewrite
Newsblaster also currently includes a module for rewrit-
ing summaries to achieve better readability. References
to people are rewritten so that the first mention includes
the person?s full name and a selected description and later
mentions are restricted to last name only. In addition to
improving readability, the rewritten version of the sum-
mary is usually shorter than the version before rewrite,
since multiple verbose descriptions of the same entity are
discarded. These changes can be seen when comparing
the summary sentence with the original document via a
link from the summary using a proxy.
5 Event Tracking and Updates
Newsblaster currently identifies events within a single
day; a new set of clusters is generated each day. We have
designed a new module for tracking events across days,
allowing the system to relate stories published on one day
to closely related stories on other days. In this way, the
user can more easily track events of interest as they un-
fold. The typical approach for tracking events across days
represents each event as one monolithic set of stories. We
have focused instead on a model where events on one day
can divide into related sub-events on the next day. For ex-
ample, a set of stories about the start of the Iraq war is an
event that can branch into multiple sets of stories, each set
representing a different facet of the war. We are currently
determining an appropriate evaluation of this approach as
well as investigating different possible interfaces.
If a user is tracking events across days, it is more useful
to have a summary that provides updates on what is new
as opposed to a summary of similarities across all days.
We have built a prototype update summarizer that scans
new articles extracted by the system and compares these
new articles with a background cluster on the same event.
The summarizer will provide the user with a summary of
only important new developments. As the tracking mod-
ule locates new articles, it will pass these to the update
summarizer, which will determine what, if anything, has
changed. This summarizer uses more syntactic and se-
mantic information about the articles to determine nov-
elty than is used in our other summarization strategies and
thus, efficiency is a challenge. We will demo these com-
ponents in a separately fromNewsblaster as they have not
yet been integrated in the development version.
Evaluating Content Selection in Summarization: The Pyramid Method
Ani Nenkova and Rebecca Passonneau
Columbia University
Computer Science Department
New York, NY 10027
fani,beckyg@cs.columbia.edu
Abstract
We present an empirically grounded method
for evaluating content selection in summariza-
tion. It incorporates the idea that no single best
model summary for a collection of documents
exists. Our method quantifies the relative im-
portance of facts to be conveyed. We argue that
it is reliable, predictive and diagnostic, thus im-
proves considerably over the shortcomings of
the human evaluation method currently used in
the Document Understanding Conference.
1 Introduction
Evaluating content selection in summarization has proven
to be a difficult problem. Our approach acknowledges
the fact that no single best model summary exists, and
takes this as a foundation rather than an obstacle. In ma-
chine translation, the rankings from the automatic BLEU
method (Papineni et al, 2002) have been shown to corre-
late well with human evaluation, and it has been widely
used since and has even been adapted for summarization
(Lin and Hovy, 2003). To show that an automatic method
is a reasonable approximation of human judgments, one
needs to demonstrate that these can be reliably elicited.
However, in contrast to translation, where the evaluation
criterion can be defined fairly precisely it is difficult to
elicit stable human judgments for summarization (Rath
et al, 1961) (Lin and Hovy, 2002).
Our approach tailors the evaluation to observed dis-
tributions of content over a pool of human summaries,
rather than to human judgments of summaries. Our
method involves semantic matching of content units to
which differential weights are assigned based on their fre-
quency in a corpus of summaries. This can lead to more
stable, more informative scores, and hence to a meaning-
ful content evaluation. We create a weighted inventory of
Summary Content Units?a pyramid?that is reliable, pre-
dictive and diagnostic, and which constitutes a resource
for investigating alternate realizations of the same mean-
ing. No other evaluation method predicts sets of equally
informative summaries, identifies semantic differences
between more and less highly ranked summaries, or con-
stitutes a tool that can be applied directly to further anal-
ysis of content selection.
In Section 2, we describe the DUC method. In Sec-
tion 3 we present an overview of our method, contrast
our scores with other methods, and describe the distribu-
tion of scores as pyramids grow in size. We compare our
approach with previous work in Section 4. In Section 5,
we present our conclusions and point to our next step, the
feasibility of automating our method. A more detailed
account of the work described here, but not including the
study of distributional properties of pyramid scores, can
be found in (Passonneau and Nenkova, 2003).
2 Current Approach: the Document
Understanding Conference
2.1 DUC
Within DUC, different types of summarization have been
studied: the generation of abstracts and extracts of differ-
ent lengths, single- and multi-document summaries, and
summaries focused by topic or opinion. Evaluation in-
volves comparison of a peer summary (baseline, or pro-
duced by human or system) by comparing its content to
a gold standard, or model. In 2003 they provided four
human summaries for each of the 30 multi-document test
sets, any one of which could serve as the model, with no
criteria for choosing among possible models.
The four human summaries for each of the 2003 docu-
ment sets made our study possible. As described in Sec-
tion 3, we used three of these sets, and collected six addi-
tional summaries per set, in order to study the distribution
of content units across increasingly many summaries.
2.2 DUC evaluation procedure
The procedure used for evaluating summaries in DUC is
the following:
1. A human subject reads the entire input set and cre-
ates a 100 word summary for it, called a model.
2. The model summary is split into content units,
roughly equal to clauses or elementary discourse
units (EDUs). This step is performed automatically
using a tool for EDU annotation developed at ISI.1
3. The summary to be evaluated (a peer) is automat-
ically split into sentences. (Thus the content units
are of different granularity?EDUs for the model,
and sentences for the peer).
1http://www.isi.edu/licensed-sw/spade/.
4. Then a human judge evaluates the peer against the
model using the following instructions: For each
model content unit:
(a) Find all peer units that express at least some
facts from the model unit and mark them.
(b) After all such peer units are marked, think about
the whole set of marked peer units and answer
the question:
(c) ?The marked peer units, taken together, express
about k% of the meaning expressed by the cur-
rent model unit?, where k can be equal to 0, 20,
40, 60, 80 and 100.
The final score is based on the content unit coverage.
In the official DUC results tables, the score for the entire
summary is the average of the scores of all the content
model units, thus a number between 0 and 1. Some par-
ticipants use slightly modified versions of the coverage
metric, where the proportion of marked peer units to the
number of model units is factored in.
The selection of units with the same content is facili-
tated by the use of the Summary Evaluation Environment
(SEE)2 developed at ISI, which displays the model and
peer summary side by side and allows the user to make
selections by using a mouse.
2.3 Problems with the DUC evaluation
There are numerous problems with the DUC human eval-
uation method. The use of a single model summary is
one of the surprises ? all research in summarization eval-
uation has indicated that no single good model exists.
Also, since not much agreement is expected between two
summaries, many model units will have no counterpart
in the peer and thus the expected scores will necessarily
be rather low. Additionally, the task of determining the
percentage overlap between two text units turns out to be
difficult to annotate reliably ? (Lin and Hovy, 2002) re-
port that humans agreed with their own prior judgment in
only 82% of the cases.
These methodological anomalies lead to unreliable
scores. Human-written summaries can score as low as
0.1 while machine summaries can score as high as 0.5.
For each of the 30 test sets, three of the four human-
written summaries and the machine summaries were
scored against the fourth human model summary: each
human was scored on ten summaries. Figure 1 shows
a scatterplot of human scores for all 30 sets, and illus-
trates an apparently random relation of summarizers to
each other, and to document sets. This suggests that the
DUC scores cannot be used to distinguish a good human
summarizer from a bad one. In addition, the DUC method
is not powerful enough to distinguish between systems.
2http://www.isi.edu/?cyl/SEE.
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
data$docset
da
ta
$d
uc
sc
ore
0 5 10 15 20 25 30
0.
2
0.
4
0.
6
0.
8
Figure 1: Scatterplot for DUC 2003 Human Summaries
3 The Pyramid Approach
Our analysis of summary content is based on Summa-
rization Content Units, or SCUs and we will now pro-
ceed to define the concept. SCUs emerge from annota-
tion of a corpus of summaries and are not bigger than a
clause. Rather than attempting to provide a semantic or
functional characterisation of what an SCU is, our anno-
tation procedure defines how to compare summaries to
locate the same or different SCUs.
The following example of the emergence of two SCUs
is taken from a DUC 2003 test set. The sentences are
indexed by a letter and number combination, the letter
showing which summary the sentence came from and the
number indicating the position of the sentence within its
respective summary.
A1 In 1998 two Libyans indicted in 1991 for the Locker-
bie bombing were still in Libya.
B1 Two Libyans were indicted in 1991 for blowing up a
Pan Am jumbo jet over Lockerbie, Scotland in 1988.
C1 Two Libyans, accused by the United States and
Britain of bombing a New York bound Pan Am jet over
Lockerbie, Scotland in 1988, killing 270 people, for 10
years were harbored by Libya who claimed the suspects
could not get a fair trail in America or Britain.
D2 Two Libyan suspects were indicted in 1991.
The annotation starts with identifying similar sen-
tences, like the four above, and then proceeds with
finer grained inspection that can lead to identifying
more tightly related subparts. We obtain two SCUs
from the underlined portions of the sentences above.
Each SCU has a weight corresponding to the number of
summaries it appears in; SCU1 has weight=4 and SCU2
has weight=33:
3The grammatical constituents contributing to an SCU are
bracketed and coindexed with the SCU ID.
SCU1 (w=4): two Libyans were officially accused of the
Lockerbie bombing
A1 [two Libyans]1 [indicted]1
B1 [Two Libyans were indicted]1
C1 [Two Libyans,]1 [accused]1
D2 [Two Libyan suspects were indicted]1
SCU2 (w=3): the indictment of the two Lockerbie
suspects was in 1991
A1 [in 1991]2
B1 [in 1991]2
D2 [in 1991.]2
The remaining parts of the four sentences above end up
as contributors to nine different SCUs of different weight
and granularity. Though we look at multidocument sum-
maries rather than single document ones, SCU annotation
otherwise resembles the annotation of factoids in (Hal-
teren and Teufel, 2003); as they do with factoids, we find
increasing numbers of SCUs as the pool of summaries
grows. For our 100 word summaries, we find about 34-
40 distinct SCUs across four summaries; with ten sum-
maries this number grows to about 60. A more complete
comparison of the two approaches follows in section 4.
An SCU consists of a set of contributors that, in their
sentential contexts, express the same semantic content.
An SCU has a unique index, a weight, and a natural
language label. The label, which is subject to revision
throughout the annotation process, has three functions.
First, it frees the annotation process from dependence on
a semantic representation language. Second, it requires
the annotator to be conscious of a specific meaning shared
by all contributors. Third, because the contributors to an
SCU are taken out of context, the label serves as a re-
minder of the full in-context meaning, as in the case of
SCU2 above where the temporal PPs are about a specific
event, the time of the indictment.
Our impression from consideration of three SCU in-
ventories is that the pattern illustrated here between
SCU1 and SCU2 is typical; when two SCUs are seman-
tically related, the one with the lower weight is semanti-
cally dependent on the other. We have catalogued a vari-
ety of such relationships, and note here that we believe it
could prove useful to address semantic interdependencies
among SCUS in future work that would involve adding a
new annotation layer.4 However, in our approach, SCUs
are treated as independent annotation values, which has
the advantage of affording a rigorous analysis of inter-
annotator reliability (see following section). We do not
attempt to represent the subsumption or implicational re-
4We are currently investigating the possibility of incorporat-
ing narrative relations into SCU pyramids in collaboration with
cognitive psychologists.
W=4
W=1
W=2
W=3
W=4
W=1
W=2
W=3
Figure 2: Two of six optimal summaries with 4 SCUs
lations that Halteren and Teufel assign to factoids (Hal-
teren and Teufel, 2003).
After the annotation procedure is completed, the final
SCUs can be partitioned in a pyramid. The partition is
based on the weight of the SCU; each tier contains all
and only the SCUs with the same weight. When we use
annotations from four summaries, the pyramid will con-
tain four tiers. SCUs of weight 4 are placed in the top tier
and SCUs of weight 1 on the bottom, reflecting the fact
that fewer SCUs are expressed in all summaries, more
in three, and so on. For the mid-range tiers, neighbor-
ing tiers sometimes have the same number of SCUs. In
descending tiers, SCUs become less important informa-
tionally since they emerged from fewer summaries.
We use the term ?pyramid of order n? to refer to a pyra-
mid with n tiers. Given a pyramid of order n, we can
predict the optimal summary content?it should contain
all the SCUs from the top tier, if length permits, SCUs
from the next tier and so on. In short, an SCU from
tier (n ? 1) should not be expressed if all the SCUs in
tier n have not been expressed. This characterization of
optimal content ignores many complicating factors (e.g.,
ordering, SCU interdependency). However, it is predic-
tive: among summaries produced by humans, many seem
equally good without having identical content. Figure
2, with two SCUs in the uppermost tier and four in the
next, illustrates two of six optimal summaries of size 4
(in SCUs) that this pyramid predicts.
The score we assign is a ratio of the sum of the weights
of its SCUs to the sum of the weights of an optimal sum-
mary with the same number of SCUs. It ranges from 0
to 1, with higher scores indicating that relatively more of
the content is as highly weighted as possible.
The exact formula we use is computed as follows. Sup-
pose the pyramid has n tiers, T
i
, with tier T
n
on top and
T
1
on the bottom. The weight of SCUs in tier T
i
will be
i.5 Let jT
i
j denote the number of SCUs in tier T
i
. Let D
i
be the number of SCUs in the summary that appear in T
i
.
SCUs in a summary that do not appear in the pyramid are
assigned weight zero. The total SCU weight D is:
D =
?
n
i=1
i  D
i
5This weight is not fixed and the method does not depend
on the specific weights assigned. The weight assignment used
is simply the most natural and intuitive one.
The optimal content score for a summary with X SCUs
is:
Max =
n
?
i=j+1
i  jT
i
j + j  (X ?
n
?
i=j+1
jT
i
j)
where j = max
i
(
n
?
t=i
jT
t
j  X) (1)
In the equation above, j is equal to the index of the
lowest tier an optimally informative summary will draw
from. This tier is the first one top down such that the
sum of its cardinality and the cardinalities of tiers above
it is greater than or equal to X (summary size in SCUs).
For example, if X is less than the cardinality of the most
highly weighted tier, then j = n and Max is simply Xn
(the product of X and the highest weighting factor).
Then the pyramid score P is the ratio of D to Max.
Because P compares the actual distribution of SCUs to
an empirically determined weighting, it provides a direct
correlate of the way human summarizers select informa-
tion from source texts.
3.1 Reliability and Robustness
We aimed for an annotation method requiring relatively
little training, and with sufficient interannotator reliabil-
ity to produce a stable pyramid score. Here we present re-
sults indicating good interannotator reliability, and pyra-
mid scores that are robust across annotations.
SCU annotation involves two types of choices: extract-
ing a contributor from a sentence, and assigning it to an
SCU. In a set of four summaries about the Philippine Air-
lines (PAL), two coders (C1 and C2; the co-authors) dif-
fered on the extent of the following contributor: f
C1
after
f
C2
the ground crew union turned down a settlementg
C1
whichg
C2
. Our approach is to separate syntactic from se-
mantic agreement, as in (Klavans et al, 2003). Because
constituent structure is not relevant here, we normalize all
contributors before computing reliability.
We treat every word in a summary as a coding unit, and
the SCU it was assigned to as the coding value. We re-
quire every surface word to be in exactly one contributor,
and every contributor to be in exactly one SCU, thus an
SCU annotation constitutes a set of equivalence classes.
Computing reliability then becomes identical to compar-
ing the equivalence classes constituting a set of corefer-
ence annotations. In (Passonneau, 2004), we report our
method for computing reliability for coreference annota-
tions, and the use of a distance metric that allows us to
weight disagreements. Applying the same data represen-
tation and reliability formula (Krippendorff?s Alpha) as
in (Passonneau, 2004), and a distance metric that takes
into account relative SCU size, to the two codings C1
and C2 yields ? = 81. Values above .67 indicate good
reliability (Krippendorff, 1980).
A H C J
C1 .97 .87 .83 .82
C2 .94 .87 .84 .74
Consensus .95 .89 .85 .76
Table 1: Pyramid scores across annotations.
1 (9) 2 (36) 3 (84) 4 (128) 5 (128) 6 (84) 7 (36) 8 (9) 9 (1)0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of summaries in the pyramid (number of pyramids)
Su
m
m
ar
y 
sc
or
e
arv d30042.b
min d30042.b
max d30042.b
arv d30042.q
min d30042.q
max d30042.q
Figure 3: Min, max and average scores for two sum-
maries ? one better than the other.
More important than interannotator reliability is the ro-
bustness of the pyramid metric, given different SCU an-
notations. Table 1 gives three sets of pyramid scores for
the same set of four PAL summaries. The rows of scores
correspond to the original annotations (C1, C2) and a
consensus. There is no significant difference in the scores
assigned across the three annotations (between subjects
ANOVA=0.11, p=0.90).
3.2 Pyramid Scores of Human Summaries
Here we use three DUC 2003 summary sets for which
four human summaries were written. In order to provide
as broad a comparison as possible for the least annotation
effort, we selected the set that received the highest DUC
scores (D30042: Lockerbie), and the two that received
the lowest (D31041: PAL; D31050: China). For each set,
we collected six new summaries from advanced under-
graduate and graduate students with evidence of superior
verbal skills; we gave them the same instructions used by
NIST. This turned out to be a large enough corpus to in-
vestigate how many summaries a pyramid needs for score
stability. Here we first compare pyramid scores of the
original summaries with DUC scores. Then we present
results demonstrating the need for at least five summaries
per pyramid, given this corpus of 100-word summaries.
Table 2 compares DUC and pyramid scores for all
three sets. The first two rows of pyramid scores are for
a pyramid of order 3 using a single pyramid with the re-
maining three original DUC summaries (n=3) versus an
Lockerbie (D30042)
Method A B C D
DUC n.a. .82 .54 .74
Pyramid (n=3) .69 .83 .75 .82
Pyramid (Avg. n=3) .68 .82 .74 .76
Pyramid (n=9) .74 .89 .80 .83
PAL (D31041)
Method A H I J
DUC .30 n.a. .30 .10
Pyramid (n=3) .76 .67 .59 .43
Pyramid (Avg. n=3) .46 .50 .52 .57
Pyramid (n=9) .52 .56 .60 .63
China (D31050)
Method C D E F
DUC n.a. .28 .27 .13
Pyramid (n=3) .57 .63 .72 .56
Pyramid (Avg. n=3) .64 .61 .72 .58
Pyramid (n=9) .69 .67 .78 .63
Table 2: Comparison of DUC and Pyramid scores; capital
letters represent distinct human summarizers.
average over all order-3 pyramids (Avg. n=3); the third
row of pyramid scores are for the single pyramid of or-
der 9 (n=9; note that the 10th summary is the one being
scored). Compared to the DUC scores, pyramid scores
show all humans performing reasonably well. While the
Lockerbie set summaries are better overall, the difference
with the PAL and China sets scores is less great than with
the DUC method, which accords with our impressions
about the relative quality of the summaries. Note that
pyramid scores are higher for larger pyramid inventories,
which reflects the greater likelihood that more SCUs in
the summary appear in the pyramid. For a given order
pyramid, the scores for the average and for a specific
pyramid can differ significantly, as, for example, PAL A
and PAL J do (compare rows n=3 and n=9).
The pyramid rows labelled ?n=3? are the most compa-
rable to the DUC scores in terms of the available data.
For the DUC scores there was always a single model, and
no attempt to evaluate the model.
Pyramid scores are quantitatively diagnostic in that
they express what proportion of the content in a summary
is relatively highly weighted, or alternatively, what pro-
portion of the highly weighted SCUs appear in a sum-
mary. The pyramid can also serve as a qualitative diag-
nostic tool. To illustrate both points, consider the PAL A
summary; its score in the n=3 row of .76 indicates that
relatively much of its content is highly weighted. That
is, with respect to the original pyramid with only three
tiers, it contained a relatively high proportion of the top
tier SCUs: 3/4 of the w=3 facts (75%). When we av-
erage over all order-3 pyramids (Avg. n=3) or use the
largest pyramid (n=9), the PAL A score goes down to .46
or .52, respectively. Given the nine-tier pyramid, PAL A
contains only 1/3 of the SCUs of w6, a much smaller
proportion of the most highly weighted ones. There are
four missing highly weighted SCUs and they express the
following facts: to deal with its financial crisis, Pal nego-
tiated with Cathay Pacific for help; the negotiations col-
lapsed; the collapse resulted in part from PAL?s refusal to
cut jobs; and finally, President Estrada brokered an agree-
ment to end the shutdown strike. These facts were in the
original order-3 pyramid with relatively lower weights.
The score variability of PAL A, along with the change
in status of SCUs from having low weights to having high
ones, demonstrates that to use the pyramid method reli-
ably, we need to ask how many summaries are needed
to produce rankings across summaries that we can have
confidence in. We now turn to this analysis.
3.3 Behavior of Scores as Pyramid Grows
Here we address two questions raised by the data from
Table 2, i.e., that scores change as pyramid size increases:
1. How does variability of scores change as pyramid
order increases?
2. At what order pyramid do scores become reliable?
To have confidence in relative ranking of summaries by
pyramid scores, we need to answer the above questions.
It has often been noted that different people write dif-
ferent summaries; we observe that with only a few sum-
maries in a pyramid, there is insufficient data for the
scores associated with a pyramid generated from one
combination of a few summaries to be relatively the same
as those using a different combination of a few sum-
maries. Empirically, we observed that as pyramids grow
larger, and the range between higher weight and lower
weight SCUS grows larger, scores stabilize. This makes
sense in light of the fact that a score is dominated by the
higher weight SCUS that appear in a summary. However,
we wanted to study more precisely at what point scores
become independent of the choice of models that pop-
ulate the pyramid. We conducted three experiments to
locate the point at which scores stabilize across our three
datasets. Each experiment supports the same conclusion,
thus reinforcing the validity of the result.
Our first step in investigating score variability was to
examine all pairs of summaries where the difference in
scores for an order 9 pyramid was greater than 0.1; there
were 68 such pairs out of 135 total. All such pairs ex-
hibit the same pattern illustrated in Figure 3 for two sum-
maries we call ?b? and ?q?. The x-axis on the plot shows
how many summaries were used in the pyramid and the
y-axis shows the min, max and average score scores for
the summaries for a given order of pyramid, 6 Of the two,
6Note that we connected data points with lines to make the
graph more readable.
?b? has the higher score for the order 9 pyramid, and is
perceivably more informative. Averaging over all order-
1 pyramids, the score of ?b? is higher than ?q? but some
individual order-1 pyramids might yield a higher score
for ?q?. The score variability at order-1 is huge: it can
be as high as 0.5. With higher order pyramids, scores
stabilize. Specifically, in our data, if summaries diverge
at some point as in Figure 3, where the minimum score
for the better summary is higher than the maximum score
for the worse summary, the size of the divergence never
decreases as pyramid order increases. For pyramids of
order > 4, the chance that ?b? and ?q? reverse ranking
approaches zero.
For all pairs of divergent summaries, the relationship
of scores follows the same pattern we see in Figure 3 and
the point of divergence where the scores for one summary
become consistently higher than those of the othere, was
found to be stable ? in all pair instances, if summary A
gets higher scores than summary B for all pyramids of
order n, than A gets higher scores for pyramids of order
 n. We analyzed the score distributions for all 67 pairs
of ?divergent? summaries in order to determine what or-
der of pyramid is required to reliably discriminate them.
The expected value for the point of divergence of scores,
in terms of number of summaries in the pyramid, is 5.5.
We take the scores assigned at order 9 pyramids as be-
ing a reliable metric on the assumption that the pattern
we have observed in our data is a general one, namely
that variance always decreases with increasing orders of
pyramid, and that once divergence of scores occurs, the
better summary never gets a lower score than the worse
for any model of higher order.
We postulate that summaries whose scores differ by
less than 0.06 have roughly the same informativeness.
The assumption is supported by two facts. First, this cor-
responds to the difference in PAL scores (D31041) we
find when we use a different one of our three PAL an-
notations (see Table 1). Second, the pairs of summaries
whose scores never clearly diverged had scores differing
by less than 0.06 at pyramid order 9.
Now, for each pair of summaries (sum1, sum2), we
can say whether they are roughly the same when evalu-
ated against a pyramid of order n and we will denote this
as jsum1j ==
n
jsum2j, (scores differ by less than 0.06
for some pyramid of order n) or different (scores differ
by more than 0.06 for all pyramids of order n) and we
will use the notation jsum1j <
n
jsum2j if the score for
sum2 is higher.
When pyramids of lower order are used, the following
errors can happen, with the associated probabilities:
E
1
: jsum1j ==
9
jsum2j but jsum1j <
n
jsum2j or
jsum1j >
n
jsum2j at some lower order n pyramid.
The conditional probability of this type of error is
p
1
= P (jsum1j >
n
jsum2jjjsum1j ==
9
jsum2j).
E
2
: jsum1j <
9
jsum2j but at a lower order
jsum1j ==
n
jsum2j. This error corresponds to
?losing ability to discern?, which means one can tol-
erate it, as long as the goal is not be able to make fine
grained distinctions between the summaries. Here,
p
2
= P (jsum1j ==
n
jsum2jjjsum1j <
9
jsum2j).
E
3
: jsum1j <
9
jsum2j but at lower level
jsum1j >
n
jsum2j Here, p
3
= P (jsum1j >
n
jsum2jjjsum1j <
9
jsum2j) + P (jsum1j <
n
jsum2jjsum1j >
n
jsum2j). This is the most
severe kind of mistake and ideally it should never
happen?the two summaries appear with scores
opposite to what they really are.7
The probabilities p
1
, p
2
and p
3
can be computed di-
rectly by counting how many times the particular error
occurs for all possible pyramids of order n. By taking
each pyramid that does not contain either of sum1 or
sum2 and comparing the scores they are assigned, the
probabilities in Table 3 are obtained. We computed prob-
abilities for pairs of summaries for the same set, then
summed the counts for error occurrence across sets. The
order of the pyramid is shown in column n. ?Data points?
shows how many pyramids of a given order were exam-
ined when computing the probabilities. The total proba-
bility of error p = p1 P (jsum1j ==
9
jsum2j) + (p2 +
p3)  (1 ? P (jsum1j ==
9
jsum2j)) is also in Table 3.
Table 3 shows that for order-4 pyramids, the errors of
type E
3
are ruled out. At order-5 pyramids, the total prob-
ability of error drops to 0.1 and is mainly due to error E
2
,
which is the mildest one.
Choosing a desirable order of pyramid involves balanc-
ing the two desiderata of having less data to annotate and
score stability. Our data suggest that for this corpus, 4 or
5 summaries provide an optimal balance of annotation ef-
fort with reliability. This is reconfirmed by our following
analysis of ranking stability.
n p1 p2 p3 p data points
1 0.41 0.23 0.08 0.35 1080
2 0.27 0.23 0.03 0.26 3780
3 0.16 0.19 0.01 0.18 7560
4 0.09 0.17 0.00 0.14 9550
5 0.05 0.14 0.00 0.10 7560
6 0.02 0.10 0.00 0.06 3780
7 0.01 0.06 0.00 0.04 1080
8 0.00 0.01 0.00 0.01 135
Table 3: Probabilities of errors E1, E2, E3 and total prob-
ability of error
7Note that such an error can happen only for models of order
lower than their point of divergence.
In order to study the issue of how the pyramid scores
behave when several summarizers are compared, not just
two, for each set we randomly selected 5 peer summaries
and constructed pyramids consisting of all possible sub-
sets of the remaining five. We computed the Spearman
rank-correlation coefficient for the ranking of the 5 peer
summaries compared to the ranking of the same sum-
maries given by the order-9 pyramid. Spearman coef-
ficent r
s
(Dixon and Massey, 1969) ranges from -1 to
1, and the sign of the coefficent shows whether the two
rankings are correlated negatively or positively and its
absolute value shows the strength of the correlation. The
statistic r
s
can be used to test the hypothesis that the two
ways to assign scores leading to the respective rankings
are independent. The null hypothesis can be rejected with
one-sided test with level of significance ? = 0.05, given
our sample size N = 5, if r
s
 0.85.
Since there are multiple pyramids of order n  5, we
computed the average ranking coefficient, as shown in
Table 4. Again we can see that in order to have a ranking
of the summaries that is reasonably close to the rankings
produces by a pyramid of order n = 9, 4 or more sum-
maries should be used.
n average r
s
# pyramids
1 0.41 15
2 0.65 30
3 0.77 30
4 0.87 15
5 1.00 3
Table 4: Spearman correlation coefficient average for
pyramids of order n  5
3.4 Rank-correlation with unigram overlap scores
Lin and Hovy (2003) have shown that a unigram co-
occurrence statistic, computed with stop words ignored,
between a summary and a set of models can be used to
assign scores for a test suite that highy correlates with the
scores assigned by human evaluators at DUC. We have
illustrated in Figure 1 above that human scores on human
summaries have large variance, and we assume the same
holds for machine summaries, so we believe the approach
is built on weak assumptions. Also, their approach is not
designed to rank individual summaries.
These qualifications aside, we wanted to test whether it
is possible to use their approach for assigning scores not
for an entire test suite but on a per set basis. We computed
the Spearman rank-coefficent r
s
for rankings assigned by
computing unigram overlap and those by pyramid of or-
der 9. For computing the scores, Lin?s original system
was used, with stop words ignored. Again 5 summaries
were chosen at random to be evaluated against models
composed of the remaining five summaries. Composite
models were obtained by concatenating different combi-
nations of the initial five summaries. Thus scores can be
computed using one, two and so on up to five reference
summaries. Table 5 shows the average values of r
s
that
were obtained.
# models average r
s
# model combinations
1 0.12 15
2 0.27 30
3 0.29 30
4 0.35 15
5 0.33 3
Table 5: Spearman correlation coefficient average for un-
igram overlap score assignment
As noted above, in order to consider the two scoring
methods as being substitutable, r
s
should be bigger than
0.85, given our sample size. Given the figures shown in
Table 5, we don?t have reason to believe that unigram
scores are correlated with pyramid scores.
4 Comparison with previous work
The work closest to ours is (Halteren and Teufel, 2003),
and we profited from the lessons they derived from an
annotation of 50 summaries of a single 600-word docu-
ment into content units that they refer to as factoids. They
found a total of 256 factoids and note that the increase in
factoids with the number of summaries seems to follow a
Zipfian distribution.
We identify four important differences between fac-
toids and SCUs. First, an SCU is a set of contributors
that are largely similar in meaning, thus SCUs differ from
each other in both meaning and weight (number of con-
tributors). In contrast, factoids are semi-formal expres-
sions in a FOPL-style semantics, which are composition-
ally interpreted. We intentionally avoid creating a rep-
resentation language for SCU labels; the function of an
SCU label is to focus the annotator?s attention on the
shared meaning of the contributors. In contrast to Hal-
tern and Teufel, we do not believe it is possible to arrive
at the correct representation for a set of summaries; they
refer to the observation that the factoids arrived at depend
on the summaries one starts with as a disadvantage in that
adding a new summary can require adjustments to the set
of factoids. Given the different knowledge and goals of
different summarizers, we believe there can be no cor-
rect representation of the semantic content of a text or
collection; a pyramid, however, represents an emergent
consensus as to the most frequently recognized content.
In addition to our distinct philosophical views regarding
the utility of a factoid language, we have methodological
concerns: the learning curve required to train annotators
would be high, and interannotator reliability might be dif-
ficult to quantify or to achieve.
Second, (Halteren and Teufel, 2003) do not make di-
rect use of factoid frequency (our weights): to construct
a model 100-word summary, they select factoids that oc-
cur in at least 30% of summaries, but within the resulting
model summary, they do not differentiate between more
and less highly weighted factoids. Third, they annotate
semantic relations among factoids, such as generalization
and implication. Finally, they report reliability of the an-
notation using recall and precision, rather than a reliabil-
ity metric that factors in chance agreement. In (Passon-
neau, 2004), we note that high recall/precision does not
preclude low interannotator reliability on a coreference
annotation task.
Radev et al (2003) also exploits relative importance of
information. Evaluation data consists of human relevance
judgments on a scale from 0 to 10 on for all sentences in
the original documents. Again, information is lost rela-
tive to the pyramid method because a unique reference
summary is produced instead of using all the data. The
reference summary consists of the sentences with highest
relevance judgements that satisfy the compression con-
straints. For multidocument summarization compression
rates are high, so even sentences with the highest rele-
vance judgments are potentially not used.
Lin and Hovy (2002) and Lin and Hovy (2003) were
the first to systematically point out problems with the
large scale DUC evaluation and to look to solutions by
seeking more robust automatic alternatives. In their stud-
ies they found that multiple model summaries lead to
more stable evaluation results. We believe a flaw in their
work is that they calibrate the method to the erratic DUC
scores. When applied to per set ranking of summaries, no
correlation was seen with pyramid scores.
5 Conclusions
There are many open questions about how to parameter-
ize a summary for specific goals, making evaluation in
itself a significant research question (Jing et al, 1998).
Instead of attempting to develop a method to elicit reli-
able judgments from humans, we chose to calibrate our
method to human summarization behavior.
The strengths of pyramid scores are that they are re-
liable, predictive, and diagnostic. The pyramid method
not only assigns a score to a summary, but also allows the
investigator to find what important information is miss-
ing, and thus can be directly used to target improvements
of the summarizer. Another diagnostic strength is that it
captures the relative difficulty of source texts. This allows
for a fair comparison of scores across different input sets,
which is not the case with the DUC method.
We hope to address two drawbacks to our method in
future work. First, pyramid scores ignore interdependen-
cies among content units, including ordering. However,
our SCU annotated summaries and correlated pyramids
provide a valuable data resource that will allow us to in-
vestigate such questions. Second, creating an initial pyra-
mid is laborious so large-scale application of the method
would require an automated or semi-automated approach.
We have started exploring the feasibility of automation
and we are collecting additional data sets.
References
Wilfrid Dixon and Frank Massey. 1969. Introduction to
statistical analysis. McGraw-Hill Book Company.
Hans Halteren and Simone Teufel. 2003. Examining
the consensus between human summaries: initial ex-
periments with factoid analysis. In HLT-NAACL DUC
Workshop.
Hongyan Jing, Regina Barzilay, Kathleen McKeown, and
Michael Elhadad. 1998. Summarization evaluation
methods: Experiments and analysis. In AAAI Sympo-
sium on Intelligent Summarization.
Judith Klavans, Sam Popper, and Rebecca J. Passonneau.
2003. Tackling the internet glossary glut: Extraction
and evaluation of genus phrases. In SIGIR Workshop:
Semantic Web, Toronto.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage Publications, Bev-
erly Hills, CA.
Chin-Yew Lin and Eduard Hovy. 2002. Manual and au-
tomatic evaluation of summaries. In Proceedings of
the Workshop on Automatic Summarization, post con-
ference workshop of ACL 2002.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurance statis-
tics. In Proceedings of HLT-NAACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In ACL.
Rebecca J. Passonneau and Ani Nenkova. 2003. Evaluat-
ing content selection in human- or machine-generated
summaries: The pyramid method. Technical Report
CUCS-025-03, Columbia University.
Rebecca J. Passonneau. 2004. Computing reliability
for coreference annotation. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation (LREC), Lisbon, Portugal.
Dragomir Radev, Simone Teufel, Horacio Saggion, and
W. Lam. 2003. Evaluation challenges in large-scale
multi-document summarization. In ACL.
G. J. Rath, A. Resnick, and R. Savage. 1961. The for-
mation of abstracts by the selection of sentences: Part
1: sentence selection by man and machines. American
Documentation, 2(12):139?208.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 577?587, Dublin, Ireland, August 23-29 2014.
Cross-lingual Discourse Relation Analysis:
A corpus study and a semi-supervised classification system
Junyi Jessy Li
1
, Marine Carpuat
2
and Ani Nenkova
1
1
University of Pennsylvania, Philadelphia, PA 19104, USA
{ljunyi, nenkova}@seas.upenn.edu
2
National Research Council Canada, Ottawa, ON K1A 0R6, Canada
marine.carpuat@nrc.gc.ca
Abstract
We present a cross-lingual discourse relation analysis based on a parallel corpus with discourse
information available only for one language. First, we conduct a corpus study to explore dif-
ferences in discourse organization between Chinese and English, including differences in infor-
mation packaging, implicit/explicit discourse expression divergence, and discourse connective
ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations,
using the parallel corpus instead of discourse annotation in the language of interest. Our result-
ing semi-supervised system reaches state-of-art performance on the task of discourse relation
detection, and outperforms a supervised system on discourse relation classification.
1 Introduction
The analysis of the way spans of text semantically connect with each other to create a coherent text has
a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al.,
1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however,
labelled datasets were rare and rather small.
The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of
maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse
parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the
use of discourse connectives in English news text and have developed methods for the identification of
discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and
Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al.,
2014). Some have applied the insights and classifiers to standard natural language processing tasks such
as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal
dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012).
A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource
in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse
relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic
(Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012).
On the other hand, for the vast majority of languages, such well-annotated resource for discourse re-
lations is not available. In our work we carry the valuable annotations in the PDTB over to another
language?Chinese?using parallel corpora. Projecting information available in one language onto an-
other has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov,
2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and
Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior
work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives
(Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly
infers discourse relations using resources only available in another language.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
577
The goal of our work is not only to measure the accuracy with which discourse relations can be iden-
tified in another language without annotations beyond the PDTB, but also to catalog the differences in
discourse relation realization across different languages, Chinese and English in our case. We show
that the two languages vastly differ in how information is packaged into a sentence, which also leads
to differences in the implicit/explicit expression of discourse relations and the ambiguities in discourse
connectives. These differences challenge the currently accepted distinctions between syntax and dis-
course between the two languages for applications such as machine translation. Then we present our
semi-supervised learning algorithm to recognize explicit discourse relations in Chinese, relying solely
on discourse information available in English. For multiway classification, our system outperforms a
supervised system trained on the existing pilot dataset of discourse relations in Chinese (Zhou and Xue,
2012). In the task of binary classification for identifying specific discourse relations, the performance of
our system is within 4% accuracy of that of the supervised system for all but one relations.
2 Data
As our parallel corpus, we use the newswire portion of the GALE Chinese-English Word Alignment and
Tagging Training corpus (parts 1 and 2). The corpus contains 2,175 newswire articles, corresponding to
6,255 translation segments with 248,999 Chinese characters. These articles were translated into English
by human translators. Gold standard word alignments are available for this corpus. A minimal match
alignment approach (Li et al., 2010) was adopted for creating the gold standard, namely, alignments are
between an English word and only the necessary Chinese characters. We repurpose this resource created
for machine translation research for our cross-lingual discourse analysis. The availability of manual
alignments between Chinese discourse connectives and their English translation makes it possible to
conduct a reliable analysis by focusing on actual cross-lingual divergences, without noise introduced by
potential errors from automatic aligners.
1
We use a highly accurate supervised classifier for English explicit discourse relations (Pitler and
Nenkova, 2009)
2
to automatically annotate the English portion of the GALE parallel corpus. The classi-
fier was trained on the PDTB to identify discourse relations explicitly signaled by a set of 100 discourse
connectives such as however, because, while or for example. For each instance of the 100 words or ex-
pressions, the classifier predicts if the expression is used as a discourse connective or if the instance is
a non-discourse connective sense of the phrase or word. For each instance predicted to be a discourse
connective, the classifier identifies the discourse relation signaled by the connective: TEMPORAL, COM-
PARISON, CONTINGENCY or EXPANSION. In our work we predict the same five categories for Chinese
expressions which can serve as discourse connectives.
For evaluation and the study of discourse connective ambiguities, we use a development set from
the Chinese Discourse Treebank (CDTB) (Zhou and Xue, 2012) consisting of 170 documents
3
. In the
CDTB, an annotation style similar to the PDTB is applied on the texts from the Chinese Treebank corpus
(Xue et al., 2005). For a discourse connective, one of eight discourse relation senses is annotated. All
of these classes are subsumed by the four top-level relations in the PDTB. We map them to the PDTB
relation senses according to their definitions:
Alternative? Expansion; Causation? Contingency; Conditional? Contingency; Conjunction? Expansion; Contrast?
Comparison; Expansion? Expansion; Purpose? Contingency; Temporal? Temporal.
3 Information packaging characteristics
The notion of sentence in Chinese is very different from that in English. Punctuation marks were in-
troduced in the early 20
th
century; sentences resemble more a collection of related information than
structurally well-defined syntactic units as in English. In fact, commas are often ambiguous, signaling
1
While cross-lingual projection could be directly applied to automatic word alignments, discourse relation analysis raises
some specific challenges because the main target of analysis (discourse connectives) are function words, which do not have
as much of an impact on the final analysis in applications focusing on content words. As a result, we exclusively use manual
alignment links in this study, and will address issues raised by automatic alignments in future work.
2
The classifier is available at http://www.cis.upenn.edu/?epitler/discourse.html
3
This is an on-going annotation project. We are grateful to the authors for providing us with their valuable development set.
578
% data avg-length std-length
1-many 18.83 61.42 28.85
1-1 81.17 35.73 25.34
Table 1: Percentage, length and standard deviation of sentences for which one Chinese source sentence
is translated into one (1-1) or multiple (1-many) English sentences. Length is calculated based on the
number of Chinese characters.
either clausal subordination, coordination or end-of-sentence (as construed from an English-centric point
of view). Automatic systems have been developed to disambiguate the function of commas (Jin et al.,
2004; Xue and Yang, 2011). This is a rather interesting phenomenon for discourse processing, as the
English equivalents of Chinese sentences are in fact multi-sentential discourses in English.
The GALE corpus allows us to examine how often this mismatch of discourse organization occurs.
Here we look for Chinese source sentences that were translated into multiple English sentences by the
human translators. Consider the following example in which the corresponding clauses on both sides are
numbered and marked in square brackets:
source [????????????????????????]
1
?[???????????????????]
2
?[??
?????????????????]
3
?
ref [In recent years, new phrases such as ?disaster relief diplomacy? and ?disaster relief aid? have appeared constantly]
1
. [In
relation to the issue of disaster relief, all countries have been silently competing with one another and comparing offerings]
2
.
[Some countries are trying to establish various kinds of international alliance in the name of disaster relief]
3
.
In this example, the Chinese sentence packed the following related content into a single sentence:
the occurrence of the new phrases about disaster relief, the competition among the countries related
to disaster relief, and alliances in the name of disaster relief. The phrases expressing this information
are separated by the commas in the source Chinese sentence because they are about a single concept
?disaster relief?. However, this information needs to be partitioned into three different sentences, each
with different subjects, when translated to English.
In the GALE corpus, we identified 1,178 (out of total 6,255) source sentences with reference transla-
tions containing more than one sentence. In other words, sentence/discourse mismatch between Chinese
and English occurs for 18.83% of the data. Table 1 shows the portion of data involved in such mismatch,
with percentage, mean and standard deviation of source sentence length. Not surprisingly, Chinese sen-
tences that require multiple sentences in their English translation are much longer. These long sentences
are fairly common, which suggests that the difference in information packaging is highly prevalent and
could potentially affect key applications such as machine translation, where systems are trained on a
sentence to sentence basis.
We will return to the discussion of this mismatch later, when we discuss how English and Chinese also
appear to differ in the way discourse relations are signaled. Briefly, the issue is that relations that are
explicit in one language may become implicit in the other, easily inferred by the reader but not marked
by a discourse connective. Also, there is an increase in the sense ambiguity of discourse connectives
related to EXPANSION relations in Chinese.
4 Implicit and explicit relations
In this section, we present two other differences between the two languages related to discourse orga-
nization. One is the need for a discourse relation expressed implicitly in one language to be expressed
explicitly in another. The other is the difference of the ambiguity of discourse connectives across the two
languages. Before the discussion of these interesting asymmetries, we first present the method for direct
projection of discourse relations using the GALE gold standard alignments, which we use to gather a set
of explicit discourse connectives in Chinese.
4.1 Direct projection
Thus far we have available a parallel Chinese/English corpus, discourse connectives automatically tagged
with their senses on the English side and manual alignments of atomic units between English and Chi-
579
Comparison Contingency Expansion Temporal
CH/EN mismatch 63 109 360 195
all 551 469 1198 885
% data 11.43 23.24 30.05 22.03
Table 2: Numbers and percentages of Chinese/English implicit/explicit mismatches.
nese. So for each discourse connective in an English sentence, it is straightforward to identify the corre-
sponding expressions in the Chinese sentence following the gold standard alignments. Then the aligned
Chinese expression can be assigned a discourse tag?non-discourse use or one of the four main discourse
relation types?which is the same as in the English translation. We call the resulting annotation on the
Chinese sentences discourse projection.
Further we discard potential expressions of Chinese connectives if they occurred with the same part of
speech only once in the entire corpus. The result is a list of a total of 118 Chinese discourse connectives
harvested using direct projection.
4.2 Implicit or Explicit?
A discourse relation can be expressed either with an explicit connective (e.g. however, since), or implic-
itly without a connective, in which case the relation would have to be inferred by the reader. Languages
may differ in how they express discourse relations.
We investigate such implicit/explicit mismatch using direct projection. Specifically, we study the cases
in which an English discourse connective is not aligned to any part of its corresponding Chinese sentence.
In this case, the human translator explicitly expressed a discourse relation that was implicitly conveyed
in the corresponding Chinese sentence.
The following four examples illustrate a Chinese/English implicit/explicit mismatch for each of the
TEMPORAL, COMPARISON, CONTINGENCY and EXPANSION relation, respectively. On the Chinese side
we also mark the position of the inserted English connective.
source [????4?27?]
1
?[??????????????????????????]
2
?when
TEMPORAL
[???
?????????????????]
3
?
ref [On april 27 local time]
1
, [Afghan president Karzai and other important officials were forced to flee the scene]
3
[when
TEMPORAL
a military parade in Kabul, Afghanistan commemorating victory in the fight against the soviet invasion was
attacked]
2
.
source [??????????????]
1
?[??????????]
2
?while
COMPARISON
[??????]
3
?
ref [However, of the ten commonly-used languages today]
1
, [Arabic only ranks fourth]
2
, [while
COMPARISON
English ranks first]
3
.
source [??????????????????????]
1
??[??????????????????
??]
2
?since
CONTINGENCY
[????????????]
3
??
ref [Tung Ta-Wei, head representative for China Airlines in Shanghai, told reporters]
1
, ?[presently, the cross-strait charter
flights are still not ?direct flights? in the true sense of the term]
2
, [since
CONTINGENCY
they still have to pass through the hong kong
flight information region]
3
. ?
source [????]
1
?[?????????????????????]
2
?and
EXPANSION
[????????????
?]
3
?
ref [Liu Binjie said]
1
, [a key area of development for the Chinese publishing industry will be participating in international
competition]
2
, [and
EXPANSION
in the future the two sides can strengthen their cooperation in this area]
3
.
The first example is particularly interesting from a discourse point of view as it combines information
ordering considerations along with the implicit/explicit expression of discourse relations: not only is the
connective when missing in Chinese but the two arguments of the connective appeared in reverse order
in the English translation of the sentence, with the comma omitted.
In Table 2, we show the numbers and percentages of Chinese/English implicit/explicit mismatches for
each relation. We also list the ten connectives that are most frequently associated with the mismatch (i.e.,
were added to the reference translation), in the format of connective (# mismatches) below:
and (341), when (120), while (45), if (37), so that (29), but (23), after (22), so (22), as (21), then (18)
This analysis reveals that the EXPANSION relation is more likely to be implicitly expressed in Chinese,
although in other relations this phenomenon is also present.
580
Connective Senses Connective Senses
? COMPARISON (7) EXPANSION (2) ? CONTINGENCY (1) EXPANSION (1)
? COMPARISON (1) EXPANSION (2) ?...?? TEMPORAL (3) EXPANSION (2)
? CONTINGENCY (1) EXPANSION (3) ?? TEMPORAL (1) EXPANSION (1)
Table 3: Ambiguous Chinese connectives, according to manual annotations in the development CDTB.
A similar mismatch also happens when an English discourse connective is aligned to a punctuation
mark in Chinese, illustrated in the following example, where the comma underlined in the source sen-
tence was translated to and, thus to an explicit EXPANSION in English:
source [????????????]
1
?[??????]
2
?[??????]
3
?[?????????]
4
?
ref [Most of the contingent?s squadrons garrisoned along the border]
1
[are stationed in remote areas]
2
[where the natural
conditions are rough]
3
[and
EXPANSION
the construction of informatization relatively lags behind]
4
.
The insertion of the explicit discourse connective and makes the use of punctuation between ?rough
conditions? and ?informatization? unnecessary in English. Through our direct projection we found 136
such implicit to explicit transformations with commas and 5 with semicolons. All of them are of the
relation EXPANSION, further highlighting the differences in information packaging between the two lan-
guages.
4.3 Ambiguity of connectives
Although most of the English discourse connectives identified in the PDTB are not ambiguous, some of
the most frequently used ones are (Pitler et al., 2008; Miltsakaki et al., 2008). For example, while can
signal both TEMPORAL and COMPARISON relations; since, as can signal both TEMPORAL and CONTIN-
GENCY relations. Discourse connectives in different languages have different ambiguities; prior work
has shown that it is easier to disambiguate the sense of an ambiguous connective when parallel cor-
pora are available (Meyer et al., 2011). The two languages analyzed in Meyer et al. (2011), English
and French, are closely related European languages; here we investigate such differences in ambiguities
between English and Chinese connectives.
Specifically, using the connectives collected from direct projection, we inspect the relations annotated
for these connectives in the Chinese Discourse Treebank development set, and extract connectives such
that the majority sense they signal constitutes less than 90% of their total occurrences. Unlike in English
where the vast majority of ambiguities are between TEMPORAL and some other sense, we find that all
such connectives in Chinese are ambiguous between some relation and EXPANSION. An example of
ambiguity between TEMPORAL and EXPANSION is shown below:
source???????????????????
TEMPORAL
????????????
ref Only in this way can Dujkovic sit back and do nothing and look on others disinterestedly when
TEMPORAL
getting his full salary
per contract.
source??????????
EXPANSION
??????????????????
ref While
EXPANSION
reducing driving time, they are also mixing gasoline with cooking oil recycled from restaurants.
In the first case, there is a synchrony relation between Dujkovic?s ?sitting back and doing nothing?,
and ?getting his full salary?. In the second case, ?reducing driving time? and ?mixing gasoline with
cooking oil? are a list of methods for saving gasoline.
In Table 3 we list these ambiguous Chinese connectives, their senses and the frequency with which
they were annotated. The ambiguities we see here are very different from those in English where the
TEMPORAL?CONTINGENCY and COMPARISON?CONTINGENCY ambiguities are most prominent.
5 Predicting discourse relation sense in Chinese
Our analysis so far has revealed considerable differences in the expression of discourse relations in
Chinese and English. We now show that projected annotations can be used to disambiguate Chinese
discourse connectives despite these differences.
581
5.1 Learning with unlabeled data
The main idea of learning by projection across parallel corpora is to use a classifier to annotate the En-
glish portion of the data, then project the discourse relation sense labels onto the corresponding Chinese
sentences. Then a classifier can be trained using features gathered on the Chinese portion of the data.
However, labels gathered from direct projections are not suitable for learning systems without extra
processing. If an English connective is aligned to one of the Chinese connectives, we can transfer its label
from English to the Chinese connective. However, it is highly likely that a Chinese connective appears in
the source sentence but the reference translation used an alternative expression or paraphrase rather than
the 100 identified connectives in the PDTB. It is difficult to distinguish through direct projection if an
explicit discourse connective in Chinese was expressed implicitly in English or if the Chinese expression
was used in a non-discourse sense.
The possibilities described above imply that in our work, we cannot assume that through direct pro-
jection we have a fully labeled dataset for discourse connective senses in Chinese. Instead we have a
mixture of data with labeled positive examples (when an explicit English connective was aligned to the
phrase) and unlabeled examples (where there was no explicit discourse connective in English, so the
Chinese expression is either used in a non-discourse sense or is expressed implicitly or using alternative
expressions in English, and thus the label is unknown).
Luckily, learning from positive and unlabeled examples, especially for binary classification, is a fairly
well studied problem in machine learning (Lee and Liu, 2003; Liu et al., 2003; Elkan and Noto, 2008).
We adopt such methods as part of our semi-supervised learning system.
In this work, we propose the following components for relation classification:
(Noisy) data labeling Classify each instance of a possible connective on the English side of the corpus
into either non-discourse use, or one of TEMPORAL, CONTINGENCY, COMPARISON or EXPANSION. If
the English connective signals one of the four relations, transfer the labels to the connectives expressed
in the corresponding Chinese sentences through alignments, as described in Section 4.1.
Train sense classifier This classifier is trained only on the Chinese expressions labeled as one of the
four main classes of discourse relation. We can train either a binary classifier to predict if a connec-
tive expresses a particular relation, or a 4-way classifier which assigns the most probable sense to each
connective. The potentially problematic labels for the non-discourse class are not used in this stage.
Train discourse use classifier This classifier has to use the potentially problematic data, where we
cannot distinguish negative examples from untagged positive examples. The problem is solved as a
cascade of classifiers, an approach developed in Elkan and Noto (2008). The idea is to train a noisy
classifier that produces a soft score for the data?a probability of being in the class rather than a strict
class assignment.
Let y be the true discourse use class to be predicted: y = 1 for examples of discourse use, and y = 0
for examples of non-discourse use. Let l indicate whether the example is labeled as discourse use (l = 1),
or unlabeled (l = 0, unknown or non-discourse use). First, we use a logistic regression classifier LR to
estimate P (l = 1|y = 1). Let?s call this estimate e. Using LR, e can be estimated as
?
x?P
LR(x)/|P |,
where P is the set of the original positively labeled examples, LR(x) is the probability of expression x
to be labeled positively. We then use the estimator e to calculate the estimated value of P (y = 1|l = 0),
the probability of an expression being discourse use from the original unlabeled examples:
w =
LR(x)
e
/
1? LR(x)
1? e
In the second stage, each of the unlabeled examples are duplicated, once as a positive example with
weight w and once as a negative example with weight 1? w. Our second stage classifier?linear-kernel
SVM with weights for each example?is trained on the combined set of positive examples (discourse use)
and the duplicated version of the unlabeled examples (unknown and non-discourse use class). When w
is close to 0.5, the example is practically noise (with labels 0.5 and -0.5) and does not affect the learning
of parameters much. Weights closer to 1 practically reassign the originally non-discourse use example to
582
the discourse use class (labels 1 and 0); a weight close to 0 leaves the example as one of the non-discourse
use instances (with labels 0 and -1).
Test phase In testing, first the second-stage SVM model for discourse vs. non-discourse use is applied.
For only the expression predicted to be discourse connectives (discourse use), we run the sense classifier
to do binary or multiway relation classification. Binary classification labels whether a connective sig-
nals a particular relation; multiway classification labels one of the five possible classes: non-discourse
use, TEMPORAL, COMPARISON, CONTINGENCY and EXPANSION. This series of classifiers results in a
system that can assign the same labels as the classifiers trained for English.
To complete our presentation of the approach, we now turn to describe the features used to represent
instances of potential discourse connectives.
5.2 Features
The following set of features for each expression we need to classify are extracted solely from the Chinese
part of the corpus
4
. The syntactic parse trees were obtained automatically (Levy and Manning, 2003).
Connective The connective expressions themselves. The vast majority of connectives (at least in En-
glish) are unambiguous, so using the identity of the connective is a hard-to-beat baseline for sense pre-
diction (Pitler et al., 2008).
Categories The syntactic category of the expression itself, as well as that of its parents, and its left and
right siblings (if any). These features are adapted from Pitler and Nenkova (2009).
Depth Depth of the expressions?s syntactic category in the parse tree for the sentence.
POS bigram Bigram of part-of-speech tags of the entire sentence.
Production pairs Parent-child node category pairs, gathered from subtrees of two ancestors starting
from the parent of the expression?s self-category. For example, a subtree IP?NP VP would yield the
features (IP NP) and (IP VP). Production rules have shown to be effective for implicit discourse relation
classification (Lin et al., 2009; Park and Cardie, 2012). This is a less sparse adaptation of such features.
Punctuation This class corresponds to two features. The first feature takes one of the three possible
values: if the expression starts a sentence, if there is a punctuation to the immediate left of the expression,
or none of above. The second feature has two values corresponding to whether there is a punctuation to
the expression?s immediate right.
Sequence pairs Left-to-right sequence pairs of node categories, gathered from subtrees of two ancestors
starting from the parent of the expression?s self-category. For example, a subtree IP?NP VP PU would
yield the features (NP VP) and (VP PU).
Size of ancestor nodes The number of children a node has, calculated with three ancestors starting from
the parent of the expression?s self-category.
# characters The number of Chinese characters in the connective expression.
5.3 Classification results
In this section, we demonstrate the effectiveness of learning discourse relations through parallel data pro-
jection and semi-supervised learning. We use the GALE corpus for training and the Chinese Discourse
Treebank development set (CDTB-dev) for testing. There are 5,136 training instances and 490 testing
instances. In addition, we compare performance with 10-fold cross validation results over CDTB-dev.
We obtain predictions for each fold and evaluate on the combined data from all folds, instead of av-
eraging performance for each fold. In this way the results from 10-fold validation and those from the
semi-supervised classifier trained on projected data are directly comparable. The LIBLINEAR pack-
age (Fan et al., 2008) was used for binary classification (including the discourse use classifier
5
), and
SVM-Multiclass (Tsochantaridis et al., 2004) with linear kernel was used for multiway classification.
4
As a reminder, the list of possible connectives was derived from direct projection after pruning items that occurred only
once with a particular part-of-speech. There is a total of 118 such expressions for Chinese.
5
http://www.csie.ntu.edu.tw/?cjlin/libsvmtools/#weights for data instances
583
Baseline Cascade Supervised
A F P/R A F P/R A F P/R
connective (C) 67.62 51.23 75.45/38.79 70.29 65.88 66.35/65.42 77.96 75.00 75.00/75.00
C+tree depth 69.39 55.36 77.50/43.06 69.80 66.36 65.18/67.59 78.57 75.86 75.34/76.39
C+categories 66.94 52.63 71.43/41.67 70.82 66.97 66.82/67.13 83.67 82.61 77.87/87.96
C+size of ancestor 67.96 52.57 75.65/40.28 71.22 67.59 67.12/68.06 76.12 72.73 73.24/72.22
C+POS bigram 70.00 58.12 75.56/47.22 74.29 70.42 71.43/69.44 81.84 80.35 76.79/84.26
C+punctuation 67.96 52.85 75.21/40.74 73.67 70.48 69.68/71.30 82.65 80.81 78.85/82.87
above, combined 70.61 60.00 75.00/50.00 75.10 71.63 71.96/71.30 82.04 80.00 78.57/81.48
Table 4: Accuracy, F-measure and precision/recall for classifying discourse/non-discourse use of con-
nective expressions, for top features and for the combined feature set.
5-way Baseline Projection 5-way Supervised
connective (C) 0.6332 0.6434 0.6114
C+tree depth 0.5959 0.6367 0.6384
C+punctuation 0.6224 0.6776 0.6425
C+size of ancestor 0.5939 0.6469 0.6073
C+categories 0.5837 0.6633 0.6359
C+POS bigram 0.6469 0.6980 0.6714
above, combined 0.6245 0.7020 0.6355
Table 5: Multiway discourse relation classification accuracies, for top and the combined features.
Discourse vs. non-discourse To demonstrate the cascade learning component in our system, we first
show results from the intermediate stage of the discourse vs. non-discourse prediction task. We compare
three systems: our cascade approach for handling noisy labels for non-discourse use, a baseline trained
only on the original noisy non-discourse labels (this corresponds to the hard-label performance of the
first stage classifier in our approach) and a supervised system trained on CDTB-dev (where predictions
are obtained in 10-fold cross validation fashion).
In Table 4 we show the accuracy, precision/recall and F measure for each system, using connective
expressions themselves and the five features that gave the best performance on the test set.
Cascade learning achieved a strong boost over the baseline with significant improvements on recall,
although it does not perform as well as the fully supervised system. The features most useful for this task
are POS bigrams and punctuations; syntactic category features are very useful for the supervised system,
but not as useful for the cascade system.
Multiway classification Now we show how our system performs for the complete task of multiway
classification of discourse relations for Chinese, recognizing each expression either as non-discourse use
or one of the four discourse relation senses. We compare our semi-supervised multiway classification
system against: (i) a baseline system that performs 5-way classification with the noisy labels from direct
projection in the GALE data (again corresponding to the hard-label performance of the first stage clas-
sifier in our approach); (ii) a supervised system for 5-way classification trained on CDTB-dev (where
predictions are obtained in 10-fold cross-validation fashion).
Table 5 records the accuracies for the connective expression and the five features performed best for
this task. The top features for multiway relation classification, in addition to connectives, are part-of-
speech bigrams, punctuations, and syntactic categories.
Notably, without any annotated data on the Chinese side, the projected semi-supervised system out-
performs the 5-way supervised system for all but one of the features, and is significantly better when the
top features are combined (70.2% vs. 63.55%). This finding justifies the idea and feasibility of using
parallel corpora for discourse relation classification.
Binary classification Finally, we present results and the most informative features for binary classifi-
cation of each relation sense individually. The semi-supervised projection system is compared against
a fully supervised binary classification system over 10-fold CDTB-dev, with accuracies and F scores
584
Projection Supervised Feature set
A F A F
COMPARISON 94.49 59.70 96.33 57.14 Connective, categories, size of ancestor, # characters, POS bigram
CONTINGENCY 92.65 41.94 96.33 70.97 Connective, production pairs
EXPANSION 85.10 69.20 87.96 77.20 Connective, categories, production pairs, sequence pairs, POS bigram
TEMPORAL 88.37 48.65 94.08 60.47 Connective, categories, production pairs, sequence pairs
Table 6: Accuracy and F measure for binary classification for each relation, including features that
significantly improves performance beyond the identity of the connective itself.
shown in Table 6. The feature sets included are the ones that significantly improve the F measure of a
relation compared to that when using the connective expressions alone.
For accuracies, the semi-supervised system is only slightly (1.8-3.7%) below that of the supervised
system for three of the four relations. On the other hand, F measures of the semi-supervised system are
not as good as the supervised system except for the COMPARISON relation. The feature categories indi-
cate that for Chinese discourse connectives, different feature sets are appropriate for different relations.
6 Conclusion
We investigated the tasks of discourse analysis and recognition without manual annotation. Instead, we
used parallel corpora to project automatic annotations available on one side (English) to the other (Chi-
nese). First, we conducted a corpus study which demonstrates the differences in information packaging
and discourse organization between English and Chinese. We highlighted the existence of long sentences
in Chinese that correspond to multiple sentences in English, mismatches between discourse expressions
that are implicit vs. explicit in the two languages, and differences in the ambiguity of discourse connec-
tives. Second, we presented a semi-supervised system that learns to predict discourse relations from the
noisy annotations derived from parallel corpora. On the multiway discourse relation classification task,
our system outperforms a fully supervised system trained using clean gold-standard annotation in the
targeted language.
References
Amal Al-Saif and Katja Markert. 2010. The Leeds Arabic Discourse Treebank: Annotating discourse connectives
for Arabic. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).
David Allbritton and Johanna Moore. 1999. Discourse cues in narrative text: Using production to predict compre-
hension. In AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 600?609.
Barbara Di Eugenio, Johanna D Moore, and Massimo Paolucci. 1997. Learning features that predict cue usage.
In Proceedings of the Annual Meeting of the Association for Computational Linguistics and Eighth Conference
of the European Chapter of the Association for Computational Linguistics (EACL), pages 80?87.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 294?
303.
Charles Elkan and Keith Noto. 2008. Learning classifiers from only positive and unlabeled data. In Proceedings
of the ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 213?220.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. The Journal of Machine Learning Research, 9:1871?1874.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projec-
tion constraints. In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International
Joint Conference on Natural Language Processing of the AFNLP (ACL), pages 369?377.
585
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via
syntactic projection across parallel texts. Natural Language Engineering, 11(3):311?325, September.
Meixun Jin, Mi-Young Kim, Dongil Kim, and Jong-Hyeok Lee. 2004. Segmentation of Chinese long sentences
using commas. In Proceedings of the SIGHAN Workshop on Chinese Language Processing, pages 1?8.
Richard Johansson and Pierre Nugues. 2006. A FrameNet-based semantic role labeler for Swedish. In Proceed-
ings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436?443.
Wee Sun Lee and Bing Liu. 2003. Learning with positive and unlabeled examples using weighted logistic regres-
sion. In Proceedings of the International Conference on Machine Learning (ICML), volume 3, pages 448?455.
Roger Levy and Christopher D. Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In
Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 439?446.
Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie Strassel, and Kazuaki Maeda. 2010. Enriching word alignment
with linguistic tags. In Proceedings of the International Conference on Language Resources and Evaluation
(LREC).
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Dis-
course Treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL-HLT), pages 997?1006.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip S Yu. 2003. Building text classifiers using positive
and unlabeled examples. In Proceedings of the IEEE International Conference on Data Mining (ICDM), pages
179?186.
William C. Mann and Sandra Thompson. 1988. Rhetorical Structure Theory: toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. The rhetorical parsing of natural language texts. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 96?103. Association for Computational Linguistics.
Thomas Meyer and Andrei Popescu-Belis. 2012. Using sense-labeled discourse connectives for statistical machine
translation. In Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and
Machine Translation and Hybrid Approaches to Machine Translation (ESIRMT-HyTra), pages 129?138.
Thomas Meyer, Andrei Popescu-Belis, Sandrine Zufferey, and Bruno Cartoni. 2011. Multilingual annotation and
disambiguation of discourse connectives for machine translation. In Proceedings of the Annual Meeting of the
Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 194?203.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Aravind Joshi. 2008. Sense annotation in the Penn Discourse
Treebank. In Proceedings of the International Conference on Computational Linguistics and Intelligent Text
Processing (CICLing), pages 275?286.
Umangi Oza, Rashmi Prasad, Sudheer Kolachina, Dipti Misra Sharma, and Aravind Joshi. 2009. The Hindi
Discourse Relation Bank. In Proceedings of the Third Linguistic Annotation Workshop, pages 158?161. Asso-
ciation for Computational Linguistics.
Sebastian Pado and Mirella Lapata. 2005. Cross-linguistic projection of role-semantic information. In Proceed-
ings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT-EMNLP), pages 859?866.
Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set
optimization. In Proceedings of the Annual Meeting of the Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 108?112.
Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
186?195.
586
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP Conference: Short Papers, pages 13?16.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily
identifiable discourse relations. In Proceedings of the Conference on Computational Linguistics (COLING):
Posters, page 87?90.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations
in text. In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International Joint
Conference on Natural Language Processing of the AFNLP (ACL), pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse TreeBank 2.0. In Proceedings of the International Conference on Language Re-
sources and Evaluation (LREC).
Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2010. Realization of discourse relations by other means:
Alternative lexicalizations. In Proceedings of the Conference on Computational Linguistics (COLING): Posters,
pages 1023?1031.
Frank Schilder. 2002. Robust discourse parsing via discourse markers, topicality and position. Natural Language
Engineering, 8(3):235?255.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine
learning for interdependent and structured output spaces. In Proceedings of the International Conference on
Machine Learning (ICML), page 104.
Lonneke van der Plas, Paola Merlo, and James Henderson. 2011. Scaling up automatic cross-lingual semantic role
annotation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL-HLT), pages 299?304.
Ben Wellner and James Pustejovsky. 2007. Automatically identifying the arguments of discourse connectives. In
Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages 92?101.
Nianwen Xue and Yaqin Yang. 2011. Chinese sentence segmentation as comma classification. In Proceedings of
the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-
HLT): Short Papers, pages 631?635.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The penn chinese treebank: Phrase structure
annotation of a large corpus. Natural Language Engineering, 11(2):207?238, June.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings of the First International Conference on Human Language
Technology Research (HLT), pages 1?8.
Deniz Zeyrek and Bonnie Webber. 2008. A discourse resource for Turkish: Annotating discourse connectives in
the METU corpus. In Proceedings of the 6th Workshop on Asian Language Resources, pages 65?72.
Yuping Zhou and Nianwen Xue. 2012. PDTB-style discourse annotation of Chinese text. In Proceedings of the
Annual Meeting of the Association for Computational Linguistics (ACL), pages 69?77.
587
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 37?47, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Lexical Differences in Autobiographical Narratives from Schizophrenic
Patients and Healthy Controls
Kai Hong1, Christian G. Kohler2, Mary E. March2, Amber A. Parker3, Ani Nenkova1
University of Pennsylvania
Philadelphia, PA, 19104, USA
1{hongkai1,nenkova}@seas.upenn.edu
2{kohler,memarch}@mail.med.upenn.edu
3{parker}@sas.upenn.edu
Abstract
We present a system for automatic
identification of schizophrenic patients
and healthy controls based on narratives
the subjects recounted about emotional
experiences in their own life. The focus of the
study is to identify the lexical features that
distinguish the two populations. We report the
results of feature selection experiments that
demonstrate that the classifier can achieve
accuracy on patient level prediction as high as
76.9% with only a small set of features. We
provide an in-depth discussion of the lexical
features that distinguish the two groups and
the unexpected relationship between emotion
types of the narratives and the accuracy of
patient status prediction.
1 Introduction
Recent studies have shown that automatic language
analysis can be successfully applied to detect
cognitive impairment and language disorders. Our
work further extends this line of investigation with
analysis of the lexical differences between patients
suffering from schizophrenia and healthy controls.
Prior work has reported on characteristic
language peculiarities exhibited by schizophrenia
patients. There are more repetitions in speech
of patients compared to controls (Manschreck et
al., 1985). Patients also tend to repeatedly refer
back to themselves (Andreasen., 1986). Deviations
from normal language use in patients on different
levels, including phonetics and syntax, have been
documented (Covington et al2005), however
lexical differences have not been investigated in
detail.
In this paper we introduce a dataset of
autobiographical narratives told by schizophrenic
patients and by healthy controls. The narratives
are related to emotional personal experiences of the
subjects for five basic emotions: ANGER, SAD,
HAPPY, DISGUST, FEAR. We train an SVM
classifier to predict subject status. Our good results
on the relatively small dataset indicate the potential
of the approach. An automatic system for predicting
patient status from autobiographical narratives can
aid psychiatrists in tracking patients over time and
can serve as an easy way to administer large
scale screening. The detailed feature analysis we
performed also pinpoints key differences between
the two populations.
We study a range of lexical features including
individual words, repetitions as well as classes
of words defined in specialized dictionaries
compiled by psychologists (Section 4). We use
several approaches for feature analysis to identify
statistically significant differences in the two
populations. There are 169 significant features
among all of the 6057 features we examined.
Through feature selection we are able to obtain a
small set of 25 highly predictive features which
lead to status classification accuracy significantly
better than chance (Section 6.3). We also show
that differences between patients and controls are
revealed best in stories related to SAD and ANGRY
narratives, they are decent in HAPPY stories, and
that distinctions are poor for DISGUST and FEAR
(Section 6.5).
37
2 Related Work
Research in psychometrics has studied patterns
of lexical usage in a large variety of scenarios.
A popular tool used for psychometric analysis
is Linguistic Inquiry and Word Count (LIWC)
(Pennebaker et al2007). One of the most
interesting discoveries in that line of research is that
people with physical or emotional pain are likely to
use first-person singular pronouns more often than
the general population (Rude et al2004). In the
view of therapy, Pennebaker discovered that writing
emotional experiences can be helpful in therapeutic
process (Pennebaker, 1997). It has also been shown
that the usage of pronouns and function words can
be indicators of writing styles, physical health and
other distinctions (Tausczik and Pennebaker, 2010).
The combination of natural language processing
(NLP) and machine learning (ML) has been
explored in many psychology related projects,
and is gaining popularity. It has been shown
that features from language models (LMs) can
be used to detect impairment in monolingual
and bilingual children (Gabani et al2009).
Even better results are achieved when features
derived from LMs are combined with other surface
features to predict language impairment. Similarly,
studies on child language development and autism
have shown that n-gram cross-entropy from LMs
representative of healthy and impaired subjects is
a highly significant feature predictive of language
impairment (Prud?hommeaux et al2011). The
feasibility of making use of lexical features
to analyze language dominance among bilingual
children has also been confirmed (Solorio et al
2011).
In non-medically related research, LIWC and
lexical features have been used to recognize
different personalities such as introvert vs extrovert,
openness vs experience, conscientiousness vs
unconscientiousness, etc. (Mairesse et al2007).
Similar features have been applied to differentiate
author personality of e-mails (Gill et al2006),
blogs (Gill et al2009) and other documents.
Speech-related features and interactional aspects
of dialog behavior such as pauses, fillers, etc,
have also been found helpful in identifying autistic
patients (Heeman et al2010).
Variables Schizophrenia Control
(# Subjects) (n=23) (n=16)
Mean age (SD) 33.81 (9.65) 32.29 (6.59)
Mean number of
words per story (SD) 192.22 (122.4) 180.79 (95.87)
Table 1: Basic demographic information
Syntax features have been used in approaches
of automatic detection of neurological problems.
Parsing texts produced by subjects and using
bag of rules as features have been applied in
analyzing language dominance (Solorio et al
2011). Methods that quantify syntactic complexity
like Yngve score and Fraizer score have been used
to analyze autism (Prud?hommeaux et al2011).
Moreover, there has been research on detecting mild
cognitive impairment, which could be an earlier
state of Alzheimer?s disease: five different ways
of evaluating syntactic complexity measures were
introduced in their paper (Roark et al2011).
In our own work, we focus our analysis
exclusively on lexical features. Similarly to prior
work, we present the most significant features
related to differences between schizophrenic
patients and healthy controls. Unlike prior work,
instead of doing class ablation studies we perform
feature selection from the full set of available
features and identify a small set of highly predictive
features which are sufficient to achieve the top
performance we report. Such targeted analysis
is more helpful for medical professionals as they
search to develop new therapies and ways to track
patient status between visits.
3 Data
For our experiments we collected autobiographical
narratives from 39 speakers. The speakers are
asked to tell their experience involving the following
emotions: HAPPY, ANGER, SAD, FEAR and
DISGUST, which comprise the set of the five basic
emotions (Cowie, 2000). Most subjects told a single
story for each of the emotions, some told two. The
total number of stories in the dataset is 201.
The stories were narrated in the doctor?s office.
The recordings of the narratives were manually
transcribed in plain text format. We show age and
length in words of the told stories for the two groups
38
in Table 1. There are 23 patients with schizophrenia
and 16 healthy controls, telling 120 and 81 stories
respectively.
4 Features
Here we introduce the large set of lexical features
that we group in three classes: a large class of
features computed for individual lexical items, basic
features, features derived on the basis of pre-existing
dictionaries and language model features. We also
detail the way we performed feature normalization
and feature selection.
4.1 Surface Features
4.1.1 Basic Features
Basic features include token to type ratio to
capture vocabulary diversity, letters per word, words
per sentence, sentences per document and words
per document. These features describe the general
properties of the language used by the subject,
without focus on specific words.
Repetitions, revisions, large amount of fillers
or disfluencies can be indicators for language
impairment. In our basic features we detect the
number of repetitions in words, punctuations and
sentences for each transcript. Then these three
measures are normalized by total number of words
or sentences.
We define repetitions as the occurrence of the
same token in a sliding window of five items
within the same sentence. We count repetitions of
words and punctuation separately. The repetition
of punctuation, mostly commas and full-stops, are
indicative of phrasing in speech which has been
indirectly captured in the transcript. Repetition of
any word is counted, regardless of which specific
word was repeated. For example, for the sentence I
am, am, afraid, that something bad would happen.
am is counted as repeated once, and comma is
counted as repeated twice. Finally, sentence
repetition captures the amount of overlapping at the
beginning of two adjacent sentences, defined as the
number of tokens from the beginning of the sentence
until the first token where the two sentences differ.
4.1.2 Lexical Features
For words in the vocabulary: we use a real
value feature equal to the word frequency for each
document. Of particular interest we track the use
of pronouns because early research has reported that
people with cognitive impairment have a tendency
to use subjective words or referring to themselves
(Rude et al2004).
In addition, for each word in the vocabulary,
we apply the presence of the repetition about one
particular word.
4.1.3 Perplexity from Language Models
Inspired by the predictive power of language
model reported in prior work, we also include
several language model features. We build language
models on words as well as part-of-speech (POS)
tags from Stanford POS-tagger (Toutanova et al
2003). We tried unigram, bigram and trigram
language models by word and POS tag. Experiments
showed that bigram performed better than random,
and the other two performed below random. Thus
in the experiments we report later we train one
model for patients and one for controls and use the
perplexity of a given text according to the bigram
language models on word and POS as features in
prediction.
4.2 Dictionaries: LIWC and Diction
Text analysis packages have been widely used in
research related to personality analysis, sentimental
analysis and psychometric studies. We use two
dictionary-based systems, LIWC (Pennebaker et al
2007)1 and Diction2, which both give scores to
transcripts based on broad categories.
4.2.1 Linguistic Inquiry&Word Count(LIWC)
LIWC calculates the degree to which people use
different categories of words. Several manually
compiled dictionaries are at the heart of the
application. Each word or word stem could be in
one or more word categories or sub-dictionaries.
For instance, the word ?cried? is part of the
following categories: sadness, negative emotion,
overall affect, verb, and past tense verb. When
a narrative contains the word ?cried?, the scale
scores corresponding to these five subcategories are
incremented. The final output for each narrative is a
real value score for each of the 69 categories.
1See http://www.liwc.net
2See http://www.dictionsoftware.com
39
Because of the elaborate development of
dictionaries and categories, LIWC has been used
for predicting emotional and cognitive problems
from subject?s spoken and written samples.
Representative applications include studying
attention focus through personal pronouns, studying
honesty and deception by emotion words and
exclusive words and identifying thinking styles
(Tausczik and Pennebaker, 2010). Thus it is
reasonable to expect that LIWC derived features
would be helpful in identifying schizophrenia
patients. In Section 6.4 we discuss in more detail
the features which turned out to be significantly
different between patients and controls within
LIWC.
4.2.2 Diction
We also use Diction to analyze the lexical
characteristics of the transcripts. Similar to
LIWC, Diction scores are computed with reference
to manually compiled dictionaries. The master
variable scores in Diction include activity, certainty,
commonality, optimism and realism. These five
main scores are computed with 33 dictionaries that
define pertinent subcategories. The master variable
scores are constructed as follows: Sm =
?n
i=1 ai ?
?m
j=1 sj , where ai are additive traits, sj are
subtractive traits (giving positive/negative evidence
for the presence of the feature, respectively).
For example, Certainty and Realism scores are
calculated as follows:
Realism = [Familiarity + Spatial Awareness +
Temporal Awareness + Present Concern + Human
Interest + Concreteness] - [Past Concern +
Complexity]
Certainty = [Tenacity + Leveling + Collectives +
Insistence] - [Numerical Terms + Ambivalence +
Self Reference + Variety]
We also give definitions for some important
categories. The complete description of categories
is available in the Diction manual (Hart, 2000).
Cognition: Words referring to cerebral processes,
both functional and imaginative.
Satisfaction: Terms associated with positive
affective states.
Insistence: A measure of code-restriction and
contentedness, with the assumption that the
repetition of key terms indicates a preference for a
limited, ordered world.
Diversity: Words describing individuals or groups
of individuals differing from the norm.
Familiarity: Consisted of the most common words
in English.
Certainty: Language indicating resoluteness,
inflexibility, and completeness and a tendency to
speak ex cathedra.
Realism: Language describing tangible, immediate,
recognizable matters that affect people?s everyday
lives.
4.3 Feature normalization
We use two feature normalization approaches:
projection normalization and binary normalization.
Both of the two approaches are applied to basic
features, dictionary features and word features. As
for repetition, we don?t use normalization, because
it is in itself binary. For transcript i, we denote
the value of the jth feature as vij . We denote
minj , maxj , averagej as the minimum, maximum
and average value for each feature in the training
corpus, respectively. Thus for each feature j,
we have: averagej = 1n
?n
i=1 vij minj =
mini{vij},maxj = maxi{vij}.
4.3.1 Projection Normalization
Here we simply normalize all feature values to a
range of [0, 1], where 0 corresponds to the smallest
observed value and 1 to the largest observed value
across all transcripts. Then we could have pij =
vij?minj
maxj?minj , where pij is the feature value after
normalization.
4.3.2 Binary normalization
Here all features are converted to binary values,
reflecting whether the value falls below or above the
average value for that feature observed in training.
The value pij of j-th feature for the i-th instance is
as below:
pij =
{
0 vij < 1n
?n
i=1 vij
1 otherwise
4.3.3 Prediction on the Test Set
All of the previous values, averagej , maxj and
minj are derived from the training set. While
doing classification, for a new testing instance, we
denote the feature vector as f = (f1, f2, . . . fn).
40
fj is then compared with averagej to do binary
normalization. We also use pj = fj?minjmaxj?minj to do
projection normalization. If pj < 0, we change pj
into 0; if pj > 1, we change pj into 1. For the
words or features that are not seen in training, we
just ignore this dimension.
4.4 Feature selection
All lexically based analysis is plagued by data
sparsity problems. In the medical domain this
problem is even more acute because collecting
patient data is difficult. The number of features
we defined outnumbers our samples by orders
of magnitude. Therefore, in our classification
procedure, we perform feature selection by doing
two-sided T-test to compare the values of features
in the patient and control groups. The features with
p-value ? 0.05 are considered as indicative and are
selected for later machine learning experiments, in
which 169 out of 6057 features have been selected.
We discuss the significant features in the full set in
Section 6.4 .
Note however that we don?t use the features
selected on the full dataset for machine learning
experiments because when T-tests are applied
on the full dataset feature selection decisions
would include information about the test set as
well. Therefore, we adopt a leave-one-subject-out
(LOSO) evaluation approach instead. In each
iteration, we set aside one subject as test set. The
data from the remaining subjects form the training
set. Feature selection is done on the training set only
and a model is trained. The predictions are tested on
the held out subject. The procedure is repeated for
every subject as test set.
The choice of p-value cut-off allows us to relax
and tighten the requirement on significance of the
features and thus the size of the feature set. We
report results with different p-values in Table 3.
We also explore alternative feature ranking and
feature selection procedures in Section 6.3. In
each fold different features may be selected. For
ease of discussing feature differences we present
a discussion of the 169 significant features on the
entire dataset.
5 Our approach
The goal of our system is to classify the person who
told a story in one of two categories: Schizophrenia
group (SC) and Control group (CO). In order to
do this, we give labels to the stories told by each
subject. Therefore we could use our model to
identify the status of the person who told each
individual story, the task is to answer the question
?Was the subject who told this story a patient or
control??. Then we combine the predictions for
stories to predict status of each subject, and the
task becomes answering the question ?Is this subject
a patient or control given that they told these five
stories??. Thus in story level prediction we use no
information about the fact that subjects told more
than one story, while in subject-level prediction we
do use this information.
First we present an experiment that relies only
on language models for the prediction. Then we
present the complete learning-based system that
uses the full set of features. Finally, we describe
the decision making approach to combine the story
level predictions to derive a subject-level prediction.
5.1 Language Model
Language models have been used previously for
language impairment on children (Gabani et al
2009) and language dominance prediction (Solorio
et al2011). Patients with speaking disorder
or cognitive impairment express themselves in
atypical ways. Language models (LMs) give a
straightforward way of estimating the probability
of the productions of a given subject. We expect
that the approach would be useful for the study of
schizophrenia as well and so start with a description
of the LM experiments.
We use LMs on words to recognize the difference
between patients and controls in vocabulary use.
We also trained a LM on POS tags because
it could reduce sparsity and focus more on
grammatical patterns. Two separate LMs are
trained on transcripts of schizophrenia and controls
respectively, using leave-one-subject-out protocol.
Story-level decisions are made by assigning the
class whose language model yields lower perplexity:
s(t) =
{
SC PERSC(t) ? PERCO(t)
CO otherwise
41
by Story (%) SC-F CO-F Accuracy Macro-F
Random 54.4 44.6 50.0 49.5
Majority 74.8 0.0 59.7 37.4
2-gram 62.5 44.4 55.2 53.5
2-gram-Pos 62.2 53.3 58.2 57.8
by Subject (%) SC-F CO-F Accuracy Macro-F
Random 54.1 45.1 50.0 49.6
Majority 74.2 0.0 59.1 37.1
2-gram 65.2 50.0 58.9 57.6
2-gram-Pos 66.7 54.5 61.5 60.6
Table 2: Language model performance
Here t means a transcript from a subject, while
PERSC and PERCO are perplexities for patients
and controls, respectively. We experimented with
unigram, bigram and trigram LMs on words and
POS tags. Laplace smoothing is used when
generating word probabilities.
5.2 Classification Phase
Language models are convenient because they
summarize information from patterns in lexical and
POS use into a single number. However, most of the
successful applications of LMs require large amount
of training data while our dataset is relatively small.
Moreover, we would like to analyze more specific
differences between the patient and control group
and this would be more appropriately done using a
larger set of features.
We have described our features and feature
selection process in Section 4. We use SVM-light
(Joachims, 1999) for our machine learning
algorithm, as its effectiveness has been proved in
various learning-based clinical tasks compared to
other classifiers (Gabani et al2009) .
5.3 Status Decision
Story level predictions are made for each transcript
either based on LM perplexity or SVM prediction.
The most intuitive way to obtain a subject-level
prediction is by voting from story-level predictions
between the stories told by the particular subject.
The subject-level prediction is simply set to equal
the majority prediction from individual stories. On
the few occasions where there are equal votes for
schizophrenia and control, the system makes a
preference towards schizophrenia, because it is more
P-value cut-off by Story by Subject # Features
0.15 59.0 58.9 450
0.10 61.7 64.1 341
0.05 62.7 64.1 169
0.01 57.7 65.4 44
0.005 64.2 71.6 32
0.001 65.7 75.6 18
0.0005 61.7 66.7 14
Table 3: Performance by subject after T-test feature
selection in different confidence levels.
dangerous to omit a potential patient.
6 Experiments and Results
We perform our experiments on the 201 transcripts
of the 39 speakers. The two baselines we
compare with are doing random assignments and
majority class, which for our datasets correspond to
predicting all subjects into the Schizophrenia group.
We report precision, recall and F-measure for
both patient and control groups, as well as overall
accuracy and Macro-F value. We get predictions
in leave-one-subject-out fashion and compute the
results over the complete set of predictions.
6.1 Language Model Performance
Our first experiment relies only on the perplexity
from language models to make the prediction.
We use the 1,2,3-gram models on word and POS
sequences. From the result in Table 2 we can
see bigram LM performed better than random
baseline for both story and subject level prediction.
3-gram and 1-gram LM did not give a credible
performance, with results worse than that of the
baselines. Because of space constraints we do not
report the specific numbers.
6.2 Classification Result after Feature Selection
Next we evaluate the performance of classification
with different number of features from the classes
we define in Section 4. As discussed above, we
performed feature selection by choosing different
levels of significance for the p-value cut-off. Feature
selection is performed 39 times for each LOSO
training fold. On the standard cut-off p-value ?
0.05, our system could achieve 62.7% accuracy on
story and 64.1% on patient level prediction. The best
performance is achieved when the cut-off p-value is
42
Schizophrenia Control General
Measurement P (%) R (%) F (%) P (%) R (%) F (%) Accuracy (%) Macro-F (%)
Story Random 59.7 50.0 54.4 40.5 50.0 44.6 50.0 49.5
Majority 59.7 100.0 74.8 NA 0.0 0.0 (NA) 59.7 37.4
25-Features 68.7 75.0 71.7 57.1 49.4 52.9 64.7 62.3
Subject Random 59.0 50.0 54.1 41.0 50.0 45.0 50.0 49.6
Majority 59.0 100.0 74.2 NA 0.0 0.0 (NA) 59.0 37.1
25-Features 75.0 91.3 82.4 81.8 56.3 66.7 76.9 74.6
Table 4: Performance on best feature-set by feature ranking using signal to noise
stricter, 0.001, where an accuracy of 75.6% can be
reached. In this case only about 18 features are used
for the classification. Detailed results are shown in
Table 3.
6.3 Performance with Different Feature Size
Next we investigate the relationship between feature
set size and accuracy of prediction. We are
interested in identifying the smallest possible set
of features which gives performance close to the
one reported on the full set of significant features.
Narrowing the feature set as much as possible will
be most useful for clinicians as they understand
the differences between the groups and look for
indicators of the illness they need to track during
regular patient visits. Physicians and psychologists
are also interested to know the most significant
lexical differences revealed by the stories.
As an alternative to ranking features by p-value,
we use the Challenge Learning Object Package
(CLOP) 3 (Guyon et al2006) . It is a toolkit
with a combination of preprocessing and feature
selection. We experiment with signal-to-noise (s2n),
Gram-Schmidt orthogonalization and Recursive
Feature Elimination for finding a subset of indicative
features (Guyon and Elisseeff, 2003). The
signal-to-noise method gives better results than the
other two by at least 6% for the top performance
feature set. Thus we pick the best k features
according to the s2n result and use only those k
features for classification.
Figure 1 shows how prediction accuracy changes
with feature sets of different sizes. From the plot
we clearly see that our top performance is achieved
with 25 to 40 features, after which performance
drops. The peak performance is achieved when
3See http://clopinet.com/CLOP/
Figure 1: Story and Subject prediction accuracy
there are 25 features, where we could reach 75.0%
precision, 91.3% recall, 82.4% F-measure for
patient, and 76.9% accuracy for overall, as shown
in Table 4. Detailed information about the top
30 features can be found in Table 5. ?+? and ?-?
means more prevalent for patient and control, while
?prj? and ?01? correspond to the two normalization
approaches in Section 4.3, projection and binary
respectively.
6.4 Analysis of Significant Features
In this section we discuss the specific features that
were revealed as most predictive by the feature
selection methods that we employed. We have seen
that it only requires about 25-40 features to obtain
peak performance.
First we briefly review the features that turned
out to be statistically significant (for 0.05 p-value
cut-off). Table 7 provides a list of the features
with higher values for Schizophrenia and Control
respectively. 4 We group the significant features
according to the feature classes we introduced in
4LM1 is defined as the ratio of CO perplexity and
SC perplexity from LMs, LM7 comes from projection
normalization of LM1. If LM perplexity for CO is smaller than
that of SC, then we set LM3 as 1; otherwise we set LM4 as 1.
43
Rank Feature Category P-value
1 Prj-Self + Diction 5.33E-06
2 01-Self + Diction 7.34E-06
3 Prj-punctuation - Basic 1.33E-05
4 01-I + LIWC 2.73E-05
5 01-sorry - Lexical 0.007
6 01-money + Lexical 6.95E-05
7 01-punctuation - Basic 4.88E-05
8 prj-I + LIWC 5.12E-05
9 01-extremely + Lexical 5.10E-05
10 prj-mildly + Lexical 0.0006
11 prj-sorry - Lexical 0.011
12 prj-I + Lexical 0.0002
13 LM1 + LM 0.0002
14 LM7 + LM 0.0002
15 I + Repeat 0.0003
Rank Feature Category P-value
16 and + Repeat 0.0002
17 01-mildly + Lexical 0.0004
18 prj-adverb - LIWC 0.0006
19 01-relationship - Lexical 0.024
20 01-late - Lexical 0.024
21 prj-comma - Lexical 0.001
22 Repeat word - Basic 0.001
23 prj-late - Lexical 0.034
24 prj-very - Lexical 0.007
25 prj-extremely + Lexical 0.001
26 01-couldn?t + Lexical 0.001
27 prj-relationship - Lexical 0.037
28 very - Repeat 0.007
29 prj-? + Lexical 0.002
30 prj-moderately + Lexical 0.006
Table 5: Table of the top 30 features by signal-to-noise ranking
Section 4. Of the 169 significant features, 111 are
more prevalent in patients, 58 are more prevalent
among the controls. If a feature was significant with
both normalizations we use, we list it only once in
Table 7.
Among the words indicative of schizophrenia,
subjective words such as I and LIWC category
self are among the most significant. This finding
conforms with prior research that patients with
mental disorders refer to themselves more often than
regular people. Patients produce more questions (as
indicated by the significance of the question mark
as a feature). It is possible that this indicates a
disruption in their thought process and they forget
what they are talking about. Further work will be
needed to understand this difference better.
In terms of words, patients talked more about
money, trouble, and used adverbs like moderately
and basically. Repetition in language is also a
revealing characteristic of the patient narratives.
There is a substantial difference in the appearance
of repetitions between the two groups, as well as
repetition of specific words: I, and, and repetition
of filled pauses um. As patients focus more on their
own feelings, they talked a lot about their family,
using words such as son, grandfather and even dogs.
Diction features revealed some unexpected
differences. The schizophrenia group scores
higher in the Self, Cognition, Past, Insistence and
Satisfaction categories. This indicates that they are
more likely to talk about past experience, using
cognitive terms and having a repetition of key
terms. We were particularly curious to understand
why patients score higher on Satisfaction ratings.
On closer inspection we discovered that patients?
stories were rated higher in Satisfaction when
they were telling SAD stories. This finding has
important clinical implications because one of the
diagnostic elements for the disease is inappropriate
emotion expression. Our study is the first to apply
an automatic measure to detect such anomaly in
patients? emotional narratives. Prompted by this
discovery, we take a closer look at the interaction
between the emotion expressed in a story and the
accuracy of status prediction in the next section.
The control group exhibited more word
complexity, sentence complexity and thoughtfulness
in their stories. They use more adverbs and exclusive
words (e.g. but, without, exclude) on general trend.
They use the word sorry significantly more often
than patients.
6.5 Status Prediction by Emotion
We also investigate if classification accuracy differs
depending on the type of conveyed emotion.
Accuracy per emotion with three feature selection
methods is shown in Table 6. When using
signal-to-noise, we can see that on SAD stories the
two groups can be distinguished better. Story-level
accuracies on HAPPY stories reach 72.5%, and
that the accuracy on HAPPY stories is the next
highest one. When applying the 0.05 p-value
cut-off to select significant features, ANGER stories
become the ones for which the status of a subject
44
Accuracy (%) s2n (25) T-test (0.05) T-test (0.001)
Happy 66.7 59.0 71.8
Disgust 63.4 61.0 51.2
Anger 61.0 70.7 70.7
Fear 60.0 55.0 67.5
Sad 72.5 60.0 67.5
Story 64.7 62.9 65.7
Patient 76.9 64.1 74.4
Majority 59.0 59.0 59.0
Table 6: Accuracy per emotion by different feature-sets
can be predicted most accurately. Using the
threshold of 0.001 for selection gives the best overall
prediction. In that case, HAPPY and ANGER are
the emotions for which recognition is best. The
changes in the recognition accuracy depending on
feature selection suggests that in future studies it
may be more beneficial to perform feature selection
only on stories from a given type because obviously
indicative features exist at least for the SAD, ANGER
and HAPPY stories.
Regardless of the feature selection approach, it
is more difficult to tell the two groups apart when
they tell DISGUST and FEAR stories. These results
seem to indicate that when talking about certain
emotions patients and controls look much more alike
than when other emotions are concerned. Future
data acquisition efforts can focus only on collecting
autobiographical narratives relevant to the emotions
for which patients and controls differ most.
Figure 2: Number of significant features by P-value
selection on different thresholds (per emotion)
In future work we would like to use only stories
from a given emotion to classify between patients
Types Significant features more common in SCH
Basic repeat-word, sentence/document
LIWC I, insight, personal-pronoun
Diction self, cognition, past, insistence, satisfaction
Lexical ?, ain?t, alone, at, aw, become, before, behind
care, chance, confused, couldn?t, December, dog
dogs, extreme, extremely, feeling, forty, friends
god, got, grandfather, guess, guy, hand, hanging
hearing, hundred, increased, looking, loved
mental, met, mild, mildly, moderate, moderately
money, my, myself, outside, paper, passed, piece
remember, sister, son, stand, step, story, take
taken, throwing, took, trouble, use, wake
wanna, way
Repeat a, and, I, um, was
LM LM1, LM4, LM7
Types Significant features more common in CO
Basic length/word, words/sentence
LIWC ?6-letters, adverb, exclusive words, inhibitive
Diction certainty, cooperation, diversity
familiarity, realism
Lexical ?,?, able, actually, are, basically, be, being, get?s
in, late, not, really, relationship, result, she?s
sleep, sorry, tell, their, there?s, very, weeks
Repeat very, ?,?
LM LM3
Table 7: Significant features (p-value ? 0.05)
and controls. Doing this with our current dataset
is not feasible because there are only about 40
transcripts per emotion. Therefore, we use our
data to identify significant features that distinguish
patients from controls only on narratives from a
particular emotion. For example, we compare the
differences of SAD stories told by patients and
controls. We count the number of significant
features between patients and controls with 11
different p-value cut-offs, and provide a plot that
visualizes the results in Figure 2. From the graph,
it is clear that there are many more differences
between the two groups in ANGER and SAD
narratives. HAPPY comes next, then DISGUST and
FEAR. However, at lower confidence levels, HAPPY
has equal number of significant features as ANGER
and SAD, which is in line with the result in Table 6.
The feature analysis performed by emotion
reveals more differences between patients and
controls, beyond common features such as self,
I, etc. For HAPPY stories, patients talk more
about their friends and relatives; they also have a
45
higher tendency of being ambivalent. For DISGUST
stories, patients are more disgusted with dogs, and
they talk more about health. The control group
shows a higher communication score, referring to
a better social interaction. ANGER is one of the
emotions that best reveals the differences between
groups, and schizophrenia patients show more
aggression and cognition while talking, according
to features derived from Diction. The control
group sometimes talks more about praise. In FEAR
stories patients talk about money more often than
controls. Meanwhile, the control group uses more
inhibition words, for instance: block, constrain and
stop. An interesting phenomenon happens in SAD
narratives. When talking about sad experiences,
patients sometimes show satisfaction and insistence,
while the controls talked more about working
experiences.
7 Conclusion
In this paper, we analyzed the predictive power
of different kinds of features for distinguishing
schizophrenia patients from healthy controls. We
provided an in-depth analysis of features that
distinguish patients from controls and showed that
the type of emotion conveyed by the personal
narratives is important for the distinction and that
stories for different emotions give different sets
indicators for subject status. We report classification
results as high as 76.9% on the subject level,
with 75.0% precision and 91.3% on recall for
schizophrenia patients.
We consider the results presented here to be
a pilot study. We are currently collecting and
transcribing additional stories from the two groups
which we would like to use as a definitive test
set to verify the stability of our findings. We
plan to explore syntactic and coherence models to
analyze the stories, as well as emotion analysis of
the narratives.
References
Nancy C. Andreasen. 1986. Scale for the assessment
of thought, language, and communication (TLC).
Schizophrenia Bulletin, 12:473 ? 482.
Michael A. Covington, Congzhou He, Cati Brown,
Lorina Naci, Jonathan T. McClain, Bess Sirmon
Fjordbak, James Semple, and John Brown. 2005.
Schizophrenia and the structure of language: The
linguist?s view. Schizophrenia Research, 77(1):85 ?
98.
Roddy Cowie. 2000. Describing the emotional states
expressed in speech. In Proceedings of the ISCA
Workshop on Speech and Emotion.
Keyur Gabani, Melissa Sherman, Thamar Solorio, Yang
Liu, Lisa Bedore, and Elizabeth Pen?a. 2009.
A corpus-based approach for the prediction of
language impairment in monolingual english and
spanish-english bilingual children. In Proceedings of
HLT-NAACL, pages 46?55.
Alastair J. Gill, Jon Oberlander, and Elizabeth Austin.
2006. Rating e-mail personality at zero acquaintance.
Personality and Individual Differences, 40(3):497 ?
507.
Alastair J. Gill, Scott Nowson, and Jon Oberlander. 2009.
What are they blogging about? personality, topic and
motivation in blogs. In Proceedings of the AAAI
ICWSM?09.
Isabelle Guyon and Andre? Elisseeff. 2003. An
introduction to variable and feature selection. J. Mach.
Learn. Res., 3:1157?1182, March.
Isabelle Guyon, Jiwen Li, Theodor Mader, Patrick A.
Pletscher, Georg Schneider, and Markus Uhr. 2006.
Feature selection with the CLOP package. Technical
report, http://clopinet.com/isabelle/Projects/ETH/
TM-fextract-class.pdf.
Rodrick Hart. 2000. Diction 5.0, the text-analysis
program user?s manual, Scolari Software, Sage Press.
http://www.dictionsoftware.com/.
Peter A. Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois M. Black, and Jan P. H. van Santen. 2010.
Autism and interactional aspects of dialogue. In
Proceedings of the SIGDIAL 2010 Conference, pages
249?252.
T. Joachims. 1999. Making large?scale SVM learning
practical. In B. Scho?lkopf, C. J. C. Burges, and
A. J. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning, pages 169?184, Cambridge,
MA. MIT Press.
F. Mairesse, M. A. Walker, M. R. Mehl, and R. K.
Moore. 2007. Using Linguistic Cues for the
Automatic Recognition of Personality in Conversation
and Text. Journal of Artificial Intelligence Research,
30:457?500.
Theo C. Manschreck, Brendan A. Maher, Toni M.
Hoover, and Donna Ames. 1985. Repetition in
schizophrenic speech. Language & Speech, 28(3):255
? 268.
J.W. Pennebaker, R.J. Booth, and Francis. 2007.
Linguistic inquiry and word count (LIWC
46
2007): A text analysis program. Austin, Texas.
http://www.liwc.net/.
James W. Pennebaker. 1997. Writing about Emotional
Experiences as a Therapeutic Process. Psychological
Science, 8(3):162?166.
Emily T. Prud?hommeaux, Brian Roark, Lois M. Black,
and Jan van Santen. 2011. Classification of atypical
language in autism. In Proceedings of the 2nd
Workshop on Cognitive Modeling and Computational
Linguistics, CMCL?11, pages 88?96.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011.
Spoken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Audio,
Speech & Language Processing, 19(7):2081?2090.
Stephanie Rude, Eva-Maria Gortner, and James
Pennebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition &
Emotion, 18(8):1121?1133.
Thamar Solorio, Melissa Sherman, Y. Liu, Lisa Bedore,
Elizabeth Pen?a, and A. Iglesias. 2011. Analyzing
language samples of spanish-english bilingual children
for the automated prediction of language dominance.
Natural Language Engineering, 17(3):367?395.
Yla R. Tausczik and James W. Pennebaker. 2010.
The Psychological Meaning of Words: LIWC and
Computerized Text Analysis Methods. Journal
of Language and Social Psychology, 29(1):24?54,
March.
Kristina Toutanova, Dan Klein, and Christopher D.
Manning. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of
HLT-NAACL 03.
47
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1157?1168, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A coherence model based on syntactic patterns
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Abstract
We introduce a model of coherence which
captures the intentional discourse structure in
text. Our work is based on the hypothesis that
syntax provides a proxy for the communica-
tive goal of a sentence and therefore the se-
quence of sentences in a coherent discourse
should exhibit detectable structural patterns.
Results show that our method has high dis-
criminating power for separating out coherent
and incoherent news articles reaching accura-
cies of up to 90%. We also show that our syn-
tactic patterns are correlated with manual an-
notations of intentional structure for academic
conference articles and can successfully pre-
dict the coherence of abstract, introduction
and related work sections of these articles.
1 Introduction
Recent studies have introduced successful automatic
methods to predict the structure and coherence of
texts. They include entity approaches for local co-
herence which track the repetition and syntactic re-
alization of entities in adjacent sentences (Barzilay
and Lapata, 2008; Elsner and Charniak, 2008) and
content approaches for global coherence which view
texts as a sequence of topics, each characterized by a
particular distribution of lexical items (Barzilay and
Lee, 2004; Fung and Ngai, 2006). Other work has
shown that co-occurrence of words (Lapata, 2003;
Soricut and Marcu, 2006) and discourse relations
(Pitler and Nenkova, 2008; Lin et al 2011) also pre-
dict coherence.
Early theories (Grosz and Sidner, 1986) posited
that there are three factors which collectively con-
tribute to coherence: intentional structure (purpose
of discourse), attentional structure (what items are
discussed) and the organization of discourse seg-
ments. The highly successful entity approaches cap-
ture attentional structure and content approaches are
related to topic segments but intentional structure
has largely been neglected. Every discourse has a
purpose: explaining a concept, narrating an event,
critiquing an idea and so on. As a result each sen-
tence in the article has a communicative goal and the
sequence of goals helps the author achieve the dis-
course purpose. In this work, we introduce a model
to capture coherence from the intentional structure
dimension. Our key proposal is that syntactic pat-
terns are a useful proxy for intentional structure.
This idea is motivated from the fact that cer-
tain sentence types such as questions and definitions
have distinguishable and unique syntactic structure.
For example, consider the opening sentences of two
descriptive articles1 shown in Table 1. Sentences
(1a) and (2a) are typical instances of definition sen-
tences. Definitions are written with the concept to
be defined expressed as a noun phrase followed by
a copular verb (is/are). The predicate contains two
parts: the first is a noun phrase reporting the concept
as part of a larger class (eg. an aqueduct is a water
supply), the second component is a relative clause
listing unique properties of the concept. These are
examples of syntactic patterns related to the com-
municative goals of individual sentences. Similarly,
sentences (1b) and (2b) which provide further de-
tails about the concept also have some distinguish-
1Wikipedia articles on ?Aqueduct? and ?Cytokine Recep-
tors?
1157
1a) An aqueduct is a water supply or navigable channel
constructed to convey water.
b) In modern engineering, the term is used for any system
of pipes, canals, tunnels, and other structures used for
this purpose.
2a) Cytokine receptors are receptors that binds cytokines.
b) In recent years, the cytokine receptors have come to
demand more attention because their deficiency has now been
directly linked to certain debilitating immunodeficiency states.
Table 1: The first two sentences of two descriptive arti-
cles
ing syntactic features such as the presence of a top-
icalized phrase providing the focus of the sentence.
The two sets of sentences have similar sequence of
communicative goals and so we can expect the syn-
tax of adjacent sentences to also be related.
We aim to characterize this relationship on a
broad scale using a coherence model based entirely
on syntax. The model relies on two assumptions
which summarize our intuitions about syntax and in-
tentional structure:
1. Sentences with similar syntax are likely to have
the same communicative goal.
2. Regularities in intentional structure will be
manifested in syntactic regularities between ad-
jacent sentences.
There is also evidence from recent work that sup-
ports these assumptions. Cheung and Penn (2010)
find that a better syntactic parse of a sentence can be
derived when the syntax of adjacent sentences is also
taken into account. Lin et al(2009) report that the
syntactic productions in adjacent sentences are pow-
erful features for predicting which discourse relation
(cause, contrast, etc.) holds between them. Cocco et
al. (2011) show that significant associations exist be-
tween certain part of speech tags and sentence types
such as explanation, dialog and argumentation.
In our model, syntax is represented either as parse
tree productions or a sequence of phrasal nodes aug-
mented with part of speech tags. Our best perform-
ing method uses a Hidden Markov Model to learn
the patterns in these syntactic items. Sections 3 and
5 discuss the representations and their specific im-
plementations and relative advantages. Results show
that syntax models can distinguish coherent and in-
coherent news articles from two domains with 75-
90% accuracies over a 50% baseline. In addition,
the syntax coherence scores turn out complementary
to scores given by lexical and entity models.
We also study our models? predictions on aca-
demic articles, a genre where intentional structure
is widely studied. Sections in these articles have
well-defined purposes and we find recurring sen-
tence types such as motivation, citations, descrip-
tion, and speculations. There is a large body of work
(Swales, 1990; Teufel et al 1999; Liakata et al
2010) concerned with defining and annotating these
sentence types (called zones) in conference articles.
In Section 6, we describe how indeed some patterns
captured by the syntax-based models are correlated
with zone categories that were proposed in prior lit-
erature. We also present results on coherence pre-
diction: our model can distinguish the introduction
section of conference papers from its perturbed ver-
sions with over 70% accuracy. Further, our model
is able to identify conference from workshop papers
with good accuracies, given that we can expect these
articles to vary in purpose.
2 Evidence for syntactic coherence
We first present a pilot study that confirms that ad-
jacent sentences in discourse exhibit stable patterns
of syntactic co-occurrence. This study validates our
second assumption relating the syntax of adjacent
sentences. Later in Section 6, we examine syntac-
tic patterns in individual sentences (assumption 1)
using a corpus of academic articles where sentences
were manually annotated with communicative goals.
Prior work has reported that certain grammatical
productions are repeated in adjacent sentences more
often than would be expected by chance (Reitter et
al., 2006; Cheung and Penn, 2010). We analyze all
co-occurrence patterns rather than just repetitions.
We use the gold standard parse trees from the
Penn Treebank (Marcus et al 1994). Our unit of
analysis is a pair of adjacent sentences (S1, S2) and
we choose to use Section 0 of the corpus which has
99 documents and 1727 sentence pairs. We enumer-
ate all productions that appear in the syntactic parse
of any sentence and exclude those that appear less
than 25 times, resulting in a list of 197 unique pro-
ductions. Then all ordered pairs2 (p1, p2) of pro-
ductions are formed. For each pair, we compute
2(p1, p2) and (p2, p1) are considered as different pairs.
1158
p1, p2 Sentence 1 Sentence 2
NP? NP NP-ADV The two concerns said they entered into a definitive Also on the takeover front, Jaguar?s ADRs rose
QP? CD CD merger agreement under which Ratners will begin a tender 1/4 to 13 7/8 on turnover of [4.4 million]QP.
offer for all of Weisfield?s common shares for [$57.50 each]NP.
VP? VB VP ?The refund pool may not [be held hostage through another? [Commonwealth Edison]NP-SBJ said it is already
NP-SBJ? NNP NNP round of appeals]VP,? Judge Curry said. appealing the underlying commission order and
is considering appealing Judge Curry?s order.
NP-LOC? NNP ?It has to be considered as an additional risk for the investor,? [?Cray Computer will be a concept?
S-TPC-1? NP-SBJ VP said Gary P. Smaby of Smaby Group Inc., [Minneapolis]NP-LOC. ?stock,?]S-TPC-1 he said.
Table 2: Example sentences for preferred production sequences. The span of the LHS of the corresponding production
is indicated by [] braces.
the following: c(p1p2) = number of sentence pairs
where p1 ? S1 and p2 ? S2; c(p1?p2) = num-
ber of pairs where p1 ? S1 and p2 6? S2; c(?p1p2)
and c(?p1?p2) are computed similarly. Then we
perform a chi-square test to understand if the ob-
served count c(p1p2) is significantly (95% confi-
dence level) greater or lesser than the expected value
if occurrences of p1 and p2 were independent.
Of the 38,809 production pairs, we found that
1,168 pairs occurred in consecutive sentences sig-
nificantly more often than chance and 172 appeared
significantly fewer times than expected. In Table 2
we list, grouped in three simple categories, the 25
pairs of the first kind with most significant p-values.
Some of the preferred pairs are indeed repetitions
as pointed out by prior work. But they form only a
small fraction (5%) of the total preferred production
pairs indicating that there are several other classes
of syntactic regularities beyond priming. Some of
these other sequences can be explained by the fact
that these articles come from the finance domain:
they involve productions containing numbers and
quantities. An example for this type is shown in Ta-
ble 2. Finally, there is also a class that is not repe-
titions or readily observed as domain-specific. The
most frequent one reflects a pattern where the first
sentence introduces a subject and predicate and the
subject in the second sentence is pronominalized.
Examples for two other patterns are given in Table
2. For the sequence (VP ? VB VP | NP-SBJ ? NNP
NNP), a bare verb is present in S1 and is often asso-
ciated with modals. In the corpus, these statements
often present hypothesis or speculation. The follow-
ing sentence S2 has an entity, a person or organiza-
tion, giving an explanation or opinion on the state-
ment. This pattern roughly correponds to a SPECU-
LATE followed by ENDORSE sequence of intentions.
p1 p2 c(p1p2)
? Repetition ?
VP? VBD SBAR VP? VBD SBAR 83
QP? $ CD CD QP? $ CD CD 18
NP? $ CD -NONE- NP? $ CD -NONE- 16
NP? QP -NONE- NP? QP -NONE- 15
NP-ADV? DT NN NP-ADV? DT NN 10
NP? NP NP-ADV NP? NP NP-ADV 7
? Quantities/Amounts ?
NP? QP -NONE- QP? $ CD CD 16
QP? $ CD CD NP? QP -NONE- 15
NP? NP NP-ADV NP? QP -NONE- 11
NP-ADV? DT NN NP? QP -NONE- 11
NP? NP NP-ADV NP-ADV? DT NN 9
NP? $ CD -NONE- NP-ADV? DT NN 8
NP-ADV? DT NN NP? $ CD -NONE- 8
NP-ADV? DT NN NP? NP NP-ADV 8
NP? NP NP-ADV QP? CD CD 6
? Other ?
S? NP-SBJ VP NP-SBJ? PRP 290
VP? VBD SBAR PP-TMP? IN NP 79
S? NP-SBJ-1 VP VP? VBD SBAR 43
VP? VBD NP VP? VBD VP 31
VP? VB VP NP-SBJ? NNP NNP 27
NP-SBJ-1? NNP NNP VP? VBD NP 13
VP? VBZ NP S? PP-TMP , NP-SBJ VP . 8
NP-SBJ? JJ NNS VP? VBP NP 8
NP-PRD? NP PP NP-PRD? NP SBAR 7
NP-LOC? NNP S-TPC-1? NP-SBJ VP 6
Table 3: Top patterns in productions from WSJ
Similarly, in all the six adjacent sentence pairs from
our corpus containing the items (NP-LOC ? NNP | S-
TPC-1 ? NP-SBJ VP), p1 introduces a location name,
and is often associated with the title of a person or
organization. The next sentence has a quote from
that person, where the quotation forms the topical-
ized clause in p2. Here the intentional structure is
INTRODUCE X / STATEMENT BY X.
In the remainder of the paper we formalize our
representation of syntax and the derived model of
coherence and test its efficacy in three domains.
3 Coherence models using syntax
We first describe the two representations of sentence
structure we adopted for our analysis.3 Next, we
3Our representations are similar to features used for rerank-
ing in parsing. Our first representation corresponds to ?rules?
features (Charniak and Johnson, 2005; Collins and Koo, 2005),
and our second representation is related to ?spines? (Carreras et
al., 2008) and edge annotation(Huang, 2008).
1159
present two coherence models: a local model which
captures the co-occurrence of structural features in
adjacent sentences and a global one which learns
from clusters of sentences with similar syntax.
3.1 Representing syntax
Our models rely exclusively on syntactic cues. We
derive representations from constituent parses of the
sentences, and terminals (words) are removed from
the parse tree before any processing is done. The
leaf nodes in our parse trees are part of speech tags.
Productions: In this representation we view each
sentence as the set of grammatical productions, LHS
? RHS, which appear in the parse of the sen-
tence. As we already pointed out, the right-hand side
(RHS) contains only non-terminal nodes. This rep-
resentation is straightforward, however, some pro-
ductions can be rather specific with long right hand
sides. Another apparent drawback of this represen-
tation is that it contains sequence information only
about nodes that belong to the same constituent.
d-sequence: In this representation we aim to pre-
serve more sequence information about adjacent
constituents in the sentence. The simplest approach
would be to represent the sentence as the sequence
of part of speech (POS) tags but then we lose all
the abstraction provided by higher level nodes in
tree. Instead, we introduce a more general represen-
tation, d-sequence where the level of abstraction can
be controlled using a parameter d. The parse tree is
truncated to depth at most d, and the leaves of the
resulting tree listed left to right form the d-sequence
representation. For example, in Figure 1, the line
depicts the cutoff at depth 2.
Next the representation is further augmented; all
phrasal nodes in the d-sequence are annotated (con-
catenated) with the left-most leaf that they domi-
nate in the full non-lexicalized parse tree. This is
shown as suffixes on the S, NP and VP nodes in
the figure. Such annotation conveys richer informa-
tion about the structure of the subtree below nodes
in the d-sequence. For example, ?the chairs?, ?his
chairs?, ?comfortable chairs? will be represented as
NPDT, NPPRP$ and NPJJ. In the resulting representa-
tions, sentences are viewed as sequences of syntactic
words (w1,w2...,wk), k ? p, where p is the length of
the full POS sequence and each wi is either POS tag
or a phrasal node+POS tag combination.
Figure 1: Example for d-sequence representation
In our example, at depth-2, the quotation sentence
gets the representation (w1=? , w2=SDT , w3=, , w4=? ,
w5=NPNNP , w6=VPVBD , w7=.) where the actual quote
is omitted. Sentences that contain attributions are
likely to appear more similar to each other when
compared using this representation in contrast to
representations derived from word or POS sequence.
The depth-3 sequence is also indicated in the figure.
The main verb of a sentence is central to its struc-
ture, so the parameter d is always set to be greater
than that of the main verb and is tuned to optimize
performance for coherence prediction.
3.2 Implementing the model
We adapt two models of coherence to operate over
the two syntactic representations.
3.2.1 Local co-occurrence model
This model is a direct extension from our pilot
study. It allows us to test the assumption that coher-
ent discourse is characterized by syntactic regulari-
ties in adjacent sentences. We estimate the proba-
bilities of pairs of syntactic items from adjacent sen-
tences in the training data and use these probabilities
to compute the coherence of new texts.
The coherence of a text T containing n sentences
(S1...Sn) is computed as:
P (T ) =
n?
i=2
|Si|?
j=1
1
|Si?1|
|Si?1|?
k=1
p(Sji |S
k
i?1)
where Syx indicates the yth item of Sx. Items
are either productions or syntactic word unigrams
depending on the representation. The conditional
probabilities are computed with smoothing:
1160
Cluster a Cluster b
ADJP ? JJ PP | VP ? VBZ ADJP VP ? VB VP | VP ? MD VP
[1] This method VP-[is ADJP-[capable of sequence-specific [1] Our results for the difference in reactivity VP-[can
detection of DNA with high accuracy]-ADJP]-VP . VP-[be linked to experimental observations]-VP]-VP .
[2] The same VP-[is ADJP-[true for synthetic polyamines [2] These phenomena taken together VP-[can VP-[be considered
such as polyallylamine]-ADJP]-VP . as the signature of the gelation process]-VP]-VP .
Table 4: Example syntactic similarity clusters. The top two descriptive productions for each cluster are also listed.
p(wj |wi) =
c(wi, wj) + ?C
c(wi) + ?C ? |V |
wherewi andwj are syntactic items and c(wi, wj) is
the number of sentences that contain the item wi im-
mediately followed by a sentence that contains wj .
|V | is the vocabulary size for syntactic items.
3.2.2 Global structure
Now we turn to a global coherence approach
that implements the assumption that sentences with
similar syntax have the same communicative goal
as well as captures the patterns in communicative
goals in the discourse. This approach uses a Hid-
den Markov Model (HMM) which has been a popu-
lar implementation for modeling coherence (Barzi-
lay and Lee, 2004; Fung and Ngai, 2006; Elsner
et al 2007). The hidden states in our model de-
pict communicative goals by encoding a probability
distribution over syntactic items. This distribution
gives higher weight to syntactic items that are more
likely for that communicative goal. Transitions be-
tween states record the common patterns in inten-
tional structure for the domain.
In this syntax-HMM, states hk are created by
clustering the sentences from the documents in the
training set by syntactic similarity. For the pro-
ductions representation of syntax, the features for
clustering are the number of times a given produc-
tion appeared in the parse of the sentence. For the
d-sequence approach, the features are n-grams of
size one to four of syntactic words from the se-
quence. Clustering was done by optimizing for av-
erage cosine similarity and was implemented using
the CLUTO toolkit (Zhao et al 2005). C clusters
are formed and taken as the states of the model. Ta-
ble 4 shows sentences from two clusters formed on
the abstracts of journal articles using the productions
representation. One of them, cluster (a), appears
to capture descriptive sentences and cluster (b) in-
volves mostly speculation type sentences.
The emission probabilities for each state are mod-
eled as a (syntactic) language model derived from
the sentences in it. For productions representa-
tion, this is the unigram distribution of produc-
tions from the sentences in hk. For d-sequences,
the distribution is computed for bigrams of syntac-
tic words. These language models use Lidstone
smoothing with constant ?E . The probability for a
sentence Sl to be generated from state hk, pE(Sl|hk)
is computed using these syntactic language models.
The transition probability pM from a state hi to
state hj is computed as:
pM (hj |hi) =
d(hi, hj) + ?M
d(hi) + ?M ? C
where d(hi) is the number of documents whose sen-
tences appear in hi and d(hi, hj) is the number of
documents which have a sentence in hi which is im-
mediately followed by a sentence in hj . In addi-
tion to the C states, we add one initial hS and one
final hF state to capture document beginning and
end. Transitions from hS to any state hk records
how likely it is for hk to be the starting state for doc-
uments of that domain. ?M is a smoothing constant.
The likelihood of a text with n sentences is given
by P (T ) =
?
h1...hn
?n
t=1 pM (ht|ht?1)pE(St|ht).
All model parameters?the number of clusters
C, smoothing constants ?C , ?E , ?M and d for
d-sequences?are tuned to optimize how well the
model can distinguish coherent from incoherent ar-
ticles. We describe these settings in Section 5.1.
4 Content and entity grid models
We compare the syntax model with content model
and entity grid methods. These approaches are the
most popular ones from prior work and also allow
us to test the complementary nature of syntax with
1161
lexical statistics and entity structure. This section
explains how we implemented these approaches.
Content models introduced by Barzilay and Lee
(2004) and Fung and Ngai (2006) use lexically
driven HMMs to capture coherence. The hidden
states represent the topics of the domain and en-
code a probability distribution over words. Transi-
tions between states record the probable succession
of topics. We built a content model using our HMM
implementation. Clusters are created using word bi-
gram features after replacing numbers and proper
names with tags NUM and PROP. The emissions are
given by a bigram language model on words from
the clustered sentences. Barzilay and Lee (2004)
also employ an iterative clustering procedure before
finalizing the states of the HMM but our method
only uses one-step clustering. Despite the differ-
ence, the content model accuracies for our imple-
mentation are quite close to that from the original.
For the entity grid model, we follow the gen-
erative approach proposed by Lapata and Barzilay
(2005). A text is converted into a matrix, where rows
correspond to sentences, in the order in which they
appear in the article. Columns are created one for
each entity appearing in the text. Each cell (i,j) is
filled with the grammatical role ri,j of the entity j
in sentence i. We computed the entity grids using
the Brown Coherence Toolkit4. The probability of
the text (T ) is defined using the likely sequence of
grammatical role transitions.
P (T ) =
m?
j=1
n?
i=1
p(ri,j |ri?1,j ...ri?h,j)
for m entities and n sentences. Parameter h controls
the history size for transitions and is tuned during
development. When h = 1, for example, only the
grammatical role for the entity in the previous sen-
tence is considered and earlier roles are ignored.
5 Evaluating syntactic coherence
We follow the common approach from prior work
and use pairs of articles, where one has the original
document order and the other is a random permuta-
tion of the sentences from the same document. Since
the original article is always more coherent than a
random permutation, a model can be evaluated using
4http://www.cs.brown.edu/~melsner/manual.html
the accuracy with which it can identify the original
article in the pair, i.e. it assigns higher probability
to the original article. This setting is not ideal but
has become the de facto standard for evaluation of
coherence models (Barzilay and Lee, 2004; Elsner
et al 2007; Barzilay and Lapata, 2008; Karamanis
et al 2009; Lin et al 2011; Elsner and Charniak,
2011). It is however based on a reasonable assump-
tion as recent work (Lin et al 2011) shows that peo-
ple identify the original article as more coherent than
its permutations with over 90% accuracy and asses-
sors also have high agreement. Later, we present
an experiment distinguishing conference from work-
shop articles as a more realistic evaluation.
We use two corpora that are widely employed for
coherence prediction (Barzilay and Lee, 2004; El-
sner et al 2007; Barzilay and Lapata, 2008; Lin et
al., 2011). One contains reports on airplane acci-
dents from the National Transportation Safety Board
and the other has reports about earthquakes from the
Associated Press. These articles are about 10 sen-
tences long. These corpora were chosen since within
each dataset, the articles have the same intentional
structure. Further, these corpora are also standard
ones used in prior work on lexical, entity and dis-
course relation based coherence models. Later in
Section 6, we show that the models perform well on
the academic genre and longer articles too.
For each of the two corpora, we have 100 arti-
cles for training and 100 (accidents) and 99 (earth-
quakes) for testing. A maximum of 20 random per-
mutations were generated for each test article to cre-
ate the pairwise data (total of 1986 test pairs for the
accident corpus and 1956 for earthquakes).5 The
baseline accuracy for random prediction is 50%.
The articles were parsed using the Stanford parser
(Klein and Manning, 2003).
5.1 Accuracy of the syntax model
For each model, the relevant parameters were tuned
using 10-fold cross validation on the training data.
In each fold, 90 documents were used for training
and evaluation was done on permutations from the
remaining articles. After tuning, the final model was
trained on all 100 articles in the training set.
5We downloaded the permutations from http://people.
csail.mit.edu/regina/coherence/CLsubmission/
1162
Table 5 shows the results on the test set. The
best number of clusters and depth for d-sequences
are also indicated. Overall, the syntax models work
quite well, with accuracies at least 15% or more ab-
solute improvement over the baseline.
In the local co-occurrence approach, both pro-
ductions and d-sequences provide 72% accuracy for
the accidents corpus. For the earthquake corpus,
the accuracies are lower and the d-sequence method
works better. The best depth setting for d-sequence
is rather small: depth of main verb (MVP) + 2 (or 1),
and indicates that a fairly abstract level of nodes is
preferred for the patterns. For comparison, we also
provide results using just the POS tags in the model
and this is worse than the d-sequence approach.
The global HMM model is better than the local
model for each representation type giving 2 to 38%
better accuracies. Here we see a different trend for
the d-sequence representation, with better results for
greater depths. At such depths (8 and 9) below the
main verb, the nodes are mostly POS tags.
Overall both productions and d-sequence work
competitively and give the best accuracies when im-
plemented with the global approach.
5.2 Comparison with other approaches
For our implementations of the content and entity
grid models, the best accuracies are 71% on the ac-
cidents corpus and 85% on the earthquakes one, sim-
ilar to the syntactic models.
Ideally, we would like to combine models but we
do not have separate training data. So we perform
the following classification experiment which com-
bines the predictions made by different models on
the test set. Each test pair (article and permutation)
forms one example and is given a class value of 0 or
1 depending on whether the first article in the pair
is the original one or the second one. The example
is represented as an n-dimensional vector, where n
is the number of models we wish to combine. For
instance, to combine content models and entity grid,
two features are created: one of these records the dif-
ference in log probabilities for the two articles from
the content model, the other feature indicates the dif-
ference in probabilities from the entity grid.
A logistic regression classifier is trained to pre-
dict the class using these features. The test pairs are
created such that an equal number of examples have
Model Accidents Earthquake
Parameter Acc Parameter Acc
A. Local co-occurrence
Prodns 72.8 55.0
d-seq dep. MVP+2 71.8 dep. MVP+1 65.1
POS 61.3 42.6
B. HMM-syntax
Prodns clus. 37 74.6 clus. 5 93.8
d-seq dep. MVP+8 82.2 dep. MVP+9 86.5
clus. 8 clus. 45
C. Other approaches
Egrid history 1 67.6 history 1 82.2
Content clus. 48 71.4 clus. 23 84.5
Table 5: Accuracies on accident and earthquake corpora
Model Accid. Earthq.
Content + Egrid 76.8 90.7
Content + HMM-prodn 74.2 95.3
Content + HMM-d-seq 82.1 90.3
Egrid + HMM-prodn 79.6 93.9
Egrid + HMM-d-seq 84.2 91.1
Egrid + Content + HMM-prodn 79.5 95.0
Egrid + Content + HMM-d-seq 84.1 92.3
Egrid + Content + HMM-prodn 83.6 95.7
+ HMM-d-seq
Table 6: Accuracies for combined approaches
class 0 and 1, so the baseline accuracy is 50%. We
run this experiment using 10-fold cross validation on
the test set after first obtaining the log probabilities
from individual models. In each fold, the training is
done using the pairs from 90 articles and tested on
permutations from the remaining 10 articles. These
accuracies are reported in Table 6. When the accu-
racy of a combination is better than that using any of
its smaller subsets, the value is bolded.
We find that syntax supplements both content and
entity grid methods. While on the airplane corpus
syntax only combines well with the entity grid, on
the earthquake corpus, both entity and content ap-
proaches give better accuracies when combined with
syntax. However, adding all three approaches does
not outperform combinations of any two of them.
This result can be due to the simple approach that
we tested for combination. In prior work, content
and entity grid methods have been combined gen-
eratively (Elsner et al 2007) and using discrimina-
tive training with different objectives (Soricut and
1163
Marcu, 2006). Such approaches might bring out
the complementary strengths of the different aspects
better and we leave such analysis for future work.
6 Predictions on academic articles
The distinctive intentional structure of academic ar-
ticles has motivated several proposals to define and
annotate the communicative purpose (argumentative
zone) of each sentence (Swales, 1990; Teufel et al
1999; Liakata et al 2010). Supervised classifiers
were also built to identify these zones (Teufel and
Moens, 2000; Guo et al 2011). So we expect that
these articles form a good testbed for our models. In
the remainder of the paper, we examine how unsu-
pervised patterns discovered by our approach relate
to zones and how well our models predict coherence
for articles from this genre.
We employ two corpora of scientific articles.
ART Corpus: contains a set of 225 Chemistry jour-
nal articles that were manually annotated for inten-
tional structure (Liakata and Soldatova, 2008). Each
sentence was assigned one of 11 zone labels: Result,
Conclusion, Objective, Method, Goal, Background,
Observation, Experiment, Motivation, Model, Hy-
pothesis. For our study, we use the annotation of
the introduction and the abstract sections. We divide
the data into training, development and test sets. For
abstracts, we have 75, 50 and 100 for these sets re-
spectively. For introductions, this split is 75, 31, 82.6
ACL Anthology Network (AAN) Corpus: Radev
et al(2009) provides the full text of publications
from ACL venues. These articles do not have any
zone annotations. The AAN corpus is produced
from OCR analysis and no section marking is avail-
able. To recreate these, we use the Parscit tagger7
(Councill et al 2008). We use articles from years
1999 to 2011. For training, we randomly choose 70
articles from ACL and NAACL main conferences.
Similarly, we obtain a development corpus of 36
ACL-NAACL articles. We create two test sets: one
has 500 ACL-NAACL conference articles and an-
other has 500 articles from ACL-sponsored work-
shops. We only choose articles in which all three
sections?abstract, introduction and related work?
6Some articles did not have labelled ?introduction? sections
resulting in fewer examples for this setup.
7http://aye.comp.nus.edu.sg/parsCit/
could be successfully identified using Parscit.8
This data was sentence-segmented using MxTer-
minator (Reynar and Ratnaparkhi, 1997) and parsed
with the Stanford Parser (Klein and Manning, 2003).
For each corpus and each section, we train all our
syntactic models: the two local coherence models
using the production and d-sequence representations
and the HMM models with the two representations.
These models are tuned on the respective develop-
ment data, on the task of differentiating the original
from a permuted section. For this purpose, we cre-
ated a maximum of 30 permutations per article.
6.1 Comparison with ART Corpus zones
We perform this analysis using the ART corpus. The
zone annotations present in this corpus allow us to
directly test our first assumption in this work, that
sentences with similar syntax have the same com-
municative goal.
For this analysis, we use the the HMM-prod
model for abstracts and the HMM-d-seq model for
introductions. These models were chosen because
they gave the best performance on the ART corpus
development sets.9 We examine the clusters cre-
ated by these models on the training data and check
whether there are clusters which strongly involve
sentences from some particular annotated zone.
For each possible pair of cluster and zone (Ci,
Zj), we compute c(Ci, Zj): the number of sentences
in Ci that are annotated as zone Zj . Then we use a
chi-square test to identify pairs for which c(Ci, Zj)
is significantly greater than expected (there is a ?pos-
itive? association between Ci and Zj) and pairs
where c(Ci, Zj) is significantly less than chance (Ci
is not associated with Zj). A 95% confidence level
was used to determine significance.
The HMM-prod model for abstracts has 9 clusters
(named Clus0 to 8) and the HMM-d-seq model for
introductions has 6 clusters (Clus0 to 5). The pair-
ings of these clusters with zones which turned out to
be significant are reported in Table 7. We also re-
port for each positively associated cluster-zone pair,
the following numbers: matches c(Ci, Zj), preci-
sion c(Ci, Zj)/|Ci| and recall c(Ci, Zj)/|Zj |.
8We also exclude introduction and related work sections
longer than 50 sentences and those shorter than 4 sentences
since they often have inaccurate section boundaries.
9Their test accuracies are reported in the next section.
1164
Abstracts (HMM-prod 9 clusters)
Positive associations matches prec. recall
Clus5 - Model 7 17.1 43.8
Clus7 - Objective 27 27.6 32.9
Clus7 - Goal 16 16.3 55.2
Clus0 - Conclusion 15 50.0 12.1
Clus6 - Conclusion 27 51.9 21.8
Not associated: Clus7 - Conclusion,
Clus8 - Conclusion
Introductions (HMM-d-seq 6 clusters)
Positive associations matches prec. recall
Clus2-Background 161 64.9 14.2
Clus3-Objective 37 7.9 38.5
Clus4-Goal 29 9.8 32.6
Clus4-Hypothesis 12 4.1 52.2
Clus5-Motivation 61 12.9 37.4
Not associated: Clus1 - Motivation, Clus2 - Goal,
Clus4 - Background, Clus 5 - Model
Table 7: Cluster-Zone mappings on the ART Corpus
The presence of significant associations validate
our intuitions that syntax provides clues about com-
municative goals. Some clusters overwhelmingly
contain the same zone, indicated by high precision,
for example 64% of sentences in Clus2 from intro-
duction sections are background sentences. Other
clusters have high recall of a zone, 55% of all goal
sentences from the abstracts training data is captured
by Clus7. It is particularly interesting to see that
Clus7 of abstracts captures both objective and goal
zone sentences and for introductions, Clus4 is a mix
of hypothesis and goal sentences which intuitively
are closely related categories.
6.2 Original versus permuted sections
We also explore the accuracy of the syntax models
for predicting coherence of articles from the test set
of ART corpus and the 500 test articles from ACL-
NAACL conferences. We use the same experimen-
tal setup as before and create pairs of original and
permuted versions of the test articles. We created a
maximum of 20 permutations for each article. The
baseline accuracy is 50% as before.
For the ART corpus, we also built an oracle model
of annotated zones. We train a first order Markov
Chain to record the sequence of zones in the training
articles. For testing, we assume that the oracle zone
is provided for each sentence and use the model to
predict the likelihood of the zone sequence. Results
from this model represent an upper bound because
an accurate hypothesis of the communicative goal is
available for each sentence.
The accuracies are presented in Table 8. Overall,
the HMM-d-seq model provides the best accuracies.
The highest results are obtained for ACL introduc-
tion sections (74%). These results are lower than
that obtained on the earthquake/accident corpus but
the task here is much harder: the articles are longer
and the ACL corpus also has OCR errors which af-
fect sentence segmentation and parsing accuracies.
When the oracle zones are known, the accuracies are
much higher on the ART corpus indicating that the
intentional structure of academic articles is very pre-
dictive of their coherence.
6.3 Conference versus workshop papers
Finally, we test whether the syntax-based model can
distinguish the structure of conference from work-
shop articles. Conferences publish more complete
and tested work and workshops often present pre-
liminary studies. Workshops are also venues to dis-
cuss a focused and specialized topic. So the way
information is conveyed in the abstracts and intro-
ductions would vary in these articles.
We perform this analysis on the ACL corpus and
no permutations are used, only the original text of
the 500 articles each in the conference and work-
shop test sets. While permutation examples provide
cheap training/test data, they have a few unrealistic
properties. For example, both original and permuted
articles have the same length. Further some permu-
tations could result in an outstandingly incoherent
sample which is easily distinguished from the origi-
nal articles. So we use the conference versus work-
shop task as another evaluation of our model.
We designed a classification experiment for this
task which combines features from the different syn-
tax models that were trained on the ACL conference
training set. We include four features indicating the
perplexity of an article under each model (Local-
prod, Local-d-seq, HMM-prod, HMM-d-seq). We
use perplexity rather than probability because the
length of the articles vary widely in contrast to the
previous permutation-based tests, where both per-
mutation and original article have the same length.
We compute perplexity as P (T )?1/n, where n is
the number of words in the article. We also obtain
the most likely state sequence for the article under
1165
Data Section Test pairs Local-prod Local-d-seq HMM-prod HMM-d-seq Oracle zones
ART Corpus
Abstract 1633 57.0 52.9 64.1 55.0 80.8
Intro 1640 44.5 54.6 58.1 64.6 94.0
ACL Conference
Abstract 8815 44.0 47.2 58.2 63.7
Intro 9966 54.5 53.0 64.4 74.0
Rel. wk. 10,000 54.6 54.4 57.3 67.3
Table 8: Accuracy in differentiating permutation from original sections on ACL and ART test sets.
HMM-prod and HMM-d-seq models using Viterbi
decoding. Then the proportion of sentences from
each state of the two models are added as features.
We also add some fine-grained features from the
local model. We represent sentences in the train-
ing set as either productions or d-sequence items and
compute pairs of associated items (xi, xj) from ad-
jacent sentences using the same chi-square test as
in our pilot study. The most significant (lowest p-
values) 30 pairs (each for production and d-seq) are
taken as features.10 For a test article, we compute
features that represent how often each pair is present
in the article such that xi is in Sm and xj is in Sm+1.
We perform this experiment for each section and
there are about 90 to 140 features for the different
sections. We cast the problem as a binary classifi-
cation task: conference articles belong to one class
and workshop to the other. Each class has 500 ar-
ticles and so the baseline random accuracy is 50%.
We perform 10-fold cross validation using logistic
regression. Our results were 59.3% accuracy for dis-
tinguishing abstracts of conference verus workshop,
50.3% for introductions and 55.4% for related work.
For abstracts and related work, these accuracies are
significantly better than baseline (95% confidence
level from a two-sided paired t-test comparing the
accuracies from the 10 folds). It is possible that in-
troductions in either case, talk in general about the
field and importance of the problem addressed and
hence have similar structure.
Our accuracies are not as high as on permutations
examples because the task is clearly harder. It may
also be the case that the prediction is more difficult
for certain papers than for others. So we also ana-
lyze our results by the confidence provided by the
classifier for the predicted class. We consider only
the examples predicted above a certain confidence
level and compute the accuracy on these predictions.
10A cutoff is applied such that the pair was seen at least 25
times in the training data.
Conf. Abstract Intro Rel wk
>= 0.5 59.3 (100.0) 50.3 (100.0) 55.4 (100.0)
>= 0.6 63.8 (67.2) 50.8 (71.1) 58.6 (75.9)
>= 0.7 67.2 (32.0) 54.4 (38.6) 63.3 (52.8)
>= 0.8 74.0 (10.0) 51.6 (22.0) 63.0 (25.7)
>= 0.9 91.7 (2.0) 30.6 (5.0) 68.1 (7.2)
Table 9: Accuracy (% examples) above each confidence
level for the conference versus workshop task.
These results are shown in Table 9. The proportion
of examples under each setting is also indicated.
When only examples above 0.6 confidence are ex-
amined, the classifier has a higher accuracy of 63.8%
for abstracts and covers close to 70% of the exam-
ples. Similarly, when a cutoff of 0.7 is applied to the
confidence for predicting related work sections, we
achieve 63.3% accuracy for 53% of examples. So
we can consider that 30 to 47% of the examples in
the two sections respectively are harder to tell apart.
Interestingly however even high confidence predic-
tions on introductions remain incorrect.
These results show that our model can success-
fully distinguish the structure of articles beyond just
clearly incoherent permutation examples.
7 Conclusion
Our work is the first to develop an unsupervised
model for intentional structure and to show that
it has good accuracy for coherence prediction and
also complements entity and lexical structure of dis-
course. This result raises interesting questions about
how patterns captured by these different coherence
metrics vary and how they can be combined usefully
for predicting coherence. We plan to explore these
ideas in future work. We also want to analyze genre
differences to understand if the strength of these co-
herence dimensions varies with genre.
Acknowledgements
This work is partially supported by a Google re-
search grant and NSF CAREER 0953445 award.
1166
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL-HLT, pages 113?120.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL, pages 9?16.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180.
Jackie C.K. Cheung and Gerald Penn. 2010. Utilizing
extra-sentential context for parsing. In Proceedings of
EMNLP, pages 23?33.
Christelle Cocco, Raphae?l Pittier, Franc?ois Bavaud, and
Aris Xanthos. 2011. Segmentation and clustering of
textual sequences: a typological approach. In Pro-
ceedings of RANLP, pages 427?433.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25?70.
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
Parscit: An open-source crf reference string parsing
package. In Proceedings of LREC, pages 661?667.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
HLT, Short Papers, pages 41?44.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of ACL-HLT, pages 125?129.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL-HLT, pages
436?443.
Pascale Fung and Grace Ngai. 2006. One story, one
flow: Hidden markov story models for multilingual
multidocument summarization. ACM Transactions on
Speech and Language Processing, 3(2):1?16.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 3(12):175?204.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of
EMNLP, pages 273?283.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-HLT, pages 586?594, June.
Nikiforos Karamanis, Chris Mellish, Massimo Poesio,
and Jon Oberlander. 2009. Evaluating centering for
information ordering using corpora. Computational
Linguistics, 35(1):29?46.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of IJCAI.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL, pages 545?552.
Maria Liakata and Larisa Soldatova. 2008. Guidelines
for the annotation of general scientific concepts. JISC
Project Report.
Maria Liakata, Simone Teufel, Advaith Siddharthan, and
Colin Batchelor. 2010. Corpora for the conceptualisa-
tion and zoning of scientific papers. In Proceedings of
LREC.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of EMNLP,
pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-
tomatically evaluating text coherence using discourse
relations. In Proceedings of ACL-HLT, pages 997?
1006.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of EMNLP, pages 186?195.
Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-
son, and Pradeep Muthukrishnan. 2009. A Bibliomet-
ric and Network Analysis of the field of Computational
Linguistics. Journal of the American Society for Infor-
mation Science and Technology.
David Reitter, Johanna D. Moore, and Frank Keller.
2006. Priming of Syntactic Rules in Task-Oriented
Dialogue and Spontaneous Conversation. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society, pages 685?690.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the fifth conference on
Applied natural language processing, pages 16?19.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of COLING-ACL, pages 803?810.
1167
John Swales. 1990. Genre analysis: English in academic
and research settings, volume 11. Cambridge Univer-
sity Press.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: determining intellectual attribution
in scientific text. In Proceedings of EMNLP, pages 9?
17.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of EACL,
pages 110?117.
Ying Zhao, George Karypis, and Usama Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discov-
ery, 10:141?168.
1168
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 636?644,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Verbose, Laconic or Just Right: A Simple Computational Model of
Content Appropriateness under Length Constraints
Annie Louis
?
School of Infomatics
University of Edinburgh
Edinburgh EH8 9AB
alouis@inf.ed.ac.uk
Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19103
nenkova@seas.upenn.edu
Abstract
Length constraints impose implicit re-
quirements on the type of content that
can be included in a text. Here we pro-
pose the first model to computationally as-
sess if a text deviates from these require-
ments. Specifically, our model predicts
the appropriate length for texts based on
content types present in a snippet of con-
stant length. We consider a range of fea-
tures to approximate content type, includ-
ing syntactic phrasing, constituent com-
pression probability, presence of named
entities, sentence specificity and inter-
sentence continuity. Weights for these fea-
tures are learned using a corpus of sum-
maries written by experts and on high
quality journalistic writing. During test
time, the difference between actual and
predicted length allows us to quantify text
verbosity. We use data from manual eval-
uation of summarization systems to as-
sess the verbosity scores produced by our
model. We show that the automatic ver-
bosity scores are significantly negatively
correlated with manual content quality
scores given to the summaries.
1 Introduction
In dialog, the appropriate length of a speaker turn
and the amount of detail in it are hugely influ-
enced by the pragmatic context. For example what
constitutes an appropriate answer to the question
?How was your vacation?? would be very different
when the question is asked as two acquaintances
pass each other in the corridor or right after two
friends have ordered dinner at a restaurant. Simi-
larly in writing, content is tailored to explicitly de-
fined or implicitly inferred constraints on the ap-
?
Work done while at University of Pennsylvania.
50 word summary:
The De Beers cartel has kept the diamond market stable
by matching supply to demand. African nations have
recently demanded better terms from the cartel. After
the Soviet breakup, De Beers contracted for diamonds
with the Yukutian Republic. The US remains the largest
diamond market, followed by Japan.
100 word summary:
The De Beers cartel, controlled by the Oppenheimer
family controls 80% of the uncut diamond market
through its Central Selling Organization. The cartel
has kept the diamond market stable by maintaining a
buffer pool of diamonds for matching supply to demand.
De Beers opened a new mine in 1992 and extended the
life of two others through underground mining.
Innovations have included automated processing and
bussing workers in daily from their homes. African
nations have recently demanded better terms. After
the Soviet breakup, De Beers contracted for diamonds
with the Yukutian Republic. The US remains the largest
diamond market, followed by Japan.
Table 1: 50 and 100 word summaries written by
the same person for the same set of documents
propriate length of text. Many academics have ex-
perienced the frustration of needing to adjust their
writing when they need to write a short abstract of
two hundred words or an answer to reviewer in no
more than five hundred words.
For a specific application-related example con-
sider the texts in Table 1. These are summaries of
a set of news articles discussing the De Beers di-
amond cartel, written by the same person.
1
The
first text is written with the instruction to produce
a summary of about 50 words while the latter is
in response to a request for a 100 word summary.
Obviously the longer summary contains more de-
tails. It doesn?t however simply extend the shorter
summary with more sentences; additional details
1
These summaries come from the Document Understand-
ing Conference dataset (year 2001).
636
are interspersed with the original shorter summary.
The performance of a range of human-machine
applications can be enhanced if they had the abil-
ity to predict the appropriate length of a system
contribution and the type of content appropriate
for that length. Such applications include docu-
ment generation (O?Donnell, 1997), soccer com-
mentator (Chen and Mooney, 2008) and question
answering with different compression rates for dif-
ferent types of questions (Kaisser et al., 2008).
Predicting the type of content appropriate for the
given length alone would be highly desirable, for
example in automatic essay grading, summariza-
tion and even in information retrieval, in which
verbose writing is particularly undesirable. In this
respect, our work supplements recent computa-
tional methods to predict varied aspects of writing
quality, such as popular writing style and phras-
ing in novels (Ganjigunte Ashok et al., 2013), sci-
ence journalism (Louis and Nenkova, 2013), and
social media content (Danescu-Niculescu-Mizil et
al., 2012; Lakkaraju et al., 2013).
Our work is the first to explore text verbosity.
We introduce a simple application-oriented defi-
nition of verbosity and a model to automatically
predict verbosity scores. We start with a brief
overview of our approach in the next section.
2 Text length and content
appropriateness
In this first model of verbosity, we do not carry
out an elaborate annotation experiment to create
labels for verbosity. There are two main reasons
for this choice: a) People find it hard to distinguish
between individual aspects of quality and often
the ratings for different aspects are highly corre-
lated (Conroy and Dang, 2008; Pitler et al., 2010)
b) Moreover, for verbosity in particular, the most
appropriate data for annotation would be concise
and verbose versions of the same text (possibly of
similar lengths). It is more likely that people can
distinguish between verbosity of these controlled
pairs compared to ratings on an individual arti-
cle. Such writing samples are not easily available.
So we have avoided the uncertainties in annotation
in this first work by adopting a simpler approach
based on three key ideas.
(i) We define a concise article of length l as ?an
article that has the appropriate types of content ex-
pected in an article of length l?. Note that length
is not equal to verbosity in our model. Our defi-
nition allows for articles of different lengths to be
considered concise. Verbosity depends on the ap-
propriateness of content for the article length.
(ii) We model this appropriateness of content
for the given length restriction via a set of easily
computable features that serve as proxies for (a)
type of content and level of detail (syntactic fea-
tures and sentence specificity) (b) sentence com-
plexity (simple readability-related features), (c)
secondary details (syntactic structures with high
compression probability) and (d) structure (dis-
course relations and inter-sentence continuity).
(iii) Forgoing any explicit annotation, we sim-
ply train the model on professionally written text
in which we assume content is appropriately tai-
lored to the length requirements. We train a re-
gression model on the well-written texts to predict
the length of an article based on a single snippet of
fixed (short) length from the article. For a new test
article, we can obtain a predicted length from this
model (length supposing the article is written con-
cisely) based on a short snippet. We use the mis-
match between the predicted and actual text length
of the article to determine if it is verbose.
We believe that this definition of verbosity has
natural uses in applications such as summariza-
tion. For example, current systems do not distin-
guish the task of summary creation for different
target lengths. They simply try to maximize esti-
mated sentence importance and to minimize repet-
itive information. They pay no attention to the fact
that the same type of sentences are unlikely to be
an optimal selection for both a 50 word and a 400
word summary.
We now briefly present the formal definition of
the problem of content appropriateness for a spec-
ified text length. Let T = (t
1
, t
2
, ...t
n
) be a collec-
tion of concisely-written texts and let l(t
i
) denote
the length of text t
i
. The learning task is to obtain
a function based on the content type properties of
t
i
which helps to predict l(t
i
). More specifically,
we are given a snippet from t
i
, called s
t
i
, of a con-
stant length k where k is a parameter of our model
and k < min
t
j
l(t
j
). The mapping f is learned
based on the constant length snippet only and the
aim is to predict the original text length.
f(s
t
i
)?
?
l(t
i
)
In our work we choose to work with topical seg-
ments from documents rather than the complete
documents themselves.
637
Once the model is trained, we identify the ver-
bosity for a test article as follows: Let us consider
a new topic segment t
x
during test time. Let the
length of the segment be l. We obtain a snippet
s
t
x
of size k from t
x
. Now assume that our model
predicts f(s
t
x
) =
?
l.
Case 1:
?
l ' l, the content type in t
x
matches
the content types generally present in articles of
length l. We consider such articles as concise.
Case 2:
?
l  l, the type of content included in
t
x
is really suitable for longer and detailed topic
segments. Thus t
x
is likely conveying too much
detail given its length i.e. it is verbose.
Case 3:
?
l  l, the content in t
x
is of the
type that a skillful writer would include in a much
shorter and less detail-oriented text. Thus t
x
is
likely lacking appropriate details (laconic).
We compute the following scores to quantify
verbosity:
Predicted length. is the model prediction
?
l.
Verbosity degree. This score is the difference
between the predicted length and the actual length
of the text,
?
l ? l. Positive values of the score indi-
cate the degree of verbosity, negative values indi-
cate that the text is laconic.
Deviation score. Since both being verbose and
being laconic is potentially problematic for text,
we define a score which does not differentiate the
type of mismatch. This score is given by the abso-
lute magnitude |
?
l ? l|.
The next section describes the features used for
indicating the content type of a snippet. In Section
4, we test the features on a four-way classification
task to predict the length of a human-written sum-
mary based on a snippet of the summary. In Sec-
tion 5, we extend our model to a regression set-
ting by learning feature weights on news articles of
varied lengths from the New York Times (NYT),
which we consider to be a sample in which content
is chosen appropriately for each article length. Fi-
nally in Section 6 we evaluate the model trained
on NYT articles on machine-produced summaries
and confirm that summaries scored with higher
verbosity by our model also receive poor content
quality scores during manual evaluation.
3 Features mapping content type to
appropriate length
We propose a diverse set of 87 features for charac-
terizing content type. These features are computed
over the constant length snippet sampled from an
article. All the syntax based features are com-
puted from the constituency trees produced from
the Stanford Parser (Klein and Manning, 2003).
Length of units (10 features).
This set of features captures basic word and
sentence length, and redundancy properties of the
snippet. It includes number of sentences, average
sentence length in words, average word length in
characters, and type to token ratio. We also in-
clude the counts of noun phrases, verb phrases
and prepositional phrases and the average length
in words of these three phrase types.
Syntactic realization (30 features).
We compute the grammatical productions in a
set of around 47,000 sentences taken from the
AQUAINT corpus (Graff, 2002) We select the
most frequent 15 productions in this set that in-
volve a description of entities, i.e the LHS (left-
hand side) of the production is a noun phrase. The
count of each of these productions is added as a
feature allowing us to track what type of informa-
tion about the entities is conveyed in the snippet.
We also add features for the most frequent 15 pro-
ductions whose LHS is not a noun phrase.
Discourse relations (5 features).
These features are based on the hypothesis that
different discourse relations would vary in their
appropriateness for articles of different lengths.
For example causal information may be included
only in more detailed texts.
We use a tool (Pitler and Nenkova, 2009) to
identify all explicit discourse connectives in our
snippets, along with the general semantic class
of the connective (temporal, comparison, contin-
gency and expansion). We use the number of dis-
course connectives of each of the four types as fea-
tures, as well as the total number of connectives.
Continuity (6 features).
These features capture the degree to which ad-
jacent sentences in the snippet are related and con-
tinue the topic. The amount of continuity for
subtopics is likely to vary for long and short texts.
We add the number of pronouns and determin-
ers as two features. Another feature is the average
word overlap value between adjacent sentences.
For computing the overlap measure, we represent
every sentence as a vector where each dimension
represents a word. The number of times the word
appears in the sentence is the value for that di-
mension. Cosine similarity is computed between
638
the vectors of adjacent sentences and the average
value of the similarity across all pairs of adjacent
sentences is the feature value.
We also run the Stanford Coreference tool
(Raghunathan et al., 2010) to identity pronoun and
entity coreference links within the snippet. The
number of total coreference links, and the number
of intra- and inter-sentence links are added as three
separate features.
Amount of detail (7 features).
To indicate descriptive words, we compute the
number of adjectives and adverbs (two features).
We also include the total number of named enti-
ties (NEs), average length of NEs in words and
the number of sentences that do not have any NEs.
The named entities were identified using the Stan-
ford NER recognition tool (Finkel et al., 2005).
We also use the predictions of a classifier
trained to identify general versus specific sen-
tences. We use a data set of general and spe-
cific sentences and features described in Louis and
Nenkova (2011) to implement a sentence speci-
ficity model. The classifier produces a binary pre-
diction and also a graded score for specificity. We
add two features?the percentage of specific sen-
tences and the average specificity score of words.
Compression likelihood (29 features).
These features use an external source of infor-
mation about content importance. Specifically, we
use data commonly employed to develop statisti-
cal models for sentence compression (Knight and
Marcu, 2002; McDonald, 2006; Galley and McK-
eown, 2007). It consists of pairs of sentences
in an original text and a professional summary
of that text. In every pair, one of the sentences
(source) appeared in the original text and the other
is a shorter version with the superfluous details
deleted. Both sentences were produced by people.
We use the dataset created by Galley and McKe-
own (2007). The sentences are taken from the Ziff
Davis Corpus which contains articles about tech-
nology products. This data also contains align-
ment between the constituency parse nodes of the
source and summary sentence pair. Through the
alignment it is possible to track nodes that where
preserved during compression.
On this data, we identify for every production
in the source sentence whether it undergoes dele-
tion in the compressed sentence. A production
(LHS ? RHS) is said to undergo deletion when
either the LHS node or any of the nodes in the
RHS do not appear in the compressed sentence.
Only productions which involve non-terminals in
the RHS are used for this analysis as lexical items
could be rather corpus-specific. The proportion
of times a production undergoes deletion is called
the deletion probability. We also incorporate fre-
quency of the production with the deletion proba-
bility to obtain a representative set of 25 produc-
tions which are frequently deleted and also occur
commonly. This deletion score is computed as:
deletion probability * log(frequency of production
in source sentences)
Parentheticals appear in the list as would be
expected and also productions involving con-
junctions, prepositional phrases and subordinate
clauses. We expect that such productions will in-
dicate the presence of details that are only appro-
priate for longer texts.
To compute the compression-related features
for a snippet, we first obtain the set of all pro-
ductions in the sentences from the snippet. We
add features that indicate the number of times each
of the top 25 ?most deleted? productions was used
in the snippet. We also use the sum, average and
product of deletion probabilities for set of snippet
productions as features. The product feature gives
the likelihood of the text being deleted. We also
add the perplexity value based on this likelihood,
P
?1/n
where P is the likelihood and n is the num-
ber of productions from the snippet for which we
have deletion information in our data.
2
For training a model, we need texts which we
can assume are written in a concise manner. We
use two sources of data?summaries written by
people and high quality news articles.
4 A classification model on expert
summaries
Here we use a collection of news summaries writ-
ten by expert analysts for four different lengths
and build a classification model to predict given
a snippet what is the length of the summary from
which the snippet was taken. This task only differ-
entiates four lengths but is a useful first approach
for testing our assumptions and features.
4.1 Data
We use human written summaries from the Doc-
ument Understanding Conference (DUC
3
) evalua-
2
Some productions may not have appeared in the Ziff
Davis Corpus.
3
http://duc.nist.gov
639
tion workshops conducted in 2001 and 2002. An
input given for summarization contains 10 to 15
documents on a topic. The person had to create
50, 100, 200 and 400 word summaries for each of
the inputs. These summary writers are retired in-
formation analysts and we can assume that their
summaries are of high quality and concise nature.
Further, the four different length summaries for an
input are produced by the same person.
4
There-
fore differences in length are not confounded by
differences in writing style of different people.
The 2001 dataset has 90 summaries for each of
the four lengths. In 2002, there are 116 summaries
for each length. All of the summaries are abstracts,
i.e. people wrote the summary in their own words,
with the exception of one set. In 2002, abstracts
were only created for 50, 100 and 200 lengths.
However, extracts created by people are available
for 400 words. In extracts, the summary writer
is only allowed to choose complete sentences (no
edits can be done), however, the sentences can be
ordered in the summary and people tend to create
coherent extractive summaries as well. Since it is
desirable to have data for another length, we also
include the 400-word extracts from the 2002 data.
4.2 Snippet selection
We choose 50 words as the snippet length for
our experiment since the length of the shortest
summaries is 50. We experiment with multiple
ways to select a snippet: the first 50 words of the
summary (START), the last 50 words (END) and
50 words starting at a randomly chosen sentence
(RANDOM). However, we do not truncate any sen-
tence in the middle to meet the constraint for 50
words. We allow a leeway of 20 words so that
snippets can range from 30 to 70 words. When a
snippet could not be created within this word limit
(eg. the summary has one sentence which is longer
than 70 words), we ignore the example.
4.3 Classification results
The task is to predict the length of the summary
from which the fixed length snippet was taken, i.e.
4-way classification?50, 100, 200 or a 400 word
summary. We trained an SVM classifier with a ra-
dial basis kernel on the 2001 data. The regulariza-
tion and kernel parameters were tuned using 10-
fold cross validation on the training set. The accu-
racies of classification on the 2002 data are shown
4
Different inputs however may be summarized by differ-
ent assessors.
snippet position accuracy
START 38.4
RANDOM 34.4
END 39.3
Table 2: Length prediction results on DUC sum-
maries
in Table 2. Since there are four equal classes, the
random baseline performance is 25%.
The START and END position snippets gave the
best accuracies, 38% and 39% which are 13-14%
absolute improvement above the baseline. At the
same time, there is much scope for improvement.
The confusion matrices showed that 50 and 400
word lengths, the extreme ones in this dataset,
were the easiest to predict. Most of the confusions
occur with the 100 and 200 word summaries.
The overall accuracy is slightly better when
snippets from the END of the summary are cho-
sen compared to those from the START. However,
with START snippets, better prediction of different
length summaries was obtained, whereas the ac-
curacy in the END case comes mainly from correct
prediction of 50 and 400 word summaries. So we
use the START selection for further experiments.
5 A regression approach based on New
York Times editorials
We next build a model where we predict a wider
range of lengths compared to just the four classes
we had before. Here our training set comprises
news articles from the New York Times (NYT)
based on the assumption that edited news from a
good source would be of high quality overall.
5.1 Data
We obtain the text of the articles from the NYT
Annotated Corpus (Sandhaus, 2008). We choose
the articles from the opinion section of the news-
paper since they are likely to have good topic con-
tinuity and related content compared to general
news which often contain lists of facts. We fur-
ther use only the editorial articles to ensure that
the articles are of high quality.
We collect 10,724 opinion articles from years
2000 to 2007 of the NYT. We divide each article
into topic segments using the unsupervised topic
segmentation method developed by Eisenstein and
Barzilay (2008). We use the following heuristic to
decide on the number of topic segments for each
article. If the article has fewer than 50 sentences,
we create segments such that the expected length
640
of a segment is 10 sentences, i.e, we assign the
number of segments as number of sentences di-
vided by 10. When the article is longer, we create
5 segments. This step gives us 18,167 topic seg-
ments, ranging in length from 14 to 773 words.
We use a stratified sampling method to select
training and test examples. Starting from 90 words
and upto a maximum length of 500 words, we di-
vide the range into bins in increments of 30 words.
From each bin we select 100 texts for training and
around 35 for testing. There are 2,100 topic seg-
ments in the training set and 681 for testing.
5.2 Training approach
We use 100 word snippets for our experiments.
We learn a linear regression model on the train-
ing data using lm function in R (R Development
Core Team, 2011). The features which turned out
significant in the model are shown in Table 3. The
significance value shown is associated with a t-test
to determine if the feature can be ignored from the
model. We report the coefficients for the signifi-
cant features under column ?Beta?. The R-squared
value of the model is 0.219.
Many of the most significant features are related
to entities. Longer texts are associated with larger
number of noun phrases but they tend not to be
proper names. Average word and sentence length
also increase with article length, at the same time,
longer articles have shorter verb phrases. Specific
sentences and determiners are also positively re-
lated to article length. At the discourse level, com-
parison relations increase with length.
5.3 Accuracy of predictions
On the test data, the lengths predicted by the
model have a Pearson correlation of 0.44 with the
true length of the topic segment. The correlation is
highly significant (p-value < 2.2e-16). The Spear-
man correlation value is 0.43 and the Kendall Tau
is 0.29, both also highly significant. These results
show that our model can distinguish content types
for a range of article lengths.
6 Text quality assessment for automatic
summaries
In the models above, we learned weights which re-
late the features to the length of concisely written
human summaries and NYT articles. Now we use
the model to compute verbosity scores and assess
Feature Beta p-value
Positive coefficients
total noun phrases 6.052e+00 ***
avg. word length 3.201e+01 ***
avg. sent. length 3.430e+00 **
avg. NP length 6.557e+00 *
no. of adverbs 4.244e+00 **
% specific sentences 4.773e+01 **
comparison relations 9.296e+00 .
determiners 2.955e+00 *
NP? NP PP 4.305e+00 *
NP? NP NP 1.174e+01 *
PP? IN S 7.268e+00 .
WHNP?WDT 1.196e+01 **
Negative coefficients
NP? NNP -8.630e+00 ***
no. of sentences -2.498e+01 **
no. of relations -1.128e+01 **
avg. VP length -2.982e+00 **
type token ratio -1.784e+02 *
NP? NP , SBAR -1.567e+01 *
NP? NP , NP -9.582e+00 *
NP? DT NN -3.423e+00 .
VP? VBD -1.189e+01 .
S? S : S . -1.951e+01 .
ADVP? RB -4.198e+00 .
Table 3: Significant regression coefficients in the
length prediction model on NYT editorials. ?***?
indicates p-value < 0.001, ?**? is p-value < 0.01,
?*? is < 0.05 and ?.? is < 0.1
how well they correlate with text quality scores as-
signed by people.
We perform this evaluation for the system sum-
maries produced during the 2006 DUC evalua-
tion workshop. There are 22 automatic systems in
that evaluation.
5
Each system produced 250 word
summaries for each of 20 multidocument inputs.
Each summary was evaluated by DUC assessors
for multiple dimensions of quality. We examine
how the verbosity predictions from our model are
related to these summary scores. In this experi-
ment, we use automatic summaries only.
6.1 Gold-standard summary scores
Two kinds of manual scores?content and linguis-
tic quality?are available for each summary from
the DUC dataset. One type of content score,
the ?pyramid score? (Nenkova et al., 2007) com-
putes the overlap of semantic units of the system
summary with that present in human-written sum-
maries for the same input. For the other content
score, called ?content responsiveness?, assessors
directly provide a rating to summaries on a scale
from 1 (very poor) to 5 (very good) without using
any reference human summaries.
5
We use only the set of systems for which pyramid scores
are also available.
641
Verbosity scores Corr. with actual length
predicted length -0.01
verbosity degree -0.29
deviation score -0.27
Table 4: Relationship between verbosity scores
and summary length
Linguistic quality is evaluated separately from
content for different aspects. Manually assigned
scores are available for non-redundancy (absence
of repetitive information), focus (well-established
topic), and coherence (good flow from sentence to
sentence). For each aspect, the summary is rated
on a scale from 1 (very poor) to 5 (very good).
This dataset is less ideal for our task in some
ways as system summaries often lack coherent
arrangement of sentences. Some of our fea-
tures which rely on coreference and adjacent sen-
tence overlaps when computed on these sum-
maries could be misleading. However, this data
contains large scale quality ratings for different
quality aspects which allow us to examine our ver-
bosity predictions across multiple dimensions.
6.2 Verbosity scores and summary quality
We choose the first 100 words of each summary
as the snippet. No topic segmentation was done
on the summary data. We use the NYT regres-
sion model to predict the expected lengths of these
summaries and compute its verbosity and devia-
tion scores as defined in Section 2.
We also compute two other measures for com-
parison.
Actual length. To understand how the ver-
bosity scores are related to the length of the sum-
mary, we also keep track of the actual number of
words present in the summary.
Redundancy score: We also add a simple score
to our analysis to indicate redundancy between ad-
jacent sentences in the summary. It is simple mea-
sure of verbosity since repetitive information leads
to lower informativeness overall. The score is the
cosine similarity based sentence overlap measure
described in Section 3.
For each of the 22 automatic systems, the scores
of its 20 summaries (one for each input) are av-
eraged. (We ignore empty summaries and those
which are much smaller than the 100 word snip-
pet that we require). We find the average val-
ues for both our verbosity based scores above
and the gold-standard scores (pyramid, content re-
sponsiveness, focus, non-redundancy and coher-
Content quality
scores Pyramid Resp.
actual length 0.64* 0.43*
predicted length -0.29 -0.11
verbosity degree -0.47* -0.23
deviation score -0.44* -0.29
redundancy score -0.01 -0.06
Linguistic quality
scores Non-red Focus Coher.
actual length -0.32 -0.25 -0.32
predicted length 0.48* 0.39
.
0.38
.
verbosity degree 0.55* 0.44* 0.46*
deviation score 0.53* 0.40
.
0.42
.
redundancy score 0.06 0.32 0.23
Table 5: Pearson correlations between verbosity
scores and gold standard summary quality scores
ence). We also compute the average value of the
summary lengths for each system.
First we examine the relationship between ver-
bosity scores and the actual summary lengths. The
Pearson correlations between the three verbosity
measures and true length of the summaries are re-
ported in Table 4. The verbosity scores are not sig-
nificantly related to summary length. They seem
to have an inverse relationship but the correlations
are not significant even at 90% confidence level.
This result supports our hypothesis that verbosity
scores based on expected length are different from
the actual summary length.
Next Table 5 presents the Pearson correlations
of the verbosity measures with gold standard sum-
mary quality scores. Since the number of points
(systems) is only 22, we indicate whether the cor-
relations are significant at two levels, 0.05 (marked
by a ?*? superscript) and 0.1 (a ?.? superscript).
The first line of the table indicates that longer
summaries are associated with higher content
scores both according to pyramid and content re-
sponsiveness evaluations. This result also supports
our hypothesis that length alone does not indicate
verbosity. Longer summaries on average have bet-
ter content quality. The length is not significantly
related to linguistic quality scores but there is a
negative relationship in general.
On the other hand, all the three verbosity scores
have a negative correlation with content scores.
The verbosity degree score is the strongest in-
dicator of summary quality with -0.47 (signifi-
cant) correlation with pyramid score. At the same
time however, verbosity is preferred for linguis-
tic quality. This effect could arise due to the fact
these summaries are bags of unordered sentences.
Therefore verbose style could be perceived as hav-
642
System 23?s summary: Actual length = 253 words, Predicted length = 343 words, Verbosity degree = 90
A senior Scotland Yard police officer apologized to the parents of a black teenager slain five years ago in a race killing
that has become the focus of debate over relations between police and ethnic minorities. Black teenager Stephen Lawrence
was stabbed to death at a bus-stop in Eltham, south London by five white youngsters six years ago. The parents of the
murdered black teenager Stephen Lawrence began legal action against the men suspected of his killing. Two suspects in the
Stephen Lawrence murder case and one other man were arrested on suspicion of theft by Kent Police. The five men
suspected of killing Stephen Lawrence were thumped and pelted with bottles by an enraged crowd Tuesday after a day of
evasive and implausible evidence that made a mockery of their appearance before the public inquiry. The dawn raids came
as police questioned three men in connection with the country?s most notorious racist crime: the unsolved 1993 murder of
black teenager Stephen Lawrence. A public inquiry after the Lawrence case found London police institutionally racist,
prompting a government pledge to take a more active role in combating racial intolerance. The report, commissioned after
police botched the investigation into the 1993 racially motivated murder of a black teenager, Stephen Lawrence has put
pressure on Sir Paul Condon, the Metropolitan Police chief, to resign. British authorities and police have learned from
the 1993 murder of black teen-ager Stephen Lawrence by a gang of white youths and the failure of the police to
System 18?s summary: Actual length = 244 words, Predicted length = 597 words, Verbosity degree = 353
The government, which has received praise from backers of the Lawrence family for its pursuit of the case, came in for
criticism on Monday for actions it took this weekend to prevent publication of a leaked version of the report, which is
due to be made public on Wednesday. Sir William Macpherson, a retired High Court justice who was the author of the
report and chairman of the eight-month government inquiry, defined institutional racism as ?the collective failure of an
organization to provide an appropriate professional service to people because of their color, culture or ethnic origin?
reflected, he said, in ?processes, attitudes and behavior which amounts to discrimination through unwitting prejudice
ignorance, thoughtlessness and racist stereotyping.? Richard Norton-Taylor, whose play about Lawrence?s killing, ?The
Color of Justice,? has been playing to rave reviews in London, said that the attention paid to the Lawrence case and
others was a sign that British attitudes toward the overarching authority of the police and other institutions were
finally being called into question. She said British authorities and police have learned from the 1993 murder of black
teenager Stephen Lawrence by a gang of white youths and the failure of the police to investigate his death adequately
A senior Scotland Yard police officer Wednesday apologized to the parents of a black teenager slain five years ago in a
race killing that has become the focus of debate over relations between police and ethnic minorities.
Table 6: Summaries produced by two systems for input D0624 (DUC 2006) shown with the verbosity
scores from our model
ing greater coherence compared to short and suc-
cinct sentences which are jumbled such that it is
hard to decipher the full story.
The simple redundancy score (last row of the
table) does not have any significant relationship
to quality scores. One reason could be that most
summarization systems make an effort to reduce
redundant information (Carbonell and Goldstein,
1998) and therefore a simple measure of word
overlap is not helpful for distinguishing quality.
As examples of the predictions from our model,
Table 6 shows two summaries produced for the
same input by two different systems. They both
have almost the same actual length but the first re-
ceived a prediction close to its actual length while
the other is predicted with a much higher verbosity
degree score. Intuitively, the second example is
more verbose compared to the first one. According
to the manual evaluations as well, the first sum-
mary receives a higher score of 0.4062 (pyramid)
compared to 0.2969 for the second summary.
7 Conclusions
There are several ways in which our approach can
be improved. In this first work, we have avoided
the complexities of manual annotation. In fu-
ture, we will explore the feasibility of human an-
notations of verbosity on a suitable corpus, such
as news articles on the same topic from different
sources. In addition, our current approach only
considers a snippet of the text or topic segment
during prediction but ignores the writing in the re-
maining text. In future work, we plan to use a slid-
ing window to obtain and aggregate length predic-
tions while considering the full text.
Acknowledgements
This work was partially supported by a NSF CA-
REER 0953445 award. We also thank the anony-
mous reviewers for their comments.
643
References
J. Carbonell and J. Goldstein. 1998. The use of mmr,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of SIGIR,
pages 335?336.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: a test of grounded language acquisition.
In Proceedings of ICML, pages 128?135.
J. M. Conroy and H. T. Dang. 2008. Mind the gap:
Dangers of divorcing evaluations of summary con-
tent from linguistic quality. In Proceedings of COL-
ING, pages 145?152.
C. Danescu-Niculescu-Mizil, J. Cheng, J. Kleinberg,
and L. Lee. 2012. You had me at hello: How phras-
ing affects memorability. In Proceedings of ACL,
pages 892?901.
J. Eisenstein and R. Barzilay. 2008. Bayesian un-
supervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL, pages 363?370.
M. Galley and K. McKeown. 2007. Lexicalized
markov grammars for sentence compression. In
Proceedings of HLT-NAACL.
V. Ganjigunte Ashok, S. Feng, and Y. Choi. 2013. Suc-
cess with style: Using writing style to predict the
success of novels. In Proceedings of EMNLP, pages
1753?1764.
D. Graff. 2002. The AQUAINT Corpus of English
News Text. Corpus number LDC2002T31, Linguis-
tic Data Consortium, Philadelphia.
M. Kaisser, M. A. Hearst, and J. B. Lowe. 2008. Im-
proving search results quality by customizing sum-
mary lengths. In Proceedings of ACL-HLT, pages
701?709.
D. Klein and C.D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, pages 423?
430.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1).
H. Lakkaraju, J. J. McAuley, and J. Leskovec. 2013.
What?s in a name? understanding the interplay be-
tween titles, content, and communities in social me-
dia. In ICWSM.
A. Louis and A. Nenkova. 2011. Automatic identifica-
tion of general and specific sentences by leveraging
discourse annotations. In Proceedings of IJCNLP,
pages 605?613.
A. Louis and A. Nenkova. 2013. What makes writing
great? first experiments on article quality prediction
in the science journalism domain. TACL, 1:341?
352.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The pyramid method: Incorporating human con-
tent selection variation in summarization evaluation.
ACM Trans. Speech Lang. Process., 4(2):4.
M. O?Donnell. 1997. Variable-length on-line docu-
ment generation. In Proceedings of the 6th Euro-
pean Workshop on Natural Language Generation.
E. Pitler and A. Nenkova. 2009. Using syntax to dis-
ambiguate explicit discourse connectives in text. In
Proceedings of ACL-IJCNLP, pages 13?16.
E. Pitler, A. Louis, and A. Nenkova. 2010. Automatic
evaluation of linguistic quality in multi-document
summarization. In Proceedings of ACL.
R Development Core Team, 2011. R: A Language and
Environment for Statistical Computing. R Founda-
tion for Statistical Computing.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010. A
multi-pass sieve for coreference resolution. In Pro-
ceedings of EMNLP, pages 492?501.
E. Sandhaus. 2008. The New York Times Annotated
Corpus. Corpus number LDC2008T19, Linguistic
Data Consortium, Philadelphia.
644
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712?721,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Improving the Estimation of Word Importance for News Multi-Document
Summarization
Kai Hong
University of Pennsylvania
Philadelphia, PA, 19104
hongkai1@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA, 19104
nenkova@seas.upenn.edu
Abstract
We introduce a supervised model
for predicting word importance that
incorporates a rich set of features. Our
model is superior to prior approaches
for identifying words used in human
summaries. Moreover we show
that an extractive summarizer using
these estimates of word importance is
comparable in automatic evaluation with
the state-of-the-art.
1 Introduction
In automatic extractive summarization, sentence
importance is calculated by taking into account,
among possibly other features, the importance
of words that appear in the sentence. In this
paper, we describe experiments on identifying
words from the input that are also included in
human summaries; we call such words summary
keywords. We review several unsupervised
approaches for summary keyword identification
and further combine these, along with features
including position, part-of-speech, subjectivity,
topic categories, context and intrinsic importance,
in a superior supervised model for predicting word
importance.
One of the novel features we develop aims
to determine the intrinsic importance of words.
To this end, we analyze abstract-article pairs in
the New York Times corpus (Sandhaus, 2008)
to identify words that tend to be preserved in
the abstracts. We demonstrate that judging word
importance just based on this criterion leads to
significantly higher performance than selecting
sentences at random. Identifying intrinsically
important words allows us to generate summaries
without doing any feature computation on the
input, equivalent in quality to the standard baseline
of extracting the first 100 words from the latest
article in the input. Finally, we integrate the
schemes for assignment of word importance into
a summarizer which greedily optimizes for the
presence of important words. We show that our
better estimation of word importance leads to
better extractive summaries.
2 Prior work
The idea of identifying words that are descriptive
of the input can be dated back to Luhn?s earliest
work in automatic summarization (Luhn, 1958).
There keywords were identified based on the
number of times they appeared in the input,
and words that appeared most and least often
were excluded. Then the sentences in which
keywords appeared near each other, presumably
better conveying the relationship between the
keywords, were selected to form a summary.
Many successful recent systems also estimate
word importance. The simplest but competitive
way to do this task is to estimate the word
probability from the input (Nenkova and
Vanderwende, 2005). Another powerful method
is log-likelihood ratio test (Lin and Hovy, 2000),
which identifies the set of words that appear in
the input more often than in a background corpus
(Conroy et al., 2006; Harabagiu and Lacatusu,
2005).
In contrast to selecting a set of keywords,
weights are assigned to all words in the input
in the majority of summarization methods.
Approaches based on (approximately) optimizing
the coverage of these words have become widely
popular. Earliest such work relied on TF*IDF
weights (Filatova and Hatzivassiloglou, 2004),
later approaches included heuristics to identify
summary-worthy bigrams (Riedhammer et al.,
2010). Most optimization approaches, however,
use TF*IDF or word probability in the input as
word weights (McDonald, 2007; Shen and Li,
2010; Berg-Kirkpatrick et al., 2011).
712
Word weights have also been estimated by
supervised approaches, with word probability and
location of occurrence as typical features (Yih et
al., 2007; Takamura and Okumura, 2009; Sipos et
al., 2012).
A handful of investigations have productively
explored the mutually reinforcing relationship
between word and sentence importance, iteratively
re-estimating each in either supervised or
unsupervised framework (Zha, 2002; Wan et
al., 2007; Wei et al., 2008; Liu et al., 2011).
Most existing work directly focuses on predicting
sentence importance, with emphasis on the
formalization of the problem (Kupiec et al., 1995;
Celikyilmaz and Hakkani-Tur, 2010; Litvak et al.,
2010). There has been little work directly focused
on predicting keywords from the input that will
appear in human summaries. Also there has been
only a few investigations of suitable features
for estimating word importance and identifying
keywords in summaries; we address this issue by
exploring a range of possible indicators of word
importance in our model.
3 Data and Planned Experiments
We carry out our experiments on two datasets from
the Document Understanding Conference (DUC)
(Over et al., 2007). DUC 2003 is used for training
and development, DUC 2004 is used for testing.
These are the last two years in which generic
summarization was evaluated at DUC workshops.
There are 30 multi-document clusters in DUC
2003 and 50 in DUC 2004, each with about 10
news articles on a related topic. The task is
to produce a 100-word generic summary. Four
human abstractive summaries are available for
each cluster.
We compare different keyword extraction
methods by the F-measure
1
they achieve against
the gold-standard summary keywords. We do not
use stemming when calculating these scores.
In our work, keywords for an input are defined
as those words that appear in at least i of the
human abstracts, yielding four gold-standard sets
of keywords, denoted by G
i
. |G
i
| is thus the
cardinality of the set for the input. We only
consider the words in the summary that also
appear in the original input
2
, with stopwords
1
2*precision*recall/(precision+recall)
2
On average 26.3% (15.0% with stemming) of the words
in the four abstracts never appear in the input.
excluded
3
. Table 1 shows the average number of
unique content words for the respective keyword
gold-standard.
i 1 2 3 4
Mean |G
i
| 102 32 15 6
Table 1: Average number of words in G
i
For the summarization task, we compare results
using ROUGE (Lin, 2004). We report ROUGE-1,
-2, -4 recall, with stemming and without removing
stopwords. We consider ROUGE-2 recall as
the main metric for this comparison due to its
effectiveness in comparing machine summaries
(Owczarzak et al., 2012). All of the summaries
were truncated to the first 100 words by ROUGE
4
.
We use Wilcoxon signed-rank test to examine
the statistical significance as advocated by Rankel
et al. (2011) for both tasks, and consider
differences to be significant if the p-value is less
than 0.05.
4 Unsupervised Word Weighting
In this section we describe three unsupervised
approaches of assigning importance weights to
words. The first two are probability and
log-likelihood ratio, which have been extensively
used in prior work. We also apply a markov
random walk model for keyword ranking, similar
to Mihalcea and Tarau (2004). In the next
section we describe a summarizer that uses these
weights to form a summary and then describe
our regression approach to combine these and
other predictors in order to achieve more accurate
predictions for the word importance in Section 7.
The task is to assign a score to each word in the
input. The keywords extracted are thus the content
words with highest scores.
4.1 Word Probability (Prob)
The frequency with which a word occurs in the
input is often considered as an indicator of its
importance. The weight for a word is computed
as p(w) =
c(w)
N
, where c(w) is the number of
times word w appears in the input and N is the
total number of word tokens in the input.
3
We use the stopword list from the SMART system
(Salton, 1971), augmented with punctuation and symbols.
4
ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n
4 -m -a -l 100 -x
713
4.2 Log-likelihood Ratio (LLR)
The log-likelihood ratio test (Lin and Hovy, 2000)
compares the distribution of a word in the input
with that in a large background corpus to identify
topic words. We use the Gigaword corpus (Graff et
al., 2007) for background counts. The test statistic
has a ?
2
distribution, so a desired confidence level
can be chosen to find a small set of topic words.
4.3 Markov Random Walk Model (MRW)
Graph methods have been successfully applied to
weighting sentences for generic (Wan and Yang,
2008; Mihalcea and Tarau, 2004; Erkan and
Radev, 2004) and query-focused summarization
(Otterbacher et al., 2009).
Here instead of constructing a graph with
sentences as nodes and edges weighted by
sentence similarity, we treat the words as vertices,
similar to Mihalcea and Tarau (2004). The
difference in our approach is that the edges
between the words are defined by syntactic
dependencies rather than depending on the
co-occurrence of words within a window of k. We
use the Stanford dependency parser (Marneffe et
al., 2006). In our approach, we consider a word
w more likely to be included in a human summary
when it is syntactically related to other (important)
words, even if w itself is not mentioned often.
The edge weight between two vertices is equal to
the number of syntactic dependencies of any type
between two words within the same sentence in
the input. The weights are then normalized by
summing up the weights of edges linked to one
node.
We apply the Pagerank algorithm (Lawrence
et al., 1998) on the resulting graph. We set the
probability of performing random jump between
nodes ?=0.15. The algorithm terminates when
the change of node weight between iterations is
smaller than 10
?4
for all nodes. Word importance
is equal to the final weight of its corresponding
node in the graph.
5 Summary Generation Process
In this section, we outline how summaries
are generated by a greedy optimization system
which selects the sentence with highest weight
iteratively. This is the main process we use in all
our summarization systems. For comparison we
also use a summarization algorithm based on KL
divergence.
5.1 Greedy Optimization Approach
Our algorithm extracts sentences by weighting
them based on word importance. The approach is
similar to the standard word probability baseline
(Nenkova et al., 2006) but we explore a range
of possibilities for assigning weights to individual
words. For each sentence, we calculate the
sentence weight by summing up the weights of
all words, normalized by the number of words in
the sentence. We sort the sentences in descending
order of their scores into a queue. To create a
summary, we iteratively dequeue one sentence,
check if the sentence is more than 8 words (as
in Erkan and Radev (2004)), then append it to
the current summary if it is non-redundant. A
sentence is considered non-redundant if it is not
similar to any sentences already in the summary,
measured by cosine similarity on binary vector
representations with stopwords excluded. We use
the cut-off of 0.5 for cosine similarity. This value
was tuned on the DUC 2003 dataset, by testing the
impact of the cut-off value on the ROUGE scores
for the final summary. Possible values ranged
from 0.1 to 0.9 with step of 0.1.
5.2 KL Divergence Summarizer
The KLSUM summarizer (Haghighi and
Vanderwende, 2009) aims at minimizing the KL
divergence between the probability distribution
over words estimated from the summary and
the input respectively. This summarizer is a
component of the popular topic model approaches
(Daum?e and Marcu, 2006; Celikyilmaz and
Hakkani-T?ur, 2011; Mason and Charniak, 2011)
and achieves competitive performance with
minimal differences compared to a full-blown
topic model system.
6 Global Indicators from NYT
Some words evoke topics that are of intrinsic
interest to people. Here we search for global
indicators of word importance regardless of
particular input.
6.1 Global Indicators of Word Importance
We analyze a large corpus of original documents
and corresponding summaries in order to identify
words that consistently get included in or excluded
from the summary. In the 2004-2007 NYT corpus,
many news articles have abstracts along with the
original article, which makes it an appropriate
714
Metric Top-30 words
KL(A ? G)(w) photo(s), pres, article, column, reviews, letter, York, Sen, NY, discusses, drawing, op-ed, holds, Bush
correction, editorial, dept, city, NJ, map, corp, graph, contends, Iraq, John, dies, sec, state, comments
KL(G ? A)(w) Mr, Ms, p.m., lot, Tuesday, CA, Wednesday, Friday, told, Monday, time, a.m., added, thing, Sunday
things, asked, good, night, Saturday, nyt, back, senator, wanted, kind, Jr., Mrs, bit, looked, wrote
Pr
A
(w) photo, photos, article, York, column, letter, Bush, state, reviews, million, American
pres, percent, Iraq, year, people, government, John, years, company, correction
national, federal, officials, city, drawing, billion, public, world, administration
Table 2: Top 30 words by three metrics from NYT corpus
resource to do such analysis. We identified
160, 001 abstract-original pairs in the corpus.
From these, we generate two language models,
one estimated from the text of all abstracts (LM
A
),
the other estimated from the corpus of original
articles (LM
G
). We use SRILM (Stolcke, 2002)
with Ney smoothing.
We denote the probability of wordw in LM
A
as
Pr
A
(w), the probability in LM
G
as Pr
G
(w), and
calculate the difference Pr
A
(w)?Pr
G
(w) and the
ratio Pr
A
(w)/Pr
G
(w) to capture the change of
probability. In addition, we calculate KL-like
weighted scores for words which reflect both the
change of probabilities between the two samples
and the overall frequency of the word. Here
we calculate both KL(A ? G) and KL(G ?
A). Words with high values for the former score
are favored in the summaries because they have
higher probability in the abstracts than in the
originals and have relatively high probability in
the abstracts. The later score is high for words that
are often not included in summaries.
KL(A ? G)(w) = Pr
A
(w) ? ln
Pr
A
(w)
Pr
G
(w)
KL(G ? A)(w) = Pr
G
(w) ? ln
Pr
G
(w)
Pr
A
(w)
Table 2 shows examples of the global
information captured from the three types
of scores?KL(A ? G), KL(G ? A) and
Pr
A
(w)?listing the 30 content words with
highest scores for each type. Words that tend to
be used in the summaries, characterized by high
KL(A ? G) scores, include locations (York, NJ,
Iraq), people?s names and titles (Bush, Sen, John),
some abbreviations (pres, corp, dept) and verbs of
conflict (contends, dies). On the other hand, from
KL(G ? A), we can see that it is unlikely for
writers to include courtesy titles (Mr, Ms, Jr.) and
relative time reference in summaries. The words
with high Pr
A
(w) scores overlaps with those
ranked highly by KL(A ? G) to some extent,
but also includes a number of generally frequent
words which appeared often both in the abstracts
and original texts, such as million and percent.
6.2 Blind Sentence Extraction
In later sections we include the measures of
global word importance as a feature of our
regression model for predicting word weights for
summarization. Before turning to that, however,
we report the results of an experiment aimed to
confirm the usefulness of these features. We
present a system, BLIND, which uses only weights
assigned to words by KL(A ? G) from NYT,
without doing any analysis of the original input.
We rank all non-stopword words from the input
according to this score. The top k words are given
weight 1, while the others are given weight 0.
The summaries are produced following the greedy
procedure described in Section 5.1.
Systems R-1 R-2 R-4
RANDOM 30.32 4.42 0.36
BLIND (80 keywords) 30.77 5.18 0.53
BLIND (300 keywords) 32.91 5.94 0.61
LASTESTLEAD 31.39 6.11 0.63
FIRST-SENTENCE 34.26 7.22 1.21
Table 3: Blind sentence extraction system,
compared with three baseline systems (%)
Table 3 shows that the BLIND system has R-2
recall of 0.0594 using the top 300 keywords,
significantly better than picking sentences from
the input randomly. It also achieves comparable
performance with the baseline in DUC 2004,
formed by selecting the first 100 words from
the latest article in the input (LASTESTLEAD).
However it is significantly worse than another
baseline of selecting the first sentences from the
input. Table 4 gives sample summaries generated
by these three approaches. These results confirm
that the information gleaned from the analysis
715
Random Summary
It was sunny and about 14 degrees C(57 degrees F) in Tashkent on Sunday. The president is a strong person, and he has been
through far more difficult political situations, Mityukov said, according to Interfax. But Yeltsin?s aides say his first term,
from 1991 to 1996, does not count because it began six months before the Soviet Union collapsed and before the current
constitution took effect. He must stay in bed like any other person, Yakushkin said. The issue was controversial earlier this
year when Yeltsin refused to spell out his intentions and his aides insisted he had the legal right to seek re-election.
NYT Summary from global keyword selection, KL(A ? G), k = 300
Russia?s constitutional court opened hearings Thursday on whether Boris Yeltsin can seek a third term. Yeltsin?s growing
health problems would also seem to rule out another election campaign. The Russian constitution has a two-term limit for
presidents. Russian president Boris Yeltsin cut short a trip to Central Asia on Monday due to a respiratory infection that
revived questions about his overall health and ability to lead Russia through a sustained economic crisis. The upper house of
parliament was busy voting on a motion saying he should resign. The start of the meeting was shown on Russian television.
First Sentence Generated Summary
President Boris Yeltsin has suffered minor burns on his right hand, his press office said Thursday. President Boris Yeltsin?s
doctors have pronounced his health more or less normal, his wife Naina said in an interview published Wednesday. President
Boris Yeltsin, on his first trip out of Russia since this spring, canceled a welcoming ceremony in Uzbekistan on Sunday
because he wasn?t feeling well, his spokesman said. Doctors ordered Russian President Boris Yeltsin to cut short his Central
Asian trip because of a respiratory infection and he agreed to return home Monday, a day earlier than planned, officials said.
Table 4: Summary comparison by Random, Blind Extraction and First Sentence systems
of NYT abstract-original pairs encodes highly
relevant information about important content
independent of the actual text of the input.
7 Regression-Based Keyword Extraction
Here we introduce a logistic regression model
for assigning importance weights to words in the
input. Crucially, this model combines evidence
from multiple indicators of importance. We have
at our disposal abundant data for learning because
each content word in the input can be treated as
a labeled instance. There are in total 32, 052
samples from the 30 inputs of DUC 2003 for
training, 54, 591 samples from the 50 inputs of
DUC 2004 for testing. For a word in the input,
we assign label 1 if the word appears in at least
one of the four human summaries for this input.
Otherwise we assign label 0.
In the rest of this section, we describe the rich
variety of features included in our system. We also
analyze and discuss the predictive power of those
features by performing Wilcoxon signed-rank test
on the DUC 2003 dataset. There are in total 9, 261
features used, among them 1, 625 are significant
(p-value < 0.05). We rank these features in
increasing p-values derived from Wilcoxon test.
Apart from the widely used features of word
frequency and positions, some other less explored
features are highly significant.
7.1 Frequency Features
We use the Probability, LLR chi-square statistic
value and MRW scores as features. Since prior
work has demonstrated that for LLR weights in
particular, it is useful to identify a small set of
important words and ignore all other words in
summary selection (Gupta et al., 2007), we use
a number of keyword indicators as features. For
these indicators, the value of feature is 1 if the
word is ranked within top k
i
, 0 otherwise. Here k
i
are preset cutoffs
5
. These cutoffs capture different
possibilities for defining the keywords in the input.
We also add the number of input documents that
contain the word as a feature. There are a total of
100 features in this group, all of which are highly
significant, ranked among the top 200.
7.2 Standard features
We now describe some standard features which
have been applied in prior work on summarization.
Word Locations: Especially in news articles,
sentences that occur at the beginning are often the
most important ones. In line with this observation,
we calculate several features related to the position
in which a word appears. We first compute
the relative positions for word tokens, where
the tokens are numbered sequentially in order of
appearance in each document in the input. The
relative position for one word token is therefore
its corresponding number, divided by total number
of tokens minus one in the document, e.g., 0
for the first token, 1 for the last token. For
each word, we calculate its earliest first location,
latest last location, average location and average
first location for tokens of this word across all
documents in the input. In addition we have a
binary feature indicating if the word appears in the
5
10, 15, 20, 30, 40, ? ? ? , 190, 200, 220, 240, 260, 280,
300, 350, 400, 450, 500, 600, 700 (in total 33 values)
716
first sentence and the number of times it appears
in a first sentence among documents in one input.
There are 6 features in this group. All of them are
very significant, ranked within the top 100.
Word type: These features include Part of
Speech (POS) tags, Name Entity (NE) labels and
capitalization information. We use the Stanford
POS-Tagger (Toutanova et al., 2003) and Name
Entity Recognizer (Finkel et al., 2005). We have
one feature corresponding to each possible POS
and NE tag. The value of this feature is the
proportion of occurrences of the word with this
tag; in most cases only one feature gets a non-zero
value. We have two features which indicate if
one word has been capitalized and the ratio of its
capitalized occurrences.
Most of the NE features (6 out of 8) are
significant: there are more Organizations and
Locations but fewer Time and Date words in the
human summaries. Of the POS tags, 11 out of 41
are significant: there are more nouns (NN, NNS,
NNPS); fewer verbs (VBG, VBP, VB) and fewer
cardinal numbers in the abstracts compared to the
input. Capitalized words also tend to be included
in human summaries.
KL: Prior work has shown that having estimates
of sentence importance can also help in estimating
word importance (Wan et al., 2007; Liu et al.,
2011; Wei et al., 2008). The summarizer based
on KL-divergence assigns importance to sentences
directly, in a complex function according to the
word distribution in the sentence. Therefore,
we use these summaries as potential indicators
of word importance. We include two features
here, the first one indicates if the word appears
in a KLSUM summary of the input, as well as
a feature corresponding to the number of times
the word appeared in that summary. Both of the
features are highly significant, ranked within the
top 200.
7.3 NYT-weights as Features
We include features from the relative rank of
a word according to KL(A ? G), KL(G ?
A), Pr
A
(w)?Pr
G
(w), Pr
A
(w)/Pr
G
(w) and
Pr
A
(w), derived from the NYT as described in
Section 6. If the rank of a word is within top-k
or bottom-k by one metric, we would label it as
1, where k is selected from a set of pre-defined
values
6
. We have in total 70 features in this
6
100, 200, 500, 1000, 2000, 5000, 10000 in this case.
category, of which 56 are significant, 47 having
a p-value less than 10
?7
. The predictive power of
those global indicators are only behind the features
which indicates frequency and word positions.
7.4 Unigrams
This is a binary feature corresponding to each
of the words that appeared at least twice in the
training data. The idea is to learn which words
from the input tend to be mentioned in the human
summaries. There are in total 8, 691 unigrams,
among which 1, 290 are significant. Despite the
high number of significant unigram features, most
of them are not as significant as the more general
ones we described so far. It is interesting to
compare the significant unigrams identified in the
DUC abstract/input data with those derived from
the NYT corpus. Unigrams that tend to appear in
DUC summaries include president, government,
political. We also find the same unigrams among
the top words from NYT corpus according to
KL(A ? G) . As for words unlikely to appear in
summaries, we see Wednesday, added, thing, etc,
which again rank high according to KL(G ? A).
7.5 Dictionary Features: MPQA and LIWC
Unigram features are notoriously sparse. To
mitigate the sparsity problem, we resort to
more general groupings to words according to
salient semantic and functional categories. We
employ two hand-crafted dictionaries, MPQA for
subjectivity analysis and LIWC for topic analysis.
The MPQA dictionary (Wiebe and Cardie,
2005) contains words with different polarities
(positive, neutral, negative) and intensities (strong,
weak). The combinations correspond to six
features. It turns out that words with strong
polarity, either positive or negative, are seldomly
included in the summaries. Most strikingly,
the p-value from significance test for the strong
negative words is less than 10
?4
?these words
are rarely included in summaries. There is no
significant difference on weak polarity categories.
Another dictionary we use is LIWC (Tausczik
and Pennebaker, 2007), which contains manually
constructed dictionaries for multiple categories
of words. The value of the feature is 1 for
one word if the word appears in the particular
dictionary for the category. 34 out of 64 LIWC
features are significant. Interesting categories
which appear at higher rate in summaries include
events about death, anger, achievements, money
717
and negative emotions. Those that appear at lower
rate in the summaries include auxiliary verbs, hear,
pronouns, negation, function words, social words,
swear, adverbs, words related to families, etc.
7.6 Context Features
We use context features here, based on the
assumption that context importance around a word
affects the importance of this word. For context
we consider the words before and after the target
word. We extend our feature space by calculating
the weighted average of the feature values of the
context words. For word w, we denote L
w
as the
set of words before w, R
w
as the set of words
after w. We denote the feature for one word as
w.f
i
, the way of calculating the newly extended
word-before feature w.l
f
i
could be written as:
w.l
f
i
=
?
i
p(w
l
) ? w
l
.f
i
, ?w
l
? L
w
Here p(w
l
) is the probability word w
l
appears
before w among all words in L
w
.
For context features, we calculate the weighted
average of the most widely used basic features,
including frequency, location and capitalization
for surrounding contexts. There are in total
220 features of this kind, among which 117 are
significant, 74 having a p-value less than 10
?4
.
8 Experiments
The performance of our logistic regression model
is evaluated on two tasks: keyword identification
and extractive summarization. We name our
system REGSUM.
8.1 Regression for Keyword Identification
For each input, we define the set of keywords
as the top k words according to the scores
generated from different models. We compare
our regression system with three unsupervised
systems: PROB, LLR, MRW. To show the
effectiveness of new features, we compare our
results with a regression system trained only
on word frequency and location related features
described in Section 7. Those features are the
ones standardly used for ranking the importance
of words in recent summarization works (Yih et
al., 2007; Takamura and Okumura, 2009; Sipos et
al., 2012), and we name this system REGBASIC.
Figure 1 shows the performance of systems
when selecting the 100 words with highest weights
Figure 1: Precision, Recall and F-score of
keyword identification, 100 words selected, G
1
as
gold-standard
as keywords. Each word from the input that
appeared in any of the four human summaries is
considered as a gold-standard keyword. Among
the unsupervised approaches, word probability
identifies keywords better than LLR and MRW
by at least 4% on F-score. REGBASIC does not
give better performance at keyword identification
compared with PROB, even though it includes
location information. Our system gets 2.2%
F-score improvement over PROB, 5.2% over
REGBASIC, and more improvement over the
other approaches. All of these improvements are
statistically significant by Wilcoxon test.
Table 5 shows the performance of keyword
identification for different G
i
and different
number of keywords selected. The regression
system has no advantage over PROB when
identifying keywords that appeared in all of the
four human summaries. However our system
achieves significant improvement for predicting
words that appeared in more than one or two
human summaries.
7
8.2 Regression for Summarization
We now show that the performance of extractive
summarization can be improved by better
estimation of word weights. We compare our
regression system with the four models introduced
in Section 8.1. We also include PEER-65, the best
system in DUC-2004, as well as KLSUM for
comparison. Apart from these, we compare our
model with two state-of-the-art systems, including
the submodular approach (SUBMOD) (Lin and
7
We also apply a weighted keyword evaluation approach,
similar to the pyramid method for summarization. Still
our system shows significant improvement over the others.
See https://www.seas.upenn.edu/~hongkai1/regsum.html for
details.
718
Gi
#words PROB LLR MRW REGBASIC REGSUM
G
1
80 43.6 37.9 38.9 39.9 45.7
G
1
100 44.3 38.7 39.2 41.0 46.5
G
1
120 44.6 38.5 39.2 40.9 46.4
G
2
30 47.8 44.0 42.4 47.4 50.2
G
2
35 47.1 43.3 42.1 47.0 49.5
G
2
40 46.5 42.4 41.8 46.4 49.2
G
3
10 51.2 46.2 43.8 46.9 50.2
G
3
15 51.4 47.5 43.7 49.8 52.9
G
3
20 49.7 47.6 42.5 49.3 51.5
G
4
5 50.0 48.8 44.9 43.6 45.1
G
4
6 51.4 46.9 43.7 45.2 47.6
G
4
7 50.9 48.2 43.7 45.8 47.8
Table 5: Keyword identification F-score (%) for different G
i
and different number of words selected.
Bilmes, 2012) and the determinantal point process
(DPP) summarizer (Kulesza and Taskar, 2012).
The summaries were kindly provided by the
authors of these systems (Hong et al., 2014).
As can been seen in Table 6, our system
outperforms PROB, LLR, MRW, PEER-65,
KLSUM and REGBASIC. These improvements
are significant on ROUGE-2 recall. Interestingly,
although the supervised system REGBASIC which
uses only frequency and positions achieve
low performance in keyword identification, the
summaries it generates are of high quality. The
inclusion of position features negatively affects the
performance in summary keyword identification
but boosts the weights for the words which appear
close to the beginning of the documents, which is
helpful for identifying informative sentences. By
including other features we greatly improve over
REGBASIC in keyword identification. Similarly
here the richer set of features results in better
quality summaries.
We also examined the ROUGE-1, -2, -4
recall compared with the SUBMOD and DPP
summarizers
8
. There is no significant difference
on R-2 and R-4 recall compared with these
two state-of-the-art systems. DPP performed
significantly better than our system on R-1 recall,
but that system is optimizing on R-1 F-score in
training. Overall, our conceptually simple system
is on par with the state of the art summarizers and
points to the need for better models for estimating
word importance.
8
The results are slightly different from the ones reported
in the original papers due to the fact that we truncated to 100
words, while they truncated to 665 bytes.
System R-1 R-2 R-4
PROB 35.14 8.17 1.06
LLR 34.60 7.56 0.83
MRW 35.78 8.15 0.99
REGBASIC 37.56 9.28 1.49
KL 37.97 8.53 1.26
PEER-65 37.62 8.96 1.51
SUBMOD 39.18 9.35 1.39
DPP 39.79 9.62 1.57
REGSUM 38.57 9.75 1.60
Table 6: System performance comparison (%)
9 Conclusion
We presented a series of experiments which
show that keyword identification can be improved
in a supervised framework which incorporates
a rich set of indicators of importance. We
also show that the better estimation of word
importance leads to better extractive summaries.
Our analysis of features related to global
importance, sentiment and topical categories
reveals rather unexpected results and confirms that
word importance estimation is a worthy research
direction. Success in the task is likely to improve
sophisticated summarization approaches too, as
well as sentence compression systems which use
only crude frequency related measures to decide
which words should be deleted from a sentence.
9
9
The work is partially funded by NSF CAREER award
IIS 0953445.
719
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010.
A hybrid hierarchical model for multi-document
summarization. In Proceedings of ACL, pages
815?824.
Asli Celikyilmaz and Dilek Hakkani-T?ur. 2011.
Discovery of topically coherent sentences for
extractive summarization. In Proceedings of
ACL-HLT, pages 491?499.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of COLING/ACL, pages 152?159.
Hal Daum?e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
ACL, pages 305?312.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22(1):457?479.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in
multi-sentence text extraction. In Proceedings of
COLING.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In Proceedings of ACL, pages
363?370.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2007.
English gigaword third edition. Linguistic Data
Consortium, Philadelphia, PA.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky.
2007. Measuring importance and query relevance
in topic-focused multi-document summarization. In
Proceedings of ACL, pages 193?196.
Aria Haghighi and Lucy Vanderwende. 2009.
Exploring content models for multi-document
summarization. In Proceedings of HLT-NAACL,
pages 362?370.
Sanda Harabagiu and Finley Lacatusu. 2005. Topic
themes for multi-document summarization. In
Proceedings of SIGIR 2005, pages 202?209.
Kai Hong, John M. Conroy, Benoit Favre, Alex
Kulesza, Hui Lin, and Ani Nenkova. 2014. A
repositary of state of the art and competitive baseline
summaries for generic news summarization. In
Proceedings of LREC, May.
Alex Kulesza and Ben Taskar. 2012. Determinantal
point processes for machine learning. Foundations
and Trends in Machine Learning, 5(2?3).
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR, pages 68?73.
Page Lawrence, Brin Sergey, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation
ranking: Bringing order to the web. Technical
report, Stanford University.
Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In UAI, pages 479?490.
Chin-Yew Lin and Eduard Hovy. 2000. The
automated acquisition of topic signatures for text
summarization. In Proceedgins of COLING, pages
495?501.
Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Text
Summarization Branches Out: Proceedings of the
ACL-04 Workshop, pages 74?81.
Marina Litvak, Mark Last, and Menahem Friedman.
2010. A new approach to improving multilingual
summarization using a genetic algorithm. In
Proceedings of ACL, pages 927?936.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised
framework for keyword extraction from meeting
transcripts. Transactions on Audio Speech and
Language Processing, 19(3):538?548.
H. P. Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of Research and
Development, 2(2):159?165, April.
M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating Typed Dependency Parses from Phrase
Structure Parses. In Proceedings of LREC-06, pages
449?454.
Rebecca Mason and Eugene Charniak. 2011.
Extractive multi-document summaries should
explicitly not contain document-specific content.
In Proceedings of the Workshop on Automatic
Summarization for Different Genres, Media, and
Languages, pages 49?54.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of ECIR, pages 557?564.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into text. In Proceedings of EMNLP,
pages 404?411.
Ani Nenkova and Lucy Vanderwende. 2005. The
impact of frequency on summarization. Technical
report, Microsoft Research.
720
Ani Nenkova, Lucy Vanderwende, and Kathleen
McKeown. 2006. A compositional context sensitive
multi-document summarizer: exploring the factors
that influence summarization. In Proceedings of
SIGIR, pages 573?580.
Jahna Otterbacher, G?unes Erkan, and Dragomir R.
Radev. 2009. Biased lexrank: Passage
retrieval using random walks with question-based
priors. Information Processing and Management,
45(1):42?54.
Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Inf. Process. Manage., 43(6):1506?1520.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in
summarization. In NAACL-HLT 2012: Workshop
on Evaluation Metrics and System Comparison for
Automatic Summarization, pages 1?9.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine
summarization systems. In Proceedings of EMNLP,
pages 467?473.
Korbinian Riedhammer, Beno??t Favre, and Dilek
Hakkani-T?ur. 2010. Long story short -
global unsupervised models for keyphrase based
meeting summarization. Speech Communication,
52(10):801?815.
G. Salton. 1971. The SMART Retrieval System:
Experiments in Automatic Document Processing.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
PA.
Chao Shen and Tao Li. 2010. Multi-document
summarization via the minimum dominating set. In
Proceedings of Coling, pages 984?992.
Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin learning of
submodular summarization models. In Proceedings
of EACL, pages 224?233.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of
ICSLP, volume 2, pages 901?904.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of EACL,
pages 781?789.
Yla R Tausczik and James W Pennebaker. 2007.
The Psychological Meaning of Words: LIWC and
Computerized Text Analysis Methods. Journal of
Language and Social Psychology, 29:24?54.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the NAACL-HLT, pages
173?180.
XiaojunWan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of SIGIR, pages 299?306.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007. Towards an iterative reinforcement approach
for simultaneous document summarization and
keyword extraction. In Proceedings of ACL, pages
552?559.
Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He. 2008.
Query-sensitive mutual reinforcement chain and
its application in query-oriented multi-document
summarization. In Proceedings of SIGIR, pages
283?290.
Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities), page 1(2).
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document
summarization by maximizing informative
content-words. In Proceedings of IJCAI, pages
1776?1782.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings
of SIGIR, pages 113?120.
721
Automatically Assessing Machine Summary
Content Without a Gold Standard
Annie Louis?
University of Pennsylvania
Ani Nenkova??
University of Pennsylvania
The most widely adopted approaches for evaluation of summary content follow some protocol
for comparing a summary with gold-standard human summaries, which are traditionally called
model summaries. This evaluation paradigm falls short when human summaries are not available
and becomes less accurate when only a single model is available. We propose three novel
evaluation techniques. Two of them are model-free and do not rely on a gold standard for the
assessment. The third technique improves standard automatic evaluations by expanding the set
of available model summaries with chosen system summaries.
We show that quantifying the similarity between the source text and its summary with
appropriately chosen measures produces summary scores which replicate human assessments
accurately. We also explore ways of increasing evaluation quality when only one human model
summary is available as a gold standard. We introduce pseudomodels, which are system sum-
maries deemed to contain good content according to automatic evaluation. Combining the
pseudomodels with the single human model to form the gold-standard leads to higher correlations
with human judgments compared to using only the one available model. Finally, we explore
the feasibility of another measure?similarity between a system summary and the pool of all
other system summaries for the same input. This method of comparison with the consensus of
systems produces impressively accurate rankings of system summaries, achieving correlation
with human rankings above 0.9.
1. Introduction
In this work, we present evaluation metrics for summary content which make use of
little or no human involvement. Evaluation methods such as manual pyramid scores
(Nenkova, Passonneau, and McKeown 2007) and automatic ROUGE scores (Lin and
Hovy 2003) rely on multiple human summaries as a gold standard (model) against
which they compare a summary to assess how informative the candidate summary
is. It is desirable that evaluation of similar quality be done quickly and cheaply
? E-mail: lannie@seas.upenn.edu.
?? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut St.,
Philadelphia, PA 19104. E-mail: nenkova@seas.upenn.edu.
Submission received: 18 June 2011; revised submission received: 23 March 2012; accepted for publication:
18 April 2012.
doi:10.1162/COLI a 00123
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 2
on non-standard test sets that have few or no human summaries, or on large test
sets for which creating human model summaries is infeasible. In our work, we aim
to identify indicators of summary content quality that do not make use of human
summaries but can replicate scores based on comparison with a gold standard very
accurately.
Such indicators would need to be easily computable from existing resources and
to provide rankings of systems that agree with rankings obtained through human
judgments. There have been some early proposals for alternative methods. Donaway,
Drummey, and Mather (2000) propose that a comparison of the source text with a
summary can tell us how good the summary is. A summary that has higher similarity
with the source text can be considered better than one with lower similarity. Radev and
Tam (2003) perform a large scale evaluation with thousands of test documents. Their
work is set up in a search engine scenario. They first rank the test documents using the
search engine. Then they perform the same experiment now substituting the summaries
from one system in place of the original documents. The systemwhose summaries have
the most similar ranking as that generated for the full documents is considered the
best system because not much information loss is introduced by the summarization
process.
But these methods did not gain much popularity and their performance was never
compared to human evaluations. Part of the reason is that only in the last decade
have several large data sets with system summaries and their ratings from human
judges become available for performing such studies. Our work is the first to provide
a comprehensive report of the strengths of such approaches and we show that human
ratings can be reproduced by these fully automatic metrics with high accuracy. Our
results are based on data for multi-document news summarization.
The key insights of our approach can be summarized as follows:
Input?summary similarity:Good summaries are representative of the input and so one
would expect that the more similar a summary is to the input, the better its content.
Identifying a suitable input?summary similarity metric will provide a means for fully
automatic evaluation of summaries. We present a quantitative analysis of this hypothe-
sis and show that input?summary similarity is highly predictive of scores assigned by
humans for the summaries. The choice of an appropriate metric to measure similarity is
critical, however, and we show that information-theoretic measures turn out to be the
most powerful for this task (Section 4).
Addition of pseudomodels: Having a larger number of model summaries has been
shown to give more stable evaluation results, but for some data sets only a single model
summary is available. We test the utility of pseudomodels, which are system summaries
that are chosen to be added to the human summary pool and that are used as additional
models. We find that augmenting the gold standard with pseudomodels helps obtain
better correlations with human judgments than if a single model is used (Section 5).
System summaries as models: Most current summarization systems perform content
selection reasonably well. We examine an approach to evaluation that exploits system
output and considers all system summaries for a given input as a gold standard (Sec-
tion 6). We find that similarity between a summary and such a gold standard constitutes
a powerful automatic evaluation measure. The correlation between this measure and
human evaluations is over 0.9.
We analyze a number of similarity metrics to identify the ones that perform best
for automatic evaluation. The tool we developed, SIMetrix (Summary Input similarity
268
Louis and Nenkova Automatic Content Evaluation
Metrics), is freely available.1 We test these resource-poor approaches to predict sum-
mary content scores assigned by human assessors. We evaluate the results on data from
the Text Analysis Conferences.2
We find that our automatic methods to estimate summary quality are highly predic-
tive of human judgments. Our best result is 0.93 correlation with human rankings using
no model summaries and this is on par with automatic evaluation methods that do use
human summaries. Our study provides some direction towards alternative methods
of evaluation on non-standard test sets. The goal of our methods is to aid system
development and tuning on new, especially large, data sets using little resources. Our
metrics complement but are not intended to replace existing manual and automatic
approaches to evaluation wherein the latter?s strength and reliability are important
for high confidence evaluations. Some of our findings are also relevant for system
development as we identify desirable properties of automatic summaries that can be
computed from the input (see Section 4). Our results are also strongly suggestive that
system combination has the potential for improving current summarization systems
(Section 6).
We start out with an outline of existing evaluation methods and the potential
shortcomings of these approaches which we wish to address.
2. Current Content Evaluation Methods
Summary quality is defined by two key aspects?content and linguistic quality. A good
summary should contain the most important content in the input and also structure
the content and present it as well-written text. Several methods have been proposed for
evaluating system-produced summaries; some only assess content, others only linguis-
tic quality, and some combine assessment of both. Some of these approaches are manual
and others can be performed automatically.
In our work, we consider the problem of automatic evaluation of content quality.
To establish the context for our work, we provide an overview of current content
evaluation methods used at the annual evaluations run by NIST.
The Text Analysis Conference (TAC, previously called the Document Understand-
ing Conference [DUC]3) conducts large scale evaluation of automatic systems on dif-
ferent summarization tasks. These conferences have been held every year since 2001
and the test sets and evaluation methods adopted by TAC/DUC have become the
standard for reporting results in publications. TAC has employed a range of manual
and automatic metrics over the years.
Manual evaluations of the systems are performed at NIST by trained assessors.
The assessors score the summaries either
a) by comparing with a gold-standard summary written by humans, or
b) by providing a direct rating on a scale (1 to 5 or 1 to 10).
The human summaries against which other summaries are compared are inter-
changeably called models, gold standards, and references. Within TAC, they are typ-
ically calledmodels.
1 SIMetrix can be downloaded at http://www.seas.upenn.edu/?lannie/IEval2.html.
2 http://www.nist.gov/tac/.
3 http://duc.nist.gov/.
269
Computational Linguistics Volume 39, Number 2
2.1 Content Coverage Scores
The methods relying on a gold standard have evolved over the years. In the first
years of DUC, a single model summary was used. System summaries were evaluated
by manually assessing how much of the model?s content is expressed in the system
summary. Each clause in the model represents one unit for the evaluation. For each of
these clauses, assessors specify the extent to which its content is expressed in a given
system summary. The average degree to which the model summary?s clauses overlap
with the system summary?s content is called coverage. These coverage scores were
taken as indicators of content quality for the system summaries.
Different people include very different content in their summaries, however, and
so the coverage scores can vary depending on which model is used (Rath, Resnick, and
Savage 1961). This problem of bias in evaluation was later addressed by the pyramid
technique, which combines information from multiple model summaries to compose
the reference for evaluation. Since 2005, the pyramid evaluation method has become
standard.
2.2 Pyramid Evaluation
The pyramid evaluation method (Nenkova and Passonneau 2004) has been developed
for reliable and diagnostic assessment of content selection quality in summarization and
has been used in several large scale evaluations (Nenkova, Passonneau, and McKeown
2007). It uses multiple human models from which annotators identify semantically
defined Summary Content Units (SCUs). Each SCU is assigned a weight equal to
the number of human model summaries that express that SCU. An ideal maximally
informative summary would express a subset of the most highly weighted SCUs, with
multiple maximally informative summaries being possible. The pyramid score for a
system summary S is equal to the following ratio:
py(S) =
sum of weights of SCUs expressed in S
sum of weights of an ideal summary with the same number of SCUs as S
(1)
In this way, a more reliable score for a summary is obtained usingmultiple reference
summaries. Four human summaries are normally used for pyramid evaluation at TAC.
2.3 Responsiveness Evaluation
Responsiveness of a summary is a measure of overall quality combining both content
selection and linguistic quality. It measures to what extent summaries convey appropri-
ate content in a structured fashion. Responsiveness is assessed by direct ratings given
by the judges. For example, a scale of 1 (poor summary) to 5 (very good summary) is
used and these assessments are done without reference to any model summaries.
Pyramid and responsiveness are the standardly used manual approaches for
content evaluation. They produce rather similar rankings of systems at TAC. The
(Spearman) correlation between the two for ranking systems that participated in
the TAC 2009 conference is 0.85 (p-value 6.8e-16, 53 systems). The responsiveness
measure involves some aspects of linguistic quality whereas the pyramid metric was
designed for content only. Such high correlation indicates that the content factor has
270
Louis and Nenkova Automatic Content Evaluation
substantial influence on the responsiveness judgments, however. The high correlation
also indicates that two types of human judgments made on very different basis?
gold-standard summaries and direct judgments?can agree and provide fairly similar
rankings of summaries.
2.4 ROUGE
Manual evaluation methods require significant human effort. Moreover, the pyramid
evaluation involves detailed annotation for identifying SCUs in human and system
summaries and requires training of assessors to perform the evaluation. Outside of
TAC, therefore, system developments and results are regularly reported using ROUGE,
a suite of automatic evaluation metrics (Lin and Hovy 2003; Lin 2004b).
ROUGE automates the comparison betweenmodel and system summaries based on
n-gram overlaps. These overlap scores have been shown to correlate well with human
assessment (Lin 2004b) and so ROUGE removes the need for manual judgments in this
part of the evaluation.
ROUGE scores are computed typically using unigram (R1) or bigram (R2) overlaps.
In TAC, four human summaries are used as models and their contents are combined
for computing the overlap scores. For fixed length summaries, the recall from the
comparison is used as the quality metric. Other metrics such as longest subsequence
match are also available. Another ROUGE variant is RSU4, which computes the overlap
in terms of skip bigrams, where two unigrams with a gap of up to four intervening
words are considered as bigrams. This latter metric provides some additional flexibility
compared to the stricter R2 scores.
The correlations between ROUGE and manual evaluations for systems in TAC
2009 are shown in Table 1 and vary between 0.76 and 0.94 for the different variants.4
Here, and in all subsequent experiments, Spearman correlations are computed using
the R toolkit (R Development Core Team 2011). In this implementation, significance
values for the correlations are produced using the AS 89 algorithm (Best and Roberts
1975).
These correlations are highly significant and show that ROUGE is a high perfor-
mance automatic evaluation metric.
We can consider the ROUGE results as the upper bound of performance for the
model-free evaluations that we propose because ROUGE involves direct comparison
with the gold-standard summaries. Our metrics are designed to be used when model
summaries are not available.
2.5 Automatic Evaluation Without Gold-Standard Summaries
All of thesemethods require significant human involvement. In evaluations where gold-
standard summaries are needed, assessors first read the input documents (10 or more
per input) and write a summary. Then manual comparison of system and gold standard
is done, which takes additional time. Gillick and Liu (2010) hypothesize that at least
17.5 hours are needed to evaluate two systems under this set up on a standard test
set. Moreover, multiple gold-standard summaries are needed for the same input, so
different assessors have to read and create summaries. The more reliable evaluation
4 The scores were computed after stemming but stop words were retained in the summaries.
271
Computational Linguistics Volume 39, Number 2
Table 1
Spearman correlation between manual scores and ROUGE metrics on TAC 2009 data
(53 systems). All correlations are highly significant with p-value < 10?10.
ROUGE variant Pyramid Responsiveness
ROUGE-1 0.88 0.76
ROUGE-2 0.94 0.82
ROUGE-SU4 0.92 0.79
methods such as pyramid involve even more annotations at the clause level. Although
responsiveness does not require gold-standard summaries, in a system development
setting, responsiveness judgments are resource-intensive. It requires judges to directly
assign scores to summaries, so humans are in the loop each time the evaluation needs
to be done, making it rather costly. For ROUGE, however, once the human summaries
are created, the scores can be computed automatically for repeated system development
runs. This benefit has made ROUGE immensely popular. But the initial investment of
time for gold-standard creation is still necessary.
Another important point is that for TAC, the gold standards are created by trained
assessors at NIST. Non-expert evaluation options such asMechanical Turk have recently
been explored by Gillick and Liu (2010). They provided annotators with gold-standard
references and system summaries and asked them to score the system summaries on a
scale from 1 to 10 with respect to how well they convey the same information as the
models. They analyzed how these scores are related to responsiveness judgments given
by the expert TAC assessors. The study assessed only eight automatic systems from
TAC 2009 and the correlation between the ratings from experts and Mechanical Turk
annotations was 0.62 (Spearman). The analysis concludes that evaluations produced in
this way tend to be noisy.
One reason was that non-expert annotators were quite influenced by the readability
of the summaries. For example, they tended to assign high scores to the baseline
summary that picks the lead paragraph. The baseline summary, however, is ranked
by expert annotators as low in responsiveness compared to other systems? summaries.
Further, the non-expert evaluation led to few significant differences in the system rank-
ings (score of system A is significantly greater/lesser than that of B) compared with the
TAC evaluations of the same systems.
Another problemwith non-expert evaluation is the quality of themodel summaries.
Evaluations based on model summaries assume that the gold standards are of high
quality. Through the years at TAC, considerable effort has been invested to ensure that
the evaluation scores do not vary depending on the particular gold standard. In the
early years of TAC only one gold-standard summary was used. During this time, papers
reported ANOVA tests examining the factors that most influenced summary scores
from the evaluations and found that the identity of the judge turned out to be the most
significant factor (McKeown et al 2001; Harman andOver 2004). But it is desirable that a
model summary or a human judgment be representative of important content in general
and does not depict the individual biases of the person who created the summary or
made the judgment. So the evaluationmethodologywas refined to remove the influence
of the assessor identity on the evaluation. The pyramid evaluation was also developed
with this goal of smoothing out the variation between judges. Gillick and Liu (2010)
point out that Mechanical Turk evaluations have this undesirable outcome: The identity
272
Louis and Nenkova Automatic Content Evaluation
of the judges turns out to be the most significant factor influencing summary scores.
Gillick and Liu do not elicit model summaries, only direct judgments on quality. We
suspect that the task would only be harder if model summaries were to be created by
non-experts.
The problem that has been little addressed by any of these discussed metrics is eval-
uation when there are no gold-standard summaries available. Systems are developed
by fine-tuning on the TAC data sets, but in non-TAC data sets in novel or very large
domains model summaries may not be available. Even though ROUGE provides good
performance in automatic evaluation, it is not usable under these conditions. Further,
pyramid and ROUGE use multiple gold-standard summaries for evaluation (ROUGE
correlates with human judgments better when computed using multiple models; we
discuss this aspect further in Section 5) so even a single gold-standard summary may
not be sufficient for reliable evaluation.
In our work, we propose fully automatic methods for content evaluation which
can be used in the absence of human summaries. We also explore methods to further
improve the evaluation performance when only one model summary is available.
3. Data and Evaluation Plan
In this section, we describe the data we use throughout our article. We carry out
our analysis on the test sets and system scores from TAC 2009. TAC 2009 is also the
year when NIST introduced a special track called AESOP (Automatically Evaluating
Summaries of Peers). The goal of AESOP is to identify automatic metrics that correlate
well with human judgments of summary quality.
We use the data from the TAC 2009 query focused-summarization task.5 Each input
consists of ten news documents. In addition, the user?s information needs associated
with each input is given by a query statement consisting of a title and narrative. An
example query statement is shown here:
Title: Airbus A380
Narrative: Describe developments in the production and launch of the Airbus A380.
A system must produce a summary that addresses the information required by the
query. The maximum length for summaries is 100 words.
The test set contains 44 inputs, and 53 automatic systems (including baselines)
participated that year. These systems were manually evaluated for content using both
pyramid and responsiveness methods. In TAC 2009, two oracle systems were intro-
duced during evaluation whose outputs are in fact summaries created by people. We
ignore these two systems and use only the automatic participant submissions and the
automatic baseline systems.
As a development set, we use the inputs, summaries, and evaluations from the
previous year, TAC 2008. There were 48 inputs in the query-focused task in 2008 and
58 automatic systems participated.
TAC 2009 also involved an update summarization task and we obtained similar
results on the summaries from this task. In this article, for clarity we only present results
5 http://www.nist.gov/tac/2009/Summarization/update.summ.09.guidelines.html.
273
Computational Linguistics Volume 39, Number 2
on evaluating the query-focused summaries, but the update task results are described
in detail in Louis and Nenkova (2008, 2009a, 2009c).
3.1 Evaluating Automatic Metrics
For each of our proposed metrics, we need to assess their performance in replicating
manually produced rankings given by the pyramid and responsiveness evaluations.
We use two measures to compare these human scores for a system with the automatic
scores from one of our metrics:
a) SPEARMAN CORRELATION: Reporting correlations with human evaluation metrics
is the norm for validating automatic metrics. We report Spearman correlation, which
compares the rankings of systems produced by the two methods instead of the actual
scores assigned to systems.
b) PAIRWISE ACCURACY: To complement correlation results with numbers that have
easier intuitive interpretation, we also report the pairwise accuracy of our metrics in
predicting the human scores. For every pair of systems (A, B), we examine whether
their pairwise ranking (eitherA > B,A < B, orA = B) according to the automatic metric
agrees with the ranking of the same pair according to human evaluation. If it does, the
pair is concordant with human judgments. The pairwise accuracy is the percentage of
concordant pairs out of the total system pairs. This accuracy measure is more inter-
pretable than correlations in terms of the errors made by a metric. A metric with 90%
accuracy incorrectly flips 10% of the pairs, on average, in a ranking it produces. This
measure is inspired by the Kendall tau coefficient.
We test themetrics for success in replicating human scores overall across the full test
set as well as identifying good and bad summaries for individual inputs. We therefore
report the correlation and accuracy of our metrics at the following two levels.
a) SYSTEM LEVEL (MACRO): The average score for a system is computed over the entire
set of test inputs using both manual and our automatic methods. The correlations
between ranks assigned to systems by these average scores will be indicative of the
strength of our features to predict overall system rankings on the test set. Similarly, the
pairwise accuracies are computed using the average scores for the systems in the pair.
b) INPUT LEVEL (MICRO): For each individual input, we compare the rankings for the
system summaries using manual and automatic evaluations. Here the correlation or
accuracy is computed for each input. For correlations, we report the percentage of inputs
for which significant correlations (p-value < 0.05) were obtained. For accuracy, the
systems are paired within each input. Then these pairs for all the inputs are put together
and the fraction of concordant pairs is computed. Micro-level analysis highlights the
ability of an evaluation metric to identify good and poor quality system summaries
produced for a specific input and this task is bound to be harder than system level
predictions. For example, even with wrong prediction of rankings on a few inputs, the
average scores (macro-level) for a system might not be affected.
In the following sections, we describe three experiments in which we analyze the
possibility of performing automatic evaluation involving only minimal or no human
judgments: Using input?summary similarity (Section 4), using system summaries as
pseudomodels alongside gold-standard summaries created by people (Section 5), and
using the collection of system summaries as a gold standard (Section 6). All the auto-
matic systems, including baselines, were evaluated.
274
Louis and Nenkova Automatic Content Evaluation
4. Input?Summary Similarity: Evaluation Using Only the Source Text
Here we present and evaluate a suite of metrics which do not require gold-standard
human summaries for evaluation. The underlying intuition is that good summaries will
tend to be similar to the input in terms of content. Accordingly, we use the similarity of
the distribution of terms in the input and summaries as a measure of summary content.
Although the motivation for this metric is highly intuitive, it is not clear how simi-
larity should be defined for this particular problem. Here we provide a comprehensive
study of input?summary similarity metrics and show that some of these measures
can indeed be very accurate predictors of summary quality even while using no gold-
standard human summaries at all.
Prior to our work, the proposal for using the input for evaluation has been brought
up in a few studies. These studies did not involve a direct evaluation of the capacity
of input?summary similarity to replicate human ratings, however, and they did not
compare similarity metrics for the task. Because large scale manual evaluation results
are available now, our work is the first to evaluate this possibility in a direct manner
and involving study of correlations with different types of human evaluations. In the
following section we detail some of the prior studies on input?summary similarity for
summary evaluation.
4.1 Related Work
One of the motivations for using the input text rather than gold-standard summaries
comes from the need to perform large scale evaluations with test sets comprised of
thousands of inputs. Creating human summaries for all of themwould be an impossible
task indeed.
In Radev and Tam (2003), therefore, a large scale fully automatic evaluation of eight
summarization systems on 18,000 documents was performed without any human effort
by using the idea of input?summary similarity. A search engine was used to rank docu-
ments according to their relevance to a given query. The summaries for each document
were also ranked for relevance with respect to the same query. For good summarization
systems, the relevance ranking of summaries is expected to be similar to that of the
full documents. Based on this intuition, the correlation between relevance rankings
of summaries and original documents was used to compare the different systems. A
system whose summaries obtained highly similar rankings to the original documents
can be considered better than a system whose rankings have little agreement.
Another situation where input?summary similarity was hypothesized as a possible
evaluation was in work concerned with reducing human bias in evaluation. Because
humans vary considerably in the content they include for the same input (Rath,
Resnick, and Savage 1961; van Halteren and Teufel 2003), rankings of systems are
rather different depending on the identity of the model summary used (also noted
by McKeown et al [2001] and Jing et al [1998]). Donaway, Drummey, and Mather
(2000) therefore suggested that there are considerable benefits to be had in adopting a
method of evaluation that does not require human gold standards but instead directly
compares the original document and its summary. In their experiments, Donaway,
Drummey, and Mather demonstrated that the correlations between manual evaluation
using a gold-standard summary and
a) manual evaluation using a different gold-standard summary
275
Computational Linguistics Volume 39, Number 2
b) automatic evaluation by directly comparing input and summary6
are the same. Their conclusion was that such automatic methods should be seriously
considered as an alternative to evaluation protocols built around the need to compare
with a gold standard.
These studies, however, do not directly assess the performance of input?summary
similarity for ranking systems. In Louis and Nenkova (2009a), we provided the first
study of several metrics for measuring similarity for this task and presented correla-
tions of these metrics with human produced rankings of systems. We have released a
tool, SIMetrix (Summary-Input Similarity Metrics), which computes all the similarity
metrics that we explored.7
4.2 Metrics for Computing Similarity
In this section, we describe a suite of similarity metrics for comparing the input and
summary content. We use cosine similarity, which is standard for many applications.
The other metrics fall under three main classes: distribution similarity, summary likeli-
hood, and use of topic signature words. The distribution similarity metrics compare the
distribution of words in the input with those in the summary. The summary likelihood
metrics are based on a generative model of word probabilities in the input and use
the model to compute the likelihood of the summary. Topic signature metrics focus on a
small set of descriptive and topical words from the input and compare them to summary
content rather than using the full vocabulary of the input.
Both input and summary words were stopword-filtered and stemmed before com-
puting the features.
4.2.1 Distribution Similarity. Measures of similarity between two probability distribu-
tions are a natural choice for our task. One would expect good summaries to be charac-
terized by low divergence between probability distributions of words in the input and
summary, and by high similarity with the input.
We experimented with three common measures: Kullback Leibler (KL) divergence,
Jensen Shannon (JS) divergence, and cosine similarity.
These three metrics have already been applied for summary evaluation, albeit in
a different context. In their study of model-based evaluation, Lin et al (2006) used KL
and JS divergences to measure the similarity between human and machine summaries.
They found that JS divergence always outperformed KL divergence. Moreover, the per-
formance of JS divergence was better than standard ROUGE scores for multi-document
summarization when multiple human models were used for the comparison.
The use of input?summary similarity in Donaway, Drummey, and Mather (2000),
which we described in the previous section, is more directly related to our work. But
here, inputs and summaries were compared using only one metric: cosine similarity.
Kullback Leibler (KL) divergence: The KL divergence between two probability distri-
butions P and Q is given by
D(P||Q) =
?
w
pP(w) log2
pP(w)
pQ(w)
(2)
6 They used cosine similarity to perform the input?summary comparison.
7 http://www.seas.upenn.edu/?lannie/IEval2.html.
276
Louis and Nenkova Automatic Content Evaluation
It is defined as the average number of bits wasted by coding samples belonging to P
using another distribution Q, an approximate of P. In our case, the two distributions
of word probabilities are estimated from the input and summary, respectively. Because
KL divergence is not symmetric, both input?summary and summary?input divergences
are introduced as metrics. In addition, the divergence is undefined when pP(w) > 0 but
pQ(w) = 0. We perform simple smoothing to overcome the problem.
p(w) =
C+ ?
N + ? ? B (3)
Here C is the count of word w and N is the number of tokens; B = 1.5|V|, where V
is the input vocabulary and ? was set to a small value of 0.0005 to avoid shifting too
much probability mass to unseen events.
Jensen Shannon (JS) divergence: The JS divergence incorporates the idea that the dis-
tance between two distributions cannot be very different from the average of distances
from their mean distribution. It is formally defined as
J(P||Q) = 1
2
[D(P||A)+D(Q||A)], (4)
where A = P+ Q2 is the mean distribution of P and Q. In contrast to KL divergence,
the JS distance is symmetric and always defined. We compute both smoothed and
unsmoothed versions of the divergence as summary scores.
Vector space similarity: The third metric is cosine overlap between the tf ? idf vector
representations of input and summary contents.
cos? =
vinp.vsumm
||vinp|| ||vsumm||
(5)
We compute two variants:
1. Vectors contain all words from input and summary.
2. Vectors contain only topic signature words from the input and all words of
the summary.
Topic signatures are words highly descriptive of the input, as determined by the
application of the log-likelihood test (Lin and Hovy 2000). Using only topic signatures
from the input to represent text is expected to be more accurate because the reduced
vector has fewer dimensions compared with using all the words from the input.
4.2.2 Summary Likelihood. For this approach, we view summaries as being generated
according to word distributions in the input. Then the probability of a word in the input
would be indicative of how likely it is to be emitted into a summary. Under this gen-
erative model, the likelihood of a summary?s content can be computed using different
methods and we expect the likelihood to be higher for better quality summaries.
We compute both a summary?s unigram probability as well as its probability under
a multinomial model.
277
Computational Linguistics Volume 39, Number 2
Unigram summary probability:
(pinpw1)
n1 (pinpw2)
n2 ...(pinpwr)
nr (6)
where pinpwi is the probability in the input of word wi, ni is the number of times wi
appears in the summary, and w1. . .wr are all words in the summary vocabulary.
Multinomial summary probability:
N!
n1!n2! . . . nr!
(pinpw1)
n1 (pinpw2)
n2 . . . (pinpwr)
nr (7)
where N = n1 + n2 + . . .+ nr is the total number of words in the summary.
4.2.3 Use of Topic Words in the Summary. Summarization systems that directly optimize
the number of topic signature words during content selection have fared very well
in evaluations (Conroy, Schlesinger, and O?Leary 2006). Hence the number of topic
signatures from the input present in a summary might be a good indicator of summary
content quality. In contrast to the previous methods, by limiting to topic words, we use
only a representative subset of the input?s words for comparing with summary content.
We experiment with two features that quantify the presence of topic signatures in a
summary:
1. The fraction of the summary composed of input?s topic signatures.
2. The percentage of topic signatures from the input that also appear in the
summary.
Although both features will obtain higher values for summaries containing many
topic words, the first is guided simply by the presence of any topic word and the second
measures the diversity of topic words used in the summary.
4.2.4 Feature Combination Using Linear Regression. We also evaluated the performance
of a linear regression metric combining all of these features. During development, the
value of the regression-based score for each summary was obtained using a leave-one-
out approach. For a particular input and system-summary combination, the training set
consisted only of examples which included neither the same input nor the same system.
Hence during training, no examples of either the test input or system were seen.
4.3 Results
We first present an analysis of all the similarity metrics on our development data,
TAC?08. In the next section, we analyze the performance of our two best features on
the TAC?09 data set.
4.3.1 Feature Analysis: Which Similarity Metric is Best?. Table 2 shows the macro-level
Spearman correlations between manual and automatic scores averaged across the
48 inputs in TAC?08.
Overall, we find that both distribution similarity and topic signature features pro-
duce system rankings very similar to those produced by humans. Summary likelihood,
on the other hand, turns out to not be predictive of content selection performance. The
278
Louis and Nenkova Automatic Content Evaluation
Table 2
Spearman correlation on the macro level for TAC?08 data (58 systems). All results are highly
significant with p-values < 0.000001 except unigram and multinomial summary probability,
which are not significant even at the 0.05 level.
Features Pyramid Responsiveness
JS div ?0.880 ?0.736
JS div smoothed ?0.874 ?0.737
% of input topic words 0.795 0.627
KL div summary?input ?0.763 ?0.694
cosine overlap, all words 0.712 0.647
% of summary = topic words 0.712 0.602
cosine overlap, topic words 0.699 0.629
KL div input?summary ?0.688 ?0.585
multinomial summary probability 0.222 0.235
unigram summary probability ?0.188 ?0.101
regression 0.867 0.705
ROUGE-1 recall 0.859 0.806
ROUGE-2 recall 0.905 0.873
linear regression combination of features obtains high correlations with manual scores
but does not lead to better results than the single best feature: JS divergence.
JS divergence obtains the best correlations with both types of manual scores?
0.88 with pyramid score and 0.74 with responsiveness. The regression metric performs
comparably, with correlations of 0.86 and 0.70. The correlations obtained by both JS
divergence and the regression metric with pyramid evaluations are in fact better than
that obtained by ROUGE-1 recall (0.85).
The best topic signature-based feature?the percentage of input?s topic signatures
that are present in the summary?ranks next only to JS divergence and regression. The
correlations between this feature and pyramid and responsiveness evaluations are 0.79
and 0.62, respectively. The proportion of summary content composed of topic words
performs worse as an evaluation metric with correlations 0.71 and 0.60. This result
indicates that summaries that cover more topics from the input are judged to have better
content than those in which fewer topics are mentioned.
Cosine overlaps and KL divergences obtain good correlations but still lower than
JS divergence and the percentage of input topic words. Further, rankings based on
unigram and multinomial summary likelihood do not correlate significantly with
manual scores.
On a per input basis, the proposed metrics are not that effective in distinguishing
which summaries have good and poor content. The minimum and maximum correla-
tions with manual evaluations across the 48 inputs are given in Table 3. The number
and percentage of inputs for which correlations were significant are also reported.
JS divergence obtains significant correlations with pyramid scores for 73%. The best
correlation was 0.71 on a particular input and the worst performance was 0.27 correla-
tion for another input. The results are worse for other features and for comparison with
responsiveness scores.
At the micro level, combining features with regression gives the best result overall,
in contrast to the findings for the macro-level setting. This result has implications for
system development; no single feature can reliably predict good content for a partic-
ular input. Even a regression combination of all features is a significant predictor of
279
Computational Linguistics Volume 39, Number 2
Table 3
Spearman correlations at micro level for TAC?08 data (58 systems). Only the minimum and
maximum values of the significant correlations are reported, together with the number and
percentage of inputs that obtained significant correlation.
Pyramid Responsiveness
number number
Features max min significant (%) max min significant (%)
JS div ?0.714 ?0.271 35 (72.9) ?0.654 ?0.262 35 (72.9)
JS div smoothed ?0.712 ?0.269 35 (72.9) ?0.649 ?0.279 33 (68.8)
KL div summary-input ?0.736 ?0.276 35 (72.9) ?0.628 ?0.261 35 (72.9)
% of input topic words 0.701 0.286 31 (64.6) 0.693 0.279 29 (60.4)
cosine overlap - all words 0.622 0.276 31 (64.6) 0.618 0.265 28 (58.3)
KL div input-summary ?0.628 ?0.262 28 (58.3) ?0.577 ?0.267 22 (45.8)
cosine overlap - topic words 0.597 0.265 30 (62.5) 0.689 0.277 26 (54.2)
% summary = topic words 0.607 0.269 23 (47.9) 0.534 0.272 23 (47.9)
multinomial summary prob. 0.434 0.268 8 (16.7) 0.459 0.272 10 (20.8)
unigram summary prob. 0.292 0.261 2 (4.2) 0.466 0.287 2 (4.2)
regression 0.736 0.281 37 (77.1) 0.642 0.262 32 (66.7)
ROUGE-1 recall 0.833 0.264 47 (97.9) 0.754 0.266 46 (95.8)
ROUGE-2 recall 0.875 0.316 48 (100) 0.742 0.299 44 (91.7)
content selection quality in only 77% of the cases. For example, a set of documents,
each describing a different opinion on an issue, is likely to have less repetition on
both the lexical and content unit levels. Because the input?summary similarity metrics
rely on the word distribution of the input for clues about important content, their
predictiveness will be limited for such inputs.8 Follow-up work to our first results on
fully automatic evaluation by Saggion et al (2010) has assessed the usefulness of the JS
divergence measure for evaluating summaries from other tasks and for languages other
than English. Whereas JS divergence was significantly predictive of summary quality
for other languages as well, it did not work well for tasks where opinion and biograph-
ical type inputs were summarized. We provide further analysis and some examples in
Section 7.
Overall, the micro level results suggest that the fully automatic measures we ex-
amined will not be useful for providing information about summary quality for an
individual input. For averages over many test sets, the fully automatic evaluations
give more reliable results, and are highly correlated with rankings produced by manual
evaluations. On the other hand, model summaries written for the specific input would
give a better indication of what information in the input was important and interesting.
This is indeed the case as we shall see from the ROUGE scores in the next section.
4.3.2 Comparison with ROUGE. The aim of our study is to assess metrics for evaluation
in the absence of human gold standards, scenarios where ROUGE cannot be used.
We do not intend to directly compare the performance of ROUGE with our metrics,
8 In fact, it would be surprising to find an automatically computable feature or feature combination which
would be able to consistently predict good content for all individual inputs. If such features existed,
an ideal summarization system would already exist.
280
Louis and Nenkova Automatic Content Evaluation
therefore. We discuss the correlations obtained by ROUGE in the following, however, to
provide an idea of the reliability of our metrics compared with evaluation quality that
is provided by ROUGE and multiple human summaries.
At the macro level, the correlation between ROUGE-1 and pyramid scores is 0.85
(Table 2). For ROUGE-2 the correlation with pyramid scores is 0.90, practically identical
with JS divergence.
Because the performance of these two measures seem close, we further analyzed
their errors. The focus of this analysis is to understand if JS divergence and ROUGE-2
are making errors in ordering the same systems or whether their errors are different.
This result would also help us to understand if ROUGE and JS divergence have com-
plementary strengths that can be combined. For this, we considered pairs of systems
and computed the better system in each pair according to the pyramid scores. Then,
for ROUGE-2 and JS divergence, we recorded how often they provided the correct
judgment for the pairs as indicated by the pyramid evaluation. There were 1,653 pairs
of systems at the macro level and the results are in Table 4.
This table shows that a large majority (80%) of the same pairs are correctly predicted
by both ROUGE and JS divergence. Another 6% of the pairs are such that both metrics
do not provide the correct judgment. Therefore, ROUGE and JS divergence appear to
agree on a large majority of the system pairs. There is a small percentage (14%) that is
correctly predicted by only one of the metrics. The chances of combining ROUGE and
JS divergence to get a better metric appears small, therefore. To test this hypothesis, we
trained a simple linear regression model combining JS divergence and ROUGE-2 scores
as predictors for the pyramid scores and tested the predictions of this model on data
from TAC 2009. The combination did not give improved correlations compared with
using ROUGE-2 alone.
In the case of manual responsiveness, which combines aspects of linguistic quality
along with content selection evaluation, the correlation with JS divergence is 0.73.
For ROUGE, it is 0.80 for R1 and 0.87 for R2. Here, ROUGE-1 outperforms all the
fully automatic evaluations. This is evidence that the human gold-standard summaries
provide information that is unlikely to ever be approximated by information from the
input alone, regardless of feature sophistication.
At the micro level, ROUGE clearly does better than all the fully automatic measures
for replicating both pyramid and responsiveness scores. The results are shown in the
last two rows of Table 3. ROUGE-1 recall obtains significant correlations for over 95%
of inputs for responsiveness and 98% of inputs for pyramid evaluation compared to
73% (JS divergence) and 77% (regression). Undoubtedly, at the input level, comparison
with model summaries is substantially more informative.
When gold-standard summaries are not available, however, our features can pro-
vide reliable estimates of system quality when averaged over a set of test inputs.
Table 4
Overlap between ROUGE-2 and JS divergence predictions for the best system in a pair
(TAC 2008, 1,653 pairs). The gold-standard judgment for a better system is computed
using the pyramid scores.
JSD correct JSD incorrect
ROUGE-2 correct 1,319 (79.8%) 133 (8.1%)
ROUGE-2 incorrect 96 (5.8%) 105 (6.3%)
281
Computational Linguistics Volume 39, Number 2
Table 5
Input?summary similarity evaluation: Results on TAC?09 (53 systems).
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Metric py resp py resp py resp py resp
JS div 0.74 0.70 84.1 75.0 78.0 75.7 65.1 50.1
Regr 0.77 0.67 81.8 65.9 80.1 74.8 64.7 49.4
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
4.3.3 Results on TAC?09 Data. To evaluate our metrics for fully automatic evaluation, we
make use of the TAC?09 data. The regression metric was trained on all of the 2008 data
with pyramid scores as the target. Table 5 shows the results on the TAC?09 data. We
also report the correlations obtained by ROUGE-SU4 because it was the official baseline
measure adopted at TAC?09 for comparison of automatic evaluation metrics.
The correlations are lower than on our development set. The highest correlation at
macro level is 0.77 (regression) in contrast to 0.88 (JS divergence) and 0.86 (regression)
obtained on the TAC?08. The regression metric turns out better than JS divergence on
the TAC?09 data for predicting pyramid scores. JS divergence continues to be the best
metric on the basis of correlations with responsiveness, however.
In terms of the pairwise scores, the automatic metrics have 80% accuracy in pre-
dicting the pyramid scores at the system level, about 8% lower than that obtained by
ROUGE. For responsiveness, the best accuracy is obtained by regression (75%). This
result shows that the ranking according to responsiveness is likely to have a large
number of flips. ROUGE is 5 percentage points better than regression for predicting
responsiveness but this value is still low compared to accuracies in replicating the
pyramid scores.
The pairwise accuracy at the micro level is 65% for the automatic metrics and here
the gap between ROUGE and our metrics is 5 percentage points but it is a significant
percentage as the total pairs at micro level are about 60,000 (all pairings of 53 systems
in 44 inputs).
Overall, the performance of the fully automatic evaluation is still high for use
during system development. A further advantage is that these metrics are consistently
predictive across two years as shown by these results. In Section 7, we analyze some
reasons for the difference in performance in the two years. In terms of best metrics, both
JS divergence and regression turn out to be useful with little difference in performance
between them.
5. Pseudomodels: Use of System Summaries in Addition to Human Summaries
Methods such as pyramid use multiple human summaries to avoid bias in evaluation
when using a single gold standard. ROUGE metrics are also currently used with mul-
tiple models, when available. But often, even if gold-standard summaries are available
on non-standard test sets, they are few in number. Data sets with one gold-standard
summary (such as abstracts of scientific papers and editor-produced summaries of news
articles) are common. The question now is whether we can provide the same quality
282
Louis and Nenkova Automatic Content Evaluation
evaluation using a single gold-standard summary as compared to using several gold
standards.
To tackle this problem, we propose the use of pseudomodel system summaries.
Our approach is as follows: We first predict the scores of systems on the basis of the few
available models. The top ranking systems from this evaluation are then considered
as ?pseudo-models;? their summaries are added to the gold-standard set alng with
the existing human models. The final evaluation scores are produced by comparison
with this expanded model set?original model summaries plus the pseudomodels. Our
hypothesis is that the scores produced after the addition of pseudomodels would be
more reliable and correlate better with human scores compared with evaluation using
a single model summary.
Before we describe our method, we provide a glimpse of the variation in evaluation
quality depending on the number of models used. Previous studies have shown that
at the system level, system rankings even with a single model will be stable when
computed over a large enough number of test inputs. Harman and Over (2004) show
that the relative ranks of systems computed using one model do not change when
computed using another model when the number of inputs is large. Again under the
same conditions of having a large number of inputs, Lin (2004a) and Owkzarzak and
Dang (2009) show that ROUGE correlations with human scores are stable when using
few human models. In machine translation evaluation, similar results are noted by
Zhang and Vogel (2010), who found that the lack of additional reference translations
can be handled by evaluating the systems on more test examples.
Multiple models are particularly important for evaluation at the level of individual
inputs, however. Table 6 shows the difference in correlations and pairwise accuracy of
ROUGE with human scores when one and four model summaries are used. We picked
the first model in alphabetical order of their names for the computation of correlation
between metrics and a single model.
At the system level, the correlations from both set-ups are similar. But at the micro
level, there is considerable difference in performance. Using all four models, significant
correlations with pyramid scores are obtained for 95% of the inputs. The evaluations
that rely on a single model produce significant correlations for only 84% of the inputs,
however. For responsiveness scores, which are model-independent, we see that the
micro-level evaluations have a smaller increase as more models are added (79% to 81%).
Again in terms of pairwise accuracy, the accuracy in predicting micro-level pyramid
scores improves by 4% when additional models are used and the improvement is 3%
for predicting responsiveness scores. Given this difference in performance when one
and many models are used, we investigate how to improve evaluation when only one
model is available.
Table 6
ROUGE evaluation with different number of models: macro level (Spearman correlations), micro
level (percentage of inputs with significant correlations on TAC?09 data). No. of systems = 53.
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Task py resp py resp py resp py resp
RSU4 - 1 model 0.92 0.80 84.1 79.5 88.3 80.3 66.1 50.7
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
283
Computational Linguistics Volume 39, Number 2
We explore the possibility of augmenting the model set with good system sum-
maries. These system summaries or ?pseudomodels? are chosen to be the ones which
receive high scores based on the one available model summary. We expect that the
benefit of pseudomodels will be noticeable in micro-level correlations with pyramid
scores. At the macro level, even with multiple human models there is no improvement
in correlations compared with a single model, and the addition of less-ideal system
summaries is not likely to be better than adding human summaries.
5.1 Related Work
The idea of using system output for evaluationwas introduced in the context of machine
translation by Albrecht andHwa (2007, 2008). In their method, Albrecht andHwa (2007)
designate some systems to act as pseudoreferences. Then, every candidate translation
to be evaluated is compared to the translations produced by the pseudoreferences using
a variety of similarity metrics. Each similarity value is then used as a feature and
trained to predict the human assigned score for that candidate translation. They show
that the scores produced by their regression metric using only system-based references
correlates with human judgments to the same extent as scores produced using multiple
human reference translations. Also, when the regression method was used with human
references and some pseudoreferences put together, the correlations obtained by the
final metric was better than using the human references alone.
In Albrecht and Hwa (2007), pseudoreferences of different quality?best, moderate
and worst?are chosen using the gold-standard judgments and evaluated for use as
pseudoreferences. They found that having the best systems as pseudoreferences worked
best, although even adding the worst system as pseudoreference gave reasonable per-
formance as their regression approach is trained to predict quality by comparison to the
standard of the reference. In their work, however, pseudoreferences of different quality
are chosen in an oracle manner (using the human-assigned scores). This setting is not
practical because it depends on the actual system scores. In later work, Albrecht and
Hwa (2008) use off-the-self machine translation systems as pseudoreferences and show
that they can contribute to good results. This later work is a more realistic set-up and
here regression is important because we have no guarantees as to the quality of the
off-the-shelf systems on the test data.
A similar idea of augmenting machine output to human gold standard was ex-
plored in Madnani et al (2007) in the context of machine translation (MT). For tuning
MT systems, often multiple reference translations are required. Madnani et al aug-
mented reference translations of a sentence with automatically generated paraphrases
of the reference. They found in the experiments that such augmentation helped in
MT tuning?the number of reference translations needed could be cut in half and
compensated with automatic paraphrases.
5.2 Choice of Pseudoreference Systems
For this evaluation, the choice of the pseudoreference system is an important step. In
this section, we detail some development experiments that we performed to understand
how to best choose such pseudoreferences for the summary evaluation task.
We examined a similar regression approach as followed by Albrecht and Hwa
(2007). We chose systems of different quality (best, mediocre, worst) based on the
284
Louis and Nenkova Automatic Content Evaluation
oracle human-assigned scores. The remaining systems were taken as the evaluation
set. For each summary in the evaluation data, we computed features to indicate their
similarity with the summaries of the chosen pseudoreference systems. Our similarity
features were the recall scores from ROUGE overlaps. We computed one feature each
for unigram, bigram, trigram, and four-gram ROUGE scores. Each of these four features
is computed for each pseudoreference summary.
The scores are used in a linear regression model to predict the summary score.
We used a cross-validation approach where the summaries from one of the evaluation
systems were used as the test set and the summaries from the remaining systems
are used for training the regression model. Then the average predicted score of each
system in the evaluation data was computed and compared with their average scores
as assigned during manual evaluations.
The experiment was performed using data from four years of DUC conferences,
2001 to 2004. The manual scores in these earlier DUC years were the content coverage
scores (described in Section 2.1), which use a single model summary for comparison.
Table 7 shows the Spearman correlations between the scores from evaluations only
against the pseudoreferences and those from the manual evaluation with the single
model. The different settings for choice of pseudoreference systems are also indicated.
These results showed that using the best systems as pseudoreferences provided the
best performance across different years. When only worst or only mediocre systems
were used, the performance was much worse for predicting system scores. Even when
the best systems were augmented with the worst systems as pseudoreferences, the
evaluation quality decreased compared with using the best systems only.
Whereas Albrecht and Hwa (2007, 2008) obtained a slight improvement by also
using a worst quality pseudoreference in the mix, for summarization it is better to have
only the best systems. One reason for this difference could be that for summary eval-
uation, examples of worse summaries are not very informative. Two good summaries
may have considerable variation in the content. When a summary is similar to a best
system, therefore, we can say that the candidate summary is also of good quality. On
the other hand, when a candidate summary is similar to a worst system summary, it
may either be a worse summary or it may be a good summary with different content
than the best system?s summary. Indeed, when ROUGE was first introduced, it was
heavily emphasized that it is a recall measure and that precision-oriented measures
do worse. Hence the weights learned for the similarity with the worst system may
not be very informative. In summarization, the space of both good summaries and
worse summaries for the same input is large. Having more examples of good sum-
maries appears to benefit evaluation more compared with having samples of worst
quality.
Table 7
Spearman correlations between pseudoreference-based regression scores and manual content
scores. The first column lists the type of pseudoreference chosen.
Pseudoreference 2001 2002 2003 task 2 2004 task 2 2004 task 5
2 best systems 0.58 0.77 0.51 0.93* 0.83*
2 worst systems 0.45 ?0.94* ?0.09 0.13 ?0.23
2 mediocre systems 0.72 0.08 0.53 0.64 0.18
2 best, 2 worst systems 0.38 0.20 0.25 0.92 0.73
*The correlation was significant with p-value < 0.05.
285
Computational Linguistics Volume 39, Number 2
Because the best systems turned out to have the maximum potential for acting
as pseudoreferences, we wanted a way to identify some best systems without having
to rely on the oracle scores, as before. This idea is feasible for our set-up. In our
evaluation, we aimed to augment an existing model, so we used the available model
to automatically obtain an idea of some of the good systems from the pool. Then we
chose some of these top systems as pseudoreferences and combined them with the one
available model to form the reference set for final evaluation. Because the reference set
has mostly best summaries, we did not use a regression approach based on similarity to
the different references. Rather, we considered all of them as models and computed
a single ROUGE score comparing a system summary with the pool of model plus
pseudomodel summaries.
5.3 Experimental Set-up
We now detail our experiments on the TAC 2009 data.
TAC provides four model summaries for each input. We assume that only one is
available and choose a model for each input: the first in alphabetical order of identifier
names. Based on this model, we compute the RSU4 scores for all systems. We use two
methods to choose the pseudomodel systems.
In the first approach, we rank all the systems based on their average scores over the
entire test set. The summaries of the top three overall best systems (global selection) are
added to the set of models for all inputs. Alternatively, we also investigate a different
selection method. For each input, the top scoring three summaries are added as models
for that input (local selection). In both cases RSU4 was used to identify the best systems
according to the single available gold standard.
The final rankings for all systems are produced using the RSU4 comparison based
on the expanded set of models (1 human model + 3 pseudomodel summaries). We
implemented a jackknifing procedure so that the systems selected to be pseudomodels
(and therefore reference systems) could also be compared to other systems. For each
input, one of the reference systems (pseudomodels or human model) was removed at a
time from the set of models and added to the set of systems. The scores for the systems
were then computed by comparison with the three remaining models. The final score
for a system summary (not a pseudomodel) is the mean value of the scores with the
four different sets of reference summaries created by the jackknifing procedure. For
pseudomodel systems, a single score value will be obtained per input resulting from
the comparison with the other three models.
5.4 Results
The system and input level performance before and after the addition of pseudomodels
is shown in Table 8. The performance using four human models is shown in the last line
for comparison.
At the macro level, the pseudomodel summaries provide little improvements. Only
for the global model is there an increase in correlation, from 0.80 to 0.82.
As expected, however, for the micro level, pseudomodels prove beneficial. Both
global and local selection methods improve the number of inputs that receive signif-
icant micro-level correlations with pyramid scores. The improvement is close to 10%
compared with using only one model summary. Also note that, after the addition of
pseudomodels, the percentage of significant correlations is 93%, which is only 2% less
compared with the results using four human models (95%).
286
Louis and Nenkova Automatic Content Evaluation
Table 8
Performance before and after the addition of pseudomodel summaries: TAC?09 data
(53 systems).
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Evaluation type py resp py resp py resp py resp
RSU4 - 1 model 0.92 0.80 84.1 79.5 88.3 80.3 66.1 50.7
Global 0.91 0.82 93.2 79.5 88.6 83.5 66.8 51.3
Local 0.92 0.79 93.2 75.0 89.6 80.8 67.4 51.3
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
For responsiveness scores that are model-independent, however, little improve-
ments are seen at both macro and micro levels. The pairwise accuracy at micro level
for responsiveness is 1% better after the addition of the pseudomodels.
Comparing the two methods for selecting the best system that can serve as a
pseudomodel, the global selection of the system that performed best over the entire
available data set appears to be more desirable. It improves the correlations with pyra-
mid scores while keeping the same correlations with responsiveness as with one model.
Local selection provides the same performance as global selection for pyramid scores,
although it decreases the micro-level evaluation quality for responsiveness.
6. Consensus-Based: Evaluation Using Only Collection of System Summaries
From our experiments with pseudomodels, we see that the addition of system sum-
maries to availablemodels proved beneficial and improved themicro-level performance
of ROUGE. One question that arises is whether the collection of system summaries
together will be useful for evaluation without any human models at all. Again, this idea
is related to model-free evaluation. When several systems are available, we investigate
if their collective knowledge can help assess summary quality.
Systems use varied methods to select content, and agreement among systems could
be indicative of important information. This intuition is similar to that behind the man-
ual pyramid method: Facts mentioned only in one human summary are less important
compared to content that is mentioned in multiple human models. For the experiments
reported in this section, we rely entirely on the combined knowledge from system
summaries as a gold standard.
6.1 Related Work
The closest work to this idea of combining system output can be found in the area of
information retrieval (IR). Soboroff, Nicholas, and Cahan (2001) proposed a method
for evaluating IR systems without relevance judgments. In addition to requiring less
human input, the need for automatic evaluation in IR is also motivated by the fact that
for systems such as those on the Internet, the documents keep changing and so it is
difficult to collect relevance judgments that are stable and meaningful for a long time.
287
Computational Linguistics Volume 39, Number 2
Soboroff, Nicholas, and Cahan (2001) combine the top n results from all the systems
and then sample a certain number of documents from this pool. Those documents
selected by many systems are more likely to be in the chosen sample and assumed
to be most relevant. The systems are then evaluated by considering this chosen set of
documents as the gold-standard relevant set.
In our work, we do not attempt to pick out common content explicitly from the
summary pool. If we were to follow the same approach as IR, we would be sampling
sentences from the summary pool. But in multi-document summarization, sentences
from different documents could contain similar content and we do not want to sample
one sentence and use it in the gold standard because then systems would be penalized
for choosing other similar sentences. In our work, therefore, we break down the sen-
tences and represent the content as a probability distribution over words. A summary
is evaluated by comparing its word distribution to that of the pool. We expect that the
distribution would implicitly capture the common content.
6.2 Evaluation Set-up
For each input, we collect all the summaries produced by automatic systems and
calculate the probabilities of words in the combined set. In this way, we obtain a global
probability distribution of words selected in system summaries. In this distribution,
the content selected by multiple systems will be more prominent, representing the
more important information. The word probabilities from each individual summary
are then calculated and compared to the overall distribution using JS divergence. If we
assume that system summaries are collectively indicative of important content, then
good summaries will tend to have properties that are similar to this global distribution,
resulting in low divergence values. We compute the correlations of these divergence
values with human-assigned summary scores and Table 9 shows the results from this
evaluation.
6.3 Results
The correlations are on par with those based on multiple human gold standards. At
both macro and micro levels, the correlations and pairwise accuracy are similar to those
obtained by ROUGE comparison with four human models. The macro-level correlation
is 0.93 with pyramid scores, which is very high for a metric that uses no human input
at all. Further, the micro-level correlations are also significant for 90% of the inputs. In
our pseudomodel experiments, the gains after the addition of system summaries were
Table 9
Performance of consensus evaluation approach on TAC?09 data (53 systems). For input level
(micro), the percentage of inputs with significant correlations is reported.
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Evaluation type py resp py resp py resp py resp
SysSumm 0.93 0.81 90.9 86.4 88.8 80.7 65.2 52.7
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
288
Louis and Nenkova Automatic Content Evaluation
modest (only at micro level). Here we see that a large collection of system summaries
by themselves have the information required for evaluation.
From this experiment, we find that consensus among system summaries is indica-
tive of important content. This result suggests that by combining the content selected
by multiple systems, one might be able to build a summary that is better than each
of them individually. In fact, this idea of system consensus has been utilized in the
development of MT systems for quite some time. One approach in MT is rescoring the
n-best list from an individual system?s decoder, and picking the (consensus) translation
that is close on average to all translations. Such rescoring is implemented using a
minimum Bayes risk technique (Kumar and Byrne 2004; Tromble et al 2008). The other
approach is system combinationwhere the output frommultiple systems is combined to
produce a new translation. Several techniques includingminimumBayes risk have been
applied to perform system combination in machine translation. Shared tasks on system
combination have also been organized in recent years to encourage the development
of such methods (Callison-Burch et al 2010, 2011). Such strategies could be a useful
direction to explore for summarization as well.
7. Discussion
In this article, we have discussed metrics for summary evaluation when human sum-
maries are not present. Our results have shown that these metrics in fact correlate highly
with human judgments. But we also need to understand how robust these metrics are
and be aware of their limitations. In this section, therefore, we provide a brief discussion
of the use of these metrics in different settings.
7.1 Including Input?Summary Similarity or Consensus-Based Measures in a
Summarization System
Firstly, because input?summary similarity features are computed using the input, they
can be useful features to incorporate in a summarization system. The combination
of systems to perform evaluation also provides a way to build a better system. The
concern would be how the usefulness of these metrics will change if systems were
also optimizing for them. To optimize a metric such as JS divergence exactly would be
difficult because the JS divergence score cannot be factored or divided among individual
sentences, a necessary condition if the problem should be solved using an Integer Linear
Program as in McDonald (2007) and Gillick and Favre (2009). Therefore only greedy
methods are possible. In fact, KL divergence was greedily optimized in Haghighi and
Vanderwende (2009) to obtain a high performance summarizer. Gaming the evaluation
should carry little concern, however, as thesemetrics are proposedwith a view to tuning
systems.
The metrics we presented are developed for evaluation in a new setting where
model summaries are not available and to aid system development and tuning. Further,
notice from the micro-level evaluation that a single metric such as JS divergence does
not predict content selection performance well for all inputs. System developers should
therefore involve other specialized features as well. Regression of similarity metrics is a
better predictor at the micro level but optimizing that would involve computation of all
metrics. Another point to note here is that these similarity measures and the consensus
pool are only indicative of summary content quality. Other key aspects of summary
quality, however, involve sentence ordering, proper generation of referring expressions
289
Computational Linguistics Volume 39, Number 2
and grammatical sentences, and maintaining non-redundancy. Systems should there-
fore be optimizing for a wide variety of factors and thus input?summary similarity and
consensus evaluation can be used in the final output to measure the content quality of
the summary. Any content evaluation should obviously be accompanied by linguistic
quality evaluation in contrast to the current trend to only report content scores.
The high performance of the JS divergence metric also has another implication
for system development. On average, the JS divergence measure is highly predictive
of summary quality. It indicates that for a large number of inputs in the TAC data
sets, good content can be predicted with high accuracy just based on the input?s term
distributions. Such inputs should therefore be easy to summarize for systems. Although
discourse-based and other semantic approaches to summarization have been proposed,
most of the systems in TAC rely on surface features such as word distributions. In
this situation, we may not be focusing on robust systems that can handle a variety of
inputs. In the early years of DUC, the test set comprised a variety of inputs such as
biographies, collections of multiple events, opinions, and descriptions of single events.
Later years switched to more single-event-type test sets. The results from our analysis
point out that current inputs might be too simple for systems and that the range of
inputs in the TAC conference should be expanded to include some input types where
more sophisticated methods become necessary. Perhaps the input?summary similarity
metrics will be helpful in picking out those inputs that need deeper analysis. In the
following section, we provide some further analysis into the cases where the input?
summary similarity turns out less predictive.
7.2 Input?Summary Similarity and Dependence on Input Characteristics
JS divergence is useful for the average rating of systems on the test set and, in our
case, we have 44 examples over which the scores are averaged. At the micro level,
certain inputs received poor evaluations from JS divergence. Here we provide some
insights into the types of inputs where JS divergence worked and the cases which
proved difficult.
Table 10 shows the titles of articles in input D0913, the input that received the best
evaluation from JSD (correlation of 0.86). These articles were all published on the same
day and deal with the same event, a Supreme Court hearing of a case. This input can
be said to be highly cohesive and to be discussing the same topic. For such inputs,
the term distribution in the input would reflect content importance since some words
have higher probability than others because they are discussed repeatedly in the input
documents. Such a term distribution when compared with summaries will give good
evaluation performance. We can also see that the human summaries for this input (also
shown in Table 10) seem to report the common issues observed in the input. In this
case, therefore, input?summary similarity scores can predict the pyramid scores that
were assigned based on the model summaries. We also show in the table the summary
that is chosen to be best according to JS divergence and the summary that had the
worst score. We find that the best summary indeed conveys some of the main issues
also reported in the human summaries. On the other hand, the low-scoring summary
presents a story line about one of the lawyers involved in the case, which is a peripheral
topic described in only one of the input documents. In fact, the summary scored as
worst by JS divergence has a pyramid score of 0, whereas the chosen best summary has
a pyramid score of 0.39.
On the other hand, summaries for input D0940 obtained only 0.3 correlation us-
ing JSD evaluation. Both ROUGE and consensus evaluation (SysSumm) methods can
290
Louis and Nenkova Automatic Content Evaluation
Table 10
Titles of articles and two human summaries for input D0913-A. The summaries chosen as best
and worst according to JS divergence are also listed.
Articles in input D0913-A
Publication date Title
Mar 02 US Supreme Court examines issue of displaying Ten Commandments
Mar 02 US Supreme Court examines Ten Commandments displays
Mar 02 Supreme Court wrestles with Ten Commandments issue
Mar 02 Justices examine 10 Commandments case
Mar 02 High Course argues in 10 Commandments case
Mar 02 Supreme Course wrestles with Ten Commandments
Mar 02 High Court argues Ten Commandments cases
Mar 02 Texas seeks to keep ten commandments display on capitol grounds
Mar 02 An unlikely journey up the legal ladder
Model summary H
The Supreme Court heard arguments in two cases on March 2nd about the conditions under which
the government could display the Ten Commandments and whether such displays violated the
First Amendment.
The Texas case concerns a 40-year-old granite monument of the Ten Commandments, one of 17 monuments
on the grounds of the state capital.
The Kansas case concerns framed copies of the Ten Commandments which hang with non-religious
documents in two county courthouses.
The displays have been modified twice in response to lower court rulings.
Justice O?Connor is expected to cast the swing votes.
Model summary C
On March 2, 2005, the Supreme Court heard two cases concerning the display of the Ten Commandments
on government property.
In Texas, a homeless former lawyer challenged the constitutionality of an inscribed monument displayed
on state capitol grounds.
In Kentucky, the ACLU claimed copies displayed in two courthouses, modified twice before in response
to court rulings, still violated the First Amendment because the original purpose was religious and the
modifications were a sham.
Supporters argued that the Ten Commandments are a recognized symbol of law, with both secular and
religious functions.
The justices will issue an opinion by late June.
Lowest JSD summary: System 26
The Supreme Court Wednesday wrestled with whether the Constitution allows displays of the
Ten Commandments on government property.
Lower courts have issued conflicting rulings in dozens of cases involving Ten Commandments displays
in recent years.
The justices also heard a challenge to Ten Commandments exhibits in Kentucky courthouses.
Abbott, making his first argument before the high court, contended that the Ten Commandments
monument, while containing a sacred religious text.
Abbott, 47, a former state Supreme Court justice.
It is hardly little ? a constitutional challenge to displaying the Ten Commandments on the grounds of the
Texas Capitol, and so on.
Highest JSD summary: System 39
He said his happiest moment was not when he heard the Supreme Court was taking his case but when
his teenage daughter read the story and tracked him down by e-mail, breaking a long estrangement.
If religious, it was unacceptable, Van Orden said.
He ate on food stamps at the upscale Central Market, pitched his tent nightly and took it down each
morning in a wooded location he did not specify, traveled on a free bus pass granted for a veteran?s
disability and read newspapers and magazines free at newsstands.
evaluate the same summaries, however, with correlation of 0.84 (ROUGE) and 0.74
(SysSumm). The titles of the articles in that input and in the human summaries are
provided in Table 11. This input?s topic is the opening of Disneyland in Hong Kong
but its articles cover varied aspects around the topic such as ticket sales, environmental
291
Computational Linguistics Volume 39, Number 2
Table 11
Titles of articles and two human summaries for input D0940-A. The summaries chosen as best
and worst by JS divergence are also listed.
Articles in input D0940-A
Publication date Title
June 22 Opening of HK Disneyland to be divided into 3 phases
June 26 Disney officials consulted feng shui experts for Hong Kong Disneyland
July 01 Hong Kong Disneyland starts on-line tickets selling
July 01 Disneyland rehearsal days to start in August
July 04 HK Disneyland ticket sale proceeds well
Sept 04 High hopes for Hong Kong Disneyland?s economic impact, but critics say Disney
magic overrated
Sept 08 Hong Kong Park: Classic Disney with an Asian accent
Sept 08 Hong Kong Disneyland won?t cut its maximum capacity despite overcrowding fears
Sept 08 All tickets for opening day of HK Disneyland sold out
Sept 10 Shark fins, stray dogs and smog - Hong Kong Disneyland has had a bumpy ride
Model summary B
Hong Kong Disneyland (HKD) was scheduled to open in three phases in the summer of 2005.
Early to mid-August transportation would be available to some areas of the park.
August-Sept 11 all public services would become available.
Sept 12 the grand opening of the park and hotels would take place.
On Aug 16 HKD will begin its rehearsals to which special guests will be invited.
In September, HKD announced it would not cut its daily capacity of 30,000 visitors.
On Sept 9 it was revealed that all tickets for the grand opening had been sold.
Model summary H
Hong Kong Disneyland, a joint venture between Disney and the Hong Kong government, was scheduled
to open on 12 September.
Rehearsal days were staged for a month before opening, giving ?cast members? a chance to practice their
performances.
Disneyland refused to reduce its daily maximum capacity of 30,000 despite complaints from early visitors
about large crowds and long lines.
All 16,000 opening day tickets were sold out.
The park is vintage Disney, with aspects of local culture including feng shui, Asian foods, and signs
in Chinese.
Protests forced them to remove shark fin soup from their menus.
Lowest JSD summary: System 54
But critics say Hong Kong Disneyland is overrated.
The opening of Hong Kong Disneyland is expected to turn on a new page of Hong Kong tourism, with
focus on family tourists, she said.
Hong Kong Disneyland will stage its rehearsal from Aug. 16 to the theme park?s opening on Sept. 12,
the park said in a press release on Friday.
Many in Hong Kong are ready to give Mickey Mouse a big hug for bringing Disneyland to them.
Hong Kong Disneyland Hotel starts at 1,600 (euro 170) a night and Disney?s Hollywood Hotel?s cheapest
room costs 1,000 (euro 106).
Highest JSD summary: System 1
Many in Hong Kong are ready to give Mickey Mouse a big hug for bringing Disneyland to them.
But not dog-lovers, shark-defenders and fireworks foes.
The opposition may seem odd, in a Chinese city where fireworks are a fixture, shark fin soup is hugely
popular, and stray dogs are summarily dealt with as health hazards.
But eight years after the British colony was returned to China, the capitalist city is much freer than the
Communist mainland, and advocacy groups are vocal.
concerns, and use of feng shui. The human summaries for this input focus on different
aspects. The term distributions in such an input by themselves do not provide an
indication of what was important in contrast to the more cohesive input we discussed
previously. The semantics of the content should be better understood to be able to
predict the content that humans would choose in their summaries. Subsequently, we
can also observe that the summary that is top ranked by JS divergence does not have
292
Louis and Nenkova Automatic Content Evaluation
much of the same information as the model summaries and talks about yet another
set of aspects such as hotel rates and family tourism. The worst summary presents a
different set of facts. The pyramid scores for both these summaries are low (0.13 for
the best JS summary and 0.0 for the worst JS summary). Input?summary similarity is
therefore less helpful here and the information provided by model summaries would
be the best gold standard.
Saggion et al (2010) report that trends can be observed in the JSD metric perfor-
mance although it does not provide good evaluations for opinion and biographical type
inputs. Automatic evaluations in different genres therefore have different requirements
and exploring these is an avenue for future work. Input?summary similarity based only
on word distribution works well for evaluating summaries of cohesive-type inputs.
We can also envision a situation where we will be able to predict whether the JS
divergence evaluation will be accurate or not on a particular test set. In prior work in
Nenkova and Louis (2008) and Louis and Nenkova (2009b), we have explored proper-
ties of summarization inputs and provided a characterization of inputs into cohesive
and less cohesive based on automatic features. The less cohesive inputs were found to
be the ones where automatic systems in general performed poorly. In that work, we
proposed features to predict if an input is cohesive or not. We now apply these features
to the TAC?09 data with the intention of automatically identifying inputs suitable for
JS divergence evaluation (the cohesive ones). The features were trained on data from
previous years of TAC evaluations. Among the top ten inputs for which JS divergence
gave the best correlations, six of them were predicted as cohesive, and, similarly for
the bottom ten, six inputs were predicted as ?not cohesive.? This result provides more
validation of the relationship between input and evaluation quality but the automatic
prediction of evaluation quality does not appear to be very accurate based on our
current features. We plan to explore this direction further in future work.
7.3 Requirements for Consensus-Based Evaluation
In a similar vein, one would like to understand the performance guarantees from
the consensus-based evaluation method (SysSumm). Here, the metric depends on the
availability of a number of diverse system summaries. In the TAC workshops, over
50 systems compete and thus we have a large pool of system summaries with which
to compute consensus. For other data sets, when we have to evaluate a few different
systems, it is unclear if the same performance can be obtained. To understand the
dependence on the number of systems, we study how well the consensus evaluation
method works when a small set of standard summarization methods is taken as the
available system pool. We expected that when the standard algorithms are chosen to
be diverse, their strengths can be combined usefully in a similar manner as the TAC
systems.
We choose a set of nine different summarization approaches. They are briefly de-
scribed here.
Baseline: One of the commonly used baseline approaches for multi-document sum-
marization. The first sentence from each document in the input is first included in the
summary. After including the first sentence from each document, the second sentence
is included and so on up to the length limit.
Mead: Radev et al (2004a, 2004b) rank sentences using a combination of three aspects
(sentence length, position in the article, and a centroid score which indicates how central
the content of the sentence is) computed by comparison with all other sentences.
293
Computational Linguistics Volume 39, Number 2
Average probability: This is a competitive summarizer (Nenkova, Vanderwende, and
McKeown 2006) using only the frequency of words as the indicator of content impor-
tance. We implement this method by first computing the unigram probability of all
content words in the documents of the input combined together. Then we score each
sentence by the average value of the probability for the content words in that sentence.
Topic word: This is a strong, yet simple, method for generic summarization (i.e., the
set of documents given as input must be summarized to reflect the sources as best as
possible; in contrast, TAC 2009 tasks can be considered as focused summarizationwhere
either a query is provided or an update is required). This method first computes a set of
topic words from the input using a loglikelihood ratio. The sentences are ranked using
the score introduced by Conroy, Schlesinger, and O?Leary (2006): the ratio of the number
of unique topic words in the sentence to the unique content words in the sentence.
Graph centrality: This approach (Erkan and Radev 2004; Mihalcea and Tarau 2005)
performs selection over a graph representation of the input sentences. Each sentence
is represented in vector space using unigram word counts. Two sentences are linked
when their vectors have a cosine similarity of at least 0.1. When this graph is converted
into aMarkov chain, we can compute the stationary distribution of the transition matrix
defined by the graph?s edges. This stationary distribution gives the probability of visit-
ing each node during repeated random walks through the graph. The high probability
nodes are the ones that are most visited and these correspond to central sentences for
summarization. The probability from the stationary distribution is the ranking score for
this method.
Latent Semantic Analysis (LSA): The LSA technique (Deerwester et al 1990) is based
on the idea of dimensionality reduction. For summarization, first, the input is repre-
sented as a matrix indexed by words (rows) and sentences (columns) and each cell indi-
cates the count of the word in that sentence. This matrix is converted by singular value
decomposition and dimensionality reduction to obtain a matrix of sentences versus
concepts where concepts implicitly capture sets of co-occurring terms. The number of
concepts is much smaller than the size of the vocabulary. The process also produces the
singular values that indicate the importance of concepts. Sentences are selected for each
concept in order of concept importance up to the summary length limit. We obtained
summaries using the approach detailed in Gong and Liu (2001).
Greedy-KL: This method selects sentences by minimizing the KL divergence of the
summary?s word distribution to that of the input. The idea is similar to findings
from our input?summary similarity evaluations. Because the selection of sentences that
minimize divergence can only be done by examining all combinations of sentences,
Haghighi and Vanderwende (2009) introduce a greedy approach that, at each step, adds
the sentence si to the existing summary E such that the combination E ? si has the lowest
KL among all options for si.
CLASSY 04: This system (Conroy and O?Leary 2001) combines the occurrence of topic
words and position of the sentence to predict the score for a sentence. In addition, it
employs a hidden Markov model?based approach, so that the probability of a sentence
being a summary sentence is dependent on the importance of its adjacent sentences in
the document. This system was introduced in the DUC evaluations in 2004 (Conroy
et al 2004). We obtained these summaries (and the subsequent CLASSY 11) from the
authors.
294
Louis and Nenkova Automatic Content Evaluation
CLASSY 11: This is a query-focused summarization system used by Conroy et al (2011)
in TAC 2011. It uses features related to topic words and other important keywords
identified using a graph-based approach. Rather than greedy selection of top sentences,
CLASSY 11 solves an approximate knapsack problem to obtain a more globally optimal
summary. Further, the scoring in this method uses bigrams as the basic unit/keyword in
contrast to the other methods we have described previously that assume that a sentence
is composed of a bag of unigrams.
We generated 100 word summaries from each described system. Except for the
CLASSY system, which performsmore sophisticated redundancy removal, for the other
methods we used the greedy Maximum Marginal Relevance technique (Carbonell and
Goldstein 1998) for reducing redundancy. After the sentence rankings were obtained,
we added each sentence in order if it was not highly similar (a threshold value on
cosine overlap is specified to indicate high similarity) to any of the already added
sentences.
Each of the original TAC systems summaries were evaluated as follows. We added
the candidate summary to the pool of summaries from these other standard methods.
Then we computed the JS divergence between the candidate summary and the com-
bined pool to obtain the score for the candidate. The procedure is the same as the one
we followed in Section 6 except that here we assumed that for each TAC system, we
only had these standard systems as peers rather than the full set of all TAC systems.
The results from this evaluation are shown in Table 12 as SysSumm-std9. The previous
evaluation results, using all TAC systems as consensus, is reproduced in the table as
SysSumm-full.
We found that even with these few systems, the consensus evaluation is rather
strong and produces correlations of 0.91 with pyramid and 0.77 with responsiveness
scores. These results provide additional support for the argument that high quality
evaluation is feasible even with standard systems as peers and that a small set of such
systems appears to be sufficient for forming the consensus.
Because the CLASSY systems are currently some of the top performing systems
at TAC, we also evaluated how useful the consensus is if the CLASSY summaries
are left out. So we evaluated the TAC systems using only the seven other standard
systems (i.e., all except CLASSY04 and CLASSY11) as the peers and the results from
this evaluation are reported in Table 12 as SysSumm-std7. We find that the correlations
Table 12
Performance of consensus evaluation approach on TAC?09 data (53 systems). For input level
(micro), the percentage of inputs with significant correlations is reported. The results using
TAC?09 systems as pseudomodels are indicated as SysSumm-full and those with off-the-shelf
systems as SysSumm-std.
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Evaluation type py resp py resp py resp py resp
SysSumm - full 0.93 0.81 90.9 86.4 88.8 80.7 65.2 52.7
SysSumm - std9 0.91 0.77 86.3 75.0 87.4 78.7 66.4 50.9
SysSumm - std7 0.91 0.78 84.0 75.0 87.7 78.8 65.9 50.6
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
295
Computational Linguistics Volume 39, Number 2
remain the same even when the strongest systems are removed. The usefulness of the
consensus therefore is not heavily dependent on the presence of best-quality systems in
the pool.
7.4 Cross-Year Variation in Metric Performance
The performance of a metric should also be discussed under the effect of different test
data sets. In our feature analysis for input?summary similarity, we found that JS diver-
gence produces very high correlations on the TAC 2008 data, about 0.88 with pyramid
scores. The performance on the TAC 2009 data, however, although still high (0.74), is
lower than the previous year?s data. Such a difference could be attributed to different
factors. One possible factor is the cohesiveness of the input, as we have discussed
earlier. In the TAC evaluations, there is no control over the input types, so inputs from
different years may not have the same characteristics. It could be, therefore, that TAC
2009 had more inputs that were less cohesive as our second example above compared
to inputs that have more homogenity in the topic discussed. A further evidence for this
hypothesis can be seen from the cross-year performance of different metrics presented
in Table 13.
Here, improved correlations from 2008 to 2009 are bolded and those that decreased
are italicized. The correlations with pyramid scores increased for SysSumm and ROUGE
evaluations but dropped for JS divergence. This trend indicates that the model-based
evaluations (SysSumm has characteristics similar to model-based evaluation) have
more strength on the 2009 data. For inputs where model information cannot be obtained
from the general term distribution in the inputs (as could have been the case in 2009),
therefore, input?summary similarity that is model-free obtains worse performance. This
model dependence can also explain why ROUGE and SysSummhave lower correlations
with responsiveness in 2009 despite being able to predict pyramid scores better. Because
ROUGE scores are computed based on these specific models, its correlations with the
model-free responsiveness judgments drops in 2009.
8. Conclusion and Future Work
We have presented successful metrics for summary evaluation that require very little
or no human input. We have explored two scenarios: fewer model summaries and no
model summaries at all. For both these cases, our newly proposed evaluation metrics
have provided good performance.
We analyzed two methods for evaluation in the absence of gold-standard
summaries. One was based on input?summary similarity. We examined different
Table 13
Cross-year system level correlations for different metrics. The ROUGE-SU4 scores use all four
human summaries for reference. Improved correlations in 2009 are bolded and decreases in
correlations are italicized.
JSD SysSumm ROUGE-SU4
year py resp py resp py resp
2008 0.89 0.74 0.85 0.82 0.88 0.83
2009 0.74 0.70 0.93 0.81 0.92 0.79
296
Louis and Nenkova Automatic Content Evaluation
possibilities for measuring similarity and quantified their accuracy in predicting
human-assigned scores. Our results showed that the strength of features varies con-
siderably. The best metric is JS divergence, which compares the distribution of terms
in the input and summary. Combination of JS divergence with other metrics such as
cosine similarity and topic word features also gave high correlations with human scores,
around 0.77.
Another method we have introduced is the addition of pseudomodel system sum-
maries to themodel set when the number of models is low. Our aim here was to improve
themicro-level evaluations and our results show that improvements along this linewere
provided by the pseudomodels.
We also proposed a model-free metric that measures the similarity of a system sum-
mary with the collection of all system summaries for that input. This method actually
provided even better performance (0.93 correlations with pyramid scores), which is
competitive with ROUGE scores computed using four human models.
Furthermore, our evaluations provide consistent performance. In Louis and
Nenkova (2009c), we report the correlations for adjacent years showing that our metrics
produce reliable performance for two consecutive years of TAC evaluation and for two
tasks, query and update summarization.
These evaluation methods highlight considerations that have received little atten-
tion so far and give indications of how to perform evaluations on non-standard test
sets with little human input. The situation of having only one model summary is not
uncommon and so are test sets where there are no model summaries at all. Here, one
could use our proposed approaches in system development and then at a later stage
use manual evaluations on a small test set to confirm the results. Further, our metrics
also provide valuable insights for system development. From our results, it is evident
that optimizing for input?summary similarity using an information-theoretic measure
such as JS divergence and optimizing for topic signatures are indeed good approaches
for building a generic summarization system. In addition, the results from consensus
evaluation show that combining summaries from different systems has the potential
of creating a system better than the pool. Currently, more than 50 systems compete in
the TAC summarization tasks and we want to explore system combination techniques
over their summaries in future work.
We also plan to focus on the incorporation of both content and linguistic quality for
evaluation. As we already saw, the correlation between system rankings based on pyra-
mid and responsiveness scores is only 0.85. Furthermore, the correlations of ROUGE as
well as our metrics are lower with responsiveness compared with the pyramid. Content
scores should therefore always be used together with assessments of linguistic quality,
and combining both scores would be necessary for obtaining better correlations with
responsiveness.
Acknowledgments
We would like to thank John Conroy
for providing us with the summaries
from the CLASSY system and Xi Lin for
the implementation of the LSA-based
summarizer. We would also like to
thank the reviewers for their comments;
in particular, the expanded evaluation
of the consenus-based metric was added
based on their feedback. The work
has been partly supported by an NSF
CAREER award (09-53445).
References
Albrecht, Joshua and Rebecca Hwa.
2007. Regression for sentence-level MT
evaluation with pseudo references.
In Proceedings of ACL, pages 296?303,
Prague.
297
Computational Linguistics Volume 39, Number 2
Albrecht, Joshua and Rebecca Hwa.
2008. The role of pseudo references
in MT evaluation. In Proceedings of the
Third Workshop on Statistical Machine
Translation, ACL, pages 187?190,
Columbus, OH.
Best, D. J. and D. E. Roberts. 1975. Algorithm
as 89: The upper tail probabilities of
Spearman?s rho. Journal of the Royal
Statistical Society. Series C (Applied
Statistics), 24(3):377?379.
Callison-Burch, Chris, Philipp Koehn,
Christof Monz, Kay Peterson, Mark
Przybocki, and Omar Zaidan. 2010.
Findings of the 2010 joint workshop on
statistical machine translation and metrics
for machine translation. In Proceedings
of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR,
pages 17?53, Uppsala.
Callison-Burch, Chris, Philipp Koehn,
Christof Monz, and Omar Zaidan.
2011. Findings of the 2011 workshop
on statistical machine translation.
In Proceedings of the Sixth Workshop
on Statistical Machine Translation,
pages 22?64, Edinburgh.
Carbonell, Jaime and Jade Goldstein.
1998. The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Proceedings of
SIGIR, pages 335?336, Melbourne.
Conroy, John M., Jade Goldstein, Judith D.
Schlesinger, and Dianne P. O?Leary. 2004.
Left-brain/right-brain multi-document
summarization. In Proceedings of the
4th Document Understanding Conference
(DUC?04), Boston, MA. Available at:
http://duc.nist.gov/pubs/2004papers/
ida.conroy.ps.
Conroy, John M. and Dianne P. O?Leary. 2001.
Text summarization via hidden Markov
models. In Proceedings of SIGIR,
pages 406?407, New Orleans, LA.
Conroy, John M., Judith D. Schlesinger,
Jeff Kubina, Peter A. Rankel, and Dianne P.
O?Leary. 2011. Classy 2011 at TAC:
Guided and multi-lingual summaries and
evaluation metrics. In Proceedings of TAC,
Gaithersburg, MD. Available at:
http://www.nist.gov/tac/publications/
2011/participant.papers/CLASSY.
proceedings.pdf.
Conroy, John M., Judith D. Schlesinger,
and Dianne P. O?Leary. 2006. Topic-focused
multi-document summarization using an
approximate oracle score. In Proceedings
of the COLING-ACL, pages 152?159,
Sydney.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
Americal Society for Information Science,
41(6):391?407.
Donaway, Robert L., Kevin W. Drummey,
and Laura A. Mather. 2000. A comparison
of rankings produced by summarization
evaluation measures. In Proceedings of the
NAACL-ANLP Workshop on Automatic
Summarization, pages 69?78, Seattle, WA.
Erkan, Gu?nes? and Dragomir R. Radev. 2004.
Lexpagerank: Prestige in multi-document
text summarization. In Proceedings of
EMNLP, pages 365?371, Barcelona.
Gillick, Dan and Benoit Favre. 2009. A
scalable global model for summarization.
In Proceedings of the Workshop on Integer
Linear Programming for Natural Language
Processing, pages 10?18, Boulder, CO.
Gillick, Dan and Yang Liu. 2010. Non-expert
evaluation of summarization systems is
risky. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and
Language Data with Amazon?s Mechanical
Turk, pages 148?151, Los Angeles, CA.
Gong, Yihong and Xin Liu. 2001. Generic
text summarization using relevance
measure and latent semantic analysis.
In Proceedings of SIGIR, pages 19?25,
New Orleans, LA.
Haghighi, Aria and Lucy Vanderwende.
2009. Exploring content models for
multi-document summarization.
In Proceedings of HLT-NAACL,
pages 362?370, Boulder, CO.
Harman, Donna and Paul Over. 2004.
The effects of human variation in
DUC summarization evaluation.
In Proceedings of the ACL-04 Workshop:
Text Summarization Branches Out,
pages 10?17, Barcelona.
Jing, Hongyan, Regina Barzilay, Kathleen
Mckeown, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In AAAI
Symposium on Intelligent Summarization,
pages 60?68, Palo Alto, CA.
Kumar, Shankar and William Byrne.
2004. Minimum Bayes-risk decoding
for statistical machine translation.
In Proceedings of HLT-NAACL,
pages 169?176, Boston, MA.
Lin, Chin-Yew. 2004a. Looking for a few
good metrics: Automatic summarization
evaluation-how many samples are
enough. In Proceedings of the NTCIR
Workshop, volume 4, pages 1?10, Tokyo.
298
Louis and Nenkova Automatic Content Evaluation
Lin, Chin-Yew. 2004b. ROUGE: A package
for automatic evaluation of summaries. In
Proceedings of the ACL Text Summarization
Workshop, pages 74?81, Barcelona.
Lin, Chin-Yew, Guihong Cao, Jianfeng
Gao, and Jian-Yun Nie. 2006. An
information-theoretic approach to
automatic evaluation of summaries.
In Proceedings of HLT-NAACL,
pages 463?470, New York, NY.
Lin, Chin-Yew and Eduard Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings of
COLING, pages 495?501, Saarbru?cken.
Lin, Chin-Yew and Eduard Hovy. 2003.
Automatic evaluation of summaries
using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 71?78,
Edmonton.
Louis, Annie and Ani Nenkova. 2008.
Automatic summary evaluation without
human models. In Proceedings of TAC,
Gaithersburg, MD. Available at:
http://www.nist.gov/tac/publications/
2008/additional.papers/Penn.
proceedings.pdf.
Louis, Annie and Ani Nenkova. 2009a.
Automatically evaluating content selection
in summarization without human models.
In Proceedings of EMNLP, pages 306?314,
Singapore.
Louis, Annie and Ani Nenkova. 2009b.
Performance confidence estimation for
automatic summarization. In Proceedings
of EACL, pages 541?548, Athens.
Louis, Annie and Ani Nenkova. 2009c.
Predicting summary quality using
limited human input. In Proceedings
of TAC, Gaithersburg, MD. Available at:
http://www.nist.gov/tac/publications/
2009/participant.papers/UPenn.
proceedings.pdf.
Madnani, Nitin, Necip Fazil Ayan,
Philip Resnik, and Bonnie J. Dorr. 2007.
Using paraphrases for parameter
tuning in statistical machine translation.
In Proceedings of the Second Workshop on
Statistical Machine Translation,
pages 120?127, Prague.
McDonald, Ryan. 2007. A study of global
inference algorithms in multi-document
summarization. In Proceedings of ECIR,
pages 557?564, Rome.
McKeown, Kathy, Regina Barzilay,
David Evans, Vasileios Hatzivassiloglou,
Barry Schiffman, and Simone Teufel.
2001. Columbia multi-document
summarization: Approach and evaluation.
In Proceedings of DUC, New Orleans, LA.
Available at: http://www-nlpir.nist.
gov/projects/duc/pubs/2001papers/
columbia redo.pdf.
Mihalcea, Rada and Paul Tarau. 2005.
Multi-document summarization with
iterative graph-based algorithms.
In Proceedings of the First International
Conference on Intelligent Analysis Methods
and Tools (IA 2005), McLean, VA.
Nenkova, Ani and Annie Louis. 2008. Can
you summarize this? Identifying correlates
of input difficulty for multi-document
summarization. In Proceedings of ACL-HLT,
pages 825?833, Columbus, OH.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method. In
Proceedings of HLT-NAACL, pages 145?152,
Boston, MA.
Nenkova, Ani, Rebecca Passonneau, and
Kathleen McKeown. 2007. The pyramid
method: Incorporating human content
selection variation in summarization
evaluation. ACM Transactions on Speech
and Language Processing, 4(2):4.
Nenkova, Ani, Lucy Vanderwende, and
Kathleen McKeown. 2006. A compositional
context sensitive multi-document
summarizer: Exploring the factors that
influence summarization. In Proceedings
of SIGIR, pages 573?580, Seattle, WA.
Owkzarzak, Karolina and Hoa Trang Dang.
2009. Evaluation of automatic summaries:
Metrics under varying data conditions.
In Proceedings of the Workshop on Language
Generation and Summarisation,
pages 23?30, Singapore.
R Development Core Team. 2011.
R: A Language and Environment for
Statistical Computing. R Foundation
for Statistical Computing, Vienna.
Radev, Dragomir, Timothy Allison, Sasha
Blair-Goldensohn, John Blitzer, Arda
C?elebi, Stanko Dimitrov, Elliott Drabek,
Ali Hakim, Wai Lam, Danyu Liu, Jahna
Otterbacher, Hong Qi, Horacio Saggion,
Simone Teufel, Michael Topper,
AdamWinkel, and Zhu Zhang. 2004a.
MEAD?A platform for multidocument
multilingual text summarization.
In Proceedings of LREC 2004, pages 1?4,
Lisbon.
Radev, Dragomir, Hongyan Jing,
Malgorzata Sty, and Daniel Tam. 2004b.
Centroid-based summarization of
multiple documents. Information
Processing and Management, 40:919?938.
Radev, Dragomir and Daniel Tam. 2003.
Single-document and multi-document
299
Computational Linguistics Volume 39, Number 2
summary evaluation via relative utility.
In Proceedings of CIKM, pages 508?511,
New Orleans, LA.
Rath, G. J., A. Resnick, and R. Savage.
1961. The formation of abstracts by
the selection of sentences: Part 1:
Sentence selection by man and
machines. American Documentation,
2(12):139?208.
Saggion, Horacio, Juan-Manuel
Torres Moreno, Iria da Cunha,
Eric SanJuan, and Patricia Velazquez-
Morales. 2010. Multilingual
summarization evaluation without
human models. In Proceedings of
COLING, pages 1,059?1,067, Beijing.
Soboroff, Ian, Charles Nicholas, and
Patrick Cahan. 2001. Ranking retrieval
systems without relevance judgments.
In Proceedings of SIGIR, pages 66?73,
New Orleans, LA.
Tromble, Roy W., Shankar Kumar,
Franz Och, and Wolfgang Macherey.
2008. Lattice minimum Bayes-risk
decoding for statistical machine
translation. In Proceedings of EMNLP,
pages 620?629, Honolulu, HI.
van Halteren, Hans and Simone Teufel.
2003. Examining the consensus
between human summaries: Initial
experiments with factoid analysis.
In Proceedings of the HLT-NAACL DUC
on Text Summarization Workshop,
pages 57?64, Edmonton.
Zhang, Ying and Stephan Vogel. 2010.
Significance tests of automatic machine
translation evaluation metrics.Machine
Translation, 24(1):51?65.
300
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 313?316,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Creating Local Coherence: An Empirical Assessment
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Abstract
Two of the mechanisms for creating natural
transitions between adjacent sentences in a
text, resulting in local coherence, involve dis-
course relations and switches of focus of at-
tention between discourse entities. These two
aspects of local coherence have been tradi-
tionally discussed and studied separately. But
some empirical studies have given strong evi-
dence for the necessity of understanding how
the two types of coherence-creating devices
interact. Here we present a joint corpus study
of discourse relations and entity coherence ex-
hibited in news texts from the Wall Street Jour-
nal and test several hypotheses expressed in
earlier work about their interaction.
1 Introduction
Coherent discourse is characterized by local prop-
erties that are crucial for comprehension. In fact, a
long line of linguistics and computational linguistics
tradition has proposed that several levels of struc-
ture contribute to the creation of coherent discourse.
Among these, the attentional structure (Grosz et
al., 1995) and the relational structure (Mann and
Thompson, 1988) of text, are the most widely dis-
cussed in the literature.
Centering theory (Grosz et al, 1995) is the dom-
inant approach for describing and analyzing atten-
tional structure. It assumes that readers of the text
focus (center) their attention on a small number of
salient discourse entities at a time and that there are
preferred patterns for switching attention between
entities mentioned in adjacent sentences. Relational
This work was partially supported by NSF Grant IIS -07-
05671.
structure theories, on the other hand, describe how
certain discourse (also called rhetorical or coher-
ence) relations such as CONTRAST and CAUSE are
inferred by the reader between adjacent units of text.
The existence of richly annotated corpora and the
development of automatic applications based on the
theories have allowed empirical assessments of the
validity and generality of these theories individually.
Such work has also provided increasingly strong
evidence that attentional and relational structures
need to be taken into account simultaneously. The
motivation behind such proposals have been the em-
pirical findings that ?weak? discourse relations such
as ELABORATION are the most common type of re-
lations, and that a large percentage of adjacent sen-
tences in fact do not have any entities in common.
In particular, a corpus based evaluation of Center-
ing theory found that only 42% of pairs of adjacent
sentences have at least one entity in common and
hypothesized that discourse relations are responsible
for creating local coherence in the remaining cases
(Poesio et al, 2004). At the same time, the work
of Knott et al (2001) has discussed several theo-
retical complications arising from the existence of
the very common and semantically weak ELABO-
RATION relation. These researchers propose that re-
placing ELABORATION by an account of entity co-
herence such as Centering will be most beneficial.
But work in information ordering (Karamanis, 2007)
has not been able to confirm such claims that better
results can be obtained by combining entity coher-
ence with discourse relations.
Till recently, the absence of large corpora anno-
tated for both discourse relations and coreference in-
formation has prohibited a detailed joint analysis of
attentional and relational structure. We combine two
313
recently released corpora: discourse relations from
the Penn Discourse Treebank and coreference an-
notations from the OntoNotes corpus, to assess the
prevalence and interplay between factors that create
local coherence in newspaper text.
Specifically, in our study we examine how three
hypotheses formulated in prior work play out in the
Wall Street Journal texts in our corpus:
Hypothesis 1 Adjacent sentences that do not share
entities are related by non-elaboration discourse re-
lations [Poesio et al (2004) Sec. 5.2.2 Pg. 354].
Hypothesis 2 Adjacent sentences joined by non-
elaboration discourse relation have lower entity co-
herence: such pairs are less likely to mention the
same entities [Knott et al (2001) Sec. 7 Pg. 10].
Hypothesis 3 Almost all pairs of sentences in a co-
herent text either share entities or participate in non-
elaboration discourse relation (Knott et al, 2001;
Poesio et al, 2004).
None of these hypotheses are validated. We find
that only 38.65% of pairs that do not share enti-
ties participate in ?core? relations such as tempo-
ral, contingency or comparison; the rate of coref-
erence in these ?core? relations is similar to that in
weaker elaboration relations; about 30% of all sen-
tence pairs neither share entities nor participate in a
?core? discourse relation.
2 Data
In order to jointly analyze both discourse relations
and noun phrase coreference between adjacent sen-
tences, we combine annotations from two corpora,
OntoNotes and the Penn Discourse Treebank. The
two individual corpora are larger, but a smaller sub-
set of 590 Wall Street Journal articles appear in both.
All our analysis is for adjacent sentences within
paragraphs in this subset of texts.
Penn Discourse Treebank The Penn Discourse
Treebank (PDTB) (Prasad et al, 2008) is the largest
available corpus of annotations for discourse rela-
tions, covering one million words of the Wall Street
Journal (WSJ). In the PDTB, two kinds of discourse
relations are annotated. In explicit relations, a dis-
course connective such as ?because?, ?but? or ?so?
is present, as in the example below.
[Ex. 1] There is an incredible pressure on school systems
and teachers to raise test scores. So efforts to beat the tests are
also on the rise.
On the other hand, relations can also exist without
an explicit signal. In Ex. 2, it is clear that the second
sentence is the result of the event mentioned in the
first.
[Ex. 2] In July, the Environmental Protection Agency im-
posed a gradual ban on virtually all uses of asbestos. By 1997,
almost all remaining uses of cancer-causing asbestos will be
outlawed.
Such relations are called implicit relations. In the
PDTB, they are annotated between all adjacent sen-
tences within a paragraph which do not already par-
ticipate in an explicit discourse relation.
For both implicit and explicit relations, the se-
mantic sense of the discourse relation is assigned
from a hierarchy of senses. There are four classes
of discourse relations at the topmost general level.
The second level senses are shown within paran-
theses: Comparison (Concession, Contrast, Pragmatic
Concession/Contrast), Contingency (Cause, Condition,
Pragmatic Cause/Condition), Temporal (Asynchronous,
Synchronous) and Expansion (Alternative, Conjunction,
Exception, Instantiation, List, Restatement).
Some of the adjacent sentences in the texts, how-
ever, were found not to have a discourse relation be-
tween the events or propositions mentioned in them.
Instead, they were related because both sentences
mention the same entity, directly or indirectly, and
the second sentence provides some further descrip-
tion of that entity. An Entity Relation (EntRel) was
annotated for such sentence pairs as below.
[Ex. 3] Pierre Vinken, 61 years old, will join the board as
a nonexecutive director Nov. 29. Mr. Vinken is chairman of
Elsevier N.V., the Dutch publishing group.
OntoNotes For coreference information, we use
the WSJ portion of the OntoNotes corpus version 2.9
(Hovy et al, 2006) which contains 590 documents.
For these documents, we also have the PDTB anno-
tations available. In OntoNotes, all noun phrases?
pronouns, names and nominals are marked for coref-
erence without any limitation to specific semantic
categories.
3 Corpus study findings
For ease of presentation in the following analy-
sis, we will call the Expansion and Entity relations
?weak? and Temporal, Contingency and Compari-
314
0 to 100 0.41 500 to 1000 0.50
100 to 200 0.37 above 1000 0.51
200 to 500 0.48
Table 1: Proportion of sentence pairs that don?t share any
entities for different document lengths (in words)
Type Relation No shared entities
Core
Temporal 122 (2.98)
Contingency 752 (18.40)
Comparison 706 (17.27)
Weak Expansion 1870 (45.74)EntRel 638 (15.61)
Table 2: Distribution of the 4088 non-entity sharing sen-
tence pairs. The proportions are shown in brackets.
son relations ?core?, following the intuition that the
semantics of the latter class is much more clearly
defined. Implicit and explicit relations were not dis-
tinguished in the analysis.1
Hypothesis 1 The first hypothesis is that adja-
cent sentences that do not share entities participate
in core relations and so remain locally coherent.
Pairs of adjacent sentences that do not share any
entities are common. In prior work, Poesio et al
(2004) found that 58% of adjacent sentence pairs in
their corpus of museum object descriptions did not
have overlapping mentions of any entity. The distri-
bution in the WSJ texts is similar, as seen in Table 1.
Depending on the length of the article, 37% to 51%
of sentence pairs do not have any entity in common.2
Table 2 shows the distribution of discourse rela-
tions for the 4088 sentence pairs in the corpus that
do not share any entities. Contrary to expectation,
the majority of such pairs, 61%, are related by a
weak discourse relation. Especially unexpected is
the high percentage of entity relations (EntRel) that
don?t have actual entity overlap:
[Ex. 4] All four demonstrators were arrested. The law,
which Bush allowed to take effect without his signature, went
into force Friday.
[Ex. 5] Authorities in Hawaii said the wreckage of a missing
commuter plane with 20 people aboard was spotted in a remote
valley on the island of Molokai. There wasn?t any evidence of
survivors.
1For brevity we present combined results for both implicit
and explicit relations. However, most of our conclusions remain
the same when the two types are distinguished.
2There are around 100 documents in each length range.
Type Relation Total Share entities
Core
Temporal 365 243 (66.57)
Contingency 1570 818 (52.10)
Comparison 1477 771 (52.20)
Weak Expansion 3569 1699 (47.60)EntRel 1424 786 (55.20)
Table 3: Rate of entity sharing
Share entities No sharing
core relations 1832 (21.80) 1580 (18.80)
weak relations 2485 (29.56) 2508 (29.84)
Table 4: Total number (proportion) of sentence pairs in
the corpus in the given categories
Hypothesis 2 The second hypothesis states that
adjacent sentences joined by a core discourse rela-
tion are less likely to mention the same entities in
comparison to weak relations. But as Table 3 re-
veals, this is generally not the case.
Adjacent sentences in Temporal relation are very
likely to share entities?almost 70% of them do.
Over half of all Contingency and Comparison rela-
tions also share entities. But, the rates of sharing in
Comparison and Contingency relations are signifi-
cantly lower compared to Entity relations (under a
two-sided binomial test). Ex. 6 shows a Contin-
gency relation without entity sharing.
[Ex. 6] Without machines, good farms can?t get bigger. So
the potato crop, once 47 million tons, is down to 35.
However, adjacent sentences with Expansion rela-
tion turn out least likely to share entities. The entity
sharing rates of all the other relations were found to
be significantly higher than Expansion.
Hypothesis 3 Finally, we test the hypothesis that
the majority of adjacent sentences exhibit coherence
because they either share entities or form the argu-
ments of a core discourse relation.
This hypothesis is not supported in the WSJ data
(see Table 4): 30% of all sentence pairs are in a weak
discourse relation?Expansion or EntRel?and do
not have any entities in common. In a sense, nei-
ther of the theories of entity or relational coherence
can explain what mechanisms create the local coher-
ence for these pairs. In order to glean some insights
of how coherence is created there, we examine the
behavior of different types of Elaboration relations
(Table 5). There is a wide variation between the dif-
315
Alternative 67 (0.63) Instantiation 490 (0.34)
Restatement 960 (0.56) List 165 (0.28)
Conjunction 1021 (0.48) EntRel 1424 (0.55)
Table 5: Total number of different Expansion relations
and their coreference probability (in brackets)
ferent types of Expansions.
When the function of an expansion sentence is to
provide an alternative explanation or restate an ut-
terance, the probability of entity overlap is very high
and patterns similarily with entity relations (around
60%). Below is an example restatement sentence.
[Ex. 7] {Researchers in Belgium}r said {they}r have de-
veloped a genetic engineering technique for creating hybrid
plants for a number of crops, such as cotton, soybeans and
rice. {The scientists at Plant Genetic Systems N.V.}r isolated
a gene that could lead to a generation of plants possessing a
high-production trait.
However, the two classes?Instantiation and List
largely appear with very little entity overlap, 37%
and 29% respectively. An Instantiation relation is
used to provide an example and hence has little over-
lap with the previous sentence (Ex. 8 and 9).
[Ex. 8] There may be a few cases where the law breaking is
well pinpointed and so completely non-invasive of the rights of
others that it is difficult to criticize it. The case of Rosa Parks,
the black woman who refused to sit at the back of the bus, comes
to mind as an illustration.
[Ex. 9] The economy is showing signs of weakness, partic-
ularly among manufacturers. Exports, which played a key role
in fueling growth over the last two years, seem to have stalled.
List relations, on the other hand, connect sen-
tences where each of them elaborates on a common
proposition mentioned earlier in the discourse. Here
is an example five sentence paragraph with list rela-
tions but no entity repetition at all.
[Ex. 10] Designs call for a L-shaped structure with
a playground in the center. [Implicit List] There
will be recreation and movie rooms. [Implicit List]
Teens will be able to listen to music with head-
sets. [Implicit List] Study halls, complete with ref-
erence materials will be available. [Explicit List]
And there will be a nurse?s station and rooms for
children to meet with the social workers.
In Ex. 10, as well as some others, a broad no-
tion of entity coherence?bridging anaphora can be
applied. Poesio et al (2004) also note this fact, but
also say that such instances are very difficult to an-
notate reliably. Our work is based on coreference
annotations which can be marked with considerably
high inter-annotator agreement.
4 Conclusions
The recent release of corpora annotated for corefer-
ence and discourse relations for the same texts have
made possible to empirically assess claims about the
interaction between two types of local coherence:
relational and entity. We find that about half of the
pairs of adjacent sentences do not share any entities
at all, and about 60% are related by weak discourse
relations. We test the hypothesis from prior work
that these two types of coherence are complemen-
tary to each other and taken together explain most
local coherence. We find that the two types of co-
herence mechanisms are neither mutually exclusive
nor do they explain all the data. Future work in dis-
course analysis will need to develop better under-
standing of how the two types of coherence interact.
References
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of dis-
course. Computational Linguistics, 21(2):203?226.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In Proceedings of NAACL-HLT, pages 57?60.
N. Karamanis. 2007. Supplementing entity coherence
with local rhetorical relations for information order-
ing. Journal of Logic, Language and Information,
16(4):445?464.
A. Knott, J. Oberlander, M. O?Donnell, and C. Mellish.
2001. Beyond elaboration: the interaction of rela-
tions and focus in coherent text. In Text Representa-
tion: Linguistic and Psycholinguistic Aspects, chapter
7, pages 181?196.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Towards a functional theory of text orga-
nization. Text, 8.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. Hitze-
man. 2004. Centering: A parametric theory and its
instantiations. Computational Linguistics, pages 309?
363.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The penn discourse
treebank 2.0. In Proceedings of LREC?08.
316
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11?19,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Acoustic-Prosodic Entrainment and Social Behavior
Rivka Levitan1, Agust??n Gravano2, Laura Willson1,
S?tefan Ben?us?3, Julia Hirschberg1, Ani Nenkova4
1 Dept. of Computer Science, Columbia University, New York, NY 10027, USA
2 Departamento de Computacio?n (FCEyN), Universidad de Buenos Aires, Argentina
3 Constantine the Philosopher University & Institute of Informatics, Slovak Academy of Sciences, Slovakia
4 Dept. of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, USA
rlevitan@cs.columbia.edu, gravano@dc.uba.ar, law2142@barnard.edu,
sbenus@ukf.sk, julia@cs.columbia.edu, nenkova@seas.upenn.edu
Abstract
In conversation, speakers have been shown
to entrain, or become more similar to each
other, in various ways. We measure entrain-
ment on eight acoustic features extracted from
the speech of subjects playing a cooperative
computer game and associate the degree of en-
trainment with a number of manually-labeled
social variables acquired using Amazon Me-
chanical Turk, as well as objective measures
of dialogue success. We find that male-female
pairs entrain on all features, while male-male
pairs entrain only on particular acoustic fea-
tures (intensity mean, intensity maximum and
syllables per second). We further determine
that entrainment is more important to the per-
ception of female-male social behavior than it
is for same-gender pairs, and it is more impor-
tant to the smoothness and flow of male-male
dialogue than it is for female-female or mixed-
gender pairs. Finally, we find that entrainment
is more pronounced when intensity or speak-
ing rate is especially high or low.
1 Introduction
Entrainment, also termed alignment, adaptation,
priming or coordination, is the phenomenon of
conversational partners becoming more similar to
each other in what they say, how they say it,
and other behavioral phenomena. Entrainment has
been shown to occur for numerous aspects of spo-
ken language, including speakers? choice of re-
ferring expressions (Brennan & Clark, 1996); lin-
guistic style (Niederhoffer & Pennebaker, 2002;
Danescu-Niculescu-Mizil et al, 2011); syntactic
structure (Reitter et al, 2006); speaking rate (Lev-
itan & Hirschberg, 2011); acoustic/prosodic fea-
tures such as fundamental frequency, intensity, voice
quality (Levitan & Hirschberg, 2011); and phonet-
ics (Pardo, 2006).
Entrainment in many of these dimensions has also
been associated with different measures of dialogue
success. For example, Chartrand and Bargh (1999)
demonstrated that mimicry of posture and behavior
led to increased liking between the dialogue par-
ticipants as well as a smoother interaction. They
also found that naturally empathetic individuals ex-
hibited a greater degree of mimicry than did oth-
ers. Nenkova et al (2008) found that entrainment
on high-frequency words was correlated with nat-
uralness, task success, and coordinated turn-taking
behavior. Natale (1975) showed that an individ-
ual?s social desirability, or ?propensity to act in
a social manner,? can predict the degree to which
that individual will match her partner?s vocal inten-
sity. Levitan et al (2011) showed that entrainment
on backchannel-preceding cues is correlated with
shorter latency between turns, fewer interruptions,
and a higher degree of task success. In a study of
married couples discussing problems in their rela-
tionships, Lee et al (2010) found that entrainment
measures derived from pitch features were signifi-
cantly higher in positive interactions than in nega-
tive interactions and were predictive of the polarity
of the participants? attitudes.
These studies have been motivated by theoreti-
cal models such as Giles? Communication Accom-
modation Theory (Giles & Coupland, 1991), which
proposes that speakers promote social approval or
11
efficient communication by adapting to their inter-
locutors? communicative behavior. Another theory
informing the association of entrainment and dia-
logue success is the coordination-rapport hypoth-
esis (Tickle-Degnen & Rosenthal, 1990), which
posits that the degree of liking between conversa-
tional partners should be correlated with the degree
of nonverbal coordination between them.
Motivated by such theoretical proposals and em-
pirical findings, we hypothesized that entrainment
on acoustic/prosodic dimensions such as pitch, in-
tensity, voice quality and speaking rate might also
be correlated with positive aspects of perceived
social behaviors as well as other perceived char-
acteristics of efficient, well-coordinated conversa-
tions. In this paper we describe a series of ex-
periments investigating the relationship between ob-
jective acoustic/prosodic dimensions of entrainment
and manually-annotated perception of a set of so-
cial variables designed to capture important as-
pects of conversational partners? social behaviors.
Since prior research on other dimensions of entrain-
ment has sometimes observed differences in degree
of entrainment between female-female, male-male
and mixed gender groups (Bilous & Krauss, 1988;
Pardo, 2006; Namy et al, 2002), we also exam-
ined our data for variation by gender pair, consid-
ering female-female, male-male, and female-male
pairs of speakers separately. If previous findings
extend to acoustic/prosodic entrainment, we would
expect female-female pairs to entrain to a greater
degree than male-male pairs and female partners in
mixed gender pairs to entrain more than their male
counterparts. Since prior findings posit that entrain-
ment leads to smoother and more natural conversa-
tions, we would also expect degree of entrainment
to correlate with perception of other characteristics
descriptive of such conversations.
Below we describe the corpus and annotations
used in this study and how our social annotations
were obtained in Sections 2 and 3. We next discuss
our method and results for the prevalence of entrain-
ment among different gender groups (Section 4). In
Sections 5 and 6, we present the results of correlat-
ing acoustic entrainment with social variables and
objective success measures, respectively. Finally, in
Section 7, we explore entrainment in cases of outlier
feature values.
2 The Columbia Games Corpus
The Columbia Games Corpus (Gravano & Hirsch-
berg, 2011) consists of approximately nine hours
of spontaneous dialogue between pairs of subjects
playing a series of computer games. Six females and
seven males participated in the collection of the cor-
pus; eleven of the subjects returned on a different
day for another session with a new partner.
During the course of each session, a pair of speak-
ers played three Cards games and one Objects game.
The work described here was carried out on the Ob-
jects games. This section of each session took 7m
12s on average. We have a total of 4h 19m of Ob-
jects game speech in the corpus.
For each task in an Objects game, the players
saw identical collections of objects on their screens.
However, one player (the Describer) had an addi-
tional target object positioned among the other ob-
jects, while the other (the Follower) had the same
object at the bottom of her screen. The Describer
was instructed to describe the position of the target
object so that the Follower could place it in exactly
the same location on her screen. Points (up to 100)
were awarded based on how well the Follower?s tar-
get location matched the describers. Each pair of
partners completed 14 such tasks, alternating roles
with each task. The partners were separated by a
curtain to ensure that all communication was oral.
The entire corpus has been orthographically tran-
scribed and words aligned with the speech source. It
has also been ToBI-labeled (Silverman et al, 1992)
for prosodic events, as well as labeled for turn-
taking behaviors.
3 Annotation of Social Variables
In order to study how entrainment in various dimen-
sions correlated with perceived social behaviors of
our subjects, we asked Amazon Mechanical Turk1
annotators to label the 168 Objects games in our cor-
pus for an array of social behaviors perceived for
each of the speakers, which we term here ?social
variables.?
Each Human Intelligence Task (HIT) presented to
the AMT workers for annotation consisted of a sin-
gle Objects game task. To be eligible for our HITs,
1http://www.mturk.com
12
annotators had to have a 95% success rate on pre-
vious AMT HITs and to be located in the United
States. They also had to complete a survey estab-
lishing that they were native English speakers with
no hearing impairments. The annotators were paid
$0.30 for each HIT they completed. Over half of the
annotators completed fewer than five hits, and only
four completed more than twenty.
The annotators listened to an audio clip of the
task, which was accompanied by an animation that
displayed a blue square or a green circle depending
on which speaker was currently talking. They were
then asked to answer a series of questions about each
speaker: Does Person A/B believe s/he is better than
his/her partner? Make it difficult for his/her partner
to speak? Seem engaged in the game? Seem to dis-
like his/her partner? Is s/he bored with the game?
Directing the conversation? Frustrated with his/her
partner? Encouraging his/her partner? Making
him/herself clear? Planning what s/he is going to
say? Polite? Trying to be liked? Trying to domi-
nate the conversation? They were also asked ques-
tions about the dialogue as a whole: Does it flow
naturally? Are the participants having trouble un-
derstanding each other? Which person do you like
more? Who would you rather have as a partner?
A series of check questions with objectively de-
terminable answers (e.g. ?Which speaker is the De-
scriber??) were included among the target questions
to ensure that the annotators were completing the
task with integrity. HITs for which the annotator
failed to answer the check questions correctly were
disqualified.
Each task was rated by five unique annotators who
answered ?yes? or ?no? to each question, yielding
a score ranging from 0 to 5 for each social vari-
able, representing the number of annotators who an-
swered ?yes.? A fuller description of the annotation
for social variables can be found in (Gravano et al,
2011).
In this study, we focus our analysis on annotations
of four social variables:
? Is the speaker trying to be liked?
? Is the speaker trying to dominate the conversa-
tion?
? Is the speaker giving encouragement to his/her
partner?
? Is the conversation awkward?
We correlated annotations of these variables with
an array of acoustic/prosodic features.
4 Acoustic entrainment
We examined entrainment in this study in eight
acoustic/prosodic features:
? Intensity mean
? Intensity max
? Pitch mean
? Pitch max
? Jitter
? Shimmer
? Noise-to-harmonics ratio (NHR)
? Syllables per second
Intensity is an acoustic measure correlated with
perceived loudness. Jitter, shimmer, and noise-to-
harmonics ratios are three measures of voice quality.
Jitter describes varying pitch in the voice, which is
perceived as a rough sound. Shimmer describes fluc-
tuation of loudness in the voice. Noise-to-harmonics
ratio is associated with perceived hoarseness. All
features were speaker-normalized using z-scores.
For each task, we define entrainment between
partners on each feature f as
ENTp = ?|speaker1f ? speaker2f |
where speaker[1,2]f represents the corresponding
speaker?s mean for that feature over the task.
We say that the corpus shows evidence of en-
trainment on feature f if ENTp, the similarities be-
tween partners, are significantly greater than ENTx,
the similarities between non-partners:
ENTx = ?
?
i |speaker1f ?Xi,f |
|X|
where X is the set of speakers of same gender and
role as the speaker?s partner who are not paired with
the speaker in any session. We restrict the compar-
isons to speakers of the same gender and role as the
speaker?s partner to control for the fact that differ-
ences may simply be due to differences in gender or
role. The results of a series of paired t-tests compar-
ing ENTp and ENTx for each feature are summarized
in Table 1.
13
Feature FF MM FM
Intensity mean X X X
Intensity max X X X
Pitch mean X
Pitch max X
Jitter X X
Shimmer X X
NHR X
Syllables per sec X X X
Table 1: Evidence of entrainment for gender pairs. A tick
indicates that the data shows evidence of entrainment on
that row?s feature for that column?s gender pair.
We find that female-female pairs in our corpus
entrain on, in descending order of significance, jitter,
intensity max, intensity mean, syllables per second
and shimmer. They do not entrain on pitch mean
or max or NHR. Male-male pairs show the least
evidence of entrainment, entraining only on inten-
sity mean, intensity max, and syllables per second,
supporting the hypothesis that entrainment is less
prevalent among males. Female-male pairs entrain
on, again in descending order of significance, inten-
sity mean, intensity max, jitter, syllables per second,
pitch mean, NHR, shimmer, and pitch max ? in fact,
on every feature we examine, with significance val-
ues in each case of p<0.01.
To look more closely at the entrainment behavior
of males and females in mixed-gender pairs, we de-
fine ENT2p as follows:
ENT2p = ?
?
i |Pi,f ? Ti,f |
|T|
where T is the set of the pause-free chunks of speech
that begin a speaker?s turns, and P is the correspond-
ing set of pause-free chunks that end the interlocu-
tor?s preceding turns. Unlike ENTp, this measure is
asymmetric, allowing us to consider each member
of a pair separately.
We compare ENT2p for each feature for males and
females of mixed gender pairs. Contrary to our hy-
pothesis that females in mixed-gender pairs would
entrain more, we found no significant differences
in partner gender. Females in mixed-gender pairs
do not match their interlocutor?s previous turn any
more than do males. This may be due to the fact
Feature FM MM F p
Intensity mean ? ? 3.83 0.02
Intensity max ? ? 4.01 0.02
Syllables per sec ? ? 2.56 0.08
Table 2: Effects of gender pair on entrainment. An arrow
pointing up indicates that the group?s normalized entrain-
ment for that feature is greater than that of female-female
pairs; an arrow pointing down indicates that it is smaller.
that, as shown in Table 1, the overall differences be-
tween partners in mixed-gender pairs are quite low,
and so neither partner may be doing much turn-by-
turn matching.
However, as we expected, entrainment is least
prevalent among male-male pairs. Although we ex-
pected female-female pairs to exhibit the highest
prevalence of entrainment, they do not show evi-
dence of entrainment on pitch mean, pitch max or
NHR, while female-male pairs entrain on every fea-
ture. In fact, although ENTp for these features is not
significantly smaller between female-female pairs
than between female-male pairs, ENTx, the overall
similarity among non-partners for these features, is
significantly larger between females than between
females and males. The degree of similarity between
female-female partners is therefore attributable to
the overall similarity between females rather than
the effect of entrainment.
All three types of pairs exhibit entrainment on in-
tensity mean, intensity max, and syllables per sec-
ond. We look more closely into the gender-based
differences in entrainment behavior with an ANOVA
with the ratio of ENTp to ENTx as the dependent
variable and gender pair as the independent variable.
Normalizing ENTp by ENTx allows us to compare
the degree of entrainment across gender pairs. Re-
sults are shown in Table 2. Male-male pairs have
lower entrainment than female-female pairs for ev-
ery feature; female-male pairs have higher entrain-
ment than female-female pairs for intensity mean
and max and lower for syllables per second (p <
0.1). These results are consistent with the general
finding that male-male pairs entrain the least and
female-male pairs entrain the most.
14
5 Entrainment and social behavior
We next correlate each of the social variables de-
scribed in Section 3 with ENTp for our eight acous-
tic features. Based on Communication Accommo-
dation Theory, we would expect gives encourage-
ment, a variable representing a desirable social char-
acteristic, to be positively correlated with entrain-
ment. Conversely, conversation awkward should be
negatively correlated with entrainment. We note that
Trying to be liked is negatively correlated with the
like more variable in our data ? that is, annotators
were less likely to prefer speakers whom they per-
ceived as trying to be liked. This reflects the in-
tuition that someone overly eager to be liked may
be perceived as annoying and socially inept. How-
ever, similarity-attraction theory states that similar-
ity promotes attraction, and someone might there-
fore entrain in order to obtain his partner?s social
approval. This idea is supported by Natale?s find-
ing that the need for social approval is predictive
of the degree of a speaker?s convergence on inten-
sity (Natale, 1975). We can therefore expect trying
to be liked to positively correlate with entrainment.
Speakers who are perceived as trying to dominate
may be overly entraining to their interlocutors in
what is sometimes called ?dependency overaccom-
modation.? Dependency overaccommodation causes
the interlocutor to appear dependent on the speaker
and gives the impression that the speaker is control-
ling the conversation (West & Turner, 2009).
The results of our correlations of social vari-
ables with acoustic/prosodic entrainment are gen-
erally consonant with these intuitions. Although it
is not straightforward to compare correlation coeffi-
cients of groups for which we have varying amounts
of data, for purposes of assessing trends, we will
consider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The results are sum-
marized in Table 3; we present only the significant
results for space considerations.
For female-female pairs, giving encouragement
is weakly correlated with entrainment on intensity
max and shimmer. Conversation awkward is weakly
correlated with entrainment on jitter. For male-male
pairs, trying to be liked is moderately correlated
with entrainment on intensity mean and weakly cor-
related with entrainment on jitter and NHR. Giv-
ing encouragement is moderately correlated with
entrainment on intensity mean, intensity max, and
NHR. For female-male pairs, trying to be liked
is moderately correlated with entrainment on pitch
mean. Giving encouragement is strongly corre-
lated with entrainment on intensity mean and max
and moderately correlated with entrainment on pitch
mean and shimmer. However, it is negatively cor-
related with entrainment on jitter, although the cor-
relation is weak. Conversation awkward is weakly
correlated with entrainment on jitter.
As we expected, giving encouragement is corre-
lated with entrainment for all three gender groups,
and trying to be liked is correlated with entrainment
for male-male and female-male groups. However,
trying to dominate is not correlated with entrainment
on any feature, and conversation awkward is actu-
ally positively correlated with entrainment on jitter.
Entrainment on jitter is a clear outlier here, with
all of its correlations contrary to our hypotheses. In
addition to being positively correlated with conver-
sation awkward, it is the only feature to be nega-
tively correlated with giving encouragement.
Entrainment is correlated with the most social
variables for female-male pairs; these correlations
are also the strongest. We therefore conclude that
acoustic entrainment is not only most prevalent for
mixed-gender pairs, it is also more important to the
perception of female-male social behavior than it is
for same-gender pairs.
6 Entrainment and objective measures of
dialogue success
We now examine acoustic/prosodic entrainment in
our corpus according to four objective measures of
dialogue success: the mean latency between turns,
the percentage of turns that are interruptions, the
percentage of turns that are overlaps, and the number
of turns in a task.
High latency between turns can be considered a
sign of an unsuccessful conversation, with poor turn-
taking behavior indicating a possible lack of rapport
and difficulty in communication between the part-
ners. A high percentage of interruptions, another ex-
ample of poor turn-taking behavior, may be a symp-
tom of or a reason for hostility or awkwardness be-
15
Social Acoustic df r p
Female-Female
Giving Int. max -0.24 0.03
enc. Shimmer -0.24 0.03
Conv. Jitter -0.23 0.03
awkward
Male-Male
Trying to Int. mean -0.30 0.006
be liked Jitter -0.27 0.01
NHR -0.23 0.03
Giving Int. mean -0.39 0.0003
enc. Int. max -0.31 0.005
NHR -0.30 0.005
Female-Male
Trying to Pitch mean -0.26 0.001
be liked
Giving Int. mean -0.36 2.8e-06
enc. Int. max -0.31 7.7e-05
Pitch mean -0.23 0.003
Jitter 0.19 0.02
Shimmer -0.16 0.04
Conv. Jitter -0.17 0.04
awkward
Table 3: Correlations between entrainment and social
variables.
tween partners. We expect these measures to be neg-
atively correlated with entrainment. Conversely, a
high percentage of overlaps may be a symptom of
a well-coordinated conversation that is flowing eas-
ily. In the guidelines for the turn-taking annotation
of the Games Corpus (Gravano, 2009), overlaps are
defined as cases in which Speaker 2 takes the floor,
overlapping with the completion of Speaker 1?s ut-
terance. Overlaps require the successful reading of
turn-taking cues and by definition preclude awkward
pauses. We expect a high percentage of overlaps to
correlate positively with entrainment.
The number of turns in a task can be interpreted
either positively or negatively. A high number is
negative in that it is the sign of an inefficient dia-
logue, one which takes many turn exchanges to ac-
complish the objective. However, it may also be
the sign of easy, flowing dialogue between the part-
ners. In our domain, it may also be a sign of a high-
achieving pair who are placing the object meticu-
Objective Acoustic df r p
Female-Female
Latency Int. mean 0.22 0.04
Int. max 0.31 0.005
Pitch mean 0.24 0.02
Jitter 0.29 0.007
Shimmer 0.33 0.002
Syllables/sec 0.39 0.0002
# Turns Int. max -0.30 0.006
Shimmer -0.34 0.002
NHR -0.24 0.03
Syllables/sec -0.28 0.01
% Overlaps Int. max -0.23 0.04
Shimmer -0.30 0.005
% Interruptions Shimmer -0.33 0.005
Male-Male
Latency Int. mean 0.57 8.8e-08
Int. max 0.43 0.0001
Pitch mean 0.52 2.4e-06
Pitch max 0.61 5.7e-09
Jitter 0.65 4.5e-10
NHR 0.40 0.0004
# Turns Int. mean -0.29 0.0002
Pitch mean -0.32 0.003
Pitch max -0.29 0.007
NHR -0.47 7.9e-06
Syllables/sec -0.25 0.02
% Overlaps Int. mean -0.39 0.0002
Int. max -0.39 0.0002
% Interruptions NHR -0.33 0.002
Female-Male
# Turns Int. mean -0.24 0.003
Int. max -0.19 0.02
Shimmer -0.16 0.04
% Overlaps Shimmer -0.26 0.001
Table 4: Correlations between entrainment and objective
variables.
lously in order to secure every single point. We
therefore expect the number of turns to be positively
correlated with entrainment. As before, we con-
sider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The significant cor-
relations are presented in Table 4.
For female-female pairs, mean latency between
16
turns is negatively correlated with entrainment on all
variables except pitch max and NHR. The correla-
tions are weak for intensity mean and pitch mean
and moderate for intensity max, jitter, shimmer, and
syllables per second. The number of turns is moder-
ately correlated with entrainment on intensity max
and shimmer and weakly correlated with entrain-
ment on syllables per second. Contrary to our expec-
tations, the percentage of interruptions is positively
(though moderately) correlated with entrainment on
shimmer; the percentage of overlaps is moderately
correlated with entrainment on shimmer and weakly
correlated with entrainment on intensity max.
Male-male pairs show the most correlations be-
tween entrainment and objective measures of dia-
logue success. The latency between turns is neg-
atively correlated with entrainment on all variables
except shimmer and syllables per second; the corre-
lations are moderate for intensity max and NHR and
strong for the rest. The number of turns in a task
is positively correlated with entrainment on every
variable except intensity mean, jitter and shimmer:
strongly for NHR; moderately for intensity mean,
pitch mean, and pitch max; and weakly for syllables
per second.. The percentage of overlaps is moder-
ately correlated with entrainment on intensity mean
and max. The percentage of interruptions is moder-
ately correlated with entrainment on NHR.
For female-male pairs, the number of turns is
moderately correlated with entrainment on intensity
mean and weakly correlated with entrainment on in-
tensity max and shimmer. The percentage of over-
laps is moderately correlated with entrainment on
shimmer.
For the most part, the directions of the correla-
tions we have found are in accordance with our hy-
potheses. Latency is negatively correlated with en-
trainment and overlaps and the number of turns are
positively correlated. A puzzling exception is the
percentage of interruptions, which is positively cor-
related with entrainment on shimmer (for female-
female pairs) and NHR (for male-male pairs).
While the strongest correlations were for mixed-
gender pairs for the social variables, we find that
the strongest correlations for objective variables are
for male-male pairs, which also have the great-
est number of correlations. It therefore seems that
while entrainment is more important to the percep-
tion of social behavior for mixed-gender pairs than
it is for same-gender pairs, it is more important to
the smoothness and flow of dialogue for male-male
pairs than it is for female-female or female-male
pairs.
7 Entrainment in outliers
Since acoustic entrainment is generally considered
an unconscious phenomenon, it is interesting to con-
sider tasks in which a particular feature of a person?s
speech is particularly salient. This will occur when a
feature differs significantly from the norm ? for ex-
ample, when a person?s voice is unusually loud or
soft. Chartrand and Bargh (1999) suggest that the
psychological mechanism behind the entrainment is
the perception-behavior link, the finding that the act
of observing another?s behavior increases the like-
lihood of the observer?s engaging in that behavior.
Based on this finding, we hypothesize that a part-
ner pair containing one ?outlier? speaker will exhibit
more entrainment on the salient feature, since that
feature is more likely to be observed and therefore
imitated.
We consider values in the 10th or 90th percentile
for a feature ?outliers.? We can consider ENTx, the
similarity between a speaker and the speakers of her
partner?s role and gender with whom she is never
paired, the ?baseline? value for the similarity be-
tween a speaker and her interlocutor when no en-
trainment occurs. ENTp ? ENTx, the difference be-
tween the similarity existing between partners and
the baseline similarity, is then a measure of how
much entrainment exists relative to baseline.
We compare ENTp ? ENTx for ?normal? versus
?outlier? speakers. ENTp should be smaller for out-
lier speakers, since their interlocutors are not likely
to be similarly unusual. However, ENTx should also
be lower for outlier speakers, since by definition they
diverge from the norm, while the normal speakers
by definition represent the norm. It is therefore rea-
sonable to expect ENTp ? ENTx to be the same for
outlier speakers and normal speakers.
If ENTp ? ENTx is higher for outlier speakers,
that means that ENTp is higher than we expect, and
entrainment is greater relative to baseline for pairs
containing an outlier speaker. If ENTp ? ENTx is
lower for outlier speakers, that means that ENTp is
17
Acoustic t df p
Intensity mean 5.66 94.26 1.7e-07
Intensity max 8.29 152.05 5.5e-14
Pitch mean -1.20 76.82 N.S.
Pitch max -0.84 76.76 N.S.
Jitter 0.36 70.23 N.S.
Shimmer 2.64 102.23 0.02
NHR -0.92 137.34 N.S.
Syllables per sec 2.41 72.60 0.02
Table 5: T-tests for relative entrainment for outlier vs.
normal speakers.
lower than we expect, and pairs containing an outlier
speaker entrain less than do pairs of normal speak-
ers, even allowing for the fact that their usual values
should be further apart to begin with.
The results for t-tests comparing ENTp ? ENTx
for ?normal? versus ?outlier? speakers are shown
in Table 5. Outlier pairs have higher relative en-
trainment than do normal pairs for intensity mean
and max, shimmer, and syllables per second. This
means that speakers confronted with an interlocutor
who diverges widely from the norm for those four
features make a larger adjustment to their speech in
order to converge to that interlocutor.
An ANOVA shows that relative entrainment on
intensity max is higher in outlier cases for male-
male pairs than for female-female pairs and even
higher for female-male pairs (F=11.33, p=5.3e-05).
Relative entrainment on NHR in these cases is lower
for male-male pairs than for female-female pairs
and higher for female-male pairs (F=11.41, p=6.5e-
05). Relative entrainment on syllables per second
is lower for male-male pairs and higher for female-
male pairs (F=5.73, p=0.005). These results differ
slightly from the results in Table 2 for differences
in entrainment in the general case among gender
pairs, reinforcing the idea that cases in which fea-
ture values diverge widely from the norm are unique
in terms of entrainment behavior.
8 Conclusion
Our study of entrainment on acoustic/prosodic vari-
ables yields new findings about entrainment be-
havior for female-female, male-male, and mixed-
gender dyads, as well as the association of entrain-
ment with perceived social characteristics and ob-
jective measures of dialogue smoothness and effi-
ciency. We find that entrainment is the most preva-
lent for mixed-gender pairs, followed by female-
female pairs, with male-male pairs entraining the
least. Entrainment is the most important to the per-
ception of social behavior of mixed-gender pairs,
and it is the most important to the efficiency and flow
of male-male dialogues.
For the most part, the directions of the correla-
tions of entrainment with success variables accord
with hypotheses motivated by the relevant literature.
Giving encouragement and trying to be liked are
positively correlated with entrainment, as are per-
centage of overlaps and number of turns. Mean la-
tency, a symptom of a poorly-run conversation, is
negatively associated with entrainment. However,
several exceptions suggest that the associations are
not straightforward and further research must be
done to fully understand the relationship between
entrainment, social characteristics and dialogue suc-
cess. In particular, the explanation behind the as-
sociations of entrainment on certain variables with
certain social and objective measures is an interest-
ing direction for future work.
Finally, we find that in ?outlier? cases where a
particular speaker diverges widely from the norm for
intensity mean, intensity max, or syllables per sec-
ond, entrainment is more pronounced. This supports
the theory that the perception-behavior link is the
mechanism behind entrainment and provides a pos-
sible direction for research into why speakers entrain
on certain features and not others. In future work we
will explore this direction and go more thoroughly
into individual differences in entrainment behavior.
Acknowledgments
This material is based upon work supported in
part by NSF IIS-0307905, NSF IIS-0803148,
UBACYT 20020090300087, ANPCYT PICT-2009-
0026, CONICET, VEGA No. 2/0202/11; and the
EUSF (ITMS 26240220060).
References
Amazon Mechanical Turk, http://www.mturk.com.
Frances R. Bilous and Robert M. Krauss 1988. Dom-
inance and accommodation in the conversational be-
18
haviours of same- and mixed-gender dyads. Language
and Communication, 8(3/4):183?194.
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual Pacts and Lexical Choice in Conversation. Jour-
nal of Experimental Psychology: Learning, Memory
and Cognition, 22(6):1482?1493.
Tanya L. Chartrand and John A. Bargh. 1999. The
Chameleon Effect: The Perception-Behavior Link and
Social Interaction. Journal of Personality and Social
Psychology, 76(6):893?910.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark My Words! Linguistic
Style Accommodation in Social Media. Proceedings
of WWW 2011.
H. Giles and N. Coupland. 1991. Language: Contexts
and Consequences. Pacific Grove, CA: Brooks/Cole.
Agust??n Gravano. 2009. Turn-Taking and Affirmative
Cue Words in Task-Oriented Dialogue. Ph.D. thesis,
Columbia University, New York.
Agust??n Gravano and Julia Hirschberg. 2011. Turn-
taking cues in task-oriented dialogue. Computer
Speech and Language, 25(3):601?634.
Agust??n Gravano, Rivka Levitan, Laura Willson, S?tefan
Ben?us?, Julia Hirschberg, Ani Nenkova. 2011. Acous-
tic and Prosodic Correlates of Social Behavior. Inter-
speech 2011.
Chi-Chun Lee, Matthew Black, Athanasios Katsama-
nis, Adam Lammert, Brian Baucom, Andrew Chris-
tensen, Panayiotis G. Georgiou, Shrikanth Narayanan.
2010. Quantification of Prosodic Entrainment in Af-
fective Spontaneous Spoken Interactions of Married
Couples. Eleventh Annual Conference of the Interna-
tional Speech Communication Association.
Rivka Levitan, Agust??n Gravano, and Julia Hirschberg.
2011. Entrainment in Speech Preceding Backchan-
nels. Proceedings of ACL/HLT 2011.
Rivka Levitan and Julia Hirschberg. 2011. Measuring
acoustic-prosodic entrainment with respect to multi-
ple levels and dimensions. Proceedings of Interspeech
2011.
Laura L. Namy, Lynne C. Nygaard, Denise Sauerteig.
2002. Gender differences in vocal accommodation:
the role of perception. Journal of Language and So-
cial Psychology, 21(4):422?432.
Michael Natale. 1975. Convergence of Mean Vocal In-
tensity in Dyadic Communication as a Function of So-
cial Desirability. Journal of Personality and Social
Psychology, 32(5):790?804.
Ani Nenkova, Agust??n Gravano, and Julia Hirschberg.
2008. High-frequency word entrainment in spoken di-
alogue. Proceedings of ACL/HLT 2008.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction. Jour-
nal of Language and Social Psychology, 21(4):337?
360.
Jennifer S. Pardo. 2006. On phonetic convergence dur-
ing conversational interaction. Journal of the Acousti-
cal Society of America, 119(4):2382?2393.
David Reitter, Johanna D. Moore, and Frank Keller.
1996. Priming of Syntactic Rules in Task-Oriented Di-
alogue and Spontaneous Conversation. Proceedings of
the 28th Annual Conference of the Cognitive Science
Society.
Kim Silverman, Mary Beckman, John Pitrelli, Mori Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, Julia Hirschberg. 1992. TOBI: A Standard for
Labeling English Prosody. ICSLP-1992, 867-870.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
Nature of Rapport and its Nonverbal Correlates. Psy-
chological Inquiry, 1(4):285?293.
Richard West & Lynn Turner. 2009. Introducing
Communication Theory: Analysis and Application.
McGraw-Hill Humanities/Social Sciences/Languages,
4th edition.
19
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 544?554,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Linguistic Quality in Multi-Document
Summarization
Emily Pitler, Annie Louis, Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,lannie,nenkova@seas.upenn.edu
Abstract
To date, few attempts have been made
to develop and validate methods for au-
tomatic evaluation of linguistic quality in
text summarization. We present the first
systematic assessment of several diverse
classes of metrics designed to capture var-
ious aspects of well-written text. We train
and test linguistic quality models on con-
secutive years of NIST evaluation data in
order to show the generality of results. For
grammaticality, the best results come from
a set of syntactic features. Focus, coher-
ence and referential clarity are best evalu-
ated by a class of features measuring local
coherence on the basis of cosine similarity
between sentences, coreference informa-
tion, and summarization specific features.
Our best results are 90% accuracy for pair-
wise comparisons of competing systems
over a test set of several inputs and 70%
for ranking summaries of a specific input.
1 Introduction
Efforts for the development of automatic text sum-
marizers have focused almost exclusively on im-
proving content selection capabilities of systems,
ignoring the linguistic quality of the system out-
put. Part of the reason for this imbalance is the
existence of ROUGE (Lin and Hovy, 2003; Lin,
2004), the system for automatic evaluation of con-
tent selection, which allows for frequent evalua-
tion during system development and for report-
ing results of experiments performed outside of
the annual NIST-led evaluations, the Document
Understanding Conference (DUC)1 and the Text
Analysis Conference (TAC)2. Few metrics, how-
ever, have been proposed for evaluating linguistic
1http://duc.nist.gov/
2http://www.nist.gov/tac/
quality and none have been validated on data from
NIST evaluations.
In their pioneering work on automatic evalua-
tion of summary coherence, Lapata and Barzilay
(2005) provide a correlation analysis between hu-
man coherence assessments and (1) semantic re-
latedness between adjacent sentences and (2) mea-
sures that characterize how mentions of the same
entity in different syntactic positions are spread
across adjacent sentences. Several of their models
exhibit a statistically significant agreement with
human ratings and complement each other, yield-
ing an even higher correlation when combined.
Lapata and Barzilay (2005) and Barzilay and
Lapata (2008) both show the effectiveness of
entity-based coherence in evaluating summaries.
However, fewer than five automatic summarizers
were used in these studies. Further, both sets
of experiments perform evaluations of mixed sets
of human-produced and machine-produced sum-
maries, so the results may be influenced by the
ease of discriminating between a human and ma-
chine written summary. Therefore, we believe it is
an open question how well these features predict
the quality of automatically generated summaries.
In this work, we focus on linguistic quality eval-
uation for automatic systems only. We analyze
how well different types of features can rank good
and poor machine-produced summaries. Good
performance on this task is the most desired prop-
erty of evaluation metrics during system develop-
ment. We begin in Section 2 by reviewing the
various aspects of linguistic quality that are rel-
evant for machine-produced summaries and cur-
rently used in manual evaluations. In Section 3,
we introduce and motivate diverse classes of fea-
tures to capture vocabulary, sentence fluency, and
local coherence properties of summaries. We eval-
uate the predictive power of these linguistic qual-
ity metrics by training and testing models on con-
secutive years of NIST evaluations (data described
544
in Section 4). We test the performance of differ-
ent sets of features separately and in combination
with each other (Section 5). Results are presented
in Section 6, showing the robustness of each class
and their abilities to reproduce human rankings of
systems and summaries with high accuracy.
2 Aspects of linguistic quality
We focus on the five aspects of linguistic qual-
ity that were used to evaluate summaries in DUC:
grammaticality, non-redundancy, referential clar-
ity, focus, and structure/coherence.3 For each of
the questions, all summaries were manually rated
on a scale from 1 to 5, in which 5 is the best.
The exact definitions that were provided to the
human assessors are reproduced below.
Grammaticality: The summary should have no datelines,
system-internal formatting, capitalization errors or obviously
ungrammatical sentences (e.g., fragments, missing compo-
nents) that make the text difficult to read.
Non-redundancy: There should be no unnecessary repeti-
tion in the summary. Unnecessary repetition might take the
form of whole sentences that are repeated, or repeated facts,
or the repeated use of a noun or noun phrase (e.g., ?Bill Clin-
ton?) when a pronoun (?he?) would suffice.
Referential clarity: It should be easy to identify who or what
the pronouns and noun phrases in the summary are referring
to. If a person or other entity is mentioned, it should be clear
what their role in the story is. So, a reference would be un-
clear if an entity is referenced but its identity or relation to
the story remains unclear.
Focus: The summary should have a focus; sentences should
only contain information that is related to the rest of the sum-
mary.
Structure and Coherence: The summary should be well-
structured and well-organized. The summary should not just
be a heap of related information, but should build from sen-
tence to sentence to a coherent body of information about a
topic.
These five questions get at different aspects of
what makes a well-written text. We therefore pre-
dict each aspect of linguistic quality separately.
3 Indicators of linguistic quality
Multiple factors influence the linguistic quality of
text in general, including: word choice, the ref-
erence form of entities, and local coherence. We
extract features which serve as proxies for each of
the factors mentioned above (Sections 3.1 to 3.5).
In addition, we investigate some models of gram-
maticality (Chae and Nenkova, 2009) and coher-
ence (Graesser et al, 2004; Soricut and Marcu,
2006; Barzilay and Lapata, 2008) from prior work
(Sections 3.6 to 3.9).
3http://www-nlpir.nist.gov/projects/
duc/duc2006/quality-questions.txt
All of the features we investigate can be com-
puted automatically directly from text, but some
require considerable linguistic processing. Several
of our features require a syntactic parse. To extract
these, all summaries were parsed by the Stanford
parser (Klein and Manning, 2003).
3.1 Word choice: language models
Psycholinguistic studies have shown that people
read frequent words and phrases more quickly
(Haberlandt and Graesser, 1985; Just and Carpen-
ter, 1987), so the words that appear in a text might
influence people?s perception of its quality. Lan-
guage models (LM) are a way of computing how
familiar a text is to readers using the distribution
of words from a large background corpus. Bigram
and trigram LMs additionally capture grammati-
cality of sentences using properties of local tran-
sitions between words. For this reason, LMs are
widely used in applications such as generation and
machine translation to guide the production of sen-
tences. Judging from the effectiveness of LMs in
these applications, we expect that they will pro-
vide a strong baseline for the evaluation of at least
some of the linguistic quality aspects.
We built unigram, bigram, and trigram lan-
guage models with Good-Turing smoothing over
the New York Times (NYT) section of the English
Gigaword corpus (over 900 million words). We
used the SRI Language Modeling Toolkit (Stol-
cke, 2002) for this purpose. For each of the three
ngram language models, we include the min, max,
and average log probability of the sentences con-
tained in a summary, as well as the overall log
probability of the entire summary.
3.2 Reference form: Named entities
This set of features examines whether named enti-
ties have informative descriptions in the summary.
We focus on named entities because they appear
often in summaries of news documents and are of-
ten not known to the reader beforehand. In addi-
tion, first mentions of entities in text introduce the
entity into the discourse and so must be informa-
tive and properly descriptive (Prince, 1981; Frau-
rud, 1990; Elsner and Charniak, 2008).
We run the Stanford Named Entity Recognizer
(Finkel et al, 2005) and record the number of
PERSONs, ORGANIZATIONs, and LOCATIONs.
First mentions to people Feature exploration on
our development set found that under-specified
545
references to people are much more disruptive
to a summary than short references to organiza-
tions or locations. In fact, prior work in Nenkova
and McKeown (2003) found that summaries that
have been rewritten so that first mentions of peo-
ple are informative descriptions and subsequent
mentions are replaced with more concise reference
forms are overwhelmingly preferred to summaries
whose entity references have not been rewritten.
In this class, we include features that reflect
the modification properties of noun phrases (NPs)
in the summary that are first mentions to people.
Noun phrases can include pre-modifiers, apposi-
tives, prepositional phrases, etc. Rather than pre-
specifying all the different ways a person expres-
sion can be modified, we hoped to discover the
best patterns automatically, by including features
for the average number of each Part of Speech
(POS) tag occurring before, each syntactic phrase
occurring before4, each POS tag occurring after,
and each syntactic phrase occurring after the head
of the first mention NP for a PERSON. To measure
if the lack of pre or post modification is particu-
larly detrimental, we also include the proportion
of PERSON first mention NPs with no words be-
fore and with no words after the head of the NP.
Summarization specific Most summarization
systems today are extractive and create summaries
using complete sentences from the source docu-
ments. A subsequent mention of an entity in a
source document which is extracted to be the first
mention of the entity in the summary is proba-
bly not informative enough. For each type of
named entity (PERSON, ORGANIZATION, LO-
CATION), we separately record the number of in-
stances which appear as first mentions in the sum-
mary but correspond to non-first mentions in the
source documents.
3.3 Reference form: NP syntax
Some summaries might not include people and
other named entities at all. To measure how en-
tities are referred to more generally, we include
features about the overall syntactic patterns found
in NPs: the average number of each POS tag and
each syntactic phrase occurring inside NPs.
4We define a linear order based on a preorder traversal of
the tree, so syntactic phrases which dominate the head are
considered occurring before the head.
3.4 Local coherence: Cohesive devices
In coherent text, constituent clauses and sentences
are related and depend on each other for their in-
terpretation. Referring expressions such as pro-
nouns link the current utterance to those where the
entities were previously mentioned. In addition,
discourse connectives such as ?but? or ?because?
relate propositions or events expressed by differ-
ent clauses or sentences. Both these categories
are known cohesive or linking devices in human-
produced text (Halliday and Hasan, 1976). The
mere presence of such items in a text would be in-
dicative of better structure and coherence.
We compute a number of shallow features that
provide a cheap way of capturing the above intu-
itions: the number of demonstratives, pronouns,
and definite descriptions as well as the number of
sentence-initial discourse connectives.
3.5 Local coherence: Continuity
This class of linguistic quality indicators is a com-
bination of factors related to coreference, adjacent
sentence similarity, and summary-specific context
of surface cohesive devices.
Summarization specific Extractive multi-
document summaries often lack appropriate
antecedents for pronouns and proper context for
the use of discourse connectives.
In fact, early work in summarization (Paice,
1980; Paice, 1990) has pointed out that the pres-
ence of cohesive devices described in the previous
section might in fact be the source of problems.
A manual analysis of automatic summaries (Ot-
terbacher et al, 2002) also revealed that anaphoric
references that cannot be resolved and unclear dis-
course relations constitute more than 30% of all
revisions required to manually rewrite summaries
into a more coherent form.
To identify these potential problems, we adapt
the features for surface cohesive devices to indi-
cate whether referring expressions and discourse
connectives appear in the summary with the same
context as in the input documents.
For each of the cohesive devices discussed in
Section 3.4?demonstratives, pronouns, definite
descriptions, and sentence-initial discourse con-
nectives?we compare the previous sentence in
the summary with the previous sentence in the in-
put article. Two features are computed for each
type of cohesive device: (1) number of times the
preceding sentence in the summary is the same
546
as the preceding sentence in the input and (2) the
number of times the preceding sentence in sum-
mary is different from that in the input. Since
the previous sentence in the input text often con-
tains the antecedent of pronouns in the current
sentence, if the previous sentence from the input
is also included in the summary, the pronoun is
highly likely to have a proper antecedent.
We also compute the proportion of adjacent sen-
tences in the summary that were extracted from the
same input document.
Coreference Steinberger et al (2007) compare the
coreference chains in input documents and in sum-
maries in order to locate potential problems. We
instead define a set of more general features re-
lated to coreference that are not specific to sum-
marization and are applicable for any text. Our
features check the existence of proper antecedents
for pronouns in the summary without reference to
the text of the input documents.
We use the publicly available pronoun reso-
lution system described in Charniak and Elsner
(2009) to mark possible antecedents for pronouns
in the summary. We then compute as features the
number of times an antecedent for a pronoun was
found in the previous sentence, in the same sen-
tence, or neither. In addition, we modified the pro-
noun resolution system to also output the probabil-
ity of the most likely antecedent and include the
average antecedent probability for the pronouns
in the text. Automatic coreference systems are
trained on human-produced texts and we expect
their accuracies to drop when applied to automat-
ically generated summaries. However, the predic-
tions and confidence scores still reflect whether
or not possible antecedents exist in previous sen-
tences that match in gender/number, and so may
still be useful for coherence evaluation.
Cosine similarity We use cosine similarity to
compute the overlap of words in adjacent sen-
tences si and si+1 as a measure of continuity.
cos? =
vsi .vsi+1
||vsi ||||vsi+1 ||
(1)
The dimensions of the two vectors (vsi and
vsi+1) are the total number of word types from
both sentences si and si+1. Stop words were re-
tained. The value of each dimension for a sentence
is the number of tokens of that word type in that
sentence. We compute the min, max, and average
value of cosine similarity over the entire summary.
While some repetition is beneficial for cohe-
sion, too much repetition leads to redundancy in
the summary. Cosine similarity is thus indicative
of both continuity and redundancy.
3.6 Sentence fluency: Chae and Nenkova
(2009)
We test the usefulness of a suite of 38 shallow
syntactic features studied by Chae and Nenkova
(2009). These features are weakly but signif-
icantly correlated with the fluency of machine
translated sentences. These include sentence
length, number of fragments, average lengths of
the different types of syntactic phrases, total length
of modifiers in noun phrases, and various other
syntactic features. We expect that these structural
features will be better at detecting ungrammatical
sentences than the local language model features.
Since all of these features are calculated over in-
dividual sentences, we use the average value over
all the sentences in a summary in our experiments.
3.7 Coh-Metrix: Graesser et al (2004)
The Coh-Metrix tool5 provides an implementation
of 54 features known in the psycholinguistic lit-
erature to correlate with the coherence of human-
written texts (Graesser et al, 2004). These include
commonly used readability metrics based on sen-
tence length and number of syllables in constituent
words. Other measures implemented in the sys-
tem are surface text properties known to contribute
to text processing difficulty. Also included are
measures of cohesion between adjacent sentences
such as similarity under a latent semantic analysis
(LSA) model (Deerwester et al, 1990), stem and
content word overlap, syntactic similarity between
adjacent sentences, and use of discourse connec-
tives. Coh-Metrix has been designed with the
goal of capturing properties of coherent text and
has been used for grade level assessment, predict-
ing student essay grades, and various other tasks.
Given the heterogeneity of features in this class,
we expect that they will provide reasonable accu-
racies for all the linguistic quality measures. In
particular, the overlap features might serve as a
measure of redundancy and local coherence.
5http://cohmetrix.memphis.edu/
547
3.8 Word coherence: Soricut and Marcu
(2006)
Word co-occurrence patterns across adjacent sen-
tences provide a way of measuring local coherence
that is not linguistically informed but which can
be easily computed using large amounts of unan-
notated text (Lapata, 2003; Soricut and Marcu,
2006). Word coherence can be considered as the
analog of language models at the inter-sentence
level. Specifically, we used the two features in-
troduced by Soricut and Marcu (2006).
Soricut and Marcu (2006) make an analogy to
machine translation: two words are likely to be
translations of each other if they often appear in
parallel sentences; in texts, two words are likely to
signal local coherence if they often appear in ad-
jacent sentences. The two features we computed
are forward likelihood, the likelihood of observ-
ing the words in sentence si conditioned on si?1,
and backward likelihood, the likelihood of observ-
ing the words in sentence si conditioned on sen-
tence si+1. ?Parallel texts? of 5 million adjacent
sentences were extracted from the NYT section of
GigaWord. We used the GIZA++6 implementa-
tion of IBM Model 1 to align the words in adjacent
sentences and obtain all relevant probabilities.
3.9 Entity coherence: Barzilay and Lapata
(2008)
Linguistic theories, and Centering theory (Grosz
et al, 1995) in particular, have hypothesized that
the properties of the transition of attention from
entities in one sentence to those in the next, play a
major role in the determination of local coherence.
Barzilay and Lapata (2008), inspired by Center-
ing, proposed a method to compute the local co-
herence of texts on the basis of the sequences of
entity mentions appearing in them.
In their Entity Grid model, a text is represented
by a matrix with rows corresponding to each sen-
tence in a text, and columns to each entity men-
tioned anywhere in the text. The value of a cell
in the grid is the entity?s grammatical role in that
sentence (Subject, Object, Neither, or Absent). An
entity transition is a particular entity?s role in two
adjacent sentences. The actual entity coherence
features are the fraction of each type of these tran-
sitions in the entire entity grid for the text. One
would expect that coherent texts would contain
a certain distribution of entity transitions which
6http://www.fjoch.com/GIZA++.html
would differ from those in incoherent sequences.
We use the Brown Coherence Toolkit7 (Elsner
et al, 2007) to construct the grids. The tool does
not perform full coreference resolution. Instead,
noun phrases are considered to refer to the same
entity if their heads are identical.
Entity coherence features are the only ones that
have been previously applied with success for pre-
dicting summary coherence. They can therefore
be considered to be the state-of-the-art approach
for automatic evaluation of linguistic quality.
4 Summarization data
For our experiments, we use data from the
multi-document summarization tasks of the Doc-
ument Understanding Conference (DUC) work-
shops (Over et al, 2007).
Our training and development data comes from
DUC 2006 and our test data from DUC 2007.
These were the most recent years in which the
summaries were evaluated according to specific
linguistic quality questions. Each input consists
of a set of 25 related documents on a topic and the
target length of summaries is 250 words.
In DUC 2006, there were 50 inputs to be sum-
marized and 35 summarization systems which par-
ticipated in the evaluation. This included 34 au-
tomatic systems submitted by participants, and a
baseline system that simply extracted the lead-
ing sentences from the most recent article. In
DUC 2007, there were 45 inputs and 32 different
summarization systems. Apart from the leading
sentences baseline, a high performance automatic
summarizer from a previous year was also used
as a baseline. All these automatic systems are in-
cluded in our evaluation experiments.
4.1 System performance on linguistic quality
Each summary was evaluated according to the
five linguistic quality questions introduced in Sec-
tion 2: grammaticality, non-redundancy, referen-
tial clarity, focus, and structure. For each of these
questions, all summaries were manually rated on a
scale from 1 to 5, in which 5 is the best.
The distributions of system scores in the 2006
data are shown in Figure 1. Systems are currently
the worst at structure, middling at referential clar-
ity, and relatively better at grammaticality, focus,
7http://www.cs.brown.edu/
?
melsner/
manual.html
548
Figure 1: Distribution of system scores on the five
linguistic quality questions
Gram Non-redun Ref Focus Struct
Content .02 -.40 * .29 .28 .09
Gram .38 * .25 .24 .54 *
Non-redun -.07 -.09 .27
Ref .89 * .76 *
Focus .80 *
Table 1: Spearman correlations between the man-
ual ratings for systems averaged over the 50 inputs
in 2006; * p < .05
and non-redundancy. Structure is the aspect of lin-
guistic quality where there is the most room for
improvement. The only system with an average
structure score above 3.5 in DUC 2006 was the
leading sentences baseline system.
As can be expected, people are unlikely to be
able to focus on a single aspect of linguistic quality
exclusively while ignoring the rest. Some of the
linguistic quality ratings are significantly corre-
lated with each other, particularly referential clar-
ity, focus, and structure (Table 1).
More importantly, the systems that produce
summaries with good content8 are not necessar-
ily the systems producing the most readable sum-
maries. Notice from the first row of Table 1 that
none of the system rankings based on these mea-
sures of linguistic quality are significantly posi-
tively correlated with system rankings of content.
The development of automatic linguistic quality
measurements will allow researchers to optimize
both content and linguistic quality.
8as measured by summary responsiveness ratings on a 1
to 5 scale, without regard to linguistic quality
5 Experimental setup
We use the summaries from DUC 2006 for train-
ing and feature development and DUC 2007
served as the test set. Validating the results on con-
secutive years of evaluation is important, as results
that hold for the data in one year might not carry
over to the next, as happened for example in Con-
roy and Dang (2008)?s work.
Following Barzilay and Lapata (2008), we re-
port summary ranking accuracy as the fraction of
correct pairwise rankings in the test set.
We use a Ranking SVM (SV M light (Joachims,
2002)) to score summaries using our features. The
Ranking SVM seeks to minimize the number of
discordant pairs (pairs in which the gold stan-
dard has x1 ranked strictly higher than x2, but the
learner ranks x2 strictly higher than x1). The out-
put of the ranker is always a real valued score, so a
global rank order is always obtained. The default
regularization parameter was used.
5.1 Combining predictions
To combine information from the different feature
classes, we train a meta ranker using the predic-
tions from each class as features.
First, we use a leave-one out (jackknife) pro-
cedure to get the predictions of our features for
the entire 2006 data set. To predict rankings of
systems on one input, we train all the individual
rankers, one for each of the classes of features in-
troduced above, on data from the remaining in-
puts. We then apply these rankers to the sum-
maries produced for the held-out input. By repeat-
ing this process for each input in turn, we obtain
the predicted scores for each summary.
Once this is done, we use these predicted scores
as features for the meta ranker, which is trained on
all 2006 data. To test on a new summary pair in
2007, we first apply each individual ranker to get
its predictions, and then apply the meta ranker.
In either case (meta ranker or individual feature
class), all training is performed on 2006 data, and
all testing is done on 2007 data which guarantees
the results generalize well at least from one year
of evaluation to the next.
5.2 Evaluation of rankings
We examine the predictive power of our features
for each of the five linguistic quality questions in
two settings. In system-level evaluation, we would
like to rank all participating systems according to
549
their performance on the entire test set. In input-
level evaluation, we would like to rank all sum-
maries produced for a single given input.
For input-level evaluation, the pairs are formed
from summaries of the same input. Pairs in which
the gold standard ratings are tied are not included.
After removing the ties, the test set consists of 13K
to 16K pairs for each linguistic quality question.
Note that there were 45 inputs and 32 automatic
systems in DUC 2007. So, there are a total of
45?
(32
2
)
= 22, 320 possible summary pairs.
For system-level evaluation, we treat the real-
valued output of the SVM ranker for each sum-
mary as the linguistic quality score. The 45 indi-
vidual scores for summaries produced by a given
system are averaged to obtain an overall score for
the system. The gold-standard system-level qual-
ity rating is equal to the average human ratings for
the system?s summaries over the 45 inputs. At the
system level, there are about 500 non-tied pairs in
the test set for each question.
For both evaluation settings, a random baseline
which ranked the summaries in a random order
would have an expected pairwise accuracy of 50%.
6 Results and discussion
6.1 System-level evaluation
System-level accuracies for each class of features
are shown in Table 2. All classes of features per-
form well, with at least a 20% absolute increase
in accuracy over the random baseline (50% ac-
curacy). For each of the linguistic quality ques-
tions, the corresponding best class of features
gives prediction accuracies around 90%. In other
words, if these features were used to fully auto-
matically compare systems that participated in the
2007 DUC evaluation, only one out of ten com-
parisons would have been incorrect. These results
set a high standard for future work on automatic
system-level evaluation of linguistic quality.
The state-of-the-art entity coherence features
perform well but are not the best for any of the five
aspects of linguistic quality. As expected, sentence
fluency is the best feature class for grammatical-
ity. For all four other questions, the best feature
set is Continuity, which is a combination of sum-
marization specific features, coreference features
and cosine similarity of adjacent sentences. Conti-
nuity features outperform entity coherence by 3 to
4% absolute difference on referential quality, fo-
cus, and coherence. Accuracies from the language
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 87.6 83.0 91.2 85.2 86.3
Named ent. 78.5 83.6 82.1 74.0 69.6
NP syntax 85.0 83.8 87.0 76.6 79.2
Coh. devices 82.1 79.5 82.7 82.3 83.7
Continuity 88.8 88.5 92.9 89.2 91.4
Sent. fluency 91.7 78.9 87.6 82.3 84.9
Coh-Metrix 87.2 86.0 88.6 83.9 86.3
Word coh. 81.7 76.0 87.8 81.7 79.0
Entity coh. 90.2 88.1 89.6 85.0 87.1
Meta ranker 92.9 87.9 91.9 87.8 90.0
Table 2: System-level prediction accuracies (%)
model features are within 1% of entity coherence
for these three aspects of summary quality.
Coh-Metrix, which has been proposed as a com-
prehensive characterization of text, does not per-
form as well as the language model and the en-
tity coherence classes, which contain considerably
fewer features related to only one aspect of text.
The classes of features specific to named enti-
ties and noun phrase syntax are the weakest pre-
dictors. It is apparent from the results that conti-
nuity, entity coherence, sentence fluency and lan-
guage models are the most powerful classes of fea-
tures that should be used in automation of evalu-
ation and against which novel predictors of text
quality should be compared.
Combining all feature classes with the meta
ranker only yields higher results for grammatical-
ity. For the other aspects of linguistic quality, it is
better to use Continuity by itself to rank systems.
One certainly unexpected result is that features
designed to capture one aspect of well-written text
turn out to perform well for other questions as
well. For instance, entity coherence and continuity
features predict grammaticality with very high ac-
curacy of around 90%, and are surpassed only by
the sentence fluency features. These findings war-
rant further investigation because we would not
expect characteristics of local transitions indica-
tive of text structure to have anything to do with
sentence grammaticality or fluency. The results
are probably due to the significant correlation be-
tween structure and grammaticality (Table 1).
6.2 Input-level evaluation
The results of the input-level ranking experiments
are shown in Table 3. Understandably, input-
level prediction is more difficult and the results are
lower compared to the system-level predictions:
even with wrong predictions for some of the sum-
maries by two systems, the overall judgment that
550
one system is better than the other over the entire
test set can still be accurate.
While for system-level predictions the meta
ranker was only useful for grammaticality, at the
input level it outperforms every individual feature
class for each of the five questions, obtaining ac-
curacies around 70%.
These input-level accuracies compare favorably
with automatic evaluation metrics for other nat-
ural language processing tasks. For example, at
the 2008 ACL Workshop on Statistical Machine
Translation, all fifteen automatic evaluation met-
rics, including variants of BLEU scores, achieved
between 42% and 56% pairwise accuracy with hu-
man judgments at the sentence level (Callison-
Burch et al, 2008).
As in system-level prediction, for referential
clarity, focus, and structure, the best feature class
is Continuity. Sentence fluency again is the best
class for identifying grammaticality.
Coh-Metrix features are now best for determin-
ing redundancy. Both Coh-Metrix and Continuity
(the top two features for redundancy) include over-
lap measures between adjacent sentences, which
serve as a good proxy for redundancy.
Surprisingly, the relative performance of the
feature classes at input level is not the same as
for system-level prediction. For example, the lan-
guage model features, which are the second best
class for the system-level, do not fare as well at
the input-level. Word co-occurrence which ob-
tained good accuracies at the system level is the
least useful class at the input level with accuracies
just above chance in all cases.
6.3 Components of continuity
The class of features capturing sentence-to-
sentence continuity in the summary (Section 3.5)
are the most effective for predicting referential
clarity, focus, and structure at the input level.
We now investigate to what extent each of its
components?summary-specific features, corefer-
ence, and cosine similarity between adjacent
sentences?contribute to performance.
Results obtained after excluding each of the
components of continuity is shown in Table 4;
each line in the table represents Continuity mi-
nus a feature subclass. Removing cosine over-
lap causes the largest drop in prediction accuracy,
with results about 10% lower than those for the
complete Continuity class. Summary specific fea-
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 66.3 57.6 62.2 60.5 62.5
Named ent. 52.9 54.4 60.0 54.1 52.5
NP Syntax 59.0 50.8 59.1 54.5 55.1
Coh. devices 56.8 54.4 55.2 52.7 53.6
Continuity 61.7 62.5 69.7 65.4 70.4
Sent. fluency 69.4 52.5 64.4 61.9 62.6
Coh-Metrix 65.5 67.6 67.9 63.0 62.4
Word coh. 54.7 55.5 53.3 53.2 53.7
Entity coh. 61.3 62.0 64.3 64.2 63.6
Meta ranker 71.0 68.6 73.1 67.4 70.7
Table 3: Input-level prediction accuracies (%)
tures, which compare the context of a sentence
in the summary with the context in the original
document where it appeared, also contribute sub-
stantially to the success of the Continuity class in
predicting structure and referential clarity. Accu-
racies drop by about 7% when these features are
excluded. However, the coreference features do
not seem to contribute much towards predicting
summary linguistic quality. The accuracies of the
Continuity class are not affected at all when these
coreference features are not included.
6.4 Impact of summarization methods
In this paper, we have discussed an analysis of the
outputs of current research systems. Almost all
of these systems still use extractive methods. The
summarization specific continuity features reward
systems that include the necessary preceding con-
text from the original document. These features
have high prediction accuracies (Section 6.3) of
linguistic quality, however note that the support-
ing context could often contain less important con-
tent. Therefore, there is a tension between strate-
gies for optimizing linguistic quality and for op-
timizing content, which warrants the development
of abstractive methods.
As the field moves towards more abstractive
summaries, we expect to see differences in both
a) summary linguistic quality and b) the features
predictive of linguistic aspects.
As discussed in Section 4.1, systems are cur-
rently worst at structure/coherence. However,
grammaticality will become more of an issue as
systems use sentence compression (Knight and
Marcu, 2002), reference rewriting (Nenkova and
McKeown, 2003), and other techniques to produce
their own sentences.
The number of discourse connectives is cur-
rently significantly negatively correlated with
structure/coherence (Spearman correlation of r =
551
Ref. Focus Struct.
Continuity 69.7 65.4 70.4
- Sum-specific 63.9 64.2 63.5
- Coref 70.1 65.2 70.6
- Cosine 60.2 56.6 60.7
Table 4: Ablation within the Continuity class;
pairwise accuracy for input-level predictions (%)
-.06, p = .008 on DUC 2006 system summaries).
This can be explained by the fact that they of-
ten lack proper context in an extractive summary.
However, an abstractive system could plan a dis-
course structure and insert appropriate connectives
(Saggion, 2009). In this case, we would expect the
presence of discourse connectives to be a mark of
a well-written summary.
6.5 Results on human-written abstracts
Since abstractive summaries would have markedly
different properties from extracts, it would be in-
teresting to know how well these sets of features
would work for predicting the quality of machine-
produced abstracts. However, since current sys-
tems are extractive, such a data set is not available.
Therefore we experiment on human-written ab-
stracts to get an estimate of the expected per-
formance of our features on abstractive system
summaries. In both DUC 2006 and DUC 2007,
ten NIST assessors wrote summaries for the var-
ious inputs. There are four human-written sum-
maries for each input and these summaries were
judged on the same five linguistic quality aspects
as the machine-written summaries. We train on the
human-written summaries from DUC 2006 and
test on the human-written summaries from DUC
2007, using the same set-up as in Section 5.
These results are shown in Table 5. We only re-
port results on the input level, as we are interested
in distinguishing between the quality of the sum-
maries, not the NIST assessors? writing skills.
Except for grammaticality, the prediction accu-
racies of the best feature classes for human ab-
stracts are better than those at input level for ma-
chine extracts. This result is promising, as it shows
that similar features for evaluating linguistic qual-
ity will be valid for abstractive summaries as well.
Note however that the relative performance of
the feature sets changes between the machine and
human results. While for the machines Continu-
ity feature class is the best predictor of referential
clarity, focus, and structure (Table 3), for humans,
language models and sentence fluency are best for
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 52.1 60.8 76.5 71.9 78.4
Named ent. 62.5 66.7 47.1 43.9 59.1
NP Syntax 64.6 49.0 43.1 49.1 58.0
Coh. devices 54.2 68.6 66.7 49.1 64.8
Continuity 54.2 49.0 62.7 61.4 71.6
Sent. fluency 54.2 64.7 80.4 71.9 72.7
Coh-Metrix 54.2 52.9 68.6 56.1 69.3
Word coh. 62.5 58.8 62.7 70.2 60.2
Entity coh. 45.8 49.0 54.9 52.6 56.8
Meta ranker 62.5 56.9 80.4 50.9 67.0
Table 5: Input-level prediction accuracies for
human-written summaries (%)
these three aspects of linguistic quality. A possi-
ble explanation for this difference could be that in
system-produced extracts, incoherent organization
influences human perception of linguistic quality
to a great extent and so local coherence features
turned out very predictive. But in human sum-
maries, sentences are clearly well-organized and
here, continuity features appear less useful. Sen-
tence level fluency seems to be more predictive of
the linguistic quality of these summaries.
7 Conclusion
We have presented an analysis of a wide variety
of features for the linguistic quality of summaries.
Continuity between adjacent sentences was con-
sistently indicative of the quality of machine gen-
erated summaries. Sentence fluency was useful for
identifying grammaticality. Language model and
entity coherence features also performed well and
should be considered in future endeavors for auto-
matic linguistic quality evaluation.
The high prediction accuracies for input-level
evaluation and the even higher accuracies for
system-level evaluation confirm that questions re-
garding the linguistic quality of summaries can be
answered reasonably using existing computational
techniques. Automatic evaluation will make test-
ing easier during system development and enable
reporting results obtained outside of the cycles of
NIST evaluation.
Acknowledgments
This material is based upon work supported under
a National Science Foundation Graduate Research
Fellowship and NSF CAREER award 0953445.
We would like to thank Bonnie Webber for pro-
ductive discussions.
552
References
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2008. Further meta-evaluation of ma-
chine translation. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 70?
106.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case studies
of machine translation and human-written text. In
Proceedings of EACL, pages 139?147.
E. Charniak and M. Elsner. 2009. EM works for pro-
noun anaphora resolution. In Proceedings of EACL,
pages 148?156.
J.M. Conroy and H.T. Dang. 2008. Mind the gap: dan-
gers of divorcing evaluations of summary content
from linguistic quality. In Proceedings of COLING,
pages 145?152.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41:391?407.
M. Elsner and E. Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of
ACL/HLT: Short Papers, pages 41?44.
M. Elsner, J. Austerweil, and E. Charniak. 2007. A
unified local and global model for discourse coher-
ence. In Proceedings of NAACL/HLT.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL, pages 363?370.
K. Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Se-
mantics, 7(4):395.
A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-Metrix: Analysis of text on co-
hesion and language. Behavior Research Methods
Instruments and Computers, 36(2):193?202.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of
discourse. Computational Linguistics, 21(2):203?
226.
K.F. Haberlandt and A.C. Graesser. 1985. Component
processes in text comprehension and some of their
interactions. Journal of Experimental Psychology:
General, 114(3):357?374.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman Group Ltd, London, U.K.
T. Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
M.A. Just and P.A. Carpenter. 1987. The psychology
of reading and language comprehension. Allyn and
Bacon Boston, MA.
D. Klein and C.D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, pages 423?
430.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
M. Lapata and R. Barzilay. 2005. Automatic evalua-
tion of text coherence: Models and representations.
In International Joint Conference On Artificial In-
telligence, volume 19, page 1085.
M. Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings
of ACL, pages 545?552.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of NAACL/HLT, page 78.
C.Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the Work-
shop on Text Summarization Branches Out (WAS
2004), pages 25?26.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. In Proceedings of
HLT/NAACL 2003 (short paper).
J. Otterbacher, D. Radev, and A. Luo. 2002. Revi-
sions that improve cohesion in multi-document sum-
maries: a preliminary study. In Proceedings of the
Workshop on Automatic Summarization, ACL.
P. Over, H. Dang, and D. Harman. 2007. Duc
in context. Information Processing Management,
43(6):1506?1520.
C.D. Paice. 1980. The automatic generation of litera-
ture abstracts: an approach based on the identifica-
tion of self-indicating phrases. In Proceedings of the
3rd annual ACM conference on Research and devel-
opment in information retrieval, pages 172?191.
C.D. Paice. 1990. Constructing literature abstracts by
computer: Techniques and prospects. Information
Processing Management, 26(1):171?186.
E.F. Prince. 1981. Toward a taxonomy of given-new
information. Radical pragmatics, 223:255.
H. Saggion. 2009. A Classification Algorithm for Pre-
dicting the Structure of Summaries. Proceedings
of the 2009 Workshop on Language Generation and
Summarisation, page 31.
553
R. Soricut and D. Marcu. 2006. Discourse generation
using utility-trained coherence models. In Proceed-
ings of ACL.
J. Steinberger, M. Poesio, M.A. Kabadjov, and K. Jeek.
2007. Two uses of anaphora resolution in sum-
marization. Information Processing Management,
43(6):1663?1680.
A. Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing, volume 3.
554
11
Automatic Summarization
Ani Nenkova University of Pennsylvania
Sameer Maskey IBM Research
Yang Liu University of Texas at Dallas
2
Why summarize?
23
Text summarization
News articles
Scientific Articles
Emails
Books
Websites
Social Media 
Streams
4
Speech summarization
MeetingPhone Conversation
Classroom
Radio NewsBroadcast News
Talk Shows
Lecture
Chat
35
How to 
summarize
Text & Speech?
-Algorithms
-Issues
-Challenges
-Systems
Tutorial
6
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Frequency, Lexical chains, TF*IDF,
Topic Words, Topic Models [LSA, EM, Bayesian]
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
47
Motivation: where does summarization 
help?
 Single document summarization 
 Simulate the work of intelligence analyst
 Judge if a document is relevant to a topic of interest
?Summaries as short as 17% of the full text length speed up 
decision making twice, with no significant degradation in 
accuracy.?
?Query-focused summaries enable users to find more relevant 
documents more accurately, with less need to consult the full text 
of the document.?
[Mani et al, 2002]
8
Motivation: multi-document summarization 
helps in compiling and presenting
 Reduce search time, especially when the goal of the 
user is to find as much information as possible about a 
given topic
 Writing better reports, finding more relevant information, 
quicker
 Cluster similar articles and provide a multi-document 
summary of the similarities
 Single document summary of the information unique to 
an article
[Roussinov and Chen, 2001; Mana-Lopez et al, 2004; McKeown et al, 2005 ]
59
Benefits from speech summarization
 Voicemail
 Shorter time spent on listening (call centers)
 Meetings
 Easier to find main points
 Broadcast News
 Summary of story from mulitiple channels
 Lectures
 Useful for reviewing of course materials
[He et al, 2000; Tucker and Whittaker, 2008; Murray et al, 2009]
10
Assessing summary quality: overview
 Responsiveness
 Assessor directly rate each summary on a scale
 In official evaluations but rarely reported in papers
 Pyramid
 Assessors create model summaries
 Assessors identifies semantic overlap between summary 
and models
 ROUGE
 Assessors create model summaries
 ROUGE automatically computes word overlap
611
Tasks in summarization
Content (sentence) selection
 Extractive summarization
Information ordering
 In what order to present the selected sentences, especially 
in multi-document summarization
Automatic editing, information fusion and compression
 Abstractive summaries
12
Extractive (multi-document) summarization
Input text2Input text1 Input text3
Summary
1. Selection
2. Ordering
3. Fusion
Compute Informativeness
713
Computing informativeness
 Topic models (unsupervised)
 Figure out what the topic of the input
 Frequency, Lexical chains, TF*IDF
 LSA, content models (EM, Bayesian) 
 Select informative sentences based on the topic
 Graph models (unsupervised)
 Sentence centrality
 Supervised approaches
 Ask people which sentences should be in a summary
 Use any imaginable feature to learn to predict human 
choices
14
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, Lexical chains, TF*IDF, 
Topic Words,Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
815
Frequency as document topic proxy
10 incarnations of an intuition
 Simple intuition, look only at the document(s)
 Words that repeatedly appear in the document are likely to 
be related to the topic of the document
 Sentences that repeatedly appear in different input 
documents represent themes in the input
 But what appears in other documents is also helpful 
in determining the topic
 Background corpus probabilities/weights for word 
16
What is an article about?
 Word probability/frequency
 Proposed by Luhn in 1958 [Luhn 1958]
 Frequent content words would be indicative of the 
topic of the article
 In multi-document summarization, words or 
facts repeated in the input are more likely to 
appear in human summaries [Nenkova et al, 2006]
917
Word probability/weights 
Libya
bombing
trail
Gadafhi
suspects
Libya refuses 
to surrender 
two Pan Am 
bombing 
suspects 
Pan Am
INPUT
SUMMARY
WORD PROBABILITY TABLE
Word Probability
pan 0.0798
am 0.0825
libya 0.0096
suspects 0.0341
gadafhi 0.0911
trail 0.0002
?.
usa 0.0007
HOW?
UK and 
USA
18
HOW: Main steps in sentence selection 
according to word probabilities
Step 1 Estimate word weights (probabilities)
Step 2 Estimate sentence weights
Step 3 Choose best sentence
Step 4 Update word weights
Step 5 Go to 2 if desired length not reached
)()( SentwCFSentWeight i ?=
10
19
More specific choices [Vanderwende et al, 2007; Yih et al, 
2007; Haghighi and Vanderwende, 2009]
 Select highest scoring sentence
 Update word probabilities for the selected sentence 
to reduce redundancy
 Repeat until desired summary length
?
?
=
Sw
wp
S
SScore )(||
1)(
pnew (w) = pold (w).pold (w)
20
Is this a reasonable approach: yes, people 
seem to be doing something similar
 Simple test
 Compute word probability table from the input
 Get a batch of summaries written by H(umans) and S(ystems)
 Compute the likelihood of the summaries given the word 
probability table 
 Results
 Human summaries have higher likelihood
HSSSSSSSSSSHSSSHSSHHSHHHHH
HIGH LIKELIHOODLOW
11
21
Obvious shortcomings of the pure 
frequency approaches
 Does not take account of related words
 suspects -- trail
 Gadhafi ? Libya
 Does not take into account evidence from 
other documents
 Function words: prepositions, articles, etc.
 Domain words: ?cell? in cell biology articles
 Does not take into account many other 
aspects
22
Two easy fixes
 Lexical chains [Barzilay and Elhadad, 1999, Silber and McCoy, 
2002, Gurevych and Nahnsen, 2005]
 Exploits existing lexical resources (WordNet)
 TF*IDF weights [most summarizers]
 Incorporates evidence from a background corpus
12
23
Lexical chains and WordNet relations
 Lexical chains
 Word sense disambiguation is performed 
 Then topically related words represent a topic
 Synonyms, hyponyms, hypernyms
 Importance is determined by frequency of the words in a 
topic rather than a single word
 One sentence per topic is selected 
 Concepts based on WordNet [Schiffman et al, 2002, Ye et al, 
2007]
 No word sense disambiguation is performed
 {war, campaign, warfare, effort, cause, operation}
 {concern, carrier, worry, fear, scare}
24
TF*IDF weights for words
Combining evidence for document topics from the 
input and from a background corpus
 Term Frequency (TF)
 Times a word occurs in the input 
 Inverse Document Frequency (IDF)
 Number of documents (df) from a background 
corpus of N documents that contain the word
)/log(* dfNtfIDFTF ?=
13
25
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
26
Topic words (topic signatures)
 Which words in the input are most descriptive?
 Instead of assigning probabilities or weights to all words, 
divide words into two classes: descriptive or not
 For iterative sentence selection approach, the binary 
distinction is key to the advantage over frequency and 
TF*IDF
 Systems based on topic words have proven to be the most 
successful in official summarization evaluations 
14
27
Example input and associated topic words
 Input for summarization: articles relevant to the 
following user need
Title: Human Toll of Tropical 
Storms Narrative: What has been the human toll in death or injury 
of tropical storms in recent years? Where and when have each of 
the storms caused human casualties? What are the approximate 
total number of casualties attributed to each of the storms?
ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent, 
coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned, 
flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane, 
insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua, 
north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt, 
st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather, 
west, winds, yesterday.
Topic Words
28
Formalizing the problem of identifying topic 
words 
 Given
 t: a word that appears in the input
 T: cluster of articles on a given topic (input)
 NT: articles not on topic T (background corpus)
 Decide if t is a topic word or not
 Words that have (almost) the same probability in T 
and NT are not topic words
15
29
Computing probabilities
 View a text as a sequence of Bernoulli trails
 A word is either our term of interest t or not
 The likelihood of observing term t which occurs with 
probability p in a text consisting of N words is given by 
 Estimate the probability of t in three ways
 Input + background corpus combines
 Input only
 Background only
t
30
Testing which hypothesis is more 
likely: log-likelihood ratio test
has a known statistical distribution: chi-square 
At a given significance level, we can decide if a word is 
descriptive of the input or not.
This feature is used in the best performing systems for 
multi-document summarization of news [Lin and Hovy, 
2000; Conroy et al, 2006]
Likelihood of the data given H1
Likelihood of the data given H2
? =
-2 log ?
16
31
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
32
The background corpus takes more 
central stage
 Learn topics from the background corpus
 topic ~ themes often discusses in the background
 topic representation ~ word probability tables
 Usually one time training step
 To summarize an input 
 Select sentences from the input that correspond 
to the most prominent topics
17
33
Latent semantic analysis (LSA) [Gong and Liu, 
2001, Hachey et al, 2006, Steinberger et al, 2007]
 Discover topics from the background corpus with n unique 
words and d documents
 Represent the background corpus as nxd matrix A
 Rows correspond to words
 Aij=number of times word I appears in document j
 Use standard change of coordinate system and dimensionality 
reduction techniques
 In the new space each row corresponds to the most important 
topics in the corpus
 Select the best sentence to cover each topic
TUPVA =
34
Notes on LSA and other approaches
 The original article that introduced LSA for 
single document summarization of news did 
not find significant difference with TF*IDF
 For multi-document summarization of news 
LSA approaches have not outperformed topic 
words or extensions of frequency approaches
 Other topic/content models have been much 
more influential
18
35
Domain dependent content models
 Get sample documents from the domain
 background corpus
 Cluster sentences from these documents 
 Implicit topics
 Obtain a word probability table for each topic
 Counts only from the cluster representing the 
topic
 Select sentences from the input with highest 
probability for main topics 
36
Text structure can be learnt
 Human-written examples from a domain
Location, time
relief efforts
magnitude
damage
19
37
Topic = cluster of similar sentences from 
the background corpus
 Sentences cluster from earthquake articles
 Topic ?earthquake location?
 The Athens seismological institute said the temblor?s epicenter 
was located 380 kilometers (238 miles) south of the capital.
 Seismologists in Pakistan?s Northwest Frontier Province said the 
temblor?s epicenter was about 250 kilometers (155 miles) north of 
the provincial capital Peshawar.
 The temblor was centered 60 kilometers (35 miles) north- west of 
the provincial capital of Kunming, about 2,200 kilometers (1,300
miles) southwest of Beijing, a bureau seismologist said.
38
Content model [Barzilay and Lee, 2004, Pascale et al, 2003] 
 Hidden Markov Model (HMM)-based 
 States - clusters of related sentences ?topics?
 Transition prob. - sentence precedence in corpus
 Emission prob. - bigram language model
location, 
magnitude casualties
relief efforts
)|()|(),|,( 11111 +++++ ?=><>< iieiitiiii hsphhphshsp
Earthquake reportsTransition from previous 
topic
Generating 
sentence in 
current topic
20
39
Learning the content model
 Many articles from the same domain
 Cluster sentences: each cluster represents a topic from 
the domain
 Word probability tables for each topic
 Transitions between clusters can be computed from 
sentence adjacencies in the original articles  
 Probabilities of going from one topic to another
 Iterate between clustering and transition probability 
estimation to obtain domain model
40
To select a summary
 Find main topics in the domain
 using a small collection of summary-input pairs
 Find the most likely topic for each sentence in 
the input 
 Select the best sentence per main topic
21
41
Historical note
 Some early approaches to multi-document 
summarization relied on clustering the 
sentences in the input alone [McKeown et al, 1999, 
Siddharthan et al, 2004]
 Clusters of similar sentences represent a theme in 
the input
 Clusters with more sentences are more important
 Select one sentence per important cluster
42
Example cluster
Choose one sentence to represent the cluster
1. PAL was devastated by a pilots' strike in June and by the 
region's currency crisis.
2. In June, PAL was embroiled in a crippling three-week 
pilots' strike.
3. Tan wants to retain the 200 pilots because they stood by 
him when the majority of PAL's pilots staged a 
devastating strike in June.
22
43
Bayesian content models
 Takes a batch of inputs for summarization
 Many word probability tables
 One for general English
 One for each of the inputs to be summarized
 One for each document in any input
To select a summary S with L words from 
document collection D given as input
The goal is to select the summary, not a 
sentence. Greedy selection vs. global will 
be discussed in detail later
S* = minS:words(S)?LKL(PD||PS)
44
KL divergence
 Distance between two probability distributions: P, Q
 P, Q: Input and summary word distributions  
KL (P || Q) = pP (w) log2 pP (w)pQ (w)w?
23
45
Intriguing side note
 In the full Bayesian topic models, word 
probabilities for all words is more important 
than binary distinctions of topic and non-topic 
word
 Haghighi and Vanderwende report that a 
system that chooses the summary with 
highest expected number of topic words 
performs as SumBasic
46
Review
 Frequency based informativeness has been 
used in building summarizers
 Topic words probably more useful
 Topic models
 Latent Semantic Analysis
 Domain dependent content model
 Bayesian content model
24
47
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
48
Using graph representations [Erkan and Radev, 
2004; Mihalcea and Tarau, 2004; Leskovec et al, 2005 ]
 Nodes
 Sentences
 Discourse entities
 Edges
 Between similar sentences
 Between syntactically related entities
 Computing sentence similarity
 Distance between their TF*IDF weighted vector 
representations
25
49
50
Sentence :
Iraqi vice president?
Sentence :
Ivanov contended?
Sim(d1s1, d3s2)
26
51
Advantages of the graph model
 Combines word frequency and sentence 
clustering
 Gives a formal model for computing 
importance: random walks
 Normalize weights of edges to sum to 1
 They now represent probabilities of transitioning 
from one node to another
52
Random walks for summarization
 Represent the input text as graph
 Start traversing from node to node 
 following the transition probabilities 
 occasionally hopping to a new node
 What is the probability that you are in any 
particular node after doing this process for a 
certain time? 
 Standard solution (stationary distribution)
 This probability is the weight of the sentence
27
53
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
54
Supervised methods 
 For extractive summarization, the task can be 
represented as binary classification
 A sentence is in the summary or not
 Use statistical classifiers to determine the score of a 
sentence: how likely it?s included in the summary
 Feature representation for each sentence
 Classification models trained from annotated data
 Select the sentences with highest scores (greedy for 
now, see other selection methods later)
28
55
Features
 Sentence length
 long sentences tend to be more important
 Sentence weight
 cosine similarity with documents
 sum of term weights for all words in a sentence
 calculate term weight after applying LSA
56
Features
 Sentence position
 beginning is often more important
 some sections are more important (e.g., in 
conclusion section)
 Cue words/phrases 
 frequent n-grams
 cue phrases (e.g., in summary, as a conclusion)
 named entities
29
57
Features
 Contextual features
 features from context sentences
 difference of a sentence and its neighboring ones 
 Speech related features (more later):
 acoustic/prosodic features
 speaker information (who said the sentence, is the 
speaker dominant?)
 speech recognition confidence measure 
58
Classifiers
 Can classify each sentence individually, or 
use sequence modeling
 Maximum entropy [Osborne, 2002]
 Condition random fields (CRF) [Galley, 2006]
 Classic Bayesian Method [Kupiec et al, 1995]
 HMM [Conroy and O'Leary, 2001; Maskey, 2006 ]
 Bayesian networks 
 SVMs [Xie and Liu, 2010]
 Regression [Murray et al, 2005]
 Others
30
59
So that is it with supervised methods? 
 It seems it is a straightforward classification 
problem
 What are the issues with this method?
 How to get good quality labeled training data
 How to improve learning
 Some recent research has explored a few 
directions
 Discriminative training, regression, sampling, co-
training, active learning
60
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
31
61
Improving supervised methods: different 
training approaches
 What are the problems with standard training 
methods?
 Classifiers learn to determine a sentence?s label 
(in summary or not)   
 Sentence-level accuracy is different from 
summarization evaluation criterion (e.g., 
summary-level ROUGE scores)
 Training criterion is not optimal
 Sentences? labels used in training may be too 
strict (binary classes)
62
Improving supervised methods: MERT 
discriminative training
 Discriminative training based on MERT [Aker et 
al., 2010]
 In training, generate multiple summary candidates 
(using A* search algorithm)
 Adjust model parameters (feature weights) 
iteratively to optimize ROUGE scores
Note: MERT has been used for machine translation discriminative training
32
63
Improving supervised methods: ranking 
approaches 
 Ranking approaches [Lin et al 2010]
 Pair-wise training
 Not classify each sentence individually
 Input to learner is a pair of sentences
 Use Rank SVM to learn the order of two sentences
 Direct optimization
 Learns how to correctly order/rank summary candidates 
(a set of sentences)
 Use AdaRank [Xu and Li 2007] to combine weak rankers
64
Improving supervised methods: regression 
model
 Use regression model [Xie and Liu, 2010]
 In training, a sentence?s label is not +1 and -1
 Each one is labeled with numerical values to 
represent their importance
 Keep +1 for summary sentence
 For non-summary sentences (-1), use their similarity to 
the summary as labels
 Train a regression model to better discriminate 
sentence candidates
33
65
Improving supervised methods: sampling
 Problems -- in binary classification setup for 
summarization, the two classes are 
imbalanced
 Summary sentences are minority class. 
 Imbalanced data can hurt classifier training
 How can we address this?
 Sampling to make distribution more balanced to 
train classifiers
 Has been studied a lot in machine learning
66
Improving supervised methods: sampling
 Upsampling: increase minority samples
 Replicate existing minority samples
 Generate synthetic examples (e.g., by some kind 
of interpolation)
 Downsampling: reduce majority samples
 Often randomly select from existing majority 
samples
34
67
Improving supervised methods: sampling
 Sampling for summarization [Xie and Liu, 2010]
 Different from traditional upsampling and downsampling
 Upsampling
 select non-summary sentences that are like summary 
sentences based on cosine similarity or ROUGE scores
 change their label to positive 
 Downsampling: 
 select those that are different from summary sentences
 These also address some human annotation disagreement
 The instances whose labels are changed are often the ones 
that humans have problems with
68
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-raining
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
35
69
Supervised methods: data issues
 Need labeled data for model training
 How do we get good quality training data? 
 Can ask human annotators to select extractive 
summary sentences
 However, human agreement is generally low
 What if data is not labeled at all? or it only 
has abstractive summary?
70
 Distributions of content units and words are similar
 Few units are expressed by everyone; many units 
are expressed by only one person
Do humans agree on summary sentence 
selection? Human agreement on word/sentence/fact selection
36
71
Supervised methods: semi-supervised 
learning
 Question ? can we use unlabeled data to 
help supervised methods? 
 A lot of research has been done on semi-
supervised learning for various tasks
 Co-training and active learning have been 
used in summarization
72
Co-training
 Use co-training to leverage unlabeled data
 Feature sets represent different views
 They are conditionally independent given the 
class label
 Each is sufficient for learning
 Select instances based on one view, to help the 
other classifier
37
73
Co-training in summarization
 In text summarization [Wong et al, 2008]
 Two classifiers (SVM, na?ve Bayes) are used on 
the same feature set
 In speech summarization [Xie et al, 2010]
 Two different views: acoustic and lexical features
 They use both sentence and document as 
selection units
74
Active learning in summarization
 Select samples for humans to label
 Typically hard samples, machines are not 
confident, informative ones
 Active learning in lecture summarization [Zhang 
et al 2009]
 Criterion: similarity scores between the extracted 
summary sentences and the sentences in the 
lecture slides are high
38
75
Supervised methods: using labeled 
abstractive summaries
 Question -- what if I only have abstractive 
summaries, but not extractive summaries? 
 No labeled sentences to use for classifier 
training in extractive summarization 
 Can use reference abstract summary to 
automatically create labels for sentences
 Use similarity of a sentence to the human written 
abstract (or ROUGE scores, other metrics)
76
Comment on supervised performance
 Easier to incorporate more information
 At the cost of requiring a large set of human 
annotated training data
 Human agreement is low, therefore labeled 
training data is noisy
 Need matched training/test conditions
 may not easily generalize to different domains
 Effective features vary for different domains
 e.g., position is important for news articles
39
77
Comments on supervised performance
 Seems supervised methods are more 
successful in speech summarization than in 
text
 Speech summarization is almost never multi-
document
 There are fewer indications about the topic of the 
input in speech domains
 Text analysis techniques used in speech 
summarization are relatively simpler 
78
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
40
79
Parameters to optimize
 In summarization methods we try to find 
1. Most significant sentences
2. Remove redundant ones
3. Keep the summary under given length
 Can we combine all 3 steps in one?
 Optimize all 3 parameters at once
80
Summarization as an optimization problem
 Knapsack Optimization Problem 
Select boxes such that amount of money is 
maximized while keeping total weight under X Kg
 Summarization Problem 
Select sentences such that summary relevance is 
maximized while keeping total length under X words
 Many other similar optimization problems  
 General Idea: Maximize a function given a set of 
constraints
41
81
Optimization methods for summarization
 Different flavors of solutions
 Greedy Algorithm
 Choose highest valued boxes
 Choose the most relevant sentence 
 Dynamic Programming algorithm
 Save intermediate computations
 Look at both relevance and length
 Integer Linear Programming
 Exact Inference
 Scaling Issues
We will now discuss these 3 types of optimization solutions
82
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
42
83
Greedy optimization algorithms
 Greedy solution is an approximate algorithm which 
may not be optimal
 Choose the most relevant + least redundant 
sentence if the total length does not exceed the 
summary length
 Maximal Marginal Relevance is one such greedy algorithm 
proposed by [Carbonell et al, 1998]
84
Maximal Marginal Relevance (MMR) 
[Carbonell et al, 1998]
 Summary: relevant and non-redundant information
 Many summaries are built based on sentences ranked by 
relevance
 E.g. Extract most relevant 30% of sentences
Relevance Redundancyvs.
 Summary should maximize relevant information as 
well as reduce redundancy
43
85
Marginal relevance
 ?Marginal Relevance? or ?Relevant Novelty?
 Measure relevance and novelty separately
 Linearly combine these two measures
 High Marginal relevance if
 Sentence is relevant to story (significant information)
 Contains minimal similarity to previously selected sentences 
(new novel information)
 Maximize Marginal Relevance to get summary that 
has significant non-redundant information
86
Relevance with query or centroid
 We can compute relevance of text snippet 
with respect to query or centroid
 Centroid as defined in [Radev, 2004]
 based on the content words of  a document 
 TF*IDF vector of all documents in corpus
 Select words above a threshold : remaining vector 
is a centroid vector
44
87
Maximal Marginal Relevance (MMR) 
[Carbonell et al, 1998]
 Q ? document centroid/user query
 D ? document collection
 R ? ranked listed
 S ? subset of documents in R already selected
 Sim ? similarity metric 
 Lambda =1 produces most significant ranked list
 Lambda = 0 produces most diverse ranked list
MMR? Argmax(Di?R?S)[?(Sim1(Di, Q))?(1??)max(Dj?S)Sim2(Di, Dj)]
88
MMR based Summarization [Zechner, 2000]
Iteratively select next sentence
Next Sentence = 
Frequency Vector 
of all content words
centroid
45
89
MMR based summarization
 Why this iterative sentence selection process 
works?
 1st Term: Find relevant sentences similar to 
centroid of the document
 2nd Term: Find redundancy ? sentences that are 
similar to already selected sentences are not 
selected
90
 MMR is an iterative sentence selection 
process
 decision made for each sentence
 Is this selected sentence globally optimal?
Sentence selection in MMR
Sentence with same level of relevance but shorter may not be 
selected if a longer relevant sentence is already selected
46
91
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
92
Global inference
D=t1, t2, , tn?1, tn
 Modify our greedy algorithm 
 add constraints for sentence length as well
 Let us define document D with tn textual 
units
47
93
Global inference
 Let us define
Relevance of ti to be in the 
summary
Redundancy between ti and tj
Length of til(i)
Red(i,j)
Rel(i)
94
Inference problem [McDonald, 2007]
 Let us define inference problem as 
Summary Score
Pairwise RedundancyMaximum Length
48
95
Greedy solution [McDonald, 2007]
Sort by Relevance
Select Sentence
 Sorted list may have longer sentences at the top
 Solve it using dynamic programming
 Create table and fill it based on length and redundancy 
requirements
No consideration of
sentence length
96
Dynamic programming solution [McDonald, 2007]
High scoring summary
of length k and i-1
text unitsHigh scoring 
summary of
length k-l(i) +
ti
Higher ?
49
97
 Better than the previously shown greedy 
algorithm
 Maximizes the space utilization by not 
inserting longer sentences
 These are still approximate algorithms: 
performance loss?
Dynamic programming algorithm [McDonald, 2007]
98
Inference algorithms comparison
[McDonald, 2007]
System 50 100 200
Baseline 26.6/5.3 33.0/6.8 39.4/9.6
Greedy 26.8/5.1 33.5/6.9 40.1/9.5
Dynamic Program 27.9/5.9 34.8/7.3 41.2/10.0
Summarization results: Rouge-1/Rouge-2
Sentence Length
50
99
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
100
Integer Linear Programming (ILP) [Gillick 
and Favre, 2009; Gillick et al, 2009; McDonald, 2007]
 Greedy algorithm is an approximate solution
 Use exact solution algorithm with ILP (scaling issues 
though)
 ILP is constrained optimization problem
 Cost and constraints are linear in a set of integer variables
 Many solvers on the web
 Define the constraints based on relevance and 
redundancy for summarization
 Sentence based ILP
 N-gram based ILP
51
101
Sentence-level ILP formulation [McDonald, 
2007]
1 if ti in summary
Constraints
Optimization Function
102
N-gram ILP formulation [Gillick and Favre, 2009; 
Gillick et al, 2009]
 Sentence-ILP constraint on redundancy is 
based on sentence pairs
 Improve by modeling n-gram-level 
redundancy
 Redundancy implicitly defined
Ci indicates presence
of n-gram i in summary 
and its weight is wi
?
i wici
52
103
N-gram ILP formulation [Gillick and Favre, 2009]
Constraints
Optimization Function n-gram level ILP has different  optimization 
function than one shown before
104
Sentence vs. n-gram ILP
System ROUGE-2 Pyramid
Baseline 0.058 0.186
Sentence ILP
[McDonald, 2007] 0.072 0.295
N-gram ILP
[Gillick and Favre, 2009] 0.110 0.345
53
105
Other optimization based summarization 
algorithms
 Submodular selection [Lin et al, 2009]
 Submodular set functions for optimization
 Modified greedy algorithm [Filatova, 2004]
 Event based features
 Stack decoding algorithm [Yih et al, 2007]
 Multiple stacks, each stack represents hypothesis of different 
length
 A* Search [Aker et al, 2010]
 Use scoring and heuristic functions
106
Submodular selection for summarization 
[Lin et al, 2009]
 Summarization Setup
 V ? set of all sentences in document
 S ? set of extraction sentences
 f(.) scores the quality of the summary
 Submodularity been used in solving many 
optimization problems in near polynomial time
 For summarization: 
Select subset S (sentences) representative of V 
given the constraint |S| =< K (budget)
54
107
Submodular selection [Lin et al, 2009]
 If V are nodes in a Graph G=(V,E) representing 
sentences
 And E represents edges (i,j) such that w(i,j) 
represents similarity between sentences i and j
 Introduce submodular set functions which measures 
?representative? S of entire set V
 [Lin et al, 2009] presented 4 submodular set functions
108
Submodular selection for summarization 
[Lin et al, 2009]
Comparison of results using different methods
55
109
Review: optimization methods
 Global optimization methods have shown to be 
superior than 2-step selection process and reduce 
redundancy
 3 parameters are optimized together
 Relevance
 Redundancy
 Length
 Various Algorithms for Global Inference
 Greedy
 Dynamic Programming 
 Integer Linear Programming
 Submodular Selection
110
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
56
111
Speech summarization
 Increasing amount of data available in 
speech form
 meetings, lectures, broadcast, youtube, voicemail
 Browsing is not as easy as for text domains
 users need to listen to the entire audio
 Summarization can help effective information 
access
 Summary output can be in the format of text 
or speech
112
Domains 
 Broadcast news
 Lectures/presentations
 Multiparty meetings
 Telephone conversations
 Voicemails
57
113
Example
Meeting transcripts and summary sentences (in red)
so it?s possible that we could do something like a 
summary node of some sort that
me003
but there is some technology you could try to applyme010
yeahme010
now I don?t know that any of these actually apply in 
this case
me010
uh so if you co- you could ima- and i-me010
mmmme003
there?re ways to uh sort of back off on the purity of 
your bayes-net-edness
me010
andme010
uh i- i slipped a paper to bhaskara and about noisy-
or?s and noisy-maxes
me010
which is there are technical ways of doing itme010
uh let me just mention something that i don?t want 
to pursue today
me010
there there are a variety of ways of doing itme010
Broadcast news transcripts and summary (in red)
try to use electrical appliances before p.m. and after p.m. and 
turn off computers, copiers and lights when they're not being 
used
set your thermostat at 68 degrees when you're home, 55 
degrees when  you're away
energy officials are offering tips to conserve electricity, they say, 
to delay holiday lighting until after at night
the area shares power across many states
meanwhile, a cold snap in the pacific northwest is putting an 
added strain on power supplies
coupled with another unit, it can provide enough power for about
2 million people
it had been shut down for maintenance
a unit at diablo canyon nuclear plant is expected to resume 
production today
california's strained power grid is getting a boost today which 
might help increasingly taxed power supplies
114
Speech vs. text summarization: similarities
 When high quality transcripts are available
 Not much different from text summarization
 Many similar approaches have been used
 Some also incorporate acoustic information
 For genres like broadcast news, style is also 
similar to text domains
58
115
Speech vs. text summarization: differences
 Challenges in speech summarization
 Speech recognition errors can be very high
 Sentences are not as well formed as in most text 
domains: disfluencies, ungrammatical
 There are not clearly defined sentences
 Information density is also low (off-topic 
discussions, chit chat, etc.)
 Multiple participants
116
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
59
117
What should be extraction units in speech 
summarization?
 Text domain
 Typically use sentences (based on punctuation 
marks)
 Speech domain
 Sentence information is not available
 Sentences are not as clearly defined
Utterance from previous example:
there there are a variety of ways of doing it uh let me just mention something 
that i don?t want to pursue today which is there are technical ways of doing it
118
Automatic sentence segmentation (side note) 
 For a word boundary, determine whether it?s a sentence 
boundary
 Different approaches: 
 Generative: HMM
 Discriminative: SVM, boosting, maxent, CRF
 Information used: word n-gram, part-of-speech, parsing 
information, acoustic info (pause, pitch, energy)
60
119
What is the effect of different 
units/segmentation on summarization?
 Research has used different units in speech 
summarization
 Human annotated sentences or dialog acts
 Automatic sentence segmentation
 Pause-based segments
 Adjacency pairs
 Intonational phrases 
 Words
120
What is the effect of different 
units/segmentation on summarization?
 Findings from previous studies
 Using intonational phrases (IP) is better than 
automatic sentence segmentation, pause-based 
segmentation [Maskey, 2008 ]
 IPs are generally smaller than sentences, also 
linguistically meaningful
 Using sentences is better than words, between 
filler segments [Furui et al, 2004]
 Using human annotated dialog acts is better than 
automatically generated ones [Liu and Xie, 2008]
61
121
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
122
Using acoustic information in 
summarization
 Acoustic/prosodic features: 
 F0 (max, min, mean, median, range)
 Energy (max, min, mean, median, range)
 Sentence duration
 Speaking rate (# of words or letters)
 Need proper normalization
 Widely used in supervised methods, in 
combination with textual features
62
123
Using acoustic information in 
summarization
 Are acoustic features useful when combining 
it with lexical information?
 Results vary depending on the tasks and 
domains 
 Often lexical features are ranked higher
 But acoustic features also contribute to overall 
system performance
 Some studies showed little impact when adding 
speech information to textual features [Penn and Zhu, 
2008]
124
Using acoustic information in 
summarization
 Can we use acoustic information only for speech 
summarization?
 Transcripts may not be available
 Another way to investigate contribution of acoustic 
information
 Studies showed using just acoustic information can 
achieve similar performance to using lexical 
information [Maskey and Hirschberg, 2005; Xie et al, 2009; Zhu et al, 
2009]
 Caveat: in some experiments, lexical information is used 
(e.g., define the summarization units)
63
125
Speech recognition errors
 ASR is not perfect, often high word error rate
 10-20% for read speech
 40% or even higher for conversational speech
 Recognition errors generally have negative 
impact on summarization performance
 Important topic indicative words are incorrectly 
recognized
 Can affect term weighting and sentence scores
126
Speech recognition errors
 Some studies evaluated effect of recognition 
errors on summarization by varying word 
error rate [Christensen et al, 2003; Penn and Zhu, 2008; Lin et al, 
2009]
 Degradation is not much when word error 
rate is not too low (similar to spoken 
document retrieval)
 Reason: better recognition accuracy in summary 
sentences than overall  
64
127
What can we do about ASR errors? 
 Deliver summary using original speech 
 Can avoid showing recognition errors in the 
delivered text summary
 But still need to correctly identify summary 
sentences/segments
 Use recognition confidence measure and 
multiple candidates to help better summarize
128
Address problems due to ASR errors
 Re-define summarization task: select 
sentences that are most informative, at the 
same time have high recognition accuracy
 Important words tend to have high recognition 
accuracy
 Use ASR confidence measure or n-gram 
language model scores in summarization
 Unsupervised methods [Zechner, 2002; Kikuchi et al, 2003; 
Maskey, 2008]
 Use as a feature in supervised methods
65
129
Address problems due to ASR errors
 Use multiple recognition candidates
 n-best lists [Liu et al, 2010]
 Lattices [Lin et al, 2010]
 Confusion network [Xie and Liu, 2010]
 Use in MMR framework
 Summarization segment/unit contains all the word 
candidates (or pruned ones based on probabilities)
 Term weights (TF, IDF) use candidate?s posteriors
 Improved performance over using 1-best recognition 
output
130
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency
66
131
Disfluencies and summarization
 Disfluencies (filler words, repetitions, revisions, 
restart, etc) are frequent in conversational speech
 Example from meeting transcript:
so so does i- just remind me of what what you were going to do with the 
what what what what's
y- you just described what you've been doing
 Existence may hurt summarization systems, also 
affect human readability of the summaries
132
Disfluencies and summarization
 Natural thought: remove disfluenices 
 Word-based selection can avoid disfluent 
words 
 Using n-gram scores tends to select fluent 
parts [Hori and Furui, 2001]
 Remove disfluencies first, then perform 
summarization 
 Does it work? not consistent results 
 Small improvement [Maskey, 2008; Zechner, 2002]
 No improvement [Liu et al, 2007]
67
133
Disfluencies and summarization
 In supervised classification, information related to 
disfluencies can be used as features for 
summarization 
 Small improvement on Switchboard data [Zhu and Penn, 2006]
 Going beyond disfluency removal, can perform 
sentence compression in conversational speech to 
remove un-necessary words [Liu and Liu, 2010]
 Help improve sentence readability
 Output is more like abstractive summaries
 Compression helps summarization
134
Review on speech summarization
 Speech summarization has been performed 
for different domains
 A lot of text-based approaches have been 
adopted
 Some speech specific issues have been 
investigated
 Segmentation 
 ASR errors
 Disfluencies
 Use acoustic information
68
135
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
136
Manual evaluations
 Task-based evaluations 
 too expensive
 Bad decisions possible, hard to fix
 Assessors rate summaries on a scale
 Responsiveness
 Assessors compare with gold-standards
 Pyramid
69
137
Automatic and fully automatic 
evaluation
 Automatically compare with gold-standard
 Precision/recall (sentence level)
 ROUGE (word level)
 No human gold-standard is used
 Automatically compare input and summary
138
Precision and recall for extractive 
summaries
 Ask a person to select the most important 
sentences
Recall: system-human choice 
overlap/sentences chosen by human
Precision: system-human choice 
overlap/sentences chosen by system
70
139
Problems?
 Different people choose different sentences
 The same summary can obtain a recall score 
that is between 25% and 50% different 
depending on which of two available human 
extracts is used for evaluation
 Recall more important/informative than 
precision?
140
More problems?
 Granularity
We need help. Fires have spread in the nearby 
forest and threaten several villages in this remote 
area.
 Semantic equivalence
 Especially in multi-document summarization
 Two sentences convey almost the same 
information: only one will be chosen in the human 
summary
71
141
Pyramid
Responsiveness
ROUGE
Fully automatic
Model 
summaries
Manual comparison/ 
ratings
Evaluation methods for content
142
Pyramid method [Nenkova and Passonneau, 2004; Nenkova et al, 
2007]
 Based on Semantic Content Units (SCU)
 Emerge from the analysis of several texts
 Link different surface realizations with the 
same meaning
72
143
SCU example
S1 Pinochet arrested in London on Oct 16 at a 
Spanish judge?s request for atrocities against 
Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has 
been arrested in London at the request of the 
Spanish government.
S3 Britain caused international controversy and 
Chilean turmoil by arresting former Chilean 
dictator Pinochet in London.
144
SCU: label, weight, contributors 
Label London was where Pinochet was 
arrested
Weight=3
S1 Pinochet arrested in London on Oct 16 at a Spanish 
judge?s request for atrocities against Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has been
arrested in London at the request of the Spanish
government.
S3 Britain caused international controversy and Chilean 
turmoil by arresting former Chilean dictator Pinochet in 
London.
73
145
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
146
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
74
147
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
148
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
75
149
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
150
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
76
151
Different equally good summaries
 Pinochet arrested
 Arrest in London
 Pinochet is a former 
Chilean dictator
 Accused of atrocities 
against Spaniards
152
Different equally good summaries
 Pinochet arrested
 Arrest in London
 On Spanish warrant
 Chile protests
77
153
Diagnostic ? why is a summary bad?
 Good  Less relevant 
summary
154
Importance of content 
 Can observe distribution in human 
summaries
 Assign relative importance
 Empirical rather than subjective
 The more people agree, the more important
78
155
Pyramid score for evaluation
 New summary with n content units
 Estimates the percentage of information that is 
maximally important
IdealWight
ightObservedWe
Ideal
Weight
n
i
i
n
i
i
=
?
?
=
=
1
1
156
ROUGE [Lin, 2004]
 De facto standard for evaluation in text 
summarization
 High correlation with manual evaluations in that 
domain
 More problematic for some other domains, 
particularly speech
 Not highly correlated with manual evaluations
 May fail to distinguish human and machine 
summaries
79
157
ROUGE details
 In fact a suite of evaluation metrics
 Unigram
 Bigram
 Skip bigram
 Longest common subsequence
 Many settings concerning
 Stopwords
 Stemming
 Dealing with multiple models
158
How to evaluate without human 
involvement? [Louis and Nenkova, 2009]
 A good summary should be similar to the 
input
 Multiple ways to measure similarity
 Cosine similarity
 KL divergence
 JS divergence
 Not all work!
80
159
 Distance between two distributions as 
average KL divergence from their mean 
distribution
JS divergence between input and 
summary
)]||()||([)||( 21 ASummKLAInpKLSummInpJS +=
SummaryandInputofondistributimeanSummInpA ,
2
+
=
160
Summary likelihood given the input
 Probability that summary is generated according to 
term distribution in the input
Higher likelihood ~ better summary
 Unigram Model
 Multinomial Model
ii
n
rInp
n
Inp
n
Inp
wwordofsummaryincountn
vocabularysummaryr
wpwpwp r
=
?
)()()( 21 21 K
sizesummarynN
wpwpwp
i
i
n
rInp
n
Inp
n
Inpnn
N r
r
==?
)()()( 21
1 21!!
! KK
81
161
 Fraction of summary = input?s topic words
 % of input?s topic words also appearing in summary 
 Capture variety
 Cosine similarity: input?s topic words and all summary 
words
 Fewer dimensions, more specific vectors
Topic words identified by log-likelihood 
test
162
How good are these metrics? 
48 inputs, 57 systems
JSD -0.880 -0.736
0.795 0.627
-0.763 -0.694
0.712 0.647
0.712 0.602
-0.688 -0.585
-0.188 -0.101
0.222 0.235
% input?s topic in summary
KL div summ-input
Cosine similarity
% of summary = topic words
KL div input-summ
Unigram summ prob.
Multinomial summ prob.
-0.699 0.629Topic word similarity
Pyramid Responsiveness
Spearman correlation on macro level for the query focused task.
82
163
 JSD correlations with pyramid scores even better than 
R1-recall
 R2-recall is consistently better
 Can extend features using higher order n-grams 
How good are these metrics?
0.870.90R2-recall
0.800.85R1-recall
-0.73-0.88JSD
Resp.Pyramid
164
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
83
165
Current summarization research  
 Summarization for various new genres
 Scientific articles
 Biography
 Social media (blog, twitter)
 Other text and speech data 
 New task definition 
 Update summarization 
 Opinion summarization
 New summarization approaches 
 Incorporate more information (deep linguistic knowledge, information 
from the web)
 Adopt more complex machine learning techniques
 Evaluation issues
 Better automatic metrics
 Extrinsic evaluations
And more?
166
 Check out summarization papers at ACL this 
year
 Workshop at ACL-HLT 2011:
 Automatic summarization for different genres, 
media, and languages [June 23, 2011]
 http://www.summarization2011.org/
84
167
References
 Ahmet Aker, Trevor Cohn, Robert Gaizauska. 2010. Multi-document summarization using A* search and 
discriminative training. Proc. of EMNLP.
 R. Barzilay and M. Elhadad. 2009. Text summarizations with lexical chains. In: I. Mani and M. Maybury (eds.): 
Advances in Automatic Text Summarization.
 Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-Based reranking for Reordering 
Documents and Producing Summaries. Proceedings of the 21st Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval.
 H. Christensen, Y. Gotoh, B. Killuru, and S. Renals. 2003. Are Extractive Text Summarization Techniques 
Portable to Broadcast News? Proc. of ASRU.
 John Conroy and Dianne O'Leary. 2001. Text Summarization via Hidden Markov Models. Proc. of SIGIR.
 J. M. Conroy, J. D. Schlesinger, and D. P. OLeary. 2006. Topic-Focused Multi-Document Summarization Using an 
Approximate Oracle Score. Proc. COLING/ACL 2006. pp. 152-159.
 Thomas Cormen, Charles E. Leiserson, and Ronald L. Rivest.1990. Introduction to algorithms. MIT Press.
 G. Erkan and D. R. Radev.2004. LexRank: Graph-based Centrality as Salience in Text Summarization. Journal of 
Artificial Intelligence Research (JAIR).
 Pascale Fung, Grace Ngai, and Percy Cheung. 2003. Combining optimal clustering and hidden Markov models for 
extractive summarization. Proceedings of ACL Workshop on Multilingual Summarization.Sadoki Furui, T. Kikuchi, 
Y. Shinnaka, and C. Hori. 2004. Speech-to-text and Speech-to-speech Summarization of Spontaneous Speech. 
IEEE Transactions on Audio, Speech, and Language Processing. 12(4), pages 401-408.
 Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. 
Proc. of EMNLP.
 Dan Gillick, Benoit Favre. 2009. A scalable global model for summarization. Proceedings of the Workshop on 
Integer Linear Programming for Natural Language Processing.
 Dan Gillick, Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-Tur. 2009. A global optimization framework for 
meeting summarization. Proceedings of ICASSP.
 Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by 
sentence extraction. Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization.
168
References
 Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. 
Proc. ACM SIGIR.
 I. Gurevych and T. Nahnsen. 2005. Adapting Lexical Chaining to Summarize Conversational Dialogues. Proc. 
RANLP.
 B. Hachey, G. Murray, and D. Reitter.2006. Dimensionality reduction aids term co-occurrence based multi-
document summarization. In: SumQA 06: Proceedings of the Workshop on Task-Focused Summarization and 
Question Answering. 
 Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. Proc. of 
NAACL-HLT.
 L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Comparing presentation summaries: Slides vs. reading vs.
listening. Proc. of SIGCHI on Human factors in computing systems.
 C. Hori and Sadaoki Furui. 2001. Advances in Automatic Speech Summarization. Proc. of Eurospeech.
 T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic Speech Summarization based on Sentence Extractive and 
Compaction. Proc. of ICSLP.  
 Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. Proc. of SIGIR.
 J. Leskovec, N. Milic-frayling, and M. Grobelnik. 2005. Impact of Linguistic Analysis on the Semantic Graph 
Coverage and Learning of Document Extracts. Proc. AAAI.
 Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on Text 
Summarization Branches Out. 
 C.Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. Proc. COLING.
 Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. 
Proc. of NAACL.
 Hui Lin and Jeff Bilmes and Shasha Xie. 2009. Graph-based Submodular Selection for Extractive Summarization. 
Proceedings of ASRU.
 Shih-Hsiang Lin and Berlin Chen. 2009. Improved Speech Summarization with Multiple-hypothesis 
Representations and Kullback-Leibler Divergence Measures. Proc. of Interspeech.
 Shih-Hsiang Lin, Berlin Chen, and H. Min Wang. 2009. A Comparative Study of Probabilistic Ranking Models for 
Chinese Spoken Document Summarization. ACM Transactions on Asian Language Information Processing.
85
169
References
 Shih Hsiang Lin, Yu Mei Chang, Jia Wen Liu, Berlin Chen. 2010 Leveraging Evaluation Metric-related Training 
Criteria for Speech Summarization. Proc. of ICASSP.
 Fei Liu and Yang Liu. 2009. From Extractive to Abstractive Meeting Summaries: Can it be done by sentence 
compression? Proc. of ACL.
 Fei Liu and Yang Liu. 2010. Using Spoken Utterance Compression for Meeting Summarization: A pilot study. Proc. 
of IEEE SLT.
 Yang Liu and Shasha Xie. 2008. Impact of Automatic Sentence Segmentation on Meeting Summarization. Proc. of 
ICASSP.
 Yang Liu, Feifan Liu, Bin Li, and Shasha Xie. 2007. Do Disfluencies Affect Meeting Summarization: A pilot study 
on the impact of disfluencies. Poster at MLMI.
 Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best Recognition Output for Extractive Summarization and 
Keyword Extraction in Meeting Speech. Proc. of ICASSP.
 Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without 
human models. Proceedings of EMNLP
 H.P. Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development 2(2).
 Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim. 2002. 
SUMMAC: a text summarization evaluation. Natual Language Engineering. 8,1 (March 2002), 43-68.
 Manuel J. Mana-Lopez, Manuel De Buenaga, and Jose M. Gomez-Hidalgo. 2004. Multidocument summarization: 
An added value to clustering in interactive retrieval. ACM Trans. Inf. Systems.
 Sameer Maskey. 2008. Automatic Broadcast News Summarization. Ph.D thesis. Columbia University.
 Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, discourse and structural 
features for speech summarization. Proceedings of Interspeech.
 Sameer Maskey and Julia Hirschberg. 2006. Summarizing Speech Without Text Using Hidden Markov Models. 
Proc. of HLT-NAACL.
 Ryan McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. Lecture Notes in 
Computer Science. Advances in Information Retrieval. 
 Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg. 2005. Do 
summaries help?. Proc. of SIGIR.
 K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin.1999. Towards multidocument
summarization by reformulation: progress and prospects. Proc. AAAI 1999.
170
References
 R. Mihalcea and P. Tarau .2004. Textrank: Bringing order into texts. Proc. of EMNLP 2004. 
 G. Murray, S. Renals, J. Carletta, J. Moore. 2005. Evaluating Automatic Summaries of Meeting Recordings. Proc. 
of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation.
 G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour. 2009. Extrinsic Summarization 
Evaluation: A Decision Audit Task. ACM Transactions on Speech and Language Processing.
 A. Nenkova and R. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method. 
Proc. HLT-NAACL.
 A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document 
summarizer: exploring the factors that influence summarization. Proc. ACM SIGIR.
 A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid Method: Incorporating human content 
selection variation in summarization evaluation. ACM Trans. Speech Lang. Processing.
 Miles Osborne. 2002. Using maximum entropy for sentence extraction. Proc. of ACL Workshop on Automatic 
Summarization.
 Gerald Penn and Xiaodan Zhu. 2008. A critical Reassessement of Evaluation Baselines for Speech 
Summarization. Proc. of ACL-HLT.
 Dmitri G. Roussinov and Hsinchun Chen. 2001. Information navigation on the web by clustering and summarizing 
query results. Inf. Process. Manage. 37, 6 (October 2001), 789-816. 
 B. Schiffman, A. Nenkova, and K. McKeown. 2002. Experiments in Multidocument Summarization. Proc. HLT.
 A. Siddharthan, A. Nenkova, and K. Mckeown.2004. Syntactic Simplification for Improving Content Selection in 
Multi-Document Summarization. Proc. COLING.
 H. Grogory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate 
representation for automatic text summarization. Computational. Linguist. 28, 4 (December 2002), 487-496.
 J. Steinberger, M. Poesio, M. A. Kabadjov, and K. Jeek. 2007. Two uses of anaphora resolution in summarization. 
Inf. Process. Manage. 43(6).
 S. Tucker and S. Whittaker. 2008. Temporal compression of speech: an evaluation. IEEE Transactions on Audio, 
Speech and Language Processing, pages 790-796.
 L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova. 2007. Beyond SumBasic: Task-focused summarization 
with sentence simplification and lexical expansion. Information Processing and Management 43.
86
171
References
 Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive Summarization using Supervised and Semi-supervised 
learning. Proc. of ACL.
 Shasha Xie and Yang Liu. 2010. Improving Supervised Learning for Meeting Summarization using Sampling and 
Regression. Computer Speech and Language. V24, pages 495-514.
 Shasha Xie and Yang Liu. 2010. Using Confusion Networks for Speech Summarization. Proc. of NAACL.
 Shasha Xie, Dilek Hakkani-Tur, Benoit Favre, and Yang Liu. 2009. Integrating Prosodic Features in Extractive 
Meeting Summarization. Proc. of ASRU.
 Shasha Xie, Hui Lin, and Yang Liu. 2010. Semi-supervised Extractive Speech Summarization via Co-training 
Algorithm. Proc. of Interspeech.
 S. Ye, T.-S. Chua, M.-Y. Kan, and L. Qiu. 2007. Document concept lattice for text understanding and 
summarization. Information Processing and Management 43(6).
 W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-Document Summarization by Maximizing 
Informative Content-Words. Proc. IJCAI 2007.
 Klaus Zechner. 2002. Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres. 
Computational Linguistics. V28, pages 447-485.
 Klaus Zechner and Alex Waibel. 2000. Minimizing word error rate in textual summaries of spoken language. 
Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference.
 Justin Zhang and Pascale Fung. 2009. Extractive Speech Summarization by Active Learning. Proc. of ASRU.
 Xiaodan Zhu and Gerald Penn. 2006. Comparing the Roles of Textual, Acoustic and Spoken-language Features 
on Spontaneous Conversation Summarization. Proc. of HLT-NAACL.
 Xiaodan Zhu, Gerald Penn, and F. Rudzicz. 2009. Summarizing Multiple Spoken Documents: Finding Evidence 
from Untranscribed Audio. Proc. of ACL.
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 131?136,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Decade of Automatic Content Evaluation of News Summaries:
Reassessing the State of the Art
Peter A. Rankel
University of Maryland
rankel@math.umd.edu
John M. Conroy
IDA / Center for Computing Sciences
conroy@super.org
Hoa Trang Dang
National Institute of Standards and Technology
hoa.dang@nist.gov
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
How good are automatic content metrics
for news summary evaluation? Here we
provide a detailed answer to this question,
with a particular focus on assessing the
ability of automatic evaluations to identify
statistically significant differences present
in manual evaluation of content. Using
four years of data from the Text Analysis
Conference, we analyze the performance
of eight ROUGE variants in terms of ac-
curacy, precision and recall in finding sig-
nificantly different systems. Our exper-
iments show that some of the neglected
variants of ROUGE, based on higher or-
der n-grams and syntactic dependencies,
are most accurate across the years; the
commonly used ROUGE-1 scores find
too many significant differences between
systems which manual evaluation would
deem comparable. We also test combina-
tions of ROUGE variants and find that they
considerably improve the accuracy of au-
tomatic prediction.
1 Introduction
ROUGE (Lin, 2004) is a suite of automatic eval-
uations for summarization and was introduced a
decade ago as a reasonable substitute for costly
and slow human evaluation. The scores it pro-
duces are based on n-gram or syntactic overlap be-
tween an automatic summary and a set of human
reference summaries. However, the field does not
have a good grasp of which of the many evalua-
tion scores is most accurate in replicating human
judgements. This state of uncertainty has led to
problems in comparing published work, as differ-
ent researchers choose to publish different variants
of scores.
In this paper we reassess the strengths of
ROUGE variants using the data from four years
of Text Analysis Conference (TAC) evaluations,
2008 to 2011. To assess the performance of the au-
tomatic evaluations, we focus on determining sta-
tistical significance1 between systems, where the
gold-standard comes from comparing the systems
using manual pyramid and responsiveness evalu-
ations. In this setting, computing correlation co-
efficients between manual and automatic scores is
not applicable as it does not take into account the
statistical significance of the differences nor does
it allow the use of more powerful statistical tests
which use pairwise comparisons of performance
on individual document sets. Instead, we report
on the accuracy of decisions on pairs of systems,
as well as the precision and recall of identifying
pairs of systems which exhibit statistically signifi-
cant differences in content selection performance.
2 Background
During 2008?2011, automatic summarization sys-
tems at TAC were required to create 100-word
summaries. Each year there were two multi-
document summarization sub-tasks, the initial
summary and the update summary, usually re-
ferred to as task A and task B, respectively. The
test inputs in each consisted of about 10 docu-
ments and the type of summary varied between
query-focused and guided. There are between 44
and 48 test inputs on which systems are compared
for each task.
In 2008 and 2009, task A was to produce a
1For the purpose of this study, we define a difference as
significant when the test statistic attains a value correspond-
ing to a p-value less than 0.05.131
query-focused summary in response to a user in-
formation need stated both as a brief statement
and a paragraph-long description of the informa-
tion the user seeks to find. In 2010 and 2011 task
A was ?guided summarization?, where the test in-
puts came from a small set of predefined domains.
These domains included accidents and natural dis-
asters, attacks, health and safety, endangered re-
sources, investigations and trials. Systems were
provided with a list of important aspects of infor-
mation for each domain and were asked to cover as
many of these aspects as possible. The writers of
the reference summaries for evaluation were given
similar instructions. In all four years, task B was
to produce an update summary for each of the in-
puts given in task A (query-focused or guided). In
each case, a new, subsequent set of documents re-
lated to the topic of the respective test set for task
A was provided to the system. The task was to
generate an update summary aimed at a user who
has already read all documents in the inputs for
task A.
The two manual evaluation approaches used in
TAC 2008?2011 are modified pyramid (Nenkova
et al, 2007) and overall responsiveness. The pyra-
mid method requires several reference summaries
for each input. These are manually analyzed to
discover content units based on meaning rather
than specific wording. Each content unit is as-
signed a weight equal to the number of reference
summaries that included that content unit. The
modified pyramid score is defined as the sum of
weights of the content units in the summary nor-
malized by the weight of an ideally informative
summary which expresses n content units, where
n is equal to the average of content units in the ref-
erence summaries. Responsiveness, on the other
hand, is based on direct human judgements, with-
out the need for reference summaries. Assessors
are presented with a statement of the user?s infor-
mation need and the summary they need to evalu-
ate. Then they rate how well they think the sum-
mary responds to the information need contained
in the topic statement. Responsiveness was rated
on a ten-point scale in 2009, and on a five-point
scale in all other years.
For each sub-task during 2008?2011, we ana-
lyze the performance of only the top 30 systems,
which roughly corresponds to the systems that per-
formed better than or around the median according
to each manual metric. Table 1 gives the number
of significant differences among the top 30 partici-
pating systems. We keep only the best performing
systems for the analysis because we are interested
in studying how well automatic evaluation metrics
can correctly compare very good systems.
Year Pyr A Pyr B Resp A Resp B
2008 82 109 68 105
2009 146 190 106 92
2010 165 139 150 128
2011 39 83 5 11
Table 1: Number of pairs of significantly different
systems among the top 30 across the years. There
is a total of 435 pairs in each year.
3 Which ROUGE is best?
In this section, we study the performance of
several ROUGE variants, including ROUGE-n,
for n = 1, 2, 3, 4, ROUGE-L, ROUGE-W-1.2,
ROUGE-SU4, and ROUGE-BE-HM (Hovy et al,
2006). ROUGE-n measures the n-gram recall of
the evaluated summary compared to the available
reference summaries. ROUGE-L is the ratio of
the number of words in the longest common sub-
sequence between the reference and the evaluated
summary and the number of words in the refer-
ence. ROUGE-W-1.2 is a weighted version of
ROUGE-L. ROUGE-SU4 is a combination of skip
bigrams and unigrams, where the skip bigrams are
formed for all words that appear in the text with
no more than four intervening words in between.
ROUGE-BE-HM computes recall of dependency
syntactic relations between the summary and the
reference.
To evaluate how well an automatic evalua-
tion metric reproduces human judgments, we use
prediction accuracy similar to Owczarzak et al
(2012). For each pair of systems in each subtask,
we compare the results of two Wilcoxon signed-
rank tests, one using the manual evaluation scores
for each system and one using the automatic evalu-
ation scores for each system (Rankel et al, 2011).2
The accuracy then is simply the percent agreement
between the results of these two tests.
2We use the Wilcoxon test as it was demonstrated by
Rankel et al (2011) to give more statistical power than un-
paired tests. As reported by Yeh (2000), other tests such as
randomized testing, may also be appropriate. There is con-
siderable variation in system performance for different inputs
(Nenkova and Louis, 2008) and paired tests remove the effect
of the input.132
Responsiveness Pyramid
Metric Acc P R BA Acc P R BA
R1 0.58 (0.61) 0.24 0.64 0.57 0.62 (0.66) 0.37 0.67 0.61
R2 0.64 (0.63) 0.28 0.60 0.59 0.68 (0.69) 0.43 0.63 0.64
R3 0.70 (0.63) 0.31 0.48 0.60 0.73 (0.68) 0.49 0.53 0.66
R4 0.73 (0.64) 0.33 0.40 0.60 0.74 (0.65) 0.50 0.45 0.65
RL 0.50 (0.59) 0.20 0.56 0.54 0.54 (0.63) 0.29 0.60 0.55
R-SU4 0.61(0.62) 0.26 0.61 0.58 0.65 (0.68) 0.40 0.65 0.63
R-W-1.2 0.52(0.62) 0.21 0.54 0.55 0.57(0.64) 0.32 0.62 0.57
R-BE-HM 0.70 (0.63) 0.30 0.49 0.59 0.74(0.68) 0.49 0.56 0.66
Table 2: Accuracy, Precision, Recall, and Balanced Accuracy of each ROUGE variant, averaged across
all eight tasks in 2008-2011, with and (without) significance.
As can be seen in Table 1, the manual evalua-
tion metrics often did not show many significant
differences between systems.3 Thus, it is clear
that the percent agreement will be high for an ap-
proach for automatic evaluation that always pre-
dicts zero significant differences. As traditionally
done when dealing which such skewed distribu-
tions of classes, we also examine the precision
and recall with respect to finding significant dif-
ferences of several ROUGE variants, to better as-
sess the quality of their prediction. To identify a
measure that is strong at both predicting signifi-
cant and non-significant differences we compute
balanced accuracy, the mean of the accuracy of
predicting significant differences and the accuracy
of predicting no significant difference.4
Each of these four measures for judging the per-
formance of ROUGE variants has direct intuitive
interpretation, unlike other opaque measures such
as correlation coefficients and F-measure which
have formal definitions which do not readily yield
to intuitive understanding.
3This is a somewhat surprising finding which may warrant
further investigation. One possible explanation is that differ-
ent systems generate similar summaries. Recent work has
shown that this is unlikely to be the case because the collec-
tion of summaries from several systems indicates better what
content is important than the single best summary (Louis and
Nenkova, 2013). The short summary length for which the
summarizers are compared may also contribute to the fact
that there are few significant difference. In early NIST eval-
uations manual evaluations could not distinguish automatic
and human summaries based on summaries of length 50 and
100 words and there were more significant differences be-
tween systems for 200-word summaries than for 100-word
summaries (Nenkova, 2005).
4More generally, one could define a utility function which
gives costs associated with errors and benefits to correct pre-
diction. Balanced accuracy weighs all errors as equally bad
and all correct prediction as equally good (von Neumann and
Morgenstern, 1953).
Few prior studies have taken statistical signifi-
cance into account during the assessment of auto-
matic metrics for evaluation. For this reason we
first briefly discuss ROUGE accuracy without tak-
ing significance into account. In this special case,
agreement simply means that the automatic and
manual evaluations agree on which of two systems
is better, based on each system?s average score for
all test inputs for a given task. It is very rare that
the average scores of two systems are equal, so
there is always a better system in each pair, and
random prediction would have 50% accuracy.
Many papers do not report the significance of
differences in ROUGE scores (for the ROUGE
variant of their choice), but simply claim that their
system X with higher average ROUGE score than
system Y is better than system Y . Table 2 lists
the average accuracy with significance taken into
account and then in parentheses, accuracy without
taking significance into account. The data demon-
strate that the best accuracy of the eight ROUGE
metrics is a meager 64% for responsiveness when
significance is not taken into account. So the con-
clusion about the relative merit of systems would
be different from that based on manual evaluation
in one out of three comparisons. However, the
best accuracy rises to 73% when significance is
taken into account; an incorrect conclusion will be
drawn in one out of four comparisons. The reduc-
tion in error is considerable.
Furthermore, ROUGE-3 and ROUGE-4, which
are rarely reported, are among the most accurate.
Note also, these results differ considerably from
those reported by Owczarzak et al (2012), where
ROUGE-2 was shown to have accuracy of 81% for
responsiveness and 89% for pyramid. The wide
differences are due to the fact we are only consid-133
ering systems which scored in the top 30. This il-
lustrates that our automatic metrics are not as good
at discriminating systems near the top. These find-
ings give strong support for the idea of requiring
authors to report the significance of the difference
between their summarization system and the cho-
sen baseline; the conclusions about relative merits
of the system would be more similar to those one
would draw from manual evaluation.
In addition to accuracy, Table 2 gives precision,
recall and balanced accuracy for each of the eight
ROUGE measures when significance is taken into
account. ROUGE-1 is arguably the most widely
used score in the literature and Table 2 reveals an
interesting property: ROUGE-1 has high recall but
low precision. This means that it reports many sig-
nificant differences, most of which do not exist ac-
cording to the manual evaluations.
Balanced accuracy helps us identify which
ROUGE variants are most accurate in finding
statistical significance and correctly predicting
that two systems are not significantly different.
For the pyramid evaluation, the variants with
best balanced accuracy (66%) are ROUGE-3 and
ROUGE-BE, with ROUGE-4 just a percent lower
at 65%. For responsiveness the configuration is
similar, with ROUGE-3 and ROUGE-4 tied for
best (60%), and ROUGE-BE just a percent lower.
The good performance of higher-order n-grams
is quite surprising because these are practically
never used for reporting results in the literature.
Based on our results however, they are much more
likely to accurately reproduce conclusions that
would have been drawn from manual evaluation
of top-performing systems.
4 Multiple hypothesis tests to combine
ROUGE variants
We now consider a method to combine multiple
evaluation scores in order to obtain a stronger en-
semble metric. The idea of combining ROUGE
variants has been explored in the prior litera-
ture. Conroy and Dang (2008), for example, pro-
posed taking linear combinations of ROUGE met-
rics. This approach was extended by Rankel et al
(2012) by including measures of linguistic quality.
Recently, Amigo? et al (2012) applied the ?hetero-
geneity principle? and combined ROUGE scores
to improve the precision relative to a human evalu-
ation metric. Their results demonstrate that a con-
sensus among ROUGE scores can predict more ac-
curately if an improvement in a human evaluation
metric will be achieved.
Along the lines of these investigations, we ex-
amine the performance of a simple combination
of variants: Call the difference between two sys-
tems significant only when all the variants in the
combination indicate significance. As in the sec-
tion above, a paired Wilcoxon signed-rank test is
used to determine the level of significance.
ROUGE Combination Acc Prec Rec BA
R1 R2 R4 RBE 0.76 0.77 0.36 0.76
R1 R4 RBE 0.76 0.76 0.36 0.76
R2 R4 RBE 0.76 0.74 0.40 0.75
R4 RBE 0.76 0.73 0.41 0.75
R1 R2 R4 0.76 0.71 0.40 0.74
R1 R4 0.75 0.70 0.40 0.73
R2 R4 0.75 0.68 0.44 0.73
R1 R2 RBE 0.75 0.66 0.48 0.72
R2 RBE 0.75 0.64 0.52 0.72
R4 0.74 0.62 0.47 0.70
R1 RBE 0.74 0.62 0.49 0.70
R1 R2 0.73 0.57 0.62 0.70
RBE 0.73 0.57 0.58 0.68
R2 0.71 0.53 0.69 0.68
R1 0.62 0.43 0.69 0.63
Table 3: Accuracy, Precision, Recall, and Bal-
anced Accuracy of each ROUGE combination on
TAC 2008-2010 pyramid.
We considered all possible combinations of four
ROUGE metrics that exhibited good properties
in the analyses presented so far: ROUGE-1 (be-
cause of its high recall), ROUGE-2 (because of
high accuracy when significance is not taken into
account) and ROUGE-4 and ROUGE-BE, which
showed good balanced accuracy.
The performance of these combinations for re-
producing the decisions in TAC 2008-2010 based
on the pyramid5 evaluation are given in Table 3.
The best balanced accuracy (76%) is for the com-
bination of all four variants. As more variants are
combined, precision increases but recalls drops.
5 Comparison with automatic
evaluations from AESOP 2011
In 2009-2011, TAC ran the task of Automatically
Evaluating Summaries of Peers (AESOP), to com-
5The ordering of the metric combinations relative to re-
sponsiveness was almost identical to the ordering relative to
the pyramid evaluation, and precision and recall exhibited the
same trend as more metrics were added to the combination.134
Pyramid A Pyramid B Responsiveness A Responsiveness B
Evaluation Metric Acc P R BA Acc P R BA Acc P R BA Acc P R BA
CLASSY1 0.60 0.02 0.60 0.50 0.84 0.03 0.18 0.50 0.61 0.14 0.64 0.54 0.70 0.21 0.22 0.52
DemokritosGR1 0.59 0.01 0.20 0.50 0.79 0.07 0.55 0.53 0.66 0.18 0.79 0.58 0.64 0.17 0.24 0.49
uOttawa3 0.44 0.01 0.60 0.50 0.48 0.02 0.36 0.50 0.52 0.13 0.77 0.55 0.43 0.13 0.36 0.46
DemokritosGR2 0.78 0.01 0.20 0.50 0.76 0.06 0.55 0.52 0.76 0.23 0.69 0.60 0.67 0.22 0.29 0.52
C-S-IIITH4 0.69 0.01 0.20 0.50 0.77 0.07 0.64 0.53 0.82 0.29 0.74 0.63 0.60 0.15 0.24 0.47
C-S-IIITH1 0.60 0.01 0.40 0.50 0.70 0.06 0.82 0.53 0.69 0.20 0.79 0.59 0.60 0.22 0.42 0.52
BEwT-E 0.73 0.01 0.20 0.50 0.80 0.01 0.09 0.49 0.79 0.25 0.72 0.61 0.72 0.31 0.39 0.58
R1-R2-R4-RBE 0.89 0.40 0.44 0.67 0.76 0.27 0.17 0.55 0.88 0.00 0.00 0.49 0.91 0.03 0.09 0.50
R1-R4-RBE 0.89 0.40 0.44 0.67 0.77 0.35 0.24 0.59 0.88 0.00 0.00 0.49 0.90 0.03 0.09 0.50
All ROUGEs 0.89 0.40 0.44 0.67 0.75 0.26 0.16 0.54 0.88 0.00 0.00 0.49 0.91 0.04 0.09 0.51
Table 4: Best performing AESOP systems from TAC 2011; Scores within the 95% confidence interval
of the best are in bold face.
pare automatic evaluation methods for automatic
summarization. Here we show how the submit-
ted AESOP metrics compare to the best ROUGE
variants that we have established so far. We report
the results on 2011 only, because even when the
same team participated in more than one year, the
metrics submitted were different and the 2011 re-
sults represent the best effort of these teams. How-
ever, as we saw in Table 1, in 2011 there were very
few significant differences between the top sum-
marization systems. In this sense the tasks that
year represent a challenging dataset for testing au-
tomatic evaluations.
The results for the best AESOP systems (ac-
cording to one or more measures), and the cor-
responding results for the ROUGE combinations
are shown in Table 4. These AESOP systems are:
CLASSY1 (Conroy et al, 2011; Rankel et al,
2012), DemokritosGR1 and 2 (Giannakopoulos et
al., 2008; Giannakopoulos et al, 2010), uOttawa3
(Kennedy et al, 2011), C-S-IITH1 and 4 (Kumar
et al, 2011; Kumar et al, 2012), and BEwT-E
(Tratz and Hovy, 2008).6 The combination metrics
achieve the highest accuracy by generally predict-
ing correctly when there are no significant differ-
ences between the systems. In addition, for 2008-
2010, where far more differences between systems
occur, the results of Table 3 show the combina-
tion metrics outperformed use of a single metric
and are competitive with the best metrics of AE-
SOP 2011. Thus, the combination metrics have
the ability to discriminate under both conditions
giving good prediction of human evaluation.
6To perform the comparison in the table the scores for
each system and document set were needed. Some systems
have changed after TAC 2011, but the data needed for these
comparisons were not available. BEwT-E did not participate
in AESOP 2011 and these data were provided by Stephen
Tratz. Special thanks to Stephen for providing these data.
6 Conclusion
We have tested the best-known automatic evalu-
ation metrics (ROUGE) on several years of TAC
data and compared their performance with re-
cently developed AESOP metrics. We discovered
that some of the rarely used variants of ROUGE
perform surprisingly well, and that by combin-
ing different ROUGEs together, one can create
an evaluation metric that is extremely competi-
tive with metrics submitted to the latest AESOP
task. Our results were reported in terms of sev-
eral different measures, and in each case, com-
pared how well the automatic metric predicted sig-
nificant differences found in manual evaluation.
We believe strongly that developers should include
statistical significance when reporting differences
in ROUGE scores of theirs and other systems,
as this improves the accuracy and credibility of
their results. Significant improvement in multi-
ple ROUGE scores is a significantly stronger in-
dicator that the developers have made a notewor-
thy improvement in text summarization. Systems
that report significant improvement using a com-
bination of ROUGE-BE (or its improved version
BEwT-E) in conjunction with ROUGE-1, 2, and
4, are more likely to give rise to summaries that
humans would judge as significantly better.
Acknowledgments
The authors would like to thank Ed Hovy who
raised the question ?How well do automatic met-
rics perform when comparing top systems?? Ed?s
comments helped motivate this work. In addition,
we would like to thank our anonymous referees for
their insightful comments, which contributed sig-
nificantly to this paper.
135
References
Enrique Amigo?, Julio Gonzalo, and Felisa Verdejo.
2012. The heterogeneity principle in evaluation
measures for automatic summarization. In Pro-
ceedings of Workshop on Evaluation Metrics and
System Comparison for Automatic Summarization,
pages 36?43, Montre?al, Canada, June. Association
for Computational Linguistics.
John M. Conroy and Hoa Trang Dang. 2008. Mind
the gap: Dangers of divorcing evaluations of sum-
mary content from linguistic quality. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 145?152,
Manchester, UK, August. Coling 2008 Organizing
Committee.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2011. Nouveau-ROUGE: A Novelty Met-
ric for Update Summarization. Computational Lin-
guistics, 37(1):1?8.
George Giannakopoulos, Vangelis Karkaletsis,
George A. Vouros, and Panagiotis Stamatopoulos.
2008. Summarization system evaluation revisited:
N-gram graphs. TSLP, 5(3).
George Giannakopoulos, George A. Vouros, and Van-
gelis Karkaletsis. 2010. Mudos-ng: Multi-
document summaries using n-gram graphs (tech re-
port). CoRR, abs/1012.2042.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-
nichi Fukumoto. 2006. Automated summarization
evaluation with basic elements. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC?06), pages 899?902.
Alistair Kennedy, Anna Kazantseva Saif Mohammad,
Terry Copeck, Diana Inkpen, and Stan Szpakowicz.
2011. Getting emotional about news. In Fourth Text
Analysis Conference (TAC 2011).
Niraj Kumar, Kannan Srinathan, and Vasudeva Varma.
2011. Using unsupervised system with least linguis-
tic features for tac-aesop task. In Fourth Text Analy-
sis Conference (TAC 2011).
N. Kumar, K. Srinathan, and V. Varma. 2012. Us-
ing graph based mapping of co-occurring words and
closeness centrality score for summarization evalua-
tion. Computational Linguistics and Intelligent Text
Processing, pages 353?365.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39:267?300.
Ani Nenkova and Annie Louis. 2008. Can you sum-
marize this? identifying correlates of input difficulty
for multi-document summarization. In ACL, pages
825?833.
Ani Nenkova, Rebecca J. Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorpo-
rating human content selection variation in summa-
rization evaluation. TSLP, 4(2).
Ani Nenkova. 2005. Discourse factors in multi-
document summarization. In AAAI, pages 1654?
1655.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization, pages 1?9, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine sum-
marization systems. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 467?473, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better metrics to automatically
predict the quality of a text summary. Algorithms,
5(4):398?420.
Stephen Tratz and Eduard Hovy. 2008. Summarisa-
tion evaluation using transformed basic elements. In
Proceedings TAC 2008. NIST.
John von Neumann and Oskar Morgenstern. 1953.
Theory of games and economic behavior. Princeton
Univ. Press, Princeton, NJ, 3. ed. edition.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics - Volume 2, COLING ?00, pages 947?
953, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
136
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 283?288,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Assessing the Discourse Factors that Influence the Quality of Machine
Translation
Junyi Jessy Li
University of Pennsylvania
ljunyi@seas.upenn.edu
Marine Carpuat
National Research Council Canada
marine.carpuat@nrc.gc.ca
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
We present a study of aspects of discourse
structure ? specifically discourse devices
used to organize information in a sen-
tence ? that significantly impact the qual-
ity of machine translation. Our analysis
is based on manual evaluations of trans-
lations of news from Chinese and Ara-
bic to English. We find that there is a
particularly strong mismatch in the no-
tion of what constitutes a sentence in Chi-
nese and English, which occurs often and
is associated with significant degradation
in translation quality. Also related to
lower translation quality is the need to em-
ploy multiple explicit discourse connec-
tives (because, but, etc.), as well as the
presence of ambiguous discourse connec-
tives in the English translation. Further-
more, the mismatches between discourse
expressions across languages significantly
impact translation quality.
1 Introduction
In this study we examine how the use of dis-
course devices to organize information in a sen-
tence ? and the mismatch in their usage across
languages ? influence machine translation (MT)
quality. The goal is to identify discourse process-
ing tasks with high potential for improving trans-
lation systems.
Historically MT researchers have focused their
attention on the mismatch of linear realization of
syntactic arguments (Galley et al, 2004; Collins
et al, 2005), lexico-morphological mismatch
(Minkov et al, 2007; Habash and Sadat, 2006)
and word polysemy (Carpuat and Wu, 2007; Chan
et al, 2007). Discourse structure has largely been
considered irrelevant to MT, mostly due to the as-
sumption that discourse analysis is needed to inter-
pret multi-sentential text while statistical MT sys-
tems are trained to translate a single sentence in
one language into a single sentence in another.
However, discourse devices are at play in the or-
ganization of information into complex sentences.
The mere definition of sentence may differ across
languages. Chinese for example is anecdotally
known to allow for very long sentences which at
times require the use of multiple English sentences
to express the same content and preserve gram-
maticality. Similarly discourse connectives like
because, but, since and while often relate informa-
tion expressed in simple sentential clauses. There
are a number of possible complications in trans-
lating these connectives: they may be ambiguous
between possible senses, e.g., English while is am-
biguous between COMPARISON and TEMPORAL;
explicit discourse connectives may be translated
into implicit discourse relations or translated in
morphology rather than lexical items (Meyer and
Webber, 2013; Meyer and Pol?akov?a, 2013).
In our work, we quantify the relationship be-
tween information packaging, discourse devices,
and translation quality.
2 Data and experiment settings
We examine the quality of translations to English
from Chinese and Arabic using Human-targeted
Translation Edit Rates (HTER) (Snover et al,
2006), which roughly captures the minimal num-
ber of edits necessary to transform the system
output into an acceptable English translation of
the source sentence. By comparing MT output
with post-edited references, HTER provides more
reliable estimates of translation quality than us-
ing translated references, especially at the seg-
ment level. The data for the analysis is drawn
from an extended set of newswire reports in the
2008/2010 NIST Metrics for Machine Translation
283
GALE Evaluation set
1
. For Chinese, there are
305 sentences (segments) translated to English by
three different translation systems. For Arabic,
there are 363 Arabic sentences (segments) trans-
lated by two systems.
The presence of discourse devices is analyzed
only on the English side: the reference, the system
hypothesis and its edited translation. Discourse
connectives and their senses are identified using
existing tools developed for English. Beyond its
practical limitations, analyzing the reference inter-
estingly reflects the choices made by the human
translator: whether to choose to use a discourse
connective, or to insert one to make an implicit re-
lation on the source side explicit on the target side.
We first conduct analysis of variance (ANOVA)
with HTER as dependent variable and the dis-
course factors as independent variables, and sys-
tems as subjects. We examine within-subject sig-
nificance in each ANOVA model. For discourse
factors that are significant at the 95% confidence
level or higher according to the ANOVA analy-
sis, we provide detailed breakdown of the system
HTER for each value of the discourse factor.
In this paper we do not compare the perfor-
mance of individual systems, but instead seek to
understand if a discourse phenomena is problem-
atic across systems.
2
3 Sentence length and HTER
The presence of complex discourse structure is
likely to be associated with longer sentences. It
stands to reason that long sentences will be harder
to process automatically and this reasoning has
motivated the first approaches to text simplifica-
tion (Chandrasekar et al, 1996). So before turning
to the analysis of discourse phenomena, we exam-
ine the correlation between translation quality and
sentence length. A strong correlation between the
two would call for revival of interest in text sim-
plification where syntactically complex sentences
are transformed into several shorter sentences as a
preprocessing step.
We find however that no strong relationship ex-
ists between the two, as shown by the correlation
coefficients between HTER values and the number
of words in each segment in Table 1.
1
Data used in this work includes more documents and the
human edits not present in the official release.
2
For the readers with keen interest in system comparison,
we note that according to ANOVA none of the differences in
system performance on this data is statistically significant.
Lan. Sys1 Sys2 Sys3
ZH 0.097 (0.099) 0.117 (0.152) 0.144 (0.173)
AR 0.071(0.148) -0.089 (-0.029) -
Table 1: Pearson (Spearman) correlation coeffi-
cient between segment length and HTER values.
Next we examine if sentence?discourse diver-
gence between languages and the presence of (am-
biguous) discourse connectives would be more in-
dicative of the expected translation quality.
4 When a sentence becomes discourse
Some languages allow more information to be
packed into a single sentence than is possible in
another language, making single-sentence transla-
tions cumbersome and often ungrammatical. Chi-
nese is known for sentences of this kind; for exam-
ple, the usage of punctuation is very different in
Chinese in the sense that a comma can sometimes
function as a full stop in English, motivating a se-
ries of disambiguation tasks (Jin et al, 2004; Xue
and Yang, 2011; Xu and Li, 2013). Special han-
dling of long Chinese sentences were also shown
to improve machine translation (Jin and Liu, 2010;
Yin et al, 2007).
To investigate the prevalence of sentences in the
source language (Chinese and Arabic in our case)
that do not confirm to the notion of sentence in the
target language (English for the purposes of this
study), we separate the translation segments in the
source language into two classes: a source sen-
tence is considered 1-1 if the reference translation
consists of exactly one sentence, and 1-many if the
reference contains more than one sentence.
For Chinese, 26.2% of the source segments are
1-many. These sentences tend to be much longer
than average (36.6% of all words in all reference
translations are part of such segments). For Ara-
bic, the numbers are 15.2% and 26.3%, respec-
tively. Below is an example of a 1-many Chinese
segment, along with the human reference and its
translation by one of the systems:
[source] ??????Erinys????????RISC?
????????????????????????
?????
[ref] Russian police claim that Erinys has an important com-
petitor RISC. The last people Litvinenko saw while he was
alive, Lugovoi and his friends, were all engaged in these in-
dustries.
[sys] Russian police have claimed that a major competitor,
Litvinenko his last meeting with friends are engaged in these
industries.
We conducted ANOVA on HTER, separately
for each language, with type of segment (1-1 or
284
AOV Arabic Chinese
Pr(> F ) 0.209 0.0045*
1-1 1-many
System HTER HTER
ZH-Sys1 16.22 19.03*
ZH-Sys2 19.54 21.02
ZH-Sys3 20.64 23.86*
Table 2: ANOVA for both languages; average
HTER for the three Chinese to English systems,
stratified on type of segment (1-1 and 1-many). An
(*) denotes significance at p < 0.05.
1-many) as the independent variable and systems
treated as subjects. The test revealed that there is
a significant difference in translation quality be-
tween 1-1 and 1-many segments for Chinese but
not for Arabic. For the Chinese to English systems
we further ran a Wilcoxon rank sum test to iden-
tify the statistical significance in performance for
individual systems. For two of the three systems
the difference is significant, as shown in Table 2.
We have now established that 1-many segments
in Chinese to English translation are highly preva-
lent and their translations are of consistently lower
quality compared to 1-1 segments. This finding
suggests a cross language discourse analysis task
of identifying Chinese sentences that cannot be
translated into single English sentences. This task
may be related to existing efforts in comma dis-
ambiguation in Chinese (Jin et al, 2004; Xue and
Yang, 2011; Xu and Li, 2013) but the relation-
ship between the two problems needs to be clar-
ified in follow up work. Once 1-many segments
are identified, source-side text simplification tech-
niques may be developed (Siddharthan, 2006) to
improve translation quality.
5 Explicit discourse relations
Explicit discourse relations such as COMPARISON,
CONTINGENCY or TEMPORAL are signaled by
an explicit connective, i.e., however or because.
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) provides annotations for the arguments
and relation senses of one hundred pre-selected
discourse connectives over the news portion of
the Penn Treebank corpus (Marcus et al, 1993).
Based on the PDTB, accurate systems for explicit
discourse relation identification have been devel-
oped (Pitler and Nenkova, 2009; Lin et al, 2014).
The accuracy of these systems is 94% or higher,
close to human performance on the task. Here we
AOV Arabic Chinese
Pr(> F ) 0.39 0.0058*
No Conn > 1 Conn
all % data (ZH) 53.77 15.08
1-many % data (ZH) 13.77 5.25
HTER mean HTER mean
all ZH-Sys1 16.11 19.84
+
ZH-Sys2 19.96 22.39
ZH-Sys3 20.70 25.00*
1-many ZH-Sys1 16.94 22.75
+
ZH-Sys2 20.47 23.25
ZH-Sys3 22.30 29.68*
Table 3: Number of connectives: ANOVA for both
languages; proportion of data in each factor level
and average HTER for the three Chinese-English
systems, of the entire dataset and of 1-many trans-
lations. An (*) or (+) sign denotes significance at
95% and 90% confidence levels, respectively.
study the influence of explicit discourse relations
on machine translation quality and their interac-
tion with 1-1 and 1-many segments.
5.1 Number of connectives
We identify discourse connectives and their senses
(TEMPORAL, COMPARISON, CONTINGENCY or
EXPANSION) in each reference segment using the
system in Pitler and Nenkova (2009)
3
. We com-
pare the translation quality obtained on segments
with reference translation containing no discourse
connective, exactly one discourse connective and
more than one discourse connective.
The ANOVA indicates that the number of con-
nectives is not a significant factor for Arabic trans-
lation, but significantly impacts Chinese transla-
tion quality. A closer inspection using Wilcoxon
rank sum tests reveals that the difference in trans-
lation quality is statistically significant only be-
tween the groups of segments with no connective
vs. those with more than one connective. Addi-
tionally, we ran Wilcoxon rank sum test over 1-
1 and 1-many segments individually and find that
the presence of discourse connectives is associated
with worse quality only in the latter case. Effects
above are illustrated in Table 3.
5.2 Ambiguity of connectives
A number of discourse connectives are ambiguous
with respect to the discourse relation they convey.
For example, while can signal either COMPARI-
3
http://www.cis.upenn.edu/?epitler/discourse.html; We
used the Stanford Parser (Klein and Manning, 2003).
285
AOV Arabic Chinese
Pr(> F ) 0.57 0.00014*
has-amb-conn no-amb-conn
System HTER mean HTER mean
ZH-Sys1 21.57 16.34*
ZH-Sys2 21.44 19.72
ZH-Sys3 27.47 20.69*
Table 4: ANOVA for both languages; average
HTER for the three Chinese systems for segments
with (11.80% of all data) and without an ambigu-
ous connective in the reference translation. An (*)
denotes significance at p < 0.05.
SON or TEMPORAL relations and since can signal
either CONTINGENCY or TEMPORAL. In transla-
tion this becomes a problem when the ambiguity
is present in one language but not in the other.
In such cases the sense in source ought to be dis-
ambiguated before translation. Here we compare
the translation quality of segments which contain
ambiguous discourse connectives in the reference
translation to those that do not. This analysis gives
lower bound on the translation quality degradation
associated with discourse phenomena as it does
not capture problems arising from connective am-
biguity on the source side.
We base our classification of discourse connec-
tives into ambiguous or not according to the dis-
tribution of their senses in the PDTB. We call a
connective ambiguous if its most frequent sense
among COMPARISON, CONTINGENCY, EXPAN-
SION, TEMPORAL accounts for less than 80% of
occurrence of that connective in the PDTB. Nine-
teen connectives meet this criterion of ambiguity.
4
In the ANOVA tests for each language, we com-
pared the quality of segments which contained an
ambiguous connective in the reference with those
that do not, with systems treated as subjects. For
Arabic the presence of ambiguous connective did
not yield a statistically significant difference. The
difference however was highly significant for Chi-
nese, as shown in Table 4.
The finding that discourse connective ambigu-
ity is associated with change in translation quality
for Chinese but not for Arabic is rather interesting.
It appears that the language pair in translation im-
pacts the expected gains from discourse analysis
on translation.
4
The ambiguous connectives are: as, as if, as long as, as
though, finally, if and when, in the end, in turn, lest, mean-
while, much as, neither...nor, now that, rather, since, ulti-
mately, when, when and if, while
AOV Event Arabic Chinese
Pr(> F ) Contingency 0.61 0.028*
Comp.:Temp. 0.047* 0.0041*
Chinese HTER HTER
Contingency ? Contingency
Sys1 20.15 16.72
Sys2 21.69 19.80
Sys3 25.87 21.16
+
Comp.?Temp. ?(Comp.?Temp.)
Sys1 23.58 16.64*
Sys2 26.16 19.63*
Sys3 27.20 21.21
+
Table 5: ANOVA for both languages; average
HTER for Chinese sentences containing a CON-
TINGENCY relation (6.89% of all data) or both
COMPARISON and TEMPORAL (4.59% of all data).
An (*) or (+) sign denotes significance at 95% and
90% confidence levels, respectively.
5.3 Relation senses
Here we study whether discourse relations of spe-
cific senses pose more difficulties on translations
than others and whether there are interactions be-
tween senses. In the ANOVA analysis we used a
binary factor for each of the four possible senses.
For example, we compare the translation quality
of segments that contain COMPARISON relations
in the reference translation with those that do not.
The relation sense makes a significant differ-
ence in translation quality for Chinese but not for
Arabic. For Chinese specifically sentences that ex-
press CONTINGENCY relations have worse qual-
ity translations than sentences that do not express
CONTINGENCY. One explanation for this ten-
dency may be that CONTINGENCY in Chinese con-
tains more ambiguity with other relations such as
TEMPORAL, as tense is expressed lexically in Chi-
nese (no morphological tense marking on verbs).
Finally, the interaction between COMPARISON and
TEMPORAL is significant for both languages.
Table 5 shows the effect of relation sense on
HTER values for Chinese.
6 Human edits of discourse connectives
A relation expressed implicitly without a connec-
tive in one language may need to be explicit in
another. Moreover, the expressions themselves
are used differently; for example, the paired con-
nective ???...??? (despite...but) in Chinese
should not be translated into two redundant con-
nectives in English. It is also possible that the
source language contains an explicit discourse
286
connective which is not translated in the target lan-
guage, as has been quantitatively studied recently
by Meyer and Webber (2013). An example from
our dataset is shown below:
[source] ????????????????????
??????????????
[ref] Still some others can receive further professional game
training in universities and later(Temporal) be employed as
technical consultants by large game manufacturers, etc.
[sys] Some people may go to the university games profes-
sional education, which is appointed as the big game manu-
facturers such as technical advisers.
[edited] Some people may go to university to receive profes-
sional game education, and later(Temporal) be appointed by
the big game manufacturers as technical advisers.
The system fails to translate the discourse con-
nective ???? (later), leading to a probable mis-
interpretation between receiving education and be-
ing appointed as technical advisors.
Due to the lack of reliable tools and resources,
we approximate mismatches between discourse
expressions in the source and MT output using
discourse-related edits. We identify explicit dis-
course connectives and their senses in the system
translation and the human edited version of that
translation. Then we consider the following mu-
tually exclusive possibilities: (i) there are no dis-
course connectives in either the system output or
the edit; (ii) the system output and its edited ver-
sion contain exactly the same discourse connec-
tives with the same senses; (iii) there is a discourse
connective present in the system output but not in
the edit or vice versa. In the ANOVA we use a
factor with three levels corresponding to the three
cases described above. The factor is significant for
both Chinese and Arabic. In both languages, the
mismatch case (iii) involves significantly higher
HTER than either case (i) or (ii). The human edit
rate in the mismatch class is on average four points
greater than that in the other classes.
Obviously, the mismatch in implicit/explicit ex-
pression of discourse relation is related to the
first problem we studied, i.e., if the source seg-
ment is translated into one or multiple sentences
in English, since discourse relations between adja-
cent sentences are more often implicit (than intra-
sentence ones). For this reason we performed a
Wilcoxon rank sum test for the translation qual-
ity of segments with discourse mismatch condi-
tioned on whether the segment was 1-1 or 1-many.
For both languages a significant difference was
found for 1-1 sentences but not 1-many. Table 6
shows the proportion of data in each of the con-
ditioned classes and the average HTER for sen-
% data Mismatch Mismatch ?Mismatch
(1-1) (1-1)
Arabic 21.27 15.47 69.34
Chinese 29.51 17.05 56.82
AOV Arabic Chinese
Pr(> F ) 4.0? 10
?6
* 4.1? 10
?11
*
HTER HTER
?Mismatch Mismatch
AR-Sys1 11.23 15.92*
AR-Sys2 11.64 15.74*
ZH-Sys1 15.57 20.72*
ZH-Sys2 19.02 22.34*
ZH-Sys3 11.64 15.74*
?Mismatch|1-1 Mismatch|1-1
AR-Sys1 10.86 16.24*
AR-Sys2 11.58 16.65*
ZH-Sys1 15.47 19.13*
ZH-Sys2 18.68 22.52*
ZH-Sys3 19.57 26.07*
Table 6: Data portions, ANOVA for both lan-
guages and average HTER for segments where
there is a discourse mismatch between system and
edited translations. An (*) denotes significance at
p < 0.05.
tences from the mismatch case (iii) where a dis-
course connective was edited and the others (no
such edits). Translation quality degrades signifi-
cantly for all systems for the mismatch case, over
all data as well as 1-1 segments.
7 Conclusion
We showed that translation from Chinese to En-
glish is made more difficult by various discourse
events such as the use of discourse connectives,
the ambiguity of the connectives and the type of
relations they signal. None of these discourse fac-
tors has a significant impact on translation qual-
ity from Arabic to English. Translation quality
from both languages is adversely affected by trans-
lations of discourse relations expressed implicitly
in one language but explicitly in the other or by
paired connectives. Our experiments indicate that
discourse usage may affect machine translation
between some language pairs but not others, and
for particular relations such as CONTINGENCY.
Finally, we established the need to identify sen-
tences in the source language that would be trans-
lated into multiple sentences in English. Espe-
cially in translating from Chinese to English, there
is a large number of such sentences which are cur-
rently translated much worse than other sentences.
287
References
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 33?40.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text
simplification. In Proceedings of the 16th Con-
ference on Computational Linguistics (COLING),
pages 1041?1044.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 531?540.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL), pages 273?280.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL): Short Papers, pages 49?52.
Yaohong Jin and Zhiying Liu. 2010. Improving
Chinese-English patent machine translation using
sentence segmentation. In International Conference
on Natural Language Processing and Knowledge
Engineering (NLP-KE), pages 1?6.
Meixun Jin, Mi-Young Kim, Dongil Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese
long sentences using commas. In Proceedings of
the Third SIGHAN Workshop on Chinese Language
Processing (SIGHAN), pages 1?8.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems, volume 15.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics - Special issue on using large
corpora, 19(2):313?330.
Thomas Meyer and Lucie Pol?akov?a. 2013. Machine
translation with many manually labeled discourse
connectives. In Proceedings of the Workshop on
Discourse in Machine Translation (DiscoMT), pages
43?50.
Thomas Meyer and Bonnie Webber. 2013. Implicita-
tion of discourse connectives in (machine) transla-
tion. In Proceedings of the Workshop on Discourse
in Machine Translation (DiscoMT), pages 19?26.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 128?135.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence: Short Papers, pages 13?16.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77?109.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Shengqin Xu and Peifeng Li. 2013. Recognizing Chi-
nese elementary discourse unit on comma. In Inter-
national Conference on Asian Language Processing
(IALP), pages 3?6.
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In Pro-
ceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies (HLT): Short Papers, pages
631?635.
Dapeng Yin, F. Ren, Peilin Jiang, and S. Kuroiwa.
2007. Chinese complex long sentences processing
method for Chinese-Japanese machine translation.
In International Conference on Natural Language
Processing and Knowledge Engineering (NLP-KE),
pages 170?175.
288
What Makes Writing Great? First Experiments on Article Quality
Prediction in the Science Journalism Domain
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104
nenkova@seas.upenn.edu
Abstract
Great writing is rare and highly admired.
Readers seek out articles that are beautifully
written, informative and entertaining. Yet
information-access technologies lack capabil-
ities for predicting article quality at this level.
In this paper we present first experiments on
article quality prediction in the science jour-
nalism domain. We introduce a corpus of
great pieces of science journalism, along with
typical articles from the genre. We imple-
ment features to capture aspects of great writ-
ing, including surprising, visual and emotional
content, as well as general features related to
discourse organization and sentence structure.
We show that the distinction between great
and typical articles can be detected fairly ac-
curately, and that the entire spectrum of our
features contribute to the distinction.
1 Introduction
Measures of article quality would be hugely bene-
ficial for information retrieval and recommendation
systems. In this paper, we describe a dataset of New
York Times science journalism articles which we
have categorized for quality differences and present
a system that can automatically make the distinction.
Science journalism conveys complex scientific
ideas, entertaining and educating at the same time.
Consider the following opening of a 2005 article by
David Quammen from Harper?s magazine:
One morning early last winter a small item appeared in
my local newspaper announcing the birth of an extraordi-
nary animal. A team of researchers at Texas A&M Uni-
versity had succeeded in cloning a whitetail deer. Never
done before. The fawn, known as Dewey, was developing
normally and seemed to be healthy. He had no mother,
just a surrogate who had carried his fetus to term. He
had no father, just a ?donor? of all his chromosomes. He
was the genetic duplicate of a certain trophy buck out
of south Texas whose skin cells had been cultured in a
laboratory. One of those cells furnished a nucleus that,
transplanted and rejiggered, became the DNA core of an
egg cell, which became an embryo, which in time be-
came Dewey. So he was wildlife, in a sense, and in an-
other sense elaborately synthetic. This is the sort of news,
quirky but epochal, that can cause a person with a mouth-
ful of toast to pause and marvel. What a dumb idea, I
marveled.
The writing is clear and well-organized but the
text also contains creative use of language and a
clever story-like explanation of the scientific con-
tribution. Such properties make science journalism
an attractive genre for studying writing quality. Sci-
ence journalism is also a highly relevant domain for
information retrieval in the context of educational
as well as entertaining applications. Article quality
measures can hugely benefit such systems.
Prior work indicates that three aspects of article
quality can be successfully predicted: a) whether
a text meets the acceptable standards for spelling
(Brill and Moore, 2000), grammar (Tetreault and
Chodorow, 2008; Rozovskaya and Roth, 2010) and
discourse organization (Barzilay et al, 2002; Lap-
ata, 2003); b) has a topic that is interesting to a par-
ticular user. For example, content-based recommen-
dation systems standardly represent user interest us-
ing frequent words from articles in a user?s history
and retrieve other articles on the same topics (Paz-
341
Transactions of the Association for Computational Linguistics, 1 (2013) 341?352. Action Editor: Mirella Lapata.
Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
zani et al, 1996; Mooney and Roy, 2000); and c) is
easy to read for a target readership. Shorter words
(Flesch, 1948), less complex syntax (Schwarm and
Ostendorf, 2005) and high cohesion between sen-
tences (Graesser et al, 2004) typically indicate eas-
ier and more ?readable? articles.
Less understood is the question of what makes an
article interesting and beautifully written. An early
and influential work on readability (Flesch, 1948)
also computed an interest measure with the hypoth-
esis that interesting articles would be easier to read.
More recently, McIntyre and Lapata (2009) found
that people?s ratings of interest for fairy tales can be
successfully predicted using token-level scores re-
lated to syntactic items and categories from a psy-
cholinguistic database. But large scale studies of in-
terest measures for adult educated readers have not
been carried out.
Further, there have been little attempts to measure
article quality in a genre-specific setting. But it is
reasonable to expect that properties related to the
unique aspects of a genre should contribute to the
prediction of quality in the same way that domain-
specific spelling and grammar correction (Cucerzan
and Brill, 2004; Bao et al, 2011; Dale and Kilgar-
riff, 2010) techniques have been successful.
Here we address the above two issues by develop-
ing measures related to interesting and well-written
nature specifically for science journalism. Central
to our work is a corpus of science news articles with
two categories: written by popular journalists and
typical articles in science columns (Section 2). We
introduce a set of genre-specific features related to
beautiful writing, visual nature and affective content
(Section 3) and show that they have high predictive
accuracies, 20% above the baseline, for distinguish-
ing our quality categories (Section 4). Our final sys-
tem combines the measures for interest and genre-
specific features with those proposed for identifying
readable, well-written and topically interesting arti-
cles, giving an accuracy of 84% (Section 5).
2 Article quality corpus
Our corpus1 contains chosen articles from the larger
New York Times (NYT) corpus (Sandhaus, 2008),
the latter containing a wealth of metadata about each
1Available from http://www.cis.upenn.edu/
?nlp/corpora/scinewscorpus.html
article including author information and manually
assigned topic tags.
2.1 General corpus
The articles in the VERY GOOD category include all
contributions to the NYT by authors whose writing
appeared in ?The Best American Science Writing?
anthology published annually since 1999. Articles
from the science columns of leading newspapers are
nominated and expert journalists choose a set they
consider exceptional to appear in these anthologies.
There are 63 NYT articles in the anthology (between
years 1999 and 2007) that are also part of the digital
NYT corpus; these articles form the seed set of the
VERY GOOD category.
We further include in the VERY GOOD category
all other science articles contributed to NYT by the
authors of the seed examples. Science articles by
other authors not in our seed set form the TYPICAL
category. We perform this expansion by first creat-
ing a relevant set of science articles. There is no
single meta-data tag in the NYT which refers to all
the science articles. So we use the topic tags from
the seed articles as an initial set of research tags.
We then compute the minimal set of research tags
that cover all best articles. We greedily add tags
into the minimal set, at each iteration choosing the
tag that is present in the majority of articles that re-
main uncovered. This minimal set contains 14 tags
such as ?Medicine and Health?, ?Space?, ?Research?,
?Physics? and ?Evolution?.
We collect articles from the NYT which have at
least one of the minimal set tags. However, even
a cursory mention of a research topic results in a
research-related tag being assigned to the article. So
we also use a dictionary of research-related terms
to determine whether the article passes a minimum
threshold for research content. We created this dic-
tionary manually and it contains the following words
and their morphological variants (total 63 items).
We used our intuition about a few categories of re-
search words to create this list. The category is
shown in capital letters below.
PEOPLE: researcher, scientist, physicist, biologist, economist,
anthropologist, environmentalist, linguist, professor, dr, student
PROCESS: discover, found, experiment, work, finding, study,
question, project, discuss
TOPIC: biology, physics, chemistry, anthropology, primatology
342
PUBLICATIONS: report, published, journal, paper, author, issue
OTHER: human, science, research, knowledge, university, lab-
oratory, lab
ENDINGS: -ology -gist, -list, -mist, -uist, -phy
The items in the ENDINGS category are used
to match word suffixes. An article is considered
science-related if at least 10 of its tokens match the
dictionary and in addition, at least 5 unique words
from the dictionary are matched. Since the time span
of the best articles is 1999 to 2007, we limit our col-
lection to this timespan. In addition, we only con-
sider articles that are at least 500 words long. This
filtered set of 23,709 articles form the relevant set of
science journalism.
The 63 seed samples of great writing were con-
tributed by about 40 authors. Some authors have
multiple articles selected for the best writing book
series, supporting the idea that these authors produce
high quality pieces that can be considered distinct
from typical articles. Separating the articles from
these authors gives us 3,467 extra samples of VERY
GOOD writing. In total, the VERY GOOD set has
3,530 articles. The remaining articles from the rele-
vant set, 20,242, written by about 3000 other authors
form the TYPICAL category.
2.2 Topic-paired corpus
The general corpus of science writing introduced so
far contains articles on diverse topics including bi-
ology, astronomy, religion and sports. The VERY
GOOD and TYPICAL categories created above al-
low us to study writing quality without regard to
topic. However a typical information retrieval sce-
nario would involve comparison between articles of
the same topic, i.e. relevant to the same query. To
investigate how quality differentiation can be done
within topics, we created another corpus where we
paired articles of VERY GOOD and TYPICAL quality.
For each article in the VERY GOOD category, we
compute similarity with all articles in the TYPICAL
set. This similarity is computed by comparing the
topic words (computed using a loglikelihood ratio
test (Lin and Hovy, 2000)) of the two articles. We
retain the most similar 10 TYPICAL articles for each
VERY GOOD article. We enumerate all pairs of VERY
GOOD with matched up TYPICAL ARTICLES (10 in
number) giving a total of 35,300 pairs.
There are two distinguishing aspects of our cor-
pus. First, the average quality of articles is high.
They are unlikely to have spelling, grammar and ba-
sic organization problems allowing us to investigate
article quality rather than the detection of errors.
Second, our corpus contains more realistic samples
of quality differences for IR or article recommen-
dation compared to prior work, where system pro-
duced texts and permuted versions of an original ar-
ticle were used as proxies for lower quality text.
2.3 Tasks
We perform two types of classification tasks. We
divide our corpus into development and test sets for
these tasks in the following way.
Any topic: Here the goal is to separate out VERY
GOOD versus TYPICAL articles without regard to
topic. The setting roughly corresponds to picking
out an interesting article from an archive or a day?s
newspaper. The test set contains 3,430 VERY GOOD
articles and we randomly sample 3,430 articles from
the TYPICAL category to comprise the negative set.
Same topic: Here we use the topic-paired VERY
GOOD and TYPICAL articles. The goal is to predict
which article in the pair is the VERY GOOD one. This
task is closer to a information retrieval setting, where
articles similar in topic (retrieved for a user query)
need to be distinguished for quality. For test set, we
selected 34,300 pairs.
Development data: We randomly selected 100
VERY GOOD articles and their paired (10 each)
TYPICAL articles from the topic-normalized corpus.
Overall, these constitute 1,000 pairs which we use
for developing the same-topic classifier. From these
selected pairs we take the 100 VERY GOOD articles
and sample 100 unique articles from the TYPICAL
articles making up the pairs. These 200 articles are
used to tune the any-topic classifier.
3 Facets of science writing
In this section, we discuss six prominent facets of
science writing which we hypothesized will have
an impact on text quality. These are the presence
of passages of highly visual nature, people-oriented
content, use of beautiful language, sub-genres, sen-
timent or affect, and the depth of research descrip-
tion. Several other properties of science writing
could also be relevant to quality such as the use of
343
humor, metaphor, suspense and clarity of explana-
tions and we plan to explore these in future work.
We describe how we computed features related to
each property and tested how these features are dis-
tributed in the VERY GOOD and TYPICAL categories.
To do this analysis, we randomly sampled 1,000 ar-
ticles from each of the two categories as representa-
tive examples. We compute the value of each feature
on these articles and use a two-sided t-test to check
if the mean value of the feature is higher in one class
of articles. A p-value less than 0.05 is taken to in-
dicate significantly different trend for the feature in
the VERY GOOD versus TYPICAL articles.
Note that our feature computation step is not
tuned for the quality prediction task in any way.
Rather we aim to represent each facet as accurately
as possible. Ideally we would require manual anno-
tations for each facet (visual, sentiment nature etc.)
to achieve this goal. At this time, we simply check
some chosen features? values on a random collection
of snippets from our corpus and check if they behave
as intended without resorting to these annotations.
3.1 Visual nature of articles
Some texts create an image in the reader?s mind. For
example, the snippet below has a high visual effect.
When the sea lions approached close, seemingly as curious
about us as we were about them, their big brown eyes were
encircled by light fur that looked like makeup. One sea lion
played with a conch shell as if it were a ball.
Such vivid descriptions can engage and entertain
a reader. Kosslyn (1980) found that people spon-
taneously form images of concrete words that they
hear and use them to answer questions or perform
other tasks. Books written for student science jour-
nalists (Blum et al, 2006; Stocking, 2010) also em-
phasize the importance of visual descriptions.
We measure the visual nature of a text by count-
ing the number of visual words. Currently, the only
resource of imagery ratings for words is the MRC
psycholinguistic database (Wilson, 1988). It con-
tains a list of 3,394 words rated for their ability to
invoke an image, so the list contains both words that
are highly visual along with words that are not visual
at all. With a cutoff value we adopted, of 4.5 for the
Gilhooly-Logie and 350 for the Bristol Norms2 we
2The visual words resource in MRC contains two lists?
obtain 1,966 visual words. So the coverage of that
lexicon is likely to be low for our corpus.
We collect a larger set of visual words from a cor-
pus of tagged images from the ESP game (von Ahn
and Dabbish, 2004). The corpus contains 83,904
total images and 27,466 unique tags. The average
number of tags per picture is 14.5. The tags were
collected in a game setting where two users individ-
ually saw the same image and had to guess words
related to it. The players increased their scores when
the word guessed by one player matched that of the
other. Due to the simple annotation method, there
is considerable noise and non-visual words assigned
as tags. So we performed filtering to find high pre-
cision image words and also group them into topics.
We use Latent Dirichlet Allocation (Blei et al,
2003) to cluster image tags into topics. We treat each
picture as a document and the tags assigned to the
picture are the document?s contents. We use sym-
metric priors set to 0.01 for both topic mixture and
word distribution within each topic. We filter out the
30 most common words in the corpus, words that ap-
pear in less than four pictures and images with fewer
than five tags. The remaining words are clustered
into 100 topics with the Stanford Topic Modeling
Toolbox3 (Ramage et al, 2009). We did not tune the
number of topics and choose the value of 100 based
on the intuition that the number of visual topics is
likely to be small.
To select clean visual clusters, we make the as-
sumption that visual words are likely to be clustered
with other visual terms. Topics that are not visual
are discarded altogether. We use the manual an-
notations available with the MRC database to de-
termine which clusters are visual. For each of the
100 topics from the topic model, we examine the
top 200 words with highest probability in that topic.
We compute the precision of each topic as the pro-
portion of these 200 words that match the MRC list
of visual words (1,966 words using the cutoff men-
tioned above). Only those topics which had a pre-
cision of at least 25% were retained, resulting in 68
visual topics. Some example topics, with manually
created headings, include:
landscape. grass, mountain, green, hill, blue, field,
brown, sand, desert, dirt, landscape, sky
Gilhooly-Logie and Bristol Norms.
3http://nlp.stanford.edu/software/tmt/tmt-0.4/
344
jewellery. silver, white, diamond, gold, necklace,
chain, ring, jewel, wedding, diamonds, jewelry
shapes. round, ball, circles, logo, dots, square, dot,
sphere, glass, hole, oval, circle
Combining these 68 topics, there are 5,347 unique
visual words because topics can overlap in the list of
most probable words. 2,832 words from this set are
not present in the MRC database. Some examples
of new words in our list are ?daffodil?, ?sailor?, ?hel-
met?, ?postcard?, ?sticker?, ?carousel?, ?kayak?, and
?camouflage?. For later experiments we consider the
5,347 words as the visual word set and also keep
the information about the top 200 words in the 68
selected topics. We compute two classes of features
one based on all visual words and the other on visual
topics. We consider only the adjectives, adverbs,
verbs and common nouns in an article as candidate
words for computing visual quality.
Overall visual use: We compute the propor-
tion of candidate words that match the visual
word list as the TOTAL VISUAL feature. We also
compute the proportions based only on the first
200 words of the article (BEG VISUAL), the last
200 words (END VISUAL) and the middle region
(MID VISUAL) as features. We also divide the arti-
cle into five equally sized bins of words where each
bin captures consecutive words in the article. Within
each bin we compute the proportion of visual words.
We treat these values as a probability distribution
and compute its entropy (ENTROPY VISUAL). We
expected these position features to indicate how the
placement of visual words is related to quality.
Topic-based features: We also compute what pro-
portion of the words we identify as visual matches
the list under each topic. The maximum proportion
from a single topic (MAX TOPIC VISUAL) is a fea-
ture. We also compute a greedy cover set of top-
ics for the visual words in the article. The topic
that matches the most visual words is added first,
and the next topic is selected based on the remain-
ing unmatched words. The number of topics needed
to cover 50% of the article?s visual words is the
TOPIC COVER VISUAL feature. These features cap-
ture the mix of visual words from different topics.
Disregarding topic information, we also compute a
feature NUM PICTURES which is the number of im-
ages in the corpus where 40% of the image?s tags are
matched in the article.
We found 8 features to vary significantly be-
tween the two types of articles. The fea-
tures with significantly higher values in VERY
GOOD articles are: BEG VISUAL, END VISUAL,
MAX TOPIC VISUAL. The features with signifi-
cantly higher values in the TYPICAL articles are:
TOTAL VISUAL, MID VISUAL, ENTROPY VISUAL,
TOPIC COVER VISUAL, NUM PICTURES.
It appears that the simple expectation that VERY
GOOD articles contain more visual words overall
does not hold true here. However the great writ-
ing samples have a higher degree of visual content
in the beginning and ends of articles. Good articles
also have lower entropy for the distribution of vi-
sual words indicating that they appear in localized
positions in contrast to being distributed throughout.
The topic-based features further indicate that for the
VERY GOOD articles, the visual words come from
only a few topics (compared to TYPICAL articles)
and so may evoke a coherent image or scene.
3.2 The use of people in the story
We hypothesized that articles containing research
findings that directly affect people in some way, and
therefore involve explicit references to people in the
story, will make a bigger impact on the reader. For
example, the most frequent topic among our VERY
GOOD samples is ?medicine and health? where ar-
ticles are often written from the view of a patient,
doctor or scientist. An example is below.
Dr. Remington was born in Reedville, Va., in 1922, to Maud
and P. Sheldon Remington, a school headmaster. Charles spent
his boyhood chasing butterflies alongside his father, also a col-
lector. During his graduate studies at Harvard, he founded the
Lepidopterists? Society with an equally butterfly-smitten under-
graduate, Harry Clench.
We approximate this facet by computing the num-
ber of explicit references to people, relying on three
sources of information about animacy of words. The
first is named entity (NE) tags (PERSON, ORGANI-
ZATION and LOCATION) returned by the Stanford
NE recognition tool (Finkel et al, 2005). We also
created a list of personal pronouns such as ?he?, ?my-
self? etc. which standardly indicate animate entities
(animate pronouns).
Our third resource contains the number of times
different noun phrases (NP) were followed by each
of the relative pronouns ?who?, ?where? and ?which?.
345
These counts for 664,673 noun phrases were col-
lected by Ji and Lin (2009) from the Google Ngram
Corpus (Lin et al, 2010). We use a simple heuris-
tic to obtain a list of animate (google animate) and
inanimate nouns (google inanimate) from this list.
The head of each NP is taken as a candidate noun.
If the noun does not occur with ?who? in any of the
noun phrases where it is the head, then it is inani-
mate. In contrast, if it appears only with ?who? in
all noun phrases, it is animate. Otherwise, for each
NP where the noun is a head, we check whether the
count of times the noun phrase appeared with ?who?
is greater than each of the occurrences of ?which?,
?where? and ?when? (taken individually) with that
noun phrase. If the condition holds for at least one
noun phrase, the noun is marked as animate.
When computing the features for an article, we
consider all nouns and pronouns as candidate words.
If the word is a pronoun and appears in our list of an-
imate pronouns, it is assigned an ?animate? label and
?inanimate? otherwise. If the word is a proper noun
and tagged with the PERSON NE tag, we mark it
as ?animate? and if it is a ORGANIZATION or LO-
CATION tag, the word is ?inanimate?. For common
nouns, we check it if appears in the google animate
and inanimate lists. Any match is labelled accord-
ingly as ?animate? and ?inanimate?. Note that this
procedure may leave some nouns without any labels.
Our features are counts of animate tokens
(ANIM), inanimate tokens (INAMIN) and both these
counts normalized by total words in the article
(ANIM PROP, INANIM PROP). Three of these fea-
tures had significantly higher mean values in the
TYPICAL category of articles: ANIM, ANIM PROP,
INANIM PROP. We found upon observation that sev-
eral articles that talk about government policies in-
volve a lot of references to people but are often in the
TYPICAL category. These findings suggest that the
?human? dimension might need to be computed not
only based on simple counts of references to people
but also involve finer distinctions between them.
3.3 Beautiful language
Beautiful phrasing and word choice can entertain
the reader and leave a positive impression. Multi-
ple studies in the education genre (Diederich, 1974;
Spandel, 2004) note that when teachers and expert
adult readers graded student writing, word choice
and phrasing always turn out as a significant factors
influencing the raters? scores.
We implement a method for detecting creative
language based on a simple idea that creative words
and phrases are sometimes those that are used in un-
usual contexts and combinations or those that sound
unusual. We compute measures of unusual language
both at the level of individual words and for the com-
bination of words in a syntactic relation.
Word level measures: Unusual words in an ar-
ticle are likely to be those with low frequencies
in a background corpus. We use the full set of
articles (not only science) from year 1996 in the
NYT corpus as a background (these do not over-
lap with our corpus for article quality). We also ex-
plore patterns of letters and phoneme sequences with
the idea that unusual combination of characters and
phonemes could create interesting words. We used
the CMU pronunciation dictionary (Weide, 1998) to
get the phoneme information for words and built a 4-
gram model of phonemes on the background corpus.
Laplace smoothing is used to compute probabilities
from the model. However, the CMU dictionary does
not contain phoneme information for several words
in our corpus. So we also compute an approximate
model using the letters in the words and obtain an-
other 4-gram model.4 Only words that are longer
than 4 characters are used in both models and we fil-
ter out proper names, named entities and numbers.
During development, we analyzed the articles
from an entire year of NYT, 1997, with the three
models to identify unusual words. Below is the list
of words with lowest frequency and those with high-
est perplexity under the phoneme and letter models.
Low frequency. undersheriff, woggle, ahmok,
hofman, volga, oceanaut, trachoma, baneful, truffler,
acrimal, corvair, entomopter
High perplexity-phoneme model. showroom, yahoo,
dossier, powwow, plowshare, oomph, chihuahua, iono-
sphere, boudoir, superb, zaire, oeuvre
High perplexity-letter model. kudzu, muumuu, qi-
pao, yugoslav, kohlrabi, iraqi, yaqui, yakuza, jujitsu, oeu-
vre, yaohan, kaffiyeh
For computing the features, we consider only
nouns, verbs, adjectives and adverbs. We also
require that the words are at least 5 letters long
4We found that higher order n-grams provided better pre-
dictions of unusual nature during development.
346
and do not contain a hyphen5. Three types of
scores are computed. FREQ NYT is the aver-
age of word frequencies computed from the back-
ground corpus. The second set of features are
based on the phoneme model. We compute the
average perplexity of words under the model,
AVR PHONEME PERP ALL. In addition, we also or-
der the words in an article based on decreasing per-
plexity values and the average perplexity of the top
10, 20 and 30 words in this list are added as fea-
tures (AVR PHONEME PERP 10, 20, 30). We ob-
tain similar features from the letter n-gram model
(AVR CHAR PERP ALL, AVR CHAR PERP 10, 20,
30). In phoneme features, we ignore words that do
not have an entry in the CMU dictionary.
Word pair measures: Next we attempt to detect un-
usual combinations of words. We do this calculation
only for certain types of syntactic relations?a) nouns
and their adjective modifiers, b) verbs with adverb
modifiers, c) adjacent nouns in a noun phrase and
d) verb and subject pairs. Counts for co-occurrence
again come from NYT 1996 articles. The syntactic
relations are obtained using the constituency and de-
pendency parses from the Stanford parser (Klein and
Manning, 2003; De Marneffe et al, 2006). To avoid
the influence of proper names and named entities,
we replace them with tags (NNP for proper names
and PERSON, ORG, LOC for named entities).
We treat the words for which the dependency
holds as a (auxiliary word, main word) pair. For
adjective-noun and adverb-verb pairs, the auxiliary
is the adjective or adverb; for noun-noun pairs, it is
the first noun; and for verb-subject pairs, the auxil-
iary is the subject. Our idea is to compute usualness
scores based on frequency with which a particular
pair of words appears in the background.
Specifically, we compute the conditional proba-
bility of the auxiliary word given the main word
as the score for likelihood of observing the pair.
We consider the main word as related to the article
topic, so we use the conditional probability of auxil-
iary given main word and not the other way around.
However, the conditional probability has no infor-
mation about the frequency of the auxiliary word. So
we apply ideas from interpolation smoothing (Chen
5We noticed that in this genre several new words are created
using hyphen to concatenate common words.
ADJ-NOUN ADV-VERB
hypoactive NNP suburbs said
plasticky woman integral was
psychogenic problems collective do
yoplait television physiologically do
subminimal level amuck run
ehatchery investment illegitimately put
NOUN-NOUN SUBJ-VERB
specification today blog said
auditory system briefer said
pal programs hr said
steganography programs knucklehead said
wastewater system lymphedema have
autism conference permissions have
Table 1: Unusual word-pairs from different categories
and Goodman, 1996) and compute the conditional
probability as a interpolated quantity together with
the unigram probability of the auxiliary word.
p?(aux|main) = ??p(aux|main)+(1??)?p(aux)
The unigram and conditional probabilities are
also smoothed using Laplace method. We train the
lambda values to optimize data likelihood using the
Baum Welch algorithm and use the pairs from NYT
1997 year articles as a development set. The lambda
values across all types of pairs tended to be lower
than 0.5 giving higher weight to the unigram proba-
bility of the auxiliary word.
Based on our observations on the development
set, we picked a cutoff of 0.0001 on the proba-
bility (0.001 for adverb-verb pairs) and consider
phrases with probability below this value as un-
usual. For each test article, we compute the num-
ber of unusual phrases (total for all categories)
as a feature (SURP) and also this value normal-
ized by total number of word tokens in the article
(SURP WD) and normalized by number of phrases
(SURP PH). We also compute features for indi-
vidual pair types and in each case, the number of
unusual phrases is normalized by the total words
in the article (SURP ADJ NOUN, SURP ADV VERB,
SURP NOUN NOUN, SURP SUBJ VERB).
A list of the top unusual words under the different
pair types are shown in Table 1. These were com-
puted on pairs from a random set of articles from our
corpus. Several of the top pairs involve hyphenated
words which are unusual by themselves, so we only
show in the table the top words without hyphens.
347
Most of these features are significantly
different between the two classes. Those
with higher values in the VERY GOOD
set include: AVR PHONEME PERP ALL,
AVR CHAR PERP (ALL, 10), SURP, SURP PH,
SURP WD, SURP ADJ NOUN, SURP NOUN NOUN,
SURP SUBJ VERB. The FREQ NYT feature has
higher value in the TYPICAL class.
All these trends indicate that unusual phrases are
associated with the VERY GOOD category of articles.
3.4 Sub-genres
There are several sub-genres within science writing
(Stocking, 2010): short descriptions of discoveries,
longer explanatory articles, narratives, stories about
scientists, reports on meetings, review articles and
blog posts. Naturally, some of these sub-genres will
be more appealing to readers. To investigate this
aspect, we compute scores for some sub-genres of
interest?narrative, attribution and interview.
Narrative texts typically have characters and
events (Nakhimovsky, 1988), so we look for entities
and past tense in the articles. We count the number
of sentences where the first verb in surface order is
in the past tense. Then among these sentences, we
pick those which have either a personal pronoun or a
proper noun before the target verb (again in surface
order). The proportion of such sentences in the text
is taken as the NARRATIVE score.
We also developed a measure to identify the de-
gree to which the article?s content is attributed to ex-
ternal sources as opposed to the author?s own state-
ments. Attribution to other sources is frequent in
the news domain since many comments and opin-
ions are not the views of the journalist (Semetko and
Valkenburg, 2000). For science news, attribution be-
comes more important since the research findings
were obtained by scientists and reported in a second-
hand manner by the journalists. The ATTRIB score
is the proportion of sentences in the article that have
a quote symbol, or the words ?said? and ?says?.
We also compute a score to indicate if the article
is the account of an interview. There are easy clues
in NYT for this genre with paragraphs in the inter-
view portion of the article beginning with either ?Q.?
(question) or ?A.? (answer). We count the total num-
ber of ?Q.? and ?A.? prefixes combined and divide
the value by the total number of sentences (INTER-
VIEW). When either the number of ?Q.? tags is zero
or ?A.? tags is zero, the score is set to zero.
All three scores are significantly higher for the
TYPICAL class.
3.5 Affective content
Some articles, for example those detailing research
on health, crime, ethics, can provoke emotional re-
actions in readers as shown in the snippet below.
Medicine is a constant trade-off, a struggle to cure the dis-
ease without killing the patient first. Chemotherapy, for exam-
ple, involves purposely poisoning someone ? but with the ex-
pectation that the short-term injury will be outweighed by the
eventual benefits.
We compute affect-related features using three
lexicons. The MPQA (Wilson et al, 2005) and Gen-
eral Inquirer (Stone et al, 1966) give lists of positive
and negative sentiment words. The third resource
is emotion-related words from FrameNet (Baker et
al., 1998). The sizes of these lexicon are 8,221,
5,395, and 653 words respectively. We compute
the counts of positive, negative, polar, and emotion
words, each normalized by the total number of con-
tent words in the article (POS PROP, NEG PROP, PO-
LAR PROP, EMOT PROP). We also include the pro-
portion of emotion and polar words taken together
(POLAR EMOT PROP) and the ratio between count
of positive and negative words (POS BY NEG).
The features with higher values in the VERY
GOOD class are NEG PROP, POLAR PROP,
EMOT POLAR PROP. In TYPICAL articles,
POS BY NEG, EMOT PROP have higher values.
VERY GOOD articles have more sentiment words,
mostly skewed towards negative sentiment.
3.6 Amount of research content
For a lay audience, a science writer presents only the
most relevant findings and methods from a research
study and interleaves research information with de-
tails about the relevance of the finding, people in-
volved in the research and general information about
the topic. As a result, the degree of explicit research
descriptions in the articles varies considerably.
To test how this aspect is associated with qual-
ity, we count references to research methods and re-
searchers in the article. We use the research dictio-
nary that we introduced in Section 2 as the source
of research-related words. We count the total num-
348
ber of words in the article that match the dictionary
(RES TOTAL) and also the number of unique match-
ing words (RES UNIQ). We also normalize these
counts by the total words in the article and create
features RES TOTAL PROP and RES UNIQ PROP.
All four features have significantly higher values
in the VERY GOOD articles which indicate that great
articles are also associated with a great amount of
direct research content and explanations.
4 Classification accuracy
We trained classifiers using all the above features for
for the two settings??any-topic? and ?same-topic? in-
troduced in Section 2.3. The baseline random accu-
racy in both cases is 50%. We use a SVM classi-
fier with a radial basis kernel (R Development Core
Team, 2011) and parameters were tuned using cross
validation on the development data.
The best parameters were then used to classify the
test set in a 10 fold cross-validation setting. We di-
vide the test set into 10 parts, train on 9 parts and
test on the held-out data. The average accuracies in
the 10 experiments are 75.3% accuracy for the ?any-
topic? setup, and 68% accuracy for the topic-paired
?same-topic? setup. These accuracies are consider-
able improvements over the baseline.
The ?same-topic? data contains article pairs with
varying similarity. So we investigate the relationship
between topic similarity and accuracy of prediction
more closely for this setting. We divide the article
pairs into bins based on the similarity value. We
compute the 10-fold cross validation predictions us-
ing the different feature classes above and collect the
predicted values across all the folds. Then we com-
pute accuracy of examples within each bin. These
results are plotted in Figure 1. int-science refers to
the full set of features and the results from the six
feature classes are also indicated.
As the similarity increases, the prediction task be-
comes harder. The combination of all features gives
66% accuracy for pairs above 0.4 similarity and 74%
when the similarity is less than 0.15. Most individ-
ual feature classes also show a similar trend. This
result is understandable because articles on simi-
lar topics could exhibit similar properties. For ex-
ample, two articles about ?controversies surround-
ing vaccination? are likely to have similar levels of
people-oriented nature or written in a narrative style
Figure 1: Accuracy on pairs with different similarity
in the same way as two space-related articles are
both likely to contain high visual content. There are
however two exceptions?affect and research. For
these features, the accuracies improve with higher
similarity; affect features give 51% for pairs with
similarity 0.1 and 62% for pairs above 0.4 similar-
ity, accuracy of research features goes from 52% to
57% for the same similarity values. This finding il-
lustrates that even articles on very similar topics can
be written differently, with the articles by the excel-
lent authors associated with greater degree of senti-
ment, and deeper study of the research problem.
5 Combining aspects of article quality
We now compare and combine the genre-specific
interest-science features (41 total) with those dis-
cussed in work on readability, well-written nature,
interest and topic classification.
Readability (16 features): We test the full set of
readability features studied in Pitler and Nenkova
(2008), involving token-type ratio, word and sen-
tence length, language model features, cohesion
scores and syntactic estimates of complexity.
Well-written nature (23 features): For well-
written nature, we use two classes of features, both
related to discourse. One is the probabilities of dif-
ferent types of entity transitions from the Entity Grid
model (Barzilay and Lapata, 2008) which we com-
pute with the Brown Coherence Toolkit (Elsner et
al., 2007). The other class of features are those de-
fined in Pitler and Nenkova (2008) for likelihoods
and counts of explicit discourse relations. We iden-
tified the relations for texts in our corpus using the
349
AddDiscourse tool (Pitler and Nenkova, 2009).
Interesting fiction (22 features): are those intro-
duced by McIntyre and Lapata (2009) for predicting
interest ratings on fairy tales. They include counts of
syntactic items and relations, and token categories
from the MRC psycholinguistic database. We nor-
malize each feature by the total words in the article.
Content: features are based on the words present
in the articles. Word features are standard in
content-based recommendation systems (Pazzani et
al., 1996; Mooney and Roy, 2000) where they are
used to pick out articles similar to those which a user
has already read. In our work the features are the
most frequent n words in our corpus after removing
the 50 most frequent ones. The word?s count in the
article is the feature value. Note that word features
indicate topic as well as other content in the article
such as sentiment and research. A random sample of
the word features for n = 1000 is shown below and
reflects this aspect. ?matter, series, wear, nation, ac-
count, surgery, high, receive, remember, support, worry,
enough, office, prevent, biggest, customer?.
Table 2 compares the accuracies of SVM classi-
fiers trained on features from different classes and
their combinations.6 The readability, well-written
nature and interesting fiction classes provide good
accuracies 60% and above. The genre-specific
interesting-science features are individually much
stronger than these classes. Different writing as-
pects (without content) are clearly complementary
and when combined give 76% to 79% accuracy for
the ?any-topic? task and 74% for the topic pairs task.
The simple bag of words features work remark-
ably well giving 80% accuracy in both settings. As
mentioned before these word features are a mix of
topic indicators as well as other content of the ar-
ticles, ie., they also implicitly indicate animacy, re-
search or sentiment. But the high accuracy of word
features above all the writing categories indicates
that topic plays an important role in article quality.
However, despite the high accuracy, word features
are not easily interpretable in different classes re-
lated to writing as we have done with other writing
features. Further, the total set of writing features is
6For classifiers involving content features, we did not tune
the SVM parameters because of the small size of development
data compared to number of features. Default SVM settings
were used instead.
Feature set Any Topic Same
Interesting-science 75.3 68.0
Readable 65.5 63.0
Well-written 59.1 59.9
Interesting-fiction 67.9 62.8
Readable + well-writ 64.7 64.3
Readable + well-writ + Int-fict 71.0 70.3
Readable + well-writ + Int-sci 79.5 73.2
All writing aspects 76.7 74.7
Content (500 words) 81.7 79.4
Content (1000 words) 81.2 82.1
Combination: Writing (all) + Content (1000w)
In feature vector 82.6* 84.0*
Sum of confidence scores 81.6 84.9
Oracle 87.6 93.8
Table 2: Accuracy of different article quality aspects
only 102 in contrast to 1000 word features. In our
interest-science feature set, we aimed to highlight
how well some of the aspects considered important
to good science writing can predict quality ratings.
We also combined writing and word features to
mix topic with writing related predictors. We do the
combination in three ways a) word and writing fea-
tures are included together in the feature vector; b)
two separate classifiers are trained (one using word
features and the other using writing ones) and the
sum of confidence measures is used to decide on the
final class; c) an oracle method: two classifiers are
trained just as in option (b) but when they disagree
on the class, we pick the correct label. The oracle
method gives a simple upper bound on the accuracy
obtainable by combination. These values are 87%
for ?any-topic? and a higher 93.8% for ?same-topic?.
The automatic methods, both feature vector combi-
nation and classifier combination also give better ac-
curacies than only the word or writing features. The
accuracies for the folds from 10 fold cross valida-
tion in the feature vector combination method were
also found to be significantly higher than those from
word features only (using a paired Wilcoxon signed-
rank test). Therefore both topic and writing features
are clearly useful for identifying great articles.
6 Conclusion
Our work is a step towards measuring overall arti-
cle quality by showing the complementary benefits
of general and domain-specific writing measures as
well as indicators of article topic. In future we plan
to focus on development of more features as well as
better methods for combining different measures.
350
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL, pages 86?90.
Z. Bao, B. Kimelfeld, and Y. Li. 2011. A graph ap-
proach to spelling correction in domain-centric search.
In Proceedings of ACL-HLT, pages 905?914.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Lin-
guistics, 34(1):1?34.
R. Barzilay, N. Elhadad, and K. McKeown. 2002.
Inferring strategies for sentence ordering in multi-
document summar ization. Journal of Artificial Intel-
ligence Research, 17:35?55.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. the Journal of machine Learning
research, 3:993?1022.
D. Blum, M. Knudson, and R. M. Henig, editors. 2006.
A field guide for science writers: the official guide of
the national association of science writers. Oxford
University Press, New York.
E. Brill and R.C. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of ACL, pages 286?293.
S. F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proceedings of ACL, pages 310?318.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of EMNLP, pages 293?
300.
R. Dale and A. Kilgarriff. 2010. Helping our own:
text massaging for computational linguistics as a new
shared task. In Proceedings of INLG, pages 263?267.
M. C. De Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454.
P. Diederich. 1974. Measuring Growth in English. Na-
tional Council of Teachers of English.
M. Elsner, J. Austerweil, and E. Charniak. 2007. A uni-
fied local and global model for discourse coherence.
In Proceedings of NAACL-HLT, pages 436?443.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
ACL, pages 363?370.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32:221 ? 233.
A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-Metrix: Analysis of text on co-
hesion and language. Behavior Research Methods In-
struments and Computers, 36(2):193?202.
H. Ji and D. Lin. 2009. Gender and animacy knowledge
discovery from web-scale n-grams for unsupervised
person name detection. In Proceedings of PACLIC.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of ACL, pages 423?430.
S.M. Kosslyn. 1980. Image and mind. Harvard Univer-
sity Press.
M. Lapata. 2003. Probabilistic text structuring: Experi-
ments with sentence ordering. In Proceedings of ACL,
pages 545?552.
C. Lin and E. Hovy. 2000. The automated acquisition of
topic signatures for text summarization. In Proceed-
ings of COLING, pages 495?501.
D. Lin, K. W. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2010. New tools for web-
scale n-grams. In Proceedings of LREC.
N. McIntyre and M. Lapata. 2009. Learning to tell tales:
A data-driven approach to story generation. In Pro-
ceedings of ACL-IJCNLP, pages 217?225.
R. J. Mooney and L. Roy. 2000. Content-based book
recommending using learning for text categorization.
In Proceedings of the fifth ACM conference on Digital
libraries, pages 195?204.
A. Nakhimovsky. 1988. Aspect, aspectual class, and the
temporal structure of narrative. Computational Lin-
guistics, 14(2):29?43, June.
M. Pazzani, J. Muramatsu, and D. Billsus. 1996. Syskill
& Webert: Identifying interesting web sites. In Pro-
ceedings of AAAI, pages 54?61.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of EMNLP, pages 186?195.
E. Pitler and A. Nenkova. 2009. Using syntax to dis-
ambiguate explicit discourse connectives in text. In
Proceedings of ACL-IJCNLP, pages 13?16.
R Development Core Team, 2011. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing.
D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009. Labeled lda: A supervised topic model for
credit attribution in multi-labeled corpora. In Proceed-
ings of EMNLP, pages 248?256.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of EMNLP, pages 961?970.
E. Sandhaus. 2008. The new york times annotated cor-
pus. Corpus number LDC2008T19, Linguistic Data
Consortium, Philadelphia.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of ACL, pages 523?
530.
351
H.A. Semetko and P.M. Valkenburg. 2000. Framing eu-
ropean politics: A content analysis of press and televi-
sion news. Journal of communication, 50(2):93?109.
V. Spandel. 2004. Creating Writers Through 6-Trait
Writing: Assessment and Instruction. Allyn and Ba-
con, Inc.
S. H. Stocking. 2010. The New York Times Reader: Sci-
ence and Technology. CQ Press, Washington DC.
P. J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press.
J. R. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in esl writing. In
Proceedings of COLING, pages 865?872.
L. von Ahn and L. Dabbish. 2004. Labeling images with
a computer game. In Proceedings of CHI, pages 319?
326.
R. L. Weide. 1998. The cmu pronunciation dictio-
nary, release 0.6. http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of HLT-EMNLP, pages 347?354.
M. Wilson. 1988. MRC psycholinguistic database:
Machine-usable dictionary, version 2.00. Behavior
Research Methods, 20(1):6?10.
352
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?62,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Using entity features to classify implicit discourse relations
Annie Louis, Aravind Joshi, Rashmi Prasad, Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
{lannie,joshi,rjprasad,nenkova}@seas.upenn.edu
Abstract
We report results on predicting the sense
of implicit discourse relations between ad-
jacent sentences in text. Our investigation
concentrates on the association between
discourse relations and properties of the
referring expressions that appear in the re-
lated sentences. The properties of inter-
est include coreference information, gram-
matical role, information status and syn-
tactic form of referring expressions. Pre-
dicting the sense of implicit discourse re-
lations based on these features is consid-
erably better than a random baseline and
several of the most discriminative features
conform with linguistic intuitions. How-
ever, these features do not perform as well
as lexical features traditionally used for
sense prediction.
1 Introduction
Coherent text is described in terms of discourse re-
lations such as ?cause? and ?contrast? between its
constituent clauses. It is also characterized by en-
tity coherence, where the connectedness of the text
is created by virtue of the mentioned entities and
the properties of referring expressions. We aim to
investigate the association between discourse rela-
tions and the way in which references to entities
are realized. In our work, we employ features re-
lated to entity realization to automatically identify
discourse relations in text.
We focus on implicit relations that hold be-
tween adjacent sentences in the absence of dis-
course connectives such as ?because? or ?but?.
Previous studies on this task have zeroed in on
lexical indicators of relation sense: dependencies
between words (Marcu and Echihabi, 2001; Blair-
Goldensohn et al, 2007) and the semantic orien-
tation of words (Pitler et al, 2009), or on general
syntactic regularities (Lin et al, 2009).
The role of entities has also been hypothesized
as important for this task and entity-related fea-
tures have been used alongside others (Corston-
Oliver, 1998; Sporleder and Lascarides, 2008).
Corpus studies and reading time experiments per-
formed by Wolf and Gibson (2006) have in fact
demonstrated that the type of discourse relation
linking two clauses influences the resolution of
pronouns in them. However, the predictive power
of entity-related features has not been studied in-
dependently of other factors. Further motivation
for studying this type of features comes from new
corpus evidence (Prasad et al, 2008), that about a
quarter of all adjacent sentences are linked purely
by entity coherence, solely because they talk about
the same entity. Entity-related features would be
expected to better separate out such relations.
We present the first comprehensive study of the
connection between entity features and discourse
relations. We show that there are notable differ-
ences in properties of referring expressions across
the different relations. Sense prediction can be
done with results better than random baseline us-
ing only entity realization information. Their per-
formance, however, is lower than a knowledge-
poor approach using only the words in the sen-
tences as features. The addition of entity features
to these basic word features is also not beneficial.
2 Data
We use 590 Wall Street Journal (WSJ) articles
with overlapping annotations for discourse, coref-
erence and syntax from three corpora.
The Penn Discourse Treebank (PDTB) (Prasad
et al, 2008) is the largest available resource of
discourse relation annotations. In the PDTB, im-
plicit relations are annotated between adjacent
sentences in the same paragraph. They are as-
signed senses from a hierarchy containing four top
level categories?Comparison, Contingency, Tem-
poral and Expansion.
59
An example ?Contingency? relation is shown
below. Here, the second sentence provides the
cause for the belief expressed in the first.
Ex 1. These rate indications aren?t directly comparable.
Lending practices vary widely by location.
Adjacent sentences can also become related
solely by talking about a common entity without
any of the above discourse relation links between
their propositions. Such pairs are annotated as En-
tity Relations (EntRels) in the PDTB, for example:
Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S
sales to remain steady at about 1,200 cars in 1990. The luxury
auto maker last year sold 1,214 cars in the U.S.
We use the coreference annotations from the
Ontonotes corpus (version 2.9) (Hovy et al, 2006)
to compute our gold-standard entity features. The
WSJ portion of this corpus contains 590 articles.
Here, nominalizations and temporal expressions
are also annotated for coreference but we use the
links between noun phrases only. We expect these
features computed on the gold-standard annota-
tions to represent an upper bound on the perfor-
mance of entity features.
Finally, the Penn Treebank corpus (Marcus et
al., 1994) is used to obtain gold-standard parse and
grammatical role information.
Only adjacent sentences within the same para-
graph are used in our experiments.
3 Entity-related features
We associate each referring expression in a sen-
tence with a set of attributes as described below.
In Section 3.2, we detail how we combine these
attributes to compute features for a sentence pair.
3.1 Referring expression attributes
Grammatical role. In exploratory analysis of
Comparison relations, we often observed parallel
syntactic realizations for entities in the subject po-
sition of the two sentences:
Ex 3. {Longer maturities}E1 are thought to indicate de-
clining interest rates. {Shorter maturities}E2 are considered
a sign of rising rates because portfolio managers can capture
higher rates sooner.
So, for each noun phrase, we record whether
it is the subject of a main clause (msubj), subject
of other clauses in the sentence (esubj) or a noun
phrase not in subject position (other).
Given vs. New. When an entity is first intro-
duced in the text, it is considered a new entity.
Subsequent mentions of the same entity are given
(Prince, 1992). New-given distinction could help
to identify some of the Expansion and Entity re-
lations. When a sentence elaborates on another, it
might contain a greater number of new entities.
We use the Ontonotes coreference annotations
to mark the information status for entities. For
an entity, if an antecedent is found in the previ-
ous sentences, it is marked as given, otherwise it
is a new entity.
Syntactic realization. In Entity relations, the sec-
ond sentence provides more information about a
specific entity in the first and a definite description
for this second mention seems likely. Also, given
the importance of named entities in news, entities
with proper names might be the ones frequently
described using Entity relations.
We use the part of speech (POS) tag associated
with the head of the noun phrase to assign one of
the following categories: pronoun, nominal, name
or expletive. When the head does not belong to
the above classes, we simply record its POS tag.
We also mark whether the noun phrase is a definite
description using the presence of the article ?the?.
Modification. We expected modification proper-
ties to be most useful for predicting Comparison
relations. Also, named or new entities in Entity
relations are very likely to have post modification.
We record whether there are premodifiers or
postmodifiers in a given referring expression. In
the absence of pre- and postmodifiers, we indicate
bare head realization.
Topicalization. Preposed prepositional or ad-
verbial phrases before the subject of a sentence
indicate the topic under which the sentence is
framed. We observed that this property is frequent
in Comparison and Temporal relations. An exam-
ple Comparison is shown below.
Ex 4. {Under British rules}T1, Blue Arrow was able to
write off at once $1.15 billion in goodwill arising from the
purchase. {As a US-based company}T2, Blue Arrow would
have to amortize the good will over as many as 40 years, cre-
ating a continuing drag on reported earnings.
When the left sibling of a referring expression is
a topicalized phrase, we mark the topic attribute.
Number. Using the POS tag of the head word, we
note whether the entity is singular or plural.
3.2 Features for classification
Next, for each sentence pair, we associate two sets
of features using the attributes described above.
60
Let S1 and S2 denote the two adjacent sentences
in a relation, where S1 occurs first in the text.
Sentence level. These features characterize S1
and S2 individually. For each sentence, we add a
feature for each of the attributes described above.
The value of the feature is the number of times that
attribute is observed in the sentence; i.e., the fea-
ture S1given would have a value of 3 if there are 3
given entities in the first sentence.
Sentence pair. These features capture the interac-
tions between the entities present in S1 and S2.
Firstly, for each pair of entities (a, b), such that
a appears in S1 and b appears in S2, we assign
one of the following classes: (i) SAME: a and b
are coreferent, (ii) RELATED: their head words are
identical, (iii) DIFFERENT: neither coreferent nor
related. The RELATED category was introduced to
capture the parallelism often present in Compari-
son relations. Even though the entities themselves
are not coreferent, they share the same head word
(i.e. longer maturities and shorter maturities).
For features, we use the combination of the
class ((i), (ii) or (iii)) with the cross product of
the attributes for a and b. For example if a has
attributes {msubj, noun, ...} and b has attributes
{esubj, defdesc, ...} and a and b are corefer-
ent, we would increment the count for features?
{sameS1msubjS2esubj, sameS1msubjS2defdesc,
sameS1nounS2esubj, sameS1nounS2defdesc ...}.
Our total set of features observed for instances
in the training data is about 2000.
We experimented with two variants of fea-
tures: one using coreference annotations from
the Ontonotes corpus (gold-standard) and an-
other based on approximate coreference informa-
tion where entities with identical head words are
marked as coreferent.
4 Experimental setup
We define five classification tasks which disam-
biguate if a specific PDTB relation holds between
adjacent sentences. In each task, we classify the
relation of interest (positive) versus a category
with a naturally occurring distribution of all of the
other relations (negative).
Sentence pairs from sections 0 to 22 of WSJ are
used as training data and we test on sections 23
and 24. Given the skewed distribution of positive
and negative examples for each task, we randomly
downsample the negative instances in the training
set to be equal to the positive examples. The sizes
of training sets for the tasks are
Expansion vs other (4716)
Contingency vs other (2466)
Comparison vs other (1138)
Temporal vs other (474)
EntRel vs other (2378)
Half of these examples are positive and the
other negative in each case.
The test set contains 1002 sentence pairs:
Comp. (133), Cont. (230), Temp. (34), Expn.
(369), EntRel (229), NoRel1 (7). We do not down-
sample our test set. Instead, we evaluate our pre-
dictions on the natural distribution present in the
data to get a realistic estimate of performance.
We train a linear SVM classifier (LIBLIN-
EAR2) for each task.3 The optimum regulariza-
tion parameter was chosen using cross validation
on the training data.
5 Results
5.1 Feature analysis
We ranked the features (based on gold-standard
coreference information) in the training sets by
their information gain. We then checked which
attributes are common among the top five features
for different classification tasks.
As we had expected, the topicalization attribute
and RELATED entities frequently appear among
the top features for Comparison.
Features with the name attribute were highly
predictive of Entity relations as hypothesized.
However, while we had expected Entity relations
to have a high rate of coreference, we found coref-
erent mentions to be very indicative of Temporal
relations: all the top features involve the SAME at-
tribute. A post-analysis showed that close to 70%
of Temporal relations involve coreferent entities
compared to around 50% for the other classes.
The number of pronouns in the second sentence
was most characteristic of the Contingency rela-
tion. In the training set for Contingency task,
about 45% of sentences pairs belonging to Contin-
gency relation have a pronoun in the second sen-
tence. This is considerably larger than 32%, which
is the percentage of sentence pairs in the negative
examples with a pronoun in second sentence.
1PDTB relation for sentence pair when both entity and
discourse relations are absent, very rare about 1% of our data.
2http://www.csie.ntu.edu.tw/?cjlin/liblinear/
3SVMs with linear kernel gave the best performance. We
also experimented with SVMs with radial basis kernel, Naive
Bayes and MaxEnt classifiers.
61
5.2 Performance on sense prediction
The classification results (fscores) are shown in
Table 1. The random baseline (Base.) represents
the results if we predicted positive and negative re-
lations according to their proportion in the test set.
Entity features based on both gold-standard
(EntGS) and approximate coreference (EntApp)
outperform the random baseline for all the tasks.
The drop in performance without gold-standard
coreference information is strongly noticable only
for Expansion relations.
The best improvement from the baseline is seen
for predicting Contingency and Entity relations,
with around 15% absolute improvement in fscore
with both EntGS and EntApp features. The im-
provements for Comparisons and Expansions are
around 11% in the approximate case. Temporal
relations benefit least from these features. These
relations are rare, comprising 3% of the test set
and harder to isolate from other relations. Overall,
our results indicate that discourse relations and en-
tity realization have a strong association.
5.3 Comparison with lexical features
In the context of using entity features for sense
prediction, one would also like to test how these
linguistically rich features compare with simpler
knowledge-lean approaches used in prior work.
Specifically, we compare with word pairs, a
simple yet powerful set of features introduced by
Marcu and Echihabi (2001). These features are the
cross product of words in the first sentence with
those in the second.
We trained classifiers on the word pairs from the
sentences in the PDTB training sets. In Table 1,
we report the performance of word pairs (WP) as
well as their combination with gold-standard en-
tity features (WP+EntGS). Word pairs turn out as
stronger predictors for all discourse relations com-
pared to our entity features (except for Expansion
prediction with EntGS features). Further, no ben-
efits over word pair results are obtained by com-
bining entity realization information.
6 Conclusion
In this work, we used a task-based approach to
show that the two components of coherence?
discourse relations and entities?are related and
interact with each other. Coreference, givenness,
syntactic form and grammatical role of entities can
predict the implicit discourse relation between ad-
Task Base. EntGS EntApp WP WP+EntGS
Comp vs Oth. 13.27 24.18 24.14 27.30 26.19
Cont vs Oth. 22.95 37.57 38.16 38.17 38.99
Temp vs Oth. 3.39 7.58 5.61 11.09 10.04
Expn vs Oth. 36.82 52.42 47.82 48.54 49.06
Ent vs Oth. 22.85 38.03 36.73 38.48 38.14
Table 1: Fscore results
jacent sentences with results better than random
baseline. However, with respect to developing au-
tomatic discourse parsers, these entity features are
less likely to be useful. They do not outperform
or complement simpler lexical features. It would
be interesting to explore whether other aspects of
entity reference might be useful for this task, such
as bridging anaphora. But currently, annotations
and tools for these phenomena are not available.
References
S. Blair-Goldensohn, K. McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic re-
lation models. In HLT-NAACL.
S.H. Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In NAACL-HLT.
Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In EMNLP.
D. Marcu and A. Echihabi. 2001. An unsupervised ap-
proach to recognizing discourse relations. In ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In ACL-IJCNLP.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In LREC.
E. Prince. 1992. The zpg letter: subject, definiteness,
and information status. In Discourse description:
diverse analyses of a fund raising text, pages 295?
325. John Benjamins.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
F. Wolf and E. Gibson. 2006. Coherence in natural
language: data structures and applications. MIT
Press.
62
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147?156,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Discourse indicators for content selection in summarization
Annie Louis, Aravind Joshi, Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
{lannie,joshi,nenkova}@seas.upenn.edu
Abstract
We present analyses aimed at eliciting
which specific aspects of discourse pro-
vide the strongest indication for text im-
portance. In the context of content selec-
tion for single document summarization of
news, we examine the benefits of both the
graph structure of text provided by dis-
course relations and the semantic sense
of these relations. We find that structure
information is the most robust indicator
of importance. Semantic sense only pro-
vides constraints on content selection but
is not indicative of important content by it-
self. However, sense features complement
structure information and lead to improved
performance. Further, both types of dis-
course information prove complementary
to non-discourse features. While our re-
sults establish the usefulness of discourse
features, we also find that lexical overlap
provides a simple and cheap alternative
to discourse for computing text structure
with comparable performance for the task
of content selection.
1 Introduction
Discourse relations such as cause, contrast or
elaboration are considered critical for text inter-
pretation, as they signal in what way parts of a text
relate to each other to form a coherent whole. For
this reason, the discourse structure of a text can be
seen as an intermediate representation, over which
an automatic summarizer can perform computa-
tions in order to identify important spans of text
to include in a summary (Ono et al, 1994; Marcu,
1998; Wolf and Gibson, 2004). In our work, we
study the content selection performance of differ-
ent types of discourse-based features.
Discourse relations interconnect units of a text
and discourse formalisms have proposed different
resulting structures for the full text, i.e. tree (Mann
and Thompson, 1988) and graph (Wolf and Gib-
son, 2005). This structure is one source of in-
formation from discourse which can be used to
compute the importance of text units. The seman-
tics of the discourse relations between sentences
could be another indicator of content importance.
For example, text units connected by ?cause? and
?contrast? relationships might be more important
content for summaries compared to those convey-
ing ?elaboration?. While previous work have fo-
cused on developing content selection methods
based upon individual frameworks (Marcu, 1998;
Wolf and Gibson, 2004; Uzda et al, 2008), little is
known about which aspects of discourse are actu-
ally correlated with content selection power.
In our work, we separate out structural and se-
mantic features and examine their usefulness. We
also investigate whether simpler intermediate rep-
resentations can be used in lieu of discourse. More
parsimonious, easy to compute representations of
text have been proposed for summarization. For
example, a text can be reduced to a set of highly
descriptive topical words, the presence of which
is used to signal importance for content selection
(Lin and Hovy, 2002; Conroy et al, 2006). Sim-
ilarly, a graph representation of the text can be
computed, in which vertices represent sentences,
and the nodes are connected when the sentences
are similar in terms of word overlap; properties of
the graph would then determine the importance of
the nodes (Erkan and Radev, 2004; Mihalcea and
Tarau, 2005) and guide content selection.
We compare the utility of discourse features for
single-document text summarization from three
frameworks: Rhetorical Structure Theory (Mann
and Thompson, 1988), Graph Bank (Wolf and
Gibson, 2005), and Penn Discourse Treebank
(PDTB) (Prasad et al, 2008). We present a de-
tailed analysis of the predictive power of different
types of discourse features for content selection
147
and compare discourse-based selection to simpler
non-discourse methods.
2 Data
We use a collection of Wall Street Journal (WSJ)
articles manually annotated for discourse infor-
mation according to three discourse frameworks.
The Rhetorical Structure Theory (RST) and Graph
Bank (GB) corpora are relatively small compared
to the Penn Discourse Treebank (PDTB) annota-
tions that cover the 1 million word WSJ part of the
Penn Treebank corpus (Marcus et al, 1994). Our
evaluation requires gold standard summaries writ-
ten by humans, so we perform our experiments on
a subset of the overlapping documents for which
we also have human summaries available.
2.1 RST corpus
RST (Mann and Thompson, 1988) proposes that
coherent text can be represented as a tree formed
by the combination of text units via discourse re-
lations. The RST corpus developed by Carlson et
al. (2001) contains discourse tree annotations for
385 WSJ articles from the Penn Treebank corpus.
The smallest annotation units in the RST corpus
are sub-sentential clauses, also called elementary
discourse units (EDUs). Adjacent EDUs combine
through rhetorical relations into larger spans such
as sentences. The larger units recursively partici-
pate in relations with others, yielding one hierar-
chical tree structure covering the entire text.
The discourse units participating in a RST re-
lation are assigned either nucleus or satellite sta-
tus; a nucleus is considered to be more central,
or important, in the text than a satellite. Rela-
tions composed of one nucleus and one satellite
are called mononuclear relations. On the other
hand, in multinuclear relations, two or more text
units participate, and all are considered equally
important. The RST corpus is annotated with 53
mononuclear and 25 multinuclear relations. Rela-
tions that convey similar meaning are grouped, re-
sulting in 16 classes of relations: Cause, Comparison,
Condition, Contrast, Attribution, Background, Elaboration,
Enablement, Evaluation, Explanation, Joint, Manner-Means,
Topic-Comment, Summary, Temporal and Topic-Change.
2.2 Graph Bank corpus
Sometimes, texts cannot be described in a tree
structure as hypothesized by the RST. For exam-
ple, crossing dependencies and nodes with multi-
ple parents appear frequently in texts and do not
allow a tree structure to be built (Lee et al, 2008).
To address this problem, general graph representa-
tion was proposed by Wolf and Gibson (2005) as
a more realistic model of discourse structure.
Graph annotations of discourse are available for
135 documents (105 from AP Newswire and 30
from the WSJ) as part of the Graph Bank cor-
pus (Wolf and Gibson, 2005). Clauses are the ba-
sic discourse segments in this annotation. These
units are represented as the nodes in a graph, and
are linked with one another through 11 differ-
ent rhetorical relations: Cause-effect, Condition, Vio-
lated expectation, Elaboration, Example, Generalization, At-
tribution, Temporal sequence, Similarity, Contrast and Same.
The edge between two nodes representing a rela-
tion is directed in the case of asymmetric relations
such as Cause and Condition and undirected for
symmetric relations like Similarity and Contrast.
2.3 Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) is theory-neutral and does not make
any assumptions about the form of the overall dis-
course structure of text. Instead, this approach fo-
cuses on local and lexically-triggered discourse re-
lations. Annotators identify explicit signals such
as discourse connectives: ?but?, ?because?, ?while?
and mark the text spans which they relate. The
relations between these spans are called explicit
relations. In addition, adjacent sentences in a dis-
course are also semantically related even in the ab-
sence of explicit markers. In the PDTB, these are
called implicit relations and are annotated between
adjacent sentences in the same paragraph.
For both implicit and explicit relations, senses
are assigned from a hierarchy containing four
top-level categories: Comparison (contrast, prag-
matic contrast, concession, pragmatic concession), Contin-
gency (cause, pragmatic cause, condition, pragmatic con-
dition) , Expansion (conjunction, instantiation, restate-
ment, alternative, exception, list) and Temporal (asyn-
chronous, synchronous). The top level senses are di-
vided into types and subtypes that represent more
fine grained senses?the second level senses are
listed in parentheses above.
PDTB also provides annotations for the text
spans of the two arguments (referred to Arg1 and
Arg2) involved in a relation. In explicit relations,
the argument syntactically bound to the discourse
connective is called Arg2. The other argument is
148
referred to as Arg1. For implicit relations, the ar-
gument occurring first in the text is named Arg1,
the one appearing later is called Arg2.
2.4 Human summaries
Human summaries are available for some of the
WSJ articles. These summaries are extractive: hu-
man judges identified and extracted important text
units from the source articles and used them as
such to compose the summary.
The RST corpus contains summaries for 150
documents. Two annotators selected the most im-
portant EDUs from these documents and created
summaries that contain about square root of the
number of EDUs in the source document. For
convenience, we adopt sentences as the common
unit for comparison across all frameworks. So,
we mapped the summary EDUs to the sentences
which contain them. Two variable length sum-
maries for each document were obtained in this
way. In some documents, it was not possible to
align EDUs automatically with gold standard sen-
tence boundaries given by the Penn Treebank and
these were not used in our work. We perform
our experiments on the remaining 124 document-
summary pairs. These documents consisted of
4,765 sentences in total, of which 1,152 were la-
beled as important sentences because they con-
tained EDUs selected by at least one annotator.
The Graph Bank corpus also contains human
summaries. However, only 15 are for documents
for which RST and PDTB annotations are also
available. These summaries were created by fif-
teen human annotators who ranked the sentences
in each document on a scale from 1 (low impor-
tance) to 7 (very important for a summary). For
each document, we ordered the sentences accord-
ing to the average rank from the annotators, and
created a summary of 100 words using the top
ranked sentences. The number of summary (im-
portant) sentences is 67, out of a total of 308 sen-
tences from the 15 documents.
3 Features for content selection
In this section, we describe two sets of discourse
features?structural and semantic. The structure
features are derived from RST trees and do not
involve specific relations. Rather they compute
the importance of a segment as a function of its
position in the global structure of the entire text.
On the other hand, semantic features indicate the
sense of a relation between two sentences and
do not involve structure information. We com-
pute these from the PDTB annotations. To un-
derstand the benefits of discourse information, we
also study the performance of some non-discourse
features standardly used in summarization.
3.1 Structural features: RST-based
Prior work in text summarization has developed
content selection methods using properties of the
RST tree: the nucleus-satellite distinction, notions
of salience and the level of an EDU in the tree.
In early work, Ono et al (1994) suggested
a penalty score for every EDU based on their
nucleus-satellite status. Since satellites of rela-
tions are considered less important than the corre-
sponding nuclei, spans that appear as satellites can
be assigned a lower score than the nucleus spans.
This intuition is implemented by Ono et al (1994)
as a penalty value for each EDU, defined as the
number of satellite nodes found on the path from
the root of the tree to that EDU. Figure 1 shows
the RST tree (Carlson et al, 2002) for the follow-
ing sentence which contains four EDUs.
1. [Mr. Watkins said] 2. [volume on Interprovincial?s sys-
tem is down about 2% since January] 3. [and is expected to
fall further,] 4. [making expansion unnecessary until perhaps
the mid-1990s.]
The spans of individual EDUs are represented
at the leaves of the tree. At the root of the tree, the
span covers the entire text. The path from EDU 1
to the root contains one satellite node. It is there-
fore assigned a penalty of 1. Paths to the root from
all other EDUs involve only nucleus nodes and
subsequently these EDUs do not incur any penalty.
Figure 1: RST tree for the example sentence in
Section 3.1.
Marcu (1998) proposed another method to uti-
lize the nucleus-satellite distinction, rewarding nu-
cleus status instead of penalizing satellite. He put
forward the idea of a promotion set, consisting of
149
salient/important units of a text span. The nu-
cleus is the more salient unit in the full span of
a mononuclear relation. In a multinuclear relation,
all the nuclei are salient units of the larger span.
For example, in Figure 1, EDUs 2 and 3 partici-
pate in a multinuclear (List) relation. As a result,
both EDUs 2 and 3 appear in the promotion set of
their combined span. The salient units (promotion
set) of each text span are shown above the horizon-
tal line which represents the span. At the leaves,
salient units are the EDUs themselves.
For the purpose of identifying important con-
tent, units in the promotion sets of nodes close to
the root are hypothesized to be more important
than those at lower levels. The highest promo-
tion of an EDU occurs at the node closest to the
root which contains that EDU in its promotion set.
The depth of the tree from the highest promotion
is assigned as the score for that EDU. Hence, the
closer to the root an EDU is promoted, the better
its score. Since EDUs 2, 3 and 4 are promoted all
the way up to the root of the tree, the score as-
signed to them is equal to 4, the total depth of the
tree. EDU 1 receives a depth score of 3.
However, notice that EDUs 2 and 3 are pro-
moted to the root from a greater depth than EDU
4 but all three receive the same depth score. But
an EDU promoted successively over multiple lev-
els should be more important than one which is
promoted fewer times. In order to make this dis-
tinction, a promotion score was also introduced by
Marcu (1998) which is a measure of the number
of levels over which an EDU is promoted. Now,
EDUs 2 and 3 receive a promotion score of three
while the score of EDU 4 is only two.
For our experiments, we use the nucleus-
satellite penalty, depth and promotion based scores
as features. Because all these scores depend on the
length of the document, another set of the same
features normalized by number of words in the
document are also included. The penalty/score for
a sentence is computed as the maximum of the
penalties/scores of its constituent EDUs.
3.2 Semantic features: PDTB-based
These features represent sentences purely in terms
of the relations which they participate in. For each
sentence, we use the PDTB annotations to encode
the sense of the relation expressed by the sentence
and the type of realization (explicit or implicit).
For example, the sentence below expresses a
Contingency relation.
In addition, its machines are easier to operate, so cus-
tomers require less assistance from software.
For such sentences that contain both the argu-
ments of a relation ie., expresses the relation by
itself, we set the feature ?expresses relation?. For
the above sentence, the binary feature ?expresses
Contingency relation? would be true.
Alternatively, sentences participating in multi-
sentential relations will have one of the following
features on: ?contains Arg1 of relation? or ?con-
tains Arg2 of relation?. Therefore, for the follow-
ing sentences in an Expansion relation, we record
the feature ?contains Arg1 of Expansion relation?
for sentence (1) and for sentence (2), ?contains
Arg2 of Expansion relation?.
(1) Wednesday?s dominant issue was Yasuda &Marine In-
surance, which continued to surge on rumors of speculative
buying. (2) It ended the day up 80 yen to 1880 yen.
We combine the implicit/explicit type distinc-
tion of the relations with the other features de-
scribed so far, doubling the number of features.
We also added features that use the second level
sense of a relation. So, the relevant features for
sentence (1) above would be ?contains Arg1 of
Implicit Expansion relation? as well as ?contains
Arg1 of Implicit Restatement relation? (Restate-
ment is a type of Expansion relation (Section 2.3)).
In addition, we include features measuring the
number of relations shared by a sentence (implicit,
explicit and total) and the distance between argu-
ments of explicit relations (the distance of Arg1
when the sentence contains Arg2).
3.3 Non-discourse features
We use standard non-discourse features used in
summarization: length of the sentence, whether
the sentence is paragraph initial or the first sen-
tence of a document, and its offsets from docu-
ment beginning as well as paragraph beginning
and end (Edmundson, 1969). We also include the
average, sum and product probabilities of the con-
tent words appearing in sentences (Nenkova et al,
2006) and the number of topic signature words in
the sentence (Lin and Hovy, 2000).
4 Predictive power of features
We used the human summaries from the RST cor-
pus to study which features strongly correlate with
the important sentences selected by humans. For
binary features such as ?does the sentence con-
150
tain a Contingency relation?, a chi-square test was
computed to measure the association between a
feature and sentence class (in summary or not in
summary). For real-valued features, comparison
between important and unimportant/non-summary
sentences was done using a two-sided t-test. The
significant features from our different classes are
reported in the Appendix?Tables 5, 6 and 7. A
brief summary of the results is provided below.
Significant features that have higher values for
sentences selected in a summary are:
Structural: depth score and promotion score?both normal-
ized and unnormalized.
Semantic-PDTB-level11: contains Arg1 of Explicit Expan-
sion, contains Arg1 of Implicit Contingency, contains Arg1
of Implicit Expansion, distance of other argument
Non-discourse: length, is the first sentence in the article, is
the first sentence in the paragraph, offset from paragraph end,
number of topic signature terms present, average probability
of content words, sum of probabilities of content words
Significant features that have higher values for
sentences not selected in a summary are:
Structural: Ono penalty?normalized and unnormalized.
Semantic-PDTB-level1: expresses Explicit Expansion, ex-
presses Explicit Contingency, contains Arg2 of Implicit Tem-
poral relation, contains Arg2 of Implicit Contingency, con-
tains Arg2 of Implicit Expansion, contains Arg2 of Implicit
Comparison, number of shared implicit relations, total shared
relations
Non-discourse: offset from paragraph beginning, offset
from article beginning, sentence probability based on content
words.
All the structural features prove to be strong in-
dicators for content selection. RST depth and pro-
motion scores are higher for important sentences.
Unimportant sentences have high penalties.
On the other hand, note that most of the sig-
nificant sense features are descriptive of the ma-
jority class of sentences?those not important or
not selected to appear in the summary (refer Ta-
ble 7). For example, the second arguments of
all the first level implicit PDTB relations are not
preferred in human summaries. Most of the sec-
ond level sense features also serve as indicators
for what content should not be included in a sum-
mary. Such features can be used to derive con-
straints on what content is not important, but there
are only few indicators associated with important
sentences. Overall, out of the 25 first and second
1Features based on the PDTB level 1 senses. The signif-
icant features based on the level 2 senses are reported in the
appendix.
level sense features which turned out to be signifi-
cantly related to a sentence class, only 8 are those
indicative of important content.
Another compelling observation is that highly
cognitively salient discourse relations such as
Contrast and Cause are not indicative of important
sentences. Of the features that indicate the occur-
rence of a particular relation in a sentence, only
two are significant, but they are predictive of non-
important sentences. These are ?expresses Ex-
plicit Expansion? (also subtypes Conjunction and
List) and ?expresses Explicit Contingency?.
An additional noteworthy fact is the differences
between implicit and explicit relations that hold
across sentences. For implicit relations, the tests
show a strong indication that the second arguments
of Implicit Contingency or Expansion would not
be included in a summary, their first arguments
however are often important and likely to appear
in a summary. At the same time, for explicit rela-
tions, there is no regularity for any of the relations
of which of the two arguments is more important.
All the non-discourse features turned out highly
significant (Table 6). Longer sentences, those in
the beginning of an article or its paragraphs and
sentences containing frequent content words are
preferred in human summaries.
5 Classification performance
We now test the strengths and complementary be-
havior of these features in a classification task to
predict important sentences from input texts.
5.1 Comparison of feature classes
Table 1 gives the overall accuracy, as well as pre-
cision and recall for the important/summary sen-
tences. Features classes were combined using lo-
gistic regression. The reported results are from 10-
fold cross-validation runs on sentences from the
124 WSJ articles for which human summaries are
available in the RST corpus. For the classifier us-
ing sense information from the PDTB, all the fea-
tures described in Section 3.2 were used.
The best class of features turn out to be the
structure-based ones. They outperform both non-
discourse (ND) and sense features by a large mar-
gin. F-measure for the RST-based classifier is
33.50%. The semantic type of relations, on the
other hand, gives no indication of content impor-
tance obtaining an F-score of only 9%. Non-
discourse features provide an F-score of 19%,
151
which is much better than the semantic class but
still less than structural discourse features.
The structure and semantic features are com-
plementary to each other. The performance of
the classifier is substantially improved when both
types of features are used (line 6 in Table 1). The
F-score for the combined classifier is 40%, which
amounts to 7% absolute improvement over the
structure-only classifier.
Discourse information is also complementary
to non-discourse. Adding discourse structure
or sense features to non-discourse (ND) features
leads to better classification decisions (lines 4, 5
in Table 1). Particularly notable is the improve-
ment when sense and non-discourse features are
combined?over 10% better F-score than the classi-
fier using only non-discourse features. The overall
best classifier is the combination of discourse?
structure as well as sense?and non-discourse fea-
tures. Here, recall for important sentences is 34%
and the precision of predictions is 62%.
We also evaluated the features using ROUGE
(Lin and Hovy, 2003; Lin, 2004). ROUGE com-
putes ngram overlaps between human reference
summaries and a given system summary. This
measure allows us to compare the human sum-
maries and classifier predictions at word level
rather than using full sentence matches.
To perform ROUGE evaluation, summaries for
our different classes of features were obtained as
follows. Important sentences for each document
were predicted using a logistic regression classi-
fier trained on all other documents. When the
number of sentences predicted to be important
was not sufficient to meet the required summary
length, sentences predicted with lowest confidence
to be non-important were selected. All summaries
were truncated to 100 words. Stemming was used,
and stop words were excluded from the calcula-
tion. Both human extracts were used as references.
The results from this evaluation are shown in
Table 2. They closely mirror the results obtained
using precision and recall. The sense features per-
form worse than the structural and non-discourse
features. The best set of features is the one com-
bining structure, sense and non-discourse features,
with ROUGE-1 score (unigram overlap) of 0.479.
Overall, combining types of features considerably
improves results in all cases. However, unlike
in the precision and recall evaluation, structural
and non-discourse features perform very similarly.
Features used Acc P R F
structural 78.11 63.38 22.77 33.50
semantic 75.53 44.31 5.04 9.05
non-discourse (ND) 77.25 67.48 11.02 18.95
ND + semantic 77.38 59.38 20.62 30.61
ND + structural 78.51 63.49 26.05 36.94
semantic + structural 77.94 58.39 30.47 40.04
structural + semantic + ND 78.93 61.85 34.42 44.23
Table 1: Accuracy (Acc) and Precision (P), Recall
(R) and F-score (F) of important sentences.
Features ROUGE Features ROUGE
structural + semantic + ND 0.479 ND 0.432
structural + ND 0.468 LEAD 0.411
structural + semantic 0.453 semantic 0.369
semantic + ND 0.444 TS 0.338
structural 0.433
Table 2: ROUGE-1 recall scores
Their ROUGE-1 recall scores are 0.433 and 0.432
respectively. The top ranked sentences by both
sets of features appear to contain similar content.
We also evaluated sentences chosen by two
baseline summarizers. The first, LEAD, includes
sentences from the beginning of the article up to
the word limit. This simple method is a very com-
petitive baseline for single document summariza-
tion. The second baseline ranks sentences based
on the proportion of topic signature (TS) words
contained in the sentences (Conroy et al, 2006).
This approach leads to very good results in identi-
fying important content for multi-document sum-
maries where there is more redundancy, but it is
the worst when measured by ROUGE-1 on this
single document task. Structure and non-discourse
features outperform both these baselines.
5.2 Tree vs. graph discourse structure
Wolf and Gibson (2004) showed that the Graph
Bank annotations of texts can be used for sum-
marization with results superior to that based on
RST trees. In order to derive the importance of
sentences from the graph representation, they use
the PageRank algorithm (Page et al, 1998). These
scores, similar to RST features, are based only on
the link structure; the semantic type of the relation
linking the sentences is not used. In Table 3, we
report the performance of structural features from
RST and Graph Bank on the 15 documents with
overlapping annotations from the two frameworks.
As discussed by Wolf and Gibson (2004), we
find that the Graph Bank discourse representation
(GB) leads to better sentence choices than using
RST trees. The F-score is 48% for the GB clas-
152
Features Acc P R F ROUGE
RST-struct. 81.61 63.00 31.56 42.05 0.569
GB-struct. 82.58 62.50 39.16 48.15 0.508
Table 3: Tree vs graph-based discourse features
sifier and 42% for the RST classifier. The better
performance of GB method comes from higher re-
call scores compared to RST. Their precision val-
ues are comparable. But, in terms of ngram-based
ROUGE scores, the results from RST (0.569)
turn out slightly better than GB (0.508). Over-
all, discourse features based on structure turn out
as strong indicators of sentence importance and
we find both tree and graph representations to be
equally useful for this purpose.
6 Lexical approximation to discourse
structure
In prior work on summarization, graph models of
text have been proposed that do not rely on dis-
course. Rather, lexical similarity between sen-
tences is used to induce graph structure (Erkan
and Radev, 2004; Mihalcea and Tarau, 2005).
PageRank-based computation of sentence impor-
tance have been used on these models with good
results. Now, we would like to see if the discourse
graphs from the Graph Bank (GB) corpus would
be more helpful for determining content impor-
tance than the general text graph based on lexi-
cal similarity (LEX). We perform this comparison
on the 15 documents that we used in the previous
section for evaluating tree versus graph structures.
We used cosine similarity to link sentences in the
lexical graph. Links with similarity less than 0.1
were removed to filter out weak relationships.
The classification results are shown in Table 4.
The similarity graph representation is even more
helpful than RST or GB: the F-score is 53% com-
pared to 42% for RST and 48% for GB. The most
significant improvement from the lexical graph is
in terms of precision 75% which is more than 10%
higher compared to RST and GB features. Using
ROUGE as the evaluation metric, the lexical sim-
ilarity graph, LEX (0.557), gives comparable per-
formance with both GB (0.508) and RST (0.569)
representations (refer Table 3). Therefore, for use
in content selection, lexical overlap information
appears to be a good proxy for building text struc-
ture in place of discourse relations.
Features Acc P R F ROUGE
LEX-struct. 83.23 75.17 41.14 53.18 0.557
Table 4: Performance of lexrank summarizer
7 Discussion
We have analyzed the contribution of different
types of discourse features?structural and seman-
tic. Our results provide strong evidence that dis-
course structure is the most useful aspect. Both
tree and graph representations of discourse can be
used to compute the importance of text units with
very good results. On the other hand, sense in-
formation from discourse does not provide strong
indicators of good content but some constraints
as to which content should not be included in
a summary. These sense features complement
structure information leading to improved perfor-
mance. Further, both these types of discourse fea-
tures are complementary to standardly used non-
discourse features for content selection.
However, building automatic parsers for dis-
course information has proven to be a hard task
overall (Marcu, 2000; Soricut and Marcu, 2003;
Wellner et al, 2006; Sporleder and Lascarides,
2008; Pitler et al, 2009) and the state of cur-
rent parsers might limit the benefits obtainable
from discourse. Moreover, discourse-based struc-
ture is only as useful for content selection as sim-
pler text structure built using lexical similarity.
Even with gold standard annotations, the perfor-
mance of structural features based on the RST
and Graph Bank representations is not better than
that obtained from automatically computed lexical
graphs. So, even if robust discourse parsers exist
to use these features on other test sets, it is not
likely that discourse features would provide better
performance than lexical similarity. Therefore, for
content selection in summarization, current sys-
tems can make use of simple lexical structures to
obtain similar performance as discourse features.
But it should be remembered that summary
quality does not depend on content selection per-
formance alone. Systems should also produce lin-
guistically well formed summaries and currently
systems perform poorly on this aspect. To address
this problem, discourse information is vital. The
most comprehensive study of text quality of au-
tomatically produced summaries was performed
by Otterbacher et al (2002). A collection of 15
automatically produced summaries was manually
edited in order to correct any problems. The study
153
found that discourse and temporal ordering prob-
lems account for 34% and 22% respectively of all
the required revisions. Therefore, we suspect that
for building summarization systems, most benefits
from discourse can be obtained with regard to text
quality compared to the task of content selection.
We plan to focus on this aspect of discourse use
for our future work.
References
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of SIGdial, pages 1?10.
L. Carlson, D. Marcu, andM. E. Okurowski. 2002. Rst
discourse treebank. Corpus number LDC 2002T07,
Linguistic Data Consortium, Philadelphia.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
ACL.
H.P. Edmundson. 1969. New methods in automatic
extracting. Journal of the ACM, 16(2):264?285.
G. Erkan and D. Radev. 2004. Lexrank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
A. Lee, R. Prasad, A. Joshi, and B. Webber. 2008. De-
partures from Tree Structures in Discourse: Shared
Arguments in the Penn Discourse Treebank. In Pro-
ceedings of the Constraints in Discourse Workshop.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING, pages 495?501.
C. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In Proceedings of the ACL
Workshop on Automatic Summarization.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In Proceedings of ACL Text
Summarization Workshop.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. Text, 8.
D. Marcu. 1998. To build text summaries of high qual-
ity, nuclearity is not sufficient. In Working Notes
of the the AAAI-98 Spring Symposium on Intelligent
Text Summarization, pages 1?8.
D. Marcu. 2000. The rhetorical parsing of unrestricted
texts: A surface-based approach. Computational
Linguistics, 26(3):395?448.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
R. Mihalcea and P. Tarau. 2005. An algorithm for
language independent single and multiple document
summarization. In Proceedings of IJCNLP.
A. Nenkova, L. Vanderwende, and K. McKeown.
2006. A compositional context sensitive multi-
document summarizer: exploring the factors that in-
fluence summarization. In Proceedings of SIGIR.
K. Ono, K. Sumita, and S. Miike. 1994. Abstract gen-
eration based on rhetorical structure extraction. In
Proceedings of COLING, pages 344?348.
J.C. Otterbacher, D.R. Radev, and A. Luo. 2002. Revi-
sions that improve cohesion in multi-document sum-
maries: a preliminary study. In Proceedings of ACL
Text Summarization Workshop, pages 27?36.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In Proceedings of ACL-IJCNLP, pages 683?
691.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
LREC.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
V.R. Uzda, T.A.S. Pardo, and M.G. Nunes. 2008.
Evaluation of automatic text summarization meth-
ods based on rhetorical structure theory. Intelligent
Systems Design and Applications, 2:389?394.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Saur??. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of SIGdial,
pages 117?125.
F. Wolf and E. Gibson. 2004. Paragraph-, word-, and
coherence-based approaches to sentence ranking: A
comparison of algorithm and human performance.
In Proceedings of ACL, pages 383?390.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational
Linguistics, 31(2):249?288.
154
Appendix: Feature analysis
This appendix provides the results from statistical tests for identifying predictive features from the dif-
ferent classes (RST-based structural features?Table 5, Non-discourse features?Table 6 and PDTB-based
sense features?Table 7).
For real-valued features, we performed a two sided t-test between the corresponding feature values
for important versus non-important sentences. For features which turned out significant in each set, the
value of the test statistic and significance levels are reported in the tables.
For binary features, we report results from a chi-square test to measure how indicative a feature is
for the class of important or non-important sentences. For results from the chi-square test, a (+/-) sign
is enclosed within parentheses for each significant feature to indicate whether the observed number of
times the feature was true in important sentences is greater (+) than the expected value (indication that
this feature is frequently associated with important sentences). When the observed frequency is less than
the expected value, a (-) sign is appended.
RST Features t-stat p-value
Ono penalty -21.31 2.2e-16
Depth score 16.75 2.2e-16
Promotion score 16.00 2.2e-16
Normalized penalty -11.24 2.2e-16
Normalized depth score 17.24 2.2e-16
Normalized promotion score 14.36 2.2e-16
Table 5: Significant RST-based features
Non-discourse features t-stat p-value
Sentence length 3.14 0.0017
Average probability of content words 9.32 2.2e-16
Sum probability of content words 11.83 2.2e-16
Product probability of content words -5.09 3.8e-07
Number of topic signature terms 9.47 2.2e-16
Offset from article beginning -12.54 2.2e-16
Offset from paragraph beginning -28.81 2.2e-16
Offset from paragraph end 7.26 5.8e-13
?
2 p-value
First sentence? 224.63 (+) 2.2e-16
Paragraph initial? 655.82 (+) 2.2e-16
Table 6: Significant non-discourse features
155
PDTB features t-stat p-value
No. of implicit relations involved -9.13 2.2e-16
Total relations involved -6.95 4.9e-12
Distance of Arg1 3.99 6.6e-05
Based on level 1 senses
?
2 p-value
Expresses explicit Expansion 12.96 (-) 0.0003
Expresses explicit Contingency 7.35 (-) 0.0067
Arg1 explicit Expansion 12.87 (+) 0.0003
Arg1 implicit Contingency 13.84 (+) 0.0002
Arg1 implicit Expansion 29.10 (+) 6.8e-08
Arg2 implicit Temporal 4.58 (-) 0.0323
Arg2 implicit Contingency 60.28 (-) 8.2e-15
Arg2 implicit Expansion 134.60 (-) 2.2e-16
Arg2 implicit Comparison 27.59 (-) 1.5e-07
Based on level 2 senses
?
2 p-value
Expresses explicit Conjunction 8.60 (-) 0.0034
Expresses explicit List 4.41 (-) 0.0358
Arg1 explicit Conjunction 10.35 (+) 0.0013
Arg1 implicit Conjunction 5.26 (+) 0.0218
Arg1 implicit Instantiation 18.94 (+) 1.4e-05
Arg1 implicit Restatement 15.35 (+) 8.9-05
Arg1 implicit Cause 12.78 (+) 0.0004
Arg1 implicit List 5.89 (-) 0.0153
Arg2 explicit Asynchronous 4.23 (-) 0.0398
Arg2 explicit Instantiation 10.92 (-) 0.0009
Arg2 implicit Conjunction 51.57 (-) 6.9e-13
Arg2 implicit Instantiation 12.08 (-) 0.0005
Arg2 implicit Restatement 28.24 (-) 1.1e-07
Arg2 implicit Cause 58.62 (-) 1.9e-14
Arg2 implicit Contrast 30.08 (-) 4.2e-08
Arg2 implicit List 12.31 (-) 1.9e-14
Table 7: Significant PDTB-based features
156
Workshop on Monolingual Text-To-Text Generation, pages 34?42,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34?42,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Text specificity and impact on quality of news summaries
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104
nenkova@seas.upenn.edu
Abstract
In our work we use an existing classifier to
quantify and analyze the level of specific and
general content in news documents and their
human and automatic summaries. We dis-
cover that while human abstracts contain a
more balanced mix of general and specific
content, automatic summaries are overwhelm-
ingly specific. We also provide an analysis of
summary specificity and the summary qual-
ity scores assigned by people. We find that
too much specificity could adversely affect the
quality of content in the summary. Our find-
ings give strong evidence for the need for a
new task in abstractive summarization: identi-
fication and generation of general sentences.
1 Introduction
Traditional summarization systems are primarily
concerned with the identification of important and
unimportant content in the text to be summarized.
Placing the focus on this distinction naturally leads
the summarizers to completely avoid the task of text-
to-text generation and instead just select sentences
for inclusion in the summary. In this work, we argue
that the general and specific nature of the content is
also taken into account by human summarizers; we
show that this distinction is directly related to the
quality of the summary and it also calls for the use
and refinement of text-to-text generation techniques.
General sentences are overview statements. Spe-
cific sentences supply details. An example general
and specific sentence from different parts of a news
article are shown in Table 1.
[1] The first shock let up as the eye of the storm moved
across the city.
[2] The National Hurricane Center in Miami reported its
position at 2 a.m. Sunday at latitude 16.1 north, longitude
67.5 west, about 140 miles south of Ponce, Puerto Rico,
and 200 miles southeast of Santo Domingo.
Table 1: General (in italics) and specific sentences
Prior studies have advocated that the distinction
between general and specific content is relevant for
text summarization. Jing and McKeown (2000)
studied what edits people use to create summaries
from sentences in the source text. Two of the op-
erations they identify are generalization and specifi-
cation where the source content gets changed in the
summary with respect to specificity. In more recent
work, Haghighi and Vanderwende (2009) built a
summarization system based on topic models, where
both topics at general document level as well as
those at specific subtopic levels were learnt. The
underlying idea here is that summaries are gener-
ated by a combination of content from both these
levels. But since the preference for these two types
of content is not known, Haghighi and Vanderwende
(2009) use some heuristic proportions.
Many systems that deal with sentence compres-
sion (Knight and Marcu, 2002; McDonald, 2006;
Galley and McKeown, 2007; Clarke and Lapata,
2008) and fusion (Barzilay and McKeown, 2005;
Filippova and Strube, 2008), do not take into ac-
count the specificity of the original or desired sen-
tence. However, Wan et al (2008) introduce a gen-
eration task where a summary sentence is created
by combining content from a key (general) sentence
and its supporting sentences in the source. More
34
recently, Marsi et al (2010) manually annotated
the transformations between source and compressed
phrases and observe that generalization is a frequent
transformation.
But it is not known what distribution of general
and specific content is natural for summaries. In ad-
dition, an analysis of whether this aspect is related
to quality of the summary has also not been done so
far. We address this issue in our work, making use
of an accurate classifier to identify general and spe-
cific sentences that we have developed (Louis and
Nenkova, 2011).
We present the first quantitative analysis of gen-
eral and specific content in a large corpus of news
documents and human and automatic summaries
produced for them. Our findings reveal that human-
written abstracts have much more general content
compared to human and system produced extractive
summaries. We also provide an analysis of how this
difference in specificity is related to aspects of sum-
mary quality. We show that too much specificity
could adversely affect the quality of summary con-
tent. So we propose the task of creating general
sentences for use in summaries. As a starting point
in this direction, we discuss some insights into the
identification and generation of general sentences.
2 Data
We obtained news documents and their sum-
maries from the Document Understanding Confer-
ence (DUC) evaluations. We use the data from
2002 because they contain the three different types
of summaries we wish to analyze?abstracts and
extracts produced by people, and automatic sum-
maries. For extracts, the person could only select
complete sentences, without any modification, from
the input articles. When writing abstracts people
were free to write the summary in their own words.
We use data from the generic multi-document
summarization task. There were 59 input sets, each
containing 5 to 15 news documents on a topic. The
task is to provide a 200 word summary. Two human-
written abstracts and two extracts were produced for
each input by trained assessors at NIST. Nine au-
tomatic systems participated in the conference that
year and we have 524 automatic summaries overall.
3 General and specific sentences in news
Before we present our analysis of general and spe-
cific content in news summaries, we provide a brief
description of our classifier and some example pre-
dictions. Our classifier is designed to predict for a
given sentence, its class as general or specific.
As in our example in Table 1, a general sentence
hints at a topic the writer wishes to convey but does
not provide details. So a reader expects to see more
explanation and specific sentences satisfy this role.
We observed that certain properties are prominent
in general sentences. They either express a strong
sentiment, are vague or contain surprising content.
Accordingly our features were based on word speci-
ficity, language models, length of syntactic phrases
and the presence of polarity words. Just the words in
the sentences were also a strong indicator of general
or specific nature. But we found the combination of
all non-lexical features to provide the best accuracy
and is the setup we use in this work.
We trained our classifier on general and specific
sentences from news texts. Initially, we utilized ex-
isting annotations of discourse relations as training
data. This choice was based on our hypotheses that
discourse relations such as exemplification relate a
general with a specific sentence. Later, we verified
the performance of the classifier on human anno-
tated general and specific sentences, also from two
genre of news articles, and obtained similar and ac-
curate predictions. Detailed description of the fea-
tures and training data can be found in Louis and
Nenkova (2011).
Our classifier uses logistic regression and so apart
from hard prediction into general/specific classes,
we can also obtain a confidence (probability) mea-
sure for membership in a particular class. In our
tests, we found that for sentences where there is high
annotator agreement for placing in a particular class,
the classifier also produces a high confidence predic-
tion on the correct class. When the agreement was
not high, the classifier confidence was lower. In this
way, the confidence score indicates the level of gen-
eral or specific content. So for our experiments in
this paper, we choose to use the confidence score for
a sentence belonging to a class rather than the clas-
sification decision.
The overall accuracy of the classifier in binary
35
[G1] ?The crisis is not over?.
[G2] No casualties have been reported, but experts are concerned that a major eruption could occur soon.
[G3] Seismologists said the volcano had plenty of built-up magma and even more severe eruptions could come later.
[G4] Their predictions might be a false alarm ? the volcano may have done its worst already.
[S1] (These volcanoes ? including Mount Lassen in Shasta County, and Mount Rainier and Mount St. Helens in Washington, all
in the Cascade Range ? arise where one of the earth?s immense crust plates is slowly diving beneath another.); Pinatubo?s
last eruption, 600 years ago, is thought to have yielded at least as much molten rock ? half a cubic kilometer ? as Mount
St. Helens did when it erupted in 1980.
[S2] The initial explosions on Mount Pinatubo at 8:51 a.m. Wednesday sent a 10-mile-high mushroom cloud of swirling ash and
rock fragments into the skies over Clark Air Base, forcing the Air Force to evacuate hundreds of American volunteers who had
stayed behind to guard it and to tend sensitive communications equipment.
[S3] Raymundo Punongbayan, director of the Philippine Institute of Vulcanology and Seismology, said Friday?s blasts were part
of a single eruption, the largest since Mount Pinatubo awoke Sunday from its 600-year slumber.
Table 2: General (G) and specific (S) sentences from input d073b
classification is 75%. More accurate predictions are
made on the examples with high annotator agree-
ment reaching over 90% accuracy on sentences
where there was complete agreement between five
annotators. So we expect the predictions from the
classifier to be reliable for analysis in a task setting.
In Table 2, we show the top general and specific
sentences (ranked by the classifier confidence) for
one of the inputs, d073b, from DUC 2002. This in-
put contains articles about the volcanic eruption at
Mount Pinatubo. Here, the specific sentences pro-
vide a lot of details such as the time and impact of
the eruption, information about previous volcanoes
and about the people and organizations involved.
In the next section, we analyze the actual distri-
bution of specific and general content in articles and
their summaries for the entire DUC 2002 dataset.
4 Specificity analysis
For each text?input, human abstract, human extract
and automatic summary?we compute a measure of
specificity as follows. We use the classifier to mark
for each sentence the confidence for belonging to the
specific class. Each token in the text is assigned the
confidence level of the sentence it belongs to. The
average specificity of words is computed as the mean
value of the confidence score over all the tokens.
The histogram of this measure for each type of
text is shown in Figure 1.
For inputs, the average specificity of words ranges
between 50 to 80% with a mean value of 65%. So,
news articles tend to have more specific content than
generic but the distribution is not highly skewed to-
wards either of the extreme ends.
The remaining three graphs in Figure 1 represent
the amount of specific content in summaries for the
same inputs. Human abstracts, in contrast to the in-
puts, are spread over a wider range of specificity lev-
els. Some abstracts have as low as 40% specificity
and a few actually score over 80%. However, the
sharper contrast with inputs comes from the large
number of abstracts that have 40 to 60% specificity.
This trend indicates that abstracts contain more gen-
eral content compared to inputs. An unpaired two-
sided t-test between the specificity values of inputs
and abstracts confirmed that abstracts have signif-
icantly lower specificity. The mean value for ab-
stracts is 62% while for inputs it is 65%.
The results of the analysis are opposite for hu-
man extracts and system summaries. The mean
specificity value for human extracts is 72%, 10%
higher compared to abstractive summaries for the
same inputs. This difference is also statistically sig-
nificant. System-produced summaries also show a
similar trend as extracts but are even more heavily
biased towards specific content. There are even ex-
amples of automatic summaries where the average
specificity level reaches 100%. The mean specificity
value is 74% which turned out significantly higher
than all other types of texts, inputs and both types of
human summaries. So system summaries appear to
be overwhelmingly specific.
The first surprising result is the opposite charac-
teristics of human abstracts and extracts. While ab-
stracts tend to be more general compared to the in-
put texts, extracts are more specific. Even though
36
Figure 1: Specific content in inputs and summaries
both types of summaries were produced by people,
we see that the summarization method deeply influ-
ences the nature of the summary content. The task of
creating extractive summaries biases towards more
specific content. So it is obvious that systems which
mainly use extractive techniques would also create
very specific summaries. Further, since high speci-
ficity arises as a result of the limitations associated
with extractive techniques, perhaps, overly specific
content would be detrimental to summary quality.
We investigate this aspect in the next section.
5 Specificity and summary quality
In this section, we examine if the difference in speci-
ficity that we have observed is related to the per-
ceived quality of the summary. Haghighi and Van-
derwende (2009) report that their topic model based
system was designed to use both a general content
distribution and distributions of content for specific
subtopics. However, using the general distribution
yielded summaries with better content than using the
specific topics. Here we directly study the relation-
ship between specificity of system summaries and
their content and linguistic quality scores. We also
examine how the specificity measure is related to
the quality of specialized summaries where people
were explicitly told to include only general content
or only specific details in their summaries. For this
analysis, we focus on system produced summaries.
5.1 Content quality
At DUC, each summary is evaluated by human
judges for content and linguistic quality. The qual-
ity of content was assessed in 2002 by means of a
coverage score. The coverage score reflects the sim-
ilarity between content chosen in a system summary
and that which is present in a human-written sum-
mary for the same input. A human abstract is cho-
sen as the reference. It is divided into clauses and
for each of these clauses, judges decide how well it
is expressed by the system produced summary (as a
percentage value). The average extent to which the
system summary expresses the clauses of the human
summary is considered as the coverage score. So
these scores range between 0 and 1.
We computed the Pearson correlation between the
specificity of a summary and its coverage score, and
obtained a value of -0.16. The correlation is not very
high but it is significant (pvalue 0.0006). So speci-
ficity does impact content quality and more specific
content indicates decreased quality.
We have seen from our analysis in the previous
section that when people produce abstracts, they
keep a mix of general and specific content but the
abstracts are neither too general nor too specific. So
it is not surprising that the correlation value is not
very high. Further, it should be remembered that the
notion of general and specific is more or less inde-
pendent of the importance of the content itself. Two
summaries can have the same level of generality but
vary greatly in terms of the importance of the con-
tent present. So we performed an analysis to check
the contribution of generality to the content scores
in addition to the importance factor.
We combine a measure of content importance
37
Predictor Mean ? Stdev. ? t value p-value
(Intercept) 0.212 0.03 6.87 2.3e-11 *
rouge2 1.299 0.11 11.74 < 2e-16 *
avgspec -0.166 0.04 -4.21 3.1e-05 *
Table 3: Results from regression test
from the ROUGE automatic evaluation (Lin and
Hovy, 2003; Lin, 2004) with generality to predict
the coverage scores. We use the same reference as
used for the official coverage score evaluation and
compute ROUGE-2 which is the recall of bigrams of
the human summary by the system summary. Next
we train a regression model on our data using the
ROUGE-2 score and specificity as predictors of the
coverage score. We then inspected the weights learnt
in the regression model to identify the influence of
the predictors. Table 3 shows the mean values and
standard deviation of the beta coefficients. We also
report the results from a test to determine if the beta
coefficient for a particular predictor could be set to
zero. The p-value for rejection of this hypothesis
is shown in the last column and the test statistic is
shown as the ?t value?. We used the lm function in
the R toolkit1 to perform the regression.
From the table, we see that both ROUGE-2 and
average specificity of words (avgspec) turn out as
significant predictors of summary quality. Relevant
content is highly important as shown by the positive
beta coefficient for ROUGE-2. At the same time, it
is preferable to maintain low specificity, a negative
value is assigned to the coefficient for this predictor.
So too much specificity should be avoided by sys-
tems and we must find ways to increase the general-
ity of summaries. We discuss this aspect in Sections
6 and 7.
5.2 Linguistic quality
We have seen from the above results that maintain-
ing a good level of generality improves content qual-
ity. A related question is the influence of specificity
on the linguistic quality of a summary. Does the
amount of general and specific content have any re-
lationship with how clear a summary is to read? We
briefly examine this aspect here.
In DUC 2002 linguistic quality scores were only
mentioned as the number of errors in a summary,
not a holistic score. Moreover, it was specified as
1http://www.r-project.org/
ling score sums. avg specificity
1, 2 202 0.71
5 400 0.72
9, 10 79 0.77
Table 4: Number of summaries at extreme levels of lin-
guistic quality scores and their average specificity values
a range?errors between 1 and 5 receive the same
score. So we use another dataset for this analy-
sis only. We use the system summaries and their
linguistic quality scores from the TAC ?09 query
focused summarization task2. Each summary was
manually judged by NIST assessors and assigned a
score between 1 to 10 to reflect how clear it is to
read. The score combines multiple aspects of lin-
guistic quality such as clarity of references, amount
of redundancy, grammaticality and coherence.
Since these scores are on an integer scale, we do
not compute correlations. Rather we study the speci-
ficity, computed in the same manner as described
previously, of summaries at different score levels.
Here there were 44 inputs and 55 systems. In Table
4, we show the number of summaries and their av-
erage specificity for 3 representative score levels?
best quality (9 or 10), worst (1 or 2) and mediocre
(5). We only used summaries with more than 2 sen-
tences as it may not be reasonable to compare the
linguistic quality of summaries of very short lengths.
From this table, we see that the summaries with
greater score have a higher level of specificity. The
specificity of the best summaries (9, 10) are signifi-
cantly higher than that with medium and low scores
(two-sided t-test). This result is opposite to our find-
ing with content quality and calls attention to an im-
portant point. General sentences cannot stand alone
and need adequate support and details. But cur-
rently, very few systems even make an attempt to
organize their summaries. So overly general con-
tent and general content without proper context can
be detrimental to the linguistic quality. Such sum-
maries can appear uncontentful and difficult to read
as the example in Table 5 demonstrates. This sum-
mary has an average specificity of 0.45 and its lin-
guistic quality score is 1.
So we see an effect of specificity on both content
2http://www.nist.gov/tac/2009/
Summarization/update.summ.09.guidelines.
html
38
?We are quite a ways from that, actually.?
As ice and snow at the poles melt, the loss of their reflective surfaces leads to exposed land and water absorbing more heat.
It is in the middle of an area whose population?and electricity demands?are growing.
It was from that municipal utility framework, city and school officials say, that the dormitory project took root.
?We could offer such a plan in Houston next year if we find customer demand, but we have n?t gone to the expense of marketing the plan.?
?We get no answers.?
Table 5: Example general summary with poor linguistic quality
and linguistic quality though in opposite directions.
5.3 Quality of general and specific summaries
So far, we examined the effect of specificity on the
quality of generic summaries. Now, we examine
whether this aspect is related to the quality of sum-
maries when they are optimized to be either gen-
eral or specific content. We perform this analysis
on DUC 20053 data where the task was to create a
general summary for certain inputs. For others, a
specific summary giving details should be produced.
The definitions of a general and specific summary
are given in the task guidelines.4
We tested whether the degree of specificity is re-
lated to the content scores5 of system summaries of
these two types?general and specific. The Pearson
correlation values are shown in Table 6. Here we
find that for specific summaries, the level of speci-
ficity is significantly positively correlated with con-
tent scores. For the general summaries there is no
relationship between specificity and content quality.
These results show that specificity scores are
not consistently predictive of distinctions within the
same class of summaries. Within general sum-
maries, the level of generality does not influence the
scores obtained by them. This finding again high-
lights the disparity between content relevance and
specific nature. When all summaries are specific or
general, their levels of specificity are no longer in-
dicative of quality. We also computed the regres-
sion models for these two sets of summaries with
ROUGE scores and specificity, and specificity level
was not a significant predictor of content scores.
Our findings in this section confirm that general
sentences are useful content for summaries. So we
3http://duc.nist.gov/duc2005/
4http://duc.nist.gov/duc2005/assessor.
summarization.instructions.pdf
5We use the official scores computed using the Pyramid
evaluation method (Nenkova et al, 2007)
Summaries correlation p-value
DUC 2005 general -0.03 0.53
DUC 2005 specific 0.18* 0.004
Table 6: Correlations between content scores and speci-
ficity for general and specific summaries in DUC 2005
face the issue of creating general sentences which
are summary-worthy. We concentrate on this aspect
for the rest of this paper. In Section 6, we pro-
vide an analysis of the types of general sentences
extracted from the source text and used in human
extracts. We move from this limited view and exam-
ine in Section 7, the possibility of generating general
sentences from specific sentences in the source text.
Our analysis is preliminary but we hope that it will
initiate this new task of using general sentences for
summary creation.
6 Extraction of general sentences
We examine general sentences that were chosen in
human extracts to understand what properties sys-
tems could use to identify such sentences from the
source text. We show in Table 7, the ten extract sen-
tences that were predicted to be general with highest
confidence. The first sentence has a 0.96 confidence
level, the last sentence has 0.81.
These statements definitely create expectation and
need further details to be included. Taken out of con-
text, these sentences do not appear very contentful.
However despite the length restriction while creat-
ing summaries, humans tend to include these gen-
eral sentences. Table 8 shows the full extract which
contains one of the general sentences ([9] ?Instead it
sank like the Bismarck.?).
When considered in the context of the extract, we
see clearly the role of this general sentence. It intro-
duces the topic of opposition to Bush?s nomination
for a defense secretary. Moreover, it provides a com-
parison between the ease with which such a propo-
sition could have been accepted and the strikingly
39
opposite situation that arose?the overwhelming re-
jection of the candidate by the senate. So sentence
[9] plays the role of a topic sentence. It conveys the
main point the author wishes to make in the sum-
mary and further details follow this sentence.
But given current content selection methods, such
sentences would rank very low for inclusion into
summaries. So the prediction of general sentences
could prove a valuable task enabling systems to se-
lect good topic sentences for their summaries. How-
ever, proper ordering of sentences will be necessary
to convey the right impact but this approach could
be a first step towards creating summaries that have
an overall theme rather than just the selection of sen-
tences with important content.
We also noticed some other patterns in the general
sentences chosen for extracts. A crude categoriza-
tion was performed on the 75 sentences predicted
with confidence above 0.65 and are shown below:
first sentence : 6 (0.08)
last sentence : 13 (0.17)
comparisons : 4 (0.05)
attributions : 14 (0.18)
A significant fraction of these general sentences
(25%) were used in the extracts to start and end
the summary, likely positions for topic sentences.
Some of these (5%) involve comparisons. We de-
tected these sentences by looking for the presence
of connectives such as ?but?, ?however? and ?al-
though?. The most overwhelming pattern is pres-
ence of quotations, covering 18% of the sentences
we examined. These quotations were identified us-
ing the words ?say?, ?says?, ?said? and the presence
of quotes. We can also see that three of the top 10
general sentences in Table 7 are quotes.
So far we have analyzed sentences chosen by
summary authors directly from the input articles.
In the next section, we analyze the edit operations
made by people while creating abstractive sum-
maries. Our focus is on the generalization operation
where specific sentences are made general. Such
a transformation would be the generation-based ap-
proach to obtain general sentences.
7 Generation of general sentences
We perform our analysis on data created for sen-
tence compression. In this line of work (Knight and
[1] Folksy was an understatement.
[2] ?Long live democracy?!
[3] The dogs are frequent winners in best of breed and
best of show categories.
[4] Go to court.
[5] Tajikistan was hit most hard.
[6] Some critics have said the 16-inch guns are outmoded
and dangerous.
[7] Details of Maxwell?s death are sketchy.
[8] ?Several thousands of people who were in the shelters
and the tens of thousands of people who evacuated inland
were potential victims of injury and death?.
[9] Instead it sank like the Bismarck.
[10] ?The buildings that collapsed did so because of a
combination of two things: very poor soil and very poor
structural design,? said Peter I. Yanev, chairman of EQE
Inc., a structural engineering firm in San Francisco.
Table 7: Example general sentences in humans extracts
Marcu, 2002; McDonald, 2006; Galley and McKe-
own, 2007), compressions are learnt by analyzing
pairs of sentences, one from the source text, the
other from human-written abstracts such that they
both have the same content. We use the sentence
pairs available in the Ziff-Davis Tree Alignment
corpus (Galley and McKeown, 2007). These sen-
tences come from the Ziff-Davis Corpus (Harman
and Liberman, 1993) which contains articles about
technology products. Each article is also associated
with an abstract. The alignment pairs are produced
by allowing a limited number of edit operations to
match a source sentence to one in the abstract. In
this corpus, alignments are kept between pairs that
have any number of deletions and upto 7 substitu-
tions. There are 15964 such pairs in this data. It is
worth noting that these limited alignments only map
25% of the abstract sentences, so they do not cover
all the cases. Still, an analysis on this data could be
beneficial to observe the trends.
We ran the classifier individually on each source
sentence and abstract sentence in this corpus. Then
we counted the number of pairs which undergo each
transformation such as general-general, general-
specific from the source to an abstract sentence.
These results are reported in Table 9. The table also
provides the average number of deletion and substi-
tution operations associated with sentence pairs in
that category as well as the length of the uncom-
pressed sentence and the compression rate. Com-
pression rate is defined as the ratio between the
40
Summary d118i-f:
- President-elect Bush designated Tower as his defense secretary on Dec. 16. [Specific]
- Tower?s qualifications for the job ?intelligence, patriotism and past chairmanship of the Armed Services Committee ?the nomination
should have sailed through with flying colors. [Specific]
- Instead it sank like the Bismarck. [General]
- In written testimony to the Senate panel on Jan. 26, Tower said he could ?recall no actions in connection with any defense activities?
in connection with his work for the U.S. subsidiary. [Specific]
- Tower has acknowledged that he drank excessively in the 1970s, but says he has reduced his intake to wine with dinner. [General]
- The Democratic-controlled Senate today rejected the nomination of former Texas Sen. John Tower as defense secretary, delivering
a major rebuke to President Bush just 49 days into his term.[Specific]
- The Senate?s 53-47 vote came after a bitter and divisive debate focused on Tower?s drinking habits, behavior toward women and his
business dealings with defense contractors. [General]
Table 8: Example extract with classifier predictions and a general sentence from Table 7
Type Total % total Avg deletions Avg subs. Orig length Compr. rate
SS 6371 39.9 16.3 3.9 33.4 56.6
SG 5679 35.6 21.4 3.7 33.5 40.8
GG 3562 22.3 9.3 3.3 21.5 60.8
GS 352 2.2 8.4 4.0 22.7 66.0
Table 9: Types of transformation of source into abstract sentences
length in words of the compressed sentence and the
length of the uncompressed sentence. So lower com-
pression rates indicate greater compression.
We find that the most frequent transformations are
specific-specific (SS) and specific-general (SG). To-
gether they constitute 75% of all transformations.
But for our analysis, the SG transformation is most
interesting. One third of the sentences in this data
are converted from originally specific content to be-
ing general in the abstracts. So abstracts do tend to
involve a lot of generalization.
Studying the SG transition in more detail, we can
see that the original sentences are much longer com-
pared to other transitions. This situation arises from
the fact that specific sentences in this corpus are
longer. In terms of the number of deletions, we see
that both SS and SG involve more than 15 deletions,
much higher than that performed on the general sen-
tences. However, we do not know if these operations
are proportional to the original length of the sen-
tences. But looking at the compression rates, we get
a clearer picture, the SG sentences after compres-
sion are only 40% their original length, the maxi-
mum compression seen for the transformation types.
For GG and GS, about 60% of the original sentence
words are kept. For the SG transition, long sentences
are chosen and are compressed aggressively. In Ta-
ble 10, we show some example sentence pairs un-
dergoing the SG transition.
Currently, compression systems do not achieve
the level of compression in human abstracts. Sen-
tences that humans create are shorter than what sys-
tems produce. Our results predict that these could be
the cases where specific sentences get converted into
general. One reason why systems do not attain this
compression level could be because they only con-
sider a limited set of factors while compressing, such
as importance and grammaticality. We believe that
generality can be an additional objective which can
be used to produce even shorter sentences which we
have seen in our work, will also lead to summaries
with better content.
8 Conclusion
In this work, we have provided the first quantitative
analysis of general and specific content as relevant
to the task of automatic summarization. We find
that general content is useful for summaries how-
ever, current content selection methods appear to not
include much general content. So we have proposed
the task of identifying general content which could
be used in summaries. There are two ways of achiev-
ing this?by identifying relevant general sentences
from the input and by conversion from specific to
41
[1] American Mitac offers free technical support for one year at a toll-free number from 7:30 to 5:30 P.S.T.
American Mitac offers toll-free technical support for one year.
[2] In addition to Yurman, several other government officials have served on the steering committee that formed the group.
Several government officials also served on the steering committee.
[3] All version of the new tape drives, which, according to Goldbach, offer the lowest cost per megabyte for HSC-based 8mm tape
storage, are available within 30 days of order.
The products are available within 30 days of order.
[4] In a different vein is Edward Tufte ?s ?The Visual Display of Quantitative Information? (Graphics Press, 1983), a book covering
the theory and practice of designing statistical charts, maps, tables and graphics.
Tufte ?s book covers the theory and practice of designing statistical charts, maps, tables and graphics.
[5] In addition, Anderson said two Ada 9X competitive procurements?a mapping and revision contract and an implementation and
demonstration contract?will be awarded in fiscal 1990.
Two competitive procurements will be awarded in fiscal 1989.
Table 10: Example specific to general (in italics) compressions
general content. We have provided a brief overview
of these two approaches.
Our work underscores the importance of com-
pression and other post-processing approaches over
extractive summaries. Otherwise system content
could contain too much extraneous details which
take up space where other useful content could have
been discussed.
Our study also highlights a semantic view of sum-
mary creation. Summaries are not just a bag of im-
portant sentences as viewed by most methods today.
Rather a text should have a balance between sen-
tences which introduce a topic and those which dis-
cuss them in detail. So another approach to content
selection could be the joint selection of a general
sentence with its substantiation. In future work, it
would be interesting to observe if such summaries
are judged more responsive and of better linguistic
quality than summaries which do not have such a
structure.
References
R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3).
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
31(1):399?429.
K. Filippova and M. Strube. 2008. Sentence fusion
via dependency graph compression. In Proceedings
of EMNLP, pages 177?185.
M. Galley and K. McKeown. 2007. Lexicalized markov
grammars for sentence compression. In Proceedings
NAACL-HLT.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of NAACL-HLT, pages 362?370.
D. Harman and M. Liberman. 1993. Tipster complete.
Corpus number LDC93T3A, Linguistic Data Consor-
tium, Philadelphia.
H. Jing and K. McKeown. 2000. Cut and paste based
text summarization. In Proceedings of NAACL.
K. Knight and D. Marcu. 2002. Summarization beyond
sentence extraction: A probabilistic approach to sen-
tence compression. Artificial Intelligence, 139(1).
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of HLT-NAACL.
C. Lin. 2004. ROUGE: a package for automatic evalua-
tion of summaries. In ACL Text Summarization Work-
shop.
A. Louis and A. Nenkova. 2011. General versus spe-
cific sentences: automatic identification and applica-
tion to analysis of news summaries. Technical Re-
port No. MS-CIS-11-07, University of Pennsylvania
Department of Computer and Information Science.
E. Marsi, E. Krahmer, I. Hendrickx, and W. Daelemans.
2010. On the limits of sentence compression by dele-
tion. In E. Krahmer and M. Theune, editors, Empirical
methods in natural language generation, pages 45?66.
Springer-Verlag, Berlin, Heidelberg.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic evidence. In EACL?06.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The pyramid method: Incorporating human content se-
lection variation in summarization evaluation. ACM
Trans. Speech Lang. Process., 4(2):4.
S. Wan, R. Dale, M. Dras, and C. Paris. 2008. Seed
and grow: augmenting statistically generated sum-
mary sentences using schematic word patterns. In Pro-
ceedings of EMNLP, pages 543?552.
42
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1?9,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Assessment of the Accuracy of Automatic Evaluation in Summarization
Karolina Owczarzak
Information Access Division
National Institute of Standards and Technology
karolina.owczarzak@gmail.com
John M. Conroy
IDA Center for Computing Sciences
conroy@super.org
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
hoa.dang@nist.gov
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
Automatic evaluation has greatly facilitated
system development in summarization. At the
same time, the use of automatic evaluation
has been viewed with mistrust by many, as its
accuracy and correct application are not well
understood. In this paper we provide an as-
sessment of the automatic evaluations used for
multi-document summarization of news. We
outline our recommendations about how any
evaluation, manual or automatic, should be
used to find statistically significant differences
between summarization systems. We identify
the reference automatic evaluation metrics?
ROUGE 1 and 2?that appear to best emu-
late human pyramid and responsiveness scores
on four years of NIST evaluations. We then
demonstrate the accuracy of these metrics in
reproducing human judgements about the rel-
ative content quality of pairs of systems and
present an empirical assessment of the rela-
tionship between statistically significant dif-
ferences between systems according to man-
ual evaluations, and the difference according
to automatic evaluations. Finally, we present a
case study of how new metrics should be com-
pared to the reference evaluation, as we search
for even more accurate automatic measures.
1 Introduction
Automatic evaluation of content selection in sum-
marization, particularly the ROUGE evaluation
toolkit (Lin and Hovy, 2003), has been enthusias-
tically adopted by researchers since its introduction
in 2003. It is now standardly used to report results in
publications; however we have a poor understanding
of the accuracy of automatic evaluation. How often
do we publish papers where we report an improve-
ment according to automatic evaluation, but never-
theless, a standard manual evaluation would have led
us to different conclusions? In our work we directly
address this question, and hope that our encouraging
findings contribute to a better understanding of the
strengths and shortcomings of automatic evaluation.
The aim of this paper is to give a better assessment
of the automatic evaluation metrics for content se-
lection standardly used in summarization research.
We perform our analyses on data from the 2008-
2011 Text Analysis Conference (TAC)1 organized
by the National Institute of Standards and Technol-
ogy (NIST). We choose these datasets because in
early evaluation initiatives, the protocol for manual
evaluation changed from year to year in search of
stable manual evaluation approaches (Over et al,
2007). Since 2008, however, the same evaluation
protocol has been applied by NIST assessors and we
consider it to be the model that automatic metrics
need to emulate.
We start our discussion by briefly presenting the
manual procedure for comparing systems (Section
2) and how these scores should be best used to iden-
tify significant differences between systems over a
given test set (Section 3). Then, we embark on our
discussion of the accuracy of automatic evaluation
and its ability to reproduce manual scoring.
To begin our analysis, we assess the accuracy of
common variants of ROUGE on the TAC 2008-2011
datasets (Section 4.1). There are two aspects of eval-
uation that we pay special attention to:
Significant difference Ideally, all system compar-
isons should be performed using a test for sta-
1http://www.nist.gov/tac/
1
tistical significance. As both manual metrics
and automatic metrics are noisy, a statistical
hypothesis test is needed to estimate the prob-
ability that the differences observed are what
would be expected if the systems are compa-
rable in their performance. When this proba-
bility is small (by convention 0.05 or less) we
reject the null hypothesis that the systems? per-
formance is comparable.
It is important to know if scoring a system via
an automatic metric will lead to conclusions
about the relative merits of two systems differ-
ent from what one would have concluded on the
basis of manual evaluation. We report very en-
couraging results, showing that automatic met-
rics rarely contradict manual metrics, and some
metrics never lead to contradictions. For com-
pleteness, given that most papers do not report
significance, we also compare the agreement
between manual and automatic metrics without
taking significance into account.
Type of comparison Established manual evalua-
tions have two highly desirable properties: (1)
they can tell apart good automatic systems from
bad automatic systems and (2) they can differ-
entiate automatic summaries from those pro-
duced by humans with high accuracy. Both
properties are essential. Obviously, choosing
the better system in development cycles is key
in eventually improving overall performance.
Being able to distinguish automatic from man-
ual summaries is a general sanity test 2 that any
evaluation adopted for wide use is expected to
pass?it is useless to report system improve-
ments when it appears that automatic methods
are as good as human performance3. As we will
see, there is no single ROUGE variant that has
both of these desirable properties.
Finally, in Section 5, we discuss ways to compare
other automatic evaluation protocols with the refer-
2For now, automatic systems do not have the performance
of humans, thus, the ability to distinguish between human and
automatically generated summaries is an exemplar of the wider
problem of distinguishing high quality summaries from others.
3Such anomalous findings, when using automatic evalua-
tion, have been reported for some summarization genres such
as summarization of meetings (Galley, 2006).
ence ROUGE metrics we have established. We de-
fine standard tests for significance that would iden-
tify evaluations that are significantly more accurate
than the current reference measures, thus warrant-
ing wider adoption for future system development
and reporting of results. As a case study we apply
these to the TAC AESOP (Automatically Evaluating
Summaries of Peers) task which called for the devel-
opment of novel evaluation techniques that are more
accurate than ROUGE evaluations.
2 Manual evaluation
Before automatic evaluation methods are developed,
it is necessary to establish a desirable manual eval-
uation which the automatic methods will need to re-
produce. The type of summarization task must also
be precisely specified?single- or multi-document
summarization, summarization of news, meetings,
academic articles, etc. Saying that an automatic
evaluation correlates highly with human judgement
in general, is disturbingly incomplete, as the same
automatic metric can predict some manual evalu-
ation scores for some summarization tasks well,
while giving poor correlation with other manual
scores for certain tasks (Lin, 2004; Liu and Liu,
2010).
In our work, we compare automatic metrics with
the manual methods used at TAC: Pyramid and Re-
sponsiveness. These manual metrics primarily aim
to assess if the content of the summary is appro-
priately chosen to include only important informa-
tion. They do not deal directly with the linguistic
quality of the summary?how grammatical are the
sentences or how well the information in the sum-
mary is organized. Subsequently, in the experiments
that we present in later sections, we do not address
the assessment of automatic evaluations of linguistic
quality (Pitler et al, 2010), but instead analyze the
performance of ROUGE and other related metrics
that aim to score summary content.
The Pyramid evaluation (Nenkova et al, 2007) re-
lies on multiple human-written gold-standard sum-
maries for the input. Annotators manually identify
shared content across the gold-standards regardless
of the specific phrasing used in each. The pyra-
mid score is based on the ?popularity? of informa-
tion in the gold-standards. Information that is shared
2
across several human gold-standards is given higher
weight when a summary is evaluated relative to the
gold-standard. Each evaluated summary is assigned
a score which indicates what fraction of the most
important information for a given summary size is
expressed in the summary, where importance is de-
termined by the overlap in content across the human
gold-standards.
The Responsiveness metric is defined for query-
focused summarization, where the user?s informa-
tion need is clearly stated in a short paragraph. In
this situation, the human assessors are presented
with the user query and a summary, and are asked
to assign a score that reflects to what extent the sum-
mary satisfies the user?s information need. There are
no human gold-standards, and the linguistic quality
of the summary is to some extent incorporated in the
score, because information that is presented in a con-
fusing manner may not be seen as relevant, while it
could be interpreted by the assessor more easily in
the presence of a human gold-standard. Given that
all standard automatic evaluation procedures com-
pare a summary with a set of human gold-standards,
it is reasonable to expect that they will be more accu-
rate in reproducing results from Pyramid evaluation
than results from Responsiveness judgements.
3 Comparing systems
Evaluation metrics are used to determine the rela-
tive quality of a summarization system in compari-
son to one or more systems, which is either another
automatic summarizer, or a human reference sum-
marizer. Any evaluation procedure assigns a score
to each summary. To identify which of the two sys-
tems is better, we could simply average the scores
of summaries produced by each system in the test
set, and compare these averages. This approach is
straightforward; however, it gives no indication of
the statistical significance of the difference between
the systems. In system development, engineers may
be willing to adopt new changes only if they lead
to significantly better performance that cannot be at-
tributed to chance.
Therefore, in order to define more precisely what
it means for a summarization system to be ?bet-
ter? than another for a given evaluation, we employ
statistical hypothesis testing comparisons of sum-
marization systems on the same set of documents.
Given an evaluation of two summarization systems
A and B we have the following:
Definition 1. We say a summarizer A ?signifi-
cantly outperforms? summarizer B for a given
evaluation score if the null hypothesis of the fol-
lowing paired test is rejected with 95% confidence.
Given two vectors of evaluation scores x and y,
sampled from the corresponding random vari-
ables X and Y, measuring the quality of sum-
marizer A and B, respectively, on the same col-
lection of document sets, with the median of x
greater than the median of y,
H0 : The median of X ? Y is 0.
Ha : The median of X ? Y is not 0.
We apply this test using human evaluation met-
rics, such as pyramid and responsiveness, as well as
automatic metrics. Thus, when comparing two sum-
marization systems we can, for example, say system
A significantly outperforms system B in responsive-
ness if the null hypothesis can be rejected. If the null
hypothesis cannot be rejected, we say system A does
not significantly perform differently than system B.
A complicating factor when the differences be-
tween systems are tested for significance, is that
some inputs are simply much harder to summarize
than others, and there is much variation in scores
that is not due to properties of the summarizers
that produced the summaries but rather properties of
the input text that are summarized (Nenkova, 2005;
Nenkova and Louis, 2008).
Given this variation in the data, the most appropri-
ate approach to assess significance in the difference
between system is to use paired rank tests such as
a paired Wilcoxon rank-sum test, which is equiva-
lent to the Mann-Whitney U test. In these tests, the
scores of the two systems are compared only for the
same input and ranks are used instead of the actual
difference in scores assigned by the evaluation pro-
cedures. Prior studies have shown that paired tests
for significance are indeed able to discover consid-
erably more significant differences between systems
than non-paired tests, in which the noise of input dif-
ficulty obscures the actual difference in system per-
3
formance (Rankel et al, 2011). For this paper, we
perform all testing using the Wilcoxon sign rank test.
4 How do we identify a good metric?
If we treat manual evaluation metrics as our gold
standard, then we require that a good automatic met-
ric mirrors the distinctions made by such a man-
ual metric. An automatic metric for summarization
evaluation should reliably predict how well a sum-
marization system would perform relative to other
summarizers if a human evaluation were performed
on the summaries. An automatic metric would hope
to answer the question:
Would summarizer A significantly outper-
form summarizer B when evaluated by a
human?
We address this question by evaluating how well
an automatic metric agrees with a human metric in
its judgements in the following cases:
? all comparisons between different summariza-
tion systems
? all comparisons between systems and human
summarizers.
Depending on the application, we may record the
counts of agreements and disagreements or we may
normalize these counts to estimate the probability
that an automatic evaluation metric will agree with a
human evaluation metric.
4.1 Which is the best ROUGE variant
In this section, we set out to identify which of the
most widely-used versions of ROUGE have highest
accuracy in reproducing human judgements about
the relative merits of pairs of systems. We exam-
ine ROUGE-1, ROUGE-2 and ROUGE-SU4. For
all experiments we use stemming and for each ver-
sion we test scores produced both with and without
removing stopwords. This corresponds to six differ-
ent versions of ROUGE that we examine in detail.
ROUGE outputs several scores including preci-
sion, recall, and an F-measure. However, the most
informative score appears to be recall as reported
when ROUGE was first introduced (Lin and Hovy,
2003). Given that in the data we work with, sum-
maries are produced for a specified length in word
s (and all summaries are truncated to the predefined
length), recall on the task does not allow for artifi-
cially high scores which would result by producing
a summary of excessive length.
The goal of our analysis is to identify which of the
ROUGE variants is most accurate in correctly pre-
dicting which of two participating systems is the bet-
ter one according to the manual pyramid and respon-
siveness scores. We use the data for topic-focused
summarization from the TAC summarization track
in 2008-20114.
Table 1 gives the overview of the 2008-2011 TAC
Summarization data, including the number of top-
ics and participants. For each topic there were four
reference (model) summaries, written by one of the
eight assessors; as a result, there were eight human
?summarizers,? but each produced summaries only
for half of the topics.
year topics automatic human references/
summarizers summarizers topic
2008 48 58 8 4
2009 44 55 8 4
2010 46 43 8 4
2011 44 50 8 4
Table 1: Data in TAC 2008-2011 Summarization track.
We compare each pair of participating systems
based on the manual evaluation score. For each pair,
we are interested in identifying the system that is
better. We consider both the case when an appropri-
ate test for statistical significance has been applied to
pick out the better system as well as the case where
simply the average scores of systems over the test set
are compared. The latter use of evaluations is most
common in research papers on summarization; how-
ever, in summarization system development, testing
for significance is important because a difference in
summarizer scores that is statistically significant is
much more likely to reflect a true difference in qual-
ity between the two systems.
Therefore, we look at agreement between
ROUGE and manual metrics in two ways:
? agreement about significant differences be-
tween summarizers, according to a paired
4In all these years systems also competed on producing up-
date summaries. We do not report results on this task for the
sake of simplifying the discussion.
4
Auto only Human-Automatic
Pyr Resp Pyr Resp
diff no diff contr diff no diff contr diff no diff contr diff no diff contr
r1m 91 59 0.85 87 51 1.34 91 75 0.06 91 100 0.45
r1ms 90 59 0.83 84 50 3.01 91 75 0.06 90 100 0.45
r2m 91 68 0.19 88 60 0.47 75 75 0.62 75 100 1.02
r2ms 88 72 0 84 62 0.65 73 75 1.56 72 100 1.95
r4m 91 64 0.62 87 56 0.91 82 75 0.43 82 100 0.83
r4ms 90 64 0.04 85 55 1.15 83 75 0.81 83 100 1.20
Table 2: Average percentage agreement between ROUGE and manual metrics about significant differences on TAC
2008-2011 data. r1 = ROUGE-1, r2 = ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; diff =
agreement on significant differences, no diff = agreement on lack of significant differences, contr = contradictions.
Auto only Human-Automatic
Pyr Resp Pyr Resp
metric sig all sig all sig all sig all
r1m 77 87 70 82 90 99 90 99
r1ms 77 88 69 80 90 98 90 98
r2m 81 89 75 83 75 94 75 94
r2ms 81 89 74 81 72 93 72 93
r4m 80 88 73 82 82 96 82 96
r4ms 79 89 71 81 83 96 83 96
Table 3: Average agreement between ROUGE and manual metrics on TAC 2008-2011 data. r1 = ROUGE-1, r2 =
ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; sig = agreement on significant differences, all
= agreement on all differences.
Wilcoxon test. No adjustments for multiple
comparisons are made.
? agreement about any differences between sum-
marizers (whether significant on not).
Agreements occur when the two evaluation met-
rics make the same distinction between System A
and System B: A is significantly better than B, A is
significantly worse than B, or A and B are not sig-
nificantly different from each other. Contradictions
occur when both metrics find a significant difference
between A and B, but in opposite directions; this is
a much more serious case than a mere lack of agree-
ment (i.e., when one metric says A and B are not
significantly different, and the other metric finds a
significant difference).
Table 2 shows the average percentage agreement
between ROUGE and Pyramid/Responsiveness
when it comes to identifying significant differences
or lack thereof. Column diff shows the recall
of significant differences between pairs of systems
(i.e., how many significant differences determined
by Pyramid/Responsiveness are found by ROUGE);
column no diff gives the recall of the cases where
there are no significant differences between two sys-
tems according to Pyramid/Responsiveness.
There are a few instances of contradictions, as
well, but their numbers are fairly small. ?Auto only?
refers to comparisons between automatic summariz-
ers only; ?Human-Automatic? refers to cases when
a human summarizer is compared to an automatic
summarizer. There are fewer human summarizers,
so there are fewer ?Human-Automatic? comparisons
than ?Auto only? ones.
There are a few exceptional cases where the hu-
man summarizer is not significantly better than the
automatic summarizers, even according to the man-
ual evaluation, which accounts for the uniform val-
ues in the ?no difference? column (this is proba-
bly because the comparison is performed for much
fewer test inputs).
Table 3 combines the number of agreements in
the ?difference? and ?no difference? columns from
Table 2 into the sig column, which shows accu-
racy: in checking system pairs for significant differ-
ences, in how many cases does ROUGE make the
same decision as the manual metric (there is/isn?t
a significant difference between A and B). Ta-
ble 3 also gives the number of agreements about
any differences between systems, not only those
that reached statistical significance; in other words,
agreements on system pairwise rankings. In both
5
tables we see that removing stopwords often de-
creases performance of ROUGE, although not al-
ways. Also, there is no clear winner in the ROUGE
comparison: while ROUGE-2 with stemming is the
best at distinguishing among automatic summariz-
ers, ROUGE-1 is the most accurate when it comes
to human?automatic comparisons. To reflect this,
we adopt both ROUGE-1 and ROUGE-2 (with stem-
ming, without removing stopwords) as our reference
automatic metrics for further comparisons.
Reporting pairwise accuracy of automatic evalua-
tion measures has several advantages over reporting
correlations between manual and automatic metrics.
In correlation analysis, we cannot obtain any sense
of how accurate the measure is in identifying statis-
tically significant differences. In addition, pairwise
accuracy is more interpretable than correlations and
gives some provisional indication about how likely
it is that we are drawing a wrong conclusion when
relying on automatic metric to report results.
Table 3 tells us that when statistical significance
is not taken into account, in 89% of cases ROUGE-
2 scores will lead to the same conclusion about the
relative merits of systems as the expensive Pyramid
evaluation. In 83% of cases the conclusions will
agree with the Responsiveness evaluation. The accu-
racy of identifying significant differences is worse,
dropping by about 10% for both Pyramid and Re-
sponsiveness.
Finally, we would like to get empirical estimates
of the relationship between the size of the difference
in ROUGE-2 scores between two systems and the
agreement between manual and ROUGE-2 evalua-
tion. The goal is to check if it is the case that if
one system scores higher than another by x ROUGE
points, then it would be safe to assume that a manual
evaluation would have led to the same conclusion.
Figure 1 shows a histogram of differences in
ROUGE-2 scores. The pairs for which this differ-
ence was significant are given in red and for those
where the difference is not significant are given in
blue. The histogram clearly shows that in general,
the size of improvement cannot be used to replace a
test for significance. Even for small differences in
ROUGE score (up to 0.007) there are about 15 pairs
out of 200 for which the difference is in fact signif-
icant according to Pyramid or Responsiveness. As
the difference in ROUGE-2 scores between the two
systems increases, there are more significant differ-
ences. For differences greater than 0.05, all differ-
ences are significant.
Figure 2 shows the histograms of differences in
ROUGE-2 scores, split into cases where the pairwise
ranking of systems according to ROUGE agrees
with manual evaluation (blue) and disagrees (red).
For score differences smaller than 0.013, about half
of the times ROUGE-2 would be wrong in identify-
ing which system in the pair is the better one accord-
ing to manual evaluations. For larger differences the
number of disagreements drops sharply. For this
dataset, a difference in ROUGE-2 scores of more
than 0.04 always corresponds to an improvement in
the same direction according to the manual metrics.
5 Looking for better metrics
In the preceding sections, we established that
ROUGE-2 is the best ROUGE variant for compar-
ing two automatic systems, and ROUGE-1 is best in
distinguishing between humans and machines. Ob-
viously, it is of great interest to develop even bet-
ter automatic evaluations. In this section, we out-
line a simple procedure for deciding if a new au-
tomatic evaluation is significantly better than a ref-
erence measure. For this purpose, we consider the
automatic metrics from the TAC 2011 AESOP task,
which called for the development of better automatic
metrics for summarization evaluation NIST ( 2011).
For each automatic evaluation metric, we estimate
the probability that it agrees with Pyramid or Re-
sponsiveness. Figure 3 gives the estimated proba-
bility of agreement with Pyramid and Overall Re-
sponsiveness for all AESOP 2011 metrics with an
agreement of 0.6 or more. The metrics are plot-
ted with error bars giving the 95% confidence in-
tervals for the probability of agreement with the
manual evaluations. The red-dashed line is the
performance of the reference automatic evaluation,
which is ROUGE-2 for machine only and ROUGE-
1 for comparing machines and human summariz-
ers. Metrics whose 95% confidence interval is be-
low this line are significantly worse (as measured
by the z-test approximation of a binomial test) than
the baseline. Conversely, those whose 95% con-
fidence interval is above the red line are signifi-
cantly better than the baseline. Thus, just ROUGE-
6
Figure 1: Histogram of the differences in ROUGE-2 score versus significant differences as determined by Pyramid
(left) or Responsiveness (right).
Figure 2: Histogram of the differences in ROUGE-2 score versus differences as determined by Pyramid (left) or
Responsiveness (right).
BE (the MINIPAR variant of ROUGE-BE), one of
NIST?s baselines for AESOP, significantly outper-
formed ROUGE-2 for predicting pyramid compar-
isons; and 4 metrics: ROUGE-BE, DemokritosGR2,
catholicasc1, and CLASSY1, all significantly out-
perform ROUGE-2 for predictiong responsiveness
comparisons. Descriptions of these metrics as well
as the other proposed metrics can be found in the
TAC 2011 proceedings (NIST, 2011).
Similarly, Figure 4 gives the estimated probabil-
ity when the comparison is made between human
and machine summarizers. Here, 10 metrics are sig-
nificantly better than ROUGE-1 in predicting com-
parisons between automatic summarization systems
and human summarizers in both pyramid and re-
sponsiveness. The ROUGE-SU4 and ROUGE-BE
baselines are not shown here but their performance
was approximately 57% and 46% respectively.
If we limit the comparisons to only those where
a significant difference was measured by Pyramid
and also Overall Responsiveness, we get the plots
given in Figure 5 for comparing automatic summa-
rization systems. (The corresponding plot for com-
parisons between machines and humans is omitted
as all differences are significant.) The results show
that there are 6 metrics that are significantly better
than ROUGE-2 for correctly predicting when a sig-
nificant difference in pyramid scores occurs, and 3
metrics that are significantly better than ROUGE-2
for correctly predicting when a significant difference
in responsiveness occurs.
6 Discussion
In this paper we provided a thorough assessment
of automatic evaluation in summarization of news.
We specifically aimed to identify the best variant
of ROUGE on several years of TAC data and dis-
covered that ROUGE-2 recall with stemming and
stopwords not removed, provides the best agreement
with manual evaluations. The results shed positive
light on the automatic evaluation, as we find that
ROUGE-2 agrees with manual evaluation in almost
90% of the case when statistical significance is not
computed, and about 80% when it is. However,
these numbers are computed in a situation where
many very different systems are compared?some
7
Figure 3: Pyramid and Responsiveness Agreement of AESOP 2011 Metrics for automatic summarizers.
Figure 4: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for all summarizers.
8
Figure 5: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for automatic
summarizers.
very good, others bad. We examine the size of dif-
ference in ROUGE score and identify that for differ-
ences less than 0.013 a large fraction of the conclu-
sions drawn by automatic evaluation will contradict
the conclusion drawn by a manual evaluation. Fu-
ture studies should be more mindful of these find-
ings when reporting results.
Finally, we compare several alternative automatic
evaluation measures with the reference ROUGE
variants. We discover that many new proposals are
better than ROUGE in distinguishing human sum-
maries from machine summaries, but most are the
same or worse in evaluating systems. The Basic El-
ements evaluation (ROUGE-BE) appears to be the
strongest contender for an automatic evaluation to
augment or replace the current reference.
References
Paul Over and Hoa Dang and Donna Harman. 2007.
DUC in context. Inf. Process. Manage. 43(6), 1506?
1520.
Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
occurrence Statistics. Proceeding of HLT-NAACL.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Impor-
tance. Proceeding of EMNLP, 364?372.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. Trans. Audio, Speech and Lang. Proc.,
187?196.
C.Y. Lin. 2004. Looking for a Few Good Metrics: Au-
tomatic Summarization Evaluation - How Many Sam-
ples are Enough? Proceedings of the NTCIR Work-
shop 4.
Ani Nenkova and Rebecca J. Passonneau and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing human content selection variation in summariza-
tion evaluation. TSLP 4(2).
Emily Pitler and Annie Louis and Ani Nenkova. 2010.
Automatic Evaluation of Linguistic Quality in Multi-
Document Summarization. Proceedings of ACL, 544?
554.
Ani Nenkova. 2005. Automatic Text Summarization of
Newswire: Lessons Learned from the Document Un-
derstanding Conference. AAAI, 1436?1441.
Ani Nenkova and Annie Louis. 2008. Can You Summa-
rize This? Identifying Correlates of Input Difficulty
for Multi-Document Summarization. ACL, 825?833.
Peter Rankel and John M. Conroy and Eric Slud and Di-
anne P. O?Leary. 2011. Ranking Human and Machine
Summarization Systems. Proceedings of EMNLP,
467?473.
National Institute of Standards and Technology.
2011. Text Analysis Workshop Proceedings
http://www.nist.gov/tac/publications/index.html.
9
Proceedings of the SIGDIAL 2014 Conference, pages 142?150,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Addressing Class Imbalance for Improved Recognition of Implicit
Discourse Relations
Junyi Jessy Li
University of Pennsylvania
ljunyi@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
In this paper we address the problem of
skewed class distribution in implicit dis-
course relation recognition. We examine
the performance of classifiers for both bi-
nary classification predicting if a particu-
lar relation holds or not and for multi-class
prediction. We review prior work to point
out that the problem has been addressed
differently for the binary and multi-class
problems. We demonstrate that adopting
a unified approach can significantly im-
prove the performance of multi-class pre-
diction. We also propose an approach that
makes better use of the full annotations
in the training set when downsampling is
used. We report significant absolute im-
provements in performance in multi-class
prediction, as well as significant improve-
ment of binary classifiers for detecting the
presence of implicit Temporal, Compari-
son and Contingency relations.
1 Introduction
Discourse relations holding between adjacent sen-
tences in text play an essential role in establishing
local coherence and contribute to the semantic in-
terpretation of the text. For example, the causal re-
lationship is helpful for textual entailment or ques-
tion answering while restatement and exemplifica-
tion are important for automatic summarization.
Predicting the type of implicit relations, which
are not signaled by any of the common explicit
discourse connectives such as because, however,
has proven to be a most challenging task in dis-
course analysis. The Penn Discourse Treebank
(PDTB) (Prasad et al., 2008) provided valuable
annotations of implicit relations. Most research to
date has focused on developing and refining lex-
ical and linguistically rich features for the task
(Pitler et al., 2009; Lin et al., 2009; Park and
Cardie, 2012). Mostly ignored remains the prob-
lem of addressing the highly skewed distribution
of implicit discourse relations. Only about 35% of
pairs of adjacent sentences in the PDTB are con-
nected by three of the four top level discourse re-
lation: 5% participate in Temporal relation, 10%
in Comparison (contrast) and 20% in Contingency
(causal) relations. The remaining pairs are con-
nected by the catch-all Expansion relation (40%)
or by some other linguistic devices (24%). Finer
grained relations of interest to particular applica-
tions account for increasingly smaller percentage
of the PDTB data.
Class imbalance is particularly problematic for
training a binary classifier to distinguish one rela-
tion from the rest. As we will show later, it also
impacts the performance of multi-class prediction
in which each pair of sentences is labeled with one
of the five possible relations.
All prior work has resorted to downsampling
the training data for binary classifiers to distin-
guish a particular relation and use the full train-
ing set for multi-class prediction. In this pa-
per we compare several methods for address-
ing the skewed class distribution during training:
downsampling, upsampling and computing fea-
ture weights and performing feature selection on
the unaltered full training data. A major motiva-
tion for our work is to establish if any of the alter-
natives to downsampling would prove beneficial,
because in downsampling most of the expensively
annotated data is not used in the model. In addi-
tion, we seek to align the treatment of data imbal-
ance for the binary and multi-class tasks. We show
that downsampling in general leads to the best pre-
diction accuracy but that the alternative models
provide complementary information and signifi-
cant improvement can be obtained by combining
both types of models. We also report significant
improvement of multi-class prediction accuracy,
142
achieved by using the alternative binary classifiers
to perform the task.
2 The Penn Discourse Treebank
In the PDTB, discourse relations are viewed as a
predicate with two arguments. The predicate is
the relation, the arguments correspond to the min-
imum spans of text whose interpretations are the
abstract objects between which the relation holds.
Consider the following example of a contrast rela-
tion. The italic and bold fonts mark the arguments
of the relation.
Commonwealth Edison said the ruling could force it to slash
its 1989 earnings by $1.55 a share. [Implicit = BY COM-
PARISON] For 1988, Commonwealth Edison reported
earnings of $737.5 million, or $3.01 a share.
For explicit relations, the predicate is marked by
a discourse connective that occurs in the text, i.e.
because, however, for example.
Implicit relations are marked between adjacent
sentences in the same paragraph. They are inferred
by the reader but are not lexically marked. Alter-
native lexicalizations (AltLex) are the ones where
there is a phrase in the sentence implying the rela-
tion but the phrase itself was not one of the explicit
discourse connectives. There are 16,224 and 624
examples of implicit and AltLex relations, respec-
tively, in the PDTB.
The sense of discourse relations in the PDTB
is organized in a three-tiered hierarchy. The four
top level relations are: Temporal (the two argu-
ments are related temporally), Comparison (con-
trast), Contingency (causal) and Expansion (one
argument is the expansion of the other and contin-
ues the context) (Miltsakaki et al., 2008). These
are the classes we focus on in our work.
Finally, 5,210 pairs of adjacent sentences were
marked as related by an entity relation (EntRel),
by virtue of the repetition of the same entity or
topic. EntRels were marked only if no other rela-
tion could be identified and they are not considered
a discourse relation, rather an alternative discourse
phenomena related to entity coherence (Grosz et
al., 1995). There are 254 pairs of sentences where
no discourse relation was identified (NoRel).
Pitler et al. (2008) has shown that performance
as high as 93% in accuracy can be easily achieved
for the explicit relations, because the connective it-
self is a highly informative feature. Efforts in iden-
tifying the argument spans have also yielded high
accuracies (Lin et al., 2014; Elwell and Baldridge,
2008; Ghosh et al., 2011).
However, in the absence of a connective, recog-
nizing non-explicit relations, which includes im-
plicit relations, alternative lexicalizations, entity
relation and no relation present, has proven to be a
real challenge. Prior work on supervised implicit
discourse recognition studied a wide range of fea-
tures including lexical, syntactic, verb classes, se-
mantic groups via General Inquirer and polarity
(Pitler et al., 2009; Lin et al., 2009). Park and
Cardie (2012) studied the combination of features
and achieved better performance with a different
combination for each individual relation. Meth-
ods for improving the sparsity of lexical represen-
tations have been proposed (Hernault et al., 2010;
Biran and McKeown, 2013), as well as web-driven
approaches which reduce the problem to explicit
relation recognition (Hong et al., 2012).
Remarkably, no prior work has discussed the
highly skewed class distribution of discourse re-
lation types. The tacitly adopted solution has been
to downsample the negative examples for one-vs-
all binary classification aimed at discovering if a
particular relation holds and keeping the full train-
ing set for multi-class prediction.
To highlight the problem, in Table 1 we show
the distribution of implicit relation classes in the
entire PDTB. In our work, we aim to develop clas-
sifiers to identify the four top-level relations listed
in the table
1
.
# of samples Percentage
Temporal 1038 4.3%
Comparison 2550 11.3%
Contingency 4532 20%
Expansion 9082 40%
Table 1: Distribution of implicit relations in the
PDTB.
3 Experimental settings
In our experiments, we used all non-explicit in-
stances in the PDTB sections 2-19 for training and
those in sections 20-24 for testing. Like most stud-
ies, we kept sections 0-1 as development set. In
order to ensure we have a large enough test set to
properly perform tests for statistical significance
over F scores and balanced accuracies, we did not
follow previous work (Lin et al., 2014; Park and
Cardie, 2012) that used only section 23 or sec-
tions 23-24 for testing. Also, the traditional rule
of thumb is to split the available data into training
1
The rest of the data are EntRel/NoRel.
143
and testing sets with 80%/20% ratio. Our choice
ensures that this is the case for all of the relations.
The only features that we use in our experiments
are production rules. We exclude features that oc-
cur fewer than five times in the training set. Pro-
duction rules are the state-of-the-art representation
for discourse relation recognition. This represen-
tation leads to only slightly lower results than a
system including a much larger variety of features
in the first end-to-end PDTB style discourse parser
(Lin et al., 2014) .
The production rule representation is based on
the constituency parse of the arguments and in-
cludes both syntactic and lexical information. A
production rule is the parent with an left-to-right
ordered list of all of its children in the parse tree
(for example, S?NP VP). All non-terminal nodes
are included as a parent, from the sentence head
to the part-of-speech of a terminal. Thus words
that occur in each sentence augmented with their
part of speech are part of the representation (for
example, NN?company), along with more gen-
eral structures of the sentence corresponding to
production rules with only non-terminals on the
right-hand side.
There are three features corresponding to a pro-
duction rule, tracking if the rule occurs in the parse
of first argument of the relation, in the second, or
in both.
Adopting this representation allows us to fo-
cus on the issue of class imbalance and how
the choices of tackling this problem affect even-
tual prediction performance. Our findings are
representation-independent and will most likely
extend to other representations.
We train and evaluate a binary classifier with
linear kernel using SVMLight
2
(Joachims, 1999)
for each of the four top level classes of relations:
Temporal, Comparison, Contingency and Expan-
sion. We used SVM-Multiclass
3
for standard mul-
tiway classification. We also develop and evaluate
two approaches for multiway classification for the
four classes plus the additional class of entity rela-
tion and no relation.
Due to the uneven distribution of classes, we use
precision, recall and f-measure to measure binary
prediction performance. For multiway classifica-
2
http://svmlight.joachims.org/
3
http://svmlight.joachims.org/svm multiclass.html
tion, we use the balanced accuracy (BAC):
BAC =
1
k
k
?
i=1
c
i
n
i
, (1)
where k is the number of relations to predict, c
i
is
the number of instances of relation i that are cor-
rectly predicted, n
i
is the total number of instances
of relation i.
Balanced accuracy (or averaged accuracy) has
a more intuitive interpretation than F-measure. It
is not dominated by the majority class as much as
standard accuracy is. For example for two classes,
in a dataset where one class makes up 90% of the
data, predicting the majority class has accuracy of
90% but balanced accuracy of 45%.
In testing, we keep the original distribution in-
tact and make predictions for all pairs of adjacent
sentences in the same paragraph that do not have
an explicit discourse relation
4
. In order to per-
form tests for statistical significance over F scores,
precision, recall and balanced accuracies, we ran-
domly partitioned the testing data into 10 groups.
We kept the data distribution in each group as
close as possible to the overall testing set. To com-
pare the performance of two different systems, a
paired t-test is performed over these 10 groups.
4 Why downsampling?
Binary classification As mentioned in the pre-
vious sections, in all prior work of supervised im-
plicit relation classification, the technique to cope
with highly skewed distribution for binary classi-
fication is to downsample the negative training in-
stances so that the sizes of positive and negative
classes are equal. The reason for doing so is that
the classifier can achieve high accuracy just by ig-
noring the small class, learning nothing and aways
predicting the larger class. We illustrate this ef-
fect in Table 2. Without downsampling, the only
reasonable F measure is achieved for Expansion
where the smaller class accounts for 40% of the
data. Note that with downsampling, the recogni-
tion of Expansion is also improved considerably.
Multiway classification In prior work multiway
classifiers are trained on all available training data.
As we just saw, however, this approach leads
4
Note the contrast with prior work where in some cases
EntRels are part of Expansion, or in some cases the perfor-
mance of methods is evaluated only on pairs of sentences
where a discourse relation holds, excluding EntRels, NoRels
or AltLexs.
144
All data Downsample
Temp. 0 (nan/0.0) 15.52 (8.8/65.4)
Comp. 2.17 (71.4/1.1) 27.65 (17.3/69.2)
Cont. 0.96 (100.0/0.5) 47.14 (34.5/74.5)
Exp. 44.27 (54.9/37.1) 55.42 (49.3/63.3)
Table 2: F measure (precision/recall) of binary
classification: including all of the data vs down-
sampling.
to poor results in identifying the core Temporal,
Comparison and Contingency discourse relations.
We propose an alternative approach to multi-class
prediction, based on binary one-against-all classi-
fiers for each of the four discourse relations, in-
cluding Expansion, trained using downsampling.
The intuition is that an instance of adjacent sen-
tences S
i
is assigned to a discourse relation R
j
if the binary classifier for R
j
recognizes S
i
as a
positive instance with confidence higher than that
of the classifiers for other relations. If none of
the binary classifiers recognizes the instance as a
positive example, the instance is assigned to class
EntRel/NoRel. This approach modifies the way
multi-class classifiers are normally constructed by
including downsampling and having special treat-
ment of the EntRel/NoRel class.
Specifically, we first use the four binary classi-
fiers C
j
for each relation j to get the confidence p
j
of instance i belonging to class j. We approximate
the confidence by the distance to the hyperplane
separating the two classes, which SVMLight pro-
vides. If at least one p
j
is greater than zero, assign
instance i the class k where the classifier confi-
dence is the highest. If none of the p
j
?s is greater
than zero, assign i to be the EntRel/NoRel class.
We show balanced accuracies of these two mul-
tiway classification methods in Table 3.
Multiway SVM One-Against-All
5-way 32.58 37.15
Table 3: Balanced accuracies for SVM-Multiclass
and one-against-all 5-way classification.
The one-against-all approach leads to 5% abso-
lute improvement in performance. A t-test anal-
ysis confirms that the difference is significant at
p < 0.05. Note that the improvement comes en-
tirely from acknowledging that skewed class dis-
tribution poses a problem for the task and by ad-
dressing the problem in the same way for binary
and multi-class prediction.
5 Using more data
Although downsampling gives much better per-
formance than simply including all of the origi-
nal data, it still appears to be an undesirable so-
lution because in essence it throws away much of
the annotated data. This means that for the small-
est relations, as much as 90% of the data will
not be used. Feature selection and feature val-
ues are computed only based on this much smaller
dataset and do not properly reflect the information
about discourse relations encoded in the PDTB. In
this section we first discuss some of the widely
used methods for handling skewed data distribu-
tion, that is, weighted cost and upsampling. First,
we show that with highly skewed distributions, the
two methods result in almost identical classifiers.
Then we introduce a method for feature selection
and shaping which computes feature weights on
the full dataset and thus captures much of the in-
formation lost in downsampling.
5.1 Weighted cost and upsampling
A number of methods have been developed for
the skewed distribution problem (Morik et al.,
1999; Veropoulos et al., 1999; Akbani et al., 2004;
Batista et al., 2004; Chawla et al., 2002). Here we
highlight weighted cost and random upsampling,
which are known to work well and widely used.
The idea behind weighted cost (Morik et al.,
1999; Veropoulos et al., 1999) is to use weights
to adjust the penalties for false positives and false
negatives in the objective function. As in Morik
et al. (1999), we specify the cost factor to be the
ratio of the size of the negative class vs. that of the
positive class.
In the case of upsampling, instead of ran-
domly downsampling negative instances, positive
instances are randomly upsampled. In our exper-
iments we randomly replicate positive instances
with replacement until the numbers of positive and
negative instances are equal to each other.
The binary and multiway classification results
for these two methods are shown in Table 4 and
Table 5. For binary classification, we can see sig-
nificantly higher F score for the smallest Temporal
class. Weighted cost is also able to achieve signif-
icantly better F-score for Expansion. For Compar-
ison and Contingency, the F-scores are similar to
that of plain downsampling. The balanced accura-
cies of multi-class classification with either meth-
ods are lower, or significantly lower in the case of
145
weighted cost, than using downsampling in one-
against-all manner.
Upsample WeightCost
Temp. 20.35* (16.8/25.9) 20.61* (16.9/26.3)
Comp. 28.11 (20.6/44.5) 28.38 (19.9/49.6)
Cont. 46.46 (37.4/61.3) 46.36 (34.6/70.1)
Exp. 54.93 (50.3/60.5) 57.43* (43.9/83.1)
Table 4: F-measure (precision/recall) of binary
classification: upsampling vs. weighted cost.
For Temporal and Comparison relations listed
in Table 4, we noticed an interesting similarity
between the F and precision values of upsam-
pling and weighted cost. To quantify this simi-
larity, we calculated the Q-statistic (Kuncheva and
Whitaker, 2003) between the two classifiers. The
Q-statistic is a measurement of classifier agree-
ment raging between -1 and 1, defined as:
Q
w,u
=
N
11
N
00
?N
01
N
10
N
11
N
00
+N
01
N
10
(2)
Where w denotes the system using weighted cost,
u denotes the upsampling system. N
11
means both
systems make a correct prediction, N
00
means
both systems are incorrect, N
10
means w is incor-
rect but u is correct, and N
01
means w is correct
but u is incorrect.
We have the following Q statistics: Tempo-
ral: 0.999, Comparison: 0.9938, Contingency:
0.9746, Expansion: 0.7762. These are good in-
dicators that for highly skewed relations, the two
methods give classifiers that behave almost identi-
cally on the test data. In the discussions that fol-
low, we discuss only weighted cost to avoid redun-
dancy.
5.2 Feature selection and shaping
While weighted cost or upsampling can give bet-
ter performance over downsampling for some rela-
tions, their disadvantages towards multi-class clas-
sification and the obvious favor towards the major-
ity class give rise to the following question: is it
possible to inform the classifier of the information
encoded in the annotation of all of the data while
still using downsampling to handle the skewed
class distribution? Our proposal is feature value
augmentation. Here we introduce a relational ma-
trix in which we calculate augmented feature val-
ues via feature shaping. We first compute the val-
ues of features on the entire training set, then use
the downsampled set for training with these val-
ues. In this way we pass on to the classifiers infor-
mation about the relative importance of features
gleaned from the entire training data.
5.2.1 Feature shaping
The idea of feature shaping was introduced in the
context of improving the performance of linear
SVMs (Forman et al., 2009). In linear SVMs
the prediction is based on a linear combination of
weight?feature values. The sign of weight indi-
cates the preference for a class (positive or nega-
tive), the value of the feature should correspond to
how strongly it indicates that class. Thus, features
that are strongly discriminative should have high
values so that they can contribute more to the final
class decision. Here we augment feature values
for a relation according to the following criteria:
1. Features are considered ?good? if they strongly
indicate the presence of the relation; 2. Features
are considered ?good? if they strongly indicate the
absence of the relation; 3. features are considered
?bad? if their presence give no information about
either the presence or the absence of the relation.
To capture this information, we first construct a
relation matrix M with each entry M
ij
defined as
the conditional probability of relationR
j
given the
feature F
i
computed as the maximum likelihood
estimate from the full training set:
M
ij
= P (R
j
|F
i
)
Each column of the relation matrix captures the
predictive power of each feature to a certain re-
lation. A feature with value M
ij
higher than the
column mean indicates that it is predictive for the
presence of relation j, while a feature with M
ij
lower than the mean is predictive for its absence;
the strength of such indication depends on how far
away M
ij
is from the mean: the further away it is,
the more valuable this feature should be for rela-
tion j. With this idea we give the following aug-
mented value for each feature:
M
?
ij
=
{
M
ij
, if M
ij
? ?
j
.
?
j
+ (?
j
?M
ij
), if M
ij
< ?
j
.
(3)
where ?
j
is the mean of the jth column corre-
sponding to the jth relation.
Given a feature F
i
, very small and very high
probabilities of a certain relation j, i.e., P (R
j
|F
i
),
are both useful information. However, in linear
SVMs, lower values of a feature would mean that
it contributes less to the decision of the class. By
146
feature shaping, we allow features that strongly in-
dicate the absence of a class to influence the deci-
sion and rely on the classifier to identify the nega-
tive association and reflect it by assigning a nega-
tive weight to these features.
When constructing the relation matrix, we used
the top four relation classes along with an En-
tRel/NoRel class. We computed the matrix before
downsampling to preserve the natural data distri-
bution and features that strongly indicate the ab-
sence of a class, then downsample the negative
data just like the previous downsampling setting.
5.2.2 Feature selection
The relation matrix also provides information for
feature selection using a binomial test for signifi-
cance, B(n,N, p), which gives the probability of
observing a feature n times in N instances of a
relation if the probability of any feature occurring
with the relation is p. For each relation, we use the
binomial test to pick the features that occur signif-
icantly more or less often than expected with the
relation. In the binomial test, p is set to be equal to
the probability of that relation in the PDTB train-
ing set. We select only the features which result in
a low p-value for the binomial test for at least some
relation. We used 9-fold cross validation on the
training data to pick the best p-values for each re-
lation individually; all best p-values were between
0.1 and 0.2.
Result listing Table 5 and Table 6 show the mul-
tiway and binary classification performance using
feature shaping and feature selection. We also
show the precision and recall for binary classifiers.
Multiway SVM One-Against-All
AllData 32.58 NA
Downsample NA 37.15
Upsample NA 36.63
Weighted Cost NA 34.23
Selection 32.52 38.42*
Shaping NA 38.81**
Shape+Sel NA 39.13**
Table 5: Balanced accuracy for multiway
SVM and one-against-all for 5-way classification.
One asterisk (*) means significantly better than
weighted cost and upsampling, and two means sig-
nificantly better than downsampling, at p < 0.05.
For multi-way classification, performing feature
shaping leads to significant improvements over
downsampling, upsampling and weighted cost.
The binomial method for feature selection that
relies on the full training data distribution has a
similar effect. Combined feature shaping and se-
lection leads to 2% absolute improvement in dis-
course relation recognition. For binary classifica-
tion, though, the improvement is significant only
for Temporal.
6 Classifier analysis and combination
6.1 Discussion of precision and recall
A careful examination of Tables 5 and 6 leads
to some intriguing observations. For the most
skewed relations, if we consider not only the F
measure, but also the precision and recall, there
is an interesting difference between the systems.
While downsampling has the lowest precision, it
gives the highest recall. The case for weighted cost
is another story. For highly skewed relations such
as Temporal and Comparison, it gives the highest
precision and the lowest recall; but as the data set
balances out in downsampling, the classifier shifts
towards high recall and low precision.
We can also rank the three feature augmentation
techniques in terms of how much they reflect dis-
tributional information in the training data. Fea-
ture selection reflects the training data least among
the three, because it uses information from all of
the data to select the features, but the feature val-
ues are still either 1 or 0. Feature shaping engages
more data because the value of a feature encodes
its relative ?effectiveness? for a relation. We can
see that feature selection gives slightly higher pre-
cision than just downsampling; feature shaping,
on the other hand, gives precision and recall val-
ues between these two. This is most obvious in
smaller relations, i.e. Temporal and Comparison.
To see if this trend is statistically significant, we
did a paired t-test over the precision and recall for
each system and each relation. For the Temporal
relation, all systems that use more data have sig-
nificantly higher (p < 0.05) precision than that
for downsampling. For Comparison, the changes
in precision are either significant or tend towards
significance for three methods: feature shaping
(p < 0.1), feature shaping+election (p < 0.1)
and weighted cost (p < 0.05). For Contingency,
feature shaping gives an improvement in precision
that tends toward significance (p < 0.1). The
drops in recall using feature shaping or weighted
cost for the above three relations are significant
(p < 0.05). For the Expansion relation, being the
largest class with 40% positive data, changes in
147
Downsample WeightCost Selection Shaping Shape+Sel
Temp. 15.52 (8.8/65.4) 20.61* (16.9/26.3) 18.47* (10.7/65.9) 20.37* (12.6/53.2) 21.30* (13.7/47.8)
Comp. 27.65 (17.3/69.2) 28.38 (19.9/49.6) 26.98 (17.4/60.1) 27.79 (18.3/58.2) 26.92 (18.7/48.2)
Cont. 47.14 (34.5/74.5) 46.36 (34.6/70.1) 47.45 (34.7/75.2) 47.62 (35.4/72.9) 46.93 (35.2/70.5)
Exp. 55.42 (49.3/63.3) 57.43* (43.9/83.1) 55.52 (49.3/63.5) 55.13 (49.3/62.5) 54.90 (49.2/62.1)
Table 6: F score (precision/recall) of classifiers with feature augmentation. Asterisk(*) means F score or
BAC is significantly greater than plain downsampling at p < 0.05.
precision and recall with downsampling systems
are not significant; yet weighted cost shifted to-
wards predicting more of the positive instances,
i.e., giving a significantly higher recall by trading
with a significantly lower precision (p < 0.05).
6.2 Discussion of classifier similarity
To better understand the differences of classi-
fier behaviors under the weighted cost and each
downsampling technique (plain downsampling,
feature selection, feature shaping, feature shap-
ing+selection), in Table 7 we show the percentage
of test instances that the weighted cost system and
each downsample system agree or do not agree. In
particular, we study the following situations:
1. The downsample system predicts correctly
but the weighted cost system does not (?D+C-?);
2. The weighted cost system predicts correctly
but the downsample system does not (?D-C+?);
3. Both systems are correct (?D+C+?).
At a glance of the Q statistic, it seems that the
systems are not behaving very differently. How-
ever, as only the sum of disagreements is reflected
in the Q statistic, we look more closely at where
the systems do not agree in each situation. If we
focus on the rarer Temporal and Comparison re-
lations, first note that in the plain downsampling
vs. weighted cost, the percentage of test instances
in the ?D+C-? column is much smaller than that
in the ?D-C+? column. This aligns with the above
observation that plain downsampling gives much
lower precision for these relations than weighted
cost. Now, as more data is engaged from first
using feature selection, then using feature shap-
ing, then using both, the percentage of instances
where both systems predict correctly increase. At
the same time, there is a drop in the percentage of
test instances in the ?D-C+? column. This trend is
also a reflection of the observation that as more
data is engaged, the precision got higher as the
recall drops lower. As the data gets more evenly
distributed, this phenomenon fades away. The ta-
ble also reveals a subtle difference between fea-
ture shaping and feature selection. Compared to
D+C- D-C+ D+C+ Q
(%) (%) (%) Stat
Temporal
Downsamp 2.56 28.27 61.47 0.73
Selection 2.91 22.04 67.71 0.77
Shaping 2.61 13.36 76.39 0.89
Sel+Shape 2.83 10.42 79.32 0.90
Comparison
Downsamp 5.74 18.24 53.76 0.84
Selection 7.72 16.14 55.85 0.80
Shaping 6.14 11.95 60.04 0.89
Sel+Shape 9.69 10.99 61.01 0.83
Contingency
Downsamp 6.88 7.89 58.74 0.93
Selection 8.01 8.92 57.70 0.91
Shaping 7.07 6.73 59.90 0.94
Sel+Shape 8.68 8.13 58.49 0.91
Expansion
Downsamp 16.39 8.23 44.66 0.82
Selection 17.87 9.71 43.18 0.76
Shaping 16.64 8.45 44.44 0.81
Sel+Shape 18.36 10.30 42.59 0.73
Table 7: Q statistics and agreements (in percent-
ages) of each downsampling system vs. weighted
cost. ?D? denotes the respective downsample sys-
tem in the left most column; ?C? denotes the
weighted cost system. A ?+? means that a system
makes a correct prediction; a ?-? means a system
makes an incorrect prediction.
downsampling, feature selection introduces an in-
crease in the column ?D+C-? (i.e. the weighted
cost system makes a mistake but the downsample
system is correct). Feature shaping, on the other
hand, do not necessarily increase this new kind of
difference between classifiers.
6.3 Classifier combination
Our classifier comparisons revealed that for highly
skewed distributions, there are consistent differ-
ences in the performance of classifiers obtained by
using the training data in different ways. It stands
to reason that a combination of these classifiers
with different strengths will result in an overall im-
proved classifier. This idea is explored here.
Suppose on a sample i, the downsampling clas-
sifier predicts the target class with confidence p
id
,
and the weighted cost classifier predicts the target
148
class with confidence p
ic
. Here again we approx-
imate the confidence of the class by the distance
from the hyperplane dividing the two classes. We
weight the two predictions and get a new predic-
tion confidence by:
p
?
i
=
?
d
p
id
+ ?
u
p
ic
?
d
+ ?
c
. (4)
where the ?s are parameters we want to encode
how much we trust each classifier. To get these
values, we train the classifiers and get the accura-
cies from each of them on the development set.
Since we are using linear SVMs in our experi-
ments, we mark the sample as positive if p
i
> 0,
and negative otherwise.
The results for the combination are shown in Ta-
ble 8. We include the original performances of the
classifiers by themselves for reference.
F measure For Temporal, the combined classi-
fier performs better than the original classifiers.
We see significant (p < 0.05) improvements over
the corresponding downsampling system and the
weighted cost system. If feature shaping is in-
volved in the combination, it is also having bet-
ter performance that tend toward significance (p <
0.1) over the weighted cost classifier. For Compar-
ison, the benefits of a combined system is also ob-
vious for feature shaping and/or selection. Feature
shaping combined with weighted cost gives sig-
nificantly (p < 0.05) better performance than ei-
ther of them individually, and feature selection and
shaping+selection combined with weighted cost is
better than themselves alone. For Contingency,
though weighted cost do not give better results, the
improvement tends toward significance (p < 0.1)
when combined with plain downsampling. For Ex-
pansion where weighted cost gives the lowest pre-
cision, combination with other classifiers do not
give significant improvements over F scores.
Precision and recall We can also compare the
precision and recall for each system before and af-
ter combination. In all but one cases for Temporal
and Comparison, we observe significantly higher
precision and much lower recall after the combi-
nation. The case for Expansion is just the opposite
as expected.
7 Conclusion
In this paper, we studied the effect of the use of an-
notated data for binary and multiway classification
Original Combined
Classifier Classifier
Temporal
WeightCost 20.61 (16.9/26.3)
Downsamp 15.52 (8.8/65.4) 21.78* (14.9/40.5)
Selection 18.47 (10.7/65.9) 22.99* (15.8/42.0)
Shaping 20.37 (12.6/53.2) 23.88* (17.5/37.6)
Sel+Shape 21.30 (13.7/47.8) 23.72* (17.7/36.1)
Comparison
WeightCost 28.38 (19.9/49.6)
Downsamp 27.65 (17.3/69.2) 28.72 (19.3/56.4)
Selection 26.98 (17.4/60.1) 29.25? (20.1/54.0)
Shaping 27.79 (18.3/58.2) 29.89*
+
(20.5/54.9)
Sel+Shape 26.92 (18.7/48.2) 29.83* (21.3/50.0)
Contingency
WeightCost 46.36 (34.6/70.1)
Downsamp 47.14 (34.5/74.5) 48.38
+
(35.9/74.4)
Selection 47.45 (34.7/75.2) 47.76
+
(35.5/72.9)
Shaping 47.62 (35.4/72.9) 48.16
+
(36.0/72.9)
Sel+Shape 46.93 (35.2/70.5) 47.37 (35.6/70.7)
Expansion
WeightCost 57.43 (43.9/83.1)
Downsamp 55.42 (49.3/63.3) 56.61* (46.4/72.7)
Selection 55.52 (49.3/63.5) 57.10* (46.5/73.0)
Shaping 55.13 (49.3/62.5) 56.74* (46.4/73.0)
Sel+Shape 54.90 (49.2/62.1) 57.06* (46.4/74.0)
Table 8: Classifier combination results for binary
classification. An asterisk(*) means significantly
better than the corresponding downsampling sys-
tem at, and a plus(+) means significantly better
than weighted cost, at p < 0.05. Improvements
that tend toward significance (p < 0.1) are not
shown here but are discussed in the text.
in supervised implicit discourse relation recogni-
tion. The starting point of our work was to estab-
lish the effectiveness of downsampling negative
examples, which was practiced but not experimen-
tally investigated in prior work. We also evalu-
ated alternative solutions to the skewed data prob-
lem, as downsampling throws away most of the
data. We examined the effect of upsampling and
weighted cost. In addition, we introduced the rela-
tion matrix to give more emphasis on informative
features through augmenting the feature value via
feature shaping. We found that as we summarize
more detailed information about the data in the full
training set, performance for multiway classifica-
tion gets better. We also observed through preci-
sion and recall that there are fundamental differ-
ences between downsampling and weighted cost,
and this difference can be beneficially exploited
by combining the two classifiers. We showed that
our way of doing such combination gives signifi-
cantly higher performance results for binary clas-
sification in the case of rarer relations.
149
References
Rehan Akbani, Stephen Kwek, and Nathalie Japkow-
icz. 2004. Applying support vector machines to
imbalanced datasets. In Machine Learning: ECML
2004, pages 39?50.
Gustavo E. A. P. A. Batista, Ronaldo C. Prati, and
Maria Carolina Monard. 2004. A study of the
behavior of several methods for balancing machine
learning training data. ACM SIGKDD Explorations
Newsletter - Special issue on learning from imbal-
anced datasets, 6(1):20?29, June.
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL): Short Papers, pages 69?73.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. SMOTE:
Synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 16(1):321?
357, June.
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In IEEE International Conference on Se-
mantic Computing (IEEE-ICSC), pages 198 ?205.
George Forman, Martin Scholz, and Shyamsundar Ra-
jaram. 2009. Feature shaping for linear SVM classi-
fiers. In Proceedings of the 15th ACM International
Conference on Knowledge Discovery and Data Min-
ing (KDD), pages 299?308.
Sucheta Ghosh, Richard Johansson, Giuseppe Ric-
cardi, and Sara Tonelli. 2011. Shallow discourse
parsing with conditional random fields. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing (IJCNLP), pages
1071?1079.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21:203?225.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 399?409.
Yu Hong, Xiaopei Zhou, Tingting Che, Jianmin Yao,
Qiaoming Zhu, and Guodong Zhou. 2012. Cross-
argument inference for implicit discourse relation
recognition. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 295?304.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169?184.
Ludmila I. Kuncheva and Christopher J. Whitaker.
2003. Measures of diversity in classifier ensembles
and their relationship with the ensemble accuracy.
Machine Learning, 51(2):181?207, May.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind Joshi. 2008. Sense annotation in the Penn
Discourse Treebank. In Proceedings of the 9th
International Conference on Computational Lin-
guistics and Intelligent Text Processing (CICLing),
pages 275?286.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning
with a knowledge-based approach - a case study in
intensive care monitoring. In Proceedings of the Six-
teenth International Conference on Machine Learn-
ing (ICML), pages 268?277.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 108?112.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING): Companion volume:
Posters, pages 87?90.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP (ACL-IJCNLP),
pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Konstantinos Veropoulos, Colin Campbell, and Nello
Cristianini. 1999. Controlling the sensitivity of sup-
port vector machines. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJCAI), volume 1999, pages 55?60.
150
Proceedings of the SIGDIAL 2014 Conference, pages 199?207,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Reducing Sparsity Improves the Recognition of Implicit Discourse
Relations
Junyi Jessy Li
University of Pennsylvania
ljunyi@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
The earliest work on automatic detec-
tion of implicit discourse relations relied
on lexical features. More recently, re-
searchers have demonstrated that syntactic
features are superior to lexical features for
the task. In this paper we re-examine the
two classes of state of the art representa-
tions: syntactic production rules and word
pair features. In particular, we focus on the
need to reduce sparsity in instance repre-
sentation, demonstrating that different rep-
resentation choices even for the same class
of features may exacerbate sparsity issues
and reduce performance. We present re-
sults that clearly reveal that lexicalization
of the syntactic features is necessary for
good performance. We introduce a novel,
less sparse, syntactic representation which
leads to improvement in discourse rela-
tion recognition. Finally, we demonstrate
that classifiers trained on different repre-
sentations, especially lexical ones, behave
rather differently and thus could likely be
combined in future systems.
1 Introduction
Implicit discourse relations hold between adjacent
sentences in the same paragraph, and are not sig-
naled by any of the common explicit discourse
connectives such as because, however, meanwhile,
etc. Consider the two examples below, drawn from
the Penn Discourse Treebank (PDTB) (Prasad et
al., 2008), of a causal and a contrast relation, re-
spectively. The italic and bold fonts mark the ar-
guments of the relation, i.e the portions of the text
connected by the discourse relation.
Ex1: Mrs Yeargin is lying. [Implicit = BECAUSE] They
found students in an advanced class a year earlier who
said she gave them similar help.
Ex2: Back downtown, the execs squeezed in a few meetings at
the hotel before boarding the buses again. [Implicit = BUT]
This time, it was for dinner and dancing - a block away.
The task is undisputedly hard, partly because it
is hard to come up with intuitive feature represen-
tations for the problem. Lexical and syntactic fea-
tures form the basis of the most successful stud-
ies on supervised prediction of implicit discourse
relations in the PDTB. Lexical features were the
focus of the earliest work in discourse recogni-
tion, when cross product of words (word pairs)
in the two spans connected via a discourse re-
lation was studied. Later, grammatical produc-
tions were found to be more effective. Features
of other classes such as verbs, inquirer tags, posi-
tions were also studied, but they only marginally
improve upon syntactic features.
In this study, we compare the most commonly
used lexical and syntactic features. We show that
representations that minimize sparsity issues are
superior to their sparse counterparts, i.e. the bet-
ter representations are those for which informative
features occur in larger portions of the data. Not
surprisingly, lexical features are more sparse (oc-
curring in fewer instances in the dataset) than syn-
tactic features; the superiority of syntactic repre-
sentations may thus be partially explained by this
property.
More surprising findings come from a closer ex-
amination of instance representation approaches
in prior work. We first discuss how choices in
prior work have in fact exacerbated the sparsity
problem of lexical features. Then, we introduce
a new syntactically informed feature class, which
is less sparse than prior lexical and syntactic fea-
tures, and improves significantly the classification
of implicit discourse relations.
Given these findings, we address the question
if any lexical information at all should be pre-
served in discourse parsers. We find that purely
syntactic representations show lower recognition
199
for most relations, indicating that lexical features,
albeit sparse, are necessary for the task. Lexical
features also account for a high percentage of the
most predictive features.
We further quantify the agreement of predic-
tions produced from classifiers using different in-
stance representations. We find that our novel syn-
tactic representation is better for implicit discourse
relation prediction than prior syntactic feature be-
cause it has higher overall accuracy and makes
correct predictions for instances for which the al-
ternative representations are also correct. Differ-
ent representation of lexical features however ap-
pear complementary to each other, with markedly
higher fraction of instances recognized correctly
by only one of the models.
Our work advances the state of the art in implicit
discourse recognition by clarifying the extent to
which sparsity issues influence predictions, by in-
troducing a strong syntactic representation and by
documenting the need for further more complex
integration of lexical information.
2 The Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) contains annotations for five types of
discourse relations over the Penn Treebank corpus
(Marcus et al., 1993). Explicit relations are those
signaled by a discourse connective that occurs in
the text, such as ?because?, ?however?, ?for ex-
ample?. Implicit relations are annotated between
adjacent sentences in the same paragraph. There
are no discourse connectives between the two sen-
tences, and the annotators were asked to insert a
connective while marking their senses. Some pairs
of sentences do not contain one of the explicit dis-
course connectives, but the insertion of a connec-
tive provides redundant information into the text.
For example, they may contain phrases such as
?the consequence of the act?. These are marked
Alternative Lexicalizations (AltLex). Entity rela-
tions (EntRel) are adjacent sentences that are only
related via the same entity or topic. Finally, sen-
tences where no discourse relations were identi-
fied were marked NoRel. In this work, we consider
AltLex to be part of the Implicit relations, and En-
tRel to be part of NoRel.
All connectives, either explicit or implicitly in-
serted, are associated with two arguments of the
minimal span of text conveying the semantic con-
tent between which the relation holds. This is il-
lustrated in the following example where the two
arguments are marked in bold and italic:
Ex: They stopped delivering junk mail. [Implicit = SO] Now
thousands of mailers go straight into the trash.
Relation senses in the PDTB are drawn from
a 3-level hierarchy. The top level relations are
Comparison (arg1 and arg2 holds a contrast rela-
tion), Contingency (arg1 and arg2 are causally re-
lated), Expansion (arg2 further describes arg1) and
Temporal (arg1 and arg2 are temporally related).
Some of the largest second-tier relations are under
Expansion, which include Conjunction (arg2 pro-
vides new information to arg1), Instantiation (arg2
exemplifies arg1) and Restatement (arg2 semanti-
cally repeats arg1).
In our experiments we use the four top level re-
lations as well as the above three subclasses of Ex-
pansion. All of these subclasses occur with fre-
quencies similar to those of the Contingency and
Comparison classes, with thousands of examples
in the PDTB.
1
We show the distribution of the
classes below:
Temporal 1038 Comparison 2550
Contingency 4532 Instantiation 1483
Restatement 3271 Conjunction 3646
EntRel/NoRel 5464
3 Experimental settings
In our experiments we use only lexical and syntac-
tic features. This choice is motivated by the fact
that lexical features have been used most widely
for the task and that recent work has demon-
strated that syntactic features are the single best
type of representation. Adding additional features
only minimally improves performance (Lin et al.,
2009). By zeroing in only on these classes of fea-
tures we are able to discuss more clearly the im-
pact that different instance representation have on
sparsity and classifier performance.
We use gold-standard parses from the original
Penn Treebank for syntax features.
To ensure that our conclusions are based on
analysis of the most common relations, we train
binary SVM classifiers
2
for the seven relations de-
scribed above. We adopt the standard practice in
1
All other sub-classes of implicit relations are too small
for general practical applications. For example the Alterna-
tive class and Concession class have only 185 and 228 oc-
currences, respectively, in the 16,224 implicit relation anno-
tations of the PDTB.
2
We use SVMLight (Joachims, 1999) with linear kernel.
200
prior work and downsampled the negative class so
the number of positive and negative samples are
equal in the training set.
3
Our training set consists of PDTB sections 2-
19. The testing set consists of sections 20-24. Like
most studies, we do not include sections 0-1 in the
training set. We expanded the test set (sections 23
or 23-24) used in previous work (Lin et al., 2014;
Park and Cardie, 2012) to ensure the number of
examples of the smaller relations, particularly of
Temporal or Instantiation, are suitable for carrying
out reliable tests for statistical significance.
Some of the discourse relations are much larger
than others, so we report our results in term of F-
measure for each relation and average unweighted
accuracy. Significance tests over F scores were
carried out using a paired t-test. To do this, the
test set is randomly partitioned into ten groups. In
each group, the relation distribution was kept as
close as possible to the overall test set.
4 Sparsity and pure lexical
representations
By far the most common features used for rep-
resenting implicit discourse relations are lexical
(Sporleder and Lascarides, 2008; Pitler et al.,
2009; Lin et al., 2009; Hernault et al., 2010;
Park and Cardie, 2012). Early studies have sug-
gested that lexical features, word pairs (cross-
product of the words in the first and second ar-
gument) in particular, will be powerful predictors
of discourse relations (Marcu and Echihabi, 2002;
Blair-Goldensohn et al., 2007). The intuition be-
hind word pairs was that semantic relations be-
tween the lexical items, such as drought?famine,
child?adult, may in turn signal causal or contrast
discourse relations. Later it has been shown that
word pair features do not appear to capture such
semantic relationship between words (Pitler et al.,
2009) and that syntactic features lead to higher ac-
curacies (Lin et al., 2009; Zhou et al., 2010; Park
and Cardie, 2012). Recently, Biran and McKeown
(2013) aggregated word pair features with explicit
connectives and reported improvements over the
original word pairs as features.
In this section, we show that the representation
of lexical features play a direct role in feature spar-
sity and ultimately affects prediction performance.
The first two studies that specifically addressed
3
We also did not include features that occurred less than
5 times in the training set.
# Features Avg. F Avg. Accuracy
word-pairs 92128 29.46 57.22
binary-lexical 12116 31.79 60.42
Table 1: F-scores and average accuracies of paired
and binary representations of words.
the problem of predicting implicit discourse re-
lations in the PDTB made use of very different
instance representations. Pitler et al. (2009) rep-
resent instances of discourse relations in a vec-
tor space defined by word pairs, i.e. the cross-
product of the words that appear in the two argu-
ments of the relation. There, features are of the
form (w
1
, w
2
) where w
1
? arg1 and w
2
? arg2.
If there are N words in the entire vocabulary, the
size of each instance would be N ?N .
In contrast, Lin et al. (2009) represent instances
by tracking the occurrences of grammatical pro-
ductions in the syntactic parse of argument spans.
There are three indicator features associated with
each production: whether the production appears
in arg1, in arg2, and in both arguments. For a
grammar with N production rules, the size of the
vector representing an instance will be 3N . For
convenience we call this ?binary representation?,
in contrast to the word-pair features in which the
cross product of words constitute the representa-
tion. Note that the cross-product approach has
been extended to a wide variety of features (Pitler
et al., 2009; Park and Cardie, 2012). In the ex-
periments that follow we will demonstrate that bi-
nary representations lead to less sparse features
and higher prediction accuracy.
Lin et al. (2009) found that their syntactic fea-
tures are more powerful than the word pair fea-
tures. Here we show that the advantage comes not
only from the inclusion of syntactic information
but also from the less sparse instance representa-
tion they used for syntactic features. In Table 1
we show the number of features for each repre-
sentation and the average F score and accuracy for
word pairs and words with binary representation
(binary-lexical). The results for each relation are
shown in Table 8 and discussed in Section 7.
Using binary representation for lexical informa-
tion outperforms word pairs. Thus, the difference
in how lexical information is represented accounts
for a considerable portion of the improvement re-
ported in Lin et al. (2009). Most notably, for the
Instantiation class, we see a 7.7% increase in F-
score. On average, the less sparse representation
201
translates into 2.34% absolute improvement in F-
score and 3.2% absolute improvement in accuracy.
From this point on we adopt the binary represen-
tation for the features discussed.
5 Sparsity and syntactic features
Grammatical production rules were first used for
discourse relation representation in Lin et al.
(2009). They were identified as the most suitable
representation, that lead to highest performance in
a couple of independent studies (Lin et al., 2009;
Park and Cardie, 2012). The comparison repre-
sentations covered a number of semantic classes
related to sentiment, polarity and verb information
and dependency representations of syntax.
Production rules correspond to tree chunks in
the constituency parse of a sentence, i.e. a node
in the syntactic parse tree with all of its children,
which in turn correspond to grammar rules ap-
plied in the derivation of the tree, such as S?NP
VP. This syntactic representation subsumes lexi-
cal representations because of the production rules
with part-of-speech on the left-hand side and a lex-
ical item on the right-hand side.
We propose that the sparsity of production rules
can be reduced even further by introducing a new
representation of the parse tree. Specifically, in-
stead of having full production rules where a sin-
gle feature records the parent and all its children,
all (parent,child) pairs in the constituency parse
tree are used. For example, the rule S?NP VP
will now become two features, S?NP and S?VP.
Note that the leaves of the tree, i.e. the part-of-
speech?word features are not changed. For ease
of reference we call this new representation ?pro-
duction sticks?. In this section we show that F
scores and accuracies for implicit discourse rela-
tion prediction based on production sticks is sig-
nificantly higher than using full production rules.
First, Table 2 illustrates the contrast in sparsity
among the lexical, production rule and stick repre-
sentations. The table gives the rate of occurrence
of each feature class, which is defined as the av-
erage fraction of features with non-zero values in
the representation of instances in the entire train-
ing set. Specifically, let N be the total number of
features, m
i
be the number of features triggered in
instance i, then the rate of occurrence is
m
i
N
.
The table clearly shows that the number of fea-
tures in the three representations is comparable,
but they vary notably in their rate of occurrence.
# Features Rate of Occurrence
sticks 14,165 0.00623
prodrules 16,173 0.00374
binary-lexical 12,116 0.00276
word-pairs 92,128 0.00113
Table 2: Number of features and rate of occur-
rence for binary lexical representation, production
rules and sticks.
Avg. F Avg. Accuracy
sticks 34.73 64.89
prodrules 33.69 63.55
binary-lexical 31.79 60.42
word-pairs 29.46 57.22
Table 3: F-scores and average accuracies of pro-
duction rules and production sticks.
Sticks have almost twice the rate of occurrence of
that of full production rules. Both syntactic rep-
resentations have much larger rate of occurrence
than lexical features, and the rate of occurrence of
word pairs is more than twice smaller than that of
the binary lexical representation.
Next, in Table 3, we give binary classifica-
tion prediction results based on both full rules
and sticks. The first two rows of Table 3 com-
pare full production rules (prodrules) with produc-
tion sticks (sticks) using the binary representation.
They both outperform the binary lexical represen-
tation. Again our results confirm that the better
performance of production rule features is partly
because they are less sparse than lexical represen-
tations, with an average of 1.04% F-score increase.
Individually the F scores of 6 of the 7 relations are
improved as shown in Table 8.
6 How important are lexical features?
Production rules or sticks include lexical items
with their part-of-speech tags. These are the sub-
set of features that contribute most to sparsity is-
sues. In this section we test if these lexical fea-
tures contribute to the performance or if they can
be removed without noticeable degradation due to
its intrinsic sparsity. It turns out that it is not ad-
visable to remove the lexical features entirely, as
performance decreases substantially if we do so.
6.1 Classification without lexical items
We start our exploration of the influence of lexical
items on the accuracy of prediction by inspecting
the performances of the classifiers with production
rules and sticks, but without the lexical items and
their parts of speech. Table 4 lists the average F
202
Avg. F Avg. Accuracy
prodrules 33.69 63.55
sticks 34.73 64.89
prodrules-nolex 32.30 62.03
sticks-nolex 33.86 63.99
Table 4: F-scores and average accuracies of pro-
duction rules and sticks, with (rows 1-2) and with-
out (rows 3-4) lexical items.
# Features Rate of Occurrence
prodrules 16,173 0.00374
sticks 14,165 0.00623
prodrules-nolex 3470 0.00902
sticks-nolex 922 0.0619
Table 5: Number of features and rate of occur-
rence for production rules and sticks, with (rows
1-2) and without (rows 3-4) lexical items.
scores and accuracies. Table 8 provides detailed
results for individual relations. Here prodrules-
nolex and sticks-nolex denote full production rules
without lexical items, and production sticks with-
out lexical items, respectively. In all but two re-
lations, lexical items contribute to better classifier
performance.
When lexical items are not included in the rep-
resentation, the number of features is reduced to
fewer than 30% of that in the original full produc-
tion rules. At the same time however, including
the lexical items in the representation improves
performance even more than introducing the less
sparse production stick representation. Production
sticks with lexical information also perform bet-
ter than the same representation without the POS-
word sticks.
The number of features and their rates of occur-
rences are listed in Table 5. It again confirms that
the less sparse stick representation leads to better
classifier performance. Not surprisingly, purely
syntactic features (without the lexical items) are
much less sparse than syntax features with lexical
items present. However the classifier performance
is worse without the lexical features. This contrast
highlights the importance of a reasonable tradeoff
between attempts to reduce sparsity and the need
to preserve lexical features.
6.2 Feature selection
So far our discussion was based on the behavior
of models trained on a complete set of relatively
frequent syntactic and lexical features (occurring
more than five times in the training data). Feature
selection is a way to reasonably prune out the set
Relation %-nonlex %-allfeats
Temporal 25.56 10.95
Comparison 25.40 15.51
Contingency 20.12 25.05
Conjunction 21.15 19.20
Instantiation 25.08 16.16
Restatement 22.16 17.35
Expansion 18.36 18.66
Table 6: Non-lexical features selected using fea-
ture selection. %-nonlex records the percentage of
non-lexical features among all features selected;
%-allfeats records the percentage of selected non-
lexical features among all non-lexical features.
and reduce sparsity issues in the model. In fact
feature selection has been used in the majority of
prior work (Pitler et al., 2009; Lin et al., 2009;
Park and Cardie, 2012).
Here we perform feature selection and exam-
ine the proportion of syntactic and lexical features
among the most informative features. We use the
?
2
test of independence, computed on the follow-
ing contingency table for each feature F
i
and for
each relation R
j
:
F
i
?R
j
|F
i
? ?R
j
?F
i
?R
j
|?F
i
? ?R
j
Each cell in the above table records the num-
ber of training instances in which F
i
and R
j
are
present or absent. We set our level of confidence
to p < 0.1.
Table 6 lists the proportions of non-lexical items
among the most informative features selected (col-
umn 2). It also lists the percentage of selected non-
lexical items among all the 922 purely syntactic
features from production rule and production stick
representations (column 3). For all relations, at
most about a quarter of the most informative fea-
tures are non-lexical and they only take up 10%-
25% of all possible non-lexical features. The pre-
diction results using only these features are either
higher than or comparable to that without feature
selection (sticks-?
2
in Table 8). These numbers
suggest that lexical terms play a significant role as
part of the syntactic representations.
In Table 8 we record the F scores and accura-
cies for each relation under each feature represen-
tation. The representations are sorted according to
descending F scores for each relation. Notice that
?
2
feature selection on sticks is the best represen-
tation for the three smallest relations: Compari-
son, Instantiation and Temporal.
203
This finding led us to look into the selected lex-
ical features for these three classes. We found that
these most prominent features in fact capture some
semantic information. We list the top ten most pre-
dictive lexical features for these three relations be-
low, with examples. Somewhat disturbingly, many
of them are style or domain specific to the Wall
Street Journal that PDTB was built on.
Comparison a1a2 NN share a1a2 NNS cents a1a2 CC or
a1a2 CD million a1a2 QP $ a1a2 NP $ a2 RB n?t
a1a2 NN % a2 JJ year a2 IN of
For Comparison (contrast), the top lexical fea-
tures are words that occur in both argument 1 and
argument 2. Contrast within the financial domain,
such as ?share?, ?cents? and numbers between ar-
guments are captured by these features. Consider
the following example:
Ex. Analyst estimate the value of the BellSouth proposal at
about $115 to $125 a share. [Implicit=AND] They value
McCaw?s bid at $112 to $118 a share .
Here the contrast clearly happens with the value
estimation for two different parties.
Instantiation a2 SINV ? a2 SINV , a2 SINV ? a2 SINV .
a1 DT some a2 S a2 VBZ says a1 NP , a2 NP , a1 DT a
For Instantiation (arg2 gives an example of
arg1), besides words such as ?some? or ?a? that
sometimes mark a set of events, many attribution
features are selected. it turns out many Instanti-
ation instances in the PDTB involve argument 2
being an inverted declarative sentence that signals
a quote as illustrate by the following example:
Ex. Unease is widespread among exchange members. [Im-
plicit=FOR EXAMPLE] ? I can?t think of any reason to
join Lloyd?s now, ? says Keith Whitten, a British business-
man and a Lloyd?s member since 1979.
Temporal a1 VBD plunged a2 VBZ is a2 RB later
a1 VBD was a2 VBD responded a1a2 PRP he
a1 WRB when a1 PRP he a1 VBZ is a2 VBP are
For Temporal, verbs like plunge and responded
are selected. Words such as plunged are quite do-
main specific to stock markets, but words such as
later and responded are likely more general indi-
cators of the relation.
The presence of pronouns was also a predictive
feature. Consider the following example:
Ex. A Yale law school graduate , he began his career in cor-
porate law and then put in years at Metromedia Inc. and the
William Morris talent agency. [Implicit=THEN] In 1976, he
joined CBS Sports to head business affairs and, five years
later, became its president.
Overall, it is fairly easy to see that certain se-
mantic information was captured by these fea-
tures, such as similar structures in a pair of sen-
tences holding a contrast relation, the use of verbs
in a Temporal relation. However, it is rather unset-
tling to also see that some of these characteristics
are largely style or domain specific. For exam-
ple, for an Instantiation in an educational scenario
where the tutor provides an example for a concept,
it is highly unlikely that attribution features will be
helpful. Therefore, part of the question of finding
a general class of features that carry over to other
styles or domains of text still remain unanswered.
7 Per-relation evaluation
Table 8 lists the F-scores and accuracies of each
representation mentioned in this work for predict-
ing individual relation classes. For each relation,
the representations are ordered by decreasing F-
score. We tested the results for statistical signifi-
cance of the change in F-score. We compare all
the representations with the best and the worse
representations for the relation. A ?Y? marks a
significance level of p ? 0.05 for the comparison
with the best or worst representation, a ?T? marks
a significance level of p ? 0.1, which means a
tendency towards significance.
For all relations, production sticks, either with
or without feature selection, is the top represen-
tation. Sticks without lexical items also under-
perform those including the lexical items for 6 of
the 7 relations. Notably, production rules without
lexical items are among the three worst represen-
tations, outperforming only the pure lexical fea-
tures in some cases. This is a strong indication
that being both a sparse syntactic representation
and lacking lexical information, these features are
not favored in this task. Pure lexical features give
the worst or second to worst F scores, significantly
worse than the alternatives in most of the cases.
In Table 7 we list the binary classification re-
sults from prior work: feature selected word pairs
(Pitler et al., 2009), aggregated word pairs (Biran
and McKeown, 2013), production rules only (Park
and Cardie, 2012), and the best combination pos-
sible from a variety of features (Park and Cardie,
2012), all of which include production rules. We
aim to compare the relative gains in performance
with different representations. Note that the abso-
lute results from prior work are not exactly com-
parable to ours for two reasons ? the training
204
Sys. Pitler et al. Biran-McKeown
Feat. wordpair-implicit aggregated wp
Comp. 20.96 (42.55) 24.38 (61.72)
Cont. 43.79 (61.92) 44.03 (66.78)
Expa. 63.84 (60.28) 66.48 (60.93)
Temp. 16.21 (61.98) 19.54 (68.09)
Sys. Park-Cardie Park-Cardie
Feat. prodrules best combination
Comp. 30.04 (75.84) 31.32 (74.66)
Cont. 47.80 (71.90) 49.82 (72.09)
Expa. 77.64 (69.60) 79.22 (69.14)
Temp. 20.96 (63.36) 26.57 (79.32)
Table 7: F-score (accuracy) of prior systems. Note
that the absolute numbers are not exactly compa-
rable with ours because of the important reasons
explained in this section.
and testing sets are different; how Expansion, En-
tRel/NoRel and AltLex relations are treated differ-
ently in each work. The only meaningful indicator
here is the absolute size of improvement. The table
shows that our introduction of production sticks
led to improvements comparable to those reported
in prior work.
The aggregated word pair is a less sparse ver-
sion of the word pair features, where each pair
is converted into weights associated with an ex-
plicit connective. Just as the less sparse binary
lexical representation presented previously, the ag-
gregated word pairs also gave better performance.
None of the three lexical features, however, sur-
passes raw production rules, which again echoes
our finding that binary lexical features are not bet-
ter than the full production rules. Finally, we
note that a combination of features gives better F-
scores.
8 Discussion: are the features
complementary?
So far we have discussed how different represen-
tations for lexical and syntactic features can af-
fect the classifier performances. We focused on
the dilemma of how to reduce sparsity while still
preserving the useful lexical features. An impor-
tant question remains as whether these representa-
tions are complementary, that is, how different is
the classifier behaving under different feature sets
and if it makes sense to combine the features.
We compare the classifier output on the test data
with two methods in Table 9: the Q-statistic and
the percentage of data which the two classifiers
disagree (Kuncheva and Whitaker, 2003).
sig- sig-
Representation F (A) best worst
Comparison
sticks-?
2
27.78 (62.83) N/A Y
prodrules 27.65 (59.5) - Y
sticks 27.50 (60.73) - Y
sticks-nolex 27.01 (59.63) - Y
prodrules-nolex 26.40 (58.47) T Y
binary-lexical 24.73 (58.32) Y -
word-pairs 22.68 (45.03) Y N/A
Conjunction
sticks 27.55 (63.82) N/A T
sticks-?
2
27.53 (64.06) - T
prodrules 27.02 (63.91) - -
sticks-nolex 26.56 (61.03) T -
binary-lexical 25.90 (61.77) Y -
prodrules-nolex 25.20 (62.83) T N/A
word-pairs 25.18 (74.51) T -
Contingency
sticks 48.90 (67.49) N/A Y
sticks-?
2
48.55 (67.76) - Y
sticks-nolex 48.08 (67.69) - Y
prodrules 47.14 (65.61) T Y
prodrules-nolex 45.79 (63.99) Y Y
binary-lexical 44.17 (62.68) Y Y
word-pairs 40.57 (50.53) Y N/A
Expansion
sticks 56.48 (61.75) N/A Y
sticks-?
2
56.30 (62.26) - Y
sticks-nolex 55.43 (60.56) - Y
prodrules 55.42 (61.05) - Y
binary-lexical 54.20 (59.26) Y -
word-pairs 53.65 (56.64) Y -
prodrules-nolex 53.53 (58.79) Y N/A
Instantiation
sticks-?
2
30.34 (74.54) N/A Y
sticks 29.93 (73.80) - Y
prodrules 29.59 (72.20) - Y
sticks-nolex 28.22 (72.66) Y Y
prodrules-nolex 27.83 (70.72) Y Y
binary-lexical 27.29 (70.05) Y Y
word-pairs 20.22 (51.00) Y N/A
Restatement
sticks 35.74 (61.45) N/A Y
sticks-?
2
34.93 (61.42) - Y
sticks-nolex 34.62 (61.08) T Y
prodrules 33.52 (58.54) T Y
prodrules-nolex 32.05 (56.84) Y -
binary-lexical 31.27 (57.41) Y T
word-pairs 29.81 (47.42) Y N/A
Temporal
sticks-?
2
17.97 (66.67) N/A Y
sticks-nolex 17.08 (65.27) T Y
sticks 17.04 (65.22) T Y
prodrules 15.51 (64.04) Y -
prodrules-nolex 15.29 (62.56) Y -
binary-lexical 14.97 (61.92) Y -
word-pairs 14.10 (75.38) Y N/A
Table 8: F-score (accuracy) of each relation for
each feature representation. The representations
in each relation are sorted in descending order.
The column ?sig-best? marks the significance test
result against the best representation, the col-
umn ?sig-worst? marks the significance test re-
sult against the worst representation. ?Y? denotes
p ? 0.05, ?T? denotes p ? 0.1.
205
Q-statistic is a measure of agreement between
two systems s
1
and s
2
formulated as follows:
Q
s
1
,s
2
=
N
11
N
00
?N
01
N
10
N
11
N
00
+N
01
N
10
Where N denotes the number of instances, a sub-
script 1 on the left means s
1
is correct, and a sub-
script 1 on the right means s
2
is correct.
There are several rather surprising findings.
Most notably, word pairs and binary lexical repre-
sentations give very different classification results
in each relation. Their predictions disagree on at
least 25% of the data. This finding drastically con-
trast the fact that they are both lexical features and
that they both make use of the argument annota-
tions in the PDTB. A comparison of the percent-
ages and their differences in F scores or accuracies
easily shows that it is not the case that binary lex-
ical models correctly predict instances word pairs
made mistakes on, but that they are disagreeing in
both ways. Thus, given the previous discussion
that lexical items are useful, it is possible the most
suitable representation would combine both views
of lexical distribution.
Even more surprisingly, the difference in classi-
fier behavior is not as big when we compare lex-
ical and syntactic representations. The disagree-
ment of production sticks with and without lexi-
cal features are the smallest, even though, as we
have shown previously, the majority of production
sticks are lexical features with part-of-speech tags.
If we compare binary lexical features with produc-
tion sticks, the disagreement becomes bigger, but
still not as big as word pairs vs. binary lexical.
Besides the differences in classification, the big-
ger picture of improving implicit discourse rela-
tion classification is finding a set of feature repre-
sentations that are able to complement each other
to improve the classification. A direct conclusion
here is that one should not limit the focus on fea-
tures in different categories (for example, lexical
or syntax), but also features in the same category
represented differently (for example, word pairs or
binary lexical).
9 Conclusion
In this work we study implicit discourse relation
classification from the perspective of the interplay
between lexical and syntactic feature representa-
tion. We are particularly interested in the trade-
off between reducing sparsity and preserving lex-
ical features. We first emphasize the important
Rel. Q-stat Disagreement
word-pairs vs. binary-lexical
Comparison 0.65 33.55
Conjunction 0.71 28.47
Contingency 0.81 26.35
Expansion 0.69 29.38
Instantiation 0.75 31.33
Restatement 0.76 28.42
Temporal 0.25 25.34
binary-lexical vs. sticks
Comparison 0.78 25.49
Conjunction 0.78 24.67
Contingency 0.86 20.68
Expansion 0.80 24.28
Instantiation 0.83 20.75
Restatement 0.76 26.72
Temporal 0.86 20.61
sticks vs. prodrules
Comparison 0.88 19.77
Conjunction 0.89 18.43
Contingency 0.94 14.00
Expansion 0.88 19.18
Instantiation 0.90 16.34
Restatement 0.89 18.88
Temporal 0.90 17.94
sticks vs. sticks-nolex
Comparison 0.94 14.61
Conjunction 0.92 16.63
Contingency 0.97 10.16
Expansion 0.91 17.35
Instantiation 0.97 9.51
Restatement 0.97 11.26
Temporal 0.98 8.42
Table 9: Q statistic and disagreement of different
classes of representations
role of sparsity for traditional word-pair represen-
tations and how a less sparse representation could
improve performance. Then we proposed a less
sparse feature representation for production rules,
the best feature category so far, that further im-
proves classification. We study the role of lexical
features and show the contrast between the spar-
sity problem they brought along and their domi-
nant presence in the highly ranked features. Also,
lexical features included in syntactic features that
are most informative to the classifiers are found to
be style or domain specific in certain relations. Fi-
nally, we compare the representations in terms of
classifier disagreement and showed that within the
same feature category different feature representa-
tion can also be complementary with each other.
References
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL): Short Papers, pages 69?73.
206
Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and refining
rhetorical-semantic relation models. In Human Lan-
guage Technologies: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 428?435.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 399?409.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169?184.
Ludmila I. Kuncheva and Christopher J. Whitaker.
2003. Measures of diversity in classifier ensembles
and their relationship with the ensemble accuracy.
Machine Learning, 51(2):181?207, May.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 368?375.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics - Special issue on using large
corpora, 19(2):313?330.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 108?112.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP (ACL-IJCNLP),
pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: An assessment. Natural Language En-
gineering, 14(3):369?416, July.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING),
pages 1507?1514.
207
