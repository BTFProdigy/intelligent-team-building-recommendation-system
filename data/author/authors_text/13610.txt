Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 725?728,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Investigations into the Crandem Approach to Word Recognition
Rohit Prabhavalkar, Preethi Jyothi, William Hartmann, Jeremy Morris, and Eric Fosler-Lussier
Department of Computer Science and Engineering
The Ohio State University, Columbus, OH
{prabhava,jyothi,hartmanw,morrijer,fosler}@cse.ohio-state.edu
Abstract
We suggest improvements to a previously pro-
posed framework for integrating Conditional
Random Fields and Hidden Markov Models,
dubbed a Crandem system (2009). The pre-
vious authors? work suggested that local la-
bel posteriors derived from the CRF were too
low-entropy for use in word-level automatic
speech recognition. As an alternative to the
log posterior representation used in their sys-
tem, we explore frame-level representations
derived from the CRF feature functions. We
also describe a weight normalization transfor-
mation that leads to increased entropy of the
CRF posteriors. We report significant gains
over the previous Crandem system on the Wall
Street Journal word recognition task.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) have recently emerged as a promising
new paradigm in the domain of Automatic Speech
Recognition (ASR). Unlike Hidden Markov Mod-
els (HMMs), CRFs are direct discriminative models:
they predict the probability of a label sequence con-
ditioned on the input. As a result, CRFs can capture
long-range dependencies in the data and avoid the
need for restrictive independence assumptions. Vari-
ants of CRFs have been successfully used in phone
recognition tasks (Gunawardana et al, 2005; Morris
and Fosler-Lussier, 2008; Hifny and Renals, 2009).
While the improvements in the phone recognition
task are encouraging, recent efforts have been di-
rected towards extending the CRF paradigm to the
word recognition level (Zweig and Nguyen, 2009;
Morris and Fosler-Lussier, 2009). The Crandem
system (Morris and Fosler-Lussier, 2009) is one of
the promising approaches in this regard. The Cran-
dem system is directly inspired by the techniques
of the Tandem system (Hermansky et al, 2000),
where phone-label posterior estimates produced by
a Multi-Layer Perceptron (MLP) are transformed
into a suitable acoustic representation for a standard
HMM. In both systems, the frame-based log poste-
rior vector of P (phone|acoustics) over all phones is
decorrelated using the Karhunen-Loeve (KL) trans-
form; unlike MLPs, CRFs take into account the en-
tire label sequence when computing local posteriors.
However, posterior estimates from the CRF tend to
be overconfident compared to MLP posteriors (Mor-
ris and Fosler-Lussier, 2009).
In this paper, we analyze the interplay between
the various steps involved in the Crandem process.
Is the local posterior representation from the CRF
the best representation? Given that the CRF poste-
rior estimates can be overconfident, what transfor-
mations to the posteriors are appropriate?
In Section 2 we briefly describe CRFs and the
Crandem framework. We suggest techniques for im-
proving Crandem word recognition performance in
Section 3. Details of experiments and our results are
discussed in Sections 4 and 5 respectively. We con-
clude with a discussion of future work in Section 6.
2 CRFs and the Crandem System
Conditional random fields (Lafferty et al, 2001) ex-
press the probability of a label sequence Q condi-
tioned on the input data X as a log-linear sum of
725
weighted feature functions,
p(Q|X) =
exp
P
t
P
j ?jsj(qt, X) +
P
j ?jfj(qt?1, qt, X)
Z(X)
(1)
where sj(?) and fj(?) are known as state feature
functions and transition feature functions respec-
tively, and ?j and ?j are the associated weights.
Z(X) is a normalization term that ensures a valid
probability distribution. Given a set of labeled ex-
amples, the CRF is trained to maximize the con-
ditional log-likelihood of the training set. The
log-likelihood is concave over the entire parameter
space, and can be maximized using standard convex
optimization techniques (Lafferty et al, 2001; Sha
and Pereira, 2003). The local posterior probability
of a particular label can be computed via a forward-
backward style algorithm. Mathematically,
p(qt = q|X) =
?t(q|X)?t(q|X)
Z(X)
(2)
where ?t(q|X) and ?t(q|X) accumulate contribu-
tions associated with possible assignments of la-
bels before and after the current time-step t. The
Crandem system utilizes these local posterior val-
ues from the CRF analogously to the way in which
MLP-posteriors are treated in the Tandem frame-
work (Hermansky et al, 2000), by applying a log
transformation to the posteriors. These transformed
outputs are then decorrelated using a KL-transform
and then dimensionality-reduced to be used as a re-
placement for MFCCs in a HMM system. While
the MLP is usually reduced to 39 dimensions, the
standard CRF benefits from a higher dimensionality
reduction (to 19 dimensions). The decorrelated out-
puts are then used as an input representation for a
conventional HMM system.
3 Improving Crandem Recognition
Results
Morris and Fosler-Lussier (2009) indicate that the
local posterior outputs from the CRF model pro-
duces features that are more heavily skewed to the
dominant phone class than the MLP system, leading
to an increase in word recognition errors. In order
to correct for this, we perform a non-linear trans-
formation on the local CRF posterior representa-
tion before applying a KL-transform and subsequent
stages. Specifically, we normalize all of the weights
?j and ?j in Equation 1 by a fixed positive constant
n to obtain normalized weights ??j and ?
?
j . We note
that the probability of a label sequence computed us-
ing the transformed weights, p?(Q|X), is equivalent
to taking the nth-root of the CRF probability com-
puted using the unnormalized weights, with a new
normalization term Z ?(X)
p?(Q|X) =
p(Q|X)1/n
Z ?(X)
(3)
where, p(Q|X) is as defined in Equation 1. Also
observe that the monotonicity of the nth-root func-
tion ensures that if p(Q1|X) > p(Q2|X) then
p?(Q1|X) > p?(Q2|X). In other words, the rank
order of the n-best phone recognition results are not
impacted by this change. The transformation does,
however, increase the entropy between the domi-
nant class from the CRF and its competitors, since
p?(Q|X) < p(Q|X). As we shall discuss in Section
5, this transformation helps improve word recogni-
tion performance in the Crandem framework.
Our second set of experiments are based on the
following observation regarding the CRF posteriors.
As can be seen from Equation 2, the CRF posteri-
ors involve a global normalization over the entire ut-
terance as opposed to the local normalization of the
MLP posteriors in the output softmax layer. This
motivates the use of representations derived from
the CRF that are ?local? in some sense. We there-
fore propose two alternative representations that are
modeled along the lines of the linear outputs from an
MLP. The first uses the sum of the state feature func-
tions, to obtain a vector f state(X, t) for each time
step t and input utterance X of length |Q| dimen-
sions, where Q is the set of possible phone labels
f state(X, t) =
?
?
?
j
?jsj(q,X)
?
?
T
?q ? Q
(4)
where q is a particular phone label. Note that the
lack of an exponential term in this representation en-
sures that the representation is less ?spiky? than the
CRF posteriors. Additionally, the decoupling of the
representation from the transition feature functions
could potentially allow the system to represent rel-
726
ative ambiguity between multiple phones hypothe-
sized for a given frame.
The second ?local? representation that we experi-
mented with incorporates the CRF transition feature
functions as follows. For each utterance X we per-
form a Viterbi decoding of the most likely state se-
quence Qbest = argmaxQ{p(Q|X)} hypothesized
for the utterance X . We then augmented the state
feature representation with the sum of the transition
features corresponding to the phone label hypothe-
sized for the previous frame (qbestt?1) to obtain a vector
f trans(X, t) of length |Q|,
f trans(X, t) =
"
X
j
?jsj(q,X) +
X
j
?jfj(q
best
t?1 , q,X)
#T
(5)
As a final note, following (Morris and Fosler-
Lussier, 2009), our CRF systems are trained using
the linear outputs of MLPs as its state feature func-
tions and transition biases as the transition feature
functions. Hence, f state is a linear transformation of
the MLP linear outputs down to |Q| dimensions.1
Both f state and f trans can thus be viewed as an im-
plicit mapping performed by the CRF of the in-
put feature function dimensions down to |Q| dimen-
sions. Note that the CRF implicitly uses informa-
tion concerning the underlying phone labels unlike
dimensionality reduction using KL-transform.
4 Experimental Setup
To evaluate our proposed techniques, we carried
out word recognition experiments on the speaker-
independent portion of the Wall Street Journal 5K
closed vocabulary task (WSJ0). Since the corpus is
not phonetically transcribed, we first trained a stan-
dard HMM recognition system using PLP features
and produced phonetic transcriptions by force align-
ing the training data. These were used to train an
MLP phone classifier with a softmax output layer,
using a 9-frame window of PLPs with 4000 hidden
layer units to predict one of the 41 phone labels (in-
cluding silence and short pause). The linear outputs
of the MLP were used to train a baseline Tandem
system. We then trained a CRF using the MLP lin-
ear outputs as its state feature functions. We extract
1We note that our system uses an additional state bias feature
that has a fixed value of 1. However, since this is a constant
term, it has no role to play in the derived representation.
System Accuracy (%)
Crandem-baseline 89.4%
Tandem-baseline 91.8%
Crandem-NormMax 91.4%
Crandem-Norm5 92.1%
Crandem-state 91.7%
Crandem-trans 91.0%
Table 1: Word recognition results on the WSJ0 task
local posteriors as well as the two ?local? representa-
tions described in Section 3. These input represen-
tations were then normalized at the utterance level,
before applying a KL-transformation to decorrelate
them and reduce dimensionality to 39 dimensions.
Finally, each of these representations was used to
train a HMM system with intra-word triphones and
16 Gaussians per mixture using the Hidden Markov
Model Toolkit (Young et al, 2002).
5 Results
Results for each of the experiments described in
Section 4 are reported in Table 1 on the 330-
sentence standard 5K non-verbalized test set. The
Crandem-baseline represents the system of (Mor-
ris and Fosler-Lussier, 2009). Normalizing the
CRF weights of the system by either the weight
with largest absolute value (CRF-NormMax) or by
5 (tuned on the development set) leads to signif-
icant improvements (p ? 0.005) over the Cran-
dem baseline. Similarly, using either the state fea-
ture sum (Crandem-state) or the representation aug-
mented with the transition features (Crandem-trans)
leads to significant improvements (p ? 0.005) over
the Crandem baseline. Note that the performance of
these systems is comparable to the Tandem baseline.
To further analyze the results obtained using the
state feature sum representations and the Tandem
baseline, we compute the mean distance for each
phone HMM from every other phone HMM ob-
tained at the end of the GMM-HMM training phase.
The distance between two HMMs is computed as a
uniformly weighted sum of the average distances be-
tween the GMMs of a one-to-one alignment of states
corresponding to the two HMMs. GMM distances
are computed using a 0.5-weighted sum of inter-
dispersions normalized by self-dispersions (Wang et
727
Figure 1: Normalized mean distances for each of the phone models from every other phone model trained using the
Tandem MLP baseline and the state feature sum representation.
al., 2004). Distances between monomodal Gaus-
sian distributions were computed using the Bhat-
tacharyya distance measure. The phone HMM dis-
tances are normalized using the maximum phone
distance for each system. As can be seen in Figure
1, the mean distances obtained from the state feature
sum representation are consistently greater than the
corresponding distances in the Tandem-MLP sys-
tem, indicating larger separability of the phones in
the feature space. Similar trends were seen with the
transition feature sum representation.
6 Conclusions and Future Work
In this paper, we report significant improvements
over the Crandem baseline. The weight normaliza-
tion experiments confirmed the hypothesis that in-
creasing the entropy of the CRF posteriors leads to
better word-level recognition. Our experiments with
directly extracting frame-level representations from
the CRF reinforce this conclusion. Although our ex-
periments with the systems using the state feature
sum and transition feature augmented representation
did not lead to improvements over the Tandem base-
line, the increased separability of the phone models
trained using these representations is encouraging.
In the future, we intend to examine techniques by
which these representations could be used to further
improve word recognition results.
Acknowledgement: The authors gratefully ac-
knowledge support by NSF grants IIS-0643901 and
IIS-0905420 for this work.
References
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. Interspeech.
H. Hermansky, D. Ellis, and S. Sharma. 2000. Tan-
dem connectionist feature stream extraction for con-
ventional hmm systems. ICASSP.
Y. Hifny and S. Renals. 2009. Speech recognition using
augmented conditional random fields. IEEE Trans-
actions on Audio, Speech, and Language Processing,
17(2):354?365.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. ICML.
J. Morris and E. Fosler-Lussier. 2008. Conditional ran-
dom fields for integrating local discriminative classi-
fiers. IEEE Transactions on Acoustics, Speech, and
Language Processing, 16(3):617?628.
J. Morris and E. Fosler-Lussier. 2009. Crandem: Con-
ditional random fields for word recognition. Inter-
speech.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. NAACL.
Xu Wang, Peng Xuan, and Wang Bingxi. 2004. A gmm-
based telephone channel classification for mandarin
speech recognition. ICSP.
S. Young, G. Evermann, T. Hain, D. Kershaw, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and
P. Woodland. 2002. The HTK Book. Cambridge Uni-
versity Press.
G. Zweig and P. Nguyen. 2009. A segmental crf ap-
proach to large vocabulary continuous speech recogni-
tion. ASRU.
728
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 41?49,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Large-scale discriminative language model reranking for voice-search
Preethi Jyothi
The Ohio State University
Columbus, OH
jyothi@cse.ohio-state.edu
Leif Johnson
UT Austin
Austin, TX
leif@cs.utexas.edu
Ciprian Chelba and Brian Strope
Google
Mountain View, CA
{ciprianchelba,bps}@google.com
Abstract
We present a distributed framework for large-
scale discriminative language models that can
be integrated within a large vocabulary con-
tinuous speech recognition (LVCSR) system
using lattice rescoring. We intentionally
use a weakened acoustic model in a base-
line LVCSR system to generate candidate hy-
potheses for voice-search data; this allows
us to utilize large amounts of unsupervised
data to train our models. We propose an ef-
ficient and scalable MapReduce framework
that uses a perceptron-style distributed train-
ing strategy to handle these large amounts of
data. We report small but significant improve-
ments in recognition accuracies on a standard
voice-search data set using our discriminative
reranking model. We also provide an analy-
sis of the various parameters of our models in-
cluding model size, types of features, size of
partitions in the MapReduce framework with
the help of supporting experiments.
1 Introduction
The language model is a critical component of an
automatic speech recognition (ASR) system that as-
signs probabilities or scores to word sequences. It
is typically derived from a large corpus of text via
maximum likelihood estimation in conjunction with
some smoothing constraints. N-gram models have
become the most dominant form of LMs in most
ASR systems. Although these models are robust,
scalable and easy to build, we illustrate a limita-
tion with the following example from voice-search.
We expect a low probability for an ungrammatical
or implausible word sequence. However, for a tri-
gram like ?a navigate to?, a backoff trigram LM
gives a fairly large LM log probability of -0.266 be-
cause both ?a? and ?navigate to? are popular words
in voice-search! Discriminative language models
(DLMs) attempt to directly optimize error rate by
rewarding features that appear in low error hypothe-
ses and penalizing features in misrecognized hy-
potheses. The trigram ?a navigate to? receives a
fairly large negative weight of -6.5 thus decreasing
its chances of appearing as an ASR output. There
have been numerous approaches towards estimat-
ing DLMs for large vocabulary continuous speech
recognition (LVCSR) (Roark et al, 2004; Gao et al,
2005; Zhou et al, 2006).
There are two central issues that we discuss re-
garding DLMs. Firstly, DLM training requires large
amounts of parallel data (in the form of correct tran-
scripts and candidate hypotheses output by an ASR
system) to be able to effectively compete with n-
gram LMs trained on large amounts of text. This
data could be simulated using voice-search logs that
are confidence-filtered from a baseline ASR sys-
tem to obtain reference transcripts. However, this
data is perfectly discriminated by first pass features
and leaves little room for learning. We propose a
novel training strategy of using lattices generated
with a weaker acoustic model (henceforth referred
to as weakAM) than the one used to generate ref-
erence transcripts for the unsupervised parallel data
(referred to as the strongAM). This provides us with
enough errors to derive large numbers of potentially
useful word features; this is akin to using a weak LM
in discriminative acoustic modeling to give more
41
room for diversity in the word lattices resulting in
better generalization (Schlu?ter et al, 1999). We con-
duct experiments to verify whether these weakAM-
trained models will provide performance gains on
rescoring lattices from a standard test set generated
using strongAM (discussed in Section 3.3).
The second issue is that discriminative estima-
tion of LMs is computationally more intensive than
regular N-gram LM estimation. The advent of dis-
tributed learning algorithms (Mann et al, 2009; Mc-
Donald et al, 2010; Hall et al, 2010) and support-
ing parallel computing infrastructure like MapRe-
duce (Ghemawat and Dean, 2004) has made it in-
creasingly feasible to use large amounts of paral-
lel data to train DLMs. We implement a distributed
training strategy for the perceptron algorithm (intro-
duced by McDonald et al (2010) using the MapRe-
duce framework. Our design choices for the MapRe-
duce implementation are specified in Section 2.2
along with its modular nature thus enabling us to
experiment with different variants of the distributed
structured perceptron algorithm. Some of the de-
scriptions in this paper have been adapted from pre-
vious work (Jyothi et al, 2012).
2 The distributed DLM framework:
Training and Implementation details
2.1 Learning algorithm
We aim to allow the estimation of large scale dis-
tributed models, similar in size to the ones in Brants
et al (2007). To this end, we make use of a dis-
tributed training strategy for the structured percep-
tron to train our DLMs (McDonald et al, 2010). Our
model consists of a high-dimensional feature vector
function ? that maps an (utterance, hypothesis) pair
(x, y) to a vector in Rd, and a vector of model pa-
rameters, w ? Rd. Our goal is to find model pa-
rameters such that given x, and a set of candidate
hypotheses Y (typically, as a word lattice or an N-
best list that is obtained from a first pass recognizer),
argmaxy?Y w ? ?(x, y) would be the y ? Y that
minimizes the error rate between y and the correct
hypothesis for x. For our experiments, the feature
vector ?(x, y) consists of AM and LM costs for y
from the lattice Y for x), as well as ?word features?
which count the number of times different N-grams
(of order up to 5 in our experiments) occur in y.
In principle, such a model can be trained us-
ing the conventional structured perceptron algo-
rithm (Collins, 2002). This is an online learning
algorithm which continually updates w as it pro-
cesses the training instances one at a time, over
multiple training epochs. Given a training utter-
ance {xi, yi} (yi ? Yi has the lowest error rate
with respect to the reference transcription for xi,
among all hypotheses in the lattice Yi for xi), if
y??i := argmaxy?Yi w ? ?(xi, y) is not yi, w is up-
dated to increase the weights corresponding to fea-
tures in yi and decrease the weights of features in y??i .
During evaluation, we use parameters averaged over
all utterances and over all training epochs. This was
shown to give substantial improvements in previous
work (Collins, 2002; Roark et al, 2004).
Unfortunately, the conventional perceptron algo-
rithm takes impractically long for the amount of
training examples we have. We make use of a
distributed training strategy for the structured per-
ceptron that was first introduced in McDonald et
al. (2010). The iterative parameter mixing strategy
used in this paradigm can be explained as follows:
the training data T = {xi, yi}
N
i=1 is suitably parti-
tioned into C disjoint sets T1, . . . , TC . Then, a struc-
tured perceptron model is trained on each data set in
parallel. After one training epoch, the parameters in
the C sets are mixed together (using a ?mixture coef-
ficient? ?i for each set Ti) and returned to each per-
ceptron model for the next training epoch where the
parameter vector is initialized with these new mixed
weights. This is formally described in Algorithm 1;
we call it ?Distributed Perceptron?. We also exper-
iment with two other variants of distributed percep-
tron training, ?Naive Distributed Perceptron? and
?Averaged Distributed Perceptron?. These models
easily lend themselves to be implemented using the
distributed infrastructure provided by the MapRe-
duce framework. The following section describes
this infrastructure in greater detail.
2.2 MapReduce implementation details
We propose a distributed infrastructure using
MapReduce (Ghemawat and Dean, 2004) to train
our large-scale DLMs on terabytes of data. The
MapReduce (Ghemawat and Dean, 2004) paradigm,
adapted from a specialized functional programming
construct, is specialized for use over clusters with
42
Algorithm 1 Distributed Perceptron (McDonald et
al., 2010)
Require: Training samples T = {xi, yi}Ni=1
1: w0 := [0, . . . , 0]
2: Partition T into C parts, T1, . . . , TC
3: [?1, . . . , ?C ] := [ 1C , . . . ,
1
C ]
4: for t := 1 to T do
5: for c := 1 to C do
6: w := wt?1
7: for j := 1 to |Tc| do
8: y?tc,j := argmaxy w ??(xc,j , y)
9: ? := ?(xc,j , yc,j)??(xc,j , y?tc,j)
10: w := w + ?
11: end for
12: wtc := w
13: end for
14: wt :=
?C
c=1 ?cw
t
c
15: end for
16: return wT
a large number of nodes. Chu et al (2007) have
demonstrated that many standard machine learning
algorithms can be phrased as MapReduce tasks, thus
illuminating the versatility of this framework. In
relation to language models, Brants et al (2007)
recently proposed a distributed MapReduce infras-
tructure to build Ngram language models having up
to 300 billion n-grams. We take inspiration from
this evidence of being able to build very large mod-
els and use the MapReduce infrastructure for our
DLMs. Also, the MapReduce paradigm allows us to
easily fit different variants of our learning algorithm
in a modular fashion by only making small changes
to the MapReduce functions.
In the MapReduce framework, any computation
is expressed as two user-defined functions: Map and
Reduce. The Map function takes as input a key/value
pair and processes it using user-defined functions to
generate a set of intermediate key/value pairs. The
Reduce function receives all intermediate pairs that
are associated with the same key value. The dis-
tributed nature of this framework comes from the
ability to invoke the Map function on different parts
of the input data simultaneously. Since the frame-
work assures that all the values corresponding to a
given key will be accummulated at the end of all
SSTable 
Feature-
Weights: 
Epoch t+1
SSTable 
Feature-
Weights: 
Epoch t
SSTable 
Utterances
SSTableService
Rerank-Mappers
Identity-Mappers
Reducers
Cache
(per Map chunk)
Figure 1: MapReduce implementation of reranking using
discriminative language models.
the Map invocations on the input data, different ma-
chines can simultaneously execute the Reduce to op-
erate on different parts of the intermediate data.
Any MapReduce application typically imple-
ments Mapper/Reducer interfaces to provide the de-
sired Map/Reduce functionalities. For our models,
we use two different Mappers (as illustrated in Fig-
ure 1) to compute feature weights for one training
epoch. The Rerank-Mapper receives as input a set
of training utterances and also requests for feature
weights computed in the previous training epoch.
Rerank-Mapper then computes feature updates for
the given training data (the subset of the training data
received by a single Rerank-Mapper instance will be
henceforth referred to as a ?Map chunk?). We also
have a second Identity-Mapper that receives feature
weights from the previous training epoch and di-
rectly maps the inputs to outputs which are provided
to the Reducer. The Reducer combines the outputs
from both Rerank-Mapper and Identity-Mapper and
outputs the feature weights for the current training
epoch. These output feature weights are persisted
on disk in the form of SSTables that are an efficient
abstraction to store large numbers of key-value pairs.
The features corresponding to a Map chunk at the
end of training epoch need to be made available to
Rerank-Mapper in the subsequent training epoch.
Instead of accessing the features on demand from
the SSTables that store these feature weights, every
Rerank-Mapper stores the features needed for the
current Map chunk in a cache. Though the number
43
wt-1
Rerank-Mapper
Reducer
1 utt
1
2 utt
2
N
c
utt
Nc
Feat
1
wt
1
Feat
2
wt
2
Feat
M
wt
M
:
:
U
Cache of w
t-1
 maintained by the Mapper
w
curr
 := w
t-1
, ? := 0
For each (key,utt) in U:
Map(key,utt) {
Rerank(utt.Nbest,w
curr
)
? := FeatureDiff(utt)
w
curr
:= w
curr
 + ?
? := Update(?,?)
}
w
t
Reduce(Feat,V[0..n]) {
//V contains all pairs 
//with primary key=Feat
//first key=Feat:0
w
old 
:= V[0]
//aggregate ? from rest
//of V (key=Feat:1)
?* := Aggregate(V[1..n])
w
t
[Feat] :=
Combine(w
old
,?*)
}
For each Feat in 1 to M:
Map(Feat,w
t-1
[Feat]) {
Emit(Feat:0,w
t-1
[Feat])
}
Identity-Mapper
For each Feat in 1 to M:
Emit(Feat:1,?[Feat])
Figure 2: Details of the Mapper and Reducer.
Naive Distributed Perceptron:
- Update(?, ?) returns ? + ?.
- Aggregate([?t1, . . . ,?
t
C ]) returns ?
? =
?C
c=1 ?
t
c.
- Combine(wt?1NP ,?
?) returns wt?1NP + ?
?.
Distributed Perceptron:
- Update and Combine are as for the Naive Distributed Perceptron.
- Aggregate([?t1, . . . ,?
t
C ]) returns ?
? =
?C
c=1 ?c?
t
c.
Averaged Distributed Perceptron: Here, wt = (wtAV , w
t
DP ), and ? = (?,?) contain pairs of values; ?
is used to maintain wtDP and ?, both of which in turn are used to maintain w
t
AV (?
t
c plays the role of ?
t
c in
Distributed Perceptron). Only wtAV is used in the final evaluation and only w
t
DP is used during training.
- Update((?,?), ?) returns (? + ? + ?,? + ?).
- Aggregate([?t1, . . . ,?
t
C ]) where ?
t
c = (?
t
c,?
t
c), returns ?
? = (??,??) where ?? =
?C
c=1 ?
t
c, and
?? =
?C
c=1 ?c?
t
c.
- Combine((wt?1AV , w
t?1
DP ), (?
?,??)) returns ( t?1t w
t?1
AV +
1
tw
t?1
DP +
1
N t?
?, wt?1DP + ?
?).
Figure 3: Update, Aggregate and Combine procedures for the three variants of the Distributed Perceptron algorithm.
of features stored in the SSTables are determined by
the total number of training utterances, the number
of features that are accessed by a Rerank-Mapper
instance are only proportional to the chunk size and
can be cached locally. This is an important imple-
mentation choice because it allows us to estimate
very large distributed models: the bottleneck is no
longer the total model size but rather the cache size
that is in turn controlled by the Map chunk size.
Section 3.2 discusses in more detail about different
model sizes and the effects of varying Map chunk
size on recognition performance.
Figure 1 is a schematic diagram of our entire
framework; Figure 2 shows a more detailed repre-
sentation of a single Rerank-Mapper, an Identity-
Mapper and a Reducer, with the pseudocode of
these interfaces shown inside their respective boxes.
Identity-Mapper gets feature weights from the pre-
vious training epoch as input (wt) and passes them
to the output unchanged. Rerank-Mapper calls the
function Rerank that takes an N-best list of a training
utterance (utt.Nbest) and the current feature weights
44
(wcurr) as input and reranks the N-best list to ob-
tain the best scoring hypothesis. If this differs from
the correct transcript for utt, FeatureDiff computes
the difference in feature vectors corresponding to
the two hypotheses (we call it ?) and wcurr is in-
cremented with ?. Emit is the output function of
a Mapper that outputs a processed key/value pair.
For every feature Feat, both Identity-Mapper and
Rerank-Mapper also output a secondary key (0 or 1,
respectively); this is denoted as Feat:0 and Feat:1.
At the Reducer, its inputs arrive sorted according to
the secondary key; thus, the feature weight corre-
sponding to Feat from the previous training epoch
produced by Identity-Mapper will necessarily ar-
rive before Feat?s current updates from the Rerank-
Mapper. This ensures that wt+1 is updated correctly
starting with wt. The functions Update, Aggregate
and Combine are explained in the context of three
variants of the distributed perceptron algorithm in
Figure 3.
2.2.1 MapReduce variants of the distributed
perceptron algorithm
Our MapReduce setup described in the previ-
ous section allows for different variants of the dis-
tributed perceptron training algorithm to be imple-
mented easily. We experimented with three slightly
differing variants of a distributed training strategy
for the structured perceptron, Naive Distributed Per-
ceptron, Distributed Perceptron and Averaged Dis-
tributed Perceptron; these are defined in terms of
Update, Aggregate and Combine in Figure 3 where
each variant can be implemented by plugging in
these definitions from Figure 3 into the pseudocode
shown in Figure 2. We briefly describe the func-
tionalities of these three variants. The weights at
the end of a training epoch t for a single feature f
are (wtNP , w
t
DP , w
t
AV ) corresponding to Naive Dis-
tributed Perceptron, Distributed Perceptron and Av-
eraged Distributed Perceptron, respectively; ?(?, ?)
correspond to feature f ?s value in ? from Algorithm
1. Below, ?tc,j = ?(xc,j , yc,j) ? ?(xc,j , y?
t
c,j) and
Nc = number of utterances in Map chunk Tc.
Naive Distributed Perceptron: At the end of epoch
t, the weight increments in that epoch from all map
chunks are added together and added to wt?1NP to ob-
tain wtNP .
Distributed Perceptron: Here, instead of adding
increments from the map chunks, at the end of epoch
t, they are averaged together using weights ?c, c = 1
to C, and used to increment wt?1DP to w
t
DP .
Averaged Distributed Perceptron: In this vari-
ant, firstly, all epochs are carried out as in the Dis-
tributed Perceptron algorithm above. But at the end
of t epochs, all the weights encountered during the
whole process, over all utterances and all chunks, are
averaged together to obtain the final weight wtAV .
Formally,
wtAV =
1
N ? t
t?
t?=1
C?
c=1
Nc?
j=1
wt
?
c,j ,
where wtc,j refers to the current weight for map
chunk c, in the tth epoch after processing j utter-
ances and N is the total number of utterances. In
our implementation, we maintain only the weight
wt?1DP from the previous epoch, the cumulative incre-
ment ?tc,j =
?j
k=1 ?
t
c,k so far in the current epoch,
and a running average wt?1AV . Note that, for all c, j,
wtc,j = w
t?1
DP + ?
t
c,j , and hence
N t ? wtAV = N (t? 1)w
t?1
AV +
C?
c=1
Nc?
j=1
wtc,j
= N (t? 1)wt?1AV +Nw
t?1
DP +
C?
c=1
?tc
where ?tc =
?Nc
j=1 ?
t
c,j . Writing ?
? =
?C
c=1 ?
t
c, we
have wtAV =
t?1
t w
t?1
AV +
1
tw
t?1
DP +
1
N t?
?.
3 Experiments and Results
Our DLMs are evaluated in two ways: 1) we ex-
tract a development set (weakAM-dev) and a test
set (weakAM-test) from the speech data that is re-
decoded with a weakAM to evaluate our learning
setup, and 2) we use a standard voice-search test
set (v-search-test) (Strope et al, 2011) to evaluate
actual ASR performance on voice-search. More de-
tails regarding our experimental setup along with a
discussion of our experiments and results are de-
scribed in the rest of the section.
3.1 Experimental setup
We generate training lattices using speech data that
is re-decoded with a weakAM acoustic model and
45
ll
l
l
l
l
0 50 100 150 200
10
20
30
40
50
N
Erro
r Ra
te
l weakAM?dev SER
weakAM?dev WER
v?search?test SER
v?search?test WER
Figure 4: Oracle error rates at word/sentence level for
weakAM-dev with the weak AM and v-search-test with
the baseline AM.
a baseline language model. We use maximum
likelihood trained single mixture Gaussians for our
weakAM. And, we use a sufficiently small base-
line LM (?21 million n-grams) to allow for sub-
real time lattice generation on the training data
with a small memory footprint, without compromis-
ing on its strength. Chelba et al (2010) demon-
strate that it takes much larger LMs to get a sig-
nificant relative gain in WER. Our largest models
are trained on 87,000 hours of speech, or ?350
million words (weakAM-train) obtained by filtering
voice-search logs at 0.8 confidence, and re-decoding
the speech data with a weakAM to generate N-best
lists. We set aside a part of this weakAM-train
data to create weakAM-dev and weakAM-test: these
data sets consist of 328,460/316,992 utterances, or
1,182,756/1,129,065 words, respectively. We use
a manually-transcribed, standard voice-search test
set (v-search-test (Strope et al, 2011)) consisting
of 27,273 utterances, or 87,360 words to evaluate
actual ASR performance using our weakAM-trained
models. All voice-search data used in the experi-
ments is anonymized.
Figure 4 shows oracle error rates, both at the sen-
tence and word level, using N-best lists of utterances
in weakAM-dev and v-search-test. These error rates
are obtained by choosing the best of the top N hy-
potheses that is either an exact match (for sentence
error rate) or closest in edit distance (for word er-
ror rate) to the correct transcript. The N-best lists
for weakAM-dev are generated using a weak AM
and N-best lists for v-search-test are generated us-
ing the baseline (strong) AM. Figure 4 shows these
error rates plotted against a varying threshold N for
the N-best lists. Note there are sufficient word errors
in the weakAM data to train DLMs; also, we observe
that the plot flattens out after N=100, thus informing
us that N=100 is a reasonable threshold to use when
training our DLMs.
Experiments in Section 3.2 involve evaluating
our learning setup using weakAM-dev/test. We
then investigate whether improvements on weakAM-
dev/test translate to v-search-test where N-best are
generated using the strongAM, and scored against
manual transcripts using fully fledged text normal-
ization instead of the string edit distance used in
training the DLM. More details about the impli-
cations of this text normalization on WER can be
found in Section 3.3.
3.2 Evaluating our DLM rescoring framework
on weakAM-dev/test
Improvements on weakAM-dev using different
variants of training for the DLMs
We evaluate the performance of all the variants of
the distributed perceptron algorithm described in
Section 2.2 over ten training epochs using a DLM
trained on ?20,000 hours of speech with trigram
word features. Figure 5 shows the drop in WER
for all the three variants. We observe that the Naive
Distributed Perceptron gives modest improvements
in WER compared to the baseline WER of 32.5%.
However, averaging over the number of Map chunks
as in the Distributed Perceptron or over the total
number of utterances and training epochs as in the
Averaged Distributed Perceptron significantly im-
proves recognition performance; this is in line with
the findings reported in Collins (2002) and McDon-
ald et al (2010) of averaging being an effective way
of adding regularization to the perceptron algorithm.
Our best-performing Distributed Perceptron
model gives a 4.7% absolute (?15% relative)
improvement over the baseline WER of 1-best
hypotheses in weakAM-dev. This, however, could
be attributed to a combination of factors: the use
of large amounts of additional training data for the
DLMs or the discriminative nature of the model.
In order to isolate the improvements brought upon
mainly by the second factor, we build an ML
trained backoff trigram LM (ML-3gram) using the
46
?
? ? ? ? ? ? ? ? ?
2 4 6 8 10
20
25
30
35
Training epochs
Wo
rd E
rror
 Ra
te(W
ER
)
? PerceptronAveragedPerceptronDistributedPerceptron
Naive Distributed-Perceptron
Distributed-Perceptron
Averaged Distributed- t
?
? ? ? ? ? ? ? ? ?
2 4 6 8 10
20
25
30
35
Training epochs
Wo
rd E
rror
 Ra
te(W
ER
)
? PerceptronAveragedPerceptronDistributedPerceptron
Figure 5: Word error rates on weakAM-dev using Per-
ceptron, Distributed Perceptron and AveragedPerceptron
models.
reference transcripts of all the utterances used to
train the DLMs. The N-best lists in weakAM-dev
are reranked using ML-3gram probabilities linearly
interpolated with the LM probabilities from the
lattices. We also experiment with a log-linear
interpolation of the models; this performs slightly
worse than rescoring with linear interpolation.
Table 1: WERs on weakAM-dev using the baseline 1-best
system, ML-3gram and DLM-1/2/3gram.
Data set Baseline
(%)
ML-
3gram
(%)
DLM-
1gram
(%)
DLM-
2gram
(%)
DLM-
3gram
(%)
weakAM-
dev
32.5 29.8 29.5 28.3 27.8
Impact of varying orders of N-gram features
Table 1 shows that our best performing model
(DLM-3gram) gives a significant ?2% absolute
(?6% relative) improvement over ML-3gram. We
Table 2: WERs on weakAM-dev using DLM-3gram,
DLM-4gram and DLM-5gram of six training epochs.
Iteration DLM-
3gram
(%)
DLM-
4gram
(%)
DLM-
5gram
(%)
1 32.53 32.53 32.53
2 29.52 29.47 29.46
3 29.26 29.23 29.22
4 29.11 29.08 29.06
5 29.01 28.98 28.96
6 28.95 28.90 28.87
also observe that most of the improvements come
from the unigram and bigram features. We do not
expect higher order N-gram features to significantly
help recognition performance; we further confirm
this by building DLM-4gram and DLM-5gram that
use up to 4-gram and 5-gram word features, re-
spectively. Table 2 gives the progression of WERs
for six epochs using DLM-3gram, DLM-4gram and
DLM-5gram showing minute improvements as we
increase the order of Ngram features from 3 to 5.
Impact of model size on WER
We experiment with varying amounts of train-
ing data to build our DLMs and assess the impact
of model size on WER. Table 3 shows each model
along with its size (measured in total number of
word features), coverage on weakAM-test in percent
of tokens (number of word features in weakAM-test
that are in the model) and WER on weakAM-test. As
expected, coverage increases with increasing model
size with a corresponding tiny drop in WER as the
model size increases. To give an estimate of the time
complexity of our MapReduce, we note that Model1
was trained in ?1 hour on 200 mappers with a Map
chunk size of 2GB. ?Larger models?, built by in-
creasing the number of training utterances used to
train the DLMs, do not yield significant gains in ac-
curacy. We need to find a good way of adjusting the
model capacity with increasing amounts of data.
Impact of varying Map chunk sizes
We also experiment with varying Map chunk sizes to
determine its effect on WER. Figure 6 shows WERs
on weakAM-dev using our best Distributed Percep-
tron model with different Map chunk sizes (64MB,
512MB, 2GB). For clarity, we examine two limit
cases: a) using a single Map chunk for the entire
training data is equivalent to the conventional struc-
tured perceptron and b) using a single training in-
Table 3: WERs on weakAM-test using DLMs of varying
sizes.
Model Size (in
millions)
Coverage
(%)
WER
(%)
Baseline 21M - 39.08
Model1 65M 74.8 34.18
Model2 135M 76.9 33.83
Model3 194M 77.8 33.74
Model4 253M 78.4 33.68
47
ll l l l l
1 2 3 4 5 6
20
25
30
35
Training epochs
Wor
d Er
ror R
ate(W
ER)
l Map chunk size 64MBMap chunk size 512MBMap chunk size 2GB
Figure 6: Word error rates on weakAM-dev using varying
Map chunk sizes of 64MB, 512MB and 2GB.
stance per Map chunk is equivalent to batch training.
We observe that moving from 64MB to 512MB sig-
nificantly improves WER and the rate of improve-
ment in WER decreases when we increase the Map
chunk size further to 2GB. We attribute these reduc-
tions in WER with increasing Map chunk size to
on-line parameter updates being done on increasing
amounts of training samples in each Map chunk.
3.3 Evaluating ASR performance on
v-search-test using DLM rescoring
We evaluate our best Distributed Perceptron DLM
model on v-search-test lattices that are generated
using a strong AM. We hope that the large rel-
ative gains on weakAM-dev/test translate to simi-
lar gains on this standard voice-search data set as
well. Table 4 shows the WERs on both weakAM-
test and v-search-test using Model 1 (from Table
3)1. We observe a small but statistically significant
(p < 0.05) reduction (?2% relative) in WER on
v-search-test over reranking with a linearly interpo-
lated ML-3gram. This is encouraging because we
attain this improvement using training lattices that
were generated using a considerably weaker AM.
Table 4: WERs on weakAM-test and v-search-test.
Data set Baseline
(%)
ML-3gram
(%)
DLM-3gram
(%)
weakAM-test 39.1 36.7 34.2
v-search-test 14.9 14.6 14.3
It is instructive to analyze why the relative gains in
1We also experimented with the larger Model 4 and saw sim-
ilar improvements on v-search-test as with Model 1.
performance on weakAM-dev/test do not translate to
v-search-test. Our DLMs are built using N-best out-
puts from the recognizer that live in the ?spoken do-
main? (SD) and the manually transcribed v-search-
data transcripts live in the ?written domain? (WD).
The normalization of training data from WD to SD
is as described in Chelba et al (2010); inverse text
normalization (ITN) undoes most of that when mov-
ing text from SD to WD, and it is done in a heuris-
tic way. There is ?2% absolute reduction in WER
when we move the N-best from SD to WD via ITN;
this is how WER on v-search-test is computed by
the voice-search evaluation code. Contrary to this,
in DLM training we compute WERs using string
edit distance between test data transcripts and the
N-best hypotheses and thus we ignore the mismatch
between domains WD and SD. It is quite likely that
part of what the DLM learns is to pick N-best hy-
potheses that come closer to WD, but may not truly
result in WER gains after ITN. This would explain
part of the mismatch between the large relative gains
on weakAM-dev/test compared to the smaller gains
on v-search-test. We could correct for this by apply-
ing ITN to the N-best lists from SD to move to WD
before computing the oracle best in the list. An even
more desirable solution is to build the LM directly
on WD text; text normalization would be employed
for pronunciation generation, but ITN is not needed
anymore (the LM picks the most likely WD word
string for homophone queries at recognition).
4 Conclusions
In this paper, we successfully build large-scale dis-
criminative N-gram language models with lattices
regenerated using a weak AM and derive small but
significant gains in recognition performance on a
voice-search task where the lattices are generated
using a stronger AM. We use a very simple weak
AM and this suggests that there is room for im-
provement if we use a slightly better ?weak AM?.
Also, we have a scalable and efficient MapReduce
implementation that is amenable to adapting mi-
nor changes to the training algorithm easily and al-
lows for us to train large LMs. The latter function-
ality will be particularly useful if we generate the
contrastive set by sampling from text instead of re-
decoding logs (Jyothi and Fosler-Lussier, 2010).
48
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP, pages 858?
867.
C. Chelba, J. Schalkwyk, T. Brants, V. Ha, B. Harb,
W. Neveitt, C. Parada, and P. Xu. 2010. Query lan-
guage modeling for voice search. In Proc. of SLT.
C.T. Chu, S.K. Kim, Y.A. Lin, Y.Y. Yu, G. Bradski, A.Y.
Ng, and K. Olukotun. 2007. Map-reduce for machine
learning on multicore. Proc. NIPS, 19:281.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
J. Gao, H. Yu, W. Yuan, and P. Xu. 2005. Minimum
sample risk methods for language modeling. In Proc.
of EMNLP.
S. Ghemawat and J. Dean. 2004. Mapreduce: Simplified
data processing on large clusters. In Proc. OSDI.
K.B. Hall, S. Gilpin, and G. Mann. 2010. MapRe-
duce/Bigtable for distributed optimization. In NIPS
LCCC Workshop.
P. Jyothi and E. Fosler-Lussier. 2010. Discriminative
language modeling using simulated ASR errors. In
Proc. of Interspeech.
P. Jyothi, L. Johnson, C. Chelba, and B. Strope.
2012. Distributed discriminative language models for
Google voice-search. In Proc. of ICASSP.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models.
Proc. NIPS.
R. McDonald, K. Hall, and G. Mann. 2010. Distributed
training strategies for the structured perceptron. In
Proc. NAACL.
B. Roark, M. Sarac?lar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In
Proc. ACL.
R. Schlu?ter, B. Mu?ller, F. Wessel, and H. Ney. 1999. In-
terdependence of language models and discriminative
training. In Proc. ASRU.
B. Strope, D. Beeferman, A. Gruenstein, and X. Lei.
2011. Unsupervised testing strategies for ASR. In
Proc. of Interspeech.
Z. Zhou, J. Gao, F.K. Soong, and H. Meng. 2006.
A comparative study of discriminative methods for
reranking LVCSR N-best hypotheses in domain adap-
tation and generalization. In Proc. ICASSP.
49
Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 1?9,
Baltimore, Maryland USA, June 27 2014.
c?2014 Association for Computational Linguistics
Revisiting Word Neighborhoods for Speech Recognition
Preethi Jyothi
?
Beckman Institute
University of Illinois, Urbana, IL
pjyothi@illinois.edu
Karen Livescu
Toyota Technological Institute at Chicago
Chicago, IL
klivescu@ttic.edu
Abstract
Word neighborhoods have been suggested
but not thoroughly explored as an ex-
planatory variable for errors in automatic
speech recognition (ASR). We revisit the
definition of word neighborhoods, propose
new measures using a fine-grained artic-
ulatory representation of word pronuncia-
tions, and consider new neighbor weight-
ing functions. We analyze the signifi-
cance of our measures as predictors of er-
rors in an isolated-word ASR system and
a continuous-word ASR system. We find
that our measures are significantly better
predictors of ASR errors than previously
used neighborhood density measures.
1 Introduction
An important pursuit for both human and ma-
chine speech recognition research is to under-
stand the factors that affect word recognition ac-
curacy. In the substantial body of work on hu-
man word recognition, it has been shown that
it is harder to recognize words that have many
?similar? neighboring words than words with few
neighbors (Luce and Pisoni, 1998), and that fre-
quent words are recognized faster and more accu-
rately than are infrequent words (Marslen-Wilson,
1987; Luce and Pisoni, 1998; Vitevitch and Luce,
1999). In the ASR research community, prior
work has also investigated various factors that
benefit or disrupt recognition. Examples of such
factors include word frequency, speaking rate,
and prosodic factors (Fosler-Lussier and Morgan,
1999; Shinozaki and Furui, 2001; Hirschberg et
al., 2004; Goldwater et al., 2010). There has also
been prior work that uses word confusability mea-
sures to predict speech recognition errors (Fosler-
Lussier et al., 2005; Jyothi and Fosler-Lussier,
2009).
?
Supported by a Beckman Postdoctoral Fellowship.
Word neighborhood measures have been stud-
ied more heavily for human word recognition than
as predictors of ASR errors. Although not stud-
ied specifically in prior work (Fosler-Lussier et al.,
2005; Jyothi and Fosler-Lussier, 2009), word con-
fusability measures used in predicting ASR errors
could be utilized to build word neighborhoods.
Goldwater et al. (2010) examine the behavior of
certain standard neighborhood density measures
as predictors of ASR errors. To our knowledge,
this is the only study that explicitly considers word
neighborhoods as a potential factor in ASR.
In this work, we investigate word neighborhood
measures as predictors of ASR errors. We pro-
pose new neighborhood measures that we find to
be more well-suited to ASR than standard neigh-
borhood density measures. We also propose a
new mechanism to incorporate frequency weight-
ing within the measures. Finally, we analyze the
measures as predictors of errors in an isolated-
word recognition system and a continuous-word
recognition system for conversational speech.
2 Related Work: Neighborhood Density
Measures
In much of the prior work in the psycholinguistics
literature, the notion of word similarity is quanti-
fied by a simple one-phone-away rule: A word w
?
is a neighbor of wordw ifw andw
?
differ by a sin-
gle phone, via a substitution, deletion, or insertion.
We refer to this density measure as ?ND?.
ND =
?
w
?
?
ND
(w,w
?
)
where ?
ND
(w,w
?
) = 1 if w and w
?
differ by a
phone and 0 otherwise.
The frequencies of the neighbors are often ac-
counted for in the neighborhood density measure
by computing the sum of the raw (or log) frequen-
cies of a word?s neighbors (Luce and Pisoni, 1998;
Vitevitch and Luce, 1999); the word frequencies
1
are derived from a large corpus. We refer to this
frequency-weighted measure as ?wND?.
wND =
?
w
?
?
ND
(w,w
?
) ? pi(w
?
)
where pi(w
?
) is the frequency of the word w
?
.
1
Both ND and wND are popular measures for word
neighborhoods that we consider to be our base-
lines; Goldwater et al. (2010) also make use of
these two density measures.
2
Neither of these measures account for the fre-
quency of the word itself. In continuous ASR,
which uses a language model, frequent words are
more likely to be recognized correctly (Fosler-
Lussier and Morgan, 1999). To account for this,
instead of using absolute frequencies of the neigh-
boring words, we use their relative frequencies to
define a third baseline density measure,?rwND?
(relative-wND):
rwND =
?
w
?
?
ND
(w,w
?
) ?
pi(w
?
)
pi(w)
Relative frequencies have appeared in prior
work (Luce, 1986; Luce and Pisoni, 1998; Scar-
borough, 2012). In fact, the measure used by Scar-
borough (2012) is the reciprocal of rwND.
3 Proposed Neighborhood Measures
Our new neighborhood measures are defined in
terms of a distance function between a pair of
words, ?, and a weighting function, ?. The pro-
posed measures are not densities in the same sense
as ND, wND, rwND, but are scores that we may
expect to correlate with recognition errors. We de-
fine the neighborhood score for a word w as:
score(w) =
?
w
?
6=w
?(w,w
?
) ??(w,w
?
) (1)
Intuitively, ? is an averaging function that weighs
the importance of each neighboring word. For ex-
ample, Yarkoni et al. (2008) use a neighborhood
measure that gives equal importance to the top
1
Here we use raw rather than log frequencies. The base-
line density measures in this section perform better with raw
rather than log frequencies on our evaluation data. Our pro-
posed measures perform significantly better than the baseline
measures using both raw and log frequencies.
2
Goldwater et al. (2010) also consider the number of ho-
mophones (words that share a pronunciation with the tar-
get word) and frequency-weighted homophones as additional
neighborhood measures. In our data there is insufficient ho-
mophony for these measures to be significant, so we do not
report on experiments using them.
20 closest neighbors and rejects the others. The
rest of the section presents multiple choices for ?
and ? which will define our various neighborhood
measures via Equation 1.
3.1 Distance Functions
All of our distance functions are based on an edit
distance between a pair of words, i.e., the mini-
mum cost incurred in converting one word to the
other using substitutions, insertions and deletions
of the sub-word units in the word. In addition
to binary edit costs, we consider edit costs that
depend on sub-phonetic properties of the phones
rather than a uniform cost across all phones. Sec-
ond, instead of a single pronunciation for a word,
we consider a distribution over multiple pronun-
ciations. These distance functions can be easily
computed via finite-state transducer (FST) opera-
tions, as explained below (see also Figure 1).
Edit Distance (?
ED
): This is the simplest edit
distance function that incurs an equal cost of 1 for
any substitution, insertion, or deletion. To com-
pute the distance between a pair of words, each
word w is represented as a finite state acceptor,
F
w
, that accepts the pronunciations (phone se-
quences) of the word. We also introduce a memo-
ryless transducer, T , that maps an input phone to
any output phone, with arc weights equal to the
corresponding substitution costs (mapping to or
from epsilon indicates a deletion or an insertion).
The weight of the shortest path in the composed
FST, F
w
?T ?F
w
?
, gives the edit distance between
w and w
?
. When either w or w
?
has more than
one pronunciation, ?
ED
is the minimum edit dis-
tance among all pairs of pronunciations. This edit
distance function has been previously proposed
as a measure of phonological similarity between
words (Hahn and Bailey, 2005). Similar distance
functions have also been used for neighborhood
density measures in visual word recognition stud-
ies (Yarkoni et al., 2008).
Simple Articulatory Feature-based Edit Dis-
tance (?
AF
): The distance function ?
ED
pe-
nalizes an incorrect substitution equally regardless
of the phone identity; for example, the phone [p]
can be substituted with [b] or [aa] with equal cost
according to ?
ED
, although we know it is more
likely for [p] to be produced as [b] than as [aa]. To
account for this, we adopt a finer-grained repre-
sentation of the phone as a vector of discrete artic-
ulatory ?features?. Our features are derived from
2
?AF
:
0 1 2 3
?
0
?
0 1 2 3
k ah m k aa
p
k:/3.364
m:p/3.464
k:k/0
? ? ?
?
AFx
:
0 1
2
3
4 5
?
0
?
0 1
2
3
4 5
k
g
gcl
kcl
ah
ax
em
kcl
m
ah
ah n
ax n
k
g
gcl
kcl
aa
ao
pcl
p
pcl
ah n
ah
ax n
k:/3.364
m:p/3.464
k:k/0
? ? ?
Figure 1: Distance functions implemented using finite-state machines.
the vocal tract variables of articulatory phonol-
ogy (Browman and Goldstein, 1992), including
the constriction degrees and locations of the lips,
tongue tip, tongue body, velum and glottis.We bor-
row a particular feature set from (Livescu, 2005).
3
The substitution cost between two phones is de-
fined as the L1 distance between the articulatory
vectors corresponding to the phones. We set the
insertion and deletion costs to the mean substitu-
tion cost between the articulatory vectors for all
phone pairs. These new costs will appear as the arc
weights on the edit transducer T . This is shown
in Figure 1; apart from the difference in the arc
weights on T , ?
AF
is the same as ?
ED
.
Extended Articulatory Feature-based Edit Dis-
tance (?
AFx
): The words in our dictionary are
associated with one or more canonical pronuncia-
tions written as sequences of phones. The distance
functions ?
ED
and ?
AF
make use of this small set
of canonical pronunciations and do not capture the
various other ways in which a word can be pro-
nounced. An alternative, explored in some prior
work on pronunciation modeling (Deng and Sun,
1994; Richardson et al., 2003; Livescu and Glass,
2004; Mitra et al., 2011; Jyothi et al., 2011), is
to model the pronunciation of a word as multiple,
possibly asynchronous streams of fine-grained ar-
ticulatory features, again inspired by articulatory
phonology. Such a model can be implemented as
a dynamic Bayesian network (DBN) with multi-
ple variables representing the articulatory features
3
The mapping of phones to their articulatory feature val-
ues is defined in Appendix B of Livescu (2005). This map-
ping includes a probability distribution over feature values
for certain phones; in these cases, we choose the articulatory
feature value with the highest probability.
in each time frame; please refer to (Livescu and
Glass, 2004; Livescu, 2005; Jyothi et al., 2011)
for more details. In this approach, deviations from
a dictionary pronunciation are the result of either
asynchrony between the articulatory streams (ac-
counting for effects such as nasalization, round-
ing, and epenthetic stops) or the substitution of one
articulatory feature value for another (accounting
for many reduction phenomena).
Jyothi et al. (2012) describe an approach to
encode such a DBN model of pronunciation as
an FST that outputs an articulatory feature tu-
ple for each frame of speech. We modify this
FST by mapping each articulatory feature tuple
to a valid phone as per the phone-to-articulatory-
feature mapping used for ?
AF
(discarding arcs
whose labels do not correspond to a valid phone).
The resulting FSTs are used to define ?
AFx
by
composing with the edit transducer T as in the
definition of ?
AF
. For computational efficiency,
we prune these FSTs to retain only paths that are
within three times the weight of the shortest path.
The pruned FSTs have hundreds of arcs and ?50
states on average. A schematic diagram is used to
illustrate the computation of ?
AFx
in Figure 1.
3.2 Weighting Functions
Our weighting functions can be appropriately de-
fined to discount the contributions of words that
are infrequent or are very far away. We note here
that unlike the density measures in Section 2, the
lower the distance-based score for a word (from
Equation 1), the more confusable it would be with
its neighbors. One approach, as pursued in Nosof-
sky (1986) and Bailey and Hahn (2001), is to use
score(w) =
?
w
?
g(?(w,w
?
)) where g is an expo-
3
r1
r
2
0
0.2
0.4
0.6
0.8
1
?
(
r
)
Figure 2: Let w
1
and w
2
be the two closest
words to w. The area of the shaded region shows
?(w,w
2
) where r
i
= R
w
(w
i
) = i. In the
weighted case given in Equation 4, r
1
= R
?
w
(w
1
),
r
2
= R
?
w
(w
2
) and r
2
? r
1
= ?
w
(w
2
).
nentially decreasing function. This, however, has
the disadvantage of being very sensitive to the dis-
tance measure used: Slight changes in the distance
can alter the score significantly, even if the overall
ordering of the distances is preserved. We propose
an alternative approach that keeps the score as a
linear function of the distances as long as the or-
dering is fixed. For this, we introduce ?(w,w
?
) in
Equation 1 and let it be a (possibly exponentially)
decreasing function of the rank of w
?
.
Formally, we define the rank of w
?
with re-
spect to w, R
w
(w
?
), as follows: Fix an ordering
of all N ? 1 words in the vocabulary other than
w as (w
1
, w
2
, . . . , w
N?1
) such that ?(w,w
i
) ?
?(w,w
i+1
) for all i ? {1, . . . , N ? 2}. Then
R
w
(w
?
) = j if w
?
= w
j
in the above ordering.
We then define ? in terms of a ?decay? function ?:
?(w,w
?
) =
?
R
w
(w
?
)
R
w
(w
?
)?1
?(r)dr (2)
If ? is monotonically decreasing, Equation 2 en-
sures that neighbors with a higher rank (i.e., fur-
ther away) contribute less weight than neighbors
with a lower rank. For example, a measure
that gives equal weight to the k closest neigh-
bors (Yarkoni et al., 2008) corresponds to
?(r) =
{
1 if r ? k
0 otherwise
Instead of a step function that gives equal weight
to all k neighbors, we define ? as an exponen-
tially decreasing function of rank: ?(r) = e
?r
.
Then, from Equation 2, we obtain ?(w,w
?
) =
(e?1)e
?R
w
(w
?
)
. Figure 2 shows the exponentially
decreasing ?(r) and a sample ?(w,w
?
).
We know from prior work that it is also impor-
tant to distinguish among the neighbors depending
on how frequently they appear in the language. To
account for this, we define a frequency-weighted
rank function, R
?
w
(w
?
):
R
?
w
(w
?
) =
R
w
(w
?
)
?
i=1
?
w
(w
i
) (3)
where ?
w
is a suitably defined frequency function
(see below). We now redefine ? as:
?(w,w
?
) =
?
R
?
w
(w
?
)
R
?
w
(w
?
)??
w
(w
?
)
?(r)dr (4)
Note that when ?
w
(w
?
) = 1 for all w
?
, Equation 4
reduces to Equation 2. ?(w,w
?
) is robust in that
it is invariant to the ordering used to define rank,
R
?
w
, i.e. words with the same distance from w can
be arbitrarily ordered. Also, multiple words at the
same distance contribute to ? equally to a single
word at the same distance with a frequency that is
the sum of their frequencies.
We use three choices for ?
w
(w
?
):
1. The first choice is simply ?
w
(w
?
) = 1 for all
w
?
.
2. Let pi(w
?
) be the unigram probability of w
?
. We
then define ?
w
(w
?
) = P ? pi(w
?
) where P is
a scaling parameter. One natural choice for
P is the perplexity of the unigram probability
distribution, pi, i.e., 2
?
?
w
pi(w) log(pi(w))
. With
this choice of P , when pi is a uniform distribu-
tion over all words in the vocabulary, we have
?
w
(w
?
) = 1 for all w
?
, and R
?
w
(w
?
) = R
w
(w
?
).
3. As defined above, ?
w
(w
?
) does not depend on
w. Our third choice for the frequency func-
tion considers the frequency of w
?
relative to
w: ?
w
(w
?
) =
pi(w
?
)
/pi(w)
To summarize, Equation 1 gives the neighbor-
hood score for w in terms of ? and ?. We use
three choices for ? as specified in Section 3. ?
is defined by Equation 4 where R
?
w
is defined
by Equation 3 in terms of the frequency function
?
w
. We use the three choices described above for
?
w
. The resulting nine score functions are sum-
marized in Table 1. For completeness, we also
include the neighborhood density baseline mea-
sures and represent them using our notation with
a distance function defined as ?
ND
(w,w
?
) =
4
Measure ?(r) ?(w,w
?
) ?
w
(w
?
)
ND
1 ?
ND
1
wND pi(w
?
)
rwND
pi(w
?
)
pi(w)
ED
e
?r
?
ED
1
wED pi(w
?
) ? P
rwED
pi(w
?
)
pi(w)
AF
?
AF
1
wAF pi(w
?
) ? P
rwAF
pi(w
?
)
pi(w)
AFx
?
AFx
1
wAFx pi(w
?
) ? P
rwAFx
pi(w
?
)
pi(w)
Table 1: Summary of neighborhood measures.
1(?
ED
(w,w
?
) = 1) (i.e. ?
ND
(w,w
?
) = 1 if
?
ED
(w,w
?
) = 1 and 0 otherwise) and ? = 1.
With ? = 1 and ?(w,w
?
) = ?
w
(w
?
), the three
choices of ?
w
give us ND, wND and rwND, as
shown in Table 1. The notation ?
ND
(w,w
?
) is
to highlight the inverse relationship of the density
measures with our distance-based measures.
4 Experiments
We provide an individual analysis of each neigh-
borhood measure as it relates to recognition error
rate. We also present a matrix of pairwise com-
parisons among all of the neighborhood measures
with respect to their ability to predict recognition
errors. We study the relationship between neigh-
borhood measures and ASR errors in two settings:
? Isolated-word ASR: Psycholinguistic stud-
ies typically use isolated words as stimuli to study
the influence of neighborhood measures on recog-
nition (e.g., see Goldwater et al. (2010) and ref-
erences therein). Motivated by this, we build an
ASR system that recognizes words in isolation
and analyze the relationship between its errors and
each neighborhood measure. Further details of
this analysis are described in Section 4.1.
? Continuous-word ASR: ASR systems typ-
ically deal with continuous speech. However,
the usefulness of neighborhood measures for
continuous-word ASR has received little atten-
tion, with the notable exception of Goldwater et
al. (2010). We further this line of investigation in
our second set of experiments by analyzing the re-
lationship between errors made by a continuous-
word ASR system and our new measures. These
are described in more detail in Section 4.2.
4.1 Isolated-Word ASR
Experimental Setup: We extract isolated words
from a subset of the Switchboard-I conversational
speech corpus (Godfrey et al., 1992) called the
Switchboard Transcription Project, STP (Green-
berg et al., 1996; STP, 1996), which is phonet-
ically labeled at a fine-grained level. Isolated
words were excised from continuous utterances in
sets 20?22 in the STP corpus. We use a total of
401 word tokens (247 unique words) derived from
the 3500 most frequent words in Switchboard-I,
excluding non-speech events and partial words.
These words make up the development and eval-
uation sets used in prior related work on pronun-
ciation modeling (Livescu and Glass, 2004; Jyothi
et al., 2011; Jyothi et al., 2012). We use the dictio-
nary that accompanies the Switchboard-I corpus
consisting of 30,241 words; ?98% of these words
are associated with a single pronunciation.
The recognition system for this isolated word
dataset was built using the Kaldi toolkit (Povey
et al., 2011; Kal, 2011). We use an acous-
tic model that is trained on all of Switchboard-
I, excluding the sentences from which our 401-
word set was drawn. The ASR system uses stan-
dard mel frequency cepstral coefficients with their
first and second derivatives (deltas and double-
deltas) as acoustic features, with standard normal-
ization and adaptation techniques including cep-
stral mean and variance normalization and maxi-
mum likelihood linear regression. Linear discrim-
inant analysis (LDA) and maximum likelihood lin-
ear transform (MLLT) feature-space transforma-
tions were applied to reduce the feature-space di-
mensionality (Povey et al., 2011). The acous-
tic models are standard Gaussian mixture model-
Hidden Markov models (GMM-HMMs) for tied-
state triphones. The recognition vocabulary in-
cludes 3328 words, consisting of the 3500 most
frequent words from Switchboard excluding par-
tial and non-speech words.
4
Since this is an
isolated-word task, the ASR system does not use
any language model.
Results and Discussion: In order to individu-
ally analyze each of the neighborhood measures,
4
Large-vocabulary automatic recognition of isolated
words is a hard task due to the absence of constraints from
a language model. Using the entire Switchboard vocabulary
would greatly deteriorate the recognition performance on an
already hard task. Thus, we restrict the vocabulary to 1/10th
of the original size in order to obtain reasonable performance
from the isolated ASR system.
5
0 10 20 30 40 50 60
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
ND
E
R
0.000 0.010 0.020
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
wND
E
R
5 10 15
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
wAFx
E
R
(a) Neighborhood measures ND, wND and wAFx as predictors of isolated-word error rate (ER).
ND ED AF AFx wND wED wAF wAFx
ND - - - - - - - -
ED - - - - - - - -
AF - - - - - - - -
AFx - - - - - - - -
wND - - - - - - - -
wED - - - - - - - -
wAF - - - - - - - -
wAFx - - - - - - - -
null 5
?6
8
?5
5
?7
1
?7
8
?8
2
?8
3
?10
6
?11
0 0.0001 0.001 0.01 0.05 0.1 1
(b) Pairwise comparison of word neighborhood measures as predictors of errors from the isolated-word ASR system using
p-values. Many low p-values (darker cells) along a column implies the corresponding measure is a significant predictor of ER.
Figure 3: Analysis of neighborhood measures with isolated word ASR.
following Goldwater et al. (2010), we use a logis-
tic regression model implemented using the glm
function in R (R Development Core Team, 2005).
The logistic regression model fits the log-odds of
a binary response variable with a linear combina-
tion of one or more predictor variables. For our
isolated-word task, the response variable takes a
value of either 1 or 0 corresponding to the pres-
ence or absence of an error, respectively; we will
refer to it as ?ER?. We build a separate logis-
tic regression model for each neighborhood mea-
sure acting as the only predictor of ER. We use
restricted cubic splines, using the rcs (Harrell Jr.,
2012) function in R, to model non-linear predic-
tive relationships. In order to determine whether
a neighborhood measure is a significant predictor
of ER, we use a likelihood-ratio test (using the
anova function in R) that compares the fit of the
model including only that neighborhood measure
as a predictor against the fit of a baseline model in-
cluding only an intercept and no other predictors.
All of the neighborhood measures were found to
be significant predictors, with our measures wAF
and wAFx being most significant. The p-values
from this test are shown in a separate row under
the header ?null? in Figure 3(b); here, 5
?6
stands
for 5? 10
?6
and so forth. We note that the neigh-
borhood measures are significantly correlated with
ER as individual predictors, but classifiers built
with each individual measure as the only feature
are not good predictors of ASR errors. This is
unsurprising as we expect many other predictors
other than neighborhood measures, as outlined in
Goldwater et al. (2010), to influence ASR errors.
This paper focuses only on analyzing each neigh-
borhood measure as an individual predictor; joint
models will be explored as part of future work.
Figure 3(a) shows the relationship between er-
rors from the isolated ASR system and three
neighborhood measures: the best-performing
measure (wAFx) and the two standard density
measures (ND, wND). The feature values are ag-
gregated into roughly equal-sized bins and the
average error rate for each bin is plotted. The
6
0.000 0.005 0.010 0.015 0.020
0.
0
0.
2
0.
4
0.
6
wND
IW
ER
0 10 20 30 40
0.
0
0.
2
0.
4
0.
6
rwND
IW
ER
2 4 6 8 10
0.
0
0.
2
0.
4
0.
6
rwAFx
IW
ER
(a) Neighborhood measures wND, rwND and rwAFx as predictors of IWER.
ND ED AF AFx wND wED wAF wAFx rwND rwED rwAF rwAFx
ND - - - - - - - - - - - -
ED - - - - - - - - - - - -
AF - - - - - - - - - - - -
AFx - - - - - - - - - - - -
wND - - - - - - - - - - - -
wED - - - - - - - - - - - -
wAF - - - - - - - - - - - -
wAFx - - - - - - - - - - - -
rwND - - - - - - - - - - - -
rwED - - - - - - - - - - - -
rwAF - - - - - - - - - - - -
rwAFx - - - - - - - - - - - -
null 0.09 0.72 0.08 0.04 0.18 0.14 0.002 0.03 0.001 0.02 2
?5
2
?5
0 0.0001 0.001 0.01 0.05 0.1 1
(b) Pairwise comparison of all word neighborhood measures as predictors of IWER from the continuous-word ASR system.
Figure 4: Analysis of neighborhood measures with continuous-word ASR system.
solid line shows the probability of an error from
the corresponding logistic regression model and
the dashed lines show a 95% confidence interval.
The dotted line is the average error rate from the
entire data set of 401 words, 0.483. The plots
clearly show the inverse relationship between our
distance-based measure (wAFx) and the density
measures (ND and wND). The slope of the fitted
probabilities from the logistic regression model
for a measure is indicative of the usefulness of the
measure in predicting ER. All of the measures are
significant predictors having non-zero slope with a
slightly larger slope for wAFx than ND and wND.
ND and wND being significant predictors of errors
for isolated words is consistent with prior stud-
ies from human speech recognition. The proposed
measures, wAF and wAFx, stand out as the best
predictors of errors. We next analyze the differ-
ences between the measures more closely.
Figure 3(b) shows a pairwise comparison of the
word neighborhood measures. Each cell {i, j}
shows a p-value range from a likelihood-ratio test
that compares the fit of a logistic regression model
using only measure i as a predictor with the fit of a
model using both measures i and j as independent
predictors. Lower p-values (darker cells) indicate
that adding the measure in column j significantly
improves the ability of the model to predict ER, as
opposed to only using the measure along row i.
5
We use such nested models to compare the model
fits using likelihood-ratio significance tests. It is
clear from Figure 3(b) that our measures wAF and
wAFx are the most significant predictors.
5
The relative frequency-weighted measures (rwND,
rwED, rwAF, rwAFx) were omitted since (wND, wED, wAF,
wAFx) are significantly better predictors. This could be be-
cause the isolated-word system has no language model and is
thus unaffected by the target word frequency.
7
4.2 Continuous-word ASR
Experimental Setup: For the continuous-word
task, our evaluation data consists of full sentences
from Switchboard-I that were used to extract the
isolated words in Section 4.1. For our analysis, we
include all the words in the evaluation sentences
that are 3 or more phonemes long and occur 100
times or more in the training set. This gives us a
total of 1223 word tokens (459 word types).
The continuous-word ASR system uses an
acoustic model trained on all of Switchboard-
I excluding the above-mentioned evaluation sen-
tences. The acoustic models are GMM-HMMs for
tied-state triphones using MFCC + delta + double-
delta features with LDA and MLLT feature-space
transformations and speaker adaptation. They are
also trained discriminatively using boosted maxi-
mum mutual information training from the Kaldi
toolkit. We use the entire Switchboard vocabu-
lary of 30,241 words and a 3-gram language model
trained on all of the training sentences. The word
error rate on the evaluation sentences is 28.3%.
6
Results and Discussion: Unlike the isolated-
word task, the continuous-word ASR system gives
word error rates over full utterances. Since we
need to measure the errors associated with the in-
dividual words, we use the individual word er-
ror rate (IWER) metric proposed by Goldwater et
al. (2010). The IWER for wordw
i
is ??in
i
+del
i
+
sub
i
where in
i
is the number of insertions adja-
cent to w
i
; del
i
or sub
i
is 1 if w
i
is either deleted
or substituted, respectively. ? is chosen such that
? ?
?
i
in
i
= I where I is the total number of inser-
tions for the entire dataset.
As in the isolated-word task, we fit logistic re-
gression models to analyze the neighborhood mea-
sures as predictors of IWER. Figure 4(a) shows fit-
ted probabilities from a logistic regression model
for IWER built individually using each of the mea-
sures wND, rwND and rwAFx as predictors. The
number of frequency-weighted neighbors, wND
(as well as the number of neighbors, ND), was
not found to be a significant predictor of IWER.
This is consistent with the findings in Goldwater
et al. (2010) that show weak correlations between
6
The training set includes other utterances from the same
speakers in the STP evaluation utterances. This allows for
an additional boost in performance from the speaker adapted
acoustic models during recognition. Ideally, the training and
evaluation sets should not contain utterances from the same
speakers. We allow for this to get word error rates that are
more comparable to state-of-the-art results on this corpus.
the number of frequency-weighted neighbors and
the probability of misrecognizing a word. How-
ever, we find that using the number of frequency-
weighted neighbors relative to the frequency of
the word (rwND) improves the correlation with
the probability of error (seen in Figure 4(a) as an
increase in slope). Using our proposed distance
measures with relative frequency weighting im-
proves the correlation even further.
Figure 4(b) shows a pairwise comparison of all
measures in Table 1; the interpretation is sim-
ilar to Figure 3(b). We observe that the rela-
tive frequency-weighted measures (rwND, rwED,
rwAF, rwAFx) are consistently better than their
unweighted (ND, ED, AF, AFx) and frequency-
weighted (wND, wED, wAF, wAFx) counterparts,
with rwAF and rwAFx being most significant.
This suggests that the relative frequency-weighted
measures are taking precedence in the continuous-
word task as significant predictors of IWER (un-
like in the isolated-word task) due to the presence
of a strong language model.
5 Conclusion
In this work, we propose new word neighborhood
measures using distances between words that em-
ploy a fine-grained articulatory feature-based rep-
resentation of the word. We present a new rank-
based averaging method to aggregate the word dis-
tances into a single neighborhood score. We also
suggest multiple ways of incorporating frequency
weighting into this score. We analyze the signifi-
cance of our word neighborhood measures as pre-
dictors of errors from an isolated-word ASR sys-
tem and a continuous-word ASR system. In both
cases, our measures perform significantly better
than standard neighborhood density measures.
This work reopens the question of whether word
neighborhood measures are a useful variable for
ASR. There are many possible directions for fu-
ture work. Our measures could be refined fur-
ther, for example by exploring alternative distance
measures, different articulatory feature sets, dif-
ferent choices of ? and ? in the weighting func-
tion, or automatically learned costs and distances.
Also, our analysis currently looks at each neigh-
borhood measure as an individual predictor; we
could jointly analyze the measures to account for
possible correlations. Finally, it may be possible
to use neighborhood measures in ASR confidence
scoring or even directly in recognition as an addi-
tional feature in a discriminative model.
8
References
T. M. Bailey and U. Hahn. 2001. Determinants
of wordlikeness: Phonotactics or lexical neigh-
borhoods? Journal of Memory and Language,
44(4):568?591.
C. P. Browman and L. Goldstein. 1992. Articulatory
phonology: An overview. Phonetica, 49(3-4):155?
180.
L. Deng and D.X. Sun. 1994. A statistical approach
to automatic speech recognition using the atomic
speech units constructed from overlapping articula-
tory features. The Journal of the Acoustical Society
of America, 95(5):2702?2719.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on pronunciations
in conversational speech. Speech Communication,
29(2):137?158.
E. Fosler-Lussier, I. Amdal, and H-K. J. Kuo. 2005. A
framework for predicting speech recognition errors.
Speech Communication, 46(2):153?170.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proc. of ICASSP.
S. Goldwater, D. Jurafsky, and C. D. Manning. 2010.
Which words are hard to recognize? Prosodic,
lexical, and disfluency factors that increase speech
recognition error rates. Speech Communication,
52(3):181?200.
S. Greenberg, J. Hollenback, and D. Ellis. 1996. In-
sights into spoken language gleaned from phonetic
transcription of the Switchboard corpus. In Proc. of
ICSLP.
U. Hahn and T. M. Bailey. 2005. What makes words
sound similar? Cognition, 97(3):227?267.
F. E. Harrell Jr. 2012. RMS: Regression Modeling
Strategies. R package version 3.5-0.
J. Hirschberg, D. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43(1):155?175.
P. Jyothi and E. Fosler-Lussier. 2009. A comparison of
audio-free speech recognition error prediction meth-
ods. In Proc. of Interspeech.
P. Jyothi, K. Livescu, and E. Fosler-Lussier. 2011.
Lexical access experiments with context-dependent
articulatory feature-based models. In Proc. of
ICASSP.
P. Jyothi, E. Fosler-Lussier, and K. Livescu. 2012. Dis-
criminatively learning factorized finite state pronun-
ciation models from dynamic Bayesian networks. In
Proc. of Interspeech.
2011. Kaldi. http://kaldi.sourceforge.
net/.
K. Livescu and J. Glass. 2004. Feature-based pronun-
ciation modeling with trainable asynchrony proba-
bilities. In Proc. of ICSLP.
K. Livescu. 2005. Feature-based Pronunciation Mod-
eling for Automatic Speech Recognition. PhD Dis-
sertation, MIT EECS department.
P. A. Luce and D. B. Pisoni. 1998. Recognizing spo-
ken words: The neighborhood activation model. Ear
and hearing, 19:1?36.
P. A. Luce. 1986. Neighborhoods of words in the men-
tal lexicon. Research on Speech Perception, (Tech-
nical Report No. 6.).
W. D. Marslen-Wilson. 1987. Functional parallelism
in spoken word-recognition. Cognition, 25(1):71?
102.
V. Mitra, H. Nam, C. Y. Espy-Wilson, E. Saltzman,
and L. Goldstein. 2011. Articulatory information
for noise robust speech recognition. IEEE Transac-
tions on Audio, Speech, and Language Processing,
19(7):1913?1924.
R. M. Nosofsky. 1986. Attention, similarity, and the
identification?categorization relationship. Journal
of Experimental Psychology: General, 115(1):39.
D. Povey, A. Ghoshal, et al. 2011. The Kaldi speech
recognition toolkit. Proc. of ASRU.
R Development Core Team. 2005. R: A language and
environment for statistical computing. R foundation
for Statistical Computing.
M. Richardson, J. Bilmes, and C. Diorio. 2003.
Hidden-articulator Markov models for speech recog-
nition. Speech Communication, 41(2-3):511?529.
R. A. Scarborough. 2012. Lexical confusability and
degree of coarticulation. In Proceedings of the An-
nual Meeting of the Berkeley Linguistics Society.
T. Shinozaki and S. Furui. 2001. Error analysis using
decision trees in spontaneous presentation speech
recognition. In Proc. of ASRU.
1996. The Switchboard Transcription Project.
http://www1.icsi.berkeley.edu/
Speech/stp/.
M. S. Vitevitch and P. A. Luce. 1999. Probabilis-
tic phonotactics and neighborhood activation in spo-
ken word recognition. Journal of Memory and Lan-
guage, 40(3):374?408.
T. Yarkoni, D. Balota, and M. Yap. 2008. Mov-
ing beyond Coltheart?s N: A new measure of ortho-
graphic similarity. Psychonomic Bulletin & Review,
15(5):971?979.
9
