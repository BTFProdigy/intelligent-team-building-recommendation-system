Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 955?964,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Acquiring Domain-Specific Dialog Information from Task-Oriented 
Human-Human Interaction through an Unsupervised Learning 
 
Ananlada Chotimongkol Alexander I. Rudnicky 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213, USA Pittsburgh, PA 15213, USA 
ananlada@cs.cmu.edu air@cs.cmu.edu 
 
 
 
 
Abstract 
We describe an approach for acquiring the 
domain-specific dialog knowledge required to 
configure a task-oriented dialog system that 
uses human-human interaction data. The key 
aspects of this problem are the design of a di-
alog information representation and a learning 
approach that supports capture of domain in-
formation from in-domain dialogs. To 
represent a dialog for a learning purpose, we 
based our representation, the form-based di-
alog structure representation, on an observa-
ble structure. We show that this representation 
is sufficient for modeling phenomena that oc-
cur regularly in several dissimilar task-
oriented domains, including information-
access and problem-solving. With the goal of 
ultimately reducing human annotation effort, 
we examine the use of unsupervised learning 
techniques in acquiring the components of the 
form-based representation (i.e. task, subtask, 
and concept). These techniques include statis-
tical word clustering based on mutual infor-
mation and Kullback-Liebler distance, 
TextTiling, HMM-based segmentation, and 
bisecting K-mean document clustering. With 
some modifications to make these algorithms 
more suitable for inferring the structure of a 
spoken dialog, the unsupervised learning algo-
rithms show promise. 
1 Introduction 
In recent dialog management frameworks, such as 
RavenClaw (Bohus and Rudnicky, 2003) and Col-
lagen (Rich et al, 2001), domain-dependent com-
ponents of a dialog manager are clearly separated 
from domain-independent components. This sepa-
ration allows rapid development of a dialog man-
agement module in a new task-oriented domain as 
dialog system developers can focus only on speci-
fying domain-specific dialog information (e.g. the 
Dialog Task Specification in RavenClaw and Task 
Models in Collagen) while general dialog beha-
viors (e.g. turn-taking, confirmation mechanism, 
and generic help) are provided by the framework. 
For task-oriented domains, the domain-specific 
dialog information is equivalent to task-specific 
information. Examples of the task-specific infor-
mation are steps in a task and domain keywords. 
Specifying task-specific knowledge by hand is 
still a time consuming process (Feng et al, 2003). 
Furthermore, the hand-crafted knowledge may not 
reflect users? perceptions of a task (Yankelovich, 
1997). To reduce the subjectivity of system devel-
opers, recorded conversations of humans perform-
ing a similar task as a target dialog system have 
been used to help the developers design the task 
specification. Nevertheless, analyzing a corpus of 
dialogs by hand requires a great deal of human ef-
fort (Bangalore et al, 2006). This paper investi-
gates the feasibility of automating this dialog 
analysis process through a machine-learning ap-
proach. By inferring the task-specific dialog in-
formation automatically from human-human 
interaction data, the knowledge engineering effort 
could be reduced as the developers need to only 
revise learned information rather than analyzing a 
large amount of data.  
Acquiring the task-specific knowledge from a 
corpus of human-human dialogs is considered a 
knowledge acquisition process, where the target 
task structure has not yet been specified but will be 
explored from data before a dialog system is built. 
This is contrasted with a dialog structure recogni-
tion process (Alexandersson and Reithinger, 1997; 
955
Bangalore et al, 2006; Hardy et al, 2004), where 
pre-specified dialog structure components are rec-
ognized as a dialog progresses. 
We use an unsupervised learning approach in 
our knowledge acquisition process as it can freely 
explore the structure in the data without any influ-
ence from human supervision. Woszczyna and 
Waibel (1994) showed that when modeling a di-
alog state transition diagram from data an unsuper-
vised approach outperformed a supervised one as it 
better reflects the characteristic of the data. It is 
also interesting to see how well a machine-learning 
approach can perform on the problem of task-
specific knowledge acquisition when no assump-
tion about the domain is made and no prior know-
ledge is used. 
Examination of task-oriented human-human di-
alogs show that task-specific information can be 
observed in dialog transcription; therefore, it 
should be feasible to be infer it through an unsu-
pervised learning approach. Figure 1 (a) shows a 
dialog in an air travel domain. This dialog is orga-
nized into three parts according to the three steps 
(i.e. reserve a flight, reserve a car, reserve a hotel) 
required to accomplish the task, creating a travel 
itinerary. Domain keywords (highlighted in bold) 
required to accomplish each step are clearly com-
municated.  
To infer task-specific knowledge from data us-
ing an unsupervised learning approach, two prob-
lems need to be addressed: 1) choosing an 
appropriate dialog representation that captures ob-
servable task-specific knowledge in a dialog, and 
2) developing an unsupervised learning approach 
that infers the task-specific knowledge modeled by 
this representation from in-domain human-human 
dialogs. The first problem is discussed in Section 3 
where a form-based dialog structure representa-
tion is proposed. After describing the definition of 
each component in the form-based dialog structure 
representation, examples of how a domain expert 
models the task-specific information in a dialog 
with the form-based representation are given Sec-
tion 3.1. Then the annotation experiment which 
was used to verify that the form-based representa-
tion can be understood and applied by other human 
annotators is discussed in Section 3.2. For the 
second problem, we modify existing unsupervised 
learning approaches to make them suitable for in-
ferring the structure of a spoken dialog. Section 4 
describes these modifications and their perfor-
mances when inferring the components of the 
form-based dialog structure representation from 
interaction data. 
 
 
Figure 1: An example of a dialog in the air travel domain and its corresponding form-based representation  
(d) 
(b) 
(c) 
Form: flight reservation 
FlightInfo: 
FlightInfo: 
Fare: 
PassengerName: 
PaymentMethod: 
Form: hotel reservation 
HotelInfo: 
PassengerName: 
PaymentMethod: 
 
Form: car reservation 
CarInfo: 
PassengerName: 
PaymentMethod: 
 
Client   1:  I?d like to fly to Houston Texas 
Agent  2:  And departing Pittsburgh on what date? 
Client:  3: Departing on February twentieth 
Agent  4: What time would you like to depart Pittsburgh? 
Client   5: Seven a.m. 
Agent  6: The only non-stop flight I have would be on Continental Air-
lines that?s at six thirty a.m. arrive Houston at eight fifty 
Client   7: That?s okay I will take that 
Agent  8: And what day would you be returning? 
Client   9: On Monday February twenty third 
  ... 
Agent  16:  Do you need a car? 
Client  17:  Yeah 
Agent  18: The least expensive rate I have is Thrifty rental car for twen-
ty three ninety a day 
Client  19:  Okay 
Agent  20:  Would you like me to book that car for you? 
Client   21:  Yes 
Agent  22:  Okay and would you need a hotel while you're in Houston? 
Client  23:  Yes 
Agent  24:  And where at in Houston? 
Client  25:  Downtown 
  ... 
re
se
rve
 a
 flig
h
t 
re
se
rve
 a
 ca
r 
re
se
rve
 a
 h
o
te
l 
A
c
tio
n
: m
ake_
 
a_
car _
reservatio
n
 
A
c
tio
n
: m
ake_
 
a_
flig
h
t _
reservatio
n
 
A
c
tio
n
: m
ake_
 
a_
h
o
tel _
reservatio
n
 
(a) 
956
2 Related Work  
Automatic task-specific knowledge acquisition for 
configuring a dialog system is a relatively new re-
search area. Supervised learning approaches were 
used to acquire a task model for a collaborative 
agent (Garland et al, 2001) and task-specific in-
formation for a customer care service (Feng et al, 
2003). These supervised algorithms were trained 
on rich knowledge sources (examples described in 
a specific annotation language and a well-
organized website respectively) annotated by do-
main experts. In contrast, the unsupervised concep-
tual clustering algorithm in DIA-MOLE (M?ller, 
1998) requires no additional human annotation to 
infer a set of domain-specific dialog acts from in-
domain dialogs. The motivation behind the use of 
an unsupervised approach is similar to ours, to re-
duce human effort in creating a new dialog system.  
3 Form-based Dialog Structure Represen-
tation 
Many models have been proposed to account for 
the structure of a human-human conversation. 
Many such models focus on other aspects of a di-
alog such as coordinated activities, i.e. turn-taking 
and grounding, (Traum and Hinkelman, 1992) and 
regular patterns in the dialog (Carletta et al, 1997) 
rather than the domain-specific information com-
municated by participants. More complicated di-
alog representations (Grosz and Sidner, 1986; 
Litman and Allen, 1987) model several aspects of 
a dialog including domain-specific information. 
However, additional components in these models, 
such as beliefs and intentions, are difficult to ob-
serve directly from a conversation and, as for the 
current technology, may not be learnable through 
an unsupervised learning approach. 
Since the task-specific information that we 
would like to model will be used for configuring a 
dialog system, we can view this information from a 
dialog system perspective. Our dialog representa-
tion is based on form, a data representation used in 
a form-based (or frame-based) dialog system. A 
form is a simple representation that captures neces-
sary task-specific information communicated 
through dialog. This information is observable 
from dialog transcription (see below) and thus 
could be inferred through an unsupervised learning 
approach.  
Typically, a form corresponds to a database 
query form while slots in the form represent search 
criteria. Nevertheless, a form can represent related 
pieces of information required to perform any do-
main action not just a database query action. With 
this more general definition of a form, a form-
based dialog structure representation can be ap-
plied to various types of task-oriented domains 
where dialog participants have to gather pieces of 
information, analogous to search criteria, through 
dialog in order to perform domain actions that ful-
fill a dialog goal. Chotimongkol (2008) provided 
examples of these domains, for instance, meeting 
(Banerjee and Rudnicky, 2006) and flight simula-
tion control (Gorman et al, 2003). 
In the form-based dialog structure representa-
tion, task-specific information in each dialog is 
organized into a three-level structure of concept, 
subtask and task. A concept is a word or a group of 
words which captures a piece of information re-
quired to perform a domain action. A subtask is a 
subset of a dialog which contains sufficient con-
cepts to execute a domain action that advances a 
dialog toward its goal. A task is a subset of a di-
alog (usually the entire dialog) which contains all 
the subtasks that belong to the same goal. A sub-
task can also be considered as a step in a task. In 
terms of representation, a task is represented by a 
set of forms, one for each of its subtasks. A con-
cept is a slot in a form.  
To model the structure of a dialog in a new do-
main with the form-based dialog structure repre-
sentation, a list of tasks, subtasks, and concepts in 
that domain has to be specified. This list is consi-
dered a domain-specific tagset. The form-based 
dialog structure framework only provides the defi-
nitions of these components (i.e. task, subtask, and 
concept), which can be regarded as meta-tags and 
are domain-independent. A list of tasks, subtasks, 
and concepts can be identified manually as shown 
in Section 3.1 or automatically through a machine-
learning approach as discussed in Section 4. Sec-
tion 3.1 illustrates how a domain expert models the 
task-specific information in two task-oriented do-
mains, air travel planning (information-accessing) 
and map reading (problem-solving), with the form-
based representation. These examples also show 
that the form-based dialog structure representation 
957
is sufficient for modeling task-specific information 
in dissimilar domains. 
Nonetheless, by focusing on observable task-
specific information and describing this informa-
tion using a simple model, the form-based dialog 
structure representation cannot model the informa-
tion that is not clearly expressed in a dialog. Ex-
ample of such information in an air travel domain 
is the pickup date of a car rental which may not be 
discussed in a dialog as it can be inferred from the 
arrival date of the corresponding flight. Further-
more, the form-based representation is not well 
suited for modeling a complex dialog that has a 
dynamic structure such as a tutoring dialog. 
3.1 Dialog Structure Modeling Examples 
Figure 1 illustrates how a dialog in the air travel 
domain (Eskenazi et al, 1999) can be represented 
with the form-based dialog structure representa-
tion. A dialog in this domain usually has a single 
goal, to create an air-travel itinerary which may 
include hotel and car reservations. Thus, the entire 
dialog corresponds to one task. The dialog in Fig-
ure 1 (a) contains three subtasks, one for each 
make_?_reservation action. The forms that 
represent these subtasks are shown in Figure 1 (b) 
? (d). Each form contains a set of concepts neces-
sary for making the corresponding reservation. For 
a display purpose, the values of these slots are 
omitted. 
A subtask can be further decomposed. For ex-
ample, to reserve a round trip ticket, two database 
lookup actions, one for each leg, are required. A 
reserve_flight subtask in Figure 1 is decomposed 
into two query_flight_info subtasks. The corres-
ponding forms of these subtasks are illustrated in 
Figure 2. Each FlighInfo concept in the flight res-
ervation form is a result of a database lookup ac-
tion that corresponds to each flight query form. 
Figure 2: An example of subtask decomposition 
Figure 3 show a dialog in the map reading do-
main (Anderson et al, 1991) and its corresponding 
form-based dialog structure representation. The 
goal of a dialog in this domain is to have a route 
follower draw a route on his/her map according to 
a description given by a route giver. Since drawing 
an entire route involves several drawing strokes, a 
draw_a_route task is divided into several 
draw_a_segment subtasks, one for each drawing 
action. This action required a set of concepts that 
describe a segment as shown in a segment descrip-
tion form. Since the landmarks on the giver?s map 
can be different from those in the follower?s map, 
the participants have to explicitly define the Loca-
tion of a mismatched Landmark before using it in 
a segment description. In this case grounding be-
comes another subtask and can be represented by a 
form. This type of grounding is not necessary in 
the air travel domain. 
 
Figure 3: An example of a dialog in the map reading domain and its corresponding form-based representation 
  ? 
Giver 3: right, below the start do you have 
a missionary camp? 
Follower 4: yeah.  
Giver 5: okay, well if you take it from the 
start just run horizontally. 
Follower 6: uh-huh. 
Giver 7: to the left for about an inch. 
Follower 8: right. 
Giver 9: then go down along the side of 
the missionary camp. 
  ?. 
Form: grounding 
Landmark: missionary camp 
Location: below the start 
Form: segment description 
StartLocation: the start 
Direction: left 
Distance: an inch 
EndLocation:  
 
 
d
ra
w
_
a
_
se
g
m
e
n
t 
g
ro
u
n
d
in
g
 
A
c
tio
n
:  
d
efin
e_
a_
lan
d
m
ark 
A
c
tio
n
: 
d
raw
in
g
 
Form: flight query 
DepartCity: Houston 
ArriveCity: Pittsburgh 
DepartDate: Monday   
February twenty third 
DepartTime: five p.m. 
Form: flight query 
DepartCity: Pittsburgh 
ArriveCity: Houston 
ArriveState: Texas 
DepartDate: February 
twentieth 
DepartTime: seven a.m. 
Form: flight reservation 
FlightInfo: 
 Airline: Continental 
 DepartTime: six thirty a.m. 
 ArriveCity: Houston 
 ArriveTime: eight fifty 
FlightInfo: 
 Airline: Continental 
 DepartCity: Houston 
 DepartTime: six forty p.m. 
 ArriveCity: Pittsburgh 
 ArriveTime: ten twenty p.m. 
Fare: four hundred dollars  
Name: 
PaymentMethod: 
958
3.2 Annotation Experiment 
The goal of this annotation experiment is to verify 
that the form-based dialog structure framework can 
be understood by human annotators other than its 
developers, and that they can consistently apply the 
framework to model task-specific information in a 
dialog. In this experiment, each annotator had to 
design a form-based dialog structure representation 
for a given task-oriented domain by specifying a 
hierarchical structure of tasks, sub-tasks and con-
cepts in that domain. Note that we are interested in 
the process of designing a domain-specific tagset 
from the definitions of task, subtask, and concept 
provided by the framework, not in the process of 
using an existing tagset to annotate data (see for 
example (Carletta et al, 1997)). The description of 
the framework is provided in annotation guidelines 
along with examples from the domains that were 
not used in the experiment.  
The experimental procedure is as follows: the 
subjects first developed their own tagset according 
to the guidelines by analyzing a set of in-domain 
dialogs, and then annotate those dialogs with the 
tagset they had designed. To obtain enough anno-
tated instances for each dialog structure component 
and to make the annotation simple, the dialog 
structure annotation part of the experiment was 
divided into two sub-parts: concept annotation and 
task/sub-task annotation. Two domains were used 
in the experiment, air travel planning and map 
reading. Four subjects were assigned to each do-
main. None had used the scheme previously. The 
average number of tags that each subject annotated 
is shown in the first row of Table 1. 
Since some variations in tagset designs are ac-
ceptable as long as they conform to the guidelines, 
each subject?s annotation is judged against the 
guidelines rather than one specific reference anno-
tation. An annotation instance is marked as incor-
rect only when it does not conform to the 
guidelines. Each subject?s annotation was eva-
luated by both a coding scheme expert and by oth-
er subjects. Accuracy is computed from the 
expert?s judgment while acceptability is computed 
from peers? judgments. Acceptability scores shown 
in Table 1 were averaged from all other subjects in 
the same group. Please note that the result pre-
sented in this table should not be compared to the 
results from machine-learning approaches pre-
sented in Table 2 and Table 3 as the evaluation 
procedures and data sets are different. 
 
Measure 
Air Travel Map Reading 
C T C T 
Number of tags 178.8 50.5 347.8 60.8 
Accuracy (%) 96.5 89.7 89.0 65.2 
Acceptability (%) 95.6 81.1 94.9 84.5 
Table 1: Accuracy and acceptability on concept annota-
tion (C), and task/subtask annotation (T) 
Both accuracy and acceptability are high for all 
annotation tasks except for the accuracy of 
task/subtask annotation in the map reading domain. 
Most of the errors come from the annotation of the 
grounding subtasks. Since its corresponding ac-
tion is quite difficult to observe, subjects may not 
have a concrete definition of grounding and were 
more likely to produce errors. In addition, they 
were less critical when judging other subjects? an-
notations. Consistency in applying the form-based 
dialog structure representation shows that the re-
presentation is unambiguous and could potentially 
be identified through a machine-learning approach. 
When comparing among components, concepts 
were annotated more consistently than tasks and 
subtasks in terms of both accuracy and acceptabili-
ty. One possible reason is that, a concept is easier 
to observe as its unit is smaller than a task or a sub-
task. Moreover, dialog participants have to clearly 
communicate the concepts in order to execute a 
domain action. The subjects usually agreed on 
tasks and top-level subtasks, but did not quite 
agree on low-level subtasks. The low-level sub-
tasks are correlated with the implementation of a 
dialog system; hence, the designs of these subtasks 
are more subjective and likely to be different.  
4 Learning Approaches  
This section describes machine-learning approach-
es for inferring the task-specific information mod-
eled by the form-based dialog structure 
representation from human-human conversations. 
Specifically, the learning approach has to infer a 
list of tasks, sub-tasks and concepts in a given do-
main from in-domain dialogs similar to what a 
human does in Section 3. To make the problem 
tractable, components in the form-based represen-
tation are acquired separately. For most task-
oriented dialogs that we encountered, each dialog 
corresponds to one task. Hence, the learning effort 
959
can be focused on identifying concept and subtask. 
Since we can only observe instances or values of 
these components in a dialog, we have to first iden-
tify these instances and then make a generalization 
for its type. For instant, to infer that there is a con-
cept City in the air travel domain, a set of city 
names has to be identified and grouped together. 
To identify a set of domain concepts from the 
transcription of in-domain dialogs, we follow the 
algorithm described in (Chotimongkol and Rud-
nicky, 2002). This algorithm utilizes an unsuper-
vised clustering algorithm which clusters words 
based on context similarity, e.g. mutual informa-
tion-based and Kullback-Liebler-based clustering, 
since the members of the same domain concept are 
usually used in similar contexts in a particular do-
main. Examples of the clusters obtained from the 
KL-based clustering algorithm are shown in Figure 
4. These clusters represent Hour, RentalCompa-
ny, and City respectively. Underlined cluster 
members belong to other concepts. The clustering 
algorithm can identify all 12 members of Hour and 
about half of RentalCompany.  In the third clus-
ter, some airport names got merged with city 
names because they occur in quite similar context. 
Figure 4: Learned concepts in the air travel domain 
The rest of this section describes an approach 
for identifying subtasks and their corresponding 
forms in a given domain. We decided to simplify 
the form-learning problem by first segmenting a 
dialog into form-filling episodes (which are equiv-
alent to sub-tasks), then grouping the ones that cor-
respond to the same form together so that we can 
determine a set of necessary slots in each form 
from the concepts present in its corresponding 
cluster. We further simplify the problem by con-
centrating on the domains that have only one top-
level task (though in principle the approach can be 
extended to the domains that have multiple top-
level tasks). Since we utilize well-known unsuper-
vised algorithms, only the modifications which are 
applied to make these algorithms suitable for infer-
ring the structure of a spoken dialog are discussed.  
Two unsupervised discourse segmentation algo-
rithms are investigated: TextTiling (Hearst, 1997) 
and Hidden Markov Modeling (Barzilay and Lee, 
2004). These algorithms only recover the sequence 
of subtasks but not the hierarchical structure of 
subtasks similar to Bangalore et al?s (2006) 
chunk-based model. Nevertheless, this simplifica-
tion is sufficient when a subtask is embedded at the 
beginning or the end of the higher-level subtask 
which is the case for most embedded structures we 
have found. Both algorithms, while performing 
well with expository text, require modifications 
when applying to a fine-grained segmentation 
problem of spoken dialogs. In WSJ text, the aver-
age topic length is 428 words (Beeferman et al, 
1999) while in the air travel domain the average 
subtask length is 84 words (10 utterances). 
For TextTiling, the modifications include a dis-
tance weight and a data-driven stop word list. For 
the subtasks that are much shorter than the average 
length, distant words in the context window can be 
irrelevant. A distance weight demotes the impor-
tance of the context word that is far away from the 
considered boundary by giving it a lower weight. 
A manually prepared stop word list, containing 
common words, may not be suitable for every ap-
plication domain. We propose a novel approach 
that determines a list of stop words directly from 
word distribution in each data set. TextTiling as-
sumes that words that occur regularly throughout a 
dialog are not informative. However, the regularity 
of a particular word is determined from its distribu-
tion over the dialog rather than from its frequency. 
A high frequency word is useful if its instances 
occur only in a specific location. For example, the 
word ?delta? which occurs many times in a re-
serve_flight subtask but does not occur in other 
subtasks is undoubtedly useful for determining 
subtask boundaries while the word ?you? which 
can occur anywhere in a dialog is not useful. Spe-
cifically, a regularity count of word w is defined as 
the number of sliding context windows in the simi-
larity score calculation of TextTiling that contain 
the word w in each dialog. A data-driven stop 
word list contains words that have a regularity 
count greater than a pre-defined threshold. 
For HMM-based segmentation, we modified 
Barzilay and Lee?s (2004) content models by using 
larger text spans when inducing the HMM states. 
HMM states are created automatically by cluster-
ing similar text spans together. When using an ut-
? ONE, TWO, THREE, NINE, SIX, FOUR, SEVEN, FIVE, 
EIGHT, TEN, TWELVE, ELEVEN 
? HERTZ, BUDGET, THRIFTY 
? MIDWAY, LAGUARDIA, GATWICK, PHILADELPHIA, 
DALLAS, DENVER, MONTEREY, BOSTON, CHICAGO, 
AUSTIN, NEWARK, PITTSBURGH, SEATTLE, OTTAWA, 
SYRACUSE, BALTIMORE, HOUSTON, MADRID, L.A., 
ATLANTA, DULLES, HONOLULU 
960
terance as a text span, it may not contain enough 
information to indicate its relevant subtask as some 
utterances in a task-oriented dialog are very short 
and can occur in any subtask (e.g. acknowledge-
ments and yes/no responses). Larger text spans, 
reference topics, were used in (Yamron et al, 
1998). Nevertheless, this approach requires true 
segment boundaries. To eliminate the need of an-
notated data in our algorithm, HMM states are in-
duced from predicted segments generated by 
TextTiling instead. 
After segmenting all dialogs into sequences of 
subtasks, the bisecting K-means clustering algo-
rithm (Steinbach et al, 2000) is used to group the 
segments that belong to the same type of subtask 
together as they represent the same form type. The 
clustering is done based on cosine similarity be-
tween segments. This unsupervised clustering al-
gorithm is also used to infer a set of HMM states in 
the HMM-based segmentation described above.  
Words are used as features for both segmenta-
tion and clustering algorithms. If a set of domain 
concepts has already been identified, we can use 
this information to enhance the features. When 
concept annotation is available, we can incorporate 
a concept label into a representation of a concept 
word. A Label+Word representation joins a word 
string and its label and can help disambiguate be-
tween similar words that belong to different con-
cepts. For instance, ?one? in ?that one? is not the 
same token as ?[Hour]:one?. A Label representa-
tion, on the other hand, only represents a concept 
word by its label. This representation is based on 
the assumption that a list of concepts occurring in 
one subtask is distinguishable from a list of con-
cepts occurring in other subtasks regardless of the 
values of the concepts; hence, a concept label is 
more informative than its value. This representa-
tion provides an abstraction over all different val-
ues of the same concept type. For example, 
[Airline]:northwest and [Airline]:delta are 
represented with the same token [Airline]. In all 
experiments, concept labels are provided by a do-
main expert as we assume that a set of domain 
concepts has already been identified. 
4.1 Dialog Segmentation Results 
To evaluate dialog segmentation performance, we 
compare predicted boundaries against subtask 
boundaries annotated by a domain expert. Subtask 
boundaries could occur only at utterance bounda-
ries. Two metrics are used: Pk (Beeferman et al, 
1999) and concept-based f-measure (C. F-1). Pk 
measures the probability of misclassifying two ut-
terances that are k utterances apart as belonging to 
the same sub-task or different sub-tasks. k is set to 
half the average sub-task length. C. F-1 is a mod-
ification of the standard f-measure (a harmonic 
mean of precision and recall) that gives credit to 
some near misses. Since the segmented dialogs 
will later be used to identify a set of forms and 
their associated slots, the segment that contains the 
same set of concepts as the reference segment is 
acceptable even if its boundaries are slightly dif-
ferent from the reference. For this reason, a near-
miss counts as a match if there is no concept be-
tween the near-miss boundary and the reference 
boundary. 
We evaluated the proposed dialog segmentation 
algorithms with 24 dialogs from the air travel do-
main and 20 dialogs from the map reading domain. 
The window size for TextTiling was set to 4 utter-
ances. The cut-off threshold for choosing subtask 
boundaries was set to ? - ?/2; where ? is the mean 
of the depth scores (Hearst, 1997), the relative 
change in word co-occurrence similarities on both 
sides of a candidate boundary, in each dialog and ? 
is their standard deviation. We found that a small 
window size and a low cut-off threshold are more 
suitable for identifying fine-grained segments as in 
the case of subtasks. However, we also found that 
TextTiling is quite robust as varying these two pa-
rameters doesn?t severely degrade its performance 
(Chotimongkol, 2008). The threshold for selecting 
data-driven stop words was set to ? + 2*?; where ? 
is the mean of the regularity counts of all the words 
in a given dialog and ? is their standard deviation. 
The performance of TextTiling and HMM-based 
segmentation algorithm is shown in Table 2. 
Augmented TextTiling, which uses a data-
driven stop word list, distance weights, and the 
Label+Word representation, performed significant-
ly better than the baseline in both domains. Each of 
these augmenting techniques can on their own im-
prove segmentation performance but not signifi-
cantly. Unsurprisingly, the proposed regularity 
counts discover stop words that are specific to spo-
961
ken dialogs, but are absent from the hand-crafted 
list1, e.g. ?okay? and ?yeah?.  
 
Algorithm 
Air Travel Map Reading 
Pk C. F-1 Pk C. F-1 
TextTiling (baseline) 0.387 0.621 0.412 0.396 
TextTiling (augmented) 0.371 0.712 0.384 0.464 
HMM-based (utterance) 0.398 0.624 0.392 0.436 
HMM-based (segment) 0.385 0.698 0.355 0.507 
HMM-based (segment + 
Label representation) 
0.386 0.706 0.250 0.686 
Table 2: Dialog segmentation results 
For HMM-based segmentation, the segmenta-
tion result obtained when modeling the HMM 
states from predicted subtasks generated by Text-
Tiling (4th row) is better than the result obtained 
when modeling the HMM states from utterances 
(3rd row). Predicted segments provide more context 
to the clustering algorithm that induces the HMM 
states. As a result a more robust state representa-
tion is obtained. A more efficient clustering algo-
rithm can also improve the performance of the 
HMM-based segmentation algorithm since it pro-
vides a state representation that better differentiates 
among dialog segments which belong to dissimilar 
subtasks. When the Label representation which 
yielded a better subtask clustering result (see Sec-
tion 4.2) was used, HMM-based segmentation pro-
duced a better result (5th row) especially in the map 
reading domain. These numbers may appear mod-
est compared to the numbers obtained when seg-
menting expository text. However, predicting the 
boundaries of fine-grained subtasks is more diffi-
cult even with a supervised learning approach (Ar-
guello and Ros?, 2006). Our results are comparable 
to Arguello and Ros??s (2006) results. 
Between the two segmentation algorithms, the 
HMM-based algorithm performed slightly worse 
than TextTiling in the air travel domain but per-
formed significantly better in the map reading do-
main. The HMM-based algorithm can identify 
more boundaries between fine-grained subtasks, 
which occur more often in the map reading do-
main. TextTiling, which relies on local lexical co-
hesion, is unlikely to find two significant drops in 
lexical similarity that are only a couple of utter-
ances apart, and thus fails to detect boundaries of 
short segments. However, HMM-based segmenta-
                                                          
1 http://search.cpan.org/~creamyg/Lingua-StopWords-
0.08/lib/Lingua/StopWords.pm. 
tion misses more boundaries between two subtask 
occurrences of the same type, which occurs more 
often in the air travel domain, as they are usually 
represented by the same state. 
4.2 Subtask Clustering Results 
We evaluated the subtask clustering algorithm on 
the same data set used in the dialog segmentation 
evaluation. Table 3 presents the quality score (QS) 
for each clustering result. These QSs were obtained 
by comparing the output clusters against a set of 
reference subtasks. See (Chotimongkol and Rud-
nicky, 2002) for the definition of QS. 
 
Feature Representation Air Travel Map Reading 
Label+Word (oracle) 0.738 0.791 
Label+Word 0.577 0.675 
Label 0.601 0.823 
Table 3: Subtask clustering results 
When predicted segments were clustered, the 
quality of the output (2nd row) is not as good as 
when the reference segments were used (1st row) as 
inaccurate segment boundaries affected the per-
formance of the clustering algorithm. However, the 
qualities of subtasks that occur frequently are not 
much different. In terms of feature representation, 
the clustering algorithm that uses the Label repre-
sentation achieved better performance in both do-
mains. When the sets of concepts in all of the 
subtasks are disjoint, the clustering algorithm that 
uses the Label representation can achieve a very 
good result as in the map reading domain. This 
result is even better than the result obtained when 
the reference segments were clustered by the algo-
rithm that uses the Label+Word representation. 
These results demonstrate that an appropriate fea-
ture representation provides more useful informa-
tion to the clustering algorithm than accurate 
segment boundaries. However, when the subtasks 
contain overlapping sets of concepts as in the air 
travel domain, the performance gain obtained from 
the Label representation is quite small.  
Figure 5 shows four types of forms in the air 
travel domain that were acquired by the proposed 
form identification approach. The slot names are 
taken from concept labels. The number in paren-
theses is slot frequency in the corresponding clus-
ter. The underlined slots are the ones that belong to 
other forms. Some slots in the car query form are 
962
missing as some instances of its corresponding 
subtask get merged into other clusters. 
 
Figure 5: Examples of forms obtained by the proposed 
unsupervised learning approach 
4.3 Discussions on Learning Approaches 
The results presented in the previous sections show 
that existing unsupervised learning algorithms are 
able to identify components of the form-based di-
alog structure representation.. However, some 
modifications are required to make these algo-
rithms more suitable for inferring the structure of a 
spoken dialog. The advantages of different learn-
ing algorithms can be combined to improve per-
formance. For example, TextTiling and HMM-
based segmentation are good at detecting different 
types of boundaries; therefore, combining the pre-
dictions made by both algorithms could improve 
segmentation performance. Additional features 
such as prosodic features could also be useful.  
Subsequent steps in the learning process are sub-
jected to propagation errors. However, the pro-
posed learning algorithms, which are based on 
generalization of recurring patterns, are able to 
learn from inaccurate information given that the 
number of errors is moderate, so that there are 
enough correct examples to learn from. Given re-
dundant information in dialog corpora, a domain 
knowledge acquisition process does not require 
high learning accuracy and an unsupervised learn-
ing approach is reasonable. The overall quality of 
the learning result is acceptable. The proposed un-
supervised learning approach can infer much use-
ful task-specific dialog information needed for 
automatically configuring a task-oriented dialog 
system from data. 
5 Conclusion and Future Directions 
To represent a dialog for a learning purpose, we 
based our representation, the form-based dialog 
structure representation, on observable informa-
tion. Components of the form-based representation 
can be acquired with acceptable accuracy from 
observable structures in dialogs without requiring 
human supervision. We show that this dialog re-
presentation can capture task-specific information 
in dissimilar domains. Additionally, it can be un-
derstood and applied by annotators other than the 
developers. 
Our investigation shows that it is feasible to au-
tomatically acquire the domain-specific dialog in-
formation necessary for configuring a task-oriented 
dialog system from a corpus of in-domain dialogs. 
This corpus-based approach could potentially re-
duce human effort in dialog system development. 
A limitation of this approach is that it can discover 
only information present in the data. For instance, 
the corpus-based approach cannot identify city 
names absent in the corpus while a human devel-
oper would know to include these. Revision may 
be required to make learned information more ac-
curate and complete before deployment; we expect 
that this effort would be less than the one required 
for manual analysis. A detailed evaluation of cor-
rection effort would be desirable. 
In this paper, task-specific knowledge was ac-
quired from in-domain dialogs without using any 
prior knowledge about the domain. In practice, 
existing knowledge sources about the world and 
the domain, such as WordNet, could be used to 
improve learning. Some human supervision can be 
valuable particularly in the form of semi-
supervised learning and active learning. In particu-
lar a process that integrates human input at appro-
priate times (for example seeding or correction) is 
likely to be part of a successful approach. 
Acknowledgments 
This research was supported by DARPA grant NB 
CH-D-03-0010. The content of the information in 
this publication does not necessarily reflect the 
position or the policy of the US Government, and 
no official endorsement should be inferred. 
Form: flight query 
Airline  (79) 
ArriveTimeMin  (46) 
DepartTimeHour (40) 
DepartTimeMin  (39) 
ArriveTimeHour (36) 
ArriveCity (27) 
FlightNumber (15) 
ArriveAirport (13) 
DepartCity (13) 
 
Form: hotel query 
Fare  (75) 
City (36) 
HotelName (33) 
Area (28) 
ArriveDateMonth (14) 
Form: flight reservation 
Fare (257) 
City (27) 
RentalCompany (17) 
HotelName (15) 
ArriveCity (14) 
AirlineCompany (11) 
Form: car query 
CarType (13) 
City (3) 
State (1) 
963
References  
J. Alexandersson and N. Reithinger. 1997. Learning 
Dialogue Structures From A Corpus. In Proceedings 
of EuroSpeech-97. Rhodes, Greece. 
A. H. Anderson, M. Bader, E. G. Bard, E. Boyle, G. 
Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllis-
ter, J. Miller, C. Sotillo, H. Thompson, and R. Wei-
nert. 1991. The HCRC Map Task Corpus. Language 
and Speech, 34(4):351-366. 
J. Arguello and C. P. Ros?. 2006. Topic Segmentation 
of Dialogue. In Proceedings of Workshop on Analyz-
ing Conversations in Text and Speech. 
S. Banerjee and A. I. Rudnicky. 2006. You Are What 
You Say: Using Meeting Participants? Speech to 
Detect their Roles and Expertise. In the NAACL-HLT 
2006 workshop on Analyzing Conversations in Text 
and Speech. New York, NY. 
S. Bangalore, G. D. Fabbrizio, and A. Stent. 2006. 
Learning the Structure of Task-Driven Human-
Human Dialogs. In Proceedings of COLING/ACL 
2006. Sydney, Australia. 
R. Barzilay and L. Lee. 2004. Catching the Drift: Prob-
abilistic Content Models, with Applications to Gen-
eration and Summarization. In Proceedings of HLT-
NAACL 2004. Boston, MA. 
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statis-
tical Models for Text Segmentation. Machine Learn-
ing, 34(1-3):177-210. 
D. Bohus and A. I. Rudnicky. 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda. In Proceedings of 
Eurospeech2003. Geneva, Switzerland. 
J. Carletta, S. Isard, G. Doherty-Sneddon, A. Isard, J. C. 
Kowtko, and A. H. Anderson. 1997. The reliability of 
a dialogue structure coding scheme. Computational 
Linguistics, 23(1):13-31. 
A. Chotimongkol. 2008. Learning the Structure of Task-
Oriented Conversations from the Corpus of In-
Domain Dialogs, Ph.D. Thesis CMU-LTI-08-001. 
Pittsburgh, Carnegie Mellon University. 
A. Chotimongkol and A. Rudnicky. 2002. Automatic 
Concept Identification in Goal-Oriented Conversa-
tions. In Proceedings of ICSLP 2002. Denver, CO. 
M. Eskenazi, A. Rudnicky, K. Gregory, P. Constanti-
nides, R. Brennan, C. Bennett, and J. Allen. 1999. 
Data Collection and Processing in the Carnegie Mel-
lon Communicator. In Proceedings of Eurospeech 
1999. Budapest, Hungary. 
J. Feng, S. Bangalore, and M. Rahim. 2003. WebTalk: 
Mining Websites for Automatically Building Dialog 
Systems. In Proceedings of ASRU '03. St. Thomas, 
U.S. Virgin Islands. 
A. Garland, N. Lesh, and C. Sidner. 2001. Learning 
Task Models for Collaborative Discourse. In Pro-
ceedings of Workshop on Adaptation in Dialogue 
Systems, NAACL '01. Pittsburgh, PA. 
J. C. Gorman, N. J. Cooke, P. W. Foltz, P. A. Kiekel, 
and M. J. Martin. 2003. Evaluation of Latent Seman-
tic Analysis-based measures of team communications 
content. In Proceedings of the Human Factors and 
Ergonomic Society 47th Annual Meeting, (HFES 
2003). 
B. J. Grosz and C. L. Sidner. 1986. Attention, Inten-
tions, and the Structure of Discourse. Computational 
Linguistics, 12(3):175-204. 
H. Hardy, A. Biermann, R. B. Inouye, A. Mckenzie, T. 
Strzalkowski, C. Ursu, N. Webb, and M. Wu. 2004. 
Data-Driven Strategies for an Automated Dialogue 
System. In Proceedings of ACL '04. Barcelona, 
Spain. 
M. A. Hearst. 1997. TextTiling: Segmenting Text into 
Multi-paragraph Subtopic Passages. Computational 
Linguistics, 23(1):33-64. 
D. Litman and J. Allen. 1987. A Plan Recognition Mod-
el for Subdialogues in Conversations. Cognitive 
Science, 11(2):163-200. 
J.-U. M?ller. 1998. Using Unsupervised Learning for 
Engineering of Spoken Dialogues. In Proceedings of 
AAAI 1998 Spring Symposium on Applying Machine 
Learning to Discourse Processing. 
C. Rich, C. L. Sidner, and N. Lesh. 2001. Collagen: 
applying collaborative discourse theory to human-
computer interaction. AI Magazine, 22(4):15-25. 
M. Steinbach, G. Karypis, and V. Kumar. 2000. A 
Comparison of Document Clustering Techniques. In 
Proceedings of KDD Workshop on Text Mining. 
D. R. Traum and E. A. Hinkelman. 1992. Conversation 
Acts in Task-Oriented Spoken Dialogue. Computa-
tional Intelligence, 8(3):575--599. 
M. Woszczyna and A. Waibel. 1994. Inferring linguistic 
structure in spoken language. In Proceedings of the 
International Conference on Spoken Language 
Processing (ICSLP). 
J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. v. 
Mulbregt. 1998. A Hidden Markov Model Approach 
to Text Segmentation and Event Tracking. In Pro-
ceedings of ICASSP '98. Seattle, WA. 
N. Yankelovich. 1997. Using Natural Dialogs as the 
Basis for Speech Interface Design. In Susann Luper-
foy (Ed.), Automated Spoken Dialog Systems. Cam-
bridge, MA: MIT Press. 
 
964
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 225?232, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Error Handling in the RavenClaw Dialog Management Framework 
Dan Bohus 
Computer Science Department 
Carnegie Mellon University 
Pittsburgh, PA, 15213 
dbohus@cs.cmu.edu 
Alexander I. Rudnicky 
Computer Science Department 
Carnegie Mellon University 
Pittsburgh, PA, 15213 
air@cs.cmu.edu 
 
 
Abstract 
We describe the error handling architect-
ture underlying the RavenClaw dialog 
management framework. The architecture 
provides a robust basis for current and fu-
ture research in error detection and recov-
ery. Several objectives were pursued in its 
development: task-independence, ease-of-
use, adaptability and scalability. We de-
scribe the key aspects of architectural de-
sign which confer these properties, and 
discuss the deployment of this architect-
ture in a number of spoken dialog systems 
spanning several domains and interaction 
types. Finally, we outline current research 
projects supported by this architecture. 
1 Introduction 
Over the last decade, improvements in speech rec-
ognition and other component technologies have 
paved the way for the emergence of complex task-
oriented spoken dialog systems. While traditionally 
the research community has focused on building 
information-access and command-and-control 
systems, recent efforts aim towards building more 
sophisticated language-enabled agents, such as 
personal assistants, interactive tutors, open-domain 
question answering systems, etc. At the other end 
of the complexity spectrum, simpler systems have 
already transitioned into day-to-day use and are 
becoming the norm in the phone-based customer-
service industry. 
Nevertheless, a number of problems remain in 
need of better solutions. One of the most important 
limitations in today?s spoken language interfaces is 
their lack of robustness when faced with under-
standing errors. This problem appears across all 
domains and interaction types, and stems primarily 
from the inherent unreliability of the speech recog-
nition process. The recognition difficulties are 
further exacerbated by the conditions under which 
these systems typically operate: spontaneous spe-
ech, large vocabularies and user populations, and 
large variability in input line quality. In these set-
tings, average word-error-rates of 20-30% (and up 
to 50% for non-native speakers) are quite common. 
Left unchecked, speech recognition errors can 
lead to two types of problems in a spoken dialog 
system: misunderstandings and non-understand-
ings. In a misunderstanding, the system obtains an 
incorrect semantic interpretation of the user?s turn. 
In the absence of robust mechanisms for assessing 
the reliability of the decoded inputs, the system 
will take the misunderstanding as fact and will act 
based on invalid information. In contrast, in a non-
understanding the system fails to obtain an inter-
pretation of the input altogether. Although no false 
information is incorporated in this case, the situa-
tion is not much better: without an appropriate set 
of recovery strategies and a mechanism for diag-
nosing the problem, the system?s follow-up options 
are limited and uninformed. In general, unless 
mitigated by accurate error awareness and robust 
recovery mechanisms, speech recognition errors 
exert a strong negative impact on the quality and 
ultimately on the success of the interactions (Sand-
ers et al 2002). 
Two pathways towards increased robustness 
can be easily envisioned. One is to improve the 
accuracy of the speech recognition process. The 
second is to create mechanisms for detecting and 
gracefully handling potential errors at the conver-
sation level. Clearly, these two approaches do not 
225
stand in opposition and a combined effort would 
lead to the best results. The error handling archi-
tecture we describe in this paper embodies the sec-
ond approach: it aims to provide the mechanisms 
for robust error handling at the dialog management 
level of a spoken dialog system. 
The idea of handling errors through conversa-
tion has already received a large amount of atten-
tion from the research community. On the theore-
tical side, several models of grounding in commu-
nication have been proposed (Clark and Schaefer, 
1989; Traum, 1998). While these models provide 
useful insights into the grounding process as it 
happens in human-human communication, they 
lack the decision-making aspects required to drive 
the interaction in a real-life spoken dialog system. 
In the Conversational Architectures project, Paek 
and Horvitz (2000) address this challenge by de-
veloping a computational implementation of the 
grounding process using Bayesian belief networks. 
However, questions still remain: the structure and 
parameters of the belief networks are handcrafted, 
as are the utilities for the various grounding ac-
tions; due to scalability and task-representation 
issues, it is not known yet how the proposed ap-
proach would transfer and scale to other domains.  
Three ingredients are required for robust error 
handling: (1) the ability to detect the errors, (2) a 
set of error recovery strategies, and (3) a 
mechanism for engaging these strategies at the 
appropriate time. For some of these issues, various 
solutions have emerged in the community. For 
instance, systems generally rely on recognition 
confidence scores to detect potential misunder-
standings (e.g. Krahmer et al, 1999; Walker et al, 
2000) and use explicit and implicit confirmation 
strategies for recovery. The decision to engage 
these strategies is typically based on comparing the 
confidence score against manually preset thresh-
olds (e.g. Kawahara and Komatani, 2000). For 
non-understandings, detection is less of a problem 
(systems know by definition when non-understand-
ings occur). Strategies such as asking the user to 
repeat or rephrase, providing help, are usually en-
gaged via simple heuristic rules. 
At the same time, a number of issues remain 
unsolved: can we endow systems with better error 
awareness by integrating existing confidence an-
notation schemes with correction detection mecha-
nisms? Can we diagnose the non-understanding 
errors on-line? What are the tradeoffs between the 
various non-understanding recovery strategies? 
Can we construct a richer set of such strategies? 
Can we build systems which automatically tune 
their error handling behaviors to the characteristics 
of the domains in which they operate? 
We have recently engaged in a research pro-
gram aimed at addressing such issues. More gener-
ally, our goal is to develop a task-independent, 
easy-to-use, adaptive and scalable approach for 
error handling in task-oriented spoken dialog sys-
tems. As a first step in this program, we have 
developed a modular error handling architecture, 
within the larger confines of the RavenClaw dialog 
management framework (Bohus and Rudnicky, 
2003). The proposed architecture provides the in-
frastructure for our current and future research on 
error handling. In this paper we describe the pro-
posed architecture and discuss the key aspects of 
architectural design which confer the desired prop-
erties. Subsequently, we discuss the deployment of 
this architecture in a number of spoken dialog sys-
tems which operate across different domains and 
interaction types, and we outline current research 
projects supported by the proposed architecture. 
2 RavenClaw Dialog Management 
We begin with a brief overview of the RavenClaw 
dialog management framework, as it provides the 
larger context for the error handling architecture.  
RavenClaw is a dialog management framework 
for task-oriented spoken dialog systems. To date, it 
has been used to construct a large number of sys-
tems spanning multiple domains and interaction 
types (Bohus and Rudnicky, 2003): information 
access (RoomLine, the Let?s Go Bus Information 
System), guidance through procedures (LARRI), 
command-and-control (TeamTalk), taskable agents 
(Vera). Together with these systems, RavenClaw 
provides the larger context as well as a test-bed for 
evaluating the proposed error handling architec-
ture. More generally, RavenClaw provides a robust 
basis for research in various other aspects of dialog 
management, such as learning at the task and dis-
course levels, multi-participant dialog, timing and 
turn-taking, etc. 
A key characteristic of the RavenClaw frame-
work is the separation it enforces between the do-
main-specific and domain-independent aspects of 
dialog control. The domain-specific dialog control 
logic is described by a Dialog Task Specification, 
226
essentially a hierarchical dialog plan provided by 
the system author. A fixed, domain-independent 
Dialog Engine manages the conversation by exe-
cuting the given Dialog Task Specification. In the 
process, the Dialog Engine also contributes a set of 
domain-independent conversational skills, such as 
error handling (discussed extensively in Section 4), 
timing and turn-taking, etc. The system authoring 
effort is therefore minimized and focused entirely 
on the domain-specific aspects of dialog control. 
2.1 The Dialog Task Specification 
A Dialog Task Specification consists of a tree of 
dialog agents, where each agent manages a sub-
part of the interaction. Figure 1 illustrates a portion 
of the dialog task specification from RoomLine, a 
spoken dialog system which can assist users in 
making conference room reservations. The root 
node subsumes several children: Welcome, which 
produces an introductory prompt, GetQuery which 
obtains the time and room constraints from the 
user, DoQuery which performs the database query, 
and DiscussResults which handles the follow-up 
negotiation dialog. Going one level deeper in the 
tree, GetQuery contains GetDate which requests the 
date for the reservation, GetStartTime and GetEnd-
Time which request the times, and so on. This type 
of hierarchical task representation has a number of 
advantages: it scales up gracefully, it can be 
dynamically extended at runtime, and it implicitly 
captures a notion of context in dialog.  
The agents located at the leaves of the tree are 
called basic dialog agents, and each of them im-
plements an atomic dialog action (dialog move). 
There are four types of basic dialog agents: Inform 
? conveys information to the user (e.g. Welcome), 
Request ? asks a question and expects an answer 
(e.g. GetDate), Expect ? expects information with-
out explicitly asking for it, and EXecute ? imple-
ments a domain specific operation (e.g. DoQuery). 
The agents located at non-terminal positions in the 
tree are called dialog agencies (e.g. RoomLine, 
GetQuery). Their role is to plan for and control the 
execution of their sub-agents. For each agent in the 
tree, the system author may specify preconditions, 
completion criteria, effects and triggers; various 
other functional aspects of the dialog agents (e.g. 
state-specific language models for request-agents, 
help-prompts) are controlled through parameters. 
The information the system acquires and ma-
nipulates in conversation is captured in concepts, 
associated with various agents in the tree (e.g. date, 
start_time). Each concept maintains a history of 
previous values, information about current candi-
date hypotheses and their associated confidence 
scores, information about when the concept was 
last updated, as well as an extended set of flags 
which describe whether or not the concept has 
been conveyed to the user, whether or not the con-
cept has been grounded, etc. This rich representa-
tion provides the necessary support for concept-
level error handling. 
Dialog Stack 
Dialog Engine 
Dialog Task 
Specification 
Expectation Agenda 
start_time: [start_time] [time] 
date: [date] 
start_time: [start_time] [time] 
end_time: [end_time] [time] 
date: [date] 
start_time: [start_time] [time] 
end_time: [end_time] [time] 
location: [location] 
network: [with_network]->true,  
                [without_network]->false 
? ? ? 
System: For when do you need the room? 
User:  let?s try two to four p.m. 
Parse:  [time](two) [end_time](to four pm) 
User Input 
RoomLine 
GetQuery 
GetStartTime 
date 
end_time start_time 
RoomLine 
I: Welcome GetQuery 
R: GetDate 
Start-Over 
R: GetStartTime R: GetEndTime 
DiscussResults X: DoQuery 
Figure 1: RavenClaw architecture 
227
2.2 The Dialog Engine 
The Dialog Engine is the core domain-independent 
component which manages the interaction by exe-
cuting a given Dialog Task Specification. The con-
trol algorithms are centered on two data-structures: 
a dialog stack, which captures the dialog structure 
at runtime, and an expectation agenda, which cap-
tures the system?s expectations for the user input at 
each turn in the dialog. The dialog is controlled by 
interleaving Execution Phases with Input Phases. 
During an Execution Phase, dialog agents from 
the tree are placed on, and executed from the dia-
log stack. At the beginning of the dialog, the root 
agent is placed on the stack. Subsequently, the en-
gine repeatedly takes the agent on the top of the 
stack and executes it. When dialog agencies are 
executed, they typically schedule one of their sub-
agents for execution by placing it on the stack. The 
dialog stack will therefore track the nested struc-
ture of the dialog at runtime. Ultimately, the execu-
tion of the basic dialog agents on the leaves of the 
tree generates the system?s responses and actions. 
During an Input Phase, the system assembles 
the expectation agenda, which captures what the 
system expects to hear from the user in a given 
turn. The agenda subsequently mediates the trans-
fer of semantic information from the user?s input 
into the various concepts in the task tree. For the 
interested reader, these mechanisms are described 
in more detail in (Bohus and Rudnicky, 2003) 
Additionally, the Dialog Engine automatically 
provides a number of conversational strategies, 
such as the ability to handle various requests for 
help, repeating the last utterance, suspending and 
resuming the dialog, starting over, reestablishing 
the context, etc. These strategies are implemented 
as library dialog agencies. Their corresponding 
sub-trees are automatically added to the Dialog 
Task Specification provided by the system author 
(e.g. the Start-Over agency in Figure 1.) The auto-
matic availability of these strategies lessens devel-
opment efforts and ensures a certain uniformity of 
behavior both within and across tasks. 
3 The Error Handling Architecture 
The error handling architecture in the RavenClaw 
dialog management framework subsumes two 
main components: (1) a set of error handling 
strategies (e.g. explicit and implicit confirmation, 
asking the user to repeat, etc.) and (2) an error 
handling process which engages these strategies. 
The error handling strategies are implemented 
as library dialog agents. The decision process 
which engages these strategies is part of the Dialog 
Engine. This design, in which both the strategies 
and the decision process are decoupled from the 
dialog task, as well as from each other, provides a 
number of advantages. First, it ensures that the er-
ror handling mechanisms are reusable across dif-
ferent dialog systems. Second, the approach 
guarantees a certain uniformity and consistency in 
error handling behaviors both within and across 
systems. Third, as new error handling strategies are 
developed, they can be easily plugged into any ex-
isting system. Last, but not least, the approach sig-
nificantly lessens the system authoring effort by 
allowing developers to focus exclusively on de-
scribing the dialog control logic. 
The responsibility for handling potential under-
standing errors1 is delegated to the Error Handling 
Process which runs in the Dialog Engine (see Fig-
ure 2). At each system turn, this process collects 
evidence and makes a decision with respect to en-
gaging any of the error handling strategies. When 
necessary, it will insert an error handling strategy 
on the dialog stack (e.g. the ExplicitConfirm 
(start_time) strategy in Figure 2), thus modifying 
on-the-fly the task originally specified by the sys-
tem author. The strategy executes and, once com-
pleted, it is removed from the stack and the dialog 
resumes from where it was left off. 
                                                          
1
 Note that the proposed framework aims to handle 
understanding errors. The corresponding strategies are generic 
and can be applied in any domain. Treatment of domain or 
task-specific errors (e.g. database access error, etc) still needs 
to be implemented as part of the dialog task specification.  
Error Handling 
Strategies 
Error Handling  
Process 
Explicit  
Confirm 
RoomLine 
GetQuery 
GetStartTime 
ExplicitConfirm 
(start_time) 
Dialog Stack 
Evidence 
Figure 2: Error Handling ? Block Diagram 
 
Dialog Task Specification 
 
Dialog Engine 
228
3.1 Error Handling Strategies 
The error handling strategies can be divided into 
two groups: strategies for handling potential mis-
understandings and strategies for handling non-
understandings. 
For handling potential misunderstandings, three 
strategies are currently available: Explicit Confir-
mation, Implicit Confirmation and Rejection. 
For non-understandings, a larger number of er-
ror recovery strategies are currently available: 
AskRepeat ? the system asks the user to repeat; 
AskRephrase ? the system asks the user to re-
phrase; Reprompt ? the system repeats the previous 
prompt; DetailedReprompt ? the system repeats a 
more verbose version of the previous prompt, 
Notify ? the system simply notifies the user that a 
non-understanding has occurred; Yield ? the sys-
tem remains silent, and thus implicitly notifies the 
user that a non-understanding has occurred; 
MoveOn ? the system tries to advance the task by 
giving up on the current question and moving on 
with an alternative dialog plan (note that this strat-
egy is only available at certain points in the dia-
log); YouCanSay ? the system gives an example of 
what the user could say at this point in the dialog; 
FullHelp ? the system provides a longer help mes-
sage which includes an explanation of the current 
state of the system, as well as what the user could 
say at this point. An in-depth analysis of these 
strategies and their relative tradeoffs is available in 
(Bohus and Rudnicky, 2005a). Several sample 
dialogs illustrating these strategies are available 
on-line (RoomLine, 2003).  
3.2 Error Handling Process 
The error handling decision process is imple-
mented in a distributed fashion, as a collection of 
local decision processes. The Dialog Engine auto-
matically associates a local error handling process 
with each concept, and with each request agent in 
the dialog task tree, as illustrated in Figure 3. The 
error handling processes running on individual 
concepts are in charge of recovering from misun-
derstandings on those concepts. The error handling 
processes running on individual request agents are 
in charge or recovering from non-understandings 
on the corresponding requests.  
At every system turn, each concept- and 
request-agent error handling process computes and 
forwards its decision to a gating mechanism, which 
queues up the actions (if necessary) and executes 
them one at a time. For instance, in the example in 
Figure 3, the error handling decision process for 
the start_time concept decides to engage an explicit 
confirmation on that concept, while the other deci-
sion processes do not take any action. In this case 
the gating mechanism creates a new instance of an 
explicit confirmation agency, passes it the pointer 
to the concept to be confirmed (start_time), and 
places it on the dialog stack. On completion, the 
strategy updates the confidence score of the con-
firmed hypothesis in light of the user response, and 
the dialog resumes from where it was left off.  
The specific implementation of the local deci-
sion processes constitutes an active research issue. 
Currently, they are modeled as Markov Decision 
Processes (MDP). The error handling processes 
running on individual concepts (concept-MDPs in 
end_time 
date 
start_time 
Explicit Confirm 
No Action 
Figure 3: A Distributed Error Handling Process 
ExplicitConfirm 
(start_time) 
 
 
 
 
 
 
 
 
 
 
 
Gating 
Mechanism 
Error Handling  
Decision Proc. 
[Concept-MDP] 
No Action 
No Action 
  
 
GetQuery 
R: GetDate 
R: GetStartTime 
R: GetEndTime 
RoomLine 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Error Handling  
Decision Proc. 
[Concept-MDP] 
Error Handling  
Decision Proc. 
[Request-MDP] 
Error Handling  
Decision Proc. 
[Concept-MDP] 
229
Figure 3) are partially-observable MDPs, with 3 
underlying hidden states: correct, incorrect and 
empty. The belief state is constructed at each time 
step from the confidence score of the top-hypothe-
sis for the concept. For instance, if the top 
hypothesis for the start_time concept is 10 a.m. with 
confidence 0.76, then the belief state for the 
POMDP corresponding to this concept is: 
{P(correct)=0.76, P(incorrect)=0.24, P(empty)=0}. 
The action-space for these models contains the 
three error recovery strategies for handling poten-
tial misunderstandings, and no-action. The third 
ingredient in the model is the policy.  A policy de-
fines which action the system should take in each 
state, and is indirectly described by specifying the 
utility of each strategy in each state. Currently, a 
number of predefined policies (e.g. always-
explicit-confirm, pessimistic, and optimistic) are 
available in the framework. Alternatively, system 
authors can specify and use their own policies. 
The error handling processes running on re-
quest agents (request-MDPs in Figure 3) are in 
charge of handling non-understandings on those 
requests. Currently, two types of models are avail-
able for this purpose. The simplest model has three 
states: non-understanding, understanding and 
inactive. A second model also includes information 
about the number of consecutive non-understand-
ings that have already happened. In the future, we 
plan to identify more features which carry useful 
information about the likelihood of success of in-
dividual recovery strategies and use them to create 
more complex models. The action-space is defined 
by the set of non-understanding recovery strategies 
presented in the previous subsection, and no-
action. Similar to the concept-MDPs, a number of 
default policies are available; alternatively, system 
authors can specify their own policy for engaging 
the strategies. 
While the MDP implementation allows us to 
encode various expert-designed policies, our ulti-
mate goal is to learn such policies from collected 
data using reinforcement learning. Reinforcement 
learning has been previously used to derive dialog 
control policies in systems operating with small 
tasks (Scheffler and Young, 2002; Singh et al 
2000). The approaches proposed to date suffer 
however from one important shortcoming, which 
has so far prevented their use in large, practical 
spoken dialog systems. The problem is lack of 
scalability: the size of the state space grows very 
fast with the size of the dialog task, and this ren-
ders the approach unfeasible in complex domains. 
A second important limitation of reinforcement 
learning techniques proposed to date is that the 
learned policies cannot be reused across tasks. For 
each new system, a new MDP has to be con-
structed, new data has to be collected, and a new 
training phase is necessary. This requires a signifi-
cant amount of expertise and effort from the sys-
tem author. 
We believe that the error handling architecture 
we have described addresses these issues in several 
ways. The central idea behind the distributed na-
ture of the approach is to keep the learning prob-
lem tractable by leveraging independence relation-
ships between different parts of the dialog. First, 
the state and action-spaces can be maintained rela-
tively small since we are only focusing on making 
error handling decisions (as opposed to other dia-
log control decisions). A more complex task 
translates into a larger number of MDP instantia-
tions rather than a more complex model structure. 
Second, both the model structure and parameters 
(i.e. the transition probabilities) can be tied across 
models: for instance the MDP for grounding the 
start_time concept can be identical to the one for 
grounding the end_time concept; all models for 
grounding Yes/No concepts could be tied together, 
etc. Model tying has the potential to greatly im-
prove scalability since data is polled together and 
the total number of model parameters to be learned 
grows sub-linearly with the size of the task. Third, 
since the individual MDPs are decoupled from the 
actual system task, the policies learned in a par-
ticular system can potentially be reused in other 
systems (e.g. we expect that grounding yes/no con-
cepts functions similarly at different locations in 
the dialog, and across domains). Last but not least, 
the approach can easily accommodate dynamic 
task generation. In traditional reinforcement 
learning approaches the state and action-spaces of 
the underlying MDP are task-specific. The task 
therefore has to be fixed, known in advance: for 
instance the slots that the system queries the user 
about (in a slot-filling system) are fixed. In con-
trast, in the RavenClaw architecture, the dialog 
task tree (e.g. the dialog plan) can be dynamically 
expanded at runtime with new questions and con-
cepts, and the corresponding request- and concept-
MDPs are automatically created by the Dialog En-
gine. 
230
4 Deployment and Current Research 
While a quantitative evaluation of design charac-
teristics such as task-independence, scalability, and 
ease-of-use is hard to perform, a first-order empiri-
cal evaluation of the proposed error handling ar-
chitecture can be accomplished by using it in 
different systems and monitoring the system au-
thoring process and the system?s operation.  
To date, the architecture has been successfully 
deployed in three different spoken dialog systems. 
A first system, RoomLine (2003), is a phone-based 
mixed-initiative system that assists users in making 
conference room reservations on campus. A sec-
ond system, the Let?s Go! Bus Information System 
(Raux et al 2005), provides information about bus 
routes and schedules in the greater Pittsburgh area 
(the system is available to the larger public). Fi-
nally, Vera is a phone-based taskable agent that 
can be instructed to deliver messages to a third 
party, make wake-up calls, etc. Vera actually con-
sists of two dialog systems, one which handles in-
coming requests (Vera In) and one which performs 
message delivery (Vera Out). In each of these sys-
tems, the authoring effort with respect to error 
handling consisted of: (1) specifying which models 
and policies should be used for the concepts and 
request-agents in the dialog task tree, and (2) 
writing the language generation prompts for ex-
plicit and implicit confirmations for each concept.  
Even though the first two systems operate in 
similar domains (information access), they have 
very different user populations: students and fac-
ulty on campus in the first case versus the entire 
Pittsburgh community in the second case. As a 
result, the two systems were configured with dif-
ferent error handling strategies and policies (see 
Table 1). RoomLine uses explicit and implicit con-
firmations with an optimistic policy to handle po-
tential misunderstandings. In contrast, the Let?s Go 
Public Bus Information System always uses ex-
plicit confirmations, in an effort to increase robust-
ness (at the expense of potentially longer dialogs). 
For non-understandings, RoomLine uses the full 
set of non-understanding recovery strategies pre-
sented in section 3.1. The Let?s Go Bus Informa-
tion system uses the YouCanSay and FullHelp 
strategies. Additionally a new GoToAQuieterPlace 
strategy was developed for this system (and is now 
available for use into any other RavenClaw-based 
system). This last strategy asks the user to move to 
a quieter place, and was prompted by the observa-
tion that a large number of users were calling the 
system from noisy environments. 
While the first two systems were developed by 
authors who had good knowledge of the Raven-
Claw dialog management framework, the third sys-
tem, Vera, was developed as part of a class project, 
by a team of six students who had no prior experi-
ence with RavenClaw. Modulo an initial lack of 
documentation, no major problems were encoun-
tered in configuring the system for automatic error 
handling. Overall, the proposed error handling ar-
chitecture adapted easily and provided the desired 
functionality in each of these domains: while new 
strategies and recovery policies were developed for 
some of the systems, no structural changes were 
required in the error handling architecture. 
Table 1: Spoken dialog systems using the RavenClaw error handling architecture 
 RoomLine Let?s Go Public Vera In / Out 
Domain room reservations bus route information task-able agent 
Initiative type mixed system mixed / mixed 
Task size: #agents ; #concepts 110 ; 25 57 ; 19 29 ; 4 / 31 ; 13 
Strategies for misunderstandings explicit and implicit explicit explicit and implicit / 
explicit only 
Policy for misunderstandings optimistic always-explicit optimistic /  
always-explicit 
Strategies for non-understandings all strategies 
(see Section 3.1) 
go-to-quieter-place, 
you-can-say, help 
all strategies /  
repeat prompt 
Policy for non-understandings choose-random author-specified  
heuristic policy 
choose-random /  
always-repeat-prompt 
Sessions collected so far 1393 2836 72 / 131 
Avg. task success rate 75% 52% (unknown) 
% Misunderstandings 17% 28% (unknown) 
% Non-understandings 13% 27% (unknown) 
% turns when strategies engage 41% 53% 36% / 44% 
231
5 Conclusion and Future Work 
We have described the error handling architecture 
underlying the RavenClaw dialog management 
framework. Its design is modular: the error han-
dling strategies as well as the mechanisms for en-
gaging them are decoupled from the actual dialog 
task specification. This significantly lessens the 
development effort: system authors focus exclu-
sively on the domain-specific dialog control logic, 
and the error handling behaviors are generated 
transparently by the error handling process running 
in the core dialog engine. Furthermore, we have 
argued that the distributed nature of the error han-
dling process leads to good scalability properties 
and facilitates the reuse of policies within and 
across systems and domains.  
The proposed architecture represents only the 
first (but an essential step) in our larger research 
program in error handling. Together with the sys-
tems described above, it sets the stage for a number 
of current and future planned investigations in er-
ror detection and recovery. For instance, we have 
recently conducted an extensive investigation of 
non-understanding errors and the ten recovery 
strategies currently available in the RavenClaw 
framework. The results of that study fall beyond 
the scope of this paper and are presented separately 
in (Bohus and Rudnicky, 2005a). In another pro-
ject supported by this architecture, we have devel-
oped a model for updating system beliefs over 
concept values in light of initial recognition confi-
dence scores and subsequent user responses to 
system actions. Initially, our confirmation strate-
gies used simple heuristics to update the system?s 
confidence score for a concept in light of the user 
response to the verification question. We have 
showed that a machine learning based approach 
which integrates confidence information with cor-
rection detection information can be used to con-
struct significantly more accurate system beliefs 
(Bohus and Rudnicky, 2005b). Our next efforts 
will focus on using reinforcement learning to 
automatically derive the error recovery policies. 
References 
Bohus, D., Rudnicky, A., 2003 ? RavenClaw: Dialogue 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proceedings of 
Eurospeech-2003, Geneva, Switzerland 
Bohus, D., Rudnicky, A., 2005a ? Sorry, I didn?t Catch 
That! An Investigation into Non-understandings and 
Recovery Strategies, to appear in SIGDial-2005, Lis-
bon, Portugal 
Bohus, D., Rudnicky, A., 2005b ? Constructing Accu-
rate Beliefs in Spoken Dialog Systems, submitted to 
ASRU-2005, Cancun, Mexico 
Clark, H.H., Schaefer, E.F., 1989 ? Contributing to Dis-
course, in Cognitive Science, vol 13, 1989. 
Kawahara, T., Komatani, K., 2000 ? Flexible mixed-
initiative dialogue management using concept-level 
confidence measures of speech recognizer output, in 
Proc. of COLING, Saarbrucken, Germany, 2000. 
Krahmer, E., Swerts, M., Theune, M., Weegels, M., 
1999 - Error Detection in Human-Machine Interac-
tion, Speaking. From Intention to Articulation, MIT 
Press, Cambridge, Massachusetts, 1999 
Paek, T., Horvitz, E., 2000 ? Conversation as Action 
Under Uncertainty, in Proceedings of the Sixteenth 
Conference on Uncertainty and Artificial Intelli-
gence, Stanford, CA, June 2000. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M., 2005 ? Let?s Go Public! Taking a Spoken Dialog 
System to the Real World, submitted to Interspeech-
2005, Lisbon, Portugal 
RoomLine web site, as of June 2005 ?  
www.cs.cmu.edu/~dbohus/RoomLine 
Sanders, G., Le, A., Garofolo, J., 2002 ? Effects of Word 
Error Rate in the DARPA Communicator Data Dur-
ing 2000 and 2001, in Proceedings of ICSLP?02, 
Denver, Colorado, 2002. 
Scheffler, K., Young, S., 2002 ? Automatic learning of 
dialogue strategy using dialogue simulation and re-
inforcement learning, in Proceedings of HLT-2002. 
Singh, S., Litman, D., Kearns, M., Walker, M., 2000 ? 
Optimizing Dialogue Management with Reinforce-
ment Learning: Experiments with the NJFun System, 
in Journal of Artificial Intelligence Research, vol. 16, 
pp 105-133, 2000. 
Traum, D., 1998 ? On Clark and Schaefer?s Contribu-
tion Model and its Applicability to Human-Computer 
Collaboration, in Proceedings of the COOP?98, May 
1998. 
Walker, M., Wright, J., Langkilde, I., 2000 ? Using 
Natural Language Processing and Discourse Fea-
tures to Identify Understanding Errors in a Spoken 
Dialogue System, in Proc. of the 17?th International 
Conference of Machine Learning, pp 1111-1118.  
 
232
Automatic Extraction of Briefing Templates
Dipanjan Das Mohit Kumar
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{dipanjan, mohitkum, air}@cs.cmu.edu
Alexander I. Rudnicky
Abstract
An approach to solving the problem of au-
tomatic briefing generation from non-textual
events can be segmenting the task into two
major steps, namely, extraction of briefing
templates and learning aggregators that col-
late information from events and automati-
cally fill up the templates. In this paper, we
describe two novel unsupervised approaches
for extracting briefing templates from hu-
man written reports. Since the problem is
non-standard, we define our own criteria for
evaluating the approaches and demonstrate
that both approaches are effective in extract-
ing domain relevant templates with promis-
ing accuracies.
1 Introduction
Automated briefing generation from non-textual
events is an unsolved problem that currently lacks a
standard approach in the NLP community. Broadly,
it intersects the problem of language generation
from structured data and summarization. The prob-
lem is relevant in several domains where the user
has to repeatedly write reports based on events in
the domain, for example, weather reports (Reiter
et al, 2005), medical reports (Elhadad et al, 2005),
weekly class project reports (Kumar et al, 2007) and
so forth. On observing the data from these domains,
we notice a templatized nature of report items. Ex-
amples (1)-(3) demonstrate equivalents in a particu-
lar domain (Reiter et al, 2005).
(1) [A warm front] from [Iceland] to
[northern Scotland] will move [SE]
across [the northern North Sea] [today
and tomorrow]
(2) [A warm front] from [Iceland] to [the
Faeroes] will move [ENE] across [the
Norwegian Sea] [this evening]
(3) [A ridge] from [the British Isles] to
[Iceland] will move [NE] across [the
North Sea] [today]
In each sentence, the phrases in square brackets at
the same relative positions form the slots that take
up different values at different occasions. The cor-
responding template is shown in (4) with slots con-
taining their respective domain entity types. Instan-
tiations of (4) may produce (1)-(3) and similar sen-
tences. This kind of sentence structure motivates an
approach of segmenting the problem of closed do-
main summarization into two major steps of auto-
matic template extraction and learning aggregators,
which are pattern detectors that assimilate informa-
tion from the events, to populate these templates.
(4) [PRESSURE ENTITY] from [LOCATION] to
[LOCATION] will move [DIRECTION] across
[LOCATION] [TIME]
In the current work we address the first problem of
automatically extracting domain templates from hu-
man written reports. We take a two-step approach to
the problem; first, we cluster report sentences based
on similarity and second, we extract template(s) cor-
responding to each cluster by aligning the instances
in the cluster. We experimented with two indepen-
dent, arguably complementary techniques for clus-
tering and aligning ? a predicate argument based ap-
proach that extracts more general templates contain-
ing one predicate and a ROUGE (Lin, 2004) based
265
approach that can extract templates containing mul-
tiple verbs. As we will see below, both approaches
show promise.
2 Related Work
There has been instances of template based sum-
marization in popular Information Extraction (IE)
evaluations like MUC (Marsh & Perzanowski, 1998;
Onyshkevych, 1994) and ACE (ACE, 2007) where
hand engineered slots were to be filled for events in
text; but the focus lay on template filling rather than
their creation. (Riloff, 1996) describes an interesting
work on the generation of extraction patterns from
untagged text, but the analysis is syntactic and the
patterns do not resemble the templates that we aim
to extract. (Yangarber et al, 2000) describe another
system called ExDisco, that extracts event patterns
from un-annotated text starting from seed patterns.
Once again, the text analysis is not deep and the pat-
terns extracted are not sentence surface forms.
(Collier, 1998) proposed automatic domain tem-
plate extraction for IE purposes where MUC type
templates for particular types of events were con-
structed. The method relies on the idea from (Luhn,
1958) where statistically significant words of a cor-
pus were extracted. Based on these words, sen-
tences containing them were chosen and aligned
using subject-object-verb patterns. However, this
method did not look at arbitrary syntactic patterns.
(Filatova et al, 2006) improved the paradigm by
looking at the most frequent verbs occurring in a
corpus and aligning subtrees containing the verb,
by using the syntactic parses as a similarity metric.
However, long distance dependencies of verbs with
constituents were not looked at and deep semantic
analysis was not performed on the sentences to find
out similar verb subcategorization frames. In con-
trast, in our predicate argument based approach we
look into deeper semantic structures, and align sen-
tences not only based on similar syntactic parses,
but also based on the constituents? roles with re-
spect to the main predicate. Also, they relied on
typical Named Entities (NEs) like location, organi-
zation, person etc. and included another entity that
they termed as NUMBER. However, for specific
domains like weather forecasts, medical reports or
student reports, more varied domain entities form
slots in templates, as we observe in our data; hence,
existence of a module handling domain specific en-
tities become essential for such a task. (Surdeanu
et al, 2003) identify arguments for predicates in a
sentence and emphasize how semantic role infor-
mation may assist in IE related tasks, but their pri-
mary focus remained on the extraction of PropBank
(Kingsbury et al, 2002) type semantic roles.
To our knowledge, the ROUGE metric has not
been used for automatic extraction of templates.
3 The Data
3.1 Data Description
Since our focus is on creating summary items from
events or structured data rather than from text, we
used a corpus from the domain of weather fore-
casts (Reiter et al, 2005). This is a freely avail-
able parallel corpus1 consisting of weather data
and human written forecasts describing them. The
dataset showed regularity in sentence structure and
belonged to a closed domain, making the variations
in surface forms more constrained than completely
free text. After sentence segmentation we arrived at
a set of 3262 sentences. From this set, we selected
3000 for template extraction and kept aside 262 sen-
tences for testing.
3.2 Preprocessing
For semantic analysis, we used the ASSERT toolkit
(Pradhan et al, 2004) that produces shallow seman-
tic parses using the PropBank conventions. As a
by product, it also produces syntactic parses of sen-
tences, using the Charniak parser (Charniak, 2001).
For each sentence, we maintained a part-of-speech
tagged (leaves of the parse tree), parsed, baseNP2
tagged and semantic role tagged version. The
baseNPs were retrieved by pruning the parse trees
and not by using a separate NP chunker. The rea-
son for having a baseNP tagged corpus will become
clear as we go into the detail of our template ex-
traction techniques. Figure 1 shows a typical out-
put from the Charniak parser and Figure 2 shows the
same tree with nodes under the baseNPs pruned.
We identified the need to have a domain entity
tagger for matching constituents in the sentences.
1http://www.csd.abdn.ac.uk/research/sumtime/
2A baseNP is a noun-phrase with no internal noun-phrase
266
ADVP
IN
A low theover Norwegian Sea will move North and weaken
DT NN DT JJ NN MD VB RB CC VB
NP
NP
PP
NP
S
VP
VP
VP
VP
Figure 1: Parse tree for a sentence in the data.
ADVP
IN
A low theover Norwegian Sea will move North and weaken
MD VB RB CC VB
NP
NP
PP
NP
S
VP
VP
VP
VP
Figure 2: Pruned parse tree for a sentence in the cor-
pus
Any tagger for named entities was not suitable for
weather forecasts since unique constituent types as-
sumed significance unlike newswire data. Since the
development of such a tagger was beyond the scope
of the present work, we developed a module that
took baseNP tagged sentences as input and produced
tags across words and baseNPs that were domain en-
tities. The development of such a module by hand
was easy because of a limited vocabulary (< 1000
words) of the data and the closed set nature of most
entity types (e.g the direction entity could take up a
finite set of values). From inspection, thirteen dis-
tinct entity types were recognized in the domain.
Figure 3 shows an example output from the entity
recognizer with the sentence from Figure 2 as input.
[ A low ]
DIRECTION and weaken
A low over the Norwegian Sea will move North and weaken 
ENTITY RECOGNIZER
LOCATIONover [ the Norwegian Sea ]PRESSURE ENTITY
will move [ North ]
Figure 3: Example output of the entity recognizer
We now provide a detailed description of our clus-
tering and template extraction algorithms.
4 Approach and Experiments
We adopted two parallel approaches. First, we
investigated a predicate-argument based approach
where we consider the set of all propositions in our
dataset, and cluster them based on their verb sub-
categorization frame. Second, we used ROUGE,
a summarization evaluation metric that is generally
used to compare machine generated and human writ-
ten summaries. We uniquely used this metric for
clustering similar summary items, after abstracting
the surface forms to a representation that facilitates
comparison of a pair of sentences. The following
subsections detail both the techniques.
4.1 A Predicate-Argument Based Approach
Analysis of predicate-argument structures seemed
appropriate for template extraction for a few rea-
sons: Firstly, complicated sentences with multiple
verbs are broken down into propositions by a seman-
tic role labeler. The propositions3 are better gen-
eralizable units than whole sentences across a cor-
pus. Secondly, long distance dependencies of con-
stituents with a particular verb, are captured well by
a semantic role labeler. Finally, if verbs are con-
sidered to be the center of events, then groups of
sentences with the same semantic role sequences
seemed to form clusters conveying similar meaning.
We explain the complete algorithm for template ex-
traction in the following subsections.
(5) [ARG0 A low over the Norwegian Sea]
[AGM-MOD will] [TARGET move ]
[ARGM-DIR North ] and weaken
(6) [ARG0 A high pressure area ] [AGM-MOD
will ] [TARGET move] [ARGM-DIR
southwestwards] and build on Sunday.
4.1.1 Verb based clustering
We performed a verb based clustering as the first
step. Instead of considering a unique set of verbs,
we considered related verbs as a single verb type.
The relatedness of verbs was derived from Word-
net (Fellbaum, 1998), by merging verbs that appear
in the same synset. This kind of clustering is not
3sentence fragments with one verb
267
ideal in a corpus containing a huge variation in event
streams, like newswire. However, the results were
good for the weather domain where the number of
verbs used is limited. The grouping procedure re-
sulted in a set of 82 clusters with 6632 propositions.
4.1.2 Matching Role Sequences
Each verb cluster was considered next. Instead
of finding structural similarities of the propositions
in one go, we first considered the semantic role
sequences for each proposition. We searched for
propositions that had exactly similar role sequences
and grouped them together. To give an exam-
ple, both sentences 5 and 6 have the matching role
sequence ARG0?ARGM-MOD?TARGET?ARGM-
DIR. The intuition behind such clustering is straight-
forward. Propositions with a matching verb type
with the same set of roles arranged in a similar fash-
ion would convey similar meaning. We observed
that this was indeed true for sentences tagged with
correct semantic role labels.
Instead of considering matching role sequences
for a set of propositions, we could as well have
considered matching bag of roles. However, for
the present corpus, we decided to use strict role se-
quence instead because of the sentences? rigid struc-
ture and absence of any passive sentences. This
subclustering step resulted in smaller clusters, and
many of them contained a single proposition. We
threw out these clusters on the assumption that the
human summarizers did not necessarily have a tem-
plate in mind while writing those summary items.
As a result, many verb types were eliminated and
only 33 verb-type clusters containing several sub-
clusters each were produced.
4.1.3 Looking inside Roles
Groups of propositions with the same verb-type
and semantic role sequences were considered in this
step. For each group, we looked at individual se-
mantic roles to find out similarity between them. We
decided at first to look at syntactic parse tree similar-
ities between constituents. However, there is a need
to decide at what level of abstraction should one con-
sider matching the parse trees. After considerable
speculation, we decided on pruning the constituents?
parse trees till the level of baseNPs and then match
the resulting tag sequences.
Scotland
IN
A low theover Sea
NP
NP
PP
NP
NP
NP PP
NP
Norwegian A frontal trough
IN
across
Figure 4: Matching ARG0s for two propositions
LOCATIONIN
A low theover SeaNorwegian A frontal trough
IN
across Scotland
PRESSURE ENTITY
LOCATION
PRESSURE ENTITY
Figure 5: Abstracted tag sequences for two con-
stituents
The parses with pruned trees from the preprocess-
ing steps provide the necessary information for con-
stituent matching. Figure 4 shows matching syntac-
tic trees for two ARG0s from two propositions of a
cluster. It is at this step that we use the domain entity
tags to abstract away the constituents? syntactic tags.
Figure 5 shows the constituents of Figure 4 with the
tree structure reduced to tag sequences and domain
entity types replacing the tags whenever necessary.
This abstraction step produces a number of unique
domain entity augmented tag sequences for a partic-
ular semantic role. As a final step of template gen-
eration, we concatenate these abstracted constituent
types for all the semantic roles in the given group.
To focus on template-like structures we only con-
sider tag sequences that occur twice or more in the
group.
The templates produced at the end of this step are
essentially tag sequences interspersed with domain
entities. In our definition of templates, the slots are
the entity types and the fixed parts are constituted
by word(s) used by the human experts for a partic-
ular tag sequence. Figure 6 shows some example
templates. The upper case words in the figure corre-
spond to the domain entities identified by the entity
tagger and they form the slots in the templates. A
total of 209 templates were produced.
268
PRESSURE_ENTITY to DIRECTION of LOCATION will drift slowly
WAVE will run_0.5/move_0.5 DIRECTION then DIRECTION
Associated PRESSURE_ENTITY will move DIRECTION across LOCATION TIME
PRESSURE_ENTITY expected over LOCATION by_0.5/on_0.5 DAY
Figure 6: Example Templates. Upper case tokens
correspond to slots. For fixed parts, when there is a
choice between words, the probability of the occur-
rence of words in that particular syntactic structure
are tagged alongside.
4.2 A ROUGE Based Approach
ROUGE (Lin, 2004) is the standard automatic eval-
uation metric in the Summarization community. It is
derived from the BLEU (Papineni et al, 2001) score
which is the evaluation metric used in the Machine
Translation community. The underlying idea in the
metric is comparing the candidate and the refer-
ence sentences (or summaries) based on their token
co-occurrence statistics. For example, a unigram
based measure would compare the vocabulary over-
lap between the candidate and reference sentences.
Thus, intuitively, we may use the ROUGE score as
a measure for clustering the sentences. Amongst
the various ROUGE statistics, the most appealing is
Weighted Longest Common Subsequence(WLCS).
WLCS favors contiguous LCS which corresponds
to the intuition of finding the common template.
We experimented with other ROUGE statistics but
we got better and easily interpretable results using
WLCS and so we chose it as the final metric. In
all the approaches the data was first preprocessed
(baseNP and NE tagged) as described in the previ-
ous subsection. In the following subsections, we de-
scribe the various clustering techniques that we tried
using the ROUGE score followed by the alignment
technique.
4.2.1 Clustering
Unsupervised Clustering: As the ROUGE score
defines a distance metric, we can use this score for
doing unsupervised clustering. We tried hierarchical
clustering approaches but did not obtain good clus-
ters, evaluated empirically. In empirical evaluation,
we manually looked at the output clusters and made
a judgement call whether the candidate clusters are
reasonably coherent and potentially correspond to
templates. The reason for the poor performance of
the approach was the classical parameter estimation
problem of determining a priori the number of clus-
ters. We could not find an elegant solution for the
problem without losing the motivation of an auto-
mated approach.
Figure 7: Deterministic clustering based on Graph
connectivity. In the figure the squares with the same
pattern belong to the same cluster.
Non-parametric Unsupervised Clustering:
Since the unsupervised technique did not give
good results, we experimented with a non-
parametric clustering approach, namely, Cross-
Association(Chakrabarti et al, 2004). It is a
non-parametric unsupervised clustering algorithm
for similarity (boolean) matrices. We obtain the
similarity matrix in our domain by thresholding the
ROUGE similarity score matrix. This technique
also did not give us good clusters, evaluated empiri-
cally. The plausible reason for the poor performance
seems to be that the technique is based on MDL
(Minimum Description Length) principle. Since in
our domain we expect a large number of clusters
with small membership along many singletons,
MDL principle is not likely to perform well.
Deterministic Clustering:
As the unsupervised techniques did not perform
well, we tried deterministic clustering based on
graph connectivity. The underlying intuition is that
all the sentences X1...n that are ?similar? to any
other sentence Yi should be in the same cluster even
though Xj and Xk may not be ?similar? to each
other. Thus we find the connected components in the
similarity matrix and label them as individual clus-
ters.4
4This approach is similar to agglomerative single linkage
clustering.
269
We created a similarity matrix by thresholding the
ROUGE score. In the event, the clusters obtained by
this approach were also not good, evaluated empir-
ically. This led us to revisit the similarity function
and tune it. We factored the ROUGE-WLCS score,
which is an F-measure score, into its component Pre-
cision and Recall scores and experimented with var-
ious combinations of using the Precision and Recall
scores. We finally chose a combined Precision and
Recall measure (not f-measure) in which both the
scores were independently thresholded. The moti-
vation for the measure is that in our domain we de-
sire to have high precision matches. Additionally
we need to control the length of the sentences in the
cluster for which we require a Recall threshold. F-
measure (which is the harmonic mean of Precision
and Recall) does not give us the required individual
control. We set up our experiments such that while
comparing two sentences the longer sentence is al-
ways treated as the reference and the shorter one as
the candidate. This helps us in interpreting the Pre-
cision/Recall measures better and thresholding them
accordingly. The approach gave us 149 clusters,
which looked good on empirical evaluation. We can
argue that using this modified similarity function for
previous unsupervised approaches could have given
better results, but we did not reevaluate those ap-
proaches as our aim of getting a reasonable cluster-
ing approach is fulfilled with this simple scheme and
tuning the unsupervised approaches can be interest-
ing future work.
4.3 Alignment
After obtaining the clusters using the Deterministic
approach we needed to find out the template corre-
sponding to each of the cluster. Fairly intuitively we
computed the Longest Common Subsequence(LCS)
between the sentences in each cluster which we then
claim to be the template corresponding to the clus-
ter. This resulted in a set of 149 templates, similar to
the Predicate Argument based approach, as shown
in figure 6.
5 Results
5.1 Evaluation Scheme
Since there is no standard way to evaluate template
extraction for summary creation, we adopted a mix
of subjective and automatic measures for evaluating
the templates extracted. We define precision for this
particular problem as:
precision = number of domain relevant templatestotal number of extracted templates
This is a subjective measure and we undertook a
study involving three subjects who were accustomed
to the language used in the corpus. We asked the
human subjects to mark each template as relevant
or non-relevant to the weather forecast domain. We
also asked them to mark the template as grammatical
or ungrammatical if it is non-relevant.
Our other metric for evaluation is automatic re-
call. It is based on using the ROUGE-WLCS met-
ric to determine a match between the preprocessed
(baseNP and NE tagged) test corpora with the pro-
posed set of correct templates, a set determined
by taking an intersection of only the relevant tem-
plates marked by each judge. For the ROUGE based
method, the test corpus consists of 262 sentences,
while for the predicate-argument based method it
consists of a set of 263 propositions extracted from
the 262 sentences using ASSERT followed by a fil-
tering of invalid propositions (e.g. ones starting
with a verb). Amongst different ROUGE scores
(precision/recall/f-measure), we consider precision
as the criterion for deciding a match and experi-
mented with different thresholding values.
Main Verb Precision Main Verb Precision
deepen 0.67 weaken 0.83
expect 0.76 lie 0.57
drift 0.93 continue 0.97
build 0.95 fill 0.80
cross 0.78 move 0.86
Table 1: Precision for top 10 most frequently occur-
ring verbs
5.2 Results: Predicate-Argument Based
Approach
Table 1 shows the precision values for top 10 most
frequently occurring verbs. (Since a major propor-
tion (> 90%) of the templates are covered by these
verbs, we don?t show all the precision values; it also
helps to contain space.) The overall precision value
achieved was 84.21%, the inter-rater Fleiss? kappa
measure (Fleiss, 1971) between the judges being
270
? = 0.69, demonstrating substantial agreement. The
precision values are encouraging, and in most cases
the reason for low precision is because of erroneous
performance of the semantic role labeler system,
which is corroborated by the percentage (47.47%) of
ungrammatical templates among the irrelevant ones.
Results for the automated recall values are shown
in Figure 8, where precision values are varied to
observe the recall. For 0.9 precision in ROUGE-
WLCS, the recall is 0.3 which shows that there is
a 30% near exact coverage over propositions, while
for 0.6 precision in ROUGE-WLCS, the recall is an
encouraging 81%.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0.4 0.5 0.6 0.7 0.8 0.9
Rec
all
Precision Threshold forMatching Test Sentences
ROUGESRL
Figure 8: Automated Recall based on ROUGE-
WLCS measure comparing the test corpora with
the set of templates extracted by the Predicate-
Argument (SRL) and the ROUGE based method.
5.3 Results: ROUGE based approach
Various precision and recall thresholds for ROUGE
were considered for clustering. We empirically set-
tled on a recall threshold of 0.8 since this produces
the set of clusters with optimum number of sen-
tences. The number of clusters and number of sen-
tences in clusters at this recall values are shown in
Figure 9 for various precision thresholds.
Precision was measured in the same way as the
predicate argument approach and the value obtained
was 76.3%, with Fleiss? kappa measure of ? = 0.79.
The percentage of ungrammatical templates among
the irrelevant ones was 96.7%, strongly indicating
that post processing the templates using a parser can,
in future, give substantial improvement. During er-
ror analysis, we observed simple grammatical er-
rors in templates; first or last word being preposi-
 130
 140
 150
 160
 170
 180
 190
 0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0
 200
 400
 600
 800
 1000
 1200
 1400
No. o
f Clus
ters
No. o
f Se
nten
ces
 in 
Cluste
rs
Precision Threshold
No. of ClustersNo. of Sentences in Clusters
Figure 9: Number of clusters and total number of
sentences in clusters for various Precision Thresh-
olds at Recall Threshold=0.8
tions. So a fairly simple error recovery module that
strips the leading and trailing prepositions was in-
troduced. 20 templates out of the 149 were mod-
ified by the error recovery module and they were
evaluated again by the three judges. The precision
obtained for the modified templates was 35%, with
Fleiss? kappa ? = 1, boosting the overall precision
to 80.98%. The overall high precision is motivat-
ing as this is a fairly general approach that does not
require any NLP resources. Figure 8 shows the auto-
mated recall values for the templates and abstracted
sentences from the held-out dataset. For high preci-
sion points, the recall is low because there is not an
exact match for most cases.
6 Conclusion and Future Work
In this paper, we described two new approaches
for template extraction for briefing generation. For
both approaches, high precision values indicate that
meaningful templates are being extracted. However,
the recall values were moderate and they hint at
possible improvements. An interesting direction of
future research is merging the two approaches and
have one technique benefit from the other. The ap-
proaches seem complementary as the ROUGE based
technique does not use the structure of the sentence
at all whereas the predicate-argument approach is
heavily dependent on it. Moreover, the predicate
argument based approach gives general templates
with one predicate while ROUGE based approach
271
can extract templates containing multiple verbs. It
would also be desirable to establish the generality
of the techniques, by using other domains such as
newswire, medical reports and others.
Acknowledgements We would like to express our
gratitude to William Cohen and Noah Smith for their
valuable suggestions and inputs during the course of
this work. We also thank the three anonymous re-
viewers for helpful suggestions. This work was sup-
ported by DARPA grant NBCHD030010. The con-
tent of the information in this publication does not
necessarily reflect the position or the policy of the
US Government, and no official endorsement should
be inferred.
References
ACE (2007). Automatic content extraction program.
http://www.nist.gov/speech/tests/ace/.
Chakrabarti, D., Papadimitriou, S., Modha, D. S.,
& Faloutsos, C. (2004). Fully automatic cross-
associations. Proceedings of KDD ?04 (pp. 79?
88). New York, NY, USA: ACM Press.
Charniak, E. (2001). Immediate-head parsing for
language models. Proceedings of ACL ?01 (pp.
116?123).
Collier, R. (1998). Automatic template creation
for information extraction. Doctoral dissertation,
University of Sheffield.
Elhadad, N., Kan, M.-Y., Klavans, J. L., & McKe-
own, K. (2005). Customization in a unified frame-
work for summarizing medical literature. Artifi-
cial Intelligence in Medicine, 33, 179?198.
Fellbaum, C. (1998). WordNet ? An Electronic Lex-
ical Database. MIT Press.
Filatova, E., Hatzivassiloglou, V., & McKeown,
K. (2006). Automatic creation of domain tem-
plates. Proceedings of COLING/ACL 2006 (pp.
207?214).
Fleiss, J. (1971). Measuring nominal scale agree-
ment among many raters. Psychological Bulletin
(pp. 378?382).
Kingsbury, P., Palmer, M., & Marcus, M. (2002).
Adding semantic annotation to the penn treebank.
Proceedings of the HLT?02.
Kumar, M., Garera, N., & Rudnicky, A. I. (2007).
Learning from the report-writing behavior of in-
dividuals. IJCAI (pp. 1641?1646).
Lin, C.-Y. (2004). ROUGE: A package for auto-
matic evaluation of summaries. Proceedings of
Workshop on Text Summarization.
Luhn, H. P. (1958). The automatic creation of litera-
ture abstracts. IBM Journal of Research Develop-
ment, 2, 159?165.
Marsh, E., & Perzanowski, D. (1998). MUC-7 Eval-
uation of IE Technology: Overview of Results.
Proceedings of MUC-7. Fairfax, Virginia.
Onyshkevych, B. (1994). Issues and methodology
for template design for information extraction.
Proceedings of HLT ?94 (pp. 171?176). Morris-
town, NJ, USA.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.
(2001). Bleu: a method for automatic evaluation
of machine translation.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., &
Jurafsky, D. (2004). Shallow semantic parsing
using support vector machines. Proceedings of
HLT/NAACL ?04. Boston, MA.
Reiter, E., Sripada, S., Hunter, J., Yu, J., & Davy,
I. (2005). Choosing words in computer-generated
weather forecasts. Artif. Intell., 167, 137?169.
Riloff, E. (1996). Automatically generating extrac-
tion patterns from untagged text. AAAI/IAAI, Vol.
2 (pp. 1044?1049).
Surdeanu, M., Harabagiu, S., Williams, J., &
Aarseth, P. (2003). Using predicate-argument
structures for information extraction. Proceedings
of ACL 2003.
Yangarber, R., Grishman, R., Tapanainen, P., & Hut-
tunen, S. (2000). Automatic acquisition of domain
knowledge for information extraction. Proceed-
ings of the 18th conference on Computational lin-
guistics (pp. 940?946). Morristown, NJ, USA.
272
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 261?264,
New York City, June 2006. c?2006 Association for Computational Linguistics
SmartNotes: Implicit Labeling of Meeting Data
through User Note?Taking and Browsing
Satanjeev Banerjee
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
banerjee@cs.cmu.edu
Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
air@cs.cmu.edu
Abstract
We have implemented SmartNotes, a sys-
tem that automatically acquires labeled
meeting data as users take notes during
meetings and browse the notes afterwards.
Such data can enable meeting understand-
ing components such as topic and ac-
tion item detectors to automatically im-
prove their performance over a sequence
of meetings. The SmartNotes system con-
sists of a laptop based note taking appli-
cation, and a web based note retrieval sys-
tem. We shall demonstrate the functional-
ities of this system, and will also demon-
strate the labeled data obtained during typ-
ical meetings and browsing sessions.
1 Goals of the SmartNotes System
Most institutions hold a large number of meetings
every day. Several of these meetings are important,
and meeting participants need to recall the details
of the discussions at a future date. In a previous
survey (Banerjee et al, 2005) of busy professors at
Carnegie Mellon University we showed that meeting
participants needed to recall details of past meetings
on average about twice a month. Performing such
retrieval is not an easy task. It is time consuming;
in our study participants took on average between
15 minutes to an hour to recall the information they
were seeking. Further, the quality of the retrieval
is dependent on whether or not the participants had
access to the notes at the meeting. On a scale of 0
to 5, with 5 denoting complete satisfaction with re-
trieval results, participants reported a satisfaction of
3.4 when they did not have notes, and 4.0 when they
did.
Despite the prevalence of important meetings and
the importance of notes, there is a relative paucity of
technology to help meeting participants take notes
easily at meetings. Some commercial applications
allow users to take notes (e.g. OneNote1) and even
record audio/video (e.g. Quindi2), but no product at-
tempts to automatically take notes. Our long term
goal is to create a system that makes note?taking
easier by performing tasks such as automatically
highlighting portions of the meeting that are likely
to be important to the user, automatically detecting
?note?worthy? phrases spoken during the meeting,
etc.
To perform such note taking, the system needs to
form an understanding of the meeting. Our short
term goal is to create a system that can detect the
topics of discussion, the action items being dis-
cussed, and the roles of the meeting participants.
Additionally, these components must adapt to spe-
cific users and groups of users since different people
will likely take different notes at the same meeting.
Thus we wish to implement the note taking system
in such a way that the user?s interactions with the
system result in labeled meeting data that can then
be used to adapt and improve the meeting under-
standing components.
Towards these goals, we have built SmartNotes
which helps users easily record and retrieve notes.
1http://office.microsoft.com/onenote
2http://www.quindi.com
261
The system also records the user interactions to form
labeled meeting data that can later be used to auto-
matically improve the meeting understanding com-
ponents. In the next section we describe the meeting
understanding components in more detail. Next we
describe SmartNotes itself, and show how it is cur-
rently helping users take and retrieve notes, while
acquiring labeled data to aid each of the meeting un-
derstanding components. Finally we end with a dis-
cussion of what functionality we plan to demonstrate
at the conference.
2 Automatic Meeting Understanding
Topic detection and segmentation: We are at-
tempting to automatically detect the topics being
discussed at meetings. This task consists of two sub-
tasks: discovering the points in a meeting when the
topic changes, and then associating a descriptive la-
bel to the segment between two topic shifts. Our cur-
rent strategy for topic shift detection (Banerjee and
Rudnicky, 2006a) is to perform an edge detection
using such features as speech activity (who spoke
when and for how long), the words that each per-
son spoke, etc. For labeling, we are currently sim-
ply associating the agenda item names recorded in
the notes with the segments they are most relevant
to, as decided by a tf.idf matching technique. Topic
detection is particularly useful during meeting infor-
mation retrieval; (Banerjee et al, 2005) showed that
when users wish to retrieve information from past
meetings, they are typically interested in a specific
discussion topic, as opposed to an entire meeting.
Action item detection: An obvious application
of meeting understanding is the automatic discovery
and recording of action items as they are discussed
during a meeting. Arguably one of the most impor-
tant outcomes of a meeting are the action items de-
cided upon, and automatically recording them could
be a huge benefit especially to those participants that
are likely to not note them down and consequently
forget about them later on.
Meeting participant role detection: Each meet-
ing participant plays a variety of roles in an insti-
tution. These roles can be based on their function
in the institution (managers, assistants, professors,
students, etc), or based on their expertise (speech
recognition experts, facilities experts, etc). Our cur-
rent strategy for role detection (Banerjee and Rud-
nicky, 2006b) is to train detectors on hand labeled
data. Our next step is to perform discovery of new
roles through clustering techniques. Detecting such
roles has several benefits. First, it allows us to build
prior expectations of a meeting between a group of
participants. For example, if we know person A is
a speech recognition expert and person B a speech
synthesis expert, a reasonable expectation is that
when they meet they are likely to talk about tech-
nologies related speech processing. Consequently,
we can use this expectation to aid the action item
detection and the topic detection in that meeting.
3 SmartNotes: System Description
We have implemented SmartNotes to help users
take multi?media notes during meetings, and re-
trieve them later on. SmartNotes consists of two ma-
jor components: The note taking application which
meeting participants use to take notes during the
meeting, and the note retrieval application which
users use to retrieve notes at a later point.
3.1 SmartNotes Note Taking Application
The note taking application is a stand?alone system,
that runs on each meeting participant?s laptop, and
allows him to take notes during the meeting. In ad-
dition to recording the text notes, it also records the
participant?s speech, and video, if a video camera is
connected to the laptop. This system is an extension
of the Carnegie Mellon Meeting Recorder (Banerjee
et al, 2004).
Figure 1 shows a screen?shot of this application.
It is a server?client application, and each participant
logs into a central server at the beginning of each
meeting. Thus, the system knows the precise iden-
tity of each note taker as well as each speaker in
the meeting. This allows us to avoid the onerous
problem of automatically detecting who is speaking
at any time during the meeting. Further, after log-
ging on, each client automatically synchronizes it-
self with a central NTP time server. Thus the time
stamps that each client associates with its recordings
are all synchronized, to facilitate merging and play
back of audio/video during browsing (described in
the next sub?section).
Once logged in, each participant?s note taking
262
Figure 1: Screen shot of the SmartNotes note?taking client
area is split into two sections: a shared note taking
area, and a private note taking area. Notes written
in the shared area are viewable by all meeting par-
ticipants. This allows meeting participants to share
the task of taking notes during a meeting: As long as
one participant has recorded an important point dur-
ing a meeting, the other participants do not need to,
thus making the note taking task easier for the group
as a whole. Private notes that a participant does not
wish to share with all participants can be taken in the
private note taking area.
The interface has a mechanism to allow meeting
participants to insert an agenda into the shared area.
Once inserted, the shared area is split into as many
boxes as there are agenda items. Participants can
then take notes during the discussion of an agenda
item in the corresponding agenda item box. This
is useful to the participants because it organizes the
notes as they are being taken, and, additionally, the
notes can later be retrieved agenda item by agenda
item. Thus, the user can access all notes he has taken
in different meetings regarding ?buying a printer?,
without having to see the notes taken for the other
agenda items in each such meeting.
In addition to being useful to the user, this act of
inserting an agenda and then taking notes within the
relevant agenda item box results in generating (un-
beknownst to the participant) labeled data for the
topic detection component. Specifically, if we de-
fine each agenda item as being a separate ?topic?,
and make the assumption that notes are taken ap-
proximately concurrent with the discussion of the
contents of the notes, then we can conclude that
there is a shift in the topic of discussion at some
point between the time stamp on the last note in
an agenda item box, and the time stamp on the first
note of the next agenda item box. This information
can then be used to improve the performance of the
topic shift detector. The accuracy of the topic shift
data thus acquired depends on the length of time be-
tween the two time points. Since this length is easy
to calculate automatically, this information can be
factored into the topic detector trainer.
The interface also allows participants to enter ac-
tion items through a dedicated action item form.
Again the advantage of such a form to the partici-
pants is that the action items (and thus the notes) are
better organized: After the meeting, they can per-
form retrieval on specific fields of the action items.
For example, they can ask to retrieve all the action
items assigned to a particular participant, or that are
due a particular day, etc.
In addition to being beneficial to the participant,
the action item form filling action results in gener-
ating labeled data for the action item detector.
Specifically, if we make the assumption that an ac-
tion item form filling action is preceded by a discus-
sion of the action item, then the system can couple
the contents of the form with all the speech within
a window of time before the form filling action, and
use this pair as a data point to retrain its action item
detector.
3.2 SmartNotes Note Retrieval Website
As notes and audio/video are recorded on each indi-
vidual participant?s laptop, they also get transferred
over the internet to a central meeting server. This
transfer occurs in the background without any in-
tervention from the user, utilizes only the left?over
bandwidth beyond the user?s current bandwidth us-
age, and is robust to system shut?downs, crashes,
etc. This process is described in more detail in
(Banerjee et al, 2004).
Once the meeting is over and all the data has been
transferred to the central server, meeting participants
can use the SmartNotes multi?media notes retrieval
system to view the notes and access the recorded
audio/video. This is a web?based application that
uses the same login process as the stand?along note
263
Figure 2: Screen shot of the SmartNotes website
taking system. Users can view a list of meetings
they have recorded using the SmartNotes applica-
tion in the past, and then for each meeting, they can
view the shared notes taken at the meeting. Figure
2 shows a screen shot of such a notes browsing ses-
sion. Additionally, participants can view their own
private notes taken during the meeting.
In addition to viewing the notes, they can also ac-
cess all recorded audio/video, indexed by the notes.
That is, they can access the audio/video recorded
around the time that the note was entered. Further
they can specify how many minutes before and af-
ter the note they wish to access. Since the server
has the audio from each meeting participant?s audio
channel, the viewer of the notes can choose to listen
to any one person?s channel, or a combination of the
audio channels. The merging of channels is done in
real time and is achievable because their time stamps
have been synchronized during recording.
In the immediate future we plan to implement a
simple key?word based search on the notes recorded
in all the recorded meetings (or in one specific meet-
ing). This search will return notes that match the
search using a standard tf.idf approach. The user
will also be provided the option of rating the qual-
ity of the search retrieval on a one bit satisfied/not?
satisfied scale. If the user chooses to provide this
rating, it can be used as a feedback to improve the
search. Additionally, which parts of the meeting the
user chooses to access the audio/video from can be
used to form a model of the parts of the meetings
most relevant to the user. This information can help
the system tailor its retrieval to individual prefer-
ences.
4 The Demonstration
We shall demonstrate both the SmartNotes note tak-
ing client as well as the SmartNotes note?retrieval
website. Specifically we will perform 2 minute long
mock meetings between 2 or 3 demonstrators. We
will show how notes can be taken, how agendas can
be created and action items noted. We will then
show how the notes and the audio/video from the 2
minute meeting can be accessed through the Smart-
Notes note retrieval website. We shall also show the
automatically labeled data that gets created both dur-
ing the mock meeting, as well as during the brows-
ing session. Finally, if time permits, we shall show
results on how much we can improve the meeting
understanding components? capabilities through la-
beled meeting data automatically acquired through
participants? use of SmartNotes at CMU and other
institutions that are currently using the system.
References
S. Banerjee and A. I. Rudnicky. 2006a. A texttiling
based approach to topic boundary detection in multi?
participant conversations. Submitted for publication.
S. Banerjee and A. I. Rudnicky. 2006b. You are what
you say: Using meeting participants? speech to detect
their roles and expertise. In Analyzing Conversations
in Text and Speech Workshop at HLT?NAACL 2006,
New York City, USA, June.
S. Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Pato-
dia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso,
A. Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky.
2004. Creating multi-modal, user?centric records of
meetings with the Carnegie Mellon meeting recorder
architecture. In Proceedings of the ICASSP Meeting
Recognition Workshop, Montreal, Canada.
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic?level annotations to meeting
browsing. In Proceedings of the Tenth International
Conference on Human-Computer Interaction, Rome,
Italy, September.
264
Proceedings of NAACL HLT 2007, Companion Volume, pages 73?76,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Implicitly Supervised Language Model Adaptation for Meeting
Transcription
David Huggins-Daines
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dhuggins@cs.cmu.edu
Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
air@cs.cmu.edu
Abstract
We describe the use of meeting metadata,
acquired using a computerized meeting
organization and note-taking system, to
improve automatic transcription of meet-
ings. By applying a two-step language
model adaptation process based on notes
and agenda items, we were able to re-
duce perplexity by 9% and word error rate
by 4% relative on a set of ten meetings
recorded in-house. This approach can be
used to leverage other types of metadata.
1 Introduction
Automatic transcription of multi-party conversa-
tions such as meetings is one of the most difficult
tasks in automatic speech recognition. In (Morgan
et al, 2003) it is described as an ?ASR-complete?
problem, one that presents unique challenges for ev-
ery component of a speech recognition system.
Though much of the literature on meeting tran-
scription has focused on the unique acoustic mod-
eling and segmentation problems incurred by meet-
ing transcription, language modeling for meetings
is an interesting problem as well. Though meet-
ing speech is spontaneous in nature, the vocabulary
and phrasing in meetings can be very specialized
and often highly technical. Speaking style can vary
greatly between speakers, and the discourse struc-
ture of multi-party interaction gives rise to cross-
speaker effects that are difficult to model with stan-
dard N-gram models (Ji and Bilmes, 2004).
Speech in meetings has one crucial advantage
over many other transcription tasks, namely that it
does not occur in isolation. Meetings are scheduled
and discussed in advance, often via e-mail. People
take notes and create agendas for meetings, and of-
ten read directly from electronic presentation mate-
rials. The structure of meetings can be exploited -
topics can be segmented both temporally and across
speakers, and these shifting topics can be modeled
as sub-languages.
We examine the effect of leveraging one partic-
ular type of external information, namely the writ-
ten agendas and meeting minutes, and we demon-
strate that, by using off-line language model adapta-
tion techniques, these can significantly (p < 0.01)
improve language modeling and speech recognition
accuracy. The language in the notes and agendas is
very similar to that used by the speakers, hence we
consider this to be a form of semi-supervised or im-
plicitly supervised adaptation.
2 Corpus
The SmartNotes system, described in (Banerjee and
Rudnicky, 2007) is a collaborative platform for
meeting organization, recording, and note-taking.
As part of our research into meeting segmentation
and recognition, we have collected a series of 10 un-
scripted meetings using SmartNotes. These meet-
ings themselves are approximately 30 minutes in
length (ranging from 1745 to 7208 words) with three
regular participants, and consist of discussions and
reporting on our ongoing research. The meetings
are structured around the agendas and action items
constructed through the SmartNotes interface. The
73
agenda itself is largely constant from meeting to
meeting, while each meeting typically reviews dis-
cusses the previous week?s action items. Each par-
ticipant is equipped with a laptop computer and an
individual headset microphone.
Each meeting was manually transcribed and seg-
mented for training and testing purposes. The tran-
scription includes speaker identification and timing
information. As part of the meeting, participants are
encouraged to take notes and define action items.
These are automatically collected on a server along
with timestamp information. In (Banerjee and Rud-
nicky, 2007), it was shown that timestamped text of
this kind is useful for topic segmentation of meet-
ings. In this work, we have not attempted to take
advantage of the timing information, nor have we
attempted to perform any topic segmentation. Given
the small quantity of text available from the notes,
we feel that the type of static language model adap-
tation presented here is most feasible when done at
the entire meeting level. A cache language model
(Kuhn and Mori, 1990) may be able to capture the
(informally attested) locality effects between notes
and speech.
Since the notes are naturalistic text, often con-
taining shorthand, abbreviations, numbers, punctu-
ation, and so forth, we preprocess them by running
them through the text-normalization component of
the Festival1 speech synthesis system and extracting
the resulting string of individual words. This yielded
an average of 252 words of adaptation data for each
of the 10 meetings.
3 System Description
Unless otherwise noted, all language models eval-
uated here are trigram models using Katz smooth-
ing (Katz, 1987) and Good-Turing discounting. Lin-
ear interpolation of multiple source models was per-
formed by maximizing the likelihood over a held-out
set of adaptation data.
For automatic transcription, our acoustic mod-
els consist of 5000 tied triphone states (senones),
each using a 64-component Gaussian mixture model
with diagonal covariance matrices. The input fea-
tures consist of 13-dimensional MFCC features,
delta, and delta-delta coefficients. These models
1http://www.festvox.org/
Corpus # Words Perplexity
Fisher English 19902585 178.41
Switchboard-I 2781951 215.52
ICSI (75 Meetings) 710115 134.94
Regular Meetings 266043 111.76
Switchboard Cellular 253977 280.81
CallHome English 211377 272.19
NIST Meetings 136932 199.40
CMU (ISL Meetings) 107235 292.86
Scenario Meetings 36694 306.43
Table 1: Source Corpora for Language Model
are trained on approximately 370 hours of speech
data, consisting of the ICSI meeting corpus (Mor-
gan et al, 2003), the HUB-4 Broadcast News cor-
pus, the NIST pilot meeting corpus, the WSJ CSR-
0 and CSR-1 corpora,2 the CMU Arctic TTS cor-
pora (Kominek and Black, 2004), and a corpus of 32
hours of meetings previously recorded by our group
in 2004 and 2005.
Our baseline language model is based on a linear
interpolation of source language models built from
conversational and meeting speech corpora, using a
held-out set of previously recorded ?scenario? meet-
ings. These meetings are unscripted, but have a fixed
topic and structure, which is a fictitious scenario in-
volving the hiring of new researchers. The source
language models contain a total of 24 million words
from nine different corpora, as detailed in Table 1.
The ?Regular Meetings? and ?Scenario Meetings?
were collected in-house and consist of the same 32
hours of meetings mentioned above, along with the
remainder of the scenario meetings. We used a vo-
cabulary of 20795 words, consisting of all words
from the locally recorded, ICSI, and NIST meetings,
combined with the Switchboard-I vocabulary (with
the exception of words occurring less than 3 times).
The Switchboard and Fisher models were pruned by
dropping singleton trigrams.
4 Interpolation and Vocabulary Closure
We created one adapted language model for each
meeting using a two-step process. First, the source
language models were re-combined using linear in-
terpolation to minimize perplexity on the set of notes
2All corpora are available through http://ldc.upenn.edu/
74
Meeting Baseline Interpolated Closure
04/17 90.05 85.96 84.41
04/24 90.16 85.54 81.88
05/02 94.27 89.24 89.19
05/12 110.95 101.68 87.13
05/18 85.78 81.50 78.04
05/23 97.51 93.07 94.39
06/02 109.70 104.49 101.90
06/12 96.80 92.88 91.05
06/16 93.93 87.71 79.17
06/20 97.19 93.88 92.48
Mean 96.57 91.59 (-5.04) 87.96 (-8.7)
S.D. 8.61 7.21 (1.69) 7.40 (6.2)
p n/a < 0.01 < 0.01
Table 2: Adaptation Results: Perplexity
for each meeting. Next, the vocabulary was ex-
panded using the notes. In order to accomplish
this, a trigram language model was trained from the
notes themselves and interpolated with the output of
the previous step using a small, fixed interpolation
weight ? = 0.1. It should be noted that this also
has the effect of slightly boosting the probabilities
of the N-grams that appear in the notes. We felt this
was useful because, though these probabilities are
not reliably estimated, it is likely that people will use
many of the same N-grams in the notes as in their
meeting speech, particularly in the case of numbers
and acronyms. The results of interpolation and N-
gram closure are shown in Table 2 in terms of test-
set perplexity, and in Table 3 in terms of word error
rate. Using a paired t-test over the 10 meetings, the
improvements in perplexity and accuracy are highly
significant (p < 0.01).
5 Topic Clustering and Dimensionality
Reduction
In examining the interpolation component of the
adaptation method described above, we noticed that
the in-house meetings and the ICSI meetings consis-
tently took on the largest interpolation weights. This
is not surprising since both of these corpora are sim-
ilar to the test meetings. However, all of the source
corpora cover potentially relevant topics, and by in-
terpolating the corpora as single units, we have no
way to control the weights given to individual top-
Meeting Baseline Interpolated Closure
04/17 45.22 44.37 43.34
04/24 47.35 46.43 45.25
05/02 47.20 47.20 46.28
05/12 49.74 48.02 46.07
05/18 45.29 44.63 43.44
05/23 43.68 43.00 42.80
06/02 48.66 48.29 47.85
06/12 45.68 45.90 45.28
06/16 45.98 45.45 44.29
06/20 47.03 46.73 46.68
Mean 46.59 46.0 (-0.58) 45.13 (-1.46)
S.D. 1.78 1.68 (0.54) 1.64 (1.0)
p n/a < 0.01 < 0.01
Table 3: Adaptation Results: Word Error
ics within them. Also, people may use different, but
related, words in writing and speaking to describe
the same topic, but we are unable to capture these
semantic associations between the notes and speech.
To investigate these issues, we conducted sev-
eral brief experiments using a reduced training cor-
pus consisting of 69 ICSI meetings. We converted
these to a vector-space representation using tf.idf
scores and used a deterministic annealing algorithm
(Rose, 1998) to create hard clusters of meetings,
from each of which we trained a source model for
linear interpolation. We compared these clusters to
random uniform partitions of the corpus. The in-
terpolation weights were trained on the notes, and
the models were tested on the meeting transcripts.
Out-of-vocabulary words were not removed from
the perplexity calculation. The results (mean and
standard deviation over 10 meetings) are shown in
Table 4. For numbers of clusters between 2 and
42, the annealing-based clusters significantly out-
perform the random partition. The perplexity with
42 clusters is also significantly lower (p < 0.01)
than the perplexity (256.5? 21.5) obtained by train-
ing a separate source model for each meeting.
To address the second issue of vocabulary mis-
matches between notes and speech, we applied prob-
abilistic latent semantic analysis (Hofmann, 1999)
to the corpus, and used this to ?expand? the vocab-
ulary of the notes. We trained a 32-factor PLSA
model on the content words (we used a simple
75
# of Clusters Random Annealed
2 546.5 ? 107.4 514.1 ? 97.9
4 462.2 ? 86.3 426.2 ? 73.9
8 397.7 ? 67.1 356.1 ? 54.9
42 281.6 ? 31.5 253.7 ? 22.9
Table 4: Topic Clustering Results: Perplexity
Meeting Baseline PLSA ?Boosted?
04/17 105.49 104.59 104.87
04/24 98.97 97.58 97.80
05/02 105.61 104.15 104.48
05/12 122.37 116.73 118.04
05/18 98.55 94.92 95.18
05/23 111.28 107.84 108.03
06/02 125.31 121.49 121.64
06/12 109.31 106.38 106.55
06/16 106.86 103.27 104.28
06/20 117.46 113.76 114.18
Mean 110.12 107.07 107.50
S.D. 8.64 7.84 7.93
p n/a < 0.01 < 0.01
Table 5: PLSA Results: Perplexity
entropy-based pruning to identify these ?content
words?) from the ICSI meeting vocabulary. To adapt
the language model, we used the ?folding-in? proce-
dure described in (Hofmann, 1999), running an iter-
ation of EM over the notes to obtain an adapted un-
igram distribution. We then simply updated the uni-
gram probabilities in the language model with these
new values and renormalized. While the results,
shown in Table 5, show a statistically significant im-
provement in perplexity, this adaptation method is
is problematic, as it increases the probability mass
given to all the words in the PLSA model. In subse-
quent results, also shown in Table 5, we found that
simply extracting these words from the original un-
igram distribution and boosting their probabilities
by the equivalent amount also reduces perplexity
by nearly as much (though the difference from the
PLSA model is statistically significant, p = 0.004).
6 Conclusions
We have shown that notes collected automatically
from participants in a structured meeting situation
can be effectively used to improve language mod-
eling for automatic meeting transcription. Further-
more, we have obtained some encouraging results
in applying source clustering and dimensionality re-
duction to make more effective use of this data. In
future work, we plan to exploit other sources of
metadata such as e-mails, as well as the structure of
the meetings themselves.
7 Acknowledgements
This research was supported by DARPA grant NG
CH-D-03-0010. The content of the information in
this publication does not necessarily reflect the po-
sition or the policy of the US Government, and no
official endorsement should be inferred.
References
S. Banerjee and A. I. Rudnicky. 2007. Segmenting meet-
ings into agenda items by extracting implicit supervi-
sion from human note-taking. In Proceedings of the
2007 International Conference on Intelligent User In-
terfaces, January.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of UAI?99, Stockholm.
G. Ji and J. Bilmes. 2004. Multi-speaker language mod-
eling. In Proceedings of HLT-NAACL.
S. M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
J. Kominek and A. Black. 2004. The CMU Arctic speech
databases. In 5th ISCA Speech Synthesis Workshop,
Pittsburgh.
R. Kuhn and R. De Mori. 1990. A cache-based natural
language model for speech recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
pages 570?583.
N. Morgan, D. Baron, S. Bhagat, R. Dhillon H. Carvey,
J. Edwards, D. Gelbart, A. Janin, A. Krupski, B. Pe-
skin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. Meetings about meetings: research at ICSI on
speech in multiparty conversation. In Proceedings of
ICASSP, Hong Kong, April.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related op-
timization problems. In Proceedings of the IEEE,
pages 2210?2239.
76
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 17?19,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Interactive ASR Error Correction for Touchscreen Devices
David Huggins-Daines
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dhuggins@cs.cmu.edu
Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
air@cs.cmu.edu
Abstract
We will demonstrate a novel graphical inter-
face for correcting search errors in the out-
put of a speech recognizer. This interface
allows the user to visualize the word lattice
by ?pulling apart? regions of the hypothesis
to reveal a cloud of words simlar to the ?tag
clouds? popular in many Web applications.
This interface is potentially useful for dicta-
tion on portable touchscreen devices such as
the Nokia N800 and other mobile Internet de-
vices.
1 Introduction
For most people, dictating continuous speech is con-
siderably faster than entering text using a keyboard
or other manual input device. This is particularly
true on mobile devices which typically have no hard-
ware keyboard whatsoever, a 12-digit keypad, or at
best a miniaturized keyboard unsuitable for touch
typing.
However, the effective speed of text input using
speech is significantly reduced by the fact that even
the best speech recognition systems make errors.
After accounting for error correction, the effective
number of words per minute attainable with speech
recognition drops to within the range attainable by
an average typist (Moore, 2004). Moreover, on a
mobile phone with predictive text entry, it has been
shown that isolated word dictation is actually slower
than using a 12-digit keypad for typing SMS mes-
sages (Karpov et al, 2006).
2 Description
It has been shown that multimodal error correction
methods are much more effective than using speech
alone (Lewis, 1999). Mobile devices are increas-
ingly being equipped with touchscreens which lend
themselves to gesture-based interaction methods.
Therefore, we propose an interactive method of
visualizing and browsing the word lattice using ges-
tures in order to correct speech recognition errors.
The user is presented with the decoding result in a
large font, either in a window on the desktop, or in a
full-screen presentation on a touchscreen device. If
the utterance is too long to fit on the screen, the user
can scroll left and right using touch gestures. The
initial interface is shown in Figure 1.
Figure 1: Initial hypothesis view
Where there is an error, the user can ?pull apart?
the result using a touch stroke (or a multitouch ges-
ture where supported), revealing a ?cloud? of hy-
pothesis words at that point in the utterance, as
shown in Figure 2.
It is also possible to expand the time interval over
which the cloud is calculated by dragging sideways,
resulting in a view like that in Figure 3. The user
can then select zero or more words to add to the hy-
pothesis string in place of the errorful text which was
?exploded?, as shown in Figure 4.
17
Figure 2: Expanded word view
Figure 3: Word cloud expanded in time
The word cloud is constructed by finding all
words active within a time interval whose log poste-
rior probability falls within range of the most prob-
able word. Word posterior probabilities are cal-
culated using the forward-backward algorithm de-
scribed in (Wessel et al, 1998). Specifically, given a
word lattice in the form of a directed acyclic graph,
whose nodes represent unique starting points t in
time, and whose edges represent the acoustic likeli-
hoods of word hypotheses wts spanning a given time
interval (s, t), we can calculate the forward variable
?t(w), which represents the joint probability of all
word sequences ending in wts and the acoustic ob-
servations up to time t, as:
?t(w) = P (O
s
1, w
t
s) =
?
vts?prev(w)
P (w|v)P (wts)?s(v)
Here, P (w|v) is the bigram probability of (v, w)
obtained from the language model and P (wts) is the
acoustic likelihood of the word model w given the
observed speech from time s to t, as approximated
by the Viterbi path score.
Figure 4: Selecting replacement words
Likewise, we can compute the backward variable
?t(w), which represents the conditional probabil-
ity of all word sequences beginning in wts and the
acoustic observations from time t + 1 to the end of
the utterance, given wts:
?t(w) = P (O
T
t |w
t
s) =
?
vet?succ(w)
P (v|w)P (vet )?e(v)
The posterior probability P (wts|O
T
1 ) can then be
obtained by multiplication and normalization:
P (wts|O
T
1 ) =
P (wts, O
T
1 )
P (OT1 )
=
?t(w)?t(w)
P (OT1 )
This algorithm has a straightforward extension to
trigram language models which has been omitted
here for simplicity.
This interface is inspired by the web browser
zooming interface used on the Apple iPhone (Ap-
ple, Inc., 2008), as well as the Speech Dasher
lattice correction tool (Vertanen, 2004). We feel
that it is potentially useful not only for auto-
matic speech recognition, but also for machine
translation and any other situation in which
a lattice representation of a possibly errorful
hypothesis is available. A video of this in-
terface in Ogg Theora format1 can be viewed at
http://www.cs.cmu.edu/?dhuggins/touchcorrect.ogg.
1For Mac OS X: http://xiph.org/quicktime/download.html
For Windows: http://www.illiminable.com/ogg/downloads.html
18
3 Script Outline
For our demonstration, we will have available
a poster describing the interaction method being
demonstrated. We will begin by describing the mo-
tivation for this work, followed by a ?silent? demo
of the correction method itself, using pre-recorded
audio. We will then demonstrate live speech input
and correction using our own voices. The audience
will then be invited to test the interaction method on
a touchscreen device (either a handheld computer or
a tablet PC).
4 Requirements
To present this demo, we will be bringing two Nokia
Internet Tablets as well as a laptop and possibly a
Tablet PC. We have no requirements from the con-
ference organizers aside from a suitable number of
power outlets, a table, and a poster board.
Acknowledgements
We wish to thank Nokia for donating an N800 Inter-
net Tablet used to develop this software.
References
E. Karpov, I. Kiss, J. Leppa?nen, J. Olsen, D. Oria, S.
Sivadas and J. Tian 2006. Short Message Sys-
tem dictation on Series 60 mobile phones. Work-
shop on Speech in Mobile and Pervasive Environ-
ments (SiMPE) in Conjunction with MobileHCI 2006.
Helsinki, Finland.
Keith Vertanen 2004. Efficient Computer Interfaces
Using Continuous Gestures, Language Models, and
Speech. M.Phil Thesis, University of Cambridge,
Cambridge, UK.
Apple, Inc. 2008. iPhone: Zoom-
ing In to Enlarge Part of a Webpage.
http://docs.info.apple.com/article.html?artnum=305899
Roger K. Moore 2004. Modelling Data Entry Rates for
ASR and Alternative Input Methods. Proceedings of
Interspeech 2004. Jeju, Korea.
James R. Lewis 1999. Effect of Error Correction Strat-
egy on Speech Dictation Throughput Proceedings of
the Human Factors and Ergonomics Society 43rd An-
nual Meeting.
Frank Wessel, Klaus Macherey, Ralf Schlu?ter 1998. Us-
ing Word Probabilities as Confidence Measures. Pro-
ceedings of ICASSP 1998.
19
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 89?92,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Predicting Barge-in Utterance Errors by using
Implicitly Supervised ASR Accuracy and Barge-in Rate per User
Kazunori Komatani
Graduate School of Informatics
Kyoto University
Yoshida, Sakyo, Kyoto 606-8501, Japan
komatani@i.kyoto-u.ac.jp
Alexander I. Rudnicky
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, U.S.A.
air@cs.cmu.edu
Abstract
Modeling of individual users is a promis-
ing way of improving the performance of
spoken dialogue systems deployed for the
general public and utilized repeatedly. We
define ?implicitly-supervised? ASR accu-
racy per user on the basis of responses
following the system?s explicit confirma-
tions. We combine the estimated ASR ac-
curacy with the user?s barge-in rate, which
represents how well the user is accus-
tomed to using the system, to predict in-
terpretation errors in barge-in utterances.
Experimental results showed that the es-
timated ASR accuracy improved predic-
tion performance. Since this ASR accu-
racy and the barge-in rate are obtainable
at runtime, they improve prediction perfor-
mance without the need for manual label-
ing.
1 Introduction
The automatic speech recognition (ASR) result
is the most important input information for spo-
ken dialogue systems, and therefore, its errors are
critical problems. Many researchers have tackled
this problem by developing ASR confidence mea-
sures based on utterance-level information and
dialogue-level information (Litman et al, 1999;
Walker et al, 2000). Especially in systems de-
ployed for the general public such as those of (Ko-
matani et al, 2005) and (Raux et al, 2006), the
systems need to correctly detect interpretation er-
rors caused by various utterances made by vari-
ous kinds of users including novices. Furthermore,
since some users access such systems repeatedly
(Komatani et al, 2007), error detection by using
individual user models would be a promising way
of improving performance.
In another aspect in dialogue systems, cer-
tain dialogue patterns indicate that ASR results
in certain positions are reliable. For exam-
ple, Sudoh and Nakano (2005) proposed ?post-
dialogue confidence scoring? in which ASR re-
sults corresponding to the user?s intention upon
dialogue completion are assumed to be correct
and are used for confidence scoring. Bohus and
Rudnicky (2007) proposed ?implicitly-supervised
learning? in which users? responses following the
system?s explicit confirmations are used for confi-
dence scoring. If ASR results can be regarded as
reliable after the dialogue, machine learning algo-
rithms can use such ASR results as teacher signals.
This approach enables the system to improve its
performance without any manual labeling or tran-
scription, a task which requires much time and la-
bor when spoken dialogue systems are developed.
We focus on users? affirmative and negative re-
sponses to the system?s explicit confirmations as
in (Bohus and Rudnicky, 2007) and estimate the
user?s ASR accuracy on the basis of his or her his-
tory of responses. The estimated ASR accuracy is
combined with the user?s barge-in rate to predict
the interpretation error in the current barge-in ut-
terance. Because the estimated ASR accuracy and
the barge-in rate per user are obtainable at runtime,
it is possible to improve prediction performance
without any manual transcription or labeling.
2 Implicitly Supervised Estimation of
ASR Accuracy
2.1 Predicting Errors in Barge-in Utterance
We aim to predict interpretation errors in barge-
in utterances at runtime. These errors are caused
by ASR errors, and barge-in utterances are more
prone to be misrecognized. A user study con-
ducted by Rose and Kim (2003) revealed that there
are many more disfluencies when users barge-in
compared with when users wait until the system
prompt ends. It is difficult to select the erroneous
utterances to be rejected by using a classifier that
89
distinguishes speech from noise on the basis of the
Gaussian Mixture Model (Lee et al, 2004); such
disfluencies and resulting utterance fragments are
parts of human speech.
Barge-in utterances are, therefore, more diffi-
cult to recognize correctly, especially when novice
users barge-in. To detect their interpretation er-
rors, other features should be incorporated instead
of speech signals or ASR results. We predicted
the interpretation errors in barge-in utterances on
the basis of each user?s barge-in rate (Komatani et
al., 2008). This rate intuitively corresponds to how
well users are accustomed to using the system, es-
pecially to its barge-in function.
Furthermore, we utilize a user?s ASR accuracy
in his or her history of all utterances including
barge-ins. The ASR accuracy also indicates the
user?s habituation. However, it has been shown
that the user?s ASR accuracy and barge-in rate
do not improve simultaneously (Komatani et al,
2007). In fact, some expert users have low barge-
in rates. We thus can predict whether a barge-in
utterance will be correctly interpreted or not by
integrating the user?s current ASR accuracy and
barge-in rate.
2.2 Estimating ASR Accuracy by using
Implicitly Supervised Labels
To perform runtime prediction, we use informa-
tion derived from the dialogue patterns to estimate
the user?s ASR accuracy. We estimate the accu-
racy on the basis of the user?s history of responses
following the system?s explicit confirmations such
as ?Leaving from Kyoto Station. Is that correct??
Specifically, we assume that the ASR results
of affirmative or negative responses following ex-
plicit confirmations are correct and that the user
utterances corresponding to the content of the af-
firmative responses are also correct. We further
assume that the remaining utterances are incorrect
because users do not often respond with ?no? for
explicit confirmations containing incorrect content
and instead repeat their original utterances. Con-
sequently, we regard that the ASR results of the
following utterances are correct: (1) affirmative
responses and their immediately preceding utter-
ances and (2) negative responses. Accordingly, all
other utterances are incorrect. We thus calculate
the user?s estimated ASR accuracy by using the
user?s utterance history, as follows:
(Estimated ASR accuracy)
=
2? (#affirmatives) + (#negatives)
(#all utterances)
(1)
2.3 Predicting Errors by Using Barge-in Rate
and ASR Accuracy
We predict the errors in barge-in utterances by us-
ing a logistic regression function:
P =
1
1 + exp(?(a
1
x
1
+ a
2
x
2
+ b))
.
Its inputs x
1
and x
2
are the barge-in rate until the
current utterance and ASR accuracy until the pre-
vious utterance. To account for temporal changes
in barge-in rates, we set a window when calculat-
ing them (Komatani et al, 2008). That is, when
the window width is N , the rates are calculated by
using only the last N utterances, and the previous
utterances are discarded. When the window width
exceeds the total number of utterances by the user,
the barge-in rates are calculated by using all the
user?s utterances. Thus, when the width exceeds
2,838, the maximum number of utterances made
by one user in our data, the barge-in rates equal
the average rates of all previous utterances by the
user.
We calculate the estimated ASR accuracy every
time a user makes an affirmative or negative re-
sponse. When the user makes other utterances, we
take the estimated accuracy when the last affirma-
tive/negative response is made to be the accuracy
of those utterances.
3 Experimental Evaluation
3.1 Target Data
We used data collected by the Kyoto City Bus In-
formation System (Komatani et al, 2005). This
system locates a bus that a user wants to ride and
tells the user how long it will be before the bus
arrives. The system was accessible to the public
by telephone. It used the safest strategy to prevent
erroneous responses, that is, to make explicit con-
firmations for all ASR results.
We used 27,519 utterances after removing calls
whose phone numbers were not recorded and
those the system developer called for debugging.
From that number, there were 7,193 barge-in ut-
terances, i.e., utterances that a user starts speaking
during a system prompt. The phone numbers of
the calls were recorded, and we assumed that each
90
Table 1: ASR accuracy by response type
Correct Incorrect Total (Acc.)
Affirmative 9,055 246 9,301 (97.4%)
Negative 2,006 289 2,295 (87.4%)
Other 8,914 7,009 15,923 (57.9%)
Total 19,975 7,544 27,519 (72.6%)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
Es
tim
at
ed
 A
SR
 A
cc
ur
ac
y.
Transcription-based ASR Accuracy
Figure 1: Correlation between transcription-based
and estimated ASR accuracy
number corresponded to one individual. Most of
the numbers were those of mobile phones, which
are usually not shared, so the assumption seems
reasonable.
Each utterance was transcribed and its interpre-
tation result, correct or not, was given manually.
We assumed that an interpretation result for an
utterance was correct if all content words in its
transcription were correctly included in the result.
The result was regarded as an error if any content
words were missed or misrecognized.
3.2 Verifying Implicitly Supervised Labels
We confirmed our assumption that the ASR re-
sults of affirmative or negative responses follow-
ing explicit confirmations are correct. We clas-
sified the user utterances into affirmatives, nega-
tives, and other, and calculated the ASR accuracies
(precision rates) as shown in Table 1. Affirmatives
include hai (?yes?), soudesu (?that?s right?), OK,
etc; and negatives include iie (?no?), chigaimasu
(?I don?t agree?), dame (?No good?), etc. The ta-
ble indicates that the ASR accuracies of affirma-
tives and negatives were high. One of the reasons
for the high accuracy was that these utterances are
much shorter than other content words, so they
were not confused with other content words. An-
other reason was that the system often gave help
messages such as ?Please answer yes or no.?
We then analyzed the correlation between the
transcription-based ASR accuracy and the esti-
 55
 60
 65
 70
 75
 80
 1  10  100  1000  10000
Pr
ed
ict
io
n 
Ac
c.
Window width
barge-in rate only
correct ASR acc. + barge-in rate
estimated ASR acc. + barge-in rate
Figure 2: Prediction accuracy with various win-
dow widths
mated ASR accuracy based on Equation 1. We
plotted the two ASR accuracies in Figure 1 for
26,231 utterances made after at least one affir-
mative/negative response by the user. The corre-
lation coefficient between them was 0.806. Al-
though the assumption that all ASR results of af-
firmative/negative responses are correct might be
strong, the estimated ASR accuracy had a high
correlation with the transcription-based ASR ac-
curacy.
3.3 Prediction using Implicitly Supervised
Labels
We measured the prediction accuracy for 7,193
barge-in utterances under several conditions. We
did not set windows when calculating the ASR ac-
curacies and thus used all previous utterances of
the user, because the windows did not improve
prediction accuracy. One of the reasons for this
lack of improvement is that the ASR accuracies
did not change as significantly as the barge-in rates
because the accuracies of frequent users converged
earlier (Komatani et al, 2007).
We first confirmed the effect of the
transcription-based (?correct?, hereafter) ASR
accuracy. As shown in Figure 2 and Table 2,
the prediction accuracy improved by using the
ASR accuracy in addition to the barge-in rate.
The best prediction accuracy (78.6%) was when
the window width of the barge-in rate was 100,
and the accuracy converged when the width was
30. The prediction accuracy was 72.7% when
only the ?correct? ASR accuracy was used, and
the prediction accuracy was 71.8% when only
the barge-in rate was used. Thus, the prediction
accuracy was better when both inputs were used
rather than when either input was used. This
91
Table 2: Best prediction accuracies for each con-
dition and window width w
Conditions (Used inputs) Prediction acc. (%)
barge-in rate 71.8 (w=30)
correct ASR acc. 72.7
+ barge-in rate 78.6 (w=100)
estimated ASR acc. 59.4
+ barge-in rate 74.3 (w=30)
fact indicates that both the barge-in rate and
ASR accuracy have different information and
contribute to the prediction accuracy.
Next, we analyzed the prediction accuracy after
replacing the correct ASR accuracy with the esti-
mated one described in Section 2.2. The best ac-
curacy (74.3%) was when the window width was
30. This accuracy was higher than that of using
only barge-in rates. Hence, the estimated ASR ac-
curacy without manual labeling is effective in pre-
dicting the errors in barge-in utterances at runtime.
4 Conclusion
We proposed a method to estimate the errors in
barge-in utterances by using a novel dialogue-level
feature obtainable at runtime. This method does
not require supervised manual labeling. The esti-
mated ASR accuracy based on the user?s utterance
history was dependable in predicting the errors in
the current utterance. We thus showed that ASR
accuracy can be estimated in an implicitly super-
vised manner.
The information obtained by our method can be
used for confidence scoring. Thus, our future work
will include integrating the proposed features with
bottom-up information such as acoustic-score-
based confidence measures. Additionally, we sim-
ply assumed in this study that all affirmative and
negative responses following the explicit confir-
mation are correct. By modeling this assumption
more precisely, prediction accuracy will improve.
Finally, we identified individuals on the basis of
their telephone numbers. If we utilize user identi-
fication techniques to account for situations when
no speaker information is available beforehand,
this method can be applied to systems other than
telephone-based ones, e.g., to human-robot inter-
action.
Acknowledgments
We are grateful to Prof. Tatsuya Kawahara of Ky-
oto University who led the project of the Kyoto
City Bus Information System.
References
Dan Bohus and Alexander Rudnicky. 2007. Implicitly-
supervised learning in spoken language interfaces: an ap-
plication to the confidence annotation problem. In Proc.
SIGdial Workshop on Discourse and Dialogue, pages
256?264.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawahara, and
Hiroshi G. Okuno. 2005. User modeling in spoken dia-
logue systems to generate flexible guidance. User Model-
ing and User-Adapted Interaction, 15(1):169?183.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of real user?s
behaviors in a spoken dialogue system. In Proc. INTER-
SPEECH, pages 142?145.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2008. Predicting ASR errors by exploiting barge-
in rate of individual users for spoken dialogue systems. In
Proc. INTERSPEECH, pages 183?186.
Akinobu Lee, Keisuke Nakamura, Ryuichi Nisimura, Hiroshi
Saruwatari, and Kiyohiro Shikano. 2004. Noice robust
real world spoken dialogue system using GMM based re-
jection of unintended inputs. In Proc. Int?l Conf. Spoken
Language Processing (ICSLP), pages 173?176.
Diane J. Litman, Marilyn A. Walker, and Michael S. Kearns.
1999. Automatic detection of poor speech recognition at
the dialogue level. In Proc. Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages 309?
316.
Antoine Raux, Dan Bohus, Brian Langner, Alan W. Black,
and Maxine Eskenazi. 2006. Doing research on a de-
ployed spoken dialogue system: One year of Let?s Go!
experience. In Proc. INTERSPEECH.
Richard C. Rose and Hong Kook Kim. 2003. A hy-
brid barge-in procedure for more reliable turn-taking in
human-machine dialog systems. In Proc. IEEE Auto-
matic Speech Recognition and Understanding Workshop
(ASRU), pages 198?203.
Katsuhito Sudoh and Mikio Nanano. 2005. Post-dialogue
confidence scoring for unsupervised statistical language
model training. Speech Communication, 45:387?400.
Marilyn Walker, Irene Langkilde, Jerry Wright, Allen Gorin,
and Diane Litman. 2000. Learning to predict problematic
situations in a spoken dialogue system: Experiments with
How May I Help You? In Proc. North American Chapter
of Association for Computational Linguistics (NAACL),
pages 210?217.
92
Stochastic Language Generation for Spoken Dialogue Systems 
Alice H. Oh 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA 15213 
aliceo+@cs.cmu.edu 
Alexander I. Rudnicky 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA 15213 
air+@cs.cmu.edu 
Abstract 
The two current approaches to language 
generation, Template-based and rule-based 
(linguistic) NLG, have limitations when 
applied to spoken dialogue systems, in part 
because they were developed for text 
generation. In this paper, we propose a new 
corpus-based approach to natural anguage 
generation, specifically designed for spoken 
dialogue systems. 
Introduct ion 
Several general-purpose rule-based generation 
systems have been developed, some of which 
are available publicly (cf. Elhadad, 1992). 
Unfortunately these systems, because of their 
generality, can be difficult to adapt to small, 
task-oriented applications. Bateman and 
Henschel (1999) have described a lower cost and 
more efficient generation system for a specific 
application using an automatically customized 
subgrammar. Busemann and Horacek (1998) 
describe a system that mixes templates and rule- 
based generation. This approach takes 
advantages of templates and rule-based 
generation as needed by specific sentences or 
utterances. Stent (1999) has proposed a similar 
approach for a spoken dialogue system. 
However, there is still the burden of writing and 
maintaining rammar rules, and processing time 
is probably too slow for sentences using 
grammar rules (only the average time for 
templates and rule-based sentences combined is 
reported in Busemann and Horacek, 1998), for 
use in spoken dialogue systems. 
Because comparatively less effort is needed, 
many current dialogue systems use template- 
based generation. But there is one obvious 
disadvantage: the quality of the output depends 
entirely on the set of templates. Even in a 
relatively simple domain, such as travel 
reservations, the number of templates necessary 
for reasonable quality can become quite large 
that maintenance becomes a serious problem. 
There is an unavoidfible trade-off between the 
amount of time and effort in creating and 
maintaining templates and the variety and 
quality of the output utterances. 
Given these shortcomings of the above 
approaches, we developed a corpus-based 
generation system, in which we model anguage 
spoken by domain experts performing the task of 
interest, and use that model to stochastically 
generate system utterances. We have applied this 
technique to sentence realization and content 
planning, and have incorporated the resulting 
generation component into a working natural 
dialogue system (see Figure 1). In this paper, we 
describe the technique and report the results of 
two evaluations. 
We used two corpora in the travel 
reservations domain to build n-gram language 
models. One corpus (henceforth, the CMU 
corpus) consists of 39 dialogues between a travel 
agent and clients (Eskenazi, et al 1999). 
Surface 
Real izat ion 
Content  
Planning 
Sentence 
Planning 
Dialogue Manager Generation Engine 
Figure 1 : Overall Architecture 
27 
query_arrive_city 
query_arrive_time 
query_confirm 
query_depart_date 
query_depart_time 
query_pay_by_card 
query_preferred_airport 
query_returndate 
query_return_time 
hotel car info 
hotel_hotel_chain 
hotel_hotel_info 
inform_airport 
inform_confirm_utterance 
inform_flight 
inform_flight_another 
inform_flight_earlier 
n form_flight_earliest 
inform_flight_later 
inform_flight_latest 
inform_not_avail 
inform_num_flights 
inform_price 
other 
Figure 2 : utterance classes 
airline 
arriveairport 
arriveSci:ty - 
arrive_date - 
arrive_time 
car company 
car price 
depart airport 
depart city 
depart_date 
depart_time 
flight_hum 
hotel_city 
hotel_price 
name 
num_flights 
pm 
price 
Figure 3 : word classes 
Another corpus (henceforth, the SRI corpus) 
consists of 68 dialogues between a travel agent 
and users in the SRI community (Kowtko and 
Price 1989). 
The utterances in the two corpora were 
tagged with utterance classes and word classes 
(see Figure 2 and Figure 3). The CMU corpus 
was manually tagged, and back-off trigram 
models built (using Clarkson and Rosenfeld, 
1997). These language models were used to 
automatically tag the SRI corpus; the tags were 
manually checked. 
1 Content P lanning 
In content planning we decide which attributes 
(represented as word classes, see Figure 3) 
should be included in an utterance. In a task- 
oriented dialogue, the number of attributes 
generally increases during the course of the 
dialogue. Therefore, as the dialogue progresses, 
we need to decide which ones to include at each 
system turn. If we include all of them every time 
(indirect echoing, see Hayes and Reddy, 1983), 
the utterances become overly lengthy, but if we 
remove all unnecessary attributes, the user may 
get confused. With a fairly high recognition 
error rate, this becomes an even more important 
issue. 
The problem, then, is to find a compromise 
between the two. We compared two ways to 
systematically generate system utterances with 
only selected attributes, such that the user hears 
repetition of some of the constraints he/she has 
specified, at appropriate points in the dialogue, 
without sacrificing naturalness and efficiency. 
The specific problems, then, are deciding what 
should be repeated, and when. We first describe 
a simple heuristic of old versus new information. 
Then we present a statistical approach, based on 
bigram models. 
1.1 First  approach:  old versus new 
As a simple solution, we can use the previous 
dialogue history, by tagging the attribute-value 
pairs as old (previously said by the system) 
information or new (not said by the system yet) 
information. The generation module would 
select only new information to be included in the 
system utterances. Consequently, information 
? given by the user is repeated only once in the 
dialogue, usually in the utterance immediately 
following the user utterance in which the new 
information was given 1. 
Although this approach seems to work fairly 
well, echoing user's constraints only once may 
not be the right thing to do. Looking at human- 
human dialogues, we observe that this is not 
very natural for a conversation; humans often 
repeat mutually known information, and they 
also often do not repeat some information at all. 
Also, this model does not capture the close 
relationship between two consecutive utterances 
within a dialogue. The second approach tries to 
address these issues. 
1.2 Second approach:  statist ical  mode l  
For this approach, we adopt the first of the two 
sub-maxims in (Oberlander, 1998) '?'Do the 
human thing". Oberlander (1998) talks about 
generation of referring expressions, but it is 
universally valid, at least within natural 
language generation, to say the best we can do is 
When the system utterance uses a template that does 
not contain the slots for the new information given in 
the previous user utterance, then that new 
information will be confirmed in the next available 
system utterance in which the template contains those 
slots. 
28 
to mimic human behavior. Hence, we built a 
two-stage statistical model of human-human 
dialogues using the CMU corpus. The model 
first predicts the number of attributes in the 
system utterance given the utterance class, then 
predicts the attributes given the attributes in the 
previous user utterance. 
1.2.1 The number of attributes model 
The first model will predict the number of 
attributes in a system utterance given the 
utterance class. The model is the probability 
distribution P(nk) = P(nklck), where nk is the 
number of attributes and Ck is the utterance class 
for system utte~anee k. 
1.2.2 The bigram model of the attributes 
This model will predict which attributes to use 
in a system utterance. Using a statistical model, 
what we need to do is find the set of attributes 
A* = {al, az . . . . .  an } such that 
A * = arg max F I  P(al, a2 ..... an) 
We assume that the distributions of the ai's 
are dependent on the attributes in the previous 
utterances. As a simple model, we look only at 
the utterance immediately preceding the current 
utterance and build a bigram model of the 
attributes. In other words, A* = arg max P(AIB), 
where B = {b l ,  b2 . . . . .  b in},  the set of m 
attributes in the preceding user utterance. 
If we took the above model and tried to 
apply it directly, we would run into a serious 
data sparseness problem, so we make two 
independence assumptions. The first assumption 
is that the attributes in the user utterance 
contribute independently to the probabilities of 
the attributes in the system utterance following 
it. Applying this assumption to the model above, 
we get the following: 
m 
A * = arg max ~ P(bk)P(A I bk) 
k=l 
The second independence assumption is that 
the attributes in the system utterance are 
independent of each other. This gives the final 
model that we used for selecting the attributes. 
m tl 
A*. = arg max ~ P(bk ) \ [ ' I  P(al I bk) 
k=l i=1 
29 
Although this independence assumption is 
an oversimplification, this simple model is a 
good starting point for our initial 
implementation f this approach. 
2 Stochastic Surface Realization 
We follow Busemann and Horacek (1998) in 
designing our generation engine with "different 
levels of granularity." The different levels 
contribute to the specific needs of the various 
utterance classes. For example, at the beginning 
of the dialogue, a system greeting can be simply 
generated by a "canned" expression. Other short, 
simple utterances can be generated efficiently by 
templates. In Busemann and Horacek (1998), the 
remaining output is generated by grammar rules. 
We replace the gefieration grammar with a 
simple statistical anguage model to generate 
more complex utterances. 
There are four aspects to our stochastic 
surface realizer: building language models, 
generating candidate utterances, scoring the 
utterances, and filling in the slots. We explain 
each of these below. 
2.1 Building Language Models 
Using the tagged utterances as described in 
the introduction, we built an unsmoothed n-gram 
language model for each utterance class. Tokens 
that belong in word classes (e.g., "U.S. 
Airways" in class "airline") were replaced by the 
word classes before building the language 
models. We selected 5 as the n in n-gram to 
introduce some variability in the output 
utterances while preventing nonsense utterances. 
Note that language models are not used here 
in the same way as in speech recognition. In 
speech recognition, the language model 
probability acts as a 'prior' in determining the 
most probable sequence of words given the 
acoustics. In other words, 
W* = arg max P(WIA) 
= arg max P(AI W)Pr(W) 
where W is the string of words, wl, ..., wn, and 
A is the acoustic evidence (Jelinek 1998). 
Although we use the same statistical tool, 
we compute and use the language model 
probability directly to predict he next word. In 
other words, the most likely utterance is W* = 
arg max P(WIu), where u is the utterance class. 
We do not, however, look for the most likely 
hypothesis, but rather generate each word 
randomly according to the distribution, as 
illustrated in the next section. 
2.2 Generat ing  Ut terances  
The input to NLG from the dialogue 
manager is a frame of attribute-value pairs. The 
first two attribute-value pairs specify the 
utterance class. The rest of the frame contains 
word classes and their values. Figure 4 is an 
example of an input frame to NLG. 
- act-query 
content depart_time 
depart_city New York 
arrive_city San Francisco 
depart_date 19991117 
} 
Figure 4 : an input frame to NLG 
The generation engine uses the appropriate 
language model for the utterance class and 
generates word sequences randomly according 
to the language model distributions. As in 
speech recognition, the probability of a word 
using the n-gram language model is 
P(wi) = P(wilwi.1, wi.2 . . . .  Wi.(n.1) , U) 
where u is the utterance class. Since we have 
built separate models for each of the utterance 
classes, we can ignore u, and say that 
P(wi) = P(wilw|.l, wi-2 . . . .  Wi.(n.1)) 
using the language model for u. 
Since we use unsmoothed 5,grams, we will 
not generate any unseen 5-grams (or smaller n- 
grams at the beginning and end of an utterance). 
This precludes generation of nonsense 
utterances, at least within the 5-word window. 
Using a smoothed n-gram would result in more 
randomness, but using the conventional back-off 
methods (Jelinek 1998), the probability mass 
assigned to unseen 5-grams would be very 
small, and those rare occurrences of unseen n- 
grams may not make sense anyway. There is the 
problem, as in speech recognition using n-gram 
language models, that long-distance dependency 
cannot be captured. 
= 
2.3 Scoring Utterances 
For each randomly generated utterance, we 
compute a penalty score. The score is based on 
the heuristics we've empirically selected. 
Various penalty scores are assigned for an 
utterance that 1. is too short or too long 
(determined by utterance-class dependent 
thresholds), 2. contains repetitions of any of the 
slots, 3. contains lots for which there is no valid 
value in the frame, or 4. does not have some 
required slots (see section 2 for deciding which 
slots are required). 
The generation engine generates a candidate 
utterance, scores it, keeping only the best-scored 
utterance up to that point. It stops and returns the 
best utterance when it finds an utterance with a 
zero penalty scoreTor uns out of time. 
2.4 Fil l ing Slots 
The last step is filling slots with the appropriate 
values. For example, the utterance "What time 
would you like to leave {depart_city}?" 
becomes "What time would you like to leave 
New York?". 
3 Evaluation 
It is generally difficult to empirically evaluate a
generation system. In the context of spoken 
dialogue systems, evaluation of NLG becomes 
an even more difficult problem. One reason is 
simply that there has been very little effort in 
building generation engines for spoken dialogue 
systems. Another reason is that it is hard to 
separate NLG from the rest of the system. It is 
especially hard to separate evaluation of 
language generation and speech synthesis. 
As a simple solution, we have conducted a
comparative evaluation by running two identical 
systems varying only the generation component. 
In this section we present results from two 
preliminary evaluations of our generation 
algorithms described in the previous ections. 
3.1 Content Planning: Experiment 
For the content planning part of the generation 
-system, we conducted a comparative evaluation 
of the two different generation algorithms: 
old/new and bigrams. Twelve subjects had two 
dialogues each, one with the old/new generation 
system, and another with the bigrams generation 
30 
system (in counterbalanced order); all other 
modules were held fixed. Afterwards, each 
subject answered seven questions on a usability 
survey. Immediately after, each subject was 
given transcribed logs of his/her dialogues and 
asked to rate each system utterance on a scale of 
1 to 3 (1 = good; 2 = okay; 3 = bad). 
3.2 Content Planning: Results 
For the usability survey, the results seem to 
indicate subjects' preference for the old/new 
system, but the difference is not statistically 
significant (p - 0.06). However, six out of the 
twelve subjects chose the bigram system to the 
question "Durqng-the session, which system's 
responses were easier to understand?" compared 
to three subjects choosing the old/new system. 
3.3 Surface Realization: Experiment 
For surface realization, we conducted a batch- 
mode evaluation. We picked six recent calls to 
our system and ran two generation algorithms 
(template-based generation and stochastic 
generation) on the input frames. We then 
presented to seven subjects the generated 
dialogues, consisting of decoder output of the 
user utterances and corresponding system 
responses, for each of the two generation 
algorithms. Subjects then selected the output 
utterance they would prefer, for each of the 
utterances that differ between the two systems. 
The results show a trend that subjects preferred 
stochastic generation over template-based 
generation, but a t-test shows no significant 
difference (p = 0.18). We are in the process of 
designing a larger evaluation. 
4 Conclusion 
We have presented a new approach to language 
generation for spoken dialogue systems. For 
content planning, we built a simple bigram 
model of attributes, and found that, in our first 
implementation, it performs as well as a 
heuristic of old vs. new information. For surface 
realization, we used an n-gram language model 
to stochastically generate ach utterance and 
found that the stochastic system performs at 
least as well as the template-based system. 
Our stochastic generation system has several 
advantages. One of those, an important issue for 
spoken dialogue systems, is the response time. 
With stochastic surface realization, the average 
generation time for the longest utterance class 
(10 - 20 words long) is about 200 milliseconds, 
which is much faster than any rule-based 
systems. Another advantage is that by using a 
corpus-based approach, we are directly 
mimicking the language of a real domain expert, 
rather than attempting to model it by rule. 
Corpus collection is usually the first step in 
building a dialogue system, so we are leveraging 
the effort rather than creating more work. This 
also means adapting this approach to new 
domains and even new languages will be 
relatively simple. 
The approach we present does require some 
amount of knowledge ngineering, though this 
appears to overlap with work needed for other 
parts of the dialogue system. First, defining the 
class of utterance and the attribute-value pairs 
requires care. Second, tagging the human-human 
corpus with the right classes and attributes 
requires effort. However, we believe the tagging 
effort is much less difficult than knowledge 
acquisition for most rule-based systems or even 
template-based systems. Finally, what may 
sound right for a human speaker may sound 
awkward for a computer, but we believe that 
mimicking a human, especially a domain expert, 
is the best we can do, at least for now. 
Acknowledgements 
We are thankful for significant contribution by 
other members of the CMU Communicator 
Project, especially Eric Thayer, Wei Xu, and 
Rande Shern. We would like to thank the 
subjects who participated in our evaluations. We 
also extend our thanks to two anonymous 
reviewers. 
References 
Bateman, J. and Henschel, R. (1999) From full 
generation to 'near-templates' without losing 
generality. In Proceedings of the KI'99 workshop, 
"May I Speak Freely?" 
Busemann, S. and Horacek, H. (1998) A flexible 
shallow approach to text generation. In 
Proceedings of the International Natural Language 
Generation Workshop. Niagara-on-the-Lake, 
Canada.. 
31 
Task-based dialog management using an agenda 
Wei Xu and Alexander I. Rudnicky 
School of Computer Science - Carnegie Mellon University 
5000 Forbes Ave - Pittsburgh, PA 15213 
{xw, air\] @cs. cmu. edu 
Abstract 
Dialog man tigement addresses two specific 
problems: (1) providing a coherent overall 
structure to interaction that extends beyond the 
single turn, (2) correctly managing mixed- 
initiative interaction. We propose a dialog 
management architecture based on the following 
elements: handlers that manage interaction 
focussed on tightly coupled sets of information, 
a product that reflects mutually agreed-upon 
information and an agenda that orders the topics 
relevant to task completion. 
1 Introduction 
Spoken language interaction can take many 
forms. Even fairly simple interaction can be very 
useful, for example in auto-attendant systems. 
For many other applications, however, more 
complex interactions eem necessary, either 
because users cannot always be expected to 
exactly specify what they want in a single 
utterance (e.g., obtaining schedule information) 
or because the task at hand requires ome degree 
of exploration of complex alternatives (e.g., 
travel planning). Additionally, unpredictable 
complexity is introduced through error or 
misunderstanding and the system needs to detect 
and deal with these cases. We are interested in 
managing interaction in the context of a goal- 
oriented task that extends oveg multiple tums. 
Dialog management i  the context of purposeful 
tasks must solve two problems: (1) Keep track 
of the overall interaction with a view to ensuring 
steady progress towards task completion. That 
is, the system must have some idea of how much 
of the task has been completed and more 
importantly some idea of what is yet to be done, 
so that it can participate in the setting of 
intermediate goals and generally shepherd the 
interaction towards a successful completion of 
the task at hand. (2) Robustly handle deviations 
from the nominal progression towards problem 
solution. Deviations are varied: the user may ask 
for something that is  not satisfiable (i. e., 
proposes a set of mutually-incompatible 
constraints), the user may misspeak (or, more 
likely, the system may misunderstand) a request 
and perhaps cause an unintended (and maybe 
unnoticed) deviation from the task. The user 
might also underspecify a request while the 
system requires that a single solution be chosen. 
Finally the user's conception of the task might 
deviate from the system's (and its developers) 
conception, requiring the system to alter the 
order in which it expects to perform the task. 
Ideally, a robust dialog management architecture 
can accommodate all of these circumstances 
within a single framework. 
We have been exploring dialog management 
issues in the context of the Communicator \[3\] 
task. The Communicator handles a complex 
travel task, consisting of air travel, hotels and 
car reservations. 
2 Model ing Dialog 
Existing approaches to dialog management are 
difficult to adapt o the current problem because 
they either impose a rigid structure on the 
interaction or because they are not capable of 
managing data structures beyond a certain level 
of complexity. Call-flow based systems (more 
generally, graph-based systems) handle the 
complexity of dialog management by explicitly 
enumerating all possible dialog states, as well as 
allowable transitions between states. This serves 
the purpose of partitioning the problem into a 
42 
finite set of states, with which can be associated 
topic-specific elements (such as grammar, 
prompts, help and interactions with other system 
components, e.g., database interaction). 
Transition between states is predicated on the 
occurrence of specific events, either the user's 
spoken inputs or through (e.g.) a change in back- 
end state. It is the nature of these systems that 
the graphs are often but not exclusively trees. 
Except for the simplest tasks, graph systems 
have several limitations: Unless the graph is 
carefully designed, users will find themselves 
unable to switch to a topic that is coded in a 
different sub-tree without going through the 
common par~e~t of the two. Often this is through 
the root node-of the dialog. Similarly it is not 
always possible to navigate an existing tree, in 
order, e.g., to correct information supplied in an 
earlier node. 
beforehand the exact type of trip an individual 
might take (though the building blocks of an 
itinerary are indeed known). The system benefits 
from being able to construct the itinerary 
dynamically; we denote these solution objects 
products. Users also expect to be able to 
manipulate and inspect the itinerary under 
construction. By contrast, frame systems do not 
afford the user the ability to manipulate the 
form, past supplying fillers for slots. The 
exception is the selection of an item from a 
solution set. We do not abandon the concept of a 
form altogether: an itinerary is actually a 
hierarchical composition of forms, where the 
forms in this case correspond to tightly-bound 
slots (e.g., those corresponding tothe constraints 
on a particular flight leg) and which can be 
treated as part of the -same topic. 
Frame-based systems provide an alternate, more 
flexible approach. Here the problem is cast as 
form filling: the form specifies all relevant 
information (slots) for an action. Dialog 
management consists of monitoring the form for 
completion, setting elements as these are 
specified by the user and using the presence of 
empty slots as a trigger for questions to the user. 
Form-filling does away with the need to specify 
a particular order in which slots need to be filled 
and allows for a more natural, unconstrained, 
form of input. While ideally suited for tasks that 
can be expressed in terms of filling a single 
form, form-filling can be combined with graph 
representations (typically ergodic) to support a 
set of (possibly) related activities, each of which 
can be cast into a form-filling format. 
Both graph and frame systems hare the property 
that the task usually has a fixed goal which is 
achieved by having the user specify information 
(fill slots) on successive turns. Using a filled out 
form the system performs ome action, such as 
information retrieval. While this capability 
encompasses a large number of useful 
applications it does not necessarily extend to 
more complex tasks, for example ones where the 
goal is to create a complex data object (e.g. \[1\]). 
We have been building a system that allows 
users to construct travel itineraries. This domain 
poses several problems: there is no "form" as 
such to fill out, since we do not know 
3 Task Structure and Scripts 
Intuitively (as well as evident from our 
empirical studies of human travel agents and 
clients) travel planning develops over time as a 
succession of episodes, each focused on a 
specific topic (such as a given flight leg, a hotel 
in a particular city, etc.). Users treat he task as a 
succession of topics, each of which ought to be 
discussed in full and closed, before moving on 
to the next topic. Topics can certainly be 
revisited, but doing so corresponds to an explicit 
conversational move on the part of the 
participants. 
Our first dialog manager took advantage of this 
task structure (\[3\]). By analogy to what we 
observed in the human-human data we refer to it 
as a script-based ialog manager. Script in this 
context simply refers to an explicit sequencing 
of task-related topics. Each topic is expressed as 
a form-filling task, with conventional free-order 
input allowed for form slots and a slot-state 
driven mixed-initiative interaction (i.e., ask the 
user about any empty slot). The topic-specific 
form is actually composed of two parts: 
constraint slots (typically corresponding to 
elements of a query) and a solution slot 
(containing the result of an executed query). 
43 
The control strategy is also actually more 
complex: slots are pre-ordered based on their 
(domain-derived) ability to constrain the 
solution; this ordering provides a default 
sequence in which the system selects elements to 
ask the user about. Control is predicated on the 
state of a slot (whether constraint or solution). 
The state can either be "empty", in which case 
the system should ask the user for a value, filled 
with a single value, in which case it is 
"complete", or filled with multiple values. The 
last case is cause to engage the user in a 
clarification sub-dialog whose goal is to reduce 
multiple values to a single value, either by 
selecting-ma item in the solution set or by 
restating a-constraint. Figure 1 shows the 
structure of the Flight Leg topic in the script- 
based system. 
Fligh, Leg 
"'?~??.o ~ 19e,,'tinatian 
~ Database lookup 
Available \]lights 
Figure 1 Task-based ialog control in a script-based 
system, as determined by the structure of a compound 
schema, with contributions from three simple schema. 
4 An Agenda-based Architecture 
While capable of efficiently handling routine 
travel arrangements, the script-based approach 
has a number of perceived limitations: the script 
is very closely identified with the product data 
structure. Specifically, we used a fixed product 
structure that served as a form to fill out. While 
the entire form does not need to be filled out to 
create a valid itinerary, it nevertheless et limits 
on what the user can construct. Instead we 
wanted a form structure that could be 
dynamically constructed over the course of a 
session, with contributions from both the user 
and the system. The script-based approach also 
seemed to make navigation over the product 
difficult. While we implemented a simple undo 
and correction mechanism that allowed the user 
to revisit preceding product elements, users had 
difficulty using it correctly. While some of the 
difficulty could be traced to inadequate support 
of orientation, the source was more likely the 
inability of the system to treat the product 
structure independent of the script. 
We sought to address-these problems by 
introducing two new data structures: an agenda 
to replace a fixed script and a dynamic product 
that can evolve over the course of a session. In 
the agenda-based system, the product is 
represented as a tree, which reflects the natural 
hierarchy, and order, of the information eeded 
to complete the task. A dynamic product is 
simply one that can be modified over the course 
of a session, for example by adding legs to a trip 
as these are requested by the user rather than 
working from a fixed form. Operationally, this 
means providing a set of operators over tree 
structures and making these available to the user 
and to the system. In our case, we  defined a 
library of sub-trees (say air travel legs or local 
arrangements) and a way to attach these to the 
product structure, triggered either by the setting 
of particular values in the existing tree or 
through explicit requests on the part of the user 
("and then I'd like to fly to Chicago"). 
Each node in the product ree corresponds to a 
handler, which encapsulates computation 
relevant to a single information item. All 
handlers have the same form: they specify a set 
of receptors corresponding to input nets, a 
transform to be applied to obtain a value and a 
specification of what the system might say to the 
user in relation to the information governed by 
the handler. Handlers correspond to the schema 
and compound schema of the script-based 
system (see Figure 1). 
The agenda is an ordered list of topics, 
represented by handlers that govern some single 
item or some collection of information. The 
agenda specifies the overall "plan" for carrying 
out a task. The system's priorities for action are 
captured by the agenda, an ordered list of 
handlers generated through traversal of the 
product structure. The handler on the top of the 
agenda has the highest priority and represents 
the focused topic. A handler can capture 
44 
relevant input from the user and can generate 
prompts to the user. A single handler deals only 
with a mini dialog centering on a particular 
piece of information (e.g. departure date). The 
agenda is a generalization f a stack. It indicates 
both the current focus of interaction (i.e., the 
top-most handler) as well as all undealt-with 
business, and captures the order in which such 
business should be dealt with. (The system's 
high-level goal is to ensure that all values in the 
current product ree have valid settings.) As all 
items in the agenda are potentially activatable 
through what the user speaks, the user has 
corresponding control over the topic in focus. 
The agenda .aLso contains generic handlers that 
sort to the bottom-of the agenda. These can be 
used to consume any inputs that are not caught 
by product-derived handlers (for example, 
requests for help). Figure 2 shows how a 
simplest handler looks like 
Handler User 
HandleOutput 
If !Valid 
NLG("query user_name") 
Return focused_waiting_answer 
Endif 
Return pass_control 
HandleInput 
If \[name I identification\] 
ReadProfile 
Validate 
Endif 
Return pass_control 
Figure 2 An example of handler 
The order of the agenda is generated from the 
left-to-right, depth-first traversal of the product 
tree. When a user input comes in, the system 
calls each handler per their order in the agenda 
and each handler will try to interpret he user 
input. We call the process of letting each handler 
to consume user input as input pass. When a 
handler captures a single piece of information, 
the information is marked as consumed. This 
guarantees that a single information item can be 
consumed by only one handler. After an input 
pass, if the user's input does not directly cause a 
particular handler to generate a question, then 
the system will go through an output pass during 
which each handler will have a chance to 
generate a prompt about itself (e.g. the handler 
for departure date can ask user for a departure 
date). 
AI: 
U2: 
A3: 
U4: 
A5: 
A6: 
U7: 
A8: 
U9: 
A10: 
Ul1: 
A12: 
U13: 
A14: 
A15: 
U16: 
A17: 
U18: 
AI9: 
Hello. You've reached the \[Communicator\]. 
Tell me your lull name 
hi i'd like to fly to seattle tuesday morning 
Traveling to Seattle on Tuesday, August 
eleventh in the morning. Your full name? 
\[John Doe\] 
Hello, \[John\], I'm glad to hear from you 
again. You can now ask me for help. 
OK. Let's see I have a United flight 
departing Pittsburgh at ten forty five a.m. 
arriving into Seattle at one fifty seven p.m. 
Is that OK? 
Yes 
Will you returnto Pittsburgh from Seattle? 
no i will go to Los Angeles 
Traveling to Los Angeles. on what date 
Sorry can we go back to talk about the 
previous leg again 
For the first leg, you are on a United flight 
1446 departing Pittsburgh at ten forty five 
A.M. tomorrow, arriving in Seattle at one 
fifty seven P.M. Do you want to take that? 
actually i want o leave on wednesday 
a flight on wednesday. 
I have a flight departing Pittsburgh at ten 
forty five a.m., arrives Seattle at one fifty 
seven p.m. Is that OK? 
Yes 
OK and on what day do you want to depart 
Seattle? 
next Monday 
Traveling on Monday, August sixteenth. 
Figure 3 An example dialog 
The framework can determine the next step from 
the return code of a handler, it can choose to 
continue the current pass, exit input pass and 
switch to output pass, exit current pass and wait 
for input from user, etc. During a pass, a handler 
can also declare itself as the focus through its 
return code. In this case, it will be promoted to 
the top of the agenda. In order to preserve the 
context of a specific topic, we use a method 
called sub-tree promotion. In this method, a 
handler is first promoted to the left-most node 
among its siblings. The system also handles the 
dependencies among the nodes of the product 
tree. A typical dependent relationship is between 
45 
The initial product ree (simplified) 
Travel 
User 
Flight 
Destl 
Date 1 
Time1 
Next 
User 
Travel 
Flightl 
Destl 
Date 1 
Time 1 
Next 
Flightl 
Travel Destl 
~ Date1 
i 
Destl rimel 
Datel User 
Time 1 travel 
Next Next 
initial A1 
Figure 4 
Next 
Flightl 
Destl 
Date 1 
Fimel 
User 
Fravel 
Product ree at utterance A 10 
Date2 Flightl Flightl 
Flight; Destl Destl 
Dest2 Destl Datel 
Time2 Timel Time1 
Next Date2 Date2 
Flightl Flight,~ Flight2 
Destl Dest2 Dest2 
Datel Time2 rime2 
rime 1 Next Next 
User User User 
travel Travel Travel 
A3 A6 A8 A10 A12 A15 
Date2 
Flight; 
Dest2 
rime2 
Flight~ 
Destl 
Datel 
rime 1 
Next 
User 
travel 
AI7 
Figure 5 The change of agenda long the session 
a parent node and a child node. Usually, the 
value of a parent node is dependent on its 
children. Each node maintains a list of its 
dependent nodes and it will notify its dependents 
about any changes of its value. The dependent 
node can then declare itself invalid and therefore 
a candidate topic for conversation. 
The dialog in figure 3, generated using the 
system, shows a number of features: the ability 
to absorb an implicit change of topic on the part 
of the user (A1-A3), adding to an existing 
itinerary (A8-A10) and handling an explicit 
topic shift (U11). Figure 2 and Figure 3 show 
how the product ree and agenda evolve over the 
course of the dialog 
5 System Implementation 
The Communicator is telephone-based and is 
implemented as a modular distributed system, 
running across NT and Linux platforms. 
Currently the task is captured in an 
approximately 2500-word language based on 
corpora derived from human-human, Wizard of 
Oz and human-computer interaction in this 
domain. Domain information is obtained from 
various sources on the Web. The system has 
information about 500 destinations worldwide, 
though with a majority of these are in the United 
States. To date, we have collected 
approximately 6000 calls, from over 360 
individuals. 
46 
6 Summary and Conclusions 
The agenda-based approach addresses the 
problem of dialog management in complex 
problem-solving tasks. It does so by treating the 
task at hand as one of cooperatively constructing 
a complex data structure, a product, and uses 
this structure to guide the task. The product 
consists of a tree of handlers, each handler 
encapsulates processing relevant o a particular 
schema. Handlers correspond to simple or 
compound schema, the latter acting essentially 
as multi-slOi=fofms. A handler encapsulates 
knowledge n~cessary for interacting about a 
specific information slot, including specification 
of user and system language and of interactions 
with domain agents. Handlers that deal with 
compound schema coordinate tightly bound 
schema and correspond to specific identifiable 
topics of conversation. We define tightly bound 
as those schema that users expect to discuss 
interchangeably, without explicit shifts in 
conversational focus. 
We believe that individual handlers can be 
authored independently of others at the same 
level of hierarchy; in turn we believe this will 
simplify the problem of developing dialog 
systems by managing the complexity of the 
process. 
The agenda contains all topics relevant o the 
current ask. The order of handlers on the agenda 
determines how user input will be will be 
attached to product nodes. Both the system and 
the user however have the ability to reorder 
items on the agenda, the system to foreground 
items that need to be discussed, the user to 
reflect heir current priorities within the task. 
factored out as independent pro~esses. 
We believe that the agenda mechanism can be 
adapted easily to less-complex domains that 
might currently be implemented as a standard 
form-based system (for example a movie 
schedule service). We do not know as yet how 
well the technique will succeed for domains of 
complexity comparable to travel planning but 
with different task structure. 
References 
\[1\] James F. Allen, Lenhart K. Schubert, George 
Ferguson, Peter Heeman, Chung Hee Hwang, 
Tsuneaki Kato, Marc Light, Nathaniel G. Martin, 
Bradford W. Miller, Massimo Poesio, and David 
R. Traum, "The TRAINS Project: A case study in 
building a conversational p anning agent" Journal 
of Experimental nd Theoretical AI, 7(I 995), 7-48. 
\[2\] Bansal, D. and Ravishankar, M. "New features for 
confidence annotation" In Proceedings of the 5th 
International Conference on Spoken Language 
Processing (ICSLP), December 1998, Sydney, 
Australia 
\[3\] Rudnicky, A., Thayer, E., Constantinides, P., 
Tchou, C., Shern, R., Lenzo, K., Xu W., Oh, A. 
"Creating natural dialogs in the Carnegie Mellon 
Communicator system" Proceedings of 
Eurospeech, 1999, Paper 014. 
\[4\] Ward, W. and Issar, S. "Recent improvements in 
the CMU spoken language understanding system" 
In Proceedings of the ARPA Human Language 
Technology Workshop, March 1994, 213-216. 
The mechanisms described in this paper do not 
cover all necessary aspects of dialog 
management but do provide an overall control 
architecture. For example, clarification 
processes, which involve possibly extended 
interaction with respect o the state of a value 
slot, fit into the confines of a single handler and 
are implemented as such. Ideally they could be 
47 
 	
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 23?30,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
You Are What You Say: Using Meeting Participants? Speech
to Detect their Roles and Expertise
Satanjeev Banerjee
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
banerjee@cs.cmu.edu
Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
air@cs.cmu.edu
Abstract
Our goal is to automatically detect the
functional roles that meeting participants
play, as well as the expertise they bring to
meetings. To perform this task, we build
decision tree classifiers that use a combi-
nation of simple speech features (speech
lengths and spoken keywords) extracted
from the participants? speech in meetings.
We show that this algorithm results in a
role detection accuracy of 83% on unseen
test data, where the random baseline is
33.3%. We also introduce a simple aggre-
gation mechanism that combines evidence
of the participants? expertise from multi-
ple meetings. We show that this aggre-
gation mechanism improves the role de-
tection accuracy from 66.7% (when ag-
gregating over a single meeting) to 83%
(when aggregating over 5 meetings).
1 Introduction
A multitude of meetings are organized every day
around the world to discuss and exchange impor-
tant information, to make decisions and to collab-
oratively solve problems. Our goal is to create sys-
tems that automatically understand the discussions
at meetings, and use this understanding to assist
meeting participants in various tasks during and af-
ter meetings. One such task is the retrieval of infor-
mation from previous meetings, which is typically
a difficult and time consuming task for the human
to perform (Banerjee et al, 2005). Another task is
to automatically record the action items being dis-
cussed at meetings, along with details such as when
the action is due, who is responsible for it, etc.
Meeting analysis is a quickly growing field of
study. In recent years, research has focussed on au-
tomatic speech recognition in meetings (Stolcke et
al., 2004; Metze et al, 2004; Hain et al, 2005), ac-
tivity recognition (Rybski and Veloso, 2004), auto-
matic meeting summarization (Murray et al, 2005),
meeting phase detection (Banerjee and Rudnicky,
2004) and topic detection (Galley et al, 2003). Rela-
tively little research has been performed on automat-
ically detecting the roles that meeting participants
play as they participate in meetings. These roles can
be functional (e.g. the facilitator who runs the meet-
ing, and the scribe who is the designated note taker
at the meeting), discourse based (e.g. the presenter,
and the discussion participant), and expertise related
(e.g. the hardware acquisition expert and the speech
recognition research expert). Some roles are tightly
scoped, relevant to just one meeting or even a part
of a meeting. For example, a person can be the fa-
cilitator of one meeting and the scribe of another, or
the same person can be a presenter for one part of
the meeting and a discussion participant for another
part. On the other hand, some roles have a broader
scope and last for the duration of a project. Thus
a single person may be the speech recognition ex-
pert in a project and have that role in all meetings
on that project. Additionally, the same person can
play multiple roles, e.g. the scribe can be a speech
recognition expert too.
Automatic role detection has many benefits, espe-
23
cially when used as a source of constraint for other
meeting understanding components. For example,
detecting the facilitator of the meeting might help
the automatic topic detection module if we know
that facilitators officially change topics and move the
discussion from one agenda item to the next. Know-
ing who the speech recognition expert is can help
the automatic action item detector: If an action item
regarding speech recognition has been detected but
the responsible person field has not been detected,
the module may place a higher probability on the
speech recognition expert as being the responsible
person for that action item. Additionally, detecting
who is an expert in which field can have benefits of
its own. For example, it can be used to automatically
direct queries on a particular subject to the person
deemed most qualified to answer the question, etc.
Basic information such as participant role and ex-
pertise needs to be robustly extracted if it is to be of
use to the more sophisticated stages of understand-
ing. Accordingly, we have based our role detection
algorithm on simple and highly accurate speech fea-
tures, as described in section 5.1.2.
(Banerjee and Rudnicky, 2004) describes the au-
tomatic detection of discourse roles in meetings.
These roles included presenter (participants who
make formal presentations using either slides or
the whiteboard), discussion participant (participants
involved in a discussion marked by frequent turn
changes), observer (participants not speaking, but
nevertheless consuming information during a pre-
sentation or discussion), etc. In this paper we focus
on automatically detecting the functional and exper-
tise based roles that participants play in a meeting.
In the next section we describe the data that is used
in all our role detection work in this paper. In subse-
quent sections we describe the role detection algo-
rithm in more detail, and present evaluation results.
2 The Y2 Meeting Scenario Data
Our research work is part of the Cognitive Assistant
that Learns and Organizes project (CALO, 2003). A
goal of this project is to create an artificial assis-
tant that can understand meetings and use this un-
derstanding to assist meeting participants during and
after meetings. Towards this goal, data is being col-
lected by creating a rich multimodal record of meet-
ings (e.g. (Banerjee et al, 2004)). While a large
part of this data consists of natural meetings (that
would have taken place even if they weren?t being
recorded), a small subset of this data is ?scenario
driven? ? the Y2 Scenario Data.
Meeting # Typical scenario
1 Hiring Joe: Buy a computer and
find office space for him
2 Hiring Cindy and Fred: Buy com?
puters & find office space for them
3 Buy printer for Joe, Cindy and Fred
4 Buy a server machine for Joe,
Cindy and Fred
5 Buy desktop and printer for the
meeting leader
Table 1: Typical Scenario Instructions
The Y2 Scenario Data consists of meetings be-
tween groups of 3 or 4 participants. Each group par-
ticipated in a sequence of up to 5 meetings. Each
sequence had an overall scenario ? the purchasing
of computing hardware and the allocation of office
space for three newly hired employees. Participants
were told to assume that the meetings in the se-
quence were being held one week apart, and that be-
tween any two meetings ?progress? was made on the
action items decided at each meeting. Participants
were given latitude to come up with their own sto-
ries of what ?progress? was made between meetings.
At each meeting, participants were asked to review
progress since the last meeting and make changes to
their decisions if necessary. Additionally, an extra
topic was introduced at each meeting, as shown in
table 1.
In each group of participants, one participant
played the role of the manager who has control over
the funds and makes the final decisions on the pur-
chases. The remaining 2 or 3 participants played the
roles of either the hardware acquisition expert or the
building facilities expert. The role of the hardware
expert was to make recommendations on the buying
of computers and printers, and to actually make the
purchases once a decision was made to do so. Sim-
ilarly the role of the building expert was to make
recommendations on which rooms were available to
fit the new employees into. Despite this role assign-
24
ment, all participants were expected to contribute to
discussions on all topics.
To make the meetings as natural as possible, the
participants were given control over the evolution of
the story, and were also encouraged to create con-
flicts between the manager?s demands and the advice
that the experts gave him. For example, managers
sometimes requested that all three employees be put
in a single office, but the facilities expert announced
that no 3 person room was available, unless the man-
ager was agreeable to pay extra for them. These
conflicts led to extended negotiations between the
participants. To promote fluency, participants were
instructed to use their knowledge of existing facili-
ties and equipment instead of inventing a completely
fictitious set of details (such as room numbers).
The data we use in this paper consists of 8 se-
quences recorded at Carnegie Mellon University and
at SRI International between 2004 and 2005. One of
these sequences has 4 meetings, the remaining have
5 meetings each, for a total of 39 meetings. 4 of
these sequences had a total of 3 participants each;
the remaining 4 sequences had a total of 4 partici-
pants each. On average each meeting was 15 min-
utes long. We partitioned this data into two roughly
equal sets, the training set containing 4 meeting se-
quences, and the test set containing the remaining
4 sets. Although a few participants participated in
multiple meetings, there was no overlap of partici-
pants between the training and the test set.
3 Functional Roles
Meeting participants have functional roles that en-
sure the smooth conduct of the meeting, with-
out regard to the specific contents of the meeting.
These roles may include that of the meeting leader
whose functions typically include starting the meet-
ing, establishing the agenda (perhaps in consulta-
tion with the other participants), making sure the
discussions remain on?agenda, moving the discus-
sion from agenda item to agenda item, etc. Another
possible functional role is that of a the designated
meeting scribe. Such a person may be tasked with
the job of taking the official notes or minutes for the
meeting.
Currently we are attempting to automatically de-
tect the meeting leader for a given meeting. In our
data (as described in section 2) the participant play-
ing the role of the manager is always the meeting
leader. In section 5 we describe our methodology
for automatically detecting the meeting leader.
4 Expertise
Typically each participant in a meeting makes con-
tributions to the discussions at the meeting (and to
the project or organization in general) based on their
own expertise or skill set. For example, a project
to build a multi?modal note taking application may
include project members with expertise in speech
recognition, in video analysis, etc. We define ex-
pertise based roles as roles based on skills that are
relevant to participants? contributions to the meeting
discussions and the project or organization in gen-
eral. Note that the expertise role a participant plays
in a meeting is potentially dependent on the exper-
tise roles of the other participants in the meeting,
and that a single person may play different expertise
roles in different meetings, or even within a single
meeting. For example, a single person may be the
?speech recognition expert? on the note taking appli-
cation project that simply uses off?the?shelf speech
recognition tools to perform note taking, but a ?noise
cancellation? expert on the project that is attempting
to improve the in?house speech recognizer. Auto-
matically detecting each participant?s roles can help
such meeting understanding components as the ac-
tion item detector.
Ideally we would like to automatically discover
the roles that each participant plays, and cluster
these roles into groups of similar roles so that
the meeting understanding components can transfer
what they learn about particular participants to other
(and newer) participants with similar roles. Such a
role detection mechanism would need no prior train-
ing data about the specific roles that participants
play in a new organization or project. Currently
however, we have started with a simplified partici-
pant role detection task where we do have training
data pertinent to the specific roles that meeting par-
ticipants play in the test set of meetings. As men-
tioned in section 2, our data consists of people play-
ing two kinds of expertise?based roles ? that of a
hardware acquisition expert, and that of a building
facilities expert. In the next section we discuss our
25
methodology of automatically detecting these roles
from the meeting participants? speech.
5 Methodology
Given a sequence of longitudinal meetings, we de-
fine our role detection task as a three?way classi-
fication problem, where the input to the classifier
consists of features extracted from the speech of a
particular participant over the given meetings, and
the output is a probability distribution over the three
possible roles. Note that although a single par-
ticipant can simultaneously play both a functional
and an expertise?based role, in the Y2 Scenario
Data each participant plays exactly one of the three
roles. We take advantage of this situation to simplify
the problem to the three way classification defined
above. We induce a decision tree (Quinlan, 1986)
classifier from hand labeled data. In the next sub-
section we describe the steps involved in training the
decision tree role classifier, and in the subsequent
subsection we describe how the trained decision tree
is used to arrive at a role label for each meeting par-
ticipant.
5.1 Training
5.1.1 Keyword List Creation
One of the sources of information that we wish
to employ to perform functional and expertise role
detection is the words that are spoken by each par-
ticipant over the course of the meetings. Our ap-
proach to harness this information source is to use
labeled training data to first create a set of words
most strongly associated with each of the three roles,
and then use only these words during the feature ex-
traction phase to detect each participant?s role, as de-
scribed in section 5.1.2.
We created this list of keywords as follows. Given
a training set of meeting sequences, we aggregated
for each role all the speech from all the participants
who had played that role in the training set. We then
split this data into individual words and removed
stop words ? closed class words (mainly articles and
prepositions) that typically contain less information
pertinent to the task than do nouns and verbs. For all
words across all the three roles, we computed the de-
gree of association between each word and each of
the three roles, using the chi squared method (Yang
and Pedersen, 1997), and chose the top 200 high
scoring word?role pairs. Finally we manually exam-
ined this list of words, and removed additional words
that we deemed to not be relevant to the task (essen-
tially identifying a domain?specific stop list). This
reduced the list to a total of 180 words. The 5 most
frequently occurring words in this list are: computer,
right, need, week and space. Intuitively the goal of
this keyword selection pre?processing step is to save
the decision tree role classifier from having to auto-
matically detect the important words from a much
larger set of words, which would require more data
to train.
5.1.2 Feature Extraction
The input to the decision tree role classifier is a set
of features abstracted from a specific participant?s
speech. One strategy is to extract exactly one set of
features from all the speech belonging to a partici-
pant across all the meetings in the meeting sequence.
However, this approach requires a very large num-
ber of meetings to train. Our chosen strategy is to
sample the speech output by each participant multi-
ple times over the course of the meeting sequence,
classify each such sample, and then aggregate the
evidence over all the samples to arrive at the overall
likelihood that a participant is playing a certain role.
To perform the sampling, we split each meeting
in the meeting sequence into a sequence of contigu-
ous windows each n seconds long, and then compute
one set of features from each participant?s speech
during each window. The value of n is decided
through parametric tests (described in section 7.1).
If a particular participant was silent during the en-
tire duration of a particular window, then features
are extracted from that silence.
Note that in the above formulation, there is no
overlap (nor gap) between successive windows. In
a separate set of experiments we used overlapping
windows. That is, given a window size, we moved
the window by a fixed step size (less than the size
of the window) and computed features from each
such overlapping window. The results of these
experiments were no better than those with non?
overlapping windows, and so for the rest of this pa-
per we simply report on the results with the non?
overlapping windows.
Given a particular window of speech of a partic-
26
ular participant, we extract the following 2 speech
length based features:
? Rank of this participant (among this meet-
ing?s participants) in terms of the length of his
speech during this window. Thus, if this partic-
ipant spoke the longest during the window, he
has a feature value of 1, if he spoke for the sec-
ond longest number of times, he has a feature
value of 2, etc.
? Ratio of the length of speech of this participant
in this window to the total length of speech
from all participants in this window. Thus if
a participant spoke for 3 seconds, and the to-
tal length of speech from all participants in
this window was 6 seconds, his feature value
is 0.5. Together with the rank feature above,
these two features capture the amount of speech
contributed by each participant to the window,
relative to the other participants.
In addition, for each window of speech of a par-
ticular participant, and for each keyword in our list
of pre?decided keywords, we extract the following
2 features:
? Rank of this participant (among this meeting?s
participants) in terms of the number of times
this keyword was spoken. Thus if in this win-
dow of time, this participant spoke the keyword
printer more often than any of the other partic-
ipants, then his feature value for this keyword
is 1.
? Ratio of the number of times this participant
uttered this keyword in this window to the total
number of times this keyword was uttered by
all the participants during this window. Thus
if a participant spoke the word printer 5 times
in this window, and in total all participants said
the word printer 7 times, then his feature value
for this keyword is 5/7. Together with the key-
word rank feature above, these two features
capture the number of times each participant
utters each keyword, relative to the other par-
ticipants.
Thus for each participant, for each meeting win-
dow, we extract two features based on the lengths
of speech, and 2 ? 180 features for each of the 180
keywords, for a total of 362 features. The true output
label for each such data point is the role of that par-
ticipant in the meeting sequence. We used these data
points to induce a classifier using the Weka Java im-
plementation (Witten and Frank, 2000) of the C4.5
decision tree learning algorithm (Quinlan, 1986).
This classifier takes features as described above as
input, and outputs class membership probabilities,
where the classes are the three roles. Note that for
the experiments in this paper we extract these fea-
tures from the manual transcriptions of the speech
of the meeting participants. In the future we plan to
perform these experiments using the transcriptions
output by an automatic speech recognizer.
5.2 Detecting Roles in Unseen Data
5.2.1 Classifying Windows of Unseen Data
Detecting the roles of meeting participants in un-
seen data is performed as follows: First the unseen
test data is split into windows of the same size as was
used during the training regime. Then the speech ac-
tivity and keywords based features are extracted (us-
ing the same keywords as was used during the train-
ing) for each participant in each window. Finally
these data points are used as input into the trained
decision tree, which outputs class membership prob-
abilities for each participant in each window.
5.2.2 Aggregating Evidence to Assign One Role
Per Participant
Thus for each participant we get as many proba-
bility distributions (over the three roles) as there are
windows in the test data. The next step is to aggre-
gate these probabilities over all the windows and ar-
rive at a single role assignment per participant. We
employ the simplest possible aggregation method:
We compute, for each participant, the average prob-
ability of each role over all the windows, and then
normalize the three average role probabilities so cal-
culated, so they still sum to 1. In the future we plan
to experiment with more sophisticated aggregation
mechanisms that jointly optimize the probabilities of
the different participants, instead of computing them
independently.
At this point, we could assign to each participant
his highest probability role. However, we wish to
ensure that the set of roles that get assigned to the
27
participants in a particular meeting are as diverse
as possible (since typically meetings are forums at
which different people of different expertise con-
vene to exchange information). To ensure such di-
versity, we apply the following heuristic. Once we
have all the average probabilities for all the roles for
each participant in a sequence of meetings, we as-
sign roles to participants in stages. At each stage
we consider all participants not yet assigned roles,
and pick that participant?role pair, say (p, r), that
has the highest probability value among all pairs un-
der consideration. We assign participant p the role r,
and then discount (by a constant multiplicative fac-
tor) the probability value of all participant?role pairs
(pi, rj) where pi is a participant not assigned a role
yet, and rj = r. This makes it less likely (but not
impossible) that another participant will be assigned
this same role r again. This process is repeated until
all participants have been assigned a role each.
6 Evaluation
We evaluated the algorithm by computing the accu-
racy of the detector?s role predictions. Specifically,
given a meeting sequence we ran the algorithm to
assign a role to each meeting participant, and com-
puted the accuracy by calculating the ratio of the
number of correct assignments to the total number
of participants in the sequence. Note that it is also
possible to evaluate the window?by?window clas-
sification of the decision tree classifiers; we report
results on this evaluation in section 7.1.
To evaluate this participant role detection algo-
rithm, we first trained the algorithm on the training
set of meetings. The training phase included key-
word list creation, window size optimization, and
the actual induction of the decision tree. On the
training data, a window size of 300 seconds resulted
in the highest accuracy over the training set. The test
at the root of the induced tree was whether the par-
ticipant?s rank in terms of speech lengths was 1, in
which case he was immediately classified as a meet-
ing leader. That is, the tree learnt that the person
who spoke the most in a window was most likely
the meeting leader. Other tests placed high in the
tree included obvious ones such as testing for the
keywords computer and printer to classify a partici-
pant as a hardware expert.
We then tested this trained role detector on the
testing set of meetings. Recall that the test set had
5 meeting sequences, each consisting of 5 meetings
and a total of 20 meeting participants. Over this test
set we obtained a role detection accuracy of 83%.
A ?classifier? that randomly assigns one of the three
roles to each participant in a meeting (without re-
gard to the roles assigned to the other participants in
the same meeting) would achieve a classification ac-
curacy of 33.3%. Thus, our algorithm significantly
beats the random classifier baseline. Note that as
mentioned earlier, the experiments in this paper are
based on the manually transcribed speech.
7 Further Experiments
7.1 Optimizing the Window Size
As mentioned above, one of the variables to be tuned
during the training phase is the size of the window
over which to extract speech features. We ran a se-
quence of experiments to optimize this window size,
the results of which are summarized in figure 1. In
this set of experiments, we performed the evaluation
on two levels of granularity. The larger granularity
level was the ?meeting sequence? granularity, where
we ran the usual evaluation described above. That
is, for each participant we first used the classifier to
obtain probability distributions over the 3 roles on
every window, and then aggregated these distribu-
tions to reach a single role assignment for the par-
ticipant over the entire meeting sequence. This role
was compared to the true role of the participant to
measure the accuracy of the algorithm. The smaller
granularity level was the ?window? level, where af-
ter obtaining the probability distribution over the
three roles for a particular window of a particu-
lar participant, we picked the role with the high-
est probability, and assigned it to the participant for
that window. Therefore, for each window we had
a role assignment that we compared to the true role
of the participant, resulting in an accuracy value for
the classifier for every window for every participant.
Note that the main difference between evaluation at
these two granularity levels is that in the ?window?
granularity, we did not have any aggregation of evi-
dence across multiple windows.
For different window sizes, we plotted the accu-
racy values obtained on the test set for the two evalu-
28
 40
 50
 60
 70
 80
 90
 100
 0  100  200  300  400  500  600  700  800  900  1000
Ac
cu
ra
cy
Window Size
Accuracy meeting sequence
Accuracy window
Figure 1: Effect of Different Window Sizes on Detection Ac-
curacy
ation granularities, as shown in figure 1. Notice that
by aggregating the evidence across the windows, the
detection accuracy improves for all window sizes.
This is to be expected since in the window gran-
ularity, the classifier has access to only the infor-
mation contained in a single window, and is there-
fore more error prone. However by merging the ev-
idence from many windows, the accuracy improves.
As window sizes increase, detection accuracy at the
window level improves, because the classifier has
more evidence at its disposal to make the decision.
However, detection at the meeting sequence level
gets steadily worse, potentially because the larger
the window size, the fewer the data points it has to
aggregate evidence from. These lines will eventu-
ally meet when the window size equals the size of
the entire meeting sequence.
A valid concern with these results is the high level
of noise, particularly in the aggregated detection ac-
curacy over the meeting sequence. One reason for
this is that there are far fewer data points at the meet-
ing sequence level than at the window level. With
larger data sets (more meeting sequences as well as
more participants per meeting) these results may sta-
bilize. Additionally, given the small amount of data,
our feature set is quite large, so a more aggressive
feature set reduction might help stabilize the results.
7.2 Automatic Improvement over Unseen Data
One of our goals is to create an expertise based role
detector system that improves over time as it has ac-
cess to more and more meetings for a given par-
 40
 50
 60
 70
 80
 90
 100
 1  2  3  4  5
Ac
cu
ra
cy
Window Size
Accuracy
Figure 2: Accuracy versus Number of Meetings over which
Roles were Detected
ticipant. This is especially important because the
roles that a participant plays can change over time;
we would like our system to be able to track these
changes. In the Y2 Scenario Data that we have used
in this current work, the roles do not change from
meeting to meeting. However observe that our evi-
dence aggregation algorithm fuses information from
all the meetings in a specific sequence of meetings
to arrive at a single role assignment for each partici-
pant.
To quantify the effect of this aggregation we com-
puted the role detection accuracy using different
numbers of meetings from each sequence. Specif-
ically, we computed the accuracy of the role detec-
tion over the test data using only the last meeting of
each sequence, only the last 2 meetings of each se-
quence, and so on until we used every meeting in ev-
ery sequence. The results are summarized in figure
2. When using only the last meeting in the sequence
to assign roles to the participants, the accuracy is
only 66.7%, when using the last two meetings, the
accuracy is 75%, and using the last three, four or
all meetings results in an accuracy of 83%. Thus,
the accuracy improves as we have more meetings to
combine evidence from, as is expected. However
the accuracy levels off at 83% when using three or
more meetings, perhaps because there is no new in-
formation to be gained by adding a fourth or a fifth
meeting.
8 Conclusions and Future Work
In this paper we have discussed our current approach
to detecting the functional and expertise based roles
of meeting participants. We have induced decision
29
trees that use simple and robust speech based fea-
tures to perform the role detection. We have used
a very simple evidence aggregation mechanism to
arrive at a single role assignment per meeting partic-
ipant over a sequence of meetings, and have shown
that we can achieve up to 83% accuracy on unseen
test data using this mechanism. Additionally we
have shown that by aggregating evidence across a
sequence of meetings, we perform better than if we
were to use a single meeting to perform the role de-
tection. As future work we plan to remove the con-
straints that we have currently imposed ? namely, we
will attempt to learn new roles in test data that do not
exist in training data. Additionally, we will attempt
to use this role information as inputs to downstream
meeting understanding tasks such as automatic topic
detection and action item detection.
9 Acknowledgements
This work was supported by DARPA grant NBCH-
D-03-0010. The content of the information in this
publication does not necessarily reflect the position
or the policy of the US Government, and no official
endorsement should be inferred.
References
S. Banerjee and A. I. Rudnicky. 2004. Using simple
speech-based features to detect the state of a meet-
ing and the roles of the meeting participants. In Pro-
ceedings of the 8th International Conference on Spo-
ken Language Processing (Interspeech 2004 ? ICSLP),
Jeju Island, Korea.
S. Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Pato-
dia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso,
A. Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky.
2004. Creating multi-modal, user?centric records of
meetings with the Carnegie Mellon meeting recorder
architecture. In Proceedings of the ICASSP Meeting
Recognition Workshop, Montreal, Canada.
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic?level annotations to meeting
browsing. In Proceedings of the Tenth International
Conference on Human-Computer Interaction, Rome,
Italy, September.
CALO. 2003. http://www.ai.sri.com/project/CALO.
M. Galley, K. McKeown, E. Fosler-Lussier, and Hongyan
Jing. 2003. Discourse segmentation of multi?party
conversation. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics, vol-
ume 1, pages 562 ? 569, Sapporo, Japan.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, and S. Renals. 2005. Transcrip-
tion of conference room meetings: An investigation.
In Proceedings of Interspeech 2005, Lisbon, Portugal,
September.
F. Metze, Q. Jin, C. Fugen, K. Laskowski, Y. Pan, and
T. Schultz. 2004. Issues in meeting transcription ?
the isl meeting transcription system. In Proceedings of
the 8th International Conference on Spoken Language
Processing (Interspeech 2004 ? ICSLP), Jeju Island,
Korea.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. In Proceedings
of Interspeech 2005, Lisbon, Portugal, September.
J. Quinlan. 1986. Induction of decision trees. Machine
Learning, 1:81?106.
Paul E. Rybski and Manuela M. Veloso. 2004. Using
sparse visual data to model human activities in meet-
ings. In Workshop on Modeling Other Agents from
Observations, International Joint Conference on Au-
tonomous Agents and Multi-Agent Systems.
A. Stolcke, C. Wooters, N. Mirghafori, T. Pirinen, I. Bu-
lyko, D. Gelbart, M. Graciarena, S. Otterson, B. Pe-
skin, and M. Ostendorf. 2004. Progress in meeting
recognition: The icsi?sri?uw spring 2004 evaluation
system. In NIST RT04 Meeting Recognition Work-
shop, Montreal.
I. Witten and E. Frank. 2000. Data Mining - Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan?Kaufmann, San Francisco,
CA.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proceedings
of the International Conference on Machine Learn-
ing, pages 412?420, Nashville, US. Morgan Kauf-
mann Publishers.
30
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 32?39,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Olympus: an open-source framework 
for conversational spoken language interface research 
 
 
Dan Bohus, Antoine Raux, Thomas K. Harris, 
Maxine Eskenazi, Alexander I. Rudnicky 
School of Computer Science 
Carnegie Mellon University 
{dbohus, antoine, tkharris, max, air}@cs.cmu.edu 
 
 
 
 
Abstract 
We introduce Olympus, a freely available 
framework for research in conversational 
interfaces. Olympus? open, transparent, 
flexible, modular and scalable nature fa-
cilitates the development of large-scale, 
real-world systems, and enables research 
leading to technological and scientific ad-
vances in conversational spoken language 
interfaces. In this paper, we describe the 
overall architecture, several systems 
spanning different domains, and a number 
of current research efforts supported by 
Olympus.  
1 Introduction  
Spoken language interfaces developed in industrial 
and academic settings differ in terms of goals, the 
types of tasks and research questions addressed, 
and the kinds of resources available.  
In order to be economically viable, most indus-
try groups need to develop real-world applications 
that serve large and varied customer populations. 
As a result, they gain insight into the research 
questions that are truly significant for current-
generation technologies. When needed, they are 
able to focus large resources (typically unavailable 
in academia) on addressing these questions. To 
protect their investments, companies do not gener-
ally disseminate new technologies and results. 
In contrast, academia pursues long-term scien-
tific research goals, which are not tied to immedi-
ate economic returns or customer populations. As a 
result, academic groups are free to explore a larger 
variety of research questions, even with a high risk 
of failure or a lack of immediate payoff. Academic 
groups also engage in a more open exchange of 
ideas and results. However, building spoken lan-
guage interfaces requires significant investments 
that are sometimes beyond the reach of academic 
researchers. As a consequence, research in acade-
mia is oftentimes conducted with toy systems and 
skewed user populations. In turn, this raises ques-
tions about the validity of the results and hinders 
the research impact.  
In an effort to address this problem and facilitate 
research on relevant, real-world questions, we have 
developed Olympus, a freely available framework 
for building and studying conversational spoken 
language interfaces. The Olympus architecture, 
described in Section 3, has its roots in the CMU 
Communicator project (Rudnicky et al, 1999). 
Based on that experience and subsequent projects, 
we have engineered Olympus into an open, trans-
parent, flexible, modular, and scalable architecture. 
To date, Olympus has been used to develop and 
deploy a number of spoken language interfaces 
spanning different domains and interaction types; 
these systems are presented in Section 4. They are 
currently supporting research on diverse aspects of 
spoken language interaction. Section 5 discusses 
three such efforts: error handling, multi-participant 
conversation, and turn-taking. 
We believe that Olympus and other similar tool-
kits, discussed in Section 6, are essential in order 
to bridge the gap between industry and academia. 
Such frameworks lower the cost of entry for re-
32
search on practical conversational interfaces. They 
also promote technology transfer through the reuse 
of components, and support direct comparisons 
between systems and technologies.  
2 Desired characteristics 
While developing Olympus, we identified a num-
ber of characteristics that in our opinion are neces-
sary to effectively support and foster research. The 
framework should be open, transparent, flexible, 
modular, and scalable.  
Open. Complete source code should be avail-
able for all the components so that researchers and 
engineers can inspect and modify it towards their 
ends. Ideally, source code should be free for both 
research and commercial purposes and grow 
through contributions from the user community. 
Transparent / Analytic. Open source code 
promotes transparency, but beyond that researchers 
must be able to analyze the system?s behavior. To 
this end, every component should provide detailed 
accounts of their internal state. Furthermore, tools 
for data visualization and analysis should be an 
integral part of the framework. 
Flexible. The framework should be able to ac-
commodate a wide range of applications and re-
search interests, and allow easy integration of new 
technologies. 
Modular / Reusable. Specific functions (e.g. 
speech recognition, parsing) should be encapsu-
lated in components with rich and well-defined 
interfaces, and an application-independent design. 
This will promote reusability, and will lessen the 
system development effort.  
Scalable. While frameworks that rely on sim-
ple, well established approaches (e.g. finite-state 
dialogs in VoiceXML) allow the development of 
large-scale systems, this is usually not the case for 
frameworks that provide the flexibility and trans-
parency needed for research. However, some re-
search questions are not apparent until one moves 
from toy systems into large-scale applications. The 
framework should strive to not compromise scal-
ability for the sake of flexibility or transparency. 
3 Architecture  
At the high level, a typical Olympus application 
consists of a series of components connected in a 
classical, pipeline architecture, as illustrated by the 
bold components in Figure 1. The audio signal for 
the user utterance is captured and passed through a 
speech recognition module that produces a recog-
nition hypothesis (e.g., two p.m.). The recognition 
hypothesis is then forwarded to a language under-
standing component that extracts the relevant con-
cepts (e.g., [time=2p.m.]), and then through a 
confidence annotation module that assigns a confi-
dence score. Next, a dialog manager integrates this 
semantic input into the current context, and pro-
duces the next action to be taken by the system in 
the form of the semantic output (e.g., {request 
end_time}). A language generation module pro-
duces the corresponding surface form, which is 
subsequently passed to a speech synthesis module 
and rendered as audio.  
Galaxy communication infrastructure. While 
the pipeline shown in bold in Figure 1 captures the 
logical flow of information in the system, in prac-
tice the system components communicate via a 
centralized message-passing infrastructure ? Gal-
axy (Seneff et al, 1998). Each component is im-
plemented as a separate process that connects to a 
traffic router ? the Galaxy hub. The messages are 
sent through the hub, which forwards them to the 
appropriate destination. The routing logic is de-
scribed via a configuration script. 
Speech recognition. Olympus uses the Sphinx 
decoding engine (Huang et al, 1992). A recogni-
tion server captures the audio stream, forwards it to 
a set of parallel recognition engines, and collects 
the corresponding recognition results. The set of 
best hypotheses (one from each engine) is then 
forwarded to the language understanding compo-
nent. The recognition engines can also generate n-
best lists, but that process significantly slows down 
the systems and has not been used live. Interfaces 
to connect Sphinx-II and Sphinx-III engines, as 
well as a DTMF (touch-tone) decoder to the recog-
nition server are currently available. The individual 
recognition engines can use either n-gram- or 
grammar-based language models. Dialog-state 
specific as well as class-based language models are 
supported, and tools for constructing language and 
acoustic models from data are readily available. 
Most of the Olympus systems described in the next 
section use two gender-specific Sphinx-II recog-
nizers in parallel. Other parallel decoder configura-
tions can also be created and used.  
Language understanding is performed by 
Phoenix, a robust semantic parser (Ward and Issar, 
33
1994). Phoenix uses a semantic grammar to parse 
the incoming set of recognition hypotheses. This 
grammar is assembled by concatenating a set of 
reusable grammar rules that capture domain-
independent constructs like [Yes], [No], [Help], 
[Repeat], and [Number], with a set of domain-
specific grammar rules authored by the system de-
veloper. For each recognition hypothesis the output 
of the parser consists of a sequence of slots con-
taining the concepts extracted from the utterance.  
Confidence annotation. From Phoenix, the set 
of parsed hypotheses is passed to Helios, the con-
fidence annotation component. Helios uses features 
from different knowledge sources in the system 
(e.g., recognition, understanding, dialog) to com-
pute a confidence score for each hypothesis. This 
score reflects the probability of correct understand-
ing, i.e. how much the system trusts that the cur-
rent semantic interpretation corresponds to the 
user?s intention. The hypothesis with the highest 
score is forwarded to the dialog manager.  
Dialog management. Olympus uses the Raven-
Claw dialog management framework (Bohus and 
Rudnicky, 2003). In a RavenClaw-based dialog 
manager, the domain-specific dialog task is repre-
sented as a tree whose internal nodes capture the 
hierarchical structure of the dialog, and whose 
leaves encapsulate atomic dialog actions (e.g., ask-
ing a question, providing an answer, accessing a 
database). A domain-independent dialog engine 
executes this dialog task, interprets the input in the 
current dialog context and decides which action to 
engage next. In the process, the dialog manager 
may exchange information with other domain-
specific agents (e.g., application back-end, data-
base access, temporal reference resolution agent). 
Language generation. The semantic output of 
the dialog manager is sent to the Rosetta template-
based language generation component, which pro-
duces the corresponding surface form. Like the 
Phoenix grammar, the language generation tem-
plates are assembled by concatenating a set of pre-
defined, domain-independent templates, with 
manually authored task-specific templates.  
Speech synthesis. The prompts are synthesized 
by the Kalliope speech synthesis module. Kalliope 
can be currently configured to use Festival (Black 
and Lenzo, 2000), which is an open-source speech 
synthesis system, or Cepstral Swift (Cepstral 
2005), a commercial engine. Finally, Kalliope also 
supports the SSML markup language.  
Other components. The various components 
briefly described above form the core of the Olym-
pus dialog system framework. Additional compo-
nents have been created throughout the 
development of various systems, and, given the 
modularity of the architecture, can be easily re-
used. These include a telephony component, a text 
Parsing 
PHOENIX 
Recognition 
Server 
Lang. Gen 
ROSETTA 
Synthesis 
KALLIOPE 
? 
SPHINX SPHINX 
SPHINX 
Confidence 
HELIOS 
HUB 
Text I/O 
TTYSERVER 
Application 
Back-end 
Dialog. Mgr. 
RAVENCLAW 
Date-Time 
resolution 
Process 
Monitor 
Until what time 
would you like  
the room? 
{request end_time} 
Figure 1. The Olympus dialog system reference architecture (a typical system) 
two p.m. [time=2pm] [time=2pm]/0.65 
34
input-and-output interface, and a temporal refer-
ence resolution agent that translates complex date-
time expressions (including relative references, 
holidays, etc.) into a canonical form. Recently, a 
Jabber interface was implemented to support inter-
actions via the popular GoogleTalk internet mes-
saging system. A Skype speech client component 
is also available.  
Data Analysis. Last but not least, a variety of 
tools for logging, data processing and data ana-
lytics are also available as part of the framework. 
These tools have been used for a wide variety of 
tasks ranging from system monitoring, to trends 
analysis, to training of internal models. 
A key characteristic shared by all the Olympus 
components is the clear separation between do-
main-independent programs and domain-specific 
resources. This decoupling promotes reuse and 
lessens the system development effort. To build a 
new system, one can focus simply on developing 
resources (e.g., language model, grammar, dialog 
task specification, generation templates) without 
having to do any programming. On the other hand, 
since all components are open-source, any part of 
the system can be modified, for example to test 
new algorithms or compare approaches. 
4 Systems 
To date, the Olympus framework has been used to 
successfully build and deploy several spoken dia-
log systems spanning different domains and inter-
action types (see Table 1). Given the limited space, 
we discuss only three of these systems in a bit 
more detail: Let?s Go!, LARRI, and TeamTalk. 
More information about the other systems can be 
found in (RavenClaw-Olympus, 2007). 
4.1 Let?s Go! 
The Let?s Go! Bus Information System (Raux et al
2005; 2006) is a telephone-based spoken dialog 
system that provides access to bus schedules. In-
teraction with the system starts with an open 
prompt, followed by a system-directed phase 
where the user is asked the missing information. 
Each of the three or four pieces of information 
provided (origin, destination, time of travel, and 
optional bus route) is explicitly confirmed. The 
system knows 12 bus routes, and about 1800 place 
names. 
Originally developed as an in-lab research sys-
tem, Let?s Go! has been open to the general public 
since March, 2005. Outside of business hours, calls 
to the bus company are transferred to Let?s Go!, 
providing a constant flow of genuine dialogs 
(about 40 calls per weeknight and 70 per weekend 
night). As of March, 2007, a corpus of about 
30,000 calls to the system has been collected and 
partially transcribed and annotated. In itself, this 
publicly available corpus constitutes a unique re-
source for the community. In addition, the system 
itself has been modified for research experiments 
(e.g., Raux et al, 2005, Bohus et al, 2006). Be-
tween-system studies have been conducted by run-
ning several versions of the system in parallel and 
picking one at random for every call. We have re-
cently opened this system to researchers from other 
groups who wish to conduct their own experi-
ments. 
4.2 LARRI 
LARRI (Bohus and Rudnicky, 2002a) is a multi-
modal system for support of maintenance and re-
pair activities for F/A-18 aircraft mechanics. The 
system implements an Interactive Electronic Tech-
nical Manual.  
LARRI integrates a graphical user interface for 
easy visualization of dense technical information 
(e.g., instructions, schematics, video-streams) with 
a spoken dialog system that facilitates information 
access and offers assistance throughout the execu-
tion of procedural tasks. The GUI is accessible via 
a translucent head-worn display connected to a 
wearable client computer. A rotary mouse (dial) 
provides direct access to the GUI elements.  
After an initial log-in phase, LARRI guides the 
user through the selected task, which consists of a 
sequence of steps containing instructions, option-
ally followed by verification questions. Basic steps 
can include animations or short video sequences 
that can be accessed by the user through the GUI 
or through spoken commands. The user can also 
take the initiative and access the documentation, 
either via the GUI or by simple commands such as 
?go to step 15? or ?show me the figure?. 
The Olympus architecture was easily adapted 
for this mobile and multi-modal setting. The wear-
able computer hosts audio input and output clients, 
as well as the graphical user interface. The Galaxy 
hub architecture allows us to easily connect these 
35
components to the rest of the system, which runs 
on a separate server computer. The rotary-mouse 
events from the GUI are rendered as semantic in-
puts and are sent to Helios which in turn forwards 
the corresponding messages to the dialog manager.  
4.3 TeamTalk 
TeamTalk (Harris et al, 2005) is a multi-modal 
interface that facilitates communication between a 
human operator and a team of heterogeneous ro-
bots, and is designed for a multi-robot-assisted 
treasure-hunt domain. The human operator uses 
spoken language in concert with pen-gestures on a 
shared live map to elicit support from teams of ro-
bots. This support comes in the forms of mapping 
unexplored areas, searching explored areas for ob-
jects of interest, and leading the human to said ob-
jects. TeamTalk has been built as a fully functional 
interface to real robots, including the Pioneer 
P2DX and the Segway RMP. In addition, it can 
interface with virtual robots within the high-
fidelity USARSim (Balakirsky et al, 2006) simula-
tion environment. TeamTalk constitutes an excel-
lent platform for multi-agent dialog research. 
To build TeamTalk, we had to address two chal-
lenges to current architecture. The multi-
participant nature of the interaction required multi-
ple dialog managers; the live map with pen-
gestured references required a multi-modal integra-
tion. Again, the flexibility and transparency of the 
Olympus framework allowed for relatively simple 
solutions to both of these challenges. To accom-
modate multi-participant dialog, each robot in the 
domain is associated with its own RavenClaw-
based dialog manager, but all robots share the 
other Olympus components: speech recognition, 
language understanding, language generation and 
speech synthesis. To accommodate the live map 
GUI, a Galaxy server was built in Java that could 
send the user?s inputs to Helios and receive outputs 
from RavenClaw. 
5 Research 
The Olympus framework, along with the systems 
developed using it, provides a robust basis for re-
search in spoken language interfaces. In this sec-
tion, we briefly outline three current research 
efforts supported by this architecture. Information 
about other supported research can be found in 
(RavenClaw-Olympus, 2007). 
5.1 Error handling  
A persistent and important problem in today?s spo-
ken language interfaces is their lack of robustness 
when faced with understanding errors. This prob-
lem stems from current limitations in speech rec-
ognition, and appears across most domains and 
interaction types. In the last three years, we con-
ducted research aimed at improving robustness in 
spoken language interfaces by: (1) endowing them 
with the ability to accurately detect errors, (2) de-
System name Domain / Description Genre 
RoomLine 
(Bohus and Rudnicky 2005) 
telephone-based system that provides support for conference 
room reservation and scheduling within the School of Com-
puter Science at CMU. 
information access (mixed 
initiative) 
Let?s Go! Public 
(Raux et al2005) 
telephone-based system that provides access to bus schedule 
information in the greater Pittsburgh area. 
information access 
(system initiative) 
LARRI 
(Bohus and Rudnicky 2002) 
multi-modal system that provides assistance to F/A-18 aircraft 
personnel during maintenance tasks. 
multi-modal task guidance 
and procedure browsing 
Intelligent Procedure  
Assistant 
(Aist et al2002) 
early prototype for a multi-modal system aimed at providing 
guidance and support to the astronauts on the International 
Space Station during the execution of procedural tasks and 
checklists. 
multi-modal task guidance 
and procedure browsing 
TeamTalk 
(Harris et al2005) 
multi-participant spoken language command-and-control inter-
face for a team of robots in the treasure-hunt domain. 
multi-participant command-
and-control 
VERA telephone-based taskable agent that can be instructed to de-liver messages to a third party and make wake-up calls. 
voice mail / message deliv-
ery 
Madeleine text-based dialog system for medical diagnosis. diagnosis 
ConQuest 
(Bohus et al2007) 
telephone-based spoken dialog system that provides confer-
ence schedule information. 
information access 
(mixed-initiative) 
RavenCalendar 
(Stenchikova et al2007). 
multimodal dialog system for managing personal calendar 
information, such as meetings, classes, appointments and 
reminders (uses Google Calendar as a back-end)  
information access and 
scheduling 
 Table 1. Olympus-based spoken dialog systems (shaded cells indicate deployed systems) 
36
veloping a rich repertoire of error recovery strate-
gies and (3) developing scalable, data-driven ap-
proaches for building error recovery policies1. Two 
of the dialog systems from Table 1 (Let?s Go! and 
RoomLine) have provided a realistic experimental 
platform for investigating these issues and evaluat-
ing the proposed solutions.   
With respect to error detection, we have devel-
oped tools for learning confidence annotation 
models by integrating information from multiple 
knowledge sources in the system (Bohus and Rud-
nicky, 2002b). Additionally, Bohus and Rudnicky 
(2006) proposed a data-driven approach for con-
structing more accurate beliefs in spoken language 
interfaces by integrating information across multi-
ple turns in the conversation. Experiments with the 
RoomLine system showed that the proposed belief 
updating models led to significant improvements 
(equivalent with a 13.5% absolute reduction in 
WER) in both the effectiveness and the efficiency 
of the interaction.  
With respect to error recovery strategies, we 
have developed and evaluated a large set of strate-
gies for handling misunderstandings and non-
understandings (Bohus and Rudnicky, 2005). The 
strategies are implemented in a task-decoupled 
manner in the RavenClaw dialog management 
framework. 
Finally, in (Bohus et al, 2006) we have pro-
posed a novel online-learning based approach for 
building error recovery policies over a large set 
of non-understanding recovery strategies. An em-
pirical evaluation conducted in the context of the 
Let?s Go! system showed that the proposed ap-
proach led to a 12.5% increase in the non-
understanding recovery rate; this improvement was 
attained in a relatively short (10-day) time period.  
The models, tools and strategies developed 
throughout this research can and have been easily 
reused in other Olympus-based systems. 
5.2 Multi-participant conversation  
Conversational interfaces are generally built for 
one-on-one conversation. This has been a workable 
assumption for telephone-based systems, and a 
useful one for many single-purpose applications. 
However this assumption will soon become 
strained as a growing collection of always-
                                                          
1
 A policy specifies how the system should choose between 
different recovery strategies at runtime.  
available agents (e.g., personal trainers, pedestrian 
guides, or calendar systems) and embodied agents 
(e.g., appliances and robots) feature spoken lan-
guage interfaces. When there are multiple active 
agents that wish to engage in spoken dialog, new 
issues arise. On the input side, the agents need to 
be able to identify the addressee of any given user 
utterance. On the output side, the agents need to 
address the problem of channel contention, i.e., 
multiple participants speaking over each other. 
Two architectural solutions can be envisioned: 
(1) the agents share a single interface that under-
stands multi-agent requirements, or (2) each agent 
uses its own interface and handles multi-participant 
behavior. Agents that provide different services 
have different dialog requirements, and we believe 
this makes a centralized interface problematic. Fur-
thermore, the second solution better fits human 
communication behavior and therefore is likely to 
be more natural and habitable.  
TeamTalk is a conversational system that fol-
lows the second approach: each robot has its own 
dialog manager. Processed user inputs are sent to 
all dialog managers in the system; each dialog 
manager decides based on a simple algorithm 
(Harris et al, 2004) whether or not the current in-
put is addressed to it. If so, an action is taken. Oth-
erwise the input is ignored; it will be processed and 
responded to by another robot. On the output side, 
to address the channel contention problem, each 
RavenClaw output message is augmented with in-
formation about the identity of the robot that gen-
erated it. The shared synthesis component queues 
the messages and plays them back sequentially 
with the corresponding voice. 
We are currently looking into two additional 
challenges related to multi-participant dialog. We 
are interested in how to address groups and sub-
groups in addition to individuals of a group, and 
we are also interested in how to cope with multiple 
humans in addition to multiple agents. Some ex-
periments investigating solutions to both of these 
issues have been conducted. Analysis of the results 
and refinements of these methods are ongoing. 
5.3 Timing and turn-taking  
While a lot of research has focused on higher lev-
els of conversation such as natural language under-
standing and dialog planning, low-level inter-
actional phenomena such as turn-taking have not 
37
received as much attention. As a result, current 
systems either constrain the interaction to a rigid 
one-speaker-at-a-time style or expose themselves 
to interactional problems such as inappropriate 
delays, spurious interruptions, or turn over-taking 
(Raux et al, 2006). To a large extent, these issues 
stem from the fact that in common dialog architec-
tures, including Olympus, the dialog manager 
works asynchronously from the real world (i.e., 
utterances and actions that are planned are as-
sumed to be executed instantaneously). This means 
that user barge-ins and backchannels are often in-
terpreted in an incorrect context, which leads to 
confusion, unexpected user behavior and potential 
dialog breakdowns. Additionally, dialog systems? 
low-level interactional behavior is generally the 
result of ad-hoc rules encoded in different compo-
nents that are not precisely coordinated. 
In order to investigate and resolve these is-
sues, we are currently developing version 2 of the 
Olympus framework. In addition to all the compo-
nents described in this paper, Olympus 2 features 
an Interaction Manager which handles the precise 
timing of events perceived from the real world 
(e.g., user utterances) and of system actions (e.g., 
prompts). By providing an interface between the 
actual conversation and the asynchronous dialog 
manager, Olympus 2 allows a more reactive behav-
ior without sacrificing the powerful dialog man-
agement features offered by RavenClaw. Olympus 
2 is designed so that current Olympus-based sys-
tems can be upgraded with minimal effort.  
This novel architecture, initially deployed in 
the Let?s Go system, will enable research on turn-
taking and other low-level conversational phenom-
ena. Investigations within the context of other ex-
isting systems, such as LARRI and TeamTalk, will 
uncover novel challenges and research directions.  
6 Discussion and conclusion 
The primary goal of the Olympus framework is to 
enable research that leads to technological and sci-
entific advances in spoken language interfaces.  
Olympus is however by no means a singular ef-
fort. Several other toolkits for research and devel-
opment are available to the community. They 
differ on a number of dimensions, such as objec-
tives, scientific underpinnings, as well as techno-
logical and implementation aspects. Several 
toolkits, both commercial, e.g., TellMe, BeVocal, 
and academic, e.g., Ariadne (2007), SpeechBuilder 
(Glass et al, 2004), and the CSLU toolkit (Cole, 
1999), are used for rapid development. Some, e.g., 
CSLU and SpeechBuilder, have also been used for 
educational purposes. And yet others, such as 
Olympus, GALATEEA (Kawamoto et al, 2002) 
and DIPPER (Bos et al, 2003) are primarily used 
for research. Different toolkits rely on different 
theories and dialog representations: finite-state, 
slot-filling, plan-based, information state-update. 
Each toolkit balances tradeoffs between complex-
ity, ease-of-use, control, robustness, flexibility, etc. 
We believe the strengths of the Olympus 
framework lie not only in its current components, 
but also in its open, transparent, and flexible na-
ture. As we have seen in the previous sections, 
these characteristics have allowed us to develop 
and deploy practical, real-world systems operating 
in a broad spectrum of domains. Through these 
systems, Olympus provides an excellent basis for 
research on a wide variety of spoken dialog issues. 
The modular construction promotes the transfer 
and reuse of research contributions across systems.  
While desirable, an in-depth understanding of 
the differences between all these toolkits remains 
an open question. We believe that an open ex-
change of experiences and resources across toolkits 
will create a better understanding of the current 
state-of-the-art, generate new ideas, and lead to 
better systems for everyone. Towards this end, we 
are making the Olympus framework, as well as a 
number of systems and dialog corpora, freely 
available to the community. 
Acknowledgements 
We would like to thank all those who have brought 
contributions to the components underlying the 
Olympus dialog system framework. Neither Olym-
pus nor the dialog systems discussed in this paper 
would have been possible without their help. We 
particularly wish to thank Alan W Black for his 
continued support and advice. Work on Olympus 
components and systems was supported in part by 
DARPA, under contract NBCH-D-03-0010, Boe-
ing, under contract CMU-BA-GTA-1, and the US 
National Science Foundation under grant number 
0208835. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the authors and do not necessarily reflect 
the views of the funding agencies.  
38
References  
Aist, G., Dowding, J., Hockey, B.A., Rayner, M., 
Hieronymus, J., Bohus, D., Boven, B., Blaylock, N., 
Campana, E., Early, S., Gorrell, G., and Phan, S., 
2003. Talking through procedures: An intelligent 
Space Station procedure assistant, in Proc. of EACL-
2003, Budapest, Hungary 
Ariadne, 2007, The Ariadne web-site, as of January 
2007, http://www.opendialog.org/. 
Balakirsky, S., Scrapper, C., Carpin, S., and Lewis, M. 
2006. UsarSim: providing a framework for multi-
robot performance evaluation, in Proc. of PerMIS. 
Black, A. and Lenzo, K., 2000. Building Voices in the 
Festival Speech System, http://festvox.org/bsv/, 2000. 
Bohus, D., Grau Puerto, S., Huggins-Daines, D., Keri, 
V., Krishna, G., Kumar, K., Raux, A., Tomko, S., 
2007. Conquest ? an Open-Source Dialog System for 
Conferences, in Proc. of HLT 2007, Rochester, USA. 
Bohus, D., Langner, B., Raux, A., Black, A., Eskenazi, 
M., Rudnicky, A.  2006.  Online Supervised Learning 
of Non-understanding Recovery Policies, in Proc. of 
SLT-2006, Aruba.  
Bohus, D., and Rudnicky, A.  2006.  A K-hypotheses + 
Other Belief Updating Model, in Proc. of the AAAI 
Workshop on Statistical and Empirical Methods in 
Spoken Dialogue Systems, 2006. 
Bohus, D., and Rudnicky, A.,  2005.  Sorry I didn?t 
Catch That: An Investigation of Non-understanding 
Errors and Recovery Strategies, in Proc. of SIGdial-
2005, Lisbon, Portugal. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proc. of Eu-
rospeech 2003, Geneva, Switzerland. 
Bohus, D., and Rudnicky, A., 2002a. LARRI: A Lan-
guage-based Maintenance and Repair Assistant, in 
Proc. of IDS-2002, Kloster Irsee, Germany. 
Bohus, D., and Rudnicky, A., 2002b. Integrating Multi-
ple Knowledge Sources in the CMU Communicator 
Dialog System, Technical Report CMU-CS-02-190. 
Bos, J., Klein, E., Lemon, O., and Oka, T., 2003. 
DIPPER: Description and Formalisation of an In-
formation-State Update Dialogue System Architec-
ture, in Proc. of SIGdial-2003, Sapporo, Japan 
Cepstral, LLC, 2005. SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Cole, R., 1999. Tools for Research and Education in 
Speech Science, in Proc. of the International Confer-
ence of Phonetic Sciences, San Francisco, USA. 
Glass, J., Weinstein, E., Cyphers, S., Polifroni, J., 2004. 
A Framework for Developing Conversational Inter-
faces, in Proc. of CADUI, Funchal, Portugal. 
Harris, T. K., Banerjee, S., Rudnicky, A., Sison, J. 
Bodine, K., and Black, A. 2004. A Research Platform 
for Multi-Agent Dialogue Dynamics, in Proc. of The 
IEEE International Workshop on Robotics and Hu-
man Interactive Communications, Kurashiki, Japan. 
Harris, T. K., Banerjee, S., Rudnicky, A. 2005. Hetero-
genous Multi-Robot Dialogues for Search Tasks, in 
AAAI Spring Symposium: Dialogical Robots, Palo 
Alto, California. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Kawamoto, S.,  Shimodaira, H., Nitta, T., Nishimoto, 
T., Nakamura, S., Itou, K., Morishima, S., Yotsukura, 
T., Kai, A., Lee, A., Yamashita, Y., Kobayashi, T., 
Tokuda, K., Hirose, K., Minematsu, N., Yamada, A., 
Den, Y., Utsuro, T., and Sagayama, S., 2002. Open-
source software for developing anthropomorphic 
spoken dialog agent, in Proc. of PRICAI-02, Interna-
tional Workshop on Lifelike Animated Agents. 
Raux, A., Langner, B., Bohus, D., Black, A., and Eske-
nazi, M.  2005, Let's Go Public! Taking a Spoken 
Dialog System to the Real World, in Proc. of Inter-
speech 2005, Lisbon, Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. 2006 Doing Research on a Deployed Spoken 
Dialogue System: One Year of Let's Go! Experience, 
in Proc. of Interspeech 2006, Pittsburgh, USA. 
RavenClaw-Olympus web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., 
Shern, R., Lenzo, K., Xu W., and Oh, A., 1999. Cre-
ating natural dialogs in the Carnegie Mellon Com-
municator system, in Proc. of Eurospeech 1999. 
Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., and 
Zue V. 1998 Galaxy-II: A reference architecture for 
conversational system development, in Proc. of 
ICSLP98, Sydney, Australia. 
Stenchikova, S., Mucha, B., Hoffman, S., Stent, A., 
2007. RavenCalendar: A Multimodal Dialog System 
for Managing A Personal Calendar, in Proc. of HLT 
2007, Rochester, USA.  
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proc. of the ARPA Human Language Technology 
Workshop, pages 213?216, Plainsboro, NJ. 
39
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 21?24,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mixture Pruning and Roughening for Scalable Acoustic Models
David Huggins-Daines
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dhuggins@cs.cmu.edu
Alexander I. Rudnicky
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
air@cs.cmu.edu
Abstract
In an automatic speech recognition system us-
ing a tied-mixture acoustic model, the main
cost in CPU time and memory lies not in
the evaluation and storage of Gaussians them-
selves but rather in evaluating the mixture
likelihoods for each state output distribution.
Using a simple entropy-based technique for
pruning the mixture weight distributions, we
can achieve a significant speedup in recogni-
tion for a 5000-word vocabulary with a negli-
gible increase in word error rate. This allows
us to achieve real-time connected-word dicta-
tion on an ARM-based mobile device.
1 Introduction
As transistors shrink and CPUs become faster and
more power-efficient, we find ourselves entering a
new age of intelligent mobile devices. We believe
that not only will these devices provide access to rich
sources of on-line information and entertainment,
but they themselves will find new applications as
personal knowledge management agents. Given the
constraints of the mobile form factor, natural speech
input is crucial to these applications. However, de-
spite the advances in processor technology, mobile
devices are still highly constrained by their memory
and storage subsystems.
2 Semi-Continuous Acoustic Models
Recent research into acoustic model compression
and optimization of acoustic scoring has focused
on ?Fully Continuous? acoustic models, where each
Hidden Markov Model state?s output probability dis-
tribution is modeled by a mixture of multivariate
Gaussian densities. This type of model allows large
amounts of training data to be efficiently exploited to
produce detailed models. However, due to the large
number of parameters in these models, approximate
computation techniques (Woszczyna, 1998) are re-
quired in order to achieve real-time recognition even
on workstation-class hardware.
Another historically popular type of acoustic
model is the so-called ?Semi-Continuous? or tied-
mixture model, where a single codebook of Gaus-
sians is shared by all HMM states (Huang, 1989).
The parameters of this codebook are updated using
the usual Baum-Welch equations during training, us-
ing sufficient statistics from all states. The mix-
ture weight distributions therefore become the main
source of information used to distinguish between
different speech sounds.
There are several benefits to semi-continuous
models for efficient speech recognition. The most
obvious is the greatly reduced number of Gaussian
densities which need to be computed. With fully
continuous models, we must compute one codebook
of 16 or more Gaussians for each HMM state, of
which there may be several thousand active for any
given frame of speech input. For semi-continuous
models, there is a single codebook with a small num-
ber of Gaussians, typically 256. In addition, since
only a few Gaussians will have non-negligible den-
sities for each frame of speech, and this set of Gaus-
sians tends to change slowly, partial computation of
each density is possible.
Another useful property of semi-continuous mod-
els is that the mixture weights for each state have
the form of a multinomial distribution, and are thus
amenable to various smoothing and adaptation tech-
niques. In particular, Bayesian and quasi-Bayes
21
adaptation (Huo and Chan, 1995) are effective and
computationally efficient.
3 Experimental Setup
All experiments in this paper were performed using
PocketSphinx (Huggins-Daines et al, 2006). The
baseline acoustic model was trained from the com-
bined WSJ0 and WSJ1 ?long? training sets (Paul and
Baker, 1992), for a total of 192 hours of speech.
This speech was converted to MFCC features us-
ing a bank of 20 mel-scale filters spaced from 0
to 4000Hz, allowing the model to work with au-
dio sampled at 8kHz, as is typical on mobile de-
vices. We used 5-state Hidden Markov Models
for all phones. Output distributions were modeled
by a codebook of 256 Gaussians, shared between
5000 tied states and 220 context-independent states.
Only the first pass of recognition (static lexicon tree
search) was performed.
Our test platform is the Nokia N800, a hand-
held Internet Tablet. It uses a Texas Instruments
OMAPTM 2420 processor, which combines an
ARM11 RISC core and a C55x DSP core on a single
chip. The RISC core is clocked at 400MHz while the
DSP is clocked at 220MHz. In these experiments,
we used the ARM core for all processing, although
we have also ported the MFCC extraction code to the
DSP. The decoder binaries, models and audio files
were stored on a high-speed SD flash card format-
ted with the ext3 journaling filesystem. Using the
standard bcb05cnp bigram language model, we
obtained a baseline word error rate of 9.46% on the
si_et_05 test set. The baseline performance of
this platform on the test set is 1.40 times real-time,
that is, for every second of speech, 1.40 seconds of
CPU time are required for recognition.
We used the oprofile utility1 on the Nokia
N800 to collect statistical profiling information for
a subset of the test corpus. The results are shown in
Table 1. We can see that three operations occupy the
vast majority of CPU time used in decoding: man-
aging the list of active HMM states, computing the
codebook of Gaussians, and computing mixture den-
sities.
The size of the files in the acoustic model is shown
in Table 2. The amount of CPU time required to
1http://oprofile.sourceforge.net/
Function %CPU
HMM evaluation 22.41
hmm vit eval 5st lr 13.36
hmm vit eval 5st lr mpx 3.71
Mixture Evaluation 21.66
get scores4 8b 14.94
fast logmath add 6.72
Lexicon Tree Search 19.89
last phone transition 5.25
prune nonroot chan 4.15
Active List Management 15.57
hmm sen active 13.75
compute sen active 1.19
Language Model Evaluation 7.80
find bg 2.55
ngram ng score 2.13
Gaussian Evaluation 5.87
eval cb 5.59
eval topn 0.28
Acoustic Feature Extraction 3.60
fe fft real 1.59
fixlog2 0.77
Table 1: CPU profiling, OMAP platform
File Size (bytes)
sendump (mixture weights) 5345920
mdef (triphone mappings) 1693280
means (Gaussians) 52304
variances (Gaussians) 52304
transition_matrices 5344
Table 2: File sizes, WSJ1 acoustic model
calculate densities is related to the size of the mix-
ture weight distribution by the fact that the N800
has a single-level 32Kb data cache, while a typical
desktop processor has two levels of cache totalling
at least 1Mb. We used cachegrind2 to simulate
the memory hierarchy of an OMAP versus an AMD
K8 desktop processor with 64Kb of L1 cache and
512Kb of L2 cache, with results shown in Table 3.
While other work on efficient recognition has fo-
cused on quantization of the Gaussian parameters
(Leppa?nen and Kiss, 2005), in a semi-continuous
model, the number of these parameters is small
2http://valgrind.org/
22
Function ARM K8
hmm vit eval 5st lr 4.71 3.95
hmm sen active 3.55 3.76
get scores4 8b 2.87 1.92
prune root chan 2.07 2.29
prune nonroot chan 1.99 1.73
eval cb 1.73 1.77
hmm vit eval 5st lr mpx 1.30 0.80
Table 3: Data cache misses (units of 107)
enough that little cost is incurred by storing and cal-
culating them as 32-bit fixed-point numbers. There-
fore, we focus here on ways to reduce the amount of
storage and computation used by the mixture weight
distributions.
4 Mixture Roughening
Our method for speeding up mixture computation is
based on the observation that mixture weight distri-
butions are typically fairly ?spiky?, with most of the
probability mass concentrated in a small number of
mixture weights. One can quantify this by calculat-
ing the perplexity of the mixture distributions:
pplx(wi) = exp
N?
k=0
wik log
1
wik
A histogram of perplexities is shown in Figure
1. The perplexity can be interpreted as the average
number of Gaussians which were used to generate
an observation drawn from a particular distribution.
Therefore, on average, the vast majority of the 256
Gaussians contribute minimally to the likelihood of
the data given a particular mixture model.
When evaluating mixture densities with pruned
models, one can either treat these mixture weights
as having a small but non-negligible value, or set
them to zero3. Note that the mixture weights are
renormalized in both cases, and thus the former is
more or less equivalent to add-one smoothing. The
latter can be thought of as exactly the opposite of
smoothing - ?roughening? the distribution. To in-
vestigate this, we set al but the top 16 values in each
mixture weight distribution to zero and ran a num-
ber of trials on a K8-based workstation, varing the
3Meaning a very small number, since they are stored in log
domain.
0 50 100 150 200Perplexity(w)0
200
400
600
800
1000
# o
f mi
xtur
e w
eigh
ts mode = 10
Figure 1: Perplexity distribution of mixture weights
3 4 5 6 7 8-log10(mixw_floor)0.00
0.05
0.10
0.15
0.20
0.25
0.30
Perf
orm
anc
e (x
RT)
5
10
15
20
25
30
Erro
r Ra
te (
%W
ER)
xRT (16 mixtures)xRT (baseline)WER (16 mixtures)WER (baseline)
Figure 2: Smoothing vs. Roughening, 16 mixtures
mixture weight floor to produce either a smoothing
or roughening effect. We discovered something ini-
tially surprising: ?roughening? the mixture weights
speeds up decoding significantly, while smoothing
them slows it down. A plot of speed and error rate
versus mixture weight floor is shown in Figure 2.
However, there is a simple explanation for this.
At each frame, only the top N Gaussian densities
are actually used to calculate the likelihood of the
data:
p(x|?i) =
?
k?topN
wikN(x; ~?ik, ~?
2
ik)
When we remove mixture weights, we increase
the probability that these top N densities will be
matched with pruned-out weights. If we smooth the
weights, we may raise some of these weights above
their maximum-likelihood estimate, thus increasing
23
3 4 5 6 7 8-log10(mixw_floor)0.05
0.10
0.15
0.20
0.25
0.30
Perf
orm
anc
e (x
RT)
0.10 xRT, 9.68 %WER
9.0
9.5
10.0
10.5
11.0
11.5
Erro
r Ra
te (
%W
ER)
xRT (64 mixtures)xRT (96 mixtures)xRT (baseline)WER (64 mixtures)WER (96 mixtures)WER (baseline)
Figure 3: Speed-accuracy tradeoff with pruned mixtures
the likelihood for models whose top mixture weights
do not overlap with the top N densities. This may
prevent HMM states whose output distributions are
modeled by said models from being pruned by beam
search, therefore slowing down the decoder. By
?roughening? the weights, we decrease the likeli-
hood of the data for these models, and hence make
them more likely to be pruned, speeding up the de-
coder and increasing the error rate. This is a kind
of ?soft? GMM selection, where instead of exclud-
ing some models, we simply make some more likely
and others less likely.
As we increase the number of retained mixture
weights, we can achieve an optimal tradeoff between
speed and accuracy, as shown in Figure 3. Addition-
ally, the perplexity calculation suggests a principled
way to vary the number of retained mixture weights
for each model. We propose setting a target number
of mixture weights, then calculating a scaling factor
based on the ratio of this target to the average per-
plexity of all models:
topKi =
target
1
N
?N
i=0 pplx(wi)
pplx(wi)
One problem is that many models have very low
perplexity, such that we end up retaining only a few
mixture weights. When the mixture weights are
?roughened?, this guarantees that these models will
score poorly, regardless of the data. We compensate
for this by keeping a minimum number of mixture
weights regardless of the perplexity. Using a tar-
get of 96 mixtures, a minimum of 16, and a mixture
weight floor of 10?8, we achieve 9.90% word error
rate in 0.09 times real-time, a 21% speedup with a
2.7% relative increase in error (baseline error rate is
9.64% on the desktop).
Using the same entropy-pruned mixture weights
on the N800, we achieve an error rate of 9.79%, run-
ning in 1.19 times real-time, a 15% speedup with a
3.4% relative increase in error. After applying ab-
solute pruning thresholds of 800 HMMs per frame
and 5 words per frame, we obtained a 10.01% word
error rate in 1.01 times real-time.
5 Conclusion
We have shown that a simple pruning technique al-
lows acoustic models trained for large-vocabulary
continuous speech recognition to be ?scaled down?
to run in real-time on a mobile device without major
increases in error. In related work, we are exper-
imenting with bottom-up clustering techniques on
the mixture weights to produce truly scalable acous-
tic models, and subvector clustering to derive semi-
continuous models automatically from well-trained
fully-continuous models.
Acknowledgments
We wish to thank Nokia for donating the N800 tablet
used in these experiments.
References
X. D. Huang. 1989. Semi-continuous Hidden Markov
Models for Speech Recognition. Ph.D. thesis, Univer-
sity of Edinburgh.
D. Huggins-Daines, M. Kumar, A. Chan, A. Black,
M. Ravishankar, and A. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recogni-
tion system for hand-held devices. In Proceedings of
ICASSP 2006, Toulouse, France.
Q. Huo and C. Chan. 1995. On-line Bayes adaptation of
SCHMM parameters for speech recognition. In Pro-
ceedings of ICASSP 1995, Detroit, USA.
J. Leppa?nen and I. Kiss. 2005. Comparison of low foot-
print acoustic modeling techniques for embedded ASR
systems. In Proceedings of Interspeech 2005, Lisbon,
Portugal.
D. Paul and J. Baker. 1992. The design for the Wall
Street Journal based CSR corpus. In Proceedings of
the ACL workshop on Speech and Natural Language.
M. Woszczyna. 1998. Fast Speaker Independent Large
Vocabulary Continuous Speech Recognition. Ph.D.
thesis, University of Karlsruhe.
24
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67?71,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Non-textual Event Summarization by Applying Machine Learning to
Template-based Language Generation
Mohit Kumar and Dipanjan Das and Sachin Agarwal and Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University, Pittsburgh, USA
mohitkum,dipanjan,sachina,air@cs.cmu.edu
Abstract
We describe a learning-based system that
creates draft reports based on observation
of people preparing such reports in a tar-
get domain (conference replanning). The
reports (or briefings) are based on a mix
of text and event data. The latter consist
of task creation and completion actions,
collected from a wide variety of sources
within the target environment. The report
drafting system is part of a larger learning-
based cognitive assistant system that im-
proves the quality of its assistance based
on an opportunity to learn from observa-
tion. The system can learn to accurately
predict the briefing assembly behavior and
shows significant performance improve-
ments relative to a non-learning system,
demonstrating that it?s possible to create
meaningful verbal descriptions of activity
from event streams.
1 Introduction
We describe a system for recommending items for
a briefing created after a session with a crisis man-
agement system in a conference replanning do-
main. The briefing system is learning-based, in
that it initially observes how one set of users cre-
ates such briefings then generates draft reports for
another set of users. This system, the Briefing
Assistant(BA), is part of a set of learning-based
cognitive assistants each of which observes users
and learns to assist users in performing their tasks
faster and more accurately.
The difference between this work from
most previous efforts, primarily based on text-
extraction approaches is the emphasis on learning
to summarize event patterns. This work also
differs in its emphasis on learning from user
behavior in the context of a task.
Report generation from non-textual sources has
been previously explored in the Natural Language
Generation (NLG) community in a variety of do-
mains, based on, for example, a database of events.
However, a purely generative approach is not suit-
able in our circumstances, as we want to summa-
rize a variety of tasks that the user is performing
and present a summary tailored to a target audi-
ence, a desirable characteristic of good briefings
(Radev and McKeown, 1998). Thus we approach
the problem by applying learning techniques com-
bined with a template-based generation system to
instantiate the briefing-worthy report items. The
task of instantiating the briefing-worthy items is
similar to the task of Content Selection (Duboue,
2004) in the Generation pipeline however our ap-
proach minimizes linguistic involvement. Our
choice of a template-based generative system was
motivated by recent discussions in the NLG com-
munity (van Deemter et al, 2005) about the prac-
ticality and effectiveness of this approach.
The plan of the paper is as follows. We describe
relevant work from existing literature in the next
section. Then, we provide brief system description
followed by experiments and results. We conclude
with a summary of the work.
2 Related Work
Event based summarization has been studied in the
summarization community. (Daniel et al, 2003)
described identification of sub-events in multiple
documents. (Filatova and Hatzivassiloglou, 2004)
mentioned the use of event-based features in ex-
tractive summarization and (Wu, 2006; Li et al,
2006) describe similar work based on events oc-
curring in text. However, unlike the case at hand,
all the work on event-based summarization used
text as source material.
Non-textual summarization has also been ex-
plored in the Natural Language Generation (NLG)
community within the broad task of generating
67
reports based on database of events in specific
domains such as medical (Portet et al, 2009),
weather (Belz, 2007), sports (Oh and Shrobe,
2008) etc. However, in our case we want to sum-
marize a variety of tasks that the user is perform-
ing and present a summary to an intended audi-
ence (as defined by a report request).
Recent advances in NLG research use statis-
tical approaches at various stages of processing
in the generation pipeline like content selection
(Duboue and McKeown, 2003; Barzilay and Lee,
2004), probabilistic generation rules (Belz, 2007).
Our proposed approach differs from these in that
we apply machine learning after generation of all
the templates, as a post-processing step, to rank
them for inclusion in the final briefing. We could
have used a general purpose template-based gen-
eration framework like TG/2 (Busemann, 2005),
but since the number of templates and their corre-
sponding aggregators is limited, we chose an ap-
proach based on string manipulation.
We found in our work that an approach based
on modeling individual users and then combining
the outputs of such models using a voting scheme
gives the best results, although our approach is
distinguishable from collaborative filtering tech-
niques used for driving recommendation systems
(Hofmann, 2004). We believe this is due to the
fact that the individual sessions from which rank-
ing models are learned, although they range over
the same collection of component tasks, can lead
to very different (human-generated) reports. That
is, the particular history of a session will affect
what is considered to be briefing-worthy.
3 System Overview
Figure 1: Briefing Assistant Data Flow.
The Briefing Assistant Model: We treat the
task of briefing generation in the current domain1
as non-textual event-based summarization. The
1More details about the domain and the interaction of BA
with the larger system are mentioned in a longer version of
the paper (Kumar et al, 2009)
Figure 2: The category tree showing the informa-
tion types that we expect in a briefing.
events are the task creation and task completion
actions logged by various cognitive assistants in
the system (so-called specialists). As part of the
design phase for the template-based generation
component, we identified a set of templates, based
on the actual briefings written by users in a sepa-
rate experiment. Ideally, we would like to adopt
a corpus-based approach to automatically extract
the templates in the domain, like (Kumar et al,
2008), but since the sample briefings available to
us were very few, the application of such corpus-
based techniques was not necessary. Based on
this set of templates we identified the patterns that
needed to be extracted from the event logs in order
to populate the templates. A ranking model was
also designed for ordering instantiations of this set
of templates and to recommend the top 4 most rel-
evant ones for a given session.
The overall data flow for BA during a session
(runtime) is shown in Figure 1. The various spe-
cialist modules generate task related events that
are logged in a database. The aggregators operate
over this database and emails to extract relevant
patterns. These patterns in turn are used to popu-
late templates which constitute candidate briefing
items. The candidate briefing items are then or-
dered by the ranking module and presented to the
user.
Template Design and Aggregators: The set
of templates used in the current instantiation of
the BA was derived from a corpus of human-
generated briefings collected in a previous exper-
iment using the same crisis management system.
The set of templates was designed to cover the
range of items that users in that experiment chose
to include in their reports corresponding to nine
categories shown in Figure 2. We found that in-
formation can be conveyed at different levels of
granularity (for example, qualitatively or quantita-
tively). The appropriate choice of granularity for
68
a particular session is a factor that the system can
learn2.
Ranking Model, Classifiers and Features: The
ranking module orders candidate templates so
that the four most relevant ones appear in the
briefing draft. The ranking system consists of
a consensus-based classifier, based on individual
classifier models for each user in the training set.
The prediction from each classifier are combined
(averaged) to produce a final rank of each tem-
plate.
We used the Minorthird package (Cohen, 2004)
for modeling. Specifically we allowed the sys-
tem to experiment with eleven different learning
schemes and select the best one based on cross-
validation within the training corpus. The schemes
were Naive Bayes, Voted Perceptron, Support
Vector Machines, Ranking Perceptron, K Nearest
Neighbor, Decision Tree, AdaBoost, Passive Ag-
gressive learner, Maximum Entropy learner, Bal-
anced Winnow and Boosted Ranking learner.
The features3 used in the system are static or
dynamic. Static features reflect the properties of
the templates irrespective of the user?s activity
whereas the dynamic features are based on the
actual events that took place. We used the In-
formation Gain (IG) metric for feature selection,
experimenting with seven different cut-off values
All, 20, 15, 10, 7, 5, 4 for the total number of se-
lected features.
4 Experiments and Results
Experimental Setup: Two experimental condi-
tions were used to differentiate performance based
on knowledge engineering, designated MinusL
and performance based on learning, designated
PlusL.4
Email Trigger: In the simulated conference
replanning crisis, the briefing was triggered by
an email containing explicit information requests,
not known beforehand. To customize the brief-
ing according to the request, a natural language
processing module identified the categories of in-
formation requested. The details of the module
are beyond the scope of the current paper as it
2The details of template design process including sample
templates, categories of templates and details of aggregators
are presented in (Kumar et al, 2009)
3Detailed description of the features are mentioned in
(Kumar et al, 2009)
4The details of the experimental setup as part of the larger
cognitive assistant system are presented in (Kumar et al,
2009).
is external to our system; it took into account
the template categories we earlier identified. Fig-
ure 4 shows a sample briefing email stimulus.
The mapping from the sample email in the figure
to the categories is as follows: ?expected atten-
dance? - Property-Session; ?how many sessions
have been rescheduled?, ?how many still need to
be rescheduled?, ?any problems you see as you
try to reschedule? - Session-Reschedule; ?status
of food service (I am worried about the keynote
lunch)? - Catering Vendors.
Training: Eleven expert users5 were asked to
provide training by using the system then generat-
ing the end of session briefing using the BA GUI.
For this training phase, no item ranking was per-
formed by the system, i.e. all the templates were
populated by the aggregators and recommenda-
tions were random. The expert user was asked
to select the best possible four items and was fur-
ther asked to judge the usefulness of the remaining
items. The resulting training data consists of the
activity log, extracted features and the user-labeled
items. The trigger message for the training users
did not contain any specific information request.
Test: Subjects were recruited to use the crisis
management system in MinusL and PlusL condi-
tion, although they were not aware of the condition
of the system and they were not involved with the
project. There were 54 test runs in the MinusL
condition and 47 in the PlusL condition. Out of
these runs, 29 subjects in MinusL and 43 subjects
in PlusL wrote a briefing using the BA. We report
the evaluation scores for this latter set.
Evaluation: The base performance metric is
Recall, defined in terms of the briefing templates
recommended by the system compared to the tem-
plates ultimately selected by the user. We justify
this by noting that Recall can be directly linked to
the expected time savings for the users. We cal-
culate two variants of Recall: Category-based?
calculated by matching the categories of the BA
recommended templates and user selected ones
ignoring the granularity and Template-based?
calculated by matching the exact templates. The
first metric indicates whether the right category of
information was selected and the latter indicates
whether the information was presented at the ap-
propriate level of detail.
We also performed subjective human evaluation
5Members of the project from other groups who were
aware of the scenario and various system functionalities but
not the ML methods
69
using a panel of three judges. The judges assigned
scores (0-4) to each of the bullets based on the
coverage of the crisis, clarity and conciseness, ac-
curacy and the correct level of granularity. They
were advised about certain briefing-specific char-
acteristics (e.g. negative bullet items are useful
and hence should be rated favorably). They were
also asked to provide a global assessment of report
quality, and evaluate the coverage of the requests
in the briefing stimulus email message. This pro-
cedure was very similar to the one used as the basis
for template selection.
Experiment: The automatic evaluation met-
ric used for the trained system configuration is
the Template-based recall measure. To obtain
the final system configuration, we automatically
evaluate the system under the various combina-
tions of parameter settings with eleven different
learning schemes and seven different feature se-
lection threshold (as mentioned in previous sec-
tions). Thus a total of 77 different configurations
are tested. For each configuration, we do a eleven-
fold cross-validation between the 11 training users
i.e. we leave one user as the test user and consider
the remaining ten users as training users. We av-
erage the performance across the 11 test cases and
obtain the final score for the configuration. We
choose the configuration with the highest score as
the final trained system configuration. The learned
system configuration in the current test includes
Balanced Winnow (Littlestone, 1988) and top 7
features.
Results: We noticed that four users in PlusL
condition took more than 8 minutes to complete
the briefing when the median time taken by the
users in PlusL condition was 55 seconds, so we
did not include these users in our analysis in order
to maintain the homogeneity of the dataset. These
four data points were identified as extreme outliers
using a procedure suggested by (NIST, 2008)6.
There were no extreme outliers in MinusL condi-
tion.
Figure 3a shows the Recall values for the Mi-
nusL and PlusL conditions. The learning delta
i.e. the difference between the recall values of
PlusL and MinusL is 33% for Template-based re-
call and 21% for Category-based recall. These
differences are significant at the p < 0.001 level.
6Extreme outliers are defined as data points that are out-
side the range [Q1?3?IQ,Q3+3?IQ] in a box plot. Q1 is
lower quartile, Q3 is upper quartile and IQ is the difference
(Q3?Q1) is the interquartile range.
The statistical significance for the Template-based
metric, which was the metric used for select-
ing system parameters during the training phase,
shows that learning is effective in this case. Since
the email stimulus processing module extracts the
briefing categories from the email the Category-
based and Template-based recall is expected to be
high for the baseline MinusL case. In our test, the
email stimuli had 3 category requests and so the
Category-based recall of 0.77 and Template-based
recall of 0.67 in MinusL is not unexpected.
Figure 3b shows the Judges? panel scores for
the briefings in MinusL and PlusL condition. The
learning delta in this case is 3.6% which is also
statistically significant, at p < 0.05. The statistical
significance of the learning delta validates that the
briefings generated during PlusL conditions are
better than MinusL condition. The absolute differ-
ence in the qualitative briefing scores between the
two conditions is small because MinusL users can
select from all candidates, while the recommenda-
tions they receive are random. Consequently they
need to spend more time in finding the right items.
The average time taken for a briefing in MinusL
condition is about 83 seconds and 62 seconds in
PlusL (see Figure 3c). While the time difference
is high (34%) it is not statistically significant due
to high variance.
Four of the top 10 most frequently selected fea-
tures across users for this system are dynamic fea-
tures. This indicates that the learning model is
capturing the user?s world state and the recom-
mendations are related to the underlying events.
We believe this validates the process we used to
generate briefing reports from non-textual events.
5 Summary
The Briefing Assistant is not designed to learn
the generic attributes of good reports; rather it?s
meant to rapidly learn the attributes of good re-
ports within a particular domain and to accom-
modate specific information needs on a report-by-
report basis. We found that learned customiza-
tion produces reports that are judged to be of bet-
ter quality. We also found that a consensus-based
modeling approach, which incorporates informa-
tion from multiple users, yields the best perfor-
mance. We believe that our approach can be used
to create flexible summarization systems for a va-
riety of applications.
70
(a) (b) (c)
Figure 3: (a) Recall values for MinusL and PlusL conditions (b) Briefing scores from the judges? panel
for MinusL and PlusL conditions (c) Briefing time taken for MinusL and PlusL conditions.
Figure 4: Template categories corresponding to
the Briefing request email.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of HLT-NAACL.
Stephan Busemann. 2005. Ten years after: An update
on TG/2 (and friends). In Proceedings of European
Natural Language Generation Workshop.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net, 10th Jun 2009.
Naomi Daniel, Dragomir Radev, and Timothy Allison.
2003. Sub-event based multi-document summariza-
tion. In Proceedings of HLT-NAACL.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of
EMNLP.
Pablo A. Duboue. 2004. Indirect supervised learning
of content selection logic. In Proceedings of INLG.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22(1):89?115.
Mohit Kumar, Dipanjan Das, and Alexander I. Rud-
nicky. 2008. Automatic extraction of briefing tem-
plates. In Proceedings of IJCNLP.
Mohit Kumar, Dipanjan Das, Sachin Agarwal, and
Alexander I. Rudnicky. 2009. Non-textual event
summarization by applying machine learning to
template-based language generation. Technical Re-
port CMU-LTI-09-012, Language Technologies In-
stitute, Carnegie Mellon University.
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa
Yuan. 2006. Extractive summarization using inter-
and intra- event relevance. In Proceedings of ACL.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. Machine Learning, 2(4):285?318.
NIST. 2008. NIST/SEMATECH e-
handbook of statistical methods.
http://www.itl.nist.gov/div898/handbook/, 10th
Jun 2009.
Alice Oh and Howard Shrobe. 2008. Generating base-
ball summaries from multiple perspectives by re-
ordering content. In Proceedings of INLG.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7-8):789?816.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Kees van Deemter, Emiel Krahmer, and Mariet The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Computa-
tional Linguistics, 31(1):15?24.
Mingli Wu. 2006. Investigations on event-based sum-
marization. In Proceedings of ACL.
71
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 71?78,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Detecting the Noteworthiness of Utterances in Human Meetings 
 
 
Satanjeev Banerjee 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
banerjee@cs.cmu.edu  
Alexander I. Rudnicky 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
air@cs.cmu.edu 
  
 
Abstract 
Our goal is to make note-taking easier in 
meetings by automatically detecting 
noteworthy utterances in verbal ex-
changes and suggesting them to meeting 
participants for inclusion in their notes. 
To show feasibility of such a process we 
conducted a Wizard of Oz study where 
the Wizard picked automatically tran-
scribed utterances that he judged as 
noteworthy, and suggested their contents 
to the participants as notes. Over 9 meet-
ings, participants accepted 35% of these 
suggestions. Further, 41.5% of their notes 
at the end of the meeting contained Wi-
zard-suggested text. Next, in order to per-
form noteworthiness detection automati-
cally, we annotated a set of 6 meetings 
with a 3-level noteworthiness annotation 
scheme, which is a break from the binary 
?in summary?/ ?not in summary? labe-
ling typically used in speech summariza-
tion. We report Kappa of 0.44 for the 3-
way classification, and 0.58 when two of 
the 3 labels are merged into one. Finally, 
we trained an SVM classifier on this an-
notated data; this classifier?s performance 
lies between that of trivial baselines and 
inter-annotator agreement.  
1 Introduction 
We regularly exchange information verbally with 
others over the course of meetings. Often we 
need to access this information afterwards. Typi-
cally we record the information we consider im-
portant by taking notes. Note taking at meetings 
is a difficult task, however, because the partici-
pant must summarize and write down the infor-
mation in a way such that it is comprehensible 
afterwards, while paying attention to and partici-
pating in the ongoing discussion. Our goal is to 
make note-taking easier by automatically extract-
ing noteworthy items from spoken interactions in 
real time, and proposing them to the humans for 
inclusion in their notes.   
Judging which pieces of information in a 
meeting are noteworthy is a very subjective task. 
The subjectivity of this task is likely to be more 
acute than even that of meeting summarization, 
where low inter-annotator agreement is typical 
e.g. (Galley, 2006), (Liu & Liu, 2008), (Penn & 
Zhu, 2008), etc ? whether a piece of information 
should be included in a participant?s notes de-
pends not only on its importance, but also on 
factors such as the participant?s need to remem-
ber, his perceived likelihood of forgetting, etc. 
To investigate whether it is feasible even for a 
human to predict what someone else might find 
noteworthy in a meeting, we conducted a Wizard 
of Oz-based user study where a human suggested 
notes (with restriction) to meeting participants 
during the meeting. We concluded from this 
study (presented in section 2) that this task ap-
pears to be feasible for humans.  
Assuming feasibility, we then annotated 6 
meetings with a 3-level noteworthiness scheme. 
Having 3 levels instead of the typical 2 allows us 
to explicitly separate utterances of middling 
noteworthiness from those that are definitely 
noteworthy or not noteworthy, and allows us to 
encode more human knowledge than a 2-level 
scheme. We describe this annotation scheme in 
more detail in section 3, and show high inter-
annotator agreement compared to that typically 
reported in the summarization literature. Finally 
in sections 4 and 5 we use this annotated data to 
train and test a simple Support Vector Machine-
based predictor of utterance noteworthiness. 
2 Can Humans Do this Task?  
As mentioned in the introduction, given the de-
gree of subjectivity involved in identifying note-
71
worthy utterances, it is reasonable to ask whether 
the notes-suggestion task can be accomplished 
by humans, let alne by automatic systems. That 
is, we ask the question: Is it possible for a human 
to identify noteworthy utterances in a meeting 
such that  
(a) For at least some fraction of the suggestions, 
one or more meeting participants agree that 
the suggested notes should indeed be in-
cluded in their notes, and 
(b) The fraction of suggested notes that meeting 
participants find noteworthy is high enough 
that, over a sequence of meetings, the meet-
ing participants do not learn to simply ignore 
the suggestions.  
Observe that this task is more restricted than that 
of generic note-taking. While a human who is 
allowed to summarize discussions and produce 
to-the-point notes is likely to be useful, we as-
sume here that our system will not be able to 
create such abstractive summaries. Rather, our 
goal here is to explore the feasibility of an ex-
tractive summarization system that simply picks 
noteworthy utterances and suggests their con-
tents to the participants. To answer this question, 
we conducted a Wizard of Oz-based pilot user 
study, as follows. 
2.1 Wizard of Oz Study Design 
We designed a user study in which a human Wi-
zard listened to the utterances being uttered dur-
ing the meeting, identified noteworthy utter-
ances, and suggested their contents to one or 
more participants for inclusion in their notes. In 
order to minimize differences between the Wi-
zard and the system (except for the Wizard?s 
human-level ability to judge noteworthiness), we 
restricted the Wizard in the following ways: 
(a) The Wizard was allowed to only suggest the 
contents of individual utterances to the par-
ticipants, and not summarize the contents of 
multiple utterances.  
(b) The Wizard was allowed to listen to the 
meeting speech, but when suggesting the 
contents of an utterance to the participants, 
he was restricted to using a real-time auto-
matic transcription of the utterance. (He was 
allowed to withhold suggestions because 
they were too erroneously transcribed.) 
(c) In order to be closer to a system that has lit-
tle or no ?understanding? of the meetings, 
we chose a human (to play the role of the 
Wizard) who had not participated in the 
meetings before, and thus had little prior 
knowledge of the meetings? contents.  
2.2 Notes Suggestion Interface 
In order to suggest notes to meeting participants 
during a meeting ? either automatically or 
through a Wizard ? we have modified the 
SmartNotes system, whose meeting recording 
and note-taking features have been described 
earlier in (Banerjee & Rudnicky, 2007). Briefly, 
each meeting participant comes to the meeting 
with a laptop running SmartNotes. At the begin-
ning of the meeting, each participant?s Smart-
Notes client connects to a server, authenticates 
the participant and starts recording and transmit-
ting his speech to the server. In addition, Smart-
Notes also provides meeting participants with a 
note-taking interface that is split into two major 
panes. In the ?notes? pane the participant types 
his notes that are then recorded for research pur-
poses. In the ?suggestions? pane, Wizard-
suggested notes are displayed. If at any time dur-
ing the meeting a participant double-clicks on 
one of the suggested notes in the ?suggestions? 
pane, its text gets included in his notes in the 
?notes? pane. The Wizard uses a different appli-
cation to select real-time utterance transcriptions, 
and insert them into each participant?s ?sugges-
tions? pane. (While we also experimented with 
having the Wizard target his suggestions at indi-
vidual participants, we do not report on those 
experiments here; those results were similar to 
the ones presented below.)  
2.3 Results 
We conducted the Wizard of Oz study on 9 
meetings that all belonged to the same sequence. 
That is, these meetings featured a largely over-
lapping group of participants who met weekly to 
discuss progress on a single project. The same 
person played the role of the Wizard in each of 
these 9 meetings. The meetings were on average 
33 minutes long, and there were 3 to 4 partici-
pants in each meeting. Although we have not 
evaluated the accuracy of the speech recognizer 
on these particular meetings, the typical average 
word error rate for these speakers is around 0.4 ? 
i.e., 4 out of 10 words are incorrectly transcribed.  
On average, the Wizard suggested the contents 
of 7 utterances to the meeting participants, for a 
total of 63 suggestions across the 9 meetings. Of 
these 63 suggestions, 22 (34.9%) were accepted 
by the participants and included in their notes. 
Thus on average, about 2.5 Wizard-suggested 
notes were accepted and included in participants? 
notes in each meeting. On average, meeting par-
ticipants took a total of 5.9 lines of notes per 
72
meeting; thus, 41.5% of the notes in each meet-
ing were Wizard-suggested.  
It cannot be ascertained if the meeting partici-
pants would have written the suggested notes on 
their own if they weren?t suggested to them. 
However the fact that some Wizard-suggested 
notes were accepted implies that the participants 
probably saw some value in including those sug-
gestions in their notes. Further, there was no 
drop-off in the fraction of meeting notes that was 
Wizard-suggested: the per-meeting average per-
centage of notes that was Wizard-suggested was 
around 41% for both the first 4 meetings, as well 
as the last 5. This implies that despite a seeming-
ly low acceptance rate (35%), participants did 
not ?give up? on the suggestions, but continued 
to make use of them over the course of the 9-
meeting meeting sequence. We conclude that an 
extractive summarization system that detects 
noteworthy utterances and suggests them to 
meeting participants can be perceived as useful 
by the participants, if the detection of noteworthy 
utterances is ?accurate enough?. 
3 Meeting Data Used in this Paper 
Assuming the feasibility of an extraction-based 
notes suggestion system, we turn our attention to 
developing a system that can automatically 
detect the noteworthiness of an utterance. Our 
goal here is to learn to do this task over a se-
quence of related meetings. Towards this end, we 
have recorded sequences of natural meetings ? 
meetings that would have taken place even if 
they weren?t being recorded. Meetings in each 
sequence featured largely overlapping participant 
sets and topics of discussion. For each meeting, 
we used SmartNotes (Banerjee & Rudnicky, 
2007) (described in section 2 above) to record 
both the audio from each participant as well as 
his notes. The audio recording and the notes 
were both time stamped, associated with the par-
ticipant?s identity, and uploaded to the meeting 
server. After the meeting was completed the au-
dio was manually segmented into utterances and 
transcribed both manually and using a speech 
recognizer (more details in section 5.2).  
In this paper we use a single sequence of 6 
meetings held between April and June of 2006. 
(These were separate from the ones used for the 
Wizard of Oz study above.) The meetings were 
on average 28 minutes and 43 seconds long (? 3 
minutes and 48 seconds standard error) counting 
from the beginning of the first recorded utterance 
to the end of the last one. On average each meet-
ing had 28 minutes and 38 seconds of speech ? 
this includes overlapped speech when multiple 
participants spoke on top of each other. Across 
the 6 meetings there were 5 unique participants; 
each meeting featured between 2 and 4 of these 
participants (average: 3.5 ? 0.31).  
The meetings had, on average, 633.67 (? 
85.60) utterances each, for a total of 3,796 utter-
ances across the 6 meetings. (In this paper, these 
3,796 utterances form the units of classification.) 
As expected, utterances varied widely in length. 
On average, utterances were 2.67 ? 0.18 seconds 
long and contained 7.73 (? 0.44) words.  
4 Multilevel Noteworthiness Annotation 
In order to develop approaches to automatically 
identify noteworthy utterances, we have manual-
ly annotated each utterance in the meeting data 
with its degree of ?noteworthiness?. While re-
searchers in the related field of speech summari-
zation typically use a binary labeling ? ?in sum-
mary? versus ?out of summary? (e.g. (Galley, 
2006), (Liu & Liu, 2008), (Penn & Zhu, 2008), 
etc) ? we have observed that there are often 
many utterances that are ?borderline? at best, and 
the decision to label them as ?in summary? or 
?out? is arbitrary. Our approach instead has been 
to create three levels of noteworthiness. Doing so 
allows us to separate the ?clearly noteworthy? 
utterances from the ?clearly not noteworthy?, 
and to label the rest as being between these two 
classes. (Of course, arbitrary choices must still 
be made between the edges of these three 
classes. However, having three levels preserves 
more information in the labels than having two, 
and it is always possible to create two labels 
from the three, as we do in later sections.)  
These multilevel noteworthiness annotations 
were done by two annotators. One of them ?
denoted as ?annotator 1? ? had attended each of 
the meetings, while the other ? ?annotator 2? ? 
had not attended any of the meetings. Although 
annotator 2 was given a brief overview of the 
general contents of the meetings, his understand-
ing of the meeting was expected to be lower than 
that of the other annotator. By using such an an-
notator, our aim was to identify utterances that 
were ?obviously noteworthy? even to a human 
being who lacks a deep understanding of the con-
text of the meetings. (In section 5.2 we describe 
how we merge the two sets of annotations.)  
The annotators were asked to make a 3-level 
judgment about the relative noteworthiness of 
each utterance. That is, for each utterance, the 
73
annotators were asked to decide whether a note-
suggestion system should ?definitely show? the 
contents of the utterance to the meeting partici-
pants, or definitely not show (labeled as ?don?t 
show?). Utterances that did not quite belong to 
either category were asked to be labeled as 
?maybe show?. Utterances labeled ?definitely 
show? were thus at the highest level of notewor-
thiness, followed by those labeled ?maybe show? 
and those labeled ?don?t show?.  Note that we 
did not ask the annotators to label utterances di-
rectly in terms of noteworthiness. Anecdotally, 
we have observed that asking people to label ut-
terances with their noteworthiness leaves the task 
insufficiently well defined because the purpose 
of the labels is unclear. On the other hand, asking 
users to identify utterances they would have in-
cluded in their notes leads to annotators taking 
into account the difficulty of writing particular 
notes, which is also not desirable for this set of 
labels. Instead, we asked annotators to directly 
perform (in some sense) the task that the even-
tual notes-assistance system will perform. 
In order to gain a modicum of agreement in 
the annotations, the two annotators discussed 
their annotation strategies after annotating each 
of the first two meetings (but not after the later 
meetings). A few general annotation patterns 
emerged, as follows: Utterances labeled 
?definitely show? typically included: 
(a) Progress on action items since the last week.  
(b) Concrete plans of action for the next week.  
(c) Announcements of deadlines. 
(d) Announcements of bugs in software, etc. 
In addition, utterances that contained the crux 
of any seemingly important discussion were 
labeled as ?definitely show?. On the other hand, 
utterances that contained no information worth 
including in the notes (by the annotators? 
judgment) were labeled as ?don?t show?. 
Utterances that did contain some additional 
elaborations of the main point, but without which 
the main point could still be understood by future 
readers of the notes were typically labeled as 
?maybe show?. 
Table 1 shows the distribution of the three la-
bels across the full set of 3,796 utterances in the 
dataset for both annotators. Both annotators la-
beled only a small percentage of utterances as 
?definitely show?, a larger fraction as ?maybe 
show? and most utterances as ?don?t show?. Al-
though the annotators were not asked to shoot for 
a certain distribution, observe that they both la-
beled a similar fraction of utterances as ?definite-
ly show?. On the other hand, annotator 2, who 
did not attend the meetings, labeled 50% more 
utterances as ?maybe show? than annotator 1 
who did attend the meetings. This difference is 
likely due to the fact that annotator 1 had a better 
understanding of the utterances in the meeting, 
and was more confident in labeling utterances as 
?don?t show? than annotator 2 who, not having 
attended the meetings, was less sure of some ut-
terances, and thus more inclined to label them as 
?maybe show?.  
 
Annotator 
# 
Definitely 
show 
Maybe 
show 
Don?t 
show 
1 13.5% 24.4% 62.1% 
2 14.9% 38.8% 46.3% 
Table 1: Distribution of Labels for Each Annotator 
4.1 Inter-Annotator Kappa Agreement 
To gauge the level of agreement between the two 
annotators, we compute the Kappa score. Given 
labels from different annotators on the same data, 
this metric quantifies the difference between the 
observed agreement between the labels and the 
expected agreement, with larger values denoting 
stronger agreement.  
For the 3-way labeling task, the two annota-
tors achieve a Kappa agreement score of 0.44 (? 
0.04). This seemingly low number is typical of 
agreement scores obtained in meeting summari-
zation. (Liu & Liu, 2008) reported Kappa agree-
ment scores between 0.11 and 0.35 across 6 an-
notators while (Penn & Zhu, 2008) with 3 anno-
tators achieved Kappa of 0.383 and 0.372 on ca-
sual telephone conversations and lecture speech. 
(Galley, 2006) reported inter-annotator agree-
ment of 0.323 on data similar to ours. 
To further understand where the disagree-
ments lie, we converted the 3-way labeled data 
into 2 different 2-way labeled datasets by merg-
ing two labels into one. First we evaluate the de-
gree of agreement the annotators have in separat-
ing utterances labeled ?definitely show? from the 
other two levels. We do so by re-labeling all ut-
terances not labeled ?definitely show? with the 
label ?others?. For the ?definitely show? versus 
?others? labeling task, the annotators achieve an 
inter-annotator agreement of 0.46. Similarly we 
compute the agreement in separating utterances 
labeled ?do not show? from the two other labels 
? in this case the Kappa value is 0.58. This im-
plies that it is easier to agree on the separation 
between ?do not show? and the other classes, 
than between ?definitely show? and the other 
classes.  
74
4.2 Inter-Annotator Accuracy, Prec/Rec/F 
Another way to gauge the agreement between the 
two sets of annotations is to compute accuracy, 
precision, recall and f-measure between them. 
That is, we can designate one annotator?s labels 
as the ?gold standard?, and use the other annota-
tor?s labels to find, for each of the 3 labels, the 
number of utterances that are true positives, false 
positives, and false negatives. Using these num-
bers we can compute precision as the ratio of 
true positives to the sum of true and false posi-
tives, recall as the ratio of true positives to the 
sum of true positives and false negatives, and f-
measure as the harmonic mean of precision and 
recall. (Designating the other annotator?s labels 
as ?gold standard? simply swaps the precision 
and recall values, and keeps f-measure the same). 
Accuracy is the number of utterances that have 
the same label from the two annotators, divided 
by the total number of utterances.  
Table 2 shows the evaluation over the 6-
meeting dataset using annotator 1?s data as ?gold 
standard?. The standard error for each cell is less 
than 0.08. Observe in Table 2 that while both the 
?definitely show? and ?maybe show? classes 
have nearly equal f-measure, the precision and 
recall values for the ?maybe show? class are 
much farther apart from each other than those for 
the ?definitely show? class. This is due to the 
fact that while both annotators label a similar 
number of utterances as ?definitely show?, they 
label very different numbers of utterances as 
?maybe show?. If the same accuracy, precision, 
recall and f-measure scores are computed for the 
?definitely show? vs. ?others? split, the accuracy 
jumps to 87%, possibly because of the small size 
of the ?definitely show? category. The accuracy 
remains at 78% for the ?don?t show? vs. ?others? 
split.  
 
 Definitely 
show 
Maybe 
show 
Don?t 
show 
Precision 0.57 0.70 0.70 
Recall 0.53  0.46 0.93 
F-measure 0.53  0.54 0.80 
Accuracy 69% 
Table 2 Inter-Annotator Agreement using Accuracy Etc.  
4.3 Inter-Annotator Rouge Scores 
Annotations can also be evaluated by computing 
the ROUGE metric (Lin, 2004). ROUGE, a pop-
ular metric for summarization tasks, compares 
two summaries by computing precision, recall 
and f-measure over ngrams that overlap between 
them. Following previous work on meeting 
summarization (e.g. (Xie, Liu, & Lin, 2008), 
(Murray, Renals, & Carletta, 2005), etc), we re-
port evaluation using ROUGE-1 F-measure, 
where the value ?1? implies that overlapping un-
igrams are used to compute the metric. Unlike 
previous research that had one summary from 
each annotator per meeting, our 3-level annota-
tion allows us to have 2 different summaries: (a) 
the text of all the utterances labeled ?definitely 
show? and, (b) the text of all the utterances la-
beled either ?definitely show? or ?maybe show?.  
On average (across both annotators over the 6 
meetings) the ?definitely show? utterance texts 
are 18.72% the size of the texts of all the utter-
ances in the meetings, while the ?definitely or 
maybe show? utterance texts are 61.6%. Thus, 
these two texts represent two distinct points on 
the compression scale. The average R1 F-
measure score is 0.62 over the 6 meetings when 
comparing the ?definitely show? texts of the two 
annotators. This is twice the R1 score ? 0.3 ? of 
the trivial baseline of simply labeling every ut-
terance as ?definitely show?. The inter-annotator 
R1 F-measure for the ?definitely or maybe show? 
texts is 0.79, marginally higher than the trivial 
?all utterances? baseline of 0.71. In the next sec-
tion, we compare the scores achieved by the au-
tomatic system against these inter-annotator and 
trivial baseline scores.  
5 Automatic Label Prediction  
So far we have presented the annotation of the 
meeting data, and various analyses thereof. In 
this section we present our approach for the 
automatic prediction of these labels. We apply a 
classification based approach to the problem of 
predicting the noteworthiness level of an 
utterance, similar to (Banerjee & Rudnicky, 
2008). We use leave-one-meeting-out cross 
validation: for each meeting m, we train the 
classifier on manually labeled utterances from 
the other 5 meetings, and test the classifier on the 
utterances of meeting m. We then average the 
results across the 6 meetings. Given the small 
amount of data, we do not test on separate data, 
nor do we perform any tuning.  
Using the 3-level annotation described above, 
we train a 3-way classifier to label each utterance 
with one of the multilevel noteworthiness labels. 
In addition, we use the two 2-way merged-label 
annotations ? ?definitely show? vs. others and 
?don?t show? vs. others ? to train two more 2-
way classifiers. In each of these classification 
75
problems we use the same set of features and the 
same classification algorithms described below.  
5.1 Features Used 
Ngram features: As has been shown by 
(Banerjee & Rudnicky, 2008), the strongest 
features for noteworthiness detection are ngram 
features, i.e. features that capture the occurrence 
of ngrams (consecutive occurrences of one or 
more words) in utterances. Each ngram feature 
represents the presence or absence of a single 
specific ngram in an utterance. E.g., the ngram 
feature ?action item? represents the occurrence 
of the bigram ?action item? in a given utterance. 
Unlike (Banerjee & Rudnicky, 2008) where each 
ngram feature captured the frequency of a 
specific ngram in an utterance, in this paper we 
use boolean-valued ngram features to capture the 
presence/absence of ngrams in utterances. We do 
so because in tests on separate data, boolean-
valued features out-performed frequency-based 
features, perhaps due to data sparseness. Before 
ngram features are extracted, utterances are 
normalized: partial words, non-lexicalized filler 
words (like ?umm?, ?uh?), punctuations, 
apostrophes and hyphens are removed, and all 
remaining words are changed to upper case. Next, 
the vocabulary of ngrams is defined as the set of 
ngrams that occur at least 5 times in the entire 
dataset of meetings, for ngram sizes of 1 through 
6 word tokens. Finally, the occurrences of each 
of these vocabulary ngrams in an utterance are 
recorded as the feature vector for that utterance. 
In the dataset used in this paper, there are 694 
unique unigrams that occur at least 5 times 
across the 6 meetings, 1,582 bigrams, 1,065 
trigrams, 1,048 4-grams, 319 5-grams and 102 6-
grams. In addition to these ngram features, for 
each utterance we also include the number of Out 
of Vocabulary ngram ? ngrams that occur less 
than 5 times across all the meetings.  
Overlap-based Features: We assume that we 
have access to the text of the agenda of the test 
meeting, and also the text of the notes taken by 
the participants in previous meetings (but not 
those taken in the test meeting). Since these 
artifacts are likely to contain important keywords 
we compute two sets of overlaps features. In the 
first set we compute the number of ngrams that 
overlap between each utterance and the meeting 
agenda. That is, for each utterance we count the 
number of unigrams, bigrams, trigrams, etc that 
also occur in the agenda of that meeting. 
Similarly in the second set we compute the 
number of ngrams in each utterance that also 
occur in the notes of previous meetings. Finally, 
we compute the degree of overlap between this 
utterance and other utterances in the meeting. 
The motivation for this last feature is to find 
utterances that are repeats (or near-repeats) of 
other utterances ? repetition may correlate with 
importance.  
Other features: In addition to the ngram and 
ngram overlap features, we also include term 
frequency ? inverse document frequency (tf-idf) 
features to capture the information content of the 
ngrams in the utterance. Specifically we compute 
the TF-IDF of each ngram (of sizes 1 through 5) 
in the utterance, and include the maximum, 
minimum, average and standard deviation of 
these values as features of the utterance. We also 
include speaker-based features to capture who is 
speaking when. We include the identity of the 
speaker of the current utterance and those of the 
previous and next utterances as features. Lastly 
we include the length of the utterance (in seconds) 
as a feature.  
5.2 Evaluation Results 
In this paper we use a Support Vector Machines-
based classifier, which is a popular choice for 
extractive meeting summarization, e.g. (Xie, Liu, 
& Lin, 2008); we use a linear kernel in this pa-
per. In the results reported here we use the output 
of the Sphinx speech recognizer, using speaker-
independent acoustic models, and language mod-
els trained on publicly available meeting data. 
The word error rate was around 44% ? more  
details of the speech recognition process are in 
(Huggins-Daines & Rudnicky, 2007). For train-
ing purposes, we merged the annotations from 
the two annotators by choosing a ?middle or 
lower ground? for all disagreements. Thus, if for 
an utterance the two labels are ?definitely show? 
and ?don?t show?, we set the merged label as the 
middle ground of ?maybe show?. On the other 
hand if the two labels were on adjacent levels, 
we chose the lower one ? ?maybe show? when 
the labels were ?definitely show? and ?maybe 
show?, and ?don?t show? when the labels were 
?maybe show? and ?don?t show?. Thus only ut-
terances that both annotators labeled as ?definite-
ly show? were also labeled as ?definitely show? 
in the merged annotation. We plan to try other 
merging strategies in the future. For testing, we 
evaluated against each annotator?s labels sepa-
rately, and averaged the results. 
 
 
 
76
 Definitely 
show 
Maybe 
show 
Don?t 
show 
Precision 0.21 0.47 0.72 
Recall 0.16  0.40 0.79 
F-measure 0.16  0.43 0.75 
Accuracy 61.4% 
Table 3 Results of the 3-Way Classification 
Table 3 presents the accuracy, precision, recall 
and f-measure results of the 3-way classification 
task. (We use the Weka implementation of SVM 
that internally devolves the 3-way classification 
task into a sequence of pair-wise classifications. 
We use the final per-utterance classification 
here.) Observe that the overall accuracy of 
61.4% is only 11% lower relative to the accuracy 
obtained by comparing the two annotators? anno-
tations (69%, Table 2). However, the precision, 
recall and f-measure values for the ?definitely 
show? class are substantially lower for the pre-
dicted labels than the agreement between the two 
annotators. The numbers are closer for the ?may-
be show? and the ?don?t show? classes. This im-
plies that it is more difficult to accurately detect 
utterances labeled ?definitely show? than it is to 
detect the other classes. One reason for this dif-
ference is the size of each utterance class. Utter-
ances labeled ?definitely show? are only around 
14% of all utterances, thus there is less data for 
this class than the others. We also ran the algo-
rithm using manually transcribed data, and found 
improvement in only the ?Definitely show? class 
with an f-measure of 0.21. This improvement is 
perhaps because the speech recognizer is particu-
larly prone to getting names and other technical 
terms wrong, which may be important clues of 
noteworthiness. 
Table 4 presents the ROUGE-1 F-measure 
scores averaged over the 6 meetings. (ROUGE is 
described briefly in section 4.3 and in detail in 
(Lin, 2004)). Similar to the inter-annotator 
agreement computations, we computed ROUGE 
between the text of the utterances labeled ?defi-
nitely show? by the system against that of utter-
ances labeled ?definitely show? by the two anno-
tators. (We computed the scores separately 
against each of the annotators in turn and then 
averaged the two values.) We did the same thing 
for the set of utterances labeled either ?definitely 
show? or ?maybe show?. Observe that the R1-F 
score for the ?definitely show? comparison is 
nearly 50% relative higher than the trivial base-
line of labeling every utterance as ?definitely 
show?. However the score is 30% lower than the 
corresponding inter-annotator agreement. The 
corresponding R1-Fmeasure score using manual 
transcriptions is only marginally better ? 0.47. 
The set of utterances labeled either definitely or 
maybe shows (second row of table 4) does not 
outperform the all-utterances baseline when us-
ing automatic transcriptions, but does so with 
manual transcriptions, whose R1-F value is 0.74.  
 
Comparing What R1-Fmeasure 
Definitely show 0.43 
Definitely or maybe show 0.63 
Table 4 ROUGE Scores for the 3-Way Classification 
These results show that while the detection of 
definitely show utterances is better than the trivi-
al baselines even when using automatic tran-
scriptions, there is a lot of room for improve-
ment, as compared to human-human agreement. 
Although direct comparisons to other results 
from the meeting summarization literature are 
difficult because of the difference in the datasets, 
numerically it appears that our results are similar 
to those obtained previously. (Xie, Liu, & Lin, 
2008) uses Rouge-1 F-measure solely, and 
achieve scores between 0.6 to 0.7. (Murray, 
Renals, & Carletta, 2005) also achieve Rouge-1 
scores in the same range with manual transcripts.  
The trend in the results for the two 2-way clas-
sifications is similar to the trend for the inter an-
notator agreements. Just as inter-annotator accu-
racy increased to 87% for the ?definitely show? 
vs. ?others? classification, so does accuracy of 
the predicted labels increase to 88.3%. The f-
measure for the ?definitely show? class falls to 
0.13, much lower than the inter-annotator f-
measure of 0.53. For the ?don?t show? vs. ?oth-
ers? classification, the automatic system achieves 
an accuracy of 66.6%. For the ?definitely plus 
maybe? class, the f-measure is 0.59, which is 
22% relatively lower than the inter-annotator f-
measure for that class. (As with the 3-way classi-
fication, these results are all slightly worse than 
those obtained using manual transcriptions.) 
5.3 Useful Features 
In order to understand which features contribute 
most to these results, we used the Chi-Squared 
test of association to find features that are most 
strongly correlated to the 3 output classes. The 
best features are those that measure word over-
laps between the utterances and the text in the 
agenda labels and the notes in previous meetings. 
This is not a surprising finding ? the occurrence 
of an ngram in an agenda label or in a previous 
note is highly indicative of its importance, and 
77
consequently that of the utterances that contain 
that ngram. Max and average TF-IDF scores are 
also highly ranked features. These features score 
highly for utterances with seldom-used words, 
signifying the importance of those utterances. 
Domain independent ngrams such as ?action 
item? are strongly correlated with noteworthiness, 
as are a few domain dependent ngrams such as 
?time shift problem?. These latter features 
represent knowledge that is transferred from ear-
lier meetings to latter ones in the same sequence. 
The identity of the speaker of the utterance does 
not seem to correlate well with the utterance?s 
noteworthiness, although this finding could 
simply be an artifact of this particular dataset. 
6 Related Work  
Noteworthiness detection is closely related to 
meeting summarization. Extractive techniques 
are popular, e.g. (Murray, Renals, & Carletta, 
2005), and many algorithms have been attempted 
including SVMs (Xie, Liu, & Lin, 2008), Gaus-
sian Mixture Models and Maximal Marginal Re-
levance (Murray, Renals, & Carletta, 2005), and 
sequence labelers (Galley, 2006). Most ap-
proaches use a mixture of ngram features, and 
other structural and semantic features ? a good 
evaluation of typical features can be found in 
(Xie, Liu, & Lin, 2008). Different evaluation 
techniques have also been tried, with ROUGE 
often being shown as at least adequate (Liu & 
Liu, 2008). Our work is an application and ex-
tension of the speech summarization field to the 
problem of assistive note-taking.  
7 Conclusions and Future Work  
In our work we investigated the problem of de-
tecting the noteworthiness of utterances pro-
duced in meetings. We conducted a Wizard-of-
Oz-based user study to establish the usefulness 
of extracting the text of utterances and suggest-
ing these as notes to the meeting participants. We 
showed that participants were willing to accept 
about 35% of these suggestions over a sequence 
of 9 meetings. We then presented a 3-level note-
worthiness annotation scheme that breaks with 
the tradition of 2-way ?in/out of summary? anno-
tation. We showed that annotators have strong 
agreement for separating the highest level of 
noteworthiness from the other levels. Finally we 
used these annotations as labeled data to train a 
Support Vector Machine-based classifier which 
performed better than trivial baselines but not as 
well as inter-annotator agreement levels.  
     For future work, we plan to use automatic 
noteworthiness predictions to suggest notes to 
meeting participants during meetings. We are 
also interested in training the noteworthiness de-
tector directly from the notes that participants 
took in previous meetings, thus reducing the 
need for manually annotated data. 
Reference 
Banerjee, S, and A. I. Rudnicky. "Segmenting Meet-
ings into Agenda Items by Extracting Implicit Su-
pervision from Human Note-Taking." Proceedings 
of the International Conference on Intelligent User 
Interfaces. Honolulu, HI, 2007. 
Banerjee, Satanjeev, and A. I. Rudnicky. "An Extrac-
tive-Summarization Baseline for the Automatic 
Detection of Noteworthy Utterances in Multi-Party 
Human-Human Dialog." IEEE Workshop on Spo-
ken Language Technology. Goa, India, 2008. 
Galley, Michel. "A Skip-Chain Conditional Random 
Field for Ranking Meeting Utterances by Impor-
tance." Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing. Syd-
ney, Australia, 2006. 
Huggins-Daines, David, and A. I. Rudnicky. "Impli-
citly Supervised Language Model Adaptation for 
Meeting Transcription." Proceedings of the HLT-
NAACL. Rochester, NY. 2007. 
Lin, Chin-Yew. "ROUGE: A Package for Automatic 
Evaluation of Summaries." Proceedings of the 
ACL-04 Workshop: Text Summarization Branches 
Out. Barcelona, Spain: Association for Computa-
tional Linguistics, 2004. 74-81. 
Liu, Feifan, and Y. Liu. "Correlation between 
ROUGE and Human Evaluation of Extractive 
Meeting Summaries." Proceedings of ACL-HLT. 
Columbus, OH, 2008. 
Murray, Gabriel, S. Renals, and J. Carletta. "Extrac-
tive Summarization of Meeting Recordings." Pro-
ceedings of Interspeech. Lisbon, Portugal, 2005. 
Penn, Gerald, and X. Zhu. "A Critical Reassessment 
of Evaluation Baselines for Speech Summariza-
tion." Proceedings of ACL-HLT. Columbus, OH, 
2008.  
Xie, Shasha, Y. Liu, and H. Lin. "Evaluating the Ef-
fectiveness of Features and Sampling in Extractive 
Meeting Summarization." IEEE Workshop on 
Spoken Language Technology. Goa, India, 2008. 
78
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 99?107,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using the Amazon Mechanical Turk to Transcribe and  
Annotate Meeting Speech for Extractive Summarization 
Matthew Marge   Satanjeev Banerjee   Alexander I. Rudnicky  
School of Computer Science, Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
{mrmarge,banerjee,air}@cs.cmu.edu 
 
Abstract 
Due to its complexity, meeting speech pro-
vides a challenge for both transcription and 
annotation. While Amazon?s Mechanical Turk 
(MTurk) has been shown to produce good re-
sults for some types of speech, its suitability 
for transcription and annotation of spontane-
ous speech has not been established. We find 
that MTurk can be used to produce high-
quality transcription and describe two tech-
niques for doing so (voting and corrective). 
We also show that using a similar approach, 
high quality annotations useful for summari-
zation systems can also be produced. In both 
cases, accuracy is comparable to that obtained 
using trained personnel.  
1 Introduction 
Recently, Amazon?s Mechanical Turk (MTurk) has 
been shown to produce useful transcriptions of 
speech data; Gruenstein et al (2009) have success-
fully used MTurk to correct the transcription out-
put from a speech recognizer, while Novotney and 
Callison-Burch (2010) used MTurk for transcrib-
ing a corpus of conversational speech. These stu-
dies suggest that transcription, formerly considered 
to be an exacting task requiring at least some train-
ing, could be carried out by casual workers. How-
ever, only fairly simple transcription tasks were 
studied.  
 We propose to assess the suitability of MTurk 
for processing more challenging material, specifi-
cally recordings of meeting speech. Spontaneous 
speech can be difficult to transcribe because it may 
contain false starts, disfluencies, mispronunciations 
and other defects. Similarly for annotation, meet-
ing content may be difficult to follow and conven-
tions difficult to apply consistently.  
 Our first goal is to ascertain whether MTurk 
transcribers can accurately transcribe spontaneous 
speech, containing speech errors and of variable 
utterance length.  
 Our second goal is to use MTurk for creating 
annotations suitable for extractive summarization 
research, specifically labeling each utterance as 
either ?in-summary? or ?not in-summary?. Among 
other challenges, this task cannot be decomposed 
into small independent sub-tasks?for example, 
annotators cannot be asked to annotate a single 
utterance independent of other utterances. To our 
knowledge, MTurk has not been previously ex-
plored for the purpose of summarization annota-
tion.  
2 Meeting Speech Transcription Task 
We recently explored the use of MTurk for tran-
scription of short-duration clean speech (Marge et 
al., 2010) and found that combining independent 
transcripts using ROVER yields very close agree-
ment with a gold standard (2.14%, comparable to 
expert agreement). But simply collecting indepen-
dent transcriptions seemed inefficient: the ?easy? 
parts of each utterance are all transcribed the same. 
In the current study our goal is determine whether 
a smaller number of initial transcriptions can be 
used to identify easy- and difficult-to-transcribe 
regions, so that the attention of subsequent tran-
scribers can be focused on the more difficult re-
gions.  
2.1 Procedure 
In this corrective strategy for transcription, we 
have two turkers to independently produce tran-
scripts. A word-level minimum edit distance me-
tric is then used to align the two transcripts and 
locate disagreements. These regions are replaced 
with underscores, and new turkers are asked to 
transcribe those regions.  
Utterances were balanced for transcription dif-
ficulty (measured by the native English back-
99
ground of the speaker and utterance length). For 
the first pass transcription task, four sets of jobs 
were posted for turkers to perform, with each pay-
ing $0.01, $0.02, $0.04, or $0.07 per approved 
transcription. Payment was linearly scaled with the 
length of the utterance to be transcribed at a rate of 
$0.01 per 10 seconds of speech, with an additional 
payment of $0.01 for providing feedback. In each 
job set, there were 12 utterances to be transcribed 
(yielding a total of 24 jobs available given two 
transcribers per utterance). Turkers were free to 
transcribe as many utterances as they could across 
all payment amounts. 
After acquiring two transcriptions, we aligned 
them, identified points of disagreement and re-
posted the transcripts and the audio as part of a 
next round of job sets. Payment amounts were kept 
the same based on utterance length. In this second 
pass of transcriptions, three turkers were recruited 
to correct and amend each transcription. Thus, a 
total of five workers worked on every transcription 
after both iterations of the corrective task. In our 
experiment 23 turkers performed the first phase of 
the task, and 28 turkers the corrective task (4 
workers did both passes).  
2.2 First and Second Pass Instructions 
First-pass instructions asked turkers to listen to 
utterances with an embedded audio player pro-
vided with the HIT. Turkers were instructed to 
transcribe every word heard in the audio and to 
follow guidelines for marking speaker mispronun-
ciations and false starts. Filled pauses (?uh?, ?um?, 
etc.) were not to be transcribed in the first pass. 
Turkers could replay the audio as many times as 
necessary. 
In the second pass, turkers were instructed to 
focus on the portions of the transcript marked with 
underscores, but also to correct any other words 
they thought were incorrect. The instructions also 
asked turkers to identify three types of filler words: 
?uh?, ?um?, and ?lg? (laughter). We selected this 
set since they were the most frequent in the gold 
standard transcripts. Again, turkers could replay 
the audio.  
2.3 Speech Corpus 
The data were sampled from a previously-collected 
corpus of natural meetings (Banerjee and Rud-
nicky, 2007). The material used in this paper 
comes from four speakers, two native English 
speakers and two non-Native English speakers (all 
male). We selected 48 audio clips; 12 from each of 
the four speakers. Within each speaker's set of 
clips, we further divided the material into four 
length categories: ~5, ~10, ~30 and ~60 sec. The 
speech material is conversational in nature; the 
gold standard transcriptions of this data included 
approximately 15 mispronunciations and 125 false 
starts. Table 1 presents word count information 
related to the utterances in each length category. 
 
Utterance 
Length 
Word Count 
(mean) 
Standard  
Deviation 
Utterance 
Count 
5 sec 14  5.58 12  
10 sec 24.5  7.26 12  
30 sec 84  22.09 12  
60 sec 146.6  53.17 12  
 Table 1. Utterance characteristics. 
3 Meeting Transcription Analysis 
Evaluation of first and second pass corrections was 
done by calculating word error rate (WER) with a 
gold standard, obtained using the transcription 
process described in (Bennett and Rudnicky, 
2002). Before doing so, we normalized the candi-
date MTurk transcriptions as follows: spell-
checking (with included domain-specific technical 
terms), and removal of punctuation (periods, com-
mas, etc.). Apostrophes were retained. 
 
Table 2. WER across transcription iterations. 
3.1 First-Pass Transcription Results 
Results from aligning our first-pass transcriptions 
with a gold standard are shown in the second col-
umn of Table 2. Overall error rate was 23.8%, 
which reveals the inadequacy of individual turker 
transcriptions, if no further processing is done. 
(Remember that first-pass transcribers were asked 
to leave out fillers even though the gold standard 
contained them, thus increasing WER). 
Utterance 
Length 
First-Pass 
WER 
Second-Pass 
WER 
ROVER-3 
WER 
5 sec. 31.5% 19.8% 15.3% 
10 sec. 26.7% 20.3% 13.8% 
30 sec. 20.8% 16.9% 15.0% 
60 sec. 24.3% 17.1% 15.4% 
Aggregate 23.8% 17.5% 15.1% 
100
In this first pass, speech from non-native speak-
ers was transcribed more poorly (25.4% WER) 
than speech from native English speakers (21.7% 
WER). In their comments sections, 17% of turkers 
noted the difficulty in transcribing non-native 
speakers, while 13% found native English speech 
difficult. More than 80% of turkers thought the 
amount of work ?about right? for the payment re-
ceived.  
3.2 Second-Pass Transcription Results 
The corrective process greatly improved agreement 
with our expert transcriptions. Aggregate WER 
was reduced from 23.8% to 17.5% (27% relative 
reduction) when turkers corrected initial transcripts 
with highlighted disagreements (third column of 
Table 2). In fact, transcriptions after corrections 
were significantly more accurate than initial tran-
scriptions (F(1, 238) = 13.4, p < 0.05). With re-
spect to duration, the WER of the 5-second utter-
ances had the greatest improvement, a relative re-
duction of WER by 37%.  Transcription alignment  
with the gold standard experienced a 39% im-
provement to 13.3% for native English speech, and 
a 19% improvement to 20.6% for non-native Eng-
lish speech (columns 2 and 3 of Table 3).  
 We found that 30% of turkers indicated that the 
second-pass correction task was difficult, as com-
pared with 15% for the first-pass transcription task. 
Work amount was perceived to be about right 
(85% of the votes) in this phase, similar to the first. 
3.3 Combining Corrected Transcriptions 
In order to improve the transcriptions further, we 
combined the three second-pass transcriptions of 
each utterance using ROVER?s word-level voting 
scheme (Fiscus, 1997). The WER of the resulting 
transcripts are presented in the fourth column of 
Table 2. Aggregate WER was further reduced by 
14% relative to 15.1%. This result is close to typ-
ical disagreement rates of 6-12% reported in the 
literature (Roy and Roy, 2009). The best im-
provements using ROVER were found with the 
transcriptions of the shorter utterances: WER 
from the second-pass of 5-second utterances tran-
scriptions was reduced by 23% to 15.3%. The 10-
second utterance transcriptions experienced the 
best improvement, 32%, to a WER of 13.8%. 
 Although segmenting audio into shorter seg-
ments may yield fast turnaround times, we found 
that utterance length is not a significant factor in 
determining alignment between combined, cor-
rected transcriptions and gold-standard transcrip-
tions (F(3, 44) = 0.16, p = 0.92). We speculate that 
longer utterances show good accuracy due to the 
increased context available to transcribers.  
Table 3. WER across transcription iterations based on 
speaker background. 
3.4 Error Analysis 
Out of 3,281 words (48 merged transcriptions of 
48 utterances), 496 were errors. Among the errors 
were 37 insertions, 315 deletions, and 144 substitu-
tions. Thus the most common error was to miss a 
word.  
 Further analysis revealed that two common cas-
es of errors occurred: the misplacement or exclu-
sion of filler words (even though the second phase 
explicitly instructed turkers to insert filler words) 
and failure to transcribe words considered to be out 
of the range of the transcriber?s vocabulary, such 
as technical terms and foreign names. Filler words 
accounted for 112 errors (23%). Removing fillers 
from both the combined transcripts and the gold 
standard improved WER by 14% relative to 
13.0%. Further, WER for native English speech 
transcriptions was reduced to 8.9%. This difference 
was however not statistically significant (F(1,94) = 
1.64, p = 0.2). 
 Turkers had difficulty transcribing uncommon 
words, technical terms, names, acronyms, etc. 
(e.g., ?Speechalyzer?, ?CTM?, ?PQs?). Investiga-
tion showed that at least 41 errors (8%) could be 
attributed to this out-of-vocabulary problem. It is 
unclear if there is any way to completely eradicate 
such errors, short of asking the original speakers. 
3.5 Comparison to One-Pass Approach 
Although the corrective model provides significant 
gain from individual transcriptions, this approach 
is logistically more complex. We compared it to 
our one-pass approach, in which five turkers inde-
pendently transcribe all utterances (Marge et al, 
2010). Five new transcribers per utterance were 
recruited for this task (yielding 240 transcriptions). 
Speaker 
Background 
First-Pass 
WER 
Second-
Pass WER 
ROVER-3 
WER 
Native  21.7% 13.3% 10.8% 
Non-native 25.4% 20.6% 18.4% 
101
Individual error rate was 24.0%, comparable to the 
overall error rate for the first step of the corrective 
approach (Table 2).  
 After combining all five transcriptions with 
ROVER, we found similar gains to the corrective 
approach: an overall improvement to 15.2% error 
rate. Thus both approaches can effectively produce 
high-quality transcriptions. We speculate that if 
higher accuracy is required, the corrective process 
could be extended to iteratively re-focus effort on 
the regions of greatest disagreement. 
3.6 Latency 
Although payment scaled with the duration of ut-
terances, we observed a consistent disparity in tur-
naround time. All HITs were posted at the same 
time in both iterations (Thursday afternoon, EST). 
Turkers were able to transcribe 48 utterances twice 
in about a day in the first pass for the shorter utter-
ances (5- and 10-second utterances), while it took 
nearly a week to transcribe the 30- and 60-second 
utterances. Turkers were likely discouraged by the 
long duration of the transcriptions compounded 
with the nature of the speech. To increase turna-
round time on lengthy utterances, we speculate that 
it may be necessary to scale payment non-linearly 
with length (or another measure of perceived ef-
fort). 
3.7 Conclusion 
Spontaneous speech, even in long segments, can 
indeed be transcribed on MTurk with a level of 
accuracy that approaches expert agreement rates 
for spontaneous speech. However, we expect seg-
mentation of audio materials into smaller segments 
would yield fast turnaround time, and may keep 
costs low. In addition, we find that ROVER works 
more effectively on shorter segments because 
lengths of candidate transcriptions are less likely to 
have large disparities. Thus, multiple transcriptions 
per utterance can be utilized best when their 
lengths are shorter.  
4 Annotating for Summarization  
4.1 Motivation 
Transcribing audio data into text is the first step 
towards making information contained in audio 
easily accessible to humans. A next step is to con-
dense the information in the raw transcription, and 
produce a short summary that includes the most 
important information. Good summaries can pro-
vide readers with a general sense of the meeting, or 
help them to drill down into the raw transcript (or 
the audio itself) for additional information. 
4.2 Annotation Challenges  
Unfortunately, summary creation is a difficult task 
because ?importance? is inherently subjective and 
varies from consumer to consumer. For example, 
the manager of a project, browsing a summary of a 
meeting, might be interested in all agenda items, 
whereas a project participant may be interested in 
only those parts of the meeting that pertain to his 
portion of the project.  
Despite this subjectivity, the usefulness of a 
summary is clear, and audio summarization is an 
active area of research. Within this field, two kinds 
of human annotations are generally created?
annotators are either asked to write a short sum-
mary of the audio, or they are asked to label each 
transcribed utterance as either ?in summary? or 
?out of summary?. The latter annotation is particu-
larly useful for training and evaluating extractive 
summarization systems?systems that create sum-
maries by selecting a subset of the utterances.  
Due to the subjectivity involved, we find very 
low inter-annotator agreement for this labeling 
task. Liu and Liu (2008) reported Kappa agreement 
scores of between 0.11 and 0.35 across 6 annota-
tors, Penn and Zhu (2008) reported 0.38 on tele-
phone conversation and 0.37 on lecture speech, 
using 3 annotators, and Galley (2006) reported 
0.32 on meeting data. Such low levels of agree-
ment imply that the resulting training data is likely 
to contain a great deal of ?noise??utterances la-
beled ?in summary? or ?out of summary?, when in 
fact they are not good examples of those classes. 
Disagreements arise due to the fact that utter-
ance importance is a spectrum.  While some utter-
ances are clearly important or unimportant, there 
are many utterances that lie between these ex-
tremes. In order to label utterances as either ?in-
summary? or not, annotators must choose an arbi-
trary threshold at which to make this decision. 
Simply asking annotators to provide a continuous 
?importance value? between 0 and 1 is also likely 
to be infeasible as the exact value for a given utter-
ance is difficult to ascertain. 
102
4.3 3-Class Formulation 
One way to alleviate this problem is to redefine the 
task as a 3-class labeling problem. Annotators can 
be asked to label utterances as either ?important?, 
?unimportant? or ?in-between?. Although this for-
mulation creates two decision boundaries, instead 
of the single one in the 2-class formulation, the 
expectation is that a large number of utterances 
with middling importance will simply be assigned 
to the ?in between? class, thus reducing the amount 
of noise in the data. Indeed we have shown (Baner-
jee and Rudnicky, 2009) that in-house annotators 
achieve high inter-annotator agreement when pro-
vided with the 3-class formulation. 
Another way to alleviate the problem of low 
agreement is to obtain annotations from many an-
notators, and identify the utterances that a majority 
of the annotators appear to agree on; such utter-
ances may be considered as good examples of their 
class. Using multiple annotators is typically not 
feasible due to cost. In this paper we investigate 
using MTurk to create 3-class-based summariza-
tion annotations from multiple annotators per 
meeting, and to combine and filter these annota-
tions to create high quality labels. 
5 Using Mechanical Turk for Annotations 
5.1 Challenges of Using Mechanical Turk 
Unlike some other tasks that require little or no 
context in order to perform the annotation, summa-
rization annotation requires a great deal of context. 
It is unlikely that an annotator can determine the 
importance of an utterance without being aware of 
neighboring utterances. Moreover, the appropriate 
length of context for a given utterance is likely to 
vary. Presenting all contiguous utterances that dis-
cuss the same topic might be appropriate, but 
would require manual segmentation of the meeting 
into topics. In this paper we experiment with show-
ing all utterances of a meeting. This is a challenge 
however, because MTurk is typically applied to 
quick low-cost tasks that need little context. It is 
unclear whether turkers would be willing to per-
form such a time-consuming task, even for higher 
payment. 
Another challenge for turkers is being able to 
understand the discussion well enough to perform 
the annotation. We experiment here with meetings 
that include significant technical content. While in-
house annotators can be trained over time to under-
stand the material well enough to perform the task, 
it is impractical to provide turkers with such train-
ing. We investigate the degree to which turkers can 
provide summarization annotation with minimal 
training.  
5.2 Data Used 
We selected 5 recorded meetings for our study. 
These meetings were not scripted?and would 
have taken place even if they weren?t being rec-
orded. They were project meetings containing dis-
cussions about software deliverables, problems, 
resolution plans, etc. The contents included tech-
nical jargon and concepts that non-experts are un-
likely to grasp by reading the meeting transcript 
alone.  
The 5 meetings had 2 to 4 participants each 
(mean: 3.5). For all meetings, the speech from each 
participant was recorded separately using head-
mounted close-talking microphones. We manually 
split these audio streams into utterances?ensuring 
that utterances did not have more than a 0.5 second 
pause in them, and then transcribed them using an 
established process (Bennett and Rudnicky, 2002). 
The meetings varied widely in length from 15 mi-
nutes and 282 utterances to 40 minutes and 948 
utterances (means: 30 minutes, 610 utterances). 
There were 3,052 utterances across the 5 meetings, 
each containing an mean of 7 words. The utter-
ances in the meetings were annotated using the 3-
class formulation by two in-house annotators. 
Their inter-annotator agreement is presented along 
with the rest of the evaluation results in Section 6. 
0
10
20
30
40
50
60
Important Neutral Unimportant
%
 o
f U
tt
er
an
ce
s
In-house Mturk
Figure 1. Label distribution of in-house and MTurk 
annotators. 
 
103
5.3 HIT Design and Instructions 
We instructed turkers to imagine that someone else 
(not them) was going to eventually write a report 
about the meeting, and it was their task to identify 
those utterances that should be included in the re-
port. We asked annotators to label utterances as 
?important? if they should be included in the report 
and ?unimportant? otherwise. In addition, utter-
ances that they thought were of medium impor-
tance and that may or may not need to be included 
in the report were to be labeled as ?neutral?. We 
provided examples of utterances in each of these 
classes. For the ?important? class, for instance, we 
included ?talking about a problem? and ?discuss-
ing future plan of action? as examples. For the ?un-
important? class, we included ?off topic joking?, 
and for the ?neutral? class ?minute details of an 
algorithm? was an example. 
In addition to these instructions and examples, 
we gave turkers a general guideline to the effect 
that in these meetings typically 1/4th of the utter-
ances are ?important?, 1/4th ?neutral? and the rest 
?unimportant?. As we discuss in section 6, it is 
unclear whether most turkers followed this guide-
line. 
Following these instructions, examples and tips, 
we provided the text of the utterances in the form 
of an HTML table. Each row contained a single 
utterance, prefixed with the name of the speaker. 
The row also contained three radio buttons for the 
three classes into which the annotator was asked to 
classify the utterance. Although we did not ensure 
that annotators annotated every utterance before 
submitting their work, we observed that for 95% of 
the utterances every annotator did provide a judg-
ment; we ignore the remaining 5% of the utter-
ances in our evaluation below. 
5.4 Number of Turkers and Payment 
For each meeting, we used 5 turkers and paid each 
one the same. That is, we did not vary the payment 
amount as an experimental variable. We calculated 
the amount to pay for a meeting based on in the 
length of that meeting. Specifically, we multiplied 
the number of utterances by 0.13 US cents to arrive 
at the payment. This resulted in payments ranging 
from 35 cents to $1.25 per meeting (mean 79 
cents). The effective hourly rate (based on how 
much time turkers took to actually finish each job) 
was $0.87. 
6 Annotation Results 
6.1 Label Distribution 
We first examine the average distribution of labels 
across the 3 classes. Figure 1 shows the distribu-
tions (expressed as percentages of the number of 
utterances) for in-house and MTurk annotators, 
averaged across the 5 meetings. Observe that the 
distribution for the in-house annotators is far more 
skewed away from a uniform 33% assignment, 
whereas the label distribution of turkers is less 
skewed. The likely reason for this difference is that 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
In house v 
In house
Turker v 
Turker
In house v 
Turker
Ka
pp
a 
Ag
re
em
en
t
Figure 3. Agreement with in-house annotators when 
turker annotations are merged through voting. 
0.0
0.2
0.4
0.6
0.8
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40 Fraction of data w
ith agreem
ent criteria
Ka
pp
a 
Ag
re
em
en
t
Agreement Fraction of data
Figure 2. Average kappa agreement between in-house 
annotators, turkers, and in-house annotators and turkers. 
104
turkers have a poorer understanding of the meet-
ings, and are more likely than in-house annotators 
to make arbitrary judgments about utterances. This 
poor understanding perhaps also explains the large 
difference in the percentage of utterances labeled 
as important?for many utterances that are difficult 
to understand, turkers probably play it safe by 
marking it important.  
The error bars represent the standard deviations of 
these averages, and capture the difference in label 
distribution from meeting to meeting. While different 
meetings are likely to inherently have different ratios 
of the 3 classes, observe that the standard deviations 
for the in-house annotators are much lower than those 
for the turkers. For example, the percentage of utter-
ances labeled ?important? by in-house annotators 
varies from 9% to 22% across the 5 meetings, whe-
reas it varies from 30% to 57% for turkers, a much 
wider range. These differences in standard deviation 
persist for each meeting as well?that is, for any giv-
en meeting, the label distribution of the turkers varies 
much more between each other than the distribution 
of the in-house annotators. 
6.2 Inter-Annotator Agreement 
Figure 2 shows the kappa values for pairs of anno-
tators, averaged across the 5 meetings, while the 
error bars represent the standard deviations. The 
kappa between the two in-house annotators. (0.4) 
is well within the range of values reported in the 
summarization literature (see section 4). The kappa 
values range from 0.24 to 0.50 across the 5 meet-
ings. The inter-annotator agreement between pairs 
of turkers, averaged across the 10 possible pairs 
per meeting (5 choose 2), and across the 5 meet-
ings show that turkers tend to agree less between 
each other than in-house annotators, although this 
kappa (0.28) is still within the range of typical 
agreement (this kappa has lower variance because 
the sample size is larger). The kappa between in-
house annotators and turkers1 (0.19) is on the low-
er end of the scale but remains within the range of 
agreement reported in the literature, suggesting 
that Mechanical Turk may be a useful tool for 
summarization.  
                                                          
1
 For each meeting, we measure agreement between every 
possible pair of annotators such that one of the annotators was 
an in-house annotator, and the other a turker. Here we present 
the average agreement across all such pairs, and across all the 
meetings. 
6.3 Agreement after Voting 
We consider merging the annotations from mul-
tiple turkers using a simple voting scheme as fol-
lows. For each utterance, if 3, 4 or 5 annotators 
labeled the utterance with the same class, we la-
beled the utterance with that class. For utterances 
in which 2 annotators voted for one class, 2 for 
another and 1 for the third, we randomly picked 
from one of the classes in which 2 annotators voted 
the same way. We then computed agreement be-
tween this ?voted turker? and each of the two in-
house annotators, and averaged across the 5 meet-
ings. Figure 3 shows these agreement values. The 
left-most point on the ?Kappa Agreement? curve 
shows the average agreement obtained using indi-
vidual turkers (0.19) while the second point shows 
the agreement with the ?voted turker? (0.22). This 
is only a marginal improvement, implying that 
simply voting and using all the data does not im-
prove much over the average agreement of indi-
vidual annotators.  
 The agreement does improve when we consider 
only those utterances that a clear majority of anno-
tators agreed on. The 3rd, 4th and 5th points on the 
?Agreement? curve plot the average agreement 
when considering only those utterances that at least 
3, 4 and 5 turkers agreed on. The ?Fraction of da-
ta? curve plots the fraction of the meeting utter-
ances that fit these agreement criteria. For 
utterances that at least 3 turkers agreed on, the 
kappa agreement value with in-house annotators is 
0.25, and this represents 84% of the data. For about 
50% of the data 4 of 5 turkers agreed, and these 
utterances had a kappa of 0.32. Finally utterances 
for which annotators were unanimous had a kappa 
of 0.37, but represented only 22% of the data. It is 
particularly encouraging to note that although the 
amount of data reduces as we focus on utterances 
that more and more turkers agree on, the utterances 
so labeled are not dominated by any one class. For 
example, among utterances that 4 or more turkers 
agree on, 48% belong to the important class, 48% 
to unimportant class, and the remaining 4% to the 
neutral class. These results show that with voting, 
it is possible to select a subset of utterances that 
have higher agreement rates, implying that they are 
annotated with higher confidence. For future work 
we will investigate whether a summarization sys-
tem trained on only the highly agreed-upon data 
outperforms one trained on all the annotation data. 
105
7 Conclusions 
In this study, we found that MTurk can be used to 
create accurate transcriptions of spontaneous meet-
ing speech when using a two-stage corrective 
process. Our best technique yielded a disagreement 
rate of 15.1%, which is competitive with reported 
disagreement in the literature of 6-12%. We found 
that both fillers and out-of-vocabulary words 
proved troublesome. We also observed that the 
length of the utterance being transcribed wasn?t a 
significant factor in determining WER, but that the 
native language of the speaker was indeed a signif-
icant factor.  
 We also experimented with using MTurk for the 
purpose of labeling utterances for extractive sum-
marization research. We showed that despite the 
lack of training, turkers produce labels with better 
than random agreement with in-house annotators. 
Further, when combined using voting, and with the 
low-agreement utterances filtered out, we can iden-
tify a set of utterances that agree significantly bet-
ter with in-house annotations.  
 In summary, MTurk appears to be a viable re-
source for producing transcription and annotation 
of meeting speech. Producing high-quality outputs, 
however, may require the use of techniques such as 
ensemble voting and iterative correction or refine-
ment that leverage performance of the same task 
by multiple workers. 
References 
 
S. Banerjee and A. I. Rudnicky. 2007. Segmenting 
 meetings into agenda items by extracting implicit 
 supervision from human note-taking. In 
 Proceedings of IUI.  
S. Banerjee and A. I. Rudnicky. 2009. Detecting the 
 noteworthiness of utterances in human meetings.  In 
 Proceedings of SIGDial.  
C. Bennett and A. I. Rudnicky. 2002. The Carnegie 
 Mellon Communicator corpus. In Proceedings of 
 ICSLP.  
J. G. Fiscus. 1997. A post-processing system to yield     
 word error rates: Recognizer Output Voting Error 
 Reduction (ROVER). In Proceedings of ASRU 
 Workshop.  
M. Galley. (2006). A skip-chain conditional ran-
 dom field for ranking meeting utterances by im
 portance. In Proceedings of EMNLP.  
 
 
A. Gruenstein, I. McGraw, and A. Sutherland. 2009. A 
 self-transcribing speech corpus: collecting 
 continuous speech with an online educational game. 
 In Proceedings of SLaTE Workshop.  
F. Liu and Y. Liu. 2008. Correlation between 
 ROUGE and human evaluation of extractive 
 meeting summaries. In Proceedings of ACL-HLT.  
M. Marge, S. Banerjee, and A. I. Rudnicky. 2010.    
 Using the Amazon Mechanical Turk for 
 transcription of spoken language. In Proceedings 
 of ICASSP.  
S. Novotney and C. Callison-Burch. 2010. Cheap, fast 
 and good enough: Automatic speech recognition 
 with non-expert transcription. In Proceedings of 
 NAACL. 
G. Penn and X. Zhu. 2008. A critical reassessment of 
 evaluation baselines for speech  summarization. In 
 Proceedings of ACL-HLT.  
B. Roy and D. Roy. 2009. Fast transcription of un-
 structured audio recordings. In Proceedings of In
 terspeech. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
106
Appendix 
 
Transcription task HIT type 1: 
 
 
 
Transcription task HIT type 2: 
 
 
 
Annotation task HIT: 
 
 
107
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 91?94,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards Improving the Naturalness of
Social Conversations with Dialogue Systems
Matthew Marge, Joa?o Miranda, Alan W Black, Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{mrmarge,jmiranda,awb,air}@cs.cmu.edu
Abstract
We describe an approach to improving
the naturalness of a social dialogue sys-
tem, Talkie, by adding disfluencies and
other content-independent enhancements
to synthesized conversations. We investi-
gated whether listeners perceive conversa-
tions with these improvements as natural
(i.e., human-like) as human-human con-
versations. We also assessed their ability
to correctly identify these conversations as
between humans or computers. We find
that these enhancements can improve the
perceived naturalness of conversations for
observers ?overhearing? the dialogues.
1 Introduction
An enduring problem in spoken dialogue systems
research is how to make conversations between
humans and computers approach the naturalness
of human-human conversations. Although this
has been addressed in several goal-oriented dia-
logue systems (e.g., for tutoring, question answer-
ing, etc.), social dialogue systems (i.e., non-task-
oriented) have not significantly advanced beyond
so-called ?chatbots?. Proper social dialogue sys-
tems (Bickmore and Cassell, 2004; Higuchi et
al., 2002) would be able to conduct open con-
versations, without being restricted to particular
domains. Such systems would find use in many
environments (e.g., human-robot interaction, en-
tertainment technology).
This paper presents an approach to improving a
social dialogue system capable of chatting about
the news by adding content-independent enhance-
ments to speech. We hypothesize that enhance-
ments such as explicit acknowledgments (e.g.,
right, so, well) and disfluencies can make human-
computer conversations sound indistinguishable
from those between two humans.
Enhancements to synthesized speech have been
found to influence perception of a synthetic
voice?s hesitation (Carlson et al, 2006) and per-
sonality (Nass and Lee, 2001). Andersson et
al. (2010) used machine learning techniques to
determine where to include conversational phe-
nomena to improve synthesized speech. Adell et
al. (2007) developed methods for inserting filled
pauses into synthesized speech that listeners found
more natural. In these studies, human judges com-
pared utterances in isolation with and without im-
provements. In our study, we focus on a holistic
evaluation of naturalness in dialogues and ask ob-
servers to directly assess the naturalness of con-
versations that they ?overhear?.
2 The Talkie System
Talkie is a spoken dialogue system capable of hav-
ing open conversations about recent topics in the
news. This system was developed for a dialogue
systems course (Lim et al, 2009). Interaction
is intended to be unstructured and free-flowing,
much like social conversations. Talkie initiates a
conversation by mentioning a recent news head-
line and invites the user to comment on it.
The system uses a database of news topics and
human-written comments from the ?most blogged
about articles? of the New York Times (NYT)1.
Comments are divided into single sentences to ap-
proximate the length of a spoken response. Given
a user?s utterance (e.g., keywords related to the
topic), Talkie responds with the comment that
most closely resembles that utterance. Talkie may
access any comment related to the topic under dis-
cussion (without repetition). The user may choose
to switch to a different topic at any time (at which
point Talkie will propose a different topic from its
set).
1http://www.nytimes.com/gst/mostblogged.html
Follow links to each article?s comment section.
91
3 Study
We performed a study to determine if the per-
ceived naturalness of conversations could be im-
proved by using heuristic enhancements to speech
output. Participants ?overheard? conversations
(similar to Walker et al (2004)). Originally typed
interactions, the conversations were later synthe-
sized into speech using the Flite speech synthesis
engine (Black and Lenzo, 2001). For distinctive-
ness, conversations were between one male voice
(rms) and one female voice (slt). The voices were
generated using the CLUSTERGEN statistical para-
metric synthesizer (Black, 2006). All conversa-
tions began with the female voice.
3.1 Dialogue Content
We considered four different conversation types:
(1 & 2) between a human and Talkie (human-
computer and computer-human depending on the
first speaker), (3) between two humans on a
topic in Talkie?s database (human-human), and
(4) between two instances of Talkie (computer-
computer). The human-computer and computer-
human conditions differed from each other by
one utterance; that is, one was a shifted version
of the other by one dialogue turn. The human-
computer conversations were collected from two
people (one native English speaker, one native
Portuguese speaker) interacting with Talkie on
separate occasions. For human-human conversa-
tions, Talkie proposed a topic for discussion. Each
conversation contained ten turns of dialogue. To
remove any potential effects from the start and end
content of the conversations, we selected the mid-
dle three turns for synthesis. Each conversation
type had five conversations, each about one of five
recent headlines (as of May 2010).
3.2 Heuristic Enhancements
We defined a set of rules that added phenomena
observed in human-human spoken conversations.
These included filled pauses, word repetitions, si-
lences, and explicit acknowledgments. Conversa-
tions in this study were enhanced manually by fol-
lowing the set of rules described in Figure 1; an
example is shown in Figure 2.
3.3 Participants and Task
Eighty participants were recruited from Ama-
zon?s Mechanical Turk2 (MTurk) for this between-
2http://www.mturk.com
Category I - Explicit Acknolwedgements
? inserted sparingly at the beginning of sentences
when grammatical (e.g., well, so, you know,
right).
Category II - Filled pauses / repetitions
? no more than three per dialogue
? no more than two in any one sentence
? placed repetitions in places where speaker may
have hesitation (e.g., after ?I think...?)
? utterances may have a filled pause or a repetition
followed by a filled pause.
Category III - Silences
? added sparingly after filled pauses.
Figure 1: Heuristics used for adding enhance-
ments to synthesized dialogue.
subjects study. Only workers with a 95% HIT (hu-
man intelligence task) approval rating could par-
ticipate. There were three experimental dimen-
sions that we counterbalanced across participants:
the four conversation ?types? described in Section
3.1, the order of the five topics, and the presence
or absence of heuristic enhancements to the syn-
thesized speech that participants heard. Each par-
ticipant listened to every topic such that exactly
five participants heard each condition.
Participants made two judgments per conversa-
tion: rating the ?naturalness? of a conversation
based on a four-point Likert scale and ranking
the conversant identities (i.e., as two humans, two
computers, or a computer and a human) from most
to least confident. We used the non-parametric
Kruskal-Wallis one-way analysis of variance to
determine statistically significant differences in
naturalness responses across groups (naturalness
being the dependent measure).
4 Results
We analyzed two measures: (1) naturalness rat-
ings of conversations, and (2) frequencies from
highest-rank to lowest-rank of who the participant
believed to be the speakers in each conversation.
We found no statistically significant ordering ef-
fects or biases due to the topic of conversation.
4.1 Speaker Type Condition Results
Results for naturalness scores are summarized in
Figure 3. As expected, mean naturalness was
highest for the human-human condition and lowest
for the computer-computer condition. We found
92
Human-computer conversation (original, enhancements)
h: Well, that sounds ridiculous, you get taxed proportionally to your paycheck.
c: So, let them retire on social security payments, then you will see changes with <uh>with an overabundance of funds.
h: I don?t see how this would be possible.
c: You know, you don?t have the money yet, and you have just increased your debt by 2.5 trillion dollars.
h: Exactly why should we be raising taxes on the rich?
c: With our current politics of division, I <umm> worry we have the will or determination to address these issues.
Figure 2: Example conversation with heuristic enhancements marked in bold.
1.9
2.5 2.3
2.7
1.5
2.0
2.5
3.0
1.0 cc ch hc hh
Figure 3: Naturalness across the speaker type con-
dition.
no statistically significant difference in naturalness
ratings for the computer-human condition com-
pared to the human-computer condition (H(1) =
2.94; p = 0.09). Also, the computer-computer
condition was significantly different from all other
conditions, suggesting that conversation flow is an
important factor in determining the naturalness of
a conversation (H(3) = 42.49, p < 0.05).
People rated conversations involving a com-
puter and a human similarly to human-human con-
versations (without enhancements). There were
no statistically significant differences between the
three conditions cc, ch, and hc (H(2) = 5.36, p =
0.06). However, a trend indicated that hc natural-
ness ratings differed from those of the ch and hh
conditions. Conversations from the hc condition
had much lower (18%) mean naturalness ratings
compared to their ch counterparts, even though
they were nearly equivalent in content.
4.2 Heuristic Enhancements Results
There were significant differences in naturalness
ratings when heuristic enhancements were present
(H(1) = 17.49, p < 0.05). Figure 4 shows that
the perceived naturalness was on average higher
with heuristic enhancements. Overall, mean natu-
ralness improved by 20%. This result agrees with
findings from Andersson et al (2010).
Computer-computer conversations had the
highest relative improvement (42%) in mean nat-
uralness. Naturalness ratings were significantly
different when comparing these conversations
with and without enhancements (H(1) = 11.77, p
< 0.05). Content-free conversational phenomena
appear to compensate for the lack of logical flow
in these conversations. According to Figure 5,
after enhancements people are no better than
chance at correctly determining the speakers in
a computer-computer conversation. Thus the
heuristic enhancements clearly affect naturalness
judgments.
Even the naturalness of conversations with good
logical flow can improve with heuristic adjust-
ments; there was a 26% relative improvement in
the mean naturalness of human-human conver-
sations. Participant ratings of naturalness were
again significantly different (H(1) = 12.45, p <
0.05). Note that these conversations were origi-
nally typed dialogue. As such, they did not capture
turn-taking properties present in conversational
speech. When enhanced with conversational phe-
nomena, they more closely resembled natural spo-
ken conversations. As shown in Figure 5, people
are more likely than chance to correctly identify
two humans as being the participants in the di-
alogue after these enhancements were applied to
speech.
Conversations with one computer and one hu-
man also benefited from heuristic enhancements.
Improvements in naturalness were marginal, how-
ever. Naturalness scores in the hc condition im-
proved by 16%, but this improvement was only
a trend (H(1) = 3.66, p = 0.06). Improvement
was negligible in the ch condition. Participants
selected the correct speakers in human-computer
dialogues no better than random. We note that
participants tended to avoid ranking conversations
as ?human & computer? with confidence (i.e., the
highest rank). A significant majority (267 out of
400) of second-rank selections were ?human &
computer.? Participants tended to order conditions
93
1.5
2.5 2.1 2.42.2
2.5 2.4
3.0
1.5
2.0
2.5
3.0
3.5
1.0 cc ch hc hh
no_enhance all_enhance
Figure 4: Mean naturalness across enhancement
conditions.
66.0%
34.0%
16.0%
30.0%30.0%
16.0%
44.0% 56.0%
10%20%
30%40%
50%60%
70%
0% cc ch hc hh
no_enhance all_enhance
Figure 5: Percentage of participants? selections of
members of the conversation that were correct.
from all human to all computer or vice-versa.
5 Conclusions
We have shown that content-independent heuris-
tics can be used to improve the perceived natural-
ness of conversations. Our conversations sampled
a variety of interactions using Talkie, a social di-
alogue system that converses about recent news
headlines. An experiment examined the factors
that could influence how external judges rate the
naturalness of these conversations.
We found that without enhancements, people
rated conversations involving a human and a com-
puter similarly to conversations involving two hu-
mans. Adding heuristic enhancements produced
different results, depending on the conversation
type: computer-computer and human-human con-
versations had the best gain in naturalness scores.
Though it remains to be seen if people are always
influenced by such enhancements, they are clearly
useful for improving the naturalness of human-
computer dialogues.
Future work will involve developing methods to
automatically inject enhancements into the synthe-
sized speech output produced by Talkie, as well
as determining whether other types of systems can
benefit from these techniques.
Acknowledgments
We would like to thank Aasish Pappu, Jose-Pablo
Gonzales Brenes, Long Qin, and Daniel Lim for
developing the Talkie dialogue system.
References
J. Adell, A. Bonafonte, and D. Escudero. Filled pauses
in speech synthesis: Towards conversational speech.
In TSD?07, Pilsen, Czech Republic, 2007.
S. Andersson, K. Georgila, D. Traum, M. Aylett, and
R.A.J. Clark. Prediction and realisation of con-
versational characteristics by utilising spontaneous
speech for unit selection. In the 5th International
Conference on Speech Prosody, Chicago, Illinois,
USA, 2010.
T. Bickmore and J. Cassell. Social Dialogue with Em-
bodied Conversational Agents. J. van Kuppevelt, L.
Dybkjaer, and N. Bernsen (eds.), Natural, Intelligent
and Effective Interaction with Multimodal Dialogue
Systems. New York: Kluwer Academic.
A. Black. CLUSTERGEN: A Statistical Parametric
Synthesizer using Trajectory Modeling. In Inter-
speech?06 - ICSLP, Pittsburgh, PA, 2006.
A. Black and K. Lenzo. Flite: a small fast run-time
synthesis engine. In ISCA 4th Speech Synthesis
Workshop, Scotland, 2001.
R. Carlson and K. Gustafson and E. Strangert. Cues for
Hesitation in Speech Synthesis. In Interspeech?06 -
ICSLP, Pittsburgh, PA, 2006.
S. Higuchi, R. Rzepka, and K. Araki. A casual conver-
sation system using modality and word associations
retrieved from the web. In EMNLP?08. Honolulu,
Hawaii, 2008.
D. Lim, A. Pappu, J. Gonzales-Brenes, and L. Qin.
The Talkie Spoken Dialogue System. Unpublished
manuscript, Carnegie Mellon Univeristy, 2009.
C. Nass and K. M. Lee. Does computer-synthesized
speech manifest personality? Experimental tests of
recognition, similarity-attraction, and consistency-
attraction. Journal of Experimental Psychology:
Applied 7 (2001) 171-181.
M. A. Walker, S. J. Whittaker, A. Stent, P. Maloor, J.
Moore, M. Johnston, G. Vasireddy. Generation and
evaluation of user tailored responses in multimodal
dialogue. Cognitive Sci. 28 (2004) 811-840.
94
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 157?164,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Comparing Spoken Language Route Instructions  for Robots across Environment Representations 
  Matthew Marge School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 mrmarge@cs.cmu.edu 
Alexander I. Rudnicky School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 air@cs.cmu.edu     Abstract 
Spoken language interaction between humans and robots in natural environments will neces-sarily involve communication about space and distance. The current study examines people?s close-range route instructions for robots and how the presentation format (schematic, vir-tual or natural) and the complexity of the route affect the content of instructions. We find that people have a general preference for providing metric-based instructions. At the same time, presentation format appears to have less im-pact on the formulation of these instructions. We conclude that understanding of spatial lan-guage requires handling both landmark-based and metric-based expressions. 1 Introduction Spoken language interaction between humans and robots in natural environments will necessar-ily involve communication about space and dis-tance. It is consequently useful to understand the nature of the language that humans would use for this purpose. In the present study we examine this question in the context of formulating route instructions given to robots. For practical pur-poses, we are also interested in understanding how presentation format affects such language. Instructions given in a physical space might dif-fer from those given in a virtual world, which in turn may differ from those given when only a schematic representation (e.g., a map or drawing) is available.  There is general agreement that landmarks play an important role in spatial language (Dan-iel and Denis, 2004; Klippel and Winter, 2005; Lovelace et al, 1999; MacMahon, 2007; Michon and Denis, 2001; Nothegger et al, 2004; Raubal 
and Winter, 2002; Weissensteiner and Winter, 2004). However, landmarks might not necessar-ily be used uniformly in instructions across pres-entation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of dis-tance in the environment. Behaviors related to spatial language may change based on the com-plexity of the route that a robot must take. This could be due to a combination of factors, includ-ing ease of use and personal assessment of a ro-bot?s ability to interpret specific distances over landmarks.   Several studies have investigated written or typed spatial language (e.g., MacMahon et al, 2006; Koulori and Lauria, 2009; Kollar et al, 2010). In addition, Ross (2008) studied models of spoken language interpretation in schematic views of areas.  In the current study we focus on close-range spoken language route instructions.  2 Related Work Interpreting spatial language is an important ca-pability for systems (e.g., mobile robots) that share space with people. Human-human commu-nication of spatial language has been extensively studied. Talmy (1983) proposed that the nature of language places constraints on how people communicate about space with others (i.e., schematization). Spatial descriptions are primar-ily influenced by how reference objects fit along fundamental axes that exhibit clear relationships with the target, and secondly by the salience of references (Carlson and Hill, 2008). People also tend to keep their spatial descriptions consistent after making an initial choice of strategy based on any existing relationships between the target to be described and other references (Vorwerg, 2009).  
157
Studies involving spatial language with robots have thus far focused on scenarios where one robot is moved around an area using spatial prepositions (Stopp et al, 1994; Moratz et al, 2003) and further with landmarks (Skubic et al, 2002; Perzanowski et al, 2003). A number of these approaches, however, were crafted by the designers of the robots themselves and not nec-essarily based on an understanding of what comes naturally to people. Indeed, Shi and Ten-brink (2009) found that a person?s internal lin-guistic representations may differ significantly from what a robot is capable of interpreting. Bugmann et al (2004) motivated the concept of corpus-based robotics, where spontaneous spo-ken commands are collected and in turn used for designing the functionality of robots. They col-lected natural language instructions from people commanding robots in a miniature of a real-world environment. Our approach follows this same reasoning; we explore naturally occurring spatial language through route instructions to robots in three distinct formats (schematic, vir-tual, and natural environments).  3 Method We designed and conducted three experiments using a navigation task that required the partici-pant to ?tell? a robot how to move to a target lo-cation. We varied the presentation formats of the stimuli (two-dimensional schematics, three-dimensional virtual scenes, real-world areas in-person). In each variant, the participant observed a static scene depicting two robots (?Mok? and ?Aki?) and a destination marker. The partici-pant?s task was to move Mok to the target desti-nation using spoken instructions. Participants were told to act as if they were an observer of the 
scene but that were themselves not present in the scene; put otherwise, the robots could hear par-ticipants but not see them (and thus the partici-pant could not figure in the instructions).  The experiment instructions directed partici-pants to assume that Mok would understand natural language and were told to use natural ex-pressions to specify instructions (that is, there was no ?special language? necessary). Partici-pants were told that they could take the orienta-tions of the robots into account when they formu-lated their instructions. They were moreover asked to include all necessary steps in a single utterance (i.e., a turn composed of one or more spatial language commands). The robots did not move in the experiments. Since our aim was to learn about spoken lan-guage route instructions, all participants recorded their requests using a simple recorder interface that could be activated while viewing the scene. A standard headset microphone was used. To avoid self-correction while speaking, the instruc-tions directed participants to think about their instructions before recording. Participants could playback their instructions, and re-record them if they deemed them unsatisfactory. All interface activity was time-stamped and logged.  3.1 General variations In their work, Hayward and Tarr (1995) found that people used spatial language with reference to landmarks most often and found it most suit-able when the objects in a scene were horizon-tally or vertically aligned. We systematically var-ied three elements of the stimuli in this study: the orientations of the two robots, Mok and Aki, and the location of the destination marker. Each ro-bot?s orientation was varied four ways: directly pointing forward, right, left, or backward. The  
 
                 (a)    (b)     (c)  Figure 1. Stimuli from the (a) schematic, (b) virtual, and (c) real-world scene experiments. Each scenario has 2 robots, Mok (left) and Aki (right). Mok is the actor in all scenarios. Outlined are possible destinations for Mok. 
    Mok                 Aki Mok                   Aki  
Mok                       Aki 
158
 Figure 2. Specified are four potential goal desti-nations for Mok, the actor in all scenarios. Only one of the destinations is shown on a particular trial.  destination marker was also varied four ways: directly in front of, behind, right of, or left of Aki. These three dimensions were varied using a factorial design, yielding 64 different configura-tions that were presented in randomized order. Thus each participant produced 64 sets of in-structions. Participants received a break at the halfway point of the session.  3.2 Schematic (2-D) Scene Experiment Participants observed two-dimensional configu-rations of schematics that contained two robots (Mok and Aki) and a destination marker in this experiment. Each participant viewed a single monitor displaying a recording interface overlaid by static slides that contained the stimuli. After each participant was shown the speech recording interface and had tried it out, they proceeded through a randomly ordered slide set. In this ex-periment, participants viewed an overhead per-spective of the scene, with the robots represented as arrows and the destination marked by purple circles (see Figures 1a and 2). The robots were represented by arrows that were meant to indi-cate their orientations in the scene. 3.3 Virtual (3-D) Scene and Distance Awareness Variation Experiment In this experiment, we crafted stimuli with a three-dimensional map builder and USARSim, a virtual simulation platform designed for conduct-ing experiments with robots (Carpin et al, 2007). The map was designed such that trials were ?rooms? in a multi-room environment. Partici-pants did not walk through the environment; they only viewed static configurations. Included in the map were instances of two Pioneer P2AT robots. All visual stimuli were presented at an eye-level view, with eyes at a height of 5?10? (see Figure 
1b). The room was designed such that walls would be too far away to serve as landmarks. Visual stimuli for this experiment required full-screen access to the game engine, so the record-ing interface was moved to an adjoining monitor.  We included an additional condition: inform-ing participants (or not) of the distance between the two robots. We recruited fourteen partici-pants for this study, seven in each of two condi-tions. In one condition (no-dist), participants were not given any information related to the scale of the robots and area in the stimuli. This is equivalent to what participants experienced in the schematic scene experiment. In the second condition (dist), the instructions indicated that the two robots, Mok and Aki, were seven feet apart. However, no scale information (e.g., a ruler) was provided in the scene itself. This would provide the option to cast instructions in terms of absolute distances. The option to use Aki as a landmark reference point remained the same as in the first experiment. We hypothesize that participants that are not given a sense of scale will use landmarks much more often than those participants that are provided distance in-formation.  3.4 Real-World Scene Experiment In natural environments, it can be assumed that people generally have a good sense of scale. In this experiment, participants viewed similar stimuli to the virtual scenarios (eye-level view), but in-person (see Figure 1c). Bins were used to represent the two robots, with two eyes placed on top of each bin to indicate orientation. As in the previous experiments, participants were told to give instructions to one robot (Mok) so that it would arrive at the destination. We recorded par-ticipant instructions for 8 different configurations of the two robots (destination varied four ways, Mok?s orientation varied two ways, right and left; Aki?s orientation did not change). We sim-plified the number of orientations because we found that orientations of Mok and Aki did not influence landmark use in the previous experi-ments. After each instruction, participants were asked to close their eyes as the experimenter changed the orientations. Since they were not at a computer screen for this experiment, only ver-bal instructions were recorded, with no task times. 3.5 Participation A total of 35 participants were recruited for this study, 10 in the schematic scene experiment, 14 
159
in the virtual scene experiment, and 11 in the real-world scene experiment. Participants ranged in age from 19 to 61 (M = 28.4 years, SD = 9.9). Of all participants, 22 were male and 14 were female. All participants were self-reported fluent English speakers. 4 Data The first study (schematic stimuli) yielded a total of 640 route instructions (64 from each of 10 participants). All of these instructions were tran-scribed in-house using the CMU Communicator guidelines (Bennett and Rudnicky, 2002). In ad-dition to the recorded instructions, we also logged participants? interactions with the speech recording interface. Since the experiment instruc-tions ask participants to think about what they plan to say before recording their speech, we as-sessed their ?thinking time? from this logging information. In the second study (virtual stimuli), more par-ticipants were recruited, but they were divided into two conditions (presence/absence of an ex-plicitly stated metric distance between the two robots in the stimuli). A total of 896 route in-structions were collected in the second study (64 
from each of 14 participants). Of the 14 partici-pants recruited for this study, 12 were transcribed using Amazon?s Mechanical Turk (Marge et al, 2010) with the same guidelines as the first study. In the real-world study, 8 route instructions were recorded from 11 participants and transcribed, yielding a total of 88 utterances.  5 Measurements Several outcomes were analyzed in this study, including the time needed to formulate directions to the robot and the number of discrete steps that participants included in their instructions. We analyzed two measures, ?thinking time? and word count. Thinking time represents the time between starting viewing a stimulus and pressing the ?Record? button. We measured utterance length by counting the number of words spoken by participants for each instruction. Utterance-level restarts and mispronunciations were ex-cluded from this count.  We also coded the instructions in terms of the number of discrete ?steps? (see Table 1). We defined a ?step? as any action where motion by Mok (the moving robot) was required to com-plete a sub-goal. For example, ?turn left and  
Environment Type Spoken language route instruction (transcribed with fillers removed) 2-D Mixed Mok turn left / and stop at the right hand side of Aki. 2-D Mixed Turn right about sixty degrees / then go forward until you're in front of Aki. 
3-D no-dist Mixed  Mok turn to your left / move towards Aki when you are pretty close to Aki stop there / turn to your right / continue moving in a straight line path you will find a blue dot to your left at some point stop there / turn to your left / and reach the blue dot which is your destination. 3-D no-dist Relative Go forward half the distance between you and Aki. 
3-D dist Absolute Rotate to your right / move forward about five feet / rotate again to your left / and move forward about seven feet. 
3-D dist Absolute Turn to your right / move forward one foot / turn to your left / move forward ten feet / turn to your left again / move forward one foot. 
Real-world Absolute Okay Mok I want you to go straight ahead for about five feet / then turn to your right forty five degrees / and go ahead and you're gonna hit the spot in about four feet from there. Real-world Mixed Mok move to Aki / turn left / and move forward three feet. 
Table 1. Spoken language route instructions for Mok, the moving robot, were transcribed and di-vided into absolute and relative steps (absolute step / relative step). Absolute steps are explicit in-structions that contain metric or metric-like distances, while relative steps include Aki (the static robot) as a reference.  
160
 Figure 3. Mean proportion of relative steps to absolute steps across distance-na?ve 2-D (sche-matic), distance-na?ve 3-D (virtual), distance-aware 3-D (virtual), and real-world scenarios (with a 1% margin of error).  
 Figure 4. Proportions of instruction types across distance-na?ve 2-D (schematic), distance-na?ve 3-D (virtual), distance-aware 3-D (virtual), and real-world scenarios.  move forward five feet? consists of two steps: (1) a ninety degree turn to the left and (2) a move-ment forward of five feet to get to a new loca-tion. We divided steps into two categories, abso-lute steps and relative steps (similar to Levin-son?s (1996) absolute and intrinsic reference sys-tems). An absolute step is one with explicit in-structions that contain metric or metric-like dis-tances (e.g., ?move forward two feet?, ?turn right ninety degrees?, ?move forward three steps?). We assume that simple turns (e.g., ?turn right?) 
are turns of 90 degrees, and thus are absolute steps. We define a relative step as one that in-cludes Aki, the static robot, in the reference (e.g., ?move forward until you reach Aki?, ?turn right until you face Aki?).  6 Results We conducted analyses based on measures of thinking time, word count, and the number of discrete ?steps? in participants? spoken language route instructions. Among the folds of the data we examined were observations from schematics without distance information (i.e., ?2-D no-dist?), virtual scenes without giving participants distance information (i.e., ?3-D no-dist?), virtual scenes with giving participants initial distance information (i.e., ?3-D dist?), and real-world scenes (i.e., ?realworld?). Since we collected an equal number of route instructions in the two virtual scene conditions (i.e., with and without being told about the distance in the environ-ment), we directly compared properties of these instructions.   In Sections 6.2 and 6.3, absolute steps, rela-tive steps, word count (log-10 transformed), and thinking timing (log-10 transformed) were the dependent measures in mixed-effects models of analysis of variance (for significance testing). ParticipantID was modeled as a random effect. We are interested in the population from which participants were drawn. 6.1 Adjusting Spatial Information Landmark use was affected by participants? awareness of scale. The fewer scale cues avail-able, the greater the number of references to landmarks. Thus, landmarks were most prevalent in instructions generated for schematic scenarios and least prevalent in the condition that explicitly specified a scale. See Figure 3 for the actual pro-portions. We did not inform participants of scale in the real-world condition. Interestingly, their absolute/relative mix was closer to the no-scale conditions even though they were observing an actual scene and could presumably make infer-ences about distances. Figure 4 shows that pres-entation format also affected participants? use of instructions that were entirely absolute in nature. There were fewer mixed instructions (i.e., in-structions where absolute instructions were sup-ported by landmarks) in conditions where par-ticipants had a sense of scale.  Though distances may be self-evident in real-world scenarios, they often are not in virtual en-
58.9% 68.5% 
93.5% 
73.1% 
41.1% 31.5% 
6.5% 
26.9% 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
100% 
2d 3d nodist 3d dist realworld 
Absolute Proportion Relative Proportion 
27.7% 36.0% 
77.6% 
54.7% 
14.4% 7.9% 
0.9% 
16.3% 
58.0% 56.2% 
21.5% 29.1% 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
100% 
2d 3d nodist 3d dist realworld 
Absolute Relative Mixed 
161
vironments. Participants behaved differently from real-world scenarios when we presented a non-trivial indication of scale. Participants? in-structions were dominated by absolute instruc-tions when they had a sense of scale in a virtual environment. This suggests that despite similari-ties in scale awareness, people formulate spatial language instructions differently when they can-not for themselves determine a sense of distance in an environment. 6.2 Sense of Distance in Virtual Stimuli We directly compared participants? spoken lan-guage route instructions with respect to the pres-ence (i.e., ?dist?) or absence (i.e., ?no-dist?) of distance information in the virtual environment. Though participants already had an initial prefer-ence toward using metric-based instructions, these became dominant when participants were aware of the distance in the virtual environment.  Participants that were not given a sense of dis-tance referred to Aki as a landmark much more than when participants were given a sense of dis-tance, confirming our initial hypothesis. We ob-served that the mean number of relative steps in the no-dist condition was nearly four times greater (1.0 relative steps per instruction) than the dist condition (0.2 relative steps per instruc-tion) (F[1, 12] = 4.6, p = 0.05). As expected, par-ticipants used absolute references more in the dist condition, given the lack of landmark use. The mean number of absolute steps was greater in the dist condition (3.3 per instruction) com-pared to the no-dist condition (mean 2.4 absolute steps per instruction) (F[1, 12] = 5.5, p < 0.05).  As shown in Figure 3, the proportions of abso-lute to relative steps in participants? instructions show clear differences in strategy. When partici-pants received distance information, an over-whelming majority of steps were absolute in na-ture (i.e., steps containing metric or metric-like distances). Aki was mentioned in steps only 6.5% of the time in the dist condition (i.e., rela-tive steps). The proportions were more balanced in the no-dist condition, with 68% of steps being absolute. The remaining 32% of steps referred to Aki. The difference between proportions from the no-dist and dist conditions was statistically significant (F[1,12] = 7.5, p < 0.05). From these analyses we can see that distance greatly influ-enced participants? language instructions in vir-tual environments.  We further classified participants? instructions as entirely absolute, relative, or mixed in nature. When participants used landmarks, they tended 
to mix them with absolute steps in their instruc-tions. Participants in the dist condition comprised most instructions with only absolute steps. How-ever, even though 6.5% of steps were absolute in nature, they were distributed among one-fifth of instructions. In the no-dist condition, though relative steps comprised only 31.5% of total steps, they were distributed among a majority of the instructions. These results suggest that se-quences of absolute steps may be sufficient on their own, but relative steps, when used, depend on the presence of some absolute terms. 6.3 Goal Location and Orientation Results Our analysis showed that the goal location in scenarios impacted participants? instructions. For word count, participants used significantly dif-ferent numbers of words based on the goal loca-tion (F[3, 1580] = 252.2, p < 0.0001). Upon fur-ther analysis, across all experiments, when the goal was closest to the Mok, the moving robot, people spoke fewer words (14 fewer words on average) compared to other locations (analysis conducted with a Tukey pairwise comparisons test). Participants also had significantly different thinking times based on the goal location (F[3, 1502] = 6.21, p < 0.05). Thinking time for the destination closest to Mok was lowest overall (on average at least 1.3s lower) and significantly dif-ferent from two of the three remaining goal loca-tions (via a Tukey pairwise comparisons test). There were no significant differences in word count and thinking time when varying Mok?s orientation or Aki?s orientation.  We also observed patterns in the steps people gave in their instructions. A landmark?s place-ment, when directly interfering with a goal, in-creased its reference in spatial language instruc-tions. When the goal location was blocked by Aki, we observed a high proportion of relative steps. For schematic stimuli, participants often required Mok to move past Aki in order to get to the destination. After observing the proportions of absolute steps and relative steps out of the to-tal number of steps across destination, we found that stimuli with this destination yielded an aver-age of 45% relative steps to 55% absolute steps. This is a greater proportion than any of the other destinations (their relative step proportions ranged from 33% to 38%). 7 Summary and Conclusions We presented a study that examines people?s close-range spoken language route instructions 
162
for robots and how the presentation format and the complexity of the route influenced the con-tent of instructions. Across all presentation for-mats, people preferred providing instructions that were absolute in nature (i.e., metric-based). De-spite this preference, landmarks were used on occasion. When they were, participants? use of them was influenced by the presentation format (schematic, virtual or natural). When participants had a general sense of distance in scenes, they were much more acclimated to using specific distances to give route instructions to a robot.     Our results indicate that the goal location can influence participant effort (i.e., time to formu-late) and the pattern (absolute/relative) in spoken language route instructions to robots. Several of these were predictable (e.g., least effort when goal location was closest to moving robot). When participants viewed these configurations in virtual environments, there were clear differ-ences in their instructions based on whether or not they were given a sense of scale.  We compared the natural language instruc-tions from the real-world condition to those from virtual stimuli. Figure 3 shows that in general, real-world participants? instructions contained similar proportions of landmarks to the 3d no-dist (virtual) condition. However, there was a greater preference to use absolute steps in the real-world than in the virtual world; participants apparently access their own sense of scale when formulating these instructions. With respect to spatial language instructions, participants tended to treat virtual environments much like real-world environments. This study provides useful information about methodology in the study of spatial language and also suggests principles for the design of spatial language understanding capabilities for robots in human environments. Specifically, virtual world representations, under suitable conditions, elicit language similar to that found under real-world situations, although the more information people have about the metric properties of the environ-ment the more likely they are to use them. But even in the absence of unambiguous metrics people seem to want to use such language in the instructions that they produce. These observa-tions can be used to inform the design of spatial language understanding for robot systems as well as guide the development of requirements for a spatial reasoning component.     
Acknowledgments This work was supported by the Boeing Com-pany and a National Science Foundation Gradu-ate Research Fellowship. The authors would like to thank Carolyn Ros?, Satanjeev Banerjee, Aasish Pappu, and the anonymous reviewers for their helpful comments on this work. The views and conclusions expressed in this document only represent those of the authors. References  C. Bennett and A. I. Rudnicky. 2002. The Carnegie Mellon Communicator Corpus, ICSLP, 2002.  G. Bugmann, E. Klein, S. Lauria, and T. Kyriacou. 2004. Corpus-based robotics: A route instruction example, Intelligent Autonomous System, pp. 96-103. L. A. Carlson and P. L. Hill. 2008. Processing the presence, placement and properties of a distractor during spatial language tasks, Memory and Cognition, 36, pp. 240-255. S. Carpin, M. Lewis, J. Wang, S. Balakirsky, and C. Scrapper. 2007. USARSim: A Robot Simulator for Research and Education, International Conference on Robotics and Automation, 2007, pp. 1400-1405. M. P. Daniel and M. Denis. 2004. The production of route directions: Investigating conditions that favour conciseness in spatial discourse, Applied Cognitive Psychology, 18, pp. 57-75. W. G. Hayward and M. J. Tarr. 1995. Spatial language and spatial representation, Cognition, 55 (1), pp. 39-84. A. Klippel and S. Winter. 2005. Structural Salience of Landmarks for Route Directions, COSIT 2005, pp. 347-362. T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward Understanding Natural Language Directions, Human Robot Interaction Conference (HRI-2010), pp. 259-266. T. Koulouri and S. Lauria. 2009. Exploring Miscommunication and Collaborative Behaviour in Human-Robot Interaction, SIGdial 2009, pp. 111-119. S. C. Levinson. 1996. Frames of reference and Molyneux?s question: cross-linguistic evidence, in P. Bloom, M. Peterson, L. Nadel, and M. Garrett (Eds.), Language and space, pp. 109-169. K. Lovelace, M. Hegarty, and D. R. Montello. 1999. Elements of good route directions in familiar and unfamiliar environments, in C. Freksa and D. M. Mark (Eds.), Spatial information theory: Cognitive and computational foundations of geographic information science. Berlin: Springer. M. MacMahon. 2007. Following Natural Language Route Instructions, Ph.D. Thesis, University of Texas at Austin. M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006. Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions, 21st 
163
National Conf. on Artificial Intelligence (AAAI), 2006, pp. 1475-1482. M. Marge, S. Banerjee, and A. I. Rudnicky. 2010. Using the Amazon Mechanical Turk for Transcription of Spoken Language, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2010. Dallas, TX. P. E. Michon and M. Denis. 2001. When and why are visual landmarks used in giving directions? in D. R. Montello (Ed.), Spatial information theory: Foundations of geographic information science, pp. 292-305. Berlin: Springer. R. Moratz, T. Tenbrink, J. Bateman, and K. Fischer. 2003. Spatial knowledge representation for human-robot interaction, Spatial Cognition III. Berlin: Springer-Verlag. C. Nothegger, S. Winter, and M. Raubal. 2004. Selection of salient features for route directions, Spatial Cognition and Computation, 4 (2), pp. 113-136. D. Perzanowski, D. Brock, W. Adams, M. Bugajska, A. C. Schultz, and J. G. Trafton. 2003. Finding the FOO: A Pilot Study for a Multimodal Interface, IEEE Systems, Man, and Cybernetics Conference, 2003. Washington, D.C. M. Raubal and S. Winter. 2002. Enriching wayfinding instructions with local landmarks, in M. J. Egenhofer and D. M. Mark (Eds.), Geographic information science, pp. 243-259. Berlin: Springer. R. Ross. 2008. Tiered Models of Spatial Language Interpretation, International Conference on Spatial Cognition, 2008. Freiburg, Germany. H. Shi and T. Tenbrink. 2009. Telling Rolland where to go: HRI dialogues on route navigation, in K. Coventry, T. Tenbrink, and J. Bateman (Eds.), Spatial Language and Dialogue (pp. 177-190). Oxford University Press. M. Skubic, D. Perzanowski, A. Schultz, and W. Adams. 2002. Using Spatial Language in a Human-Robot Dialog, IEEE International Conference on Robotics and Automation, 2002, pp. 4143-4148. Washington, D.C. E. Stopp, K. P. Gapp, G. Herzog, T. Laengle, and T. Lueth. 1994. Utilizing Spatial Relations for Natural Language Access to an Autonomous Mobile Robot, 18th German Annual Conference on Artificial Intelligence, 1994, pp. 39-50. Berlin. L. Talmy. 1983. How language structures space, in H. Pick, and L. Acredolo (Eds.), Spatial Orientation: Theory, Research and Application.  C. Vorwerg. 2009. Consistency in successive spatial utterances, in K. Coventry, T. Tenbrink, and J. Bateman (Eds.), Spatial Language and Dialogue. Oxford University Press. E. Weissensteiner and S. Winter. 2004. Landmarks in the communication of route instructions, in M. Egenhofer, C. Freksa, and H. Miller (Eds.), GIScience. Berlin: Springer.   
164
Proceedings of the SIGDIAL 2013 Conference, pages 242?250,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Predicting Tasks in Goal-Oriented Spoken Dialog Systems
using Semantic Knowledge Bases
Aasish Pappu and Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University
{aasish, air}@cs.cmu.edu
Abstract
Goal-oriented dialog agents are expected
to recognize user-intentions from an utter-
ance and execute appropriate tasks. Typi-
cally, such systems use a semantic parser
to solve this problem. However, semantic
parsers could fail if user utterances contain
out-of-grammar words/phrases or if the se-
mantics of uttered phrases did not match
the parser?s expectations. In this work,
we have explored a more robust method
of task prediction. We define task predic-
tion as a classification problem, rather than
?parsing? and use semantic contexts to im-
prove classification accuracy. Our classi-
fier uses semantic smoothing kernels that
can encode information from knowledge
bases such as Wordnet, NELL and Free-
base.com. Our experiments on two spoken
language corpora show that augmenting
semantic information from these knowl-
edge bases gives about 30% absolute im-
provement in task prediction over a parser-
based method. Our approach thus helps
make a dialog agent more robust to user
input and helps reduce number of turns re-
quired to detected intended tasks.
1 Introduction
Spoken dialog agents are designed with particular
tasks in mind. These agents could provide infor-
mation or make reservations, or other such tasks.
Many dialog agents often can perform multiple
tasks: think of a customer service kiosk system
at a bank. The system has to decide which task it
has to perform by talking to its user. This problem
of identifying what to do based on what a user has
said is called task prediction.
Task prediction is typically framed as a parsing
problem: A grammar is written to semantically
parse the input utterance from users, and these se-
mantic labels in combination are used to decide
what the intended task is. However, this method
is less robust to errors in user-input. A dialog sys-
tem consists of a pipeline of cascaded modules,
such as speech recognition, parsing, dialog man-
agement. Any errors made by these modules pro-
pogate and accumulate through the pipeline. Bo-
hus and Rudnicky (2005) have shown that this
cascade of errors, coupled with users employ-
ing out-of-grammar phrases results in many ?non-
understanding? and ?misunderstanding? errors.
There have been other approaches to perform
dialog task prediction. Gorin et al (1997) has pro-
posed a salience-phrase detection technique that
maps phrases to their corresponding tasks. Chu-
Carroll and Carpenter (1999) casted the task de-
tection as an information retrieval ? detect tasks
by measuring the distance between the query vec-
tor and representative text for each task. Bui
(2003) and Blaylock and Allen (2006) have cast it
as a hierarchical sequence labeling problem using
Hidden Markov Models (HMM). More recently,
(Bangalore and Stent, 2009) built an incremen-
tal parser that gradually determines the task based
on the incoming dialog utterances. (Chen and
Mooney, 2010) have developed a route instruc-
tions frame parser to determine the task in the con-
text of a mobile dialog robot. These approaches
mainly use local features such as dialog context,
speech features and grammar-based-semantic fea-
tures to determine the task. However grammar-
based-semantic features would be insufficient if
an utterance uses semantically similar phrases that
are not in the system?s domain or semantics. If
the system could explore semantic information be-
yond the scope of its local knowledge and use ex-
ternal knowledge sources then they will help im-
prove the task prediction.
(Cristianini et al, 2002) (Wang and Domeni-
coni, 2008) (Moschitti, 2009) found that open-
242
domain semantic knowledge resources are use-
ful for text classification problems. Their success
in limited data scenario is an attractive prospect,
since most dialog agents operate in scarce train-
ing data scenarios. (Bloehdorn et al, 2006) has
proposed a semantic smoothing kernel based ap-
proach for text classification. The intuition be-
hind their approach is that terms (particularly con-
tent words) of two similar sentences or documents
share superconcepts (e.g., hypernyms) in a knowl-
edge base. Semantic Similarity between two terms
can be computed using different metrics (Pedersen
et al, 2004) based on resources like WordNet.
Open domain resources such as world-wide-
web, had been used in the context of speech recog-
nition. (Misu and Kawahara, 2006) and (Creutz
et al, 2009) used web-texts to improve the lan-
guage models for speech recognition in a target
domain. They have used a dialog corpus in or-
der to query relevant web-texts to build the target
domain models. Although (Araki, 2012) did not
conduct empirical experiments, yet they have pre-
sented an interesting architecture that exploits an
open-domain resource like Freebase.com to build
spoken dialog systems.
In this work, we have framed the task prediction
problem as a classification problem. We use the
user?s utterances to extract lexical semantic fea-
tures and classify it into being one of the many
tasks the system was designed to perform. We
harness the power of semantic knowledge bases
by bootstraping an utterance with semantic con-
cepts related to the tokens in the utterance. The se-
mantic distance/similarity between concepts in the
knowledge base is incorporated into the model us-
ing a kernel. We show that our approach improves
the task prediction accuracy over a grammar-based
approach on two spoken corpora (1) Navagati
(Pappu and Rudnicky, 2012): a corpus of spo-
ken route instructions, and (2) Roomline (Bohus,
2003): a corpus of spoken dialog sessions in room-
reservation domain.
This paper is organized as following: Section
2 describes the problem of dialog task predic-
tion and the standard grammar based approach to
predict the dialog task. Then in Section 3, we
describe the open-domain knowledge resources
that were used in our approach and their advan-
tages/disadvantages. We will discuss our semantic
kernel based approach in the Section 4. We report
our experiment results on task prediction in Sec-
tion 5. In Section 6, we will analyze the errors that
occur in our approach, followed by concluding re-
marks and possible directions to this work.
2 Parser based Dialog Task Prediction
In a dialog system, there are two functions of a
semantic grammar ? encode linguistic constructs
used during the interactions and represent the do-
main knowledge in-terms of concepts and their in-
stances. Table 1 illustrates the tasks and the con-
cepts used in a navigation domain grammar. The
linguistic constructions help the parser to segment
an utterance into meaningful chunks. The domain
knowledge helps in labeling the tokens/phrases
with concepts. The parser uses the labeled tokens
and the chunked form of the utterance, to classify
the utterance into one of the tasks.
Table 1: Tasks and Concepts in Grammar
Tasks Examples
Imperative GoToPlace, Turn, etc
Advisory Instructions You_Will_See_Location
Grounding Instructions You_are_at_Location
Concepts Examples
Locations buildings, other landmarks
Adjectives-of-Locations large, open, black, small etc.
Pathways hallway, corridor, bridge, etc.
LiftingDevice elevator, staircase, etc.
Spatial Relations behind, above, on left, etc.
Numbers turn-angles, distance, etc.
Ordinals first, second, etc. floor numbers
The dialog agent uses the root node of a parser
output as the task. Figure 1 illustrates a semantic
parser output for a fictitious utterance in the nav-
igation domain. The dialog manager would con-
sider the utterance as an ?Imperative? for this ex-
ample.
Imperative
go direction
forward
distance
number
five
units
meters
Figure 1: Illustration of Semantic Parse Tree used
in a Dialog System
243
2.1 Grammar: A Knowledge Resource
Grammar is a very useful resource for a dialog sys-
tem because it could potentially represent an ex-
pert?s view of the domain. Since knowledge en-
gineering requires time and effort, very few di-
alog systems can afford to have grammars that
are expert-crafted and robust to various artefacts
of spoken language. This becomes a major chal-
lenge for real world dialog systems. If the sys-
tem?s grammar or the domain knowledge does not
conform to its users and their utterances, the parser
will fail to produce a correct parse, if the parse
is incorrect and/or the concept labeling is incor-
rect. Lack of comprehensive semantic knowledge
is the cause of this problem. An open-domain
knowledge base like Wordnet (Miller, 1995), Free-
base (Bollacker et al, 2008) or NELL (Carlson
et al, 2010) contains comprehensive information
about concepts and their relationships present in
the world. If used appropriately, open-domain
knowledge resources can help compensate for in-
complete semantic knowledge of the system.
3 Open-Domain Semantic Knowledge
Bases
Like grammars, open-domain knowledge re-
sources contain concepts, instances and relations.
The purpose of these resources is to organize
common sense and factoid information known to
the mankind in a machine-understandable form.
These resources, if filtered appropriately, contain
valuable domain-specific information for a dialog
agent. To this end, we propose to use three knowl-
edge resources along with the domain grammar for
the task prediction. A brief overview of each of the
knowledge resources is given below:
3.1 Wordnet: Expert Knowledge Base
Wordnet (Miller, 1995) is an online lexical
database of words and their semantics curated
by language experts. It organizes the words and
their morphological variants in a hierarchical fash-
ion. Every word has at least one synset i.e.,
sense and a synset has definite meaning and a
gloss to illustrate the usage. Synsets are con-
nected through relationships such as hypernyms,
hyponyms, meronyms, antonyms etc. Each synset
can be considered as an instance and their par-
ent synsets as concepts. Although Wordnet con-
tains several ( 120,000) word forms, some of our
domain-specific word forms (e.g., locations in a
navigation domain) will not be present. Therefore,
we would like to use other open-domain knowl-
edge bases to augment the agent?s knowledge.
3.2 Freebase: Community Knowledge Base
Freebase.com (Bollacker et al, 2008) is a col-
laboratively evolving knowledge base with the
effort of volunteers. It organizes the facts
based on types/concepts along with several predi-
cates/properties and their values for each fact. The
types are arranged in a hierarchy and the hierar-
chy is rooted at ?domain?. Freebase facts are con-
stantly updated by the volunteers. Therefore, it is a
good resource to help bootstrap the domain knowl-
edge of a dialog agent.
3.3 NELL: Automated Knowledge Base
Never-Ending Language Learner(NELL) (Carlson
et al, 2010) is a program that learns and organizes
the facts from the web in an unsupervised fashion.
NELL is on the other end of the knowledge base
spectrum which is not curated either by experts or
by volunteers. NELL uses a two-step approach to
learn new facts: (1) extract information from the
text using pattern-based, semi-structured relation
extractors (2) improve the learning for next itera-
tion based on the evidence from previous iteration.
Every belief/fact in its knowledge base has con-
cepts, source urls, extraction patterns, predicate,
the surface forms of the facts and a confidence
score for the belief. Although the facts could be
noisy in comparison to ones in other knowledge
bases, NELL continually adds and improves the
facts without much human effort.
4 Semantic Kernel based Dialog Task
Prediction
We would like to use this apriori knowledge about
the world and the domain to help us predict the
dialog task. The task prediction problem can be
treated as a classification problem. Classification
algorithms typically use bag-of-words representa-
tion that converts a document or sentence into a
vector with terms as components of the vector.
This representation produces very good results in
scenarios with sufficient training data. However
in a limited training data or extreme sparseness
scenario such as ours, (Siolas and d?Alch? Buc,
2000) has shown that Semantic Smoothing Ker-
nel technique is a promising approach. The major
advantage of this approach is that they can incor-
244
porate apriori knowledge from existing knowledge
bases. The semantic dependencies between terms,
dependencies between concepts and instances, can
be encoded in these kernels. The semantic kernels
can be easily plugged into a kernel based classi-
fier help us predict the task from the goal-oriented
dialog utterances.
In our experiments, we used an implementation
of Semantic Kernel from (Bloehdorn et al, 2006)
and plugged it into a Support Vector Machine
(SVM) classifier (SVMlight) (Joachims, 1999). As
a part of experimental setup, we will describe the
details of how did we extract the semantic depen-
dencies from each knowledge base and encoded
them into the kernel.
5 Experiments
Our goal is to improve the task prediction for a
given spoken dialog utterance by providing addi-
tional semantic context to the utterance with the
help of relevant semantic concepts from the se-
mantic knowledge bases. The baseline approach
would use the Phoenix parser?s output to deter-
mine the intended task for an utterance. From our
experiments, we show that our knowledge-driven
approach will improve upon the baseline perfor-
mance on two corpora (1) Navagati Corpus: a nav-
igation directions corpus (2) Roomline Corpus: a
room reservation dialog corpus.
5.1 Setup
We have divided each corpus into training and test-
ing datasets. We train our task classification mod-
els on the manual transcriptions of the training
data and evaluated the models on the ASR output
of the testing data. Both Navagati and Roomline
corpora came with manually annotated task labels
and manual transcriptions for the utterances. We
filtered out the non-task utterances such as ?yes?,
?no? and other clarifications from the Roomline
corpus. We obtained the ASR output for the Nava-
gati corpus by running the test utterances through
PocketSphinx (Huggins-Daines et al, 2006). The
Roomline corpus already had the ASR output for
the utterances. Table 2 illustrates some of the
statistics for each corpus.
Our baseline model for the task detection is the
Phoenix (Ward, 1991) parser output, which is the
default method used in the Ravenclaw/Olympus
dialog systems (Bohus et al, 2007). For the Nava-
gati Corpus we have obtained the parser output us-
ing the grammar and method described in (Pappu
and Rudnicky, 2012). For the Roomline corpus,
we extracted the parser output from the session
logs from the the corpus distribution.
Corpus-Stats Navagati RoomLine
Tasks 4 7
Words 503 498
Word-Error-rate 46.3% 25.6%
Task Utts 934 18911
Task Training-Utts 654 1324
Task Testing-Utts 280 567
Tasks
N1. Meta R1. NeedRoom
N2. Advisory R2. ChooseRoom
N3. Imperative R3. QueryFeatures
N4. Grounding R4. ListRooms
R5. Identification
R6. CancelReservation
R7. RejectRooms
Table 2: Corpus Statistics
5.1.1 Semantic Facts to Semantic Kernel
The semantic kernel takes a term proximity ma-
trix as an input, then produces a positive semidef-
inite matrix which can be used inside the kernel
function. In our case, we build a term proxim-
ity matrix for the words in the recognition vocabu-
lary. (Bloehdorn et al, 2006) found that using the
term-concept pairs in the proximity matrix is more
meaningful following the intuition that terms that
share more number of concepts are similar as op-
posed to terms that share fewer concepts. We have
used following measures to compute the proximity
value P and some of them are specific to respec-
tive knowledge bases:
? gra: No weighting for term-concept pairs in
the Grammar, i.e.,
P = 1, for all concepts ci of t, P = 0 other-
wise.
? fb: Weighting using normalized Free-
base.com relevance score, i.e.,
P = fbscore(t, ci)? fbscore(t, cmin)fbscore(t, cmax)? fbscore(t, cmin)(1)
? nell: Weighting for the NELL term-concept
pairs using the probability for a belief i.e.,
P = nellprob(t, ci) (2)
, for all concepts ci of t, P = 0 otherwise.
1Originally has 10356 utts; filtered out non-task utts.
245
? wnpath: Weighting for the term-concept
pairs in the Wordnet based on the shortest
path, i.e.,
P = wnPATH(t, ci) (3)
for all concepts ci of t, P = 0 otherwise.
? wnlch: Weighting for the term-concept
pairs in the Wordnet based on the Leacock-
Chodorow Similiarity, i.e.,
P = wnLCH(t, ci) (4)
for all concepts ci of t, P = 0 otherwise.
? wnwup: Weighting for the term-concept
pairs in the Wordnet based on the Wu-Palmer
Similarity, i.e.,
P = wnWUP (t, ci) (5)
for all concepts ci of t, P = 0 otherwise.
? wnres: Weighting for the term-concept
pairs in the Wordnet based on the Resnik
Similarity using Information Content, i.e.,
P = wnRES(t, ci) (6)
for all concepts ci of t, P = 0 otherwise.
To create a grammar-based proximity matrix,
we extracted the concept-token pairs from the
parser output on the reference transcriptions in
both corpora. In order to create a wordnet-based
proximity matrix, we retrieve the hypernyms for
the corresponding from Wordnet using the Word-
net 3.0 database 2. For the freebase concept-token
pairs, we query tokens for a list of types with the
help of the MQL query interface3 to the freebase.
To retrieve beliefs from NELL we downloaded a
tsv formatted database called every-belief-in-the-
KB4 and then queried for facts using unix grep
command.
5.2 Results
Our objective is to evalute the effect of augmented
semantic features on the task detection. As noted
earlier, we divided both corpora into training and
testing datasets. We build our models on the man-
ual transcriptions from the training data and eval-
uate on the ASR hypotheses of the testing data.
2http://www.princeton.edu/wordnet/download/
3https://www.googleapis.com/freebase/v1/search
4http://rtw.ml.cmu.edu/rtw/resources
For the Navagati corpus, we use the same training-
testing split that we used in our previous work be-
cause the grammar was developed based on the
training data. For the Roomline corpus, we ran-
domly sample 30% of the testing data from the
entire corpus.
Our first semantic-kernel based model SEM-
GRA uses the domain grammar as a ?knowledge
base?. This is a two step process: (1) we extract
the concept-token pairs from the parse output of
the training data. (2) Then, assign a uniform prox-
imity score (1.0) for all pairs of words that ap-
pear under a particular concept otherwise 0.0 (gra
as mentioned in the previous section). We aug-
ment the grammar concepts to the utterances in
the datasets, learn SEM-GRA model and classify
the test-hypotheses. For all our models we use
a fixed C = 0.07 value (soft-margin parameter)
for the SVM classifiers. This model achieved high-
est performance at this value during a parameter-
sweep. SEM-GRA model outperformed the parser-
baseline on both datasets (see Table 3). It clearly
takes advantage of the domain knowledge encoded
in the form of semantic-relatedness between con-
cepts and token pairs.
What if a dialog system does not have gram-
mar to begin with? We use the same two step pro-
cess to build semantic-kernel based models using
one open-domain knowledge base at a time. We
built Wordnet based models (SEM-WNWUP, SEM-
WNPATH, SEM-WNLCH, SEM-WNRES) using dif-
ferent proximity measures described in the previ-
ous section. From Table 3 SEM-WNRES model,
one that uses information content performs the
best among all wordnet based models. In order
to compute the information content we used the
pair-wise mutual information scores available for
brown-corpus.dat in the NLTK (Bird et al, 2009)
distribution. Other path based scores were also
computed using NLTK API for Wordnet.
We observed that our wordnet-based models
capture relatedness between most-common nouns
(e.g., room numbers) and their concepts but not
for some of the less-common ones (e.g., loca-
tion names). To compensate this imbalance, we
use larger knowledge resources freebase.com and
NELL. First we searched for the facts in each of
these knowledge bases using every token in the vo-
cabulary of both corpora. We pick the top concept
for each token based on the score provided by the
respective search interfaces. In freebase we have
246
Table 3: F1 (in %) comparison of parse baseline against semantic-kernel models with their corresponding
similarity metrics
Corpus baseline SEMGRA SEMWNWUP SEMWNPATH SEMWNLCH SEMWNRES SEMFBASE SEMNELL
Navagati 40.1 65.8 67.1 67.7 66.4 69 68.7 66.2
Roomline 54.3 79.7 77.3 79.5 79.6 80.6 83.3 81.1
about 100 concepts that are relevant to the vocab-
ulary and in the NELL model we have about 250
concepts that are relevant to the vocabulary in each
of the corpora. The models based on NELL (SEM-
NELL) and Freebase (SEM-FBASE) capture relat-
edness between less-common nouns and their con-
cepts. We can see that both of these models per-
form comparable to the domain grammar model
SEM-GRA which also captures the relatedness be-
tween less-common nouns and their concepts. We
believe that both freebase and NELL has a supe-
rior performance because of wider-range of con-
cept coverage and non-uniform proximity mea-
sures used in the semantic kernel, which gives
a better judgement of relatedness than a uniform
measure used in the SEM-GRA model.
Since we observed that an individual model is
good at capturing a particular aspect of an utter-
ance, we extended the individual semantic models
by combining the proximity matrices from each
of them and augmenting their semantic concepts
to the training and testing datasets. We built four
combined models as shown in Table 4 by varying
the wordnet?s proximity metric to identify which
one of them works best in combination with other
semantic metrics. The wnres metric performs the
best both in standalone and combination settings.
(Bloehdorn et al, 2006) also found that wnres
particularly performs well for lower values of the
soft-margin parameter in their experiments.
Table 4: F1-Score (in %): Models with semantics
combined from different KBs (ALL-KB)
Model Navagati Roomline
GRA+WNWUP+FBASE+NELL 70.8 82.2
GRA+WNPATH+FBASE+NELL 70.1 81.4
GRA+WNLCH+FBASE+NELL 70.8 81.3
GRA+WNRES+FBASE+NELL 73.4 83.7
6 Discussion
We have built a model that exploits different se-
mantic knowledge bases and predicts the task on
both corpora with high accuracy. But how is it af-
fected by factors like misrecognition and context
ambiguity?
6.1 Influence of Recognition Errors
When the recognition is bad, it is obvious that the
accuracy would go down. We would like to know
which of these knowledge resources can augment
useful semantics despite misrecognitions. Table 2
shows that WER on the Navagati corpus is about
46% and the Roomline corpus is about 25%. We
compared the F1-score of different models on ut-
terances for different ranges of WER as shown in
the Figure 2 on the Navagati Corpus. We notice
that the model built using all knowledge bases is
more robust even at higher WER. We did similar
analysis on the Roomline corpus and did not no-
tice any differences across models due to relatively
lower WER (25.6%).
0 20 40 60
50
60
70
80
90
Word Error Rate
F1
Sc
or
e
all-kb
wnres
nell
fbase
Figure 2: Word Error Rate vs F1-Score for KB-
based Models on Navagati Corpus
6.2 Confusion among Tasks
We found that a particular pair of tasks are more
confusing than others. Here we present an analysis
of such confusion pairs for both corpora for dif-
ferent classification models. Table 5 and Table 6
show the pairs of tasks that are most confused in
the experiments. The ALL-KB model (a combina-
tion of all knowledge bases) has least number of
247
Table 5: Most confusable pairs of tasks in Navagati Corpus for KB based classification models
(See Table 2 for task labels)
KBType ALL-KB SEM-WNRES SEM-NELL SEM-FBASE
ActualTask N2 N4 N2 N4 N2 N4 N1 N2 N4
Predicted N3 N1 N3 N3 N3 N3 N3 N3 N3
ConfusionPerTask 10.5% 27.7% 26.3% 33.3% 26.3% 38.8% 22.2% 28.9% 44.4%
Table 6: Most confusable pairs of tasks in Roomline Corpus for KB based classification models
(See Table 2 for task labels)
KBType ALL-KB SEM-WNRES SEM-NELL SEM-FBASE
ActualTask R4 R4 R6 R4 R6 R3 R4 R5 R6
Predicted R3 R5 R5 R1 R1 R1 R3 R1 R1
ConfusionPerTask 36.6% 48.7% 44.4% 25.6% 44.5% 32.5% 23% 53.4% 55.5%
confusion pairs among all the models. This is due
to more relevant concepts are augmented to an ut-
terance compared to fewer relevant concepts that
augmented while using individual models.
We inspected the confused tasks by examin-
ing the feature vectors of misclassified examples.
While using the ALL-KB model 10% of the utter-
ances from N2 (Advisory) were confused for N3
(Imperative) because of phrases like ?your left?,
?your right?. These phrases were often associated
with N3 utterances. To recovery from such ambi-
guities, the agent could ask a clarification question
e.g., ?are we talking about going there or find it
on the way?? to resolve the differences between
these tasks. The system could not only get clar-
ification but also bootstrap the original utterance
of the user with the clarification to gather addi-
tional context to retrain the task detection models.
The individual models were also confused about
N2 and N3 tasks, where we could use similar clar-
ification strategies to improve the task prediction.
27% of the N4 (grounding about current robot?s
position) utterances were confused for N1 (meta
comments about the robot?s rounavigation route)
because these utterances shared more number of
freebase concepts with the N1 model. The system
could resolve such utterances by asking a clarifi-
cation question ?are we talking about the current
position??. Individual models i.e., SEM-WNRES,
SEM-FBASE and SEM-NELL suffered mostly from
the lack of concepts capturing semantics related
to all types of entities (e.g., most common nouns,
less common entities etc.,) found in an utterance.
We examined the confusion pairs in the Room-
line corpus and observed that R4 (ListRooms) and
R3 (Queries) tasks were most confused in the
ALL-KB model. On closer inspection, we found
that R4 utterances are about listing the rooms that
are retrieved by the system. Whereas, R3 utter-
ances are about asking whether a room has a facil-
ity (e.g., projector availability). In the ambiguous
utterances, often the R4 utterances were about fil-
tering the list of rooms by a facility type.
7 Conclusion
We proposed framing the dialog task prediction
problem as a classification problem. We used an
SVM classifier with semantic smoothing kernels
that incorporate information from external knowl-
edge bases such as Wordnet, NELL, Freebase. Our
method shows good improvements over a parser-
based baseline. Our analysis also shows that our
proposed method makes task prediction be more
robust to moderate recognition errors.
We presented an analysis on task ambiguity and
found that these models can confuse one task for
another. We believe that this analysis highlights
the need for dialog based clarification strategies
that cannot only help the system for that instance
but also help the system improve its task predic-
tion accuracy in future dialog sessions.
8 Future Work
This work stands as a platform to make a spoken
dialog system learn relevant semantic information
from external knowledge sources. We would like
to extend this paradigm to let the system acquire
more information through dialog with a user. The
system could elicit new references to a known se-
mantic concept. For example, a navigation agent
knows a task called ?GoToRestaurant? but the
user-utterance had the word ?diner? and it was
248
not seen in the context of ?restaurant?. The agent
somewhat predicts this utterance is related to ?Go-
ToRestaurant? using the approach described in this
paper. It could ask the user an elicitation question:
?You used diner in the context of a restaurant, is
diner really a restaurant??. The answer to this
question will help the system gradually understand
what parts of an open-domain knowledge base can
be added into its own domain knowledge base. We
believe that the holistic approach of learning from
automated processes and learning through dialog,
will help the dialog systems get better interaction
by interaction.
References
Masahiro Araki. 2012. Rapid development process of
spoken dialogue systems using collaboratively con-
structed semantic resources. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 70?73, Seoul,
South Korea, July. Association for Computational
Linguistics.
Srinivas Bangalore and Amanda J Stent. 2009. In-
cremental parsing models for dialog task structure.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 94?102. Association for Compu-
tational Linguistics.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python. O?Reilly
Media.
Nate Blaylock and James Allen. 2006. Hierarchical
instantiated goal recognition. In Proceedings of the
AAAI Workshop on Modeling Others from Observa-
tions.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic ker-
nels for text classification based on topological mea-
sures of feature similarity. In Data Mining, 2006.
ICDM?06. Sixth International Conference on, pages
808?812. IEEE.
Dan Bohus and Alexander I Rudnicky. 2005.
Sorry, I didn?t catch that!-an investigation of non-
understanding errors and recovery strategies. In 6th
SIGdial Workshop on Discourse and Dialogue.
Dan Bohus, Antoine Raux, Thomas K Harris, Maxine
Eskenazi, and Alexander I Rudnicky. 2007. Olym-
pus: an open-source framework for conversational
spoken language interface research. In Proceedings
of the workshop on bridging the gap Academic and
industrial research in dialog technologies, number
April, pages 32?39. Association for Computational
Linguistics.
Dan Bohus. 2003. Roomline. http://www.cs.
cmu.edu/~dbohus/RoomLine.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. SIGMOD 08 Proceedings of the
2008 ACM SIGMOD international conference on
Management of data, pages 1247?1249.
Hung H Bui. 2003. A general model for online proba-
bilistic plan recognition. In International Joint Con-
ference on Artificial Intelligence, volume 18, pages
1309?1318. Citeseer.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka Jr., and Tom M
Mitchell. 2010. Toward an Architecture for Never-
Ending Language Learning. Artificial Intelligence,
2(4):1306?1313.
D.L. Chen and R.J. Mooney. 2010. Learning to in-
terpret natural language navigation instructions from
observations. Journal of Artificial Intelligence Re-
search, 37:397?435.
Jennifer Chu-Carroll and Bob Carpenter. 1999.
Vector-based natural language call routing. Compu-
tational linguistics, 25(3):361?388.
Mathias Creutz, Sami Virpioja, and Anna Kovaleva.
2009. Web augmentation of language models for
continuous speech recognition of sms text messages.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 157?165. Association for Com-
putational Linguistics.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent semantic kernels. Journal of
Intelligent Information Systems, 18(2):127?152.
Allen L Gorin, Giuseppe Riccardi, and Jeremy H
Wright. 1997. How may i help you? Speech com-
munication, 23(1-2):113?127.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recogni-
tion system for hand-held devices. In ICASSP, vol-
ume 1. IEEE.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169?184. MIT Press.
George A Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Teruhisa Misu and Tatsuya Kawahara. 2006. A boot-
strapping approach for developing language model
of new spoken dialogue systems by selecting web
texts. In Proc. Interspeech, pages 9?12.
249
Alessandro Moschitti. 2009. Syntactic and semantic
kernels for short text pair categorization. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 576?584.
Aasish Pappu and Alexander I Rudnicky. 2012. The
Structure and Generality of Spoken Route Instruc-
tions. Proceedings of the 13th Annual Meeting of
the Special Interest Group on Discourse and Dia-
logue, pages 99?107.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, pages 38?41. Association for
Computational Linguistics.
Georges Siolas and Florence d?Alch? Buc. 2000. Sup-
port vector machines based on a semantic kernel
for text categorization. In Neural Networks, 2000.
IJCNN 2000, Proceedings of the IEEE-INNS-ENNS
International Joint Conference, volume 5, pages
205?209. IEEE.
Pu Wang and Carlotta Domeniconi. 2008. Build-
ing semantic kernels for text classification using
wikipedia. In Proceeding of the 14th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 713?721. ACM.
W. Ward. 1991. Understanding spontaneous speech:
the phoenix system. In ICASSP. IEEE.
250
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 63?67,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Conversational Strategies for Robustly Managing Dialog in Public Spaces
Aasish Pappu Ming Sun
Language Technologies Institute
Carnegie Mellon University
Pittsburgh PA, USA
{aasish,mings,seshadrs,air}@cs.cmu.edu
Seshadri Sridharan Alexander I. Rudnicky
Abstract
Open environments present an attention
management challenge for conversational
systems. We describe a kiosk system
(based on Ravenclaw?Olympus) that uses
simple auditory and visual information to
interpret human presence and manage the
system?s attention. The system robustly
differentiates intended interactions from
unintended ones at an accuracy of 93%
and provides similar task completion rates
in both a quiet room and a public space.
1 Introduction
Dialog systems designers try to minimize disrup-
tive influences by introducing physical and be-
havioral constraints to create predictable environ-
ments. This includes using a closed-talking mi-
crophone or limiting interaction to one user at a
time. But such constraints are difficult to apply
in public environments such as kiosks (Bohus and
Horvitz, 2010; Foster et al., 2012; Nakashima et
al., 2014), in-car assistants (Kun et al., 2007; Hof-
mann et al., 2013; Misu et al., 2013) or on mo-
bile robots (Haasch et al., 2004; Sabanovic et al.,
2006; Kollar et al., 2012). To implement dialog
systems that operate in public spaces, we have to
relax some of these constraints and deal with addi-
tional challenges. For example, the system needs
to select the correct interlocutor, who may be only
one of several possible ones in the vicinity, then
determine whether they are initiating the process
of engaging with the system.
In this paper we focus on the problems of
identifying a potential interlocutor in the environ-
ment, engaging them in conversation and provid-
ing suitable channel-maintenance cues (Bruce et
al., 2002; Fukuda et al., 2002; Al Moubayed and
Skantze, 2011). We address these problems in the
context of a simple application, a kiosk agent that
Figure 1: Ravenclaw?Olympus augmented with
multimodal input and output functions.
accepts tasks such as taking a message to a named
recipient. To evaluate the effectiveness of our ap-
proach we compared the system?s ability to man-
age conversations in a quiet room and in a public
area.
The remainder of this paper is organized as fol-
lows: we first describe the system architecture,
then present the evaluation setup and the results,
then review related work and finally conclude with
an analysis of the study.
2 System Architecture
Figure 1 shows the architecture; it incorporates
Ravenclaw/Olympus (Bohus et al., 2007) stan-
dard components (in white), new components (in
black) and modified ones (shaded). In the system
pipeline, the Audio Server receives audio from a
microphone, endpoints it and sends it to the ASR
63
Figure 2: Face states; some are animations.
engine (PocketSphinx); the decoding is passed to
NLU (Phoenix parser). ICE (Input Confidence Es-
timation) (Helios) assigns confidence scores for
the input concepts. Based on user?s input and
the context, the Dialog Manager (DM) determines
what to do next, perhaps using data from the
Domain Reasoner (DR). An Interaction Manager
(IM) initiates a spoken response using Natural
Language Generation (NLG) and Text-to-Speech
(TTS) component.
Three components were added: (1) Multimodal
Capture acquires audio and human position data
using a Kinect device
1
. (2) Awareness deter-
mines whether there is a potential interlocutor in
the vicinity and their current position, using skele-
tal and azimuth information. (3) Talking Head
that conveys the system?s state (as shown in Fig-
ure 2): whether it?s active (conversing and hint-
ing) or idle (asleep and doze) and whether fo-
cused concepts are grounded (conversing and non-
understanding); certain state representations (e.g.,
conversing) are coordinated with the TTS compo-
nent.
3 Evaluation
A robust system should be able to function as well
in a difficult situation as in a controlled one. We
compare the system?s performance in two environ-
ments, public and quiet, and evaluate the (a) sys-
tem?s awareness of intended users, and its (b) end-
to-end performance.
The same twenty subjects participated in both
1
See http://www.microsoft.com/en-us/
Kinectforwindows/develop/. Three sources are
tapped: the beam-formed audio, the sound source azimuth
and skeleton coordinates. Video data are not used.
experiments: a mix of American, Indian, Chinese
and Hispanic with different fluency levels of En-
glish. None of them had previously interacted with
this system prior to this study.
The subjects were told that they would interact
with a virtual agent displayed on a screen. Their
task for the awareness experiment was to make the
agent aware that they wished to interact. For the
end-to-end system performance, the task was to
instruct the agent to send a message to a named
recipient.
3.1 Situated Awareness
We define situated awareness as correctly engag-
ing the intended interlocutor (i.e., verbally ac-
knowledge the user?s presence) under two con-
ditions. When the user is positioned (i) inside
the visual range of the Kinect at LOC-0 in Fig-
ure 3(a); and (ii) outside the visual range of the
Kinect at LOC-1 in Figure 3(a). We used the effec-
tive range of the camera?s documented horizontal
field of view (57
?
); hereafter referred as its cone-
of-awareness.
We conducted the awareness experiment in a
public space, a lounge at a hub connecting mul-
tiple corridors. The area has tables and seating,
self-serve coffee, a microwave oven, etc. The ex-
periment was conducted during regular hours, be-
tween 10am to 6pm on weekdays. During these
times we observed occupants discussing projects,
preparing food, making coffee, etc. No direct at-
tempt was made to influence their behavior and we
believe that they made no attempt to accommo-
date our activities. Accordingly, the natural sound
level in the room varied in unpredictable ways. To
supplement naturally-occurring sounds, we played
audio of a conversation between two humans, an
extract from the SwitchBoard corpus (Graff et al.,
2001). It was played using a loudspeaker placed at
LOC-2 in Figure 3(a). The locations (0, 1, and 2)
are all 1.5m from the Kinect, which we deemed to
be a comfortable distance for the subjects. LOC-
1 and LOC-2 are 70
?
to the left and right of the
Kinect, outside its cone.
To detect the presence of an intended user, we
build an awareness model that uses three sensory
streams viz., voice activity, skeleton, and sound
source azimuth. This model relies on the co-
incidence of azimuth angle and the skeleton angle
(along with voice activity) to determine the pres-
ence of an intended user. We compare the pro-
64
Figure 3: (a) Plan of Public Space (lounge);(b) Plan of Quiet Room (lab). Dark circled markers indicate
locations (LOC-0, LOC-1, LOC-2), discussed in the text.
Condition Voice +Skeleton +Azimuth
Outside
the cone 28% ? 93%
Inside
the cone ? 25% 93%
Table 1: Accuracy for the Awareness Detection
posed model with two baselines: (1) conventional
voice-activity-detection (VAD): once speech is de-
tected the system responds as if a conversation is
initiated and (2) based on skeleton plus VAD: once
the skeleton appears in front of the Kinect and a
voice is heard, the system engages in conversation.
Table 1 shows the combination of sensory
streams we used under two conditions. For the
outside-the-cone condition, the participants stand
in LOC-1 as shown in Figure 3(a) and follow the
instructions from the agent. Initially, the sub-
ject?s skeleton is invisible to the agent; however
the subject is audible to the agent. Therefore, in
certain combinations of sensors (e.g., voice +
skeleton model and voice + skeleton
+ azimuth model) the system attempts to guide
them to move in front of it, i.e. to LOC-0, an
ideal position for interacting with the system. For
inside-the-cone condition, subjects stand at LOC-
0 where the agent can sense their skeleton.
When user stands at LOC-1 i.e., outside-
the-cone voice + skeleton model and
voice + skeleton + azimuth models
are functionally the same since the source of
distraction has no skeleton in the cone. When
user stands at LOC-0, i.e., inside-the-cone voice
alone is the same as voice + skeleton
model since the agent always sees a skeleton in
front of it. Therefore, this variant was not used.
We treated awareness detection as a binary de-
cision. An utterance is classified either as ?in-
tended? or ?unintended?. We manually labeled the
utterances whether they were directed at the sys-
tem (?intended?), ?unintended? otherwise. Accu-
racy on ?intended? speech is reported in the Ta-
ble 1. Within each condition, the order of the ex-
periments with different awareness strategies was
randomized.
We observe that the voice + skeleton +
azimuth model proves to be robust in the pub-
lic space. Its performance is significantly better,
t(38) = 8.1, p ? 0.001, compared to the other
baselines in both conditions. This result agrees
with previous research (Haasch et al., 2004; Bohus
and Horvitz, 2009) showing that a fusion of multi-
modal features improves performance over a uni-
modal approach. Our result indicates that a sim-
ple heuristic approach, using minimal visual and
audio features, provides usable attention manage-
ment in open environments. This approach helped
the system handle a complex interaction scenario
such as out-of-cone speech directed to the sys-
tem. If the speaker is out of range but is producing
possibly system-directed utterances, system urges
them to step to the front. We believe it can be ex-
tended to other complex cases by introducing ad-
ditional logic.
3.2 End-to-End System Performance
To investigate the effect of the environment, we
compare the system?s performance in public space
and quiet room. The average noise level in the
quiet room is about 47dB(A) with computers as
65
Metric Public
Space
Quiet
Room
Success Ratio 15/20 16/20
Avg # Turns 14.2 16.4
Concept Acc 67% 68%
Table 2: Public Space vs Quiet Room Performance
the primary source of noise. The background
sound level in the public space was 46dB; other
natural sources ranged up to 57dB. The audio dis-
tractor measured 57dB. The same ASR acoustic
models and processing parameters were used in
both environments. The participant stood at LOC-
0 in Figure 3(a) during the public space experi-
ment and Figure 3(b) during the quiet room ex-
periment. In both experiments, LOC-0 is 1.5m
away from the system. We used the voice +
skeleton + azimuth model to discriminate
user speech from distractions in the environment.
We gave each participant a randomized series
of message-sending tasks, e.g. ?send a message
to ?person? who is in room ?number??. Subjects
had a maximum of 3 minutes to complete; each
task required 7 turns. The number of tasks com-
pleted (over the group) is reported in terms of
task ?success-ratio?. Table 2 shows the success-
ratio of the task, the average number of turns
needed to complete the task, and the system?s per-
utterance concept accuracy (Boros et al., 1996).
There were no statistically significant differences
between quiet room and public space, (t(38) <
2, p > 0.5, on any metric). We conclude that
the channel maintenance technique we tested was
equally effective in both environments.
4 Related Work
The problem of deploying social agents in public
spaces has been of enduring interest; (Bohus and
Horvitz, 2010) list engagement as a challenge for
a physically situated agent in open-world interac-
tions. But the problem was noted earlier and solu-
tions were proposed; e.g a ?push-to-talk? protocol
to signal the onset of intended user speech (Stent
et al., 1999). (Sharp et al., 1997; Hieronymus et
al., 2006) described the use of attention phrase as
a required prefix to each user input. Although ex-
plicit actions are effective, they need to be learned
by users. This may not be practical for systems in
public areas engaged by casual users.
A more robust approach involves fusing sev-
eral sources of information such as audio, gaze
and pose(Horvitz et al., 2003; Bohus and Horvitz,
2009) (Hosoya et al., 2009; Nakano and Ishii,
2010). Previous works have shown that fusion
of different sensory information can improve at-
tention management. The drawback of such ap-
proaches is in the complexity of the sensor equip-
ment. Our work attempts to create the rele-
vant capabilities using a simple sensing device
and relying on explicitly modeled conversational
strategies. Others are also using the Microsoft
Kinect device for research in dialog. For example,
(Skantze and Al Moubayed, 2012) and (Foster et
al., 2012) presented a multiparty interaction sys-
tems that use Kinect for face tracking and skeleton
tracking combined with speech recognition.
In our current work, we show that situational
awareness can be integrated into an existing dia-
log framework, Ravenclaw?Olympus, that was not
originally designed with this functionality in mind.
The source code of the framework presented in
this work is publicly available for download
1
and
the acoustic models that have been adapted to the
Kinect audio channel
2
5 Conclusion
We found that a conventional spoken dialog sys-
tem can be adapted to a public space with mini-
mal modifications to accommodate additional in-
formation sources. Investigating the effectiveness
of different awareness strategies, we found that a
simple heuristic approach that uses a combination
of sensory streams viz., voice, skeleton and az-
imuth, can reliably identify the likely interlocutor.
End-to-end system performance in a public space
is similar to that observed in a quiet room, indi-
cating that, at least under the conditions we cre-
ated, usable performance can be achieved. This
is a useful finding. We believe that on this level,
channel maintenance is a matter of articulating a
model that specifies appropriate behavior in dif-
ferent states defined by a small number of dis-
crete features (presence, absence, coincidence).
We conjecture that such a framework is likely to
be extensible to more complex situations, for ex-
ample ones involving multiple humans in the en-
vironment.
1
http://trac.speech.cs.cmu.edu/repos/
olympus/tags/KinectOly2.0/
2
http://trac.speech.cs.cmu.edu/repos/
olympus/tags/KinectOly2.0/Resources/
DecoderConfig/AcousticModels/Semi_
Kinect.cd_semi_5000/
66
References
[Al Moubayed and Skantze2011] S. Al Moubayed and
G. Skantze. 2011. Turn-taking control using gaze in
multiparty human-computer dialogue: Effects of 2d and
3d displays. In Proceedings of AVSP, Florence, Italy,
pages 99?102.
[Bohus and Horvitz2009] D. Bohus and E. Horvitz. 2009.
Dialog in the open world: platform and applications. In
Proceedings of the 2009 international conference on Mul-
timodal interfaces, pages 31?38. ACM.
[Bohus and Horvitz2010] D. Bohus and E. Horvitz. 2010. On
the challenges and opportunities of physically situated dia-
log. In 2010 AAAI Fall Symposium on Dialog with Robots.
AAAI.
[Bohus et al.2007] D. Bohus, A. Raux, T.K. Harris, M. Eske-
nazi, and A.I. Rudnicky. 2007. Olympus: an open-source
framework for conversational spoken language interface
research. In Proceedings of the workshop on bridging the
gap: Academic and industrial research in dialog technolo-
gies, pages 32?39. Association for Computational Lin-
guistics.
[Boros et al.1996] M. Boros, W. Eckert, F. Gallwitz, G. Gorz,
G. Hanrieder, and H. Niemann. 1996. Towards under-
standing spontaneous speech: Word accuracy vs. concept
accuracy. In Spoken Language, 1996. ICSLP 96. Pro-
ceedings., Fourth International Conference on, volume 2,
pages 1009?1012. IEEE.
[Bruce et al.2002] A. Bruce, I. Nourbakhsh, and R. Simmons.
2002. The role of expressiveness and attention in human-
robot interaction. In Proceedings of 2002 IEEE Interna-
tional Conference on Robotics and Automation, volume 4,
pages 4138?4142. IEEE.
[Foster et al.2012] M.E. Foster, A. Gaschler, M. Giuliani,
A. Isard, M. Pateraki, and R.P.A. Petrick. 2012. ?two
people walk into a bar?: Dynamic multi-party social inter-
action with a robot agent. In Proc. of the 14th ACM Inter-
national Conference on Multimodal Interaction ICMI.
[Fukuda et al.2002] T. Fukuda, J. Taguri, F. Arai,
M. Nakashima, D. Tachibana, and Y. Hasegawa.
2002. Facial expression of robot face for human-robot
mutual communication. In Proceedings of 2002 IEEE
International Conference on Robotics and Automation,
volume 1, pages 46?51. IEEE.
[Graff et al.2001] D. Graff, K. Walker, and D. Miller. 2001.
Switchboard cellular part 1 transcribed audio. In Linguis-
tic Data Consortium, Philadelphia.
[Haasch et al.2004] A. Haasch, S. Hohenner, S. H?uwel,
M. Kleinehagenbrock, S. Lang, I. Toptsis, GA Fink,
J. Fritsch, B. Wrede, and G. Sagerer. 2004. Biron?the
bielefeld robot companion. In Proc. Int. Workshop on Ad-
vances in Service Robotics, pages 27?32. Stuttgart, Ger-
many: Fraunhofer IRB Verlag.
[Hieronymus et al.2006] J. Hieronymus, G. Aist, and
J. Dowding. 2006. Open microphone speech under-
standing: correct discrimination of in domain speech. In
Proceedings of 2006 IEEE international conference on
acoustics, speech, and signal processing, volume 1. IEEE.
[Hofmann et al.2013] H. Hofmann, U. Ehrlich, A. Berton,
A. Mahr, R. Math, and C. M?uller. 2013. Evaluation of
speech dialog strategies for internet applications in the car.
In Proceedings of the SIGDIAL 2013 Conference, pages
233?241, Metz, France, August. Association for Compu-
tational Linguistics.
[Horvitz et al.2003] E. Horvitz, C. Kadie, T. Paek, and
D. Hovel. 2003. Models of attention in computing and
communication: from principles to application. In Com-
munications of the ACM, volume 46, pages 52?59.
[Hosoya et al.2009] K. Hosoya, T. Ogawa, and T. Kobayashi.
2009. Robot auditory system using head-mounted square
microphone array. In Intelligent Robots and Systems,
2009. IROS 2009. IEEE/RSJ International Conference on,
pages 2736?2741. IEEE.
[Kollar et al.2012] T. Kollar, A. Vedantham, C. Sobel,
C. Chang, V. Perera, and M. Veloso. 2012. A multi-modal
approach for natural human-robot interaction. In Proceed-
ings of 2012 International Conference on Social Robots.
[Kun et al.2007] A. Kun, T. Paek, and Z. Medenica. 2007.
The effect of speech interface accuracy on driving perfor-
mance. In INTERSPEECH, pages 1326?1329.
[Misu et al.2013] T. Misu, A. Raux, I. Lane, J. Devassy, and
R. Gupta. 2013. Situated multi-modal dialog system in
vehicles. In Proceedings of the 6th Workshop on Eye Gaze
in Intelligent Human Machine Interaction: Gaze in Multi-
modal Interaction, pages 25?28. ACM.
[Nakano and Ishii2010] Y. Nakano and R. Ishii. 2010. Es-
timating user?s engagement from eye-gaze behaviors in
human-agent conversations. In Proceedings of the 15th
international conference on Intelligent user interfaces,
pages 139?148. ACM.
[Nakashima et al.2014] Taichi Nakashima, Kazunori Ko-
matani, and Satoshi Sato. 2014. Integration of multiple
sound source localization results for speaker identifica-
tion in multiparty dialogue system. In Natural Interaction
with Robots, Knowbots and Smartphones, pages 153?165.
Springer New York.
[Sabanovic et al.2006] S. Sabanovic, M.P. Michalowski, and
R. Simmons. 2006. Robots in the wild: Observing
human-robot social interaction outside the lab. In Ad-
vanced Motion Control, 2006. 9th IEEE International
Workshop on, pages 596?601. IEEE.
[Sharp et al.1997] R.D. Sharp, E. Bocchieri, C. Castillo,
S. Parthasarathy, C. Rath, M. Riley, and J. Rowland. 1997.
The watson speech recognition engine. In Proceedings of
1997 IEEE international conference on acoustics, speech,
and signal processing, volume 5, pages 4065?4068. IEEE.
[Skantze and Al Moubayed2012] G. Skantze and
S. Al Moubayed. 2012. Iristk: a statechart-based
toolkit for multi-party face-to-face interaction. In Proc.
of the 14th ACM International Conference on Multimodal
Interaction ICMI.
[Stent et al.1999] A. Stent, J. Dowding, J. Gawron, E. Bratt,
and R. Moore. 1999. The commandtalk spoken dialogue
system. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Computa-
tional Linguistics, pages 183?190. ACL.
67
Proceedings of the SIGDIAL 2014 Conference, pages 194?198,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems
Aasish Pappu Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
{aasish, air}@cs.cmu.edu
Abstract
Many goal-oriented dialog agents are ex-
pected to identify slot-value pairs in a
spoken query, then perform lookup in
a knowledge base to complete the task.
When the agent encounters unknown slot-
values, it may ask the user to repeat or re-
formulate the query. But a robust agent
can proactively seek new knowledge from
a user, to help reduce subsequent task fail-
ures. In this paper, we propose knowledge
acquisition strategies for a dialog agent
and show their effectiveness. The acquired
knowledge can be shown to subsequently
contribute to task completion.
1 Introduction
Many spoken dialog agents are designed to per-
form specific tasks in a specified domain e.g., in-
formation about public events in a city. To carry
out its task, an agent parses an input utterance, fills
in slot-value pairs, then completes the task. Some-
times, information on these slot-value pairs may
not be available in its knowledge base. In such
cases, typically the agent categorizes utterances as
non-understanding errors. Ideally the incident is
recorded and the missing knowledge is incorpo-
rated into the system with a developer?s assistance
? a slow offline process.
There are other sources of knowledge: automat-
ically crawling the web, as done by NELL [Carl-
son et al., 2010], and community knowledge
bases such as Freebase [Bollacker et al., 2008].
These approaches provide globally popular slot-
values [Araki, 2012] and high-level semantic con-
texts [Pappu and Rudnicky, 2013]. Despite their
size, these knowledge bases may not contain in-
formation about the entities in a specific target
domain. However, users in the agent?s domain
can potentially provide specific information on
slot/values that are unavailable on the web, e.g.,
regarding a recent interest/hobby of the user?s
friend. Lasecki et al. [2013] have elicited natu-
ral language dialogs from humans to build NLU
models for the agent and Bigham et al. [2010]
have elicited answers to visual questions by in-
tegrating users into the system. One observation
from this work is that both users and non-users
can impart useful knowledge to system. In this
paper we propose spoken language strategies that
allow an agent to elicit new slot-value pairs from
its own user population to extend its knowledge
base. Open-domain knowledge may be elicited
through text-based questionnaires from non-users
of the system, but in a situated interaction scenario
spoken strategies may be more effective. We ad-
dress the following research questions:
1. Can an agent elicit reliable knowledge about
its domain from users? Particularly knowl-
edge it cannot locate elsewhere (e.g., on-line
knowledge bases). Is the collective knowl-
edge of the users sufficient to allow the agent
to augment its knowledge through interactive
means?
2. What strategies elicit useful knowledge from
users? Based on previous work in com-
mon sense knowledge acquisition [Von Ahn,
2006, Singh et al., 2002, Witbrock et al.,
2003], we devise spoken language strategies
that allow the system to solicit information by
presenting concrete situations and by asking
user-centric questions.
We address these questions in the context of the
EVENTSPEAK dialog system, an agent that provides
information about seminars and talks in an aca-
demic environment. This paper is organized as
follows. In Section 2, we discuss knowledge ac-
quisition strategies. In Section 3, we describe a
user study on these strategies. Then, we present
an evaluation on system acquired knowledge and
finally we make concluding remarks.
194
Table 1: System initiated strategies used by the agent for knowledge acquisition in the EVENTSPEAK system.
StrategyType Strategy Example Prompt
QUERYDRIVEN
QUERYEVENT I know events on campus. What do you want to know?
QUERYPERSON I know some of the researchers on campus.Whom do you want to know about?
PERSONAL
BUZZWORDS What are some of the popular phrases in your research?
FAMOUSPEOPLE Tell me some well-known people in your research area
SHOW&ASK
TWEET How would you describe this talk in a sentence, say a tweet.
KEYWORDS Give keywords for this talk in your own words.
PEOPLE Do you know anyone who might be interested in this talk?
2 Knowledge Acquisition Strategies
We posit three different circumstances that can
trigger knowledge acquisition behavior: (1) initi-
ated by expert users of the system [Holzapfel et al.,
2008, Spexard et al., 2006, L?utkebohle et al., 2009,
Rudnicky et al., 2010], (2) triggered by ?misun-
derstanding? of the user?s input [Chung et al.,
2003, Filisko and Seneff, 2005, Prasad et al., 2012,
Pappu et al., 2014], or (3) triggered by the system.
They are described below:
QUERYDRIVEN. The system prompts a user
with an open-ended question akin to ?how-may-I-
help-you? to learn what ?values? of a slot are of
interest to the user. This strategy does not ground
user about system?s knowledge limitations. How-
ever, it allows the system to acquire information
(slot-value pairs) from user?s input. The system
can choose to respond to the input or ignore the
input depending on its knowledge about the slot-
value pairs in the input. Table 1 shows strategies
of this kind i.e., QUERYEVENT and QUERYPERSON.
PERSONAL. The system asks a user about their
own interests and people who may share those in-
terests. This is an open-ended request as well, but
the system expects the response to be confined to
the user?s knowledge about specific entities in the
environment. BUZZWORDS and FAMOUSPEOPLE ex-
pects the user to provide values for the slots.
SHOW&ASK. The system provides a descrip-
tion of an event and asks questions to ground
user?s responses in relation to that event. E.g.,
given the title and abstract of a technical talk,
the system asks the user questions about the talk.
TWEET strategy is expected to elicit a concise de-
scription of the event, which eventually may help
the agent to both summarize events for other users
and identify keywords for an event. KEYWORDS
strategy expects the user to explicitly supply key-
words for an event. PEOPLE strategy expects the
user to provide names of likely event participants.
We hypothesized that these strategies may allow
the agent to learn new slot-value pairs that may
help towards better task performance.
3 Knowledge Acquisition Study
We conducted a user study to determine reliability
of the information acquired by the system. We per-
formed this study using the EVENTSPEAK
1
dialog
system, which provides information about upcom-
ing talks and other events that might be of inter-
est, and about ongoing research on campus. The
system presents material on a screen and accepts
spoken input, in a context similar to a kiosk.
The study evaluated performance of the seven
strategies described above. For SHOW&ASK strate-
gies, we had users respond regarding a specific
event. We used descriptions of research talks col-
lected from the university?s website. We used a
web-based interface for data collection; the inter-
face presented the prompt material and recorded
the subject?s voice response. Testvox
2
was used
to setup the experiments and Wami
3
for audio
recording.
3.1 User Study Design
We recruited 40 researchers (graduate students)
from the School of Computer Science, at Carnegie
Mellon, representative of the user population for
the EVENTSPEAK dialog system. Each subject re-
sponded to prompts from the QUERYDRIVEN, PER-
SONAL and SHOW&ASK strategies.
In the QUERYDRIVEN tasks, the QUERYEVENT
strategy, the system responds to the user?s query
with a list of talks. The user?s response is
recorded, then sent to an open-vocabulary speech
recognizer; the result is used as a query to a
database of talks. The results are then displayed on
the screen. The system applies the QUERYPERSON
strategy in a similar way. In the PERSONAL tasks,
the system applies the BUZZWORDS strategy to ask
the user about popular keyphrases in their research
1
http://www.speech.cs.cmu.edu/apappu/kacq
2
https://bitbucket.org/happyalu/testvox/wiki/Home
3
https://code.google.com/p/wami-recorder/
195
Figure 1: Time per Task for all strategies
Q
u
e
r
y
E
v
e
n
t
Q
u
e
r
y
P
e
r
s
o
n
B
u
z
z
w
o
r
d
s
F
a
m
o
u
s
P
e
o
p
l
e
T
w
e
e
t
P
e
o
p
l
e
K
e
y
w
o
r
d
s
0
1
2
3
4
1.51
2.23
0.91
0.71
2.51
0.69
0.97
T
i
m
e
i
n
m
i
n
u
t
e
s
Figure 2: Time per Task vs Expertise
E
x
p
e
r
t
L
e
v
e
l
1
E
x
p
e
r
t
L
e
v
e
l
2
E
x
p
e
r
t
L
e
v
e
l
3
E
x
p
e
r
t
L
e
v
e
l
4
0
1
2
3
4
T
i
m
e
i
n
m
i
n
u
t
e
s
tweet
people
keywords
area. The system then asks about well-known re-
searchers (FAMOUSPEOPLE) in the user?s area.
In the SHOW&ASK tasks, we use two seminar
descriptions per subject (in our pilot study, we
found that people provide more diverse responses
(in term of entities) in the SHOW&ASK based on
the event abstract, compared to PERSONAL, QUERY-
DRIVEN). We used a set of 80 research talk an-
nouncements (consisting of a title, abstract and
other information). For each talk, the system used
all three strategies viz., TWEET, KEYWORDS and PEO-
PLE. For the TWEET tasks, subjects were asked to
provide a one sentence description. They were al-
lowed to give a non-technical/high-level descrip-
tion if they were unfamiliar with the topic. For
the PEOPLE task, subjects had to give names of col-
leagues who might be interested in the talk. For
the KEYWORDS task, subjects provided keywords,
either their own words or ones selected from the
abstract.
Since the material is highly technical, we were
interested whether the tasks are cognitively de-
manding for people who are less familiar with the
subject of a talk. Therefore, users were asked to
indicate their familiarity with a particular talk (re-
search area in general) using a scale of 1?4: 4 be-
ing more familiar and 1 being less familiar.
3.2 Corpus Description
This user study produced 64 minutes of audio data,
on average 1.6 minutes per subject. We tran-
scribed the speech then annotated the corpus for
people names, and for research interests. Table 2
shows the number of unique slot-values found in
the corpus. We observe that the number of unique
research interests produced during SHOW&ASK is
higher than for other strategies. This confirms
our initial observations that this strategy elicits
diverse responses. The PERSONAL task produced
a relatively higher number of researcher names
(FAMOUSPEOPLE strategy) than other tasks. One ex-
planation might be that people may find it easier
to recall names in their own research area, as com-
pared to other areas. Overall, we identified 139
unique researcher names and 485 interests.
Table 2: Corpus Statistics
StrategyType
Unique
Researcher
Names
Unique
Research
Interests
QUERYDRIVEN 21 30
PERSONAL 77 107
SHOW&ASK 76 390
Overall 139 485
3.3 Corpus Analysis
One of the objectives of this work is to determine
What strategies can the agent use to elicit knowl-
edge from users? Although, time-cost will vary
with task and domain, a usable strategy should, in
general, be less demanding. We analyzed the time-
per-task for each strategy, shown in Figure 1. We
found that the TWEET strategy is not only more de-
manding, it has higher variance than other tasks.
One explanation is that people would attempt to
summarize the entire abstract including technical
details, despite the instructions indicated that a
non-technical description was acceptable. We can
see a similar trend in Figure 2 that irrespective
of expertise-level, subjects take more time to give
one sentence descriptions. We also observe high
variance and higher time-per-task for QUERYPER-
SON; this is due to the system deliberately not re-
turning any results for this task. This was done to
196
Table 3: Mean Precision for 200 researchers, broken down by the ?source? strategy used to acquire their name
Note: Only 85 of 200 researchers had Google Scholar pages, GScholar Accuracy is computed for only those 85.
Metric Description Text SHOW&ASK PERSONAL QUERYDRIVEN mean
Mean Precision 89.5% 86.9% 93.6% 86.2% 90.5%
GScholar Acc. 78.3% 82.3% 86.1% 100% 80.0%
find out whether subjects would repeat the task on
failure. Ideally the system needs to only rarely use
this strategy to not lose user?s trust and solicit mul-
tiple values for a given slot (e.g., person name) as
opposed to requesting list of values as in FAMOUS-
PEOPLE and PEOPLE strategies. We find that PEOPLE,
KEYWORDS, FAMOUSPEOPLE and BUZZWORDS strate-
gies are efficient with a time-per-task of less than
one minute. As shown in Figure 2, subjects do not
take much time to speak a list of names or key-
words.
4 Evaluation of Acquired Knowledge
To answer Can an agent elicit reliable knowl-
edge about its domain from users? we analyzed
the relevance of acquired knowledge. We have
two disjoint list of entities, (a) researchers and
(b) research interests; in addition we have speaker
names from the talk descriptions. Our goal is
to implicitly infer a list of interests for each re-
searcher without soliciting the user for the inter-
ests of every researcher exhaustively. To each re-
searcher in the list, we attribute list of interests that
were mentioned in the same context as researcher
was mentioned. We tag list of names acquired
from the FAMOUSPEOPLE strategy with list of key-
words acquired from the BUZZWORDS strategy ?
both lists acquired from same user. We repeat this
process for each name mentioned in relation to a
talk in the SHOW&ASK strategy. We tag keywords
mentioned in the KEYWORDS strategy to researchers
mentioned in the PEOPLE strategy.
4.1 Analysis
We produced 200 entries for researchers and their
set of interests. We then had two annotators (se-
nior graduate students) mark whether the system-
predicted interests were relevant/accurate. The an-
notators were allowed to use information found on
researchers? home pages and Google Scholar
4
to
evaluate the system-predicted interests.
This can be seen as an information retrieval (IR)
problem, where researcher is ?query? and interests
are ?documents?. So, we use Mean Precision, a
4
scholar.google.com
common metric in IR, to evaluate retrieval. In our
case, the ground truth for relevant interests comes
from the annotators. The results are shown in Ta-
ble 3. Our approach has high precision, 90.5%,
for all 200 researchers. We see that irrespective
of the strategy used to acquire entities, precision
is good. We also compared our predicted inter-
ests with interests listed by researchers themselves
on Google Scholar. There are only 85 researchers
from our list with a Google Scholar page; for these
our accuracy is 80%, again good. Moreover, sig-
nificant knowledge is absent from the web (at least
in our domain) yet can be elicited from users fa-
miliar with the domain.
5 Conclusion
We describe a set of knowledge acquisition strate-
gies that allow a system to solicit novel informa-
tion from users in a situated environment. To in-
vestigate the usability of these strategies, we con-
ducted a user study in the domain of research talks.
We analyzed a corpus of system-acquired knowl-
edge and have made the material available
5
. Our
data show that users on average take less than a
minute to provide new information using the pro-
posed elicitation strategies. The reliability of ac-
quired knowledge in predicting relationships be-
tween researchers and interests is quite good, with
a mean precision of 90.5%. We note that the PER-
SONAL strategy, which tries to tap personal knowl-
edge, appears to be particularly effective. More
generally, automated elicitation appears to be a
promising technique for continuous learning in
spoken dialog systems.
6 Appendix
System Predicted Researcher-Interests 1
rich stern deep neural networks, speech recog-
nition, signal processing, neural networks, machine
learning, speech synthesis
5
www.speech.cs.cmu.edu/apappu/pubdl/eventspeak corpus.zip
197
System Predicted Researcher-Interests 2
kishore prahallad dialogue systems, prosody,
speech synthesis, text to speech, pronunciation mod-
eling, low resource languages
System Predicted Researcher-Interests 3
carolyn rose crowdsourcing, meta discourse clas-
sification, statistical analysis, presentation skills in-
struction, man made system, education models, human
learning
System Predicted Researcher-Interests 4
florian metze dialogue systems, speech recogni-
tion, nlp, prosody, speech synthesis, text to speech,
pronunciation modeling, low resource languages, au-
tomatic accent identification
System Predicted Researcher-Interests 5
madhavi ganapathiraju protein structure, contin-
uous graphical models, generative models, structural
biology, protein structure dynamics, molecular dy-
namics
System Predicted Researcher-Interests 6
alexander hauptmann discriminatively trained
models, deep learning, computer vision, big data
System Predicted Researcher-Interests 7
jamie callan learning to rank, search, large scale
search, web search, click prediction, information re-
trieval, web mining, user activity, recommendation,
relevance, machine learning, web crawling, distributed
systems, structural similarity
System Predicted Researcher-Interests 8
lori levin natural language understanding, knowl-
edge reasoning, construction grammar, knowledge
bases, natural language processing
References
Masahiro Araki. Rapid development process of spoken dia-
logue systems using collaboratively constructed semantic
resources. In Proceedings of the SIGDIAL 2012 Confer-
ence, pages 70?73. ACL, 2012.
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little,
Andrew Miller, Robert C Miller, Robin Miller, Aubrey
Tatarowicz, Brandyn White, Samual White, et al. Vizwiz:
nearly real-time answers to visual questions. In Proceed-
ings of the 23rd ACM Symposium on User Interface soft-
ware and technology, pages 333?342. ACM, 2010.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge,
and Jamie Taylor. Freebase: a collaboratively created
graph database for structuring human knowledge. Pro-
ceedings of the SIGMOD, pages 1247?1249, 2008.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Set-
tles, Estevam R Hruschka Jr., and Tom M Mitchell. To-
ward an Architecture for Never-Ending Language Learn-
ing. Artificial Intelligence, 2(4):1306?1313, 2010.
Grace Chung, Stephanie Seneff, and Chao Wang. Automatic
acquisition of names using speak and spell mode in spo-
ken dialogue systems. In Proceedings of the NAACL-HLT,
pages 32?39. ACL, 2003.
Edward Filisko and Stephanie Seneff. Developing city name
acquisition strategies in spoken dialogue systems via user
simulation. In 6th SIGdial Workshop on Discourse and
Dialogue, 2005.
Hartwig Holzapfel, Daniel Neubig, and Alex Waibel. A dia-
logue approach to learning object descriptions and seman-
tic categories. Robotics and Autonomous Systems, 56(11):
1004?1013, November 2008.
Walter Stephen Lasecki, Ece Kamar, and Dan Bohus. Con-
versations in the crowd: Collecting data for task-oriented
dialog learning. In First AAAI Conference on Human
Computation and Crowdsourcing, 2013.
Ingo L?utkebohle, Julia Peltason, Lars Schillingmann,
Christof Elbrechter, Britta Wrede, Sven Wachsmuth, and
Robert Haschke. The Curious Robot: Structuring Inter-
active Robot Learning. In ICRA?09, pages 4156?4162.
IEEE, 2009.
Aasish Pappu and Alexander Rudnicky. Predicting tasks
in goal-oriented spoken dialog systems using semantic
knowledge bases. In Proceedings of the SIGDIAL, pages
242?250. ACL, 2013.
Aasish Pappu, Teruhisa Misu, and Rakesh Gupta. Investi-
gating critical speech recognition errors in spoken short
messages. In Proceedings of IWSDS, pages 39?49, 2014.
Rohit Prasad, Rohit Kumar, Sankaranarayanan Ananthakr-
ishnan, Wei Chen, Sanjika Hewavitharana, Matthew Roy,
Frederick Choi, Aaron Challenner, Enoch Kan, Arvind
Neelakantan, et al. Active error detection and resolu-
tion for speech-to-speech translation. In Proceedings of
IWSLT, 2012.
Alexander I Rudnicky, Aasish Pappu, Peng Li, and Matthew
Marge. Instruction Taking in the TeamTalk System. In
Proceedings of the AAAI Fall Symposium on Dialog with
Robots, pages 173?174, 2010.
Push Singh, Thomas Lin, Erik T Mueller, Grace Lim, Trav-
ell Perkins, and Wan Li Zhu. Open mind common
sense: Knowledge acquisition from the general public. In
CoopIS, DOA, and ODBASE, pages 1223?1237. Springer,
2002.
Thorsten Spexard, Shuyin Li, Britta Wrede, Jannik Fritsch,
Gerhard Sagerer, Olaf Booij, Zoran Zivkovic, Bas Ter-
wijn, and Ben Krose. BIRON, where are you? Enabling
a robot to learn new places in a real home environment by
integrating spoken dialog and visual localization. Integra-
tion The VLSI Journal, (section II):934?940, 2006.
Luis Von Ahn. Games with a purpose. Computer, 39(6):
92?94, 2006.
Michael Witbrock, David Baxter, Jon Curtis, Dave Schneider,
Robert Kahlert, Pierluigi Miraglia, Peter Wagner, Kathy
Panton, Gavin Matthews, and Amanda Vizedom. An inter-
active dialogue system for knowledge acquisition in cyc.
In Proceedings of the 18th IJCAI, pages 138?145, 2003.
198
Proceedings of the 8th International Natural Language Generation Conference, pages 99?102,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Two-Stage Stochastic Email Synthesizer
Yun-Nung Chen and Alexander I. Rudnicky
School of Computer Science, Carnegie Mellon University
5000 Forbes Ave., Pittsburgh, PA 15213-3891, USA
{yvchen, air}@cs.cmu.edu
Abstract
This paper presents the design and im-
plementation details of an email synthe-
sizer using two-stage stochastic natural
language generation, where the first stage
structures the emails according to sender
style and topic structure, and the second
stage synthesizes text content based on the
particulars of an email structure element
and the goals of a given communication
for surface realization. The synthesized
emails reflect sender style and the intent of
communication, which can be further used
as synthetic evidence for developing other
applications.
1 Introduction
This paper focuses on synthesizing emails that re-
flect sender style and the intent of the communica-
tion. Such a process might be used for the gener-
ation of common messages (for example a request
for a meeting without direct intervention from the
sender). It can also be used in situations where nat-
uralistic emails are needed for other applications.
For instance, our email synthesizer was developed
to provide emails to be used as part of synthetic
evidence of insider threats for purposes of train-
ing, prototyping, and evaluating anomaly detec-
tors (Hershkop et al., 2011).
Oh and Rudnicky (2002) showed that stochas-
tic generation benefits from two factors: 1) it
takes advantage of the practical language of a do-
main expert instead of the developer and 2) it re-
states the problem in terms of classification and
labeling, where expertise is not required for de-
veloping a rule-based generation system. In the
present work we investigate the use of stochastic
techniques for generation of a different class of
communications and whether global structures can
be convincingly created. Specifically we inves-
tigate whether stochastic techniques can be used
to acceptably model longer texts and individual
sender characteristics in the email domain, both of
which may require higher cohesion to be accept-
able (Chen and Rudnicky, 2014).
Our proposed system involves two-stage
stochastic generation, shown in Figure 1, in which
the first stage models email structures according
to sender style and topic structure (high-level
generation), and the second stage synthesizes
text content based on the particulars of a given
communication (surface-level generation).
2 The Proposed System
The whole architecture of the proposed system is
shown in left part of Figure 1, which is composed
of preprocessing, first-stage generation for email
organization, and second-stage generation for sur-
face realization.
In preprocessing, we perform sentence segmen-
tation for each email, and then manually anno-
tate each sentence with a structure element, which
is used to create a structural label sequence for
each email and then to model sender style and
topic structure for email organization (1st stage in
the figure). The defined structural labels include
greeting, inform, request, suggestion, question,
answer, regard, acknowledgement, sorry, and sig-
nature. We also annotate content slots, including
general classes automatically created by named
entity recognition (NER) (Finkel et al., 2005) and
hand-crafted topic classes, to model text content
for surface realization (2nd stage in the figure).
The content slots include person, organization, lo-
cation, time, money, percent, and date (general
classes), and meeting, issue, and discussion (topic
classes).
2.1 Modeling Sender Style and Topic
Structure for Email Organization
In the first stage, given the sender and the fo-
cused topic from the input, we generate the email
structures by predicted sender-topic-specific mix-
ture models, where the detailed is illustrated as be-
99
Predicting 
Mixture 
Models 
Email Archive 
Building 
Structure 
LM 
Structural Label Annotation 
Structural Label Sequences 
Generating 
Email 
Structures 
Generated Structural 
Label Sequences 
<greeting> 
<inform> 
? 
Slot-Value Pairs 
Slot Annotation 
Emails w/ Slots 
Building 
Content 
LM 
Generating 
Text 
Content 
Scoring 
Email 
Candidates 
Filling 
Slots 
Synthesized Emails 
Hi Peter 
Today?s ... 
1st-Stage Generation 
2nd-Stage Generation 
Sender 
Model 
Topic 
Model 
Sender Topic 
System Input Preprocessing 
Synthesized Emails 
with Slots 
Hi [person] 
Today?s ... 
Request is generated at 
the narrative level. 
Form filling: 
? Topic Model 
? Sender Model 
? Slot fillers 
Figure 1: The system architecture (left) and the demo synthesizer (right).
low.
2.1.1 Building Structure Language Models
Based on the annotation of structural labels, each
email can be transformed into a structural label
sequence. Then we train a sender-specific struc-
ture model using the emails from each sender and
a topic-specific model using the emails related to
each topic. Here the structure models are tri-
gram models with Good-Turing smoothing (Good,
1953).
2.1.2 Predicting Mixture Models
With sender-specific and topic-specific structure
models, we predict the sender-topic-specific mix-
ture models by interpolating the probabilities of
two models.
2.1.3 Generating Email Structures
We generate structural label sequences randomly
according to the distribution from sender-topic-
specific models. Smoothed trigram models may
generate any unseen trigrams based on back-off
methods, resulting in more randomness. In ad-
dition, we exclude unreasonable emails that don?t
follow two simple rules.
1. The structural label ?greeting? only occurs at
the beginning of the email.
2. The structural label ?signature? only occurs
at the end of the email.
2.2 Surface Realization
In the second stage, our surface realizer consists
of four aspects: building content language models,
generating text content, scoring email candidates,
and filling slots.
2.2.1 Building Content Language Models
After replacing the tokens with the slots, for each
structural label, we train an unsmoothed 5-gram
language model using all sentences belonging to
the structural label. Here we assume that the usage
of within-sentence language is independent across
senders and topics, so generating the text content
only considers the structural labels. Unsmoothed
5-gram language models introduce some variabil-
ity in the output sentences while preventing non-
sense sentences.
2.2.2 Generating Text Content
The input to surface realization is the generated
structural label sequences. We use the correspond-
ing content language model for the given struc-
tural label to generate word sequences randomly
according to distribution from the language model.
Using unsmoothed 5-grams will not generate
any unseen 5-grams (or smaller n-grams at the
beginning and end of a sentence), avoiding gen-
eration of nonsense sentences within the 5-word
window. With a structural label sequence, we can
generate multiple sentences to form a synthesized
email.
100
2.3 Scoring Email Candidates
The input to the system contains the required in-
formation that should be included in the synthe-
sized result. For each synthesized email, we penal-
ize it if the email 1) contains slots for which there
is no provided valid value, or 2) does not have
the required slots. The content generation engine
stochastically generates a candidate email, scores
it, and outputs it when the synthesized email with
a zero penalty score.
2.4 Filling Slots
The last step is to fill slots with the appropriate
values. For example, the sentence ?Tomorrow?s
[meeting] is at [location].? becomes ?Tomorrow?s
speech seminar is at Gates building.? The right
part of Figure 1 shows the process of the demo sys-
tem,where based on a specific topic, a sender, and
an interpolation weight, the system synthesizes an
email with structural labels first and then fills slots
with given slot fillers.
3 Experiments
We conduct a preliminary experiment to evaluate
the proposed system. The corpus used for our ex-
periments is the Enron Email Dataset1, which con-
tains a total of about 0.5M messages. We selected
the data related to daily business for our use. This
includes data from about 150 users, and we ran-
domly picked 3 senders, ones who wrote many
emails, and define additional 3 topic classes (meet-
ing, discussion, issue) as topic-specific entities
for the task. Each sender-specific model (across
topics) or topic-specific model (across senders) is
trained on 30 emails.
3.1 Evaluation of Sender Style Modeling
To evaluate the performance of sender style, 7 sub-
jects were given 5 real emails from each sender
and then 9 synthesized emails. They were asked
to rate each synthesized email for each sender on
a scale between 1 to 5.
With higher weight for sender-specific model
when predicting mixture models, average normal-
ized scores the corresponding senders receives ac-
count for 45%, which is above chance (33%). This
suggests that sender style can be noticed by sub-
jects. In a follow-up questionnaire, subjects indi-
cated that their ratings were based on greeting us-
age, politeness, the length of email and other char-
acteristics.
1https://www.cs.cmu.edu/?enron/
3.2 Evaluation of Surface Realization
We conduct a comparative evaluation of two
different generation algorithms, template-based
generation and stochastic generation, on the
same email structures. Given a structural label,
template-based generation consisted of randomly
selecting an intact whole sentence with the target
structural label. This could be termed sentence-
level NLG, while stochastic generation is word-
level NLG.
We presented 30 pairs of (sentence-, word-)
synthesized emails, and 7 subjects were asked to
compare the overall coherence of an email, its
sentence fluency and naturalness; then select their
preference. The experiments showed that word-
based stochastic generation outperforms or per-
forms as well as the template-based algorithm
for all criteria (coherence, fluency, naturalness,
and preference). Some subjects noted that nei-
ther email seemed human-written, perhaps an ar-
tifact of our experimental design. Nevertheless,
we believe that this stochastic approach would re-
quire less effort compared to most rule-based or
template-based systems in terms of knowledge en-
gineering.
In the future, we plan to develop an automatic
email structural label annotator in order to build
better language models (structure language mod-
els and content language models) by increasing
training data, and then improve the naturalness of
synthesized emails.
4 Conclusion
This paper illustrates a design and implementation
of an email synthesizer with two-stage stochastic
NLG: first a structure is generated, and then text is
generated for each structure element. Here sender
style and topic structure can be modeled. We be-
lieve that this system can be applied to create re-
alistic emails and could be carried out using mix-
tures containing additional models based on other
characteristics. The proposed system shows that
emails can be synthesized using a small corpus of
labeled data, and the performance seems accept-
able; however these models could be used to boot-
strap the labeling of a larger corpus which in turn
could be used to create more robust models.
Acknowledgments
The authors wish to thank Brian Lindauer and Kurt
Wallnau from the Software Engineering Institute
of Carnegie Mellon University for their guidance,
advice, and help.
101
References
Yun-Nung Chen and Alexander I. Rudnicky. 2014.
Two-stage stochastic natural language generation for
email synthesis by modeling sender style and topic
structure. In Proceedings of the 8th International
Natural Language Generation Conference.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Irving J Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3-4):237?264.
Shlomo Hershkop, Salvatore J Stolfo, Angelos D
Keromytis, and Hugh Thompson. 2011. Anomaly
detection at multiple scales (ADAMS).
Alice H Oh and Alexander I Rudnicky. 2002.
Stochastic natural language generation for spoken
dialog systems. Computer Speech & Language,
16(3):387?407.
102
Proceedings of the 8th International Natural Language Generation Conference, pages 152?156,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Two-Stage Stochastic Natural Language Generation for Email Synthesisby Modeling Sender Style and Topic Structure
Yun-Nung Chen and Alexander I. Rudnicky
School of Computer Science, Carnegie Mellon University
5000 Forbes Ave., Pittsburgh, PA 15213-3891, USA
{yvchen, air}@cs.cmu.edu
Abstract
This paper describes a two-stage pro-
cess for stochastic generation of email, in
which the first stage structures the emails
according to sender style and topic struc-
ture (high-level generation), and the sec-
ond stage synthesizes text content based
on the particulars of an email element
and the goals of a given communication
(surface-level realization). Synthesized
emails were rated in a preliminary experi-
ment. The results indicate that sender style
can be detected. In addition we found
that stochastic generation performs better
if applied at the word level than at an
original-sentence level (?template-based?)
in terms of email coherence, sentence flu-
ency, naturalness, and preference.
1 Introduction
This paper focuses on generating language for the
email domain, with the goal of producing mails
that reflect sender style and the intent of the com-
munication. Such a process might be used for the
generation of common messages (for example a
request for a meeting without direct intervention
from the sender). It can also be used in situations
where naturalistic email is needed for other ap-
plications. For instance, our email generator was
developed to provide emails to be used as part of
synthetic evidence of insider threats for purposes
of training, prototyping, and evaluating anomaly
detectors (Hershkop et al., 2011).
There are two approaches to natural language
generation (NLG), one focuses on generating text
using templates or rules (linguistic) methods, the
another uses corpus-based statistical techniques.
Oh and Rudnicky (2002) showed that stochastic
generation benefits from two factors: 1) it takes
advantage of the practical language of a domain
expert instead of the developer and 2) it restates
the problem in terms of classification and label-
ing, where expertise is not required for developing
a rule-based generation system. They found that
naive listeners found such utterances as accept-
able as human-generated utterances. Belz (2005)
also proposed a probabilistic NLG approach to
make systems more robust and components more
reusable, reducing manual corpus analysis.
However, most work usually focused on well-
structured documents such as news and Wikipedia,
while email messages differ from them, which
reflect senders? style and are more spontaneous.
Lampert et al. (2009) segmented email messages
into zones, including sender zones, quoted con-
versation zones, and boilerplate zones. This paper
only models the text in the sender zone, new con-
tent from the current sender. In the present work,
we investigate the use of stochastic techniques for
generation of a different class of communications
and whether global structures can be convincingly
created in the email domain.
A lot of NLG systems are applied in dialogue
systems, some of which focus on topic model-
ing (Sauper and Barzilay, 2009; Barzilay and Lap-
ata, 2008; Barzilay and Lee, 2004), proposing al-
gorithms to balance local fit of information and
global coherence. However, they seldom con-
sider to model the speaker?s characteristics. Gill
et al. (2012) considered sentiment such as open-
ness and neuroticism to specify characters for di-
alogue generation. In stead of modeling authors?
attitudes, this paper proposes the first approach of
synthesizing emails by modeling their writing pat-
terns. Specifically we investigate whether stochas-
tic techniques can be used to acceptably model
longer texts and individual speaker characteristics
in the emails, both of which may require higher
cohesion to be acceptable.
2 Overview of Framework
Our proposed NLG approach has three steps: pre-
processing training data, modeling sender style
and topic structure for email organization, fol-
lowed by surface realization, shown in Figure 1.
In preprocessing, we segment sentences for
each email, and label email structural elements.
This is used to create a structural label sequence
for each email, and then used to model sender
style and topic structure for email organization
(1st stage in the figure). Content slots are also
annotated for surface realization (2nd stage in the
figure). Details are in Section 3.
From the annotated corpus, we build sender-
specific and topic-specific structure language
models based on structural label sequences, and
use a mixture sender-topic-specific model to
stochastically generate email structure in the first
stage. The process is detailed in Section 4.
152
Predicting 
Mixture 
Models 
Email 
Document 
Archive 
Building 
Structure 
LM Structural Label 
Annotation Structural 
Label 
Sequences 
Generating 
Email 
Structures 
Generated Structural 
Label Sequences 
<greeting> 
<inform> 
? 
Slot-Value Pairs 
Slot 
Annotation Emails w/ 
Slots 
Building 
Content 
LM 
Generating 
Text Content 
Scoring 
Email 
Candidates 
Email Candidates 
Filling 
Slots 
Synthesized Emails 
Hi Peter 
Today?s ... 
1st Stage: Modeling Sender Style and Topic Structure for Email Organization 
2nd Stage: Surface Realization 
Hi [Person] 
Today?s ... 
Sender-Specific 
Model 
Topic-Specific 
Model 
Sender Topic 
Input to NLG 
Training Data Preprocessing 
Figure 1: The proposed framework of two-stage NLG component.
In the second stage, we build a content lan-
guage model for each structural element and then
stochastically generate sentences using the se-
quence generated in the first stage. To ensure that
required slot-value pairs occur in the text, candi-
dates emails are filtered to retain only those texts
that contain the desired content slots. These slots
are then filled to produce the final result. Section 5
explains the process.
3 Training Data Preprocessing
To model sender style and topic structure, we an-
notate the data with defined structural labels in
Section 3.1, and data with slots to model text con-
tent of language in Section 3.2.
3.1 Structural Label Annotation
Based on examination of the corpus, we defined
10 email structure elements:
1. greeting: a friendly expression or respectful
phrase, typically at the start of an email.
2. inform: to give or impart knowledge of a fact
or circumstance.
3. request: the act of asking for something to be
given or done, especially as a favor or cour-
tesy.
4. suggestion: to mention or introduce (an idea,
proposition, plan, etc.) for consideration or
possible action.
5. question: an interrogative sentence in an
form, requesting information in reply.
6. answer: a reply or response to a question, etc.
7. regard: to have or show respect or concern
for, usually at the end of an email.
8. acknowledgement: to show or express appre-
ciation or gratitude.
9. sorry: express regret, compunction, sympa-
thy, pity, etc.
10. signature: a sender?s name usually at the end
of the email.
We perform sentence segmentation using punc-
tuation and line-breaks and then manually tag each
sentence with a structure label. We exclude the
header of emails for labeling. Figure 2 shows an
example email with structural labels.
From:  Kitchen, Louise Sent: Thursday, April 05, 2001 11:15 AM To: Beck, Sally Cc: Piper, Greg; Jafry, Rahil Subject: Re: Costs   Shukaly resigned and left. But I assume the invitation will be extended to all of their groups so that whoever they want can attend. 
 I would actually prefer that the presentation is actually circulated to the groups on Friday rather than presented as we will wait forever on getting an offsite together. How about circulating the presentation and then letting them refer all questions to Rahil - see how much interest you get. One on ones are much better and I think this is how Rahil should proceed. 
 We need to get in front of customers in the next couple of weeks. Let's aim to get a least three customers this quarter.  Louise 
suggestion 
inform 
request 
signature 
header 
content 
Figure 2: The email with structural labels.
3.2 Slot Annotation
The input to NLG may contain the information
that needs to be included in the synthesized emails.
Tokens in the corpus text corresponding to slots
are replaced by slot (or concept) tokens prior to
building content language models. Slots are clas-
sified into general class and topic class below.
3.2.1 General Class
We use existing named entity recognition (NER)
tools for identifying general classes. Finkel et al.
(2005) used CRF to label sequences of words in
text that are names of things, such as person, or-
ganization, etc. There are three models trained on
different data, which are a 4-class model trained
for CoNLL1, a 7-class model trained for MUC,
and a 3-class model trained on both data sets for
the intersection of those class sets below.
? 4-class: location, person, organization, misc
? 7-class: location, person, organization, time,
money, percent, date
Considering that 3-class model performs higher
accuracy and 7-class model provides better cover-
age, we take the union of outputs produced by 3-
class and 7-class models and use the labels output
by 3-class model if the two models give different
results, since the 3-class model is trained on both
data sets and provides better accuracy.
1http://www.cnts.ua.ac.be/conll2003/
ner/
153
sender-specific model 
topic-specific model 
mixture model 
Figure 3: The visualization of the mixture model.
3.2.2 Topic Class
Many named entities cannot be recognized by a
general NER, because they are topic-specific in-
formation. Accordingly we define additional enti-
ties that are part of the email domain.
4 Modeling Sender Style and TopicStructure for Email Organization
Given a target sender and topic focus specified in
system input, email structures can be generated by
predicted sender-topic-specific mixture models.
4.1 Building Structure Language Models
Based on the annotation of structural labels, each
email can be expressed as a structural label se-
quence. Then we can train a sender-specific and
a topic-specific structure model using the emails
from each sender and the emails related to each
topic respectively. Here the structure models are
n-gram models with Good-Turing smoothing (n =
3) (Good, 1953).
4.2 Predicting Mixture Models
Using sender-specific and topic-specific structure
models, we predict sender-topic-specific mixture
models by interpolation:
Pi,j(l) = ?P si (l) + (1? ?)P tj (l), (1)
where Pi,j(l) is the estimated probability that thestructural label l occurs from the sender i and for
the topic j, P si (l) is the probability of the struc-tural label l from the sender i (regardless of top-
ics), P tj (l) is the probability of the structural label
l related to the topic j (regardless of senders), and
? is the interpolation weight, balancing between
sender style and topic focus. Figure 3 illustrates
the mixture models combined by sender-specific
and topic-specific models.
4.3 Generating Email Structure
We generate structural label sequences randomly
according to the distribution from sender-topic-
specific models. To generate the structural label
sequences from the sender i and related to the
topic j, the probability of the structural label lkusing n-gram language model is
Pi,j(lk) = Pi,j(lk | lk?1, lk?2, ..., lk?(n?1)). (2)
Since we use smoothed trigrams, we may gen-
erate unseen trigrams based on back-off methods,
resulting in some undesirable randomness. We
therefore exclude unreasonable emails that don?t
follow two simple rules.
1. The structural label ?greeting? only occurs at
the beginning of the email.
2. The structural label ?signature? only occurs
at the end of the email.
5 Surface Realization
Our surface realizer has four elements: building
language models, generating text content, scoring
email candidates, and filling slots.
5.1 Building Content Language Models
After replacing the tokens with slots, for each
structural label, we train an unsmoothed n-gram
language model using all sentences with that struc-
tural label. We make a simplifying assumption
that the usage of within-sentence language can be
treated as independent across senders; generating
the text content only considers the structural la-
bels. We use 5-gram to balance variability in gen-
erated sentences while minimizing nonsense sen-
tences.
Given a structural label, we use the content lan-
guage model probability directly to predict the
next word. The most likely sentence is W ? =
argmaxP (W | l), where W is a word sequence
and l is a structural label. However, in order to
introduce more variation, we do not look for the
most likely sentence but generate each word ran-
domly according to the distribution similar to Sec-
tion 4.3 and illustrated below.
5.2 Generating Text Content
The input to surface realization is the generated
structural label sequence. We use the correspond-
ing content language model trained for the given
structural label to generate word sequences ran-
domly according to the distribution from the lan-
guage model. The probability of a word wi usingthe n-gram language model is
P (wi) = P (wi | wi?1, wi?2, ..., wi?(n?1), l),
(3)
where l is the input structural label. Since we build
separate models for different structural labels, (3)
can be written as
P (wi) = P (wi | wi?1, wi?2, ..., wi?(n?1)) (4)
using the model for l.
Using unsmoothed 5-grams will not generate
any unseen 5-grams (or smaller n-grams at the be-
ginning and end of a sentence). This precludes
generation of nonsense sentences within the 5-
word window. Given a generated structural label
sequence, we can generate multiple sentences to
create a synthesized email.
154
5.3 Scoring Email Candidates
The input to NLG contains the required informa-
tion that needs to be in the output email, as de-
scribed in Section 3.2. For each synthesized email,
we penalize it if the email 1) contains slots for
which there is no provided valid value, or 2) does
not have the required slots.
The content generation engine stochastically
generates an email candidate and scores it. If the
email has a zero penalty it is passed on.
5.4 Filling Slots
The last step is to fill slots with the appropriate
values. For example, the sentence ?Tomorrow?s
[meeting] is at [location].? could become ?Tomor-
row?s speech seminar is at Gates building.?
6 Experiments
6.1 Setup
The corpus used for our experiments is the Enron
Email Dataset2, which contains a total of about
0.5M messages. We selected the data related to
daily business for our use, including data from
about 150 users. We randomly picked 3 senders,
ones who wrote many emails, and defined addi-
tional 3 topic classes (meeting, discussion, issue)
as topic-specific entities for the task. Each sender-
specific model (across topics) or topic-specific
model (across senders) is trained on 30 emails.
6.2 Evaluation of Sender Style Modeling
To evaluate the performance of sender style, 7 sub-
jects were given 5 real emails from each sender
and then 9 synthesized emails. They were asked
to rate each synthesized email for each sender on
a scale of 1 (highly confident that the email is not
from the sender) to 5 (highly confident that the
email is from that sender).
With ? = 0.75 in (1) for predicting mix-
ture models (higher weight for sender-specific
model), average normalized scores the corre-
sponding senders receives account for 45%; this
is above chance (which would be 33%). This sug-
gests that sender style can be noticed by subjects,
although the effect is weak, and we are in the pro-
cess of designing a larger evaluation. In a follow-
up questionnaire, subjects indicated that their rat-
ings were based on greeting usage, politeness, the
length of email and other characteristics.
6.3 Evaluation of Surface Realization
We conduct a comparative evaluation of two dif-
ferent generation algorithms, template-based gen-
eration and stochastic generation, on the same
email structures. The average number of sen-
tences in synthesized emails is 3.8, because our
data is about daily business and has relatively short
emails. Given a structural label, template-based
2https://www.cs.cmu.edu/?enron/
generation consisted of randomly selecting an in-
tact whole sentence with the target structural label.
This could be termed sentence-level NLG, while
stochastic generation is word-level NLG.
We presented 30 pairs of (sentence-, word-)
synthesized emails, and 7 subjects were asked to
compare the overall coherence of an email, its sen-
tence fluency and naturalness; then select their
preference. Table 1 shows subjects? preference
according to the rating criteria. The word-based
stochastic generation outperforms or performs as
well as the template-based algorithm for all cri-
teria, where a t-test on an email as a random vari-
able shows no significant improvement but p-value
is close to 0.05 (p = 0.051). Subjects indicated
that emails from word-based stochastic genera-
tion are more natural; word-level generation is less
likely to produce an unusual sentences from the
real data; word-level generation produces more
conventional sentences. Some subjects noted that
neither email seemed human-written, perhaps an
artifact of our experimental design. Nevertheless,
we believe that this stochastic approach would re-
quire less effort compared to most rule-based or
template-based systems in terms of knowledge en-
gineering.
Template Stochastic No Diff.
Coherence 36.19 38.57 25.24
Fluency 28.10 40.48 31.43
Naturalness 35.71 45.71 18.57
Preference 36.67 42.86 20.48
Overall 34.17 41.90 23.93
Table 1: Generation algorithm comparison (%).
7 Conclusion
This paper presents a two-stage stochastic NLG
for synthesizing emails: first a structure is gener-
ated, and then text is generated for each structure
element, where sender style and topic structure
can be modeled. Subjects appear to notice sender
style and can also tell the difference between tem-
plates using original sentences and stochastically
generated sentences. We believe that this tech-
nique can be used to create realistic emails and that
email generation could be carried out using mix-
tures containing additional models based on other
characteristics. The current study shows that email
can be synthesized using a small corpus of labeled
data; however these models could be used to boot-
strap the labeling of a larger corpus which in turn
could be used to create more robust models.
Acknowledgments
The authors wish to thank Brian Lindauer and Kurt
Wallnau from the Software Engineering Institute
of Carnegie Mellon University for their guidance,
advice, and help.
155
References
Regina Barzilay and Mirella Lapata. 2008. Modeling lo-
cal coherence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to genera-
tion and summarization. In HLT-NAACL, pages 113?120.
Anja Belz. 2005. Corpus-driven generation of weather fore-
casts. In Proc. 3rd Corpus Linguistics Conference.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into in-
formation extraction systems by gibbs sampling. In Proc.
of ACL, pages 363?370.
Alastair J Gill, Carsten Brockmann, and Jon Oberlander.
2012. Perceptions of alignment and personality in gener-
ated dialogue. In Proceedings of the Seventh International
Natural Language Generation Conference, pages 40?48.
Association for Computational Linguistics.
Irving J Good. 1953. The population frequencies of species
and the estimation of population parameters. Biometrika,
40(3-4):237?264.
Shlomo Hershkop, Salvatore J Stolfo, Angelos D Keromytis,
and Hugh Thompson. 2011. Anomaly detection at multi-
ple scales (ADAMS).
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2009. Seg-
menting email message text into zones. In Proceedings
of the 2009 Conference on Empirical Methods in Natural
Language Processing, volume 2, pages 919?928. Associ-
ation for Computational Linguistics.
Alice H Oh and Alexander I Rudnicky. 2002. Stochastic nat-
ural language generation for spoken dialog systems. Com-
puter Speech & Language, 16(3):387?407.
Christina Sauper and Regina Barzilay. 2009. Automati-
cally generating wikipedia articles: A structure-aware ap-
proach. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1, pages 208?216. Association
for Computational Linguistics.
156
