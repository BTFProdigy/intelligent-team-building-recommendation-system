Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
SPEECHALATOR: TWO-WAY SPEECH-TO-SPEECH TRANSLATION IN YOUR HAND
Alex Waibel
 
, Ahmed Badran
 
, Alan W Black
 
, Robert Frederking
 
, Donna Gates
 
Alon Lavie
 
, Lori Levin
 
, Kevin Lenzo

, Laura Mayfield Tomokiyo
Juergen Reichert

, Tanja Schultz   , Dorcas Wallace   , Monika Woszczyna , Jing Zhang
 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA

Cepstral, LLC,

Multimodal Technologies Inc,

Mobile Technologies Inc.
speechalator@speechinfo.org
ABSTRACT
This demonstration involves two-way automatic speech-
to-speech translation on a consumer off-the-shelf PDA. This
work was done as part of the DARPA-funded Babylon project,
investigating better speech-to-speech translation systems for
communication in the field. The development of the Speecha-
lator software-based translation system required addressing
a number of hard issues, including a new language for the
team (Egyptian Arabic), close integration on a small device,
computational efficiency on a limited platform, and scalable
coverage for the domain.
1. BACKGROUND
The Speechalator was developed in part as the next genera-
tion of automatic voice translation systems. The Phrasalator
is a one-way device that can recognize a set of pre-defined
phrases and play a recorded translation, [1]. This device
can be ported easily to new languages, requiring only a
hand translation of the phrases and a set of recorded sen-
tences. However, such a system severely limits communica-
tion as the translation is one way, thus reducing one party?s
responses to simple pointing and perhaps yes and no.
The Babylon project addresses the issues of two-way
communication where either party can use the device for
conversation. A number of different groups throughout the
US were asked to address specific aspects of the task, such
as different languages, translation techniques and platform
specifications. The Pittsburgh group was presented with
three challenges. First, we were to work with Arabic, a lan-
guage with which the group had little experience, to test our
capabilities in moving to new languages quickly. Second,
we were instructed to use an interlingua approach to trans-
lation, where the source language is translated into an in-
termediate form that is shared between all languages. This
step streamlines expansion to new languages, and CMU has
a long history in working with interlingua based translation
systems. Third, we were constrained to one portable PDA-
class device to host the entire two-way system: two recog-
nizers, two translation engines, and two synthesizers.
2. RECOGNITION
We used an HMM-based recognizer, developed by Multi-
modal Technologies Inc, which has been specifically tuned
for PDAs. The recognizer allows a grammar to be tightly
coupled with the recognizer, which offers important effi-
ciencies considering the limited computational power of the
device. With only minor modification we were able to gen-
erate our interlingua interchange format (IF) representation
directly as output from the recognizer, removing one mod-
ule from the process.
MTI?s recognizer requires under 1M of memory with
acoustic models of around 3M per language. Special op-
timizations deal with the slow processor and ensure low
use of memory during decoding. The Arabic models were
bootstrapped from the GlobalPhone [2] Arabic collections
as well as data collected as part of this project.
3. TRANSLATION
As part of this work we investigated two different tech-
niques for translation, both interlingua based. The first was
purely knowledge-based, following our previous work [3].
The engine developed for this was too large to run on the
device, although we were able to run the generation part off-
line seamlessly connected by a wireless link from the hand-
held device. The second technique we investigated used
a statistical training method to build a model to translate
structured interlingua IF to text in the target language. Be-
cause this approach was developed with the handheld in
mind, it is efficient enough to run directly on the device,
and is used in this demo.
4. SYNTHESIS
The synthesis engine is Cepstral?s Theta system. As the
Speechalator runs on very small hardware devices (at least
small compared to standard desktops), it was important that
the synthesis footprint remained as small as possible.
The speechalator is to be used for people with little ex-
posure to synthetic speech, and the output quality must be
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 29-30
                                                         Proceedings of HLT-NAACL 2003
very high. Cepstral?s unit selection voices, tailored to the
domain, meet the requirements for both quality and size.
Normal unit selection voices may take hundreds of megabytes,
but the 11KHz voices developed by Cepstral were around 9
megabytes each.
5. ARABIC
The Arabic language poses a number of challenges for any
speech translation system. The first problem is the wide
range of dialects of the language. Just as Jamaican and
Glaswegian speakers may find it difficult to understand each
other?s dialect of English, Arabic speakers of different di-
alects may find it impossible to communicate.
Modern Standard Arabic (MSA) is well-defined and widely
understood by educated speakers across the Arab world.
MSA is principally a written language and not a spoken lan-
guage, however. Our interest was in dealing with a normal
spoken dialect, and we chose Egyptian Arabic; speakers of
that dialect were readily accessible to us, and media influ-
ences have made it perhaps the most broadly understood of
the regional dialects.
Another feature of Arabic is that the written form, ex-
cept in specific rare cases, does not include vowels. For
speech recognition and synthesis, this makes pronunciations
hard. Solutions have been tested for recognition where the
vowels are not explicitly modeled, but implicitly modeled
by context. This would not work well for synthesis; we have
defined an internal romanization, based on the CallHome
[4] romanization, from which full phonetic forms can easily
be derived. This romanization is suitable for both recog-
nizer and synthesis systems, and can easily be transformed
into the Arabic script for display.
6. SYSTEM
The end-to-end system runs on a standard Pocket PC de-
vice. We have tested it on a number of different machines,
including various HP (Compaq) iPaq machines (38xx 39xx)
and Dell Axims. It can run on 32M machines, but runs best
on a 64M machine with about 40M made available for pro-
gram space. Time from the end of spoken input to start of
translated speech is around 2-4 seconds depending on the
length of the sentence and the actual processor. We have
found StrongARM 206MHz processors, found on the older
Pocket PCs, slightly faster than XScale 400MHz, though no
optimization for the newer processors has been attempted.
Upon startup, the user is presented with the screen as
shown in Figure 1. A push-to-talk button is used and the
speaker speaks in his language. The recognized utterance
is first displayed, with the translation following, and the ut-
terance is then spoken in the target language. Buttons are
provided for replaying the output and for switching the in-
put to the other language.
7. DISCUSSION
The current demonstration is designed for the medical inter-
view domain, with the doctor speaking English and the pa-
tient speaking Arabic. At this point in the project no formal
evaluation has taken place. However, informally, in office-
like acoustic environments, accuracy within domain is well
over 80%.
Arabic input Screen
Speechalator snapshot
8. REFERENCES
[1] Sarich, A., ?Phraselator, one-way speech translation
system,? http://www.sarich.com/translator/, 2001.
[2] T. Schultz and A. Waibel, ?The globalphone project:
Multilingual lvcsr with janus-3,? in Multilingual Infor-
mation Retrieval Dialogs: 2nd SQEL Workshop, Plzen,
Czech Republic, 1997, pp. 20?27.
[3] A. Lavie, et al ?A multi-perspective evaluation of
the NESPOLE! speech-to-speech translation system,?
in Proceedings of ACL 2002 workshop on Speech-to-
speech Translation: Algorithms and Systems, Philadel-
phia, PA., 2002.
[4] Linguistic Data Consortium, ?Callhome egyptian ara-
bic speech,? 1997.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 136?143,
New York, June 2006. c?2006 Association for Computational Linguistics
Understanding Temporal Expressions in Emails
Benjamin Han, Donna Gates and Lori Levin
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave, Pittsburgh PA 15213
{benhdj|dmg|lsl}@cs.cmu.edu
Abstract
Recent years have seen increasing re-
search on extracting and using temporal
information in natural language applica-
tions. However most of the works found
in the literature have focused on identi-
fying and understanding temporal expres-
sions in newswire texts. In this paper
we report our work on anchoring tempo-
ral expressions in a novel genre, emails.
The highly under-specified nature of these
expressions fits well with our constraint-
based representation of time, Time Cal-
culus for Natural Language (TCNL). We
have developed and evaluated a Tempo-
ral Expression Anchoror (TEA), and the
result shows that it performs significantly
better than the baseline, and compares fa-
vorably with some of the closely related
work.
1 Introduction
With increasing demand from ever more sophisti-
cated NLP applications, interest in extracting and
understanding temporal information from texts has
seen much growth in recent years. Several works
have addressed the problems of representing tem-
poral information in natural language (Setzer, 2001;
Hobbs and Pan, 2004; Saur?? et al, 2006), extracting
and/or anchoring (normalizing) temporal and event
related expressions (Wiebe et al, 1998; Mani and
Wilson, 2000; Schilder and Habel, 2001; Vazov,
2001; Filatova and Hovy, 2001), and discovering the
ordering of events (Mani et al, 2003). Most of these
works have focused on capturing temporal informa-
tion contained in newswire texts, and whenever both
recognition and normalization tasks of temporal ex-
pressions were attempted, the latter almost always
fell far behind from the former in terms of perfor-
mance.
In this paper we will focus on a different combi-
nation of the problems: anchoring temporal expres-
sions in scheduling-related emails. In our project
work of building personal agents capable of schedul-
ing meetings among different users1, understand-
ing temporal expressions is a crucial step. We have
therefore developed and evaluated our system Tem-
poral Expression Anchorer (TEA) that is capable of
normalizing such expressions in texts. As input TEA
takes English text with temporal expressions al-
ready identified, and transduces the expressions into
their representations using Time Calculus for Nat-
ural Language (TCNL) (Han and Kohlhase, 2003).
These representations, or TCNL formulae, are then
evaluated by incorporating the contextual informa-
tion to give the final normalized output. TCNL has
the following characteristics: (1) a human calendar
(e.g., the Gregorian calendar) is explicitly modeled
as a constraint system to deal with the highly under-
specified nature of many temporal expressions, and
it allows easy extension to include new temporal
primitives; (2) a set of NL-motivated operators with
a granularity-enriched type system facilitates the
representation of the intensional meaning of a tem-
1Project RADAR,
http://www.radar.cs.cmu.edu/external.asp
136
poral expression in a compositional way; and (3) the
use of temporal references such as ?focus? in the
representation cleanly separates the core meaning of
an expression from its contextual dependency.
The rest of this paper is organized as follows.
Sec. 2 first surveys the characteristics of temporal
expressions in emails compared to those in newswire
texts, and motivates the design of our representation.
Sec 3 then introduces the formalism TCNL. The sys-
tem TEA and the anchoring process is detailed in
Sec. 4, and the evaluation of the system is reported
in Sec. 5. Finally Sec. 6 concludes this paper and
outlines the future work.
2 Temporal Expressions in Emails
The extent of temporal expressions considered in
this paper includes most of the expressions using
temporal terms such as 2005, summer, evening,
1:30pm, tomorrow, etc. These expressions can be
classified into the following categories:
? Explicit: These expressions can be immedi-
ately anchored, i.e., positioned on a timeline.
E.g., June 2005, 1998 Summer, etc.
? Deictic: These expressions form a specific re-
lation with the speech time (timestamp of an
email). E.g., tomorrow, last year, two weeks
from today.
? Relative: These include the other expressions
that form a specific relation with a temporal fo-
cus, i.e., the implicit time central to the discus-
sion. E.g., from 5 to 7, on Wednesday, etc. Dif-
ferent from the speech time, a temporal focus
can shift freely during the discourse.
? Durational: These are the expressions that de-
scribe certain length in time. E.g., for about
an hour, less than 20 minutes. This is differ-
ent from an interval expression where both the
starting point and the ending point are given
(e.g., from 5 to 7). Most durational expres-
sions are used to build more complex expres-
sions, e.g., for the next 20-30 minutes.
It is worth emphasizing the crucial difference be-
tween deictic expressions and relative expressions:
anchoring the former only relies on the fixed speech
time while normalizing the latter requires the usually
hidden focus. As illustrated below the latter task can
be much more challenging:
?I?m free next week. Let?s meet on
Wednesday.?
?Are you free on Wednesday??
In the first example the ?Wednesday? denotes a dif-
ferent date since the first sentence sets up a different
focus. To make things even more interesting, ver-
bal tense can also play a role, e.g., ?He finished the
report on Wednesday.?
There are other types of temporal expressions
such as recurrence (?every Tuesday?) and rate ex-
pressions (?twice on Wednesday?) that are not sup-
ported in our system, although they are planned in
our future work (Sec. 6).
To appreciate the different nature of emails as a
genre, an interesting observation can be made by
comparing the distributions of temporal expressions
in emails and in newswire texts. The email cor-
pora we used for development and testing were col-
lected from MBA students of Carnegie Mellon Uni-
versity over the year 1997 and 1998. The 277 stu-
dents, organized in approximately 50 teams of 4 to
6 members, were participating in a 14-week course
and running simulated companies in a variety of
market scenarios (Kraut et al, 2004). The original
dataset, the CSpace email corpus, contains approx-
imately 15,000 emails. We manually picked 1,196
emails that are related to scheduling - these include
scheduling meetings, presentations, or general plan-
ning for the groups. The emails are then randomly
divided into five sets (email1 to email5), and only
four of them are used in this work: email1 was used
to establish our baseline, email2 and email5 were
used for development, and part of email4 was used
for testing. Table 1 shows some basic statistics of
these three datasets2, and an edited sample email is
shown in Fig. 1 (names altered). The most appar-
ent difference comparing these emails to newswire
texts is in the percentage of explicit expressions oc-
curring in the two different genres. In (Mani et al,
2003) it was reported that the proportion of such ex-
pressions is about 25% in the newswire corpus they
2The percentages in some rows do not add up to 100% be-
cause some expressions like coordination can be classified into
more than one type.
137
Date: Thu, 11 Sep 1997 00:14:36 -0500
I have put an outline out in the n10f1 OpReview directory...
(omitted)
We have very little time for this. Please call me Thursday
night to get clarification. I will need graphs and prose in
files by Saturday Noon.
? Mary
ps. Mark and John , I waited until AFTER midnight to
send this .
Figure 1: A sample email (edited)
used3. In contrast, explicit expressions on average
only account for around 9.5% in the three email
datasets. This is not surprising given that people
tend to use under-specified expressions in emails for
economic reasons. Another thing to note is that there
are roughly the same number of relative expressions
and non-relative expressions. Since non-relative ex-
pressions (including deictic expressions) can be an-
chored without tracking the temporal focus over a
discourse and therefore can be dealt with in a fairly
straightforward way, we may assign 50% as a some-
what generous baseline performance of any anchor-
ing system4.
Another difference between emails and newswire
texts is that the former is a medium for communi-
cation: an email can be used as a reply, or can be
attached within another email, or even be used to
address to multiple recipients. All of this compli-
cates a great deal of our task. Other notable dif-
ferences are that in emails hour ambiguity tend to
appear more often (?I?ll be home at 2.?), and peo-
ple tend to be more creative when they compose
short messages such as using tables (e.g., an entire
column of numbers to denote the number of min-
utes alloted for each presenter), bullet lists, abbrevi-
ations, and different month/day formats (?1/9? can
mean January 9 or September 1), etc. Emails also
contain more ?human errors? such as misspellings
(?Thusday? to mean Thursday) and confusion about
dates (e.g., using ?tomorrow? when sending emails
3Using the North American News Corpus.
4This is a bit generous since solving simple calendric arith-
metics such as anchoring last summer still requires a non-trivial
modeling of human calendars; see Sec. 3.
around midnight), etc. Overall it is very difficult to
recover from this type of errors.
3 Representing Times in Natural
Language
This section provides a concise overview of TCNL;
readers are referred to (Han and Kohlhase, 2003;
Han et al, 2006) for more detail.
TCNL has two major components: a constraint-
based model for human calendars and a represen-
tational language built on top of the model. Dif-
ferent from the other representations such as Zeit-
Gram (Stede and Haas, 1998), TOP (Androut-
sopoulos, 1999), and TimeML/Timex3 (Saur?? et al,
2006), the language component of TCNL is essen-
tially ?calendar-agnostic? - any temporal unit can be
plugged in a formula once it is defined in the cal-
endar model, i.e., the calendar model serves as the
lexicon for the TCNL language.
Fig. 2 shows a partial model for the Gregorian cal-
endar used in TEA. The entire calendar model is ba-
sically a constraint graph with partial ordering. The
nodes labeled with ?year? etc. represent temporal
units (or variables when viewed as a constraint sat-
isfaction problem (CSP) (Ruttkay, 1998)), and each
unit can take on a set of possible values. The undi-
rected edges represent constraints among the units,
e.g., the constraint between month and day man-
dates that February cannot have more than 29 days.
A temporal expression in NL is then viewed as if
it assigns values to some of the units, e.g., ?Friday
the 13th? assigns values to only units dow (day-
of-week) and day. An interval-based AC-3 algo-
rithm with a chronological backtracking mechanism
is used to derive at the consistent assignments to the
other units, therefore allowing us to iterate to any
one of the possible Friday the 13th.
The ordering among the units is designated by two
relations: measurement and periodicity (arrows in
Fig. 2). These relations are essential for supporting
various operations provided by the TCNL language
such as determining temporal ordering of two time
points, performing arithmetic, and changing tempo-
ral granularity, etc. For example, to interpret the ex-
pression ?early July?, we identify that July is a value
of unit month, and month is measured by day. We
then obtain the size of July in terms of day (31) and
138
Table 1: Basic statistics of the email corpora
# of
emails
# of
tempex
explicit deictic relative durational
email1 253 300 3 (1%) 139 (46.33%) 158 (52.67%) N/A
email2 253 344 19 (5.5%) 112 (32.6%) 187 (54.4%) 27 (7.8%)
email4 (part.) 149 279 71 (25.4%) 77 (27.6%) 108 (38.7%) 22 (7.9%)
email5 126 213 14 (6.6%) 105 (49.3%) 92 (43.2%) 3 (1.4%)
Year
Month Day
Hour
Minute
Second
Week
Day-of-week
Time-of-day
Time-of-week
Year component Week component
?
X component
unit constraints
alignment constraints
is-measured-by relation
is-periodic-in relation
*
*
*
*
*
*
*
(* marks a representative)
*
temporal unit
Figure 2: A partial model of the Gregorian calendar
designate the first 10 days (31/3) as the ?early? part
of July.
Internally the calendar model is further parti-
tioned into several components, and different com-
ponents are aligned using non-binary constraints
(e.g., in Fig. 2 the year component and the week
component are aligned at the day and dow units).
This is necessary because the top units in these com-
ponent are not periodic within one another. All of
the operations are then extended to deal with multi-
ple calendar components.
Built on top of the calendar model is the typed
TCNL language. The three major types are coor-
dinates (time points; e.g., {sep,6day} for Septem-
ber 6), quantities (durations; e.g., |1hour| for one
hour) and enumerations (sets of points, including
intervals; e.g., [{wed},{fri}] for Wednesday and
Friday). More complex expressions can be rep-
resented by using various operators, relations and
temporal references; e.g., {now?|1day|} for yes-
terday, {|1mon|@{>= }} for the coming Monday
(or the first coming Monday in the future; the
? ? represents the temporal focus), | < |1hour|| for
less than one hour, [{wed}:{fri}] for Wednes-
day to Friday, [f {sat, noon}] for by Saturday
noon5, and [[{15hour}:{17hour}]&{wed}] for 3-5pm
on Wednesday. The TCNL language is designed
in such a way that syntactically different formu-
lae can be evaluated to denote the same date;
e.g., {tue, now+|1week|} (?Tuesday next week?) and
{now+|1tue|} (?next Tuesday?) can denote the same
date.
Associated with the operators are type and granu-
larity requirements. For example, when a focus is
specified down to second granularity, the formula
{now+|1day|} will return a coordinate at the day
granularity - essentially stripping away information
finer than day. This is because the operator ?+?
(called fuzzy forward shifting) requires the left-hand
side operand to have the same granularity as that of
the right-hand side operand. Type coercion can also
happen automatically if it is required by an operator.
For example, the operator ?@? (ordinal selection) re-
quires that the right-hand side operand to be of type
enumeration. When presenting a coordinate such as
{>= } (some point in the future), it will be coerced
5The f denotes the relation ?finishes? (Allen, 1984); the for-
mula denotes a set of coordinates no later than a Saturday noon.
139
Table 2: Summary of operators in TCNL; LHS/RHS is the left/right operand, g(e) returns the granularity of
e and min(s) returns the set of minimal units among s.
operator Type requirement Granularity requirement Semantics Example
+ and ? C ? Q ? C g(LHS) ? g(RHS) fuzzy forward/backward
shifting
{now+|1day|}
(?tomorrow?)
++ and ?? C ? Q ? C g(LHS) ?
min(g(LHS)?g(RHS))
exact forward/backward
shifting
{now++|2hour|}
(?2 hours from now?)
@ Q ? E ? C g(RHS) ? g(LHS) ordinal {|2{sun}|@{may}}
(?the 2nd Sunday in May?)
& C ? C ? C
C ? E ? E
E ? C ? E
E ? E ? E
g(LHS) ?
min(g(LHS)?g(RHS))
distribution {now &{now+|1year|}}
(?this time next year?)
[{15hour}&[{wed}:{fri}]]
(?3pm from Wednesday to
Friday?)
into an enumeration so that the ordinal operator can
select a requested element out of it. These designs
make granularity change and re-interpretation part
of a transparent process. Table 2 lists the operators
in the TCNL language.
Most of under-specified temporal expressions still
lack necessary information in themselves in order to
be anchored. For example, it is not clear what to
make out of ?on Wednesday? with no context. In
TCNL more information can be supplied by using
one of the coordinate prefixes: the ?+?/??? prefix
signifies the relation of a coordinate with the fo-
cus (after/before the focus), and the ?f?/?p? indicates
the relation of a coordinate with the speech time
(future/past). For example, the Wednesday in ?the
company will announce on Wednesday? is repre-
sented as +f{wed}, while ?the company announced
on Wednesday? is represented as ?p{wed}. When
evaluating these formulae, TEA will rewrite the for-
mer into {|1wed|@{>= , >= now}} and the latter
into {?|1wed|@{<= , <= now}} if necessary, es-
sentially trying to find the nearest Wednesday ei-
ther in the future or in the past. Since TCNL for-
mulae can be embedded, prefixed coordinates can
also appear inside a more complex formula; e.g.,
{{|2{sun}|@f{may}}+|2day|} represents ?2 days af-
ter a future Mother?s day?6.
Note that TCNL itself does not provide a mecha-
nism to instantiate the temporal focus (? ?). The re-
sponsibility of shifting a focus whenever necessary
(focus tracking) is up to TEA, which is described in
the next section.
6This denotes a possible range of dates, but it is still different
from an enumeration.
4 TEA: Temporal Expression Anchorer
The input to our system TEA is English texts with
temporal expression markups, and the output is a
time string for each temporal expression. The format
of a time string is similar to the ISO 8601 scheme:
for a time point the format is YYYYMMDDTHHMMSS
(T is a separator), for an interval it is a pair of points
separated by ?/? (slash). Also whenever there are
slots that lack information, we use ??? (question
mark) in its place. If a points can reside at any place
between two bounds, we use (lower..upper)
to represent it. Table. 3 shows the TEA output over
the example email given in Fig. 1 (min and max are
the minimal and the maximal time points TEA can
reason with).
TEA uses the following procedure to anchor each
temporal expression:
1. The speech time (variable now) and the focus
(? ?) is first assigned to a timestamp (e.g., the
received date of an email).
2. For each temporal expression, its nearest verb
chunk is identified using the part-of-speech
tags of the sentence. Expressions associated
with a verb of past tense or present imperfective
will be given prefix ??p? to its TCNL formula,
otherwise it is given ?+f?7.
3. A finite-state parser is then used to transduce an
expression into its TCNL formula. At the pars-
ing stage the tense and granularity information
is available to the parser.
7This is of course a simplification; future work needs to be
done to explore other possibilities.
140
Table 3: Anchoring example for the email in Fig. 1
Expression TCNL formula Temporal focus (f ) Anchored time string
(timestamp) 19970911T001436
Thursday night +f{thu,night} 19970911T001436 (19970911T18????..
19970911T23????)
by Saturday Noon [f +f{sat,noon}] (19970911T18????..
19970911T23????)
min/19970913T12????
until AFTER mid-
night
[f{>= ?p{midnight}}] 19970911T001436 min/(19970911..max)
4. The produced TCNL formula (or formulae
when ambiguity arises) is then evaluated with
the speech time and the current focus. In case
of ambiguity, one formula will be chosen based
on certain heuristics (below). The result of the
evaluation is the final output for the expression.
5. Recency-based focus tracking: we use the fol-
lowing procedure to determine if the result ob-
tained above can replace the current focus (be-
low). In cases where the result is an ambigu-
ous coordinate (i.e., it denotes a possible range
of points), if one of the bounds is min or max,
we use the other to be the new focus; if it is
not possible, we choose to keep the focus un-
changed. On the other hand, if the result is
an enumeration, we go through a similar pro-
cedure to avoid using an enumeration with a
min/max bound as the new focus. Finally no
quantity can become a focus.
Note that in Step 3 the decision to make partial
semantics of a temporal expression available to our
parser is based on the following observation: con-
sider the two expressions below
?Tuesday before Christmas?
= {tue, < {|25day|@{dec}}}
?Tuesday before 6pm?
= {< {tue,18hour}, de {tue}}
Both expressions share the same ?X before Y ? pat-
tern, but their interpretations are different8. The key
to discriminate the two is to compare the granulari-
ties of X and Y : if Y if at a coarser granularity then
the first interpretation should be adopted.
In Step 4 we use the following procedure to dis-
ambiguate the result:
8de denotes a relation ?during or equal? (Allen, 1984).
1. Remove any candidate that resulted in an in-
consistency when solving for a solution in the
calendar CSP.
2. If the result is meant to be a coordinate, pick
the one that is closest to the focus.
3. If the result is supposed to be an enumeration,
pick the one whose starting point is closest to
the focus, and whose length is the shortest one.
4. Otherwise pick the first one as the result.
For example, if the current time is 2:00 pm, for ex-
pression ?at 3? with a present/future tense, the best
answer is 15:00. For expression ?from 3 to 5?, the
best answer is from 3 pm to 5 pm.
When deciding whether a temporal expression
can become the next focus, we use simple heuris-
tics to rule out any expression that behaves like a
noun modifier. This is motivated by the following
example (timestamp: 19970919T103315):
IT basically analyses the breakdown on
labor costs and compares our 1998 labor
costs with their demands for 1999-2000.
...
I will check mail on Sunday and see any
feedback.
Without blocking the expression 1999-2000 from
becoming the focus, the last expression will be in-
correctly anchored in year 2000. The key obser-
vation here is that a noun-modifying temporal ex-
pression usually serves as a temporal co-reference
instead of representing a new temporal entity in the
discourse. These references tend to have a more con-
fined effect in anchoring the subsequent expressions.
141
Table 4: Development and testing results
Accuracy Parsing errors Human errors Anchoring errors
email2 (dev) 78.2% 10.47% 1.7% 9.63%
email5 (dev) 85.45% 5.16% 1% 8.39%
email4 (test-
ing)
76.34% 17.92% < 1% 5.74%
5 Evaluation
The temporal expressions in all of the datasets were
initially tagged using rules developed for Minor-
Third9, and subsequently corrected manually by two
of the authors. We then developed a prototype sys-
tem and established our baseline over email1 (50%).
The system at that time did not have any focus track-
ing mechanism (i.e., it always used the timestamp
as the focus), and it did not use any tense infor-
mation. The result confirms our estimate given in
Sec. 2. We then gradually developed TEA to its cur-
rent form using email1, email2 and email5. Dur-
ing the four-month development we added the focus
tracking mechanism, incorporating the tense infor-
mation into each TCNL formula via the coordinate
prefixes, and introduced several representational im-
provements. Finally we tested the system on the un-
seen dataset email4, and obtained the results shown
in Table 4. Note that the percentages reported in
the table are accuracies, i.e., the number of cor-
rectly anchored expressions over the total number
of temporal expressions over a dataset, since we are
assuming correct tagging of all of the expressions.
Our best result was achieved in the dev set email5
(85.45%), and the accuracy over the test set email4
was 76.34%.
Table 4 also lists the types of the errors made by
our system. The parsing errors are mistakes made
at transducing temporal expressions using the finite-
state parser into their TCNL formulae, the human
errors are described in Sec. 2, and the rest are the
anchoring errors. The accuracy numbers are all
compared favorably to the baseline (50%). To put
this performance in perspective, in (Wiebe et al,
1998) a similar task was performed over transcribed
scheduling-related phone conversations. They re-
ported an average accuracy 80.9% over the CMU
9http://minorthird.sourceforge.net/
test set and 68.9% over the NMSU test set. Although
strictly speaking the two results cannot be compared
due to differences in the nature of the corpora (tran-
scription vs. typing), we nevertheless believe it rep-
resents a closer match compared to the other works
done on newswire genre.
It should also be noted that we adopted a simi-
lar recency-based focus model as in (Wiebe et al,
1998). Although simple to implement, this naive
approach proved to be one major contributor to the
anchoring errors in our experiments. An example is
given below (the anchored times are shown in sub-
script):
This research can not proceed until the
trade-offs are known on Monday19970818 .
...
Mary will perform this by
Friday(min..19970822) using the data
from Monday19970825 .
The last expression received an incorrect date: it
should be the same date the expression ?on Mon-
day? refers to. Our system made this error because
it blindly used the most recently mentioned time
((min..19970822)) as the focus to anchor the
formula +f{mon}. This error later also propagated
to the anchoring of the subsequent expressions.
6 Conclusion and Future Work
In this paper we have adopted a constraint-based
representation of time, Time Calculus for Natural
Language (TCNL), to tackle the task of anchoring
temporal expressions in a novel genre, emails. We
believe that the genre is sufficiently different from
newswire texts, and its highly under-specified nature
fits well with a constraint-based modeling of human
calendars. TCNL also allows for an explicit repre-
sentation of temporal focus, and many of our intu-
itions about granularity change and temporal arithe-
142
matics are encapsulated in its type system and oper-
ators. The performance of our anchoring system is
significantly better than baseline, and compares fa-
vorably with some of the closely related work.
In the future we will re-examine our focus track-
ing mechanism (being the most significant source of
errors), and possibly treat it as a classification prob-
lem (similar to (Mani et al, 2003)). We also need to
investigate the disambiguation procedure and pos-
sibly migrate the functionality into a separate dis-
course module. In addition, the co-referencing ten-
dency of noun-modifying expressions could lead to
a better way to anchoring this particular type of tem-
poral expressions. Finally we would like to ex-
pand our coverage of temporal expressions to in-
clude other types of expressions such as recurrence
expressions10.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. NBCHD030010.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA), or the Department of Interior-National
Business Center (DOI-NBC).
References
J. F. Allen. 1984. Towards a General Theory of Action
and Time. Artificial Intelligence, 23:123?154.
I. Androutsopoulos. 1999. Temporal Meaning Rep-
resentations in a Natural Language Front-end. In
M. Gergatsoulis and P. Rondogiannis, editors, Inten-
sional Programming II (Proceedings of the 12th In-
ternational Symposium on Languages for Intensional
Programming, Athens, Greece.
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps To Event-Clauses. In Proceedings of ACL-
2001: Workshop on Temporal and Spatial Information
Processing, Toulouse, France, 7.
10The current design of TCNL allows for a more restricted
type of recurrence: e.g., ?3pm from Wednesday to Friday? is
represented as [{15hour}&[{wed}:{fri}]]. However this is in-
sufficient to represent expressions such as ?every 4 years?.
Benjamin Han and Michael Kohlhase. 2003. A Time
Calculus for Natural Language. In The 4th Work-
shop on Inference in Computational Semantics, Nancy,
France, September.
B. Han, D. Gates, and L. Levin. 2006. From Language to
Time: A Temporal Expression Anchorer. In Proceed-
ings of the 13th International Symposium on Tempo-
ral Representation and Reasoning (TIME 2006), Bu-
dapest, Hungary.
J. R. Hobbs and Feng. Pan. 2004. An ontology of time
for the semantic web. TALIP Special Issue on Spa-
tial and Temporal Information Processing, 3(1):66?
85, March.
R. E. Kraut, S. R. Fussell, F. J. Lerch, and A Espinosa.
2004. Coordination in teams: Evidence from a sim-
ulated management game. Journal of Organizational
Behavior, to appear.
I. Mani and G. Wilson. 2000. Robust Temporal Process-
ing of News. In Proceedings of ACL-2000.
I. Mani, B. Schiffman, and J. Zhang. 2003. Inferring
Temporal Ordering of Events in News. In Proceedings
of the Human Language Technology Conference (HLT-
NAACL?03).
Zso?fia Ruttkay. 1998. Constraint Satisfaction - a Survey.
Technical Report 11(2-3), CWI.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2006. TimeML Annotation Guidelines, Version 1.2.1,
January 31.
F. Schilder and C. Habel. 2001. From Temporal Expres-
sions To Temporal Information: Semantic Tagging Of
News Messages. In Proceedings of ACL-2001: Work-
shop on Temporal and Spatial Information Processing,
Toulouse, France, 7.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield.
M. Stede and S. Haas. 1998. Understanding and track-
ing temporal descriptions in dialogue. In B. Schro?der,
W. Lenders, W. Hess, and T. Portele, editors, Proceed-
ings of the 4th Conference on Natural Language Pro-
cessing - KONVENS ?98.
N. Vazov. 2001. A System for Extraction of Tempo-
ral Expressions from French Texts Based on Syntac-
tic and Semantic Constraints. In Proceedings of ACL-
2001: Workshop on Temporal and Spatial Information
Processing, Toulouse, France, 7.
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, and
K. J. McKeever. 1998. An Empirical Approach to
Temporal Reference Resolution. Journal of Artificial
Intelligence Research, 9:247?293.
143
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 5?8,
New York, June 2006. c?2006 Association for Computational Linguistics
The MILE Corpus for Less Commonly Taught Languages 
 
Alison Alvarez, Lori Levin, Robert 
Frederking, Simon Fung, Donna 
Gates 
Language Technologies Institute 
5000 Forbes Avenue 
Pittsburgh, PA 15213 
[nosila, lsl, ref+, 
sfung, dmg] 
@cs.cmu.edu  
Jeff Good 
Max Planck Institute for Evolutionary 
Anthropology 
Deutscher Platz 6 
04103 Leipzig 
good@eva.mpg.de 
 
 
Abstract 
This paper describes a small, struc-
tured English corpus that is 
designed for translation into Less 
Commonly Taught Languages 
(LCTLs), and a set of re-usable 
tools for creation of similar cor-
pora. 1  The corpus systematically 
explores meanings that are known to 
affect morphology or syntax in the 
world?s languages.  Each sentence 
is associated with a feature structure 
showing the elements of meaning 
that are represented in the sentence.   
The corpus is highly structured so 
that it can support machine learning 
with only a small amount of data.   
As part of the REFLEX program, 
the corpus will be translated into 
multiple LCTLs, resulting in paral-
lel corpora can be used for training 
of MT and other language technolo-
gies. Only the untranslated English 
corpus is described in this paper.  
 
1   Introduction 
 
Of the 6,000 living languages in the world 
only a handful have the necessary monolin-
gual or bilingual resources to build a 
working statistical or example-based ma-
chine translation system.  Currently, there 
                                                 
1 AVENUE/MILE is supported by the US Na-
tional Science Foundation NSF grant number 
IIS-0121-631 and the US Government?s 
REFLEX Program. 
are efforts to build language packs for Less 
Commonly Taught Languages (LCTLs).  
Each language pack includes parallel cor-
pora consisting of naturally occurring text 
translated from English into the LCTL or 
vice versa.  
This paper describes a small corpus 
that supplements naturally occurring text 
with highly systematic enumeration of 
meanings that are known to affect morphol-
ogy and syntax in the world?s languages.   
The supplemental corpus will enable the 
exploration of constructions that are sparse 
or obscured in natural data.  The corpus 
consists of 12,875 English sentences, total-
ing 76,202 word tokens.    
This paper describes the construc-
tion of the corpus, including tools and 
resources that can be used for the construc-
tion of similar corpora.   
 
2 Structure of the corpus 
 
| 247: John said "The woman is a teacher." 
| 248: John said the woman is not a teacher. 
| 249: John said "The woman is not a teacher." 
| 250: John asked if the woman is a teacher. 
| 251: John asked "Is the woman a teacher?" 
| 252: John asked if the woman is not a teacher. 
| ?
| 1488: Men are not baking cookies. 
| 1489: The women are baking cookies.
| ?
| 1537: The ladies' waiter brought appetizers. 
| 1538: The ladies' waiter will bring appetizers. 
Figure 1: A sampling of sentences from 
the complete elicitation corpus 
5
srcsent: Mary was not a doctor. 
context: Translate this as though it were spoken to a peer co-worker; 
 
((actor ((np-function fn-actor)(np-animacy anim-human)(np-biological-gender bio-gender-female) 
(np-general-type  proper-noun-type)(np-identifiability identifiable) 
 (np-specificity specific)?))     
(pred ((np-function fn-predicate-nominal)(np-animacy anim-human)(np-biological-gender bio-
gender-female) (np-general-type common-noun-type)(np-specificity specificity-neutral)?)) 
(c-v-lexical-aspect state)(c-copula-type copula-role)(c-secondary-type secondary-copula)(c-
solidarity solidarity-neutral) (c-power-relationship power-peer) (c-v-grammatical-aspect gram-
aspect-neutral)(c-v-absolute-tense past) (c-v-phase-aspect phase-aspect-neutral) (c-general-
type declarative-clause)(c-polarity polarity-negative)(c-my-causer-intentionality intentionality-
n/a)(c-comparison-type comparison-n/a)(c-relative-tense relative-n/a)(c-our-boundary boundary-
n/a)?) 
Figure 2:  An abridged feature structure, sentence and context field 
The MILE (Minor Language Elicitation) 
corpus is a highly structured set of English 
sentences.  Each sentence represents a 
meaning or combination of meanings that 
we want to elicit from a speaker of an 
LCTL.  For example, the corpus excerpts 
in Figure 1 explore quoted and non quoted 
sentential complements, embedded ques-
tions, negation, definiteness, biological 
gender, and possessive noun phrases.   
Underlying each sentence is a feature 
structure that serves to codify its meaning.  
Additionally, sentences are accompanied by 
a context field that provides information that 
may be present in the feature structure, but 
not inherent in the English sentence.  For 
example, in Figure 2, the feature structure 
specifies solidarity with the hearer and 
power relationship of the speaker and hearer, 
as evidenced by the features-value pairs (c-
solidarity solidarity-neutral) and (c-power-
relationship power-peer).  Because this is 
not an inherent part of English grammar, this 
aspect of meaning is conveyed in the context 
field.   
 
3 Building the Corpus 
 
Figure 3 shows the steps in creating the 
corpus.  Corpus creation is driven by a Fea-
ture Specification.  The Feature 
Specification defines features such as tense, 
person, and number, and values for each 
feature such past, present, future, remote 
past, recent past, for tense.  Additionally, 
the feature specification defines illegal com-
binations of features, such as the use of a 
singular number with an inclusive or exclu-
sive pronoun (We = you and me vs we = me 
and other people).  The inventory of fea-
tures and values is informed by typological 
studies of which elements of meaning are 
known to affect syntax and morphology in 
some of the world?s languages. The feature 
specification currently contains 42 features 
and 340 values and covers. In order to select 
the most relevant features we drew guidance 
from Comrie and Smith (1977) and Bouqui-
aux and Thomas (1992).  We also used the 
World Atlas of Language Structures 
(Haspelmath et al 2005) as a catalog of ex-
isting language features and their prevalence.  
In the process of corpus creation, feature 
structures are created before their corre-
sponding English sentences.   There are 
three reasons for this.  First, as mentioned 
above, the feature structure may contain 
elements of meaning that are not explicitly 
represented in the English sentence.  Sec-
ond, multiple elicitation languages can be 
generated from the same set of feature struc-
tures.  For example, when we elicit South 
American languages we use Spanish instead 
of English sentences.  Third, what we want 
to know about each LCTL is not how it 
translates the structural elements of English 
such as determiners and auxiliary verbs, but 
how it renders certain meanings such as 
6
List of semantic 
features and 
values 
The Corpus
Feature Maps:  which 
combinations of 
features and values 
are of interest 
Clause-
Level 
Noun-
Phrase
Tense &
Aspect Modality 
Feature Structure Sets
Feature 
Specification 
Reverse Annotated Feature Structure
Sets: add English sentences
Smaller   CorpusSampling
?
Figure 3: An overview of the elicitation corpus production process 
definiteness, tense, and modality, which are 
not in one-to-one correspondence with Eng-
lish words.    
Creation of feature structures takes place 
in two steps.  First, we define which com-
binations of features and values are of 
interest.  Then the feature structures are 
automatically created from the feature speci-
fication.    
Combinations of features are specified 
in Feature Maps (Figure 3).  These maps 
identify features that are known to interact 
syntactically or morphologically in some 
languages.  For example, tense in English 
is partially expressed using the auxiliary 
verb system.  An unrelated aspect of mean-
ing, whether a sentence is declarative or 
interrogative, interacts with the tense system 
in that it affects the word order of auxiliary 
verbs (He was running, Was he running), 
Thus there is an interaction of tense with 
interrogativity.   We use studies of lan-
guage typology to identify combinations of 
features that are known to interact.   
Feature Maps are written in a concise 
formalism that is automatically expanded 
into a set of feature structures.  For exam-
ple, we can formally specify that we want 
three values of tense combined with three 
values of person, and nine feature structures 
will be produced.  These are shown as Fea-
ture Structure Sets in Figure 3.   
 
 
4 Sentence Writing 
 
 As stated previously, our corpus 
consists of feature structures that have been 
human annotated with a sentence and con-
text field.  Our feature structures contain 
functional-typological information, but do 
not contain specific lexical items.  This 
means that our set of feature structures can 
be interpreted into any language using ap-
propriate word choices and used for 
elicitation.  Additionally, this leaves the 
human annotator with some freedom when 
selecting vocabulary items.  Due to feed-
back from previous elicitation subjects we 
chose basic vocabulary words while steering 
clear of overly primitive subject matter that 
may be seen as insulting.  Moreover, we 
did our best to avoid lexical gaps; for exam-
ple, many languages do not have a single 
word that means winner.   
7
 Translator accuracy was also an im-
portant objective and we took pains to 
construct natural sounding, unambiguous 
sentences.  The context field is used to 
clarify the sentence meaning and spell out 
features that may not manifest themselves in 
English. 
 
5 Tools 
 
 In conjunction with this project we 
created several tools that can be reused to 
make new corpora with other purposes. 
? An XML schema and XSLT can be used 
to make different feature specifications 
? A feature structure generator that can be 
used as a guide to specify and design 
feature maps 
? A feature structure browser can be used 
to make complicated feature structures 
easier to read and annotate 
 
6 Conclusion 
 
The basic steps for creating a func-
tional-typological corpus are: 
  
1. Combinations of features are selected 
2. Sets of feature structures representing all 
feature combinations are generated 
3. Humans write sentences with basic vo-
cabulary that represent the meaning in 
the feature structure 
4. If the corpus is too large, some or all of 
the corpus can be sampled 
 
We used sampling and assessments of 
the most crucial features in order to compile 
our corpus and restrict it to a size small 
enough to be translatable by humans.  As a 
result it is possible that this corpus will miss 
important feature combinations in some lan-
guages.  However, a corpus containing all 
possible combinations of features would 
produce hundreds of billions of feature 
structures.   
Our future research includes building a 
Corpus Navigation System to dynamically 
explore the full feature space.  Using ma-
chine learning we will use information de-
tected from translated sentences in order to 
decide what parts of the feature space are 
redundant and what parts must be explored 
and translated next. A further description of 
this process can be read in Levin et al 
(2006). 
Additionally, we will change from using 
humans to write sentences and context fields 
to having them generated by using a natural 
language generation system (Alvarez et al 
2005).   
We also ran small scale experiments to 
measure translator accuracy and consistency 
and encountered positive results. Hebrew 
and Japanese translators provided consistent, 
accurate translations.  Large scale experi-
ments will be conducted in the near future to 
see if the success of the smaller experiments 
will carry over to a larger scale. 
 
7 References 
 
Alvarez, Alison, and Lori Levin, Robert  
  Frederking, Jeff Good, Erik Peterson  
 September 2005, Semi-Automated Elicitation 
 Corpus Generation. In Proceedings of MT 
 Summit X, Phuket: Thailand. 
 
Bouquiaux, Luc and J.M.C. Thomas. 1992.  
Studying and Describing Unwritten Lan-
guages. Dallas, TX: The Summer Institute of 
Linguistcs. 
 
Comrie, Bernard and N. Smith. 1977.  
  Lingua descriptive series: Questionnaire. In:      
  Lingua, 42:1-72. 
 
Haspelmath, Martin and Matthew S. Dryer,     
  David Gil, Bernard Comrie, editors. 2005    
  World Atlas of Language Strucutures. Oxford  
  University Press. 
 
Lori Levin, Alison Alvarez, Jeff Good, and     
  Robert Frederking. 2006 "Automatic Learning    
  of Grammatical Encoding." To appear in Jane 
  Grimshaw, Joan Maling, Chris Manning, Joan    
  Simpson and Annie Zaenen (eds)  
  Architectures, Rules and Preferences: A  
  Festschrift for Joan Bresnan , CSLI Publica  
  tions.  In Press. 
 
8
Evaluation of a Practical Interlingua 
for Task-Oriented Dialogue 
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi, 
Dorcas Wallace, Taro Watanabe, Monika Woszczyna 
Language Technologies Inst i tute,  Carnegie Mellon Univers i ty  and 
IRST  ITC,  Trento, I taly 
Internet:  l s l?cs ,  cmu. edu 
Abstract 
IF (Interchange Format), the interlingua used by 
the C-STAR consortium, is a speech-act based in- 
terlingua for task-oriented ialogue. IF was de- 
signed as a practical interlingua that could strike 
a balance between expressivity and simplicity. If 
it is too simple, components of meaning will be 
lost and coverage of unseen data will be low. On 
the other hand, if it is too complex, it cannot be 
used with a high degree of consistency by collab- 
orators on different continents. In this paper, we 
suggest methods for evaluating the coverage of IF 
and the consistency with which it was used in the 
C-STAR consortium. 
Introduction 
IF (Interchange Format) is an interlingua used by 
the C-STAR consortium 1 for task-oriented ia- 
logues. Because it is used in five different coun- 
tries for six different languages, it had to achieve 
a careful balance between being expressive hough 
and being simple enough to be used consistently. 
If it was not expressive nough, components of 
meaning would be lost and coverage of unseen data 
would be low. On the other hand, if was not sim- 
ple enough, different system developers would use 
it inconsistently and the wrong meanings would be 
translated. IF is described in our previous papers 
(\[PT98, LGLW98, LLW+\]). 
For this paper, we have proposed methods for 
evaluating the coverage of IF and the degree to 
which it can be used consistently across C-STAR 
sites. Coverage was measured by having human IF 
specialists annotate unseen data. Consistency was 
measured by two means. The first was inter-coder 
agreement among IF specialists at Carnegie Mel- 
lonUniversity and ITC-irst (Centre per la ricerca 
lhttp://www.c-star.org 
18 
scientifica e tecnologica). The second, less direct 
method, was a cross-site nd-to-end evaluation of 
English-to-Italian translation where the English- 
to-IF analysis grammars were written at CMU and 
IF-to-Italian generation was developed at IRST. If 
the English and Italian grammar writers did not 
agree on the meaning of the IF, wrong transla- 
tions will be produced. In this way, the cross-site 
evaluation can be an indirect indicator of whether 
the CMU and IRST IF specialists agreed on the 
meaning of IF representations. For comparison, 
we also present within-site nd-to-end evaluations 
of English-to-German, English-to-Japanese, and 
English-to-IF-to-English, where all of the analysis 
and generation grammars were written at CMU. 
The  In terchange Format  
Because we are working with task-oriented dia- 
logues, adequate rendering of the speech act in the 
target language often overshadows the need for lit- 
eral translation of the words. IF is therefore based 
on domain actions (DAs), which consist of on 
speech acts plus domain-specific concepts. An ex- 
ample of a DA is give-information+price+room 
(giving information about the price of a room). 
DAs are composed from 45 general speech acts 
(e.g., acknowledge, give- information, accept) 
and about 96 domain-specific oncepts (e.g, 
pr ice,  temporal, room, f l ight ,  ava i lab i l i ty ) .  
In addition to the DA, IF representations can con- 
tain arguments such as room-type, dest inat ion,  
and price. There are about 119 argument types. 
In the following example, the DA consists 
of a speaker tag (a: for agent), the speech- 
act give- information,  and two main concepts, 
+price and +room. The DA is followed by a list 
of arguments: room-type= and price=. The ar- 
guments have values that represent-information 
for the type of room double and the cost repre- 
Percent 
Cumulatlve Percent Count 
Coverage 
15.7 15,7 652 
19.8 4.1 172 
28.3 3.4 143 
26.0 2.7 113 
28.0 2.0 85 
30.1 2.0 85 
31,9 1.9 78 
33.7 1.8 75 
35.5 1.8 73 
37.2 1.7 70 
38.8 1.6 66 
40.3 l .S 64 
41.7 1,4 60 
43.2 1.4 60 
44.5 1.3 56 
45.8 1.3 52 
46.9 1.2 48 
48.0 1.1 46 
49.1 1.1 44 
50.1 1.0 42 
NA* ;:; 244 
DA 
acknowledge 
aff i rm 
thank 
introduce-self 
give-lnformation+prtce 
greeting 
give-lnfor marion+tern poral 
give-lnformatlon+numeral 
give-in formation+ pr ice+room 
request-in for matio n+ payment 
give-information + payment 
g ive- inform+features+room 
give-in form -t- availabil ity + room 
accept 
give-information+personal-data 
req-act +reserv+ feat ures+room 
req- verif-give-inforra +numera l  
offer+help 
apologize 
request-inform+personal-data 
no-tag 
Figure 1: Coverage of Top20 DAs and No-tag in 
development data 
sented with the complex argument price= which 
has its own arguments quantity=, currency= and 
per-unit=. This IF representation is neutral be- 
tween sentences that have different verbs, sub- 
jects, and objects uch as A double room costs 150 
dollars a night, The price of  a double room is 150 
dollars a night, and A double room is 150 dollars 
a night. ~ 
AGENT: ''a double room costs $150 a night.'' 
a:give-information+price+room 
( room-type=doub le ,  
price=(quantity=lSO, 
currency=dollar, 
per-unit=night) 
Coverage and D is t r ibut ion  of  
Dia logue  Acts  
In this section, we address the coverage of IF for 
task-oriented dialogues about ravel planning. We 
want to know whether a very simple interlingua 
like IF can have good coverage. We are using a 
rather subjective measure of coverage: IF experts 
hand-tagged unseen data with IF representations 
and counted the percentage ofutterances towhich 
no IF could be assigned. (When they tagged the 
unseen data, they were not told that the IF was 
being tested for coverage. The tagging was done 
for system development purposes.) Our end-to- 
end evaluation described in the following sections 
can be taken as a less subjective measure of cov- 
2When we add anaphora resolution, we will need 
to know whether a verb (cost) or a noun (price) was 
used. This will be an issue our new project, NESPOLEI 
(http://nespole. itc. it/). 
Percent 
Cumulative Percent Count Speech Act 
Coverage 
30.1 80.1 1250 glve-lnformation 
45,8 15.7 655 acknowledge 
57,7 11.9 498 request- lnformation 
62,7 5,0 209 request-verif ication-give-inform 
87.6 4.9 203 request-actlon 
71.7 4.1 172 affirm 
75,1 3.4 143 thank 
77,9 2.7 113 introduce-self 
80.2 2.4 98 offer 
82,4 2.1 89 accept 
84.4 2.0 85 greeting 
85.7 1.3 55 suggest 
66.8 I . I  44 apologize 
87.8 1.0 41 closing 
88.5 0.8 32 negate.give-information 
89.2 0.6 27 delay-action 
89,8 0.6 25 introduce-topic 
90,2 0.5 19 please-wait 
90.6 0.4 15 reject 
91.0 0.4 15 request-suggestlon 
Figure 2: 
data 
Coverage of speech-acts in development 
erage. However, the score of an end-to-end evalu- 
ation encompasses grammar coverage problems as 
well as IF coverage problems. 
The development portion of the coverage x- 
periment proceeded as follows. Over a period of 
two years, a database of travel planning dialogues 
was collected by C-STAR partners in the U.S., 
Italy, and Korea. The dialogues were role-playing 
dialogues between a person pretending to be a 
traveller and a person pretending to be a travel 
agent. For the English and Italian dialogues, the 
traveller and agent were talking face-to-face in the 
same language - -  both speaking English or both 
speaking Italian. The Korean dialogues were also 
role playing dialogues, but one participant was 
speaking Korean and the other was speaking En- 
glish. From these dialogues, only the Korean ut- 
terances are included in the database. Each utter- 
ance in the database is annotated with an English 
translation and an IF representation. Table 1 sum- 
marizes the amount of data in each language. The 
English, Italian, and Korean data was used for IF 
development. 
The development database contains over 4000 
dialogue act units, which are covered by a total of 
about 542 distinct DAs (346 agent DAs and 278 
client DAs). Figures 1 and 2 show the cumulative 
coverage of the top twenty DA's and speech acts 
in the development data. Figure 1 also shows the 
percentage ofno-tag utterances (the ones we de- 
cided not to cover) in the development data. The 
first column shows the percent of the development 
data that is covered cumulatively by the DA's or 
speech acts from the top of the table to the cur- 
rent line. For example, acknowledg e and aff irm 
together account for 19.8 percent of the data. The 
19 
Language(s) Type  of Dialogue Number  of DA Units  
D'evelopment Data: 
English 
Italian 
Korean-English 
Test Data: 
Japanese-English 
monolingual 
monolingual 
biiingual (only 'Korean 
utterances are included) 
bilingual (Japanese and 
English utterances are 
included) 
Table 1: The IF Database 
2698 
1142 
6069 
Percent 
' Cumulat ive Percent Count DA 
Cover~,--= - " 4.6 263 no-tag 
15.6 15.6 ? - 885 acknowledge 
20.2 4,6 260 thank 
23.7 3.5 200 introduce-self 
27.0 3.4 191 affirm 
29.7 2.7 153 apologize 
32.3 2.6 147 greeting 
34.6 2.3 128 closing 
36.3 1.7 98 give- information+personal-data 
38.0 1.7 95 glve-inform ation +t  em poraI 
39.5 1.6 89 give-in formation +price 
41.1 1.5 88 please-wait 
42.5 1.4 82 give-inform+telephone-number 
43.8 1.3 75 g ive- informat ion+features+room 
45.0 I . I  65 request- inform+personal-data 
46.0 1.0 59 give-in for m ?temp oral-.{- arrival 
47.0 1.0 55 accept 
48.0 l.O 55 give-infor m +avai labi l i ty + room 
48.9 1.0 55 give-information+price-broom 
49.8 0.9 50 verify 
50.7 0.9 49 request-in form +tempora l+arr iva l  
Figure 3: Coverage of Top 20 DAs and No-tag in 
test data 
Percent 
Cumulat ive 
Coverage 
25.6 
Percent Count DA 
25.6 1454 give-information 
41.7 16.1 916 acknowledge 
53.6 11.9 677 request- information 
58.2 4.6 260 thank 
62,0 3.7 213 request-verification-give-inform 
65.5 3.5 200 introduce-self 
68.8 3.4 191 a f f i rm 
72.0 3.2 181 request -act ion  
74.8 2.8 159 accept 
77.5 2.7 153 apologize 
80.1 2.6 147 greet ing 
82.4 2.3 130 closing 
84.4 2.1 117 suggest 
86.3 1.8 104 verlfy-give-information 
87.9 1.7 94 offer 
89.5 1.5 88 please-wait 
90.6 I . I  65 negate-glve-lnformation 
91.5 0.9 50 verify 
92.0 0.5 30 negate 
92.5 0.5 . 26 request-aff irmatlon 
Figure 4: Coverage of Top 20 SAs in test data 
second column shows the percent of the develop- 
ment data covered by each DA or speech act. The 
third column shows the number of times each DA 
or speech act occurs in the development data. 
The evaluation portion of the coverage x- 
periment was carried out on 124 dialogues (6069 
dialogue act units) that were collected at ATR, 
Japan. One participant in each dialogue was 
speaking Japanese and the other was speaking En- 
glish. Both Japanese and English utterances are 
included in the data. The 124 Japanese-English 
dialogues were not examined closely by system de- 
velopers during IF development. After the IF de- 
sign was finalized and frozen in Summer 1999, the 
Japanese-English data was tagged with IFs. No 
further IF development took place at this point 
except hat values for arguments were added. For 
example, Miyako could be added as a hotel name, 
but no new speech acts, concepts, or argument 
types could be added. Sentences were tagged as 
no-tag if the IF did not cover them. 
Figures 3 and 4 show the cumulative cover- 
age of the top twenty DAs and speech acts in the 
Japanese-English data, including the percent of 
no-tag sentences. 
Notice that the percentage of no-tag was 
lower in our test data than in our development 
data. This is because the role playing instructions 
for the test data were more restrictive than the 
role playing instructions for the development data. 
Figures 1 and 3 show that slightly more of the test 
data is covered by slightly fewer DAs. 
Cross-Site Reliability of IF 
Representations 
In this section we attempt o measure how reliably 
IF is used by researchers at different sites. Recall 
that one of the design criteria of IF was consis- 
tency of use by researchers who are separated by 
oceans. This criterion limits the complexity of IF. 
Two measures of consistency are used - inter-coder 
agreement and a cross-site nd-to-end evaluation. 
Inter -Coder  Agreement:  Inter-coder agree- 
ment is a direct measure of consistency among 
20 
Percent Agreement 
Speech-act 82.14 
Dialog-act 65.48 
Concept lists 88.00 
Argument lists I 85.79 
Table 2: Inter-coder Agreement between CMU 
and IRST 
C-STAR partners. We used 84 DA units from 
the Japanese-English data described above. The 
84 DA units consisted of some coherent dialogue 
fragments and and some isolated sentences. The 
data was coded at CMU and at IRST. We counted 
agreement on ~he components ofthe IF separately. 
Table 2 shows agreement on speech acts, dialogue 
acts (speech act plus concepts), concepts, and ar- 
guments. The results are reported in Table 2 in 
terms of percent agreement. Further work might 
include some other calculation of agreement such 
as Kappa or precision and recall of the coders 
against each other. Figure 5 shows a fragment of 
a dialogue coded by CMU and IRST. The coders 
disagreed on the IF middle sentence, I'd like a twin 
room please. One coded it as an acceptance of a 
twin room, the other coded it as a preference for 
a twin room. 
Cross-Site Evaluation: As an approximate and 
indirect measure of consistency, we have compared 
intra-site end-to-end evaluation with cross-site 
end-to-end evaluation. An end-to-end evaluation 
includes an analyzer, which maps the source lan- 
guage input into IF and a generator, which maps 
IF into target language sentences. The intra-site 
evaluation was carried out on English-German, 
English-Japanese, and English-IF-English trans- 
lation. The English analyzer and the German, 
Japanese, and English generators were all writ- 
ten at CMU by IF experts who worked closely 
with each other. The cross-site valuation was car- 
ried out on English-Italian translation, involving 
an English analyzer written at CMU and an Ital- 
ian generator written at IRST. The IF experts at 
CMU and IRST were in occasional contact with 
each other by email, and met in person two or 
three times between 1997 and 1999. 
A number of factors contribute to the success 
of an inter-site valuation, just one of which is that 
the sites used IF consistently with each other. An- 
other factor is that the two sites used similar de- 
velopment data and have approximately the same 
coverage. If the inter-site valuation results are 
about as good as the intra-site results, we can con- 
clude that all factors are handled acceptably, in- 
cluding consistency of IF usage. If the inter-site 
results are worse than the intra-site results, con- 
sistency of IF use or some other factor may be 
to blame. Before conducting this evaluation, we 
already knew that there was some degree of cross- 
site consistency in IF usage because we conducted 
successful inter-continental demos with speech 
translation and video conferencing in Summer 
1999. (The demos and some of the press coverage 
are reported on the C-STAR web site.) The de- 
mos included ialogues in English-Italian, English- 
German, English-Japanese, English-Korean, and 
English-French. At a later date, an Italian-Korean 
demo was produced with no additional work, thus 
illustrating the well-cited advantage of an inter- 
lingual approach in a multi-lingual situation. The 
end-to-end evaluation reported here goes beyond 
the demo situation to include data that was un- 
seen by system developers. 
Evaluation Data: The Summer 1999 intra-site 
evaluation was conducted on about 130 utterances 
from a CMU user study. The traveller was played 
by a second time user - -  someone who had partici- 
pated in one previous user study, but had no other 
experience with our MT system. The travel agent 
was played by a system developer. Both people 
were speaking English, but they were in different 
rooms, and their utterances were paraphrased us- 
ing IF. The end-to-end procedure was that (1) an 
English utterance was spoken and decoded by the 
JANUS speech recognizer, (2) the output of the rec- 
ognizer was parsed into an IF representation, and 
(3) a different English utterance (supposedly with 
the same meaning) was generated from the IF rep- 
resentation. The speakers had no other means of 
communication with each other. 
In order to evaluate English-German and 
English-Japanese translation, the IFs of the 130 
test sentences were fed into German and Japanese 
generation components atCMU. The data used in 
the evaluation was unseen by system developers 
at the time of the evaluation. For English-Italian 
translation, the IF representations produced by 
the English analysis component were sent to IRST 
to be generated in Italian. 
Evaluation Scoring: In order to score the eval- 
uation, input and output sentences were compared 
by bilingual people, or monolingual people in the 
case of English-IF-English evaluation. A score of 
ok is assigned if the target language utterance is
comprehensible and no components ofmeaning are 
deleted, added, or" changed by the translation. A
21 
We have singles, and t,ins and also Japanese rooms available on the eleventh. 
CMU a:give-information+availability+room 
(room-type=(single ~ twin ~ japanese_style), time=mdll) 
IRST a:give-in2ormation+availability+room 
(room-type=(single ~ twin & japanese_style), time=mdll) 
I'd like a twin room, please. 
CMU c:accept+features+room (room-typeffitwin) 
IBST c:give-information+preference+features+room (room-type=twin) 
A twin room is fourteen thousand yen. 
CMU a:give-information+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
IRST a:give-in.formation+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
Figure 5: Examples of IF coding from CMU and IRST 
.o  
Method 
1 Recosnition only 
2 Transcription 
3 Recosnition 
4 Transcription 
5 Recognition 
6 Transcription 
7 Recognition 
8 Transcription 
9 Recognition 
10 Transcription 
11 Recognition 
I OutPut Language II OK+Perfect Perfect Grader I No. of Graders 
En$1ish 
English. 
En$1ish 
Japanese 
Japanese 
German 
German 
German 
German 
Italian 
Italian 
78 % 62 % CMU 3 
74 % 54 % CMU 3 
59 ~ 42 % CMU 3 
777 % 59 % CMU 2 
62 % 4,5 % CMU 2 
70 %- ..... s9 % CMU " 
58 % 34 % CMU 2 
67 ~ 43 % IRST 2 
59 % 36 % IRST 2 
73 % 51% IRST .... . .  6 
61% 42 % IRST 6 
Figure 6: Translation Grades for English to English, Japanese, German, and Italian 
score of perfect is assigned if, in addition to the 
previous criteria, the translation is fluent in the 
target language. A score of bad is assigned if the 
target language sentence is incomprehensible or 
some element of meaning has been added, deleted, 
or changed. The evaluation procedure isdescribed 
in detail in \[GLL+96\]. In Figure 6, acceptable is
the sum of per fec t  and ok scores, s 
Figure 6 shows the results of the intra-site 
and inter-site evaluations. The first row grades 
the speech recognition output against a human- 
produced transcript of what was said. This gives 
us a ceiling for how well we could do if trans- 
lation were perfect, given speech recognition er- 
rors. Rows 2 through 7 show the results of the 
intra-site evaluation. All analyzers and genera- 
tors were written at CMU, and the results were 
graded by CMU researchers. (The German re- 
sults are a lower than the English and Japanese 
results because a shorter time was spent on gram- 
mar development.) Rows 8 and 9 report on CMU's 
intra~site valuation of English-German transla~ 
Sin another paper (\[LBL+00\]), we describe a task- 
based evaluation which focuses on success of commu- 
nicative goals and how long it takes to achieve them. 
tion (the same system as in Rows 6 and 7), but 
the results were graded by researchers at IRST. 
Comparing Rows 6 and 7 with Rows 8 and 9, we 
can check that CMU and IRST graders were us- 
ing roughly the same grading criteria: a difference 
of up to ten percent among graders is normal in 
our experience. Rows 10 and 11 show the results 
of the inter-site English-Italian evaluation. The 
CMU English analyzer produced IF representa- 
tions which were sent to IRST and were fed into 
IRST's Italian generator. The results were graded 
by IRST researchers. 
Conclusions drawn from the inter-site valuation: 
Since the inter-site evaluation results are compa- 
rable to the intra-site results, we conclude that re- 
searchers at IRST and CMU are using IF at least 
as consistently as researchers within CMU. 
Future Plans 
In the next phase of C-STAR, we will cover de- 
scriptive sentences (e.g., The castle was built in 
the thirteenth century and someone was impris- 
oned in the tower) as well as task-oriented sen- 
tences. Descriptive sentences will be represented 
22 
in a more traditional frame-based interlingua fo- 
cusing on lexical meaning and grammatical fea- 
tures of the sentences. We are working on disam- 
biguating literal from task-oriented meanings in 
context. For example That's great could be an ac- 
ceptance (like I'll take it) (task oriented) or could 
just express appreciation. Sentences may also con- 
tain a combination of task oriented (e.g., Can you 
tell me) and descriptive (how long the castle has 
been standing) components. 
\[GLL+96\] 
\[LBL+O0\] 
\[LGLW98\] 
Re ferences  
Donna Gates, A. Lavie, L. Levin, 
A. Waibel, M. Gavald~, L. Mayfield, 
M:-Woszczyna, and P. Zhan. End-to- 
End Evaluation in JANUS: A Speech- 
to-Speech Translation System. In Pro- 
ceedings of ECAI-96, Budapest, Hun- 
gary, 1996. 
Lori Levin, Boris Bartlog, Ari- 
adna Font Llitjos, Donna Gates, Alon 
Lavie, Dorcas Wallace, Taro Watan- 
abe, and Monika Woszczyna. Lessons 
Learned from a Task-Based Evaluation 
of Speech-to-Speech MT. In Proceed- 
ings of LREC 2000, Athens, Greece, 
June to appear, 2000. 
Lori Levin, D. Gates, A. Lavie, and 
A. Waibel. An Interlingua Based on 
Domain Actions for Machine Transla- 
tion of Task-Oriented Dialogues. In 
Proceedings of the International Con- 
ference on Spoken Language Process- 
ing (ICSLP'98), Sydney, Australia, 
1998. 
\[LLW +\] 
\[PT98\] 
Lori Levin, A. Lavie, M. Woszczyna, 
D. Gates, M. Gavald~, D. Koll, and 
A. Waibel. The Janus-III Translation 
System. Machine Translation. To ap- 
pear. 
Fabio Pianesi and Lucia Tovena. Us- 
ing the Interchange Format for Encod- 
ing Spoken Dialogue. In Proceedings of
SIG-IL Workshop, 1998. 
23 
Spoken Language Parsing Using Phrase-Level Grammars and Trainable 
Classifiers 
Chad Langley, Alon Lavie, Lori Levin, Dorcas Wallace, Donna Gates, and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
{clangley|alavie|lsl|dorcas|dmg|kay}@cs.cmu.edu 
 
Abstract 
In this paper, we describe a novel 
approach to spoken language analysis 
for translation, which uses a combination 
of grammar-based phrase-level parsing 
and automatic classification. The job of 
the analyzer is to produce a shallow 
semantic interlingua representation for 
spoken task-oriented utterances. The 
goal of our hybrid approach is to provide 
accurate real-time analyses while 
improving robustness and portability to 
new domains and languages. 
1 Introduction 
Interlingua-based approaches to Machine 
Translation (MT) are highly attractive in systems 
that support a large number of languages. For each 
source language, an analyzer that converts the 
source language into the interlingua is required. 
For each target language, a generator that converts 
the interlingua into the target language is needed. 
Given analyzers and generators for all supported 
languages, the system simply connects the source 
language analyzer with the target language 
generator to perform translation. 
Robust and accurate analysis is critical in 
interlingua-based translation systems. In speech-to-
speech translation systems, the analyzer must be 
robust to speech recognition errors, spontaneous 
speech, and ungrammatical inputs as described by 
Lavie (1996). Furthermore, the analyzer should run 
in (near) real time. 
In addition to accuracy, speed, and robustness, 
the portability of the analyzer with respect to new 
domains and new languages is an important 
consideration. Despite continuing improvements in 
speech recognition and translation technologies, 
restricted domains of coverage are still necessary 
in order to achieve reasonably accurate machine 
translation. Porting translation systems to new 
domains or even expanding the coverage in an 
existing domain can be very difficult and time-
consuming.  This creates significant challenges in 
situations where translation is needed for a new 
domain within relatively short notice. Likewise, 
demand can be high for translation systems that 
can be rapidly expanded to include new languages 
that were not previously considered important. 
Thus, it is important that the analysis approach 
used in a translation system be portable to new 
domains and languages. 
One approach to analysis in restricted domains 
is to use semantic grammars, which focus on 
parsing semantic concepts rather than syntactic 
structure. Semantic grammars can be especially 
useful for parsing spoken language because they 
are less susceptible to syntactic deviations caused 
by spontaneous speech effects. However, the focus 
on meaning rather than syntactic structure 
generally makes porting to a new domain quite 
difficult. Since semantic grammars do not exploit 
syntactic similarities across domains, completely 
new grammars must usually be developed. 
While grammar-based parsing can provide very 
accurate analyses on development data, it is 
difficult for a grammar to completely cover a 
domain, a problem that is exacerbated by spoken 
input. Furthermore, it generally takes a great deal 
of effort by human experts to develop a high-
coverage grammar. On the other hand, machine 
learning approaches can generalize beyond training 
data and tend to degrade gracefully in the face of 
noisy input. Machine learning methods may, 
however, be less accurate on clearly in-domain 
input than grammars and may require a large 
amount of training data. 
We describe a prototype version of an analyzer 
that combines phrase-level parsing and machine 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 15-22.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
learning techniques to take advantage of the 
benefits of each. Phrase-level semantic grammars 
and a robust parser are used to extract low-level 
interlingua arguments from an utterance. Then, 
automatic classifiers assign high-level domain 
actions to semantic segments in the utterance. 
2 MT System Overview 
The analyzer we describe is used for English and 
German in several multilingual human-to-human 
speech-to-speech translation systems, including the 
NESPOLE! system (Lavie et al, 2002). The goal 
of NESPOLE! is to provide translation for 
common users within real-world e-commerce 
applications. The system currently provides 
translation in the travel and tourism domain 
between English, French, German and Italian.  
NESPOLE! employs an interlingua-based 
translation approach that uses four basic steps to 
perform translation. First, an automatic speech 
recognizer processes spoken input. The best-
ranked hypothesis from speech recognition is then 
passed through the analyzer to produce interlingua. 
Target language text is then generated from the 
interlingua. Finally, the target language text is 
synthesized into speech. 
This interlingua-based translation approach 
allows for distributed development of the 
components for each language. The components 
for each language are assembled into a translation 
server that accepts speech, text, or interlingua as 
input and produces interlingua, text, and 
synthesized speech. In addition to the analyzer 
described here, the English translation server uses 
the JANUS Recognition Toolkit for speech 
recognition, the GenKit system (Tomita & Nyberg, 
1988) for generation, and the Festival system 
(Black et al, 1999) for synthesis. 
NESPOLE! uses a client-server architecture 
(Lavie et al, 2001) to enable users who are 
browsing the web pages of a service provider (e.g. 
a tourism bureau) to seamlessly connect to a 
human agent who speaks a different language. 
Using commercially available software such as 
Microsoft NetMeeting?, a user is connected to the 
NESPOLE! Mediator, which establishes 
connections with the agent and with translation 
servers for the appropriate languages. During a 
dialogue, the Mediator transmits spoken input from 
the users to the translation servers and synthesized 
translations from the servers to the users. 
3 The Interlingua 
The interlingua used in the NESPOLE! system is 
called Interchange Format (IF) (Levin et al, 1998; 
Levin et al, 2000). The IF defines a shallow 
semantic representation for task-oriented 
utterances that abstracts away from language-
specific syntax and idiosyncrasies while capturing 
the meaning of the input. Each utterance is divided 
into semantic segments called semantic dialog 
units (SDUs), and an IF is assigned to each SDU. 
An IF representation consists of four parts: a 
speaker tag, a speech act, an optional sequence of 
concepts, and an optional set of arguments. The 
representation takes the following form: 
 
speaker : speech act +concept* (argument*) 
 
The speaker tag indicates the role of the speaker 
in the dialogue. The speech act captures the 
speaker?s intention. The concept sequence, which 
may contain zero or more concepts, captures the 
focus of an SDU. The speech act and concept 
sequence are collectively referred to as the domain 
action (DA). The arguments use a feature-value 
representation to encode specific information from 
the utterance. Argument values can be atomic or 
complex. The IF specification defines all of the 
components and describes how they can be legally 
combined. Several examples of utterances with 
corresponding IFs are shown below. 
 
Thank you very much. 
a:thank 
Hello. 
c:greeting (greeting=hello) 
How far in advance do I need to book a room for the Al-
Cervo Hotel? 
c:request-suggestion+reservation+room ( 
   suggest-strength=strong, 
   time=(time-relation=before, 
     time-distance=question), 
   who=i, 
   room-spec=(room, identifiability=no, 
     location=(object-name=cervo_hotel))) 
4 The Hybrid Analysis Approach 
Our hybrid analysis approach uses a combination 
of grammar-based parsing and machine learning 
techniques to transform spoken utterances into the 
IF representation described above. The speaker tag 
is assumed to be given. Thus, the goal of the 
analyzer is to identify the DA and arguments.  
The hybrid analyzer operates in three stages. 
First, semantic grammars are used to parse an 
utterance into a sequence of arguments. Next, the 
utterance is segmented into SDUs. Finally, the DA 
is identified using automatic classifiers. 
4.1 Argument Parsing 
The first stage in analysis is parsing an utterance 
for arguments. During this stage, utterances are 
parsed with phrase-level semantic grammars using 
the robust SOUP parser (Gavald?, 2000). 
4.1.1 The Parser 
The SOUP parser is a stochastic, chart-based, top-
down parser that is designed to provide real-time 
analysis of spoken language using context-free 
semantic grammars. One important feature 
provided by SOUP is word skipping. The amount 
of skipping allowed is configurable and a list of 
unskippable words can be defined. Another feature 
that is critical for phrase-level argument parsing is 
the ability to produce analyses consisting of 
multiple parse trees. SOUP also supports modular 
grammar development (Woszczyna et al, 1998). 
Subgrammars designed for different domains or 
purposes can be developed independently and 
applied in parallel during parsing. Parse tree nodes 
are then marked with a subgrammar label. When 
an input can be parsed in multiple ways, SOUP can 
provide a ranked list of interpretations. 
In the prototype analyzer, word skipping is only 
allowed between parse trees. Only the best-ranked 
argument parse is used for further processing. 
4.1.2 The Grammars 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument 
grammar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains phrase-
level rules for parsing arguments defined in the IF. 
Top-level argument grammar nonterminals 
correspond to top-level arguments in the IF. 
The pseudo-argument grammar contains top-
level nonterminals that do not correspond to 
interlingua concepts. These rules are used for 
parsing common phrases that can be grouped into 
classes to capture more useful information for the 
classifiers. For example, all booked up, full, and 
sold out might be grouped into a class of phrases 
that indicate unavailability. In addition, rules in the 
pseudo-argument grammar can be used for 
contextual anchoring of ambiguous arguments. For 
example, the arguments [who=] and [to-whom=] 
have the same values. To parse these arguments 
properly in a sentence like ?Can you send me the 
brochure??, we use a pseudo-argument grammar 
rule, which refers to the arguments [who=] and [to-
whom=] within the appropriate context.  
The cross-domain grammar contains rules for 
parsing whole DAs that are domain-independent. 
For example, this grammar contains rules for 
greetings (Hello, Good bye, Nice to meet you, etc.). 
Cross-domain grammar rules do not cover all 
possible domain-independent DAs. Instead, the 
rules focus on DAs with simple or no argument 
lists. Domain-independent DAs with complex 
argument lists are left to the classifiers. Cross-
domain rules play an important role in the 
prediction of SDU boundaries. 
Finally, the shared grammar contains common 
grammar rules that can be used by all other 
subgrammars. These include definitions for most 
of the arguments, since many can also appear as 
sub-arguments. RHSs in the argument grammar 
contain mostly references to rules in the shared 
grammar. This method eliminates redundant rules 
in the argument and shared grammars and allows 
for more accurate grammar maintenance. 
4.2 Segmentation 
The second stage of processing in the hybrid 
analysis approach is segmentation of the input into 
SDUs. The IF representation assigns DAs at the 
SDU level. However, since dialogue utterances 
often consist of multiple SDUs, utterances must be 
segmented into SDUs before DAs can be assigned. 
Figure 1 shows an example utterance containing 
four arguments segmented into two SDUs. 
 
SDU1  SDU2  
greeting= disposition= visit-spec= location= 
hello i would like to take a vacation in val di fiemme 
Figure 1. Segmentation of an utterance into SDUs. 
The argument parse may contain trees for cross-
domain DAs, which by definition cover a complete 
SDU. Thus, there must be an SDU boundary on 
both sides of a cross-domain tree. Additionally, no 
SDU boundaries are allowed within parse trees. 
The prototype analyzer drops words skipped 
between parse trees, leaving only a sequence of 
trees. The parse trees on each side of a potential 
boundary are examined, and if either tree was 
constructed by the cross-domain grammar, an SDU 
boundary is inserted. Otherwise, a simple statistical 
model similar to the one described by Lavie et al 
(1997) estimates the likelihood of a boundary. 
The statistical model is based only on the root 
labels of the parse trees immediately preceding and 
following the potential boundary position. Suppose 
the position under consideration looks like 
[A1?A2], where there may be a boundary between 
arguments A1 and A2. The likelihood of an SDU 
boundary is estimated using the following formula: 
 
])C([A  ])C([A
])AC([  ])C([A])AF([A
21
21
21
+
?+?
??  
 
The counts C([A1?]), C([?A2]), C([A1]), C([A2]) 
are computed from the training data. An evaluation 
of this baseline model is presented in section 6.  
4.3 DA Classification 
The third stage of analysis is the identification of 
the DA for each SDU using automatic classifiers. 
After segmentation, a cross-domain parse tree may 
cover an SDU. In this case, analysis is complete 
since the parse tree contains the DA. Otherwise, 
automatic classifiers are used to assign the DA. In 
the prototype analyzer, the DA classification task 
is split into separate subtasks of classifying the 
speech act and concept sequence. This reduces the 
complexity of each subtask and allows for the 
application of specialized techniques to identify 
each component. 
One classifier is used to identify the speech act, 
and a second classifier identifies the concept 
sequence. Both classifiers are implemented using 
TiMBL (Daelemans et al, 2000), a memory-based 
learner. Speech act classification is performed first. 
Input to the speech act classifier is a set of binary 
features that indicate whether each of the possible 
argument and pseudo-argument labels is present in 
the argument parse for the SDU. No other features 
are currently used. Concept sequence classification 
is performed after speech act classification. The 
concept sequence classifier uses the same feature 
set as the speech act classifier with one additional 
feature: the speech act assigned by the speech act 
classifier. We present an evaluation of this baseline 
DA classification scheme in section 6. 
4.4 Using the IF Specification 
The IF specification imposes constraints on how 
elements of the IF representation can legally 
combine. DA classification can be augmented with 
knowledge of constraints from the IF specification, 
providing two advantages over otherwise na?ve 
classification. First, the analyzer must produce 
valid IF representations in order to be useful in a 
translation system. Second, using knowledge from 
the IF specification can improve the quality of the 
IF produced, and thus the translation. 
Two elements of the IF specification are 
especially relevant to DA classification. First, the 
specification defines constraints on the 
composition of DAs. There are constraints on how 
concepts are allowed to pair with speech acts as 
well as ordering constraints on how concepts are 
allowed to combine to form a valid concept 
sequence. These constraints can be used to 
eliminate illegal DAs during classification. The 
second important element of the IF specification is 
the definition of how arguments are licensed by 
speech acts and concepts. In order for an IF to be 
valid, at least one speech act or concept in the DA 
must license each argument. 
The prototype analyzer uses the IF specification 
to aid classification and guarantee that a valid IF 
representation is produced. The speech act and 
concept sequence classifiers each provide a ranked 
list of possible classifications. When the best 
speech act and concept sequence combine to form 
an illegal DA or form a legal DA that does not 
license all of the arguments, the analyzer attempts 
to find the next best legal DA that licenses the 
most arguments. Each of the alternative concept 
sequences (in ranked order) is combined with each 
of the alternative speech acts (in ranked order). For 
each possible legal DA, the analyzer checks if all 
of the arguments found during parsing are licensed. 
If a legal DA is found that licenses all of the 
arguments, then the process stops. If not, one 
additional fallback strategy is used. The analyzer 
then tries to combine the best classified speech act 
with each of the concept sequences that occurred in 
the training data, sorted by their frequency of 
occurrence. Again, the analyzer checks if each 
legal DA licenses all of the arguments and stops if 
such a DA is found. If this step fails to produce a 
legal DA that licenses all of the arguments, the 
best-ranked DA that licenses the most arguments is 
returned. In this case, any arguments that are not 
licensed by the selected DA are removed. This 
approach is used because it is generally better to 
select an alternative DA and retain more arguments 
than to keep the best DA and lose the information 
represented by the arguments. An evaluation of 
this strategy is presented in the section 6. 
5 Grammar Development and 
Classifier Training 
During grammar development, it is generally 
useful to see how changes to the grammar affect 
the IF representations produced by the analyzer. In 
a purely grammar-based analysis approach, full 
interlingua representations are produced as the 
result of parsing, so testing new grammars simply 
requires loading them into the parser. Because the 
grammars used in our hybrid approach parse at the 
argument level, testing grammar modifications at 
the complete IF level requires retraining the 
segmentation model and the DA classifiers. 
 When new grammars are ready for testing, 
utterance-IF pairs for the appropriate language are 
extracted from the training database. Each 
utterance-IF pair in the training data consists of a 
single SDU with a manually annotated IF. Using 
the new grammars, the argument parser is applied 
to each utterance to produce an argument parse. 
The counts used by the segmentation model are 
then recomputed based on the new argument 
parses. Since each utterance contains a single 
SDU, the counts C([?A2]) and C([A1?]) can be 
computed directly from the first and last arguments 
in the parse respectively. 
Next, the training examples for the DA 
classifiers are constructed. Each training example 
for the speech act classifier consists of the speech 
act from the annotated IF and a vector of binary 
features with a positive value set for each argument 
or pseudo-argument label that occurs in the 
argument parse. The training examples for the 
concept sequence classifiers are similar with the 
addition of the annotated speech act to the feature 
vector. After the training examples are constructed, 
new classifiers are trained. 
Two tools are available to support easy testing 
during grammar development. First, the entire 
training process can be run using a single script. 
Retraining for a new grammar simply requires 
running the script with pointers to the new 
grammars. Then, a special development mode of 
the translation servers allows the grammar writers 
to load development grammars and their 
corresponding segmentation model and DA 
classifiers. The translation server supports input in 
the form of individual utterances or files and 
allows the grammar developers to look at the 
results of each stage of the analysis process. 
6 Evaluation 
We present the results from recent experiments to 
measure the performance of the analyzer 
components and of end-to-end translation using the 
analyzer. We also report the results of an ablation 
experiment that used earlier versions of the 
analyzer and IF specification. 
6.1 Translation Experiment 
 
Acceptable Perfect 
SR Hypotheses 66% 56% 
Translation from 
Transcribed Text 58% 43% 
Translation from 
SR Hypotheses 45% 32% 
Table 1. English-to-English end-to-end translation 
 
Acceptable Perfect 
Translation from 
Transcribed Text 55% 38% 
Translation from 
SR Hypotheses 43% 27% 
Table 2. English-to-Italian end-to-end translation 
Tables 1 and 2 show end-to-end translation 
results of the NESPOLE! system. In this 
experiment, the input was a set of English 
utterances. The utterances were paraphrased back 
into English via the interlingua (Table 1) and 
translated into Italian (Table 2). The data used to 
train the DA classifiers consisted of 3350 SDUs 
annotated with IF representations. The test set 
contained 151 utterances consisting of 332 SDUs 
from 4 unseen dialogues. Translations were 
compared to human transcriptions and graded as 
described in (Levin et al, 2000). A grade of 
perfect, ok, or bad was assigned to each 
translation by human graders. A grade of perfect 
or ok is considered acceptable. The table shows the 
average of grades assigned by three graders. 
The row in Table 1 labeled SR Hypotheses 
shows the grades when the speech recognizer 
output is compared directly to human transcripts. 
As these grades show, recognition errors can be a 
major source of unacceptable translations. These 
grades provide a rough bound on the translation 
performance that can be expected when using input 
from the speech recognizer since meaning lost due 
to recognition errors cannot be recovered. The 
rows labeled Translation from Transcribed Text 
show the results when human transcripts are used 
as input. These grades reflect the combined 
performance of the analyzer and generator. The 
rows labeled Translation from SR Hypotheses 
show the results when the speech recognizer 
produces the input utterances. As expected, 
translation performance was worse with the 
introduction of recognition errors. 
 
Precision Recall 
70% 54% 
Table 3. SDU boundary detection performance 
Table 3 shows the performance of the 
segmentation model on the test set. The SDU 
boundary positions assigned automatically were 
compared with manually annotated positions. 
 
 
Classifier Accuracy 
Speech Act 65% 
Concept Sequence 54% 
Domain Action 43% 
Table 4. Classifier accuracy on transcription 
 
Frequency 
Speech Act 33% 
Concept Sequence 40% 
Domain Action 14% 
Table 5. Frequency of most common DA elements 
Table 4 shows the performance of the DA 
classifiers, and Table 5 shows the frequency of the 
most common DA, speech act, and concept 
sequence in the test set. Transcribed utterances 
were used as input and were segmented into SDUs 
before analysis. This experiment is based on only 
293 SDUs. For the remaining SDUs in the test set, 
it was not possible to assign a valid representation 
based on the current IF specification. 
These results demonstrate that it is not always 
necessary to find the canonical DA to produce an 
acceptable translation. This can be seen by 
comparing the Domain Action accuracy from Table 
4 with the Transcribed grades from Table 1. 
Although the DA classifiers produced the 
canonical DA only 43% of the time, 58% of the 
translations were graded as acceptable. 
 
 
Changed 
Speech Act 5% 
Concept Sequence 26% 
Domain Action 29% 
Table 6. DA elements changed by IF specification 
In order to examine the effects of using IF 
specification constraints, we looked at the 182 
SDUs which were not parsed by the cross-domain 
grammar and thus required DA classification. 
Table 6 shows how many DAs, speech acts, and 
concept sequences were changed as a result of 
using the constraints. DAs were changed either 
because the DA was illegal or because the DA did 
not license some of the arguments. Without the IF 
specification, 4% of the SDUs would have been 
assigned an illegal DA, and 29% of the SDUs 
(those with a changed DA) would have been 
assigned an illegal IF. Furthermore, without the IF 
specification, 0.38 arguments per SDU would have 
to be dropped while only 0.07 arguments per SDU 
were dropped when using the fallback strategy. 
The mean number of arguments per SDU was 1.47. 
6.2 Ablation Experiment 
Classification Accuracy (16-fold Cross 
Validation)
0
0.2
0.4
0.6
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
e
a
n
 
A
cc
ur
a
c
y
Speech Act
Concept
Sequence
Domain Action
 
Figure 2: DA classifier accuracy with varying 
amounts of data 
Figure 2 shows the results of an ablation 
experiment that examined the effect of varying the 
training set size on DA classification accuracy. 
Each point represents the average accuracy using a 
16-fold cross validation setup. 
The training data contained 6409 SDU-
interlingua pairs. The data were randomly divided 
into 16 test sets containing 400 examples each. In 
each fold, the remaining data were used to create 
training sets containing 500, 1000, 2000, 3000, 
4000, 5000, and 6009 examples. 
The performance of the classifiers appears to 
begin leveling off around 4000 training examples. 
These results seem promising with regard to the 
portability of the DA classifiers since a data set of 
this size could be constructed in a few weeks. 
7 Related Work 
Lavie et al (1997) developed a method for 
identifying SDU boundaries in a speech-to-speech 
translation system. Identifying SDU boundaries is 
also similar to sentence boundary detection. 
Stevenson and Gaizauskas (2000) use TiMBL 
(Daelemans et al, 2000) to identify sentence 
boundaries in speech recognizer output, and Gotoh 
and Renals (2000) use a statistical approach to 
identify sentence boundaries in automatic speech 
recognition transcripts of broadcast speech. 
Munk (1999) attempted to combine grammars 
and machine learning for DA classification. In 
Munk?s SALT system, a two-layer HMM was used 
to segment and label arguments and speech acts. A 
neural network identified the concept sequences. 
Finally, semantic grammars were used to parse 
each argument segment. One problem with SALT 
was that the segmentation was often inaccurate and 
resulted in bad parses. Also, SALT did not use a 
cross-domain grammar or interlingua specification. 
Cattoni et al (2001) apply statistical language 
models to DA classification. A word bigram model 
is trained for each DA in the training data. To label 
an utterance, the most likely DA is assigned. 
Arguments are identified using recursive transition 
networks. IF specification constraints are used to 
find the most likely valid DA and arguments. 
8 Discussion and Future Work 
One of the primary motivations for developing the 
hybrid analysis approach described here is to 
improve the portability of the analyzer to new 
domains and languages. We expect that moving 
from a purely grammar-based parsing approach to 
this hybrid approach will help attain this goal. 
The SOUP parser supports portability to new 
domains by allowing separate grammar modules 
for each domain and a grammar of rules shared 
across domains (Woszczyna et al, 1998). This 
modular grammar design provides an effective 
method for adding new domains to existing 
grammars. Nevertheless, developing a full 
semantic grammar for a new domain requires 
significant effort by expert grammar writers. 
The hybrid approach reduces the manual labor 
required to port to new domains by incorporating 
machine learning. The most labor-intensive part of 
developing full semantic grammars for producing 
IF is writing DA-level rules. This is exactly the 
work eliminated by using automatic DA classifiers. 
Furthermore, the phrase-level argument grammars 
used in the analyzer contain fewer rules than a full 
semantic grammar. The argument-level grammars 
are also less domain-dependent than the full 
grammars and thus more reusable. The DA 
classifiers should also be more tolerant than full 
grammars of deviations from the domain. 
We analyzed the grammars from a previous 
version of the translation system, which produced 
complete IFs using strictly grammar-based parsing, 
to estimate what portion of the grammar was 
devoted to the identification of domain actions. 
Approximately 2200 rules were used to cover 400 
DAs. Nonlexical rules made up about half of the 
grammar, and the DA rules accounted for about 
20% of the nonlexical rules. Using these figures, 
we can project the number of DA rules that would 
have to be added to the current system, which uses 
our hybrid analysis approach. The database for the 
new system contains approximately 600 DAs. 
Assuming the average number of rules per DA is 
the same as before, roughly 3300 DA-level rules 
would have to be added to the current grammar, 
which has about 17500 nonlexical rules, to cover 
the DAs in the database. 
Our hybrid approach should also improve the 
portability of the analyzer to new languages. Since 
grammars are language specific, adding a new 
language still requires writing new argument 
grammars. Then the DA classifiers simply need to 
be retrained on data for the new language. If 
training data for the new language were not 
available, DA classifiers using only language-
independent features, from the IF for example, 
could be trained on data for existing languages and 
used for the new language. Such classifiers could 
be used as a starting point until training data was 
available in the new language. 
The experimental results indicate the promise 
of the analysis approach we have described. The 
level of performance reported here was achieved 
using a simple segmentation model and simple DA 
classifiers with limited feature sets. We expect that 
performance will substantially improve with a 
more informed design of the segmentation model 
and DA classifiers. We plan to examine various 
design options, including richer feature sets and 
alternative classification techniques. We are also 
planning experiments to evaluate robustness and 
portability when the coverage of the NESPOLE! 
system is expanded to the medical domain later 
this year. In these experiments, we will measure 
the effort needed to write new argument grammars, 
the extent to which existing argument grammars 
are reusable, and the effort required to expand the 
argument grammar to include DA-level rules. 
9 Acknowledgements 
The research work reported here was supported by 
the National Science Foundation under Grant 
number 9982227. Special thanks to Alex Waibel 
and everyone in the NESPOLE! group for their 
support on this work. 
References 
Black, A., P. Taylor, and R. Caley. 1999. The 
Festival Speech Synthesis System: System 
Documentation. Human Computer Research 
Centre, University of Edinburgh, Scotland. 
http://www.cstr.ed.ac.uk/projects/festival/ma
nual 
Cattoni, R., M. Federico, and A. Lavie. 2001. 
Robust Analysis of Spoken Input Combining 
Statistical and Knowledge-Based Information 
Sources. In Proceedings of the IEEE Automatic 
Speech Recognition and Understanding 
Workshop, Trento, Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2000. TiMBL: Tilburg Memory 
Based Learner, version 3.0, Reference Guide. 
ILK Technical Report 00-01. 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz 
Gavald?, M. 2000. SOUP: A Parser for Real-
World Spontaneous Speech. In Proceedings of 
the IWPT-2000, Trento, Italy. 
Gotoh, Y. and S. Renals. Sentence Boundary 
Detection in Broadcast Speech Transcripts. 2000. 
In Proceedings on the International Speech 
Communication Association Workshop: 
Automatic Speech Recognition: Challenges for 
the New Millennium, Paris. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. 
Enhancing the Usability and Performance of 
NESPOLE! ? a Real-World Speech-to-Speech 
Translation System. In Proceedings of HLT-
2002, San Diego, CA. 
Lavie, A., C. Langley, A. Waibel, et al 2001. 
Architecture and Design Considerations in 
NESPOLE!: a Speech Translation System for E-
commerce Applications. In Proceedings of HLT-
2001, San Diego, CA. 
Lavie, A., D. Gates, N. Coccaro, and L. Levin. 
1997. Input Segmentation of Spontaneous Speech 
in JANUS: a Speech-to-speech Translation 
System. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-
96 Workshop, E. Maier, M. Mast, and S. 
Luperfoy (eds.), LNCS series, Springer Verlag. 
Lavie, A. 1996. GLR*: A Robust Grammar-
Focused Parser for Spontaneously Spoken 
Language. PhD dissertation, Technical Report 
CMU-CS-96-126, Carnegie Mellon University, 
Pittsburgh, PA. 
Levin, L., D. Gates, A. Lavie, et al 2000. 
Evaluation of a Practical Interlingua for Task-
Oriented Dialogue. In Workshop on Applied 
Interlinguas: Practical Applications of 
Interlingual Approaches to NLP, Seattle. 
Levin, L., D. Gates, A. Lavie, and A. Waibel. 
1998. An Interlingua Based on Domain Actions 
for Machine Translation of Task-Oriented 
Dialogues. In Proceedings of ICSLP-98, Vol. 4, 
pp. 1155-1158, Sydney, Australia. 
Munk, M. 1999. Shallow Statistical Parsing for 
Machine Translation. Diploma Thesis, Karlsruhe 
University. 
Stevenson, M. and R. Gaizauskas. Experiments on 
Sentence Boundary Detection. 2000. In 
Proceedings of ANLP and NAACL-2000, Seattle. 
Tomita, M. and E. H. Nyberg. 1988. Generation 
Kit and Transformation Kit, Version 3.2: User?s 
Manual. Technical Report CMU-CMT-88-
MEMO, Carnegie Mellon University, Pittsburgh, 
PA. 
Woszczyna, M., M. Broadhead, D. Gates, et al 
1998. A Modular Approach to Spoken Language 
Translation for Large Domains. In Proceedings 
of AMTA-98, Langhorne, PA. 
Balancing Expressiveness and Simplicity
in an Interlingua for Task Based Dialogue
Lori Levin, Donna Gates, Dorcas Wallace,
Kay Peterson, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
email: lsl@cs.cmu.edu
Fabio Pianesi, Emanuele Pianta,
Roldano Cattoni, Nadia Mana
IRST-itc, Italy
Abstract
In this paper we compare two interlin-
gua representations for speech transla-
tion. The basis of this paper is a distri-
butional analysis of the C-star II and
Nespole databases tagged with inter-
lingua representations. The C-star II
database has been partially re-tagged
with the Nespole interlingua, which
enables us to make comparisons on the
same data with two types of interlin-
guas and on two types of data (C-
star II and Nespole) with the same
interlingua. The distributional infor-
mation presented in this paper show
that the Nespole interlingua main-
tains the language-independence and
simplicity of the C-star II speech-act-
based approach, while increasing se-
mantic expressiveness and scalability.
1 Introduction
Several speech translation projects have chosen
interlingua-based approaches because of its con-
venience (especially in adding new languages)
in multi-lingual projects. However, interlingua
design is notoriously dicult and inexact. The
main challenge is deciding on the grain size of
meaning to represent and what facets of mean-
ing to include. This may depend on the do-
main and the contexts in which the translation
system is used. For projects that take place at
multiple research sites, another factor becomes
important in interlingua design: if the interlin-
gua is too complex, it cannot be used reliably by
researchers at remote sites. Furthermore, the in-
terlingua should not be biased toward one fam-
ily of languages. Finally, an interlingua should
clearly distinguish general and domain specic
components for easy scalability and portability
between domains.
Sections 2 and 3 describe how we balanced
the factors of grain-size, language independence,
and simplicity in two interlinguas for speech
translation projects | the C-star II Inter-
change Format (Levin et al, 1998) and the Ne-
spole Interchange Format. Both interlinguas
are based in the framework of domain actions
as described in (Levin et al, 1998). We will
show that the Nespole interlingua has a ner
grain-size of meaning, but is still simple enough
for collaboration across multiple research sites,
and still maintains language-independence.
Section 4 will address the issue of scalabil-
ity of interlinguas based on domain actions to
larger domains. The basis of Section 4 is a dis-
tributional analysis of the C-star II and Ne-
spole databases tagged with interlingua repre-
sentations. The C-star II database has been
partially re-tagged with the Nespole interlin-
gua, which enables us to make comparisons on
the same data with two types of interlinguas and
on two types of data (C-star II and Nespole)
with the same type of interlingua.
2 The C-star II Domain, Database,
and Interlingua
The C-star II interlingua (Levin et al, 1998)
was developed between 1997 and 1999 for use
in the C-star II 1999 demo (www.c-star.org).
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 53-60.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
c: can I have some flight times
that would leave some time around June sixth
a: the there are several flights leaving D C
there?d be one at one twenty four
there?s a three fifty nine flight
that arrives at four fifty eight
...
what time would you like to go
c: I would take the last one that you mentioned
...
a: what credit card number would you like
to reserve this with
c: I have a visa card
and the number is double oh five three
three one one six
ninety nine eighty seven
a okay
c: the expiration date is eleven ninety seven
...
a okay they should be ready tomorrow
c: okay thank you very much
Figure 1: Excerpt from a C-star II dialogue
with six participating research sites. The seman-
tic domain was travel, including reservations
and payments for hotels, tours, and transporta-
tion. Figure 1 shows a sample dialogue from
the C-star II database. (C is the client and a
is the travel agent.) The C-star II database
contains 2278 English sentences and 7148 non-
English (Japanese, Italian, Korean) sentences
tagged with interlingua representations. Most
of the database consists of transcripts of role-
playing conversations.
The driving concept behind the C-star II
interlingua is that there are a limited num-
ber of actions in the domain | requesting the
price of a room, telling the price of a room,
requesting the time of a flight, giving a credit
card number, etc. | and that each utter-
ance can be classied as an instance of one
of these domain actions . Figure 2 illustrates
the components of the C-star II interlingua:
(1) the speaker tag, in this case c for client,
(2) a speech act (request-action), (3) a list
of concepts (reservation, temporal, hotel),
(4) arguments (e.g., time), and (5) values of ar-
guments. The C-star II interlingua specica-
tion document contains denitions for 44 speech
acts, 93 concepts, and 117 argument names.
The domain action is the part of the interlin-
gua consisting of the speech act and concepts, in
this case request-action+reservation+tem-
poral+hotel. The domain action does not in-
clude the list of argument-value pairs.
First it is important to point out that do-
main actions are created compositionally. A do-
main action consists of a speech act followed by
zero or more concepts. (Recall that argument-
value pairs are not part of the domain action.)
The Nespole interlingua includes 65 speech
acts and 110 concepts. An interlingua speci-
cation document denes the legal combinations
of speech acts and arguments.
The linguistic justication for an interlingua
based on domain-actions is that many travel do-
main utterances contain xed, formulaic phrases
(e.g., can you tell me; I was wondering; how
about; would you mind, etc.) that signal domain
actions, but either do not translate literally into
other languages or have a meaning that is su-
ciently indirect that the literal meaning is irrele-
vant for translation. To take two examples, how
about as a signal of a suggestion does not trans-
late into other languages with the words corre-
sponding to how and about . Also, would you
mind might translate literally into some Euro-
pean languages as a way of signaling a request,
but the literal meaning of minding is not rel-
evant to the translation, only the fact that it
signals politeness.
The measure of success for the domain-action
based interlingua (as described in (Levin et al,
2000a)) is that (1) it covers the data in the C-
star II database with less than 8% no-tag rate,
(2) inter-coder agreement across research sites
is reasonably high: 82% for speech acts, 88%
for concepts, and 65% for domain actions, and
(3) end-to-end translation results using an an-
alyzer and generator written at dierent sites
were about the same as end-to-end translation
results using an analyzer and generator written
at the same site.
3 The Nespole Domain, Database,
and Interlingua
The Nespole interlingua has been under devel-
opment for the last two years as part of the Ne-
spole project (http://nespole.itc.it). Fig-
I would like to make a hotel reservation for the fourth through
the seventh of july
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Figure 2: Example of a C-star II interlingua representation
ure 3 shows a Nespole dialogue. The Ne-
spole domain does not include reservations and
payments, but includes more detailed inquiries
about hotels and facilities for ski vacations and
summer vacations in Val di Fiemme, Italy. (The
tourism board of the Trentino area is a partner
of the Nespole project.) Most of the database
consists of transcripts of dialogues between an
Italian-speaking travel agent and an English or
German speaker playing the role of a traveller.
There are fewer xed, formulaic phrases in the
Nespole domain, prompting us to move toward
domain actions that are more general, and also
requiring more detailed interlingua representa-
tions. Changes from the C-star II interlingua
fall into several categories:
1. Extending semantic expressivity and
syntactic coverage: Increased coverage of
modality, tense, aspect, articles, fragments,
coordinate structures, number, and rhetor-
ical relations. In addition, we have added
more explicit representation of grammati-
cal relations and improved capabilities for
representing modication and embedding.
2. Additional Domain-Specic Con-
cepts: New concepts include giving
directions, describing sizes and dimensions
of objects, traveling routes, equipment and
gear, airports, tourist services, facilities,
vehicles, information objects (brochures,
web pages, rules and regulations), hours
of operation of businesses and attractions,
etc.
3. Utterances that accompany multi-
modal gestures: The Nespole system
includes capabilities to share web pages
and draw marks such as circles and arrows
on web pages. The interlingua was ex-
tended to cover colord, descriptions of two-
dimensional objects, and actions of show-
ing.
4. General concept names from Word-
Net: The Nespole interlingua includes
conventions for making new concept names
based on WordNet synsets.
5. More general domain actions replac-
ing specic ones: For example, replacing
hotel with accommodation.
Interlinguas based on domain actions con-
trast with interlinguas based on lexical seman-
tics (Dorr, 1993; Lee et al, 2001; Goodman and
Nirenburg, 1991). A lexical-semantic interlingua
includes a representation of predicates and their
arguments. For example, the sentence I want to
take a vacation has a predicate want with two
arguments I and to take a vacation, which in
turn has a predicate take and two arguments, I
and a vacation. Of course, predicates like take
may be represented as word senses that are less
language-dependent like participate-in. The
strength and weakness of the lexical-semantic
approach is that it is less domain dependent
than the domain-action approach.
In order to cover the less formulaic utterances
of the Nespole domain, we have taken a step
closer to the lexical-semantic approach. How-
ever, we have maintained the overall framework
of the domain-action approach because there are
still many formulaic utterances that are better
represented in a non-literal way. Also, in or-
der to abstract away from English syntax, con-
cepts such as disposition, eventuality, and obli-
gation are not represented in the interlingua as
argument-taking main verbs in order to accom-
modate languages in which these meanings are
c: and I have some questions about coming about a trip I?m gonna be taking to Trento
a: okay what are your questions
c: I currently have a hotel booking at the
Panorama-Hotel in Panchia but at the moment I have no idea how to get to my hotel from Trento
and I wanted to ask what would be the best way for me to get there
a: okay I?m gonna show you a map that and then describe the directions to you
okay so right so you will arrive in the train station in Trento
the that is shown in the middle of the map stazione FFSS
and just below that here is a bus stop labeled number forty
so okay on the map that I?m showing you here
the hotel is the orange building off on the right hand side
...
c: I also wanted to ask about skiing in the area once I?m in Panchia
a: all right just a moment and I?ll show you another map
c: okay
a: okay so on the map you see now Panchia is right in the center of the map
c: I see it
Figure 3: Excerpt from a Nespole dialogue
represented as adverbs or suxes on verbs. Fig-
ure 4 shows the Nespole interlingua represen-
tation corresponding to the C-star II interlin-
gua in Figure 2. The specication document for
the Nespole interlingua denes 65 speech acts,
110 concepts, 292 arguments, and 7827 values
grouped into 222 value classes. As in the C-
star II interlingua, domain actions are dened
compositionally from speech acts and arguments
in combinations that are allowed by the interlin-
gua specication.
3.1 Comparison of Nespole and
C-star II Interlinguas
It is useful to compare the Nespole and C-
star II Interlinguas in expressivity, language in-
dependence, and simplicity.
Expressivity of the Nespole interlingua,
Argument 1: The metric we use for expres-
sivity is the no-tag rate in the databases. The
no-tag rate is the percentage of sentences that
cannot be assigned an interlingua representation
by a human expert. The C-star II database
tagged with C-star II interlingua had a no-
tag rate of 7.3% (Levin et al, 2000a). The
C-star II database tagged with Nespole in-
terlingua has a no-tag rate of 2.4%. More than
300 English sentences in the C-star II database
that were not covered by the C-star II interlin-
gua are now covered by the Nespole interlin-
gua. (See Table 2.) We conclude from this that
the Nespole interlingua is more expressive in
that it covers more data.
Language-independence of the Nespole
interlingua: We do not have a numerical
measure of language-independence, but we note
that interlinguas based on domain actions are
particularly suitable for avoiding translation
mismatches (Dorr, 1994), particularly head-
switching mismatches (e.g., I just arrived and
Je vient d?arriver where the meaning of recent
past is expressed by an adverb just or a syn-
tactic verb vient (venir).) Interlinguas based
on domain actions resolve head-switching mis-
matches by identifying the types of meanings
that are often involved in mismatches | modal-
ity, evidentiality, disposition, and so on | and
assigning them a representation that abstracts
away from predicate argument structure. In-
terlinguas based on domain actions also neu-
tralize the dierent ways of expressing indirect
speech acts within and across languages (for ex-
ample, Would you mind..., I was wondering if
you could...., and Please.... as ways of request-
ing an action). Although Nespole domain ac-
tions are more general than C-star II domain
actions, they maintain language independence
by abstracting away from predicate-argument
structure.
Simplicity and cross-site reliability of the
Nespole interlingua: Simplicity of an inter-
lingua is measured by cross-site reliability in
I would like to make a hotel reservation for the fourth through
the seventh of july
C-star II Interlingua:
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Nespole Interlingua:
c:give-information+disposition+reservation+accommodation
(disposition=(who=i, desire),
reservation-spec=(reservation, identifiability=no),
accommodation-spec=hotel,
object-time=(start-time=(md=4), end-time=(md=7, month=7, incl-excl=inclusive)))}
Figure 4: Example of Nespole interlingua representation
inter-coder agreement and end-to-end transla-
tion performance. At the time of writing this pa-
per we have not conducted cross-site inter-coder
agreement experiments using the Nespole in-
terlingua. We have, however, conducted cross-
site evaluations (Lavie et al, 2002), in which the
analyzer and generator were written at dier-
ent sites. Experiments at the end of C-star II
showed that cross-site evaluations were compa-
rable to intra-site evaluations (analyzer and gen-
erator written at the same site) (Levin et al,
2000b). Nespole evaluations so far show a loss
of cross-site reliability: intra-site evaluations are
noticeably better than cross-site evaluations, as
reported in (Lavie et al, 2002). This seems to
indicate that developers at dierent sites have
a lower level of agreement on the Nespole in-
terlingua. However there are other possible ex-
planations for the discrepancy | for example
developers at dierent sites may have focused
their development on dierent sub-domains |
that are currently under investigation.
4 Scalability of the Nespole
Interlingua
The rest of this paper addresses the scalability
of the Nespole interlingua. A possible criti-
cism of domain actions is that they are domain
dependent and that the number of domain ac-
tions might increase too quickly with the size
of the domain. In this section, we will examine
the rate of increase in the number of domain ac-
tions as a function of the amount of data and
the diversity of the data.
Dierences in the C-star and Nespole Do-
mains: We will rst show that the C-star
and Nespole domains are signicantly dierent
even though they both pertain to travel. The
combination of the two domains is therefore sig-
nicantly larger than either domain alone.
In order to demonstrate the dierences be-
tween the C-star travel domain and the Ne-
spole travel domain, we measured the overlap
in vocabulary. The numbers in Table 4 are based
on the rst 7900 word tokens in the C-star En-
glish database and the rst 7900 word tokens
in the Nespole English database. The table
shows the number of unique word types in each
database, the number of word types that occur
in both databases, and the number of word types
that occur in one of the databases, but not in the
other. In each database, about half of the word
types overlap with the other database. The non-
overlapping vocabulary (402 C-star word types
and 344 Nespole word types) indicates that the
two databases cover quite dierent aspects of the
travel domain.
Scalability: Argument 1: We will now be-
gin to address the issue of scalability of the
domain action approach to interlingua design.
Our rst argument concerns the number of
Number of unique word types
CSTAR English 745
Nespole English 687
Word types in both CSTAR and Nespole 343
Words types in CSTAR not in Nespole 402
Words types n Nespole not in CSTAR 344
Table 1: Number of overlapping word types in the C-star English and Nespole English
databases
SA Con. Snts. Domain Ac-
tions
Old C-star English 44 93 2278 358
New C-star English 65 110 2564 452
Nespole English 65 110 1446 337
Nespole German 65 110 3298 427
Nespole Italian 65 110 1063 206
Table 2: Number of unique domain actions in interlingua databases
speech acts and concepts in the combined C-
star/Nespole domain. The C-star II in-
terlingua, designed for coverage of the C-star
travel domain, included 44 speech acts and 93
concepts. The Nespole interlingua, designed
for coverage of the combined C-star and Ne-
spole domains, has 65 speech acts and 110 con-
cepts. Thus a relatively small increase in the
number of speech acts and concepts is required
to cover a signicantly larger domain.
The increased size of the C-star/Nepsole
domain is reflected in the number of arguments
and values. The C-star II interlingua contained
denitions for 117 arguments, whereas the Ne-
spole interlingua contains denitions for 292 ar-
guments. The number of values for arguments
also has increased signicantly in the Nespole
domain. There are 7827 values grouped into 222
classes (airport names, days of the week, etc.).
Distributional Data: number of domain
actions in each database: Next we will
present distributional data concerning the num-
ber of domain actions as a function of database
size. We will compare several databases: Old
C-star English (around 2278 sentences tagged
with C-star II interlingua), New C-star En-
glish (2564 sentences tagged with Nespole in-
terlingua, including the 2278 sentences from Old
C-star English), Nespole English, Nespole
German, and Nespole Italian. Table 2 shows
the number of sentences and the number of do-
main actions in each database. The number of
domain actions refers to the number of types,
not tokens, of domain actions.
Distributional data: Coverage of the top
50 domain actions: Table 3 shows the per-
centage of each database that is covered by the
5, 10, 20, and 50 most frequent domain actions
in that database. For each database, the do-
main actions were ordered by frequency. The
percentage of sentences covered by the top-n
domain actions was then calculated. For this
experiment, we separated sentences spoken by
the traveller (client) and sentences spoken by
the travel agent (agent). C-star data in Ta-
ble 3 refers to 2564 English sentences from the
C-star database that were tagged with Ne-
spole interlingua. Nespole data refers to the
English portion of the Nespole database (1446
sentences). Combined data refers to the combi-
nation of the two (4014 sentences).
Two points are worth noting about Table 3.
First, the Nespole agent data has a higher cov-
erage rate than the Nespole client data. That
is, more data is covered by the top-n domain
actions. This may be because there was was
Domain Actions Top 5 Top 10 Top 20 Top 50
Client
C-star data 33.6 42.7 53.1 66.7
Nespole data 31.7 43.5 53.9 66.5
Combined data 31.6 40.0 50.3 62.9
Agent
C-star data 33.8 42.8 54.1 67.3
Nespole data 39.0 47.8 56.1 71.4
Combined data 33.6 41.5 51.7 64.0
Table 3: DA Coverage using Nespole interlingua on English data for both C-star and
Nespole
only a small amount of English agent data and
it was spoken by non-native speakers. Second,
the combined data has a slightly lower cover-
age rate than either the C-star or Nespole
databases alone. This is expected because, as
shown above, the combined domain is signi-
cantly more diverse than either domain by itself.
Scalability: Argument 2: Table 3 provides
additional evidence for the scalability of the Ne-
spole interlingua to larger domains. In the
combined C-star and Nespole domain, the
top 50 domain actions cover only slightly less
data than the top 50 domain actions in either
domain separately. There is not, in fact, an ex-
plosion of domain actions when the two C-star
and Nespole domains are combined.
Distributional Data: domain actions as a
function of database size: Table 3 shows
that in each of our databases, the 50 most fre-
quent domain actions cover approximately 65%
of the sentences. The next issue we address is
the nature of the \tail" of less frequent domain
actions covering the remainder of the data.
Figure 5 shows the number of domain actions
as a function of data set size. Sampling was done
for intervals of 25 sentences starting at 100 sen-
tences. For each sample size s there was ten-fold
cross-validation. Ten random samples of size s
were chosen, and the number of dierent domain
actions in each sample was counted. The aver-
age of the number of domain actions in each of
the ten samples of size s are plotted in Figure 5.
The four databases represented in Figure 5 are
IF Coverage of Four Datasets
0
100
200
300
400
500
600
700
10
0
70
0
13
00
19
00
25
00
31
00
number of SDUs in sample
av
er
ag
e 
nu
m
be
r o
f u
ni
qu
e 
DA
s 
o
ve
r 
10
 ra
nd
om
 s
am
pl
es
Old CSTAR
New CSTAR
NESPOLE
Combined
Figure 5: Number of domain actions as a function of
database size
the C-star English database tagged with C-
star II interlingua, the C-star II database
tagged with Nespole interlingua, the Nespole
English database, and the combined C-star
and Nespole English databases.
Expressivity, Argument 2: Figure 5 pro-
vides evidence for the increased expressivity of
the Nespole interlingua. In contrast to Ta-
ble 3, which deals with samples containing the
most frequent domain actions, the samples plot-
ted in Figure 5 contain random mixtures of fre-
quent and non-frequent domain actions. The
curve representing the C-star data with C-
star II interlingua is the slowest growing of the
four curves. This is because the grain-size of
meaning represented in the C-star II interlin-
gua was larger than in the Nespole interlin-
gua. Also many infrequent domain actions were
not covered by the C-star II interlingua. The
faster growth of the curve representing the C-
star data with Nespole interlingua indicates
improved expressivity of the Nespole interlin-
gua | it covers more of the infrequent domain
actions. The highest curve in Figure 5 repre-
sents the combined C-star and Nespole do-
mains. This curve is higher than the others be-
cause, as shown above, the two travel domains
are signicantly dierent from each other.
Expressivity and Simplicity, the right bal-
ance: Comparing Table 3 and Figure 5, we ar-
gue that the Nespole interlingua strikes a good
balance between expressivity and simplicity. Ta-
ble 3 shows evidence for the simplicity of the Ne-
spole interlingua: Only 50 domain actions are
needed to cover 60-70% of the sentences in the
database. Figure 5 shows evidence for expressiv-
ity: because domain actions are compositionally
formed from speech acts and concepts, it is pos-
sible to form a large number of low-frequency
domain actions in order to cover the domain.
Over 600 domain actions are used in the com-
bined C-star and Nespole domains.
5 Conclusions
We have presented a comparison of a purely
domain-action-based interlingua (the C-star II
interlingua) and a more expressive, but still
domain-action-based interlingua (the Nespole
interlingua). The data that we have presented
show that the more expressive interlingua has
better coverage of the domain (a decrease from
7.3% to 2.4% uncovered data in the C-star II
domain) and can also scale up to larger domains
without an explosion of domain actions. Thus
we have a reasonable compromise between sim-
plicity and expressiveness of the interlingua.
Acknowledgments
We would like to acknowledge Hans-Ulrich Block
for rst proposing the domain-action-based in-
terlingua to the C-star consortium. We would
also like to thank all of the C-star and Ne-
spole partners who have participated in the de-
sign of the interlingua. This work was supported
by NSF Grant 9982227 and EU Grant IST 1999-
11562 as part of the joint EU/NSF MLIAM re-
search initiative.
References
Bonnie J. Dorr. 1993. Machine Translation: A View
from the Lexicon. The MIT Press, Cambridge,
Massachusetts.
Bonnie J. Dorr. 1994. Machine Translation Diver-
gences: A Formal Description and Proposed Solu-
tion. Computational Linguistics, 20(4):597{633.
Kenneth Goodman and Sergei Nirenburg. 1991.
The KBMT Project: A Case Study in Knowledge-
Based Machine Translation. Morgan Kaufmann,
San Mateo, CA.
Alon Lavie, Florian Metze, Roldano Cattoni, and Er-
ica Constantini. 2002. A Multi-Perspective Eval-
uation of the NESPOLE! Speech-to-Speech Trans-
lation System. In Proceedings of Speech-to-Speech
Translation: Algorithms and Systems.
Young-Suk Lee, W. Yi, Cliord Weinstein, and
Stephanie Sene. 2001. Interlingua-based broad-
coverage korean-to-english translation. In Pro-
ceedings of HLT, San Diego.
Lori Levin, Donna Gates, Alon Lavie, and Alex
Waibel. 1998. An Interlingua Based on Domain
Actions for Machine Translation of Task-Oriented
Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (IC-
SLP?98), pages Vol. 4, 1155{1158, Sydney, Aus-
tralia.
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi,
Dorcas Wallace, Taro Watanabe, and Monika
Woszczyna. 2000a. Evaluation of a Practical In-
terlingua for Task-Oriented Dialogue. In Work-
shop on Applied Interlinguas: Practical Applica-
tions of Interlingual Approaches to NLP, Seattle.
Lori Levin, Alon Lavie, Monika Woszczyna, Donna
Gates, Marsal Gavalda, Detlef Koll, and Alex
Waibel. 2000b. The Janus-III Translation Sys-
tem. Machine Translation.
Domain Specific Speech Acts for Spoken Language Translation 
Lori Levin, Chad Langley, Alon Lavie,  
Donna Gates, Dorcas Wallace and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, United States 
{lsl,clangley,alavie,dmg,dorcas,kay+}@cs.cmu.edu 
Abstract 
We describe a coding scheme for ma-
chine translation of spoken task-
oriented dialogue. The coding scheme 
covers two levels of speaker intention ? 
domain independent speech acts and 
domain dependent domain actions. Our 
database contains over 14,000 tagged 
sentences in English, Italian, and Ger-
man. We argue that domain actions, and 
not speech acts, are the relevant dis-
course unit for improving translation 
quality. We also show that, although 
domain actions are domain specific, the 
approach scales up to large domains 
without an explosion of domain actions 
and can be coded with high inter-coder 
reliability across research sites. Fur-
thermore, although the number of do-
main actions is on the order of ten times 
the number of speech acts, sparseness is 
not a problem for the training of classi-
fiers for identifying the domain action. 
We describe our work on developing 
high accuracy speech act and domain 
action classifiers, which is the core of 
the source language analysis module of 
our NESPOLE machine translation sys-
tem. 
1 Introduction 
The NESPOLE and C-STAR machine translation 
projects use an interlingua representation based 
on speaker intention rather than literal meaning. 
The speaker's intention is represented as a 
domain independent speech act followed by do-
main dependent concepts. We use the term 
domain action to refer to the combination of a 
speech act with domain specific concepts. Exam-
ples of domain actions and speech acts are shown 
in Figure 1. 
 
c:give-information+party  
?I will be traveling with my husband and 
our two children ages two and eleven? 
 
c:request-information+existence+facility  
?Do they have parking available?" 
?Is there someplace to go ice skating?" 
 
c:give-information+view+information-
object  
?I see the bus icon?  
 
Figure 1: Examples of Speech Acts and Domain 
Actions. 
 
Domain actions are constructed compositionally 
from an inventory of speech acts and an inven-
tory of concepts. The allowable combinations of 
speech acts and concepts are formalized in a hu-
man- and machine-readable specification docu-
ment. The specification document is supported 
by a database of over 14,000 tagged sentences in 
English, German, and Italian. 
The discourse community has long recog-
nized the potential for improving NLP systems 
by identifying speaker intention. It has been hy-
pothesized that predicting speaker intention of 
the next utterance would improve speech recog-
nition (Reithinger et al, Stolcke et al), or reduce 
ambiguity for machine translation (Qu et al, 
1996, Qu et al, 1997). Identifying speaker inten-
tion is also critical for sentence generation. 
We argue in this paper that the explicit repre-
sentation of speaker intention using domain ac-
tions can serve as the basis for an effective 
language-independent representation of meaning 
for speech-to-speech translation and that the 
relevant units of speaker intention are the domain 
specific domain action as well as the domain in-
dependent speech act. After a brief description of 
our database, we present linguistic motivation for 
domain actions. We go on to show that although 
domain actions are domain specific, there is not 
an explosion or exponential growth of domain 
actions when we scale up to a larger domain or 
port to a new domain. Finally we will show that, 
although the number of domain actions is on the 
order of ten times the number of speech acts, 
data sparseness is not a problem in training a 
domain action classifier. We present extensive 
work on developing a high-accuracy classifier 
for domain actions using a variety of classifica-
tion approaches and conclusions on the adequacy 
of these approaches to the task of domain action 
classification.  
2 Data Collection Scenario and Data-
base 
Our study is based on data that was collected for 
the NESPOLE and C-STAR speech-to-speech 
translation projects. Three domains are included. 
The NESPOLE travel domain covers inquiries 
about vacation packages. The C-STAR travel 
domain consists largely of reservation and pay-
ment dialogues and overlaps only about 50% in 
vocabulary with the NESPOLE travel domain. 
The medical assistance domain includes dia-
logues about chest pain and flu-like symptoms. 
There were two data collection protocols for 
the NESPOLE travel domain ? monolingual and 
bilingual. In the monolingual protocol, an Eng-
lish speaker in the United States had a conversa-
tion with an Italian travel agent speaking (non-
native) English in Italy. Monolingual data was 
also collected for German, French and Italian. 
Bilingual data was collected during user studies 
with, for example, an English speaker in the 
United States talking to an Italian-speaking travel 
agent in Italy, with the NESPOLE system pro-
viding the translation between the two parties. 
The C-STAR data consists of only monolingual 
role-playing dialogues with both speakers at the 
same site. The medical dialogues are monolin-
gual with doctors playing the parts of both doctor 
and patient. 
The dialogues were transcribed and multi-
sentence utterances were broken down into mul-
tiple Semantic Dialogue Units (SDUs) that each 
correspond to one domain action. Some SDUs 
have been translated into other NESPOLE or C-
STAR languages. Over 14,000 SDUs have been 
tagged with interlingua representations including 
domain actions as well as argument-value pairs. 
Table 1 summarizes the number of tagged SDUs 
in complete dialogues in the interlingua database. 
There are some additional tagged dialogue frag-
ments that are not counted. Figure 2 shows an 
excerpt from the database. 
 
 
 
English NESPOLE Travel 4691 
English C-STAR Travel 2025 
German NESPOLE Travel 1538 
Italian NESPOLE Travel 2248 
English Medical Assistance 2001 
German Medical Assistance 1152 
Italian Medical Assistance 935 
Table 1: Tagged SDUs in the Interlingua Data-
base. 
 
e709wa.19.0  comments: DATA from 
e709_1_0018_ITAGOR_00 
 
e709wa.19.1  olang ITA  lang ITA Prv CMU   
?hai in mente una localita specifica?" 
e709wa.19.1  olang ITA  lang GER  Prv CMU   
?haben Sie einen bestimmten Ort im Sinn?" 
e709wa.19.1  olang ITA  lang FRE  Prv 
CLIPS ?" 
e709wa.19.1  olang ITA  lang ENG  Prv CMU   
?do you have a specific place in mind" 
e709wa.19.1                   IF  Prv CMU   
a:request-information+disposition+object  
(object-spec=(place, modifier=specific, 
identifiability=no), disposi-
tion=(intention, who=you)) 
e709wa.19.1  comments: Tagged by dmg 
 
Figure 2: Excerpt from the Interlingua Database. 
3 Linguistic Argument for Domain Ac-
tions 
Proponents of Construction Grammar (Fillmore 
et. al. 1988, Goldberg 1995) have argued that 
human languages consist of constructional units 
that include a syntactic structure along with its 
associated semantics and pragmatics. Some con-
structions follow the typical syntactic rules of the 
language but have a semantic or pragmatic focus 
that is not compositionally predictable from the 
parts. Other constructions do not even follow the 
typical syntax of the language (e.g., Why not go? 
with no tensed verb). 
Our work with multilingual machine transla-
tion of spoken language shows that fixed expres-
sions cannot be translated literally. For example, 
Why not go to the meeting? can be translated 
into Japanese as Kaigi ni itte mitara doo? (meet-
ing to going see/try-if how), which differs from 
the English in several ways. It does not have a 
word corresponding to not; it has a word that 
means see/try that does not appear in the English 
sentence; and so on. In order to produce an ac-
ceptable translation, we must find a common 
ground between the English fixed expression 
Why not V-inf? and the Japanese fixed expression 
-te mittara doo?. The common ground is the 
speaker's intention (in this case, to make a sug-
gestion) rather than the syntax or literal meaning. 
Speaker intention is partially captured with a 
direct or indirect speech act. However, whereas 
speech acts are generally domain independent, 
task-oriented language abounds with fixed ex-
pressions that have domain specific functions. 
For example, the phrases We have? or There 
are? in the hotel reservation domain express 
availability of rooms in addition to their more 
literal meanings of possession and existence. In 
the past six years, we have been successful in 
using domain specific domain actions as the ba-
sis for translation of limited-domain task-
oriented spoken language (Levin et al, 1998, 
Levin et al 2002; Langley and Lavie, 2003) 
4 Scalability and Portability of Domain 
Actions 
Domain actions, like speech acts, convey speaker 
intention. However, domain actions also repre-
sent components of meaning and are therefore 
more numerous than domain independent speech 
acts. 1168 unique domain actions are used in our 
NESPOLE database, in contrast to only 72 
speech acts. We show in this section that domain 
actions yield good coverage of task-oriented do-
mains, that domain actions can be coded effec-
tively by humans, and that scaling up to larger 
domains or porting to new domains is feasible 
without an explosion of domain actions.  
 
Coverage of Task-Oriented Domains: Our 
NESPOLE domain action database contains dia-
logues from two task-oriented domains: medical 
assistance and travel. Table 2 shows the number 
of speech acts and concepts that are used in the 
travel and medical domains.  The 1168 unique 
domain actions that appear in our database are 
composed of the 72 speech acts and 125 con-
cepts. 
 
 Travel Medical Combined 
DAs 880 459 1168 
SAs 67 44 72 
Concepts 91 74 125 
Table 2: DA component counts in NESPOLE 
data. 
 
Our domain action based interlingua has quite 
high coverage of the travel and medical dia-
logues we have collected. To measure how well 
the interlingua covers a domain, we define the 
no-tag rate as the percent of sentences that are 
not covered by the interlingua, according to a 
human expert. The no-tag rate for the English 
NESPOLE travel dialogues is 4.3% for dialogues 
that have been used for system development.  
We have also estimated the domain action no-
tag rate for unseen data using the NESPOLE 
travel database (English, German, and Italian 
combined). We randomly selected 100 SDUs as 
seen data and extracted their domain actions. We 
then randomly selected 100 additional SDUs 
from the remaining data and estimated the no-tag 
rate by counting the number of SDUs not cov-
ered by the domain actions in the seen data. We 
then added the unseen data to the seen data set 
and randomly selected 100 new SDUs. We re-
peated this process until the entire database had 
been seen, and we repeated the entire sampling 
process 10 times. Although the number of do-
main actions increases steadily with the database 
size (Figure 4), the no-tag rate for unseen data 
stabilizes at less than 10%.  
We also randomly selected half of the SDUs 
(4200) from the database as seen data and ex-
tracted the domain actions. Holding the seen data 
set fixed, we then estimated the no-tag rates in 
increasing amounts of unseen data from the re-
maining half of the database. We repeated this 
process 10 times. With a fixed amount of seen 
data, the no-tag rate remains stable for increasing 
amounts of unseen data. We observed similar no-
tag rate results for the medical assistance domain 
and for the combination of travel and medical 
domains. 
It is also important to note that although there 
is a large set of uncommon domain actions, the 
top 105 domain actions cover 80% of the sen-
tences in the travel domain database. Thus do-
main actions are practical for covering task-
oriented domains. 
 
Intercoder Agreement: Intercoder agreement is 
another indicator of manageability of the domain 
action based interlingua. We calculate intercoder 
agreement as percent agreement. Three interlin-
gua experts at one NESPOLE site achieved 94% 
agreement (average pairwise agreement) on 
speech acts and 88% agreement on domain ac-
tions. Across sites, expert agreement on speech 
acts is still quite high (89%), although agreement 
on domain actions is lower (62%). Since many 
domain actions are similar in meaning, some dis-
agreement can be tolerated without affecting 
translation quality. 
 
Figure 3: DAs to cover data (English). 
Figure 4: DAs to cover data (All languages). 
 
Scalability and Portability: The graphs in Figure 
3 and Figure 4 illustrate growth in the number of 
domain actions as the database size increases and 
as new domains are added. The x-axis represents 
the sample size randomly selected from the data-
base. The y-axis shows the number of unique 
domain actions (types) averaged over 10 samples 
of each size. Figure 3 shows the growth in do-
main actions for three English databases 
(NESPOLE travel, C-STAR travel, and medical 
assistance) as well as the growth in domain ac-
tions for a database consisting of equal amounts 
of data from each domain. Figure 4 shows the 
growth in domain actions for combined English, 
German, and Italian data in the NESPOLE travel 
and medical domains.  
Figure 3 and Figure 4 show that the number 
of domain actions increases steadily as the data-
base grows. However, closer examination reveals 
that scalability to larger domains and portability 
to new domains are in fact feasible.  The curves 
representing combined domains (travel plus 
medical in Figure 4 and NESPOLE travel, C-
STAR travel, and medical in Figure 3) show only 
a small increase in the number of domain actions 
when two domains are combined. In fact, there is 
a large overlap between domains.  In Table 3 the 
Overlap columns show the number of DA types 
and tokens that are shared between the travel and 
medical domains. We can see around 70% of DA 
tokens are covered by DA types that occur in 
both domains. 
 
 
DA 
Types 
Type 
Overlap 
DA 
Tokens 
Token 
Overlap 
NESPOLE 
Travel 880 171 8477 
6004 
(70.8%) 
NESPOLE 
Medical 459 171 4088 
2743 
(67.1%) 
Table 3: DA Overlap (All languages). 
5 A Hybrid Analysis Approach for Pars-
ing Domain Actions 
Langley et al (2002; Langley and Lavie, 2003) 
describe the hybrid analysis approach that is used 
in the NESPOLE! system (Lavie et al, 2002). 
The hybrid analysis approach combines gram-
mar-based phrasal parsing and machine learning 
techniques to transform utterances into our inter-
lingua representation. Our analyzer operates in 
three stages to identify the domain action and 
arguments. 
First, an input utterance is parsed into a se-
quence of arguments using phrase-level semantic 
grammars and the SOUP parser (Gavald?, 2000). 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument gram-
mar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains 
phrase-level rules for parsing arguments defined 
in the interlingua. The pseudo-argument gram-
mar contains rules for parsing common phrases 
that are not covered by interlingua arguments. 
For example, all booked up, full, and sold out 
might be grouped into a class of phrases that in-
dicate unavailability. The cross-domain grammar 
contains rules for parsing complete DAs that are 
domain independent. For example, this grammar 
contains rules for greetings (Hello, Good bye, 
Nice to meet you, etc.). Finally, the shared 
grammar contains low-level rules that can be 
used by all other subgrammars. 
After argument parsing, the utterance is seg-
mented into SDUs using memory-based learning 
(k-nearest neighbor) techniques. Spoken utter-
ances often consist of several SDUs. Since DAs 
are assigned at the SDU level, it is necessary to 
segment utterances before assigning DAs. 
0
100
200
300
400
500
600
700
800
900
0 1000 2000 3000 4000 5000 6000 7000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical C-STAR
Nespole Travel+Medical C-STAR + Nespole Travel+Medical
0
100
200
300
400
500
600
700
800
900
1000
0 1000 2000 3000 4000 5000 6000 7000 8000 9000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical Nespole Travel+Medical
The final stage in the hybrid analysis ap-
proach is domain action classification.  
6 Domain Action Classification 
Identifying the domain action is a critical step in 
the analysis process for our interlingua-based 
translation systems. One possible approach 
would be to manually develop grammars de-
signed to parse input utterances all the way to the 
domain action level. However, while grammar-
based parsing may provide very accurate analy-
ses, it is generally not feasible to develop a 
grammar that completely covers a domain. This 
problem is exacerbated with spoken input, where 
disfluencies and deviations from the grammar are 
very common. Furthermore, a great deal of effort 
by human experts is generally required to de-
velop a wide-coverage grammar. 
An alternative to writing full domain action 
grammars is to train classifiers to identify the 
DA. Machine learning approaches allow the ana-
lyzer to generalize beyond training data and tend 
to degrade gracefully in the face of noisy input. 
Machine learning methods may, however, be less 
accurate than grammars, especially on common 
in-domain input, and may require a large amount 
of training data in order to achieve adequate lev-
els of performance. In the hybrid analyzer de-
scribed above, classifiers are used to identify the 
DA for domain specific portions of utterances 
that are not covered by the cross-domain gram-
mar. 
We tested classifiers trained to classify com-
plete DAs. We also split the DA classification 
task into two subtasks: speech act classification 
and concept sequence classification. This simpli-
fies the task of each classifier, allows for the use 
of different approaches and/or feature sets for 
each task, and reduces data sparseness. Our hy-
brid analyzer uses the output of each classifier 
along with the interlingua specification to iden-
tify the DA (Langley et al, 2002; Langley and 
Lavie, 2003). 
7 Experimental Setup 
We conducted experiments to assess the per-
formance of several machine-learning ap-
proaches on the DA classification tasks. We 
evaluated all of the classifiers on English and 
German input in the NESPOLE travel domain.  
7.1 Corpus 
The corpus used in all of the experiments was the 
NESPOLE! travel and tourism database. Since 
our goal was to evaluate the SA and concept se-
quence classifiers and not segmentation, we cre-
ated training examples for each SDU in the 
database rather than for each utterance. Table 4 
contains statistics regarding the contents of the 
corpus for our classification tasks. Table 5 shows 
the frequency of the most common domain ac-
tion, speech act, and concept sequence in the 
corpus. These frequencies provide a baseline that 
would be achieved by a simple classifier that al-
ways returned the most common class. 
 
 English German 
SDUs 8289 8719 
Domain Actions 972 1001 
Speech Acts 70 70 
Concept Sequences 615 638 
Vocabulary Size 1946 2815 
Table 4: Corpus Statistics. 
 
 English German 
DA (acknowledge) 19.2% 19.7% 
SA (give-information) 41.4% 40.7% 
Concept Sequence 
(No concepts) 
38.9% 40.3% 
Table 5: Most frequent DAs, SAs, and CSs. 
 
All of the results presented in this paper were 
produced using a 20-fold cross validation setup. 
The corpus was randomly divided into 20 sets of 
equal size. Each of the sets was held out as the 
test set for one fold with the remaining 19 sets 
used as training data. Within each language, the 
same random split was used for all of the classi-
fication experiments. Because the same split of 
the data was used for different classifiers, the 
results of two classifiers on the same test set are 
directly comparable. Thus, we tested for signifi-
cance using two-tailed matched pair t-tests. 
7.2 Machine Learning Approaches 
We evaluated the performance of four different 
machine-learning approaches on the DA classifi-
cation tasks: memory-based learning (k-Nearest-
Neighbor), decision trees, neural networks, and 
na?ve Bayes n-gram classifiers. We selected 
these approaches because they vary substantially 
in the their representations of the training data 
and their methods for selecting the best class. 
Our purpose was not to implement each ap-
proach from scratch but to test the approach for 
our particular task. Thus, we chose to use exist-
ing software for each approach ?off the shelf.? 
The ease of acquiring and setting up the software 
influenced our choice. Furthermore, the ease of 
incorporating the software into our online trans-
lation system was also a factor. 
Our memory-based classifiers were imple-
mented using TiMBL (Daelemans et al, 2002). 
We used C4.5 (Quinlan, 1993) for our decision 
tree classifiers. Our neural network classifiers 
were implemented using SNNS (Zell et al, 
1998). We used Rainbow (McCallum, 1996) for 
our na?ve Bayes n-gram classifiers. 
8 Experiments 
In our first experiment, we compared the per-
formance of the four machine learning ap-
proaches. Each SDU was parsed using the 
argument and pseudo-argument grammars de-
scribed above. The feature set for the DA and SA 
classifiers consisted of binary features indicating 
the presence or absence of labels from the 
grammars in the parse forest for the SDU. The 
feature set included 212 features for English and 
259 features for German. The concept sequence 
classifiers used the same feature set with the ad-
dition of the speech act. 
In the SA classification experiment, the 
TiMBL classifier used the IB1 (k-NN) algorithm 
with 1 neighbor and gain ratio feature weighting. 
The C4.5 classifier required at least one instance 
per branch and used node post-pruning. Both the 
TiMBL and C4.5 classifiers used the binary fea-
tures described above and produced the single 
best class as output. The SNNS classifier used a 
simple feed-forward network with 1 input unit 
for each binary feature, 1 hidden layer containing 
15 units, and 1 output unit for each speech act. 
The network was trained using backpropagation. 
The order of presentation of the training exam-
ples was randomized in each epoch, and the 
weights were updated after each training exam-
ple presentation. In order to simulate the binary 
features used by the other classifiers as closely as 
possible, the Rainbow classifier used a simple 
unigram model whose vocabulary was the set of 
labels included in the binary feature set. The 
setup for the DA classification experiment was 
identical except that the neural network had 50 
hidden units. 
The setup of the classifiers for the concept se-
quence classification experiment was very simi-
lar. The TiMBL and C4.5 classifiers were set up 
exactly as in the DA and SA experiments with 
one extra feature whose value was the speech act. 
The SNNS concept sequence classifier used a 
similar network with 50 hidden units. The SA 
feature was represented as a set of binary input 
units. The Rainbow classifier was set up exactly 
as in the DA and SA experiments. The SA fea-
ture was not included. 
As mentioned above, both experiments used a 
20-fold cross-validation setup. In each fold, the 
TiMBL, C4.5, and Rainbow classifiers were sim-
ply trained on 19 subsets of the data and tested 
on the remaining set. The SNNS classifiers re-
quired a more complex setup to determine the 
number of epochs to train the neural network for 
each test set. Within each fold, a cross-validation 
setup was used to determine the number of train-
ing epochs. Each of the 19 training subsets for a 
fold was used as a validation set. The network 
was trained on the remaining 18 subsets until the 
accuracy on the validation set did not improve 
for 50 consecutive epochs. The network was then 
trained on all 19 training subsets for the average 
number of epochs from the validation sets. This 
process was used for all 20-folds in the SA clas-
sification experiment. For the DA and concept 
sequence experiments, this process ran for ap-
proximately 1.5 days for each fold. Thus, this 
process was run for the first two folds, and the 
average number of epochs from those folds was 
used for training. 
 
 English German 
TiMBL 49.69% 46.51% 
C4.5 48.90% 46.58% 
SNNS 49.39% 46.21% 
Rainbow 39.74% 38.32% 
Table 6: Domain Action classifier accuracy. 
 
 English German 
TiMBL 69.82% 67.57% 
C4.5 70.41% 67.90% 
SNNS 71.52% 67.61% 
Rainbow 51.39% 46.00% 
Table 7: Speech Act classifier accuracy. 
 
 English German 
TiMBL 69.59% 67.08% 
C4.5 68.47% 66.45% 
SNNS 71.35% 68.67% 
Rainbow 51.64% 51.50% 
Table 8: Concept Sequence classifier accuracy. 
 Table 6, Table 7, and Table 8 show the aver-
age accuracy of each learning approach on the 
20-fold cross validation experiments for domain 
action, speech act, and concept classification re-
spectively. For DA classification, there were no 
significant differences between the TiMBL, 
C4.5, and SNNS classifiers for English or Ger-
man. In the SA experiment, the difference be-
tween the TiMBL and C4.5 classifiers for 
English was not significant. The SNNS classifier 
was significantly better than both TiMBL and 
C4.5 (at least p=0.0001). For German SA classi-
fication, there were no significant differences 
between the TiMBL, C4.5, and SNNS classifiers. 
For concept sequence classification, SNNS was 
significantly better than TiMBL and C4.5 (at 
least p=0.0001) for both English and German. 
For English only, TiMBL was significantly better 
than C4.5 (p=0.005). 
For both languages, the Rainbow classifier 
performed much worse than the other classifiers. 
However, the unigram model over arguments did 
not exploit the strengths of the n-gram classifica-
tion approach. Thus, we ran another experiment 
in which the Rainbow classifier was trained on 
simple word bigrams. No stemming or stop 
words were used in building the bigram models. 
 
 English German 
Domain Action 48.59% 48.09% 
Speech Act 79.00% 77.46% 
Concept Sequence 56.87% 57.77% 
Table 9: Rainbow accuracy with word bigrams. 
 
Table 9 shows the average accuracy of the 
Rainbow word bigram classifiers using the same 
20-fold cross-validation setup as in the previous 
experiments. As we expected, using word bi-
grams rather than parse label unigrams improved 
the performance of the Rainbow classifiers. For 
German DA classification, the word bigram clas-
sifier was significantly better than all of the pre-
vious German DA classifiers (at least p=0.005). 
Furthermore, the Rainbow word bigram SA clas-
sifiers for both languages outperformed all of the 
SA classifiers that used only the parse labels. 
Although the argument parse labels provide 
an abstraction of the words present in an SDU, 
the words themselves also clearly provided use-
ful information for classification, at least for the 
SA task. Thus, we conducted additional experi-
ments to examine whether combining parse and 
word information could further improve per-
formance. 
We chose to incorporate word information 
into the TiMBL classifiers used in the first ex-
periment. Although the SNNS SA classifier per-
formed significantly better than the TiMBL SA 
classifier for English, there was no significant 
difference for SA classification in German. Fur-
thermore, because of the complexity and time 
required for training with SNNS, we preferred 
working with TiMBL. 
We tested two approaches to adding word in-
formation to the TiMBL classifier. In both ap-
proaches, the word-based information for each 
fold was computed only based on the data in the 
training set. In our first approach, we added bi-
nary features for the 250 words that had the 
highest mutual information with the class. Each 
feature indicated the presence or absence of the 
word in the SDU. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 3 neighbors, and unweighted voting. The 
second approach we tested combined the Rain-
bow word bigram classifier with the TiMBL 
classifier. We added one input feature for each 
possible speech act to the TiMBL classifier. The 
value of each SA feature was the probability of 
the speech act computed by the Rainbow word 
bigram classifier. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 11 neighbors, and inverse linear distance 
weighted voting. 
 
 English German 
TiMBL + words 78.59% 75.98% 
TiMBL + Rainbow 81.25% 78.93% 
Table 10: Word+Parse SA classifier accuracy. 
 
Table 10 shows the average accuracy of the 
SA classifiers that combined parse and word in-
formation using the same 20-fold cross-
validation setup as the previous experiments. 
Although adding binary features for individual 
words improved performance over the classifiers 
with no word information, it did not allow the 
combined classifiers to outperform the Rainbow 
word bigram classifiers. However, for both 
languages, adding the probabilities computed by 
the Rainbow bigram model resulted in a SA clas-
sifier that outperformed all previous classifiers. 
The improvement in accuracy was highly signifi-
cant for both languages. 
We conducted a similar experiment for com-
bining parse and word information in the concept 
sequence classifiers. The first condition was 
analogous to the first condition in the combined 
SA classification experiment. The second condi-
tion was slightly different. A concept sequence 
can be broken down into a set of individual con-
cepts. The set of individual concepts is much 
smaller than the set of concept sequences (110 
for English and 111 for German). Thus, we used 
a Rainbow word bigram classifier to compute the 
probability of each individual concept rather than 
the complete concept sequence. The probabilities 
for the individual concepts were added to the 
parse label features for the combined classifier. 
In both conditions, the performance of the com-
bined classifiers was roughly the same as the 
classifiers that used only parse labels as features. 
 
 English German 
TiMBL + words 56.48% 54.98% 
Table 11: Word+Parse DA classifier accuracy. 
 
Table 11 shows the average accuracy of DA 
classifiers for English and German using a setup 
similar to the first approach in the combined SA 
experiment. In this experiment, we added binary 
features for the 250 words that the highest mu-
tual information with the class. We used a 
TiMBL classifier with gain ratio feature weight-
ing and one neighbor. The improvement in accu-
racy for both languages was highly significant. 
 
 English German 
TiMBL SA 
+ TiMBL CS 49.63% 46.50% 
TiMBL+Rainbow SA 
+ TiMBL CS 57.74% 53.93% 
Table 12: DA accuracy of SA+CS classifiers. 
 
Finally, Table 12 shows the results from two 
tests to compare the performance of combining 
the best output of the SA and concept sequence 
classifiers with the performance of the complete 
DA classifiers. In the first test, we combined the 
output from the TiMBL SA and CS classifiers 
shown in Table 7 and Table 8. The performance 
of the combined SA+CS classifiers was almost 
identical to that of the TiMBL DA classifiers 
shown in Table 6. In the second test, we com-
bined our best SA classifier (TiMBL+Rainbow, 
shown in Table 10) with the TiMBL CS classi-
fier. In this case, we had mixed results. The per-
formance of the combined classifiers was better 
than our best DA classifier for English and worse 
for German. 
9 Discussion 
One of our main goals was to determine the fea-
sibility of automatically classifying domain ac-
tions. As the data in Table 4 show, DA 
classification is a challenging problem with ap-
proximately 1000 classes. Even when the task is 
divided into subproblems of identifying the SA 
and concept sequence, the subtasks remain diffi-
cult. The difficulty is compounded by relatively 
sparse training data with unevenly distributed 
classes. Although the most common classes in 
our training corpus had over 1000 training exam-
ples, many of the classes had only 1 or 2 exam-
ples. 
Despite these difficulties, our results indicate 
that domain action classification is feasible. For 
SA classification in particular we were able to 
achieve very strong performance. Although per-
formance on concept sequence and DA classifi-
cation is not as high, it is still quite strong, 
especially given that there are an order of magni-
tude more classes than in SA classification. 
Based on our experiments, it appears that all of 
the learning approaches we tested were able to 
cope with data sparseness at the level found in 
our data, with the possible exception of the na?ve 
Bayes n-gram approach (Rainbow) for the con-
cept sequence task. 
One additional point worth noting is that there 
is evidence that domain action classification 
could be performed reasonably well using only 
word-based information. Although our best-
performing classifiers combined word and argu-
ment parse information, the na?ve Bayes word 
bigram classifier (Rainbow) performed very well 
on the SA classification task. With additional 
data, the performance of the concept sequence 
and DA word bigram classifiers could be ex-
pected to improve. Cattoni et al (2001) also ap-
ply statistical language models to DA 
classification. A word bigram model is trained 
for each DA, and the DA with the highest likeli-
hood is assigned to each SDU. Arguments are 
identified using recursive transition networks, 
and interlingua specification constraints are used 
to find the most likely valid interlingua represen-
tation. Although it is clear that argument infor-
mation is useful for the task, it appears that 
words alone can be used to achieve reasonable 
performance. 
Another goal of our experiments was to help 
in the selection of a machine learning approach 
to be used in our hybrid analyzer. Certainly one 
of the most important considerations is how well 
the learning approach performs the task. For SA 
classification, the combination of parse features 
and word bigram probabilities clearly gave the 
best performance. For concept sequence classifi-
cation, no learning approach clearly outper-
formed any other (with the exception that the 
na?ve Bayes n-gram approach performed worse 
than other approaches). However, the perform-
ance of the classifiers is not the only considera-
tion to be made in selecting the classifier for our 
hybrid analyzer. 
Several additional factors are also important 
in selecting the particular machine learning ap-
proach to be used. One important attribute of the 
learning approach is the speed of both classifica-
tion and training. Since the classifiers are part of 
a translation system designed for use between 
two humans to facilitate (near) real-time com-
munication, the DA classifiers must classify in-
dividual utterances online very quickly. 
Furthermore, since humans must write and test 
the argument grammars, training and batch 
classification should be fast so that the grammar 
writers can update the grammars, retrain the clas-
sifiers, and test efficiently. 
The machine learning approach should also 
be able to easily accommodate both continuous 
and discrete features from a variety of sources. 
Possible sources for features include words 
and/or phrases in an utterance, the argument 
parse, the interlingua representation of the argu-
ments, and properties of the dialogue (e.g. 
speaker tag). The classifier should be able to eas-
ily combine features from any or all of these 
sources. 
Another desirable attribute for the machine 
learning approach is the ability to produce a 
ranked list of possible classes. Our interlingua 
specification defines how speech acts and con-
cepts are allowed to combine as well as how ar-
guments are licensed by the domain action. 
These constraints can be used to select an alter-
native DA if the best DA violates the specifica-
tion. 
Based on all of these considerations, the 
TiMBL+Rainbow classifier, which combines 
parse label features with word bigram probabili-
ties, seems like an excellent choice for speech act 
classification. It was the most accurate classifier 
that we tested. Furthermore, the main TiMBL 
classifier meets all of the requirements discussed 
above except the ability to produce a complete 
ranked list of the classes for each instance. How-
ever, such a list could be produced as a backup 
from the Rainbow probability features. Adding 
new features to the combined classifier would 
also be very easy because TiMBL was the pri-
mary classifier in the combination. Finally, since 
both TiMBL and Rainbow provide an online 
server mode for classifying single instances, in-
corporating the combined classifier into an 
online translation system would not be difficult. 
Since there were no significant differences in the 
performance of most of the concept sequence 
classifiers, this combined approach is probably 
also a good option for that task. 
10 Conclusion 
We have described a representation of 
speaker intention that includes domain independ-
ent speech acts as well as domain dependent do-
main actions. We have shown that domain 
actions are a useful level of abstraction for ma-
chine translation of task-oriented dialogue, and 
that, in spite of their domain specificity, they are 
scalable to larger domains and portable to new 
domains.  
We have also presented classifiers for domain 
actions that have been comparatively tested and 
used successfully in the NESPOLE speech-to-
speech translation system. We experimentally 
compared the effectiveness of several machine-
learning approaches for classification of domain 
actions, speech acts, and concept sequences on 
two input languages. Despite the difficulty of the 
classification tasks due to a large number of 
classes and relatively sparse data, the classifiers 
exhibited strong performance on all tasks. We 
also demonstrated how the combination of two 
learning approaches could be used to improve 
performance and overcome the weaknesses of 
the individual approaches. 
Acknowledgements: NESPOLE was funded 
by NSF (Grant number 9982227) and the EU. 
The NESPOLE partners are ITC-irst, Universite 
Joseph Fourrier, Universitat Karlsruhe, APT 
Trentino travel board, and AETHRA telecom-
munications. We would like to acknowledge the 
contribution of the following people in particu-
lar: Fabio Pianesi, Emanuele Pianta, Nadia 
Mana, and Herve Blanchon. 
References 
Cattoni, R., M. Federico, and A. Lavie. 2001. Robust 
Analysis of Spoken Input Combining Statistical 
and Knowledge-Based Information Sources. In 
Proceedings of the IEEE ASRU Workshop, Trento, 
Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2002. TiMBL: Tilburg Memory 
Based Learner, version 4.3, Reference Guide. ILK 
Technical Report 02-10. Available from 
http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.
gz. 
Fillmore, C.J., Kay, P. and O'Connor, M.C. 1988. 
Regularity and Idiomaticity in Grammatical Con-
structions. Language, 64(3), 501-538. 
Gavald?, M. 2000. SOUP: A Parser for Real-World 
Spontaneous Speech. In Proceedings of IWPT-
2000, Trento, Italy. 
Goldberg, Adele E. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. 
Chicago University Press. 
Langley, C. and A. Lavie. 2003. Parsing Domain Ac-
tions with Phrase-Level Grammars and Memory-
Based Learners. To appear in Proceedings of 
IWPT-2003. Nancy, France. 
Langley, C., A. Lavie, L. Levin, D. Wallace, D. 
Gates, and K. Peterson. 2002. Spoken Language 
Parsing Using Phrase-Level Grammars and Train-
able Classifiers. In Workshop on Algorithms for 
Speech-to-Speech Machine Translation at ACL-02. 
Philadelphia, PA. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. Enhancing 
the Usability and Performance of NESPOLE! ? a 
Real-World Speech-to-Speech Translation System. 
In Proceedings of HLT-2002. San Diego, CA. 
Levin, L., D. Gates, A. Lavie, A. Waibel. 1998. An 
Interlingua Based on Domain Actions for Machine 
Translation of Task-Oriented Dialogues. In Pro-
ceedings of ICSLP 98, Vol. 4, pages 1155-1158, 
Sydney, Australia. 
Levin, L., D. Gates, D. Wallace, K. Peterson, A. La-
vie F. Pianesi, E. Pianta, R. Cattoni, N. Mana. 
2002. Balancing Expressiveness and Simplicity in 
an Interlingua for Task Based Dialogue. In Pro-
ceedings of Workshop on Spoken Language Trans-
lation. ACL-02, Philadelphia. 
McCallum, A. K. 1996. Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. 
http://www.cs.cmu.edu/~mccallum/bow. 
Qu, Y., B. DiEugenio, A. Lavie, L. Levin and C.P. 
Rose. 1997. Minimizing Cumulative Error in Dis-
course Context. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-96 
Workshop, E. Maier, M. Mast and S. LuperFoy 
(eds.), LNCS series, Springer Verlag. 
Qu, Y., C. P. Rose, and B. DiEugenio. 1996. Using 
Discourse Predictions for Ambiguity Resolution. In 
Proceedings of COLING-1996. 
Quinlan, J. R. 1993. C4.5: Programs for Machine 
Learning. San Mateo: Morgan Kaufmann. 
Reithinger, N., R. Engel, M. Kipp, M. Klesen. 1996. 
Predicting Dialogue Acts for a Speech-To-Speech 
Translation System. DFKI GmbH Saarbruecken. 
Verbmobil-Report 151. 
http://verbmobil.dfki.de/cgi-
bin/verbmobil/htbin/doc-access.cgi 
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. 
Bates, D. Jurafsky, P. Taylor, R. Martin, M. 
Meteer, and C. Van Ess-Dykema. 2000. Dialogue 
Act Modeling for Automatic Tagging and Recogni-
tion of Conversational Speech. Computational Lin-
guistics 26:3, 339-371.  
Zell, A., G. Mamier, M. Vogt, et al 1998. SNNS: 
Stuttgart Neural Network Simulator User Manual, 
Version 4.2. 
Unsupervised Induction of Natural Language Morphology Inflection Classes 
Christian Monson, Alon Lavie, Jaime Carbonell, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University  
5000 Forbes Ave. 
Pittsburgh, USA 15213 
{cmonson, alavie+, jgc+, lsl+}@cs.cmu.edu 
 
Abstract 
We propose a novel language-independent 
framework for inducing a collection of mor-
phological inflection classes from a monolin-
gual corpus of full form words.  Our approach 
involves two main stages.  In the first stage, 
we generate a large data structure of candidate 
inflection classes and their interrelationships.  
In the second stage, search and filtering tech-
niques are applied to this data structure, to 
identify a select collection of "true" inflection 
classes of the language.  We describe the basic 
methodology involved in both stages of our 
approach and present an evaluation of our 
baseline techniques applied to induction of 
major inflection classes of Spanish.  The pre-
liminary results on an initial training corpus 
already surpass an F1 of 0.5 against ideal 
Spanish inflectional morphology classes. 
1 Introduction 
Many natural language processing tasks, such as 
morphological analysis and parsing, have mature 
solutions when applied to resource-rich European 
and Asian languages.  Addressing these same tasks 
in less studied low-density languages, however, 
poses exciting challenges.   
These languages have limited available re-
sources: with perhaps a few million speakers there 
is likely no native speaker linguist and frequently 
there is little electronic text readily available.  To 
compound the difficulties, while low-density lan-
guages abound, comparatively little financial re-
sources are available to address their challenges.  
These considerations suggest developing systems 
to automatically induce solutions for NLP tasks in 
new languages. 
The AVENUE project (Lavie et al 2003; Car-
bonell et al, 2002; Probst et al, 2002) at Carnegie 
Mellon University seeks to apply automatic induc-
tion methods to develop rule-based machine trans-
lation systems between pairs of languages where 
one of the languages is low-density and the other is 
resource-rich.  We are currently pursuing MT sys-
tems with Mapudungun, an indigenous language 
spoken by 900,000 people in southern Chile and 
Argentina, and Aymara, spoken by 3 million peo-
ple in Bolivia, Peru, and northern Chile, as low-
density languages and Spanish the resource rich 
language. 
A vital first step in a rule-based machine transla-
tion system is morphological analysis.  This paper 
outlines a framework for automatic natural lan-
guage morphology induction inspired by the tradi-
tional and linguistic concept of inflection classes.  
Additional details concerning the candidate inflec-
tion class framework can be found in Monson 
(2004).  This paper then goes on to describe one 
implemented search strategy within this frame-
work, presenting both a simple summary of results 
and an in depth error analysis. 
While the intent of this research direction is to 
define techniques applicable to low-density lan-
guages, this paper employs English to illustrate the 
main conjectures and Spanish, a language with a 
reasonably complex morphological system, for 
quantitative analysis.  All experiments detailed in 
this paper are over a Spanish newswire corpus of 
40,011 tokens and 6,975 types. 
2 Previous Work 
It is possible to organize much of the recent 
work on unsupervised morphology induction by 
considering the bias each approach has toward dis-
covering morphologically related words that are 
also orthographically similar. 
At one end of the spectrum is the work of 
Yarowsky et al (2001), who derive a morphologi-
cal analyzer for a language, L, by projecting the 
morphological analysis of a resource-rich language 
onto L through a clever application of statistical 
machine translation style word alignment prob-
abilities.  The word alignments are trained over a 
sentence aligned parallel bilingual text for the lan-
guage pair.  While the probabilistic model they use 
to generalize their initial system contains a bias 
toward orthographic similarity, the unembellished 
algorithm contains no assumptions on the ortho-
graphic shape of related word forms. 
Next along the spectrum of orthographic similar-
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
ity bias is the work of Schone and Jurafsky (2000), 
who first acquire a list of pairs of potential mor-
phological variants (PPMV?s) using an ortho-
graphic similarity technique due to Gaussier 
(1999), in which pairs of words from a corpus vo-
cabulary with the same initial string are identified.  
They then apply latent semantic analysis (LSA) to 
score each PPMV with a semantic distance.  Pairs 
measuring a small distance, those whose potential 
variants tend to occur where a neighborhood of the 
nearest hundred words contains similar counts of 
individual high-frequency forms, are then pro-
posed as true morphological variants of one anther.  
In later work, Schone and Jurafsky (2001) extend 
their technique to identify not only suffixes but 
also prefixes and circumfixes by building both 
forward and backward tries over a corpus. 
Goldsmith (2001), by searching over a space of 
morphology models limited to substitution of suf-
fixes, ties morphology yet closer to orthography.  
Segmenting word forms in a corpus, Goldsmith 
creates an inventory of stems and suffixes.  Suf-
fixes which can interchangeably concatenate onto 
a set of stems form a signature.  After defining the 
space of signatures, Goldsmith searches for that 
choice of word segmentations resulting in a mini-
mum description length local optimum. 
Finally, the work of Harris (1955; 1967), and 
later Hafer and Weiss (1974), has direct bearing on 
the approach taken in this paper.  Couched in mod-
ern terms, their work involves first building tries 
over a corpus vocabulary, and then selecting, as 
morpheme boundaries, those character boundaries 
with high branching count in the tries. 
The work in this paper also has a strong bias to-
ward discovering morphologically related words 
that share a similar orthography.  In particular, the 
morphology model we use is, akin to Goldsmith, 
limited to suffix substitution.  The novel proposal 
we bring to the table, however, is a formalization 
of the full search space of all candidate inflection 
classes.  With this bulwark in place, defining 
search strategies for morpheme discovery becomes 
a natural and straightforward activity. 
3 Inflection Classes as Motivation 
When learning the morphology of a foreign lan-
guage, it is common for a student to study tables of 
inflection classes.  In Spanish, for example, a regu-
lar verb belongs to one of three inflection 
classes?verbs that take the -ar infinitive suffix 
inflect for various syntactic features with one set of 
suffixes, verbs that take the -er infinitive suffix 
realize the same set of syntactic features with a 
second set of suffixes, while -ir verbs take yet a 
third set. 
Carstairs-McCarthy formalizes the concept of an 
inflection class in chapter 16 of The Handbook of 
Morphology (1998).  In his terminology, a lan-
guage with inflectional morphology contains lex-
emes which occur in a variety of word forms.  
Each word form carries two pieces of information: 
1) Lexical content and 
2) Morphosyntactic properties. 
For example, the English word form gave ex-
presses the lexeme GIVE plus the morphosyntactic 
property Past, while gives expresses GIVE plus the 
properties 3rd Person, Singular, and Non-Past. 
A set of morphosyntactic properties realized 
with a single word form is defined to be a cell, 
while a paradigm is a set of cells exactly expressed 
by the word forms of some lexeme.   
A particular natural language may have many 
paradigms.  In English, a language with very little 
inflectional morphology, there are at least two 
paradigms, a noun paradigm consisting of two 
cells, Singular and Plural, and a paradigm for 
verbs, consisting of the five cells given (with one 
choice of naming convention) as the first column 
of Table 1. 
Lexemes that belong to the same paradigm may 
still differ in their morphophonemic realizations of 
various cells in that paradigm?each paradigm 
may have several associated inflection classes 
which specify, for the lexemes belonging to that 
inflection class, the surface instantiation for each 
cell of the paradigm.   
Three of the inflection classes within the English 
verb paradigm are found in Table 1 under the col-
umns labeled A through C.  Each inflection class 
Inflection Classes Verb 
Paradigm A B C 
Basic 
blame 
roam 
solve 
show 
sow 
saw 
sing 
ring 
3rd Person 
Singular     
Non-past 
-/z/ 
blames 
roams 
solves 
-/z/ 
shows 
sows 
saws 
-/z/ 
sings 
rings 
 
Past 
-/d/ 
blamed 
roamed 
solved 
-/d/ 
showed 
sowed 
sawed 
V? /eI/ 
sang 
rang 
 
Perfective       
or Passive 
-/d/ 
blamed 
roamed 
solved 
-/n/ 
shown 
sown 
sawn 
V? /?/ 
sung 
rung 
 
Progressive 
-/i?/ 
blaming 
roaming 
solving 
-/i?/ 
showing 
sowing 
sawing 
-/i?/ 
singing 
ringing 
 
 
Table 1: A few inflection classes of the Eng-
lish verb paradigm 
column consists of entries corresponding to the 
cells of the verb paradigm.  Each entry contains an 
informal notation for the morphophonemic process 
which the inflection class applies to the basic form 
of a lexeme and examples of word forms filling the 
corresponding paradigm cell. 
Inflection class A is one of the largest and most 
productive verb inflection classes in English, in-
flection class B contains the Perfective/Passive 
suffix -/n/, and C is a small ?irregular? inflection 
class of strong verbs. 
The task our morphology induction system en-
gages is exactly the discovery of the inflection 
classes of a natural language.  Unlike the analysis 
in Table 1, however, the rest of this paper treats 
word forms as simply strings of characters as op-
posed to strings of phonemes. 
4 Empirical Inflection Classes 
There are two stages in our approach to unsu-
pervised morphology induction.  First, we define a 
search space over a set of candidate inflection 
classes, and second, we search this space for those 
candidates most likely to be part of a true inflec-
tion class in the language.  In both stages of our 
approach we intentionally exploit the fact that suf-
fixes belonging to the same natural language in-
flection class frequently occur interchangeably on 
the same stems. 
4.1 Candidate Inflection Class Search Space 
To define a search space wherein we hope to 
identify inflection classes of a natural language, 
our algorithm accepts as input a monolingual cor-
pus for that language and proposes candidate mor-
pheme boundaries at every character boundary in 
every word form in the corpus vocabulary.  We 
call each string before a candidate morpheme 
boundary a candidate stem or c-stem, and each 
string after a boundary a c-suffix.  We define a 
candidate inflection class (CIC) to be a set of c-
suffixes for which there exists at least one c-stem, 
t, such that each c-suffix in the CIC concatenated 
to t produces a word form in the vocabulary.  For 
convenience, let the set of c-stems which generate 
a CIC, C, be called the adherent c-stems of C; let 
the number of adherent c-stems of C be C?s adher-
ent size; and let the size of the set of c-suffixes in 
C be the level of C.  We denote a CIC in this paper 
by a period delimited sequence of c-suffixes. 
While CIC?s effectively model suffix substitu-
tion on bound stems, we would also like to model 
suffix concatenation onto free stems.  To this end, 
the set of candidate morpheme boundaries our al-
gorithm proposes include those boundaries after 
the final character in each word form.  In this paper 
we assume a suffix, which we denote as ?, follows 
all word form final boundaries.  A CIC contains 
the ? c-suffix when each c-stem in the CIC can 
occur, not only bound to other c-suffixes in the 
CIC, but also as a free stem.  For generality, the 
boundary before the first character of each word 
form is also a candidate morpheme boundary. 
 Table 2 illustrates the type of CIC?s produced 
by our algorithm.  The CIC?s in this table, arranged 
in a systematic but arbitrary order, are each derived 
Vocabulary: blame
blames roams
blamed roamed
roaming
?.s.d
blame
?.s
blame.solve
?.d
blame
s.d s.ed.ing e.es.ing
blame roam solv
s.ed e.ing
roam solv
s s.ing es.ing
blame.roam.solve roam solv
d ed.ing ng
blame.roame roam roami.solvi
ing g
roam.solv roamin.solvin
lame
b
solves
solve
ame.ames.amed
bl
me.mes.med e.es.ed
solving
oams.oamed.oaming
bla blam r
me.mes e.es olve.olves.olvingame.ames
ame.amed
bla blam.solv sbl
me.med e.ed
...bla blam
bl
mes.med es.ed
bla blam
me e
bla blam.solv
amed
bl.ro
mes es
bla blam.solvbl
ames
med ed
bla.roa blam.roam
?
blame.blames.blamed.roams.roamed.roaming.solve.solves.solving
lame.lames.lamed
b
lame.lames
b
lame.lamed
b
lames.lamed
b
lames
b
lamed
b
bl
ames.amed
bl
ame
Table 2: Some of the CIC's, arranged in a systematic but arbitrary order, derived from a toy vo-
cabulary. Each entry is specified as a period delimited sequence of c-suffixes in bold above a   
period delimited sequence of adherent c-stems in italics 
from one or more forms in a small vocabulary con-
sisting of a subset of the word forms found under 
inflection class A from Table 1.  Proposing, as our 
procedure does, morpheme boundaries at every 
character boundary in every word form necessarily 
produces many ridiculous CIC?s, such as 
ame.ames.amed, from the forms blame, blames, 
and blamed and the c-stem bl.  Dispersed among 
the incorrect CIC?s generated by our algorithm, 
however, are also CIC?s that seem very reasonable, 
such as ?.s, from the c-stems blame and tease.   
Note that where Table 1 lists all the surface 
forms of the three lexemes BLAME, ROAM, and 
SOLVE, the vocabulary of Table 2 mimics the vo-
cabulary of a text corpus from a highly inflected 
language where we expect few, if any, lexemes to 
occur in the complete set of possible surface forms.  
Specifically, the vocabulary of Table 2 lacks the 
surface form blaming of the lexeme BLAME, solved 
of the lexeme SOLVE, and the root form roam of 
the lexeme ROAM.  Hence, while the reasonable 
CIC ?.s arises from the pairs of surface forms 
(blame, blames) and (solve, solves), there is no 
way for the form roams to contribute to the ?.s 
CIC because the surface form roam is missing 
from this vocabulary.  In other words, we lack evi-
dence for a ? suffix on the c-stem roam.  Also no-
tice that, as a result of English spelling rules, the 
CIC s.ed generated from the pair of surface forms 
(roams, roamed) is separate from each of the 
CIC?s s.d and es.ed generated from the pair of sur-
face forms (blames, blamed).   
Looking at Table 2, it is clear there is structure 
among the CIC?s.  In particular, at least two types 
of relations hold between CIC?s.  First, hierarchi-
cally, the c-suffixes of one CIC may be a superset 
of the c-suffixes of another CIC.  For example the 
c-suffixes in the CIC e.es.ed are a superset of the 
c-suffixes in the CIC e.ed.  Second, cutting across 
this hierarchical structure there is structure be-
tween CIC?s which propose different morpheme 
boundaries within the same word forms.  Compare 
the CIC?s me.mes.med and e.es.ed; each is de-
rived from exactly the triple of word forms blame, 
blames, and blamed, but differ in the placement of 
the hypothesized morpheme boundary.   
Taken together the hierarchical c-suffix set in-
clusion relations and the morpheme boundary rela-
tions impose a lattice structure on the space of 
CIC?s.  Figure 1 diagrams the CIC lattice over an 
interesting subset of the columns of Table 2.  Hier-
archical links, represented by solid lines, connect 
any given CIC often to more than one parent and 
more than one child.  The empty CIC (not pictured 
in Figure 1) can be considered the child of all level 
one CIC?s (including the ? CIC), but there is no 
universal parent of all top level CIC?s.  Moving up 
the lattice always results in a monotonic decrease 
in adherent size because a parent CIC requires 
each adherent c-stem to form a word with a super-
set of the c-suffixes of each child. 
Horizontal morpheme boundary links, dashed 
lines, connect a CIC, C, with a neighbor to the 
right if each c-suffix in C begins with the same 
character.  This entails that there is at most one 
morpheme boundary link leading to the right of 
each CIC.  There may, however, be as many links 
leading to the left as there are characters in the or-
thography.  The only CIC with depicted multiple 
left links in Figure 1 is ?, which has left links to 
the CIC?s e, s, and d.  A number of left links ema-
nating from the CIC?s in Figure 1 are not shown; 
among others absent from the figure is the left link 
from the CIC e.es leading to the CIC ve.ves with 
the adherent sol.  Since left links effectively divide 
a CIC into separate CIC?s, one for each character 
in the orthography, adherent count monotonically 
decreases as left links are followed. 
To better visualize what a CIC lattice looks like 
when derived from real data, Figure 2 contains a 
portion of a hierarchical lattice automatically gen-
erated from our Spanish newswire corpus.  Each 
entry in Figure 2 contains the c-suffixes compris-
ing the CIC, the adherent size of the CIC, and a 
sample of adherent c-stems.  The lattice in Figure 2 
covers: 
e.es 
blam 
solv 
e.ed 
blam 
es 
blam 
solv 
?.s.d 
blame 
?.s 
blame 
solve 
? 
blame 
blames 
blamed 
roams 
roamed 
roaming 
solve 
solves 
solving 
e.es.ed 
blam 
ed 
blam 
roam 
d 
blame 
roame 
?.d 
blame 
s.d 
blame 
s 
blame 
roam 
solve 
es.ed 
blam 
e 
blam 
solv 
me.mes 
bla 
me.med 
bla 
mes 
bla 
me.mes.med 
bla 
med 
bla 
roa 
mes.med 
bla 
me 
bla 
Figure 1: Portion of a CIC lattice from the 
toy vocabulary in Table 2 
c-suffix set inclusion links 
morpheme boundary links 
1) The productive Spanish inflection class for 
adjectives, a.as.o.os, covering the four adjec-
tive paradigm cells: feminine singular, femi-
nine plural, masculine singular, and mascu-
line plural, respectively, 
2) All possible CIC subsets of the adjective 
CIC, e.g. a.as.o, a.os, etc. and, 
3) The imposter CIC a.as.o.os.tro, together 
with its rogue descendents, a.tro, and tro.   
Other CIC?s that are descendents of a.as.o.os.tro 
and that contain the c-suffix tro do not supply ad-
ditional adherents and hence are not present either 
in Figure 2 or in our program?s representation of 
the CIC lattice.  The CIC?s a.as.tro and os.tro, for 
example, both have only the one adherent, cas, 
already possessed by their common ancestor 
a.as.o.os.tro.  Strictly speaking we have simplified 
for exposition, as the CIC a.as.o.os.tro is not actu-
ally present in the algorithm?s representation ei-
ther, because the c-stem cas occurred with a num-
ber of additional c-suffixes yielding the CIC: 
a.as.i.o.os.sandra.tanier.ter.tro.trol.  
4.2 Search 
Given the framework of CIC lattices, the key 
task for automatic morphology induction is to 
autonomously separate the nonsense CIC?s from 
the useful ones, thus identifying linguistically 
plausible inflection classes.  This section treats the 
CIC lattices as a hypothesis space of valid inflec-
tion classes and searches this space for CIC?s most 
likely to be true inflection classes in a language. 
There are many possible search strategies and 
heuristics applicable to the CIC lattice, and while 
for future work we intend to explore a variety of 
search techniques, this paper presents a reasonable 
and intuitive baseline search procedure.  We have 
investigated a series of algorithms which build 
upon each other.  Each algorithm employs a num-
ber of parameters which are tuned by hand.  These 
parameters are only interesting in so far as they 
help us find true CIC?s from among the many in 
the lattice.  The performance of each algorithm is 
described in section 6. 
4.2.1 Vertical Only 
 To motivate the general approach we have 
taken, compare the adherent sizes of the various 
CIC?s in Figure 2.  The target CIC a.as.o.os, corre-
sponding to the Spanish adjective inflection class, 
has 43 adherents.  Its various descendents must 
occur with monotonically increasing adherent 
sizes, but frequently a child will not more than 
double or triple its immediate parent?s adherent 
size, and never is there a difference greater than a 
factor of ten. Notice also the large adherent counts 
of the level one descendents of a.as.o.os, the 
smallest is as with 404 adherents.   
Contrast this behavior with that of CIC?s involv-
ing the spurious suffix tro.  The CIC a.as.o.os.tro 
occurs in the corpus with exactly one adherent, 
cas.  Additionally, the word forms cena, supper, 
and centro, center, occur yielding the CIC a.tro 
with two adherents.  In total tro is the final string 
of only 16 individual word forms. 
In general, we expect that true suffixes in a lan-
guage will both occur frequently and occur at-
tached to a large number of stems which also ac-
cept other suffixes from the same inflection class.  
These considerations led us to propose three pa-
rameters for our basic search strategy: 
L1 SIZE:  A level one adherent size cutoff 
TOP SIZE:  An absolute adherent size cutoff 
RATIO:  A parent-to-child adherent size      
ratio cutoff 
The L1 SIZE parameter requires a c-suffix to be 
frequent, while the TOP SIZE and RATIO parame-
ters require a suffix to be substitutable for other c-
suffixes in a reasonable number of c-stems. 
a.as.o.os 
43 
african 
cas 
jur?dic 
l 
... 
a.as.o.os.tro 
1 
cas 
a.as.os 
50 
afectad 
cas 
jur?dic 
l 
... 
a.as.o 
59 
cas 
citad 
jur?dic 
l 
... 
a.o.os 
105 
impuest 
indonesi 
italian 
jur?dic 
... 
a.as 
199 
huelg 
incluid 
industri 
inundad 
... 
a.os 
134 
impedid 
impuest 
indonesi 
inundad 
... 
as.os 
68 
cas 
implicad 
inundad 
jur?dic 
... 
a.o 
214 
id 
indi 
indonesi 
inmediat 
... 
as.o 
85 
intern 
jur?dic 
just 
l 
... 
a.tro 
2 
cas 
cen 
a 
1237 
huelg 
ib 
id 
iglesi 
... 
as 
404 
huelg 
huelguist 
incluid 
industri 
... 
os 
534 
humor?stic 
human 
h?gad 
impedid 
... 
o 
1139 
hub 
hug 
human 
huyend 
... 
tro 
16 
catas 
ce 
cen 
cua 
... 
as.o.os 
54 
cas 
implicad 
jur?dic 
l 
... 
 
Figure 2: Hierarchical CIC lattice automati-
cally derived from Spanish 
o.os 
268 
human 
implicad 
indici 
indocumentad 
... 
We apply these three parameters by beginning 
our search at the bottom of the lattice.  Each level 
one CIC with an adherent count larger than L1 
SIZE is placed in a list of path CIC?s.  Then for 
each path CIC, C, we remove C from the list of 
path CIC?s, and in turn consider each of C?s hier-
archical parents, Pi.  If Pi?s adherent size is at least 
TOP SIZE, and if the ratio of Pi?s adherent size to 
C?s adherent size is larger than RATIO, then Pi is 
placed in the list of path CIC?s.  If no parent of C 
can be placed in the list of path CIC?s, and if C?s 
level is greater than one, then C is placed in a list 
of selected CIC?s.  When there are no more CIC?s 
in the list of path CIC?s, the search ends and the 
CIC?s in the selected list are the CIC?s the algo-
rithm believes are true CIC?s of the language. 
As an illustration suppose we explored the lattice 
in Figure 2 with the following parameter settings: 
L1 SIZE:  100 
TOP SIZE:  2 
RATIO:  0.1 
Our search algorithm begins by comparing the 
adherent size of each level one CIC to L1 SIZE.  
The only level one CIC with an adherent count less 
than 100 is tro with 16 adherents, preventing tro 
from being placed in the list of path CIC?s.   
Each of the surviving level one CIC?s is then 
considered in turn.  The algorithm comes to the 
CIC a, where the ratios of adherent sizes between 
each of its parents a.tro, a.as, a.o, and a.os and 
itself are 0.002, 0.161, 0.173, and 0.108 respec-
tively.  Each of these ratios, except that between a 
and a.tro, at 0.002, is larger than 0.1.  And since 
the adherent sizes of a.as, a.o, and a.os are each 
larger than TOP SIZE, these three CIC?s are placed 
in the list of path CIC?s.   
From this point, every hierarchical link in Figure 
2 leading to the CIC a.as.o.os passes the TOP SIZE 
and RATIO cutoffs.  Thus the algorithm reaches a 
state where the only CIC in the list of path CIC?s is 
a.as.o.os.  When this good CIC is removed from 
the list of path CIC?s, the algorithm finds that its 
only parent is a.as.o.os.tro with its lone adherent.  
Since TOP SIZE requires a parent to have at least 
two adherents, a.as.o.os.tro cannot be placed in 
the list of path CIC?s.  As no parent can be placed 
in the list of path CIC?s, a.as.o.os is placed in the 
list of selected CIC?s?which is the desired out-
come.  The list of path CIC?s is now empty and the 
search ends. 
4.2.2 Horizontal Blocking 
 To improve performance over the Vertical Only 
algorithm we next incorporated knowledge from 
the horizontal morpheme boundary links.  Monson 
(2004) describes how morpheme boundary links in 
a CIC lattice can be thought of as branchings in a 
vocabulary trie where identical subtries are con-
flated.  Harris (1955) discusses how the branching 
count in a suffix trie can be exploited to identify 
morpheme boundaries.  We extend the spirit of 
Harris? work in our algorithm through the use of 
two search parameters: 
HORIZ RATIO: A cutoff over: 
sizeadherent 
character in  ending adherents of #
argmax cc  
HORIZ SIZE: An adherent size cutoff 
Left Blocking 
In the first variant of horizontal blocking we ap-
ply these two horizontal parameters when consid-
ering a CIC, C, removed from the list of path 
CIC?s.  If the adherent size of C is larger than 
HORIZ SIZE and the maximum percentage of ad-
herents of C that end in any one character is larger 
than HORIZ RATIO, then C is simply thrown out. 
For example, suppose we used the following 
horizontal parameter settings: 
HORIZ RATIO:  0.5 
HORIZ SIZE:  10 
 The CIC da.do in our Spanish corpus has 62 
adherents, 46, or a fraction of 0.742, of which end 
in the character a (ada and ado fill the feminine 
and masculine past participle cells for the -ar verb 
inflection class).  If our Left Blocking search algo-
rithm reached the CIC da.do, it would be dis-
carded because while its adherent size is larger 
than HORIZ SIZE more than half of its adherents 
end with the same character.  Notice that this algo-
rithm does not explicitly follow leftward mor-
pheme boundary links.  The rationale for this be-
havior is that ada.ado will likely be explored inde-
pendently by a separate vertical path.  In future 
experiments we intend to investigate the effect of 
ensuring that the CIC to the left is explored by 
overtly placing the leftward CIC in the list of path 
CIC?s. 
Right Blocking 
 So far we have only described an algorithm to 
block paths where the correct morpheme boundary 
is to the left of the current hypothesis.  There are 
also CIC?s where a morpheme boundary should be 
moved to the right. The CIC cada.cado with seven 
adherents is one such. 
Accordingly, whenever we encounter a CIC, C, 
all of whose c-suffixes begin with the same charac-
ter (e.g. c in cada.cado) our algorithm poses the 
question, if we were considering the CIC to the 
right (e.g. ada.ado) would we have triggered Left 
Blocking?  If Left Blocking would not have been 
triggered then we throw C out.  In other words, we 
prefer the rightmost possible morpheme boundary, 
unless there is some reason to believe the mor-
pheme boundary should be to the left. 
Taking a closer look at cada.cado, the CIC to its 
right, ada.ado, has 46 adherents of which the char-
acter c ends the most, 7 or a fraction of 0.152.  If 
we were using a HORIZ RATIO of 0.5 as in the pre-
vious section, Left Blocking would not be trig-
gered from ada.ado and so Right Blocking is trig-
gered, throwing out cada.cado.  On the other hand, 
if we were considering blocking ada.ado, where 
both c-suffixes begin with a, the HORIZ RATIO pa-
rameter would need to be larger than 0.742 before 
right blocking would throw out ada.ado.   
Right Blocking Recursive 
 In addition to standard Right Blocking we ex-
plored recursively looking at the next most right 
neighbor of a CIC if the immediate right neighbor 
falls below the HORIZ SIZE threshold.  The ration-
ale behind this variant stems from CIC?s such as 
icada.icado with 4 adherents, crit, publ, ratif, and 
ub.  Since icada.icado?s immediate right neighbor 
cada.cado has only 7 adherents itself we may not 
want to base our blocking decision on so little data.  
Instead we consider the CIC ada.ado, discussed in 
the previous section, which has a large enough ad-
herent size that we might feel confident in our 
judgment.   
Full Horizontal Blocking 
The final version of the search we tried was to 
combine Left Blocking and Right Blocking Recur-
sive while constraining both to use the same values 
for the HORIZ RATIO and HORIZ SIZE parameters. 
5 Evaluation 
To evaluate the performance of the various base-
line search strategies, we first decided on a stan-
dard set of six inflection classes for Spanish: two 
for nouns, ?.s and ?.es, one for adjectives, 
a.as.o.os, and three for verbs, corresponding to the 
traditional -ar, -er, and -ir verb inflection classes.  
We call these six inflection classes our set of stan-
dard IC?s.  We make no claim as to the truth or 
completeness of the set of standard inflection 
classes we used in this evaluation.  The standard 
IC?s we compiled were simply some of the most 
common suffixes filling some of the most common 
morphosyntactic properties marked in Spanish. 
We then defined measures of recall, precision, 
and fragmentation over these standard IC?s (Figure 
3).  As defined, recall measures the fraction of 
unique suffixes in the standard IC?s that are found 
within those selected CIC?s that are subsets of 
some inflection class in the standard; precision 
measures the fraction of unique suffixes among all 
the selected CIC?s that are found within those se-
lected CIC?s that are subsets of an inflection class 
in the standard; and fragmentation measures re-
dundancy, specifically calculating the ratio of the 
number of selected CIC?s that are subsets of stan-
dard IC?s to the number of inflection classes in the 
standard.  High values for recall and precision are 
desirable, while a fragmentation of exactly 1 im-
plies that the number of usefully selected CIC?s is 
the same as the number of inflection classes in the 
standard. 
6 Results and Error Analysis 
For each of the search variants described in sec-
tion 4.2 we executed a by-hand search over the 
relevant parameters for those settings that optimize 
the F1 measure (the harmonic mean of recall and 
precision).  The best performing parameter settings 
are presented in Table 3 while quantitative results 
using these settings are plotted in Figure 4.   
Examining the performance of each algorithm 
(Figure 4) reveals that the simple Vertical only 
search achieves a high precision at the expense of a 
low recall measure.  The simple Vertical search 
also gives the smallest fragmentation, which, when 
combined with the high precision score, indicates a 
conservative algorithm that selects few CIC?s.  The 
parameter settings which achieve the highest F1 for 
Left Block alone and Right Block alone each pro-
duce much higher recall than the simple Vertical 
search.  Right Block Recursive increases precision 
significantly over simple Right Block and achieves 
U
U
sIC' standard 
sIC' standard 
sCIC' selected 
 of elements
 if  of elements
Recall
?
?
?
?
=
I
I
C
I
ICC
U
U
sCIC' selected 
sIC' standard 
sCIC' selected 
 of elements
 if  of elements
Precision
?
?
?
?
=
C
I
C
C
ICC
 
 {
sIC' standard
ionFragmentat sIC' standard 
sCIC' selected 
 if 1 
 if 0 ?
?
?
?
?
=
I
C
IC
IC
 
Figure 3: Three performance measures to   
optimize 
the highest F1 measure of any search variant.  
While Full Horizontal Block also performs well, 
sharing the values of HORIZ RATIO and HORIZ 
SIZE forced a compromise between Left Block and 
Right Block Recursive that did not significantly 
outperform either algorithm alone. 
Of the 83 unique suffixes in the hand compiled 
standard inflection classes, 21 did not share a c-
stem with any other c-suffix in the Spanish news-
wire corpus used for this evaluation?placing an 
upper limit on recall of 0.75 for the search algo-
rithms presented in this paper. 
Examining the parameter settings that yielded 
the highest F1 measure for each search variant 
(Table 3) is also enlightening.  Early experiments 
with Vertical only search clearly demonstrated that 
a TOP SIZE of two, or restricting the CIC?s permit-
ted to be selected to those with at least two adher-
ents, always resulted in better performance than 
other possible settings.  A TOP SIZE of one places 
no restriction on the adherent size of a CIC, ram-
pantly selecting CIC?s, such as the level 10 CIC 
given at the end of section 4.1, that consist of 
many c-suffixes that happen to validly concatenate 
onto a single c-stem?obliterating reasonable pre-
cision.  Higher settings for TOP SIZE induce a 
graceful degradation in recall.  Thus all experi-
ments reported here used a TOP SIZE of two. 
Beyond TOP SIZE the only parameters available 
to the basic Vertical algorithm are L1 SIZE and 
RATIO, which provide only crude means to halt the 
search of bad paths.  In particular, if a level one 
CIC, C, has more than L1 SIZE adherents, and has 
some parent which passes the RATIO cutoff, then 
some ancestor of C will be selected by the algo-
rithm as a good CIC.  Hence, the Vertical only al-
gorithm ensures search gets off on the right foot by 
using the highest values for the L1 SIZE and RATIO 
parameters of any algorithm variant.  Performance 
falls off quickly above L1 SIZE settings of 192, 
indicating that this parameter in this algorithm is 
sensitive to the size of the training corpus. 
In contrast, the horizontal blocking search algo-
rithms have additional parameters available to cull 
out bad search paths, and can hence afford to use 
lower (and more stable) values for L1 SIZE and 
RATIO.  Recall that the Left Blocking algorithm 
discards paths determined to be using a morpheme 
boundary too far to the right, while the Right 
Blocking algorithm discards paths using mor-
pheme boundaries too far to the left.  Notice that 
since, as reasoned in section 4.1, adherent count 
monotonically decreases as morpheme boundary 
links are followed to the left, if the L1 SIZE cutoff 
blocks a particular CIC, C, all CIC?s to the left of 
C will also be blocked.  From these facts it follows 
that a large L1 SIZE will reject some paths result-
ing from morpheme boundaries chosen too far to 
the left, which would otherwise have been pursued 
in the Left Blocking algorithm.  The Right Block-
ing algorithm, however, receives no such benefit, 
and achieves its best performance by maximizing 
recall with a small L1 SIZE. 
Examining the best performing parameter values 
for the Right Blocking Recursive algorithm reveals 
a curious behavior in which low values for L1 SIZE 
and RATIO allow a permissive vertical search while 
stringent values of HORIZ RATIO and, particularly, 
HORIZ SIZE constrain the search.  One explanation 
for these facts might be that following the mono-
tonically increasing chain of CIC adherent sizes 
along right horizontal links allows the algorithm to 
Figure 4: Recall, Precision, F1 and Fragmen-
tation Results for each search algorithm:     
Vertical, Left Blocking, Right Blocking,   
Right Blocking Recursive, and                     
Full Horizontal Blocking 
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
V
LB
RB
RBR
FHB
A
lg
o
ri
th
m
Recall/Precision/F-Measure
0 3 6 9 12 15 18 21 24 27 30
Fragmentation
Recall Precision
F-Measure Fragmentation
Table 3: Hand tuned optimal parameter set-
tings for each search algorithm:                       
Vertical, Left Blocking, Right Blocking,       
Right Blocking Recursive, and                      
Full Horizontal Blocking 
Algorithm TOP SIZE 
L1 
SIZE RATIO 
HORIZ 
RATIO 
HORIZ 
SIZE 
V 2 192 0.3   
LB 2 64 0.1 0.3 3 
RB 2 27 0.2 0.5 27 
RBR 2 27 0.05 0.5 243 
FHB 2 27 0.2 0.3 3 
 
make intelligent blocking decisions backed by suf-
ficient data. 
The best performing parameter values for the 
Full Horizontal Search are a compromise between 
the well performing values for the Left Blocking 
and those for the Right Blocking algorithms.  This 
parameter value compromise does not draw benefit 
from the recursion in the Right Block Recursive 
algorithm, but instead employs Right Block as a 
replacement for the relatively higher L1 SIZE pa-
rameter in the Left Blocking algorithm. 
It is also interesting to examine CIC?s selected 
by the search algorithms.  Table 4 lists all of the 
CIC?s selected by the conservative Vertical search 
algorithm together with a random sample of CIC?s 
selected by Right Blocking Recursive, the algo-
rithm which reached the highest F1 measure of any 
algorithm variant.   
Perhaps the most striking feature of Table 4 is 
the extent to which the CIC?s overlap.  Very few 
individual c-suffixes occur in only one CIC.  Of all 
the CIC?s in Table 4, only ?.s and a.as.o.os, both 
among the CIC?s selected by the Vertical algo-
rithm, represent complete inflection classes in the 
standard IC?s.  The remaining CIC?s are proper 
subsets of various verbal inflection classes.  The 
overlapping nature of the selected CIC?s suggests 
an additional step, which we do not investigate 
here, of conflating CIC?s into a fewer number of 
meta-CIC?s. 
The only verbal inflection class for which sub-
sets are able to pass the large L1 SIZE cutoff im-
posed by the Vertical search algorithm is -ar, the 
most frequent of the three major inflection classes 
in Spanish.  The Right Blocking Recursive algo-
rithm on the other hand identifies significant por-
tions of all three verbal inflection classes.  
The c-suffixes appearing in italics in Table 4 
correspond to no suffix found in any standard IC.  
These alien c-suffixes fall into two categories. 
1) The c-suffixes aciones, aci?n, and adores 
are noun forming derivational suffixes.   
2) The remaining c-suffixes were formed by 
choosing a morpheme boundary too far to 
the right.   
It is the second type of mistake that the Left 
Blocking search algorithm was specifically de-
signed to address.  Unfortunately na?vely combin-
ing the Right Blocking Recursive with the Left 
Blocking algorithm did not improve performance.  
We expect that by using separate horizontal pa-
Vertical
ar er ir 23 of 23 Selected CIC's
? ?.s
? a.aba.ada.adas.ado.ar.as
? a.aba.ada.ado.ando.ar
? a.aba.ada.ado.ar.ar?.en.?
a.aciones.aci?n .ada.adas.ar.aron
? a.ada.adas.ado.ar.ar?
? a.ada.adas.ar.aron.?
? a.ada.ado.ar.aron.ar?.?
? a.ada.ado.ar.ar?.ar?n.en.?
? a.ada.ado.ar.o.?
? a.ada.ados.ar.aron.?
? a.ado.ar.ara.aron.e.?
? a.ado.ar.aron.?
? a.an.ar.?
? a.as.o.os
? ? ? ? a.as
? aba.ado.ando.ar.aron.ar?
? aba.ado.ar.aron.ar?.en
? ada.ado.ados.ar.aron.?
? ada.ado.ando.ar.aron.?
? ada.ado.ar.ar?.o.?
? ada.ado.ar.en.o.?
? ado.ar.aron.ar?.ar?n.en
N A
dj Verbs
 
Table 4: All of the CIC?s selected by the conservative Vertical search algorithm (left), and a random 
sample of CIC?s selected by the algorithm with best F1 measure, Right Blocking Recursive (right).  For 
each CIC row, a dot is placed in the columns representing standard IC?s for which that CIC is a subset.  
The c-suffixes in italics are in no standard IC. 
Right Blocking Recursive
ar er ir 23 of 204 Selected CIC's
?.ba.n.ndo
? a.aba.ado.ados.ar.ar?.ar?n
a.aciones.aci?n .adas.ado.ar
? a.ada.adas.ado.ar.ar?
? a.adas.ado.an.ar
? a.ado.ados.ar.?
? a.ado.an.arse.?
? a.ado.aron.arse.?
? aba.ada.ado.ar.o.os
aciones.aci?n .ado.ados
aciones .ado.ados.ar?
aci?n .ado.an.e
? ada.adas.ado.ados.aron.?
? ada.ado.ados.ar.o
ado.adores .o
? ado.ados.arse.e
? ado.ar.aron.arse.ar?
do.dos.ndo.r.ron
? ? e.ida.ido
? emos.ido.?a.?an
? ida.ido.idos.ir.i?
? ido.iendo.ir
? ido.ir.ro
N A
dj Verbs
 
rameters for left blocking and for right blocking 
we could combine these two algorithms in a less 
constrained fashion that would result in better 
overall performance. 
7 Future Work  
We believe the heuristic search strategy de-
scribed in this paper can be significantly improved 
upon.  We plan to investigate search strategies for 
both the vertical and horizontal links in our CIC 
lattices.  We currently have plans to employ statis-
tical independence and correlation tests to adjacent 
CIC?s as a guide to search (Monson, 2004).  Other 
search criteria we are considering are information 
gain and minimum description length measures. 
There are also modifications to the search strat-
egy that may significantly improve performance.  
For example, it may be advantageous to actively 
follow horizontal morpheme boundary links, in-
stead of merely blocking paths, when a morpheme 
boundary error is discovered.  The next immediate 
step we will take is to scale our implementation to 
investigate performance changes as we increase 
the size of our Spanish corpus. 
The intention of this work is to produce a lan-
guage independent morphology induction algo-
rithm.  Hence, we plan to apply this work to a vari-
ety of languages, both well studied resource-rich 
languages as well as low-density languages of in-
terest to the AVENUE project. 
8 Acknowledgements 
The research reported in this paper was funded 
in part by NSF grant number IIS-0121631. 
References 
Jaime Carbonell, Katharina Probst, Erik Peterson, 
Christian Monson, Alon Lavie, Ralf Brown, and 
Lori Levin.  2002. Automatic Rule Learning for 
Resource-Limited MT. In Proceedings of the 5th 
Conference of the Association for Machine 
Translation in the Americas (AMTA-02). 
Andrew Carstairs-McCarthy. 1998. ?Paradigmatic 
Structure: Inflectional Paradigms and Morpho-
logical Classes.? The Handbook of Morphology. 
Eds. Andrew Spencer and Arnold M. Zwicky. 
Blackwell Publishers Inc., Massachusetts, USA, 
322-334. 
?ric Gaussier. 1999. Unsupervised learning of 
derivational morphology from inflectional lexi-
cons. In Proceedings of ACL ?99 Workshop: Un-
supervised Learning in Natural Language Proc-
essing. 
John Goldsmith. 2001. Unsupervised learning of 
the morphology of a natural language. Computa-
tional Linguistics, 27(2): 153-198. 
Margaret A. Hafer and Stephen F. Weiss. 1974. 
Word segmentation by letter successor varieties. 
Information Storage and Retrieval, 10:371-385. 
Zellig Harris. 1955. From phoneme to morpheme. 
Language, 31:190-222. Reprinted in Harris 
1970. 
Zellig Harris. 1967. Morpheme boundaries within 
words: Report on a computer test. Transforma-
tion and Discourse Analysis Papers 73, Depart-
ment of Linguistics, University of Pennsylvania. 
Reprinted in Harris 1970. 
Zellig Harris. 1970. Papers in Structural and 
Transformational Linguistics. D. Reidel, 
Dordrecht, Holland. 
Alon Lavie, Stephan Vogel, Lori Levin, Erik Pe-
terson, Katharina Probst, Ariadna Font Llitj?s, 
Rachel Reynolds, Jaime Carbonell, and Richard 
Cohen. 2003. Experiments with a Hindi-to-
English Transfer-based MT System under a Mis-
erly Data Scenario. ACM Transactions on Asian 
Language Information Processing (TALIP), to 
appear in 2(2). 
Christian Monson. 2004. A Framework for Unsu-
pervised Natural Language Morphology Induc-
tion.  In Proceedings of the Student Workshop at 
ACL-04. 
Katharina Probst, Lori Levin, Erik Peterson, Alon 
Lavie, and Jaime Carbonell. 2002. MT for Re-
source-Poor Languages using Elicitation-based 
Learning of Syntactic Transfer Rules. Machine 
Translation, Special Issue on Embedded MT, 
17(4): 245-270. 
Patrick Schone and Daniel Jurafsky. 2000. Knowl-
edge-free Induction of Morphology Using Latent 
Semantic Analysis. In Proceedings of the Fourth 
Conference on Computational Natural Language 
Learning and of the Second Learning Language 
in Logic Workshop, 67-72. 
Patrick Schone and Daniel Jurafsky. 2001. Knowl-
edge-free Induction of Inflectional Morpholo-
gies. In Proceedings of the North American 
Chapter of the Association of Computational 
Linguistics. 183-191. 
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis 
tools via robust projection across aligned cor-
pora. In Proceedings of the Human Language 
Technology Conference, 161-168. 
 
Interlingual Annotation of Multilingual Text Corpora 
 
Stephen Helmreich 
David Farwell 
Computing Research Laboratory 
New Mexico State University 
david@crl.nmsu.edu
shelmrei@crl.nmsu.edu
Florence Reeder 
Keith Miller 
Information Discovery & Understanding 
MITRE Corporation 
freeder@mitre.org
keith@mitre.org   
Bonnie Dorr 
Nizar Habash 
Institute for Advanced Computer Studies 
University of Maryland 
bonnie@umiacs.umd.edu
habash@umiacs.umd.edu
 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
hovy@isi.edu 
Lori Levin 
Teruko Mitamura 
Language Technologies Institute 
Carnegie Mellon University 
lsl@cs.cmu.edu
teruko@cs.cmu.edu 
Owen Rambow 
Advaith Siddharthan 
Department of Computer Science 
Columbia University 
rambow@cs.columbia.edu
as372@cs.columbia.edu  
 
 
Abstract 
This paper describes a multi-site project to 
annotate six sizable bilingual parallel corpora 
for interlingual content. After presenting the 
background and objectives of the effort, we 
describe the data set that is being annotated, 
the interlingua representation language used, 
an interface environment that supports the an-
notation task and the annotation process itself. 
We will then present a preliminary version of 
our evaluation methodology and conclude 
with a summary of the current status of the 
project along with a number of issues which 
have arisen.  
1 Introduction 
This paper describes a multi-site National Science 
Foundation project focusing on the annotation of six 
sizable bilingual parallel corpora for interlingual content 
with the goal of providing a significant data set for im-
proving knowledge-based approaches to machine trans-
lation (MT) and a range of other Natural Language 
Processing (NLP) applications. The project participants 
include the Computing Research Laboratory at NMSU, 
the Language Technologies Institute at CMU, the In-
formation Science Institute at USC, UMIACS at the 
University of Maryland, the MITRE Corporation and 
Columbia University. In the remainder of the paper, we 
first present the background and objectives of the pro-
ject. We then describe the data set that is being anno-
tated, the interlingual representation language being 
used, an interface environment that is designed to sup-
port the annotation task, and the process of annotation 
itself. We will then outline a preliminary version of our 
evaluation methodology and conclude with a summary 
of the current status of the project along with a set of 
issues that have arisen since the project began.  
2 Project Goals and Expected Outcomes 
The central goals of the project are: 
? to produce a practical, commonly-shared system 
for representing the information conveyed by a 
text, or ?interlingua?, 
? to develop a methodology for accurately and 
consistently assigning such representations to 
texts across languages and across annotators, 
? to annotate a sizable multilingual parallel corpus 
of source language texts and translations for IL 
content. 
This corpus is expected to serve as a basis for improving 
meaning-based approaches to MT and a range of other 
natural language technologies.  The tools and annotation 
standards will serve to facilitate more rapid annotation 
of texts in the future. 
3 Corpus 
The target data set is modeled on and an extension of 
the DARPA MT Evaluation data set (White and 
O?Connell 1994) and includes data from the Linguistic 
Data Consortium (LDC) Multiple Translation Arabic, 
Part 1 (Walker et al, 2003). The data set consists of 6 
bilingual parallel corpora. Each corpus is made up of 
125 source language news articles along with three 
translations into English, each produced independently 
by different human translators. However, the source 
news articles for each individual language corpus are 
different from the source articles in the other language 
corpora.  Thus, the 6 corpora themselves are comparable 
to each other rather than parallel. The source languages 
are Japanese, Korean, Hindi, Arabic, French and Span-
ish.  Typically, each article is between 300 and 400 
words long (or the equivalent) and thus each corpus has 
between 150,00 and 200,000 words. Consequently, the 
size of the entire data set is around 1,000,000 words. 
Thus, for any given corpus, the annotation effort is 
to assign interlingual content to a set of 4 parallel texts, 
3 of which are in the same language, English, and all of 
which theoretically communicate the same information. 
The following is an example set from the Spanish cor-
pus: 
S: Atribuy? esto en gran parte a
una pol?tica que durante muchos a?os
tuvo un "sesgo concentrador" y repre-
sent? desventajas para las clases me-
nos favorecidas.
T1: He attributed this in great
part to a type of politics that
throughout many years possessed a
"concentrated bias" and represented
disadvantages for the less favored
classes.
T2: To a large extent, he attrib-
uted that fact to a policy which had
for many years had a "bias toward
concentration" and represented disad-
vantages for the less favored
classes.
T3: He attributed this in great
part to a policy that had a "centrist
slant" for many years and represented
disadvantages for the less-favored
classes.
 
The annotation process involves identifying the 
variations between the translations and then assessing 
whether these differences are significant. In this case, 
the translations are, for the most part, the same although 
there are a few interesting variations.  
For instance, where this appears as the translation 
of esto in the first and third translations, that fact 
appears in the second. The translator choice potentially 
represents an elaboration of the semantic content of the 
source expression and the question arises as to whether 
the annotation of the variation in expressions should be 
different or the same.  
More striking perhaps is the variation between 
concentrated bias, bias toward concen-
tration and centrist slant as the translation 
for sesgo concentrador. Here, the third transla-
tion offers a clear interpretation of the source text au-
thor?s intent. The first two attempt to carry over the 
vagueness of the source expression assuming that the 
target text reader will be able to figure it out. But even 
here, the two translators appear to differ as to what the 
source language text author?s intent actually was, the 
former referring to bias of a certain degree of strength 
and the second to a bias  in a certain direction. Seem-
ingly, then, the annotation of each of these expressions 
should differ. 
Furthermore, each source language has different 
methods of encoding meaning linguistically. The resul-
tant differing types of translation mismatch with English 
should provide insight into the appropriate structure and 
content for an interlingual representation. 
The point is that a multilingual parallel data set of 
source language texts and English translations offers a 
unique perspective and unique problem for annotating 
texts for meaning. 
4 Interlingua 
Due to the complexity of an interlingual annotation as 
indicated by the differences described in the previous 
section, the representation has developed through three 
levels and incorporates knowledge from sources such as 
the Omega ontology and theta grids.  Since this is an 
evolving standard, the three levels will be presented in 
order as building on one another. Then the additional 
data components will be described.  
4.1 Three Levels of Representation 
We now describe three levels of representation, referred 
to as IL0, IL1 and IL2. The aim is to perform the annota-
tion process incrementally, with each level of represen-
tation incorporating additional semantic features and 
removing existing syntactic ones. IL2 is intended as the 
interlingua, that abstracts away from (most) syntactic 
idiosyncrasies of the source language. IL0 and IL1 are 
intermediate representations that are useful starting 
points for annotating at the next level. 
4.1.1 IL0 
IL0 is a deep syntactic dependency representation. It 
includes part-of-speech tags for words and a parse tree 
that makes explicit the syntactic predicate-argument 
structure of verbs. The parse tree is labeled with syntac-
tic categories such as Subject or Object , which refer to 
deep-syntactic grammatical function (normalized for 
voice alternations).  IL0 does not contain function words 
(determiners, auxiliaries, and the like): their contribu-
tion is represented as features.  Furthermore, semanti-
cally void punctuation has been removed.  While this 
representation is purely syntactic, many disambiguation 
decisions, relative clause and PP attachment for exam-
ple, have been made, and the presentation abstracts as 
much as possible from surface-syntactic phenomena.  
Thus, our IL0 is intermediate between the analytical and 
tectogrammatical levels of the Prague School (Haji? et 
al 2001). IL0 is constructed by hand-correcting the out-
put of a dependency parser (details in section 6) and is a 
useful starting point for semantic annotation at  IL1, 
since it allows annotators to see how textual units relate 
syntactically when making semantic judgments.  
4.1.2 IL1 
IL1 is an intermediate semantic representation. It asso-
ciates semantic concepts with lexical units like nouns, 
adjectives,  adverbs and verbs (details of the ontology in 
section 4.2). It also replaces the syntactic relations in 
IL0, like subject and object, with thematic roles, like 
agent, theme and goal (details in section 4.3). Thus, like 
PropBank (Kingsbury et al2002), IL1 neutralizes dif-
ferent alternations for argument realization.  However, 
IL1 is not an interlingua; it does not normalize over all 
linguistic realizations of the same semantics. In particu-
lar, it does not address how the meanings of individual 
lexical units combine to form the meaning of a phrase or 
clause. It also does not address idioms, metaphors and 
other non-literal uses of language.  Further, IL1 does not 
assign semantic features to prepositions; these continue 
to be encoded as syntactic heads of their phrases, al-
though these might have been annotated with thematic 
roles such as location or time. 
4.1.3 IL2 
IL2 is intended to be an interlingua, a representation of 
meaning that is reasonably independent of language. IL2 
is intended to capture similarities in meaning across 
languages and across different lexical/syntactic realiza-
tions within a language. For example, IL2 is expected to 
normalize over conversives (e.g. X bought a book from 
Y vs. Y sold a book to X)  (as does FrameNet (Baker et 
al 1998)) and non-literal language usage (e.g. X started 
its business vs. X opened its doors to customers).  The 
exact definition of IL2 will be the major research con-
tribution of this project. 
4.2 The Omega Ontology 
In progressing from IL0 to IL1, annotators have to se-
lect semantic terms (concepts) to represent the nouns, 
verbs, adjectives, and adverbs present in each sentence.  
These terms are represented in the 110,000-node ontol-
ogy Omega (Philpot et al, 2003), under construction at 
ISI.  Omega has been built semi-automatically from a 
variety of sources, including Princeton's WordNet (Fell-
baum, 1998), NMSU?s Mikrokosmos (Mahesh and Ni-
renburg, 1995), ISI's Upper Model (Bateman et al, 
1989) and ISI's SENSUS (Knight and Luk, 1994).  After 
the uppermost region of Omega was created by hand, 
these various resources? contents were incorporated and, 
to some extent, reconciled.  After that, several million 
instances of people, locations, and other facts were 
added (Fleischman et al, 2003).  The ontology, which 
has been used in several projects in recent years (Hovy 
et al, 2001), can be browsed using the DINO browser at 
http://blombos.isi.edu:8000/dino; this browser forms a 
part of the annotation environment.  Omega remains 
under continued development and extension.  
4.3 The Theta Grids 
Each verb in Omega is assigned one or more theta grids 
specifying the arguments associated with a verb and 
their theta roles (or thematic role).  Theta roles are ab-
stractions of deep semantic relations that generalize 
over verb classes.  They are by far the most common 
approach in the field to represent predicate-argument 
structure.  However, there are numerous variations with 
little agreement even on terminology (Fillmore, 1968; 
Stowell, 1981; Jackendoff, 1972; Levin and Rappaport-
Hovav, 1998). 
The theta grids used in our project were extracted 
from the Lexical Conceptual Structure Verb Database 
(LVD) (Dorr, 2001).  The WordNet senses assigned to 
each entry in the LVD were then used to link the theta 
grids to the verbs in the Omega ontology.  In addition to 
the theta roles, the theta grids specify the mapping be-
tween theta roles and their syntactic realization in argu-
ments, such as Subject, Object or Prepositional Phrase, 
and the Obligatory/Optional nature of the argument, 
thus facilitating IL1 annotation.  For example, one of the 
theta grids for the verb ?load? is listed in Table 1 (at the 
end of the paper). 
Although based on research in LCS-based MT 
(Dorr, 1993; Habash et al 2002), the set of theta roles 
used has been simplified for this project.  This list (see 
Table 2 at the end of the paper), was used in the Inter-
lingua Annotation Experiment 2002 (Habash and 
Dorr).1  
4.4 Incremental Annotation 
As described earlier, the development and annota-
tion of the interlingual notation is incremental in nature.  
This necessitates constraining the types and categories 
of attributes included in the annotation during the be-
ginning phases.  Other topics not addressed here, but 
considered for future work include time, aspect, loca-
tion, modality, type of reference, types of speech act, 
causality, etc.  
Thus, IL2 itself is not a final interlingual representa-
tion, but one step along the way. IL0 and IL1 are also 
intermediate representations, and as such are an occa-
sionally awkward mixture of syntactic and semantic 
information. The decisions as to what to annotate, what 
to normalize, what to represent as features at each level 
are semantically and syntactically principled, but also 
governed by expectations about reasonable annotator 
tasks. What is important is that at each stage of trans-
formation, no information is lost, and the original lan-
guage recoverable in principle from the representation. 
5 Annotation Tool 
We have assembled a suite of tools to be used in the 
annotation process.  Some of these tools are previously 
existing resources that were gathered for use in the pro-
ject, and others have been developed specifically with 
the annotation goals of this project in mind.  Since we 
are gathering our corpora from disparate sources, we 
need to standardize the text before presenting it to 
automated procedures.  For English, this involves sen-
tence boundary detection, but for other languages, it 
may involve segmentation, chunking of text, or other 
?text ecology? operations.  The text is then processed 
with a dependency parser, the output of which is viewed 
and corrected in TrED (Haji?, et al, 2001), a graphi-
cally-based tree editing program, written in Perl/Tk2.  
The revised deep dependency structure produced by this 
process is the IL0 representation for that sentence. 
In order to derive IL1 from the IL0 representation, 
annotators use Tiamat, a tool developed specifically for 
                                                           
1 Other contributors to this list are Dan Gildea and Karin 
Kipper Schuler. 
2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree_Editors/Tre
d/ 
this project.  This tool enables viewing of the IL0 tree 
with easy reference to all of the IL resources described 
in section 4 (the current IL representation, the ontology, 
and the theta grids).  This tool provides the ability to 
annotate text via simple point-and-click selections of 
words, concepts, and theta-roles.  The IL0 is displayed 
in the top left pane, ontological concepts and their asso-
ciated theta grids, if applicable, are located in the top 
right, and the sentence itself is located in the bottom 
right pane.  An annotator may select a lexical item (leaf 
node) to be annotated in the sentence view; this word is 
highlighted, and the relevant portion of the Omega on-
tology is displayed in the pane on the left.  In addition, 
if this word has dependents, they are automatically un-
derlined in red in the sentence view.  Annotators can 
view all information pertinent to the process of deciding 
on appropriate ontological concepts in this view.  Fol-
lowing the procedures described in section 6, selection 
of concepts, theta grids and roles appropriate to that 
lexical item can then be made in the appropriate panes. 
Evaluation of the annotators? output would be daunt-
ing based solely on a visual inspection of the annotated 
IL1 files.  Thus, a tool was also developed to compare 
the output and to generate the evaluation measures that 
are described in section 7.  The reports generated by the 
evaluation tool allow the researchers to look at both 
gross-level phenomena, such as inter-annotator agree-
ment, and at more detailed points of interest, such as 
lexical items on which agreement was particularly low, 
possibly indicating gaps or other inconsistencies in the 
ontology being used. 
6 Annotation Task 
To describe the annotation task, we first present the 
annotation process and tools used with it as well as the 
annotation manuals.  Finally, setup issues relating to 
negotiating multi-site annotations are discussed. 
6.1 Annotation process 
The annotation process was identical for each text. For 
the initial testing period, only English texts were anno-
tated, and the process described here is for English text. 
The process for non-English texts will be, mutatis mu-
tandis, the same. 
Each sentence of the text is parsed into a depend-
ency tree structure. For English texts, these trees were 
first provided by the Connexor parser at UMIACS 
(Tapanainen and Jarvinen, 1997), and then corrected by 
one of the team PIs. For the initial testing period, anno-
tators were not permitted to alter these structures. Al-
ready at this stage, some of the lexical items are 
replaced by features (e.g., tense), morphological forms 
are replaced by features on the citation form, and certain 
constructions are regularized (e.g., passive) and empty 
arguments inserted.  It is this dependency structure that 
is loaded into the annotation tool and which each anno-
tator then marks up. 
The annotator was instructed to annotate all nouns, 
verbs, adjectives, and adverbs. This involves annotating 
each word twice ? once with a concept from Wordnet 
SYNSET and once with a Mikrokosmos concept; these 
two units of information are merged, or at least inter-
twined in Omega. One of the goals and results of this 
annotation process will be a simultaneous coding of 
concepts in both ontologies, facilitating a closer union 
between them.  
In addition, users were instructed to provide a se-
mantic case role for each dependent of a verb. In many 
cases this was ?NONE? since adverbs and conjunctions 
were dependents of verbs in the dependency tree. LCS 
verbs were identified with Wordnet classes and the LCS 
case frames supplied where possible. The user, how-
ever, was often required to determine the set of roles or 
alter them to suit the text. In both cases, the revised or 
new set of case roles was noted and sent to a guru for 
evaluation and possible permanent inclusion. Thus the 
set of event concepts in the ontology supplied with roles 
will grow through the course of the project. 
6.2 The annotation manuals 
Markup instructions are contained in three manuals: a 
users guide for Tiamat (including procedural instruc-
tions), a definitional guide to semantic roles, and a 
manual for creating a dependency structure (IL0). To-
gether these manuals allow the annotator to (1) under-
stand the intention behind aspects of the dependency 
structure; (2) how to use Tiamat to mark up texts; and 
(3) how to determine appropriate semantic roles and 
ontological concepts. In choosing a set of appropriate 
ontological concepts, annotators were encouraged to 
look at the name of the concept and its definition, the 
name and definition of the parent node, example sen-
tences, lexical synonyms attached to the same node, and 
sub- and super-classes of the node. All these manuals 
are available on the IAMTC website3. 
6.3 The multi-site set up 
For the initial testing phase of the project, all annotators 
at all sites worked on the same texts. Two texts were 
provided by each site as were two translations of the 
same source language (non-English) text. To test for the 
effects of coding two texts that are semantically close, 
since they are both translations of the same source 
document, the order in which the texts were annotated 
differed from site to site, with half the sites marking one 
translation first, and the other half of the sites marking 
the second translation first. Another variant tested was 
                                                           
3 http://sparky.umiacs.umd.edu:8000/IAMTC/annotation
_manual.wiki?cmd=get&anchor=Annotation+Manual 
to interleave the two translations, so that two similar 
sentences were coded consecutively. 
During the later production phase, a more complex 
schedule will be followed, making sure that many texts 
are annotated by two annotators, often from different 
sites, and that regularly all annotators will mark the 
same text. This will help ensure continued inter-coder 
reliability. 
In the period leading up to the initial test phase, 
weekly conversations were held at each site by the an-
notators, going over the texts coded. This was followed 
by a weekly conference call among all the annotators. 
During the test phase, no discussion was permitted. 
One of the issues that arose in discussion was how 
certain constructions should be displayed and whether 
each word should have a separate node or whether cer-
tain words should be combined into a single node. In 
view of the fact that the goal was not to tag individual 
words, but entities and relations, in many cases words 
were combined into single nodes to facilitate this proc-
ess. For instance, verb-particle constructions were com-
bined into a single node. In a sentence like ?He threw it 
up?, ?throw? and ?up? were combined into a single 
node ?throw up? since one action is described by the 
combined words. Similarly, proper nouns, compound 
nouns and copular constructions required specialized 
handling.    In addition, issues arose about whether an-
notators should change dependency trees; and in in-
structing the annotators on how best to determine an 
appropriate ontology node.    
7 Evaluation 
The evaluation criteria and metrics continue to evolve 
and are in the early stages of formation and implementa-
tion.  Several possible courses for evaluating the annota-
tions and resulting structures exist.  In the first of these, 
the annotations are measured according to inter-
annotator agreement.  For this purpose, data is collected 
reflecting the annotations selected, the Omega nodes 
selected and the theta roles assigned.  Then, inter-coder 
agreement is measured by a straightforward match, with 
agreement calculated by a Kappa measure (Carletta, 
1993) and a Wood standard similarity (Habash and 
Dorr, 2002).  This is done for three agreement points:  
annotations, Omega selection and theta role selection.  
At this time, the Kappa statistic?s expected agreement is 
defined as 1/(N+1) where N is the number of choices at 
a given data point.  In the case of Omega nodes, this 
means the number of matched Omega nodes (by string 
match) plus one for the possibility of the annotator trav-
ersing up or down the hierarchy. Multiple measures are 
used because it is important to have a mechanism for 
evaluating inter-coder consistency in the use of the IL 
representation language which does not depend on the 
assumption that there is a single correct annotation of a 
given text.  The tools for evaluation have been modified 
from pervious use (Habash and Dorr, 2002). 
Second, the accuracy of the annotation is measured.  
Here accuracy is defined as correspondence to a prede-
fined baseline.  In the initial development phase, all 
sites annotated the same texts and many of the varia-
tions were discussed at that time, permitting the devel-
opment of a baseline annotation.  While not a useful 
long-term strategy, this produced a consensus baseline 
for the purpose of measuring the annotators? task and 
the solidity of the annotation standard.  
The final measurement technique derives from the 
ultimate goal of using the IL representation for MT, 
therefore, we are measuring the ability to generate accu-
rate surface texts from the IL representation as anno-
tated.  At this stage, we are using an available generator, 
Halogen (Knight and Langkilde, 2000).  A tool to con-
vert the representation to meet Halogen requirements is 
being built.  Following the conversion, surface forms 
will be generated and then compared with the originals 
through a variety of standard MT metrics (ISLE, 2003).   
8 Accomplishments and Issues 
In a short amount of time, we have identified languages 
and collected corpora with translations.  We have se-
lected representation elements, from parser outputs to 
ontologies, and have developed an understanding of 
how their component elements fit together.  A core 
markup vocabulary (e.g., entity-types, event-types and 
participant relations) was selected.  An initial version of 
the annotator?s toolkit (Tiamat) has been developed and 
has gone through alpha testing.  The multi-layered ap-
proach to annotation  decided upon reduces the burden 
on the annotators for any given text as annotations build 
upon one another.  In addition to developing individual 
tools, an infrastructure exists for carrying out a multi-
site annotation project.   
In the coming months we will be fleshing out the 
current procedures for evaluating the accuracy of an 
annotation and measuring inter-coder consistency.  
From this, a multi-site evaluation will be produced and  
results reported.  Regression testing, from the interme-
diate stages and representations will be able to be car-
ried out.  Finally, a growing corpus of annotated texts 
will become available.   
In addition to the issues discussed throughout the 
paper, a few others have not yet been identified.  From a 
content standpoint, looking at IL systems for time and 
location should utilize work in personal name, temporal 
and spatial annotation (e.g., Ferro et al, 2001).  Also, an 
ideal IL representation would also account for causality, 
co-reference, aspectual content, modality, speech acts, 
etc.  At the same time, while incorporating these items, 
vagueness and redundancy must be eliminated from the 
annotation language.  Many inter-event relations would 
need to be captured such as entity reference, time refer-
ence, place reference, causal relationships, associative 
relationships, etc.  Finally, to incorporate these, cross-
sentence phenomena remain a challenge.     
From an MT perspective, issues include evaluating 
the consistency in the use of an annotation language 
given that any source text can result in multiple, differ-
ent, legitimate translations (see Farwell and Helmreich, 
2003) for discussion of evaluation in this light.  Along 
these lines, there is the problem of annotating texts for 
translation without including in the annotations infer-
ences from the source text.   
9 Conclusions  
This is a radically different annotation project from 
those that have focused on morphology, syntax or even 
certain types of semantic content (e.g., for word sense 
disambiguation competitions). It is most similar to 
PropBank (Kingsbury et al2002) and FrameNet (Baker 
et al1998).  However, it is novel in its emphasis on:  (1) 
a more abstract level of mark-up (interpretation); (2) the 
assignment of a well-defined meaning representation to 
concrete texts; and (3) issues of a community-wide con-
sistent and accurate annotation of meaning. 
By providing an essential, and heretofore non-
existent, data set for training and evaluating natural lan-
guage processing systems, the resultant annotated multi-
lingual corpus of translations is expected to lead to 
significant research and development opportunities for 
Machine Translation and a host of other Natural Lan-
guage Processing technologies including Question-
Answering and Information Extraction.  
References 
Baker, C., J. Fillmore and J B. Lowe, 1998.  The Berke-
ley FrameNet Project.  Proceedings of ACL. 
Bateman, J.A., R. Kasper, J. Moore, and R. Whitney. 
1989. A General Organization of Knowledge for 
Natural Language Processing: The Penman Upper 
Model. Unpublished research report, USC / Informa-
tion Sciences Institute, Marina del Rey, CA.  
Carletta, J. C. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2), 249-254 
Conceptual Structures and Documentation, UMCP. 
http://www.umiacs.umd.edu/~bonnie/LCS_Database
_Documentation.html  
Dorr, B. J. 2001.  LCS Verb Database, Online Software 
Database of Lexical  
Dorr, B. J., 1993. Machine Translation: A View from the 
Lexicon, MIT Press, Cambridge, MA. 
Farwell, D., and S. Helmreich.  2003.  Pragmatics-based 
Translation and MT Evaluation.  In Proceedings of 
Towards Systematizing MT Evaluation.  MT-Summit 
Workshop, New Orleans, LA. 
Fellbaum, C. (ed.). 1998. WordNet: An On-line Lexical 
Database and Some of its Applications. MIT Press, 
Cambridge, MA. 
Ferro, L., I. Mani, B. Sundheim and G. Wilson.  2001. 
TIDES Temporal Annotation Guidelines. Version 
1.0.2 MITRE Technical Report, MTR 01W0000041 
Fillmore, C..  1968. The Case for Case. In E. Bach and 
R. Harms, editors, Universals in Linguistic Theory, 
pages 1--88. Holt, Rinehart, and Winston.  
Fleischman, M., A. Echihabi, and E.H. Hovy. 2003. 
Offline Strategies for Online Question Answering: 
Answering Questions Before They Are Asked.  Pro-
ceedings of the ACL Conference. Sapporo, Japan. 
Habash, N. and B. Dorr. 2002. Interlingua Annotation 
Experiment Results. AMTA-2002 Interlingua Reli-
ability Workshop. Tiburon, California, USA. 
Habash, N., B. J. Dorr, and D. Traum, 2002. "Efficient 
Language Independent Generation from Lexical 
Conceptual Structures," Machine Translation, 17:4. 
Haji?, J.; B. Vidov?-Hladk?; P. Pajas.  2001: The Pra-
gue Dependency Treebank: Annotation Structure and 
Support. In Proceeding of the IRCS Workshop on 
Linguistic Databases, pp. . University of Pennsyl-
vania, Philadelphia, USA, pp. 105-114. 
Hovy, E., A. Philpot, J. Ambite, Y. Arens, J. Klavans, 
W. Bourne, and D. Saroz.  2001. Data Acquisition 
and Integration in the DGRC's Energy Data Collec-
tion Project, in Proceedings of the NSF's dg.o 2001. 
Los Angeles, CA. 
ISLE 2003.  Framework for Evaluation of Machine 
Translation in ISLE.  
http://www.issco.unige.ch/projects/isle/femti/ 
Jackendoff, R. 1972. Grammatical Relations and Func-
tional Structure. Semantic Interpretation in Genera-
tive Grammar. The MIT Press, Cambridge, MA. 
Kingsbury, P and M Palmer and M Marcus , 2002.  
Adding Semantic Annotation to the Penn TreeBank. 
Proceedings of the Human Language Technology 
Conference (HLT 2002).  
Knight, K., and I. Langkilde. 2000.  Preserving Ambi-
guities in Generation via Automata Intersection. 
American Association for Artificial Intelligence con-
ference (AAAI). 
Knight, K, and S. K. Luk.  1994. Building a Large-Scale 
Knowledge Base for Machine Translation.  Proceed-
ings of AAAI. Seattle, WA. 
Levin, B. and M. Rappaport-Hovav. 1998. From Lexical 
Semantics to Argument Realization. Borer, H. (ed.) 
Handbook of Morphosyntax and Argument Structure. 
Dordrecht: Kluwer Academic Publishers. 
Mahesh, K., and Nirenberg, S.  1995. A Situated Ontol-
ogy for Practical NLP, in Proceedings on the Work-
shop on Basic Ontological Issues in Knowledge 
Sharing at IJCAI-95. Montreal, Canada. 
Philpot, A., M. Fleischman, E.H. Hovy. 2003. Semi-
Automatic Construction of a General Purpose Ontol-
ogy.  Proceedings of the International Lisp Confer-
ence.  New York, NY. Invited. 
Stowell, T. 1981. Origins of Phrase Structure. PhD the-
sis, MIT, Cambridge, MA.  
Tapanainen, P. and T Jarvinen.  1997.  A non-projective 
dependency parser.  In the 5th Conference on Applied 
Natural Language Processing / Association for Com-
putational Linguistics, Washington, DC. 
White, J., and T. O?Connell.  1994.  The ARPA MT 
evaluation methodologies: evolution, lessons, and fu-
ture approaches.  Proceedings of the 1994 Confer-
ence, Association for Machine Translation in the 
Americas 
Walker, K., M. Bamba, D. Miller, X. Ma, C. Cieri, and 
G. Doddington 2003.  Multiple-Translation Arabic 
Corpus, Part 1. Linguistic Data Consortium (LDC) 
catalog num. LDC2003T18 & ISBN 1-58563-276-7. 
 
 
Role Description Grid Syntax Type 
Agent The entity that does the action Agent:  load 
Theme  with possessed 
SUBJ OBLIGATORY 
Theme The entity that is worked on Agent:  load 
Theme with possessed 
OBJ OBLIGATORY 
Possessed The entity controlled or owned Agent:  load 
Theme  with possessed 
PP OPTIONAL 
Table 1 :  A theta grid for the verb "load" 
 
 
Role and Definition Examples 
Agent:  Agents have the features of volition, sentience, causation and 
independent exist 
? Henry pushed/broke the vase. 
Instrument: An instrument should have causation but no volition. Its 
sentience and existence are not relevant. 
? The Hammer broke the vase. 
? She hit him with a baseball bat 
Experiencer: An experiencer has no causation but is sentient and 
exists independently. Typically an experiencer is the subject of verbs 
like feel, hear, see, sense, smell, notice, detect, etc. 
? John heard the vase shatter.   
? John shivered. 
Theme: The theme is typically causally affected or experiences a 
movement and/or change in state. The theme can appear as the infor-
mation in verbs like acquire, learn, memorize, read, study, etc. It can 
also be a thing, event or state (clausal complement). 
? John went to school.  
? John broke the vase.   
? John memorized his lines.  
? She buttered the bread with marga-
rine.   
Perceived: Refers to a perceived entity that isn't required by the verb 
but further characterizes the situation. The perceived is neither caus-
ally affected nor causative. It doesn't experience a movement or 
change in state. Its volition and sentience are irrelevant. Its existence 
is independent of an experiencer. 
? He saw the play.   
? He looked into the room.  
? The cat's fur feels good to John.   
? She imagined the movie to be loud.    
Predicate: Indicates new modifying information about other thematic 
roles. 
? We considered him a fool.   
? She acted happy.   
Source: Indicates where/when the theme started in its motion, or 
what its original state was, or where its original (possibly abstract) 
location/time was. 
? John left the house. 
Goal: Indicates where the theme ends up in its motion, or what its 
final state is, or where/when its final (possibly abstract) location/time 
is. It also can indicate the thing/event resulting from the verb's occur-
rence (the result). 
? John ran home.   
? John ran to the store.  
? John gave a book to Mary.   
? John gave Mary a book. 
Location: Indicates static locations---as opposed to a source or goal, 
i.e., the (possibly abstract) location of the theme or event. 
? He lived in France.   
? The water fills the box.   
? This cabin sleeps five people 
Time Indicates time. ? John sleeps for five hours.   
? Mary ate during the meeting. 
Beneficiary: Indicates the thing that receives the benefit/result of the 
event/state. 
? John baked the cake for Mary.   
? John baked Mary a cake.  
? An accident happened to him.   
Purpose: Indicates the purpose/reason behind an event/state ? He studied for the exam.  
? He searched for rabbits.  
Possessed: Indicates the possessed entity in verbs such as own, have, 
possess, fit, buy, and carry. 
? John has five bucks.  
? He loaded the cart with hay.   
? He bought it for five dollars 
Proposition: Indicates the secondary event/state ? He wanted to study for the exam. 
Modifier: Indicates a property of a thing such as color, taste, size, 
etc. 
? The red book sitting on the table is 
old.  
Null Indicates no thematic contribution. Typical examples are imper-
sonal it and there. 
? It was raining all morning in Miami. 
 
TABLE 2:  List of Theta Roles 
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 117?125,
Prague, June 2007. c?2007 Association for Computational Linguistics
ParaMor: Minimally Supervised Induction of Paradigm  
 Structure and Morphological Analysis 
 
Christian Monson, Jaime Carbonell, Alon Lavie, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA, USA 15213 
{cmonson, alavie+, jgc+, lsl+}@cs.cmu.edu 
Abstract 
Paradigms provide an inherent 
organizational structure to natural language 
morphology. ParaMor, our minimally 
supervised morphology induction 
algorithm, retrusses the word forms of raw 
text corpora back onto their paradigmatic 
skeletons; performing on par with state-of-
the-art minimally supervised morphology 
induction algorithms at morphological 
analysis of English and German. ParaMor 
consists of two phases. Our algorithm first 
constructs sets of affixes closely mimicking 
the paradigms of a language. And with 
these structures in hand, ParaMor then 
annotates word forms with morpheme 
boundaries. To set ParaMor?s few free 
parameters we analyze a training corpus of 
Spanish. Without adjusting parameters, we 
induce the morphological structure of 
English and German. Adopting the 
evaluation methodology of Morpho 
Challenge 2007 (Kurimo et al, 2007), we 
compare ParaMor?s morphological 
analyses with Morfessor (Creutz, 2006), a 
modern minimally supervised morphology 
induction system. ParaMor consistently 
achieves competitive F1 measures. 
1 Introduction 
Words in natural language (NL) have internal 
structure. Morphological processes derive new lex-
emes from old ones or inflect the surface form of 
lexemes to mark morphosyntactic features such as 
tense, number, person, etc. This paper address 
minimally supervised induction of productive natu-
ral language morphology from text. Minimally su-
pervised induction of morphology interests us both 
for practical and theoretical reasons. In linguistic 
theory, the morpheme is often defined as the 
smallest unit of language which conveys meaning. 
And yet, without annotating for meaning, recent 
work on minimally supervised morphology induc-
tion from written corpora has met with some suc-
cess (Creutz, 2006). We are curious how far this 
program can be pushed. From a practical perspec-
tive, minimally supervised morphology induction 
would help create morphological analysis systems 
for languages outside the traditional scope of NLP. 
However, to develop our method we induce the 
morphological structure of three well-understood 
languages, English, German, and Spanish. 
1.1 Inherent Structure in NL Morphology 
The approach we have taken to induce morpho-
logical structure has explicit roots in linguistic the-
ory. Cross-linguistically, natural language organ-
izes inflectional morphology into paradigms and 
inflection classes. A paradigm is a set of mutually 
exclusive operations that can be performed on a 
word form. Each mutually exclusive morphologi-
cal operation in a paradigm marks a lexeme for 
some set or cell of morphosyntactic features. An 
inflection class, meanwhile, specifies the proce-
dural details that a particular set of adherent lex-
emes follow to realize the surface form filling each 
paradigm cell. Each lexeme in a language adheres 
to a single inflection class for each paradigm the 
lexeme realizes. The lexemes belonging to an in-
flection class may have no relationship binding 
them together beyond an arbitrary morphological 
stipulation that they adhere to the same inflection 
class. But for this paper, an inflection class may 
117
also refer to a set of lexemes that inflect similarly 
for phonological or orthographic reasons. Working 
with text we intentionally blur phonology and or-
thography. 
A simple example will help illustrate paradigms, 
inflection classes, and the mutual exclusivity of 
cells. As shown in Table 1, all English verbs 
belong to a single common paradigm of five cells: 
One cell marks a verb for the morphosyntactic 
feature values present tense 3rd person, as in eats; 
another cell marks past tense, as in ate; a third cell 
holds a surface form typically used to mark 
progressive aspect, eating; a fourth produces a 
passive participle, eaten; and finally there is the 
unmarked cell, in this example eat.  
Aside from inflection classes each containing 
only a few irregular lexemes, such as that 
containing eat, there are no English verbal 
inflection classes that arbitrarily differentiate 
lexemes on purely morphological grounds. There 
are, however, several inflection classes that realize 
surface forms only for verbs with particular 
phonology or orthography. The ?silent-e? inflection 
class is one such. To adhere to the ?silent-e? 
inflection class a lexeme must fill the unmarked 
paradigm cell with a form that ends in an unspoken 
character e, as in dance. The other paradigm cells 
in the ?silent-e? inflection class are filled by 
applying orthographic rules such as:  
Progressive Aspect Cell ? replace the final e of 
the unmarked form with the string ing, 
dance  dancing  
Past Cell ? substitute ed, dance  danced  
Paradigm cells are mutually exclusive. In the Eng-
lish verbal paradigm, although English speakers 
can express progressive past actions with a 
grammatical construction, viz. was eating, there is 
no surface form of the lexeme eat that 
simultaneously fills both the progressive and the 
past cells of the verbal paradigm, *ateing. 
1.2 ParaMor 
Paradigms and inflection classes, the inherent 
structure of natural language morphology, form the 
basis of ParaMor, our minimally supervised 
morphological induction algorithm. In ParaMor?s 
first phase, we find sets of mutually exclusive 
strings which closely mirror the inflection classes 
of a language?although ParaMor does not 
differentiate between syncretic word forms of the 
same lexeme filling different paradigm cells, such 
as ed-suffixed forms which can fill either the past 
or the passive cells of English verbs. In ParaMor?s 
second phase we employ the structured knowledge 
contained within the discovered inflection classes 
to segment word forms into morpheme-like pieces.  
Languages employ a variety of morphological 
processes to arrive at grammatical word forms?
processes including suffix-, prefix-, and infixation, 
reduplication, and template filling. Furthermore, 
the application of word forming processes often 
triggers phonological (or orthographic) change, 
such a as the dropped final e of the ?silent-e? 
inflection class, see Table 1. Despite the wide 
range of morphological processes and their 
complicating concomitant phonology, a large caste 
of inflection classes, and hence paradigms, can be 
represented as mutually exclusive substring 
substitutions. In the ?silent-e? inflection class, for 
example, the word-final strings e.ed.es.ing can be 
substituted for one another to produce the surface 
forms that fill the paradigm cells of lexemes 
belonging to this inflection class. In this paper we 
focus on identifying word final suffix morphology. 
While we focus on suffixes, the methods we 
employ can be straightforwardly generalized to 
prefixes and ongoing work seeks to model 
sequences of concatenative morphemes. 
Inducing the morphology of a language from a 
naturally occurring text corpus is challenging. In 
languages with a rich morphological structure, sur-
face forms filling particular cells of an inflection 
class may be relatively rare. In the Spanish news-
wire text over which we developed ParaMor there 
are 50,000 unique types. Among these types, in-
Table 1: The English verbal paradigm, left col-
umn, and two inflection classes of the verbal 
paradigm. The verb eat fills the cells of its in-
flection class with the five surface forms 
shown in the second column. Verbs belonging 
to the ?silent-e? inflection class inflect follow-
ing the pattern of the third column. 
            Inflection Class Paradigm 
Cells ?eat? ?silent-e? 
Unmarked eat dance, erase, ? 
Present, 3rd eats dances, erases, ? 
Past Tense ate danced, erased, ? 
Progressive eating dancing, erasing, ? 
Passive eaten danced, erased, ? 
 
118
stances of first and second person verb forms are 
few. The suffix imos which fills the first person 
plural indicative present cell for the ir verbal in-
flection class of Spanish occurs on only 77 unique 
lexemes. And yet we aim to identify candidate in-
flection classes which closely model the true in-
flection classes of a language, covering as many 
inflectional paradigm cells as possible. 
Fortunately, we can leverage the paradigm struc-
ture of natural language morphology itself to retain 
many inflections which, because of data sparse-
ness, might be missed if considered in isolation. 
ParaMor begins with a recall-centric search for 
partial candidate inflection classes. Many of the 
candidates which result from this initial search are 
incorrect. But intermingled with the false positives 
are candidates which collectively model significant 
fractions of true inflection classes. Hence, Pa-
raMor?s next step is to cluster the initial partial 
candidate inflection classes into larger groups. This 
clustering effectively uses the larger correct initial 
candidates as nuclei to which smaller correct can-
didates accrete. With as many initial true candi-
dates as possible safely corralled with other candi-
dates covering the same inflection class, ParaMor 
completes the paradigm discovery phase by dis-
carding the large number of erroneous initially se-
lected candidate inflection classes. Finally, with a 
strong grasp on the paradigm structure, ParaMor 
straightforwardly segments the words of a corpus 
into morphemes. 
1.3 Related Work 
In this section we highlight previously proposed 
minimally supervised approaches to the induction 
of morphology that, like ParaMor, draw on the 
unique structure of natural language morphology. 
One facet of NL morphological structure com-
monly leveraged by morphology induction algo-
rithms is that morphemes are recurrent building 
blocks of words. Brent et al (1995), Goldsmith 
(2001), and Creutz (2006) emphasize the building 
block nature of morphemes when they each use 
recurring word segments to efficiently encode a 
corpus. These approaches then hypothesize that 
those recurring segments which most efficiently 
encode a corpus are likely morphemes. Another 
technique that exploits morphemes as repeating 
sub-word segments encodes the lexemes of a cor-
pus as a  character tree, i.e. trie, (Harris, 1955; 
Hafer and Weis, 1974), or as a finite state automa-
ton (FSA) over characters (Johnson, H. and Martin, 
2003; Altun and M. Johnson, 2001). A trie or FSA 
conflates multiple instances of a morpheme into a 
single sequence of states. Because the choice of 
possible succeeding characters is highly con-
strained within a morpheme, branch points in the 
trie or FSA are likely morpheme boundaries. Often 
trie similarities are used as a first step followed by 
further processing to identify morphemes (Schone 
and Jurafsky, 2001).  
The paradigm structure of NL morphology has 
also been previously leveraged. Goldsmith (2001) 
uses morphemes to efficiently encode a corpus, but 
he first groups morphemes into paradigm like 
structures he calls signatures. To date, the work 
that draws the most on paradigm structure is 
Snover (2002). Snover incorporates paradigm 
structure into a generative statistical model of 
morphology. Additionally, to discover paradigm 
like sets of suffixes, Snover designs and searches 
networks of partial paradigms. These networks are 
the direct inspiration for ParaMor?s morphology 
scheme networks described in section 2.1. 
2 ParaMor: Inflection Class Identification 
2.1 Search 
A Search Space: The first stage of ParaMor is a 
search procedure designed to identify partial in-
flection classes containing as many true productive 
suffixes of a language as possible. To search for 
these partial inflection classes we must first define 
a space to search over. In a naturally occurring 
corpus not all possible surface forms occur. In a 
corpus, each stem adhering to an inflection class 
will likely be observed in combination with only a 
subset of the suffixes in that inflection class. Each 
box in Figure 1 depicts a small portion of the em-
pirical co-occurrence of suffixes and stems from a 
Spanish newswire corpus of 50,000 types. Each 
box in this figure contains a list of suffixes at the 
top in bold, together with the total number, and a 
few examples (in italics), of stems that occurred in 
separate word forms with each suffix in that box. 
For example, the box containing the suffixes e, 
er?, ieron, and i? contains the stems deb and 
padec because the word forms debe, padece, de-
ber?, padecer?, etc. all occurred in the corpus. We 
call each possible pair of suffix and stem sets a 
scheme, and say that the e.er?.ieron.i? scheme 
covers the words debe, padece, etc. Note that a 
scheme contains both stems that occurred with ex-
actly the set of suffixes in that scheme, as well as 
119
stems that occurred with suffixes beyond just those 
in the scheme. For example, in addition to the four 
suffixes e, er?, ieron, and i?, the stem deb oc-
curred with the suffixes er and ido, as evident from 
the top left scheme e.er.er?.ido.ieron.i? which 
contains the stem deb. Intuitively, a scheme is a 
subset of the suffixes filling the paradigm cells of a 
true inflection class together with the stems that 
empirically occurred with that set of suffixes.  
The schemes in Figure 1 cover portions of the er 
and the ir Spanish verbal inflection classes. The 
top left scheme of the figure contains suffixes in 
the er inflection class, while the top center scheme 
contains suffixes in the ir inflection class. The six 
suffixes in the top left scheme and the six suffixes 
in the top center scheme are just a few of the 
suffixes in the full er and ir inflection classes. As 
is fairly common for inflection classes across 
languages, the sets of suffixes in the Spanish er 
and ir inflection classes overlap. That is, verbs that 
belong to the er inflection class can take as a suffix 
certain strings of characters that verbs belonging to 
the ir inflection class can also take. The suffixes 
that are unique to the er verb inflection class in the 
top left scheme are er and er?; while the unique 
suffixes for the ir class in the top center scheme are 
ir and ir?. In the third row of the figure, the 
scheme e.ido.ieron.i? contains only suffixes found 
in both the er and ir schemes. 
 While the example schemes in Figure 1 are cor-
rect and do occur in a real Spanish newswire cor-
pus, the schemes are atypically perfect. There is 
only one suffix appearing in Figure 1 that is not a 
true suffix of Spanish?azar in the upper right 
scheme. In unsupervised morphology induction we 
do not know a priori the correct suffixes of a lan-
guage. Hence, we form schemes by proposing can-
didate morpheme boundaries at every character 
boundary in every word, including the character 
boundary after the final character in each word 
form, to allow for empty suffixes. 
Schemes of suffixes and their exhaustively co-
occurring stems define a natural search space over 
partial inflection classes because schemes readily 
organize by the suffixes and stems they contain. 
We define a parent-child relationship between a 
parent scheme, P  and a child scheme C , when P  
contains all the suffixes that C  contains and when 
P  contains exactly one more suffix than C . In 
Figure 1, parent child relations are represented by 
solid lines connecting boxed schemes. The scheme 
e.er.er?.ido.ieron.i?, for example, is the parent of 
three depicted children in Figure 1, one of which is 
e.er.er?.ieron.i?.  
Our search strategy exploits a fundamental 
aspect of the relationship between parent and child 
schemes. Consider the number of stems in a parent 
scheme P  as compared to the number of stems in 
any one of its children C . Since P  contains all the 
suffixes which C  contains, and because P  only 
contains stems that occurred with every suffix in 
P , P  can at most contain exactly the stems C  
contains and typically will contain fewer. In the 
Spanish corpus from which the scheme network of 
Figure 1 was built, 32 stems occur in forms with 
each of the five suffixes e, er, er?, ieron, and i? 
attached. But only 28 of these 32 stems occur in 
yet another form involving ido?the stem deb did 
but the stems padec and romp did not, for example. 
A Search Strategy: To search for schemes 
which cover portions of the true inflection classes 
of a language, ParaMor?s search starts at the bot-
tom of the network. The lowest level in the scheme 
e.er.er?.ido.ieron.i? 
28: deb, escog, ofrec, roconoc, vend, ... 
e.ido.ieron.ir.ir?.i? 
28: asist, dirig, exig, ocurr, sufr, ... 
e.er?.ido.ieron.i? 
28: deb, escog, ... 
e.er.ido.ieron.i? 
46: deb, parec, recog... 
e.ido.ieron.ir?.i? 
28: asist, dirig, ... 
 
e.ido.ieron.ir.i? 
39: asist, bat, sal, ... 
e.er.er?.ieron.i? 
32: deb, padec, romp, ... 
e.ido.ieron.i? 
86: asist, deb, hund,... 
e.er?.ieron.i? 
32: deb, padec, ... 
er.ido.ieron.i? 
58: ascend, ejerc, recog, ... 
ido.ieron.ir.i? 
44: interrump, sal, ... 
Figure 1: A small portion of a morphology scheme network?our search space of partial empirical in-
flection classes. This network was built from a Spanish Newswire corpus of 50,000 types, 1.26 million 
tokens. Each box contains a scheme. The suffixes of each scheme appear in bold at the top of each box. 
The total number of adherent stems for each scheme, together with a few exemplar stems, is in italics. 
Stems are underlined if they do not appear in any parent shown in this figure. 
azar.e.ido.ieron.ir.i? 
1: sal 
120
network consists of schemes which contain exactly 
one suffix together with all the stems that occurred 
in the corpus with that suffix attached. ParaMor 
considers each one-suffix scheme in turn beginning 
with that scheme containing the most stems, work-
ing toward schemes containing fewer. From each 
bottom scheme, ParaMor follows a single greedy 
upward path from child to parent. As long as an 
upward path takes at least one step, making it to a 
scheme containing two or more alternating suf-
fixes, our search strategy accepts the terminal 
scheme of the path as likely modeling a portion of 
a true inflection class. 
Each greedily chosen upward step is based on 
two criteria. The first criterion considers the 
number of adherent stems in the current scheme as 
compared to its parents? adherent sizes. A variety 
of statistics could judge the stem-strength of parent 
schemes: ranging from simple ratios through 
(dis)similarity measures, such as the dice 
coefficient or mutual information, to full fledged 
statistical tests. After experimenting with a range 
of such statistics we found, somewhat surprisingly, 
that measuring the ratio of parent stem size to child 
stem size correctly identifies parent schemes which 
contain only true suffixes just as consistently as 
more sophisticated tests. While a full report of our 
experiments is beyond the scope of this paper, the 
short explanation of this behavior is data 
sparseness. Many upward search steps start from 
schemes containing few stems. And when little 
data is available no statistic is particularly reliable.  
Parent-child stem ratios have two additional 
computational advantages over other measures. 
First, they are quick to compute and second, the 
parent with the largest stem ratio is always that 
parent with the most stems. So, being greedy, each 
search step simply moves to that parent, P , with 
the most stems, as long as the parent-child stem 
ratio to P  is large. The threshold above which a 
stem ratio is considered large enough to warrant an 
upward step is a free parameter. As the goal of this 
initial search stage is to identify schemes contain-
ing as wide a variety of productive suffixes as pos-
sible, we want to set the parent-child stem ratio 
threshold as low as possible. But a ratio threshold 
that is too small will allow search paths to schemes 
containing unproductive and spurious suffixes. In 
practice, for Spanish, we have found that setting 
the parent-child stem ratio cutoff much below 0.25 
results in schemes that begin to include only mar-
ginally productive derivational suffixes. For this 
paper we leave the parent-child stem ratio cutoff 
parameter at 0.25.  
Alone, stem strength assessments of parent 
schemes, such as parent-child stem ratios, falter as 
a search path nears the top of the morphology 
scheme network. Monotonically decreasing adher-
ent stem size causes statistics that assess parents? 
stem-strength to become less and less reliable. 
Hence, the second criterion governing each search 
step helps to halt upward search paths before judg-
ing parents? worth becomes impossible. While 
there are certainly many possible stopping criteria, 
ParaMor?s policy stops each upward search path 
when there is no parent scheme with more stems 
than it has suffixes. We devised this halting condi-
tion for two reasons. First, requiring each path 
scheme to contain more stems than suffixes attains 
high suffix recall. High recall results from setting a 
low bar for upward movement at the bottom of the 
network. Search paths which begin from schemes 
whose single suffix is rare in the text corpus can 
often take one or two upward search steps and 
reach a scheme containing the necessary three or 
four stems. Second, this halting criterion requires 
the top scheme of search paths that climb high in 
the network to contain a comparatively large num-
ber of stems. Reigning in high-reaching search 
paths before the stem count falls too far, captures 
path-terminal schemes which cover a large number 
of word types. In the second stage of ParaMor?s 
inflection class identification phase these larger 
terminal schemes effectively vacuum up the useful 
smaller paths that result from the more rare suf-
fixes. Figure 2 contains examples of schemes se-
lected by ParaMor?s initial search. 
To evaluate ParaMor at paradigm identification, 
we hand compiled an answer key of the inflection 
classes of Spanish. This answer key contains nine 
productive inflection classes. Three contain the 
suffixes of the ar, er, and ir verbal inflection 
classes. There are two orthographically differenti-
ated inflection classes for nouns in the answer key: 
one for nouns that form the plural by adding s, and 
one for nouns that take es. Adjectives in Spanish 
inflect for gender and number. Arguably, gender 
and number each constitute separate paradigms, 
each with two cells. But here we conflated these 
into a single inflection class with four cells. Fi-
nally, there are three inflection classes in our an-
swer key covering Spanish clitics. Spanish verbal 
clitics behave orthographically as agglutinative 
sequences of suffixes.  
121
In a corpus of Spanish newswire text of 50,000 
types and 1.26 million tokens, the initial search 
identifies schemes containing 92% of all ideal in-
flectional suffixes of Spanish, or 98% of the ideal 
suffixes that occurred at least twice in the corpus. 
There are selected schemes which contain portions 
of each of the nine inflection classes in the answer 
key. The high recall of the initial search comes, of 
course, at the expense of precision. While there are 
nine inflection-classes and 87 unique suffixes in 
the hand-built answer key for Spanish, 8339 
schemes are selected containing 9889 unique can-
didate suffixes.  
2.2 Clustering Partial Inflection Classes 
While the third step of inflection class identifica-
tion, discussed in Section 2.3, directly improves 
the initial search?s low precision by filtering out 
bogus schemes, the second step, described here, 
conflates selected schemes which model portions 
of the same inflection class. Consider the fifth and 
twelfth schemes selected by ParaMor from our 
Spanish corpus, as shown in Figure 2. Both of 
these schemes contain a large number of suffixes 
from the Spanish ar verbal inflection class. And 
while each contains many overlapping suffixes, 
each possesses correct suffixes which the other 
does not. Meanwhile, the 1591st selected scheme 
contains four suffixes of the ir verbal inflection 
class, including the only instance of ir? that occurs 
in any selected scheme. Containing only six stems, 
the 1591st scheme could accidentally be filtered out 
during the third phase of inflection class identifica-
tion. Hence, the rationale for clustering initial se-
lected schemes is two fold. First, by consolidating 
schemes which cover portions of the same inflec-
tion class we produce sets of suffixes which more 
closely model the paradigm structure of natural 
language morphology. And, second, corralling cor-
rect schemes safeguards against losing unique suf-
fixes. 
The clustering of schemes presents two unique 
challenges. First, we must avoid over-clustering 
schemes which model distinct inflection classes. 
As noted in Section 2.1, it is common, cross-
linguistically, for the suffixes of inflection classes 
to overlap. Looking at Figure 2, we must be careful 
not to merge the 209th selected scheme, which 
models a portion of the er verbal inflection class, 
with the 1591st selected scheme, which models the 
ir class?despite these schemes sharing two suf-
fixes, ido and idos. As the second challenge, the 
many small schemes which the search strategy 
produces act as distractive noise during clustering. 
While small schemes containing correct suffixes 
do exist, e.g. the 1591st scheme, the vast majority 
of schemes containing few stems and suffixes are 
incorrect collections of word final strings that hap-
pen to occur in corpus word forms attached to a 
small number of shared initial strings. ParaMor?s 
clustering algorithm should, for example, avoid 
placing ?.s and ?.ipo, respectively the 1st and 
1590th selected schemes, in the same cluster. Al-
though ?.ipo shares the null suffix with the valid 
nominal scheme ?.s, the string ?ipo? is not a mor-
phological suffix of Spanish. 
To form clusters of related schemes while ad-
dressing both the challenge of observing a lan-
guage?s paradigm structure as well as the challenge 
of merging in the face of many small incorrectly 
selected schemes, ParaMor adapts greedy hierar-
chical agglomerative clustering. We modify vanilla 
bottom-up clustering by placing restrictions on 
which clusters are allowed to merge. The first re-
striction helps ensure that schemes modeling dis-
tinct but overlapping inflection classes remain 
separated. The restriction: do not place into the 
same cluster suffixes which share no stem in the 
corpus. This restriction retains separate clusters for 
separate inflection classes because a lexeme?s stem 
Figure 2: The suffixes of some schemes selected 
by the initial search over a Spanish corpus of 
50,000 types. While some selected schemes 
contain large numbers of correct suffixes, such 
as the 1st, 2nd, 5th, 12th, 209th, and 1591st selected 
schemes; many others are incorrect collections 
of word final strings. 
 1) ?.s 5501 stems 
 2) a.as.o.os 892 stems 
... 
 5) a.aba.aban.ada.adas.ado.ados.an.ando.   
ar.aron.arse.ar?.ar?n.? 25 stems 
... 
 12) a.aba.ada.adas.ado.ados.an.ando.ar.   
aron.ar?.ar?n.e.en.? 21 stems 
... 
 209) e.er.ida.idas.ido.idos.imiento.i? 9 stems 
... 
1590) ?.ipo 4 stems 
1591) ido.idos.ir.ir? 6 stems 
1592) ?.e.iu 4 stems 
1593) iza.izado.izan.izar.izaron.izar?n.iz? 
... 8 stems 
122
occurring with suffixes unique to that lexeme?s 
inflection class will not occur with suffixes unique 
to some other inflection class.  
Alone, requiring all pairs of suffixes in a cluster 
to occur in the corpus with some common stem 
will not prevent small bogus schemes, such as 
?.ipo from attaching to correct schemes, such as 
?.s?the ipo.s scheme contains two ?stems,? the 
word form initial strings ?ma? and ?t?. And so a 
second restriction is required. This second restric-
tion employs a heuristic specifically adapted to 
ParaMor?s initial search strategy. As discussed in 
Section 2.1, in addition to many schemes which 
contain only few suffixes, ParaMor?s initial net-
work search also identifies multiple overlapping 
schemes containing significant subsets of the suf-
fixes in an inflection class. The 5th, 12th, and 209th 
selected schemes of Figure 2 are three such larger 
schemes. ParaMor restricts cluster merges heuristi-
cally by requiring at least one large scheme for 
each small scheme the cluster contains, where we 
measure the size of a scheme as the number of 
unique word forms it covers. The threshold size 
above which schemes are considered large is the 
second of ParaMor?s two free parameters. The 
scheme size threshold is reused during ParaMor?s 
filtering stage. We discuss the unsupervised proce-
dure we use to set the size threshold when we pre-
sent the details of cluster filtering in Section 2.3. 
We have found that with these two cluster re-
strictions in place, the particular metric we use to 
measure the similarity of scheme-clusters does not 
significantly affect clustering. For the experiments 
we report here, we measure the similarity of 
scheme-clusters as the cosine between the sets of 
all possible stem-suffix pairs the clusters contain. 
A stem-suffix pair occurs in a cluster if some 
scheme belonging to that cluster contains both that 
stem and that suffix. With these adaptations, we 
allow agglomerative clustering to proceed until 
there are no more clusters that can legally be 
merged.  
2.3 Filtering of Inflection Classes 
With most valid schemes having found a safe ha-
ven in a cluster with other schemes modeling the 
same inflection class, we turn our attention to im-
proving scheme-cluster precision. ParaMor applies 
a series of filters, culling out unwanted scheme-
clusters. The first filter is closely related to the 
cluster restriction on scheme size discussed in Sec-
tion 2.2. ParaMor discards all unclustered schemes 
falling below the size threshold used during clus-
tering. Figure 3 graphs the number of Spanish clus-
ters which survive this size-based filtering step as 
the threshold size is varied. Figure 3 also contains 
a plot of the recall of unique Spanish suffixes as a 
function of this threshold. As the size threshold is 
increased the number of remaining clusters quickly 
drops. But suffix recall only slowly falls during the 
steep decline in cluster count, indicating ParaMor 
discards mostly bogus schemes containing illicit 
suffixes. Because recall is relatively stable, the ex-
act size threshold we use should have only a minor 
effect on ParaMor?s final morphological analyses. 
In fact, we have not fully explored the ramifica-
tions various threshold values have on the final 
morphological word segmentations, but have sim-
ply picked a reasonable setting, 37 covered word 
types. At this threshold, the number of scheme-
clusters is reduced by more than 98%, while the 
number of unique candidate suffixes in any cluster 
is reduced by more than 85%. Note that the initial 
number of selected schemes, 8339, falls outside the 
scale of Figure 3. 
Of the scheme-clusters which remain after size 
based filtering is complete, by far the largest cate-
gory of incorrect clusters contains schemes which, 
like the 1593rd selected scheme, shown in Figure 2, 
incorrectly hypothesize morpheme boundaries one 
or more characters to the left of the true boundary. 
To filter out these incorrectly segmented clusters 
we use a technique inspired by Harris (1955). For 
each initial string common to all suffixes in the 
cluster, for each scheme in the cluster, we examine 
the network scheme containing the suffixes formed 
by stripping the initial string from the scheme?s 
Figure 3: The # of clusters and their recall of 
unique Spanish suffixes as the scheme-cluster 
size cutoff is varied. The value of each function 
at the threshold we use in all experiments re-
ported in this paper is that of the larger symbol. 
0
200
400
600
800
1000
0 50 100 150
Scheme or Cluster Size
# 
o
f C
lu
st
er
s
0
0.2
0.4
0.6
0.8
1
R
ec
a
ll
# of Clusters
Recall
 
123
suffixes. We then measure the entropy of leftward 
trie characters of the stripped scheme. If the en-
tropy is large, then the character stripped scheme is 
likely at a morpheme boundary and the original 
scheme is likely modeling an incorrect morpheme 
boundary. This algorithm would throw out the 
1593rd selected scheme because the stems in the 
scheme a.ado.an.ar.aron.ar?n.? end in a wide 
variety of characters, yielding high trie entropy, 
and signaling a likely morpheme boundary. 
Because we apply morpheme boundary filtering 
after we have clustered, the redundancy of the 
many schemes in the cluster makes this filter quite 
robust, letting us set the cutoff parameter as low as 
we like avoiding another free parameter. 
2.4 Segmentation and Evaluation 
Word segmentation is our final step of morpholo-
gical analysis. ParaMor?s current segmentation 
algorithm is perhaps the most simple paradigm 
inspired segmentation algorithm possible. Essen-
tially, ParaMor strips off suffixes which likely par-
ticipate in a paradigm. To segment any word, w , 
ParaMor identifies all scheme-clusters that contain 
a non-empty suffix that matches a word final string 
of w . For each such matching suffix, Cf ? , 
where C is the cluster containing f , we strip f  
from w  obtaining a stem t . If there is some sec-
ond suffix Cf ??  such that ft ?.  is a word form 
found in either of the training or the test corpora, 
then ParaMor proposes a segmentation of w  be-
tween t  and f . ParaMor, here, identifies f  and 
f ?  as mutually exclusive suffixes from the same 
paradigm. If ParaMor finds no complex analysis, 
then we propose w  itself as the sole analysis of the 
word. Note that for each word form, ParaMor may 
propose multiple separate segmentation analyses 
each containing a single proposed stem and suffix. 
To evaluate ParaMor?s morphological segmenta-
tions we follow the methodology of Morpho Chal-
lenge 2007 (Kurimo et al, 2007), a minimally su-
pervised morphology induction competition. Word 
segmentations are evaluated in Morpho Challenge 
2007 by comparing against hand annotated mor-
phological analyses. The correctness of proposed 
morphological analyses is computed in Morpho 
Challenge 2007 by comparing pairs of word forms 
which share portions of their analyses. Recall is 
measured by first sampling pairs of words from the 
answer analyses which share a stem or morphosyn-
tactic feature and then noting if that pair of word 
forms shares a morpheme in any of their proposed 
analyses. Precision is measured analogously, sam-
pling morpheme-sharing pairs of words from the 
proposed analyses and noting if that pair of words 
shares a feature in any correct analysis of those 
words.  
We evaluate ParaMor on two languages not 
examined during the development of ParaMor?s 
induction algorithms: English and German. And 
we evaluate with each of these two languages at 
two tasks:  
1. Analyzing inflectional morphology alone 
2. Jointly analyzing inflectional and derivational 
morphology.  
We constructed Morpho Challenge 2007 style 
answer keys for each language and each task using 
the Celex database (Burnage, 1990). The English 
and German corpora we test over are the corpora 
available through Morpho Challenge 2007. The 
English corpus contains nearly 385,000 types, 
while the German corpus contains more than 1.26 
million types. ParaMor induced paradigmatic 
scheme-clusters over these larger corpora by 
reading just the top 50,000 most frequent types. 
But with the scheme-clusters in hand, ParaMor 
segmented all the types in each corpus. 
We compare ParaMor to Morfessor v0.9.2 
(Creutz, 2006), a state-of-the-art minimally super-
vised morphology induction algorithm. Morfessor 
has a single free parameter. To make for stiff com-
petition, we report results for Morfessor at that pa-
rameter setting which maximized F1 on each sepa-
rate test scenario. We did not vary the two free pa-
rameters of ParaMor, but hold each of ParaMor?s 
parameters at a setting which produced reasonable 
Spanish suffix sets, see sections 2.1-2.2. Table 2 
contains the evaluation results. To estimate the 
variance of our experimental results we measured 
Morpho Challenge 2007 style precision, recall, and 
F1 on multiple non-overlapping pairs of 1000 fea-
ture-sharing words.  
Neither ParaMor nor Morfessor arise in Table 2 
as clearly superior. Each algorithm outperforms the 
other at F1 in some scenario. Examining precision 
and recall is more illuminating. ParaMor attains 
particularly high recall of inflectional affixes for 
both English and German. We conjecture that Pa-
raMor?s strong performance at identifying inflec-
tional morphemes comes from closely modeling 
the natural paradigm structure of language. Con-
versely, Morfessor places its focus on precision 
and does not rely on any property exclusive to in-
flectional (or derivational) morphology. Hence, 
124
Morfessor attains high precision with reasonable 
recall when graded against an answer key contain-
ing both inflectional and derivational morphology. 
We are excited by ParaMor?s strong 
performance and are eager to extend our algorithm. 
We believe the precision of ParaMor?s simple 
segmentation algorithm can be improved by 
narrowing down the proposed analyses for each 
word to the most likely. Perhaps ParaMor and 
Morfessor?s vastly different strategies for 
morphology induction could be combined into a 
hybrid strategy more successful than either alone. 
And ambitiously, we hope to extend ParaMor to 
analyze languages with agglutinative sequences of 
affixes by generalizing the definition of a scheme.  
Acknowledgements 
The research reported in this paper was funded in 
part by NSF grant number IIS-0121631. 
References 
Altun, Yasemin, and Mark Johnson. "Inducing 
SFA with -Transitions Using Minimum 
Description Length." Finite State Methods in 
Natural Language Processing Workshop at 
ESSLLI Helsinki: 2001.  
Brent, Michael R., Sreerama K. Murthy, and 
Andrew Lundberg. "Discovering Morphemic 
Suffixes: A Case Study in MDL Induction." The 
Fifth International Workshop on Artificial Intel-
ligence and Statistics Fort Lauderdale, Florida, 
1995.  
Burnage, Gavin. Celex?A Guide for Users. 
Springer, Centre for Lexical information, 
Nijmegen, the Netherlands, 1990. 
Creutz, Mathias. ?Induction of the Morphology of 
Natural Language: Unsupervised Morpheme 
Segmentation with Application to Automatic 
Speech Recognition.? Ph.D. Thesis in Computer 
and Information Science, Report D13. Helsinki: 
University of Technology, Espoo, Finland, 2006. 
Goldsmith, John. "Unsupervised Learning of the 
Morphology of a Natural Language." Computa-
tional Linguistics 27.2 (2001): 153-198.  
Hafer, Margaret A., and Stephen F. Weiss. "Word 
Segmentation by Letter Successor Varieties." 
Information Storage and Retrieval 10.11/12 
(1974): 371-385. 
Harris, Zellig. "From Phoneme to Morpheme." 
Language 31.2 (1955): 190-222. Reprinted in 
Harris 1970. 
Harris, Zellig. Papers in Structural and 
Transformational Linguists. Ed. D. Reidel, 
Dordrecht 1970. 
Johnson, Howard, and Joel Martin. "Unsupervised 
Learning of Morphology for English and Inuk-
titut." Human Language Technology Conference 
/ North American Chapter of the Association for 
Computational Linguistics (HLT-NAACL). 
Edmonton, Canada: 2003. 
Kurimo, Mikko, Mathias Creutz, and Matti 
Varjokallio. ?Unsupervised Morpheme Analysis 
? Morpho Challenge 2007.? March 26, 2007. 
<http://www.cis.hut.fi/morphochallenge2007/> 
Schone, Patrick, and Daniel Jurafsky. "Know-
ledge-Free Induction of Inflectional Morpho-
logies." North American Chapter of the 
Association for Computational Linguistics 
(NAACL). Pittsburgh, Pennsylvania: 2001. 183-
191. 
Snover, Matthew G. "An Unsupervised Knowledge 
Free Algorithm for the Learning of Morphology 
in Natural Languages." Sever Institute of Tech-
nology, Computer Science Saint Louis, Mis-
souri: Washington University, M.S. Thesis, 
2002. 
Table 2: ParaMor segmentations compared to Morfessor?s (Creutz, 2006) evaluated for Precision, Recall, 
F1, and standard deviation of F1, , in four scenarios. Segmentations over English and German are each 
evaluated against correct morphological analyses consisting, on the left, of inflectional morphology 
only, and on the right, of both inflectional and derivational morphology. 
 Inflectional Morphology Only Inflectional & Derivational Morphology 
 English German English German 
 P R F1  P R F1  P R F1  P R F1  
Morfessor 53.3 47.0 49.9 1.3 38.7 44.2 41.2 0.8 73.6 34.0 46.5 1.1 66.9 37.1 47.7 0.7 
ParaMor 33.0 81.4 47.0 0.9 42.8 68.6 52.7 0.8 48.9 53.6 51.1 0.8 60.0 33.5 43.0 0.7 
 
125
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 87?96,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The North American Computational Linguistics Olympiad (NACLO) 
 
 
Dragomir R. Radev Lori S. Levin Thomas E. Payne 
SI, EECS, and Linguistics Language Technologies Institute Department of Linguistics 
University of Michigan Carnegie-Mellon University University of Oregon 
radev@umich.edu lsl@cs.cmu.edu tpayne@uoregon.edu 
 
 
 
 
 
 
Abstract 
1 Introduction 
NACLO (North American Computational Linguis-
tics Olympiad) is an annual Olympiad-style contest 
for high school students, focusing on linguistics, 
computational linguistics, and language technolo-
gies. 
The goal of NACLO is to increase participation 
in these fields by introducing them before students 
reach college. Since these subjects are not nor-
mally taught in high school, we do not expect stu-
dents to have any background of these areas before 
the contest.  The contest consists of self-contained 
problems that can be solved with analytical think-
ing, but in the course of solving each problem, the 
students learn something about a language, culture, 
linguistic phenomenon, or computational tool.  
The winners of NACLO are eligible to partici-
pate in the International Linguistics Olympiad as 
part of the US team. 
1.1 History of the LO and ILO 
The International Olympiad in Linguistics is one 
of twelve international Science Olympiads (the 
others include Mathematics, Physics, Chemistry, 
Biology, Informatics, Philosophy, Astronomy, Ge-
ography, and Earth Science). It has existed since 
2003 and has, so far, been held exclusively in 
Europe (Russia, Estonia, Bulgaria, and the Nether-
lands). ILO 2007 took place in Zelenogorsk near 
St. Petersburg, Russia whereas ILO 2008 will be in 
Slantchev Bryag near Burgas, Bulgaria. ILO 2009 
will be held in Poland.  
Individual national linguistics Olympiads have 
been held in Russia since 1965 (based on an initia-
tive by Andrey Zaliznyak) and in other countries 
more recently1. Recently, a collection of problems 
from different decades appeared in Russian (Be-
likov et al, 2007). 
1.2 Linguistics Contests in the US 
Thomas Payne pioneered LO-style competitions 
in the USA by organizing three consecutive con-
tests for middle and high school students in the 
Eugene, Oregon area in 1998-2000. In the course 
of publicizing NACLO, we have discovered that 
other local linguistics contests have taken place in 
Tennessee, San Jose, and New York City. 
1.3 Origin of NACLO 
NACLO began with a planning workshop 
funded by NSF in September 2006. The attendees 
included faculty and graduate students from about 
ten universities as well as representatives from 
NSF and ACL.  Two high school teachers were 
present.  The workshop opened with presentations 
from organizers of other Olympiads and contests in 
linguistics and computer programming. In particu-
lar we received excellent advice from Ivan Derz-
hanski, representing the International Linguistics 
Olympiad, and Boris Iomdin, representing the 
Moscow Olympiad. The remainder of the work-
shop dealt with scheduling the first contest, elect-
                                                          
1
 The first author of this paper participated in the Bulgarian 
national LO in the early 1980s. 
87
ing committee chairs, and making organizational 
decisions. 
1.4 Pedagogical goals 
We have two goals in organizing NACLO.  We 
want to increase broad participation and diversity 
in all language-related careers.  We want every 
student to have a fun and educational experience 
and have a positive attitude toward taking linguis-
tics and language technologies courses in college.  
However, we also want to conduct a talent search 
for the most promising future researchers in our 
field.     NACLO uses two mechanisms to be sure 
that we reach all levels of participation.  The first 
mechanism is to separate an open round with easier 
problems from an invitation-only round with 
harder problems.  The second mechanism is related 
to grading the problems.  Forty percent of the score 
is for a correct answer and sixty percent is for ex-
plaining the answer.   The students who write the 
most insightful explanations are the focus of our 
talent search.    
 
When publicizing NACLO in high schools we 
have been focusing on certain aspects of linguistics 
and computer science.  With respect to linguistics, 
we emphasize that languages have rules and pat-
terns that native speakers are not aware of; that 
there are procedures by which these rules and pat-
terns can be discovered in your own language; and 
that the same procedures can be used to discover 
rules and patterns in languages other than your 
own.  With respect to computer science the term 
computational thinking has been coined (Wing 
2006) to refer to those parts of the field that are not 
about computers or programming:  thinking algo-
rithmically, using abstraction to model a problem, 
structuring and reducing a search space, etc.   
1.5 Organization at the national level 
NACLO has two co-chairs, currently Lori 
Levin, Carnegie Mellon University, and Thomas 
Payne, University of Oregon.  Dragomir Radev is 
the program chair and team coach.  Amy Troyani, 
a high school teacher with board certification, is 
the high school liaison and advisor on making the 
contest appropriate and beneficial to high school 
students. 
NACLO has several committees.  James Puste-
jovsky currently chairs the sponsorship committee.  
The other committees are currently unchaired, al-
though we would like to thank William Lewis (out-
reach and publicity) and Barbara Di Eugenio 
(followup) for chairing them in the first year.  
NACLO is not yet registered as a non-profit or-
ganization and does not yet have a constitution.  
We would welcome assistance in these areas. 
The national level organization provides materi-
als that are used at many local sites.  The materials 
include a comprehensive web site 
(http://www.naclo.cs.cmu.edu), practice 
problems, examples of flyers and press releases, 
PowerPoint presentations for use in high schools, 
as well as contest booklets from previous competi-
tions. 
The contest is held on the same day in all loca-
tions (universities and "online" sites as described 
below).  In 2007 there was a single round with 195 
participants.  In 2008 there was an open round with 
763 participants and an invitation-only round with 
115 participants.  Grading is done centrally.  Each 
problem is graded at one location to ensure consis-
tency. 
Three national prizes are awarded for first, sec-
ond, and third place. National prizes are also given 
for the best solution to each problem. Local hosts 
can also award prizes for first, second, and third 
place at their sites based on the national scores. 
1.6 Funding 
The main national expenses are prizes, planning 
meetings, and the trip to the International Linguis-
tics Olympiad (ILO).  The trip to the ILO is the 
largest expense, including airfare for eight team 
members (two teams of four), a coach, and two 
chaperones.  The national level sponsors are the 
National Science Foundation (2007, 2008), Google 
(2007, 2008), Cambridge University Press (2007, 
2008), and the North American Chapter of the As-
sociation for Computational Linguistics (2007). 
The organizers constantly seek additional sponsors. 
1.7 Publicity before the contest 
At the national level, NACLO is publicized 
through its own web site as well as on LinguistList 
and Language Log.  From there, word spreads 
through personal email and news groups. No press 
releases have been picked up by national papers 
that we know of.  Local level publicity depends on 
the organization of local schools and the hosting 
88
university's high school outreach programs. In 
Pittsburgh, publicity is facilitated by a central mail-
ing list for gifted program coordinators in the city 
and county. Some of the other local organizers (in-
cluding James Pustejosvky at Brandeis, Alina 
Johnson at the University of Michigan and Barry 
Schiffman at Columbia University as well as sev-
eral others) sent mail to literally hundreds of high 
schools in their areas. Word of mouth from the 
2007 contest also helped reach out to more places.  
1.8 Registration 
NSF REU-funded Justin Brown at CMU created 
an online registration site for the 2008 contest 
which proved very helpful. Without such a site, the 
overhead of dealing with close to 1,000 students, 
teachers, and other organizers would have been 
impossible. 
1.9 Participation of graduate and under-
graduate students 
Graduate and undergraduate students participate 
in many activities including:  web site design, vis-
iting high schools, formulating problems, testing 
problems, advising on policy decisions, and facili-
tating local competitions. 
2 Problem selection  
We made a difficult decision early on not to re-
quire knowledge of linguistics, programming or 
mathematics.  Requiring these subjects would have 
reduced diversity in our pool of contestants as well 
as its overall size.   Enrollment in high school pro-
gramming classes has dropped, perhaps because of 
a perception that programming jobs are not inter-
esting.  NACLO does not require students to know 
programming, but by introducing a career option, it 
gives them a reason to take programming classes 
later. 
2.1 Problem types 
The NACLO problem sets include two main 
categories of problems: ?traditional? and ?compu-
tational/formal?. The ILO includes mostly tradi-
tional problems which include translations from 
unknown languages, glyph decoding, calendar sys-
tems, kinship systems, mathematical expressions 
and counting systems, among others. The other 
category deals with linguistic phenomena (often in 
English) as well as algorithms and formal analyses 
of text. 
2.2 Problem committee 
A problem committee was formed each year to 
work on the creation, pre-testing, and grading of 
problems. The members in 2007 included Emily 
Bender, John Blatz, Ivan Derzhanski, Jason Eisner, 
Eugene Fink, Boris Iomdin, Mahesh Joshi, Anagha 
Kulkarni, Will Lewis, Patrick Littell, Ruslan Mit-
kov, Thomas Payne, James Pustejovsky, Roy 
Tromble, and Dragomir Radev (chair). In 2008, the 
following people were members: Emily Bender, 
Eric Breck, Lauren Collister, Eugene Fink, Adam 
Hesterberg, Joshua Katz, Stacy Kurnikova, Lori 
Levin, Will Lewis, Patrick Littell, David 
Mortensen, Barbara Partee, Thomas Payne, James 
Pustejovsky, Richard Sproat, Todor Tchervenkov, 
and Dragomir Radev (chair). 
2.3 Problem pool 
At all times, the problem committee maintains a 
pool of problems which are constantly being 
evaluated and improved. Professional linguists and 
language technologists contribute problems or 
problem ideas that reflect cutting-edges issues in 
their disciplines. These are edited and tested for 
age appropriateness,  and the data are thoroughly 
checked with independent experts. 
2.4 Booklets 
The three booklets (one for 2007 and two for 
2008) were prepared using MS Publisher. Addi-
tionally, booklets with solutions were prepared in 
MS Word. All of these are available from the 
NACLO web site. 
2.5 List of problems 
This is the list of problems for NACLO 2007 (8 
problems) and 2008 (12 problems). They can be 
divided into two categories: traditional (2007: C, 
D, G and 2008: A, C, D, E, G, J, K) and for-
mal/computational (2007: A, B, E, F, H and 2008: 
B, F, H, I, L). The traditional problems addressed 
topics such as phonology, writing systems, calen-
dar systems, and cognates, among others. The 
other category included problems on stemming, 
89
finite state automata, clustering, sentence similarity 
identification, and spectrograms. 
 
 
2007 
 
A. English (Molistic) 
B. English (Encyclopedia) 
C. Ancient Greek 
D. Hmong 
E. English (Verb forms) 
F. English (Spelling correction) 
G. Huishu (Phonology) 
H. English (Sentence processing) 
 
2008 (A-E Open; F-L Invitational) 
 
A. Apinaye (Brazil) 
B. Hindi 
C. Ilocano (Philippines) 
D. Swedish and Norwegian 
E. Aymara (South America) 
F. Japanese 
G. Manam Pile (Papua New Guinea) 
H. English (Stemming) 
I. Rotokas (Automata; Bougainville Island) 
J. Irish 
K. Mayan (Calendar) 
L. English (Spectrograms) 
 
Figure 1: List of languages used in NACLO 2007 and 
2008. 
3 Contest administration  
NACLO is run in a highly distributed fashion and 
involves a large number of sites across the USA in 
Canada. 
3.1 Local administration 
NACLO is held at hosting universities and also 
"online".  The online category includes students 
who cannot get to one of the hosting universities, 
but instead are monitored by a teacher at a conven-
ient location, usually the student's high school.  
There were three hosting universities (Carnegie-
Mellon, Brandeis, and Cornell) in 2007 and thir-
teen hosting universities (the three above + U. 
Michigan, U. Illinois, U. Oregon, Columbia, Mid-
dle Tennessee State, San Jose State, U. Wisconsin, 
U. Pennsylvania, U. Ottawa, and U. Toronto) in 
2008.  Any university in the US or Canada may 
host NACLO.  Local organizers are responsible for 
providing a room for the contest, contacting high 
local high schools, and facilitating the contest on 
the specified contest date.  Local organizers may 
decide on the number of participants.  The number 
of participants at the 2008 sites ranged from a 
handful to almost 200 (CMU-Pitt). 
Local organizers may choose their level of in-
vestment of time and money.  They may spend 
only a few hours recruiting participants from one 
or two local high schools and may spend a small 
amount of money on school visits and copying.  
But they may also run large scale operations in-
cluding extensive fundraising and publicity.  The 
site with the largest local participation, Carnegie 
Mellon/University of Pittsburgh, donated adminis-
trative staff time, invested hundreds of volunteer 
hours, and raised money for snacks and souvenirs 
from local sponsors2. The CMU-Pitt site also hosts 
a problem club for faculty and students where 
problems are proposed, fleshed out, and tested.  At 
the University of Oregon, a seminar course was 
taught on Language Task Creation (formulation of 
problems) for which university students received 
academic credit.    
3.2 Remote (?online?) sites 
We had about 65 such sites in 2008. All local 
teachers and other facilitators did an amazing job 
following the instructions for administering the 
competition and for promptly returning the sub-
missions by email or regular mail. 
3.3 Clarifications 
During each of the three competitions, the jury 
was online (in some cases for 8 hours in a row) to 
provide live clarifications. Each local facilitator 
was asked to be online during the contest and relay 
to the jury any questions from the students. The 
jury then, typically within 10 minutes, either re-
plied ?no clarification needed? (the most frequent 
reply) or provided an answer which was than 
posted online for all facilitators to see. We re-
ceived dozens of clarifications requests at each of 
the rounds. 
3.4 Grading 
Grading was done by the PC with assistance 
from local colleagues. To ensure grade consis-
tency, each problem was assigned to a single 
                                                          
2
 We are grateful to the Pittsburgh sponsors: M*Modal, 
Viv?simo, JustSystems Evans Research, and Carnegie Mel-
lon's Leonard Gelfand Center for Service Learning and Out-
reach. 
90
grader or team of graders. Graders were asked to 
provide grading rubrics which assigned individual 
points for both ?practice? (that is, getting the right 
answers) and ?theory? (justifying the answers). 
3.5 Results from 2007 
195 students participated in 2007. The winners 
are shown here. One of the students was a high 
school sophomore (15 years old) while three were 
seniors at the time of the 2007 NACLO. 
 
 
1. Rachel Zax, Ithaca, NY 
2. Ryan Musa, Ithaca, NY 
3. Adam Hesterberg, Seattle, WA 
4. Jeffrey Lim, Arlington, MA 
5. (tie) Rebecca Jacobs, Encino, CA 
5. (tie) Michael Gottlieb, Tarrytown, NY 
7. (tie) Mitha Nandagopalan, San Jose, CA 
7. (tie) Josh Falk, Pittsburgh, PA 
Alternate. Anna Tchetchetkine, San Jose, CA 
 
Figure 2: List of team members from 2007. Mitha was 
unable to travel and was replaced by Anna Tchetchet-
kine. 
3.6 2008 Winners 
The 2008 contest included 763 participants in 
the Open Round and 115 participants in the Invita-
tional Round. The winners of the Invitational 
Round are listed below. These are the eight stu-
dents who are eligible to represent the USA at the 
2008 ILO. As of the writing of this paper, all eight 
were available for the trip. One of the eight is a 
high school freshman (9th grade). 
 
 
1. Guy Tabachnick, New York, NY 
2. Jeffrey Lim, Arlington, MA 
3. Josh Falk, Pittsburgh, PA 
4. Anand Natarajan, San Jose, CA 
5. Jae-Kyu Lee, Andover, MA 
6. Rebecca Jacobs, Encino, CA 
7. Hanzhi Zhu, Shrewsbury, MA 
8. Morris Alper, San Jose, CA 
 
Figure 3: List of team members from 2008. 
3.7 Canadian Participation 
Canada participated for the first time in 2008 
(about 20 students from Toronto, a handful from 
Ottawa and one from Vancouver). Two students 
did really well at the 2008 Open (one ranked sec-
ond and two tied for 13th) but were not in the top 
20 at the Invitational. 
3.8 Diversity 
About half of the participants in NACLO were 
girls in 2007 and 2008. In 2007, 25 out of the top 
50 students were female. 
The two US teams that went to the ILO in 2007 
included three girls, out of eight total team mem-
bers (two teams of four).  The 2008 teams include 
only one girl. 
3.9 Other statistics 
Some random statistics: (a) of the top 20 stu-
dents in 2008, 14 are from public schools, (b) 26 
states, 3 Canadian provinces, and the District of 
Columbia were represented in 2008. 
4 Preparation for the ILO  
Preparation for the ILO was a long and painful 
process. We had to obtain visas for Russia, fund 
and arrange for the trip, and do a lot of practices. 
4.1 Teams 
 One of the students who was eligible to be on 
the second USA team was unable to travel. We 
went down the list of alternates and picked a dif-
ferent student to replace her. 
4.2 Funding 
The ILO covered room and board for the first 
team and the team coach. The second team was 
largely self-funded (including airfare and room and 
board). Everyone else was funded as part of the 
overall NACLO budget. The University of Michi-
gan covered the coach?s airfare. 
4.3 Training 
We ran multiple training sessions. The activities 
included individual problem solving, team problem 
solving (using Skype?s chat facility), readings, as 
well as live lectures (both at the summer school in 
Estonia and on the day before the main ILO in 
Russia).  
4.4 Travel logistics 
Four students, two chaperones, and one parent 
left early to attend a summer school organized by 
the Russian team in Narva, Estonia. The third 
91
chaperone and three students traveled directly to 
the ILO. The eighth student traveled with her par-
ents and did some sightseeing in Russia prior to the 
ILO. 
5 Participation in the ILO  
The ILO was organized by a local committee from 
St. Petersburg chaired by Stanislav Gurevych. The 
organization was extraordinary. Everything (prob-
lem selection, grading, hotel, activities, food) was 
excellent. 
5.1 Organization of the ILO 
The ILO was held at a decent hotel in Ze-
lenogorsk, a suburb of St. Petersburg on the Baltic 
Sea. The first day included an orientation, the sec-
ond day was the individual contest and team build-
ing activities, the third day ? an excursion to St. 
Petersburg, the fourth day ? the team contest and 
awards ceremony. 
5.2 Problems 
The problems given at the ILO were quite di-
verse and difficult. The hardest problems were the 
one in the Ndom language which involved a non-
standard number system and the Hawaiian problem 
given at the team contests which involved a very 
sophisticated kinship system. 
 
 
Turkish/Tatar 
Braille 
Ndom (Papua New Guinea) 
Movima (Bolivia) 
Georgian (Caucasus) 
Hawaiian 
 
Figure 4: List of languages used in ILO 2007. 
5.3 Results 
Adam Hesterberg scored the highest score in the 
individual contest. One of the two US teams (Re-
becca Jacobs, Joshua Falk, Michael Gottlieb, and 
Anna Tchetchetkine) tied for first place in the team 
event. 
6 Future directions  
The unexpected interest in the NACLO poses a 
number of challenges for the organizers. Further 
challenges arise from our desire to cover more 
computational problems. 
6.1 Grading and decentralization? 
Grading close to 5,000 submissions from 763 
students in 2008 took a toll on our problem com-
mittee. The process took more than two weeks. We 
are considering different options for future years, 
e.g., reducing the number of problems in the first 
round or involving some sort of self-selection (e.g., 
asking each potential participant to do a practice 
test and obtain a minimal score on it). These op-
tions are suboptimal as they detract from some of 
the stated goals of the NACLO and we will not 
consider them seriously unless all other options 
(e.g., recruiting more graders). have been ex-
hausted.  
6.2 Problem diversity 
We would like to include more problem types, 
especially on the computational end of the contest. 
This is somewhat of a conflict with the ILO which 
includes mostly ?traditional? LO problems. One 
possibility is to have the first round be more com-
putational whereas the invitational round would be 
more aimed at picking the team members for the 
ILO by focusing more on traditional problems. 
6.3 Practice problems 
We will be looking to recruit a larger pool of 
problem writers who can contribute problems of 
various levels of difficulty (including very easy 
problems and problems based on the state of the art 
in research in NLP). We are also looking for vol-
unteers to translate problems from Russian, includ-
ing the recently published collection ?Zadachi 
Lingvisticheskyh Olimpiad?. 
6.4 Other challenges 
The biggest challenges for the NACLO in both 
years were funding and time management.  
In 2007, four of the students had to pay for their 
own airfare and room and board. At the time of 
writing, the budget for 2008 is still not fully cov-
ered. The current approach with regard to sponsor-
ship is not sustainable since NSF cannot fund 
recurring events and the companies that we ap-
proached either gave nothing or gave a relatively 
92
small amount compared to the overall annual 
budget. 
The main organizers of the NACLO each spent 
several hundred hours (one of them claims ?the 
equivalent to 20 ACL program committee chair-
manships?), mostly above and beyond their regular 
appointments. For NACLO to scale up and be suc-
cessful in the future, a much wider pool of organ-
izers will be needed. 
6.5 Other countries 
Dominique Estival told us recently that an LO 
will take place in Australia in Winter 2008 (that is, 
Summer 2008 in the Northern Hemisphere). OzLO 
(as it is called) will be collaborating with NACLO 
on problem sets. Other countries such as the 
United Kingdom and the Republic of Ireland are 
considering contests as well. One advantage that 
these countries all have is that they can share (Eng-
lish-language) problem sets with NACLO. 
6.6 Participant self-selection 
Some Olympiads provide self-selection prob-
lems. Students who score poorly on these problem 
sets are effectively discouraged from participation 
in the official contest. If the number of participants 
keeps growing, we may need to consider this op-
tion for NACLO. 
6.7 More volunteers 
NACLO exerted a tremendous toll on the organ-
izers. Thousands of hours of volunteer work went 
into the event each year. NACLO desperately 
needs more volunteers to help at all levels (prob-
lem writing, local organization, web site mainte-
nance, outreach, grading, etc). 
7 Overall assessment  
While it will take a long time to properly assess the 
impact of NACLO 2007 and 2008, we have some 
preliminary observations to share. 
7.1 Openness 
We made a very clear effort to reach out to all 
high school students in the USA and Canada. 
Holding the contest online helped make it truly 
within everyone?s reach. Students and teachers 
overwhelmingly appreciated the opportunity to 
participate at no cost (other than postage to send 
the submissions back to the jury) and at their own 
schools. Students who participated at the university 
sites similarly expressed great satisfaction at the 
opportunity to meet with peers who share their in-
terests. 
7.2 Diversity and outreach 
We were pleased to see that the number of male 
and female participants was nearly equal. A num-
ber of high schools indicated that clubs in Linguis-
tics were being created or were in the works. 
7.3 Success at the ILO 
Even though the US participated for the first 
time at the ILO, the performance shown there (in-
cluding first place individually and a tie for first 
place in the team contest) was outstanding. 
Acknowledgments 
We want to thank everyone who helped turn 
NACLO into a successful event. Specifically, Amy 
Troyani from Taylor Allderdice High School in 
Pittsburgh, Mary Jo Bensasi of CMU, all problem 
writers and graders (which include the PC listed 
above as well as Rahel Ringger and Julia Work-
man) and all local contest organizers (James Puste-
jovsky, Lillian Lee, Claire Cardie, Mitch Marcus, 
Kathy McKeown, Barry Schiffman, Lori Levin, 
Catherine Arnott Smith, Richard Sproat, Roxana 
Girju, Steve Abney, Sally Thomason, Aleka 
Blackwell, Roula Svorou, Thomas Payne, Stan 
Szpakowicz, Diana Inkpen, Elaine Gold). James 
Pustejovsky was also the sponsorship chair, with 
help from Paula Chesley. Ankit Srivastava, Ronnie 
Sim and Willie Costello co-wrote some of the 
problems with members of the PC. Eugene Fink 
helped with the solutions booklets, Justin Brown 
worked on the web site, and Adam Hesterberg was 
an invaluable member of the team throughout. 
Other people who deserve our gratitude include 
Cheryl Hickey, Alina Johnson, Patti Kardia, Josh 
Cannon, Christina Hunt, Jennifer Wofford, and 
Cindy Robinson. Finally, NACLO couldn?t have 
happened without the leadership and funding pro-
vided by NSF and Tanya Korelsky in particular as 
well as the generous sponsorship from Google, 
Cambridge University Press, and the North Ameri-
can Chapter of the ACL (NAACL). 
93
The authors of this paper are also thankful to 
Martha Palmer for giving us feedback on an earlier 
draft.  
NACLO was partially funded by the National 
Science Foundation under grant IIS 0633871 Plan-
ning Workshop for a Computational Linguistics 
Olympiad. 
References  
Vasileios Hatzivassiloglou and Kathleen McKeown. 
1997. Predicting the Semantic Orientation of Adjec-
tives, ACL 1997.  
Jeannette Wing, Computational Thinking, CACM vol. 
49, no. 3, March 2006, pp. 33-35. 
V. I. Belikov, E. V. Muravenko and M. E. Alexeev, 
editors. Zadachi Lingvisticheskikh Olimpiad. 
MTsNMO. Moscow, 2007. 
Appendix A. Summary of freeform com-
ments 
?I think it's a great outreach tool to high schools.  I was es-
pecially impressed by the teachers who came and talked to 
[the linguistics professors] about starting a linguistics club? 
?The problems are great.  One of our undergraduates ex-
pressed interest in a linguistics puzzle contest (on the model of 
Google's and MS's puzzle contests) at the undergrad level.? 
?We got a small but very high-quality group of students.  
To get a larger group, we'd need to start earlier.? 
?Things could be more streamlined.  I think actually *less* 
communication, but at key points in the process, would be 
more effective.? 
?It also would have been nice if there were a camp, like 
with the other US olympiads, so that more students would get 
the chance to learn about linguistics? 
?Just get the word out to as many schools as possible. You 
could also advertise on forums like AOPS, Cogito, and even 
CollegeConfidential ? where students are looking for intel-
lectual challenges?. 
?The problems helped develop the basic code breaking.? 
?Having a camp would be a huge benefit, but otherwise I 
think the contest was done very well.  Thank you for bringing 
it to the US.? 
?Maybe send a press release to school newspapers and ask 
them to print something about it.? 
?My 9 students enjoyed participating even though none of 
them made it to the second round.  Several have indicated that 
they want to do it again next year now that they know what it 
is like.? 
?I used every opportunity to utter the phrase "computa-
tional linguistics" to other administrators, at meetings, with 
parents, students, other teachers. People inevitably want to 
know more!? 
?As I mentioned previously, we are all set to start up a new 
math/WL club next year. YAY!? 
?Advertise with world language professional organizations 
(i.e., ACTFL) and on our ListServs (i.e., FLTeach)? 
?It was wonderful. KUDOS!? 
?There were several practice sessions, about half run by a 
math teacher (who organizes many of the competitions of this 
nature) and half by the Spanish teacher. Also, several of the 
English teachers got really excited about it (especially the 
teacher who teaches AP English Language, who teaches often 
about logical reasoning) and offered extra credit to the stu-
dents who took it.? 
?The preparation for the naclo was done entirely by the 
math club.? 
?It was a very useful competition. First, it raised awareness 
about linguistics among our students. They knew nothing 
about this area before, and now they are looking for opportuni-
ties to study linguistics and some started visiting linguistic 
research seminars at the University of Washington.? 
?The Olympiad was interesting to most students because it 
was very different from all the other math Olympiads we par-
ticipate in. Students saw possibilities for other application of 
their general math skills. In addition, the students who won 
(reasonably succeeded in) this Olympiad were not the same 
students that usually win math contests at  our school. This 
was very useful for their confidence, and showed everybody 
that broadening skills is important.? 
?I was the only one to take the contest from my school, so 
it didn't really increase awareness that much. I, however, 
learned a lot about linguistics, and the people who I told about 
the contest seemed to find it interesting also.? 
?As a result of this competition, an Independent-Study 
Linguistics Course was offered this spring for a few interested 
students.? 
?Three students who participated in NACLO are now do-
ing an Independent Study course with my colleague from th 
e World Languages dept (who had a linguistics course in 
college)? 
?I'd like to see more linguistic indoctrination, so that math 
nerds are converted over to the good side.? 
?next year I will  teach a Computational Linguistics semi-
nar? 
Appendix B. Related URLs 
 
http://www.naclo.cs.cmu.edu/ 
http://www.cogito.org/ContentRedirect.aspx? 
    ContentID=16832 
http://www.cogito.org/Interviews/ 
    InterviewsDetail.aspx?ContentID=16901 
http://www.ilolympiad.spb.ru/ 
http://cty.jhu.edu/imagine/PDFs/Linguistics.pdf 
http://www.nsf.gov/news/news_summ.jsp? 
    cntn_id=109891 
http://photofile.name/users/anna_stargazer/2949079/ 
 
Figure 5: List of additional references URLs. 
Appendix C. Sample problems 
We include here some sample problems as well as 
one solution. The rest of the solutions are available 
on the NACLO Web site. 
94
C.1. Molistic 
This is a problem from 2007 written by 
Dragomir Radev and based on [Hatzivassiloglou 
and McKeown 1997]. 
 
 
Imagine that you heard these sentences: 
 
      Jane is molistic and slatty. 
      Jennifer is cluvious and brastic. 
      Molly and Kyle are slatty but danty. 
      The teacher is danty and cloovy. 
      Mary is blitty but cloovy. 
      Jeremiah is not only sloshful but also weasy. 
      Even though frumsy, Jim is sloshful. 
      Strungy and struffy, Diane was a pleasure to watch. 
      Even though weasy, John is strungy. 
      Carla is blitty but struffy. 
      The salespeople were cluvious and not slatty. 
 
1. Then which of the following would you be likely to 
hear? 
 
      a. Meredith is blitty and brastic. 
      b. The singer was not only molistic but also cluvious. 
      c. May found a dog that was danty but sloshful. 
 
2. What quality or qualities would you be looking for in a 
person? 
 
      a. blitty 
      b. weasy 
      c. sloshful 
      d. frumsy 
 
3. Explain all your answers. (Hint: The sounds of the words 
are not relevant to their meanings.) 
 
Figure 6: ?Molistic? problem from 2007. 
C.2. Garden Path 
This is another problem from 2007. 
 
 
True story: a major wireless company recently started an advertising 
campaign focusing on its claim that callers who use its phones 
experience fewer dropped calls. 
 
The billboards for this company feature sentences that are split into 
two parts. The first one is what the recipient of the call hears, and the 
second one - what the caller actually said before realizing that the call 
got dropped. The punch line is that dropped calls can lead to serious 
misunderstandings. We will use the symbol // to separate the two parts 
of such sentences. 
 
(1) Don't bother coming // early. 
(2) Take the turkey out at five // to four. 
(3) I got canned // peaches. 
 
These sentences are representative of a common phenomenon in 
language, called "garden path sentences". Psychologically, people 
interpret sentences incrementally, before waiting to hear the full text. 
When they hear the ambiguous start of a garden path sentence, they 
assume the most likely interpretation that is consistent with what they 
have heard so far. They then later backtrack in search of a new parse, 
should the first one fail. 
 
In the specific examples above, on hearing the first part, one 
incorrectly assumes that the sentence is over. However, when more 
words arrive, the original interpretation will need to be abandoned. 
 
(4) All Americans need to buy a house // is a large amount of money. 
(5) Melanie is pretty // busy. 
(6) Fat people eat // accumulates in their bodies. 
 
1. Come up with two examples of garden path sentences that are not 
just modifications of the ones above and of each other. Split each of 
these two sentences into two parts and indicate how hearing the 
second part causes the hearer to revise his or her current parse.  
 
For full credit, your sentences need to be such that the interpretation 
of the first part should change as much as possible on hearing the 
second part. For example, in sentence (6) above, the interpretation of 
the word "fat" changes from an adjective ("fat people") to a noun ("fat 
[that] people eat...").  Note: sentences like "You did a great job..., // 
NOT!" don't count.  
 
2.  Rank sentences (4), (5), (6) as well as the two sentences from your 
solution to H1 above, based on how surprised the hearer is after 
hearing the second part.  What, in your opinion, makes a garden path 
sentence harder to process by the hearer?  
 
Figure 7: ?Garden Path? problem from 2007. 
 
95
C.3. Ilocano 
This 2008 problem was written by Patrick Littell 
of the University of Pittsburgh. 
 
 
The Ilocano language is one of the major languages of the Philippines, 
spoken by more than 8 million people.  Today is it written in the 
Roman alphabet, which was introduced by the Spanish, but before that 
Ilocano was written in the Baybayin script.  Baybayin (which literally 
means ?spelling?) was used to write many Philippine languages and 
was in use from the 14th to the 19th centuries. 
 
1. Below are twelve Ilocano words written in Baybayin. Match them 
to their English translations, listed in scrambled order below.  
 
____________ 
  ____________ 
  ____________ 
   ____________ 
	 
   ____________ 
	  
 	 
   ____________ 
	 	 
 ____________ 
	 	 	 	 
 ____________ 
	  	 	 	 
 ____________ 

   ____________ 
  ____________ 
   ____________ 
 
{ to look, is skipping for joy, is becoming a skeleton, to buy, various 
skeletons, various appearances, to reach the top, is looking, 
appearance, summit, happiness, skeleton } 
 
2. Fill in the missing forms. 
 
	  	 
 ____________ 
    ____________ 
     ____________ 
____________          (the/a) purchase 
____________          is buying 
 
3. Explain your answers to 1 and 2. 
 
Figure 8: Ilocano problem from 2008. 
 
 
Practical: 11 points 
 
1. Translations (1/2 point each) 

appearance 
  various appearances 
  to look 
   is looking 
	 
   happiness 
	  
 	 
   is skipping for joy 
	 	 
 skeleton 
	 	 	 	 
 various skeletons 
	  	 	 	 
 is becoming a skeleton 

   to buy 
  summit 
   to reach the top 
 
2. Missing forms (1 point each) 
 
	  	 
 to become a skeleton 
    various summits 
     is reaching the top 

            (the/a) purchase 

  
             is buying 
 
Assign ? point each if the basic symbols (the consonants) are correct, 
and the other ? point if the diacritics (the vowels) are correct. 
 
Theoretical: 9 points 
* The first step in this problem must be to divide the English items 
into semantically similar groups (1 pt) and divide the Baybayin items 
into groups based on shared symbols (1 pt). 
* From this they can deduce that the group including  must 
correspond to the ?look/appearances? group (4 members each), that 
including 	 	 
 to the ?skeleton? group (3 members each), and 

   must be ?to buy? (1 each).  For getting this far they should 
get another 2 points. 
* Figuring out the nature of the Baybayin alternations is the tricky 
part.  A maximally good explanation will discover that there are two 
basic processes: 
? From the basic form, copy the initial two symbols and add 
them to the beginning.  The first should retain whatever 
diacritic it might have, but the second should have its dia-
critic (if any) replaced by a cross below. 
? Insert  as the second symbol, and move the initial sym-
bol?s diacritic (if any) to this one.  Add an underdot to the 
first symbol. 
* Discovering these two processes, and determining that the third 
process is the result of doing both, is worth 3 points.  Discovering 
these two processes, and describing the third as an unrelated process ? 
that is, not figuring out that it?s just a combination of the first two ? is 
worth 2 points.  Figuring out these processes without reference to the 
diacritics is worth 1 point, whether or not they correctly determine the 
nature of the third process. 
* All that remains is to match up which processes indicate which 
categories, which shouldn?t be hard if they?ve gotten this far.  Their 
description of how to determine this is worth another 1 point. 
* The remaining 1 point is reserved to distinguish particularly elegant 
solutions described with unusual clarity. 
 
 
96
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 78?86,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Inductive Detection of Language Features via Clustering Minimal Pairs:
Toward Feature-Rich Grammars in Machine Translation
Jonathan H. Clark, Robert Frederking, Lori Levin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jhclark,ref,lsl}@cs.cmu.edu
Abstract
Syntax-based Machine Translation systems
have recently become a focus of research
with much hope that they will outperform
traditional Phrase-Based Statistical Machine
Translation (PBSMT). Toward this goal, we
present a method for analyzing the mor-
phosyntactic content of language from an
Elicitation Corpus such as the one included in
the LDC?s upcoming LCTL language packs.
The presented method discovers a mapping
between morphemes and linguistically rele-
vant features. By providing this tool that
can augment structure-based MT models with
these rich features, we believe the discrimina-
tive power of current models can be improved.
We conclude by outlining how the resulting
output can then be used in inducing a mor-
phosyntactically feature-rich grammar for AV-
ENUE, a modern syntax-based MT system.
1 Introduction
Recent trends in Machine Translation have begun
moving toward the incorporation of syntax and
structure in translation models in hopes of gaining
better translation quality. In fact, some structure-
based systems have already shown that they can out-
perform phrase-based SMT systems (Chiang, 2005).
Still, even the best-performing data-driven systems
have not fully explored the depth of such linguistic
features as morphosyntax.
Certainly, many have brought linguistically moti-
vated features into their models in the past. Huang
and Knight (2006) explored relabeling of non-
terminal symbols to embed more information di-
rectly into the backbone of the grammar. Bonneau-
Maynard et al (2007) argue that incorporation of
morphosyntax in the form of a part of speech (POS)
language model can improve translation. While
these approaches do make use of various linguis-
tic features, we have only begun to scratch the sur-
face of what actually occurs in the languages of the
world. We wish to address such issues as case mark-
ing, subject-verb agreement, and numeral-classifier
agreement by providing models with information
about which morphemes correspond to which gram-
matical meanings.
2 Task Overview
Feature Detection is the process of determining from
a corpus annotated with feature structures (Figure 2)
which feature values (Figure 1) have a distinct rep-
resentation in a target language in terms of mor-
phemes (Figure 3). By leveraging knowledge from
the field of language typology, we know what types
of phenomena are possible across languages and,
thus, which features to include in our feature speci-
fication.
But not every language will display each of these
phenomena. Our goal is to determine which fea-
ture values (e.g. singular, dual, plural) have a dis-
tinct encoding in a given target language. Viewed
differently, we can ask which feature values can be
clustered by similarity. For instance, in Chinese, we
would expect singular, plural and dual to be mem-
bers of the same cluster (since they are typically not
explicitly expressed), while for Arabic we should
place each of these into separate clusters to indicate
they are each grammaticalized differently. Similarly,
78
Feature Name Feature Value Comment
np-gen m ,f, n Biological Gender
np-def +, - Definiteness
np-num sg, dl, pl Number
c-ten past, pres, fut Tense
np-function act, und Actor and undergoer participant roles
c-function main, rel Main and relative clause roles
Figure 1: An example feature specification.
ID Source Language Target Language Lexical Cluster Feature Structure
s1 He loves her. El ama a ella. `1 ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s2 She loves her. Ella ama a ella. `1 ((act (np-gen f) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s3 He loved her. El *ama a ella. `1 ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten past))
s4 The boy eats. El nin?o come. `2 ((act (np-gen m) (np-num sg) (np-def +)) (c-ten pres))
s5 The girl eats. La nin?a come. `2 ((act (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s6 A girl eats. Una nin?a come. `2 ((act (np-gen f) (np-num sg) (np-def -)) (c-ten pres))
s7 The girls eat. Las nin?as comen. `2 ((act (np-gen f) (np-num pl) (np-def +)) (c-ten pres))
s8 The girls eat. Las nin?as comen. `2 ((act (np-gen f) (np-num dl) (np-def +)) (c-ten pres))
s9 Girls eat. Unas nin?as comen. `2 ((act (np-gen f) (np-num pl) (np-def -)) (c-ten pres))
Figure 2: An example of sentences that might be found in an elicitation corpus. Notice that each sentence differs from
some other sentence in the corpus by exactly one feature value. This enables us to see how the written form of the
language changes (or does not change) when the grammatical meaning changes.
English would have two clusters for the feature num-
ber: (singular) and (dual, plural). Further, we would
like to determine which morphemes express each of
these values (or value clusters). For example, En-
glish expresses negation with the morphemes no and
not, whereas questions are expressed by reordering
of the auxiliary verb or the addition of a wh-word.
Though many modern corpora contain feature-
annotated utterances, these corpora are often not
suitable for feature detection. For this purpose, we
use an Elicitation Corpus (see Figure 2), a corpus
that has been carefully constructed to provide a large
number of minimal pairs of sentences such as He
sings and She sings so that only a single feature (e.g.
gender) differs between the two sentences. Also, no-
tice that the feature structures are sometimes more
detailed than the source language sentence. For ex-
ample, English does not express dual number, but
we might want to include this feature in our Elicita-
tion Corpus (especially for a language such as Ara-
bic). For these cases, we include a context field for
the translator with an instruction such as ?Translate
this sentence as if there are two girls.?
In the past, we proposed deductive (rule-based)
methods for feature detection (Clark et al, 2008).
In this paper, we propose the use of inductive fea-
ture detection, which operates directly on the feature
set that the corpus has been annotated with, remov-
ing the need for manually written rules. We define
inductive feature detection as a recall-oriented task
since its output is intended to be analyzed by a Mor-
phosyntactic Lexicon Generator, which will address
the issue of precision. This, in turn, allows us to in-
form a rule learner about which language features
can be clustered and handled by a single set of rules
and which must be given special attention. How-
ever, due to the complexity of this component, de-
scribing it is beyond the scope of this paper. We also
note that future work will include the integration of a
morphology analysis system such as ParaMor (Mon-
son et al, 2007) to extract and annotate the valuable
morphosyntactic information of inflected languages.
An example of this processing pipeline is given in
Figure 4.
79
Feature Value Candidate Morphemes
np-gen m el, nin?o
np-gen f ella, nin?a
np-gen n *unobserved*
np-def + el, la, las
np-def - una, unas
np-num sg el, ella, la, una, come, nin?o, nin?a
np-num dl-pl las, unas, comen, nin?as
c-ten past-pres ?
c-ten fut *unobserved*
Figure 3: An example of the output of our system for the above corpus: a list of feature-morpheme pairings.
Elicitation
Corpus
Inductive
Feature
Detection
Morphosyntactic
Lexicon
Generator
Unsupervised
Morphology
Induction
Grammar
Rule
Learner
Decoder
Figure 4: An outline of the steps from an input Elicitation Corpus to the application of a morphosyntactically feature
rich grammar in a MT decoder. This paper discusses the highlighted inductive feature detection component. Note that
this is just one possible configuration for integrating inductive feature detection into system training.
3 The Need to Observe Real Data
One might argue that such information could be ob-
tained from a grammatical sketch of a language.
However, these sketches often focus on the ?inter-
esting? features of a language, rather than those that
are most important for machine translation. Fur-
ther, not all grammatical functions are encoded in
the elements that most grammatical sketches focus
on. According to Construction Grammar, such in-
formation is also commonly found in constructions
(Kay, 2002). For example, future tense is not gram-
maticalized in Japanese according to most reference
sources, yet it may be expressed with a construction
such as watashi wa gakoo ni iku yode desu (lit. ?I
have a plan to go to school.?) for I will go to school.
Feature detection informs us of such constructional-
ized encodings of language features for use in im-
proving machine translation models.
Recognizing the need for this type of data, the
LDC has included our Elicitation Corpus in their
Less Commonly Taught Languages (LCTL) lan-
guage packs (Simpson et al, 2008). Already, these
language packs have been translated into Thai, Ben-
gali, Urdu, Hungarian, Punjabi, Tamil, and Yoruba.
With structured elicitation corpora already being
produced on a wide scale, there exists plenty of data
that can be exploited via feature detection. Some of
these language packs have already been released for
use in MT competitions and they will start being re-
leased to the general research community this year
through LDC?s catalog.
4 Applications
4.1 Induction of Feature-Rich Grammars
Given these outputs, a synchronous grammar in-
duction system can then use these feature-annotated
morphemes and the knowledge of which features are
expressed to create a feature rich grammar. Consider
the example in Figure 5, which shows Urdu subject-
verb agreement taking place while being separated
by 12 words. Traditional n-gram Language Mod-
els (LM?s) would not be able to detect any disagree-
ments more than n words away, which is the nor-
mal case for a trigram LM. Even most syntax-based
systems would not be able to detect this problem
without using a huge number of non-terminals, each
marked for all possible agreements. A syntax-based
system might be able to check this sort of agree-
80
ek talb alm arshad jo mchhlyoN ke liye pani maiN aata phink raha tha . . .
a.SG student named Irshad who fish for water in flour throw PROG.SG.M be.PAST.SG.M
?A student named Irshad who was throwing flour in the water for the fish . . . ?
Figure 5: A glossed example from parallel text in LDC?s Urdu-English LCTL language pack showing subject-verb
agreement being separated by 12 words.
ment if it produced a target-side dependency tree as
in Ding and Palmer (2005). However, we are not
aware of any systems that attempt this. Therefore,
the correct hypotheses, which have correct agree-
ment, will likely be produces as hypotheses of tra-
ditional beam-search MT systems, but their features
might not be able to discern the correct hypothe-
sis, allowing it to fall below the 1-best or out of the
beam entirely. By constructing a feature-rich gram-
mar in a framework that allows unification-based
feature constraints such as AVENUE (Carbonell et
al., 2002), we can prune these bad hypotheses lack-
ing agreement from the search space.
Returning to the example of subject-verb agree-
ment, consider the following Urdu sentences taken
from the Urdu-English Elicitation Corpus in LDC?s
LCTL language pack:
Danish ne Amna ko sza di
Danish ERG Amna DAT punish give.PERF
?Danish punished Amna.?
Danish Amna ko sza dita hai
Danish Amna DAT punish give.HAB be.PRES
?Danish punishes Amna.?
These examples show the split-ergativity of Urdu
in which the ergative marker ne is used only for
the subject of transitive, perfect aspect verbs. In
particular, since these sentences have the perfect
aspect marked on the light verb di, a closed-class
word (Poornima and Koenig, 2008), feature detec-
tion will allow the induction of a grammar that per-
colates a feature up from the VP containing di in-
dicating that its aspect is perfect. Likewise, the NP
containing Danish ne will percolate a feature up in-
dicating that the use of ne requires perfect aspect.
If, during translation, a hypothesis is proposed that
does not meet either of these conditions, unification
will fail and the hypothesis will be pruned 1.
Certainly, unification-based grammars are not the
1If the reader is not familiar with Unification Grammars, we
recommend Kaplan (1995)
only way in which this rich source of linguistic infor-
mation could be used to augment a structure-based
translation system. One could also imagine a system
in which the feature annotations are simply used to
improve the discriminative power of a model. For
example, factored translation models (Koehn and
Hoang, 2007) retain the simplicity of phrase-based
SMT while adding the ability to incorporate addi-
tional features. Similarly, there exists a continuum
of degrees to which this linguistic information can
be used in current syntax-based MT systems. As
modern systems move toward integrating many fea-
tures (Liang et al, 2006), resources such as this will
become increasingly important in improving trans-
lation quality.
5 System Description
In the following sections, we will describe the pro-
cess of inductive feature detection by way of a run-
ning example.
5.1 Feature Specification
The first input to our system is a feature specification
(Figure 1). The feature specification used for this ex-
periment was written by an expert in language typol-
ogy and is stored in a human-readable XML format.
It is intended to cover a large number of phenom-
ena that are possible in the languages of the world.
Note that features beginning with np- are partici-
pant (noun) features while features beginning with
c- are clause features. The feature specification al-
lows us to know which values are unobserved during
elicitation (that is, no sentence having that feature
value was given to the bilingual person to translate).
This is the case for the first four features and their
values in Figure 1. The last two function features
and their values tell us what possible roles partici-
pants and clauses can take in sentences.
81
5.2 Elicitation Corpus
As outlined in Section 3, feature detection uses an
Elicitation Corpus (see Figure 2), a corpus that has
been carefully constructed to provide a large num-
ber of minimal pairs of sentences such as He sings
and She sings so that only a single feature (e.g. gen-
der) differs between the two sentences (Levin et al,
2006; Alvarez et al, 2006). If two features had var-
ied at once (e.g. It sang) or lexical choice varied
(e.g. She reads), then making assertions about which
features the language does and does not express be-
comes much more difficult.
Notice that each input sentence has been tagged
with an identifier for a lexical cluster as a pre-
processing step. Specifying lexical clusters ensures
that we don?t compare sentences with different con-
tent just because their feature structures match. For
example, we would not want to compare Dog bites
man and Man bites dog nor The student snored
and The professor snored. Note that bag-of-words
matching is insufficient for this purpose.
Though any feature-annotated corpus can be used
in feature detection, the amount of useful informa-
tion extracted from the corpus is directly dependent
on how many minimal pairs can be formed from the
corpus. For instance, one might consider using a
morphologically annotated corpus or even an auto-
matically parsed corpus in place of the elicitation
corpus. Even though these resources are likely to
suffer from having very sparse minimal pairs due to
their uncontrolled usage of vocabulary, they might
still contain some amount of useful information.
However, since we seek both to apply these methods
to language for which there are currently no man-
ually annotated corpora and to investigate features
that existing parsers generally cannot identify (e.g.
generic nouns and evidentiality), we will not men-
tion these types of resources any further.
5.3 Minimal Pair Clustering
Minimal pair clustering is the process of grouping
all possible sets of minimal pairs, those pairs of sen-
tences that have exactly one difference between their
feature structures. We use wildcard feature struc-
tures to represent each minimal pair cluster. We de-
fine a wildcard feature as any feature whose value
is *, which denotes that the value matches another *
rather than its original feature value. Similarly, we
define the feature context of the wildcard feature be
the enclosing participant and clause type for a np-
feature or the enclosing clause for a c- type fea-
ture. Then, for each sentence s in the corpus, we
substitute a wildcard feature for each of the values v
in its feature structure, and we append s to the list
of sentences associated with this wildcard feature
structure. A sample of some of the minimal pairs
for our running example are shown in Figure 6.
Here, we show minimal pairs for just one wild-
card, though multiple wildcards may be created if
one wishes to examine how features interact with
one another. This could be useful in cases such as
Hindi where the perfective verb aspect interacts with
the past verb tense and the actor NP function to add
the case marker ne (for split ergativity of Urdu, see
Section 4.1). That said, a downstream component
such as a Morphosyntactic Lexicon Generator would
perhaps be better suited for the analysis of feature in-
teractions. Also, note that the feature context is not
used when there is only one wildcard feature. The
feature context becomes useful when multiple wild-
cards are added in that it may also act as a wildcard
feature.
The next step is to organize the example sentences
into a table that helps us decide which examples can
be compared and stores information that will inform
our comparison. Briefly, any two sentences belong-
ing to the same minimal pair cluster and lexical clus-
ter will eventually get compared. As specified in Al-
gorithm 1, we create a table like that in Figure 7.
Having collected this information, we are now ready
to begin clustering feature values.
Algorithm 1 Organize()
Require: Minimal pairs, lexical clusters, and the
feature specification.
Ensure: A table T of comparable examples.
for all pair m ? minimalPairs do
for all sentence s ? m do
f? wildcardFeature(s, m)
v? featureValue(s, f)
c? featureContext(m)
`? lexCluster(s)
T[f,m, c, `, v]? T[f,m, c, `, v]? s
return T
82
ID Set Members Feature Feature Context Feature Structure
m1 {s1, s2} np-gen ((act)) ((act (np-gen *) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
m2 {s1, s3} np-ten () ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten *))
m3 {s4, s5, s7, s8} np-gen ((act)) ((act (np-gen *) (np-num sg) (np-def +)) (c-ten pres))
m4 {s5, s7, s8} np-num ((act)) ((act (np-gen f) (np-num *) (np-def +)) (c-ten pres))
m5 {s6, s9} np-num ((act)) ((act (np-gen f) (np-num *) (np-def -)) (c-ten pres))
m6 {s5, s6} np-def ((act)) ((act (np-gen f) (np-num sg) (np-def *)) (c-ten pres))
m7 {s7, s9} np-def ((act)) ((act (np-gen f) (np-num pl) (np-def *)) (c-ten pres))
Figure 6: An example subset of minimal pairs that can be formed from the corpus in Figure 2.
Feature Min. Pair Feat. Context Lex. Cluster Feat. Value. Sentence
np-gen m1 ((act)) `1 m s1
np-gen m1 ((act)) `1 f s2
np-ten m2 () `1 pres s1
np-ten m2 () `1 past s3
np-num m4 ((act)) `2 sg s5
np-num m4 ((act)) `2 pl s7
np-num m4 ((act)) `2 dl s8
np-num m5 ((act)) `2 sg s6
np-num m5 ((act)) `2 pl s9
Figure 7: An example subset of the organized items that can be formed from the minimal pairs in Figure 6. Each item
that has a matching minimal pair ID, feature context, and lexical cluster ID can be compared during feature detection.
5.4 Feature Value Clustering
During the process of feature value clustering, we
collapse feature values that do not have a distinct
encoding in the target language into a single group.
This is helpful both as information to components
using the output of inductive feature detection and
later as a method of reducing data sparseness when
creating morpheme-feature pairings. We represent
the relationship between the examples we have gath-
ered for each feature as a feature expression graph.
We define a feature expression graph (FEG) for a
feature f to be a graph on |v| vertices where v is
the number of possible values of f (though for most
non-trivial cases, it is more conveniently represented
as a triangular matrix).
Each vertex of the FEG corresponds to a feature
value (e.g. singular, dual) while each arc contains
the list of examples that are comparable according
to the table from the previous step. The examples at
each arc are organized into those that had the same
target language string, indicating that the feature val-
ues are not distinctly expressed, and those that had
a different target language string, indicating that the
change in grammatical meaning represented in the
feature structure has a distinct encoding in the tar-
get language. Algorithm 2 more formally specifies
the creation of a FEG. The FEG?s for our running
example are shown in Figure 8. From these statis-
tics generated from these graphs, we then estimate
the maximum likelihood probability of each feature
value pair being distinctly encoded as shown in Fig-
ure 9.
The interpretation of these probabilities might not
be obvious. They estimate the likelihood of a lan-
guage encoding a feature given that the meaning of
that feature is intended to be conveyed. These proba-
bilities should not be interpreted as a traditional like-
lihood of encountering a given lexical item.
Finally, we cluster by randomly selecting a start-
ing vertex for a new cluster and adding vertices to
that cluster, following arcs out from the cluster that
have a weight lower than some threshold ?. When
no more arcs may be followed, a new start vertex is
selected and another cluster is formed. This is re-
peated until all feature values have been assigned to
a cluster. For our running example, we use ? = 0.6,
83
fm
n
{(s1, s2, NEQ), (s4, s5, NEQ), 
(s4, s7, NEQ), (s4, s8, NEQ)}
np-gen
{} {}
pls
dl
{(s5,s7, NEQ), (s6, s9, NEQ)}
{(s5, s8, NEQ)}
{(s7, s8, EQ)}
np-num
-+
{(s5, s6, NEQ), 
(s7, s9, NEQ))}
np-def
prespast
fut
{(s1, s2, NEQ)}
c-ten
{} {}
Figure 8: An example subset of the Feature Expression Graphs that are formed from the minimal pairs in Figure 7.
fm
n
| arcs[m,f] with (s
m
,s
f
,x,NEQ) |
| arcs[m,f] |
| arcs[m,n] with (s
m
,s
n
,x,NEQ) |
| arcs[m,n] |
| arcs[f,n] with (s
f
,s
n
,x,NEQ) |
| arcs[f,n] |
Figure 9: An example of how probabilities are estimated for each feature value pair in a Feature Expression Graph for
the feature np-gender.
Algorithm 2 Collecting statistics for each FEG.
Require: The table T from the previous step.
Ensure: A complete graph as an arc list with the
observed similarities and differences for each fea-
ture value.
for all si, sj ? T s.t. (mi, ci, `i) = (mj , cj , `j)
do
(vi, vj)? (featureValue(si), featureValue(sj))
if tgt(si) = tgt(sj) then
arcs[vi, vj ]? arcs[vi, vj ] ? (si, sj ,m,EQ)
else
arcs[vi, vj ]? arcs[vi, vj ] ? (si, sj ,m,NEQ)
return arcs
which results in the following clusters being formed:
np-gen: m, f
np-num: s, pl/dl
np-def: +, -
c-ten: past, pres
5.5 Morpheme-Feature Pairing
Finally, using the information from above about
which values should be examined as a group and
which sentence pairs exemplify an orthographic dif-
ference, we examine each pair of target language
sentences to determine which words changed to re-
flect the change in grammatical meaning. This pro-
cess is outlined in Algorithm 3. The general idea is
that for each arc going out of a feature value vertex
we examine all of the target language sentence pairs
that expressed a difference. We then take the words
that were in the vocabulary of the target sentence
for the current feature value, but not in the sentence
it was being compared to and add them to the list
of words that could be used to express this feature
value (Figure 3).
6 Evaluation and Results
We evaluated the output of feature detection with
one wildcard feature as applied to the Elicitation
Corpus from the LDC?s Urdu-English LCTL lan-
guage pack. Threshold parameters were set to small
values (? = 0.05). Note that an increase in precision
might be possible by tuning this value; however, as
stated, we are most concerned with recall.
An initial attempt was made to create a gold stan-
dard against which recall could be directly calcu-
lated. However, the construction of this gold stan-
dard was both noisier and more time consuming
than expected. That is, even though the task is
based on how a linguistic field worker might col-
84
Algorithm 3 Determine which morphemes are as-
sociated with which feature values.
Require: List of clusters C and list of FEGs F
Ensure: A list of morphemes associated with each
feature value
for all feature ? F do
for all vertex ? feature do
for all arc ? vertex do
for all (s1, s2,m,NEQ) ? arc do
v1 ? featureValue(s1,m)
v2 ? featureValue(s2,m)
if v1 6= v then (s1, v1)? (s2, v2)
w1 ? vocabulary(s1)
w2 ? vocabulary(s2)
? ?W1 ?W2
for all w ? freq do
freq[w]++
for all w ? freq do
p = freq[w] / ?w freq[w]
if p ? ?? then
morphemes[v]? morphemes[v]? w
return morphemes
lect data, it was more difficult for a human than
anticipated. Therefore, we instead produced a list
of hypothesized morpheme-feature pairs and had a
human trained in linguistics who was also bilingual
in Hindi/Urdu-English mark each pair as ?Correct,?
?Incorrect,? or ?Ambiguous.? The results of this
evaluation are summarized in Figure 10. The reader
may be surprised by how many incorrect hypothe-
ses were generated, given the controlled nature of
the Elicitation Corpus. However, there are two im-
portant factors to consider. First, features can in-
teract in complex and often unexpected ways. For
instance, in English, the only feature difference in
minimal pair Cats yawned and A cat yawned is the
number of the actor. However, this causes an in-
teraction with definiteness that would cause the pre-
sented algorithms to associate a with the number of
nouns even though it is canonically associated with
definiteness. Second, the bilingual people translat-
ing the Elicitation Corpus are prone to make errors.
Though a fair number of incorrect hypotheses
were produced, the number of correct hypotheses
is encouraging. We also note that the words be-
ing identified are largely function words and multi-
Judgment Morpheme-Feature Pairings
Correct 68
Ambiguous 29
Incorrect 109
TOTAL 206
Figure 10: The results of feature detection. Being a
recall-oriented approach, inductive feature detection is
geared toward overproduction of morpheme-feature pair-
ings as shown in the number of ambiguous and incorrect
pairings.
morpheme tokens from which closed-class func-
tional morphemes will be extracted. One might
think the counts extracted seem low when compared
to the typical MT vocabulary size, but these function
words that we extract cover a much larger probabil-
ity mass of the language than content words.
We are confident that the Morphosyntactic Lex-
icon Generator designed to operate directly down-
stream from this process will be sufficiently discrim-
inant to use these morpheme-feature pairings to cre-
ate a high precision lexicon. However, since this
component is, in itself, highly complex, its specifics
are beyond the scope of this paper and so we leave it
to be discussed in future work.
7 Conclusion
We have presented a method for inductive feature
detection of an annotated corpus, which determines
which feature values have a distinct representation
in a target language and what morphemes can be
used to express these grammatical meanings. This
method exploits the unique properties of an Elici-
tation Corpus, a resource which is becoming widely
available from the LDC. Finally, we have argued that
the output of feature detection is useful for exploit-
ing these linguistic features via a feature-rich gram-
mar for a machine translation system.
Acknowledgments
We would like to thank our colleagues Alon Lavie,
Vamshi Ambati, Abhaya Agarwal, and Alok Par-
likar for their insights. Thanks to Keisuke Kamataki
for the Japanese example and to Shakthi Poornima
for her help with the Urdu examples. This work was
supported by US NSF Grant Number 0713-292.
85
References
Alison Alvarez, Lori Levin, Robert Frederking, Simon
Fung, Donna Gates, and Jeff Good. 2006. The MILE
corpus for less commonly taught languages. In HLT-
NAACL, New York, New York, June.
H. Bonneau-Maynard, A. Allauzen, D. De?chelotte, and
H. Schwenk. 2007. Combining morphosyntactic en-
riched representation with n-best reranking in statis-
tical translation. In Proceedings of the Workshop on
Structure and Syntax in Statistical Translation (SSST)
at NAACL-HLT.
Jaime Carbonell, Kathrina Probst, Erik Peterson, Chris-
tian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource limited
MT. In Association for Machine Translation in the
Americas (AMTA), October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Association for
Computational Linguistics (ACL).
Jonathan H. Clark, Robert Frederking, and Lori Levin.
2008. Toward active learning in corpus creation: Au-
tomatic discovery of language features during elicita-
tion. In Proceedings of the Language Resources and
Evaluation Conference (LREC).
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Meeting
of the Association for Computational Linguistics ACL.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of (NAACL-HLT).
Ronald Kaplan. 1995. The formal architecture of lexi-
cal functional grammar. In Mary Dalrymple, Ronald
Kaplan, J. Maxwell, and A. Zaenen, editors, Formal
Issues in Lexical Functional Grammar. CSLI Publica-
tions.
Paul Kay. 2002. An informal sketch of a formal archi-
tecture for construction grammar. In Grammars.
Phillipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Lori Levin, Jeff Good, Alison Alvarez, and Robert Fred-
erking. 2006. Parallel reverse treebanks for the dis-
covery of morpho-syntactic markings. In Proceedings
of Treebanks and Linguistic Theory, Prague.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics, Sydney.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2007. Paramor: Minimally supervised induc-
tion of paradigm structure and morphological analysis.
In Proceedings of the 9th ACL SIGMORPH.
Shakthi Poornima and Jean-Pierre Koenig. 2008. Re-
verse complex predicates in Hindi. In Proceedings of
the 24th Northwest Linguistic Conference.
Heather Simpson, Christopher Cieri, Kazuaki Maeda,
Kathryn Baker, and Boyan Onyshkevych. 2008. Hu-
man language technology resources for less commonly
taught languages: Lessons learned toward creation of
basic language resources. In Proceedings of the LREC
2008 Workshop on Collaboration: interoperability be-
tween people in the creation of language resources for
less-resourced langauges.
86
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 49?58,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Evaluating an Agglutinative Segmentation Model for ParaMor 
Christian Monson, Alon Lavie, Jaime Carbonell, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15217, USA 
 {cmonson, alavie, jgc, lsl}@cs.cmu.edu
Abstract 
This paper describes and evaluates a modifica-
tion to the segmentation model used in the un-
supervised morphology induction system, Pa-
raMor. Our improved segmentation model 
permits multiple morpheme boundaries in a 
single word. To prepare ParaMor to effectively 
apply the new agglutinative segmentation 
model, two heuristics improve ParaMor?s pre-
cision. These precision-enhancing heuristics 
are adaptations of those used in other unsuper-
vised morphology induction systems, including 
work by Hafer and Weiss (1974) and Gold-
smith (2006). By reformulating the segmenta-
tion model used in ParaMor, we significantly 
improve ParaMor?s performance in all lan-
guage tracks and in both the linguistic evalua-
tion as well as in the task based information re-
trieval (IR) evaluation of the peer operated 
competition Morpho Challenge 2007. Para-
Mor?s improved morpheme recall in the lin-
guistic evaluations of German, Finnish, and 
Turkish is higher than that of any system which 
competed in the Challenge. In the three lan-
guages of the IR evaluation, our enhanced Pa-
raMor significantly outperforms, at average 
precision over newswire queries, a morpho-
logically na?ve baseline; scoring just behind the 
leading system from Morpho Challenge 2007 
in English and ahead of the first place system 
in German.  
1 Unsupervised Morphology Induction 
Analyzing the morphological structure of words 
can benefit natural language processing (NLP) ap-
plications from grapheme-to-phoneme conversion 
(Demberg et al, 2007) to machine translation 
(Goldwater and McClosky, 2005). But many of the 
world?s languages currently lack morphological 
analysis systems. Unsupervised induction could fa-
cilitate, for these lesser-resourced languages, the 
quick development of morphological systems from 
raw text corpora. Unsupervised morphology induc-
tion has been shown to help NLP tasks including 
speech recognition (Creutz, 2006) and information 
retrieval (Kurimo et al, 2007b). In this paper we 
work with languages like Spanish, German, and 
Turkish for which morphological analysis systems 
already exist. 
The baseline ParaMor algorithm which we ex-
tend here competed in the English and German 
tracks of Morpho Challenge 2007 (Monson et al, 
2007b). The peer operated competitions of the 
Morpho Challenge series standardize the evalua-
tion of unsupervised morphology induction algo-
rithms (Kurimo et al, 2007a; 2007b). The ParaMor 
algorithm showed promise in the 2007 Challenge, 
placing first in the linguistic evaluation of German. 
Developed after the close of Morpho Challenge 
2007, our improvements to the ParaMor algorithm 
could not officially compete in this Challenge. 
However, the Morpho Challenge 2007 Organizing 
Committee (Kurimo et al, 2008) graciously over-
saw the quantitative evaluation of our agglutinative 
version of ParaMor.  
1.1 Related Work 
A variety of approaches to unsupervised morphol-
ogy induction have shown promise in past work: 
Here we highlight three techniques which have 
been used in a number of unsupervised morphol-
ogy induction algorithms. Since character se-
quences are less predictable at morpheme bounda-
ries than within any particular morpheme (see dis-
cussion in section 2.1), a first unsupervised mor-
49
phology induction technique measures the predict-
ability of word-internal character sequences. Harris 
(1955) was the first to propose the branching factor 
of the character tree of a corpus vocabulary as a 
measure of character predictability. Character trees 
have been incorporated into a number of more re-
cently proposed unsupervised morphology induc-
tion systems (Schone and Jurafsky, 2001; Wicen-
towski, 2002; Goldsmith, 2006; Bordag, 2007). 
Johnson and Martin (2003) generalize from charac-
ter trees and model morphological character se-
quences with minimized finite state automata. 
Bernhard (2007) measures character predictability 
by directly computing transitional probabilities be-
tween substrings of words. 
A second successful technique has used the 
minimum description length principle to capture 
the morpheme as a recurrent structure of morphol-
ogy. The Linguistica system of Goldsmith (2006), 
the Morfessor system of Creutz (2006), and the 
system described in Brent et al (1995) take this 
approach. 
A third technique leverages inflectional para-
digms as the organizational structure of morphol-
ogy. The ParaMor algorithm, which this paper ex-
tends, joins Snover (2002), Zeman (2007), and 
Goldsmith?s Linguistica in building morphology 
models around the paradigm.  
ParaMor tackles three challenges that face mor-
phology induction systems which Goldsmith's Lin-
guistica algorithm does not yet address. First, sec-
tion 2.2 of this paper introduces an agglutinative 
segmentation model. This agglutinative model seg-
ments words into as many morphemes as the data 
justify. Although Goldsmith (2001) and Goldsmith 
and Hu (2004) discuss ideas for segmenting indi-
vidual words into more than two morphemes, the 
implemented Linguistica algorithm, as presented in 
Goldsmith (2006), permits at most a single mor-
pheme boundary in each word. Second, ParaMor 
decouples the task of paradigm identification from 
that of word segmentation (Monson et al, 2007b). 
In contrast, morphology models in Linguistica in-
herently encode both a belief about paradigm 
structure on individual words as well as a segmen-
tation of those words. Without ParaMor?s decoup-
ling of paradigm structure from specific segmenta-
tion models, our algorithm for agglutinative seg-
mentation (section 2.2) would not have been possi-
ble. Third, the evaluation of ParaMor in this paper 
is over much larger corpora than any published 
evaluation of Linguistica. Goldsmith (2006) seg-
ments the Brown corpus of English, which, after 
discarding numbers and punctuation, has a vocabu-
lary size of 47,607 types. Using Linguistica, Creutz 
(2006) successfully segments a Finnish corpus of 
250,000 tokens (approximately 130,000 types), but 
Creutz notes that Linguistica is memory intensive 
and not runable for larger corpora. In the evalua-
tions of Morpho Challenge 2007, ParaMor seg-
mented the words from corpora with over 42 mil-
lion tokens and vocabularies as large as 2.2 million 
types.  
2 ParaMor 
This section briefly outlines the high level struc-
ture of ParaMor as described in detail in Monson et 
al. (2007a; 2007b). ParaMor takes the inflectional 
paradigm as the basic building block of morphol-
ogy. A paradigm is a mutually substitutable set of 
morphological operations. For example, most ad-
jectives in Spanish inflect for two paradigms. First, 
adjectives are marked for gender: an a suffix 
marks feminine, an o masculine. Then Spanish ad-
jectives mark number: an s suffix signals plural, 
while no marking, ? in this paper, indicates singu-
lar. The four surface forms of the cross-product of 
the gender and number paradigms on the Spanish 
word for ?beautiful? are then: bello, bella, bellos, 
and bellas.  
ParaMor is a two stage algorithm. In the first 
stage, ParaMor identifies candidate paradigms 
which likely model suffixes of morphological pa-
radigms and their cross-products. Since some 70% 
of the world?s languages are significantly suffixing 
(Dryer, 2005), ParaMor only attempts to identify 
suffix paradigms. ParaMor?s first stage consists of 
three pipelined steps. In the first step, ParaMor 
searches a space of candidate partial paradigms, 
called schemes, for those which possibly model 
suffixes of true paradigms. The second step merges 
selected schemes which appear to model the same 
paradigm. And in the third step, ParaMor discards 
scheme clusters which likely do not model true 
paradigms.  
The second stage of the ParaMor algorithm 
segments word forms using the candidate para-
digms identified in the first stage. Section 2.2 of 
this paper introduces a new segmentation model 
for ParaMor?s second stage that allows more than 
one morpheme boundary in a single word?as is 
50
needed to correctly segment Spanish plural adjec-
tives. As this agglutinative segmentation model re-
lies on the paradigms learned in ParaMor?s first 
stage, section 2.1 presents solutions to two types of 
paradigm model error that the baseline ParaMor 
system makes. The solutions to these two error 
types are similar in nature to ideas proposed in the 
unsupervised morphology induction work of Hafer 
and Weiss (1974) and Goldsmith (2006). 
2.1 Precision at Paradigm Identification 
Table 1 presents 14 of the more than 8000 schemes 
identified during one baseline run of ParaMor?s 
scheme search step. Each row of Table 1 lists a 
scheme that was selected while searching over a 
Spanish newswire corpus of 50,000 types. On the 
far left of Table 1, the Rank column states the or-
dinal rank at which that row?s scheme was selected 
during the search procedure: the first scheme Pa-
raMor selects is ?.s; a.as.o.os is the second; ido.-
idos.ir.ir? is the 1566th selected scheme, etc. The 
right four columns of Table 1, present raw data on 
the selected schemes, giving the number of can-
didate suffixes in that scheme, the proposed suf-
fixes themselves, the number of candidate stems in 
the scheme, and a sample of those candidate stems. 
Each candidate stem in a ParaMor scheme forms a 
word that occured in the input corpus with each 
candidate suffix belonging to that scheme; for 
example, from the first selected scheme, the candi-
date stem apoyada joins to the candidate suffix s to 
form the word apoyadas ?supported (adjective 
feminine plural)??a word which occured in the 
Spanish newswire corpus.  
Between the rank on the left and the scheme 
details on the right of Table 1, are columns which 
categorize the scheme on its success, or failure, to 
model a true paradigm of Spanish. A dot appears in 
the columns marked Noun, Adjective, or Verb if the 
majority of the candidate suffixes in a row?s 
scheme attempt to model suffixes in a paradigm of 
that part of speech. A dot appears in the Derivation 
column if one or more candidate suffixes of the 
scheme models a Spanish derivational suffix. The 
Good column is marked if the candidate suffixes of 
a scheme take the surface form of true paradig-
matic suffixes. Initially selected schemes in Table 
1 that correctly capture suffixes of real Spanish 
paradigms are the 1st, 2nd, 5th, 13th, 30th, and 1566th 
selected schemes. While some smaller paradigms 
of Spanish are perfectly identified (including ?.s, 
which marks singular and plural on many nouns 
and adjectives, and the adjectival cross-product 
paradigm of gender and number, a.as.o.os) many 
selected schemes do not satisfactorily model Span-
ish suffixes. Incorrect schemes in Table 1 are 
marked in the Error columns.  
The vast majority of unsatisfactory paradigm 
models fail for one of two reasons. First, many 
schemes contain candidate suffixes which system-
Model of Error 
Verb 
Ra
nk
 
No
un
 
Ad
jec
tiv
e 
ar er ir 
De
riv
ati
on
 
Go
od
 
St
em
 In
ter
na
l 
Su
ffi
x I
nt
er
na
l 
Ch
an
ce
 Candidate Suffixes Candidate Stems 
1 ?  ?      ?     2 ?.s 5513 apoyada, barata, hombro, oficina, reo, ? 
2  ?      ?     4 a.as.o.os 899 apoyad, captad, dirigid, junt, pr?xim, ? 
3   ?       ?   14 ?.ba.ban.da.das.do.dos.n.ndo.r.ron.rse.r?.r?n 25 apoya, disputa, lanza, lleva, toma, ? 
5   ?     ?     15 a.aba.aban.ada.adas.ado.ados.an.ando.ar.aron.arse.ar?.ar?n.? 24 apoy, desarroll, disput, lanz, llev, ? 
11  ?     ?   ?    5 ta.tamente.tas.to.tos 22 cier, direc, ins?li, modes, sangrien, ? 
12   ?    ?    ?   14 ?.ba.ci?n.da.das.do.dos.n.ndo.r.ron.r?.r?n.r?a 16 acepta, concentra, fija, provoca, ? 
13   ?     ?     15 a.aba.ada.adas.ado.ados.an.ando.ar.aron.ar?.ar?n.e.en.? 20 apoy, declar, enfrent, llev, tom, ? 
30    ?  ?   ?     11 a.e.en.ida.idas.ido.idos.iendo.ieron.i?.?a 15 cumpl, escond, recib, transmit, vend, ? 
1000          ?  3 ?.g.gs 4 h, k, on, s 
1566     ?   ?     4 ido.idos.ir.ir? 6 conclu, cumpl, distribu, exclu, reun, segu 
2000      ?   ?    2 lia.liana 5 austra, ita, ju, sici, zu 
3000          ?  3 ?.a.anar 4 all, am, g, s 
4000          ?  3 ?.e.ince 4 l, pr, qu, v 
8000   ?      ?    2 trada.trarnos 3 concen, demos, encon 
               
 
Table 1. Candidate partial paradigms, or schemes, that the baseline ParaMor algorithm selected during its first step, 
search, of its first stage, paradigm identification. This baseline ParaMor run was over a Spanish newswire corpus of 
50,000 types. While some selected schemes contain suffixes from true paradigms, other schemes contain incorrectly 
segmented candidate suffixes. 
  
51
atically misanalyze word forms. These schemes 
consistently hypothesize either stem-internal or 
suffix-internal morpheme boundaries. Schemes 
which hypothesize incorrect morpheme boundaries 
include the 3rd, 11th, 12th, 2000th, and 8000th se-
lected schemes of Table 1. Among these, the 3rd 
and 12th selected schemes place morpheme boun-
daries internal to true suffixes. For example, the 3rd 
selected scheme contains truncated forms of suf-
fixes that occur correctly in the 5th selected 
scheme. Symmetrically, the candidate suffixes in 
the 11th, 2000th, and 8000th selected schemes hy-
pothesize morpheme boundaries internal to true 
Spanish stems, inadvertently including portions of 
stems within their suffix lists. In a random sample 
of 100 schemes from the 8240 schemes that the 
baseline ParaMor algorithm selects over our Span-
ish corpus, 59 schemes hypothesized an incorrect 
morpheme boundary. 
The second most prevalent reason for model 
failure occurs when the candidate suffixes of a 
scheme are related not by belonging to the same 
paradigm, but rather by a chance co-occurrence on 
a few candidate stems of the text. Schemes which 
arise from chance string collisions in Table 1 in-
clude the 1000th, 3000th, and 4000th selected 
schemes. The string lengths of the candidate stems 
and candidate suffixes of these chance schemes are 
often quite short. The longest candidate stem in 
any of the three chance-error schemes of Table 1 is 
three characters long; and all three selected 
schemes propose the suffix ?, which has length 
zero. Short stems and short suffixes in selected 
schemes are easily explained combinatorially: The 
inventory of possible strings grows exponentially 
with the length of the string. Because there just 
aren?t very many length one, length two, or even 
length three strings, it should come as no surprise 
when a variety of candidate suffixes happen to oc-
cur attached to the same set of short stems. In our 
random sample of 100 initially selected schemes, 
35 were erroneously selected as a result of a 
chance collision of word types. 
The next two sub-sections present solutions to 
the two types of paradigm model failure in the 
baseline algorithm that are exemplified in Table 1. 
These first two extensions aim to improve preci-
sion by reducing the number of schemes ParaMor 
erroneously selects. 
 
Correcting Morpheme Boundary Errors 
Most of the baseline selected schemes which incor-
rectly hypothesize a morpheme boundary do so at 
stem-internal positions. Indeed, in our random 
sample of 100 schemes, 51 of the 59 schemes with 
morpheme boundary errors incorrectly hypothe-
sized a boundary stem-internally. For this reason, 
the baseline ParaMor algorithm already discarded 
schemes that likely misplace a boundary stem-
internally (Monson et al, 2007b). Although there 
are fewer schemes that misplace a morpheme 
boundary suffix-internally, suffix-internal error 
schemes contain short suffixes that can generalize 
to segment a large number of word forms. (See 
section 2.2 for a description of ParaMor?s morpho-
logical segmentation model). To measure the in-
fluence of suffix-internal error schemes on mor-
pheme segmentation, we examined ParaMor?s 
baseline segmentations of a random sample of 100 
word forms from the 50,000 words of our Spanish 
corpus. In these 100 words, 82 morpheme bounda-
ries were introduced that should not have been. 
And 40 of these 82 incorrectly proposed bounda-
ries were placed by schemes which hypothesized a 
morpheme boundary internal to true suffixes.  
To address the problem of suffix-internal mis-
placed boundaries we adapt an idea originally pro-
posed by Harris (1955) and extended by Hafer and 
Weiss (1974): Take any string t. Let F be the set of 
strings such that for each Ff ? , t.f is a word form 
of a particular natural language. Harris noted that 
when the boundaries between t and each f fall at 
morpheme boundaries, the strings in F typically 
begin in a wide variety of characters; but when the 
t-f boundaries are morpheme-internal, each legiti-
mate word final string must first complete the er-
roneously split morpheme, and so the strings in F 
will begin with one of a very few characters. This 
argument similarly holds when the roles of t and f 
are reversed. Hafer and Weiss (1974) describe a 
number of variations to Harris? letter variety algo-
rithm. Their most successful variation uses entropy 
to measure character variety.  
Goldsmith?s (2006) Linguistica algorithm pio-
neered the use of entropy in a paradigm-based un-
supervised morphology induction system. Linguis-
tica measures the entropy of stem-final characters 
in a set of initially selected paradigm models. 
When entropy falls below a threshold, Linguistica 
considers relocating the morpheme boundary of 
52
each word covered by that paradigm model. If, af-
ter boundary relocation, the resulting description 
length of Linguistica?s morphology model de-
creases, Linguistica accepts the relocated bounda-
ries.  
To identify suffix-internal morpheme boundary 
errors among ParaMor?s initially selected schemes, 
we follow Hafer and Weiss (1974) and Goldsmith 
(2006) in using entropy as a measure of the variety 
in boundary-adjacent character distributions. In a 
ParaMor style scheme, the candidate stems form a 
set of word-initial strings, and the candidate suf-
fixes a set of word-final strings. If a scheme?s 
stems end in a very few unique characters, the 
scheme has likely hypothesized an incorrect suffix-
internal morpheme boundary. Consider the 3rd se-
lected scheme in Table 1. All 25 of the 3rd 
scheme?s stems end in the character ?a?. Conse-
quently, we measure the entropy of the distribution 
of final characters in each scheme?s candidate 
stems. Where Linguistica modifies paradigm mod-
els which appear to incorrectly place morpheme 
boundaries, our extension to ParaMor permanently 
removes schemes. To avoid introducing a free pa-
rameter, our extension to ParaMor flags a scheme 
as a likely boundary error only when virtually all 
of that scheme?s candidate stems end in the same 
character. We flag a scheme if its entropy is below 
a threshold set close to zero, 0.5. The baseline Pa-
raMor algorithm discards schemes which it be-
lieves hypothesize an incorrect stem-internal mor-
pheme boundary only after the scheme clustering 
step of ParaMor?s paradigm identification stage. 
Our extension follows suit: If we flag more than 
half of the schemes in a cluster as likely proposing 
a suffix-internal boundary, then we discard that 
cluster. Referencing Table 1, this first extension to 
ParaMor successfully removes both the 3rd and the 
12th selected schemes.  
Correcting Chance String Collision Errors 
Scheme errors due to chance string collisions are 
the second most prevalent error type. As described 
above, the string lengths of the candidate stems 
and suffixes of chance schemes are typically short. 
When the stems and suffixes of a scheme are short, 
then the underlying types which support a scheme 
are also short. Where the baseline ParaMor algo-
rithm explicitly builds schemes over all types in a 
corpus, we modify ParaMor to exclude short types 
from the vocabulary during morphology induction. 
Goldsmith (2006) also uses string-length thresh-
olds to restrict what paradigm models the Linguis-
tica algorithm produces. 
Excluding short types during ParaMor?s mor-
phology induction stage does not preclude short 
types from being analyzed as containing multiple 
morphemes during ParaMor?s segmentation stage. 
As section 2.2 describes, ParaMor?s segmentation 
algorithm is independent of the set of types from 
which schemes and scheme clusters are built. 
The string length that types must meet to join 
the induction vocabulary is a free parameter. Pa-
raMor is designed to identify the productive inflec-
tional paradigms of a language. Unless a paradigm 
is restricted to occur only with short stems, a pos-
sible but unusual scenario (as with the English ad-
jectival comparative, c.f. faster but *exquisiter) we 
can expect a productive paradigm to occur with a 
reasonable number of longer stems in a corpus. 
Hence, ParaMor needn?t be overly concerned 
about discarding short types. A qualitative exam-
ination of Spanish data suggested discarding types 
five characters or less in length; we use this cutoff 
in all experiments described in this paper. 
Excluding short types from the paradigm induc-
tion vocabulary virtually eliminates the entire cate-
gory of chance scheme. In a random sample of 100 
schemes that ParaMor selected when short types 
were excluded, only one scheme contained types 
related only by chance string similarity, down from 
35 when short types were not excluded. Returning 
to Table 1, excluding types five characters or less 
in length bars ten of the twelve word types which 
support the erroneous 3000th selected scheme ?.a.-
anar. Among the excluded types are valid Spanish 
words such as ganar ?to gain?. But also eliminated 
are several meaningless acronyms such as the sin-
gle letters g and s. Without these short types, Pa-
raMor rightly cannot select the 3000th scheme. 
2.2 Segmentation 
An Agglutinative Model 
With the improvement in scheme precision that re-
sults from the two extensions discussed in section 
2.1, we are ready to propose a more realistic model 
of morphology. ParaMor?s baseline segmentation 
algorithm distrusts ParaMor?s induced scheme 
models. The baseline algorithm assumes each word 
form can contain at most a single morpheme 
boundary. If it detects more than one morpheme 
53
boundary, then the baseline algorithm proposes a 
separate morphological analysis for each possible 
boundary. In contrast, our extended model of seg-
mentation vests more trust in the induced schemes, 
assuming that scheme clusters which propose dif-
ferent morpheme boundaries are simply modeling 
different valid morpheme boundaries. And our ex-
tension proposes a single morphological analysis 
containing all hypothesized morpheme boundaries.  
To detect morpheme boundaries, ParaMor 
matches each word, w, in the full vocabulary of a 
corpus against the clusters of schemes which are 
the final output of ParaMor?s paradigm identifica-
tion stage. When a suffix, f, of some scheme-
cluster, C, matches a word-final string of w, i.e. 
fuw .= , ParaMor attempts to replace f in turn with 
each suffix f ?  of C. If the string fu ?.  occurs in 
the full corpus vocabulary, then, on the basis of 
this paradigmatic evidence, ParaMor identifies a 
morpheme boundary in w between u and f . 
For example, to detect morpheme boundaries in 
the Spanish word apoyados ?supports (adjective 
masculine plural)?, ParaMor matches all word-
final strings of apoyados against the candidate suf-
fixes of ParaMor?s induced scheme clusters. The 
word-final strings of apoyados are s, os, dos, ados, 
yados, ?. The scheme clusters that our extended 
version of ParaMor induces include clusters which 
contain schemes very similar to the 1st, 2nd, and 5th 
baseline selected schemes, see Table 1. In particu-
lar, our extended ParaMor identifies separate 
scheme clusters that contain the candidate suffixes: 
s and ?; os and o; and ados and ado. Substituting 
? for s, o for os, or ado for ados yields the Spanish 
string apoyado ?supports (adjective masculine sin-
gular)?. It so happens, that apoyado does occur in 
our Spanish corpus, and so ParaMor has found 
paradigmatic evidence for three morpheme boun-
daries. Crucially, our ParaMor extension from sec-
tion 2.1 that removes schemes which hypothesize 
suffix internal morpheme boundaries correctly dis-
cards all schemes which contained the candidate 
suffix dos. Consequently, no scheme cluster exists 
to incorrectly suggest the morpheme boundary 
*apoya + dos, as the 3rd baseline selected scheme 
would have. Where ParaMor?s baseline segmenta-
tion algorithm would propose three separate analy-
ses of apoyados, one for each detected morpheme 
boundary: apoy +ados, apoyad +os, and apoyado 
+s; our extended segmentation algorithm produces 
the single correct analysis: apoy +ad +o +s.  
It is interesting to note that although each of Pa-
raMor?s individual paradigm models proposes a 
single morpheme boundary, our agglutinative seg-
mentation model can recover multiple boundaries 
in a single word. Using this idea it may be possible 
to quickly adapt Linguistica for agglutinative lan-
guages. Instead of interpreting the sets of stems 
and affixes that Goldsmith?s Linguistica algorithm 
produces as immediate segmentations of words, 
these signatures can be thought of as models of 
paradigms that may generalize to new words. 
Augmenting ParaMor?s Segmentations 
With its focus on the paradigm, ParaMor special-
izes at analyzing inflectional morphology (Monson 
et al, 2007a). Morpho Challenge 2007 requires al-
gorithms to analyze both inflectional and deriva-
tional morphology (Kurimo et al, 2007a; 2007b). 
To compete in the challenge, we combine Pa-
raMor?s morphological segmentations with seg-
mentations from Morfessor (Creutz, 2006), an un-
supervised morphology induction algorithm which 
learns both inflectional and derivational morphol-
ogy. We incorporate the segmentations from Mor-
fessor into the segmentations that the ParaMor sys-
tem produces by straightforwardly adding the Mor-
fessor segmentation for each word as an additional 
separate analysis to those ParaMor produces (Mon-
son et al, 2007b). Morfessor has one free parame-
ter, which we optimize separately for each lan-
guage of Morpho Challenge 2007.  
ParaMor also has several free parameters, in-
cluding the type length parameter and the parame-
ter over stem-final character entropy described in 
section 2.1. We do not adjust any of ParaMor?s pa-
rameters from language to language, but fix them 
at values that produce reasonable Spanish para-
digms and segmentations. As in Monson et al 
(2007b), to avoid adjusting ParaMor?s parameters 
we limit ParaMor?s paradigm induction vocabulary 
to 50,000 frequent types for each language.  
3 Evaluation 
To evaluate our extensions to the ParaMor algo-
rithm, we follow the methodology of the peer op-
erated Morpho Challenge 2007. All segmentations 
produced by our extensions were sent to the Mor-
pho Challenge Organizing Committee (Kurimo et 
al., 2008). The Organizing Committee evaluated 
our segmentations and returned the automatically 
54
calculated quantitative results. Using the evalua-
tion methodology of Morpho Challenge 2007 per-
mits us to compare our algorithms against the un-
supervised morphology induction systems which 
competed in the 2007 Challenge. Of the many al-
gorithms for unsupervised morphology induction 
discussed with the related work in section 1.1, five 
participated in Morpho Challenge 2007. Unless an 
algorithm has been given an explicit name, mor-
phology induction algorithms will be denoted in 
this paper by the name of their lead author. The 
five algorithms which participated in the 2007 
Challenge are: Bernhard (2007), Bordag (2007), 
Zeman (2007), Creutz?s (2006) Morfessor, and Pa-
raMor (2007b). 
Morpho Challenge 2007 had participating algo-
rithms analyze words in four languages: English, 
German, Finnish, and Turkish. The Challenge 
evaluated each algorithm?s morphological analyses 
in two ways. First, a linguistic evaluation measured 
each algorithm?s precision, recall, and F1 at mor-
pheme identification against an answer key of mor-
phologically analyzed word forms. Scores were 
normalized when a system proposed multiple 
analyses of a single word, as our combined Pa-
raMor-Morfessor submissions do. For further de-
tails on the linguistic evaluation in Morpho Chal-
lenge 2007, see Kurimo et al (2007a). The second 
evaluation of Morpho Challenge 2007 was a task 
based evaluation. Each algorithm?s analyses were 
imbedded in an information retrieval (IR) system. 
The IR evaluation consisted of queries over a lan-
guage specific collection of newswire articles. All 
word forms in all queries and all documents were 
replaced with the morphological decompositions of 
each individual analysis algorithm. Separate IR 
tasks were run for English, German, and Finnish, 
but not Turkish. For additional details on the IR 
evaluation of Morpho Challenge 2007 please refer-
ence Kurimo et al (2007b). 
Tables 2 and 3 present, respectively, the lin-
guistic and IR evaluation results. In these two ta-
bles, the top two rows contain results for segmen-
tations produced by versions of ParaMor that in-
clude our extensions. The topmost row in each ta-
ble, labeled ?+P +Seg?, gives the results for our 
fully augmented version of ParaMor, which in-
cludes our two extensions designed to improve 
precision as well as our new segmentation model 
which can propose multiple morpheme boundaries 
in a single analysis of a word form. The second 
row of each table, labeled ?+P ?Seg?, augments Pa-
raMor only with the two enhancements designed to 
improve precision. The third row of each table 
gives the Challenge results for the ParaMor base-
line algorithm. Rows four through seven of each 
table give scores from Morpho Challenge 2007 for 
the best performing unsupervised systems. If mul-
tiple versions of a single algorithm competed in the 
Challenge, the scores reported here are the highest 
F1 or Average Precision score of any algorithm 
variant at a particular task. In all test scenarios but 
Finnish IR, we produced Morfessor segmentations 
to augment ParaMor that are independent of the 
Morfessor runs which competed in Morpho Chal-
lenge. If our Morfessor runs gave a higher F1 or 
Average Precision, then we report this higher 
score. Finally, scores reported on rows eight and 
beyond are from reference algorithms that are not 
unsupervised. Reference algorithms appear in ital-
ics. A double line bisects both Table 2 and Table 3 
horizontally. All results which appear above the 
double line were evaluated after the final deadline 
of Morpho Challenge 2007. In particular, ParaMor 
officially competed only in the English and Ger-
man tracks of the Challenge.  
The Linguistic Evaluation 
Table 2 contains the results from the linguistic 
evaluation of Morpho Challenge. The Morpho 
Challenge Organizing Committee did not provide 
us with data on the statistical significance of the 
results for the enhanced versions of ParaMor. But 
most score differences are statistically signifi-
cant?All F1 differences of more than 0.5 between 
systems which officially competed in Morpho 
Challenge 2007 were statistically significant (Ku-
rimo et al, 2007a).  
In German, Finnish, and Turkish our fully en-
hanced version of ParaMor achieves a higher F1 
than any system that competed in Morpho Chal-
lenge 2007. In English, ParaMor?s precision score 
drags F1 under that of the first place system, Bern-
hard; In Finnish, the Bernhard system?s F1 is likely 
not statistically different from that of our system. 
Our final segmentation algorithm demonstrates 
consistent performance across all four languages. 
In Turkish, where the morpheme recall of other 
unsupervised systems is anomalously low, our al-
gorithm achieves a recall in a range similar to its 
recall scores for the other languages. ParaMor?s ul-
timate recall is double that of any other unsuper-
55
vised Turkish system, leading to an improvement 
in F1 over the next best system, Morfessor alone, 
of 13.5% absolute or 22.0% relative.  
In all four languages, as expected, the combina-
tion of removing short types from the training data, 
and the additional filtering of scheme clusters, 
?+P?, significantly improves precision scores over 
the ParaMor baseline. Allowing multiple mor-
pheme boundaries in a single word, ?+Seg?, in-
creases the number of words ParaMor believes 
share a morpheme. Some of these new words do in 
fact share a morpheme, some, in reality do not. 
Hence, our extension of ParaMor to agglutinative 
sequences of morphemes increases recall but low-
ers precision across all four languages. The effect 
of agglutinative segmentations on F1, however, dif-
fers with language. For the two languages which 
make limited use of suffix sequences, English and 
German, a model which hypothesizes multiple 
morpheme boundaries can only moderately in-
crease recall and does not justify, by F1, the many 
incorrect segmentations which result. On the other 
hand, an agglutinative model significantly im-
proves recall for true agglutinative languages like 
Finnish and Turkish, more than compensating in F1 
for the drop in precision over these languages. But 
in all four languages, the agglutinative version of 
ParaMor outperforms the baseline unenhanced ver-
sion at F1. 
The final row of Table 2 is the evaluation of a 
reference algorithm submitted by Tepper (2007). 
While not an unsupervised algorithm, Tepper?s 
reference parallels ParaMor in augmenting seg-
mentations produced by Morfessor. Where Pa-
raMor augments Morfessor with special attention 
to inflectional morphology, Tepper augments Mor-
fessor with hand crafted morphophonology rules 
that conflate multiple surface forms of the same 
underlying suffix. Like ParaMor, Tepper?s algo-
rithm significantly improves on Morfessor?s recall. 
With two examples of successful system augmen-
tation, we suggest that future research take a closer 
look at building on existing unsupervised mor-
phology induction systems. 
The IR Evaluation 
Turn now to results from the IR evaluation in Ta-
ble 3. Although ParaMor does not fair as well in 
Finnish, in German, the fully enhanced version of 
ParaMor places above the best system from the 
2007 Challenge, Bernhard, while our score on 
English rivals this same best system. Morpho Chal-
lenge 2007 did not measure the statistical signifi-
cance of uninterpolated average precision scores in 
the IR evaluation. It is not clear what feature of Pa-
raMor?s Finnish analyses causes comparatively 
low average precision. Perhaps it is simply that Pa-
raMor attains a lower morpheme recall over Fin-
nish than over English or German. And unfortu-
nately, Morpho Challenge 2007 did not run IR ex-
periments over the other agglutinative language in 
the competition, Turkish. When ParaMor does not 
combine multiple morpheme boundaries into a sin-
gle analysis, as in the baseline and ?+P ?Seg? sce-
Table 2. Unsupervised morphology induction systems evaluated for precision (P), recall (R), and F1 at morpheme 
identification using the methodology of the linguistic competition of Morpho Challenge 2007. 
English German Finnish Turkish 
 P R F1 P R F1 P R F1 P R F1 
 +P +Seg 50.6 63.3 56.3 49.5 59.5 54.1 49.8 47.3 48.5 51.9 52.1 52.0 
 +P ?Seg 56.2 60.9 58.5 57.4 53.5 55.4 60.5 33.9 43.5 62.0 38.2 47.3 
ParaMor  
&        
Morfessor 
Baseline 41.6 65.1 50.7 51.5 55.6 53.4 55.0 35.6 43.2 53.2 41.6 46.7 
Bernhard 61.6 60.0 60.8 49.1 57.4 52.9 59.7 40.4 48.2 73.7 14.8 24.7 
Bordag 59.7 32.1 41.8 60.5 41.6 49.3 71.3 24.4 36.4 81.3 17.6 28.9 
Morfessor 82.2 33.1 47.2 67.6 36.9 47.8 76.8 27.5 40.6 73.9 26.1 38.5 
Zeman 53.0 42.1 46.9 52.8 28.5 37.0 58.8 20.9 30.9 65.8 18.8 29.2 
Tepper 69.2 52.6 59.8 - - - 62.0 46.2 53.0 70.3 43.0 53.3 
 
56
narios, average precision is comparatively poor. 
Where the linguistic evaluation did not always pe-
nalize a system for proposing multiple partial 
analyses, real NLP applications, such as IR, can. 
The reference algorithms for the IR evaluation 
are: Dummy, no morphological analysis; Oracle, 
where all words in the queries and documents for 
which the linguistic answer key contains an entry 
are replaced with that answer; Porter, the standard 
English Porter stemmer; and Tepper described 
above. While the hand built Porter stemmer still 
outperforms the best unsupervised systems on Eng-
lish, these same best unsupervised systems outper-
form both the Dummy and Oracle references for all 
three evaluated languages?strong evidence that 
unsupervised induction algorithms are not only 
better than no morphological analysis, but that they 
are better than incomplete analysis as well.  
4 Conclusions and Future Directions 
Augmenting ParaMor with an agglutinative model 
of segmentation produces an unsupervised mor-
phology induction system with consistent and 
strong performance at morpheme identification 
across all four languages of Morpho Challenge 
2007. By first cleaning up the paradigm models 
that ParaMor learns, we raise ParaMor?s segmenta-
tion precision and allow the agglutinative model to 
significantly improve ParaMor?s morpheme recall.  
Looking forward to future improvements, we 
examined by hand the final set of scheme clusters 
that the current version of ParaMor produces over 
our newswire corpus of 50,000 Spanish types. Pa-
raMor?s paradigm identification stage outputs 41 
separate clusters. Among these final scheme clus-
ters are those which model all major productive 
paradigms of Spanish. In fact, there are often mul-
tiple scheme clusters which model portions of the 
same true paradigm. As an extreme case, 12 sepa-
rate scheme clusters contain suffixes from the 
Spanish ar verbal paradigm. Relaxing restrictions 
on ParaMor?s clustering algorithm (Monson et al, 
2007a) may address this paradigm fragmentation.  
The second significant shortcoming which sur-
faces among ParaMor?s 41 final scheme clusters is 
that ParaMor currently does not address morpho-
phonology. Among the final scheme clusters, 12 
attempt to model morphophonological change by 
incorporating the phonological change either into 
the stems or into the suffixes of the scheme cluster. 
But ParaMor currently has no mechanism for de-
tecting when a cluster is modeling morphophonol-
ogy. Perhaps ideas on morphophonology from 
Goldsmith (2006) could be adapted to work with 
the ParaMor algorithm. Finally, we plan to look at 
scaling the size of the vocabulary used both during 
paradigm induction and during morpheme segmen-
tation. We are particularly interested in the possi-
bility that ParaMor may  be able to identify para-
digms from much less data than 50,000 types. 
Acknowledgements 
We kindly thank Mikko Kurimo, Ville Turunen, 
Matti Varjokallio, and the full Organizing Com-
mittee of Morpho Challenge 2007, for running the 
evaluations of ParaMor. These dedicated workers 
produced impressively fast turn around for evalua-
tions on sometimes rather short notice. 
The research described in this paper was sup-
ported by NSF grants IIS-0121631 (AVENUE) and 
IIS-0534217 (LETRAS), with supplemental fund-
ing from NSF?s Office of Polar Programs and Of-
fice of International Science and Education. 
Table 3. Unsupervised morphology induction sys-
tems evaluated for uninterpolated average precision 
using the methodology of the IR competition of 
Morpho Challenge 2007. These results use Okapi 
term weighting (Kurimo et al, 2008b). 
*Only a subset of the words which occurred in the 
IR evaluation of this language was analyzed by this 
system.  
 Eng. Ger. Finn. Tur. 
 +P +Seg 39.3 48.4 42.6 - 
 +P ?Seg 35.1 43.1 37.1 - 
ParaMor 
&        
Morfessor 
Baseline 34.4 40.1 35.9 - 
Bernhard 39.4 47.3 49.2 - 
Bordag 34.0 43.1 43.1 - 
Morfessor 38.8 46.0 44.1 - 
Zeman  26.7*  25.7*  28.1* - 
Dummy 31.2 32.3 32.7 - 
Oracle 37.7 34.7 43.1 - 
Porter 40.8 - - - 
Tepper  37.3* - - - 
 
57
References 
Bernhard, Delphine. Simple Morpheme Labeling in Un-
supervised Morpheme Analysis. Working Notes for 
the CLEF 2007 Workshop. Budapest, Hungary, 2007. 
Bordag, Stefan. Unsupervised and Knowledge-free 
Morpheme Segmentation and Analysis. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007. 
Brent, Michael R., Sreerama K. Murthy, and Andrew 
Lundberg. Discovering Morphemic Suffixes: A Case 
Study in MDL Induction. The Fifth International 
Workshop on Artificial Intelligence and Statistics. 
Fort Lauderdale, Florida, 1995.  
Creutz, Mathias. Induction of the Morphology of Natu-
ral Language: Unsupervised Morpheme Segmenta-
tion with Application to Automatic Speech Recogni-
tion. Ph.D. Thesis. Computer and Information Sci-
ence, Report D13. Helsinki: University of Technol-
ogy, Espoo, Finland, 2006. 
Demberg, Vera, Helmut Schmid, and Gregor M?hler. 
Phonological Constraints and Morphological Pre-
processing for Grapheme-to-Phoneme Conversion. 
Association for Computational Linguistics. Prague, 
Czech Republic, 2007. 
Dryer, Matthew S. Prefixing vs. Suffixing in Inflec-
tional Morphology.  In The World Atlas of Language 
Structures. Eds. Martin Haspelmath, Matthew S. 
Dryer, David Gil, and Bernard Comrie. 2005. 
Goldsmith, John. Unsupervised Learning of the Mor-
phology of a Natural Language. Computational Lin-
guistics. 27.2:153-198. 2001. 
Goldsmith, John. An Algorithm for the Unsupervised 
Learning of Morphology. Natural Language Engi-
neering. 12.4:335-351. 2006. 
Goldsmith, John, and Yu Hu. From Signatures to Finite 
State Automata. Paper presented at the Midwest 
Computational Linguistics Colloquium. Blooming-
ton, Indiana, 2004. 
Goldwater, Sharon, and David McClosky. Improving 
Statistic MT through Morphological Analysis. Em-
pirical Methods in Natural Language Processing. 
Vancouver, Canada, 2005. 
Hafer, Margaret A. and Stephen F. Weiss. Word Seg-
mentation by Letter Successor Varieties. Information 
Storage and Retrieval, 10:371-385. 1974. 
Harris, Zellig. From Phoneme to Morpheme. Language 
31.2:190-222. 1955. Reprinted in Harris (1970). 
Harris, Zellig. Papers in Structural and Transforma-
tional Linguists. Ed. D. Reidel, Dordrecht. 1970. 
Johnson, Howard, and Joel Martin. Unsupervised 
Learning of Morphology for English and Inuktitut. 
Human Language Technology Conference / North 
American Chapter of the Association for Computa-
tional Linguistics. Edmonton, Canada, 2003. 
Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 
Unsupervised Morpheme Analysis Evaluation by a 
Comparison to a Linguistic Gold Standard ? Morpho 
Challenge 2007. Working Notes for the CLEF 2007 
Workshop. Budapest, Hungary, 2007a. 
Kurimo, Mikko, Mathias Creutz, and Ville Turunen. 
Unsupervised Morpheme Analysis Evaluation by IR 
Experiments ? Morpho Challenge 2007. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007b. 
Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 
Unsupervised Morpheme Analysis -- Morpho Chal-
lenge 2007. January 10, 2008. <http://www.cis.hut.-
fi/morphochallenge2007/>. 2008. 
Monson, Christian, Jaime Carbonell, Alon Lavie, and 
Lori Levin. ParaMor: Minimally Supervised Induc-
tion of Paradigm Structure and Morphological 
Analysis. Computing and Historical Phonology: The 
Ninth Meeting of the ACL Special Interest Group in 
Computational Morphology and Phonology. Prague, 
Czech Republic, 2007a. 
Monson, Christian, Jaime Carbonell, Alon Lavie, and 
Lori Levin. ParaMor: Finding Paradigms across 
Morphology. Working Notes for the CLEF 2007 
Workshop. Budapest, Hungary, 2007b. 
Schone, Patrick, and Daniel Jurafsky. Knowledge-Free 
Induction of Inflectional Morphologies. North 
American Chapter of the Association for Computa-
tional Linguistics. Pittsburgh, Pennsylvania, 2001. 
Snover, Matthew G. An Unsupervised Knowledge Free 
Algorithm for the Learning of Morphology in Natural 
Languages. M.S. Thesis. Computer Science, Sever 
Institute of Technology, Washington University, 
Saint Louis, Missouri, 2002. 
Tepper, Michael A. Using Hand-Written Rewrite Rules 
to Induce Underlying Morphology. Working Notes 
for the CLEF 2007 Workshop. Budapest, Hungary, 
2007. 
Wicentowski, Richard. Modeling and Learning Multi-
lingual Inflectional Morphology in a Minimally Su-
pervised Framework. Ph.D. Thesis. Johns Hopkins 
University, Baltimore, Maryland, 2002. 
Zeman, Daniel. Unsupervised Acquiring of Morpho-
logical Paradigms from Tokenized Text. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007. 
 
 
58
Committed Belief Annotation and Tagging
Mona T. Diab Lori Levin
CCLS LTI
Columbia U. CMU
mdiab@cs.columbia.edu lsl@cs.cmu.edu
Teruko Mitamura Owen Rambow
LTI CCLS
CMU Columbia U.
teruko+@cs.cmu.edu rambow@ccls.columbia.edu
Vinodkumar Prabhakaran Weiwei Guo
CS CS
Columbia U. Columbia U.
Abstract
We present a preliminary pilot study of
belief annotation and automatic tagging.
Our objective is to explore semantic mean-
ing beyond surface propositions. We aim
to model people?s cognitive states, namely
their beliefs as expressed through linguis-
tic means. We model the strength of their
beliefs and their (the human) degree of
commitment to their utterance. We ex-
plore only the perspective of the author of
a text. We classify predicates into one of
three possibilities: committed belief, non
committed belief, or not applicable. We
proceed to manually annotate data to that
end, then we build a supervised frame-
work to test the feasibility of automati-
cally predicting these belief states. Even
though the data is relatively small, we
show that automatic prediction of a belief
class is a feasible task. Using syntactic
features, we are able to obtain significant
improvements over a simple baseline of
23% F-measure absolute points. The best
performing automatic tagging condition is
where we use POS tag, word type fea-
ture AlphaNumeric, and shallow syntac-
tic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
1 Introduction
As access to large amounts of textual informa-
tion increases, there is a strong realization that
searches and processing purely based on surface
words is highly limiting. Researchers in infor-
mation retrieval and natural language processing
(NLP) have long used morphological and (in a
more limited way) syntactic analysis to improve
access and processing of text; recently, interest has
grown in relating text to more abstract representa-
tions of its propositional meaning, as witnessed by
work on semantic role labeling, word sense disam-
biguation, and textual entailment. However, there
are more levels to ?meaning? than just proposi-
tional content. Consider the following examples,
and suppose we find these sentences in the New
York Times:1
(1) a. GM will lay off workers.
b. A spokesman for GM said GM will lay off
workers.
c. GM may lay off workers.
d. The politician claimed that GM will lay
off workers.
e. Some wish GM would lay of workers.
f. Will GM lay off workers?
g. Many wonder if GM will lay off workers.
If we are searching text to find out whether GM
will lay off workers, all of the sentences in (1) con-
1In this paper, we concentrate on written communication,
and we use the terms reader and writer. However, nothing in
the approach precludes applying it to spoken communication.
tain the proposition LAYOFF(GM,WORKERS).
However, the six sentences clearly allow us very
different inferences about whether GM will lay off
workers or not. Supposing we consider the Times
a trustworthy news source, we would be fairly cer-
tain with (1a) and (1b). (1c) suggests the Times is
not certain about the layoffs, but considers them
possible. When reading (1d), we know that some-
one else thinks that GM will lay off workers, but
that the Times does not necessarily share this be-
lief. (1e), (1f), and (1g) do not tell us anything
about whether anyone believes whether GM will
lay off workers.
In order to tease apart what is happening, we
need to refine a simple IR-ish view of text as a
repository of propositions about the world. We use
two theories to aid us. The first theory is that in ad-
dition to facts about the world (GM will or will not
lay off workers), we have facts about people?s cog-
nitive states, and these cognitive states relate their
bearer to the facts in the world. (Though perhaps
there are only cognitive states, and no facts about
the world.) Following the literature in Artificial
Intelligence (Cohen and Levesque, 1990), we can
model cognitive state as beliefs, desires, and inten-
tions. In this paper, we are only interested in be-
liefs (and in distinguishing them from desires and
intentions). The second theory is that communi-
cation is intention-driven, and understanding text
actually means understanding the communicative
intention of the writer. Furthermore, communica-
tive intentions are intentions to affect the reader?s
cognitive state ? his or her beliefs, desires, and/or
intentions. This view has been worked out in the
text generation and dialog community more than
in the text understanding community (Mann and
Thompson, 1987; Hovy, 1993; Moore, 1994).
In this paper we are interested in exploring the
following: we would like to recognize what the
text wants to make us believe about various peo-
ple?s cognitive states, including the speaker?s. As
mentioned, we are only interested in people?s be-
lief. In this view, the result of text processing is
not a list of facts about the world, but a list of facts
about different people?s cognitive states.
This paper is part of an on-going research effort.
The goals of this paper are to summarize a pilot
annotation effort, and to present the results of ini-
tial experiments in automatically extracting facts
about people?s beliefs from open domain running
text.
2 Belief Annotation
We have developed a manual for annotating be-
lief, which we summarize here. For more de-
tailed information, we refer to the cited works. In
general, we are interested in the writer?s intention
as to making us believe that various people have
certain beliefs, desires, and intentions. We sim-
plify the annotation in two ways: we are only in-
teretsed in beliefs, and we are only interested in
the writer?s beliefs. This is not because we think
this is the only interesting information in text, but
we do this in order to obtain a manageable anno-
tation in our pilot study. Specifically, we annotate
whether the writer intends the reader to interpret
a stated proposition as the writer?s strongly held
belief, as a proposition which the writer does not
believe strongly (but could), or as a proposition
towards which the writer has an entirely differ-
ent cognitive attitude, such as desire or intention.
We do not annotate subjectivity (Janyce Wiebe and
Martin, 2004; Wilson and Wiebe, 2005), nor opin-
ion (for example: (Somasundaran et al, 2008)):
the nature of the proposition (opinion and type of
opinion, statement about interior world, external
world) is not of interest. Thus, this work is or-
thogonal to the extensive literature on opinion de-
tection. And we do not annotate truth: real-world
(encyclopedic) truth is not relevant.
We have three categories:
? Committed belief (CB): the writer indicates
in this utterance that he or she believes the
proposition. For example, GM has laid off
workers, or, even stronger, We know that GM
has laid off workers.
A subcase of committed belief concerns
propositions about the future, such as GM
will lay off workers. People can have equally
strong beliefs about the future as about the
past, though in practice probably we have
stronger beliefs about the past than about the
future.
? Non-committed belief (NCB): the writer
identifies the propositon as something which
he or she could believe, but he or she hap-
pens not to have a strong belief in. There are
two subcases. First, there are cases in which
the writer makes clear that the belief is not
strong, for example by using a modal auxil-
iary:2 GM may lay off workers. Second, in
reported speech, the writer is not signaling to
us what he or she believes about the reported
speech: The politician claimed that GM will
lay off workers. However, sometimes, we can
use the speech act verb to infer the writer?s
attitude,3 and we can use our own knowledge
2The annotators must distinguish epistemic and deontic
uses of modals.
3Some languages may also use grammatical devices; for
to infer the writer?s beliefs; for example, in
A GM spokesman said that GM will lay off
workers, we can assume that the writer be-
lieves that GM intends to lay off workers, not
just the spokesman. However, this is not part
of the annotation, and all reported speech is
annotated as NCB. Again, the issue of tense
is orthogonal.
? Not applicable (NA): for the writer, the
proposition is not of the type in which he or
she is expressing a belief, or could express a
belief. Usually, this is because the proposi-
tion does not have a truth value in this world
(be it in the past or in the future). This covers
expressions of desire (Some wish GM would
lay of workers), questions (Will GM lay off
workers? or Many wonder if GM will lay
off workers, and expressions of requirements
(GM is required to lay off workers or Lay off
workers!).
This sort of annotation is part of an annotation
of all ?modalities? that a text may express. We
only annotate belief. A further complication is
that these modalities can be nested: one can ex-
press a belief about someone else?s belief, and one
may be strong and the other weak (I believe John
may believe that GM will lay off workers). At this
phase, we only annotate from the perspective of
the writer, i.e. what the writer of the text that is
being annotated believes.
The annotation units (annotatables) are, con-
ceptually, propositions as defined by PropBank
(Kingsbury et al, 2002). In practice, annotators
are asked to identify full lexical verbs (whether
in main or embedded clauses, whether finite or
non-finite). In predicative constructions (John is a
doctor/in the kitchen/drunk), we ask them to iden-
tify the nominal, prepositional, or adjectival head
rather than the form of to be, in order to also han-
dle small clauses (I think [John an idiot]).
The interest of the annotation is clear: we want
to be able to determine automatically from a given
text what beliefs we can ascribe to the writer,
and with what strengths he or she holds them.
Across languages, many different linguistic means
are used to denote this attitude towards an uttered
proposition, including syntax, lexicon, and mor-
phology. To our knowledge, no systematic empir-
ical study exists for English, and this annotation is
a step towards that goal.
example, in German, the choice between indicative mood and
subjunctive mood in reported speech can signal the writer?s
attitude.
3 Related Work
The work of Roser et al (2006) is, in many re-
spects, very similar to ours. In particular, they are
concerned with extracting information about peo-
ple?s beliefs and the strength of these beliefs from
text. However, their annotation is very different
from ours. They extend the TimeML annotation
scheme to include annotation of markers of belief
and strength of belief. For example, in the sen-
tence The Human Rights Committee regretted that
discrimination against women persisted in prac-
tice, TimeML identifies the events associated with
the verbs regret and persist, and then the extension
to the annotation adds the mark that there is a ?fac-
tive? link between the regret event and the persist
event, i.e., if we regret something, then we assume
the truth of that something. In contrast, in our
annotation, we directly annotate events with their
level of belief. In this example, we would annotate
persist as being a committed belief of the Human
Rights Committee (though in this paper we only
report on beliefs attributed to the writer). This dif-
ference is important, as in the annotation of Roser
et al (2006), the annotator must analyze the situ-
ation and find evidence for the level of belief at-
tributed to an event. As a result, we cannot use
the annotation to discover how natural language
expresses level of belief. Our annotation is more
primitively semantic: we ask the annotators sim-
ply to annotate meaning (does X believe the event
takes place), as opposed to annotating the linguis-
tic structures which express meaning. As a conse-
quence of the difference in annotation, we cannot
compare our automatic prediction results to theirs.
Other related works explored belief systems in
an inference scenario as opposed to an intentional-
ity scenario. In work by (Ralf Krestel and Bergler,
2007; Krestel et al, 2008), the authors explore
belief in the context of news media exploring re-
ported speech where they track newspaper text
looking for elements indicating evidentiality. The
notion of belief is more akin to finding statements
that support or negate specific events with differ-
ent degrees of support. This is different from our
notion of committed belief in this work, since we
seek to make explicit the intention of the author or
the speaker.
4 Our Approach
4.1 Data
We create a relatively small corpus of English
manually annotated for the three categories: CB,
NCB, NA. The data covers different domains and
genres from newswire, to blog data, to email cor-
respondence, to letter correspondence, to tran-
scribed dialogue data. The data comprises 10K
words of running text. 70% of the data was dou-
bly annotated comprising 6188 potentially anno-
tatable tokens. Hence we had a 4 way manual clas-
sification in essence between NONE, CB, NCB,
and NA. Most of the confusions between NONE
and CB from both annotators, for 103 tokens.
The next point of disagreement was on NCB and
NONE for 48 tokens.They disagreed on NCB and
CB for 32 of the tokens. In general the interanno-
tator agreements were high as they agreed 95.8%
of the time on the annotatable and the exact belief
classification.4 Here is an example of a disagree-
ment between the two annotators, The Iraqi gov-
ernment has agreed to let Rep Tony Hall visit the
country next week to assess a humanitarian cri-
sis that has festered since the Gulf War of 1991
Hall?s office said Monday. One annotator deemed
?agreed? a CB while the other considered it an
NCB.
4.2 Automatic approach
Once we had the data manually annotated and re-
vised, we wanted to explore the feasibility of au-
tomatically predicting belief states based on lin-
guistic features. We apply a supervised learning
framework to the problem of both identifying and
classifying a belief annotatable token in context.
This is a three way classification task where an
annotatable token is tagged as one of our three
classes: Committed Belief (CB), Non Committed
Belief (NCB), and Not Applicable (NA). We adopt
a chunking approach to the problem using an In-
side Outside Beginning (IOB) tagging framework
for performing the identification and classification
of belief tokens in context. For chunk tagging,
we use YamCha sequence labeling system.5 Yam-
Cha is based on SVM technology. We use the de-
fault parameter settings most importantly the ker-
nels are polynomial degree 2 with a c value of 0.5.
We label each sentence with standard IOB tags.
Since this is a ternary classification task, we have
7 different tags: B-CB (Beginning of a commit-
ted belief chunk), I-CB (Inside of a committed be-
lief chunk), B-NCB (Beginning of non commit-
ted belief chunk), I-NCB (Inside of a non com-
mitted belief chunk), B-NA (Beginning of a not
applicable chunk), I-NA (Inside a not applicable
chunk), and O (Outside a chunk) for the cases
that are not annotatable tokens. As an example
of the annotation, a sentence such as Hall said
he wanted to investigate reports from relief agen-
cies that a quarter of Iraqi children may be suffer-
4This interannotator agreement number includes the
NONE category.
5http://www.tado-chasen.com/yamcha
ing from chronic malnutrition. will be annotated
as follows: {Hall O said B-CB he O wanted B-
NCB to B-NA investigate I-NA reports O from O
relief O agencies O that O a O quarter O of O
Iraqi O children O may O be O suffering B-NCB
from O chronic O malnutrition O.}
We experiment with some basic features and
some more linguistically motivated ones.
CXT: Since we adopt a sequence labeling
paradigm, we experiment with different window
sizes for context ranging from ?/+2 tokens after
and before the token of interest to ?/+5.
NGRAM: This is a character n-gram feature,
explicity representing the first and last character
ngrams of a word. In this case we experiment with
up to ?/+4 characters of a token. This feature
allows us to capture implicitly the word inflection
morphology.
POS: An important feature is the Part-of-Speech
(POS) tag of the words. Most of the annotatables
are predicates but not all predicates in the text are
annotatables. We obtain the POS tags from the
TreeTagger POS tagger tool which is trained on
the Penn Treebank.6
ALPHANUM: This feature indicates whether
the word has a digit in it or not or if it is a non
alphanumeric token.
VerbType: We classify the verbs as to whether
they are modals (eg. may, might, shall, will,
should, can, etc.), auxilliaries (eg. do, be, have),7
or regular verbs. Many of our annotatables occur
in the vicinity of modals and auxilliaries. The list
of modals and auxilliaries is deterministic.
Syntactic Chunk (CHUNK): This feature ex-
plicitly models the syntactic phrases in which our
tokens occur. The possible phrases are shallow
syntactic representations that we obtain from the
TreeTagger chunker:8 ADJC (Adjective Chunk),
ADVC (Adverbial Chunk), CONJC (Conjunc-
tional Chunk), INTJ (Interjunctional Chunk), LST
(numbers 1, 2,3 etc), NC (Noun Chunk), PC
(Prepositional Chunk), PRT (off,out,up etc), VC
(Verb Chunk).
5 Experiments and Results
5.1 Conditions
Since the data is very small, we tested our au-
tomatic annotation using 5 fold cross validation
6http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
7We realize in some of the grammar books auxilliaries
include modal verbs.
8http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
where 10% of the data is set aside as development
data, then 70% is used for training and 20% for
testing. The reported results are averaged over the
5 folds for the Test data for each of our experimen-
tal conditions.
Our baseline condition is using the tokenized
words only with no other features (TOK). We em-
pirically establish that a context size of ?/+3
yields the best results in the baseline condition as
evaluated on the development data set. Hence all
the results are yielded from a CXT of size 3.
The next conditions present the impact of
adding a single feature at a time and then combin-
ing them. It is worth noting that the results reflect
the ability of the classifier to identify a token that
could be annotatable and also classify it correctly
as one of the possible classes.
5.2 Evaluation Metrics
We use F?=1 (F-measure) as the harmonic mean
between (P)recision and (R)ecall. All the pre-
sented results are the F-measure. We report the
results separately for the three classes CB, NCB,
and NA as well as the overall global F measure for
any one condition averaged over the 5 folds of the
TEST data set.
5.3 Results
In Table 1 we present the results yielded per con-
dition including the baseline TOK and presented
for the three different classes as well as the overall
F-measure.
All the results yielded by our experiments
outperform the baseline TOK. We highlight
the highest performing conditions in Ta-
ble 1: TOK+AlphaNum+POS +CHUNK,
TOK+AN+POS and TOK+POS. Even though
all the features independently outperform the
baseline TOK in isolation, POS is the single most
contributing feature. The least contributing factor
independently is the AlphaNumeric feature AN.
However combining AN with character Ngram
NG yields better results than using each of them
independently. We note that adding NG to any
other feature combination is not helpful, in fact
it seems to add noise rather than signal to the
learning process in the presence of more sophis-
ticated features such as POS or syntactic chunk
information. Adding the verbtype VT explicitly
as a feature is not helpful for all categories, it
seems most effective with CB. As mentioned
earlier we deterministically considered all modal
verbs to be modal. This might not be the case
for all modal auxilliaries since some of them
are used epistemically while others deontically,
hence our feature could be introducing an element
of noise. Adding syntactic chunk information
helps boost the results by a small margin from
53.5 to 53.97 F-measure. All the results seem to
suggest the domination of the POS feature and it?s
importance for such a tagging problem. In general
our performance on CB is the highest, followed
by NA then we note that NCB is the hardest
category to predict. Examining the data, NCB
has the lowest number of occurrence instances
in this data set across the board in the whole
data set and accordingly in the training data,
which might explain the very low performance.
Also in our annotation effort, it was the hardest
category to annotate since the annotation takes
more than the sentential context into account.
Hence a typical CB verb such as ?believe? in the
scope of a reporting predicate such as ?say? as
in the following example Mary said he believed
the suspect with no qualms. The verb believed
should be tagged NCB however in most cases it
is tagged as a CB. Our syntactic feature CHUNK
helps a little but it does not capture the overall
dependencies in the structure. We believe that
representing deeper syntactic structure should
help tremendously as it will model these relatively
longer dependencies.
We also calculated a confusion matrix for the
different classes. The majority of the errors are
identification errors where an annotatable is con-
sidered an O class as opposed to one of the 3 rel-
evant classes. This suggests that identifying the
annotatable words is a harder task than classifica-
tion into one of the three classes, which is consis-
tent with our observation from the interannotator
disagreements where most of their disagreements
were on the annotatable tokens, though a small
overall number of tokens, 103 tokens out of 6188,
it was the most significant disagreement category.
We find that for the TOK+POS condition, CBs are
mistagged as un-annotatable O 55% of the time.
We find most of the confusions between NA and
CB, and NCB and CB, both cases favoring a CB
tag.
6 Conclusion
We presented a preliminary pilot study of belief
annotation and automatic tagging. Even though
the data is relatively tiny, we show that automatic
prediction of a belief class is a feasible task. Us-
ing syntactic features, we are able to obtain signif-
icant improvements over a simple baseline of 23%
F-measure absolute points. The best performing
automatic tagging condition is where we use POS
tag, word type feature AlphaNumeric, and shallow
syntactic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
CB NA NCB Overall F
TOK 25.12 41.18 13.64 30.3
TOK+NG 33.18 42.29 5 34.25
TOK+AN 30.43 44.57 12.24 33.92
TOK+AN+NG 37.17 42.46 9.3 36.61
TOK+POS 54.8 59.23 13.95 53.5
TOK+NG+POS 43.15 50.5 22.73 44.35
TOK+AN+POS 54.79 58.97 22.64 53.54
TOK+NG+AN+POS 43.09 54.98 18.18 45.91
TOK+POS+CHUNK 55.45 57.5 15.38 52.77
TOK+POS+VT+CHUNK 53.74 57.14 14.29 51.43
TOK+AN+POS+CHUNK 55.89 59.59 22.58 53.97
TOK+AN+POS+VT+CHUNK 56.27 58.87 12.9 52.89
Table 1: Final results averaged over 5 folds of test data using different features and their combinations:
NG is NGRAM, AN is AlphaNumeric, VT is verbtype
In the future we are looking at ways of adding
more sophisticated deep syntactic and semantic
features using lexical chains from discourse struc-
ture. We will also be exploring belief annotation in
Arabic and Urdu on a parallel data collection since
these languages express evidentiality in ways that
differ linguistically from English. Finally we will
explore ways of automatically augmenting the la-
beled data pool using active learning.
Acknowledgement
This work was supported by grants from the Hu-
man Language Technology Center of Excellence.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the sponsor.
References
Philip R. Cohen and Hector J. Levesque. 1990. Ratio-
nal interaction as the basis for communication. In
Jerry Morgan Philip Cohen and James Allen, edi-
tors, Intentions in Communication. MIT Press.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Rebecca Bruce Matthew Bell Janyce Wiebe,
Theresa Wilson and Melanie Martin. 2004.
Learning subjective language. In Computational
Linguistics, Volume 30 (3).
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In European
Language Resources Association (ELRA), editor,
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco, May 28?30.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Johanna Moore. 1994. Participating in Explanatory
Dialogues. MIT Press.
Rene? Witte Ralf Krestel and Sabine Bergler. 2007.
Processing of Beliefs extracted from Reported
Speech in Newspaper Articles. In International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2007), Borovets, Bul-
garia, September 27?29.
Saur?? Roser, Marc Verhagen, and James Pustejovsky.
2006. Annotating and Recognizing Event Modality
in Text. In FLAIRS 2006, editor, In Proceedings
of the 19th International FLAIRS Conference, Mel-
bourne Beach, Florida, May 11-13.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801?808, Manchester, UK, August.
Coling 2008 Organizing Committee.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky, pages 53?60, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1059?1070, Dublin, Ireland, August 23-29 2014.
Automatic Classification of Communicative Functions of Definiteness
Archna Bhatia
?,?
Chu-Cheng Lin
?
Nathan Schneider
?
Yulia Tsvetkov
?
Fatima Talib Al-Raisi
?
Laleh Roostapour
?
Jordan Bender
?
Abhimanu Kumar
?
Lori Levin
?
Mandy Simons
?
Chris Dyer
?
?
Carnegie Mellon University
?
University of Pittsburgh
Pittsburgh, PA 15213 Pittsburgh, PA 15260
?archnab@cs.cmu.edu
Abstract
Definiteness expresses a constellation of semantic, pragmatic, and discourse properties?the
communicative functions?of an NP. We present a supervised classifier for English NPs that
uses lexical, morphological, and syntactic features to predict an NP?s communicative function in
terms of a language-universal classification scheme. Our classifiers establish strong baselines for
future work in this neglected area of computational semantic analysis. In addition, analysis of
the features and learned parameters in the model provides insight into the grammaticalization of
definiteness in English, not all of which is obvious a priori.
1 Introduction
Definiteness is a morphosyntactic property of noun phrases (NPs) associated with semantic and pragmatic
characteristics of entities and their discourse status. Lyons (1999), for example, argues that definite
markers prototypically reflect identifiability (whether a referent for the NP can be identified by the
discourse participants or not); other aspects identified in the literature include uniqueness of the entity
in the world and whether the hearer is already familiar with the entity given the context and preceding
discourse (Roberts, 2003; Abbott, 2006). While some morphosyntactic forms of definiteness are employed
by all languages?namely, demonstratives, personal pronouns, and possessives?languages display a vast
range of variation with respect to the form and meaning of definiteness. For example, while languages
like English make use of definite and indefinite articles to distinguish between the discourse status of
various entities (the car vs. a car vs. cars), many other languages?including Czech, Indonesian, and
Russian?do not have articles (although they do have demonstrative determiners). Sometimes definiteness
is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in
Chinese (a language without articles), where the existential construction can be used to express indefinite
subjects and the ba- construction can be used to express definite direct objects (Chen, 2004).
Aside from this variation in the form of (in)definite NPs within and across languages, there is also vari-
ability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites
expressing these functions. We refer to these as communicative functions of definiteness, following
Bhatia et al. (2014). Croft (2003, pp. 6?7) shows that even when two languages have access to the
same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite
or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French
translations (both languages use definite as well as indefinite articles) such as:
(1) He showed extreme care. (unmarked)
Il montra un soin extr?me. (indef.)
(2) I love artichokes and asparagus. (unmarked)
J?aime les artichauts et les asperges. (def.)
(3) His brother became a soldier. (indef.)
Son fr?re est devenu soldat. (unmarked)
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
1059
? NONANAPHORA [?A,?B] 999
? UNIQUE [+U] 287
*
UNIQUE_HEARER_OLD [+F,?G,+S] 251
? UNIQUE_PHYSICAL_COPRESENCE [+R] 13
? UNIQUE_LARGER_SITUATION [+R] 237
? UNIQUE_PREDICATIVE_IDENTITY [+P] 1
*
UNIQUE_HEARER_NEW [?F] 36
? NONUNIQUE [?U] 581
*
NONUNIQUE_HEARER_OLD [+F] 169
? NONUNIQUE_PHYSICAL_COPRESENCE [?G,+R,+S] 39
? NONUNIQUE_LARGER_SITUATION [?G,+R,+S] 117
? NONUNIQUE_PREDICATIVE_IDENTITY [+P] 13
*
NONUNIQUE_HEARER_NEW_SPEC [?F,?G,+R,+S] 231
*
NONUNIQUE_NONSPEC [?G,?S] 181
? GENERIC [+G,?R] 131
*
GENERIC_KIND_LEVEL 0
*
GENERIC_INDIVIDUAL_LEVEL 131
? ANAPHORA [+A] 1574
? BASIC_ANAPHORA [?B,+F] 795
*
SAME_HEAD 556
*
DIFFERENT_HEAD 329
? EXTENDED_ANAPHORA [+B] 779
*
BRIDGING_NOMINAL [?G,+R,+S] 43
*
BRIDGING_EVENT [+R,+S] 10
*
BRIDGING_RESTRICTIVE_MODIFIER [?G,+S] 614
*
BRIDGING_SUBTYPE_INSTANCE [?G] 0
*
BRIDGING_OTHER_CONTEXT [+F] 112
? MISCELLANEOUS [?R] 732
? PLEONASTIC [?B,?P] 53
? QUANTIFIED 248
? PREDICATIVE_EQUATIVE_ROLE [?B,+P] 58
? PART_OF_NONCOMPOSITIONAL_MWE 100
? MEASURE_NONREFERENTIAL 125
? OTHER_NONREFERENTIAL 148
+ ? 0 + ? 0 + ? 0 + ? 0
Anaphoric 1574 999 732 Generic 131 1476 1698 Predicative 72 53 3180 Specific 1305 181 1819
Bridging 779 1905 621 Familiar 1327 267 1711 Referential 690 863 1752 Unique 287 581 2437
Figure 1: CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the
corpus. Internal (non-leaf) labels are in bold; these are not annotated or predicted. +/? values are shown
for ternary attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique; these are inherited from supercategories, but otherwise default to 0. Thus, for example, the
full attribute specification for UNIQUE_PHYSICAL_COPRESENCE is [?A,?B,+F,?G,0P,+R,+S,+U].
Counts for these attributes are shown in the table at bottom.
A cross-linguistic classification of communicative functions should be able to characterize the aspects
of meaning that account for the different patterns of definiteness marking exhibited in (1?3): e.g., that
(2) concerns a generic class of entities while (3) concerns a role filled by an individual. For more on
communicative functions, see ?2.
This paper develops supervised classifiers to predict communicative function labels for English NPs
using lexical, morphological, and syntactic features. The contribution of our work is in both the output of
the classifiers and the models themselves (features and weights). Each classifier predicts communicative
function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. Such
functions are useful in a variety of language processing applications. For example, they should usually be
preserved in translation, even when the grammatical mechanisms for expressing them are different. The
communicative function labels also represent the discourse status of entities, making them relevant for
entity tracking, knowledge base construction, and information extraction.
Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphological
features to properties of communicative functions. The learned weights of this model can, e.g., gener-
ate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorously
through controlled experiments. This hypothesis generation is linguistically significant as it indicates new
grammatical mechanisms beyond the obvious a and the articles that are used for expressing definiteness
in English.
To build our models, we leverage a cross-lingual definiteness annotation scheme (?2) and annotated
English corpus (?3) developed in prior work (Bhatia et al., 2014). The classifiers, ?4, are supervised
models with features that combine lexical and morphosyntactic information and the prespecified attributes
or groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in fig. 1) to
predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (?5) include one that
exploits these label groupings to award partial credit according to relatedness. ?6 presents experiments
comparing several models and discussing their strengths and weaknesses; computational work and
applications related to definiteness are addressed in ?7.
1060
2 Annotation scheme
The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoric-
ity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel
et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell,
1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define
it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003)
proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite
descriptions. However, possessive definite descriptions (John?s daughter) and the weak definites (the son
of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they
are spoken. In contrast to the reductionist approaches are approaches to grammaticalization (Hopper and
Traugott, 2003) in which grammar develops over time in such a way that each grammatical construction
has some prototypical communicative functions, but may also have many non-prototypical communica-
tive functions. The scheme we are adopting for this work?the annotation scheme for Communicative
Functions of Definiteness (CFD) as described in Bhatia et al. (2014)?assumes that there may be multiple
functions to definiteness. CFD is based on a combination of these functions and is summarized in fig. 1. It
was developed by annotating texts in two languages (English and Hindi) for four different genres?namely
TED talks, a presidential inaugural speech, news articles, and fictional narratives?keeping in mind the
communicative functions that have been associated with definiteness in the linguistic literature.
CFD is hierarchically organized. This hierarchical organization serves to reduce the number of decisions
that an annotator needs to make for speed and consistency. We now highlight some of the major distinctions
in the hierarchy.
At the highest level, the distinction is made between Anaphora, Nonanaphora, and Miscellaneous
functions of an NP (the annotatable unit). Anaphora and Nonanaphora respectively describe whether
an entity is old or new in the discourse; the Miscellaneous function is mainly assigned to various kinds of
nonreferential NPs.
The Anaphora category has two subcategories: Basic_Anaphora and Extended_Anaphora. Ba-
sic_Anaphora applies to NPs referring to entities that have been mentioned before. Extended_Anaphora
applies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentioned
entity. For example, after mentioning a wedding, the bride, the groom, and the cake are considered to be
Extended_Anaphora.
Within the Nonanaphora category, a first distinction is made between Unique, Nonunique, and
Generic. The Unique function applies to NPs whose referent becomes unique in a context for any of
several reasons. For example, Obama can safely be considered unique in contemporary political discourse
in the United States. The function Nonunique applies to NPs that start out with multiple possible referents
and that may or may not become identifiable in a speech situation. For example, a little riding hood of
red velvet in fig. 2 could be annotated with the label Nonunique. Finally, Generic NPs refer to classes
or types of entities rather than specific entities. For example, Dinosaurs in Dinosaurs are extinct. is a
Generic NP.
Another important distinction CFD makes is between Hearer_Old for references to entities that are
familiar to the hearer (e.g., if they are physically present in the speech situation), versus Hearer_New
for nonfamiliar references. This distinction cuts across the two subparts of the hierarchy, Anaphora
and Nonanaphora; thus, labels marking Hearer_Old or Hearer_New also encode other distinctions
(e.g., Unique_Hearer_Old, Unique_Hearer_New, Nonunique_Hearer_Old). For further details on
the annotation scheme, see fig. 1 and Bhatia et al. (2014).
Because the ordering of distinctions determines the tree structure of the hierarchy, the same commu-
nicative functions could have been organized in a superficially different way. In fact, Komen (2013) has
proposed a hierarchy with similar leaf nodes, but different internal structure. Since it is possible that
some natural groupings of labels are not reflected in the hierarchy we used, we also decompose each
label into fundamental communicative functions, which we call attributes. Each label type is associated
with values for attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique. These attributes can have values of +, ?, or 0, as shown in fig. 1. For instance, with the Anaphoric
1061
Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother,
and there was nothing that she would not have given to the child.
Once she
SAME_HEAD
gave her
DIFFERENT_HEAD
a little riding hood of red velvet
OTHER_NONREFERENTIAL
NONUNIQUE_HEARER_NEW_SPEC
, which suited her
SAME_HEAD
so well that
she
SAME_HEAD
would never wear anything else
QUANTIFIED
; so she
SAME_HEAD
was always called ?Little Red Riding Hood
UNIQUE_HEARER_NEW
.?
Figure 2: An annotated sentence from ?Little Red Riding Hood.? The previous sentence is shown for
context.
attribute, a value of + applies to labels that can never mark NPs new to the discourse, ? applies to labels
that can only apply if the NP is new in the discourse, and 0 applies to labels such as Pleonastic (where
anaphoricity is not applicable because there is no discourse referent).
3 Data
We use the English definiteness corpus of Bhatia et al. (2014), which consists of texts from multiple genres
annotated with the scheme described in ?2.
1
The 17 documents consist of prepared speeches (TED talks
and a presidential address), published news articles, and fictional narratives. The TED data predominates
(75% of the corpus);
2
the presidential speech represents about 16%, fictional narratives 5%, and news
articles 4%. All told, the corpus contains 13,860 words (868 sentences), with 3,422 NPs (the annotatable
units). Bhatia et al. (2014) report high inter-annotator agreement, estimating Cohen?s ? = 0.89 within the
TED genre as well as for all genres.
Figure 2 is an excerpt from the ?Little Red Riding Hood? annotated with the CFD scheme.
4 Classification framework
To model the relationship between the grammar of definiteness and its communicative functions in a
data-driven fashion, we work within the supervised framework of feature-rich discriminative classification,
treating the functional categories from ?2 as output labels y and various lexical, morphological, and
syntactic characteristics of the language as features of the input x. Specifically, we learn two kinds
of probabilistic models. The first is a log-linear model similar to multiclass logistic regression, but
deviating in that logistic regression treats each output label (response) as atomic, whereas we decompose
each into attributes based on their linguistic definitions, enabling commonalities between related labels
to be recognized. Each weight in the model corresponds to a feature that mediates between percepts
(characteristics of the input NP) and attributes (characteristics of the label). This is aimed at attaining
better predictive accuracy as well as feature weights that better describe the form?function interactions we
are interested in recovering. We also train a random forest model on the hypothesis that it would allow us
to sacrifice interpretability of the learned parameters for predictive accuracy.
Our setup is formalized below, where we discuss the mathematical models and linguistically motivated
features.
4.1 Models
We experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemble
model. Due to their consistency and interpretability, linear models are a valuable tool for quantifying and
analyzing the effects of individual features. Non-linear models, while less interpretable, often outperform
logistic regression (Perlich et al., 2003), and thus could be desirable when the predictions are needed for a
downstream task.
1
The data can be obtained from http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus.
2
The TED talks are from a large parallel corpus obtained from http://www.ted.com/talks/.
1062
4.1.1 Log-linear model
At test time, we model the probability of communicative function label y conditional on an NP x as
follows:
p
?
(y?x) = log
exp?
?
f(x,y)
?
y
??Y exp?
?
f(x,y?)
(1)
where ? ?Rd is a vector of parameters (feature weights), and f ?X ?Y ?Rd is the feature function over
input?label pairs. The feature function is defined as follows:
f(x,y) = ? (x)? ??(y) (2)
where the percept function ? ?X ?Rc produces a vector of real-valued characteristics of the input, and
the attribute function
?
? ?Y ? {0,1}a encodes characteristics of each label. There is a feature for every
percept?attribute pairing: so d = c ?a and f(i?1)a+ j(x,y) = ?i(x) ?? j(y),1 ? i ? c,1 ? j ? a.
3
The contents of
the percept and attribute functions are detailed in ?4.2 and ?4.3 respectively.
For prediction, having learned weights
?
? we use the Bayes-optimal decision rule for minimizing
misclassification error, selecting the y that maximizes this probability:
y?? argmax
y?Y
p
?
?
(y?x) (3)
Training optimizes
?
? so as to maximize a convex L
2
-regularized
4
learning objective over the training data
D:
?
? = argmax
?
?? ??? ??
2
2
+ ?
?x,y??D
log
exp?
?
f(x,y)
?
y
??Y exp(?
?
f(x,y?))
(4)
With
?
?(y) = the identity of the label, this reduces to standard logistic regression.
4.1.2 Non-linear model
We employ a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learned
from many independent subsamples of the training data. Given an input, each tree classifier assigns a
probability to each label; those probabilities are averaged to compute the probability distribution across
the ensemble.
An important property of the random forests, in addition to being an effective tool in prediction, is
their immunity to overfitting: as the number of trees increases, they produce a limiting value of the
generalization error.
5
Thus, no hyperparameter tuning is required. Random forests are known to be
robust to sparse data and to label imbalance (Chen et al., 2004), both of which are challenges with the
definiteness dataset.
4.2 Percepts
The characteristics of the input that are incorporated in the model, which we call percepts to distinguish
them from model features linking inputs to outputs, see ?4.1, are intended to capture the aspects of English
morphosyntax that may be relevant to the communicative functions of definiteness.
After preprocessing the text with a dependency parser and coreference resolver, which is described in
?6.1, we extract several kinds of percepts for each NP.
4.2.1 Basic
Words of interest. These are the head within the NP, all of its dependents, and its governor (external to
the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing
the dependency path upward from the head. For each of these words, we have separate percepts capturing:
the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a
3
Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection.
4
As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are
excluded from regularization.
5
See Theorem 1.2 in Breiman (2001) for details.
1063
binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we
have additional features specific to the first and the last one. Moreover, to better capture tense, aspect
and modality, we collect the attached verb?s auxiliaries. We also make note of the negative particle (with
dependency label neg) if it is a dependent of the verb.
Structural. The structural percepts are: the path length from the head up to the root, and to the attached
verb. We also have percepts for the number of dependents, and the number of dependency relations that
link non-neighbors. Integer values were binarized with thresholding.
Positional. These percepts are the token length of the NP, the NP?s location in the sentence (first or
second half), and the attached verb?s position relative to the head (left or right). 12 additional percept
templates record the POS and lemma of the left and right neighbors of the head, governor, and attached
verb.
4.2.2 Contextual NPs
When extracting features for a given NP (call it the ?target?), we also consider NPs in the following
relationship with the target NP: its immediate parent, which is the smallest NP whose span fully subsumes
that of the target; the immediate child, which is the largest NP subsumed within the target; the immediate
precedent and immediate successor within the sentence; and the nearest preceding coreferent mention.
For each of these related NPs, we include all of their basic percepts conjoined with the nature of the
relation to the target.
4.3 Attributes
As noted above, though CFD labels are organized into a tree hierarchy, there are actually several dimensions
of commonality that suggest different groupings. These attributes are encoded as ternary characteristics;
for each label (including internal labels), every one of the 8 attributes is assigned a value of +, ?, or 0
(refer to fig. 1). In light of sparse data, we design features to exploit these similarities via the attribute
vector function
?(y) = [y,A(y),B(y),F(y),G(y),P(y),R(y),S(y),U(y)]
?
(5)
where A ?Y ? {+,?,0} returns the value for Anaphoric, B(y) for Bridging, etc. The identity of the label
is also included in the vector so that different labels are always recognized as different by the attribute
function. The categorical components of this vector are then binarized to form
?
?(y); however, instead
of a binary component that fires for the 0 value of each ternary attribute, there is a component that fires
for any value of the attribute?a sort of bias term. The weights assigned to features incorporating + or ?
attribute values, then, are easily interpreted as deviations relative to the bias.
5 Evaluation
The following measures are used to evaluate our predictor against the gold standard for the held-out
evaluation (dev or test) set E :
? Exact Match: This accuracy measure gives credit only where the predicted and gold labels are identical.
? By leaf label: We also compute precision and recall of each leaf label to determine which categories
are reliably predicted.
? Soft Match: This accuracy measure gives partial credit where the predicted and gold labels are
related. It is computed as the proportion of attributes-plus-full-label whose (categorical) values match:
??(y)??(y?)?/9.
6 Experiments
6.1 Experimental Setup
Data splits. The annotated corpus of Bhatia et al. (2014) (?3) contains 17 documents in 3 genres:
13 prepared speeches (mostly TED talks),
6
2 newspaper articles, and 2 fictional narratives. We arbitrarily
choose some documents to hold out from each genre; the resulting test set consists of 2 TED talks
6
We have combined the TED talks and presidential speech genres since both involved prepared speeches.
1064
Condition ?? ? ? Exact Match Acc. Soft Match Acc.
Majority baseline ? ? 12.1 47.8
Log-linear classifier, attributes only 473,064 100 38.7 77.1
Log-linear classifier, labels only 413,931 100 40.8 73.6
Full log-linear classifier (labels + attributes) 926,417 100 43.7 78.2
Random forest classifier 20,363 ? 49.7 77.5
Table 1: Classifiers and baseline, as measured on the test set. The first two columns give the number of
parameters and the tuned regularization hyperparameter, respectively; the third and fourth columns give
accuracies as percentages. The best in each column is bolded.
(?Alisa_News?, ?RobertHammond_park?), 1 newspaper article (?crime1_iPad_E?), and 1 narrative
(?Little Red Riding Hood?). The test set then contains 19,28 tokens (111 sentences), in which there are
511 annotated NPs; while the training set contains 2,911 NPs among 11,932 tokens (757 sentences).
Preprocessing. Automatic dependency parses and coreference information were obtained with the
parser and coreference resolution system in Stanford CoreNLP v. 3.3.0 (Socher et al., 2013; Recasens
et al., 2013) for use in features (?4.2). Syntactic features were extracted from the Basic dependencies
output by the parser. To evaluate the performance of Stanford system on our data, we manually inspected
the dependencies and coreference information for a subset of sentences from our corpus (using texts
from TED talks and fictional narratives genres) and recorded the errors. We found that about 70% of the
sentences had all correct dependencies, and only about 0.04% of the total dependencies were incorrect
for our data. However, only 62.5% of the coreference links were correctly identified by the coreference
resolver. The rest of them were either missing or incorrectly identified. We believe this may have caused a
portion of the classifier errors while predicting the Ananphoic labels.
Throughout our experiments (training as well as testing), we use the gold NP boundaries identified by
the human annotators. The automatic dependency parses are used to extract percepts for each gold NP.
If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extracting
misleading percepts, we assign a default value.
Learning. The log-linear model variants are trained with an in-house implementation of supervised
learning with L
2
-regularized AdaGrad (Duchi et al., 2011). Hyperparameters are tuned on a development
set formed by holding out every tenth instance from the training set (test set experiments use the full
training set): the power of 10 giving the highest Soft Match accuracy was chosen for ? .
7
The Python
scikit-learn toolkit (Pedregosa et al., 2011) was used for the random forest classifier.8
6.2 Results
Measurements of overall classification performance appear in table 1. While far from perfect, our
classifiers achieve promising accuracy levels given the small size of the training data and the number of
labels in the annotation scheme. The random forest classifier is the most accurate in Exact Match, likely
due to the robustness of that technique under conditions where the data are small and the frequencies
of individual labels are imbalanced. By the Soft Match measure, our attribute-aware log-linear models
perform very well. The most successful of the log-linear models is the richest model, which combines the
fine-grained communicative function labels with higher-level attributes of those labels. But notably the
attribute-only model, which decomposes the semantic labels into attributes without directly considering
the full label, performs almost as well as the random forest classifier in Soft Match. This is encouraging
because it suggests that the model has correctly exploited known linguistic generalizations to account for
the grammaticalization of definiteness in English.
Table 2 reports the precision and recall of each leaf label predicted. Certain leaf labels are found
to be easier for the classifier to predict: e.g., the communicative function label Pleonastic has a high
F
1
score. This is expected as the Ploenastic CFD for English is quite regular and captured by the EX
7
Preliminary experiments with cross-validation on the training data showed that the value of ? was stable across folds.
8
Because it is a randomized algorithm, the results may vary slightly between runs; however, a cross-validation experiment on
the training data found very little variance in accuracy.
1065
Leaf label N P R F
1
Leaf label N P R F
1
Pleonastic 44 100 78 88 Part_of_Noncompositional_MWE 88 20 17 18
Bridging_Restrictive_Modifier 552 58 84 68 Bridging_Nominal 33 33 10 15
Quantified 213 57 57 57 Generic_Individual_Level 113 14 11 13
Unique_Larger_Situation 97 52 58 55 Nonunique_Nonspec 173 9 25 13
Same_Head 452 41 41 41 Bridging_Other_Context 96 33 6 11
Measure_Nonreferential 98 88 26 40 Bridging_Event 9 ? 0 ?
Nonunique_Hearer_New_Spec 190 36 46 40 Nonunique_Physical_Copresence 36 0 0 ?
Other_Nonreferential 134 39 36 37 Nonunique_Predicative_Identity 10 ? 0 ?
Different_Head 271 32 33 32 Predicative_Nonidentity 57 0 0 ?
Nonunique_Larger_Situation 97 29 25 27 Unique_Hearer_New 26 ? 0 ?
Table 2: Number of training set instances and precision, recall, and F
1
percentages for leaf labels.
part-of-speech tag. The classifier finds predictions of certain CFD labels, such as Bridging_Event,
Bridging_Nominal and Nonunique_Nonspecific, to be more difficult due to data sparseness: it appears
that there were not enough training instances for the classifier to learn the generalizations corresponding
to these CFDs. Bridging_Other_Context was hard to predict as this was a category which referred not
to the entities previously mentioned but to the whole speech event from the past. There seem to be no
clear morphosyntactic cues associated with this CFD, so to train a classifier to predict this category label,
we would need to model more complex semantic and discourse information. This also applies to the
classifier confusion between the Same_Head and Different_Head, since both of these labels share all
the semantic attributes used in this study.
An advantage of log-linear models is that inspecting the learned feature weights can provide useful
insights into the model?s behavior. Figure 3 lists 10 features that received the highest positive weights
in the full model for the + and ? values of the Specific attribute. These confirm some known properties
of English definites and indefinites. The definite article, possessives (PRP$), proper nouns (NNP), and the
second person pronoun are all associated with specific NPs, while the indefinite article is associated with
nonspecific NPs. The model also seems to have picked up on the less obvious but well-attested tendency
of objects to be nonspecific (Aissen, 2003).
In addition to confirming known grammaticalization patterns of definiteness, we can mine the highly-
weighted features for new hypotheses: e.g., in figs. 3 and 4, the model thinks that objects of ?from? are
especially likely to be Specific, and that NPs with comparative adjectives (JJR) are especially likely to be
nonspecific (fig. 3). From fig. 3, we also know that Num. of dependents, dependent?s POS: 1,PRP$ has
a higher weight than, say, Num. of dependents, dependent?s POS: 2,PRP$. This observation suggests a
hypothesis that in English the NPs which have possessive pronouns immediately preceding the head are
more likely to be specific than the NPs which have intervening words between the possessive pronoun
and the head. Similarly, looking at another example in fig. 4, the following two percepts get high weights
for the NP the United States of America to be Specific: last dependent?s POS: NNP and first dependent?s
lemma: the. Since frequency and other factors affect the feature weights learned by the classifier, these
differences in weights may or may not reflect an inherent association with Specificity. Whether these
are general trends, or just an artifact of the sentences that happened to be in the training data and our
statistical learning procedure, will require further investigation, ideally with additional datasets and more
rigorous hypothesis testing.
Finally, we can remove features to test their impact on predictive performance. Notably, in experiments
ablating features indicating articles?the most obvious exponents of definiteness in English?we see
a decrease in performance, but not a drastic one. This suggests that the expression of communicative
functions of definiteness is in fact much richer than morphological definiteness.
Errors. Several labels are unattested or virtually unattested in the training data, so the models unsurpris-
ingly fail to predict them correctly at test time. Same_Head and Different_Head, though both common,
are confused quite frequently. Whether the previous coreferent mention has the same or different head is a
simple distinction for humans; low model accuracy is likely due to errors propagated from coreference
resolution. This problem is so frequent that merging these two categories and retraining the random
forest model improves Exact Match accuracy by 8% absolute and Soft Match accuracy by 5% absolute.
1066
Percepts
+Specific ?Specific
First dependent?s POS PRP$ First dependent?s lemma a
Head?s left neighbor?s POS PRP$ Last dependent?s lemma a
Last dependent?s lemma you Num. of dependents, dependent?s lemma 1,a
Num. of dependents, dependent?s lemma 1,you Head?s left neighbor?s POS JJR
Num. of dependents, dependent?s POS 1,PRP$ Last dependent?s POS JJR
Governor?s right neighbor?s POS PRP$ Num. of dependents, dependent?s lemma 2,a
Last dependent?s POS NNP First dependent?s lemma new
Last dependent?s POS PRP$ Last dependent?s lemma new
First dependent?s lemma the Num. of dependents, dependent?s POS 2,JJR
Governor?s lemma from Governor?s left neighbor?s POS VB
Figure 3: Percepts receiving highest positive weights in association with values of the Specific attribute.
Example Relevant percepts from fig. 3 CFD annotation
This is just for the United States of America. Last dependent?s POS: NNP
First dependent?s lemma: the
Unique_Larger_Situation
We were driving from our home in Nashville
to a little farm we have 50 miles east of
Nashville ? driving ourselves.
First dependent?s POS: PRP$
Head?s left neighbor?s POS: PRP$
Governor?s right neighbor?s POS: PRP$
Governor?s lemma: from
Bridging_Restrictive_Modifier
Figure 4: Sentences from our corpus illustrating percepts fired for gold NPs and their CFD annotations.
Another common confusion is between the highly frequent category Unique_Larger_Situation and the
rarer category Unique_Hearer_New; the latter is supposed to occur only for the first occurrence of a
proper name referring to a entity that is not already part of the knowledge of the larger community. In
other words, this distinction requires world knowledge about well-known entities, which could perhaps be
mined from the Web or other sources.
7 Related Work
Because semantic/pragmatic analysis of referring expressions is important for many NLP tasks, a compu-
tational model of the communicative functions of definiteness has the potential to leverage diverse lexical
and grammatical cues to facilitate deeper inferences about the meaning of linguistic input. We have used
a coreference resolution system to extract features for modeling definiteness, but an alternative would be
to predict definiteness functions as input to (or jointly with) the coreference task. Applications such as
information extraction and dialogue processing could be expected to benefit not only from coreference
information, but also from some of the semantic distinctions made in our framework, including specificity
and genericity.
Better computational processing of definiteness in different languages stands to help machine translation
systems. It has been noted that machine translation systems face problems when the source and the target
language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov
et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either
(a) preprocessing the source language to make it look more like the target language (Collins et al., 2005;
Habash, 2007; Nie?en and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine
translation output to match the target language, (e.g., Popovi
?
c et al., 2006). Attempts have also been made
to use syntax on the source and/or the target sides to capture the syntactic differences between languages
(Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite
articles has been found beneficial in a variety of applications, including postediting of MT output (Knight
and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction
of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013)
trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and
used this classifier to improve the quality of statistical machine translation.
While definiteness morpheme prediction has been thoroughly studied in computational linguistics,
1067
studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit
linguistically-motivated features in a supervised approach to distinguish between generic and specific
NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve
the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong
et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been
conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness
more broadly.
Our work is related to research in linguistics on the modeling of syntactic constructions such as dative
shift and the expression of possession with ?of? or ??s?. Bresnan and Ford (2010) used logistic regression
with semantic features to predict syntactic constructions. Although we are doing the opposite (using
syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as
mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper
and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in
multiple communicative functions for each grammatical construction. Other attempts have also been made
to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have
been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation,
Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by
prepositions.
8 Conclusion
We have presented a data-driven approach to modeling the relationship between universal communicative
functions associated with (in)definiteness and their lexical/grammatical realization in a particular language.
Our feature-rich classifiers can give insights into this relationship as well as predict communicative
functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear
classifier compares favorably to the random forest classifier in Soft Match accuracy. Further improvements
to the classifier may come from additional features or better preprocessing. This work has focused on
English, but in future work we plan to build similar models for other languages?including languages
without articles, under the hypothesis that such languages will rely on other, subtler devices to encode
many of the functions of definiteness.
Acknowledgments
This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533. We thank the reviewers for their useful comments.
References
Barbara Abbott. 2006. Definite and indefinite. In Keith Brown, editor, Encyclopedia of Language and Linguistics,
pages 3?392. Elsevier.
Judith Aissen. 2003. Differential object marking: iconicity vs. economy. Natural Language & Linguistic Theory,
21(3):435?483.
Archna Bhatia, Mandy Simons, Lori Levin, Yulia Tsvetkov, Chris Dyer, and Jordan Bender. 2014. A unified anno-
tation scheme for the semantic/pragmatic components of definiteness. In Proc. of LREC. Reykjav?k, Iceland.
Betty Birner and Gregory Ward. 1994. Uniqueness, familiarity and the definite article in English. In Proc. of the
Twentieth Annual Meeting of the Berkeley Linguistics Society, pages 93?102.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Joan Bresnan and Marilyn Ford. 2010. Predicting syntax: Processing dative constructions in American and Aus-
tralian varieties of English. Language, 86(1):168?213.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proc. of EMNLP, pages 1677?1687. Seattle, Washington, USA.
1068
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Using random forest to learn imbalanced data. University of
California, Berkeley.
Ping Chen. 2004. Identifiability and definiteness in Chinese. Linguistics, 42:1129?1184.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation.
In Proc. of ACL, pages 531?540. Ann Arbor, Michigan.
Cleo Condoravdi. 1992. Strong and weak novelty and familiarity. In Proc. of SALT II, pages 17?37.
William Croft. 2003. Typology and Universals. Cambridge University Press.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121?2159.
Michael Elhadad. 1993. Generating argumentative judgment determiners. In Proc. of AAAI, pages 344?349.
Gareth Evans. 1977. Pronouns, quantifiers and relative clauses. Canadian Journal of Philosophy, 7(3):46.
Gareth Evans. 1980. Pronouns. Linguistic Inquiry, 11.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1988. The generation and interpretation of demonstrative
expressions. In Proc. of XIIth International Conference on Computational Linguistics, pages 216?221.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1993. Cognitive status and the form of referring expres-
sions in discourse. Language, 69:274?307.
Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In MT Summit XI, pages 215?222.
Copenhagen.
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?129.
Irene Heim. 1990. E-type pronouns and donkey anaphora. Linguistics and Philosophy, 13:137?177.
Iris Hendrickx, Orph?e De Clercq, and V?ronique Hoste. 2011. Analysis and reference resolution of bridge
anaphora across different text genres. In Iris Hendrickx, Sobha Lalitha Devi, Antonio Horta Branco, and Ruslan
Mitkov, editors, DAARC, volume 7099 of Lecture Notes in Computer Science, pages 1?11. Springer.
Paul J. Hopper and Elizabeth Closs Traugott. 2003. Grammaticalization. Cambridge University Press.
Nirit Kadmon. 1987. On unique and non-unique reference and asymmetric quantification. Ph.D. thesis, University
of Massachusetts.
Nirit Kadmon. 1990. Uniqueness. Linguistics and Philosophy, 13:273?324.
Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proc. of the National Conference
on Artificial Intelligence, pages 779?779. Seattle, WA.
Erwin Ronald Komen. 2013. Finding focus: a study of the historical development of focus in English. LOT,
Utrecht.
Fang Kong, Guodong Zhou, Longhua Qian, and Qiaoming Zhu. 2010. Dependency-driven anaphoricity determi-
nation for coreference resolution. In Proc. of COLING, pages 599?607. Beijing, China.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proc. of COLING/ACL, pages 609?616. Sydney, Australia.
Christopher Lyons. 1999. Definiteness. Cambridge University Press.
Guido Minnen, Francis Bond, and Ann Copestake. 2000. Memory-based learning for article generation. In Proc. of
1069
the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language
Learning, pages 43?48.
Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING. Taipei, Taiwan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proc. of
COLING, pages 1081?1085.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, M. Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree induction vs. logistic regression: a learning-
curve analysis. Journal of Machine Learning Research, 4:211?255.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of German compound words.
In Advances in Natural Language Processing, pages 616?624. Springer.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen
Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowd-
sourcing. In Proc. of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,
ExProM ?12, pages 57?64.
Ellen F. Prince. 1992. The ZPG letter: Subjects, definiteness and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a fund raising text, pages 295?325. John Benjamins.
Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse
entities: identifying singleton mentions. In Proc. of NAACL-HLT, pages 627?633. Atlanta, Georgia, USA.
Roi Reichart and Ari Rappoport. 2010. Tense sense disambiguation: A new syntactic polysemy task. In Proc. of
EMNLP, EMNLP ?10, pages 325?334.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proc. of ACL, pages 40?49. Uppsala,
Sweden.
Craig Roberts. 2003. Uniqueness in definite noun phrases. Linguistics and Philosophy, 26:287?350.
Alla Rozovskaya and Dan Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Proc.
of NAACL-HLT, pages 154?162.
Bertrand Russell. 1905. On denoting. Mind, New Series, 14:479?493.
Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector
grammars. In Proc. of ACL, pages 455?465. Sofia, Bulgaria.
Vivek Srikumar and Dan Roth. 2013. An inventory of preposition relations. CoRR, abs/1305.5785.
Sara Stymne. 2009. Definite noun phrases in statistical machine translation into Danish. In Proc. of Workshop on
Extracting and Using Constructions in NLP, pages 4?9.
Yulia Tsvetkov, Chris Dyer, Lori Levi, and Archna Bhatia. 2013. Generating English determiners in phrase-based
translation with synthetic translation options. In Proc. of WMT.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proc. of ACL, pages 303?310.
Philadelphia, Pennsylvania, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved chunk-level reordering for statistical machine
translation. In IWSLT 2007: International Workshop on Spoken Language Translation, pages 21?28.
1070
Use of Modality and Negation in
Semantically-Informed Syntactic MT
Kathryn Baker?
U.S. Department of Defense
Michael Bloodgood??
University of Maryland
Bonnie J. Dorr?
University of Maryland
Chris Callison-Burch?
Johns Hopkins University
Nathaniel W. Filardo?
Johns Hopkins University
Christine Piatko?
Johns Hopkins University
Lori Levin||
Carnegie Mellon University
Scott Miller#
BBN Technologies
? U.S. Department of Defense, 9800 Savage Rd., Suite 6811, Fort Meade, MD 20755.
E-mail: kathrynlb@gmail.com.
?? Center for Advanced Study of Language, University of Maryland, 7005 52nd Avenue, College Park, MD
20742. E-mail: meb@umd.edu.
? Department of Computer Science and UMIACS, University of Maryland, AV Williams Building 3153,
College Park, MD 20742. E-mail: bonnie@umiacs.umd.edu.
? Center for Language and Speech Processing, Johns Hopkins University, 3400 N. Charles Street,
Hackerman Hall 320, Baltimore MD 21218. E-mail: {ccb,nwf}@cs.jhu.edu.
? Applied Physics Laboratory, Johns Hopkins University, 11000 Johns Hopkins Rd., Laurel, MD 20723.
E-mail: christine.piatko@jhuapl.edu.
|| Carnegie Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: lsl@cs.cmu.edu.
# BNN Technologies, 10 Moulton Street, Cambridge, MA 02138. E-mail: smiller@bbn.com.
Submission received: 27 March 2011; revised submission received: 28 September 2011; accepted for
publication: 30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
This article describes the resource- and system-building efforts of an 8-week Johns Hopkins
University Human Language Technology Center of Excellence Summer Camp for Applied Lan-
guage Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT). We
describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available)
MN lexicon, and two automated MN taggers that we built using the annotation scheme and
lexicon. Our annotation scheme isolates three components of modality and negation: a trigger
(a word that conveys modality or negation), a target (an action associated with modality or
negation), and a holder (an experiencer of modality). We describe how our MN lexicon was
semi-automatically produced and we demonstrate that a structure-based MN tagger results in
precision around 86% (depending on genre) for tagging of a standard LDC data set.
We apply our MN annotation scheme to statistical machine translation using a syntactic
framework that supports the inclusion of semantic annotations. Syntactic tags enriched with
semantic annotations are assigned to parse trees in the target-language training texts through
a process of tree grafting. Although the focus of our work is modality and negation, the tree
grafting procedure is general and supports other types of semantic information. We exploit this
capability by including named entities, produced by a pre-existing tagger, in addition to the MN
elements produced by the taggers described here. The resulting system significantly outperformed
a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the
NIST 2009 Urdu?English test set. This finding supports the hypothesis that both syntactic and
semantic information can improve translation quality.
1. Introduction
This article describes the resource- and system-building efforts of an 8-week Johns
Hopkins Human Language Technology Center of Excellence Summer Camp for Ap-
plied Language Exploration (SCALE-2009) on Semantically InformedMachine Translation
(SIMT) (Baker et al 2010a, 2010b, 2010c, 2010d). Specifically, we describe our modal-
ity/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two
automated MN taggers that were built using the lexicon and annotation scheme.
Our annotation scheme isolates three components of modality and negation: a
trigger (a word that conveys modality or negation), a target (an action associated with
modality or negation), and a holder (an experiencer of modality). Two examples of MN
tagging are shown in Figure 1.
Note that modality and negation are unified into single MN tags (e.g., the ?Able?
modality tag is combined with ?NOT? to form the ?NOTAble? tag) and also that
Figure 1
Modality/negation tagging examples.
412
Baker et al Modality and Negation in SIMT
MN tags occur in pairs of triggers (e.g., TrigAble and TrigNegation) and targets (e.g.,
TargNOTAble).
We apply our modality and negation mechanism to the problem of Urdu?English
machine translation using a technique that we call tree grafting. This technique incorpo-
rates syntactic labels and semantic annotations in a unified and coherent framework for
implementing semantically informed machine translation. Our framework is not lim-
ited to the semantic annotations produced by the MN taggers that are the subject of this
article and we exploit this capability to additionally include named-entity annotations
produced by a pre-existing tagger. By augmenting hierarchical phrase-based translation
rules with syntactic labels that were extracted from a parsed parallel corpus, and further
augmenting the parse trees with markers for modality, negation, and entities (through
the tree grafting process), we produced a better model for translating Urdu and English.
The resulting system significantly outperformed the linguistically naive baseline Hiero
model, and reached the highest scores yet reported on the NIST 2009 Urdu?English
translation task.
We note that although our largest gains were from syntactic enrichments to the
model, smaller (but significant) gains were achieved by injecting semantic knowledge
into the syntactic paradigm. Verbal semantics (modality and negation) contributed
slightly more gains than nominal semantics (named entities) and their combined gains
were the sum of their individual contributions.
Of course, the limited semantic types we explored (modality, negation, and en-
tities) are only a small piece of the much larger semantic space, but demonstrating
success on these semantic aspects of language, the combination of which has been
unexplored by the statistical machine translation community, bodes well for (larger)
improvements based on the incorporation of other semantic aspects (e.g., relations and
temporal knowledge). Moreover, we believe this syntactic framework to be well suited
for further exploration of the impact of many different types of semantics on the quality
of machine-translation (MT) output. Indeed, it would not have been possible to initiate
the current study without the foundational work that gave rise to a syntactic paradigm
that could support these semantic enrichments.
In the SIMT paradigm, semantic elements (e.g., modality/negation) are identified
in the English portion of a parallel training corpus and projected to the source language
(in our case, Urdu) during a process of syntactic alignment. These semantic elements are
subsequently used in the translation rules that are extracted from the parallel corpus.
The goal of adding them to the translation rules is to constrain the space of possible
translations to more grammatical and more semantically coherent output. We explored
whether including such semantic elements could improve translation output in the face
of sparse training data and few source language annotations. Results were encouraging.
Translation quality, as measured by the Bleu metric (Papineni et al 2002), improved
when the training process for the Joshua machine translation system (Li et al 2009)
used in the SCALE workshop included MN annotation.
We were particularly interested in identifying modalities and negation because
they can be used to characterize events in a variety of automated analytic processes.
Modalities and negation can distinguish realized events from unrealized events, beliefs
from certainties, and can distinguish positive and negative instances of entities and
events. For example, the correct identification and retention of negation in a particular
language?such as a single instance of the word ?not??is very important for a correct
representation of events and likewise for translation.
The next two sections examine related work and the motivation behind the SIMT
approach. Section 4 defines the theoretical framework for ourMN lexicon and automatic
413
Computational Linguistics Volume 38, Number 2
MN taggers. Section 5 presents the MN annotation scheme used by our human annota-
tors and describes the creation of a MN lexicon based on this scheme. Section 6 presents
two types of MN taggers?one that is string-based and one that is structure-based?
and evaluates the effectiveness of the structure-based tagger. Section 7 then presents
implementation details of the semantically informed syntactic system and describes
the results of its application. Finally, Section 8 presents conclusions and future work.
2. Related Work
The development of annotation schemes has become an area of computational lin-
guistics development in its own right, often separate from machine learning applica-
tions. Many projects began as strictly linguistic projects that were later adapted for
computational linguistics. When an annotation scheme is consistent and well devel-
oped, its subsequent application to NLP systems is most effective. For example, the
syntactic annotation of parse trees in the Penn Treebank (Marcus, Marcinkiewicz, and
Santorini 1993) had a tremendous effect on parsing and onNatural Language Processing
in general.
In the case of semantic annotations, each tends to have its unique area of focus.
Although the labeling conventions may differ, a layer of modality annotation over
verb role annotation, for example, can have a complementary effect of providing more
information, rather than being viewed as a competing scheme. We review some of the
major semantic annotation efforts here.
Propbank (Palmer, Gildea, and Kingsbury 2005) is a set of annotations of predicate?
argument structure over parse trees. First annotated as an overlay to the Penn
Treebank, Propbank annotation now exists for other corpora. Propbank annotation aims
to answer the question Who did what to whom? for individual predicates. It is tightly
coupled with the behavior of individual verbs. FrameNet (Baker, Fillmore, and Lowe
1998), a frame-based lexical database that associates each word in the database with
a semantic frame and semantic roles, is also associated with annotations at the lexical
level.WordNet (Fellbaum 1998) is a verywidely used online lexical taxonomywhich has
been developed in numerous languages.WordNet nouns, verbs, adjectives, and adverbs
are organized into synonym sets. PropBank, FrameNet, and WordNet cover the word
senses and argument-taking properties of many modal predicates.
The Prague Dependency Treebank (Hajic? et al 2001; Bo?hmova?, Cinkova?, and
Hajic?ova? 2005) (PDT) is a multi-level system of annotation for texts in Czech and other
languages, with its roots in the Prague school of linguistics. Besides a morphological
layer and an analytical layer, there is a Tectogrammatical layer. The Tectogrammatical
layer includes functional relationships, dependency relations, and co-reference. The
PDT also integrates propositional and extra-propositional meanings in a single anno-
tation framework.
The Penn Discourse Treebank (PDTB) (Webber et al 2003; Prasad et al 2008)
annotates discourse connectives and their arguments over a portion of the Penn
Treebank. Within this framework, senses are annotated for the discourse connectives
in a hierarchical scheme. Relevant to the current work, one type of tag in the scheme is
the Conditional tag, which includes hypothetical, general, unreal present, unreal past,
factual present, and factual past arguments.
The PDTB work is related to that of Wiebe, Wilson, and Cardie (2005) for estab-
lishing the importance of attributing a belief or assertion expressed in text to its agent
(equivalent to the notion of holder in our scheme). The annotation scheme is designed to
capture the expression of opinions and emotions. In the PDTB, each discourse relation
414
Baker et al Modality and Negation in SIMT
and its two arguments are annotated for attribution. The attribute features are the
Source or agent, the Type (assertion propositions, belief propositions, facts, and eventu-
alities), scopal polarity, and determinacy. Scopal polarity is annotated on relations and
their arguments to identify cases when verbs of attribution are negated on the surface
but the negation takes scope over the embedded clause. An example is the sentence
?Having the dividend increases is a supportive element in the market outlook but I don?t
think it?s a main consideration.? Here, the second argument (the clause following but) is
annotated with a ?Neg? marker, meaning ?I think it?s not a main consideration.?
Wilson, Wiebe, and Hoffman (2009) describe the importance of correctly inter-
preting polarity in the context of sentiment analysis, which is the task of identifying
positive and negative opinions, emotions, and evaluations. The authors have estab-
lished a set of features to distinguish between positive and negative polarity and discuss
the importance of correctly analyzing the scope of the negation and the modality (e.g.,
whether the proposition is asserted to be real or not real).
A major annotation effort for temporal and event expressions is the TimeML spec-
ification language, which has been developed in the context of reasoning for question
answering (Saur??, Verhagen, and Pustejovsky 2006). TimeML, which includes modality
annotation on events, is the basis for creating the TimeBank and FactBank corpora
(Pustejovsky et al 2006; Saur?? and Pustejovsky 2009). In FactBank, event mentions are
marked with their degree of factuality.
Recent work incorporating modality annotation includes work on detecting cer-
tainty and uncertainty. Rubin (2007) describes a scheme for five levels of certainty,
referred to as Epistemic modality, in news texts. Annotators identify explicit certainty
markers and also take into account Perspective, Focus, and Time. Focus separates
certainty into facts and opinions, to include attitudes. In our scheme, Focus would be
covered by want and belief modality. Also, separating focus and uncertainty can allow
the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010)
describe a scheme for automatic committed belief tagging. Committed belief indicates
the writer believes the proposition. The authors use a previously annotated corpus of
committed belief, non-committed belief, and not applicable (Diab et al 2009), and derive
features for machine learning from parse trees. The authors desire to combine their
work with FactBank annotation.
The CoNLL-2010 shared task (Farkas et al 2010) was about the detection of cues
for uncertainty and their scope. The task was described as ?hedge detection,? that is,
finding statements which do not or cannot be backed up with facts. Auxiliary verbs
such as may, might, can, and so forth, are one type of hedge cue. The training data for
the shared task included the BioScope corpus (Szarvas et al 2008), which is manually
annotated with negation and speculation cues and their scope, and paragraphs from
Wikipedia possibly containing hedge information. Our scheme also identifies cues in
the form of triggers, but our desired outcome is to cover the full range of modalities
and not just certainty and uncertainty. To identify scope, we use syntactic parse trees,
as was allowed in the CoNLL task.
The textual entailment literature includes modality annotation schemes. Identifying
modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al
(2007) include polarity based rules and negation and modality annotation rules. The
polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and
Karttunen 2006). The annotation rules for negation andmodality of predicates are based
on identifying modal verbs, as well as conditional sentences and modal adverbials.
The authors read the modality off parse trees directly using simple structural rules for
modifiers.
415
Computational Linguistics Volume 38, Number 2
Earlier work describing the difficulty of correctly translating modality using ma-
chine translation includes Sigurd and Gawro?nska (1994) andMurata et al (2005). Sigurd
and Gawro?nska (1994) write about rule based frameworks and how using alternate
grammatical constructions such as the passive can improve the rendering of the modal
in the target language. Murata et al (2005) analyze the translation of Japanese into
English by several systems, showing they often render the present incorrectly as the
progressive. The authors trained a support vector machine to specifically handle modal
constructions, whereas our modal annotation approach is a part of a full translation
system.
We now consider other literature, relating to tree-grafting and machine translation.
Our tree-grafting approach builds on a technique used for tree augmentation in Miller
et al (2000), where parse-tree nodes are augmented with semantic categories. In that
earlier work, tree nodes were augmented with relations, whereas we augmented tree
nodes with modality and negation. The parser is subsequently retrained for both
semantic and syntactic processing. The semantic annotations were done manually by
students who were provided a set of guidelines and then merged with the syntactic
trees automatically. In our work we tagged our corpus with entities, modality, and
negation automatically and then grafted them onto the syntactic trees automatically,
for the purpose of training a statistical machine translation system. An added benefit of
the extracted translation rules is that they are capable of producing semantically tagged
Urdu parses, despite the fact that the training data were processed by only an English
parser and tagger.
Related work in syntax-based MT includes that of Huang and Knight (2006), where
a series of syntax rules are applied to a source language string to produce a target
language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz,
and Santorini 1993) is used as the source for the syntactic labels and syntax trees are
relabeled to improve translation quality. In this work, node-internal and node-external
information is used to relabel nodes, similar to earlier work where structural context
was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein
and Manning?s methods include lexicalizing determiners and percent markers, making
more fine-grained verb phrase (VP) categories, and marking the properties of sister
nodes on nodes. All of these labels are derivable from the trees themselves and not
from an auxiliary source. Wang et al (2010) use this type of node splitting in machine
translation and report a small increase in BLEU score.
We use the methods described in Zollmann and Venugopal (2006) and Venugopal,
Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which
requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel
(2007) use generic non-terminal category symbols, as in Chiang (2005), as well as gram-
matical categories from the Stanford parser (Klein and Manning 2003). Their method
for rule induction generalizes to any set of non-terminals. We further refine this process
by adding semantic notations onto the syntactic non-terminals produced by a Penn
Treebank trained parser, thus making the categories more informative.
In the parsing domain, the work of Petrov and Klein (2007) is related to the current
work. In their work, rule splitting and rule merging are applied to refine parse trees
during machine learning. Hierarchical splitting leads to the creation of learned cate-
gories that have linguistic relevance, such as a breakdown of a determiner category into
two subcategories of determiners by number, that is, this and that group together as do
some and these. We augment parse trees by category insertion in cases where a semantic
category is inserted as a node in a parse tree, after the English side of the corpus has
been parsed by a statistical parser.
416
Baker et al Modality and Negation in SIMT
3. SIMTMotivation
As in many of the frameworks described herein, the aim of the SIMT effort was to
provide a generalized framework for representing structured semantic information,
such as modality and negation. Unlike many of the previous semantic annotation efforts
(where the emphasis tends to be on English), however, our approach is designed to
be directly integrated into a translation engine, with the goal of translating highly
divergent language pairs, such as Urdu and English. As such, our choice of annotation
scheme?illustrated in the trigger-target example shown in Figure 1?was based on a
simplified structural representation that is general enough to accommodate divergent
modality/negation phenomena, easy for language experts to follow, and straightfor-
ward to integrate into a tree-grafting mechanism for MT. Our objective is to investigate
whether incorporating this sort of information into machine translation systems could
produce better translations, particularly in settings where only small parallel corpora
are available.
It is informative to look at an example translation to understand the challenges of
translating important semantic elements when working with a low-resource language
pair. Figure 2 shows an example taken from the 2008 NIST Urdu?English translation
task, and illustrates the translation quality of a state-of-the-art Urdu?English system
(prior to the SIMT effort). The small amount of training data for this language pair (see
Figure 2
An example of Urdu?English translation. Shown are an Urdu source document, a reference
translation produced by a professional human translator, and MT output from a phrase-based
model (Moses) without linguistic information, which is representative of state-of-the-art MT
quality before the SIMT effort.
417
Computational Linguistics Volume 38, Number 2
Table 1
The size of the various data sets used for the experiments in this article including the training,
development (dev), incremental test set (devtest), and blind test set (test). The dev/devtest was a
split of the NIST08 Urdu?English test set, and the blind test set was NIST09.
Urdu English
set lines tokens types tokens types
training 202k 1.7M 56k 1.7M 51k
dev 981 21k 4k 19k 4k
devtest 883 22k 4k 19?20k 4k
test 1,792 42k 6k 38?41k 5k
Table 1) results in significantly degraded translation quality compared, for example, to
an Arabic?English system that has more than 100 times the amount of training data.
The output in Figure 2 was produced using Moses (Koehn et al 2007), a state-of-
the-art phrase-based MT system that by default does not incorporate any linguistic
information (e.g., syntax or morphology or transliteration knowledge). As a result,
words that were not directly observed in the bilingual training data were untranslatable.
Names, in particular, are problematic. For example, the lack of translation for Nagaland
and Nagas induces multiple omissions throughout the translated text, thus producing
several instances where the holder of a claim (or belief ) is missing. This is because out-of-
vocabulary words are deleted from the Moses output.
We use syntactic and semantic tags as higher-order symbols inside the translation
rules used by the translation models. Generic symbols in translation rules (i.e., the
non-terminal symbol ?X?) were replaced with structured information at multiple levels
of abstraction, using a tree-grafting approach that we describe subsequently. Figure 3
Figure 3
The evolution of a semantically informed approach to our synchronous context-free grammars.
At the start of the 8 weeks the decoder used translation rules with a single generic non-terminal
symbol. Later syntactic categories were used, and by the end of the workshop the translation
rules included semantic elements such as modalities and negation, as well as named entities.
418
Baker et al Modality and Negation in SIMT
illustrates the evolution of the translation rules that we used, first replacing ?X? with
grammatical categories and then with categories corresponding to semantic units.
The semantic units that we examined in this effort weremodalities and negation (in-
dications that a statement represents something that has/hasn?t taken place or is/isn?t
a belief or an intention) and named entities (such as people or organizations). Other
semantic units, such as relations between entities and events, were not part of this effort
but we believe they could be similarly incorporated into the framework. We chose to
examine semantic units that canonically exhibit two different syntactic types: verbal, in
the case of modality and negation, and nominal, in the case of named entities.
Although used in this effort, named entities were not the focus of our research
efforts in SIMT. Rather, we focused on the development of an annotation scheme
for modality and negation and its use in MT, while relying on a pre-existing hidden
Markov model (HMM)-based tagger derived from Identifinder (Bikel, Schwartz, and
Weischedel 1999) to produce entity tags. Thus, the remainder of this article will focus
on our MN annotation scheme, two MN taggers produced by the effort, and on the
integration of semantics in the SIMT paradigm.
4. Modality and Negation
Modality is an extra-propositional component of meaning. In John may go to NY, the
basic proposition is John go to NY and the word may indicates modality and is called the
trigger in our work. van der Auwera and Amman (2005) define core cases of modality:
John must go to NY (epistemic necessity), John might go to NY (epistemic possibility),
John has to leave NY now (deontic necessity), and John may leave NY now (deontic pos-
sibility). Larreya (2009) defines the core cases slightly differently as root and epistemic.
Root modality in Larreya?s taxonomy includes physical modality (He had to stop. The
road was blocked) and deontic modality (You have to stop). Epistemic modality includes
problematic modality (You must be tired) and implicative modality (You have to be mad to
do that). Many semanticists (Kratzer 1991, von Fintel and Iatridou 2006) define modality
as quantification over possible worlds. John might leave NY means that there exist some
possible worlds in which John leaves NY. Another view of modality relates more to a
speaker?s attitude toward a proposition (McShane, Nirenburg, and Zacharski).
We incorporate negation as an inextricably intertwined component of modality,
using the term ?modality/negation (MN)? to refer to our resources (lexicons) and
processes (taggers). We adopt the view that modality includes several types of attitudes
that a speaker might have (or not have) toward an event or state. From the point of
view of the reader or listener, modality might indicate factivity, evidentiality, or senti-
ment. Factivity is related to whether an event, state, or proposition happened or didn?t
happen. It distinguishes things that happened from things that are desired, planned,
or probable. Evidentiality deals with the source of information and may provide clues
to the reliability of the information. Did the speaker have first-hand knowledge of
what he or she is reporting, or was it hearsay or inferred from indirect evidence?
Sentiment deals with a speaker?s positive or negative feelings toward an event, state,
or proposition.
Our project was limited to modal words and phrases?and their negations?that
are related to factivity. Beyond the core cases of modality, however, we include some
aspects of speaker attitude such as intent and desire. We included these because they
are often not separable from the core cases of modality. For example, He had to go may
include the ideas that someone wanted him to go, that he might not have wanted to go,
419
Computational Linguistics Volume 38, Number 2
that at some point after coercion he intended to go, and that at some point he was able
to go (Larreya 2009).
Our focus was on the eight modalities in Figure 4, where P is a proposition (the
target of the triggering modality) and H is the holder (experiencer or cognizer of the
modality). Some of the eight factivity-related modalities may overlap with sentiment
or evidentiality. For example, want indicates that the proposition it scopes over may
not be a fact (it may just be desired), but it also expresses positive sentiment toward
the proposition it scopes over. We assume that sentiment and evidentiality are covered
under separate coding schemes, and that words like want would have two tags, one for
sentiment and one for factivity.
5. The Modality/Negation Annotation Scheme
The challenge of creating an MN annotation scheme was to deal with the complex
scoping ofmodalities with each other andwith negation, while at the same time creating
a simplified operational procedure that could be followed by language experts without
special training. Here we describe our MN annotation framework, including a set
of linguistic simplifications, and then we present our methodology for creation of a
publicly available MN lexicon. The modality annotation scheme is fully documented in
a set of guidelines that were written with English example sentences (Baker et al 2010c).
The guidelines can be used to derive hand-tagged evaluation data for English and they
also include a section that contains a set of Urdu trigger-word examples.
During the SCALE workshop, some Urdu speakers used the guidelines to annotate
a small corpus of Urdu by hand, which we reserved for future work. The Urdu corpus
could be useful as an evaluation corpus for automatically tagged Urdu, such as one
derived from rule projection in the Urdu?English MT system, a method we describe
further in Section 7. Also, although we did not annotate a very large Urdu corpus, more
data could be manually annotated to train an automatic Urdu tagger in the future.
5.1 Anatomy of Modality/Negation in Sentences
In sentences that express modality, we identify three components: a trigger, a target, and
a holder. The trigger is the word or string of words that expresses modality or negation.
The target is the event, state, or relation over which the modality scopes. The holder is
Figure 4
Eight modalities used for tagging. H = the holder of the modality; P = the proposition over
which the modality has scope.
420
Baker et al Modality and Negation in SIMT
the experiencer or cognizer of themodality. The trigger can be a word such as should, try,
able, likely, or want. It can also be a negative element such as not or n?t. Often, modality
or negation is expressed without a lexical trigger. For a typical declarative sentence
(e.g., John went to NY), the default modality is strong belief when no lexical trigger is
present. Modality can also be expressed constructionally. For example, Requirement can
be expressed in Urdu with a dative subject and infinitive verb followed by a verb that
means to happen or befall.
5.2 Linguistic Simplifications for Efficient Operationalization
Six linguistic simplifications were made for the sake of efficient operationalization of
the annotation task. The first linguistic simplification deals with the scope of modality
and negation. The first given sentence indicates scope of modality over negation. The
second sentence indicates scope of negation over modality:
 He tried not to criticize the president.
 He didn?t try to criticize the president.
The interaction of modality with negation is complex, but was operationalized eas-
ily in the menu of 13 choices shown in Figure 5. First consider the case where negation
scopes over modality. Four of the 13 choices are composites of negation scoping over
modality. For example, the annotators can choose try or not try as two separate modali-
ties. Five modalities (Require, Permit, Want, Firmly Believe, and Believe) do not have a
negated form. For three of these modalities (Want, Firmly Believe, and Believe), this is
because they are often transparent to negation. For example, I do not believe that he left NY
sometimes means the same as I believe he didn?t leave NY. Merging the two is obviously
a simplification, but it saves the annotators from having to make a difficult decision.
Figure 5
Thirteen menu choices for Modality/Negation annotation. H = the holder of the modality;
P = the proposition over which the modality has scope.
421
Computational Linguistics Volume 38, Number 2
The second linguistic simplification is related to a duality in meaning between
require and permit. Not requiring P to be true is similar in meaning to permitting P to
be false. Thus, annotators were instructed to label not require P to be true as Permit P to be
false. Conversely, not Permit P to be truewas labeled as Require P to be false.
After the annotator chooses the modality, the scoping of modality over negation
takes place as a second decision. For example, for the sentence John tried not to go to NY,
the annotator first identifies go as the target of a modality and then chooses try as the
modality. Finally, the annotator chooses false as the polarity of the target.
The third simplification relates to entailments between modalities. Many words
have complex meanings that include components of more than one modality. For ex-
ample, if one managed to do something, one tried to do it and one probably wanted to
do it. Thus, annotators were provided a specificity-ordered modality list as in Figure 5,
andwere asked to choose the first applicable modality. We note that this list corresponds
to two independent ?entailment groupings,? ordered by specificity:
 {requires ? permits}
 {succeeds ? tries ? intends ? is able ? wants}
Inside the entailment groupings, the ordering corresponds to an entailment relation:
For example, succeeds can only occur if tries has occurred. Also, the {requires ? . . . }
entailment grouping is taken to be more specific than (ordered before) the {succeeds ?
. . . } entailment grouping. Moreover, both entailment groupings are taken to be more
specific than believes, which is not in an entailment relation with any of the other
modalities.
The fourth simplification, already mentioned, is that sentences without an overt
trigger word are tagged as firmly believes. This heuristic works reasonably well for the
types of documents we were working with, although one could imagine genres such
as fiction in which many sentences take place in an alternate possible world (imagined,
conditional, or counterfactual) without explicit marking.
The fifth linguistic simplification is that we did not require annotators to mark
nested modalities. For a sentence like He might be able to go to NY the target word go
is marked as ability, but might is not annotated for Belief modality. This decision was
based on time limits on the annotation task; there was not enough time for annotators
to deal with syntactic scoping of modalities over other modalities.
Finally, we did not mark the holder H because of the short time frame for workshop
preparation. We felt that identifying the triggers and targets would be most beneficial
in the context of machine translation.
5.3 The English Modality/Negation Lexicon
Using the given framework, we created an MN lexicon that was incorporated into an
MN tagging scheme to be described in Section 6. Entries in the MN lexicon consist of:
(1) A string of one or more words: for example, should or have need of . (2) A part of
speech for each word: The part of speech helps us avoid irrelevant homophones such as
the noun can. (3) An MN designator: one of the 13 modality/negation cases described
previously. (4) A head word (or trigger): the primary phrasal constituent to cover
cases where an entry is a multi-word unit (e.g., the word hope in hope for). (5) One or
more subcategorization codes derived from the Longman Dictionary of Contemporary
English (LDOCE).
422
Baker et al Modality and Negation in SIMT
We produced the full English MN lexicon semi-automatically. First, we gathered a
small seed list of MN trigger words and phrases from our modality annotation manual
(Baker et al 2010c). Then, we expanded this small list of MN trigger words by running
an on-line search for each of the words, specifically targeting free on-line thesauri (e.g.,
thesaurus.com), to find both synonymous and antonymous words. From these we
manually selected the words we thought triggered modality (or their corresponding
negative variants) and filtered out words that we thought didn?t trigger modality. The
resulting list of MN trigger words and phrases contained about 150 lemmas.
We note that most intransitive (LDOCE) codes were not applicable to modality/
negation constructions. For example, hunger (in the Want modality class) has a modal
reading of ?desire? when combined with the preposition for (as in she hungered for a
promotion), but we do not consider it to be modal when it is used in the somewhat
archaic sentence He hungered, meaning that he did not have enough to eat. Thus the
LDOCE code I associated with the verb hungerwas hand-changed to I-FOR. There were
43 such cases. Once the LDOCE codes were hand-verified (and modified accordingly),
the mapping to subcategorization codes was applied.
The MN lexicon is publicly available at http://www.umiacs.umd.edu/?bonnie/
ModalityLexicon.txt. An example of an entry is given in Figure 6, for the verb need.
6. Automatic Modality/Negation Annotation
An MN tagger produces text or structured text in which modality or negation triggers
and/or targets are identified. Automatic identification of the holders of modalities was
beyond the scope of our project because the holder is often not explicitly stated in the
sentence in which the trigger and target occur. This section describes two types of MN
taggers?one that is string-based and one that is structure-based.
6.1 The String-Based English Modality/Negation Tagger
The string-based tagger operates on text that has been tagged with parts of speech
by a Collins-style statistical parser (Miller et al 1998). The tagger marks spans of
words/phrases that exactly match MN trigger words in the MN lexicon described
previously, and that exactly match the same parts of speech. This tagger identifies the
target of each modality/negation using the heuristic of tagging the next non-auxiliary
verb to the right of the trigger. Spans of words can be tagged multiple times with
different types of triggers and targets.
Figure 6
Modality lexicon entry for need.
423
Computational Linguistics Volume 38, Number 2
We found the string-based MN tagger to produce output that matched about 80%
of the sentence-level tags produced by our structure-based tagger, the results of which
are described next. Although string-based tagging is fast and reasonably accurate in
practice, we opted to focus on the indepth analysis of modality/negation of our SIMT
results using the more accurate structure-based tagger.
6.2 The Structure-Based English Modality/Negation Tagger
The structure-based MN tagger operates on text that has been parsed (Miller et al
1998). We used a version of the parser that produces flattened trees. In particular, the
flattener deletes VP nodes that are immediately dominated by VP or S and noun phrase
(NP) nodes that are immediately dominated by PP or NP. The parsed sentences are
processed by TSurgeon rules. Each TSurgeon rule consists of a pattern and an action.
The pattern matches part of a parse tree and the action alters the parse tree. More
specifically, the pattern finds an MN trigger word and its target and the action inserts
tags such as TrigRequire and TargRequire for triggers and targets for the modality
Require. Figure 7 shows output from the structure-based MN tagger. (Note that the
sentence is disfluent: Pakistan which could not reach semi-final, in a match against South
African team for the fifth position Pakistan defeated South Africa by 41 runs.) The example
shows that could is a trigger for the Ability modality and not is a trigger for negation.
Reach is a target for both Ability and Negation, which means that it is in the category of
?H is not able [to make P true/false]? in our coding scheme. Reach is also a trigger for
the Succeed modality and semi-final is its target.
The TSurgeon patterns are automatically generated from the verb class codes in
the MN lexicon along with a set of 15 templates. Each template covers one situation
such as the following: the target is the subject of the trigger; the target is the direct
object of the trigger; the target heads an infinitival complement of the trigger; the target
is a noun modified by an adjectival trigger, and so on. The verb class codes indicate
Figure 7
Sample output from the structure-based MN tagger.
424
Baker et al Modality and Negation in SIMT
which templates are applicable for each trigger word. For example, a trigger verb in the
transitive class may use two target templates, one in which the trigger is in active voice
and the target is a direct object (need tents) and one in which the trigger is in passive
voice and the target is a subject (tents are needed).
In developing the TSurgeon rules, we first conducted a corpus analysis for 40 of the
most common trigger words in order to identify and debug the most broadly applicable
templates. We then used LDOCE to assign verb classes to the remaining verbal triggers
in the MN lexicon, and we associated one or more debugged templates with each verb
class. In this way, the initial corpus work on a limited number of trigger words was
generalized to a longer list of trigger words. Because the TSurgeon patterns are tailored
to the flattened structures produced by our parser, it is not easily ported to new parser
outputs. The MN lexicon itself is portable, however. Switching parsers would entail
writing new TSurgeon templates, but the trigger words in the MN lexicon would still
be automatically assigned to templates based on their verb classes.
The following example shows an example of a TSurgeon pattern?action pair for a
sentence like They were required to provide tents. The pattern?action pair is intended to
be used after a pre-processing stage in which labels such as ?VoicePassive? and ?AUX?
have been assigned. ?VoicePassive? is inserted by a pre-processing TSurgeon pattern
because, in some cases, the target of a passive modality trigger word is in a different
location from the target of the corresponding active modality trigger word. ?AUX? is
inserted during pre-processing to distinguish auxiliary uses of have and be from their
uses as main verbs. The pattern portion of the pattern?action pair matches a node with
label VB that is not already tagged as a trigger and that is passive and dominates the
string ?required?. The VB node is also a sister to an S node, and the S node dominates a
VB that is not an auxiliary (provide in this case). The action portion of the pattern?action
pair inserts the string ?TargReq? as the second daughter of the second VB and inserts
the string ?TrigReq? as the second daughter of the first VB.
VB=trigger !< /^Trig/ < VoicePassive < required $..
(S < (VB=target !< AUX))
insert (TargReq) >2 target
insert (TrigReq) >2 trigger
Verb-specific patterns such as this one were generalized in order to gain coverage of
the whole modality lexicon. The specific lexical item, required, was replaced with a vari-
able, as were the labels ?TrigReq? and ?TargReq.? The pattern was then given a name,
V3-passive-basic, where V3 is a verb class tag from LDOCE (described in Section 5.3)
for verbs that take infinitive complements. We then looked up the LDOCE verb class
labels for all of the verbs in the modality lexicon. Using this information, we could then
generate a set of new, verb-specific patterns for each V3 verb in the modality lexicon.
6.3 Evaluating the Effectiveness of Structure-Based MN Tagging
We performed amanual inspection of the structure-based tagging output. We calculated
precision by examining 229 instances of modality triggers that were tagged by our
tagger from the English side of the NIST 09 MTEval training sentences. We analyzed
precision in two steps, first checking for the correct syntactic position of the target and
then checking the semantic correctness of the trigger and target. For 192 of the 229
triggers (around 84%), the targets were tagged in the correct syntactic location.
For example, for the sentence A solution must be found to this problem shown in
Figure 8, the word must is a modality trigger word, and the correct target is the first
425
Computational Linguistics Volume 38, Number 2
Figure 8
Example of embedded target head found inside VP must be found.
non-auxiliary verb heading a verb phrase that is contained in the syntactic complement
of must. The syntactic complement of must is the verb phrase be found to this problem.
The syntactic head of that verb phrase, be, is skipped because it is an auxiliary verb. The
correct (embedded) target found is the head of the syntactic complement of be.
The 192 modality instances with structurally correct targets do not all have seman-
tically correct tags. In this example, must is tagged as TrigBelief, where the correct tag
would be TrigRequire. Also, because theMN lexiconwas usedwithout respect to word
sense, words were sometimes erroneously identified as triggers. This includes non-
modal uses of work (work with refugees), reach (reach a destination), and attack (attack
a physical object), in constrast to modal uses of these words: work for peace (effort), reach
a goal (succeed), and attack a problem (effort). Fully correct tagging of modality would
need to include word sense disambiguation.
For 37 of the 229 triggers we examined, a target was not tagged in the correct syn-
tactic position. In 12 of 37 incorrectly tagged instances the targets are inside compound
nouns or coordinate structures (NP or VP), which are not yet handled by the modality
tagger. The remaining 25 of the 37 incorrectly tagged instances had targets that were lost
because the tagger does not yet handle all cases of nested modalities. Nested modalities
occur in sentences like They did not want to succeed in winning where the target words
want and succeed are also modality trigger words. Proper treatment of nested modalities
requires consideration of scope and compositional semantics.
Nesting was treated in two steps. First, the modality tagger marked each word as a
trigger and/or target. In They did not want to succeed in winning, not is marked as a trigger
for negation, want is marked as a target of negation and a trigger of wanting, succeed is
marked as a trigger of succeeding and a target of wanting, and win is marked as a target
of succeeding. The second step in the treatment of nested modalities occurs during tree
grafting, where the meanings of the nested modalities are composed. The tree grafting
program correctly composes some cases of nested modalities. For example, the tag
TrigAble composed with TrigNegation results in the target tag TargNOTAble, as shown
in Figure 9. In other cases, where compositional semantics are not yet accommodated,
the tree grafting program removed target labels from the trees, and those cases were
counted as incorrect for the purpose of this evaluation.
Figure 9
Example of modality composed with negation: TrigAble and TrigNegation combine to form
NOTAble.
426
Baker et al Modality and Negation in SIMT
In the 229 instances that we examined, there were 14 in which a light verb or noun
was the correct syntactic target, but not the correct semantic target. Decision would be
a better target than taken in The decision should be taken on delayed cases on the basis of
merit.We counted sentences with semantically light targets as correct in our evaluation
because our goal was to identify the syntactic head of the target. The semantics of the
target is a general issue, and we often find lexico-syntactic fluff between the trigger and
the most semantically salient target in sentences likeWe succeeded in our goal of winning
the war where ?success in war? is the salient meaning.
With respect to recall, the tagger primarily missed special forms of negation in
noun phrases and prepositional phrases: There was no place to seek shelter; The buildings
should be reconstructed, not with RCC, but with the wood and steel sheets. More complex
constructional and phrasal triggers were also missed: President Pervaiz Musharraf has said
that he will not rest unless the process of rehabilitation is completed. Finally, we discovered
some omissions from our MN lexicon: It is not possible in the middle of winter to re-open
the roads. Further annotation experiments are planned, which will be analyzed to close
such gaps and update the lexicon as appropriate.
Providing a quantitative measure of recall was beyond the scope of this project.
At best we could count instances of sentences containing trigger words that were not
tagged. We are also aware of many cases of modality that were not covered such as
the modal uses of the future tense auxiliary will as in That?ll be John (conjecture), I?ll do
the dishes (volition), He won?t do it (non-volition), and It will accommodate five (ability)
(Larreya 2009). Because of the complexity and subtlety of modality and negation, how-
ever, it would be impractical to count every clause (such as the not rest unless clause
above) that had a nuance of non-factivity.
7. Semantically Informed Syntactic MT
This section describes the incorporation of our structured-based MN tagging into an
Urdu?English machine-translation system using tree grafting for combining syntactic
symbols with semantic categories (e.g., modality/negation). We note that a de facto
Urdu MN tagger resulted from identifying the English MN trigger and target words in
a parallel English?Urdu corpus, and then projecting the trigger and target labels to the
corresponding words in Urdu syntax trees.
7.1 Refinement of Translation Grammars with Semantic Categories
We used synchronous context-free grammars (SCFGs) as the underlying formalism
for our statistical models of translation. SCFGs provide a convenient and theoretically
grounded way of incorporating linguistic information into statistical models of transla-
tion, by specifying grammar rules with syntactic non-terminals in the source and target
languages. We refine the set of non-terminal symbols so that they not only include
syntactic categories, but also semantic categories.
Chiang (2005) re-popularized the use of SCFGs for machine translation, with the
introduction of his hierarchical phrase-based machine translation system, Hiero. Hiero
uses grammars with a single non-terminal symbol ?X? rather than using linguistically
informed non-terminal symbols. When moving to linguistic grammars, we use Syntax
Augmented Machine Translation (SAMT) developed by Venugopal, Zollmann, and
Vogel (2007). In SAMT the ?X? symbols in translation grammars are replaced with
nonterminal categories derived from parse trees that label the English side of the
427
Computational Linguistics Volume 38, Number 2
Figure 10
A sentence on the English side of the bilingual parallel training corpus is parsed with a
syntactic parser, and also tagged with our modality tagger. The tags are then grafted onto
the syntactic parse tree to form new categories like VP-TargNOTAble and VP-TargRequire.
Grafting happens prior to extracting translation rules, which happens normally except for
the use of the augmented trees.
Urdu?English parallel corpus.1 We refine the syntactic categories by combining them
with semantic categories. Recall that this progression was illustrated in Figure 3.
We extracted SCFG grammar rules containing modality, negation, and named enti-
ties using an extraction procedure that requires parse trees for one side of the parallel
corpus. Although it is assumed that these trees are labeled and bracketed in a syntac-
tically motivated fashion, the framework places no specific requirement on the label
inventory. We take advantage of this characteristic by providing the rule extraction
algorithm with augmented parse trees containing syntactic labels that have semantic
annotations grafted onto them so that they additionally express semantic information.
Our strategy for producing semantically grafted parse trees involves three steps:
1. The English sentences in the parallel training data are parsed with a
syntactic parser. In our work, we used the lexicalized probabilistic context
free grammar parser provided by Basis Technology Corporation.
2. The English sentences are MN-tagged by the system described herein and
named-entity-tagged by the Phoenix tagger (Richman and Schone 2008).
3. The modality/negation and entity markers are grafted onto the syntactic
parse trees using a tree-grafting procedure. The grafting procedure was
implemented as part of the SIMT effort. Details are further spelled out in
Section 7.2.
Figure 10 illustrates how modality tags are grafted onto a parse tree. Note that
although we focus the discussion here on the modality and negation, our framework
is general and we were able to incorporate other semantic elements (specifically, named
entities) into the SIMT effort.
Once the semantically grafted trees have been produced for the parallel corpus, the
trees are presented, along with word alignments (produced by the Berkeley aligner),
to the rule extraction software to extract synchronous grammar rules that are both
1 For non-constituent phrases, composite CCG-style categories are used (Steedman 1999).
428
Baker et al Modality and Negation in SIMT
syntactically and semantically informed. These grammar rules are used by the decoder
to produce translations. In our experiments, we used the Joshua decoder (Li et al 2009),
the SAMT grammar extraction software (Venugopal and Zollmann 2009), and special
purpose-built tree-grafting software.
Figure 11 shows example semantic rules that are used by the decoder. The verb
phrase rules are augmented with modality and negation, taken from the semantic
categories listed in Table 2. Because these get marked on the Urdu source as well as
the English translation, semantically enriched grammars also act as very simple named
entity or MN taggers for Urdu. Only entities, modality, and negation that occurred in
the parallel training corpus are marked in the output, however.
7.2 Tree-Grafting Algorithm
The overall scheme of our tree-grafting algorithm is to match semantic tags to syntactic
categories. There are two inputs to the process. Each is derived from a common text
file of sentences. The first input is a list of standoff annotations for the semantically
tagged word sequences in the input sentences, indexed by sentence number. The second
is a list of parse trees for the sentences in Penn Treebank format, indexed by sentence
number.
Table 2 lists the modality/negation types that were produced by the MN tagger. For
example, the sentence The students are able to swim is tagged as The students are ?TrigAble?
to ?TargAble swim?. The distinction between ?Negation? and ?NOT? corresponds to the
difference between negation that is inherently expressed in the triggering lexical item
and negation that is expressed explicitly as a separate lexical item. Thus, I achieved
my goal is tagged ?Succeed? and I did not achieve my goal is tagged as ?NOTSucceed,?
Figure 11
Example translation rules with tags for modality, negation, and entities combined with
syntactic categories.
429
Computational Linguistics Volume 38, Number 2
Table 2
Modality tags with their negated versions. Note that Require and Permit are in a dual relation,
and thus RequireNegation is represented as NOTPermit and PermitNegation is represented
as NOTRequire.
Require NOTRequire
Permit NOTPermit
Succeed NOTSucceed
SucceedNegation NOTSucceedNegation
Effort NOTEffort
EffortNegation NOTEffortNegation
Intend NOTIntend
IntendNegation NOTIntendNegation
Able NOTAble
AbleNegation NOTAbleNegation
Want NOTWant
WantNegation NOTWantNegation
Belief NOTBelief
BeliefNegation NOTBeliefNegation
Firm Belief NOTFirm Belief
Firm BeliefNegation NOTFirm BeliefNegation
Negation
but I failed to win is tagged as ?SucceedNegation,? and I did not fail to win is tagged as
?NOTSucceedNegation.?
The tree-grafting algorithm proceeds as follows. For each tagged sentence, we
iterate over the list of semantic tags. For each semantic tag, there is an associated word
or sequence of words. For example, the modality tag TargAble may tag the word swim.
For each semantically tagged word, we find the parent node in the correspond-
ing syntactic parse tree that dominates that word. For a word sequence, we find and
compare the parent nodes for all of the words. Each node in the syntax tree has a
category label. The following tests are then made and tree grafts applied:
 If there is a single node in the parse tree that dominates all and only the
words with the semantic tag, graft the name of the semantic tag onto
the highest corresponding syntactic constituent in the tree. For example,
in Figure 10, which shows the grafting process for modality tagging,
the semantic tag TargNOTAble that ?hand over? receives is grafted onto
the VB node that dominates all and only the words ?hand over.? Then the
semantic tag TargNOTAble is passed up the tree to the VP node, which is
the highest corresponding syntactic constituent.
 If the semantic tag corresponds to words that are adjacent daughters in
a syntactic constituent, but less than the full constituent, insert a node
dominating those words into the parse tree, as a daughter of the original
syntactic constituent. The name of the semantic tag is grafted onto the new
node and becomes its category label. This is a case of tree augmentation by
node insertion.
 If a syntactic constituent selected for grafting has already been labeled
with a semantic tag, overlay the previous tag with the current tag. We
chose to tag in this manner simply because our system was not set up to
handle the grafting of multiple tags onto a single constituent. An example
430
Baker et al Modality and Negation in SIMT
of this occurs in the sentence ?The Muslims had obtained Pakistan.? If the
NP node dominating Pakistan is grafted with a named entity tag such as
NP-GPE, we overlay this with the NP-TargSucceed tag in a modality
tagging scheme.
 In the case of a word sequence, if the words covered by the semantic tag
fall across two different syntactic constituents, do nothing. This is a case of
crossing brackets.
Our tree-grafting procedure was simplified to accept a single semantic tag per
syntactic tree node as the final result. The algorithm keeps the last tag seen as the tag of
precedence. In practice, we established a precedence ordering for modality/negation
tags over named entity tags by grafting named entity tags first and modality/negation
second. Our intuition was that, in case of a tie, finer-grained verbal categories would be
more helpful to parsing than finer-grained nominal categories.2 In cases where a word
was tagged both as a MN target and a MN trigger, we gave precedence to the target tag.
This is because, although MN targets vary, MN triggers are generally identifiable with
lexical items. Finally, we used the simplified specificity ordering of MN tags described
in Section 5.2 to ensure precedence of more specific tags over more general ones. Table 2
lists the modality/negation types from highest (Require modality) to lowest (Negation)
precedence.3
7.3 SIMT Results
We evaluated our tree grafting approach by performing a series of translation experi-
ments. Each version of our translation systemwas trained on the same bilingual training
data. The bilingual parallel corpus that we used was distributed as part of the 2008
NIST Open Machine Translation Evaluation Workshop.4 The training set contained
88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.
For our development and test sets, we split the NIST MT-08 test set into two portions
(with each document going into either test or dev, and preserving the genre split).
Our test set contained 883 Urdu sentences, each with four translations into English,
and our dev set contained 981 Urdu sentences, each with four reference translations.
To extract a syntactically informed translation model, we parsed the English side of
the training corpus using a Penn Treebank?trained parser (Miller et al 1998). For the
experiments that involved grafting named entities onto the parse trees, we tagged
the English side of the training corpus with the Phoenix tagger (Richman and Schone
2008). We word-aligned the parallel corpus with the Berkeley aligner. All models used a
5-gram language model trained on the English Gigaword corpus (v5) using the SRILM
toolkit with modified KN smoothing. The Hiero translation grammar was extracted
using the Joshua toolkit (Li et al 2009). The other translation grammars were extracted
using the SAMT toolkit (Venugopal and Zollmann 2009).
2 In testing we found that grafting named entities first and MN last yielded a slightly higher BLEU score
than the reverse order.
3 Future work could include exploring additional methods of resolving tag conflicts or combining tag
types on single nodes, for example, by inserting multiple intermediate nodes (effectively using unary
rewrite rules) or by stringing tag names together.
4 http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/.
431
Computational Linguistics Volume 38, Number 2
Figure 12
Results for a range of experiments conducted during the SIMT effort show the score for our
top-performing baseline systems derived from a hierarchical phrase-based model (Hiero).
Substantial improvements obtained when syntax was introduced along with feature functions
(FFs) and further improvements resulted from the addition of semantic elements. The scores
are lowercased BLEU calculated on the held-out devtest set. NE = named entities.
Figure 12 gives the results for a number of experiments conducted during the SIMT
effort.5 The experiments are broken into three groups: baselines, syntax, and semantics.
To contextualize our results we experimented with a number of different baselines
that were composed from two different approaches to statistical machine translation?
phrase-based and hierarchical phrase-based SMT?along with different combinations
of language model sizes and word aligners. Our best-performing baseline was a Hiero
model. The Bleu score for this baseline on the development set was 22.9 Bleu points.
After experimenting with syntactically motivated grammar rules, we conducted
experiments on the effects of incorporating semantic elements (e.g., named entities and
modality/negation) into the translation grammars. In our devtest set our taggers tagged
on average 3.5 named entities per sentence and 0.35 MN markers per sentence. These
were included by grafting modality, negation, and named-entity markers onto the parse
trees. Individually, each of these made modest improvements over the syntactically
informed system alone. Grafting named entities onto the parse trees improved the Bleu
score by 0.2 points. Modality/negation improved it by 0.3 points. Doing both simulta-
neously had an additive effect and resulted in a 0.5 Bleu score improvement over syntax
alone. This improvement was the largest improvement that we got from anything other
than the move from linguistically naive models to syntactically informed models.
We used bootstrap resampling to test whether the differences in Bleu scores were
statistically significant (Koehn 2004). All of the results were a significant improvement
over Hiero (at p ? 0.01). The difference between the syntactic system and the syntactic
system with named entities is not significant (p = 0.38). The differences between the
5 These experiments were conducted on the devtest set, containing 883 Urdu sentences (21,623 Urdu
words) and four reference translations per sentence. The BLEU score for these experiments is measured
on uncased output.
432
Baker et al Modality and Negation in SIMT
syntactic system and the syntactic system with MN, and between the syntactic system
and the syntactic system with both MN and named entities were both significant at
(p ? 0.05).
Figure 13 shows example output from the final SIMT system in comparison to
the pre-SIMT results and the translation produced by a human (reference). An error
analysis of this example output illustrates that SIMT enhancements have resulted in the
elimination of misleading translation output in several cases:
1. pre-SIMT: China had the experience of Pakistan?s first nuclear bomb.
SIMT: China has the first nuclear bomb test.
reference: China has conducted the experiment of Pakistan?s first nuclear bomb.
2. pre-SIMT: the nuclear bomb in 1998 that Pakistan may experience
SIMT: the experience of the atom bomb Pakistan in May 1998
reference: the atom bomb, whose experiment was done in 1998
by Pakistan
3. pre-SIMT: He said that it is also present proof of that Dr. Abdul Qadeer
Khan after the Chinese design
SIMT: He said that there is evidence that Dr. Abdul Qadeer Khan has
also used the Chinese design
reference: He said that the proof to this also exists in that Dr. Abdul
Qadeer Khan used the Chinese design
The article in question pertains to claims by Thomas Reid that China allowed Pakistan
to detonate a nuclear weapon at its test site. In the first example, however, the reader is
potentially misled by the pre-SIMT output to believe that Pakistan launched a nuclear
bomb on China. The SIMT output leaves out the mention of Pakistan, but correctly con-
veys the firm belief that the bomb event is a test (closely resembling the term experiment
in the human reference), not a true bombing event. This is clearly an improvement over
the misleading pre-SIMT output.
In the second example, the pre-SIMT output misleads the reader to believe that
Pakistan is (or will be) attacked, through the use of the phrase may experience, where
may is poorly placed. (We note here that this is a date translation error, i.e., the month
of May should be next to the year 1998, further adding to the potential for confusion.)
Unfortunately, the SIMT output also uses the term experience (rather than experiment,
which is in the human reference), but in this case the month is correctly positioned in
the output, thus eliminating the potential for confusionwith respect to themodality. The
lack of a modal appropriately neutralizes the statement so that it refers to an abstract
event associated with the atom bomb, rather than an attack on the country.
In the third example, where the Chinese design used by Dr. Abdul Qandeer Khan is
argued to be proof of the nuclear testing relationship between Pakistan and China, the
first pre-SIMT output potentially leads the reader to believe that Dr. Abdul Qadeer is
after the Chinese design (not that he actually used it), whereas the SIMT output conveys
the firm belief that the Chinese design has been used by Dr. Abdul Qadeer. This output
very closely matches the human reference.
Note that even in the title of the article, the SIMT system produces much more
coherent English output than that of the linguistically naive system. The figure also
shows improvements due to transliteration, which are described in Irvine et al (2010).
The scores reported in Figure 12 do not include transliteration improvements.
433
Computational Linguistics Volume 38, Number 2
Figure 13
An example of the improvements to Urdu?English translation before and after the SIMT effort.
Output is from the baseline Hiero model, which does not use linguistic information, and from
the final model, which incorporates syntactic and semantic information.
434
Baker et al Modality and Negation in SIMT
8. Conclusions and Future Work
We developed a modality/negation lexicon and a set of automatic MN taggers, one of
which?the structure-based tagger?results in 86% precision for tagging of a standard
LDC data set. The MN tagger has been used to improve machine translation output
by imposing semantic constraints on possible translations in the face of sparse training
data. The tagger is also an important component of a language-understanding module
for a related project.
We have described a technique for translation that shows particular promise
for low-resource languages. We have integrated linguistic knowledge into statistical
machine translation in a unified and coherent framework. We demonstrated that
augmenting hierarchical phrase-based translation rules with semantic labels (through
?grafting?) resulted in a 0.5 Bleu score improvement over syntax alone.
Although our largest gains were from syntactic enrichments to the Hiero model,
demonstrating success on the integration of semantic aspects of language bodes well
for additional improvements based on the incorporation of other semantic aspects. For
example, we hypothesize that incorporating relations and temporal knowledge into
the translation rules would further improve translation quality. The syntactic grafting
framework is well-suited to support the exploration of the impact of many different
types of semantics on MT quality, though in this article we focused on exploring the
impact of modality and negation.
An important future study is one that focuses on demonstrating whether further
improvements in modality/negation identification are likely to lead to further gains in
translation performance. Such a study would benefit from the inclusion of a more de-
tailed manual evaluation to determine if modality and negation is adequately conveyed
in the downstream translations. This work would be additionally enhanced through
experimentation on other language pair(s) and larger corpora.
The work presented here represents the first small steps toward a full integration
of MT and semantics. Efforts underway in DARPA?s GALE program demonstrated the
potential for combining MT and semantics (termed distillation) to answer the informa-
tion needs of monolingual speakers using multilingual sources. Proper recognition of
modalities and negation is crucial for handling those information needs effectively.
In previous work, however, semantic processing proceeded largely independently of
the MT system, operating only on the translated output. Our approach is significantly
different in that it combines syntax, semantics, and MT into a single model, offering
the potential advantages of joint modeling and joint decision-making. It would be
interesting to explore whether the integration of MT with syntax and semantics can be
extended to provide a single-model solution for tasks such as cross-language informa-
tion extraction and question answering, and to evaluate our integrated approach (e.g.,
using GALE distillation metrics).
Acknowledgments
We thank Aaron Phillips for help with
conversion of the output of the entity tagger
for ingest by the tree-grafting program. We
thank Anni Irvine and David Zajic for their
help with experiments on an alternative
Urdu modality/negation tagger based on
projection and training an HMM-based
tagger derived from Identifinder (Bikel,
Schwartz, and Weischedel 1999). For their
helpful ideas and suggestions during the
development of the modality framework,
we are indebted to Mona Diab, Eduard
Hovy, Marge McShane, Teruko Mitamura,
Sergei Nirenburg, Boyan Onyshkevych,
and Owen Rambow. We also thank Basis
Technology Corporation for their generous
contribution of software components to this
work. This work was supported, in part,
by the Johns Hopkins Human Language
435
Computational Linguistics Volume 38, Number 2
Technology Center of Excellence (HLTCOE),
by the National Science Foundation under
grant IIS-0713448, and by BBN Technologies
under GALE DARPA/IPTO contract no.
HR0011-06-C-0022. Any opinions, findings,
and conclusions or recommendations
expressed in this material are those of the
authors and do not necessarily reflect the
views of the sponsor.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics - Volume 1,
ACL ?98, pages 86?90, Stroudsburg, PA.
Baker, Kathryn, Steven Bethard, Michael
Bloodgood, Ralf Brown, Chris Callison-
Burch, Glen Coppersmith, Bonnie J. Dorr,
Nathaniel W. Filardo, Kendall Giles, Ann
Irvine, Michael Kayser, Lori Levin, Justin
Martineau, James Mayfield, Scott Miller,
Aaron Phillips, Andrew Philpot, Christine
Piatko, Lane Schwartz, and David Zajic.
2010a. Semantically informed machine
translation. Technical Report 002,
Human Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Chris Callison-Burch, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori Levin, Scott
Miller, and Christine Piatko. 2010b.
Semantically-informed machine
translation: A tree-grafting approach.
In Proceedings of The Ninth Biennial
Conference of the Association for Machine
Translation in the Americas, Denver, CO.
Baker, Kathryn, Michael Bloodgood, Mona
Diab, Bonnie J. Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
Sergei Nirenburg, Christine Piatko, Owen
Rambow, and Gramm Richardson. 2010c.
SIMT SCALE 2009?Modality annotation
guidelines. Technical Report 004, Human
Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Bonnie J. Dorr, Nathanial W. Filardo,
Lori Levin, and Christine Piatko.
2010d. A modality lexicon and its use
in automatic tagging. In Proceedings of
the Seventh International Conference on
Language Resources and Evaluation
(LREC), pages 1402?1407, Mediterranean
Conference Center, Valletta.
Bar-Haim, Roy, Ido Dagan, Iddo Greental,
and Eyal Shnarch. 2007. Semantic
inference at the lexical-syntactic level.
In Proceedings of the 22nd National
Conference on Artificial intelligence -
Volume 1, pages 871?876, Vancouver,
British Columbia.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name.Machine
Learning, 34(1?3):211?231.
Bo?hmova?, Alena, Silvie Cinkova?, and
Eva Hajic?ova?. 2005. A manual for
tectogrammatical layer annotation of the
Prague Dependency Treebank [English
translation]. Technical Report #30, U?FAL
MFF UK, Prague, Czech Republic.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL-2005),
pages 263?270, Ann Arbor, MI.
Diab, Mona T., Lori Levin, Teruko Mitamura,
Owen Rambow, Vinodkumar
Prabhakaran, and Weiwei Guo. 2009.
Committed belief annotation and tagging.
In Proceedings of the Third Linguistic
Annotation Workshop, ACL-IJCNLP ?09,
pages 68?73, Stroudsburg, PA.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 shared task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning?Shared Task,
CoNLL ?10: Shared Task, pages 1?12,
Stroudsburg, PA.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Hajic?, Jan, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova?
Hladka?. 2001. Prague Dependency
Treebank 1.0 (Final Production Label),
UFAL MFF UK, Prague, Czech Republic.
Huang, Bryant and Kevin Knight. 2006.
Relabeling syntax trees to improve
syntax-based machine translation
quality. In HLT-NAACL, New York.
Irvine, Ann, Mike Kayser, Zhifei Li, Wren
Thornton, and Chris Callison-Burch.
2010. Integrating output from specialized
modules in machine translation:
Transliteration in Joshua. Proceedings
of the Human Language Technology
436
Baker et al Modality and Negation in SIMT
and North American Chapter of the
Association for Computational Linguistics,
pages 240?247. The Prague Bulletin of
Mathematical Linguistics, 93:107?116.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388?395,
Barcelona.
Koehn, Philipp, Hieu Hoang, Alexandra
Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007
Demo and Poster Sessions, Prague, Czech
Republic, pages 177?180.
Kratzer, Angelika. 1991. Modality. In
Arnim von Stechow and Dieter, editors,
Semantics: An International Handbook of
Contemporary Research. De Gruyter,
Berlin, pages 639?650.
Larreya, Paul. 2009. Towards a typology of
modality in language. In Raphael Salkie,
Pierre Busuttil, and Johan van der Auwera,
editors,Modality in English: Theory and
Description. Mouton de Gruyter, Paris,
pages 9?30.
Li, Zhifei, Chris Callison-Burch, Chris Dyer,
Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar
Zaidan. 2009. Joshua: An open source
toolkit for parsing-based machine
translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 135?139, Athens.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
McShane, Marjorie, Sergei Nirenburg, and
Ron Zacharski. 2004. Mood and modality:
Out of the theory and into the fray. Natural
Language Engineering, 19(1):57?89.
Miller, Scott, Heidi Fox, Lance Ramshaw,
and Ralph Weischedel. 1998. SIFT:
Statistically-derived information from
text. In Seventh Message Understanding
Conference (MUC-7), Washington, DC,
Miller, Scott, Heidi J. Fox, Lance A.
Ramshaw, and Ralph M. Weischedel. 2000.
A novel use of statistical parsing to extract
information from text. In Proceedings of
Applied Natural Language Processing
and the North American Association for
Computational Linguistics, pages 226?233,
Seattle, Washington.
Murata, Masaki, Kiyotaka Uchimoto, Qing
Ma, Toshiyuki Kanamaru, and Hitoshi
Isahara. 2005. Analysis of machine
translation systems? errors in tense,
aspect, and modality. In Proceedings of the
19th Asia-Pacific Conference on Language,
Information and Computing (PACLIC 2005),
Taipei, Taiwan.
Nairn, Rowan, Cleo Condorovdi, and
Lauri Karttunen. 2006. Computing
relative polarity for textual inference.
In Proceedings of the International Workshop
on Inference in Computational Semantics
(ICoS-5), pages 66?76, Buxton, England.
Palmer, Martha, Daniel Gildea, and
Paul Kingsbury. 2005. The Proposition
Bank: An annotated corpus of semantic
roles. Computational Linguistics,
31:71?106.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, PA.
Petrov, Slav and Dan Klein. 2007. Learning
and inference for hierarchically split
PCFGs. In Proceedings of the 22nd American
Association for Artificial Intelligence,
pages 1663?1666, Vancouver, British
Columbia, Canada.
Prabhakaran, Vinodkumar, Owen Rambow,
and Mona Diab. 2010. Automatic
committed belief tagging. In Proceedings
of the 23rd International Conference on
Computational Linguistics: Posters, COLING
?10, pages 1014?1022, Beijing, China.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings of
the Sixth International Language Resources
and Evaluation (LREC?08), pages 28?30,
Marrakech.
Pustejovsky, James, Marc Verhagen, Roser
Saur??, Jessica Littman, Robert Gaizauskas,
Graham Katz, Inderjeet Mani, Robert
Knippen, and Andrea Setzer. 2006.
TimeBank 1.2. Linguistic Data Consortium,
Philadelphia, PA.
Richman, Alexander and Patrick Schone.
2008. Mining wiki resources for
multilingual named entity recognition.
437
Computational Linguistics Volume 38, Number 2
In Proceedings of ACL-08: HLT, pages 1?9,
Columbus, OH.
Rubin, Victoria L. 2007. Stating with
certainty or stating with doubt:
Intercoder reliability results for manual
annotation of epistemically modalized
statements. In Proceedings of the Human
Language Technology and North American
Chapter of the Association for Computational
Linguistics (Short Papers), pages 141?144,
Rochester, NY.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43(3):227?268.
Saur??, Roser, Marc Verhagen, and James
Pustejovsky. 2006. Annotating and
recognizing event modality in text.
In Proceedings of the 19th International
Florida Artificial Intelligence Research
Society Conference, pages 333?339,
Melbourne Beach, FL.
Sigurd, Bengt and Barbara Gawro?nska.
1994. Modals as a problem for MT. In
Proceedings of the 15th International
Conference on Computational Linguistics
(COLING) - Volume 1, pages 120?124,
Kyoto, Japan.
Steedman, Mark. 1999. Alternating
quantifier scope in CCG. In Proceedings of
the 37th Annual Meeting of the Association
for Computational Linguistics (ACL),
College Park, MD.
Szarvas, Gyo?rgy, Veronika Vincze, Richa?rd
Farkas, and Ja?nos Csirik. 2008. The
BioScope corpus: Annotation for negation,
uncertainty and their scope in biomedical
texts. In Proceedings of the Workshop on
Current Trends in Biomedical Natural
Language Processing, pages 38?45,
Stroudsburg, PA.
van der Auwera, Johan and Andreas
Ammann. 2005. Overlap between
situational and epistemic modal marking.
In Martin Haspelmath, Matthew S. Dryer,
David Gil, and Bernard Comrie, editors,
World Atlas of Language Structures. Oxford
University Press, New York, chapter 76,
pages 310?313.
Venugopal, Ashish and Andreas Zollmann.
2009. Grammar based statistical MT on
Hadoop: An end-to-end toolkit for large
scale PSCFG based MT. Prague Bulletin of
Mathematical Linguistics, 91:67?78.
Venugopal, Ashish, Andreas Zollmann, and
Stephan Vogel. 2007. An efficient two-pass
approach to synchronous-CFG driven
statistical MT. In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics
(HLT/NAACL-2007), pages 500?507,
Rochester, NY.
von Fintel, Kai and Sabine Iatridou.
2006. How to say ought in foreign: The
composition of weak necessity modals.
In Proceedings of the 6th Workshop on
Formal Linguistics, Florianopolis, Brazil,
August 2006.
Wang, Wei, Jonathan May, Kevin Knight,
and Daniel Marcu. 2010. Re-structuring,
re-labeling, and re-aligning for
syntax-based machine translation.
Computational Linguistics, 36(2):247?277.
Webber, Bonnie, Aravid Joshi, Matthew
Stone, and Alistair Knott. 2003. Anaphora
and discourse structure. Computational
Linguistics, 29:545?587.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions
of opinions and emotions in language.
Language Resources and Evaluation,
39(2?3):165?210.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffman. 2009. Recognizing contextual
polarity: An exploration of features
for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In
Proceedings on the Workshop on Statistical
Machine Translation, pages 138?141,
New York City.
438
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 765?770,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The Effects of Lexical Resource Quality on Preference Violation Detection
Jesse Dunietz
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
jdunietz@cs.cmu.edu
Lori Levin and Jaime Carbonell
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{lsl,jgc}@cs.cmu.edu
Abstract
Lexical resources such as WordNet and
VerbNet are widely used in a multitude
of NLP tasks, as are annotated corpora
such as treebanks. Often, the resources
are used as-is, without question or exam-
ination. This practice risks missing sig-
nificant performance gains and even entire
techniques.
This paper addresses the importance of
resource quality through the lens of a
challenging NLP task: detecting selec-
tional preference violations. We present
DAVID, a simple, lexical resource-based
preference violation detector. With as-
is lexical resources, DAVID achieves an
F1-measure of just 28.27%. When the
resource entries and parser outputs for
a small sample are corrected, however,
the F1-measure on that sample jumps
from 40% to 61.54%, and performance
on other examples rises, suggesting that
the algorithm becomes practical given re-
fined resources. More broadly, this pa-
per shows that resource quality matters
tremendously, sometimes even more than
algorithmic improvements.
1 Introduction
A variety of NLP tasks have been addressed
using selectional preferences or restrictions, in-
cluding word sense disambiguation (see Navigli
(2009)), semantic parsing (e.g., Shi and Mihalcea
(2005)), and metaphor processing (see Shutova
(2010)). These semantic problems are quite chal-
lenging; metaphor analysis, for instance, has long
been recognized as requiring considerable seman-
tic knowledge (Wilks, 1978; Carbonell, 1980).
The advent of extensive lexical resources, an-
notated corpora, and a spectrum of NLP tools
presents an opportunity to revisit such challenges
from the perspective of selectional preference vio-
lations. Detecting these violations, however, con-
stitutes a severe stress-test for resources designed
for other tasks. As such, it can highlight shortcom-
ings and allow quantifying the potential benefits of
improving resources such as WordNet (Fellbaum,
1998) and VerbNet (Schuler, 2005).
In this paper, we present DAVID (Detector of
Arguments of Verbs with Incompatible Denota-
tions), a resource-based system for detecting pref-
erence violations. DAVID is one component of
METAL (Metaphor Extraction via Targeted Anal-
ysis of Language), a new system for identifying,
interpreting, and cataloguing metaphors. One pur-
pose of DAVID was to explore how far lexical
resource-based techniques can take us. Though
our initial results suggested that the answer is ?not
very,? further analysis revealed that the problem
lies less in the technique than in the state of exist-
ing resources and tools.
Often, it is assumed that the frontier of perfor-
mance on NLP tasks is shaped entirely by algo-
rithms. Manning (2011) showed that this may not
hold for POS tagging ? that further improvements
may require resource cleanup. In the same spirit,
we argue that for some semantic tasks, exemplified
by preference violation detection, resource qual-
ity may be at least as essential as algorithmic en-
hancements.
2 The Preference Violation Detection
Task
DAVID builds on the insight of Wilks (1978) that
the strongest indicator of metaphoricity is the vi-
olation of selectional preferences. For example,
only plants can literally be pruned. If laws is
the object of pruned, the verb is likely metaphori-
cal. Flagging such semantic mismatches between
verbs and arguments is the task of preference vio-
lation detection.
765
We base our definition of preferences on the
Pragglejaz guidelines (Pragglejaz Group, 2007)
for identifying the most basic sense of a word as
the most concrete, embodied, or precise one. Sim-
ilarly, we define selectional preferences as the se-
mantic constraints imposed by a verb?s most basic
sense. Dictionaries may list figurative senses of
prune, but we take the basic sense to be cutting
plant growth.
Several types of verbs were excluded from the
task because they have very lax preferences. These
include verbs of becoming or seeming (e.g., trans-
form, appear), light verbs, auxiliaries, and aspec-
tual verbs. For the sake of simplifying implemen-
tation, phrasal verbs were also ignored.
3 Algorithm Design
To identify violations, DAVID employs a simple
algorithm based on several existing tools and re-
sources: SENNA (Collobert et al, 2011), a seman-
tic role labeling (SRL) system; VerbNet, a com-
putational verb lexicon; SemLink (Loper et al,
2007), which includes mappings between Prop-
Bank (Palmer et al, 2005) and VerbNet; and
WordNet. As one metaphor detection component
of METAL?s several, DAVID is designed to favor
precision over recall. The algorithm is as follows:
1. Run the Stanford CoreNLP POS tagger
(Toutanova et al, 2003) and the TurboParser
dependency parser (Martins et al, 2011).
2. Run SENNA to identify the semantic argu-
ments of each verb in the sentence using the
PropBank argument annotation scheme (Arg0,
Arg1, etc.). See Table 1 for example output.
3. For each verb V , find all VerbNet entries for
V . Using SemLink, map each PropBank argu-
ment name to the corresponding VerbNet the-
matic roles in these entries (Agent, Patient,
etc.). For example, the VerbNet class for prune
is carve-21.2-2. SemLink maps Arg0 to
the Agent of carve-21.2-2 and Arg1 to
the Patient.
4. Retrieve from VerbNet the selectional restric-
tions of each thematic role. In our running
example, VerbNet specifies +int control
and +concrete for the Agent and Patient of
carve-21.2-2, respectively.
5. If the head of any argument cannot be inter-
preted to meet V ?s preferences, flag V as a vi-
olation.
?The politician pruned laws regulating plastic
bags, and created new fees for inspecting dairy
farms.?
Verb Arg0 Arg1
pruned The politician laws . . . bags
regulating laws plastic bags
created The politician new fees
inspecting - - dairy farms
Table 1: SENNA?s SRL output for the example
sentence above. Though this example demon-
strates only two arguments, SENNA is capable of
labeling up to six.
Restriction WordNet Synsets
animate animate being.n.01
people.n.01
person.n.01
concrete physical object.n.01
matter.n.01
substance.n.01
organization social group.n.01
district.n.01
Table 2: DAVID?s mappings between some
common VerbNet restriction types and WordNet
synsets.
Each VerbNet restriction is interpreted as man-
dating or forbidding a set of WordNet hypernyms,
defined by a custom mapping (see Table 2).
For example, VerbNet requires both the Patient
of a verb in carve-21.2-2 and the Theme
of a verb in wipe manner-10.4.1-1 to
be concrete. By empirical inspection, concrete
nouns are hyponyms of the WordNet synsets
physical object.n.01, matter.n.03,
or substance.n.04. Laws (the Patient of
prune) is a hyponym of none of these, so prune
would be flagged as a violation.
4 Corpus Annotation
To evaluate our system, we assembled a corpus
of 715 sentences from the METAL project?s cor-
pus of sentences with and without metaphors. The
corpus was annotated by two annotators follow-
ing an annotation manual. Each verb was marked
for whether its arguments violated the selectional
preferences of the most basic, literal meaning of
the verb. The annotators resolved conflicts by dis-
766
Error source Frequency
Bad/missing VN entries 4.5 (14.1%)
Bad/missing VN restrictions 6 (18.8%)
Bad/missing SL mappings 2 (6.3%)
Parsing/head-finding errors 3.5 (10.9%)
SRL errors 8.5 (26.6%)
VN restriction system too weak 4 (12.5%)
Confounding WordNet senses 3.5 (10.9%)
Endemic errors: 7.5 (23.4%)
Resource errors: 12.5 (39.1%)
Tool errors: 12 (37.5%)
Total: 32 (100%)
Table 3: Sources of error in 90 randomly selected
sentences. For errors that were due to a combi-
nation of sources, 1/2 point was awarded to each
source. (VN stands for VerbNet and SL for Sem-
Link.)
cussing until consensus.
5 Initial Results
As the first row of Table 4 shows, our initial eval-
uation left little hope for the technique. With
such low precision and F1, it seemed a lexical
resource-based preference violation detector was
out. When we analyzed the errors in 90 randomly
selected sentences, however, we found that most
were not due to systemic problems with the ap-
proach; rather, they stemmed from SRL and pars-
ing errors and missing or incorrect resource entries
(see Table 3). Armed with this information, we de-
cided to explore how viable our algorithm would
be absent these problems.
6 Refining The Data
To evaluate the effects of correcting DAVID?s in-
puts, we manually corrected the tool outputs and
resource entries that affected the aforementioned
90 sentences. SRL output was corrected for ev-
ery sentence, while SemLink and VerbNet entries
were corrected only for each verb that produced an
error.
6.1 Corrections to Tool Output (Parser/SRL)
Guided by the PropBank database and annotation
guidelines, we corrected all errors in core role
assignments from SENNA. These corrections in-
cluded relabeling arguments, adding missed argu-
ments, fixing argument spans, and deleting anno-
tations for non-verbs. The only parser-related er-
ror we corrected was a mislabeled noun.
6.2 Correcting Corrupted Data in VerbNet
The VerbNet download is missing several sub-
classes that are referred to by SemLink or that
have been updated on the VerbNet website. Some
roles also have not been updated to the latest ver-
sion, and some subclasses are listed with incor-
rect IDs. These problems, which caused SemLink
mappings to fail, were corrected before reviewing
errors from the corpus.
Six subclasses needed to be fixed, all of which
were easily detected by a simple script that did not
depend on the 90-sentence subcorpus. We there-
fore expect that few further changes of this type
would be needed for a more complete resource re-
finement effort.
6.3 Corpus-Based Updates to SemLink
Our modifications to SemLink?s mappings in-
cluded adding missing verbs, adding missing roles
to mappings, and correcting mappings to more ap-
propriate classes or roles. We also added null map-
pings in cases where a PropBank argument had no
corresponding role in VerbNet. This makes the
system?s strategy for ruling out mappings more re-
liable.
No corrections were made purely based on the
sample. Any time a verb?s mappings were edited,
VerbNet was scoured for plausible mappings for
every verb sense in PropBank, and any nonsensi-
cal mappings were deleted. For example, when
the phrase go dormant caused an error, we in-
spected the mappings for go. Arguments of all but
2 of the 7 available mappings were edited, either
to add missing arguments or to correct nonsensi-
cal ones. These changes actually had a net neg-
ative impact on test set performance because the
bad mappings had masked parsing and selectional
preference problems.
Based on the 90-sentence subcorpus, we mod-
ified 20 of the existing verb entries in SemLink.
These changes included correcting 8 role map-
pings, adding 13 missing role mappings to existing
senses, deleting 2 incorrect senses, adding 11 verb
senses, correcting 2 senses, deleting 1 superfluous
role mapping, and adding 46 null role mappings.
(Note that although null mappings represented the
largest set of changes, they also had the least im-
pact on system behavior.) One entirely new verb
was added, as well.
767
6.4 Corpus-Based Updates to VerbNet
Nineteen VerbNet classes were modified, and one
class had to be added. The modifications gener-
ally involved adding, correcting, or deleting se-
lectional restrictions, often by introducing or re-
arranging subclasses. Other changes amounted to
fixing clerical errors, such as incorrect role names
or restrictions that had been ANDed instead of
ORed.
An especially difficult problem was an inconsis-
tency in the semantics of VerbNet?s subclass sys-
tem. In some cases, the restrictions specified on
a verb in a subclass did not apply to subcatego-
rization frames inherited from a superclass, but in
other cases the restrictions clearly applied to all
frames. The conflict was resolved by duplicating
subclassed verbs in the top-level class whenever
different selectional restrictions were needed for
the two sets of frames.
As with SemLink, samples determined only
which classes were modified, not what modifica-
tions were made. Any non-obvious changes to
selectional restrictions were verified by examin-
ing dozens of verb instances from SketchEngine?s
(Kilgarriff et al, 2004) corpus. For example, the
Agent of seek was restricted to +animate, but
the corpus confirmed that organizations are com-
monly described non-metaphorically as seeking,
so the restriction was updated to +animate |
+organization.
7 Results After Resource Refinement
After making corrections for each set of 10 sen-
tences, we incrementally recomputed F1 and pre-
cision, both on the subcorpus corrected so far and
on a test set of all 625 sentences that were never
corrected. (The manual nature of the correction ef-
fort made testing k-fold subsets impractical.) The
results for 30-sentence increments are shown in
Table 4.
The most striking feature of these figures is how
much performance improves on corrected sen-
tences: for the full 90 sentences, F1 rose from
30.43% to 61.54%, and precision rose even more
dramatically from 31.82% to 80.00%. Interest-
ingly, resource corrections alone generally made a
larger difference than tool corrections alone, sug-
gesting that resources may be the dominant fac-
tor in resource-intensive tasks such as this one.
Even more compellingly, the improvement from
correcting both the tools and the resources was
nearly double the sum of the improvements from
each alone: tool and resource improvements inter-
act synergistically.
The effects on the test corpus are harder to
interpret. Due to a combination of SRL prob-
lems and the small number of sentences cor-
rected, the scores on the test set improved little
with resource correction; in fact, they even dipped
slightly between the 30- and 60-sentence incre-
ments. Nonetheless, we contend that our results
testify to the generality of our corrections: after
each iteration, every altered result was either an
error fixed or an error that should have appeared
before but had been masked by another. Note also
that all results on the test set are without corrected
tool output; presumably, these sentences would
also have improved synergistically with more ac-
curate SRL. How long corrections would continue
to improve performance is a question that we did
not have the resources to answer, but our results
suggest that there is plenty of room to go.
Some errors, of course, are endemic to the ap-
proach and cannot be fixed either by improved re-
sources or by better tools. For example, we con-
sider every WordNet sense to be plausible, which
produces false negatives. Additionally, the selec-
tional restrictions specified by VerbNet are fairly
loose; a more refined set of categories might cap-
ture the range of verbs? restrictions more accu-
rately.
8 Implications for Future Refinement
Efforts
Although improving resources is infamously
labor-intensive, we believe that similarly refining
the remainder of VerbNet and SemLink would be
doable. In our study, it took about 25-35 person-
hours to examine about 150 verbs and to mod-
ify 20 VerbNet classes and 25 SemLink verb en-
tries (excluding time for SENNA corrections, fix-
ing corrupt VerbNet data, and analysis of DAVID?s
errors). Extrapolating from our experience, we es-
timate that it would take roughly 6-8 person-weeks
to systematically fix this particular set of issues
with VerbNet.
Improving SemLink could be more complex,
as its mappings are automatically generated from
VerbNet annotations on top of the PropBank cor-
pus. One possibility is to correct the generated
mappings directly, as we did in our study, which
we estimate would take about two person-months.
768
With the addition of some metadata from the gen-
eration process, it would then be possible to follow
the corrected mappings back to annotations from
which they were generated and fix those annota-
tions. One downside of this approach is that if the
mappings were ever regenerated from the anno-
tated corpus, any mappings not encountered in the
corpus would have to be added back afterwards.
Null role mappings would be particularly thorny
to implement. To add a null mapping, we must
know that a role definitely does not belong, and
is not just incidentally missing from an exam-
ple. For instance, VerbNet?s defend-85 class
truly has no equivalent to Arg2 in PropBank?s
defend.01, but Arg0 or Arg1 may be missing
for other reasons (e.g., in a passive). It may be best
to simply omit null mappings, as is currently done.
Alternatively, full parses from the Penn Treebank,
on which PropBank is based, might allow distin-
guishing phenomena such as passives where argu-
ments are predictably omitted.
The maintainers of VerbNet and PropBank are
aware of many of the issues we have raised, and
we have been in contact with them about possi-
ble approaches to fixing them. They are particu-
larly aware of the inconsistent semantics of selec-
tional restrictions on VerbNet subclasses, and they
hope to fix this issue within a larger attempt at re-
tooling VerbNet?s selectional restrictions. In the
meantime, we are sharing our VerbNet modifica-
tions with them for them to verify and incorporate.
We are also sharing our SemLink changes so that
they can, if they choose, continue manual correc-
tion efforts or trace SemLink problems back to the
annotated corpus.
9 Conclusion
Our results argue for investing effort in developing
and fixing resources, in addition to developing bet-
ter NLP tools. Resource and tool improvements
interact synergistically: better resources multiply
the effect of algorithm enhancements. Gains from
fixing resources may sometimes even exceed what
the best possible algorithmic improvements can
provide. We hope the NLP community will take
up the challenge of investing in its resources to the
extent that its tools demand.
Acknowledgments
Thanks to Eric Nyberg for suggesting building a
system like DAVID, to Spencer Onuffer for his an-
Sent. Tools Rsrcs P F1
715 0 0 27.14% 28.27%
625 0 0 26.55% 27.98%
625 0 corr. 26.37% 28.15%
30 0 0 50.00% 40.00%
30 30 0 66.67% 44.44%
30 0 corr.+30 62.50% 50.00%
30 30 corr.+30 87.50% 70.00%
625 0 corr.+30 27.07% 28.82%
60 0 0 35.71% 31.25%
60 60 0 54.55% 31.38%
60 0 corr.+60 53.85% 45.16%
60 60 corr.+60 90.91% 68.97%
625 0 corr.+60 26.92% 28.74%
90 0 0 31.82% 30.43%
90 90 0 44.44% 38.10%
90 0 corr.+90 47.37% 41.86%
90 90 corr.+90 80.00% 61.54%
625 0 corr.+90 27.37% 28.99%
Table 4: Performance on preference violation de-
tection task. Column 1 shows the sentence count.
Columns 2 and 3 show how many sentences?
SRL/parsing and resource errors, respectively, had
been fixed (?corr.? indicates corrupted files).
notation efforts, and to Davida Fromm for curating
METAL?s corpus of Engish sentences.
This work was supported by the Intelligence
Advanced Research Projects Activity (IARPA)
via Department of Defense US Army Research
Laboratory contract number W911NF-12-C-0020.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoD/ARL, or the U.S. Govern-
ment.
References
Jaime G. Carbonell. 1980. Metaphor: a key to ex-
tensible semantic analysis. In Proceedings of the
18th annual meeting on Association for Computa-
tional Linguistics, ACL ?80, pages 17?21, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
769
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Bradford Books.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of EURALEX.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics, Tilburg, the Netherlands.
Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Computational Linguistics and Intelligent
Text Processing, pages 171?189. Springer.
Andre? F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Ma?rio A. T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 238?249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Pragglejaz Group. 2007. MIP: A method for iden-
tifying metaphorically used words in discourse.
Metaphor and Symbol, 22(1):1?39.
Karin K. Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania, Philadelphia, PA.
AAI3179808.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Alexander
Gelbukh, editor, Computational Linguistics and In-
telligent Text Processing, volume 3406 of Lec-
ture Notes in Computer Science, pages 100?111.
Springer Berlin Heidelberg.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 688?697, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11:197?223.
770
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 57?64, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Statistical Modality Tagging
from Rule-based Annotations and Crowdsourcing
Vinodkumar Prabhakaran Michael Bloodgood Mona Diab
CS CASL CCLS
Columbia University University of Maryland Columbia University
vinod@cs.columbia.edu meb@umd.edu mdiab@ccls.columbia.edu
Bonnie Dorr Lori Levin Christine D. Piatko
CS and UMIACS LTI APL
University of Maryland Carnegie Mellon University Johns Hopkins University
bonnie@umiacs.umd.edu lsl@cs.cmu.edu christine.piatko@jhuapl.edu
Owen Rambow Benjamin Van Durme
CCLS HLTCOE
Columbia University Johns Hopkins University
rambow@ccls.columbia.edu vandurme@cs.jhu.edu
Abstract
We explore training an automatic modality
tagger. Modality is the attitude that a speaker
might have toward an event or state. One of
the main hurdles for training a linguistic tag-
ger is gathering training data. This is par-
ticularly problematic for training a tagger for
modality because modality triggers are sparse
for the overwhelming majority of sentences.
We investigate an approach to automatically
training a modality tagger where we first gath-
ered sentences based on a high-recall simple
rule-based modality tagger and then provided
these sentences to Mechanical Turk annotators
for further annotation. We used the resulting
set of training data to train a precise modality
tagger using a multi-class SVM that delivers
good performance.
1 Introduction
Modality is an extra-propositional component of
meaning. In John may go to NY, the basic propo-
sition is John go to NY and the word may indi-
cates modality. Van Der Auwera and Ammann
(2005) define core cases of modality: John must
go to NY (epistemic necessity), John might go to
NY (epistemic possibility), John has to leave now
(deontic necessity) and John may leave now (de-
ontic possibility). Many semanticists (e.g. Kratzer
(1981), Kratzer (1991), Kaufmann et al (2006)) de-
fine modality as quantification over possible worlds.
John might go means that there exist some possi-
ble worlds in which John goes. Another view of
modality relates more to a speakers attitude toward
a proposition (e.g. McShane et al (2004)).
Modality might be construed broadly to include
several types of attitudes that a speaker wants to ex-
press towards an event, state or proposition. Modal-
ity might indicate factivity, evidentiality, or senti-
ment (McShane et al, 2004). Factivity is related to
whether the speaker wishes to convey his or her be-
lief that the propositional content is true or not, i.e.,
whether it actually obtains in this world or not. It
distinguishes things that (the speaker believes) hap-
pened from things that he or she desires, plans, or
considers merely probable. Evidentiality deals with
the source of information and may provide clues to
the reliability of the information. Did the speaker
57
have firsthand knowledge of what he or she is re-
porting, or was it hearsay or inferred from indirect
evidence? Sentiment deals with a speaker?s positive
or negative feelings toward an event, state, or propo-
sition.
In this paper, we focus on the following five
modalities; we have investigated the belief/factivity
modality previously (Diab et al, 2009b; Prab-
hakaran et al, 2010), and we leave other modalities
to future work.
? Ability: can H do P?
? Effort: does H try to do P?
? Intention: does H intend P?
? Success: does H succeed in P?
? Want: does H want P?
We investigate automatically training a modality
tagger by using multi-class Support Vector Ma-
chines (SVMs). One of the main hurdles for training
a linguistic tagger is gathering training data. This is
particularly problematic for training a modality tag-
ger because modality triggers are sparse for the over-
whelming majority of the sentences. Baker et al
(2010) created a modality tagger by using a semi-
automatic approach for creating rules for a rule-
based tagger. A pilot study revealed that it can boost
recall well above the naturally occurring proportion
of modality without annotated data but with only
60% precision. We investigated an approach where
we first gathered sentences based on a simple modal-
ity tagger and then provided these sentences to an-
notators for further annotation, The resulting anno-
tated data also preserved the level of inter-annotator
agreement for each example so that learning algo-
rithms could take that into account during training.
Finally, the resulting set of annotations was used for
training a modality tagger using SVMs, which gave
a high precision indicating the success of this ap-
proach.
Section 2 discusses related work. Section 3 dis-
cusses our procedure for gathering training data.
Section 4 discusses the machine learning setup
and features used to train our modality tagger and
presents experiments and results. Section 5 con-
cludes and discusses future work.
2 Related Work
Previous related work includes TimeML (Sauri et
al., 2006), which involves modality annotation on
events, and Factbank (Sauri and Pustejovsky, 2009),
where event mentions are marked with degree of fac-
tuality. Modality is also important in the detection of
uncertainty and hedging. The CoNLL shared task in
2010 (Farkas et al, 2010) deals with automatic de-
tection of uncertainty and hedging in Wikipedia and
biomedical sentences.
Baker et al (2010) and Baker et al (2012) ana-
lyze a set of eight modalities which include belief,
require and permit, in addition to the five modalities
we focus on in this paper. They built a rule-based
modality tagger using a semi-automatic approach to
create rules. This earlier work differs from the work
described in this paper in that the our emphasis is on
the creation of an automatic modality tagger using
machine learning techniques. Note that the anno-
tation and automatic tagging of the belief modality
(i.e., factivity) is described in more detail in (Diab et
al., 2009b; Prabhakaran et al, 2010).
There has been a considerable amount of inter-
est in modality in the biomedical domain. Negation,
uncertainty, and hedging are annotated in the Bio-
scope corpus (Vincze et al, 2008), along with infor-
mation about which words are in the scope of nega-
tion/uncertainty. The i2b2 NLP Shared Task in 2010
included a track for detecting assertion status (e.g.
present, absent, possible, conditional, hypothetical
etc.) of medical problems in clinical records.1 Apos-
tolova et al (2011) presents a rule-based system for
the detection of negation and speculation scopes us-
ing the Bioscope corpus. Other studies emphasize
the importance of detecting uncertainty in medical
text summarization (Morante and Daelemans, 2009;
Aramaki et al, 2009).
Modality has also received some attention in the
context of certain applications. Earlier work de-
scribing the difficulty of correctly translating modal-
ity using machine translation includes (Sigurd and
Gawro?nska, 1994) and (Murata et al, 2005). Sig-
urd et al (1994) write about rule based frameworks
and how using alternate grammatical constructions
such as the passive can improve the rendering of the
modal in the target language. Murata et al (2005)
1https://www.i2b2.org/NLP/Relations/
58
analyze the translation of Japanese into English
by several systems, showing they often render the
present incorrectly as the progressive. The authors
trained a support vector machine to specifically han-
dle modal constructions, while our modal annotation
approach is a part of a full translation system.
The textual entailment literature includes modal-
ity annotation schemes. Identifying modalities is
important to determine whether a text entails a hy-
pothesis. Bar-Haim et al (2007) include polarity
based rules and negation and modality annotation
rules. The polarity rules are based on an indepen-
dent polarity lexicon (Nairn et al, 2006). The an-
notation rules for negation and modality of predi-
cates are based on identifying modal verbs, as well
as conditional sentences and modal adverbials. The
authors read the modality off parse trees directly us-
ing simple structural rules for modifiers.
3 Constructing Modality Training Data
In this section, we will discuss the procedure we
followed to construct the training data for build-
ing the automatic modality tagger. In a pilot study,
we obtained and ran the modality tagger described
in (Baker et al, 2010) on the English side of the
Urdu-English LDC language pack.2 We randomly
selected 1997 sentences that the tagger had labeled
as not having the Want modality and posted them on
Amazon Mechanical Turk (MTurk). Three differ-
ent Turkers (MTurk annotators) marked, for each of
the sentences, whether it contained the Want modal-
ity. Using majority rules as the Turker judgment,
95 (i.e., 4.76%) of these sentences were marked as
having a Want modality. We also posted 1993 sen-
tences that the tagger had labeled as having a Want
modality and only 1238 of them were marked by the
Turkers as having a Want modality. Therefore, the
estimated precision of this type of approach is only
around 60%.
Hence, we will not be able to use the (Baker et
al., 2010) tagger to gather training data. Instead,
our approach was to apply a simple tagger as a first
pass, with positive examples subsequently hand-
annotated using MTurk. We made use of sentence
data from the Enron email corpus,3 derived from the
2LDC Catalog No.: LDC2006E110.
3http://www-2.cs.cmu.edu/?enron/
version owing to Fiore and Heer,4 further processed
as described by (Roark, 2009).5
To construct the simple tagger (the first pass), we
used a lexicon of modality trigger words (e.g., try,
plan, aim, wish, want) constructed by Baker et al
(2010). The tagger essentially tags each sentence
that has a word in the lexicon with the corresponding
modality. We wrote a few simple obvious filters for a
handful of exceptional cases that arise due to the fact
that our sentences are from e-mail. For example, we
filtered out best wishes expressions, which otherwise
would have been tagged as Want because of the word
wishes.
The words that trigger modality occur with very
different frequencies. If one is not careful, the
training data may be dominated by only the com-
monly occurring trigger words and the learned tag-
ger would then be biased towards these words. In
order to ensure that our training data had a diverse
set of examples containing many lexical triggers and
not just a lot of examples with the same lexical trig-
ger, for each modality we capped the number of sen-
tences from a single trigger to be at most 50. After
we had the set of sentences selected by the simple
tagger, we posted them on MTurk for annotation.
The Turkers were asked to check a box indicat-
ing that the modality was not present in the sentence
if the given modality was not expressed. If they did
not check that box, then they were asked to highlight
the target of the modality. Table 1 shows the number
of sentences we posted on MTurk for each modal-
ity.6 Three Turkers annotated each sentence. We
restricted the task to Turkers who were adults, had
greater than a 95% approval rating, and had com-
pleted at least 50 HITs (Human Intelligence Tasks)
on MTurk. We paid US$0.10 for each set of ten sen-
tences.
Since our data was annotated by three Turkers,
for training data we used only those examples for
which at least two Turkers agreed on the modality
and the target of the modality. This resulted in 1,008
examples. 674 examples had two Turkers agreeing
and 334 had unanimous agreement. We kept track
of the level of agreement for each example so that
4http://bailando.sims.berkeley.edu/enron/enron.sql.gz
5Data received through personal communication
6More detailed statistics on MTurk annotations are available
at http://hltcoe.jhu.edu/datasets/.
59
Modality Count
Ability 190
Effort 1350
Intention 1320
Success 1160
Want 1390
Table 1: For each modality, the number of sentences re-
turned by the simple tagger that we posted on MTurk.
our learner could weight the examples differently
depending on the level of inter-annotator agreement.
4 Multiclass SVM for Modality
In this section, we describe the automatic modal-
ity tagger we built using the MTurk annotations de-
scribed in Section 3 as the training data. Section 4.1
describes the training and evaluation data. In Sec-
tion 4.2, we present the machinery and Section 4.3
describes the features we used to train the tagger.
In Section 4.4, we present various experiments and
discuss results. Section 4.5, presents additional ex-
periments using annotator confidence.
4.1 Data
For training, we used the data presented in Section 3.
We refer to it as MTurk data in the rest of this paper.
For evaluation, we selected a part of the LU Corpus
(Diab et al, 2009a) (1228 sentences) and our expert
annotated it with modality tags. We first used the
high-recall simple modality tagger described in Sec-
tion 3 to select the sentences with modalities. Out
of the 235 sentences returned by the simple modal-
ity tagger, our expert removed the ones which did
not in fact have a modality. In the remaining sen-
tences (94 sentences), our expert annotated the tar-
get predicate. We refer to this as the Gold dataset
in this paper. The MTurk and Gold datasets differ in
terms of genres as well as annotators (Turker vs. Ex-
pert). The distribution of modalities in both MTurk
and Gold annotations are given in Table 2.
4.2 Approach
We applied a supervised learning framework us-
ing multi-class SVMs to automatically learn to tag
Modality MTurk Gold
Ability 6% 48%
Effort 25% 10%
Intention 30% 11%
Success 24% 9%
Want 15% 23%
Table 2: Frequency of Modalities
modalities in context. For tagging, we used the Yam-
cha (Kudo and Matsumoto, 2003) sequence labeling
system which uses the SVMlight (Joachims, 1999)
package for classification. We used One versus All
method for multi-class classification on a quadratic
kernel with a C value of 1. We report recall and pre-
cision on word tokens in our corpus for each modal-
ity. We also report F?=1 (F)-measure as the har-
monic mean between (P)recision and (R)ecall.
4.3 Features
We used lexical features at the token level which can
be extracted without any parsing with relatively high
accuracy. We use the term context width to denote
the window of tokens whose features are considered
for predicting the tag for a given token. For example,
a context width of 2 means that the feature vector
of any given token includes, in addition to its own
features, those of 2 tokens before and after it as well
as the tag prediction for 2 tokens before it. We did
experiments varying the context width from 1 to 5
and found that a context width of 2 gives the optimal
performance. All results reported in this paper are
obtained with a context width of 2. For each token,
we performed experiments using following lexical
features:
? wordStem - Word stem.
? wordLemma - Word lemma.
? POS - Word?s POS tag.
? isNumeric - Word is Numeric?
? verbType - Modal/Auxiliary/Regular/Nil
? whichModal - If the word is a modal verb,
which modal?
60
We used the Porter stemmer (Porter, 1997) to ob-
tain the stem of a word token. To determine the
word lemma, we used an in-house lemmatizer using
dictionary and morphological analysis to obtain the
dictionary form of a word. We obtained POS tags
from Stanford POS tagger and used those tags to
determine verbType and whichModal features. The
verbType feature is assigned a value ?Nil? if the word
is not a verb and whichModal feature is assigned a
value ?Nil? if the word is not a modal verb. The fea-
ture isNumeric is a binary feature denoting whether
the token contains only digits or not.
4.4 Experiments and Results
In this section, we present experiments performed
considering all the MTurk annotations where two
annotators agreed and all the MTurk annotations
where all three annotators agreed to be equally cor-
rect annotations. We present experiments applying
differential weights for these annotations in Section
4.5. We performed 4-fold cross validation (4FCV)
on MTurk data in order to select the best feature
set configuration ?. The best feature set obtained
waswordStem,POS,whichModal with a context
width of 2. For finding the best performing fea-
ture set - context width configuration, we did an ex-
haustive search on the feature space, pruning away
features which were proven not useful by results at
stages. Table 3 presents results obtained for each
modality on 4-fold cross validation.
Modality Precision Recall F Measure
Ability 82.4 55.5 65.5
Effort 95.1 82.8 88.5
Intention 84.3 61.3 70.7
Success 93.2 76.6 83.8
Want 88.4 64.3 74.3
Overall 90.1 70.6 79.1
Table 3: Per modality results for best feature set ? on
4-fold cross validation on MTurk data
We also trained a model on the entire MTurk data
using the best feature set ? and evaluated it against
the Gold data. The results obtained for each modal-
ity on gold evaluation are given in Table 4. We at-
tribute the lower performance on the Gold dataset to
its difference from MTurk data. MTurk data is en-
tirely from email threads, whereas Gold data con-
tained sentences from newswire, letters and blogs
in addition to emails. Furthermore, the annotation
is different (Turkers vs expert). Finally, the distri-
bution of modalities in both datasets is very differ-
ent. For example, Ability modality was merely 6%
of MTurk data compared to 48% in Gold data (see
Table 2).
Modality Precision Recall F Measure
Ability 78.6 22.0 34.4
Effort 85.7 60.0 70.6
Intention 66.7 16.7 26.7
Success NA 0.0 NA
Want 92.3 50.0 64.9
Overall 72.1 29.5 41.9
Table 4: Per modality results for best feature set ? evalu-
ated on Gold dataset
We obtained reasonable performances for Effort
and Want modalities while the performance for other
modalities was rather low. Also, the Gold dataset
contained only 8 instances of Success, none of which
was recognized by the tagger resulting in a recall
of 0%. Precision (and, accordingly, F Measure) for
Success was considered ?not applicable? (NA), as no
such tag was assigned.
4.5 Annotation Confidence Experiments
Our MTurk data contains sentence for which at least
two of the three Turkers agreed on the modality and
the target of the modality. In this section, we investi-
gate the role of annotation confidence in training an
automatic tagger. The annotation confidence is de-
noted by whether an annotation was agreed by only
two annotators or was unanimous. We denote the set
of sentences for which only two annotators agreed as
Agr2 and that for which all three annotators agreed
as Agr3.
We present four training setups. The first setup
is Tr23 where we train a model using both Agr2
and Agr3 with equal weights. This is the setup we
used for results presented in the Section 4.4. Then,
we have Tr2 and Tr3, where we train using only
Agr2 and Agr3 respectively. Then, for Tr23W , we
61
TrainingSetup
Tested on Agr2 and Agr3 Tested on Agr3 only
Precision Recall F Measure Precision Recall F Measure
Tr23 90.1 70.6 79.1 95.9 86.8 91.1
Tr2 91.0 66.1 76.5 95.6 81.8 88.2
Tr3 88.1 52.3 65.6 96.8 71.7 82.3
Tr23W 89.9 70.5 79.0 95.8 86.5 90.9
Table 5: Annotator Confidence Experiment Results; the best results per column are boldfaced
(4-fold cross validation on MTurk Data)
train a model giving different cost values for Agr2
and Agr3 examples. The SVMLight package al-
lows users to input cost values ci for each training
instance separately.7 We tuned this cost value for
Agr2 and Agr3 examples and found the best value
at 20 and 30 respectively.
For all four setups, we used feature set ?. We per-
formed 4-fold cross validation on MTurk data in two
ways ? we tested against a combination of Agr2
and Agr3, and we tested against only Agr3. Results
of these experiments are presented in Table 5. We
also present the results of evaluating a tagger trained
on the whole MTurk data for each setup against the
Gold annotation in Table 6. The Tr23 tested on both
Agr2 andAgr3 presented in Table 5 and Tr23 tested
on Gold data presented in Table 6 correspond to the
results presented in Table 3 and Table 4 respectively.
TrainingSetup Precision Recall F Measure
Tr23 72.1 29.5 41.9
Tr2 67.4 27.6 39.2
Tr3 74.1 19.1 30.3
Tr23W 73.3 31.4 44.0
Table 6: Annotator Confidence Experiment Results; the
best results per column are boldfaced
(Evaluation against Gold)
One main observation is that including annota-
tions of lower agreement, but still above a threshold
(in our case, 66.7%), is definitely helpful. Tr23 out-
performed both Tr2 and Tr3 in both recall and F-
7This can be done by specifying ?cost:<value>? after the
label in each training instance. This feature has not yet been
documented on the SVMlight website.
measure in all evaluations. Also, even when evaluat-
ing against only the high confident Agr3 cases, Tr2
gave a high gain in recall (10 .1 percentage points)
over Tr3, with only a 1.2 percentage point loss on
precision. We conjecture that this is because there
are far more training instances in Tr2 than in Tr3
(674 vs 334), and that quantity beats quality.
Another important observation is the increase in
performance by using varied costs for Agr2 and
Agr3 examples (the Tr23W condition). Although
it dropped the performance by 0.1 to 0.2 points
in cross-validation F measure on the Enron cor-
pora, it gained 2.1 points in Gold evaluation F mea-
sure. These results seem to indicate that differential
weighting based on annotator agreement might have
more beneficial impact when training a model that
will be applied to a wide range of genres than when
training a model with genre-specific data for appli-
cation to data from the same genre. Put differently,
using varied costs prevents genre over-fitting. We
don?t have a full explanation for this difference in
behavior yet. We plan to explore this in future work.
5 Conclusion
We have presented an innovative way of combining
a high-recall simple tagger with Mechanical Turk
annotations to produce training data for a modality
tagger. We show that we obtain good performance
on the same genre as this training corpus (annotated
in the same manner), and reasonable performance
across genres (annotated by an independent expert).
We also present experiments utilizing the number of
agreeing Turkers to choose cost values for training
examples for the SVM. As future work, we plan to
extend this approach to other modalities which are
62
not covered in this study.
6 Acknowledgments
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor. We thank several anony-
mous reviewers for their constructive feedback.
References
Emilia Apostolova, Noriko Tomuro, and Dina Demner-
Fushman. 2011. Automatic extraction of lexico-
syntactic patterns for detection of negation and spec-
ulation scopes. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 283?287, Portland, Oregon.
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. Text2table: Medical text summarization
system based on named entity recognition and modal-
ity identification. In Proceedings of the BioNLP 2009
Workshop, pages 185?192, Boulder, Colorado, June.
Association for Computational Linguistics.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori S. Levin, and Christine D.
Piatko. 2010. A modality lexicon and its use in auto-
matic tagging. In LREC.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Chris Callison-Burch, Nathaniel W. Filardo, Christine
Piatko, Lori Levin, and Scott Miller. 2012. Use of
modality and negation in semantically-informed syn-
tactic mt. Computational Linguistics, 38(22).
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of the 22nd Na-
tional Conference on Artificial intelligence - Volume 1,
pages 871?876, Vancouver, British Columbia, Canada.
AAAI Press.
Mona Diab, Bonnie Dorr, Lori Levin, Teruko Mitamura,
Rebecca Passonneau, Owen Rambow, and Lance
Ramshaw. 2009a. Language Understanding Anno-
tation Corpus. Linguistic Data Consortium (LDC),
USA.
Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009b. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68?73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Szarvas,
Gyo?rgy Mo?ra, and Ja?nos Csirik, editors. 2010. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Uppsala, Sweden, July.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical, pages 169?184.
MIT Press, Cambridge, MA, USA.
Stefan Kaufmann, Cleo Condoravdi, and Valentina
Harizanov, 2006. Formal Approaches to Modality,
pages 72?106. Mouton de Gruyter.
Angelika Kratzer. 1981. The Notional Category of
Modality. In H. J. Eikmeyer and H. Rieser, editors,
Words, Worlds, and Contexts, pages 38?74. de Gruyter,
Berlin.
Angelika Kratzer. 1991. Modality. In Arnim von Ste-
chow and Dieter Wunderlich, editors, Semantics: An
International Handbook of Contemporary Research.
de Gruyter.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In 41st Meeting of the
Association for Computational Linguistics (ACL?03),
Sapporo, Japan.
Marjorie McShane, Sergei Nirenburg, and Ron
Zacharsky. 2004. Mood and modality: Out of
the theory and into the fray. Natural Language
Engineering, 19(1):57?89.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, Toshiyuki
Kanamaru, and Hitoshi Isahara. 2005. Analysis of
machine translation systems? errors in tense, aspect,
and modality. In Proceedings of the 19th Asia-Pacific
Conference on Language, Information and Computa-
tion (PACLIC), Tapei.
Rowan Nairn, Cleo Condorovdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of the International Workshop on
Inference in Computational Semantics, ICoS-5, pages
66?76, Buxton, England.
M. F. Porter, 1997. An algorithm for suffix stripping,
pages 313?316. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014?1022, Beijing,
China, August. Coling 2010 Organizing Committee.
63
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
report, Oregon Health and Science University.
Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Sauri, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In FLAIRS Conference, pages 333?339.
Bengt Sigurd and Barbara Gawro?nska. 1994. Modals
as a problem for MT. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics
(COLING) Volume 1, COLING ?94, pages 120?124,
Kyoto, Japan.
Johan Van Der Auwera and Andreas Ammann, 2005.
Overlap between situational and epistemic modal
marking, chapter 76, pages 310?313. Oxford Univer-
sity Press.
Veronika Vincze, Gy orgy Szarvas, Richa?d Farkas,
Gy orgy Mora, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
64
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271?280,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Generating English Determiners in Phrase-Based Translation with
Synthetic Translation Options
Yulia Tsvetkov Chris Dyer Lori Levin Archna Bhatia
Language Technologies Institute
Carnegie Mellon University
Pittspurgh, PA, 15213, USA
{ytsvetko, cdyer, lsl, archna}@cs.cmu.edu
Abstract
We propose a technique for improving
the quality of phrase-based translation
systems by creating synthetic translation
options?phrasal translations that are gen-
erated by auxiliary translation and post-
editing processes?to augment the de-
fault phrase inventory learned from par-
allel data. We apply our technique to
the problem of producing English deter-
miners when translating from Russian and
Czech, languages that lack definiteness
morphemes. Our approach augments the
English side of the phrase table using a
classifier to predict where English arti-
cles might plausibly be added or removed,
and then we decode as usual. Doing
so, we obtain significant improvements in
quality relative to a standard phrase-based
baseline and to a to post-editing complete
translations with the classifier.
1 Introduction
Phrase-based translation works as follows. A set
of candidate translations for an input sentence is
created by matching contiguous spans of the in-
put against an inventory of phrasal translations,
reordering them into a target-language appropri-
ate order, and choosing the best one according to a
discriminative model that combines features of the
phrases used, reordering patterns, and target lan-
guage model (Koehn et al, 2003). This relatively
simple approach to translation can be remarkably
effective, and, since its introduction, it has been
the basis for further innovations, including devel-
oping better models for distinguishing the good
translations from bad ones (Chiang, 2012; Gim-
pel and Smith, 2012; Cherry and Foster, 2012;
Eidelman et al, 2013), improving the identifica-
tion of phrase pairs in parallel data (DeNero et al,
2008; DeNero and Klein, 2010), and formal gen-
eralizations to gapped rules and rich nonterminal
types (Chiang, 2007; Galley et al, 2006). This
paper proposes a different mechanism for improv-
ing phrase-based translation: the use of synthetic
translation options to supplement the standard
phrasal inventory used in phrase-based translation
systems.
In the following, we argue that phrase tables ac-
quired in usual way will be expected to have gaps
in their coverage in certain language pairs and
that supplementing these with synthetic translation
options is a priori preferable to alternative tech-
niques, such as post processing, for generalizing
beyond the translation pairs observable in training
data (?2). As a case study, we consider the prob-
lem of producing English definite/indefinite arti-
cles (the, a, and an) when translating from Russian
and Czech, two languages that lack overt definite-
ness morphemes (?3). We develop a classifier that
predicts the presence and absence of English arti-
cles (?4). This classifier is used to generate syn-
thetic translation options that are used to augment
phrase tables used the usual way (?5). We eval-
uate their performance relative to post-processing
approach and to a baseline phrase-based system,
finding that synthetic translation options reliably
outperform the other approaches (?6). We then
discuss how our approach relates to previous work
(?7) and conclude by discussing further applica-
tions of our technique (?8).
2 Why Synthetic Translation Options?
Before turning to the problem of generating En-
glish articles, we give arguments for why syn-
thetic translation options are a useful extension of
271
standard phrase-based translation approaches, and
why this technique might be better than some al-
ternative proposals that been made for generaliz-
ing beyond translation examples directly observ-
able in the training data.
In language pairs that are typologically sim-
ilar (i.e., when both languages lexicalize the
same kinds of semantic and syntactic informa-
tion), words and phrases map relatively directly
from source to target languages, and the standard
approach to learning phrase pairs is quite effec-
tive.1 However, in language pairs in which in-
dividual source language words have many dif-
ferent possible translations (e.g., when the target
language word could have many different inflec-
tions or could be surrounded by different func-
tion words that have no direct correspondence in
the source language), we can expect the standard
phrasal inventory to be incomplete, except when
very large quantities of parallel data are available
or for very frequent words. There simply will not
be enough examples from which to learn the ideal
set of translation options. Therefore, since phrase
based translation can only generate input/output
word pairs that were directly observed in the train-
ing corpus, the decoder?s only hope for produc-
ing a good output is to find a fluent, meaning-
preserving translation using incomplete transla-
tion lexicons. Synthetic translation option genera-
tion seeks to fill these gaps using secondary gener-
ation processes that produce possible phrase trans-
lation alternatives that are not directly extractable
from the training data. We hypothesize that by
filling in gaps in the translation options, discrim-
inative translation models will be more effective
(leading to better translation quality).
The creation of synthetic translation options can
be understood as a kind of translation or post-
editing of phrasal units/translations. This raises
a question: if we have the ability to post-edit a
phrasal translation or retranslate a source phrase
so as to fill in gaps in the phrasal inventory, we
should be able to use the same technique to trans-
late the sentence; why not do this? While the ef-
fectiveness of this approach will ultimately be as-
sessed empirically, translation option generation is
appealing because the translation option synthe-
sizer need not produce only single-best guesses?
1When translating from a language with a richer lexical
inventory to a simpler one, approximate matching or backing
off to (e.g.) morphologically simpler forms likewise reliably
produces good translations.
??????
?
?????
saw +1SG +PST cat+ACC1SG+NOM
I
saw
saw a
saw the
cat
a
the cat
cat
saw the cat
saw a cat
I saw
I saw a
I saw the
Figure 1: Russian-English phrase-based transla-
tion example. Since Russian lacks a definiteness
morpheme the determiners a, the must be part of
a translation option containing ?????? or ?????
in order to be present in the right place in the En-
glish output. Translation options that are in dashed
boxes should exist but were not observed in the
training data. This work seeks to produce such
missing translation options synthetically.
if multiple possibilities appear to be equally good
(say, multiple inflections of a translated lemma),
then multiple translation options may be synthe-
sized. Ultimately, of course, the global translation
model must select one translation for every phrase
it uses, but the decoder will have access to global
information that it can use to pick better transla-
tion options.
3 Case Study: English Definite Articles
We now turn to a translation problem that we will
use to assess the value of synthetic translation op-
tions: generating English in/definite articles when
translating from Russian.
Definiteness is a semantic property of noun
phrases that expresses information such as iden-
tifiability, specificity, familiarity and unique-
ness (Lyons, 1999). In English, it is expressed
through the use of article determiners and non-
article determiners. Although languages may ex-
press definiteness through such morphemes, many
languages use alternative mechanisms. For exam-
ple they may use noncanonical word orders (Mo-
hanan, 1994)2 or different constructions such as
existentials, differential object marking (Aissen,
2003), and the ba (?) construction in Chinese
2See pp. 11?12 for an example in Hindi, a language with-
out articles.
272
(Chen, 2004). While these languages lack arti-
cles, they may use demonstratives and the quan-
tifier one to emphasize definiteness and indefinite-
ness, respectively.
Russian and Czech are examples of languages
that use non-lexical means to express definiteness.
As such, in Russian to English translation systems,
we expect that most Russian nouns should have at
least three translation options?the bare noun, the
noun preceded by the, and the noun preceded a/an.
Fig. 1 illustrates how the definiteness mismatch
between Russian and English can result in ?gaps?
in the phrasal inventory learned from a relatively
large parallel corpus. The Russian input should
translate (depending on context) as either I saw a
cat or I saw the cat; however, the phrase table we
learned is only able to generate the former.3
4 Predicting English Definite Articles
Although English articles express semantic con-
tent, their use is largely predictable in context,
both for native English speakers and for automated
systems (Knight and Chander, 1994).4 In this sec-
tion we describe a classifier that uses local contex-
tual features to predict whether an article belongs
in a particular position in a sequence of words, and
if so, whether it is definite or indefinite (the form
of the indefinite article is deterministic given the
pronunciation of the following word).
4.1 Model
The classifier takes an English word sequence w =
?w1, w2, . . . , w|w|?with missing articles and an in-
dex i and predicts whether no article, a definite ar-
ticle, or an indefinite article should appear before
wi. We parameterize the classifier as a multiclass
3The phrase table for this example was extracted from the
WMT 2013 shared task training data consisting of 1.2M sen-
tence pairs.
4An interesting contribution of this work is a discussion
on lower and upper bounds that can be achieved by native
English speakers in predicting determiners. 67% is a lower
bound, obtained by guessing the for every instance. The up-
per bound was obtained experimentally, and was measured on
noun phrases (NP) without context, in a context of 4 words
(2 before and 2 after NP), and given full context. Human
subjects achieved an accuracy of 94-96% given full context,
83-88% for NPs in a context of 4 words, and 79-80% for NPs
without context. Since in the current state-of-the-art building
an automated determiners prediction in a full context (repre-
senting meaning computationally) is not a feasible task, we
view 83-88% accuracy as our goal, and 88% as an upper
bound for our method.
logistic regression:
p(y | w, i) ? exp?
j
?jhj(y,w, i),
where hj(?) are feature functions, ?j are the corre-
sponding weights, and y ? {D,I,N} refer, respec-
tively, to the outputs: definite article, indefinite ar-
ticle, and no article.5
4.2 Features
The English article system is extremely com-
plex (as non-native English speakers will surely
know!): in addition to a general placement rule
that articles must precede a noun or its modifiers
in an NP, multiple other factors can also affect ar-
ticle selection, including countability of the head
noun, syntactic properties of an adjective modi-
fying a noun (superlative, ordinal), discourse fac-
tors, general knowledge, etc. In this section, we
define morphosyntactic features aimed at reflect-
ing basic grammatical rules, we define statistical,
semantic and shallow lexical features to capture
additional regular and idiosyncratic usages of def-
inite and indefinite articles in English. Below we
provide brief details of the features and their mo-
tivation.
Lexical. Because training data can be con-
structed inexpensively (from any unannotated En-
glish corpus), n-gram indicator features, such as
[[wi?1ywiwi+1 = with y lot of]], can be es-
timated reliably and capture construction-specific
article use.
Morphosyntactic. We used part-of-speech
(POS) tags produced by the Stanford POS tagger
(Toutanova and Manning, 2000) to capture gen-
eral article patterns. These are relevant features
in the prediction of articles as we observe certain
constraints regarding the use of articles in the
neighborhood of certain POS tags. For example,
we do not expect to predict an article following an
adjective (JJ).
Semantic. We extract further information indi-
cating whether a named entity, as identified by the
Stanford NE Recognizer (Finkel et al, 2005) be-
gins at wi. These features are relevant as there
5Realization of the classes D and N as lexical items is
straightforward. To convert I into a or an, we use the
CMU pronouncing dictionary (http://www.speech.
cs.cmu.edu/cgi-bin/cmudict) and select an if wi
starts with a phonetic vowel.
273
is, in general, a constraint on the co-occurrence
of articles with named entities which can help us
predict the use of articles in such constructions.
For example, proper nouns do not tend to co-
occur with articles in English. Although there are
some proper nouns that have an article included in
them, such as the Netherlands, the United States
of America, but these are fixed expressions and the
model is easily able to capture such cases with lex-
ical features.
Statistical. Statistical features capture probabil-
ity of co-occurrences of a sample with each of
the determiner classes, e.g., for wi?1ywi we
collect probabilities of wi?1Iwi, wi?1Dwi, and
wi?1Nwi.6
4.3 Training and evaluation
We employ the creg regression modeling frame-
work to train a ternary logistic regression classi-
fier.7 All features were computed for the target-
side of the Russian-English TED corpus (Cettolo
et al, 2012); from 117,527 sentences we removed
5K sentences used as tuning and test sets in the
MT system. We extract statistical features from
monolingual English corpora released for WMT-
11 (Callison-Burch et al, 2011).
In the training corpus there are 65,075 I in-
stances, 114,571 D instances, and 2,435,287 N in-
stances. To create a balanced training set we
randomly sample 65K instances from each set of
collected instances.8 This training set of feature
vectors has 142,604 features and 285,210 param-
eters. To minimize the number of free parame-
ters in our model we use `1 regularization. We
perform 10-fold cross validation experiments with
various feature combinations, evaluating the clas-
sifier accuracy for all classes and for each class
independently. The performance of the classifier
on individual classes and consolidated results for
all classes are listed in Table 1.
We observe that morphosyntactic and lexical
features are highly significant, reducing the er-
ror rate of statistical features by 25%. A combi-
6Although statistical features are de rigeur in NLP, they
are arguably justified for this problem on linguistic grounds
since human subjects use frequency-based in addition to their
grammatical knowledge. For example, we say He is at school
rather than He is at the school, but Americans say He is in
the hospital while UK English speakers might prefer He is in
hospital.
7https://github.com/redpony/creg
8Preliminary experiments indicated that the excess of N
labels resulted in poor performance.
Feature combination All I D N
Statistical 0.80 0.76 0.79 0.87
Lexical 0.82 0.79 0.80 0.87
Morphosyntactic 0.75 0.71 0.64 0.86
Semantic 0.35 0.99 0.02 0.04
Statistical+Lexical 0.85 0.83 0.82 0.89
+ Morphosyntactic 0.87 0.86 0.83 0.92
+ Semantic 0.87 0.86 0.83 0.92
Table 1: 10-fold cross validation accuracy of the
classifier over all and by class.
nation of morphosyntactic, lexical, and statistical
features is also helpful, reducing 13% more errors.
Semantic features do not contribute to the classi-
fier accuracy (we believe, mainly due to the feature
sparsity).
5 Experimental Setup
Our experimental workflow includes the follow-
ing steps. First, we select a phrase table PTsource
from which we generate synthetic phrases. For
each phrase pair ?f, e? in PTsource we generate n
synthetic variants of the target side phrase e which
we then append to PTbaseline. We annotate both
the original and synthetic phrases with additional
translation features in PTbaseline.
For this language pair, we have several options
for how to construct PTsource. The most straight-
forward way is to extract the phrasal inventory as
usual; a second option is to extract phrases from
training data from which definite articles have
been removed (since we will rely on the classifier
to reinsert them where they belong).
To synthesize phrases, we employ two differ-
ent techniques: LM-based and classifier-based.
We use a LM for one- or two-word phrases or
an auxiliary classifier for longer phrases and cre-
ate a new phrase in which we insert, remove or
substitute an article between each adjacent pair of
words in the original phrase. Such distinction be-
tween short and longer phrases has clear motiva-
tion: phrases without context may allow alterna-
tive, equally plausible options for article selection,
therefore we can just rely on a LM, trained on
large monolingual corpora, to identify phrases un-
observed in MT training corpus. Longer context
restricts determiners usage and statistical model
decisions are less prone to generating ungrammat-
ical synthetic phrases.
LM-based method is applied to phrases shorter
than three words. These phrases are numerous,
roughly 20% of a phrase table, and extracted from
274
many sites in the training data. For each short (tar-
get) phrase we add all possible alternative entries
observed in the LM and not observed in the orig-
inal translation model. For example, for a short
target phrase a cat we extract the cat.
We apply an auxiliary classifier to longer
phrases, containing three or more words. Based
on the classifier prediction, we use the maximally
probable class to insert, remove or substitute an
article between each adjacent pair of words in
the original phrase. Synthetic phrases are gener-
ated by linguistically-informed features and can
introduce alternative grammatically-correct trans-
lations of source phrases by adding or removing
existing articles (since the English article selection
in a local context is often ambiguous and not cat-
egorical). We add a synthetic phrase only if the
phrase pair not observed in the original model.
We compare two possible applications of a clas-
sifier: one-pass and iterative prediction. With
one-pass prediction we decide on the prediction
for each position independently of other deci-
sions. With iterative update we adopt the best
first (greedy) strategy, selecting in each iteration
the update-location in which the classifier obtains
highest confidence score. In each iteration we in-
corporate a prediction in a target phrase, and in the
next iteration the best first decision is made on an
updated phrase. Iterative prediction stops when no
updates are introduced.
Synthetic phrases are added to a phrase table
with the five standard phrasal translation features
that were found in the source phrase, and with sev-
eral new features. First, we add a boolean fea-
ture indicating the origin of a phrase: synthetic or
original. Second, we experiment with a posterior
probability of a classifier averaged over all loca-
tions where it could be extracted from the training
data. The next feature is derived from this score:
it is a boolean feature indicating a confidence of
the classifier: the feature value is 1 iff the average
classifier score is higher than some threshold.
Consider again a phrase I saw a cat discussed
in Section 1. Synthetic entry generation from the
original phrase table entry is illustrated in Fig-
ure 2.
6 Translation Results
We now review the results of experiments using
synthetic translation options in a machine trans-
lation system. We use the Moses toolkit (Koehn
et al, 2007) to train a baseline phrase-based SMT
system. Each configuration we compare has a dif-
ferent phrase table, with synthetic phrases gen-
erated with best-first or iterative strategies, from
a phrase table with- or without-determiners, with
variable number of translation features. To verify
that system improvement is consistent, and is not a
result of optimizer instability (Clark et al, 2011),
we replicate each experimental setup three times,
and then estimate the translation quality of the me-
dian MT system using the MultEval toolkit.9
The corpus is the same as in Section 4.3:
the training part contains 112,527 sentences from
Russian-English TED corpus, randomly sampled
3K sentences are used for tuning and a disjoint set
of 2K sentences is used for test. We lowercase
both sides, and use Stanford CoreNLP10 tools to
tokenize the corpora. We employ SRILM toolkit
(Stolcke, 2002) to linearly interpolate the target
side of the training corpus with the WMT En-
glish corpus, optimizing towards the MT tuning
set. This LM is used in all experiments.
The rest of this section is organized as follows.
First, we compare two approaches to the deter-
miners classifier application. Then, we provide
detailed description of experiments with synthetic
phrases. We evaluate various aspects of synthetic
phrases generation and summarize all the results
in Table 3. In Table 5 we show examples of im-
proved translations.
Classifier application: one-pass vs. iterative.
First, as an intrinsic evaluation of the prediction
strategy we remove definite and indefinite articles
from the reference translations (2K test sentences)
and then employ the determiners classifier to re-
produce the original sentences. In Table 2 we re-
port on the word error rate (WER) derived from
the Levenshtein distance between the original sen-
tences and the sentences (1) without articles, (2)
with articles recovered using one-pass prediction,
and (3) articles recovered using iterative predic-
tion. The WER is averaged over all test sentences.
Both one-pass and iterative approaches are effec-
tive in the task of determiners prediction, reducing
the number of errors by 44%. The iterative ap-
proach yields slightly lower WER, hence we em-
ploy the iterative prediction in the future experi-
ments with synthetic phrases.
9https://github.com/jhclark/multeval
10http://nlp.stanford.edu/software/corenlp.shtml
275
the
? ?????? ????? ||| i saw the cat ||| f0 f1 f2 f3 f4 exp(1) exp(0) |||
<s>      I      saw   a   cat </s>
None
None
? ?????? ????? ||| i saw a cat ||| f0 f1 f2 f3 f4 exp(0) exp(0) |||
original phrase
post-processing
synthetic phrase
is synthetic
is no-context
Figure 2: Synthetic entry generation example. The original parallel phrase has two additional boolean
features (set to false) indicating that this is not a synthetic phrase and not a short phrase. We apply
our determiners classifier to predict an article at each location marked with a dashed box. Based on a
classifier prediction we derive a new phrase I saw the cat. Since corresponding parallel entry is not in
the original phrase table, we set the synthetic indicator feature to 1.
Post-processing WER
None 5.6%
One-pass 3.2%
Iterative 3.1%
Table 2: WER (lower is better) of reference trans-
lations without articles and of post-processed ref-
erence translations. Both one-pass and iterative
approaches are effective in the task of determin-
ers prediction.
MT output post-processing. We then evaluate
the post-processing strategy directly on the MT
output. We experiment with one-pass and itera-
tive post-processing of two variants of the base-
line system outputs: original output and the out-
put without articles (we remove the articles prior
to post-processing). The results are listed in Ta-
ble 3. Interestingly, we do not obtain any improve-
ments applying the determiners classifier in a con-
ventional way of a MT output post-processing. It
is the combination of linguistically-motivated fea-
tures with synthetic phrases that contribute to the
best performance.
LM-based synthetic phrases. As discussed
above, LM-based (short) phrases are shorter than
3 tokens and their synthetic variants contain same
words with articles inserted or deleted between
each adjacent pair of words. The phrase table
of the baseline system contains 2,441,678 phrase
pairs. There are 518,453 original short phrases,
and our technique yields 842,252 new synthetic
entries which we append to the baseline phrase ta-
ble. Table 3 shows the evaluation of the median
SMT system (derived from three systems) with
short phrases. In these systems the five phrasal
translation features are the same as in the base-
line systems. Improvement in the BLEU score
(Papineni et al, 2002) is statistically significant
(p < .05), compared to the baseline system
Classifier-generated synthetic phrases We ap-
ply classifier with the iterative prediction directly
on the baseline phrase table entries and synthe-
size 944,145 new parallel phrases, increasing the
phrase table size by 38%. The phrasal transla-
tion features in each synthetic phrase are the same
as in the phrase it was derived from. The BLEU
score of the median SMT system with synthetic
phrases is 22.9 ? .1, the improvement is statisti-
cally significant (p < .01). Post-processing of a
phrase table created from corpora without articles
and adding synthetic phrases to the baseline phrase
table yielded similar results.
Translation features for synthetic phrases In
the following experiments we aim to establish the
optimal set of translation features that should be
used with synthetic phrases. We train several SMT
systems, each containing synthetic phrases derived
from the original phrase table by iterative classifi-
cation, and with LM-based short phrases. Each
synthetic phrase has five translation features as an
original phrase it was derived from. The additional
features that we evaluate are:
1. Boolean feature for LM-based synthetic
phrases
276
MT System BLEU
Baseline 22.6? .1
MT output post-processing
one-pass, MT output with articles 20.8
one-pass, MT output without articles 19.7
iterative, MT output with articles 22.6
iterative, MT output without articles 21.8
With synthetic phrases
LM-based phrases 22.9? .1
+ classifier-generated phrases 22.9? .1
+ features 1,2 23.0 ? .1
+ features 1,2,3 22.8? .1
+ features 1,2,3,4 22.8? .1
+ feature 5 22.9? .1
Table 3: Summary of experiments with MT out-
put post-processing and with synthetic translation
options in a phrase table. Post-processing of the
MT output do not improve translations. Best per-
forming system with synthetic phrases has five
original phrase translation features and two addi-
tional boolean features indicating if the phrase is
LM-based or not, is classifier-generated or not. All
the synthetic systems are significantly better than
the baseline system.
2. Boolean feature for classifier-generated syn-
thetic phrases
3. Classifier confidence: posterior probability of
the classifier averaged over all samples in a tar-
get phrase.
4. Boolean feature indicating a confidence of the
classifier: the feature value is 1 iff the Fea-
ture 3 scores higher than some threshold. The
threshold was set to 0.8, we did not experiment
with other values.
5. Boolean feature for a synthetic phrase of any
type: LM-based or classifier-generated
Table 3 details the change in the BLEU score
of each experimental setup. The best perform-
ing system has five original phrase translation fea-
tures and two additional boolean features indicat-
ing if the phrase is LM-based or not, is classifier-
generated or not. Note that all the synthetic sys-
tems are significantly better than the baseline.
Czech-English. Our technique was developed
using Russian-English system in the TED domain,
so we want to see how our method generalizes to a
different domain when translating from a different
language. We therefore applied our most success-
ful configuration to a Czech-English news transla-
tion task.11 For training, we use the WMT Czech-
English parallel corpus CzEng0.7; we tune using
the WMT2011 test set and test on the WMT2012
test set. The LM is trained on the target side of the
training corpus. Determiners classifier, re-trained
on the English side of this corpus, with statistical,
lexical, morphosyntactic and dependency features
obtained an accuracy of 88%.
In Table 4, we report the results of evaluat-
ing the performance of the Russian-to-English
and Czech-to-English MT systems with synthetic
phrases. The results of both systems show a statis-
tically significant (p < .01) improvement in terms
of BLEU score.
Russian Czech
Baseline 22.6? .1 16.0? .05
Synthetic 23.0? .1 16.2? .03
Table 4: BLEU score of Russian-to-English
and Czech-to-English MT systems with synthetic
phrases and features 1 and 2 show a significant im-
provement.
Qualitative analysis. Table 5 shows some ex-
amples from the output of our Russian-to-English
systems. Although both systems produce compre-
hensible translations, the system augmented with
determiner classifier is more fluent. The first ex-
ample represents a case where a singular count
noun (piece) is present which requires an article.
The baseline is not able to identify this require-
ment and hence does not insert the article an be-
fore the phrase extraordinary engineering piece.
Our system, however, correctly identifies the con-
struction requiring an article and thus provides an
appropriate form of the article (an- Indefinite arti-
cle for lexical items beginning with a vowel). Thus
we see that our system is able to capture the lin-
guistic requirement of the singular count nouns to
co-occur with an article. In the second row, the
lexical item poor is used as an adjective. The base-
line has inserted an article in front of it, chang-
ing it to a noun. Our system, however, is able to
maintain the status of poor as an adjective since
it has the option not to insert an article. Thus we
see that besides fluency, our system also does bet-
ter in maintaining the grammatical category of a
lexical item. In the third row, the phrase three
11Like Russian, Czech is a Slavic language that does not
have definite or indefinite articles.
277
Source: ?? ??? ?? ????? , ??? ?????????? ???????????? ??????????? ????????? .
Reference: but nonetheless , it ?s an extraordinary piece of engineering .
Baseline: but nevertheless , it ?s extraordinary engineering piece of art .
Ours: but nevertheless , it ?s an extraordinary piece of engineering art .
Source: ? ?? ?????? ?????????? ??? ??? ?? ?????? .
Reference: and by many definitions she is no longer poor .
Baseline: and in a lot definitions , it ?s not a poor .
Ours: and in a lot definitions she ?s not poor .
Source: ??? ????? ????????? ??? ????????? ????????? ??????? .
Reference: we must feed three billion people in cities .
Baseline: we need to feed the three billion urban hundreds of them .
Ours: we need to feed three billion people in the city .
Table 5: Examples of translations with improved articles handling.
billion people refers to a nonidentifiable referent.
The baseline inserts the definite article the. If a
human subject reads this translation, it would mis-
lead him/her to interpret the object three billion
people as referring to a specific identifiable set.
Our system, on the other hand, correctly selects
the determiner class N and hence does not insert an
article. Thus we see that our system does not just
add fluency but it also captures a semantic distinc-
tion, namely identifiability, that a human subject
makes when producing or interpreting a phrase.
7 Related Work
Automated determiner prediction has been found
beneficial in a variety of applications, including
postediting of MT output (Knight and Chander,
1994), text generation (Elhadad, 1993; Minnen
et al, 2000), and more recently identification and
correction of ESL errors (Han et al, 2006; De Fe-
lice and Pulman, 2008; Gamon et al, 2009; Ro-
zovskaya and Roth, 2010). Our work on determin-
ers extends previous studies in several dimensions.
While all previous approaches were tested only on
NP constructions, we evaluate our classifier on any
sequence of tokens.
To the best of our knowledge, the only stud-
ies that directly address generation of synthetic
phrase table entries was conducted by Chen et al
(2011) and Koehn and Hoang (2007). The former
find semantically similar source phrases and pro-
duce ?fabricated? translations by combining these
source phrases with a set of their target phrases;
however, they do not observe improvements. The
later work integrates the synthesis of translation
options into the decoder. While related in spirit,
their method only supports a limited set of gen-
erative processes for producing the candidate set
(lacking, for instance, the simple and effective
phrase post-editing process we have used), and
their implementation has been plagued by compu-
tational challenges.
Post-processing techniques have been ex-
tremely popular. These can be understood as using
a translation model to generate a translation skele-
ton (or k-best skeletons) and then post-editing
these in various ways. These have been applied
to translation into morphologically rich languages,
such as Japanese, German, Turkish, and Finnish
(de Gispert et al, 2005; Suzuki and Toutanova,
2006; Suzuki and Toutanova, 2007; Fraser et al,
2012; Clifton and Sarkar, 2011; Oflazer and Dur-
gar El-Kahlout, 2007).
8 Conclusions and future work
The contribution of this work is twofold. First, we
propose a new supervised method to predict defi-
nite and indefinite articles. Our log-linear model
trained on a linguistically-motivated set of fea-
tures outperforms previously reported results, and
obtains an upper bound of an accuracy achieved
by human subjects given a context of four words.
However, more important result of this work is the
experimentally verified idea of improving phrase-
based SMT via synthetic phrases. While we have
focused on a limited problem in this paper, there
are numerous alternative applications including
translation into morphologically rich languages, as
a method for incorporating (source) contextual in-
formation in making local translation decisions,
enriching the target language lexicon using lexical
translation resources, and many others.
Acknowledgments
We are grateful to Shuly Wintner for insightful sugges-
tions and support. This work was supported in part by the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-1-
0533.
278
References
J. Aissen. 2003. Differential object marking: Iconic-
ity vs. economy. Natural Language and Linguistic
Theory, 21(3):435?483.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
22?64, Edinburgh, Scotland, July. Association for
Computational Linguistics.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web inventory of transcribed and translated talks. In
Proceedings of the 16th Conference of the European
Association for Machine Translation (EAMT), pages
261?268, Trento, Italy, May.
B. Chen, R. Kuhn, and G. Foster. 2011. Semantic
smoothing and fabrication of phrase pairs for SMT.
In Proceedings of the International Workshop on
Spoken Lanuage Translation (IWSLT-2011).
P. Chen. 2004. Identifiability and definiteness in chi-
nese. Linguistics, 42(6):1129?1184.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proceedings of
HLT-NAACL 2012, volume 12, pages 34?35.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. The Jour-
nal of Machine Learning Research, 98888:1159?
1187.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In In Proc. of ACL.
A. Clifton and A. Sarkar. 2011. Combining
morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
ACL.
R. De Felice and S. G. Pulman. 2008. A classifier-
based approach to preposition and determiner error
correction in L2 English. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 169?176. Association
for Computational Linguistics.
A. de Gispert, J. B. Marin?o, and J. M. Crego. 2005.
Improving statistical machine translation by classi-
fying and generalizing inflected verb forms. In Pro-
ceedings of InterSpeech.
J. DeNero and D. Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1453?
1463. Association for Computational Linguistics.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008.
Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 314?323. Association for Com-
putational Linguistics.
V. Eidelman, Y. Marton, and P. Resnik. 2013. Online
relative margin maximization for statistical machine
translation. In Proceedings of ACL.
M. Elhadad. 1993. Generating argumentative judg-
ment determiners. In AAAI, pages 344?349.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 363?
370, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
A. Fraser, M. Weller, A. Cahill, and F. Cap. 2012.
Modeling inflection and word-formation in SMT. In
Proceedings of EACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic transla-
tion models. In ACL-44: Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 961?968,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2009. Us-
ing contextual speller techniques and language mod-
eling for ESL error correction. Urbana, 51:61801.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Pro-
ceedings of 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies HLT-
NAACL 2012, Montreal, Canada.
N.-R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers.
K. Knight and I. Chander. 1994. Automated poste-
diting of documents. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
779?779, Seattle, WA.
P. Koehn and H. Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
279
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54. Association for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
C. Lyons. 1999. Definiteness. Cambridge University
Press.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based learning for article generation. In
Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Compu-
tational natural language learning-Volume 7, pages
43?48. Association for Computational Linguistics.
T. Mohanan. 1994. Argument Structure in Hindi.
CSLI Publications.
K. Oflazer and I. Durgar El-Kahlout. 2007. Explor-
ing different representational units in English-to-
Turkish statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 25?32, Prague, Czech Republic,
June. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
A. Rozovskaya and D. Roth. 2010. Training
paradigms for correcting errors in grammar and us-
age. Urbana, 51:61801.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Procedings of International
Conference on Spoken Language Processing, pages
901?904.
H. Suzuki and K. Toutanova. 2006. Learning to pre-
dict case markers in Japanese. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 1049?
1056. Association for Computational Linguistics.
H. Suzuki and K. Toutanova. 2007. Generating case
markers in machine translation. In Proceedings of
HLT-NAACL 2007, pages 49?56.
K. Toutanova and C. D. Manning. 2000. Enriching
the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the 2000
Joint SIGDAT conference on Empirical methods in
natural language processing and very large corpora,
pages 63?70, Morristown, NJ, USA. Association for
Computational Linguistics.
280
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Introducing Computational Concepts in a Linguistics Olympiad
Patrick Littell
Department of Linguistics
University of British Columbia
Vancouver, BC V6T1Z4, Canada
littell@interchange.ubc.ca
Lori Levin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
lsl@cs.cmu.edu
Jason Eisner
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Dragomir R. Radev
Department of EECS
School of Information
and Department of Linguistics
University of Michigan
radev@umich.edu
Abstract
Linguistics olympiads, now offered
in more than 20 countries, provide
secondary-school students a compelling
introduction to an unfamiliar field. The
North American Computational Lin-
guistics Olympiad (NACLO) includes
computational puzzles in addition to
purely linguistic ones. This paper ex-
plores the computational subject matter
we seek to convey via NACLO, as well
as some of the challenges that arise
when adapting problems in computational
linguistics to an audience that may have
no background in computer science,
linguistics, or advanced mathematics.
We present a small library of reusable
design patterns that have proven useful
when composing puzzles appropriate for
secondary-school students.
1 What is a Linguistics Olympiad?
A linguistics olympiad (LO) (Payne and Derzhan-
ski, 2010) is a puzzle contest for secondary-school
students in which contestants compete to solve
self-contained linguistics problem sets. LOs have
their origin in the Moscow Traditional Olympiad
in Linguistics, established in 1965, and have since
spread around the world; an international contest
(http://www.ioling.org) has been held
yearly since 2003.
In an LO, every problem set is self-contained,
so no prior experience in linguistics is necessary
to compete. In fact, LO contests are fun and re-
warding for exactly this reason: by the end of the
contest, contestants are managing to read hiero-
glyphics, conjugate verbs in Swahili, and perform
other amazing feats. Furthermore, they have ac-
complished this solely through their own analyti-
cal abilities and linguistic intuition.
Based on our experience going into high
schools and presenting our material, this ?linguis-
tic? way of thinking about languages almost al-
ways comes as a novel surprise to students. They
largely think about languages as collections of
known facts that you learn in classes and from
books, not something that you can dive into and
figure out for yourself. This is a hands-on antidote
to the common public misconception that linguists
are fundamentally polyglots, rather than language
scientists, and students come out of the experience
having realized that linguistics is a very different
field (and hopefully a more compelling one) than
they had assumed it to be.
2 Computational Linguistics at the LO
Our goal, since starting the North American
Computational Linguistics Olympiad (NACLO) in
2007 (Radev et al, 2008), has been to explore how
this LO experience can be used to introduce stu-
dents to computational linguistics. Topics in com-
putational linguistics have been featured before in
LOs, occasionally in the Moscow LO and with
some regularity in the Bulgarian LO.
Our deliberations began with some trou-
bling statistics regarding enrollments in computer
science programs (Zweben, 2013). Between
2003 and 2007 enrollments in computer science
dropped dramatically. This was attributed in part
to the dip in the IT sector, but it also stemmed in
18
part from a perception problem in which teenagers
view computer science careers as mundane and
boring: ?I don?t want to be Dilbert,1 sitting in a
cubicle programming payroll software my whole
life.? This is an unrealistically narrow percep-
tion of the kinds of problems computer scientists
tackle, and NACLO began in part as a way to pub-
licize to teenagers that many interesting problems
can be approached using computational methods.
Although enrollments are not yet back to the
2003 levels, there has been a sharp increase since
2007 (Zweben, 2013). The resurgence can be at-
tributed in part to the strength of the IT sector, but
also to the realization that computer science is rel-
evant to almost every area of science and technol-
ogy (Thibodeau, 2013). NACLO aims to be part
of this trend by showing students that computer
science is used in studying fascinating problems
related to human language.
Even ?traditional? LO puzzles are inherently
computational in that they require pattern recog-
nition, abstraction, generalization, and establish-
ing and pruning a solution space. However, we
also want to teach computational linguistics more
explicitly. NACLO puzzles have featured a wide
variety of topics in computational linguistics and
computer science; they may focus on the applica-
tion itself, or on concepts, tools, and algorithms
that underlie the applications. Broadly, computa-
tional LO topics fall into three types, summarized
below.
2.1 Technological applications
NACLO has included puzzles on technologies that
most people are familiar with, including spell
checking, information retrieval, machine transla-
tion, document summarization, and dialogue sys-
tems. In a typical applications puzzle, the contes-
tants would discover how the application works,
how it handles difficult cases, or what its limita-
tions are. In ?Summer Eyes? (Radev and Hester-
berg, 2009), the contestant discovers the features
that are used for selecting sentences in a sum-
marization program, including the position of a
sentence in the article, the number of words the
sentence shares with the title, etc. In ?Spring-
ing Up Baby? (Srivastava and Bender, 2008) and
?Running on MT? (Somers, 2011), contestants ex-
plore word sense disambiguation in the context of
1An engineer in the eponymous American comic strip,
Dilbert has a famously dysfunctional workplace and unre-
warding job.
machine translation, while ?Tiger Tale? (Radev,
2011) highlights some realistic sources of knowl-
edge for machine translation such as cognates
and cross-language syntactic similarities. ?Thorny
Stems? (Breck, 2008) and ?A fox among the h?
(Littell, 2012b) introduce stemming.
2.2 Formal grammars and algorithms
Some puzzles introduce the formal tools of com-
putational linguistics and linguistic concepts that
are important in computational linguistics, of-
ten in a whimsical way. For example, ?Sk8
Parsr? (Littell, 2009) introduces shift-reduce pars-
ing by means of a hypothetical skateboarding
video game. ?Aw-TOM-uh-tuh? (Littell, 2008)
introduces a finite-state machine that determines
which strings form legal words in the Rotokas
language. ?Orwellspeak? (Eisner, 2009) asks
solvers to modify a simple context-free grammar,
and then to discover that a 4-gram model can-
not model this language without precision or re-
call errors. ?Twodee? (Eisner, 2012) invents a
two-dimensional writing system, shown below, as
a vehicle for helping students discover parsing
ambiguity?and production ambiguity?without
the full formal apparatus of grammars, nontermi-
nals, or tree notation.
?The Little Engine That Could. . . Read? (Littell
and Pustejovsky, 2012) explores quantifier mono-
tonicity, while ?Grice?s Grifter Gadgets? (Boyd-
Graber, 2013) covers Grice?s maxims as part of the
specification of a computerized game assistant.
2.3 Computational concepts
NACLO puzzles have also introduced computa-
tional concepts that go beyond computational lin-
guistics. ?Texting, Texting, One Two Three? (Lit-
tell, 2010b) and ?The Heads and Tails of Huff-
man? (DeNero, 2013) introduce data compression.
?One, Two, Tree? (Smith et al, 2012) introduces
the Catalan numbers and other recurrences via bi-
nary bracketing of ambiguous compound nouns.
19
?Nok-nok? (Fink, 2009) introduces Levenshtein
distance by describing a hypothetical typing tutor
for very bad spellers.
3 The Challenge of Writing
Computational Problems
To achieve our goals, it becomes necessary to
write computational linguistics puzzles in such a
way that they are self-contained, requiring no prior
experience in linguistics, computer science, or ad-
vanced math. This has proven very difficult, but
not impossible, and in the past seven years we have
managed to learn a lot about how to (and how not
to) write them.
Perhaps the hardest part of writing any LO puz-
zle is that authors have to remove themselves from
their knowledge and experience: to forget techni-
cal definitions of ?phrase? or ?noun? or ?string? or
?function,? and to forget the facts and insights and
history that formed our modern understanding of
these. This is doubly hard when it comes to puz-
zles involving computational methods. The ability
to write an algorithm that a computer could actu-
ally interpret is a specialized skill that we learned
through education, and it is very, very hard to back
up and imagine what it would be like to not be
able to think like this. (It is almost like trying to
remember what it was like to not be able to read?
not simply not knowing a particular alphabet or
language, but not even understanding how reading
would work.)
Here is an illustration of an interesting but
nonetheless inappropriate LO puzzle:
Here are fourteen English compound
words:
birdhouse housework
blackbird tablespoon
blackboard teacup
boardroom teaspoon
boathouse workhouse
cupboard workroom
houseboat worktable
Even if you didn?t know any English, you
could probably determine by looking at
this list which English words were used
to make up the compounds: ?black?,
?bird?, ?board?, etc...
How would you do this if you were a
computer?
This task, although potentially appropriate for a
programming competition, is inappropriate for an
LO: the intended task requires some prior knowl-
edge about what computers can and cannot do.
Note that nowhere in the puzzle itself are the prop-
erties of this imaginary computer specified. It is
assumed that the solver knows roughly the state of
modern computing machinery and what kinds of
instructions it can execute.
Imagine for a moment what a right answer to
this puzzle would look like, and then picture what
a wrong answer might look like. Your right answer
was probably an algorithm that could run on an ab-
stract computer with capabilities very much like
real computers. The wrong answer probably made
incorrect assumptions about what sorts of opera-
tions computers are capable of, or treated enor-
mously complex operations as if they were primi-
tive.2
The problem with the above puzzle is that it is
very open-ended, and in the absence of a large
body of shared knowledge between the author and
the solver, the solver cannot know what it is the
author wants or when they have solved it to the
author?s satisfaction.
In order to avoid this, it is best to set up the puz-
zle so that the ?search space? for possible answers
is relatively constrained, and the ?win? conditions
are clear. Ideally, if a contestant has solved a puz-
zle, they should know they have solved it, and thus
be able to move on confidently to the next puz-
zle.3 In this respect, LO puzzles are akin to cross-
word puzzles, problems from other Olympiads, or
online puzzle games. This feeling of accomplish-
ment is key to the kind of rewarding learning ex-
perience that have made LOs so successful.
4 Design Patterns for CL Puzzles
Over the years, we have found several reliable
strategies for turning ideas and topics from com-
putational linguistics into solvable, rewarding puz-
2Keep in mind that today?s contestants were born in the
late 1990s. They are unlikely to even remember a world with-
out ubiquitous Internet and powerful natural language search.
Their conception of ?what computers basically do? is not nec-
essarily going to be the same as those of us who encountered
computers when they were still recognizable as a kind of so-
phisticated calculator.
3This is not to say, however, that only those who solve a
puzzle in its entirety should feel accomplished or rewarded.
The best puzzles often contain layers of mysteries: it may be
that only a few will solve every mystery in the puzzle, but
most contestants come away with the satisfaction of having
discovered something.
20
zles.
Not every computational puzzle makes use of
these?some are entirely unique?but many do.
In addition, these strategies are not mutually ex-
clusive; many computational puzzles utilize sev-
eral of these at once. For example, a ?Broken Ma-
chine? puzzle may then present the solver with a
?Troublemaker? task, or an ?Assembly Required?
machine may, upon assembly, turn out to be a
?Broken? one.
4.1 Assembly Required
The solver is presented with a task to complete,
and also a partially specified algorithm for doing
so. The partial specification illustrates the de-
sired formal notation and the model of computa-
tion. But it may be missing elements, or the or-
dering or relationship between the elements is un-
clear, or some other aspect of the system remains
unfinished. The solver is asked to complete the
system so that it performs the appropriate task or
produces the appropriate outputs.
For example, NACLO 2008 included a puzzle
on stemming, ?Thorny Stems? (Breck, 2008), in
which contestants help develop an algorithm to
isolate the stems of various words. In this puzzle,
the solver is not required to invent an algorithm
ex nihilo; this would merely have rewarded those
who already understand algorithms, not introduce
algorithmic thinking to neophytes. Instead, the
overall structure of the intended algorithm (an or-
dered sequence of if-thens) is made explicit, and
the solver?s task is to fill in the details:
Rule 1: If a word ends in , then
replace with to form the
stem.
Rule 2: If a word ends in , then
replace with to form the
stem.
In another puzzle from the same contest, ?Aw-
TOM-uh-tuh? (Littell, 2008), the solver must
complete an unfinished finite-state automaton so
that it performs a language recognition task. The
solver is given a brief introduction to FSAs and a
simple sample FSA, and then given an incomplete
FSA whose labels lack edges. The solver?s task is
to place the labels on the correct edges so that the
FSA accepts certain inputs and rejects others.
Other examples of the ?Assembly Required?
pattern can be found in the puzzles ?Sk8 Parsr?
(Littell, 2009), ?The Heads and Tails of Huff-
man? (DeNero, 2013), and ?BrokEnglish!? (Lit-
tell, 2011).
4.2 Black Box
The solver is presented with the inputs to a system
and the outputs, and must work out how the system
generated the outputs. Unlike in the ?Assembly
Required? pattern, little or no information about
the algorithm is provided to the solver; the solver?s
fundamental task is to characterize this unknown
algorithm as thoroughly as possible.
For example, NACLO 2010 featured a puzzle
on Huffman text compression, ?Texting, Texting,
One Two Three? (Littell, 2010b), in which an un-
specified algorithm converts strings of letters to
strings of numbers:
Testing testing = 33222143224142341-
1222143224142341331
Does anyone copy = 33233322143131-
42343324221124232342343331
Working out the basic number-letter correspon-
dences is relatively straightforward, but the real
puzzle is working out the rationale behind these
correspondences. Some of the answers require let-
ters (like ?r? and ?x?) that do not occur anywhere
in the data, but can be deduced once the system as
a whole is fully understood.
NACLO 2009 featured a puzzle on Levenshtein
distance, ?Nok-nok!? (Fink, 2009), that also
used this pattern. In it, a spell-checker is rat-
ing how well (or poorly) a user has spelled a word.
21
User Input Correct word Output
owll owl ?almost right?
ples please ?quite close?
reqird required ?quite close?
plez please ?a bit confusing?
mispeln misspelling ?very confusing?
The solver?s task is to work out the algorithm suf-
ficiently to predict how the system would respond
to novel inputs.
Other examples of the ?Black Box? pattern can
be found in ?The Deschamps Codice? (Piperski,
2012) and ?The Little Engine that Could. . . Read?
(Littell and Pustejovsky, 2012).
Depending on the intended algorithm, the
?Black Box? pattern may or may not be appro-
priate. This pattern works best when the nature
of the transformation between input and output is
relatively straightforward and the purpose of the
transformation is relatively clear. In the Huff-
man coding puzzle, for example, the nature of
the transformation is entirely obvious (replace let-
ters with number sequences) and thus the solution
space of the puzzle is relatively constrained (figure
out which letters correspond to which number se-
quences and then try to figure out why). In the
spell-checking puzzle, the purpose of the trans-
formation is easily understood, giving the solver
a head start on figuring out which features of the
input the algorithm might be considering.
When the nature of the transformation is less
obvious?for example, the generation of numbers
of unclear significance, rating some unknown as-
pect of a text passage??Black Box? is not as ap-
propriate as the other patterns. The potential prob-
lem is that not only must the solver come up with
an algorithm on their own, they must come up with
the same algorithm the author did. Given a com-
plicated algorithm, even small implementation de-
tails may lead to very different outputs, so a solver
can even have found a basically correct solution
but nevertheless not managed to produce the in-
tended outputs.
In such cases, the ?Assembly Required? or
?Broken Machine? patterns are potentially more
appropriate.
4.3 Broken Machine
The solver is presented with a system that purports
to perform a particular task, but actually fails on
particular inputs. The solver is tasked with fig-
uring out what went wrong and, potentially, fixing
the system so that it works. In some cases, the sys-
tem simply has an error in it; in others, the system
is correct but cannot handle certain difficult cases.
NACLO has featured a wide variety of broken
machines, often with humorous outputs. ?Help my
Camera!? (Bender, 2009) features a dialogue sys-
tem that could not correctly resolve pronoun refer-
ences:
Human: ?There?s this restaurant on
Bancroft that?s supposed to be really
good that I heard about from my mother.
Can you help me find it??
Computer: ?Where did you last see your
mother??
?BrokEnglish!? (Littell, 2011) features a run-
away script that replaced certain ISO 639-1 codes
with language names:
Hebrewy, ChamorRomanianrICHebre-
wcHebrewnlandic! whEnglish you
get a FrEnglishcHebrewe momEnglisht,
cHebrewck out thICHebrewcHebrewn-
landic niCHebrewcHebrewn little pRo-
maniangram i wRomaniante.
Solvers are then tasked with determining why
this script produced such a bizarre output, and ad-
ditionally tasked with determining in what order
the replacements had to have occurred in order to
get this exact output.
?Orwellspeak? (Eisner, 2009) involves a
context-free grammar that produces sentences
that were grammatically correct but counter to the
ideals of a fictional totalitarian Party. The solver
must rewrite the grammar so that only ?correct?
thoughts can be uttered. In the second part of the
puzzle, the solver must show that Markov models
would be inherently broken.
Other examples of ?Broken Machines? are ?The
Lost Tram? (Iomdin, 2007), ?Sk8 Parsr? (Lit-
tell, 2009), ?A fox among the h? (Littell, 2012b),
?The Little Engine that Could. . . Read? (Littell and
Pustejovsky, 2012), and ?Grice?s Grifter Gadgets?
(Boyd-Graber, 2013).
4.4 Troublemaker
The solver is presented with a system and some
sample inputs and outputs, and must discover an
input that causes the system to fail, or produce out-
puts that are strange, suboptimal, or have some un-
usual property.
22
Few puzzles make use of only the ?Trouble-
maker? pattern. Many are basically ?Assembly
Required? or ?Broken Machine? puzzles that use a
?Troublemaker? task to get the contestant thinking
about the ways in which the system is limited or
imperfect. They are also often creative?the con-
testant usually invents their own inputs?and thus
can serve as a refreshing change of pace.4
NACLO 2009 featured a ?Broken Machine?
puzzle about shift-reduce parsing (?Sk8 Parsr?)
(Littell, 2009), couched in terms of a fictional
skateboarding videogame. The solver is given an
algorithm by which button presses are transformed
into skateboard trick ?combos? like those shown
below, but many well-formed ?combos? cannot
correctly be parsed due to a shift-reduce conflict.
The solver is given an example of one such class
of inputs, and then asked to discover other classes
of inputs that likewise fail.
?Troublemaker? puzzles are not always
couched in terms of bugs. ?This problem is pretty
// easy? (Radev, 2007a) asks solvers to construct
eye-catching garden path sentences. In the
Huffman text compression puzzle detailed above
(?Texting, Texting, One Two Three?) (Littell,
2010b), a ?Troublemaker? task is introduced to
get contestants thinking about the limits of com-
pression. Although the compression algorithm
is not ?broken? in any way, any compression
algorithm will ?fail? on some possible input and
return an output longer than the input, and the
solver is tasked to discover such an input.
?Troublemaker? tasks can also be found in
?Grammar Rules? (Schalley and Littell, 2013) and
?Yesbot? (Mitkov and Littell, 2013).
4If the ?Troublemaker? task asks for an input with a par-
ticular formal property (i.e., a sentence generated or not gen-
erated from a particular grammar), automated grading scripts
can determine the correctness of the answer without human
intervention. This means that contestants can get a chance
to enter ?creative? answers even in large contests (like the
NACLO Open Round) that utilize automatic grading.
4.5 Jabberwock
Not all puzzle types revolve around abstract ma-
chines. Another recurring puzzle type, the ?Jab-
berwock?, involves asking the solver to puzzle out
the syntactic or semantic properties of unknown
words. Often these words are nonsense words, but
this puzzle type can also work on natural language
data. To perform this task, solvers often have to
use the same methods that a computer would.
?We are all molistic in a way? (Radev, 2007b)
asks solvers to infer the polarity of various non-
sense adjectives based on a series of sentences.5
The teacher is danty and cloovy.
Mary is blitty but cloovy.
Strungy and struffy, Diane was a plea-
sure to watch.
Even though weasy, John is strungy.
Carla is blitty but struffy.
The solver must work out from sentences such
as these whether words like ?danty? and ?weasy?
have positive or negative associations. In doing so,
the solver has essentially constructed and solved a
semi-supervised learning problem.
In ?Gelda?s House of Gelbelgarg? (Littell,
2010a), solvers are presented with a page of fab-
ricated restaurant reviews for an entirely fictional
cuisine:
?A hidden gem in Lower Uptown! Get
the fa?rsel-fo?rsel with gorse-weebel and
you?ll have a happy stomach for a week.
And top it off with a flebba of sweet-
bolger while you?re at it!?
5The list given here includes a subset of the examples used
in the real puzzle in 2007.
23
?I found the food confusing and disori-
enting. Where is this from? I randomly
ordered the fa?rsel-fo?rsel and had to send
them back!?
Using various grammatical cues (article and pro-
noun choice, ?less? vs. ?fewer?, etc.), solvers have
to sort the items into things most likely to be dis-
crete, countable objects, things most likely to be
liquids or masses, and things most likely to be con-
tainers or measures.
This type of puzzle often violates the common
LO restriction on using nonsense words and made-
up languages, but it is not always possible to base
this sort of puzzle on a completely unfamiliar lan-
guage. Many ?Jabberwock? puzzles involve infer-
ring syntactic or semantic information about un-
known words in an otherwise known language.
The two puzzles above therefore require contes-
tants to consult their own intuitions about English.
These puzzles would have been entirely different
(and prohibitively difficult) if the language had
been completely unfamiliar.
Other Jabberwock puzzles include ?Tiger Tale?
(Radev, 2011) and ?Cat and Mouse Story? (Littell,
2012a).
4.6 Combinatorial Problems
Some puzzles effectively force the solver to design
and run an algorithm, to get an answer that would
be too difficult to compute by brute force. Such
puzzles involve computational thinking. But since
the solver only has to give the output of the algo-
rithm, there is no need to agree on a type of com-
puting device or a notation for writing algorithms
down.
Such puzzles include combinatorial tasks that
involve the counting, maximization, or existence
of linguistic objects. They require mathematical
and algorithmic skills (just as in math or program-
ming competitions), and demonstrate how these
skills apply to linguistics or NLP.
Portions of ?One, Two, Tree? (Smith et
al., 2012) and ?Twodee? (Eisner, 2012) require
solvers to count all ways to parse a sentence, or
to count all sentences of a certain type. Because
the counts are large, the solver must find the pat-
tern, which involves writing down a closed-form
formula such as 2n or a more complex dynamic
programming recurrence.
5 Conclusions
Researchers and teachers from the ACL commu-
nity are invited to contact the NACLO organizing
committee at naclo14org@umich.edu6 with
their ideas for new puzzles or new types of puz-
zles. All of the past puzzles and solutions can
be browsed at http://www.naclo.cs.cmu.
edu/practice.html. In general, puzzles in
Round 1 each year should be easier and automat-
ically gradable. Puzzles in Round 2 permit more
involved questions and answers; this is a smaller
contest in which the top Round 1 scorers (usu-
ally, the top 10 percent) can qualify for the Inter-
national Linguistic Olympiad.
Thus far, NACLO?s computational puzzles have
reached at least 6,000 students at more than 150
testing sites7 in the U.S. and Canada, as well as at
least 10,000 students in the three other English-
language countries that share LO puzzles with
NACLO.
We observe that most computational puzzles do
not need obscure languages, staying on the contes-
tant?s home turf of English and technology. This
does not mean, however, that the computational
puzzles are purely formal and lack linguistic con-
tent. Some of them in fact probe subtle facts about
English (the introspective method in linguistics),
and some of them cover areas of linguistics that
are underserved by traditional LO puzzles. Tra-
ditional LO puzzles instead ask the solver to sort
out vocabulary and basic morphophonological or
orthographic patterns in a mystery language (the
fieldwork method in linguistics). Students who en-
joy ?top-down? thinking or who are deeply inter-
ested in ?how to do things with words? may prefer
the former kind of puzzle.
Competitions are popular in many North Amer-
ican high schools, perhaps in part as a way to im-
press college admissions officers. We have ex-
ploited this to give students a taste of our inter-
disciplinary field before they choose a college ma-
jor. Some students may be specifically attracted to
NACLO by the word ?computational? or the word
?linguistics,? or may be intrigued by their juxta-
position. Many NACLO participants reveal that
they had started to study linguistics on their own
before encountering NACLO, and have welcomed
6Or nacloXXorg@umich.edu, where XX is the last
two digits of the calendar year of the upcoming February.
7NACLO tests have been given at more than 100 high
schools and more than 50 university sites; the latter are open
to students from all local high schools.
24
NACLO as an outlet for their enthusiasm and a
place where they can interact with other students
who have the same interests.
NACLO?s past puzzles remain freely available
on the web for anyone who is interested. Two
volumes of NACLO-style puzzles (most of them
from real competitions), edited by program chair
Dragomir Radev, have recently been published by
Springer (Radev, 2013a; Radev, 2013b). Adult
hobbyists and home-schooled students may dis-
cover computational linguistics through encoun-
tering these puzzles. Avid LO contestants use
them to prepare for upcoming contests. Finally,
high school and college teachers can use them
as the basis of whole-class or small-group class-
room activities that expose students to computa-
tional thinking.
Acknowledgments
We would like to thank the National Science Foun-
dation for supporting NACLO through the fol-
lowing grants: IIS0633871, BCS1137828, and
IIS0838848. We also express our gratitude to NSF
program managers Tatiana Korelsky, Terry Lan-
gendoen, and Joan Maling for their effort in ini-
tiating and maintaining NACLO. The Linguistic
Society of America and the North American Chap-
ter of the Association for Computational Linguis-
tics provide ongoing support. Other sponsors, vol-
unteers, and problem writers are too numerous to
name. They are listed on the contest booklets each
year, which can be found on the NACLO web site:
http://www.naclo.cs.cmu.edu.
References
Emily Bender. 2009. Help my camera! In
North American Computational Linguistics
Olympiad 2009. http://www.naclo.cs.cmu.edu/
assets/problems/naclo09F.pdf.
Jordan Boyd-Graber. 2013. Grice?s grifter gad-
gets. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND2.pdf.
Eric Breck. 2008. Thorny stems. In North Amer-
ican Computational Linguistics Olympiad 2008.
http://www.naclo.cs.cmu.edu/assets/problems/
NACLO08h.pdf.
John DeNero. 2013. The heads and tails of Huff-
man. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND1.pdf.
Jason Eisner. 2009. Orwellspeak. In North Amer-
ican Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09M.pdf.
Jason Eisner. 2012. Twodee. In North
American Computational Linguistics Olympiad
2013. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Eugene Fink. 2009. Nok-nok! In North Ameri-
can Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09B.pdf.
Boris Iomdin. 2007. The lost tram. In North Amer-
ican Computational Linguistics Olympiad 2007.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo07 f.pdf.
Patrick Littell and James Pustejovsky. 2012.
The little engine that could. . . read. In North
American Computational Linguistics Olympiad
2012. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Patrick Littell. 2008. Aw-TOM-uh-tuh. In North
American Computational Linguistics Olympiad
2008. http://www.naclo.cs.cmu.edu/assets/
problems/NACLO08i.pdf.
Patrick Littell. 2009. Sk8 parsr. In North Ameri-
can Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09G.pdf.
Patrick Littell. 2010a. Gelda?s house of gelbel-
garg. In North American Computational Linguis-
tics Olympiad 2010. http://www.naclo.cs.cmu.edu/
problems2010/A.pdf.
Patrick Littell. 2010b. Texting, texting, one two
three. In North American Computational Linguis-
tics Olympiad 2010. http://www.naclo.cs.cmu.edu/
problems2010/E.pdf.
Patrick Littell. 2011. BrokEnglish! In North Amer-
ican Computational Linguistics Olympiad 2011.
http://www.naclo.cs.cmu.edu/problems2011/E.pdf.
Patrick Littell. 2012a. Cat and mouse story. In North
American Computational Linguistics Olympiad
2012. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND1.pdf.
Patrick Littell. 2012b. A fox among the
h. In North American Computational Linguis-
tics Olympiad 2012. http://www.naclo.cs.cmu.edu/
problems2012/NACLO2012ROUND2.pdf.
Ruslan Mitkov and Patrick Littell. 2013. Grammar
rules. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND2.pdf.
25
Thomas E. Payne and Ivan Derzhanski. 2010. The lin-
guistics olympiads: Academic competitions in lin-
guistics for secondary school students. In Kristin
Denham and Anne Lobeck, editors, Linguistics at
school. Cambridge University Press.
Alexander Piperski. 2012. The Deschamps
codice. In North American Computational Linguis-
tics Olympiad 2012. http://www.naclo.cs.cmu.edu/
problems2012/NACLO2012ROUND2.pdf.
Dragomir Radev and Adam Hesterberg. 2009. Sum-
mer eyes. In North American Computational Lin-
guistics Olympiad 2009. http://www.naclo.cs.cmu.
edu/assets/problems/naclo09E.pdf.
Dragomir R. Radev, Lori Levin, and Thomas E.
Payne. 2008. The North American Computa-
tional Linguistics Olympiad (NACLO). In Proceed-
ings of the Third Workshop on Issues in Teaching
Computational Linguistics, pages 87?96, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics. http://www.aclweb.org/anthology/W/W08/
W08-0211.
Dragomir Radev. 2007a. This problem is pretty //
easy. In North American Computational Linguis-
tics Olympiad 2007. http://www.naclo.cs.cmu.edu/
assets/problems/naclo07 h.pdf.
Dragomir Radev. 2007b. We are all molistic in a
way. In North American Computational Linguis-
tics Olympiad 2007. http://www.naclo.cs.cmu.edu/
assets/problems/naclo07 a.pdf.
Dragomir Radev. 2011. Tiger tale. In North American
Computational Linguistics Olympiad 2011. http://
www.naclo.cs.cmu.edu/problems2011/F.pdf.
Dragomir Radev, editor. 2013a. Puzzles in Logic,
Languages, and Computation: The Green Book.
Springer: Berlin.
Dragomir Radev, editor. 2013b. Puzzles in Logic, Lan-
guages, and Computation: The Red Book. Springer:
Berlin.
Andrea Schalley and Patrick Littell. 2013. Grammar
rules! In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND1.pdf.
Noah Smith, Kevin Gimpel, and Jason Eisner.
2012. One, two, tree. In North American
Computational Linguistics Olympiad 2012.
http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Harold Somers. 2011. Running on MT. In North
American Computational Linguistics Olympiad
2011. http://www.naclo.cs.cmu.edu/problems2011/
A.pdf.
Ankit Srivastava and Emily Bender. 2008. Springing
up baby. In North American Computational Lin-
guistics Olympiad 2008. http://www.naclo.cs.cmu.
edu/assets/problems/prob08b.pdf.
Patrick Thibodeau. 2013. Computer science en-
rollments soared last year, rising 30%, March.
http://www.computerworld.com/s/article/9237459/
Computer science enrollments soared last year
rising 30 .
Stuart Zweben. 2013. Computing degree and enroll-
ment trends, March. http://cra.org/govaffairs/blog/
wp-content/uploads/2013/03/CRA Taulbee CS
Degrees and Enrollment 2011-12.pdf.
26
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 80?86,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The CMU Submission for the Shared Task on Language Identification in
Code-Switched Data
Chu-Cheng Lin Waleed Ammar Lori Levin Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{chuchenl,wammar,lsl,cdyer}@cs.cmu.edu
Abstract
We describe the CMU submission for
the 2014 shared task on language iden-
tification in code-switched data. We
participated in all four language pairs:
Spanish?English, Mandarin?English,
Nepali?English, and Modern Standard
Arabic?Arabic dialects. After describing
our CRF-based baseline system, we
discuss three extensions for learning from
unlabeled data: semi-supervised learning,
word embeddings, and word lists.
1 Introduction
Code switching (CS) occurs when a multilingual
speaker uses more than one language in the same
conversation or discourse. Automatic idenefica-
tion of the points at which code switching occurs
is important for two reasons: (1) to help sociolin-
guists analyze the frequency, circumstances and
motivations related to code switching (Gumperz,
1982), and (2) to automatically determine which
language-specific NLP models to use for analyz-
ing segments of text or speech.
CS is pervasive in social media due to its in-
formal nature (Lui and Baldwin, 2014). The first
workshop on computational approaches to code
switching in EMNLP 2014 organized a shared task
(Solorio et al., 2014) on identifying code switch-
ing, providing training data of multilingual tweets
with token-level language-ID annotations. See
?2 for a detailed description of the shared task.
This short paper documents our submission in the
shared task.
We note that constructing a CS data set that is
annotated at the token level requires remarkable
manual effort. However, collecting raw tweets is
easy and fast. We propose leveraging both labeled
and unlabeled data in a unified framework; condi-
tional random field autoencoders (Ammar et al.,
2014). The CRF autoencoder framework consists
of an encoding model and a reconstruction model.
The encoding model is a linear-chain conditional
random field (CRF) (Lafferty et al., 2001) which
generates a sequence of labels, conditional on a
token sequence. Importantly, the parameters of
the encoding model can be interpreted in the same
way a CRF model would. This is in contrary to
generative model parameters which explain both
the observation sequence and the label sequence.
The reconstruction model, on the other hand, inde-
pendently generates the tokens conditional on the
corresponding labels. Both labeled and unlabeled
data can be efficiently used to fit parameters of this
model, minimizing regularized log loss. See ?4.1
for more details.
After modeling unlabeled token sequences, we
explore two other ways of leveraging unlabeled
data: word embeddings and word lists. The word
embeddings we use capture monolingual distribu-
tional similarities and therefore may be indicative
of a language (see ?4.2). A word list, on the other
hand, is a collection of words which have been
manually or automatically constructed and share
some property (see ?4.3). For example, we extract
the set of surface forms in monolingual corpora.
In ?5, we describe the experiments and discuss
results. According to the results, modeling unla-
beled data using CRF autoencoders did not im-
prove prediction accuracy. Nevertheless, more ex-
periments need to be run before we can conclude
this setting. On the positive side, word embed-
dings and word lists have been shown to improve
CS prediction accuracy, provided they have decent
coverage of tokens in the test set.
2 Task Description
The shared task training data consists of code?
switched tweets with token-level annotations.
The data is organized in four language pairs:
English?Spanish (En-Es), English?Nepali (En-
80
Ne), Mandarin?English (Zh-En) and Modern
Standard Arabic?Arabic dialects (MSA-ARZ).
Table 1 shows the size of the data sets provided
for the shared task in each language pair.
For each tweet in the data set, the user ID, tweet
ID, and a list of tokens? start offset and end offset
are provided. Each token is annotated with one
of the following labels: lang1, lang2, ne (i.e.,
named entities), mixed (i.e., mixed parts of lang1
and lang2), ambiguous (i.e., cannot be identified
given context), and other.
Two test sets were used to evaluate each sub-
mission for the shared task in each language pair.
The first test set consists of Tweets, similar to the
training set. The second test set consists of token
sequences from a surprise genre. Since partici-
pants were not given the test sets, we only report
results on a Twitter test set (a subset of the data
provided for shared task participants). Statistics
of our train/test data splits are given in Table 5.
lang. pair split tweets tokens users
En?Ne all 9, 993 146, 053 18
train 7, 504 109, 040 12
test 2, 489 37, 013 6
En?Es all 11, 400 140, 738 9
train 7, 399 101, 451 6
test 4, 001 39, 287 3
Zh?En all 994 17, 408 995
train 662 11, 677 663
test 332 5, 731 332
MSA?ARZ all 5, 862 119, 775 7
train 4, 800 95, 352 6
test 1, 062 24, 423 1
Table 1: Total number of tweets, tokens, and Twit-
ter user IDs for each language pair. For each lan-
guage pair, the first line represents all data pro-
vided to shared task participants. The second and
third lines represent our train/test data split for the
experiments reported in this paper. Since Twit-
ter users are allowed to delete their tweets, the
number of tweets and tokens reported in the third
and fourth columns may be less than the number
of tweets and tokens originally annotated by the
shared task organizers.
3 Baseline System
We model token-level language ID as a sequence
of labels using a linear-chain conditional ran-
dom field (CRF) (Lafferty et al., 2001) described
in ?3.1 with the features in ?3.2.
3.1 Model
A linear-chain CRF models the conditional proba-
bility of a label sequence y given a token sequence
x and given extra context ?, as follows:
p(y | x,?) =
exp?
>
?
|x|
i=1
f(x, y
i
, y
i?1
,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i
, y
?
i?1
,?)
where ? is a vector of feature weights, and f is
a vector of local feature functions. We use ? to
explicitly represent context information necessary
to compute the feature functions described below.
In a linear-chain structure, y
i
only depends on
observed variables x,? and the neighboring labels
y
i?1
and y
i+1
. Therefore, we can use dynamic
programming to do inference in run time that is
quadratic in the number of unique labels and lin-
ear in the sequence length. We use L-BFGS to
learn the feature weights ?, maximizing the L
2
-
regularized log-likelihood of labeled examples L:
``
supervised
(?) =
c
L
2
||?||
2
2
+
?
?x,y??L
log p(y | x,?)
After training the model, we use again use dy-
namic programming to find the most likely label
sequence, for each token sequence in the test set.
3.2 Features
We use the following features in the baseline sys-
tem:
? character n-grams (loweredcased tri- and quad-
grams)
? prefixes and suffixes of lengths 1, 2, 3 and 4
? unicode page of the first character
1
? case (first-character-uppercased vs. all-
characters-uppercased vs. all-characters-
alphanumeric)
? tweet-level language ID predictions from two
off-the-shelf language identifiers: cld2
2
and
ldig
3
1
http://www.unicode.org/charts/
2
https://code.google.com/p/cld2/
3
https://github.com/shuyo/ldig
81
encod
ing 
recon
struc
tion 
x 
yi-1 yi yi+1 
xi-1 xi xi+1 ? ? ? 
? 
Figure 1: A diagram of the CRF autoencoder
4 Using Unlabeled Data
In ?3, we learn the parameters of the CRF model
parameters in a standard fully supervised fashion,
using labeled examples in the training set. Here,
we attempt to use unlabeled examples to improve
our system?s performance in three ways: model-
ing unlabeled token sequences in the CRF autoen-
coder framework, word embeddings, and word
lists.
4.1 CRF Autoencoders
A CRF autoencoder (Ammar et al., 2014) consists
of an input layer, an output layer, and a hidden
layer. Both input and output layer represent the
observed token sequence. The hidden layer rep-
resents the label sequence. Fig. 1 illustrates the
model dependencies for sequence labeling prob-
lems with a first-order Markov assumption. Con-
ditional on an observation sequence x and side in-
formation ?, a traditional linear-chain CRF model
is used to generate the label sequence y. The
model then generates x? which represents a recon-
struction of the original observation sequence. El-
ements of this reconstruction (i.e., x?
i
) are then in-
dependently generated conditional on the corre-
sponding label y
i
using simple categorical distri-
butions.
The parametric form of the model is given by:
p(y, x? | x,?) =
|x|
?
i=1
?
x?
i
|y
i
?
exp?
>
?
|x|
i=1
f(x, y
i?1
, y
i
, i,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i?1
, y
?
i
, i,?)
where ? is a vector of CRF feature weights, f is a
vector of local feature functions (we use the same
features described in ?3.2), and ?
x?
i
|y
i
are categor-
ical distribution parameters of the reconstruction
model representing p(x?
i
| y
i
).
We can think of a label sequence as a low-
cardinality lossy compression of the correspond-
ing token sequence. CRF autoencoders explic-
itly model this intuition by creating an information
bottleneck where label sequences are required to
regenerate the same token sequence despite their
limited capacity. Therefore, when only unlabeled
examples U are available, we train CRF autoen-
coders by maximizing the regularized likelihood
of generating reconstructions x?, conditional on x,
marginalizing values of label sequences y:
``
unsupervised
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)
where R
Dirichlet
is a regularizer based on a vari-
ational approximation of a symmetric Dirichlet
prior with concentration parameter ? for the re-
construction parameters ?.
Having access to labeled examples, it is easy to
modify this objective to learn from both labeled
and unlabeled examples as follows:
``
semi
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
c
unlabeled
?
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)+
c
labeled
?
?
?x,y??L
log p(y | x)
We use block coordinate descent to optimize
this objective. First, we use c
em
iterations of
the expectation maximization algorithm to opti-
mize the ?-block while the ?-block is fixed, then
we optimize the ?-block with c
lbfgs
iterations of
L-BFGS (Liu et al., 1989) while the ?-block is
fixed.
4
4.2 Unsupervised Word Embeddings
For many NLP tasks, using unsupervised
word representations as features improves
accuracy (Turian et al., 2010). We use
word2vec (Mikolov et al., 2013) to train
100?dimensional word embeddings from a
large Twitter corpus of about 20 million tweets
extracted from the live stream, in multiple lan-
guages. We define an additional feature function
4
An open source efficient c++ imple-
mentation of our method can be found at
https://github.com/ldmt-muri/alignment-with-openfst
82
in the CRF autoencoder model ?4.1 for each of
the 100 dimensions, conjoined with the label y
i
.
The feature value is the corresponding dimension
for x
i
. A binary feature indicating the absence of
word embeddings is fired for out-of-vocabulary
words (i.e., words for which we do not have word
embeddings). The token-level coverage of the
word embeddings for each of the languages or
dialects used in the training data is reported in
Table 2.
4.3 Word List Features
While some words are ambiguous, many words
frequently occur in only one of the two lan-
guages being considered. An easy way to iden-
tify the label of such unambiguous words is to
check whether they belong to the vocabulary of
either language. Moreover, named entity recog-
nizers typically rely on gazetteers of named enti-
ties to improve their performance. We generalize
the notion of using monolingual vocabularies and
gazetteers of named entities to general word lists.
Using K word lists {l
1
, . . . , l
K
}, when a token x
i
is labeled with y
i
, we fire a binary feature that con-
joins ?y
i
, ?(x
i
? l
1
), . . . , ?(x
i
? l
K
)?, where ? is
an indicator boolean function. We use the follow-
ing word lists:
? Hindi and Nepali Wikipedia article titles
? multilingual named entities from the JRC
dataset
5
and CoNLL 2003 shared task
? word types in monolingual corpora in MSA,
ARZ, En and Es.
? set difference between the following pairs of
word lists: MSA-ARZ, ARZ-MSA, En-Es, Es-
En.
Transliteration from Devanagari The Nepali?
English tweets in the dataset are romanized. This
renders our Nepali word lists, which are based
on the Devanagari script, useless. Therefore, we
transliterate the Hindi and Nepali named entities
lists using a deterministic phonetic mapping. We
romanize the Devanagari words using the IAST
scheme.
6
We then drop all accent marks on the
characters to make them fit into the 7?bit ASCII
range.
5
http://datahub.io/dataset/jrc-names
6
http://en.wikipedia.org/wiki/
International_Alphabet_of_Sanskrit_
Transliteration
embeddings word lists
language coverage coverage
ARZ 30.7% 68.8%
En 73.5% 55.7%
MSA 26.6% 76.8%
Ne 14.5% 77.0%
Es 62.9% 78.0%
Zh 16.0% 0.7%
Table 2: The type-level coverage of annotated data
according to word embeddings (second column)
and according to word lists (third column), per lan-
guage.
5 Experiments
We compare the performance of five models for
each language pair, which correspond to the five
lines in Table 3. The first model, ?CRF? is the
baseline model described in ?3. The second ?CRF
+ U
test
? and the third ?CRF + U
all
? are CRF au-
toencoder models (see ?4.1) with two sets of un-
labeled data: (1) U
test
which only includes the test
set,
7
and (2) U
all
which includes the test set as well
as all tweets by the set of users who contributed
any tweets in L. The fourth model ?CRF + U
all
+
emb.? is a CRF autoencoder which uses word em-
bedding features (see ?4.2), as well as the features
described in ?3.2. Finally, the fifth model ?CRF +
U
all
+ emb. + lists? further adds word list features
(see ?4.3). In all but the ?CRF? model, we adopt a
transductive learning setup.
Since the CRF baseline is used as the encoding
part of the CRF autoencoder model, we use the
supervisedly-trained CRF parameters to initialize
the CRF autoencoder models. The categorical dis-
tributions of the reconstruction model are initial-
ized with discrete uniforms. We set the weight
of the labeled data log-likelihood c
labeled
= 0.5,
the weight of the unlabeled data log-likelihood
c
unlabeled
= 0.5, the L
2
regularization strength
c
L
2
= 0.3, the concentration parameter of the
Dirichlet prior ? = 0.1, the number of L-BFGS
iterations c
LBFGS
= 4, and the number of EM iter-
ations c
EM
= 4.
8
We stop training after 50 itera-
tions of block coordinate descent.
7
U
test
is potentially useful when the test set belongs to a
different domain than the labeled examples, which is often
referred to as ?domain adaptation?. However we were unable
to test this hypothesis since all the CS annotations we had
access to are from Twitter.
8
Hyper-parameters c
L
2
and ? were tuned using cross-
validation. The remaining hyper-parameters were not tuned.
83
config En?Ne MSA?ARZ En?Es Zh?En
CRF 95.2% 80.5% 94.6% 94.9%
+T
test
95.2% 80.6% 94.6% 94.9%
+T
all
95.2% 80.7% 94.6% 94.9%
+emb. 95.3% 81.3% 95.1% 95.0%
+lists 97.0% 81.2% 96.7% 95.3%
Table 3: Token level accuracy results for each of
the four language pairs.
label predicted predicted
MSA ARZ
true MSA 93.9% 5.3%
true ARZ 32.1% 65.2%
Table 4: Confusion between MSA and ARZ in the
Baseline configuration.
Results. The CRF baseline results are reported
in the first line in Table 3. For three language
pairs, the overall token-level accuracy ranges be-
tween 94.6% and 95.2%. In the fourth language
pair, MSA-ARZ, the baseline accuracy is 80.5%
which indicates the relative difficulty of this task.
The second and third lines in Table 3 show the
results when we use CRF autoencoders with the
unlabeled test set (U
test
), and with all unlabeled
tweets (U
all
), respectively. While semi-supervised
learning did not hurt accuracy on any of the lan-
guages, it only resulted in a tiny increase in accu-
racy for the Arabic dialects task.
The fourth line in Table 3 extends the CRF au-
toencoder model (third line) by adding unsuper-
vised word embedding features. This results in
an improvement of 0.6% for MSA-ARZ, 0.5% for
En-Es, 0.1% for En-Ne and Zh-En.
The fifth line builds on the fourth line by adding
word list features. This results in an improvement
of 1.7% in En-Ne, 1.6% in En-Es, 0.4% in Zh-En,
and degradation of 0.1% in MSA-ARZ.
Analysis and Discussion The baseline perfor-
mance in the MSA-ARZ task is considerably
lower than those of the other tasks. Table 4 illus-
trates how the baseline model confuses lang1 and
lang2 in the MSA-ARZ task. While the baseline
system correctly labels 93.9% of MSA tokens, it
only correctly labels 65.2% of ARZ tokens.
Although the reported semi-supervised results
did not significantly improve on the CRF baseline,
more work needs to be done in order to conclude
these results:
lang. pair |U
test
| |U
all
| |L|
En?Ne 2489 6230 7504
MSA?ARZ 1062 2520 4800
Zh?En 332 332 663
En?Es 4001 7177 7399
Table 5: Number of tweets inL, U
test
andU
all
used
for semi-supervised learning of CRF autoencoders
models.
? Use an out-of-domain test set where some adap-
tation to the test set is more promising.
? Vary the number of labeled examples |L| and
the number of unlabeled examples |U|. Table 5
gives the number of labeled and unlabeled ex-
amples used for training the model. It is pos-
sible that semi-supervised learning would have
been more useful with a smaller |L| and a larger
|U|.
? Tune c
labeled
and c
unlabeled
.
? Split the parameters ? into two subsets: ?
labeled
and ?
unlabeled
; where ?
labeled
are the parameters
which have a non-zero value for any input x in
L and ?
unlabeled
are the remaining parameters in
? which only have non-zero values with unla-
beled examples but not with the labeled exam-
ples.
? Use a richer reconstruction model.
? Reconstruct a transformation of the token se-
quences instead of their surface forms.
? Train a token-level language ID model trained
on a large number of languages, as opposed to
disambiguating only two languages at a time.
Word embeddings improve the results for all
language pairs, but the largest improvement is in
MSA-ARZ and En-Es. Looking into the word em-
beddings coverage of those languages (i.e., MSA,
ARZ, Es, En in Table 2), we find that they are bet-
ter covered than the other languages (Ne, Zh). We
conclude that further improvements on En-Ne and
Zh-En may be expected if they are better repre-
sented in the corpus used to learn word embed-
dings.
As for the word lists, the largest improvement
we get is the romanized word lists of Nepali,
which have a 77.0% coverage and improve the
accuracy by 1.7%. This shows that our translit-
erated word lists not only cover a lot of tokens,
and are also useful for language ID. The Spanish
84
Config lang1 lang2 ne
+lists 84.1% 76.5% 73.7%
-lists 84.2% 77.1% 71.5%
Table 6: F?Measures of two Arabic configura-
tions. lang1 is MSA. lang2 is ARZ.
word lists also have a wide coverage, improving
the overall accuracy by 1.6%. The overall accu-
racy of the Arabic dialects slightly degrades with
the addition of the word lists. Closer inspection
in table 6 reveals that it improves the F?Measure
of the named entities at the expense of both MSA
(lang1) and ARZ (lang2).
6 Related Work
Previous work on identifying languages in a mul-
tilingual document includes (Singh and Gorla,
2007; King and Abney, 2013; Lui et al., 2014).
Their goal is generally more about identifying the
languages that appear in the document than intra?
sentential CS points.
Previous work on computational models of
code?switching include formalism (Joshi, 1982)
and language models that encode syntactic con-
straints from theories of code?switching, such as
(Li and Fung, 2013; Li and Fung, 2014). These
require the existence of a parser for the languages
under consideration. Other work on prediction
of code?switching points, such as (Elfardy et al.,
2013; Nguyen and Dogruoz, 2013) and ours, do
not depend upon such NLP infrastructure. Both of
the aforementioned use basic character?level fea-
tures and dictionaries on sequence models.
7 Conclusion
We have shown that a simple CRF baseline with
a handful of feature templates obtains strong re-
sults for this task. We discussed three methods
to improve over the supervised baseline using un-
labeled data: (1) modeling unlabeleld data using
CRF autoencoders, (2) using pre-trained word em-
beddings, and (3) using word list features.
We show that adding word embedding features
and word lists features is useful when they have
good coverage of words in a data set. While mod-
est improvements are observed due to modeling
unlabeled data with CRF autoenocders, we iden-
tified possible directions to gain further improve-
ments.
While bilingual disambiguation was a good first
step for identifying code switching, we suggest a
reformulation of the task such that each label can
take on one of many languages.
Acknowledgments
We thank Brendan O?Connor who helped assem-
ble the Twitter dataset. We also thank the work-
shop organizers for their hard work, and the re-
viewers for their comments. This work was
sponsored by the U.S. Army Research Labora-
tory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533. The
statements made herein are solely the responsibil-
ity of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014.
Conditional random field autoencoders for unsuper-
vised structured prediction. In Proc. of NIPS.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
John J. Gumperz. 1982. Discourse Strategies. Studies
in Interactional Sociolinguistics. Cambridge Univer-
sity Press.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of
the 9th Conference on Computational Linguistics -
Volume 1, COLING ?82, pages 145?150, Czechoslo-
vakia. Academia Praha.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML.
Ying Li and Pascale Fung. 2013. Improved mixed lan-
guage speech recognition using asymmetric acous-
tic model and language model with code-switch in-
version constraints. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2013 IEEE International
Conference on, pages 7368?7372, May.
Ying Li and Pascale Fung. 2014. Code switch lan-
guage modeling with functional head constraint. In
Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pages
4913?4917, May.
85
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.
Marco Lui, Han Jey Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation of Computational Linguistics, 2:27?40.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Dong Nguyen and Seza A. Dogruoz. 2013. Word level
language identification in online multilingual com-
munication. Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 857?862. Association for Computational Lin-
guistics.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identi-
fication of languages and encodings in a multilingual
document. In Building and Exploring Web Corpora
(WAC3-2007): Proceedings of the 3rd Web as Cor-
pus Workshop, Incorporating Cleaneval, volume 4,
page 95.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
86
