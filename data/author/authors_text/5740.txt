Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 367?374,
New York, June 2006. c?2006 Association for Computational Linguistics
Incorporating Speaker and Discourse Features into Speech
Summarization
Gabriel Murray, Steve Renals,
Jean Carletta, Johanna Moore
University of Edinburgh, School of Informatics
Edinburgh EH8 9LW, Scotland
gabriel.murray@ed.ac.uk, s.renals@ed.ac.uk,
jeanc@inf.ed.ac.uk, j.moore@ed.ac.uk
Abstract
We have explored the usefulness of incorporat-
ing speech and discourse features in an automatic
speech summarization system applied to meeting
recordings from the ICSI Meetings corpus. By an-
alyzing speaker activity, turn-taking and discourse
cues, we hypothesize that such a system can out-
perform solely text-based methods inherited from
the field of text summarization. The summariza-
tion methods are described, two evaluation meth-
ods are applied and compared, and the results
clearly show that utilizing such features is advanta-
geous and efficient. Even simple methods relying
on discourse cues and speaker activity can outper-
form text summarization approaches.
1. Introduction
The task of summarizing spontaneous spoken di-
alogue from meetings presents many challenges:
information is sparse; speech is disfluent and frag-
mented; automatic speech recognition is imper-
fect. However, there are numerous speech-specific
characteristics to be explored and taken advantage
of. Previous research on summarizing speech has
concentrated on utilizing prosodic features [1, 2].
We have examined the usefulness of additional
speech-specific characteristics such as discourse
cues, speaker activity, and listener feedback. This
speech features approach is contrasted with a sec-
ond summarization approach using only textual
features?a centroid method [3] using a latent se-
mantic representation of utterances. These indi-
vidual approaches are compared to a combined ap-
proach as well as random baseline summaries.
This paper also introduces a new evalua-
tion scheme for automatic summaries of meeting
recordings, using a weighted precision score based
on multiple human annotations of each meeting
transcript. This evaluation scheme is described
in detail below and is motivated by previous find-
ings [4] suggesting that n-gram based metrics like
ROUGE [5] do not correlate well in this domain.
2. Previous Work
In the field of speech summarization in general, re-
search investigating speech-specific characteristics
has focused largely on prosodic features such as F0
mean and standard deviation, pause information,
syllable duration and energy. Koumpis and Re-
nals [1] investigated prosodic features for summa-
rizing voicemail messages in order to send voice-
mail summaries to mobile devices. Hori et al [6]
have developed an integrated speech summariza-
tion approach, based on finite state transducers, in
which the recognition and summarization compo-
nents are composed into a single finite state trans-
ducer, reporting results on a lecture summariza-
tion task. In the Broadcast News domain, Maskey
and Hirschberg [7] found that the best summariza-
tion results utilized prosodic, lexical, and structural
features, while Ohtake et al [8] explored using
only prosodic features for summarization. Maskey
and Hirschberg similarly found that prosodic fea-
tures alone resulted in good quality summaries of
367
Broadcast News.
In the meetings domain (using the ICSI cor-
pus), Murray et al [2] compared text summariza-
tion approaches with feature-based approaches us-
ing prosodic features, with human judges favoring
the feature-based approaches. Zechner [9] inves-
tigated summarizing several genres of speech, in-
cluding spontaneous meeting speech. Though rel-
evance detection in his work relied largely on tf.idf
scores, Zechner also explored cross-speaker infor-
mation linking and question/answer detection, so
that utterances could be extracted not only accord-
ing to high tf.idf scores, but also if they were linked
to other informative utterances.
Similarly, this work aims to detect important
utterances that may not be detectable according
to lexical features or prosodic prominence, but
are nonetheless linked to high speaker activity,
decision-making, or meeting structure.
3. Summarization Approaches
The following subsections give detailed descrip-
tions of our two summarization systems, one of
which focuses on speech and discourse features
while the other utilizes text summarization tech-
niques and latent semantic analysis.
3.1. Speech and Discourse Features
In previous summarization work on the ICSI cor-
pus [2, 4], Murray et al explored multiple ways
of applying latent semantic analysis (LSA) to a
term/document matrix of weighted term frequen-
cies from a given meeting, a development of the
method in [10]. A central insight to the present
work is that additional features beyond simple term
frequencies can be included in the matrix before
singular value decomposition (SVD) is carried out.
We can use SVD to project this matrix of features
to a lower dimensionality space, subsequently ap-
plying the same methods as used in [2] for extract-
ing sentences.
The features used in these experiments in-
cluded features of speaker activity, discourse cues,
listener feedback, simple keyword spotting, meet-
ing location and dialogue act length (in words).
For each dialogue act, there are features indi-
cating which speaker spoke the dialogue act and
whether the same speaker spoke the preceding and
succeeding dialogue acts. Another set of features
indicates how many speakers are active on either
side of a given dialogue act: specifically, how
many speakers were active in the preceding and
succeeding five dialogue acts. To further gauge
speaker activity, we located areas of high speaker
interaction and indicated whether or not a given
dialogue act immediately preceded this region of
activity, with the motivation being that informa-
tive utterances are often provocative in eliciting re-
sponses and interaction. Additionally, we included
a feature indicating which speakers most often ut-
tered dialogue acts that preceded high levels of
speaker interaction, as one way of gauging speaker
status in the meeting. Another feature relating to
speaker activity gives each dialogue act a score ac-
cording to how active the speaker is in the meeting
as a whole, based on the intuition that the most ac-
tive speakers will tend to utter the most important
dialogue acts.
The features for discourse cues, listener feed-
back, and keyword spotting were deliberately su-
perficial, all based simply on detecting informative
words. The feature for discourse cues indicates the
presence or absence of words such as decide, dis-
cuss, conclude, agree, and fragments such as we
should indicating a planned course of action. Lis-
tener feedback was based on the presence or ab-
sence of positive feedback cues following a given
dialogue act; these include responses such as right,
exactly and yeah. Keyword spotting was based
on frequent words minus stopwords, indicating the
presence or absence of any of the top twenty non-
stopword frequent words. The discourse cues of
interest were derived from a manual corpus analy-
sis rather than being automatically detected.
A structural feature scored dialogue acts ac-
cording to their position in the meeting, with di-
alogue acts from the middle to later portion of the
meeting scoring higher and dialogue acts at the be-
ginning and very end scoring lower. This is a fea-
ture that is well-matched to the relatively unstruc-
tured ICSI meetings, as many meetings would be
expected to have informative proposals and agen-
das at the beginning and perhaps summary state-
ments and conclusions at the end.
Finally, we include a dialogue act length fea-
ture motivated by the fact that informative utter-
ances will tend to be longer than others.
The extraction method follows [11] by rank-
ing sentences using an LSA sentence score. The
368
matrix of features is decomposed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n?n matrix of right-singular vectors.
Using sub-matrices S and V T , the LSA sentence
scores are obtained using:
ScLSAi =
?
?
?
?
n
?
k=1
v(i, k)2 ? ?(k)2 ,
where v(i, k) is the kth element of the ith sen-
tence vector and ?(k) is the corresponding singular
value.
Experiments on a development set of 55 ICSI
meetings showed that reduction to between 5?15
dimension was optimal. These development ex-
periments also showed that weighting some fea-
tures slightly higher than others resulted in much
improved results; specifically, the discourse cues
and listener feedback cues were weighted slightly
higher.
3.2. LSA Centroid
The second summarization method is a textual ap-
proach incorporating LSA into a centroid-based
system [3]. The centroid is a pseudo-document
representing the important aspects of the docu-
ment as a whole; in the work of [3], this pseudo-
document consists of keywords and their modi-
fied tf.idf scores. In the present research, we take
a different approach to constructing the centroid
and to representing sentences in the document.
First, tf.idf scores are calculated for all words in
the meeting. Using these scores, we find the top
twenty keywords and choose these as the basis for
our centroid. We then perform LSA on a very large
corpus of Broadcast News and ICSI data, using the
Infomap tool1. Infomap provides a query language
with which we can retrieve word vectors for our
twenty keywords, and the centroid is thus repre-
sented as the average of its constituent keyword
vectors [12] [13].
Dialogue acts from the meetings are repre-
sented in much the same fashion. For each dia-
logue act, the vectors of its constituent words are
1http://infomap.stanford.edu
retrieved, and the dialogue act as a whole is the av-
erage of its word vectors. Extraction then proceeds
by finding the dialogue act with the highest cosine
similarity with the centroid, adding this to the sum-
mary, then continuing until the desired summary
length is reached.
3.3. Combined
The third summarization method is simply a com-
bination of the first two. Each system produces a
ranking and a master ranking is derived from these
two rankings. The hypothesis is that the strength
of one system will differ from the other and that
the two will complement each other and produce
a good overall ranking. The first system would be
expected to locate areas of high activity, decision-
making, and planning, while the second would lo-
cate information-rich utterances. This exempli-
fies one of the challenges of summarizing meeting
recordings: namely, that utterances can be impor-
tant in much different ways. A comprehensive sys-
tem that relies on more than one idea of importance
is ideal.
4. Experimental Setup
All summaries were 350 words in length, much
shorter than the compression rate used in [2] (10%
of dialogue acts). The ICSI meetings themselves
average around 10,000 words in length. The rea-
sons for choosing a shorter length for summaries
are that shorter summaries are more likely to be
useful to a user wanting to quickly overview and
browse a meeting, they present a greater summa-
rization challenge in that the summarizer must be
more exact in pinpointing the important aspects of
the meeting, and shorter summaries make it more
feasible to enlist human evaluators to judge the nu-
merous summaries on various criteria in the future.
Summaries were created on both manual tran-
scripts and speech recognizer output. The unit of
extraction for these summaries was the dialogue
act, and these experiments used human segmented
and labeled dialogue acts rather than try to detect
them automatically. In future work, we intend to
incorporate dialogue act detection and labeling as
part of one complete automatic summarization sys-
tem.
369
4.1. Corpus Description
The ICSI Meetings corpus consists of 75 meetings,
lasting approximately one hour each. Our test set
consists of six meetings, each with multiple hu-
man annotations. Annotators were given access
to a graphical user interface (GUI) for browsing
an individual meeting that included earlier human
annotations: an orthographic transcription time-
synchronized with the audio, and a topic segmen-
tation based on a shallow hierarchical decompo-
sition with keyword-based text labels describing
each topic segment. The annotators were told to
construct a textual summary of the meeting aimed
at someone who is interested in the research being
carried out, such as a researcher who does similar
work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for
each heading, and told that there must be text for
the general abstract, but that the other headings
may have null annotations for some meetings. An-
notators who were new to the data were encour-
aged to listen to a meeting straight through before
beginning to author the summary.
Immediately after authoring a textual sum-
mary, annotators were asked to create an extractive
summary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding [14]. Annotators were told to ex-
tract dialogue acts that together would convey the
information in the textual summary, and could be
used to support the correctness of that summary.
They were given no specific instructions about the
number or percentage of acts to extract or about
redundant dialogue acts. For each dialogue act ex-
tracted, they were then required in a second pass
to choose the sentences from the textual summary
supported by the dialogue act, creating a many-
to-many mapping between the recording and the
textual summary. Although the expectation was
that each extracted dialogue act and each summary
sentence would be linked to something in the op-
posing resource, we told the annotators that under
some circumstances dialogue acts and summary
sentences could stand alone.
We created summaries using both manual tran-
scripts as well as automatic speech recognition
(ASR) output. The AMI-ASR system [15] is de-
scribed in more detail in [4] and the average word
error rate (WER) for the corpus is 29.5%.
4.2. Evaluation Frameworks
The many-to-many mapping of dialogue acts to
summary sentences described in the previous sec-
tion allows us to evaluate our extractive summaries
according to how often each annotator linked a
given extracted dialogue act to a summary sen-
tence. This is somewhat analogous to Pyramid
weighting [16], but with dialogue acts as the SCUs.
In fact, we can calculate weighted precision, recall
and f-score using these annotations, but because
the summaries created are so short, we focus on
weighted precision as our central metric. For each
dialogue act that the summarizer extracts, we count
the number of times that each annotator links that
dialogue act to a summary sentence. For a given
dialogue act, it may be that one annotator links it
0 times, one annotator links it 1 time, and the third
annotator links it two times, resulting in an aver-
age score of 1 for that dialogue act. The scores for
all of the summary dialogue acts can be calculated
and averaged to create an overall summary score.
ROUGE scores, based on n-gram overlap be-
tween human abstracts and automatic extracts,
were also calculated for comparison [5]. ROUGE-
2, based on bigram overlap, is considered the most
stable as far as correlating with human judgments,
and this was therefore our ROUGE metric of inter-
est. ROUGE-SU4, which evaluates bigrams with
intervening material between the two elements of
the bigram, has recently been shown in the con-
text of the Document Understanding Conference
(DUC)2 to bring no significant additional informa-
tion as compared with ROUGE-2. Results from
[4] and from DUC 2005 also show that ROUGE
does not always correlate well with human judg-
ments. It is therefore included in this research in
the hope of further determining how reliable the
2http://duc.nist.gov
370
ROUGE metric is for our domain of meeting sum-
marization.
5. Results
The experimental results are shown in figure 1
(weighted precision) and figure 2 (ROUGE-2) and
are discussed below.
5.1. Weighted Precision Results
For weighted precision, the speech features ap-
proach was easily the best and scored significantly
better than the centroid and random approaches
(ANOVA,p<0.05), attaining an averaged weighted
precision of 0.52. The combined approach did
not improve upon the speech features approach
but was not significantly worse either. The ran-
domly created summaries scored much lower than
all three systems.
The superior performance of the speech fea-
tures approach compared to the LSA centroid
method closely mirrors results on the ICSI devel-
opment set, where the centroid method scored 0.23
and the speech features approach scored 0.42. For
the speech features approach on the test set, the
best feature by far was dialogue act length. Re-
moving this feature resulted in the precision score
being nearly halved. This mirrors results from
Maskey and Hirschberg [7], who found that the
length of a sentence in seconds and its length in
words were the two best features for predicting
summary sentences. Both the simple keyword
spotting and the discourse cue detection features
caused a lesser decline in precision when removed,
while other features of speaker activity had a neg-
ligible impact on the test results.
Interestingly, the weighted precision scores on
ASR were not significantly worse for any of the
summarization approaches. In fact, the centroid
approach scored very slightly higher on ASR out-
put than on manual transcripts. In [17] and [2] it
was similarly found that summarizing with ASR
output did not cause great deterioration in the qual-
ity of the summaries. It is not especially surpris-
ing that the speech features approach performed
similarly on both manual and ASR transcripts, as
many of its features based on speaker exchanges
and speaker activity would be unaffected by ASR
errors. The speech features approach is still signif-
icantly better than the random and centroid sum-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
PRECISION-MAN
PRECISION-ASR
Figure 1: Weighted Precision Results on Test Set
maries, and is not significantly better than the com-
bined approach on ASR.
5.2. ROUGE Results
The ROUGE results greatly differed from the
weighted precision results in several ways. First,
the centroid method was considered to be the best,
with a ROUGE-2 score of 0.047 compared with
0.041 for the speech features approach. Second,
there were not as great of differences between the
four systems according to ROUGE as there were
according to weighted precision. In fact, the ran-
dom summaries of manual transcripts are not sig-
nificantly worse than the other approaches, accord-
ing to ROUGE-2. Neither the combined approach
nor the speech features approach is significantly
worse than the centroid system, with the combined
approach generally scoring on par with the cen-
troid scores.
The third difference relates to summarization
on ASR output. ROUGE-2 has the random system
and the combined system showing sharp declines
when applied to ASR transcripts. The speech fea-
tures and centroid approaches do not show de-
clines. Random summaries are significantly worse
than both the centroid summaries (p<0.1) and
speech features summaries (p<0.05). Though the
combined approach declines on ASR output, it is
not significantly worse than the other systems.
To get an idea of a ROUGE-2 upper bound, for
each meeting in the test set we left one human ab-
stract out and compared it with the remaining ab-
stracts. The result was an average ROUGE-2 score
of .086.
371
 0.02
 0.04
 0.06
 0.08
 0.1
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
ROUGE2-MAN
ROUGE2-ASR
UPPER BOUND
Figure 2: ROUGE-2 Results on Test Set
ROUGE-1 and ROUGE-SU4 show no signif-
icant differences between the centroid and speech
features approaches.
5.3. Correlations
There is no significant correlation between
macroaveraged ROUGE and weighted precision
scores across the meeting set, on both ASR and
manual transcripts. The Pearson correlation is
0.562 with a significance of p < 0.147. The Spear-
man correlation is 0.282 with a significance of p <
0.498. The correlation of scores across each test
meeting is worse yet, with a Pearson correlation
of 0.185 (p<0.208) and a Spearman correlation of
0.181 (p<0.271).
5.4. Sample Summary
The following is the text of a summary of meeting
Bed004 using the speech features approach:
-so its possible that we could do something like a summary
node of some sort that
-and then the question would be if if those are the things that you
care about uh can you make a relatively compact way of getting from
the various inputs to the things you care about
-this is sort of th the second version and i i i look at this maybe just
as a you know a a whatever uml diagram or you know as just a uh
screen shot not really as a bayes net as john johno said
-and um this is about as much as we can do if we dont w if we want
to avoid uh uh a huge combinatorial explosion where we specify ok if
its this and this but that is not the case and so forth it just gets really
really messy
-also it strikes me that we we m may want to approach the point
where we can sort of try to find a uh a specification for some interface
here that um takes the normal m three l looks at it
-so what youre trying to get out of this deep co cognitive linguistics is
the fact that w if you know about source source paths and goals and
nnn all this sort of stuff that a lot of this is the same for different tasks
-what youd really like of course is the same thing youd always like
which is that you have um a kind of intermediate representation
which looks the same o over a bunch of inputs and a bunch of outputs
-and pushing it one step further when you get to construction
grammar and stuff what youd like to be able to do is say you have
this parser which is much fancier than the parser that comes with uh
smartkom
-in independent of whether it about what is this or where is it or
something that you could tell from the construction you could pull
out deep semantic information which youre gonna use in a general
way
6. Discussion
Though the speech features approach was consid-
ered the best system, it is unclear why the com-
bined approach did not yield improvement. One
possibility relates to the extreme brevity of the
summaries: because the summaries are only 350
words in length, it is possible to have two sum-
maries of the same meeting which are equally
good but completely non-overlapping in content.
In other words, they both extract informative dia-
logue acts, but not the same ones. Combining the
rankings of two such systems might create a third
system which is comparable but not any better than
either of the first two systems alone. However, it
is still possible that the combined system will be
better in terms of balancing the two types of im-
portance discussed above: utterances that contain a
lot of informative content and keywords and utter-
ances that relate to decision-making and meeting
structure.
ROUGE did not correlate well with the
weighted precision scores, a result that adds to the
previous evidence that this metric may not be reli-
able in the domain of meeting summarization.
It is very encouraging that the summarization
approaches in general seem immune to the WER
of the ASR output. This confirms previous find-
ings such as [17] and [2], and the speech and
structural features used herein are particularly un-
affected by a moderately high WER. The reason
for the random summarizaton system not suffering
372
a sharp decline when applied to ASR may be due
to the fact that its scores were already so low that
it couldn?t deteriorate any further.
7. Future Work
The above results show that even a relatively small
set of speech, discourse, and structural features can
outperform a text summarization approach on this
data, and there are many additional features to be
explored. Of particular interest to us are features
relating to speaker status, i.e. features that help us
determine who is leading the meeting and who it is
that others are deferring to. We would also like to
more closely investigate the relationship between
areas of high speaker activity and informative ut-
terances.
In the immediate future, we will incorporate
these features into a machine-learning framework,
building support vector models trained on the ex-
tracted and non-extracted classes of the training
set.
Finally, we will apply these methods to the
AMI corpus [18] and create summaries of compa-
rable length for that meeting set. There are likely
to be differences regarding usefulness of certain
features due to the ICSI meetings being relatively
unstructured and informal and the AMI hub meet-
ings being more structured with a higher informa-
tion density.
8. Conclusion
The results presented above show that using fea-
tures related to speaker activity, listener feedback,
discourse cues and dialogue act length can outper-
form the lexical methods of text summarization ap-
proaches. More specifically, the fact that there are
multiple types of important utterances requires that
we use multiple methods of detecting importance.
Lexical methods and prosodic features are not nec-
essarily going to detect utterances that are relevant
to agreement, decision-making or speaker activity.
This research also provides further evidence that
ROUGE does not correlate well with human judg-
ments in this domain. Finally, it has been demon-
strated that high WER for ASR output does not
significantly decrease summarization quality.
9. Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication AMI-
150).
10. References
[1] K. Koumpis and S. Renals, ?Automatic sum-
marization of voicemail messages using lex-
ical and prosodic features,? ACM Transac-
tions on Speech and Language Processing,
vol. 2, pp. 1?24, 2005.
[2] G. Murray, S. Renals, and J. Carletta, ?Ex-
tractive summarization of meeting record-
ings,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[3] D. Radev, S. Blair-Goldensohn, and
Z. Zhang, ?Experiments in single and multi-
document summarization using mead,? in
The Proceedings of the First Document
Understanding Conference, New Orleans,
LA, September 2001.
[4] G. Murray, S. Renals, J. Carletta, and
J. Moore, ?Evaluating automatic summaries
of meeting recordings,? in Proceedings of
the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop on Machine Translation and Summa-
rization Evaluation (MTSE), Ann Arbor, MI,
USA, June 2005.
[5] C.-Y. Lin and E. H. Hovy, ?Automatic
evaluation of summaries using n-gram co-
occurrence statistics,? in Proceedings of
HLT-NAACL 2003, Edmonton, Calgary,
Canada, May 2003.
[6] T. Hori, C. Hori, and Y. Minami, ?Speech
summarization using weighted finite-state
transducers,? in Proceedings of the 8th Eu-
ropean Conference on Speech Communica-
tion and Technology, Geneva, Switzerland,
September 2003.
373
[7] S. Maskey and J. Hirschberg, ?Compar-
ing lexial, acoustic/prosodic, discourse and
structural features for speech summariza-
tion,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[8] K. Ohtake, K. Yamamoto, Y. Toma, S. Sado,
S. Masuyama, and S. Nakagawa, ?Newscast
speech summarization via sentence shorten-
ing based on prosodic features,? in Proceed-
ings of the ISCA and IEEE Workshop on
Spontaneous Speech Processing and Recog-
nition, Tokyo, Japan, April 2003,.
[9] K. Zechner, ?Automatic summarization of
open-domain multiparty dialogues in diverse
genres,? Computational Linguistics, vol. 28,
no. 4, pp. 447?485, 2002.
[10] Y. Gong and X. Liu, ?Generic text sum-
marization using relevance measure and la-
tent semantic analysis,? in Proceedings of
the 24th Annual International ACM SI-
GIR Conference on Research and Develop-
ment in Information Retrieval, New Orleans,
Louisiana, USA, September 2001, pp. 19?25.
[11] J. Steinberger and K. Jez?ek, ?Using latent
semantic analysis in text summarization and
summary evaluation,? in Proceedings of ISIM
2004, Roznov pod Radhostem, Czech Repub-
lic, April 2004, pp. 93?100.
[12] P. Foltz, W. Kintsch, and T. Landauer, ?The
measurement of textual coherence with la-
tent semantic analysis,? Discourse Processes,
vol. 25, 1998.
[13] B. Hachey, G. Murray, and D. Reitter, ?The
embra system at duc 2005: Query-oriented
multi-document summarization with a very
large latent semantic space,? in Proceedings
of the Document Understanding Conference
(DUC) 2005, Vancouver, BC, Canada, Octo-
ber 2005.
[14] E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, ,
and H. Carvey, ?The ICSI meeting recorder
dialog act (MRDA) corpus,? in Proceedings
of the 5th SIGdial Workshop on Discourse
and Dialogue, Cambridge, MA, USA, April-
May 2004, pp. 97?100.
[15] T. Hain, J. Dines, G. Garau, M. Karafiat,
D. Moore, V. Wan, R. Ordelman,
I.Mc.Cowan, J.Vepa, and S.Renals, ?An
investigation into transcription of conference
room meetings,? Proceedings of the 9th
European Conference on Speech Commu-
nication and Technology, Lisbon, Portugal,
September 2005.
[16] A. Nenkova and B. Passonneau, ?Evaluat-
ing content selection in summarization: The
pyramid method,? in Proceedings of HLT-
NAACL 2004, Boston, MA, USA, May 2004.
[17] R. Valenza, T. Robinson, M. Hickey, and
R. Tucker, ?Summarization of spoken audio
through information extraction,? in Proceed-
ings of the ESCA Workshop on Accessing In-
formation in Spoken Audio, Cambridge UK,
April 1999, pp. 111?116.
[18] J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, W. Kraaij, M. Kronen-
thal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and
P. Wellner, ?The AMI meeting corpus:
A pre-announcement,? in Proceedings of
MLMI 2005, Edinburgh, UK, June 2005.
374
NAACL HLT Demonstration Program, pages 9?10,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Automatic Segmentation and Summarization of Meeting Speech
Gabriel Murray, Pei-Yun Hsueh, Simon Tucker
Jonathan Kilgour, Jean Carletta, Johanna Moore, Steve Renals
University of Edinburgh
Edinburgh, Scotland
fgabriel.murray,p.hsuehg@ed.ac.uk
1 Introduction
AMI Meeting Facilitator is a system that per-
forms topic segmentation and extractive sum-
marisation. It consists of three components: (1)
a segmenter that divides a meeting into a num-
ber of locally coherent segments, (2) a summa-
rizer that selects the most important utterances
from the meeting transcripts. and (3) a com-
pression component that removes the less im-
portant words from each utterance based on the
degree of compression the user specied. The
goal of the AMI Meeting Facilitator is two-fold:
rst, we want to provide sucient visual aids for
users to interpret what is going on in a recorded
meeting; second, we want to support the devel-
opment of downstream information retrieval and
information extraction modules with the infor-
mation about the topics and summaries in meet-
ing segments.
2 Component Description
2.1 Segmentation
The AMI Meeting Segmenter is trained using a
set of 50 meetings that are seperate from the in-
put meeting. We rst extract features from the
audio and video recording of the input meeting
in order to train the Maximum Entropy (Max-
Ent) models for classifying topic boundaries and
non-topic boundaries. Then we test each utter-
ance in the input meeting on the Segmenter to
see if it is a topic boundary or not. The features
we use include the following ve categories: (1)
Conversational Feature: These include a set
of seven conversational features, including the
amount of overlapping speech, the amount of
silence between speaker segments, the level of
similarity of speaker activity, the number of cue
words, and the predictions of LCSEG (i.e., the
lexical cohesion statistics, the estimated poste-
rior probability, the predicted class). (2) Lex-
ical Feature: Each spurt is represented as a
vector space of uni-grams, wherein a vector is 1
or 0 depending on whether the cue word appears
in the spurt. (3) Prosodic Feature: These
include dialogue-act (DA) rate-of-speech, max-
imum F0 of the DA, mean energy of the DA,
amount of silence in the DA, precedent and sub-
sequent pauses, and duration of the DA. (4)
Motion Feature: These include the average
magnitude of speaker movements, which is mea-
sured by the number of pixels changed, over the
frames of 40 ms within the spurt. (5) Contex-
tual Feature: These include the dialogue act
types and the speaker role (e.g., project man-
ager, marketing expert). In the dialogue act an-
notations, each dialogue act is classied as one
of the 15 types.
2.2 Summarization
The AMI summarizer is trained using a set of
98 scenario meetings. We train a support vec-
tor machine (SVM) on these meetings, using 26
features relating to the following categories: (1)
Prosodic Features: These include dialogue-
act (DA) rate-of-speech, maximum F0 of the
DA, mean energy of the DA, amount of silence
in the DA, precedent and subsequent pauses,
9
and duration of the DA. (2) Speaker Fea-
tures: These features relate to how dominant
the speaker is in the meeting as a whole, and
they include percentage of the total dialogue
acts which each speaker utters, percentage of
total words which speaker utters, and amount
of time in meeting that each person is speak-
ing. (3) Structural Features: These features
include the DA position in the meeting, and the
DA position in the speaker's turn. (4) Term
Weighting Features: We use two types of
term weighting: tf.idf, which is based on words
that are frequent in the meeting but rare across
a set of other meetings or documents, and a sec-
ond weighting feature which relates to how word
usage varies between the four meeting partici-
pants.
After training the SVM, we test on each meet-
ing of the 20 meeting test set in turn, ranking
the dialogue acts from most probable to least
probable in terms of being extract-worthy. Such
a ranking allows the user to create a summary
of whatever length she desires.
2.3 Compression
Each dialogue act has its constituent words
scored using tf.idf, and as the user compresses
the meeting to a greater degree the browser
gradually removes the less important words from
each dialogue act, leaving only the most infor-
mative material of the meeting.
3 Related Work
Previous work has explored the eect of lexi-
cal cohesion and conversational features on char-
acterizing topic boundaries, following Galley et
al.(2003). In previous work, we have also studied
the problem of predicting topic boundaries at
dierent levels of granularity and showed that a
supervised classication approach performs bet-
ter on predicting a coarser level of topic segmen-
tation (Hsueh et al, 2006).
The amount of work being done on speech
summarization has accelerated in recent years.
Maskey and Hirschberg(September 2005) have
explored speech summarization in the domain
of Broadcast News data, nding that combin-
ing prosodic, lexical and structural features yield
the best results. On the ICSI meeting corpus,
Murray et al(September 2005) compared apply-
ing text summarization approaches to feature-
based approaches including prosodic features,
while Galley(2006) used skip-chain Conditional
Random Fields to model pragmatic dependen-
cies between meeting utterances, and ranked
meeting dialogue acts using a combination or
prosodic, lexical, discourse and structural fea-
tures.
4 acknowledgement
This work was supported by the European
Union 6th FWP IST Integrated Project AMI
(Augmented Multi- party Interaction, FP6-
506811)
References
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
M. Galley. 2006. A skip-chain conditional ran-
dom eld for ranking meeting utterances by im-
portance. In Proceedings of EMNLP-06, Sydney,
Australia.
P. Hsueh, J. Moore, and S. Renals. 2006. Automatic
segmentation of multiparty dialogue. In the Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
S. Maskey and J. Hirschberg. September 2005. Com-
paring lexial, acoustic/prosodic, discourse and
structural features for speech summarization. In
Proceedings of the 9th European Conference on
Speech Communication and Technology, Lisbon,
Portugal.
G. Murray, S. Renals, and J. Carletta. Septem-
ber 2005. Extractive summarization of meeting
recordings. In Proceedings of the 9th European
Conference on Speech Communication and Tech-
nology, Lisbon, Portugal.
10
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 33?40, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating Automatic Summaries of Meeting Recordings
Gabriel Murray
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Steve Renals
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Jean Carletta
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Johanna Moore
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Abstract
The research below explores schemes for
evaluating automatic summaries of busi-
ness meetings, using the ICSI Meeting
Corpus (Janin et al, 2003). Both au-
tomatic and subjective evaluations were
carried out, with a central interest be-
ing whether or not the two types of eval-
uations correlate with each other. The
evaluation metrics were used to compare
and contrast differing approaches to au-
tomatic summarization, the deterioration
of summary quality on ASR output ver-
sus manual transcripts, and to determine
whether manual extracts are rated signifi-
cantly higher than automatic extracts.
1 Introduction
In the field of automatic summarization, it is widely
agreed upon that more attention needs to be paid
to the development of standardized approaches to
summarization evaluation. For example, the cur-
rent incarnation of the Document Understanding
Conference is putting its main focus on the de-
velopment of evaluation schemes, including semi-
automatic approaches to evaluation. One semi-
automatic approach to evaluation is ROUGE (Lin
and Hovy, 2003), which is primarily based on n-
gram co-occurrence between automatic and human
summaries. A key question of the research con-
tained herein is how well ROUGE correlates with
human judgments of summaries within the domain
of meeting speech. If it is determined that the two
types of evaluations correlate strongly, then ROUGE
will likely be a valuable and robust evaluation tool in
the development stage of a summarization system,
when the cost of frequent human evaluations would
be prohibitive.
Three basic approaches to summarization are
evaluated and compared below: Maximal Marginal
Relevance, Latent Semantic Analysis, and feature-
based classification. The other major comparisons
in this paper are between summaries on ASR ver-
sus manual transcripts, and between manual and au-
tomatic extracts. For example, regarding the for-
mer, it might be expected that summaries on ASR
transcripts would be rated lower than summaries on
manual transcripts, due to speech recognition errors.
Regarding the comparison of manual and automatic
extracts, the manual extracts can be thought of as
a gold standard for the extraction task, represent-
ing the performance ceiling that the automatic ap-
proaches are aiming for.
More detailed descriptions of the summarization
approaches and experimental setup can be found in
(Murray et al, 2005). That work relied solely on
ROUGE as an evaluation metric, and this paper pro-
ceeds to investigate whether ROUGE alone is a reli-
able metric for our summarization domain, by com-
paring the automatic scores with recently-gathered
human evaluations. Also, it should be noted that
while we are at the moment only utilizing intrinsic
evaluation methods, our ultimate plan is to evalu-
ate these meeting summaries extrinsically within the
context of a meeting browser (Wellner et al, 2005).
33
2 Description of the Summarization
Approaches
2.1 Maximal Marginal Relevance (MMR)
MMR (Carbonell and Goldstein, 1998) uses the
vector-space model of text retrieval and is particu-
larly applicable to query-based and multi-document
summarization. The MMR algorithm chooses
sentences via a weighted combination of query-
relevance and redundancy scores, both derived using
cosine similarity. The MMR score ScMMR(i)for a
given sentence Si in the document is given by
ScMMR(i) =
?(Sim(Si, D))? (1? ?)(Sim(Si, Summ)) ,
where D is the average document vector, Summ
is the average vector from the set of sentences al-
ready selected, and ? trades off between relevance
and redundancy. Sim is the cosine similarity be-
tween two documents.
This implementation of MMR uses lambda an-
nealing so that relevance is emphasized while the
summary is still short and minimizing redundancy is
prioritized more highly as the summary lengthens.
2.2 Latent Semantic Analysis (LSA)
LSA is a vector-space approach which involves pro-
jecting the original term-document matrix to a re-
duced dimension representation. It is based on the
singular value decomposition (SVD) of an m ? n
term-document matrix A, whose elements Aij rep-
resent the weighted term frequency of term i in doc-
ument j. In SVD, the term-document matrix is de-
composed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n ? n matrix of right-singular vectors.
The rows of V T may be regarded as defining top-
ics, with the columns representing sentences from
the document. Following Gong and Liu (Gong and
Liu, 2001), summarization proceeds by choosing,
for each row in V T , the sentence with the highest
value. This process continues until the desired sum-
mary length is reached.
Two drawbacks of this method are that dimen-
sionality is tied to summary length and that good
sentence candidates may not be chosen if they do
not ?win? in any dimension (Steinberger and Jez?ek,
2004). The authors in (Steinberger and Jez?ek, 2004)
found one solution, by extracting a single LSA-
based sentence score, with variable dimensionality
reduction.
We address the same concerns, following the
Gong and Liu approach, but rather than extracting
the best sentence for each topic, the n best sentences
are extracted, with n determined by the correspond-
ing singular values from matrix S. The number of
sentences in the summary that will come from the
first topic is determined by the percentage that the
largest singular value represents out of the sum of all
singular values, and so on for each topic. Thus, di-
mensionality reduction is no longer tied to summary
length and more than one sentence per topic can be
chosen. Using this method, the level of dimension-
ality reduction is essentially learned from the data.
2.3 Feature-Based Approaches
Feature-based classification approaches have been
widely used in text and speech summarization, with
positive results (Kupiec et al, 1995). In this work
we combined textual and prosodic features, using
Gaussian mixture models for the extracted and non-
extracted classes. The prosodic features were the
mean and standard deviation of F0, energy, and du-
ration, all estimated and normalized at the word-
level, then averaged over the utterance. The two lex-
ical features were both TFIDF-based: the average
and the maximum TFIDF score for the utterance.
For our second feature-based approach, we de-
rived single LSA-based sentence scores (Steinberger
and Jez?ek, 2004) to complement the six features de-
scribed above, to determine whether such an LSA
sentence score is beneficial in determining sentence
importance. We reduced the original term-document
matrix to 300 dimensions; however, Steinberger and
Jez?ek found the greatest success in their work by re-
ducing to a single dimension (Steinberger, personal
communication). The LSA sentence score was ob-
tained using:
ScLSAi =
?
?
?
?
n?
k=1
v(i, k)2 ? ?(k)2 ,
34
where v(i, k) is the kth element of the ith sentence
vector and ?(k) is the corresponding singular value.
3 Experimental Setup
We used human summaries of the ICSI Meeting cor-
pus for evaluation and for training the feature-based
approaches. An evaluation set of six meetings was
defined and multiple human summaries were created
for these meetings, with each test meeting having ei-
ther three or four manual summaries. The remaining
meetings were regarded as training data and a single
human summary was created for these. Our sum-
maries were created as follows.
Annotators were given access to a graphical user
interface (GUI) for browsing an individual meeting
that included earlier human annotations: an ortho-
graphic transcription time-synchronized with the au-
dio, and a topic segmentation based on a shallow hi-
erarchical decomposition with keyword-based text
labels describing each topic segment. The annota-
tors were told to construct a textual summary of the
meeting aimed at someone who is interested in the
research being carried out, such as a researcher who
does similar work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for each
heading, and told that there must be text for the gen-
eral abstract, but that the other headings may have
null annotations for some meetings.
Immediately after authoring a textual summary,
annotators were asked to create an extractive sum-
mary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding (Shriberg et al, 2004) (The dialogue
act categories themselves were not displayed, just
the segmentation). Annotators were told to extract
dialogue acts that together would convey the infor-
mation in the textual summary, and could be used to
support the correctness of that summary. They were
given no specific instructions about the number or
percentage of acts to extract or about redundant dia-
logue act. For each dialogue act extracted, they were
then required in a second pass to choose the sen-
tences from the textual summary supported by the
dialogue act, creating a many-to-many mapping be-
tween the recording and the textual summary.
The MMR and LSA approaches are both unsuper-
vised and do not require labelled training data. For
both feature-based approaches, the GMM classifiers
were trained on a subset of the training data repre-
senting approximately 20 hours of meetings.
We performed summarization using both the hu-
man transcripts and speech recognizer output. The
speech recognizer output was created using base-
line acoustic models created using a training set
consisting of 300 hours of conversational telephone
speech from the Switchboard and Callhome cor-
pora. The resultant models (cross-word triphones
trained on conversational side based cepstral mean
normalised PLP features) were then MAP adapted
to the meeting domain using the ICSI corpus (Hain
et al, 2005). A trigram language model was em-
ployed. Fair recognition output for the whole corpus
was obtained by dividing the corpus into four parts,
and employing a leave one out procedure (training
the acoustic and language models on three parts of
the corpus and testing on the fourth, rotating to ob-
tain recognition results for the full corpus). This
resulted in an average word error rate (WER) of
29.5%. Automatic segmentation into dialogue acts
or sentence boundaries was not performed: the dia-
logue act boundaries for the manual transcripts were
mapped on to the speech recognition output.
3.1 Description of the Evaluation Schemes
A particular interest in our research is how automatic
measures of informativeness correlate with human
judgments on the same criteria. During the devel-
opment stage of a summarization system it is not
feasible to employ many hours of manual evalua-
tions, and so a critical issue is whether or not soft-
ware packages such as ROUGE are able to measure
informativeness in a way that correlates with subjec-
tive summarization evaluations.
35
3.1.1 ROUGE
Gauging informativeness has been the focus
of automatic summarization evaluation research.
We used the ROUGE evaluation approach (Lin
and Hovy, 2003), which is based on n-gram co-
occurrence between machine summaries and ?ideal?
human summaries. ROUGE is currently the stan-
dard objective evaluation measure for the Document
Understanding Conference 1; ROUGE does not as-
sume that there is a single ?gold standard? summary.
Instead it operates by matching the target summary
against a set of reference summaries. ROUGE-1
through ROUGE-4 are simple n-gram co-occurrence
measures, which check whether each n-gram in the
reference summary is contained in the machine sum-
mary. ROUGE-L and ROUGE-W are measures of
common subsequences shared between two sum-
maries, with ROUGE-W favoring contiguous com-
mon subsequences. Lin (Lin and Hovy, 2003) has
found that ROUGE-1 and ROUGE-2 correlate well
with human judgments.
3.1.2 Human Evalautions
The subjective evaluation portion of our research
utilized 5 judges who had little or no familiarity with
the content of the ICSI meetings. Each judge eval-
uated 10 summaries per meeting, for a total of sixty
summaries. In order to familiarize themselves with
a given meeting, they were provided with a human
abstract of the meeting and the full transcript of the
meeting with links to the audio. The human judges
were instructed to read the abstract, and to consult
the full transcript and audio as needed, with the en-
tire familiarization stage not to exceed 20 minutes.
The judges were presented with 12 questions at
the end of each summary, and were instructed that
upon beginning the questionnaire they should not re-
consult the summary itself. 6 of the questions re-
garded informativeness and 6 involved readability
and coherence, though our current research concen-
trates on the informativeness evaluations. The eval-
uations used a Likert scale based on agreement or
disagreement with statements, such as the following
Informativeness statements:
1. The important points of the meeting are repre-
sented in the summary.
1http://duc.nist.gov/
2. The summary avoids redundancy.
3. The summary sentences on average seem rele-
vant.
4. The relationship between the importance of
each topic and the amount of summary space
given to that topic seems appropriate.
5. The summary is repetitive.
6. The summary contains unnecessary informa-
tion.
Statements such as 2 and 5 above are measuring
the same impressions, with the polarity of the state-
ments merely reversed, in order to better gauge the
reliability of the answers. The readability/coherence
portion consisted of the following statements:
1. It is generally easy to tell whom or what is be-
ing referred to in the summary.
2. The summary has good continuity, i.e. the sen-
tences seem to join smoothly from one to an-
other.
3. The individual sentences on average are clear
and well-formed.
4. The summary seems disjointed.
5. The summary is incoherent.
6. On average, individual sentences are poorly
constructed.
It was not possible in this paper to gauge how
responses to these readability statements correlate
with automatic metrics, for the reason that auto-
matic metrics of readability and coherence have not
been widely discussed in the field of summariza-
tion. Though subjective evaluations of summaries
are often divided into informativeness and readabil-
ity questions, only automatic metrics of informative-
ness have been investigated in-depth by the summa-
rization community. We believe that the develop-
ment of automatic metrics for coherence and read-
ability should be a high priority for researchers in
summarization evaluation and plan on pursuing this
avenue of research. For example, work on coher-
ence in NLG (Lapata, 2003) could potentially in-
form summarization evaluation. Mani (Mani et al,
36
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
ROUGE-1-MANROUGE-2-MANROUGE-L-MANROUGE-1-ASRROUGE-2-ASRROUGE-L-ASR
Figure 1: ROUGE Scores for the Summarization Ap-
proaches
1999) is one of the few papers to have discussed
measuring summary readability automatically.
4 Results
The results of these experiments can be analyzed
in various ways: significant differences of ROUGE
results across summarization approaches, deterio-
ration of ROUGE results on ASR versus manual
transcripts, significant differences of human eval-
uations across summarization approaches, deterio-
ration of human evaluations on ASR versus man-
ual transcripts, and finally, the correlation between
ROUGE and human evaluations.
4.1 ROUGE results across summarization
approaches
All of the machine summaries were 10% of the orig-
inal document length, in terms of the number of di-
alogue acts contained. Of the four approaches to
summarization used herein, the latent semantic anal-
ysis method performed the best on every meeting
tested for every ROUGE measure with the excep-
tion of ROUGE-3 and ROUGE-4. This approach
was significantly better than either feature-based ap-
proach (p<0.05), but was not a significant improve-
ment over MMR. For ROUGE-3 and ROUGE-4,
none of the summarization approaches were signifi-
cantly different from each other, owing to data spar-
sity. Figure 1 gives the ROUGE-1, ROUGE-2 and
ROUGE-L results for each of the summarization ap-
proaches, on both manual and ASR transcripts.
4.1.1 ASR versus Manual
The results of the four summarization approaches
on ASR output were much the same, with LSA and
MMR being comparable to each other, and each of
them outperforming the feature-based approaches.
On ASR output, LSA again consistently performed
the best.
Interestingly, though the LSA approach scored
higher when using manual transcripts than when
using ASR transcripts, the difference was small and
insignificant despite the nearly 30% WER of the
ASR. All of the summarization approaches showed
minimal deterioration when used on ASR output
as compared to manual transcripts, but the LSA
approach seemed particularly resilient, as evidenced
by Figure 1. One reason for the relatively small
impact of ASR output on summarization results is
that for each of the 6 meetings, the WER of the
summaries was lower than the WER of the meeting
as a whole. Similarly, Valenza et al(Valenza et
al., 1999) and Zechner and Waibel (Zechner and
Waibel, 2000) both observed that the WER of
extracted summaries was significantly lower than
the overall WER in the case of broadcast news. The
table below demonstrates the discrepancy between
summary WER and meeting WER for the six
meetings used in this research.
Meeting Summary WER Meeting WER
Bed004 27.0 35.7
Bed009 28.3 39.8
Bed016 39.6 49.8
Bmr005 23.9 36.1
Bmr019 28.0 36.5
Bro018 25.9 35.6
WER% for Summaries and Meetings
There was no improvement in the second feature-
based approach (adding an LSA sentence score) as
compared with the first feature-based approach. The
sentence score used here relied on a reduction to 300
dimensions, which may not have been ideal for this
data.
The similarity between the MMR and LSA ap-
proaches here mirrors Gong and Liu?s findings, giv-
ing credence to the claim that LSA maximizes rele-
vance and minimizes redundancy, in a different and
more opaque manner then MMR, but with similar
37
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 5.03 4.53 4.67 4.83
NO REDUN. 4.33 2.60 3.00 3.77
RELEVANT 4.83 4.07 4.33 4.53
TOPIC SPACE 4.43 3.83 3.87 4.30
REPETITIVE 3.37 4.70 4.60 3.83
UNNEC. INFO. 4.70 6.00 5.83 5.00
Table 1: Human Scores for 4 Approaches on Manual
Transcripts
results. Regardless of whether or not the singular
vectors of V T can rightly be thought of as topics or
concepts (a seemingly strong claim), the LSA ap-
proach was as successful as the more popular MMR
algorithm.
4.2 Human results across summarization
approaches
Table 1 presents average ratings for the six state-
ments across four summarization approaches on
manual transcripts. Interestingly, the first feature-
based approach is given the highest marks on each
criterion. For statements 2, 5 and 6 FB1 is signif-
icantly better than the other approaches. It is par-
ticularly surprising that FB1 would score well on
statement 2, which concerns redundancy, given that
MMR and LSA explicitly aim to reduce redundancy
while the feature-based approaches are merely clas-
sifying utterances as relevant or not. The second
feature-based approach was not significantly worse
than the first on this score.
Considering the difficult task of evaluating ten ex-
tractive summaries per meeting, we are quite satis-
fied with the consistency of the human judges. For
example, statements that were merely reworded ver-
sions of other statements were given consistent rat-
ings. It was also the case that, with the exception
of evaluating the sixth statement, judges were able
to tell that the manual extracts were superior to the
automatic approaches.
4.2.1 ASR versus Manual
Table 2 presents average ratings for the six state-
ments across four summarization approaches on
ASR transcripts. The LSA and MMR approaches
performed better in terms of having less deteri-
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 3.53 4.13 3.73 3.50
NO REDUN. 3.40 2.97 2.63 3.57
RELEVANT 3.47 3.57 3.00 3.47
TOPIC SPACE 3.27 3.33 3.00 3.20
REPETITIVE 4.43 4.73 4.70 4.20
UNNEC. INFO. 5.37 6.00 6.00 5.33
Table 2: Human Scores for 4 Approaches on ASR
Transcripts
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-1-MANHUMAN-1-ASR
Figure 2: INFORMATIVENESS-1 Scores for the
Summarization Approaches
oration of scores when used on ASR output in-
stead of manual transcripts. LSA-ASR was not
significantly worse than LSA on any of the 6 rat-
ings. MMR-ASR was significantly worse than
MMR on only 3 of the 6. In contrast, FB1-
ASR was significantly worse than FB1 for 5 of
the 6 approaches, reinforcing the point that MMR
and LSA seem to favor extracting utterances with
fewer errors. Figures 2, 3 and 4 depict the
how the ASR and manual approaches affect the
INFORMATIVENESS-1, INFORMATIVENESS-4
and INFORMATIVENESS-6 ratings, respectively.
Note that for Figure 6, a higher score is a worse rat-
ing.
4.3 ROUGE and Human correlations
According to (Lin and Hovy, 2003), ROUGE-
1 correlates particularly well with human judg-
ments of informativeness. In the human eval-
uation survey discussed here, the first statement
(INFORMATIVENESS-1) would be expected to
correlate most highly with ROUGE-1, as it is ask-
38
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-4-MANHUMAN-4-ASR
Figure 3: INFORMATIVENESS-4 Scores for the
Summarization Approaches
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-6-MANHUMAN-6-ASR
Figure 4: INFORMATIVENESS-6 Scores for the
Summarization Approaches
ing whether the summary contains the important
points of the meeting. As could be guessed from the
discussion above, there is no significant correlation
between ROUGE-1 and human evaluations when
analyzing only the 4 summarization approaches
on manual transcripts. However, when looking
at the 4 approaches on ASR output, ROUGE-1
and INFORMATIVENESS-1 have a moderate and
significant positive correlation (Spearman?s rho =
0.500, p < 0.05). This correlation on ASR out-
put is strong enough that when ROUGE-1 and
INFORMATIVENESS-1 scores are tested for corre-
lation across all 8 summarization approaches, there
is a significant positive correlation (Spearman?s rho
= 0.388, p < 0.05).
The other significant correlations for ROUGE-
1 across all 8 summarization approaches are with
INFORMATIVENESS-2, INFORMATIVENESS-5
and INFORMATIVENESS-6. However, these are
negative correlations. For example, with regard to
INFORMATIVENESS-2, summaries that are rated
as having a high level of redundancy are given high
ROUGE-1 scores, and summaries with little redun-
dancy are given low ROUGE-1 scores. Similary,
with regard to INFORMATIVENESS-6, summaries
that are said to have a great deal of unnecessary in-
formation are given high ROUGE-1 scores. It is
difficult to interpret some of these negative correla-
tions, as ROUGE does not measure redundancy and
would not necessarily be expected to correlate with
redundancy evaluations.
5 Discussion
In general, ROUGE did not correlate well with the
human evaluations for this data. The MMR and
LSA approaches were deemed to be significantly
better than the feature-based approaches according
to ROUGE, while these findings were reversed ac-
cording to the human evaluations. An area of agree-
ment, however, is that the LSA-ASR and MMR-
ASR approaches have a small and insignificant de-
cline in scores compared with the decline of scores
for the feature-based approaches. One of the most
interesting findings of this research is that MMR and
LSA approaches used on ASR tend to select utter-
ances with fewer ASR errors.
ROUGE has been shown to correlate well with
human evaluations in DUC, when used on news cor-
pora, but the summarization task here ? using con-
versational speech from meetings ? is quite different
from summarizing news articles. ROUGE may sim-
ply be less applicable to this domain.
6 Future Work
It remains to be determined through further ex-
perimentation by researchers using various corpora
whether or not ROUGE truly correlates well with
human judgments. The results presented above are
mixed in nature, but do not present ROUGE as being
sufficient in itself to robustly evaluate a summariza-
tion system under development.
We are also interested in developing automatic
metrics of coherence and readability. We now have
human evaluations of these criteria and are ready to
39
begin testing for correlations between these subjec-
tive judgments and potential automatic metrics.
7 Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for the speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication).
References
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. ACM SIGIR,
pages 335?336.
Y. Gong and X. Liu. 2001. Generic text summarization
using relevance measure and latent semantic analysis.
In Proc. ACM SIGIR, pages 19?25.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, I.Mc.Cowan, J.Vepa, and
S.Renals. 2005. An investigation into transcription of
conference room meetings. Submitted to Eurospeech.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. IEEE ICASSP.
J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In ACM SIGIR ?95, pages 68?
73.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In ACL, pages 545?
552.
C.-Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL.
Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999.
Improving summaries by revising them. In Proceed-
ings of the 37th conference on Association for Compu-
tational Linguistics, pages 558?565, Morristown, NJ,
USA. Association for Computational Linguistics.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. Submitted to
Eurospeech.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proc. 5th SIGdial Workshop on
Discourse and Dialogue, pages 97?100.
J. Steinberger and K. Jez?ek. 2004. Using latent semantic
analysis in text summarization and summary evalua-
tion. In Proc. ISIM ?04, pages 93?100.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proc. ESCA Workshop on Ac-
cessing Information in Spoken Audio, pages 111?116.
Pierre Wellner, Mike Flynn, Simon Tucker, and Steve
Whittaker. 2005. A meeting browser evaluation test.
In CHI ?05: CHI ?05 extended abstracts on Human
factors in computing systems, pages 2021?2024, New
York, NY, USA. ACM Press.
K. Zechner and A. Waibel. 2000. Minimizing word error
rate in textual summaries of spoken language. In Proc.
NAACL-2000.
40
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 1?7,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Dimensionality Reduction Aids Term Co-Occurrence Based
Multi-Document Summarization
Ben Hachey, Gabriel Murray & David Reitter
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
bhachey@inf.ed.ac.uk, gabriel.murray@ed.ac.uk, dreitter@inf.ed.ac.uk
Abstract
A key task in an extraction system for
query-oriented multi-document summari-
sation, necessary for computing relevance
and redundancy, is modelling text seman-
tics. In the Embra system, we use a repre-
sentation derived from the singular value
decomposition of a term co-occurrence
matrix. We present methods to show the
reliability of performance improvements.
We find that Embra performs better with
dimensionality reduction.
1 Introduction
We present experiments on the task of query-
oriented multi-document summarisation as ex-
plored in the DUC 2005 and DUC 2006 shared
tasks, which aim to model real-world complex
question-answering. Input consists of a detailed
query1 and a set of 25 to 50 relevant docu-
ments. We implement an extractive approach
where pieces of the original texts are selected to
form a summary and then smoothing is performed
to create a discursively coherent summary text.
The key modelling task in the extraction phase
of such a system consists of estimating responsive-
ness to the query and avoiding redundancy. Both
of these are often approached through some tex-
tual measure of semantic similarity. In the Embra2
system, we follow this approach in a sentence ex-
traction framework. However, we model the se-
mantics of a sentence using a very large distri-
butional semantics (i.e. term co-occurrence) space
reduced by singular value decomposition. Our hy-
1On average, queries contain approximately 34 words
words and three sentences.
2Edinburgh Multi-document Breviloquence Assay
pothesis is that this dimensionality reduction us-
ing a large corpus can outperform a simple term
co-occurrence model.
A number of papers in the literature look at sin-
gular value decomposition and compare it to unre-
duced term ? document or term co-occurrence
matrix representations. These explore varied tasks
and obtain mixed results. For example, Peder-
sen et al (2005) find that SVD does not improve
performance in a name discrimination task while
Matveeva et al (2005) and Rohde et al (In prep)
find that dimensionality reduction with SVD does
help on word similarity tasks.
The experiments contained herein investigate
the contribution of singular value decomposition
on the query-oriented multi-document summarisa-
tion task. We compare the singular value decom-
position of a term co-occurrence matrix derived
from a corpus of approximately 100 million words
(DS+SVD) to an unreduced version of the matrix
(DS). These representations are described in Sec-
tion 2. Next, Section 3 contains a discussion of
related work using SVD for summarisation and a
description of the sentence selection component in
the Embra system. The paper goes on to give an
overview of the experimental design and results in
Section 4. This includes a detailed analysis of the
statistical significance of the results.
2 Representing Sentence Semantics
The following three subsections discuss various
ways of representing sentence meaning for infor-
mation extraction purposes. While the first ap-
proach relies solely on weighted term frequencies
in a vector space, the subsequent methods attempt
to use term context information to better represent
the meanings of sentences.
1
2.1 Terms and Term Weighting (TF.IDF)
The traditional model for measuring semantic sim-
ilarity in information retrieval and text mining is
based on a vector representation of the distribution
of terms in documents. Within the vector space
model, each term is assigned a weight which sig-
nifies the semantic importance of the term. Often,
tf.idf is used for this weight, which is a scheme
that combines the importance of a term within the
current document3 and the distribution of the term
across the text collection. The former is often
represented by the term frequency and the latter
by the inverse document frequency (idfi = Ndfi ),
where N is the number of documents and dfi is
the number of documents containing term ti.
2.2 Term Co-occurrence (DS)
Another approach eschews the traditional vector
space model in favour of the distributional seman-
tics approach. The DS model is based on the in-
tuition that two words are semantically similar if
they appear in a similar set of contexts. We can
obtain a representation of a document?s semantics
by averaging the context vectors of the document
terms. (See Besanc?on et al (1999), where the DS
model is contrasted with a term ? document vec-
tor space representation.)
2.3 Singular Value Decomposition
(DS+SVD)
Our third approach uses dimensionality reduction.
Singular value decomposition is a technique for
dimensionality reduction that has been used ex-
tensively for the analysis of lexical semantics un-
der the name of latent semantic analysis (Landauer
et al, 1998). Here, a rectangular (e.g., term ?
document) matrix is decomposed into the product
of three matrices (Xw?p = Ww?nSn?n(Pp?n)T )
with n ?latent semantic? dimensions. W and P
represent terms and documents in the new space.
And S is a diagonal matrix of singular values in
decreasing order.
Taking the product Ww?kSk?k(Pp?k)T over
the first k columns gives the best least square ap-
proximation of the original matrix X by a matrix
of rank k, i.e. a reduction of the original matrix to
k dimensions. Similarity between documents can
then be computed in the space obtained by taking
the rank k product of S and P .
3The local importance of a term can also be computed
over other textual units, e.g. sentence in extractive summari-
sation or the context of an entity pair in relation discovery.
This decomposition abstracts away from terms
and can be used to model a semantic similarity
that is more linguistic in nature. Furthermore, it
has been successfully used to model human intu-
itions about meaning. For example, Landauer et
al. (1998) show that latent semantic analysis cor-
relates well with human judgements of word sim-
ilarity and Foltz (1998) shows that it is a good es-
timator for textual coherence.
It is hoped that these latter two techniques (di-
mensionality reduction and the DS model) will
provide for a more robust representation of term
contexts and therefore better representation of sen-
tence meaning, enabling us to achieve more reli-
able sentence similarity measurements for extrac-
tive summarisation.
3 SVD in Summarisation
This section describes ways in which SVD has
been used for summarisation and details the im-
plementation in the Embra system.
3.1 Related Work
In seminal work by Gong and Liu (2001), the au-
thors proposed that the rows of P T may be re-
garded as defining topics, with the columns rep-
resenting sentences from the document. In their
SVD method, summarisation proceeds by choos-
ing, for each row in P T , the sentence with the
highest value. This process continues until the de-
sired summary length is reached.
Steinberger and Jez?ek (2004) have offered two
criticisms of the Gong and Liu approach. Firstly,
the method described above ties the dimension-
ality reduction to the desired summary length.
Secondly, a sentence may score highly but never
?win? in any dimension, and thus will not be ex-
tracted despite being a good candidate. Their solu-
tion is to assign each sentence an SVD-based score
using:
ScSV Di =
?
?
?
?
n?
i=1
v(i, k)2 ? ?(k)2 ,
where v(i, k) is the kth element of the ith sen-
tence vector and ?(k) is the corresponding singu-
lar value.
Murray et al (2005a) address the same concerns
but retain the Gong and Liu framework. Rather
than extracting the best sentence for each topic,
the n best sentences are extracted, with n deter-
mined by the corresponding singular values from
2
matrix S. Thus, dimensionality reduction is no
longer tied to summary length and more than one
sentence per topic can be chosen.
A similar approach in DUC 2005 using term
co-occurrence models and SVD was presented by
Jagarlamudi et al (2005). Their system performs
SVD over a term ? sentence matrix and combines
a relevance measurement based on this representa-
tion with relevance based on a term co-occurrence
model by a weighted linear combination.
3.2 Sentence Selection in Embra
The Embra system developed for DUC 2005 at-
tempts to derive more robust representations of
sentences by building a large semantic space us-
ing SVD on a very large corpus. While researchers
have used such large semantic spaces to aid in au-
tomatically judging the coherence of documents
(Foltz et al, 1998; Barzilay and Lapata, 2005), to
our knowledge this is a novel technique in sum-
marisation.
Using a concatenation of Aquaint and DUC
2005 data (100+ million words), we utilised the
Infomap tool4 to build a semantic model based on
singular value decomposition (SVD). The decom-
position and projection of the matrix to a lower-
dimensionality space results in a semantic model
based on underlying term relations. In the current
experiments, we set dimension of the reduced rep-
resentation to 100. This is a reduction of 90% from
the full dimensionality of 1000 content-bearing
terms in the original DS matrix. This was found
to perform better than 25, 50, 250 and 500 dur-
ing parameter optimisation. A given sentence is
represented as a vector which is the average of its
constituent word vectors. This sentence represen-
tation is then fed into an MMR-style algorithm.
MMR (Maximal Marginal Relevance) is a com-
mon approach for determining relevance and re-
dundancy in multi-document summarisation, in
which candidate sentences are represented as
weighted term-frequency vectors which can thus
be compared to query vectors to gauge similarity
and already-extracted sentence vectors to gauge
redundancy, via the cosine of the vector pairs
(Carbonell and Goldstein, 1998). While this has
proved successful to a degree, the sentences are
represented merely according to weighted term
frequency in the document, and so two similar sen-
tences stand a chance of not being considered sim-
4http://infomap.stanford.edu/
for each sentence in document:
for each word in sentence:
get word vector from semantic model
average word vectors to form sentence vector
sim1 = cossim(sentence vector, query vector)
sim2 = highest(cossim(sentence vector, all extracted vectors))
score = ?*sim1 - (1-?)*sim2
extract sentence with highest score
repeat until desired length
Figure 1: Sentence extraction algorithm
ilar if they do not share the same terms.
Our implementation of MMR (Figure 1) uses ?
annealing following (Murray et al, 2005a). ? de-
creases as the summary length increases, thereby
emphasising relevance at the outset but increas-
ingly prioritising redundancy removal as the pro-
cess continues.
4 Experiment
The experimental setup uses the DUC 2005 data
(Dang, 2005) and the Rouge evaluation met-
ric to explore the hypothesis that query-oriented
multi-document summarisation using a term co-
occurrence representation can be improved using
SVD. We frame the research question as follows:
Does SVD dimensionality reduction
lead to an increase in Rouge score com-
pared to the DS representation?
4.1 Materials
The DUC 2005 task5 was motivated by Amigo et
al.?s (2004) suggestion of evaluations that model
real-world complex question answering. The goal
is to synthesise a well-organised, fluent answer of
no more than 250 words to a complex question
from a set of 25 to 50 relevant documents. The
data includes a detailed query, a document set, and
at least 4 human summaries for each of 50 topics.
The preprocessing was largely based on LT TTT
and LT XML tools (Grover et al, 2000; Thomp-
son et al, 1997). First, we perform tokenisation
and sentence identification. This is followed by
lemmatisation.
At the core of preprocessing is the LT TTT
program fsgmatch, a general purpose transducer
which processes an input stream and adds annota-
tions using rules provided in a hand-written gram-
mar file. We also use the statistical combined part-
of-speech (POS) tagger and sentence boundary
disambiguation module from LT TTT (Mikheev,
5http://www-nlpir.nist.gov/projects/
duc/duc2005/tasks.html
3
1997). Using these tools, we produce an XML
markup with sentence and word elements. Further
linguistic markup is added using the morpha lem-
matiser (Minnen et al, 2000) and the C&C named
entity tagger (Curran and Clark, 2003) trained on
the data from MUC-7.
4.2 Methods
The different system configurations (DS,
DS+SVD, TF.IDF) were evaluated against
the human upper bound and a baseline using
Rouge-2 and Rouge-SU4. Rouge estimates the
coverage of appropriate concepts (Lin and Hovy,
2003) in a summary by comparing it several
human-created reference summaries. Rouge-2
does so by computing precision and recall based
on macro-averaged bigram overlap. Rouge-SU4
allows bigrams to be composed of non-contiguous
words, with as many as four words intervening.
We use the same configuration as the official DUC
2005 evaluation,6 which is based on word stems
(rather than full forms) and uses jackknifing (k?1
cross-evaluation) so that human gold-standard and
automatic system summaries can be compared.
The independent variable in the experiment is
the model of sentence semantics used by the sen-
tence selection algorithm. We are primarily inter-
ested in the relative performance of the DS and
DS+SVD representations. As well as this, we
include the DUC 2005 baseline, which is a lead
summary created by taking the first 250 words of
the most recent document for each topic. We also
include a tf.idf -weighted term ? sentence repre-
sentation (TF.IDF) for comparison with a conven-
tional MMR approach.7 Finally, we include an up-
per bound calculated using the DUC 2005 human
reference summaries. Preprocessing and all other
aspects of the sentence selection algorithm remain
constant over all systems.
In general, Rouge shows a large variance across
data sets (and so does system performance). It is
important to test whether obtained nominal differ-
ences are due to chance or are actually statistically
significant.
To test whether the Rouge metric showed a re-
liably different performance for the systems, the
6i.e. ROUGE-1.5.5.pl -n 2 -x -m -2 4 -u
-c 95 -r 1000 -f A -p 0.5 -t 0 d
7Specifically, we use tfi,j ? log( Ndfi ) for term weighting
where tfi,j is the number of times term i occurs in sentence
j, N is the number of sentences, and dfi is the number of
sentences containing term i.
p Metric hypothesis
0.000262 Rouge-2 base<TF.IDF ***
0.021640 Rouge-2 base<DS *
0.000508 Rouge-2 base<DS+SVD ***
0.014845 Rouge-2 DS<TF.IDF *
0.507702 Rouge-2 TF.IDF<DS+SVD
0.047016 Rouge-2 DS<DS+SVD *
0.000080 Rouge-SU4 base<TF.IDF ***
0.006803 Rouge-SU4 base<DS **
0.000006 Rouge-SU4 base<DS+SVD ***
0.012815 Rouge-SU4 DS<TF.IDF *
0.320083 Rouge-SU4 TF.IDF<DS+SVD
0.001053 Rouge-SU4 DS<DS+SVD **
Table 1: Holm-corrected Wilcoxon hypothesis test
results.
Friedman rank sum test (Friedman, 1940; Dems?ar,
2006) can be used. This is a hypothesis test not
unlike an ANOVA, however, it is non-parametric,
i.e. it does not assume a normal distribution of
the measures (i.e. precision, recall and F-score).
More importantly, it does not require homogene-
ity of variances.
To (partially) rank the systems against each
other, we used a cascade of Wilcoxon signed ranks
tests. These tests are again non-parametric (as they
rank the differences between the system results for
the datasets). As discussed by Dems?ar (2006), we
used Holm?s procedure for multiple tests to correct
our error estimates (p).
4.3 Results
Friedman tests for each Rouge metric (with
F-score, precision and recall included as ob-
servations, with the dataset as group) showed
a reliable effect of the system configuration
(?2F,SU4 = 106.6, ?2P,SU4 = 96.1,
?2R,SU4 = 105.5, all p < 0.00001).
Post-hoc analysis (Wilcoxon) showed (see Ta-
ble 1) that all three systems performed reliably
better than the baseline. TF.IDF performed bet-
ter than simple DS in Rouge-2 and Rouge-SU4.
DS+SVD performed better than DS (p2 < 0.05,
pSU4 < 0.005). There is no evidence to support
a claim that DS+SVD performed differently from
TF.IDF.
However, when we specifically compared the
performance of TF.IDF and DS+SVD with the
Rouge-SU4 F score for only the specific (as
opposed to general) summaries, we found that
DS+SVD scored reliably, but only slightly better
4
baseline
TF.IDF MMR
DS MMR
DS+SVD MMR
Human
Mean Rouge F?Scores          
0.00 0.05 0.10 0.15
Figure 2: Mean system performance over 50
datasets (F-scores). Precision and Recall look
qualitatively similar.
(Wilcoxon, p<0.05). This result is unadjusted,
and post-hoc comparisons with other scores or for
the general summaries did not show reliable dif-
ferences.
Having established the reliable performance im-
provement of DS+SVD over DS, it it important
to take the effect size into consideration (with
enough data, small effects may be statistically sig-
nificant, but practically unimportant). Figure 2 il-
lustrates that the gain in mean performance is sub-
stantial. If the mean Rouge-SU4 score for human
performance is seen as upper bound, the DS+SVD
system showed a 25.4 percent reduction in error
compared to the DS system.8
A similar analysis for precision and recall gives
qualitatively comparable results.
5 Discussion and Future Work
The positive message from the experimental re-
sults is that SVD dimensionality reduction im-
proves performance over a term co-occurrence
model for computing relevance and redundancy in
a MMR framework. We note that we cannot con-
clude that the DS or DS+SVD systems outper-
form a conventional tf.idf -weighted term ? sen-
tence representation on this task. However, results
from Jagarlamudi et al (2005) suggest that the DS
and term ? sentence representations may be com-
plementary in which case we would expect a fur-
ther improvement through an ensemble technique.
Previous results comparing SVD with unre-
duced representations show mixed results. For
example, Pedersen et al (2005) experiment with
term co-occurrence representations with and with-
out SVD on a name discrimination task and find
8Pairwise effect size estimates over datasets aren?t sensi-
ble. Averaging of differences between pairs was affected by
outliers, presumably caused by Rouge?s error distribution.
that the unreduced representation tends to perform
better. Rohde et al (In prep), on the other hand,
find that a reduced matrix does perform better on
word pair similarity and multiple-choice vocabu-
lary tests. One crucial factor here may be the size
of the corpus. SVD may not offer any reliable ?la-
tent semantic? advantage when the corpus is small,
in which case the efficiency gain from dimension-
ality reduction is less of a motivation anyway.
We plan to address the question of corpus size
in future work by comparing DS and DS+SVD
derived from corpora of varying size. We hypoth-
esise that the larger the corpus used to compile
the term co-occurrence information, the larger the
potential contribution from dimensionality reduc-
tion. This will be explored by running the experi-
ment described in this paper a number of times us-
ing corpora of different sizes (e.g. 0.5m, 1m, 10m
and 100m words).
Unlike official DUC evaluations, which rely on
human judgements of readability and informative-
ness, our experiments rely solely on Rouge n-
gram evaluation metrics. It has been shown in
DUC 2005 and in work by Murray et al (2005b;
2006) that Rouge does not always correlate well
with human evaluations, though there is more sta-
bility when examining the correlations of macro-
averaged scores. Rouge suffers from a lack of
power to discriminate between systems whose per-
formance is judged to differ by human annotators.
Thus, it is likely that future human evaluations
would be more informative. Another way that the
evaluation issue might be addressed is by using an
annotated sentence extraction corpus. This could
proceed by comparing gold standard alignments
between abstract and full document sentences with
predicted alignments using correlation analysis.
6 Conclusions
We have presented experiments with query-
oriented multi-document summarisation. The ex-
periments explore the question of whether SVD
dimensionality reduction offers any improvement
over a term co-occurrence representation for sen-
tence semantics for measuring relevance and re-
dundancy. While the experiments show that
our system does not outperform a term ? sen-
tence tf.idf system, we have shown that the SVD
reduced representation of a term co-occurrence
space built from a large corpora performs better
than the unreduced representation. This contra-
5
dicts related work where SVD did not provide
an improvement over unreduced representations
on the name discrimination task (Pedersen et al,
2005). However, it is compatible with other work
where SVD has been shown to help on the task
of estimating human notions of word similarity
(Matveeva et al, 2005; Rohde et al, In prep).
A detailed analysis using the Friedman test and
a cascade of Wilcoxon signed ranks tests suggest
that our results are statistically valid despite the
unreliability of the Rouge evaluation metric due to
its low variance across systems.
Acknowledgements
This work was supported in part by Scottish Enter-
prise Edinburgh-Stanford Link grant R36410 and,
as part of the EASIE project, grant R37588. It
was also supported in part by the European Union
6th FWP IST Integrated Project AMI (Augmented
Multiparty Interaction, FP6-506811, publication).
We would like to thank James Clarke for de-
tailed comments and discussion. We would also
like to thank the anonymous reviewers for their
comments.
References
Enrique Amigo, Julio Gonzalo, Victor Peinado,
Anselmo Penas, and Felisa Verdejo. 2004. An
empirical study of information synthesis tasks. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, Barcelona,
Spain.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor, MI,
USA.
Romaric Besanc?on, Martin Rajman, and Jean-Ce?dric
Chappelier. 1999. Textual similarities based on
a distributional approach. In Proceedings of the
10th International Workshop on Database And Ex-
pert Systems Applications, Firenze, Italy.
Jaime G. Carbonell and Jade Goldstein. 1998. The
use of mmr, diversity-based reranking for reordering
documents and producing summaries. In Proceed-
ings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, Melbourne, Australia.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tag-
ger. In Proceedings of the 2003 Conference on Com-
putational Natural Language Learning, Edmonton,
Canada.
Hoa T. Dang. 2005. Overview of DUC 2005. In
Proceedings of the Document Understanding Con-
ference, Vancouver, B.C., Canada.
Janez Dems?ar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30, Jan.
Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse Processes,
25.
Milton Friedman. 1940. A comparison of alternative
tests of significance for the problem of m rankings.
The Annals of Mathematical Statistics, 11:86?92.
Yihon Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, New Orleans,
LA, USA.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. LT TTT?a flexible tokeni-
sation tool. In Proceedings of the 2nd International
Conference on Language Resources and Evaluation,
Athens, Greece.
Ben Hachey and Claire Grover. 2004. A rhetorical
status classifier for legal text summarisation. In
Proceedings of the ACL-2004 Text Summarization
Branches Out Workshop, Barcelona, Spain.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva
Varma. 2005. A relevance-based language mod-
eling approach to DUC 2005. In Proceedings of
the Document Understanding Conference, Vancou-
ver, B.C., Canada.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to latent semantic analysis.
Discourse Processes, 25.
Chin-Yew Lin and Eduard H. Hovy. 2003. Au-
tomatic evaluation of summaries using n-gram
co-occurrence statistics. In Proceedings of the
Joint Human Language Technology Conference and
North American Chapter of the Association for
Computational Linguistics Annual Meeting, Edmon-
ton, Alberta, Canada.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat,
and Christiaan Royer. 2005. Term represetation
with generalized latent semantic analysis. In Pro-
ceedings of the 2005 Conference on Recent Ad-
vances in Natural Language Processing, Borovets,
Bulgaria.
Andrei Mikheev. 1997. Automatic rule induction for
unknown word guessing. Computational Linguis-
tics, 23(3).
6
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference, Mitzpe Ramon, Israel.
Gabriel Murray, Steve Renals, and Jean Carletta.
2005a. Extractive summarization of meeting record-
ings. In Proceedings of the 9th European Con-
ference on Speech Communication and Technology,
Lisbon, Portugal.
Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2005b. Evaluating automatic sum-
maries of meeting recordings. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, MI, USA.
Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2006. Incorporating speaker and
discourse features into speech summarization. In
Proceedings of the Joint Human Language Technol-
ogy Conference and North American Chapter of the
Association for Computational Linguistics Annual
Meeting, New York City, NY, USA.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering simi-
lar contexts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Com-
putational Linguistics, Mexico City, Mexico.
Douglas L. T. Rohde, Laur M. Gonnerman, and
David C. Plaut. In prep. An improved
method for deriving word meaning from lexical
co-occurrence. http://dlt4.mit.edu/?dr/
COALS/Coals.pdf (1 May 2006).
Josef Steinberger and Karel Jez?ek. 2004. Using latent
semantic analysis in text summarization and sum-
mary evaluation. In Proceedings of the 5th Inter-
national Conference on Information Systems Imple-
mentation and Modelling, Ostrava, Czech Republic.
Henry Thompson, Richard Tobin, David McK-
elvie, and Chris Brew. 1997. LT XML:
Software API and toolkit for XML processing.
http://www.ltg.ed.ac.uk/software/.
7
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 1?7,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Prosodic Correlates of Rhetorical Relations
Gabriel Murray
Centre for Speech Technology Research
University of Edinburgh
Edinburgh EH8 9LW
gabriel.murray@ed.ac.uk
Maite Taboada
Dept. of Linguistics
Simon Fraser University
Vancouver V5A 1S6
mtaboada@sfu.ca
Steve Renals
Centre for Speech Technology Research
University of Edinburgh
Edinburgh EH8 9LW
s.renals@ed.ac.uk
Abstract
This paper investigates the usefulness of
prosodic features in classifying rhetori-
cal relations between utterances in meet-
ing recordings. Five rhetorical relations
of contrast, elaboration, summary, ques-
tion and cause are explored. Three train-
ing methods - supervised, unsupervised,
and combined - are compared, and classi-
fication is carried out using support vector
machines. The results of this pilot study
are encouraging but mixed, with pairwise
classification achieving an average of 68%
accuracy in discerning between relation
pairs using only prosodic features, but
multi-class classification performing only
slightly better than chance.
1 Introduction
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) attempts to describe a given text
in terms of its coherence, i.e. how it is that the parts
of the text are related to one another and how each
part plays a role. Two adjacent text spans will of-
ten exhibit a nucleus-satellite relationship, where the
satellite plays a role that is relative to the nucleus.
For example, one sentence might make a claim and
the following sentence give evidence for the claim,
with the second sentence being a satellite and the
evidence relation existing between the two spans.
In a text containing many sentences, these nucleus-
satellite pairs can be built up to produce a document-
wide rhetorical tree. Figure 1 gives an example of a
rhetorical tree for a three-sentence text1.
Theories such as RST have been popular for some
time as a way of describing the multi-levelled rhetor-
ical relations that exist in text, with relevant appli-
cations such as automatic summarization (Marcu,
1997) and natural language generation (Knott and
Dale, 1996). However, implementing automatic
rhetorical parsers has been a problematic area of
research. Techniques that rely heavily on explicit
signals, such as discourse markers, are of limited
use both because only a small percentage of rhetori-
cal relations are signalled explicitly and because ex-
plicit markers can be ambiguous. RST trees are bi-
nary branching trees distinguishing between nuclei
and satellites, and automatically determining nucle-
arity is also far from trivial. Furthermore, there
are some documents which are simply not amenable
to being described by a document-wide rhetorical
tree (Mann and Thompson, 1988). Finally, some-
times more than one relation can hold between two
given units (Moore and Pollack, 1992). Given the
problems of automatically parsing text for rhetori-
cal relations, it seems prohibitively difficult to at-
tempt rhetorical parsing of speech documents - data
which are marked by disfluencies, low information
density, and sometimes little cohesion. For that rea-
son, this pilot study sets out a comparatively mod-
est task: to determine whether one of five relations
holds between two adjacent dialogue acts in meet-
ing speech. All relations are of the form nucleus-
satellite, and the five relation types are contrast,
1Contrast is in fact often realized with a multi-nuclear struc-
ture
1
 	





 ffProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773?782,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Summarizing Spoken and Written Conversations
Gabriel Murray and Giuseppe Carenini
Department of Computer Science
University of British Columbia
Vancouver, BC V6T 1Z4 Canada
Abstract
In this paper we describe research on sum-
marizing conversations in the meetings and
emails domains. We introduce a conver-
sation summarization system that works in
multiple domains utilizing general conversa-
tional features, and compare our results with
domain-dependent systems for meeting and
email data. We find that by treating meet-
ings and emails as conversations with general
conversational features in common, we can
achieve competitive results with state-of-the-
art systems that rely on more domain-specific
features.
1 Introduction
Our lives are increasingly comprised of multimodal
conversations with others. We email for business
and personal purposes, attend meetings in person
and remotely, chat online, and participate in blog or
forum discussions. It is clear that automatic summa-
rization can be of benefit in dealing with this over-
whelming amount of interactional information. Au-
tomatic meeting abstracts would allow us to prepare
for an upcoming meeting or review the decisions of a
previous group. Email summaries would aid corpo-
rate memory and provide efficient indices into large
mail folders.
When summarizing in each of these domains,
there will be potentially useful domain-specific fea-
tures ? e.g. prosodic features for meeting speech,
subject headers for emails ? but there are also un-
derlying similarites between these domains. They
are all multiparty conversations, and we hypothe-
size that effective summarization techniques can be
designed that would lead to robust summarization
performance on a wide array of such conversation
types. Such a general conversation summarization
system would make it possible to summarize a wide
variety of conversational data without needing to
develop unique summarizers in each domain and
across modalities. While progress has been made in
summarizing conversations in individual domains,
as described below, little or no work has been done
on summarizing unrestricted, multimodal conversa-
tions.
In this research we take an extractive approach
to summarization, presenting a novel set of conver-
sational features for locating the most salient sen-
tences in meeting speech and emails. We demon-
strate that using these conversational features in a
machine-learning sentence classification framework
yields performance that is competitive or superior
to more restricted domain-specific systems, while
having the advantage of being portable across con-
versational modalities. The robust performance of
the conversation-based system is attested via several
summarization evaluation techniques, and we give
an in-depth analysis of the effectiveness of the indi-
vidual features and feature subclasses used.
2 Related Work on Meetings and Emails
In this section we give a brief overview of previous
research on meeting summarization and email sum-
marization, respectively.
773
2.1 Meeting Summarization
Among early work on meeting summarization,
Waibel et al (1998) implemented a modified version
of the Maximal Marginal Relevance algorithm (Car-
bonell and Goldstein, 1998) applied to speech tran-
scripts, presenting the user with the n best sentences
in a meeting browser interface. Zechner (2002) in-
vestigated summarizing several genres of speech, in-
cluding spontaneous meeting speech. Though rele-
vance detection in his work relied largely on tf.idf
scores, Zechner also explored cross-speaker infor-
mation linking and question/answer detection.
More recently, researchers have investigated
the utility of employing speech-specific features
for summarization, including prosodic information.
Murray et al (2005a; 2005b) compared purely
textual summarization approaches with feature-
based approaches incorporating prosodic features,
with human judges favoring the feature-based ap-
proaches. In subsequent work (2006; 2007), they
began to look at additional speech-specific char-
acteristics such as speaker status, discourse mark-
ers and high-level meta comments in meetings, i.e.
comments that refer to the meeting itself. Galley
(2006) used skip-chain Conditional Random Fields
to model pragmatic dependencies between paired
meeting utterances (e.g. QUESTION-ANSWER re-
lations), and used a combination of lexical, prosodic,
structural and discourse features to rank utterances
by importance. Galley found that while the most
useful single feature class was lexical features, a
combination of acoustic, durational and structural
features exhibited comparable performance accord-
ing to Pyramid evaluation.
2.2 Email Summarization
Work on email summarization can be divided into
summarization of individual email messages and
summarization of entire email threads. Muresan et
al. (2001) took the approach of summarizing indi-
vidual email messages, first using linguistic tech-
niques to extract noun phrases and then employ-
ing machine learning methods to label the extracted
noun phrases as salient or not. Corston-Oliver et al
(2004) focused on identifying speech acts within a
given email, with a particular interest in task-related
sentences.
Rambow et al (2004) addressed the challenge of
summarizing entire threads by treating it as a binary
sentence classification task. They considered three
types of features: basic features that simply treat the
email as text (e.g. tf.idf, which scores words highly if
they are frequent in the document but rare across all
documents), features that consider the thread to be a
sequence of turns (e.g. the position of the turn in the
thread), and email-specific features such as number
of recipients and subject line similarity.
Carenini et al (2007) took an approach to thread
summarization using the Enron corpus (described
below) wherein the thread is represented as a
fragment quotation graph. A single node in the
graph represents an email fragment, a portion of
the email that behaves as a unit in a fine-grain
representation of the conversation structure. A
fragment sometimes consists of an entire email and
sometimes a portion of an email. For example, if a
given email has the structure
A
> B
C
where B is a quoted section in the middle of
the email, then there are three email fragments in
total: two new fragments A and C separated by
one quoted fragment B. Sentences in a fragment
are weighted according to the Clue Word Score
(CWS) measure, a lexical cohesion metric based
on the recurrence of words in parent and child
nodes. In subsequent work, Carenini et al (2008)
determined that subjectivity detection (i.e., whether
the sentence contains sentiments or opinions from
the author) gave additional improvement for email
thread summaries.
Also on the Enron corpus, Zajic et al (2008) com-
pared Collective Message Summarization (CMS)
to Individual Message Summarization (IMS) and
found the former to be a more effective technique
for summarizing email data. CMS essentially treats
thread summarization as a multi-document summa-
rization problem, while IMS summarizes individual
emails in the thread and then concatenates them to
form a thread summary.
In our work described below we also address the
task of thread summarization as opposed to sum-
774
marization of individual email messages, following
Carenini et al and the CMS approach of Zajic et al
3 Experimental Setup
In this section we describe the classifier employed
for our machine learning experiments, the corpora
used, the relevant summarization annotations for
each corpus, and the evaluation methods employed.
3.1 Statistical Classifier
Our approach to extractive summarization views
sentence extraction as a classification problem. For
all machine learning experiments, we utilize logistic
regression classifiers. This choice was partly moti-
vated by our earlier summarization research, where
logistic regression classifiers were compared along-
side support vector machines (SVMs) (Cortes and
Vapnik, 1995). The two classifier types yielded very
similar results, with logistic regression classifiers
being much faster to train and thus expediting fur-
ther development.
The liblinear toolkit 1 implements simple feature
subset selection based on the F statistic (Chen and
Lin, 2006) .
3.2 Corpora Description
For these experiments we utilize two corpora, the
Enron corpus for email summarization and the AMI
corpus for meeting summarization.
3.2.1 The Enron Email Corpus
The Enron email corpus2 is a collection of emails
released as part of the investigation into the Enron
corporation (Klimt and Yang, 2004). It has become
a popular corpus for NLP research (e.g. (Bekkerman
et al, 2004; Yeh and Harnly, 2006; Chapanond et al,
2005; Diesner et al, 2005)) due to being realistic,
naturally-occurring data from a corporate environ-
ment, and moreover because privacy concerns mean
that there is very low availability for other publicly
available email data.
39 threads have been annotated for extractive
summarization, with five annotators assigned to
each thread. The annotators were asked to select
30% of the sentences in a thread, subsequently la-
beling each selected sentence as either ?essential? or
1http://www.csie.ntu.edu.tw/?cjlin/liblinear/
2http://www.cs.cmu.edu/?enron/
?optional.? Essential sentences are weighted three
times as highly as optional sentences. A sentence
score, or GSValue, can therefore range between 0
and 15, with the maximum GSValue achieved when
all five annotators consider the sentence essential,
and a score of 0 achieved when no annotator selects
the given sentence. For the purpose of training a bi-
nary classifier, we rank the sentences in each email
thread according to their GSValues, then extract sen-
tences until our summary reaches 30% of the to-
tal thread word count. We label these sentences as
positive instances and the remainder as the negative
class. Approximately 19% of sentences are labeled
as positive, extractive examples.
Because the amount of labeled data available for
the Enron email corpus is fairly small, for our classi-
fication experiments we employ a leave-one-out pro-
ceedure for the 39 email threads. The labeled data as
a whole total just under 1400 sentences.
3.2.2 The AMI Meetings Corpus
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Carletta
et al, 2005). The corpus consists of about 100 hours
of recorded and annotated meetings. In the scenario
meetings, groups of four participants take part in a
series of four meetings and play roles within a ficti-
tious company. While the scenario given to them is
artificial, the speech and the actions are completely
spontaneous and natural. There are 96 meetings in
the training set, 24 in the development set, and 20
meetings for the test set.
For this corpus, annotators wrote abstract sum-
maries of each meeting and extracted transcript dia-
logue act segments (DAs) that best conveyed or sup-
ported the information in the abstracts. A many-
to-many mapping between transcript DAs and sen-
tences from the human abstract was obtained for
each annotator, with three annotators assigned to
each meeting. It is possible for a DA to be extracted
by an annotator but not linked to the abstract, but for
training our binary classifiers, we simply consider a
dialogue act to be a positive example if it is linked
to a given human summary, and a negative example
otherwise. This is done to maximize the likelihood
that a data point labeled as ?extractive? is truly an
informative example for training purposes. Approx-
imately 13% of the total DAs are ultimately labeled
775
as positive, extractive examples.
The AMI corpus contains automatic speech
recognition (ASR) output in addition to manual
meeting transcripts, and we report results on both
transcript types. The ASR output was provided by
the AMI-ASR team (Hain et al, 2007), and the word
error rate for the AMI corpus is 38.9%.
3.3 Summarization Evaluation
For evaluating our extractive summaries, we imple-
ment existing evaluation schemes from previous re-
search, with somewhat similar methods for meet-
ings versus emails. These are described and com-
pared below. We also evaluate our extractive classi-
fiers more generally by plotting the receiver operator
characteristic (ROC) curve and calculating the area
under the curve (AUROC). This allows us to gauge
the true-positive/false-positive ratio as the posterior
threshold is varied.
We use the differing evaluation metrics for emails
versus meetings for two primary reasons. First,
the differing summarization annotations in the AMI
and Enron corpora naturally lend themselves to
slightly divergent metrics, one based on extract-
abstract links and the other based on the essen-
tial/option/uninformative distinction. Second, and
more importantly, using these two metrics allow us
to compare our results with state-of-the-art results
in the two fields of speech summarization and email
summarization. In future work we plan to use a sin-
gle evaluation metric.
3.3.1 Evaluating Meeting Summaries
To evaluate meeting summaries we use the
weighted f-measure metric (Murray et al, 2006).
This evaluation scheme relies on the multiple human
annotated summary links described in Section 3.2.2.
Both weighted precision and recall share the same
numerator
num =
M
?
i=1
N
?
j=1
L(si, aj) (1)
where L(si, aj) is the number of links for a DA
si in the machine extractive summary according to
annotator ai, M is the number of DAs in the ma-
chine summary, and N is the number of annotators.
Weighted precision is defined as:
precision = numN ? M (2)
and weighted recall is given by
recall = num
?O
i=1
?N
j=1 L(si, aj)
(3)
where O is the total number of DAs in the meeting,
N is the number of annotators, and the denominator
represents the total number of links made between
DAs and abstract sentences by all annotators. The
weighted f-measure is calculated as the harmonic
mean of weighted precision and recall. The intuition
behind weighted f-score is that DAs that are linked
multiple times by multiple annotators are the most
informative.
3.3.2 Evaluating Email Summaries
For evaluating email thread summaries, we follow
Carenini et al (2008) by implementing their pyra-
mid precision scheme, inspired by Nenkova?s pyra-
mid scheme (2004). In Section 3.2.1 we introduced
the idea of a GSValue for each sentence in an email
thread, based on multiple human annotations. We
can evaluate a summary of a given length by com-
paring its total GSValues to the maximum possible
total for that summary length. For instance, if in a
thread the three top scoring sentences had GSValues
of 15, 12 and 12, and the sentences selected by a
given automatic summarization method had GSVal-
ues of 15, 10 and 8, the pyramid precision would be
0.85.
Pyramid precision and weighted f-score are simi-
lar evaluation schemes in that they are both sentence
based (as opposed to, for example, n-gram based)
and that they score sentences based on multiple hu-
man annotations. Pyramid precision is very simi-
lar to equation 3 normalized by the maximum score
for the summary length. For now we use these two
slightly different schemes in order to maintain con-
sistency with prior art in each domain.
4 A Conversation Summarization System
In our conversation summarization approach, we
treat emails and meetings as conversations com-
prised of turns between multiple participants. We
follow Carenini et al (2007) in working at the finer
776
granularity of email fragments, so that for an email
thread, a turn consists of a single email fragment in
the exchange. For meetings, a turn is a sequence of
dialogue acts by one speaker, with the turn bound-
aries delimited by dialogue acts from other meet-
ing participants. The features we derive for summa-
rization are based on this view of the conversational
structure.
We calculate two length features. For each sen-
tence, we derive a word-count feature normalized
by the longest sentence in the conversation (SLEN)
and a word-count feature normalized by the longest
sentence in the turn (SLEN2). Sentence length has
previously been found to be an effective feature in
speech and text summarization (e.g. (Maskey and
Hirschberg, 2005; Murray et al, 2005a; Galley,
2006)).
There are several structural features used, in-
cluding position of the sentence in the turn (TLOC)
and position of the sentence in the conversation
(CLOC). We also include the time from the begin-
ning of the conversation to the current turn (TPOS1)
and from the current turn to the end of the conversa-
tion (TPOS2). Conversations in both modalities can
be well-structured, with introductory turns, general
discussion, and ultimate resolution or closure, and
sentence informativeness might significantly corre-
late with this structure. We calculate two pause-style
features: the time between the following turn and the
current turn (SPAU), and the time between the cur-
rent turn and previous turn (PPAU), both normalized
by the overall length of the conversation. These fea-
tures are based on the email and meeting transcript
timestamps. We hypothesize that pause features may
be useful if informative turns tend to elicit a large
number of responses in a short period of time, or if
they tend to quickly follow a preceding turn, to give
two examples.
There are two features related to the conversation
participants directly. One measures how dominant
the current participant is in terms of words in the
conversation (DOM), and the other is a binary fea-
ture indicating whether the current participant ini-
tiated the conversation (BEGAUTH), based simply
on whether they were the first contributor. It is hy-
pothesized that informative sentences may more of-
ten belong to participants who lead the conversation
or have a good deal of dominance in the discussion.
There are several lexical features used in these
experiments. For each unique word, we calculate
two conditional probabilities. For each conversation
participant, we calculate the probability of the par-
ticipant given the word, estimating the probability
from the actual term counts, and take the maximum
of these conditional probabilities as our first term
score, which we will call Sprob.
Sprob(t) = max
S
p(S|t)
where t is the word and S is a participant. For ex-
ample, if the word budget is used ten times in total,
with seven uses by participant A, three uses by par-
ticipant B and no uses by the other participants, then
the Sprob score for this term is 0.70. The intuition
is that certain words will tend to be associated with
one conversation participant more than the others,
owing to varying interests and expertise between the
people involved.
Using the same procedure, we calculate a score
called Tprob based on the probability of each turn
given the word.
Tprob(t) = max
T
p(T |t)
The motivating factor for this metric is that certain
words will tend to cluster into a small number of
turns, owing to shifting topics within a conversation.
Having derived Sprob and Tprob, we then calcu-
late several sentence-level features based on these
term scores. Each sentence has features related to
max, mean and sum of the term scores for the
words in that sentence (MXS, MNS and SMS for
Sprob, and MXT, MNT and SMT for Tprob). Us-
ing a vector representation, we calculate the cosine
between the conversation preceding the given sen-
tence and the conversation subsequent to the sen-
tence, first using Sprob as the vector weights (COS1)
and then using Tprob as the vector weights (COS2).
This is motivated by the hypothesis that informative
sentences might change the conversation in some
fashion, leading to a low cosine between the preced-
ing and subsequent portions. We similarly calculate
two scores measuring the cosine between the cur-
rent sentence and the rest of the converation, using
each term-weight metric as vector weights (CENT1
for Sprob and CENT2 for Tprob). This measures
777
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 1: Features Key
whether the candidate sentence is generally similar
to the conversation overall.
There are three word entropy features, calculated
using the formula
went(s) =
?N
i=1 p(xi) ? ? log(p(xi))
( 1N ? ? log( 1N )) ? M
where s is a string of words, xi is a word type
in that string, p(xi) is the probability of the word
based on its normalized frequency in the string, N
is the number of word types in the string, and M is
the number of word tokens in the string.
Note that word entropy essentially captures infor-
mation about type-token ratios. For example, if each
word token in the string was a unique type then the
word entropy score would be 1. We calculate the
word entropy of the current sentence (THISENT),
as well as the word entropy for the conversation up
until the current sentence (PENT) and the word en-
tropy for the conversation subsequent to the current
sentence (SENT). We hypothesize that informative
sentences themselves may have a diversity of word
types, and that if they represent turning points in the
conversation they may affect the entropy of the sub-
sequent conversation.
Finally, we include a feature that is a rough ap-
proximation of the ClueWordScore (CWS) used by
Carenini et al (2007). For each sentence we remove
stopwords and count the number of words that occur
in other turns besides the current turn. The CWS is
therefore a measure of conversation cohesion.
For ease of reference, we hereafter refer to this
conversation features system as ConverSumm.
5 Comparison Summarization Systems
In order to compare the ConverSumm system with
state-of-the-art systems for meeting and email sum-
marization, respectively, we also present results us-
ing the features described by Murray and Renals
(2008) for meetings and the features described by
Rambow (2004) for email. Because the work by
Murray and Renals used the same dataset, we can
compare our scores directly. However, Rambow car-
ried out summarization work on a different, unavail-
able email corpus, and so we re-implemented their
summarization system for our current email data.
In their work on meeting summarization, Murray
and Renals creating 700-word summaries of each
meeting using several classes of features: prosodic,
lexical, structural and speaker-related. While there
are two features overlapping between our systems
(word-count and speaker/participant dominance),
their system is primarily domain-dependent in its
use of prosodic features while our features represent
a more general conversational view.
Rambow presented 14 features for the summa-
rization task, including email-specific information
such as the number of recipients, number of re-
sponses, and subject line overlap. There is again a
slight overlap in features between our two systems,
as we both include length and position of the sen-
tence in the thread/conversation.
6 Results
Here we present, in turn, the summarization results
for meeting and email data.
6.1 Meeting Summarization Results
Figure 1 shows the F statistics for each Conver-
summ feature in the meeting data, providing a mea-
sure of the usefulness of each feature in discriminat-
ing between the positive and negative classes. Some
778
 0
 0.05
 0.1
 0.15
 0.2
 0.25
CENT2
CENT1
CWS
BEGAUTH
SPAU
PPAU
THISENT
SENT
PENT
COS2
COS1
DOM
TPOS2
TPOS1
SLEN2
SLEN
CLOC
TLOC
SMT
MNT
MXT
SMS
MNS
MXS
f s
ta
tis
tic
feature ID (see key)
manual
ASR
Figure 1: Feature F statistics for AMI meeting corpus
System Weighted F-Score AUROC
Speech - Man 0.23 0.855
Speech - ASR 0.24 0.850
Conv. - Man 0.23 0.852
Conv. - ASR 0.22 0.853
Table 2: Weighted F-Scores and AUROCs for Meeting
Summaries
features such as participant dominance have very
low F statistics because each sentence by a given
participant will receive the same score; so while the
feature itself may have a low score because it does
not discriminate informative versus non-informative
sentences on its own, it may well be useful in con-
junction with the other features. The best individual
ConverSumm features for meeting summarization
are sentence length (SLEN), sum of Sprob scores,
sum of Tprob scores, the simplified CWS score
(CWS), and the two centroid measures (CENT1 and
CENT2). The word entropy of the candidate sen-
tence is very effective for manual transcripts but
much less effective on ASR output. This is due to
the fact that ASR errors can incorrectly lead to high
entropy scores.
Table 2 provides the weighted f-scores for all
summaries of the meeting data, as well as AUROC
scores for the classifiers themselves. For our 700-
word summaries, the Conversumm approach scores
comparably to the speech-specific approach on both
manual and ASR transcripts according to weighted
f-score. There are no significant differences accord-
ing to paired t-test. For the AUROC measures, there
are again no significant differences between the con-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
lexical features
structural features
participant features
length features
Fea. Subset AUROC
Structural 0.652
Participant 0.535
Length 0.837
Lexical 0.852
Figure 2: AUROC Values for Feature Subclasses, AMI
Corpus
versation summarizers and speech-specific summa-
rizers. The AUROC for the conversation system
is slightly lower on manual transcripts and slightly
higher when applied to ASR output.
For all systems the weighted f-scores are some-
what low. This is partly owing to the fact that out-
put summaries are very short, leading to high pre-
cision and low recall. The low f-scores are also in-
dicative of the difficulty of the task. Human perfor-
mance, gauged by comparing each annotator?s sum-
maries to the remaining annotators? summaries, ex-
hibits an average weighted f-score of 0.47 on the
same test set. The average kappa value on the test set
is 0.48, showing the relatively low inter-annotator
agreement that is typical of summarization annota-
tion. There is no additional benefit to combining the
conversational and speech-specific features. In that
case, the weighted f-scores are 0.23 for both manual
and ASR transcripts. The overall AUROC is 0.85
for manual transcripts and 0.86 for ASR.
We can expand the features analysis by consid-
ering the effectiveness of certain subclasses of fea-
tures. Specifically, we group the summarization fea-
tures into lexical, structural, participant and length
features. Figure 2 shows the AUROCs for the fea-
ture subset classifiers, illustrating that the lexical
subclass is very effective while the length features
also constitute a challenging baseline. A weakness
779
System Pyramid Precision AUROC
Rambow 0.50 0.64
Conv. 0.46 0.75
Table 3: Pyramid Precision and AUROCs for Email Sum-
maries
of systems that depend heavily on length features,
however, is that recall scores tend to decrease be-
cause the extracted units are much longer - weighted
recall scores for the 700 word summaries are sig-
nificantly worse according to paired t-test (p<0.05)
when using just length features compared to the full
feature set.
6.2 Email Summarization Results
Figure 3 shows the F statistic for each ConverSumm
feature in the email data.The two most useful fea-
tures are sentence length and CWS. The Sprob and
Tprob features rate very well according to the F
statistic. The two centroid features incorporating
Sprob and Tprob are comparable to one another and
are very effective features as well.
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0.09
CENT2
CENT1
CWS
BEGAUTH
SPAU
PPAU
THISENT
SENT
PENT
COS2
COS1
DOM
TPOS2
TPOS1
SLEN2
SLEN
CLOC
TLOC
SMT
MNT
MXT
SMS
MNS
MXS
f s
ta
tis
tic
feature ID (see key)
Figure 3: Feature F statistics for Enron email corpus
After creating 30% word compression summaries
using both the ConverSumm and Rambow ap-
proaches, we score the 39 thread summaries using
Pyramid Precision. The results are given in Table 3.
On average, the Rambow system is slightly higher
with a score of 0.50 compared with 0.46 for the con-
versational system, but there is no statistical differ-
ence according to paired t-test.
The average AUROC for the Rambow system is
0.64 compared with 0.75 for the ConverSumm sys-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
lexical features
structural features
participant features
length features
Fea. Subset AUROC
Structural 0.63
Participant 0.51
Length 0.71
Lexical 0.71
Figure 4: AUROC Values for Feature Subclasses, Enron
Corpus
tem, with ConverSumm system significantly better
according to paired t-test (p<0.05). Random classi-
fication performance would yield an AUROC of 0.5.
Combining the Rambow and ConverSumm fea-
tures does not yield any overall improvement. The
Pyramid Precision score in that case is 0.47 while
the AUROC is 0.74.
Figure 4 illustrates that the lexical and length
features are the most effective feature subclasses,
though the best results overall are derived from a
combination of all feature classes.
7 Discussion
According to multiple evaluations, the ConverSumm
features yield competitive summarization perfor-
mance with the comparison systems. There is a clear
set of features that are similarly effective in both do-
mains, especially CWS, the centroid features, the
Sprob features, the Tprob features, and sentence
length. There are other features that are more ef-
fective in one domain than the other. For exam-
ple, the BEGAUTH feature, indicating whether the
current participant began the conversation, is more
useful for emails. It seems that being the first per-
son to speak in a meeting is not as significant as
being the first person to email in a given thread.
SLEN2, which normalizes sentence length by the
longest sentence in the turn, also is much more ef-
780
fective for emails. The reason is that many meet-
ing turns consist of a single, brief utterance such as
?Okay, yeah.?
The finding that the summary evaluations are
not significantly worse on noisy ASR compared
with manual transcripts has been previously attested
(Valenza et al, 1999; Murray et al, 2005a), and it is
encouraging that our ConverSumm features are sim-
ilarly robust to this noisy data.
8 Conclusion
We have shown that a general conversation summa-
rization approach can achieve results on par with
state-of-the-art systems that rely on features specific
to more focused domains. We have introduced a
conversation feature set that is similarly effective in
both the meetings and emails domains. The use of
multiple summarization evaluation techniques con-
firms that the system is robust, even when applied
to the noisy ASR output in the meetings domain.
Such a general conversation summarization system
is valuable in that it may save time and effort re-
quired to implement unique systems in a variety of
conversational domains.
We are currently working on extending our sys-
tem to other conversation domains such as chats,
blogs and telephone speech. We are also investigat-
ing domain adaptation techniques; for example, we
hypothesize that the relatively well-resourced do-
main of meetings can be leveraged to improve email
results, and preliminary findings are encouraging.
References
R. Bekkerman, A. McCallum, and G. Huang. 2004. Au-
tomatic categorization of email into folders: Bench-
mark experiments on Enron and SRI corpora. Tech-
nical Report IR-418, Center of Intelligent Information
Retrieval, UMass Amherst.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval 1998, Melbourne, Australia, pages 335?
336.
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proc. of ACM
WWW 07, Banff, Canada.
G. Carenini, X. Zhou, and R. Ng. 2008. Summarizing
emails with conversational cohesion and subjectivity.
In Proc. of ACL 2008, Columbus, Ohio, USA.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Well-
ner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
A. Chapanond, M. Krishnamoorthy, and B. Yener. 2005.
Graph theoretic and spectral analysis of enron email
data. Comput. Math. Organ. Theory, 11(3):265?281.
Y-W. Chen and C-J. Lin. 2006. Combining SVMs
with various feature selection strategies. In I. Guyon,
S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature
extraction, foundations and applications. Springer.
S. Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-
bell. 2004. Integration of email and task lists. In Proc.
of CEAS 2004, Mountain View, CA, USA.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273?297.
J. Diesner, T. Frantz, and K. Carley. 2005. Communi-
cation networks from the enron email corpus ?it?s al-
ways about the people. enron is no different?. Comput.
Math. Organ. Theory, 11(3):201?228.
M. Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP 2006, Sydney, Australia, pages 364?
372.
T. Hain, L. Burget, J. Dines, G. Garau, V. Wan,
M. Karafiat, J. Vepa, and M. Lincoln. 2007. The
AMI system for transcription of speech in meetings.
In Proc. of ICASSP 2007,, pages 357?360.
B. Klimt and Y. Yang. 2004. Introducing the enron cor-
pus. In Proc. of CEAS 2004, Mountain View, CA, USA.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features for
speech summarization. In Proc. of Interspeech 2005,
Lisbon, Portugal, pages 621?624.
S. Muresan, E. Tzoukermann, and J. Klavans. 2001.
Combining linguistic and machine learning techniques
for email summarization. In Proc. of ConLL 2001,
Toulouse, France.
G. Murray and S. Renals. 2008. Meta comments for
summarizing meeting speech. In Proc. of MLMI 2008,
Utrecht, Netherlands.
G. Murray, S. Renals, and J. Carletta. 2005a. Extrac-
tive summarization of meeting recordings. In Proc. of
Interspeech 2005, Lisbon, Portugal, pages 593?596.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005b.
Evaluating automatic summaries of meeting record-
ings. In Proc. of the ACL 2005 MTSE Workshop, Ann
Arbor, MI, USA, pages 33?40.
781
G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. In-
corporating speaker and discourse features into speech
summarization. In Proc. of the HLT-NAACL 2006,
New York City, USA, pages 367?374.
G. Murray. 2007. Using Speech-Specific Features for
Automatic Speech Summarization. Ph.D. thesis, Uni-
versity of Edinburgh.
A. Nenkova and B. Passonneau. 2004. Evaluating con-
tent selection in summarization: The Pyramid method.
In Proc. of HLT-NAACL 2004, Boston, MA, USA,
pages 145?152.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004, Boston, USA.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proc. of the ESCA Workshop on
Accessing Information in Spoken Audio, Cambridge
UK, pages 111?116.
A. Waibel, M. Bett, M. Finke, and R. Stiefelhagen. 1998.
Meeting browser: Tracking and summarizing meet-
ings. In D. E. M. Penrose, editor, Proc. of the Broad-
cast News Transcription and Understanding Work-
shop, Lansdowne, VA, USA, pages 281?286.
J. Yeh and A. Harnly. 2006. Email thread reassembly
using similarity matching. In Proc of CEAS 2006.
D. Zajic, B. Dorr, and J. Lin. 2008. Single-document and
multi-document summarization techniques for email
threads using sentence compression. Information Pro-
cessing and Management, to appear.
K. Zechner. 2002. Automatic summarization of open-
domain multiparty dialogues in diverse genres. Com-
putational Linguistics, 28(4):447?485.
782
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1348?1357,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Predicting Subjectivity in Multimodal Conversations
Gabriel Murray and Giuseppe Carenini
University of British Columbia
Vancouver, Canada
(gabrielm, carenini)@cs.ubc.ca
Abstract
In this research we aim to detect sub-
jective sentences in multimodal conversa-
tions. We introduce a novel technique
wherein subjective patterns are learned
from both labeled and unlabeled data, us-
ing n-gram word sequences with vary-
ing levels of lexical instantiation. Ap-
plying this technique to meeting speech
and email conversations, we gain signifi-
cant improvement over state-of-the-art ap-
proaches. Furthermore, we show that cou-
pling the pattern-based approach with fea-
tures that capture characteristics of gen-
eral conversation structure yields addi-
tional improvement.
1 Introduction
Conversations are rich in subjectivity. Conversa-
tion participants agree and disagree with one other,
argue for and against various proposals, and gen-
erally take turns expressing their private states.
Being able to separate these subjective utterances
from more objective utterances would greatly fa-
cilitate the analysis, mining and summarization of
a large number of conversations.
Two of the most prevalent conversational me-
dia are meetings and emails. Face-to-face meet-
ings enable numerous people to exchange a large
amount of information and opinions in a short pe-
riod of time, while emails allow for concise ex-
changes between potentially far-flung participants.
Meetings and emails can also feed into one an-
other, with face-to-face meetings occurring at reg-
ular intervals and emails continuing the conver-
sations in the interim. This poses several inter-
esting questions, such as whether subjective utter-
ances are more or less likely to be found in email
exchanges compared with meetings, and whether
the ratios of positive and negative subjective utter-
ances differ between the two modalities.
In this paper we describe a novel approach for
predicting subjectivity, and test it in two sets of
experiments on meetings and emails. Our ap-
proach combines a new general purpose method
for learning subjective patterns, with features that
capture basic characteristics of conversation struc-
ture across modalities. The subjective patterns are
essentially n-gram sequences with varying levels
of lexical instantiation, and we demonstrate how
they can be learned from both labeled and un-
labeled data. The conversation features capture
structural characteristics of multimodal conversa-
tions as well as participant information.
We test our approach in two sets of experi-
ments. The goal of the first set of experiments is to
discriminate subjective from non-subjective utter-
ances, comparing the novel approach to existing
state-of-the-art techniques. In the second set of
experiments, the goal is to discriminate positive-
subjective and negative-subjective utterances, es-
tablishing their polarity. In both sets of experi-
ments, we assess the impact of features relating
to conversation structure.
2 Related Research
Raaijmakers et al (2008) have approached
the problem of detecting subjectivity in meeting
speech by using a variety of multimodal features
such as prosodic features, word n-grams, charac-
ter n-grams and phoneme n-grams. For subjec-
tivity detection, they found that a combination of
all features was best, while prosodic features were
less useful for discriminating between positive and
negative utterances. They found character n-grams
to be particularly useful.
Riloff and Wiebe (2004) presented a method for
learning subjective extraction patterns from a large
amount of data, which takes subjective and non-
subjective text as input, and outputs significant
lexico-syntactic patterns. These patterns are based
on syntactic structure output by the Sundance shal-
1348
low dependency parser (Riloff and Phillips, 2004).
They are extracted by exhaustively applying syn-
tactic templates such as < subj > passive-verb
and active-verb < dobj > to a training cor-
pus, with an extracted pattern for every instan-
tiation of the syntactic template. These patterns
are scored according to probability of relevance
given the pattern and frequency of the pattern. Be-
cause these patterns are based on syntactic struc-
ture, they can represent subjective expressions that
are not fixed word sequences and would therefore
be missed by a simple n-gram approach.
Riloff et al (2006) explore feature subsumption
for opinion detection, where a given feature may
subsume another feature representationally if the
strings matched by the first feature include all of
the strings matched by the second feature. To give
their own example, the unigram happy subsumes
the bigram very happy. The first feature will be-
haviorally subsume the second if it representa-
tionally subsumes the second and has roughly the
same information gain, within an acceptable mar-
gin. They show that they can improve opinion
analysis results by modeling these relations and
reducing the feature set.
Our approach for learning subjective patterns
like Raaijmakers et al relies on n-grams, but like
Riloff et al moves beyond fixed sequences of
words by varying levels of lexical instantiation.
Yu and Hatzivassiloglou (2003) addressed three
challenges in the news article domain: discrimi-
nating between objective documents and subjec-
tive documents such as editorials, detecting sub-
jectivity at the sentence level, and determining po-
larity at the sentence level. They found that the
latter two tasks were substantially more difficult
than classification at the document level. Of par-
ticular relevance here is that they found that part-
of-speech (POS) features were especially useful
for assigning polarity scores, with adjectives, ad-
verbs and verbs comprising the best set of POS
tags. This work inspired us to look at generaliza-
tion of n-grams based on POS.
On the slightly different task of classifying the
intensity of opinions, Wilson et al (2006) em-
ployed several types of features including depen-
dency structures in which words can be backed off
to POS tags. They found that this feature class im-
proved the overall accuracy of their system.
Somasundaran et al (2007) investigated subjec-
tivity classification in meetings. Their findings in-
dicate that both lexical features (list of words and
expressions) and discourse features (dialogue acts
and adjacency pairs) can be beneficial. In the same
spirit, we effectively combine lexical patterns and
conversational features.
The approach to predicting subjectivity we
present in this paper is a novel contribution to the
field of opinion and sentiment analysis. Pang and
Lee (2008) give an overview of the state of the art,
discussing motivation, features, approaches and
available resources.
3 Subjectivity Detection
In this section we describe our approach to sub-
jectivity detection. We begin by describing how
to learn subjective n-gram patterns with varying
levels of lexical instantiation. We then describe a
set of features characterizing multimodal conver-
sation structure which can be used to supplement
the n-gram approach. Finally, we describe the
baseline subjectivity detection approaches used
for comparison.
3.1 Partially Instantiated N-Grams
Our approach to subjectivity detection and polar-
ity detection is to learn significant patterns that
correlate with the subjective and polar utterances.
These patterns are word trigrams, but with varying
levels of lexical instantiation, so that each unit of
the n-gram can be either a word or the word?s part-
of-speech (POS) tag. This contrasts, then, with
work such as that of Raaijmakers et al (2008)
who include trigram features in their experiments,
but where their learned trigrams are fully instanti-
ated. As an example, while they may learn that a
trigram really great idea is positive, we may addi-
tionally find that really great NN and RB great NN
are informative patterns, and these patterns may
sometimes be better cues than the fully instanti-
ated trigrams. To differentiate this approach from
the typical use of trigrams, we will refer to it as the
VIN (varying instantiation n-grams) method.
In some respects, our approach to subjectiv-
ity detection is similar to Riloff and Wiebe?s
work cited above, in the sense that their extrac-
tion patterns are partly instantiated. However,
the AutoSlog-TS approach relies on deriving syn-
tactic structure with the Sundance shallow parser
(Riloff and Phillips, 2004). We hypothesize that
our trigram approach may be more robust to dis-
fluent and fragmented meeting speech and emails
1349
1 2 3
really great idea
really great NN
really JJ idea
RB great idea
really JJ NN
RB great NN
RB JJ idea
RB JJ NN
Table 1: Sample Instantiation Set
on which syntactic parsers may perform poorly.
Also, our learned trigram patterns range from fully
instantiated to completely uninstantiated. For ex-
ample, we might find that the pattern RB JJ NN
is a very good indicator of subjective utterances
because it matches a variety of scenarios where
people are ascribing qualities to things, e.g. re-
ally bad movie, horribly overcooked steak. Notice
that we do not see our approach and AutoSlog-TS
as mutually exclusive, and indeed we demonstrate
through these experiments that they can be effec-
tively combined.
Our approach begins by running the Brill POS
tagger (Brill, 1992) over all sentences in a doc-
ument. We then extract all of the word trigrams
from the document, and represent each trigram us-
ing every possible instantiation. Because we are
working at the trigram level, and each unit of the
trigram can be a word or its POS tag there are
2
3
= 8 representations in each trigram?s instantia-
tion set. To continue the example from above, the
instantiation set for the trigram really great idea is
given in Table 1. As we scan down the instanti-
ation set, we can see that the level of abstraction
increases until it is completely uninstantiated. It is
this multilevel abstraction that we are hypothesiz-
ing will be useful for learning new subjective and
polar cues.
All trigrams are then scored according to their
prevalence in relevant versus irrelevant documents
(e.g. subjective vs. non-subjective sentences),
following the scoring methodology of Riloff and
Wiebe (2003). We calculate the conditional prob-
ability p(relevance|trigram) using the actual tri-
gram counts in relevant and irrelevant text. For
learning negative-subjective patterns, we treat all
negative sentences as the relevant text and the re-
mainder of the sentences as irrelevant text, and
conduct the same process for learning positive-
subjective patterns. We consider significant pat-
terns to be those where the conditional proba-
bility is greater than 0.65 and the pattern occurs
more than five times in the entire document set
(slightly higher than probability >= 0.60 and
frequency >= 2 used by Riloff and Wiebe
(2003)).
We possess a fairly small amount of conversa-
tional data annotated for subjectivity and polarity.
The AMI meeting corpus and BC3 email corpus
are described in more detail in Section 4.1. To ad-
dress this shortfall in annotated data, we take two
approaches to learning patterns. In the first, we
learn a set of patterns from the annotated conversa-
tion data. In the second approach, we complement
those patterns by learning additional patterns from
unannotated data that are typically overwhelm-
ingly subjective or objective in nature. We de-
scribe these two approaches here in turn.
3.1.1 Supervised Learning of Patterns from
Conversation Data
The first learning strategy is to apply the above-
described methods to the annotated conversation
data, learning the positive patterns by compar-
ing positive-subjective utterances to all other ut-
terances, and learning the negative patterns by
comparing the negative-subjective utterances to
all other utterances, using the described methods.
This results in 759 significant positive patterns and
67 significant negative patterns. This difference in
pattern numbers can be explained by negative ut-
terances being less common in the AMI meetings,
as noted by Wilson (2008). It may be that people
are less comfortable in expressing negative sen-
timents in face-to-face conversations, particularly
when the meeting participants do not know each
other well (in the AMI scenario meetings, many
participants were meeting each other for the first
time). But there may be a further explanation for
why we learn many more positive than negative
patterns. When conversation participants do ex-
press negative sentiments, they may couch those
sentiments in more euphemistic or guarded terms
compared with positive sentiments. Table 2 gives
examples of significant positive and negative pat-
terns learned from the labeled meeting data. The
last two rows in Table 2 show how two patterns
in the same instantiation set can have substantially
different probabilities.
1350
POS p(r|t) NEG p(r|t)
you MD change 1.0 VBD not RB 1.0
should VBP DT 1.0 doesn?t RB VB 0.875
very easy to 0.88 a bit JJ 0.66
we could VBP 0.78 think PRP might 0.66
NNS should VBP 0.71 be DT problem 0.71
PRP could do 0.66 doesn?t really VB 0.833
it could VBP 83 doesn?t RB VB 0.875
Table 2: Example Pos. and Neg. Patterns (AMI)
3.1.2 Unsupervised Learning of Patterns
from Blog Data
The second pattern learning strategy we take to
learning subjective patterns is to use a relevant,
but unannotated corpus. We focus on weblog
(blog) data for several reasons. First, blog posts
share many characteristics with both meetings and
emails: they are conversational, informal and the
language can be very ungrammatical. Second,
blog posts are known for being subjective; blog-
gers post on issues that are passionate to them, of-
fering arguments, opinions and invective. Third,
there is a huge amount of available blog data. But
because we do not possess blog data annotated
for subjectivity, we take the following approach
to learning subjective patterns from this data. We
work on the assumption that a great many blog
posts are inherently subjective, and that compar-
ing this data to inherently objective text such as
newswire articles, treating the latter as our irrele-
vant text, should lead to the detection of many new
subjective patterns and greatly increase our cover-
age. While the patterns learned will be noisy, we
hypothesize that the increased coverage will im-
prove our subjectivity detection overall.
For our blog data, we use the BLOG06 Corpus1
that was featured as training and testing data for
the Text Analysis Conference (TAC) 2008 track
on summarizing blog opinions. The portion used
totals approximately 4,000 documents on all man-
ner of topics. Treating that dataset as our rele-
vant, subjective data, we then learn the subjec-
tive trigrams by comparing with the irrelevant
TAC/DUC newswire data from the 2007 and 2008
update summarization tasks. To try to reduce the
amount of noise in our learned patterns, we set the
conditional probability threshold at 0.75 (vs. 0.65
for annotated data), and stipulate that all signif-
icant patterns must occur at least once in the ir-
relevant text. This last rule is meant to prevent
1http://ir.dcs.gla.ac.uk/test collections/blog06info.html
Pattern p(r|t)
can not VB 0.99
i can RB 0.99
i have not 0.98
do RB think 0.97
RB think that 0.95
RB agree with 0.95
IN PRP opinion 0.95
Table 3: Example Subjective Patterns (BLOG06)
us from learning completely blog-specific patterns
such as posted by NN or linked to DT. In the end,
more than 20,000 patterns were learned from the
blog data. While manual inspection does show
that many undesirable patterns were extracted,
among the highest-scoring patterns are many sen-
sible subjective trigrams such as those indicated in
Table 3.
This approach is similar in spirit to the work of
Biadsy et al (2008) on unsupervised biography
production. Without access to labeled biographi-
cal data, the authors chose to use sentences from
Wikipedia biographies as their positive set and
sentences from newswire articles as their negative
set, on the assumption that most of the Wikipedia
sentences would be relevant to biographies and
most of the newswire sentences would not.
3.2 Deriving VIN Features
For our machine learning experiments, we derive,
for each sentence, features indicating the presence
of the significant VIN patterns. Patterns are binned
according to their conditional probability range
(i.e., 0.65 <= p < 0.75, 0.75 <= p < 0.85,
0.85 <= p < 0.95, and 0.95 <= p). There are
three bins for the blog patterns, since the proba-
bility cutoff is 0.75 For each bin, there is a feature
indicating the count of its patterns in the given sen-
tence. When attempting to match these trigram
patterns to sentences, we allow up to two wild-
card lexical items between the trigram units. In
this way a sentence can match a learned pattern
even if the units of the n-gram are not contiguous
(Raaijmakers et al (2008) similarly include an n-
gram feature allowing such intervening material).
A key reason for counting the number of
matched patterns for each probability range as just
described, rather than including a feature for each
individual pattern, is to maintain the same level
of dimensionality in our machine learning exper-
iments when comparing the VIN approach to the
baseline approaches described in Section 3.4.
1351
3.3 Conversational Features
While we hypothesize that the general pur-
pose pattern-based approach described above will
greatly aid subjectivity and polarity detection, we
also recognize that there are many additional fea-
tures specific for characterizing multimodal con-
versations that may correlate well with subjectiv-
ity and polarity. Such features include structural
characteristics like the position of a sentence in a
turn and the position of a turn in the conversation,
and participant features relating to dominance or
leadership. For example, it may be that subjective
sentences are more likely to come at the end of a
conversation, or that a person who dominates the
conversation may utter more negative sentences.
We use the feature set provided by Murray and
Carenini (2008), which they used for automatic
summarization of conversations and which are
shown in Table 4. Many of the features are based
on so-called Sprob and Tprob term-weights, the
former of which weights words based on their dis-
tributions across conversation participants and the
latter of which similarly weights words based on
their distributions across conversation turns. Other
features include word entropy of the candidate
sentence, lexical cohesion of the sentence with the
greater conversation, and structural features indi-
cating position of the candidate sentence in the
turn and in the conversation, such as the elapsed
time since the beginning of the conversation.
3.4 Baseline Approaches
There are two baselines in particular to which
we are interested in comparing the VIN ap-
proach. As stated earlier, we are hypothesiz-
ing that the increasing levels of abstraction found
with partially instantiated trigrams will lead to im-
proved classification compared with using only
fully instantiated trigrams. To test this, we
also run the subjective/non-subjective and posi-
tive/negative experiments using only fully instan-
tiated trigrams. There are 71 such positive tri-
grams and 5 such negative trigrams learned from
the AMI data. There are just over 1200 fully in-
stantiated trigrams learned from the unannotated
BLOG06 data.
Believing that the current approach may offer
benefits over state-of-the-art pattern-based subjec-
tivity detection, we also implement the AutoSlog-
TS method of Riloff and Wiebe (2003) for extract-
ing subjective extraction patterns. In AutoSlog-
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cosine of conv. splits, w/ Sprob
COS2 cosine of conv. splits, w/ Tprob
PENT entropy of conv. up to sentence
SENT entropy of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore (cohesion)
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 4: Features Key
TS, once all of the patterns are extracted using
the Sundance parser, the scoring methodology is
much the same as desribed in Section 3.1. Con-
ditional probabilities are calculated by comparing
pattern occurrences in the relevant text with oc-
currences in all text, and we again use a thresh-
old of p >= 0.65 and frequency >= 5 for sig-
nificant patterns. For the BLOG06 data, we use
a probability cutoff of 0.75 as before. For deriv-
ing the features used in our machine learning ex-
periments, the patterns are similarly grouped ac-
cording to conditional probability. From the anno-
tated data, 48 patterns are learned in total, 46 pos-
itive and only 2 negative. From the BLOG06 data,
more than 3000 significant patterns are learned.
Among significant patterns learned from the AMI
corpus are < subj > BE good, change < dobj >,
< subj > agree and problem with < NP >.
To gauge the effectiveness of the various feature
types, for both sets of experiments we build multi-
ple models on a variety of feature combinations:
fully instantiated trigrams (TRIG), varying in-
stantiation n-grams (VIN), AutoSlog-TS (SLOG),
conversational structure features (CONV), and the
set of all features.
4 Experimental Setup
In this section we describe the corpora used, the
relevant subjectivity annotation, and the statistical
1352
classifiers employed.
4.1 Corpora
We use two annotated corpora for these experi-
ments. The AMI corpus (Carletta et al, 2005) con-
sists of meetings in which participants take part in
role-playing exercises concerning the design and
development of a remote control. Participants are
grouped in fours, and each group takes part in a
sequence of four meetings, bringing the remote
control from design to market. The four members
of the group are assigned roles of project man-
ager, industrial designer, user interface designer,
and marketing expert. In total there are 140 such
scenario meetings, with individual meetings rang-
ing from approximately 15 to 45 minutes.
The BC3 corpus (Ulrich et al, 2008) contains
email threads from the World Wide Web Consor-
tium (W3C) mailing list. The threads feature a va-
riety of topics such as web accessibility and plan-
ning face-to-face meetings. The annotated portion
of the mailing list consists of 40 threads.
4.2 Subjectivity Annotation
Wilson (2008) has annotated 20 AMI meetings for
a variety of subjective phenomena which fall into
the broad classes of subjective utterances, objec-
tive polar utterances and subjective questions. It
is this first class in which we are primarily in-
terested here. Two subclasses of subjective utter-
ances are positive subjective and negative subjec-
tive utterances. Such subjective utterances involve
the expression of a private state, such as a posi-
tive/negative opinion, positive/negative argument,
and agreement/disagreement. The 20 meetings
were labeled by a single annotator, though Wilson
(2008) did conduct a study of annotator agreement
on two meetings, reporting a ? of 0.56 for detect-
ing subjective utterances. Of the roughly 20,000
dialogue acts total in the 20 AMI meetings, nearly
4000 are labeled as positive-subjective and nearly
1300 as negative-subjective. For the first exper-
imental task, we consider the subjective class to
be the union of positive-subjective and negative-
subjective dialogue acts. For the second experi-
mental task, the goal is to discriminate positive-
subjective from negative-subjective.
For the BC3 emails, annotators were initially
asked to create extractive and abstractive sum-
maries of each thread, in addition to labeling a
variety of sentence-level phenomena, including
whether each sentence was subjective. In a second
round of annotations, three different annotators
were asked to go through all of the sentences pre-
viously labeled as subjective and indicate whether
each sentence was positive, negative, positive-
negative, or other. The definitions for positive and
negative subjectivity mirrored those given by Wil-
son (2008). For the purpose of these experiments,
we consider a sentence to be subjective if at least
two of the annotators labeled it as subjective, and
similarly consider a subjective sentence to be pos-
itive or negative if at least two annotators label it
as such. Using this majority vote labeling, 172
of 1800 sentences are considered subjective, with
44% of those labeled as positive-subjective and
37% as negative-subjective, showing that there is
much more of a balance between positive and neg-
ative sentiment in these email threads compared
with meeting speech (note that some subjective
sentences are not positive or negative). The ? for
labeling subjective sentences in the email corpus
is 0.32. The lower annotator agreement on emails
compared with meetings suggests that subjectiv-
ity in email text may be manifested more subtly or
conveyed somewhat amibiguously.
4.3 Classifier and Experimental Setup
For these experiments we use a maximum entropy
classifier using the liblinear toolkit2 (Fan et al,
2008). Feature subset selection is carried out by
calculating the F-statistic for each feature, ranking
the features according to the statistic, and train-
ing on increasingly smaller subsets of feature in
a cross-validation procedure, ultimately choosing
the feature set with the highest balanced accuracy
during cross-validation.
Because the annotated portions of our corpora
are fairly small (20 meetings, 40 email threads),
we employ a leave-one-out method for training
and testing rather than using dedicated training
and test sets. For the polarity labeling task ap-
plied to the BC3 corpus, we pool all of the sen-
tences and perform 10-fold cross-validation at the
sentence level.
4.4 Evaluation Metrics
We employ two sets of metrics for evaluating all
classifiers: precision/recall/f-measure and the re-
ceiver operator characteristic (ROC) curve. The
ROC curve plots the true-positive/false-positive
ratio while the posterior threshold is varied, and
2http://www.csie.ntu.edu.tw/ cjlin/liblinear/
1353
we report the area under the curve (AUROC) as the
measure of interest. Random performance would
feature an AUROC of approximately 0.5, while
perfect classification would yield an AUROC of
1. The advantage of the AUROC score compared
with precision/recall/f-measure is that it evaluates
a given classifier across all thresholds, indicating
the classifier?s overall discriminating power. This
metric is also known to be appropriate when class
distributions are skewed (Fawcett, 2003), as is our
case. For completeness we report both AUROC
and p/r/f, but our discussions focus primarily on
the AUROC comparisons.
5 Results
In this section we describe the experimental re-
sults, first for the subjective/non-subjective clas-
sification task, and subsequently for the positive-
negative classification task.
5.1 Subjective / Non-Subjective Classification
For the subjectivity detection task, the results on
the AMI and BC3 data closely mirrored each
other, with the VIN approach constituting a very
effective feature set, outperforming both baselines.
We report the results on meeting and emails in
turn.
5.1.1 AMI corpus
For the subjectivity task with the AMI corpus, we
first report the precision, recall and f-measure re-
sults in Table 5 where the various classifiers are
compared with a lower bound (LB) in which the
positive class is always predicted, leading to per-
fect recall. It can be seen that the novel systems
exhibit substantial improvement in precision and
f-measure over this lower-bound. While the VIN
approach yields the best precision scores, the full
feature set achieves the highest f-measure.
As shown in Figure 1, the average AUROC with
the VIN approach is 0.69, compared with 0.61 for
AutoSlog-TS, a significant difference according to
paired t-test (p<0.01). The VIN approach is also
significantly better than the standard fully instan-
tiated trigram pattern approach (p<0.01). This
latter result suggests that the increased level of
abstraction found in the varying instantiation n-
grams does improve performance.
The conversational features alone give compa-
rable performance to the VIN method (no signifi-
cant difference), and the best results are found us-
ing the full feature set, which gives an average AU-
Sys Precision Recall F-Measure
AMI Corpus
LB 26 100 41
Trig 25 63 36
Slog 39 48 43
VIN 41 58 48
Conv 36 73 49
All Feas 38 70 49
BC3 Corpus
LB 10 100 17
Trig 27 10 14
Slog 24 13 17
VIN 27 22 24
Conv 25 29 27
All Feas 33 34 33
Table 5: P/R/F Results, Subjectivity Task
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Trig - AMI
Trig - BC3
Slog - AMI
Slog - BC3
VIN - AMI
VIN - BC3
Conv - AMI
Conv - BC3
All Feas - AMI
All Feas - BC3
AU
RO
C
Figure 1: AUROCs on Subjectivity Task for AMI
and BC3 corpora
ROC of 0.71, a significant improvement over VIN
only (p<0.05).
5.1.2 BC3 corpus
For the subjectivity task with the BC3 corpus, the
best precision and f-measure scores are found by
combining all features, as displayed in Table 5.
The f-measure for the VIN approach is ten points
higher than for the standard trigram approach.
The average AUROC with the VIN approach is
0.77, compared with 0.70 for AutoSlog-TS (sig-
nificant at p<0.05). The varying instantiation ap-
proach is significantly better than the standard tri-
gram pattern approach (p<0.01), where the aver-
age AUROC is 0.66. We again find that conver-
sational features are very useful for this task, and
that the best overall results utilize the entire fea-
ture set. These results are displayed in Figure 1.
5.1.3 Impact of Blog Data
An interesting question is whether our use of the
BLOG06 data was worthwhile. We can measure
this by comparing the VIN AUROC results re-
1354
Sys Precision Recall F-Measure
AMI Corpus
LB 76 100 86
Trig 87 8 14
Slog 75 46 57
VIN 83 60 70
Conv 82 47 60
All Feas 83 56 67
BC3 Corpus
LB 54 100 70
Trig 50 84 63
Slog 58 56 57
VIN 53 84 65
Conv 63 80 71
All Feas 60 76 67
Table 6: P/R/F Results, Polarity Task
ported above with the VIN AUROC scores using
only the annotated data for learning the significant
patterns. The finding is that the blog data was
very helpful, as the VIN approach averages only
0.66 on the BC3 data and 0.63 on the AMI data
when the blog patterns are not used, both signif-
icantly lower (p<0.01). Figure 2 shows the ROC
curves for the VIN approach with and without blog
patterns applied to the AMI subjectivity detection
task, illustrating the impact of the unsupervised
pattern-learning strategy.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
VIN with Blog Patterns
VIN without Blog Patterns
chance level
Figure 2: Effect of Blog Patterns on AMI Subjec-
tivity Task
5.2 Positive / Negative Classification
For the polarity classification task, the results dif-
fer between the two corpora. We describe the re-
sults on meetings and emails in turn.
5.2.1 AMI corpus
The p/r/f results for the AMI polarity task are pre-
sented in Table 6, with the scores pertaining to
the positive-subjective class. The VIN classifier
and full features classifier achieve the highest pre-
cision, but the f-measures are below the lower-
bound.
Comparing AUROC results, the VIN approach
is again significantly better than AutoSlog-TS,
averaging 0.65 compared with 0.56, and signifi-
cantly better than the standard trigram approach
(p<0.01 in both cases). The results are dis-
played in Figure 3. The conversational features are
significantly less effective than the VIN features
(p<0.05), and the best overall results are found by
utilizing all features, with significant improvement
over VIN only at p<0.05 and significant improve-
ment over AutoSlog-TS only at p<0.01.
5.2.2 BC3 corpus
The results of the polarity task on the BC3 cor-
pus are markedly different from the other exper-
imental results. In this case, neither VIN nor
AutoSlog-TS are particularly good for discrimi-
nating between positive and negative sentences,
and the best strategy is to use features relating to
conversational structure. According to p/r/f (Ta-
ble 6), the only method outperforming the lower-
bound in terms of f-measure is the conversational
features classifier. According to AUROC scores
shown in Figure 3, the conversational features by
themselves are significantly better than the VIN
approach (p<0.01 for non-paired t-test). So for
emails, we are more likely to correctly classify
positive and negative sentence by looking at fea-
tures such as position in the turn and participant
dominance than by matching our learned patterns.
While we showed previously that pattern-based
approaches perform well for the subjectivity task
on this dataset, there was less success in using the
patterns to discern the polarity of email sentences.
We are again interested in whether the use of the
BLOG06 data was beneficial. For the BC3 data,
there is very little difference between the VIN ap-
proach with and without the blog patterns, as they
both perform poorly, but with the AMI corpus, the
blog patterns yield significant improvement in po-
larity classification, increasing from an average of
0.56 without the blog patterns to 0.65 with them
(p<0.01).
6 Discussion and Future Work
A key difference between the AMI and BC3 data
with regards to subjectivity is that negative ut-
terances are much more common in the BC3
email threads. Additionally, the pattern-based ap-
proaches fared worst in discriminating between
1355
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Trig - AMI
Trig - BC3
Slog - AMI
Slog - BC3
VIN - AMI
VIN - BC3
Conv - AMI
Conv - BC3
All Feas - AMI
All Feas - BC3
AU
RO
C
Figure 3: AUROCs on Polarity Task for AMI and
BC3 corpora
negative and positive utterances in that corpus.
Positive and negative email sentences are more
easily recognized via features relating to conver-
sation structure and participant status than through
the learned lexical patterns.
The use of patterns learned from unlabeled blog
data significantly improved performance. We are
currently developing further techniques for learn-
ing subjective and polar patterns from such raw,
natural text.
A potential area of improvement is to reduce the
feature set by eliminating some of the subjective
patterns. In Section 2, we briefly described the
work of Riloff et al (2006) on feature subsump-
tion relationships. In our case, in a VIN instantia-
tion set a given trigram instantiation subsumes the
less abstract instantiations in the set, so the most
abstract instantiation (i.e. completely uninstanti-
ated trigram) representationally subsumes the rest.
Eliminating some of the representationally sub-
sumed instantiations when they are also behav-
iorally subsumed may improve our results.
It is difficult to compare our results directly with
those of Raaijmakers et al (2008) as they used a
smaller set of AMI meetings for their experiments,
and because for the first experiment we consider
the subjective class to be the union of positive-
subjective and negative-subjective dialogue acts
whereas they additionally include subjective ques-
tions and dialogue acts expressing uncertainty.
These differences are reflected by the substantially
differing scores reported for majority-vote base-
lines on each task. However, their success with
character n-gram features suggests that we could
improve our system by incorporating a variety of
character features. Character n-grams were the
best single feature class in their experiments.
The VIN representation is a general one and
may hold promise for learning patterns relevant to
other interesting conversation phenomena such as
decision-making and action items. We plan to ap-
ply the methods described here to these other ap-
plications in the near future.
7 Conclusion
In this work we have shown that learning subjec-
tive trigrams with varying instantiation levels from
both annotated and raw data can improve subjec-
tivity detection and polarity labeling for meeting
speech and email threads. The novel pattern-based
approach was significantly better than standard tri-
grams for three of the four tasks, and was signif-
icantly better than a state-of-the-art syntactic ap-
proach for those same tasks. We also found that
features relating to conversational structure were
beneficial for all tasks, and particularly for polar-
ity labeling in email data. Interestingly, in three
out of four cases combining all the features pro-
duced the best performance.
References
F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An un-
supervised approach to biography production using
wikipedia. In Proc. of ACL-HLT 2008, Columbus,
OH, USA.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of DARPA Speech and Natural Lan-
guage Workshop, San Mateo, CA, USA, pages 112?
116.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871?1874.
T. Fawcett. 2003. Roc graphs: Notes and practical
considerations for researchers.
G. Murray and G. Carenini. 2008. Summarizing spo-
ken and written conversations. In Proc. of EMNLP
2008, Honolulu, HI, USA.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 1-2(2):1?135.
1356
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In Proc. of EMNLP 2008, Honolulu, HI, USA.
E. Riloff and W. Phillips. 2004. An introduction to the
sundance and autoslog systems.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP
2003, Sapporo, Japan.
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Fea-
ture subsumption for opinion analysis. In Proc. of
EMNLP 2006, Sydney, Australia.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
Proc. of SIGDIAL 2007, Antwerp, Belgium.
J. Ulrich, G. Murray, and G. Carenini. 2008. A
publicly available annotated corpus for supervised
email summarization. In Proc. of AAAI EMAIL-
2008 Workshop, Chicago, USA.
T. Wilson, J. Wiebe, and R. Hwa. 2006. Recognizing
strong and weak opinion clauses. Computational In-
telligence, 22(2):73?99.
T. Wilson. 2008. Annotating subjective content in
meetings. In Proc. of LREC 2008, Marrakech, Mo-
rocco.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. of EMNLP 2003, Sapporo, Japan.
1357
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 388?398,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Exploiting Conversation Structure in Unsupervised Topic Segmentation for
Emails
Shafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymond T. Ng
{rjoty, carenini, gabrielm, rng}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
Abstract
This work concerns automatic topic segmen-
tation of email conversations. We present a
corpus of email threads manually annotated
with topics, and evaluate annotator reliabil-
ity. To our knowledge, this is the first such
email corpus. We show how the existing topic
segmentation models (i.e., Lexical Chain Seg-
menter (LCSeg) and Latent Dirichlet Alloca-
tion (LDA)) which are solely based on lex-
ical information, can be applied to emails.
By pointing out where these methods fail and
what any desired model should consider, we
propose two novel extensions of the models
that not only use lexical information but also
exploit finer level conversation structure in a
principled way. Empirical evaluation shows
that LCSeg is a better model than LDA for
segmenting an email thread into topical clus-
ters and incorporating conversation structure
into these models improves the performance
significantly.
1 Introduction
With the ever increasing popularity of emails and
web technologies, it is very common for people to
discuss issues, events, agendas or tasks by email.
Effective processing of the email contents can be
of great strategic value. In this paper, we study
the problem of topic segmentation for emails, i.e.,
grouping the sentences of an email thread into a
set of coherent topical clusters. Adapting the stan-
dard definition of topic (Galley et al, 2003) to con-
versations/emails, we consider a topic is something
about which the participant(s) discuss or argue or
express their opinions. For example, in the email
thread shown in Figure 1, according to the major-
ity of our annotators, participants discuss three top-
ics (e.g., ?telecon cancellation?, ?TAG document?,
and ?responding to I18N?). Multiple topics seem to
occur naturally in social interactions, whether syn-
chronous (e.g., chats, meetings) or asynchronous
(e.g., emails, blogs) conversations. In multi-party
chat (Elsner and Charniak, 2008) report an average
of 2.75 discussions active at a time. In our email cor-
pus, we found an average of 2.5 topics per thread.
Topic segmentation is often considered a pre-
requisite for other higher-level conversation analy-
sis and applications of the extracted structure are
broad, encompassing: summarization (Harabagiu
and Lacatusu, 2005), information extraction and or-
dering (Allan, 2002), information retrieval (Dias et
al., 2007), and intelligent user interfaces (Dredze et
al., 2008). While extensive research has been con-
ducted in topic segmentation for monologues (e.g.,
(Malioutov and Barzilay, 2006), (Choi et al, 2001))
and synchronous dialogs (e.g., (Galley et al, 2003),
(Hsueh et al, 2006)), none has studied the problem
of segmenting asynchronous multi-party conversa-
tions (e.g., email). Therefore, there is no reliable an-
notation scheme, no standard corpus, and no agreed-
upon metrics available. Also, it is our key hypothe-
sis that, because of its asynchronous nature, and the
use of quotation (Crystal, 2001), topics in an email
thread often do not change in a sequential way. As a
result, we do not expect models which have proved
successful in monologue or dialog to be as effective
when they are applied to email conversations.
Our contributions in this paper aim to remedy
388
these problems. First, we present an email corpus
annotated with topics and evaluate annotator agree-
ment. Second, we adopt a set of metrics to mea-
sure the local and global structural similarity be-
tween two annotations from the work on multi-party
chat disentanglement (Elsner and Charniak, 2008).
Third, we show how the two state-of-the-art topic
segmentation methods (i.e., LCSeg and LDA) which
are solely based on lexical information and make
strong assumptions on the resulting topic models,
can be effectively applied to emails, by having them
to consider, in a principled way, a finer level struc-
ture of the underlying conversations. Experimen-
tal results show that both LCSeg and LDA benefit
when they are extended to consider the conversa-
tional structure. When comparing the two methods,
we found that LCSeg is better than LDA and this
advantage is preserved when they are extended to
incorporate conversational structure.
2 Related Work
Three research areas are directly related to our study:
a) text segmentation models, b) probabilistic topic
models, and c) extracting and representing the con-
versation structure of emails.
Topic segmentation has been extensively studied
both for monologues and dialogs. (Malioutov and
Barzilay, 2006) uses the minimum cut model to seg-
ment spoken lectures (i.e., monologue). They form a
weighted undirected graph where the vertices repre-
sent sentences and the weighted links represent the
similarity between sentences. Then the segmenta-
tion problem can be solved as a graph partitioning
problem, where the assumption is that the sentences
in a segment should be similar, while sentences in
different segments should be dissimilar. They op-
timize the ?normalized cut? criterion to extract the
segments. In general, the minimization of the nor-
malized cut criterion is NP-complete. However, the
linearity constraint on text segmentation for mono-
logue allows them to find an exact solution in poly-
nomial time. In our extension of LCSeg, we use
a similar method to consolidate different segments;
however, in our case the linearity constraint is ab-
sent. Therefore, we approximate the optimal solu-
tion by spectral clustering (Shi and Malik, 2000).
Moving to the task of segmenting dialogs, (Galley
et al, 2003) first proposed the lexical chain based
unsupervised segmenter (LCSeg) and a supervised
segmenter for segmenting meeting transcripts. Their
supervised approach uses C4.5 and C4.5 rules binary
classifiers with lexical and conversational features
(e.g., cue phrase, overlap, speaker, silence, and lex-
ical cohesion function). Their supervised approach
performs significantly better than LCSeg. (Hsueh
et al, 2006) follow the same approaches as (Galley
et al, 2003) on both manual transcripts and ASR
output of meetings. They perform segmentation at
both coarse (topic) and fine (subtopic) levels. For
the topic level, they achieve similar results as (Gal-
ley et al, 2003), with the supervised approach out-
performing LCSeg. However, for the subtopic level,
LCSeg performs significantly better than the super-
vised one. In our work, we show how LCSeg per-
forms when applied to the temporal ordering of the
emails in a thread. We also propose its extension to
leverage the finer conversation structure of emails.
The probabilistic generative topic models, such
as LDA and its variants (e.g., (Blei et al, 2003),
(Steyvers and Griffiths, 2007)), have proven to be
successful for topic segmentation in both mono-
logue (e.g., (Chen et al, 2009)) and dialog (e.g.,
(Georgescul et al, 2008)). (Purver et al, 2006) uses
a variant of LDA for the tasks of segmenting meet-
ing transcripts and extracting the associated topic
labels. However, their approach for segmentation
does not perform better than LCSeg. In our work,
we show how the general LDA performs when ap-
plied to email conversations and describe how it can
be extended to exploit the conversation structure of
emails.
Several approaches have been proposed to cap-
ture an email conversation . Email programs (e.g.,
Gmail, Yahoomail) group emails into threads using
headers. However, our annotations show that top-
ics change at a finer level of granularity than emails.
(Carenini et al, 2007) present a method to capture an
email conversation at the finer level by analyzing the
embedded quotations in emails. A fragment quota-
tion graph (FQG) is generated, which is shown to be
beneficial for email summarization. In this paper, we
show that topic segmentation models can also bene-
fit significantly from this fine conversation structure
of email threads.
389
3 Corpus and Evaluation Metrics
There are no publicly available email corpora anno-
tated with topics. Therefore, the first step was to
develop our own corpus. We have annotated the
BC3 email corpus (Ulrich et al, 2008) with top-
ics1. The BC3 corpus, previously annotated with
sentence level speech acts, meta sentence, subjectiv-
ity, extractive and abstractive summaries, is one of a
growing number of corpora being used for email re-
search. The corpus contains 40 email threads from
the W3C corpus2. It has 3222 sentences and an av-
erage of 5 emails per thread.
3.1 Topic Annotation
Topic segmentation in general is a nontrivial and
subjective task (Hsueh et al, 2006). The conver-
sation phenomenon called ?Schism? makes it even
more challenging for conversations. In schism a
new conversation takes birth from an existing one,
not necessarily because of a topic shift but because
some participants refocus their attention onto each
other, and away from whoever held the floor in the
parent conversation and the annotators can disagree
on the birth of a new topic (Aoki et al, 2006). In the
example email thread shown in Figure 1, a schism
takes place when people discuss about ?responding
to I18N?. All the annotators do not agree on the fact
that the topic about ?responding to I18N? swerves
from the one about ?TAG document?. The annota-
tors can disagree on the number of topics (i.e., some
are specific and some are general), and on the topic
assignment of the sentences3. To properly design an
effective annotation manual and procedure we per-
formed a two-phase pilot study before carrying out
the actual annotation. For the pilot study we picked
five email threads randomly from the corpus. In the
first phase of the pilot study we selected five uni-
versity graduate students to do the annotation. We
then revised our instruction manual based on their
feedback and the source of disagreement found. In
1The BC3 corpus had already been annotated for email sum-
marization, speech act recognition and subjectivity detection.
This new annotation with topics will be also made publicly
available at http://www.cs.ubc.ca/labs/lci/bc3.html
2http://research.microsoft.com/en-
us/um/people/nickcr/w3c-summary.html
3The annotators also disagree on the topic labels, however
in this work we are not interested in finding the topic labels.
the second phase we tested with a university postdoc
doing the annotation.
For the actual annotation we selected three com-
puter science graduates who are also native speakers
of English. They annotated 39 threads of the BC3
corpus4. On an average they took seven hours to an-
notate the whole dataset.
BC3 contains three human written abstract sum-
maries for each email thread. With each email thread
the annotators were also given an associated human
written summary to give a brief overview of the cor-
responding conversation. The task of finding topics
was carried out in two phases. In the first phase, the
annotators read the conversation and the associated
summary and list the topics discussed. They spec-
ify the topics by a short description (e.g., ?meeting
agenda?, ?location and schedule?) which provides a
high-level overview of the topic. The target number
of topics and the topic labels were not given in ad-
vance and they were instructed to find as many top-
ics as needed to convey the overall content structure
of the conversation.
In the second phase the annotators identify the
most appropriate topic for each sentence. However,
if a sentence covers more than one topic, they were
asked to label it with all the relevant topics according
to their order of relevance. If they find any sentence
that does not fit into any topic, they are told to label
those as the predefined topic ?OFF-TOPIC?. Wher-
ever appropriate they were also asked to make use of
two other predefined topics: ?INTRO? and ?END?.
INTRO (e.g., ?hi?, ?hello?) signifies the section (usu-
ally at the beginning) of an email that people use to
begin their email. Likewise, END (e.g., ?Cheers?,
?Best?) signifies the section (usually at the end) that
people use to end their email. The annotators car-
ried out the task on paper. We created the hierar-
chical thread view (?reply to? relation) using ?TAB?s
(indentation) and each participant?s name is printed
in a different color as in Gmail.
Table 1 shows some basic statistics computed on
the three annotations of the 39 email threads5. On
4The annotators in the pilot and in the actual study were dif-
ferent so we could reuse the threads used in pilot study. How-
ever, one thread on which the pilot annotators agree fully, was
used as an example in the instruction manual. This gives 39
threads left for the actual study.
5We got 100% agreement on the two predefined topics ?IN-
390
average we have 26.3 sentences and 2.5 topics per
thread. A topic contains an average of 12.6 sen-
tences. The average number of topics active at a
time is 1.4. The average entropy is 0.94 and cor-
responds (as described in detail in the next section)
to the granularity of the annotation. These statistics
(number of topics and topic density) indicate that the
dataset is suitable for topic segmentation.
Mean Max Min
Number of sentences 26.3 55 13
Number of topics 2.5 7 1
Avg. topic length 12.6 35 3
Avg. topic density 1.4 3.1 1
Entropy 0.94 2.7 0
Table 1: Corpus statistics of human annotations
Metrics Mean Max Min
1-to-1 0.804 1 0.31
lock 0.831 1 0.43
m-to-1 0.949 1 0.61
Table 2: Annotator agreement in the scale of 0 to 1
3.2 Evaluation Metrics
In this section we describe the metrics used to com-
pare different human annotations and system?s out-
put. As different annotations (or system?s output)
can group sentences in different number of clusters,
metrics widely used in classification, such as the ?
statistic, are not applicable. Again, our problem of
topic segmentation for emails is not sequential in na-
ture. Therefore, the standard metrics widely used in
sequential topic segmentation for monologues and
dialogs, such as Pk and WindowDiff(WD), are
also not applicable. We adopt the more appropri-
ate metrics 1-to-1, lock and m-to-1, introduced re-
cently by (Elsner and Charniak, 2008). The 1-to-1
metric measures the global similarity between two
annotations. It pairs up the clusters from the two
annotations in a way that maximizes (globally) the
total overlap and then reports the percentage of over-
lap. lock measures the local agreement within a con-
TRO? and ?END?. In all our computation (i.e., statistics, agree-
ment, system?s input) we excluded the sentences marked as ei-
ther ?INTRO? or ?END?
text of k sentences. To compute the loc3 metric for
the m-th sentence in the two annotations, we con-
sider the previous 3 sentences: m-1, m-2 and m-3,
and mark them as either ?same? or ?different? de-
pending on their topic assignment. The loc3 score
between two annotations is the mean agreement on
these ?same? or ?different? judgments, averaged over
all sentences. We report the agreement found in 1-
to-1 and lock in Table 2. In both of the metrics we
get high agreement, though the local agreement (av-
erage of 83%) is little higher than the global agree-
ment (average of 80%).
If we consider the topic of a randomly picked sen-
tence as a random variable then its entropy measures
the level of detail in an annotation. If the topics are
evenly distributed then the uncertainty (i.e., entropy)
is higher. It also increases with the increase of the
number of topics. Therefore, it is a measure of how
specific an annotator is and in our dataset it varies
from 0 6 to 2.7. To measure how much the annota-
tors agree on the general structure we use the m-to-1
metric. It maps each of the source clusters to the
single target cluster with which it gets the highest
overlap, then computes the total percentage of over-
lap. This metric is asymmetrical and not a measure
to be optimized7, but it gives us some intuition about
specificity (Elsner and Charniak, 2008). If one an-
notator divides a cluster into two clusters then, the
m-to-1 metric from fine to coarse is 1. In our corpus
by mapping from fine to coarse we get an m-to-1
average of 0.949.
4 Topic Segmentation Models
Developing automatic tools for segmenting an email
thread is challenging. The example email thread in
Figure 1 demonstrates why. We use different col-
ors and fonts to represent sentences of different top-
ics8. One can notice that email conversations are
different from written monologues (e.g., newspaper)
and dialogs (e.g., meeting, chat) in various ways.
As a communication media Email is distributed (un-
like face to face meeting) and asynchronous (unlike
60 uncertainty happens when there is only one topic found
7hence we do not use it to compare our models.
82 of the 3 annotators agree on this segmentation. Green rep-
resents topic 1 (?telecon cancellation?), orange indicates topic 2
(?TAG document?) and magenta represents topic 3 (?responding
to I18N?)
391
chat), meaning that different people from different
locations can collaborate at different times. There-
fore, topics in an email thread may not change in
sequential way. In the example, we see that topic 1
(i.e., ?telecon cancellation?) is revisited after some
gaps.
The headers (i.e., subjects) do not convey much
information and are often misleading. In the exam-
ple thread, participants use the same subject (i.e.,
20030220 telecon) but they talk about ?responding
to I18N? and ?TAG document? instead of ?telecon
cancellation?. Writing style varies among partici-
pants, and many people tend to use informal, short
and ungrammatical sentences. These properties of
email limit the application of techniques that have
been successful in monologues and dialogues.
LDA and LCSeg are the two state-of-the-art mod-
els for topic segmentation of multi-party conversa-
tion (e.g., (Galley et al, 2003), (Hsueh et al, 2006),
(Georgescul et al, 2008)). In this section, at first we
describe how the existing models of topic segmen-
tation can be applied to emails. We then point out
where these methods fail and propose extensions of
these basic models for email conversations.
4.1 Latent Dirichlet Allocation (LDA)
Our first model is the probabilistic LDA model
(Steyvers and Griffiths, 2007). This model relies on
the fundamental idea that documents are mixtures of
topics, and a topic is a multinomial distribution over
words. The generative topic model specifies the fol-
lowing distribution over words within a document:
P (wi) =
T?
j=1
P (wi|zi = j)P (zi = j)
Where T is the number of topics. P (wi|zi = j) is
the probability of word wi under topic j and P (zi =
j) is the probability that jth topic was sampled for
the ith word token. We refer the multinomial dis-
tributions ?(j) = P (w|zi = j) and ?(d) = P (z)
as topic-word distribution and document-topic dis-
tribution respectively. (Blei et al, 2003) refined this
basic model by placing a Dirichlet (?) prior on ?.
(Griffiths and Steyvers, 2003) further refined it by
placing a Dirichlet (?) prior on ?. The inference
problem is to find ? and ? given a document set.
Variational EM has been applied to estimate these
two parameters directly. Instead of estimating ? and
?, one can also directly estimate the posterior distri-
bution over z = P (zi = j|wi) (topic assignments
for words). One efficient estimation technique uses
Gibbs sampling to estimate this distribution.
This framework can be directly applied to an
email thread by considering each email as a doc-
ument. Using LDA we get z = P (zi = j|wi)
(i.e., topic assignments for words). By assuming the
words in a sentence occur independently we can esti-
mate the topic assignments for sentences as follows:
P (zi = j|sk) =
?
wi?sk
P (zi = j|wi)
where, sk is the kth sentence for which we can
assign the topic by: j? = argmaxjP (zi = j|sk).
4.2 Lexical Chain Segmenter (LCSeg)
Our second model is the lexical chain based seg-
menter LCSeg, (Galley et al, 2003). LCSeg as-
sumes that topic shifts are likely to occur where
strong term repetitions start and end9. LCSeg at first
computes ?lexical chains? for each non-stop word
based on word repetitions. It then ranks the chains
according to two measures: ?number of words in the
chain? and ?compactness of the chain?. The more
compact (in terms of number of sentences) and the
more populated chains get higher scores.
The algorithm then works with two adjacent anal-
ysis windows, each of a fixed size k which is em-
pirically determined. For each sentence boundary,
LCSeg computes the cosine similarity (or lexical co-
hesion function) at the transition between the two
windows. Low similarity indicates low lexical cohe-
sion, and a sharp change signals a high probability
of an actual topic boundary. This method is similar
to TextTiling (Hearst, 1997) except that the similar-
ity is computed based on the scores of the ?lexical
chains? instead of ?term counts?. In order to apply
LCSeg on email threads we arrange the emails based
on their temporal relation (i.e., arrival time) and ap-
ply the LCSeg algorithm to get the topic boundaries.
9One can also consider other lexical semantic relations (e.g.,
synonym, hypernym, hyponym) in lexical chaining. However,
Galley et al, (Galley et al, 2003) uses only repetition relation
as previous research results (e.g., (Choi, 2000)) account only
for repetition.
392
From: Brian To: rdf core Subject: 20030220 telecon Date: Tue Feb 17 13:52:15 
 	
		
	

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 894?902,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interpretation and Transformation for Abstracting Conversations
Gabriel Murray
gabrielm@cs.ubc.ca
Giuseppe Carenini
carenini@cs.ubc.ca
Department of Computer Science, University of British Columbia
Vancouver, Canada
Raymond Ng
rng@cs.ubc.ca
Abstract
We address the challenge of automatically ab-
stracting conversations such as face-to-face
meetings and emails. We focus here on
the stages of interpretation, where sentences
are mapped to a conversation ontology, and
transformation, where the summary content
is selected. Our approach is fully developed
and tested on meeting speech, and we subse-
quently explore its application to email con-
versations.
1 Introduction
The dominant approach to the challenge of auto-
matic summarization has been extraction, where in-
formative sentences in a document are identified and
concatenated to form a condensed version of the
original document. Extractive summarization has
been popular at least in part because it is a binary
classification task that lends itself well to machine
learning techniques, and does not require a natural
language generation (NLG) component. There is ev-
idence that human abstractors at times use sentences
from the source documents nearly verbatim in their
own summaries, justifying this approach to some ex-
tent (Kupiec et al, 1995). Extrinsic evaluations have
also shown that, while extractive summaries may be
less coherent than human abstracts, users still find
them to be valuable tools for browsing documents
(He et al, 1999; McKeown et al, 2005; Murray et
al., 2008).
However, these same evaluations also indicate
that concise abstracts are generally preferred by
users and lead to higher objective task scores. The
limitation of a cut-and-paste summary is that the
end-user does not know why the selected sentences
are important; this can often only be discerned by
exploring the context in which each sentence origi-
nally appeared. One possible improvement is to cre-
ate structured extracts that represent an increased
level of abstraction, where selected sentences are
grouped according to phenomena such as decisions,
action items and problems, thereby giving the user
more information on why the sentences are being
highlighted. For example, the sentence Let?s go with
a simple chip represents a decision. An even higher
level of abstraction can be provided by generating
new text that synthesizes or extrapolates on the in-
formation contained in the structured summary. For
example, the sentence Sandra and Sue expressed
negative opinions about the remote control design
can be coupled with extracted sentences containing
these negative opinions, forming a hybrid summary.
Our summarization system ultimately performs both
types of abstraction, grouping sentences according
to various sentence-level phenomena, and generat-
ing novel text that describes this content at a higher
level.
In this work we describe the first two components
of our abstractive summarization system. In the in-
terpretation stage, sentences are mapped to nodes
in a conversation ontology by utilizing classifiers
relating to a variety of sentence-level phenomena
such as decisions, action items and subjective sen-
tences. These classifiers achieve high accuracy by
using a very large feature set integrating conversa-
tion structure, lexical patterns, part-of-speech (POS)
tags and character n-grams. In the transformation
stage, we select the most informative sentences by
maximizing a function based on the derived ontol-
ogy mapping and the coverage of weighted enti-
ties mentioned in the conversation. This transforma-
tion component utilizes integer linear programming
(ILP) and we compare its performance with several
greedy selection algorithms.
We do not discuss the generation compo-
nent of our summarization system in this pa-
per. The transformation component is still ex-
894
tractive in nature, but the sentences that are se-
lected in the transformation stage correspond to
objects in the ontology and the properties link-
ing them. Specifically, these are triples of the
form < participant, relation, entity > where a
participant is a person in the conversation, an
entity is an item under discussion, and a relation
such as positive opinion or action item links the two.
This intermediate output enables us to create struc-
tured extracts as described above, with the triples
also acting as input to the downstream NLG com-
ponent.
We have tested our approach in summarization
experiments on both meeting and email conversa-
tions, where the quality of a sentence is measured
by how effectively it conveys information in a model
abstract summary according to human annotators.
On meetings the ILP approach consistently outper-
forms several greedy summarization methods. A
key finding is that emails exhibit markedly varying
conversation structures, and the email threads yield-
ing the best summarization results are those that are
structured similarly to meetings. Other email con-
versation structures are less amenable to the current
treatment and require further investigation and pos-
sibly domain adaptation.
2 Related Research
The view that summarization consists of stages of
interpretation, transformation and generation was
laid out by Sparck-Jones (1999). Popular ap-
proaches to text extraction essentially collapse inter-
pretation and transformation into one step, with gen-
eration either being ignored or consisting of post-
processing techniques such as sentence compres-
sion (Knight and Marcu, 2000; Clarke and Lapata,
2006) or sentence merging (Barzilay and McKeown,
2005). In contrast, in this work we clearly separate
interpretation from transformation.
The most relevant research to ours is by Klein-
bauer et al (2007), similarly focused on meet-
ing abstraction. They create an ontology for the
AMI scenario meeting corpus (Carletta et al, 2005),
described in Section 5.1. The system uses topic
segments and topic labels, and for each topic seg-
ment in the meeting a sentence is generated that de-
scribes the most frequently mentioned content items
in that topic. Our systems differ in two major re-
spects: their summarization process uses human
gold-standard annotations of topic segments, topic
labels and content items from the ontology, while
our summarizer is fully automatic; and the ontology
used by Kleinbauer et al is specific not just to meet-
ings but to the AMI scenario meetings, while our
ontology applies to conversations in general.
While the work by Kleinbauer et al is among
the earliest research on abstracting multi-party dia-
logues, much attention in recent years has been paid
to extractive summarization of such conversations,
including meetings (Galley, 2006), emails (Rambow
et al, 2004; Carenini et al, 2007), telephone con-
versations (Zhu and Penn, 2006) and internet relay
chats (Zhou and Hovy, 2005).
Recent research has addressed the challenges of
detecting decisions (Hsueh et al, 2007), action items
(Purver et al, 2007; Murray and Renals, 2008) and
subjective sentences (Raaijmakers et al, 2008). In
our work we perform all of these tasks but rely on
general conversational features without recourse to
meeting-specific or email-specific features.
Our approach to transformation is an adaptation
of an ILP sentence selection algorithm described by
Xie et al (2009). We describe both ILP approaches
in Section 4.
3 Interpretation - Ontology Mapping
Source document interpretation in our system re-
lies on a simple conversation ontology. The ontol-
ogy is written in OWL/RDF and contains two core
upper-level classes: Participant and Entity. When
additional information is available about participant
roles in a given domain, Participant subclasses such
as ProjectManager can be utilized. The ontology
also contains six properties that express relations be-
tween the participants and the entities. For example,
the following snippet of the ontology indicates that
hasActionItem is a relationship between a meeting
participant (the property domain) and a discussed
entity (the property range).
<owl:ObjectProperty rdf:ID="hasActionItem">
<rdfs:domain rdf:resource="#Participant"/>
<rdfs:range rdf:resource="#Entity"/>
</owl:ObjectProperty>
Similar properties exist for decisions, actions,
problems, positive-subjective sentences, negative-
895
subjective sentences and general extractive sen-
tences (important sentences that may not match the
other categories), all connecting conversation par-
ticipants and entities. The goal is to populate the
ontology with participant and entity instances from
a given conversation and determine their relation-
ships. This involves identifying the important en-
tities and classifying the sentences in which they
occur as being decision sentences, action item sen-
tences, etc.
Our current definition of entity is simple. The en-
tities in a conversation are noun phrases with mid-
range document frequency. This is similar to the
definition of concept as defined by Xie et al (Xie
et al, 2009), where n-grams are weighted by tf.idf
scores, except that we use noun phrases rather than
any n-grams because we want to refer to the enti-
ties in the generated text. We use mid-range doc-
ument frequency instead of idf (Church and Gale,
1995), where the entities occur in between 10% and
90% of the documents in the collection. In Section 4
we describe how we use the entity?s term frequency
to detect the most informative entities. We do not
currently attempt coreference resolution for entities;
recent work has investigated coreference resolution
for multi-party dialogues (Muller, 2007; Gupta et
al., 2007), but the challenge of resolution on such
noisy data is highlighted by low accuracy (e.g. F-
measure of 21.21) compared with using well-formed
text (e.g. monologues).
We map sentences to our ontology?s object prop-
erties by building numerous supervised classifiers
trained on labeled decision sentences, action sen-
tences, etc. A general extractive classifier is also
trained on sentences simply labeled as important.
After predicting these sentence-level properties, we
consider a participant to be linked to an entity if
the participant mentioned the entity in a sentence in
which one of these properties is predicted. We give a
specific example of the ontology mapping using this
excerpt from the AMI corpus:
1. A: And you two are going to work together on
a prototype using modelling clay.
2. A: You?ll get specific instructions from your
personal coach.
3. C: Cool.
4. A: Um did we decide on a chip?
5. A: Let?s go with a simple chip.
Example entities are italicized. Sentences 1 and
2 are classified as action items. Sentence 3 is clas-
sified as positive-subjective, but because it contains
no entities, no < participant, relation, entity >
triple can be added to the ontology. Sentence
4 is classified as a decision sentence, and Sen-
tence 5 is both a decision sentence and a positive-
subjective sentence (because the participant is advo-
cating a particular position). The ontology is pop-
ulated by adding all of the sentence entities as in-
stances of the Entity class, all of the participants
as instances of the Participant class, and adding
< participant, relation, entity > triples for Sen-
tences 1, 2, 4 and 5. For example, Sentence 5 results
in the following two triples being added to the on-
tology:
<ProjectManager rdf:ID="participant-A">
<hasDecision rdf:resource="#simple-chip"/>
</ProjectManager>
<ProjectManager rdf:ID="participant-A">
<hasPos rdf:resource="#simple-chip"/>
</ProjectManager>
Elements in the ontology are associated with lin-
guistic annotations used by the generation compo-
nent of our system; since we do not discuss the gen-
eration task here, we presently skip the details of this
aspect of the ontology. In the following section we
describe the features used for the ontology mapping.
3.1 Feature Set
The interpretation component uses general features
that are applicable to any conversation domain. The
first set of features we use for ontology mapping are
features relating to conversational structure. These
are listed and briefly described in Table 1. The
Sprob and Tprob features measure how terms clus-
ter between conversation participants and conver-
sation turns. There are simple features measur-
ing sentence length (SLEN, SLEN2) and position
(TLOC, CLOC). Pause-style features indicate how
much time transpires between the previous turn, the
current turn and the subsequent turn (PPAU, SPAU).
For email conversations, pause features are based on
the timestamps between consecutive emails. Lexical
features capture cohesion (CWS) and cosine sim-
ilarity between the sentence and the conversation
(CENT1, CENT2). All structural features are nor-
malized by document length.
896
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 1: Features Key
While these features have been found to work
well for generic extractive summarization, we use
additional features for capturing the more specific
sentence-level phenomena of this research.
? Character trigrams We derive all of the char-
acter trigrams in the collected corpora and in-
clude features indicating the presence or ab-
sence of each trigram in a given sentence.
? Word bigrams We similarly derive all of the
word bigrams in the collected corpora.
? POS bigrams We similarly derive all of the
POS-tag bigrams in the collected corpora.
? Word pairs We consider w1, w2 to be a word
pair if they occur in the same sentence and w1
precedes w2. We derive all of the word pairs
in the collected corpora and includes features
indicating the presence or absence of each word
pair in the given sentence. This is essentially a
skip bigram where any amount of intervening
material is allowed as long as the words occur
in the same sentence.
? POS pairs We calculate POS pairs in the same
manner as word pairs, above. These are essen-
tially skip bigrams for POS tags.
? Varying instantiation ngrams We derive a
simplified set of VIN features for these exper-
iments. For each word bigram w1, w2, we fur-
ther represent the bigram as p1, w2 and w1, p2
so that each pattern consists of a word and a
POS tag. We include a feature indicating the
presence or absence of each of these varying
instantiation bigrams.
After removing features that occur fewer than five
times, we end up with 218,957 total features.
4 Transformation - ILP Content Selection
In the previous section we described how we
identify sentences that link participants and enti-
ties through a variety of sentence-level phenom-
ena. Having populated our ontology with these
triples to form a source representation, we now turn
to the task of transforming the source representa-
tion to a summary representation, identifying the <
participant, relation, entity > triples for which
we want to generate text. We adapt a method pro-
posed by Xie et al (2009) for extractive sentence
selection. They propose an ILP approach that cre-
ates a summary by maximizing a global objective
function:
maximize (1 ? ?) ?
?
i
wici + ? ?
?
j
ujsj (1)
subject to
?
j
ljsj < L (2)
where wi is the tf.idf score for concept i, uj is the
weight for sentence j using the cosine similarity to
the entire document, ci is a binary variable indicat-
ing whether concept i is selected (with the concept
represented by a unique weighted n-gram), sj is a
binary variable indicating whether sentence j is se-
lected, lj is the length of sentence j and L is the
desired summary length. The ? term is used to bal-
ance concept and sentence weights. This method se-
lects sentences that are weighted strongly and which
cover as many important concepts as possible. As
described by Gillick et al (2009), concepts and
sentences are tied together by two additional con-
straints:
?
j
sjoij ? ci ?i (3)
sjoij ? ci ?i,j (4)
897
where oij is the occurence of concept i in sentence
j. These constraints state that a concept can only be
selected if it occurs in a sentence that is selected,
and that a sentence can only be selected if all of its
concepts have been selected.
We adapt their method in several ways. As men-
tioned in the previous section, we use weighted noun
phrases as our entities instead of n-grams. In our
version of Equation 1, wi is the tf score of en-
tity i (the idf was already used to identify entities
as described previously). More importantly, our
sentence weight uj is the sum of all the posterior
probabilities for sentence j derived from the various
sentence-level classifiers. In other words, sentences
are weighted highly if they correspond to multiple
object properties in the ontology. To continue the
example from Section 3, the sentence Let?s go with
the simple chip may be selected because it represents
both a decision and a positive-subjective opinion, as
well as containing the entity simple chip which is
mentioned frequently in the conversation.
We include constraint 3 but not 4; it is possi-
ble for a sentence to be extracted even if not all
of its entities are. We know that all the sentences
under consideration will contain at least one en-
tity because sentences with no entities would not
have been mapped to the ontology in the form of
< participant, relation, entity > triples in the
first place. To begin with, we set the ? term at 0.75
as we are mostly concerned with identifying impor-
tant sentences containing multiple links to the on-
tology. In our case L is 20% of the total document
word count.
5 Experimental Setup
In this section we describe our conversation cor-
pora, the statistical classifiers used, and the evalu-
ation metrics employed.
5.1 Corpora
These experiments are conducted on both meeting
and email conversations, which we describe in turn.
5.1.1 The AMI Meetings Corpus
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Carletta
et al, 2005), where groups of four participants take
part in a series of four meetings and play roles within
a fictitious company. There are 140 of these meet-
ings in total, including a 20 meeting test set contain-
ing multiple human summary annotations per meet-
ing (the others are annotated by a single individual).
We report results on both manual and ASR tran-
scripts. The word error rate for the ASR transcripts
is 38.9%.
For the summary annotation, annotators wrote ab-
stract summaries of each meeting and extracted sen-
tences that best conveyed or supported the informa-
tion in the abstracts. The human-authored abstracts
each contain a general abstract summary and three
subsections for ?decisions,? ?actions? and ?prob-
lems? from the meeting. A many-to-many mapping
between transcript sentences and sentences from the
human abstract was obtained for each annotator. Ap-
proximately 13% of the total transcript sentences are
ultimately labeled as extracted sentences. A sen-
tence is considered a decision item if it is linked to
the decision portion of the abstract, and action and
problem sentences are derived similarly.
For the subjectivity annotation, we use annota-
tions of positive-subjective and negative-subjective
utterances on a subset of 20 AMI meetings (Wil-
son, 2008). Such subjective utterances involve
the expression of a private state, such as a pos-
itive/negative opinion, positive/negative argument,
and agreement/disagreement. Of the roughly 20,000
total sentences in the 20 AMI meetings, nearly 4000
are labeled as positive-subjective and nearly 1300 as
negative-subjective.
5.1.2 The BC3 Email Corpus
While our main experiments focus on the AMI
meeting corpus, we follow these up with an inves-
tigation into applying our abstractive techniques to
email data. The BC3 corpus (Ulrich et al, 2008)
contains email threads from the World Wide Web
Consortium (W3C) mailing list. The threads fea-
ture a variety of topics such as web accessibility and
planning face-to-face meetings. The annotated por-
tion of the mailing list consists of 40 threads. The
threads are annotated in the same manner as the AMI
corpus, with three human annotators per thread first
authoring abstracts and then linking email thread
sentences to the abstract sentences. The corpus also
contains speech act annotations. Unlike the AMI
corpus, however, there are no annotations for deci-
898
sions, actions and problems, an issue addressed later.
5.2 Classifiers
For these experiments we use a maximum entropy
classifier using the liblinear toolkit1 (Fan et al,
2008). For each of the AMI and BC3 corpora, we
perform 10-fold cross-validation on the data. In all
experiments we apply a 20% compression rate in
terms of the total document word count.
5.3 Evaluation
We evaluate the various classifiers described in Sec-
tion 3 using the ROC curve and the area under the
curve (AUROC), where a baseline AUROC is 0.5
and an ideal classifier approaches 1.
To evaluate the content selection in the transfor-
mation stage, we use weighted recall.This evaluation
metric is based on the links between extracted sen-
tences and the human gold-standard abstracts, with
the underlying motivation being that sentences with
more links to the human abstract are generally more
informative, as they provide the content on which an
effective abstract summary should be built. If M is
the number of sentences selected in the transforma-
tion step, O is the total number of sentences in the
document, and N is the number of annotators, then
Weighted Recall is given by
recall =
?M
i=1
?N
j=1 L(si, aj)
?O
i=1
?N
j=1 L(si, aj)
where L(si, aj) is the number of links for a sen-
tence si according to annotator aj . We can com-
pare machine performance with human performance
in the following way. For each annotator, we rank
their sentences from most-linked to least-linked and
select the best sentences until we reach the same
word count as our selections. We then calculate their
weighted recall score by using the other N-1 annota-
tions, and then average over all N annotators to get
an average human performance. We report all trans-
formation scores normalized by human performance
for that dataset.
6 Results
In this section we present results for our interpreta-
tion and transformation components.
1http://www.csie.ntu.edu.tw/ cjlin/liblinear/
6.1 Interpretation: Meetings
Figure 1 shows the ROC curves for the sentence-
level classifiers applied to manual transcripts. On
both manual and ASR transcripts, the classifiers
with the largest AUROCs are the action item and
general extractive classifiers. Action item sentences
can be detected very well with this feature set, with
the classifier having an AUROC of 0.92 on man-
ual transcripts and 0.93 on ASR, a result compa-
rable to previous findings of 0.91 and 0.93 (Mur-
ray and Renals, 2008) obtained using a speech-
specific feature set. General extractive classification
is also similar to other state-of-the-art extraction ap-
proaches on spoken data using speech features (Zhu
and Penn, 2006)2 with an AUROC of 0.87 on man-
ual and 0.85 on ASR. Decision sentences can also
be detected quite well, with AUROCs of 0.81 and
0.77. Positive-subjective, negative-subjective and
problem sentences are the most difficult to detect,
but the classifiers still give credible performance
with AUROCs of approximately 0.76 for manual
and 0.70-0.72 for ASR.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
actions
decisions
problems
positive-subjective
negative-subjective
extractive
random
Figure 1: ROC Curves for Ontology Mapping Classifiers
(Manual Transcripts)
6.2 Transformation: Meetings
In this section we present the weighted recall scores
for the sentences selected using the ILP method de-
scribed in Section 4. Remember, weighted recall
measures how useful these sentences would be in
generating sentences for an abstract summary. We
also assess the performance of three baseline sum-
marizers operating at the same compression level.
2Based on visual inspection of their reported best ROC curve
899
The simplest baseline (GREEDY) selects sentences
by ranking the posterior probabilites output by the
general extractive classifier. The second baseline
(CLASS COMBO) averages the posterior proba-
bilites output by all the classifiers and ranks sen-
tences from best to worst. The third baseline (RE-
TRAIN) uses the posterior probability outputs of all
the classifiers (except for the extractive classifier) as
new feature inputs for the general extractive classi-
fier.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Greedy
Class Combo
Retrain
ILP
W
ei
gh
te
d 
Re
ca
ll, 
No
rm
al
ize
d
manual
ASR
Figure 2: Weighted Recall Scores for AMI Meetings
Figure 2 shows the weighted recall scores, nor-
malized by human performance, for all approaches
on both manual and ASR transcripts. On man-
ual transcripts, the ILP approach (0.76) is better
than GREEDY (0.71) with a marginally significant
difference (p=0.07) and is significantly better than
CLASS COMBO and RETRAIN (both 0.68) ac-
cording to t-test (p < 0.05) . For ASR transcripts,
the ILP approach is significantly better than all other
approaches (p < 0.05). Xie et al (2009) reported
ROUGE-1 F-measures on a different meeting cor-
pus, and our ROUGE-1 scores are in the same range
of 0.64-0.69 (they used 18% compression ratio).
6.3 Interpretation: Emails
We applied the same summarization method to the
40 BC3 email threads, with contrasting results. Be-
cause the BC3 corpus does not currently contain an-
notations for decisions, actions and problems, we
simply ran the AMI-trained models over the data
for those three phenomena. We can assess the
performance of the extractive, positive-subjective
and negative-subjective classifiers by examining the
ROC curves displayed in Figure 3. Both the general
extractive and negative-subjective classifiers have
AUROCs of around 0.75. The positive-subjective
classifier initially has the worst performance with
an AUROC of 0.66, but we found that positive-
subjective performance increased dramatically to an
AUROC of 0.77 when we used only conversational
features and not word bigrams, character trigrams or
POS tags.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
positive-subjective
negative-subjective
extractive
random
Figure 3: ROC Curves for Ontology Mapping Classifiers
(BC3 Corpus)
6.4 Transformation: Emails
If we examine the weighted recall scores in Fig-
ure 4 we see that the ILP approach is worse than
the greedy summarizers on the BC3 dataset. How-
ever, the differences are not significant between ILP
and COMBO CLASS (p=0.15) and only marginally
significant compared with RETRAIN and GREEDY
(both p=0.08). The performance of the ILP approach
varies greatly across email threads. The top 15
threads (out of 40) yield ILP weighted recall scores
that are on par with human performance, while the
worst 15 are half that.
6.4.1 Email Corpus Analysis
Due to the large discrepancy in performance on
BC3 emails, we conducted additional experiments
for error analysis. We first explored whether we
could build a classifier that could discriminate the
best 15 emails from the worst 15 emails in terms of
weighted recall scores with the ILP approach, to de-
termine whether there are certain features that cor-
relate with good performance. Using the same fea-
900
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Greedy
Class Combo
Retrain
ILP
W
ei
gh
te
d 
Re
ca
ll, 
No
rm
al
ize
d
Figure 4: Weighted Recall Scores for BC3 Threads
tures described in Section 3.1, we built a logistic re-
gression classifier on the two classes and found that
they can be discriminated quite well (80% accuracy
on an approximately balanced dataset) and that the
conversation structure features are the most useful
for discerning them. Table 2 shows the weighted
recall scores and several conversation features that
were weighted most highly by the logistic regres-
sion model. In particular, we found that the email
threads that yielded good performance tended to fea-
ture more active participants (# Participants), were
not dominated by a single individual (BEGAUTH),
and featured a higher number of turns (# Turns)
that followed each other in quick succession without
long pauses (PPAU, pause as percentage of conver-
sation length). In other words, these emails were
structured more similarly to meetings. Note that
since we normalize weighted recall by human per-
formance, it is possible to have a weighted recall
score higher than 1. On the 15 best threads, our sys-
tem achieves human-level performance. Because we
used AMI-trained models for detecting decisions,
actions and problems in the BC3 data, it is not sur-
prising that performance was better on those emails
structured similarly to meetings. All of this indicates
that there are many different types of emails and that
we will have to focus on improving performance on
emails that differ markedly in structure.
7 Conclusion
We have presented two components of an abstractive
conversation summarization system. The interpreta-
tion component is used to populate a simple conver-
Metric Worst 15 Best 15
Weighted Recall 0.49 1.05
# Turns 6.27 6.73
# Participants 4.67 5.4
PPAU 0.18 0.12
BEGAUTH 0.31 0.18
Table 2: Selected Email Features, Averaged
sation ontology where conversation participants and
entities are linked by object properties such as deci-
sions, actions and subjective opinions. For this step
we show that highly accurate classifiers can be built
using a large set of features not specific to any con-
versation modality.
In the transformation step, a summary is cre-
ated by maximizing a function relating sentence
weights and entity weights, with the sentence
weights determined by the sentence-ontology map-
ping. Our evaluation shows that the sentences we
select are highly informative to generate abstract
summaries, and that our content selection method
outperforms several greedy selection approaches.
The system described thus far may appear extrac-
tive in nature, as the transformation step is iden-
tifying informative sentences in the conversation.
However, these selected sentences correspond to
< participant, relation, entity > triples in the
ontology, for which we can subsequently gener-
ate novel text by creating linguistic annotations of
the conversation ontology (Galanis and Androut-
sopolous, 2007). Even without the generation step,
the approach described above allows us to create
structured extracts by grouping sentences according
to specific phenomena such as action items and de-
cisions. The knowledge represented by the ontology
enables us to significantly improve sentence selec-
tion according to intrinsic measures and to generate
structured output that we hypothesize will be more
useful to an end user compared with a generic un-
structured extract.
Future work will focus on the generation compo-
nent and on applying the summarization system to
conversations in other modalities such as blogs and
instant messages. Based on the email error analysis,
we plan to pursue domain adaptation techniques to
improve performance on different types of emails.
901
References
R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3):297?328.
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proc. of ACM
WWW 07, Banff, Canada.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Well-
ner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
K. Church and W. Gale. 1995. Inverse document fre-
quency IDF: A measure of deviation from poisson. In
Proc. of the Third Workshop on Very Large Corpora,
pages 121?130.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proc. of COLING/ACL 2006, pages 144?
151.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large linear
classification. Journal of Machine Learning Research,
9:1871?1874.
D. Galanis and I. Androutsopolous. 2007. Generating
multilingual descriptions from linguistically annotated
owl ontologies: the naturalowl system. In Proc. of
ENLG 2007, Schloss Dagstuhl, Germany.
M. Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP 2006, Sydney, Australia, pages 364?
372.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-
Tu?r. 2009. A global optimization framework for meet-
ing summarization. In Proc. of ICASSP 2009, Taipei,
Taiwan.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky. 2007.
Resolving ?You? in multi-party dialog. In Proc. of
SIGdial 2007, Antwerp, Belgium.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In Proc.
of ACM MULTIMEDIA ?99, Orlando, FL, USA, pages
489?498.
P-Y. Hsueh, J. Kilgour, J. Carletta, J. Moore, and S. Re-
nals. 2007. Automatic decision detection in meeting
speech. In Proc. of MLMI 2007, Brno, Czech Repub-
lic.
K. Spa?rck Jones. 1999. Automatic summarizing: Factors
and directions. In I. Mani and M. Maybury, editors,
Advances in Automatic Text Summarization, pages 1?
12. MITP.
T. Kleinbauer, S. Becker, and T. Becker. 2007. Com-
bining multiple information layers for the automatic
generation of indicative meeting abstracts. In Proc. of
ENLG 2007, Dagstuhl, Germany.
K. Knight and D. Marcu. 2000. Statistics-based summa-
rization - step one: Sentence compression. In Proc. of
AAAI 2000, Austin, Texas, USA, pages 703?710.
J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In Proc. of the 18th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval. Seattle, Wash-
ington, USA, pages 68?73.
K. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005. From text to speech summarization. In Proc. of
ICASSP 2005, Philadelphia, USA, pages 997?1000.
C. Muller. 2007. Resolving It, This and That in un-
restricted multi-party dialog. In Proc. of ACL 2007,
Prague, Czech Republic.
G. Murray and S. Renals. 2008. Detecting action items
in meetings. In Proc. of MLMI 2008, Utrecht, the
Netherlands.
G. Murray, T. Kleinbauer, P. Poller, S. Renals, T. Becker,
and J. Kilgour. 2008. Extrinsic summarization evalu-
ation: A decision audit task. In Proc. of MLMI 2008,
Utrecht, the Netherlands.
M. Purver, J. Dowding, J. Niekrasz, P. Ehlen, and
S. Noorbaloochi. 2007. Detecting and summariz-
ing action items in multi-party dialogue. In Proc. of
the 9th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Multi-
modal subjectivity analysis of multiparty conversation.
In Proc. of EMNLP 2008, Honolulu, HI, USA.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004, Boston, USA.
J. Ulrich, G. Murray, and G. Carenini. 2008. A publicly
available annotated corpus for supervised email sum-
marization. In Proc. of AAAI EMAIL-2008 Workshop,
Chicago, USA.
T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proc. of LREC 2008, Marrakech, Morocco.
S. Xie, B. Favre, D. Hakkani-Tu?r, and Y. Liu. 2009.
Leveraging sentence weights in a concept-based op-
timization framework for extractive meeting summa-
rization. In Proc. of Interspeech 2009, Brighton, Eng-
land.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet relay
chats. In Proc. of ACL 2005, Ann Arbor, MI, USA.
X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proc. of Interspeech 2006,
Pittsburgh, USA, pages 1531?1534.
902
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 16?22,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Domain Adaptation to Summarize Human Conversations 
  Oana Sandu, Giuseppe Carenini, Gabriel Murray, and Raymond Ng University of British Columbia Vancouver, Canada {oanas,carenini,gabrielm,rng}@cs.ubc.ca     Abstract  We are interested in improving the sum-marization of conversations by using domain adaptation. Since very few email corpora have been annotated for summa-rization purposes, we attempt to leverage the labeled data available in the multi-party meetings domain for the summari-zation of email threads. In this paper, we compare several approaches to super-vised domain adaptation using out-of-domain labeled data, and also try to use unlabeled data in the target domain through semi-supervised domain adapta-tion. From the results of our experiments, we conclude that with some in-domain labeled data, training in-domain with no adaptation is most effective, but that when there is no labeled in-domain data, domain adaptation algorithms such as structural correspondence learning can improve summarization.  1 Introduction On a given day, many people engage in conver-sations via several modalities, including face-to-face speech, telephone, email, SMS, chat, and blogs. Being able to produce automatic summa-ries of multi-party conversations occurring in one or several of these modalities would enable the parties involved to keep track of and make sense of this diverse data. However, summarizing spo-ken dialogue is more challenging than summariz-ing written monologues such as books and arti-cles, as speech tends to be more fragmented and disfluent. We are interested in using both fully and semi-supervised techniques to produce extractive summaries for conversations, where each sen-
tence of a text is labeled with its informativeness, and a subset of sentences are concatenated into an extractive summary of the text. In previous work (Murray and Carenini, 2008), it has been shown that conversations in different modalities can be effectively characterized by a set of ?con-versational? features that are useful in detecting informativeness for the task of extractive sum-marization. However, because of privacy con-cerns, annotated corpora are rarely publicly available for conversational data, including for the email domain. One promising solution to this problem is domain adaptation, which aims to use labeled data in a well-studied source domain and a limited amount of labeled data from a different target domain to train a model that performs well in that target domain. In this work, we investi-gate using domain adaptation that leverages la-beled data in the domain of meetings along with labeled and unlabeled email data for summariz-ing email threads. We evaluate several domain adaptation algorithms, using both a small set of conversational features and a large set of simple lexical features to determine what settings will yield the best results for summarizing email con-versations. In our experiments, we do not get a significant improvement from using out-of-domain data in addition to in-domain data in su-pervised domain adaptation, though in the setting where only unlabeled in-domain data is avail-able, we gain from using it through structural correspondence learning. We also observe that conversational features are more useful in super-vised methods, whereas lexical features are bet-ter leveraged in semi-supervised adaptation.  The next section surveys past research in do-main adaptation and in summarizing conversa-tional data. In section 3 we present the corpora and feature sets we used, and we describe our experimental setting in section 4. We then com-pare the performance of different methods in sec-tion 5 and draw conclusions in section 6.  
16
2 Related Work We give an overview first of work on supervised and semi-supervised domain adaptation, then of research on summarization of conversations. 2.1 Supervised Domain Adaptation Many domain adaptation methods have been proposed for the supervised case, where a small amount of labeled data in the target domain is used along with a larger amount of labeled source data. Two baseline approaches are to train only on the source data or only on target training data. One way of using information from both domains is merging the source and target labeled data sets and training a model on the combina-tion. A method inspired by boosting is to take a linear combination of the predictions of two clas-sifiers, one trained on the source and one trained on the target training data. Another simple me-thod is to train a predictor on the source data, run it on the target data, and then use its predictions on each instance as additional features for a target-trained model. This was first introduced by Florian et al (2004), who applied it to multilingual named entity recognition. The prior method of domain adaptation by Chelba and Acero (2006) involves using the source data to find optimal parameter values of a maximum entropy model on that data, and then setting these as a prior on the values of a model trained on the target data. They find improve-ment in a capitalizer that adapts using out-of-domain and a small amount of in-domain data versus only training on out-of-domain WSJ data.  Similar to the prior method, Daume?s MEGA model also trains a MEMM. It achieves domain adaptation through hyperparameters that indicate whether an instance is generated by a source, target, or general distribution, and finds the op-timal values of the parameters through condi-tional EM (Daume and Marcu, 2006). A simpler method of domain adaptation, that achieves a performance similar to prior and MEGA, was proposed by Daume (2007) and successfully ap-plied to a variety of NPL sequence labeling prob-lems, such as named entity recognition, shallow parsing, and part-of-speech (POS) tagging. Fur-thermore, this approach is straightforward to ap-ply by copying feature values so there is a source version, a target version, and a general version of the feature, and was found to be faster to train than MEGA and prior. For all these reasons, we use Daume?s method and not the other two in our experiments. 
2.2 Semi-supervised Domain Adaptation Because unlabeled data is usually much easier to collect than labeled data in a new domain, semi-supervised domain adaptation methods that ex-ploit unlabeled data are potentially very useful.  In self-training, a training set is used that is originally composed of labeled data, and repeat-edly augmented with the highest confidence pre-dictions on unlabeled data. McClosky et al (2006) apply this in a domain adaptation setting for parsing: with only unlabeled data in the target Brown domain, and labeled and unlabeled datasets in the news domain (WSJ and NANC respectively), a self-trained reranking parser per-forms almost as well as a parser trained only on Brown labeled data. However, McClosky con-cludes that self-training alone is not beneficial, and most of the improvements they get over pre-vious work on domain adaptation for parsing are due to using the reranker to select the candidate instances produced in each iteration of self-training. Thus, one of the issues addressed in this paper is to asses whether self-training is useful for domain adaptation. A more sophisticated semi-supervised domain adaptation method is structural correspondence learning (SCL). SCL uses unlabeled data to de-termine correspondences between features in the two domains by correlating them with so-called pivot features, which are features exhibiting similar behaviors in the source and target do-mains. Blitzer applied this algorithm successfully to POS tagging (Blitzer et al, 2006) and senti-ment classification (Blitzer et al, 2007). SCL seems promising for other tasks as well, for ex-ample parse disambiguation (Plank, 2009). 2.3 Summarization We would like to use domain adaptation to aid in summarizing multi-party conversations hailing from different modalities. This contrasts with much of previous work on summarization of conversations, which has focused on domain-specific features (e.g., Rambow et al 2004). We will treat summarization as a supervised binary classification problem where the sentences of a conversation are rated by their informativeness and a subset is selected to form an extractive summary. Research in meeting summarization relevant to our task has investigated the utility of employing a large feature set including prosodic information, speaker status, lexical and structural discourse features (Murray et al, 2006; Galley, 2006). For email summarization, we view an 
17
email thread as a conversation. For summarizing email threads, Rambow (2004) used lexical fea-tures such as tf.idf, features that considered the thread to be a sequence of turns, and email-specific features such as number of recipients and the subject line. Asynchronous multi-party conversations were successfully represented for summarization through a small number of con-versational features by Murray and Carenini (2008). This paved the way to cross-domain conversation summarization by representing both email threads and meetings with a set of common conversational features. The work we present here investigates using data from both emails and meetings in summarizing emails, and compares using conversational versus lexical features. 3 Summarization setting Because the meetings domain has a large corpus, AMI, annotated for summarization, we will use it as the source domain for adaptation and the email domain as the target, with data from the Enron corpus as unlabeled email data, and the BC3 corpus as test data. 3.1 Datasets The AMI meeting corpus: We use the scenario portion of the AMI corpus (Carletta et al, 2005), for which groups of four participants take part in a series of four meetings and play roles within a fictitious company. While the scenario given to them is artificial, the speech and the actions are completely spontaneous and natural. The dataset contains approximately 115000 dialogue act (DA) segments. For the annotation, annotators wrote abstract summaries of each meeting and extracted transcript DA segments that best con-veyed or supported the information in the ab-stracts. A many-to-many mapping between tran-script DAs and sentences from the human ab-stract was obtained for each annotator, with three annotators assigned to each meeting. We con-sider a dialogue act to be a positive example if it is linked to a given human summary, and a nega-tive example otherwise. Approximately 13% of the total DAs are ultimately labeled as positive. The BC3 email corpus 1 : composed of 40 email threads from the World Wide Web Con-sortium (W3C) mailing list which feature a vari-ety of topics such as web accessibility and plan-ning face-to-face meetings. Each thread is anno-tated similarly to the AMI corpus, with three an-                                                1 http://www.cs.ubc.ca/labs/lci/bc3.html 
notators authoring abstracts and linking email thread sentences to the abstract sentences. The Enron email corpus 2 : a collection of emails released as part of the investigation into the Enron corporation, it has become a popular corpus for NLP research due to being realistic, naturally-occurring data from a corporate envi-ronment. We use 39 threads from this corpus to supplement the BC3 email data. 3.2 Features Used We consider two sets of features for each sen-tence: a small set of conversational structure fea-tures, and a large set of lexical features.  Conversational features: We extract 24 con-versational features from both the email and meetings domain, and which consider both emails and meetings to be conversations com-prised of turns between multiple participants. For an email thread, a turn consists of a single email fragment in the exchange. Similarly, for meet-ings, a turn is a sequence of dialogue acts by the same speaker. The conversational features, which are described in detail in (Murray and Carenini, 2008), include sentence length, sen-tence position in the conversation and in the cur-rent turn, pause-style features, lexical cohesion, centroid scores, and features that measure how terms cluster between conversation participants and conversation turns. Lexical features: We derive an extensive set of lexical features, originally proposed in (Murray et al, 2010) from the AMI and BC3 datasets, and then compute their occurrence in the Enron cor-pus. After throwing out features that occur less than five times, we end up with approximately 200,000 features. The features derived are: char-acter trigrams, word bigrams, POS tag bigrams, word pairs, POS pairs, and varying instantiation ngram (VIN) features. For word pairs, we extract the ordered pairs of words that occur in the same sentence, and similarly for POS pairs. To derive VIN features, we take each word bigram w1,w2 and further represent it as two patterns p1,w2 and w1,p2 each consisting of a word and a POS tag.  3.3 Classifier In all of our experiments, we train logistic re-gression classifiers using the liblinear toolkit3. This choice was partly motivated by our earlier summarization research, where logistic regres-sion classifiers were compared alongside support                                                 2 http://www.cs.cmu.edu/?enron/  3 http://www.csie.ntu.edu.tw/?cjlin/liblinear/ 
18
vector machines. The two types of classifier yielded very similar results, with logistic regres-sion classifiers being much faster to train.  3.4 Evaluation Metric Given the predicted labels on a test set and the existing gold-standard labels of the test set data, in each of our experiments we compute the area under the receiver operator curve as a measure of  performance. The area under the ROC (auROC) is a common summary statistic used to measure the quality of binary classification, where a per-fect classifier would achieve an auROC of 1.0, and a random classifier, near 0.5. 4 Experiments 4.1 Experimental Design The available labeled BC3 data totals about 3000 sentences, and the available labeled AMI data totals over 100,000 sentences, so for both effi-ciency and to not overwhelm the in-domain data, in each of our runs we subsample 10,000 sen-tences from the AMI data to use for training. Af-ter some initial experiments, where increasing the amount of target data beyond this did not im-prove accuracy, we decided not to incur the run-time cost of training on larger amounts of source data. Similarly, given that we extracted about 200,000 lexical features from our corpora, from our initial experiments trading off auROC and runtime, we decided to select a subset of 10,000 lexical features chosen by having the top mutual information with respect to the summarization labels. We did 5-fold cross-validation to split the target set into training and testing portions, and ran all the domain adaptation methods using the same split. We report the auROC performance of each method averaged over three runs of the 5-fold cross-validation. To test for significant dif-ferences between the performances of the various methods, we compute pairwise t-tests between the auROC values obtained on the same run. To account for an increased chance of false positives in reporting results of several pairwise t-tests, we report significance for p-values < 0.005 rather than at the customary 0.05 level.  4.2 Methods Implemented We compare supervised domain adaptation me-thods to the baseline INDOMAIN, in which only the training folds of the target data are used for training. In the MERGE method, we simply combine the labeled source and target sets and train on their combination. For ENSEMBLE, we 
train a classifier on the source training data, a classifier on the target training data, run each of them on the target test data, and for each test in-stance compute the average of the two probabili-ties predicted by the classifiers and use it to make a label prediction. We could vary the trade-off between the contribution of the source and target classifier in ENSEMBLE and determine the optimal parameter by cross-validation, though for simplicity we used 0.5 which pro-duced satisfying results. For the PRED approach, we use the source data to train a classifier, use it to make a prediction for the label of each point in the target data, and add the predicted probability as an additional feature to an in-domain trained classifier. The final supervised method FEAT-COPY (Daume, 2007) takes the existing features and extends the feature space by making a gen-eral, a source-specific, and a target-specific ver-sion of each feature. Hence, a sentence with fea-tures (x) gets represented as (x, x, 0) if it comes from the source domain, and as (x, 0, x) if it comes from the target domain.  For semi-supervised domain adaptation meth-ods, our baseline does not exploit any unlabeled target data. We train a classifier on the source data only, and call this TRANSFER. In contrast our two semi-supervised methods try to leverage unlabeled target data to help a classifier trained with labeled source data be more suited to the target domain.  For the SCL approach, we implemented Blit-zer?s structural correspondence learning (SCL) algorithm. An important part of the algorithm is training a classifier for each of a set of m selected pivot features to determine the correlations of the other features with respect to the pivot. The m models? weights are combined in a matrix, and its SVD with truncation factor of k is then ap-plied to the data to yield k new features for the data, that are added to the existing features. For the larger set of lexical features, we ran SCL with Blitzer?s original choice of m=1000 and k=50, but since the computation was extremely time consuming we scale down m to 100. For the tests with conversational features, since the number of features is 24, we picked m=24 and k=24. We also test SCLSMALL, which uses the same algorithm as SCL to find augmented fea-tures, except it then uses only these k features to train, not adding them to the original features.  This possibility was suggested in (Blitzer 2008).  As a second semi-supervised method, we im-plemented SELFTRAIN. The standard self-training algorithm we implemented, inspired by 
19
Blum and Mitchell (1998), is to start with a la-beled training set T, create a subset of a fixed size of the unlabeled data U, and then iterate training a classifier on T, making a prediction on the data in U, and take the highest-confidence positive p predictions and highest-confidence negative n predictions from U with their pre-
dicted labels to add to T before replenishing U from the rest of the unlabeled data. We picked the size of the subset U as 200, and to select the top p=3 and bottom n=17 predictions at each step in order to achieve a ratio of summary to total sentences of 15%, which is near to the known ratio of the labels for AMI.  
method indomain merge ensem-ble featcopy pred transfer selftrain scl sclsmall using conversational features auROC 0.838 0.747 0.751 0.839 0.838 0.677 0.678 0.663 0.646 time(s) 0.79 2.42 2.64 8.44 5.38 2.08 100.2 52.85 66.74 using lexical features auROC 0.623 0.638 0.667 0.615 0.625 0.636 0.636 0.651 0.742 time(s) 4.87 13.64 13.77 78.63 30.99 9.73 448.8 813.7 828.3 Table 1. Performance and time of domain adaptation methods with the two feature sets 
5 Results In our first experiment, we ran all the domain adaptation methods on the data with conversa-tional features; in our second experiment, we did the same on the data with lexical features. We computed the average of the auROCs and run-ning times obtained for each method in each ex-periment. Table 1 lists the results of the super-vised methods MERGE, ENSEMBLE, and FEATCOPY with baseline INDOMAIN, and the semi-supervised methods SELFTRAIN, SCL, and SCLSMALL with baseline TRANSFER.  The best results for supervised methods (and overall) are achieved by FEATCOPY, PRED, and INDOMAIN with the conversational fea-tures, with a similar performance that is signifi-cantly better than for MERGE and ENSEMBLE. However, for lexical features MERGE and EN-SEMBLE beat their performance, with the sig-nificant differences from the baseline INDO-MAIN being those of ENSEMBLE and FEAT-COPY, the latter now being the worst performer.  For the set of lexical features, all semi-supervised methods improve on TRANSFER. In this setting, all of the differences are significant, with SCLSMALL generating a considerable gain of 10%. For the set of conversational features, SELFTRAIN yields an auROC similar to TRANSFER, and the small difference between the two is not significant. Unlike when using lexical features, SCL and SCLSMALL perform 
significantly worse than TRANSFER, though this is not unexpected. Because it relies on de-termining correlation between features, we be-lieve that structural correspondence learning is more appropriate in a high rather than low-dimensional feature space. Figure 1 shows, for each of the methods, a dark grey bar representing the auROC obtained with the set of conversational features next to a lighter grey one for the lexical features. For the super-vised methods on the left (INDOMAIN to PRED), the conversational features yield better performance, and this by an absolute ROC dif-ference of more than 5%. However, notice that no method outperform the baseline INDOMAIN. For the semi-supervised methods on the right, the difference in performance between the two fea-ture sets is less marked, although the auROC of SCLSMALL with lexical features is exception-ally larger. As shown in Table 1, every one of the domain adaptation methods has a higher average time with lexical features than with conversational features. The semi-supervised methods take longer than the fully supervised methods, and this is due to their algorithms involving more steps. Both SCL and SELFTRAIN take minutes instead of seconds to make a prediction, though their running times are more reasonable than with the initial parameter settings we used in pre-liminary experiments.  
20
 Figure 1. Comparison of auROCs of all domain adaptation methods and baselines 6 Conclusions and Future Work This paper is a comparative study of the per-formance of several domain adaptation methods on the task of summarizing conversational data when a large amount of annotated data is avail-able in the domain of meetings and a smaller (or no) amount of annotation exists in the target do-main of email threads.  One surprising finding of our experiments is that of the methods we implemented, the best performance is achieved by training on in-domain data using conversational features. Hence, it seems that when sufficient labeled in-domain data is available, supervised domain ad-aptation is not useful for summarization of emails with the features and amounts of labeled data we used. However, semi-supervised methods using unla-beled data and labeled out-of-domain data are useful in the absence of these labels, with the SCLSMALL method greatly outperforming the baseline. This is a promising result for using an-notated corpora in well-studied domains or con-versational modalities to summarize data in new domains.  In our experiments, we have explored the effec-tiveness of conversational and lexical features separately. The two sets of features differ in their impact on domain adaptation: with conversa-tional features, no method improves significantly over the baseline, whereas with lexical features, the semi-supervised methods given no labeled target data perform better than the supervised baseline of training in-domain. One hypothesis to explain this is that lexical features behave simi-larly in the two domains, so training on the larger amount of labeled target data is beneficial, while conversational features are more domain spe-
cific, likely because emails and meetings are structured differently. As the next step in our work, we intend to combine the two sets of fea-tures. In doing this, we will have to ensure that the conversational features are not washed out by a very large number of lexical features.   A scenario of practical interest in domain adap-tation for new domains is when the target domain has a considerable amount of unlabeled data and a subset of this data can easily be annotated by hand, for example five threads in the email do-main. We are currently exploring injecting a small amount of labeled target data into the semi-supervised methods we have implemented to ac-count for differences that cannot be observed in the unlabeled data. Blitzer (2008) did such an adjustment to SCL using a small amount of la-beled target data to correct misaligned features and thus improve accuracy. Finally, it may be worth investigating how to combine several of the methods, for example by adding the feature of PRED based on training a classifier on the source, alongside augmented features using more unlabeled data through SCL, and adding the highest-confidence labels from SELFTRAIN to the training set.  References  Blitzer, J. (2008). Domain Adaptation of Natural Language Processing Systems. PhD Thesis. Blitzer, J., Dredze, M., & Pereira, F. (2007). Biogra-phies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proc. of ACL 2007. Blitzer, J., McDonald, R., & Pereira, F. (2006). Do-main adaptation with structural correspondence learning. In Proc. of EMNLP 2006. Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proc. CLT. Carletta, J., Ashby, S., Bourban, S., Flynn, M., Guillemot, M. et al (2005). The AMI meeting cor-pus: A pre-announcement. In Proc. of MLMI 2005. Chelba, C., & Acero, A. (2006). Adaptation of maxi-mum entropy capitalizer: Little data can help a lot. Computer Speech & Language, 20(4), 382-399. Daume III, H. (2007). Frustratingly easy domain ad-aptation. In Proc. of ACL 2007. Daume III, H., & Marcu, D. (2006). Domain Adapta-tion for Statistical Classifiers. Journal of Artificial Intelligence Research, 26, 101?126. Florian, R., Hassan, H., Ittycheriah, A., Jing, H., Kambhatla, N., Luo, X., et al (2004). A statistical 
21
model for multilingual entity detection and track-ing. In Proc. HLT-NAACL 2004. Galley, M. (2006). A skip-chain conditional random field for ranking meeting utterances by importance. In Proc. of EMNLP 2006. McClosky, D., Charniak, E., & Johnson, M. (2006). Effective self-training for parsing. In Proc. of HLT-NAACL 2006 . Murray, G., & Carenini, G. (2008). Summarizing spoken and written conversations. In Proc. of EMNLP 2008. Murray, G., Carenini, G., & Ng, R. (2010). Interpreta-tion and transformation for abstracting conversa-tions. In Proc. of HLT-NAACL 2010. Murray, G., Renals, S., Moore, J., & Carletta, J. (2006). Incorporating speaker and discourse fea-tures into speech summarization. In Proc. of HLT-NAACL 2006. Plank, B. (2009). Structural correspondence learning for parse disambiguation. In Proc. of EACL 2009: Student Research Workshop. Rambow, O., Shrestha, L., & Chen, J. (2004). Sum-marizing email threads. In Proc. of HLT-NAACL 2004. 
22
Generating and Validating Abstracts of Meeting Conversations: a User
Study
Gabriel Murray
gabrielm@cs.ubc.ca
Giuseppe Carenini
carenini@cs.ubc.ca
Department of Computer Science, University of British Columbia
Vancouver, Canada
Raymond Ng
rng@cs.ubc.ca
Abstract
In this paper we present a complete sys-
tem for automatically generating natural
language abstracts of meeting conversa-
tions. This system is comprised of com-
ponents relating to interpretation of the
meeting documents according to a meet-
ing ontology, transformation or content
selection from that source representation
to a summary representation, and gener-
ation of new summary text. In a forma-
tive user study, we compare this approach
to gold-standard human abstracts and ex-
tracts to gauge the usefulness of the dif-
ferent summary types for browsing meet-
ing conversations. We find that our auto-
matically generated summaries are ranked
significantly higher than human-selected
extracts on coherence and usability crite-
ria. More generally, users demonstrate a
strong preference for abstract-style sum-
maries over extracts.
1 Introduction
The most common solution to the task of summa-
rizing spoken and written data is sentence (or ut-
terance) extraction, where binary sentence classi-
fication yields a cut-and-paste summary compris-
ing informative sentences from the document con-
catenated in a new, condensed document. Such
extractive approaches have dominated the field of
automatic summarization for decades, in large part
because extractive systems do not require a natu-
ral language generation (NLG) component since
the summary sentences are simply lifted from the
source document.
Extrinsic evaluations have shown that, while ex-
tractive summaries may be less coherent than hu-
man abstracts, users still find them to be valuable
tools for browsing documents (He et al, 1999;
McKeown et al, 2005; Murray et al, 2009). How-
ever, these previous evaluations also illustrate that
concise abstracts are generally preferred by users
and lead to higher objective task scores. A weak-
ness of typical extractive summaries is that the end
user does not know why the extracted sentences
are important; exploring the original sentence con-
text may be the only way to resolve this uncer-
tainty. And if the input source document consists
of noisy, unstructured text such as ungrammatical,
disfluent multi-party speech, then the resultant ex-
tract is likely to be noisy and unstructured as well.
Herein we describe a complete and fully auto-
matic system for generating abstract summaries
of meeting conversations. Our abstractor maps
input sentences to a meeting ontology, generates
messages that abstract over multiple sentences,
selects the most informative messages, and ulti-
mately generates new text to describe these rele-
vant messages at a high level. We conduct a user
study where participants must browse a meeting
conversation within a very constrained timeframe,
having a summary at their disposal. We compare
our automatic abstracts with human abstracts and
extracts and find that our abstract summaries sig-
nificantly outperform extracts in terms of coher-
ence and usability according to human ratings. In
general, users rate abstract-style summaries much
more highly than extracts for these conversations.
2 Related Research
Automatic summarizaton has been described as
consisting of interpretation, transformation and
generation (Jones, 1999). Popular approaches to
text extraction essentially collapse interpretation
and transformation into one step, with genera-
tion either being ignored or consisting of post-
processing techniques such as sentence compres-
sion (Knight and Marcu, 2000; Clarke and Lapata,
2006) or sentence merging (Barzilay and McKe-
own, 2005). In contrast, in this work we clearly
separate interpretation from transformation and in-
corporate an NLG component to generate new text
to describe meeting conversations.
While extraction remains the most common ap-
proach to text summarization, one application in
which abstractive summarization is widely used is
data-to-text generation. Summarization is critical
for data-to-text generation because the amount of
collected data may be massive. Examples of such
applications include the summarization of inten-
sive care unit data in the medical domain (Portet
et al, 2009) and data from gas turbine sensors (Yu
et al, 2007). Our approach is similar except that
our input is text data in the form of conversations.
We otherwise utilize a very similar architecture of
pattern recognition, pattern abstraction, pattern
selection and summary generation.
Kleinbauer et al (2007) carry out topic-based
meeting abstraction. Our systems differ in two
major respects: their summarization process uses
human gold-standard annotations of topic seg-
ments, topic labels and content items from the on-
tology, while our summarizer is fully automatic;
secondly, the ontology they used is specific not
just to meetings but to the AMI scenario meetings
(Carletta et al, 2005), while our ontology applies
to conversations in general, allowing our approach
to be extended to emails, blogs, etc.
In this work we conduct a user study where par-
ticipants use summaries to browse meeting tran-
scripts. Some previous work has compared ex-
tracts and abstracts for the task of a decision au-
dit (Murray et al, 2009) , finding that human ab-
stracts are a challenging gold-standard in terms
of enabling participants to work quickly and cor-
rectly identify the relevant information. For that
task, automatic extracts and the semi-automatic
abstracts of Kleinbauer et al (2007) were found
to be competitive with one another in terms of
user satisfaction and resultant task scores. Other
research on comparing extracts and abstracts has
found that an automatic abstractor outperforms a
generic extractor in the domains of technical ar-
ticles (Saggion and Lapalme, 2002) and evalua-
tive reviews (Carenini and Cheung, 2008), and that
human-written abstracts were rated best overall.
3 Interpretation - Ontology Mapping
Source document interpretation in our system re-
lies on a general conversation ontology. The on-
tology is written in OWL/RDF and contains upper-
level classes such as Participant, Entity, Utterance,
and DialogueAct. When additional information is
available about participant roles in a given domain,
Participant subclasses such as ProjectManager can
be utilized. Object properties connect instances of
ontology classes; for example, the following entry
in the ontology states that the object property has-
Speaker has an instance of Utterance as its domain
and an instance of Participant as its range.
<owl:ObjectProperty rdf:about="#hasSpeaker">
<rdfs:range rdf:resource="#Participant"/>
<rdfs:domain rdf:resource="#Utterance"/>
</owl:ObjectProperty>
The DialogueAct class has subclasses cor-
responding to a variety of sentence-level phe-
nomena: decisions, actions, problems, positive-
subjective sentences, negative-subjective sen-
tences and general extractive sentences (important
sentences that may not match the other categories).
Utterance instances are connected to DialogueAct
subclasses through an object property hasDAType.
A single utterance may correspond to more than
one DialogueAct; for example, it may represent
both a positive-subjective sentence and a decision.
Our current definition of Entity instances is
simple. The entities in a conversation are noun
phrases with mid-range document frequency. This
is similar to the definition of concept proposed by
Xie et al (2009), where n-grams are weighted
by tf.idf scores, except that we use noun phrases
rather than any n-grams because we want to refer
to the entities in the generated text. We use mid-
range document frequency instead of idf (Church
and Gale, 1995), where the entities occur in be-
tween 10% and 90% of the documents in the col-
lection. We do not currently attempt coreference
resolution for entities; recent work has investi-
gated coreference resolution for multi-party dia-
logues (Muller, 2007; Gupta et al, 2007), but the
challenge of resolution on such noisy data is high-
lighted by low accuracy (e.g. F-measure of 21.21)
compared with using well-formed text.
We map sentences to our ontology classes by
building numerous supervised classifiers trained
on labeled decision sentences, action sentences,
etc. A general extractive classifier is also trained
on sentences simply labeled as important. We give
a specific example of the ontology mapping using
the following excerpt from the AMI corpus, with
entities italicized and resulting sentence classifica-
tions shown in bold:
? A: And you two are going to work together
on a prototype using modelling clay. [action]
? A: You?ll get specific instructions from your
personal coach. [action]
? C: Cool. [positive-subjective]
? A: Um did we decide on a chip? [decision]
? A: Let?s go with a simple chip. [decision,
positive-subjective]
The ontology is populated by adding all of
the sentence entities as instances of the Entity
class, all of the participants as instances of the
Participant class (or its subclasses such as Pro-
jectManager when these are represented), and all
of the utterances as instances of Utterance with
their associated hasDAType properties indicating
the utterance-level phenomena of interest. Here
we show a sample Utterance instance:
<Utterance rdf:about="#ES2014a.B.dact.37">
<hasSpeaker rdf:resource="#IndustrialDesigner"/>
<hasDAType rdf:resource="#PositiveSubjective"/>
<begTime>456.58</begTime>
<endTime>458.832</endTime>
</Utterance>
3.1 Feature Set
The interpretation component as just described re-
lies on supervised classifiers for the detection of
items such as decisions, actions, and problems.
This component uses general features that are ap-
plicable to any conversation domain. The first set
of features we use for this ontology mapping are
features relating to conversational structure. They
include sentence length, sentence position in the
conversation and in the current turn, pause-style
features, lexical cohesion, centroid scores, and
features that measure how terms cluster between
conversation participants and conversation turns.
While these features have been found to work
well for generic extractive summarization (Murray
and Carenini, 2008), we use additional features
for capturing the more specific sentence-level phe-
nomena of this research. These include character
trigrams, word bigrams, part-of-speech bigrams,
word pairs, part-of-speech pairs, and varying in-
stantiation n-grams, described in more detail in
(Murray et al, 2010). After removing features
that occur fewer than five times, we end up with
218,957 total features.
3.2 Message Generation
Rather than merely classifying individual sen-
tences as decisions, action items, and so on, we
also aim to detect larger patterns ? or messages
? within the meeting. For example, a given par-
ticipant may repeatedly make positive comments
about an entity throughout the meeting, or may
give contrasting opinions of an entity. In or-
der to determine which messages are essential for
summarizing meetings, three human judges con-
ducted a detailed analysis of four development
set meetings. They first independently examined
previously-written human abstracts for the meet-
ings to identify which messages were present in
the summaries. In the second step, the judges met
together to decide on a final message set. This
resulted in a set of messages common to all the
meetings and agreed upon by all the judges. The
messages that our summarizer will automatically
generate are defined as follows:
? OpeningMessage and ClosingMessage: Briefly de-
scribes opening/closing of the meeting
? RepeatedPositiveMessage and RepeatedNegativeMes-
sage: Describes a participant making positive/negative
statements about a giv en entity
? ActionItemsMessage: Indicates that a participant has
action items relating to some entity
? DecisionMessage: Indicates that a participant was in-
volved in a decision-making process regarding some
entity
? ProblemMessage: Indicates that a participant repeat-
edly discussed problems or issues about some entity
? GeneralDiscussionMessage: Indicates that a partici-
pant repeatedly discussed a given entity
Message generation takes as input the ontology
mapping described in the previous section, and
outputs a set of messages for a particular meeting.
This is done by identifying pairs of Participants
and Entities that repeatedly co-occur with the var-
ious sentence-level predictions. For example, if
the project manager repeatedly discusses the inter-
face using utterances that are classified as positive-
subjective, a RepeatedPositiveMessage is gener-
ated for that Participant-Entity pair. Messages are
generated in a similar fashion for all other mes-
sage types except for the opening and closing mes-
sages. These latter two messages are created sim-
ply by identifying which participants were most
active in the introductory and concluding portions
of the meeting and generating messages that de-
scribe that participant opening or closing the meet-
ing.
Messages types are defined within the OWL on-
tology, and the ontology is populated with mes-
sage instances for each meeting. The following
message describes the Marketing Expert making
a decision concerning the television, and lists the
relevant sentences contained by that decision mes-
sage.
<DecisionMessage rdf:about="#dec9">
<messageSource rdf:resource="#MarketingExpert"/>
<messageTarget rdf:resource="#television"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.55"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.63"/>
</DecisionMessage>
4 Transformation - ILP Content
Selection for Messages
Having detected all the messages for a given meet-
ing conversation, we now turn to the task of
transforming the source representation to a sum-
mary representation, which involves identifying
the most informative messages for which we will
generate text. We choose an integer linear pro-
gramming (ILP) approach to message selection.
ILP has previously been used for sentence selec-
tion in an extractive framework. Xie et al (2009)
used ILP to create a summary by maximizing a
global objective function combining sentence and
entity weights. Our method is similar except that
we are selecting messages based on optimizing
an objective function combining message and sen-
tence weights:
maximize (1??)?
?
i
wisi +??
?
j
ujmj (1)
subject to
?
i
lisi < L (2)
where wi is the score for sentence i, uj is the
score for message j, si is a binary variable in-
dicating whether sentence i is selected, mj is a
binary variable indicating whether message j is
selected, li is the length of sentence i and L is
the desired summary length. The ? term is used
to balance sentence and message weights. Our
sentence weight wi is the sum of all the poste-
rior probabilities for sentence i derived from the
various sentence-level classifiers. In other words,
sentences are weighted highly if they correspond
to multiple object properties in the ontology. To
continue the example from Section 3, the sen-
tence Let?s go with the simple chip will be highly
weighted because it represents both a decision and
a positive-subjective opinion. The message score
uj is the number of sentences contained by the
message j. For instance, the DecisionMessage
at the end of Section 3.2 contains two sentences.
We can create a higher level of abstraction in our
summaries if we select messages which contain
numerous utterances. Similar to how sentences
and concepts are combined in the previous ILP ex-
traction approach (Xie et al, 2009; Gillick et al,
2009), messages and sentences are tied together by
two additional constraints:
?
j
mjoij ? si ?i (3)
mjoij ? si ?ij (4)
where oij is the occurence of sentence i in mes-
sage j. These constraints state that a sentence can
only be selected if it occurs in a message that is
selected, and that a message can only be selected
if all of its sentences have also been selected.
For these initial experiments, ? is set to 0.5. The
summary length L is set to 15% of the conver-
sation word count. Note that this is a constraint
on the length of the selected utterances; we ad-
ditionally place a length constraint on the gener-
ated summary described in the following section.
The reason for both types of length constraint is to
avoid creating an abstract that is linked to a great
many conversation utterances but is very brief and
likely to be vague and uninformative.
5 Summary Generation
The generation component of our system fol-
lows the standard pipeline architecture (Reiter and
Dale, 2000), comprised of a text planner, a micro-
planner and a realizer. We describe each of these
in turn.
5.1 Text Planning
The input to the document planner is an ontol-
ogy which contains the selected messages from
the content selection stage. We take a top-
down, schema-based approach to document plan-
ning (Reiter and Dale, 2000). This method is ef-
fective for summaries with a canonical structure,
as is the case with meetings. There are three high-
level schemas invoked in order: opening mes-
sages, body messages, and closing messages. For
the body of the summary, messages are retrieved
from the ontology using SPARQL, an SQL-style
query language for ontologies, and are clustered
according to entities. Entities are temporally or-
dered according to their average timestamp in the
meeting. In the overall document plan tree struc-
ture, the body plan is comprised of document sub-
plans for each entity, and the document sub-plan
for each entity is comprised of document sub-
plans for each message type. The output of the
document planner is a tree structure with messages
as its leaves and document plans for its internal
nodes. Our text planner is implemented within the
Jena semantic web programming framework1.
5.2 Microplanning
The microplanner takes the document plan as in-
put and performs two operations: aggregation and
generation of referring expressions.
5.2.1 Aggregation
There are several possibilities for aggregation in
this domain, such as aggregating over participants,
entities and message types. The analysis of our
four development set meetings revealed that ag-
gregation over meeting participants is quite com-
mon in human abstracts, so our system supports
such aggregation. This involves combining mes-
sages that differ in participants but share a com-
mon entity and message type; for example, if there
are two RepeatedPositiveMessage instances about
the user interface, one with the project manager
as the source and one with the industrial designer
as the source, a single RepeatedPositiveMessage
instance is created that contains two sources. We
do not aggregate over entities for the sole reason
that the text planner already clustered messages
according to entity. The entity clustering is in-
tended to give the summary a more coherent struc-
ture but has the effect of prohibiting aggregation
over entities.
5.2.2 Referring Expressions
To reduce redundancy in our generated abstracts,
we generate alternative referring expressions when
a participant or an entity is mentioned multiple
times in sequence. For participants, this means
the generation of a personal pronoun. For entities,
rather than referring repeatedly to, e.g., the remote
control, we generate expressions such as that issue
or this matter.
5.3 Realization
The text realizer takes the output of the microplan-
ner and generates a textual summary of a meet-
ing. This is accomplished by first associating ele-
ments of the ontology with linguistic annotations.
For example, participants are associated with a
noun phrase denoting their role, such as the project
manager. Since entities were defined simply as
noun phrases with mid-frequency IDF scores, an
entity instance is associated with that noun phrase.
Messages themselves are associated with verbs,
1to be made publicly available upon publicaton
subject templates and object templates. For exam-
ple, instances of DecisionMessage are associated
with the verb make, have a subject template set to
the noun phrase of the message source, and have
an object template [NP a decision PP [concern-
ing ]] where the object of the prepositional
phrase is the noun phrase associated with the mes-
sage target.
To give a concrete example, consider the fol-
lowing decision message:
<DecisionMessage rdf:about="#dec9">
<rdf:type rdf:resource="&owl;Thing"/>
<hasVerb>make</hasVerb>
<hasCompl>a decision</hasCompl>
<messageSource rdf:resource="#MarketingExpert"/>
<messageSource rdf:resource="#ProjectManager"/>
<messageTarget rdf:resource="#television"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.55"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.63"/>
</DecisionMessage>
There are two message sources,
ProjectManager and MarketingExpert,
and one message target, television. The
subjects of the message are set to be the noun
phrases associated with the marketing expert and
the project manager, while the object template is
filled with the noun phrase the television. This
message is realized as The project manager and
the marketing expert made a decision about the
television.
For our realizer we use simpleNLG2. We tra-
verse the document plan output by the microplan-
ner and generate a sentence for each message leaf.
A new paragraph is created when both the message
type and target of the current message are different
than the message type and target for the previous
message.
6 Task-Based User Study
We carried out a formative user study in order to
inform this early work on automatic conversation
abstraction. This task required participants to re-
view meeting conversations within a short time-
frame, having a summary at their disposal. We
compared human abstracts and extracts with our
automatically generated abstracts. The interpre-
tation component and a preliminary version of
the transformation component have already been
tested in previous work (Murray et al, 2010). The
sentence-level classifiers were found to perform
well according to the area under the receiver op-
erator characteristic (AUROC) metric, which eva-
lutes the true-positive/false-positive ratio as the
2http://www.csd.abdn.ac.uk/?ereiter/simplenlg/
posterior threshold is varied, with scores ranging
from 0.76 for subjective sentences to 0.92 for ac-
tion item sentences. In the following, we focus
on the formative evaluation of the complete sys-
tem. We first describe the corpus we used, then
the materials, participants and procedure. Finally
we discuss the study results.
6.1 AMI Meeting Corpus
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Car-
letta et al, 2005), where groups of four partici-
pants take part in a series of four meetings and
play roles within a fictitious company. There are
140 of these meetings in total. For the sum-
mary annotation, annotators wrote abstract sum-
maries of each meeting and extracted sentences
that best conveyed or supported the information
in the abstracts. The human-authored abstracts
each contain a general abstract summary and three
subsections for ?decisions,? ?actions? and ?prob-
lems? from the meeting. A many-to-many map-
ping between transcript sentences and sentences
from the human abstract was obtained for each an-
notator. Approximately 13% of the total transcript
sentences are ultimately labeled as extracted sen-
tences. A sentence is considered a decision item
if it is linked to the decision portion of the ab-
stract, and action and problem sentences are de-
rived similarly. We additionally use subjectivity
and polarity annotations for the AMI corpus (Wil-
son, 2008).
6.2 Materials, Participants and Procedures
We selected five AMI meetings for this user study,
with each stage of the four-stage AMI scenario
represented. The meetings average approximately
500 sentences each. We included the follow-
ing three types of summaries for each meeting:
(EH) gold-standard human extracts, (AH) gold-
standard human abstracts described in Section
6.1, and (AA) the automatic abstracts output by
our abstractor. All three conditions feature man-
ual transcriptions of the conversation. Each sum-
mary contains links to the sentences in the meet-
ing transcript. For extracts, this is a one-to-one
mapping. For the two abstract conditions, this can
be a many-to-many mapping between abstract sen-
tences and transcript sentences.
Participants were given instructions to browse
each meeting in order to understand the gist of
the meeting, taking no longer than 15 minutes per
meeting. They were asked to consider the sce-
nario in which they were a company employee
who wanted to quickly review a previous meet-
ing by using a browsing interface designed for this
task. Figure 1 shows the browsing interface for
meeting IS1001d with an automatically generated
abstract on the left-hand side and the transcript on
the right. In the screenshot, the user has clicked
the abstract sentence The industrial designer made
a decision on the cost and has been linked to a
transcript utterance, highlighted in yellow, which
reads Also for the cost, we should only put one bat-
tery in it. Notice that this output is not entirely cor-
rect, as the decision pertained to the battery, which
impacted the cost. This sentence was generated
because the entity cost appeared in several deci-
sion sentences.
The time constraint meant that it was not fea-
sible to simply read the entire transcript straight
through. Participants were free to adopt whatever
browsing strategy suited them, including skim-
ming the transcript and using the summary as they
saw fit. Upon finishing their review of each meet-
ing, participants were asked to rate their level of
agreement or disagreement on several Likert-style
statements relating to the difficulty of the task and
the usefulness of the summary. There were six
statements to be evaluated on a 1-5 scale, with
1 indicating strong disagreement and 5 indicating
strong agreement:
? Q1: I understood the overall content of the discussion.
? Q2: It required a lot of effort to review the meeting in
the allotted time.
? Q3: The summary was coherent and readable.
? Q4: The information in the summary was relevant.
? Q5: The summary was useful for navigating the dis-
cussion.
? Q6: The summary was missing relevant information.
Participants were also asked if there was any-
thing they would have liked to have seen in the
summary, and whether they had any general com-
ments on the summary.
We recruited 19 participants in total, with each
receiving financial reimbursement for their partic-
ipation. Each participant saw one summary per
meeting and rated every summary condition dur-
ing the experiment. We varied the order of the
meetings and summary conditions. With 19 sub-
jects, three summary conditions and six Likert
statements, we collected a total of 342 user judg-
ments. To ensure fair comparison between the
three summary types, we limit summary length to
Figure 1: Summary Interface
be equal to the length of the human abstract for
each meeting. This ranges from approximately
190 to 350 words per meeting summary.
6.2.1 Results and Discussion
Participants took approximately 12 minutes on av-
erage to review each meeting, slightly shorter than
the maximum allotted fifteen minutes.
Figure 2 shows the average ratings for each
summary condition on each Likert statement. For
Q1, which concerns general comprehension of
the meeting discussion, condition AH (human
abstracts) is rated significantly higher than EH
(human extracts) and AA (automatic abstracts)
(p=0.0016 and p=0.0119 according to t-test, re-
spectively). However, for the other statement that
addresses the overall task, Q2, AA is rated best
overall. Note that for Q2 a lower score is better.
While there are no significantly differences on this
criterion, it is a compelling finding that automatic
abstracts can greatly reduce the effort required for
reviewing the meeting, at a level comparable to
human abstracts.
Q3 concerns coherence and readability. Condi-
tion AH is significantly better than both EH and
AA (p<0.0001 and p=0.0321). Our condition AA
is also significantly better than the extractive con-
dition EH (p=0.0196). In the introduction we men-
tioned that a potential weakness of extractive sum-
maries is that coherence and readability decrease
when sentences are removed from their original
contexts, and that extracts of noisy, unstructured
source documents will tend to be noisy and un-
structured as well. These ratings confirm that ex-
tracts are not rated well on coherence and readabil-
ity.
Q4 concerns the perceived relevance of the
summary. Condition AH is again significantly bet-
ter than EH and AH (both p<0.0001). AA is rated
substantially higher than EH on summary rele-
vance, but not at a significant level.
Q5 is a key question because it directly ad-
dresses the issue of summary usability for such a
task. Condition AH is significantly better than EH
and AA (both p<0.0001), but we also find that AA
is significantly better than EH (p=0.0476). Ex-
tracts have an average score of only 2.37 out of
5, compared with 3.21 and 4.63 for automatic and
human abstracts, respectively. For quickly review-
ing a meeting conversation, abstracts are much
more useful than extracts.
Q6 indicates whether the summaries were miss-
ing any relevant information. As with Q2, a lower
score is better. Condition AH is significantly bet-
ter than EH and AA (p<0.0001 and p=0.0179),
while AA is better than EH with marginal signif-
icance (p=0.0778). This indicates that our auto-
matic abstracts were better at containing all the
relevant information than were human-selected
extracts.
All participants gave written answers to the
open-ended questions, yielding insights into the
strengths and weaknesses of the different sum-
mary types. Regarding the automatic abstracts
(AA), the most common criticisms were that the
 0
 1
 2
 3
 4
 5
Q1 - Understood Meeting
Q2 - Required Effort**
Q3 - Summary Coherent
Q4 - Summary Relevant
Q5 - Summary Useful
Q6 - Summary Missing Info**
Av
era
ge 
Us
er 
Ra
ting
s
Human AbstractsAuto AbstractsHuman Extracts
Figure 2: User Ratings (** indicates lower score
is better)
summaries are too vague (e.g. ?more concrete
would help?) and that the phrasing can be repet-
itive. There is a potential many-to-many map-
ping between abstract sentences and transcript
sentences, and some participants felt that it was
unnecessarily redundant to be linked to the same
transcript sentence more than once (e.g. ?quite a
few repetitive citations?). Several participants felt
that the sentences regarding positive-subjective
and negative-subjective opinions were overstated
and that the actual opinions were either more sub-
tle or neutral. One participant wrote that these sen-
tences constituted ?a lot of bias in the summary.?
On the positive side, several participants consid-
ered the links between abstract sentences and tran-
script sentences to be very helpful, e.g. ?it re-
ally linked to the transcript well? and ?I like how
the summary has links connected to the transcript.
Easier to follow-up on the meeting w/ the aid of
the summary.? One participant particularly liked
the subjectivity-oriented sentences: ?Lifting some
of the positive/negative from the discussion into
the summary can mean the discussion does not
even need to be included to get understanding.?
The written comments on the extractive condi-
tion (EH) were almost wholly negative. Many par-
ticipants felt that the extracts did not even con-
stitute a summary or that a cut-and-paste from
the transcript does not make a sufficient summary
(e.g. ?The summary was not helpful @ all be-
cause it?s just cut from the transcript?, ?All copy
and paste not a summary?, ?Not very clear sum-
mary - looked like the transcript?, and ?No ef-
fort was made in the summary to put things into
context?). Interestingly, several participants criti-
cized the extracts for not containing the most im-
portant sentences from the transcript despite these
being human-selected extracts, demonstrating that
a good summary is a subjective matter.
The comments on human abstracts (AH) were
generally very positive, e.g. ?easy to follow?, ?it
was good, clear?, and ?I could?ve just read the
summary and still understood the bulk of the meet-
ing?s content.? The most frequent negative criti-
cisms were that the abstract sentences sometimes
contained too many links to the transcript (?mas-
sive amount of links look daunting?), and that the
summaries were sometimes too vague (?perhaps
some points from the discussion can be included,
instead of just having topic outlines?, ?[want] spe-
cific details?). It is interesting to observe that this
latter criticism is shared between human abstracts
and our automatic abstracts. When generalizing
over the source document, details are sometimes
sacrificed.
7 Conclusion
We have presented a system for automatically gen-
erating abstracts of meeting conversations. This
summarizer relies on first mapping sentences to
a conversation ontology representing phenomena
such as decisions, action items and sentiment, then
identifying message patterns that abstract over
multiple sentences. We select the most informa-
tive messages through an ILP optimization ap-
proach, aggregate messages, and finally generate
text describing all of the selected messages. A
formative user study shows that, overall, our auto-
matic abstractive summaries rate very well in com-
parison with human extracts, particularly regard-
ing readability, coherence and usefulness. The
automatic abstracts are also significantly better in
terms of containing all of the relevant information
(Q6), and it is impressive that an automatic ab-
stractor substantially outperforms human-selected
content on such a metric. In future work we aim
to bridge the performance gap between automatic
and human abstracts by identifying more specific
messages and reducing redundancy in the sentence
mapping. We plan to improve the NLG output by
introducing more linguistic variety and better text
structuring. We are also investigating the impact
of ASR transcripts on abstracts and extracts, with
encouraging early results.
Acknowledgments Thanks to Nicholas Fitzgerald for
work on implementing the top-down planner.
References
R. Barzilay and K. McKeown. 2005. Sentence fusion
for multidocument news summarization. Computa-
tional Linguistics, 31(3):297?328.
G. Carenini and JCK Cheung. 2008. Extractive vs.
nlg-based abstractive summarization of evaluative
text: The effect of corpus controveriality. In Proc.
of the 5th International Natural Generation Confer-
ence.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
K. Church and W. Gale. 1995. Inverse document fre-
quency IDF: A measure of deviation from poisson.
In Proc. of the Third Workshop on Very Large Cor-
pora, pages 121?130.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proc. of COLING/ACL 2006, pages 144?
151.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-
Tu?r. 2009. A global optimization framework for
meeting summarization. In Proc. of ICASSP 2009,
Taipei, Taiwan.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007. Resolving ?You? in multi-party dialog. In
Proc. of SIGdial 2007, Antwerp, Belgium.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999.
Auto-summarization of audio-video presentations.
In Proc. of ACM MULTIMEDIA ?99, Orlando, FL,
USA, pages 489?498.
K. Spa?rck Jones. 1999. Automatic summarizing: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization,
pages 1?12. MITP.
T. Kleinbauer, S. Becker, and T. Becker. 2007. Com-
bining multiple information layers for the automatic
generation of indicative meeting abstracts. In Proc.
of ENLG 2007, Dagstuhl, Germany.
K. Knight and D. Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. In
Proc. of AAAI 2000, Austin, Texas, USA, pages 703?
710.
K. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005. From text to speech summarization. In Proc.
of ICASSP 2005, Philadelphia, USA, pages 997?
1000.
C. Muller. 2007. Resolving It, This and That in un-
restricted multi-party dialog. In Proc. of ACL 2007,
Prague, Czech Republic.
G. Murray and G. Carenini. 2008. Summarizing spo-
ken and written conversations. In Proc. of EMNLP
2008, Honolulu, HI, USA.
G. Murray, T. Kleinbauer, P. Poller, S. Renals,
T. Becker, and J. Kilgour. 2009. Extrinsic sum-
marization evaluation: A decision audit task. ACM
Transactions on SLP, 6(2).
G. Murray, G. Carenini, and R. Ng. 2010. Interpre-
tation and transformation for abstracting conversa-
tions. In Proc. of NAACL 2010, Los Angeles, USA.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173:789?816.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press, Cambridge, GB.
H. Saggion and G. Lapalme. 2002. Generat-
ing indicative-informative summaries with sumum.
Computational Linguistics, 28(4):497?526.
T. Wilson. 2008. Annotating subjective content in
meetings. In Proc. of LREC 2008, Marrakech, Mo-
rocco.
S. Xie, B. Favre, D. Hakkani-Tu?r, and Y. Liu. 2009.
Leveraging sentence weights in a concept-based op-
timization framework for extractive meeting sum-
marization. In Proc. of Interspeech 2009, Brighton,
England.
J. Yu, E. Reiter, J. Hunter, and C. Mellish. 2007.
Choosing the content of textual summaries of large
time-series data sets. Journal of Natural Language
Engineering, 13:25?49.
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 10?18,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using the Omega Index for Evaluating Abstractive Community Detection
Gabriel Murray
Computer Information Systems
University of the Fraser Valley
gabriel.murray@ufv.ca
Giuseppe Carenini
Computer Science
University of British Columbia
carenini@cs.ubc.ca
Raymond Ng
Computer Science
University of British Columbia
rng@cs.ubc.ca
Abstract
Numerous NLP tasks rely on clustering or
community detection algorithms. For many
of these tasks, the solutions are disjoint, and
the relevant evaluation metrics assume non-
overlapping clusters. In contrast, the relatively
recent task of abstractive community detection
(ACD) results in overlapping clusters of sen-
tences. ACD is a sub-task of an abstractive
summarization system and represents a two-
step process. In the first step, we classify sen-
tence pairs according to whether the sentences
should be realized by a common abstractive
sentence. This results in an undirected graph
with sentences as nodes and predicted abstrac-
tive links as edges. The second step is to
identify communities within the graph, where
each community corresponds to an abstrac-
tive sentence to be generated. In this paper,
we describe how the Omega Index, a met-
ric for comparing non-disjoint clustering so-
lutions, can be used as a summarization eval-
uation metric for this task. We use the Omega
Index to compare and contrast several commu-
nity detection algorithms.
1 Introduction
Automatic summarization has long been proposed
as a helpful tool for managing the massive amounts
of language data in our modern lives (Luhn, 1958;
Edmundson, 1969; Teufel and Moens, 1997; Car-
bonell and Goldstein, 1998; Radev et al, 2001).
Most summarization systems are extractive, mean-
ing that a subset of sentences from an input docu-
ment forms a summary of the whole. Particular sig-
nificance may be attached to the chosen sentences,
e.g. that they are relevant to a provided query, gen-
erally important for understanding the overall doc-
ument, or represent a particular phenomenon such
as action items from a meeting. In any case, ex-
traction consists of binary classification of candidate
sentences, plus post-processing steps such as sen-
tence ranking and compression. In contrast, recent
work attempts to replicate the abstractive nature of
human-authored summaries, wherein new sentences
are generated that describe the input document from
a higher-level perspective. While some abstractive
summary sentences are very similar to individual
sentences from the document, others are created
by synthesizing multiple document sentences into
a novel abstract sentence. In this paper, we ad-
dress a component of this latter task, namely iden-
tifying which sentences from the source documents
should be combined in generated abstract sentences.
We call this task abstractive community detection
(ACD), and apply the task to a publicly available
meeting dataset.
Herein we focus on describing how the Omega
Index (Collins and Dent, 1988), a metric for com-
paring non-disjoint clustering solutions, can be used
as a summarization evaluation metric for the ACD
task. Metrics such as the Rand Index (Rand, 1971)
are insufficient since they are intended only for dis-
joint clusters.
ACD itself is carried out in two steps. First, we
classify sentence pairs according to whether they
should be realized by a common abstractive sen-
tence. For this step, we use supervised machine
learning that exploits human-annotated links be-
tween abstracts and extracts for a given document.
This results in an undirected graph with nodes repre-
senting sentences and edges representing predicted
abstractive links. Second, we identify communi-
ties within the graph, where each community cor-
responds to an abstractive sentence to be generated.
We experiment with several divisive community de-
10
tection algorithms, and highlight the importance of
selecting an algorithm that allows overlapping com-
munities, owing to the fact that a document sentence
can be expressed by, and linked to, more than one
abstract summary sentence in the gold-standard.
The structure of the paper is as follow. In Sec-
tion 2, we compare and contrast ACD with other
relevant tasks such as extractive summarization and
topic clustering. In Sections 3-4, we describe the
two ACD steps before we can fully discuss evalua-
tion methods. Section 5 describes the experimental
setup and corpora used, including a description of
the abstractive and extractive summary annotations
and the links between them. In Section 6, we give a
detailed description of the Omega Index and explain
how it differs from the more common Rand Index.
In Sections 7-8 we present results and draw conclu-
sions.
2 Related Work
The ACD task differs from more common extrac-
tive summarization (Mani, 2001a; Jurafsky and Mar-
tin, 2008). Whereas extraction involves simply clas-
sifying sentences as important or not, ACD is a
sub-task of abstractive summarization wherein doc-
ument sentences are grouped according to whether
they can be jointly realized by a common abstrac-
tive sentence. The first step of ACD, where we pre-
dict links between sentence pairs, can be seen to en-
compass extraction since the link is via an as-yet-
ungenerated abstract sentence, i.e. each linked sen-
tence is considered summary-worthy. However, the
second step moves away from extraction by cluster-
ing the linked sentences from the document in order
to generate abstract summary sentences.
ACD also differs from topic clustering (Malioutov
and Barzilay, 2006; Joty et al, 2010), though there
are superficial similarities. A first observation is that
topic links and abstract links are genuinely differ-
ent phenomena, though sometimes related. A sin-
gle abstract sentence can reference more than one
topic, e.g. They talked about the interface design
and the budget report, and a single topic can be
referenced in numerous abstract sentences. From a
practical standpoint, in our work on ACD we can-
not use many of the methods and evaluation metrics
designed for topic clustering, due to the fact that a
document sentence can belong to more than one ab-
stract sentence. This leads to overlapping commu-
nities, whereas most work on topic clustering has
focused primarily on disjoint communities where a
sentence belongs to a single topic. In Section 4, we
discuss community detection algorithms and evalu-
ation metrics that allow overlapping communities.
Work on detecting adjacency pairs (Shriberg et
al., 2004; Galley et al, 2004) also involves classify-
ing sentence pairs as being somehow related. For ex-
ample, if sentence B directly follows sentence A, we
might determine that they have a relationship such
as question-answer or request-accept. In contrast,
with ACD there is no requirement that sentence pairs
be adjacent or even in proximity to one another, nor
must they be in a rhetorical relation.
Work on sentence fusion (Barzilay and McKe-
own, 2005) identifies sentences containing similar
or repeated information and combines them into
new sentences. In contrast, in our task sentences
need not contain repeated information in order to be
linked. For example, two sentences could be linked
to a common abstract sentence due to a more com-
plex rhetorical relationship such as proposal-reject
or question-answer.
ACD is a more general problem that may incor-
porate elements of topic clustering, adjacency pair
detection and other sentence clustering or pairing
tasks. Here we try to directly learn the abstrac-
tive sentence links using lower-level features such as
shared n-grams and cosine similarity, as described in
Section 3, but in future work we will model higher-
level features of topics and rhetorical structure.
3 Step 1: Building a Sentence Pair Graph
In order to describe the use of the Omega Index for
the ACD task, we must first introduce the ACD task
in some detail. The first step in ACD is to determine
which sentence pairs are linked. If two sentences are
linked, it means they can be at least partly realized
in the abstract summary by a common sentence. A
document sentence may ?belong? to more than one
abstract sentence. We take a supervised classifica-
tion approach to this problem, training on a dataset
containing explicit links between extract sentences
and abstract sentences. The corpus and relevant an-
notation are described in detail in Section 5. For
11
Figure 1: Linked Sentences
our gold-standard data, a sentence pair is considered
linked if both sentences are linked to a common ab-
stract sentence and not-linked otherwise.
Figure 1 shows an example snippet of linked sen-
tences from our corpus. The first and second sen-
tences are linked via one abstract sentence while the
first and third sentences are linked via a different ab-
stract sentence. While it is not shown in this exam-
ple, note that two sentences can also be linked via
more than one abstract sentence.
We take a supervised machine learning approach
toward predicting whether a sentence pair is linked.
For each pair, we extract features that can be classed
as follows:
? Structural: The intervening number of sen-
tences, the document position as indicated by
the midpoint of the two sentences, the com-
bined length and the difference in length be-
tween the two sentences, and whether the two
sentences share the same speaker.
? Linguistic: The number of shared bigrams,
shared part-of-speech tags, the sum and aver-
age of tf.idf weights, and the cosine similarity
of the sentence vectors.
We run the trained classifier over sentence pairs,
predicting abstractive links between sentences in the
document. This results in an unweighted, undirected
graph where nodes represent sentences and edges
Figure 2: Graph with Sentence Nodes
represent an abstractive link. Continuing with the
conversation snippet from Figure 1, we would end
up with a graph like Figure 2. This very simple
example of a graph shows that there are abstractive
links predicted between sentences s1 and s2 and be-
tween sentences s1 and s3. There is no direct link
predicted between sentences s2 and s3. However,
it is possible for two sentences with no predicted
link between them to wind up in the same abstractive
community after running a community detection al-
gorithm on the graph. We discuss this community
detection step in the following section.
4 Step 2: Discovering Abstractive
Sentence Communities
In the first step of ACD, we predicted whether pairs
of sentences can be at least partly realized by a com-
mon abstractive sentence. We then want to identify
communities or clusters within the graph. Each of
these communities will correspond to an abstractive
12
Figure 3: Overlapping Communities in Graph
sentence that we will generate. Continuing with our
simple example, Figure 3 shows two communities
that have been identified in the graph. Note that
the communities are overlapping, as each contains
sentence s1; we would generate one abstractive sen-
tence describing sentences s1 and s2 and another de-
scribing sentences s1 and s3. We will return to this
critical issue of overlapping communities shortly.
The task of identifying communities in networks
or graphs has received considerable attention (Porter
et al, 2009). The Girvan-Newman algorithm (Gir-
van and Newman, 2002) is a popular community de-
tection method based on a measure of betweenness.
The betweenness score for an edge is the number of
shortest paths between pairs of nodes in the graph
that run along that edge. An edge with a high be-
tweenness score is likely to be between two commu-
nities and is therefore a good candidate for removal,
as the goal is to break the initial graph into distinct
communities. The Girvan-Newman algorithm pro-
ceeds as follows:
1. Calculate the betweenness of each edge in the
graph.
2. Remove the edge with the highest betweenness.
3. For any edge affected by Step 2, recalculate be-
tweenness.
4. Repeat steps 2 and 3 until no edges remain
In this way we proceed from the full graph with all
edges intact to the point where no edges remain and
each node is in its own community. The intermediate
steps can be visualized by the resulting dendrogram,
such as seen in Figure 4 1.
The top row, the ?leaves? of the dendrogram, rep-
resents the individual nodes in the graph. The rest
1Image Source: Wikimedia Commons (Mhbrugman)
Figure 4: Community Dendrogram
of the dendrogram shows how these nodes are sit-
uated in nested communities, e.g. b and c form a
community bc that combines with def to form bcdef.
In our case, where nodes are sentences, the dendro-
gram shows us how sentences combine into nested
communities. This can be useful for generating ab-
stracts of different granularities, e.g. we could de-
scribe bcdef in one sentence or generate two sen-
tences to separately describe bc and def.
The drawback of Girvan-Newman for our pur-
poses is that it does not allow overlapping commu-
nities, and we know that our human-annotated data
contain overlaps. Note from Figure 4 that all com-
munities decompose into disjoint nested communi-
ties, such as bcdef being comprised of bc and def,
not bc and bdef or some other overlapping case.
We therefore hypothesize that Girvan-Newman in its
traditional form is not sufficient for our current re-
search. For the same reason, recent graph-based ap-
proaches to topic clustering (Malioutov and Barzi-
lay, 2006; Joty et al, 2010) are not directly applica-
ble here.
It is only in recent years that much attention has
been paid to the problem of overlapping (or non-
disjoint) communities. Here we consider two recent
modifications to the Girvan-Newman algorithm that
allow for overlaps. The CONGA algorithm (Gre-
gory, 2007) extends Girvan-Newman so that instead
of removing an edge on each iteration, we either
remove an edge or copy a node. When a node is
copied, an overlap is created. Nodes are associated
with a betweenness score (called the split between-
ness) derived from the edge betweenness scores, and
at each step we either remove the edge with the high-
est betweenness score or copy the node with the
13
Figure 5: CONGA algorithm
highest split betweenness, if it is greater. The edge
and node betweenness scores are then recalculated.
In such a manner we can detect overlapping com-
munities. Figure 5 shows the CONGA copying and
splitting operations applied to our simple example,
so that sentence s1 now exists in two communities.
The CONGO algorithm (Gregory, 2008) is an ap-
proximation of CONGA that is more efficient for
large graphs. Girvan-Newman (and hence CONGA)
are not feasible algorithms for very large graphs, due
to the number of repeated betweenness calculations.
CONGO addresses this problem by using local be-
tweenness scores. Instead of calculating between-
ness using the shortest paths of every pair of nodes
in the graph, only nodes within a given horizon h of
an edge are considered. When h =? then CONGO
and CONGA are identical. Gregory (Gregory, 2008)
found good results using h = 2 or h = 3 on a va-
riety of datasets including blog networks; here we
experiment h = 2.
For the community detection step of our system,
we run both CONGA and CONGO on our graphs
and compare our results with the Girvan-Newman
algorithm. For all community detection methods,
as well as human annotations, any sentences that
are not linked to at least one other sentence in Step
1 are assigned to their own singleton communities.
Also, the algorithms we are evaluating are hierarchi-
cal (see Figure 4), and we evaluate at n = 18 clus-
ters, since that is the average number of sentences
per abstractive meeting summary in the training set.
5 Experimental Setup
In this section we describe the dataset used, includ-
ing relevant annotations, as well as the statistical
classifiers used for Step 1.
5.1 AMI Corpus
For these experiments we use the AMI meeting cor-
pus (Carletta, 2006), specifically, the subset of sce-
nario meetings where participants play roles within
a fictional company. For each meeting, an annotator
first authors an abstractive summary. Multiple an-
notators then create extractive summaries by linking
sentences from the meeting transcript to sentences
within the abstract. This generates a many-to-many
mapping between transcript sentences and abstract
sentences, so that a given transcript sentence can
relate to more than one abstract sentence and vice-
verse. A sample of this extractive-abstractive linking
was shown in Figure 1.
It is known that inter-annotator agreement can be
quite low for the summarization task (Mani et al,
1999; Mani, 2001b), and this is the case with the
AMI extractive summarization codings. The aver-
age ? score is 0.45.
In these experiments, we use only human-
authored transcripts and plan to use speech recog-
nition transcripts in the future. Note that our overall
approach is not specific to conversations or to speech
data. Step 2 is completely general, while Step 1 uses
a single same-speaker feature that is specific to con-
versations. That feature can be dropped to make our
approach completely general (or, equivalently, that
binary feature can be thought of as always 1 when
applied to monologic text).
5.2 Classifiers
For Step 1, predicting abstractive links between sen-
tences, we train a logistic regression classifier us-
ing the liblinear toolkit2. The training set consists
of 98 meetings and there are nearly one million sen-
tence pair instances since we consider every pairing
of sentences within a meeting. The test set consists
of 20 meetings on which we perform our evaluation.
6 Evaluation Metrics
In this section, we present our evaluation metrics for
the two steps of the task.
6.1 Step 1 Evaluation: PRF and AUROC
For evaluating Step 1, predicting abstractive sen-
tence links, we present both precision/recall/f-score
2http://www.csie.ntu.edu.tw/ cjlin/liblinear/
14
as well as the area under the receiver operator char-
acteristic curve (AUROC). While the former scores
evaluate the classifier at a particular posterior proba-
bility threshold, the AUROC evaluates the classifier
more generally by comparing the true-positive and
false-positive rates at varying probability thresholds.
6.2 Step 2 Evaluation: The Omega Index
For evaluating Step 2, ACD, we employ a metric
called the Omega Index which is designed for com-
paring disjoint clustering solutions. To describe and
motivate our use of this metric, it is necessary to de-
scribe previous metrics upon which the Omega In-
dex improves. The Rand Index (Rand, 1971) is a
way of comparing disjoint clustering solutions that
is based on pairs of the objects being clustered. Two
solutions are said to agree on a pair of objects if they
each put both objects into the same cluster or each
into different clusters. The Rand Index can then be
formalized as
(a+ d)/N
where N is the number of pairs of objects, a is
the number of times the solutions agree on putting
a pair in the same cluster and d is the number of
times the solutions agree on putting a pair in differ-
ent clusters. That is, the Rand Index is the number of
pairs that are agreed on by the two solutions divided
by the total number of pairs. The Rand Index is in-
sufficient for overlapping solutions because pairs of
objects can exist together in more than one commu-
nity. In those cases, two solutions might agree on
the occurrence of a pair of objects in one commu-
nity but disagree on the occurrence of that pair in
another community. The Rand Index cannot capture
that distinction.
An improvement to the Rand Index is the Ad-
justed Rand Index (Hubert and Arabie, 1985) which
adjusts the level of agreement according to the ex-
pected amount of agreement based on chance. How-
ever, the Adjusted Rand Index also cannot account
for disjoint solutions.
The Omega Index (Collins and Dent, 1988) builds
on both the Rand Index and Adjusted Rand Index
by accounting for disjoint solutions and correcting
for chance agreement. The Omega Index considers
the number of clusters in which a pair of objects is
together. The observed agreement between solutions
is calculated by
Obs(s1, s2) =
min(J,K)?
j=0
Aj/N
where J and K represent the maximum number of
clusters in which any pair of objects appears together
in solutions 1 and 2, respectively, Aj is the number
of the pairs agreed by both solutions to be assigned
to number of clusters j, and N is again the number
of pairs of objects. That is, the observed agreement
is the proportion of pairs classified the same way by
the two solutions. The expected agreement is given
by:
Exp(s1, s2) =
min(J,K)?
j=0
Nj1Nj2/N
2
where Nj1 is the total number of pairs assigned
to number of clusters j in solution 1, and Nj2 is the
total number of pairs assigned to number of clusters
j in solution 2. The Omega Index is then calculated
as
Omega(s1, s2) =
Obs(s1, s2)? Exp(s1, s2)
1? Exp(s1, s2)
The numerator is the observed agreement adjusted
by expected agreement, while the denominator is
maximum possible agreement adjusted by expected
agreement. The highest possible score of 1 indicates
that two solutions perfectly agree on how each pair
of objects is clustered. With the Omega Index, we
can now evaluate the overlapping solutions discov-
ered by our community detection algorithms.3
7 Results
In this section we present the results for both steps
of ACD. Because the Omega Index is not used for
evaluating Step 1, we keep that discussion brief.
7.1 Step 1 Results: Predicting Abstractive
Sentence Links
For the task of predicting abstractive links within
sentence pairs, the resulting graphs have an aver-
age of 133 nodes and 1730 edges, though this varies
3Software for calculating the Omega Index will be released
upon publication of this paper.
15
System Prec. Rec. F-Score AUROC
Lower-Bound 0.18 1 0.30 0.50
Message Links 0.30 0.03 0.05 -
Abstractive Links 0.62 0.54 0.54 0.89
Table 1: P/R/F and AUROCs for Link Prediction
widely depending on meeting length (from 37 nodes
and 61 edges for one short meeting to 224 edges and
5946 edges for a very long meeting). In compar-
ison, the gold-standard graphs have an average of
113 nodes and 1360 edges. The gold-standards sim-
ilarly show huge variation in graph size depending
on meeting length.
Table 1 reports both the precision/recall/f-scores
as well as the AUROC metrics. We compare our
supervised classifier (labeled ?Abstractive Links?)
with a lower-bound where all instances are predicted
as positive, leading to perfect recall and low preci-
sion. Our system scores moderately well on both
precision and recall, with an average f-score of 0.54.
The AUROC for the abstractive link classifier is
0.89.
It is difficult to compare with previous work since,
to our knowledge, nobody has previously modeled
these extractive-abstractive mappings between doc-
ument sentences and associated abstracts. We can
compare with the results of Murray et al (2010),
however, who linked sentences by aggregating them
into messages. In that work, each message is com-
prised of sentences that share a dialogue act type
(e.g. an action item) and mention at least one com-
mon entity (e.g. remote control). Similar to our
work, sentences can belong to more than one mes-
sage. We assess how well their message-based ap-
proach captures these abstractive links, reporting
their precision/recall/f-scores for this task in Table 1,
with their system labeled ?Message Links?. While
their precision is above the lower-bound, the recall
and f-score are extremely low. This demonstrates
that their notion of message links does not capture
the phenomenon of abstractive sentence linking.
7.2 Step 2 Results: Discovering Abstractive
Communities
For the task of discovering abstractive communi-
ties in our sentence graphs, Table 2 reports the
Omega Index for the CONGA, CONGO and Girvan-
Newman algorithms. We also report the average
Omega Index for the human annotators themselves,
derived by comparing each pair of annotator solu-
tions for each meeting.
It is not surprising that the Omega Index is low for
the inter-annotator comparison; we reported previ-
ously that the ? score for the extractive summaries of
this corpus is 0.45. That ? score indicates that there
is high disagreement about which sentences are most
important in a meeting. We should not be surprised
then that there is further disagreement about how the
sentences are linked to one another. What is surpris-
ing is that the automatic community detection al-
gorithms achieve higher Omega Index scores than
do the annotators. Note that the higher scores of
the community detection algorithms relative to hu-
man agreement is not simply an artefact of identify-
ing clustering solutions that have more overlap than
human solutions, since even the disjoint Girvan-
Newman solutions are higher than inter-annotator
levels. One possible explanation is that the annota-
tors are engaged in a fairly local task when they cre-
ate extractive summaries; for each abstractive sen-
tence, they are looking for a set of sentences from
the document that relate to that abstract sentence,
and because of high redundancy in the document the
different annotators choose subsets of sentences that
have little overlap but are still similar (Supporting
this, we have found that we can train on annotator
A?s extractive codings and test on annotator B?s and
get good classification results even if A and B have a
low ? score.). In contrast, the community detection
algorithms are taking a more comprehensive, global
approach by considering all predicted links between
sentences (Step 1) and identifying the overlapping
communities among them (Step 2).
When looking for differences between automatic
and human community detection, we observed that
the algorithms assigned more overlap to sentences
16
System Omega
Girvan-Newman 0.254
CONGA 0.263
CONGO 0.241
Human 0.209
Table 2: Omega Index for Community Detection
than did the human annotators. For example, the
CONGA algorithm assigned each sentence to an av-
erage of 1.1 communities while the human annota-
tors assigned each to an average of 1.04 communi-
ties. Note that every sentence belongs to at least one
community since unlinked sentences belong to their
own singleton communities, and most sentences are
unlinked, explaining why both scores are close to 1.
Comparing the algorithms themselves, we find
that CONGA is better than both Girvan-Newman
(marginally significant, p = 0.07) and CONGO
(p = 0.015) according to paired t-test. We be-
lieve that the superiority of CONGA over Girvan-
Newman points to the importance of allowing over-
lapping communities. And while CONGO is an ef-
ficient approximation of CONGA that can be useful
for very large graphs where CONGA and Girvan-
Newman cannot be applied, in these experiments the
local betweenness used by CONGO leads to lower
overall scores. Furthermore, our networks are small
enough that both CONGA and Girvan-Newman are
able to finish quickly and there is therefore no need
to rely on CONGO.
Our Step 2 results are dependent on the qual-
ity of the Step 1 results. We therefore test how
good our community detection results would be if
we had gold-standard graphs rather than the imper-
fect output from Step 1. We report two sets of re-
sults. In the first case, we take an annotator?s gold-
standard sentence graph showing links between sen-
tences and proceed to run our algorithms over that
graph, comparing our community detection results
with the communities detected by all annotators. In
the second case, we again take an annotator?s gold-
standard graph and apply our algorithms, but then
only compare our community detection results with
the communities detected by the annotator who sup-
plied the gold-standard graph. Table 3 shows both
sets of results. We can see that the latter set contains
System Omega Omega
All Annots. 1 Annot.
Girvan-Newman 0.445 0.878
CONGA 0.454 0.896
CONGO 0.453 0.894
Table 3: Omega Index, Gold-Standard Graphs
much higher scores, again reflecting that annotators
disagree with each other on this task.
Given gold-standard sentence graphs, CONGA
and CONGO perform very similarly; the differences
are negligible. Both are substantially better than the
Girvan-Newman algorithm (all p < 0.01). This tells
us that it is necessary to employ community detec-
tion algorithms that allow overlapping communities.
These results also tell us that the CONGO algorithm
is more sensitive to errors in the Step 1 output since
it performed well using the gold-standard but worse
than Girvan-Newman when using the automatically
derived graphs.
8 Conclusion
After giving an overview of the ACD task and our
approach to it, we described how the Omega Index
can be used as a summarization evaluation metric for
this task, and explained why other community de-
tection metrics are insufficient. The Omega Index is
suitable because it can account for overlapping clus-
tering solutions, and corrects for chance agreement.
The main surprising result was that all of the com-
munity detection algorithms have higher Omega In-
dex scores than the human-human Omega scores
representing annotator agreement. We have offered
one possibe explanation; namely, that while the hu-
man annotators have numerous similar candidate
sentences from the document that each could be
linked to a given abstract sentence, they may be sat-
isfied to only link (and thus extract) a small repre-
sentative handful, whereas the community detection
algorithms work to find all extractive-abstractive
links. We plan to further research this issue, and po-
tentially derive other evaluation metrics that better
account for this phenomenon.
17
References
R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3):297?328.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval 1998, Melbourne, Australia, pages 335?
336.
J. Carletta. 2006. Unleashing the killer corpus: expe-
riences in creating the multi-everything ami meeting
corpus. In Proc. of LREC 2006, Genoa, Italy, pages
181?190.
L. Collins and C. Dent. 1988. Omega: A general formu-
lation of the rand index of cluster recovery suitable for
non-disjoint solutions. Multivariate Behavioral Re-
search, 23:231?242.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. J. ACM, 16(2):264?285.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proc. of ACL 2004.
M. Girvan and M.E.J. Newman. 2002. Community
structure in social and biological networks. Proc. of
the National Academy of Sciences, 99:7821?7826.
S. Gregory. 2007. An algorithm to find overlap-
ping community structure in networks. In Proc. of
ECML/PKDD 2007, Warsaw, Poland.
S. Gregory. 2008. A fast algorithm to find overlapping
communities in networks. In Proc. of ECML/PKDD
2008, Antwerp, Belgium.
L. Hubert and P. Arabie. 1985. Comparing partitions.
Journal of Classification, 2:193?218.
S. Joty, G. Carenini, G. Murray, and R. Ng. 2010. Ex-
ploiting conversation structure in unsupervised topic
segmentation for emails. In Proc. of EMNLP 2010,
Cambridge, MA, USA.
D. Jurafsky and J. H. Martin, 2008. Speech and Lan-
guage Processing. Prentice Hall.
H. P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBM Journal of Research Development,
2(2):159?165.
I. Malioutov and R. Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. In Proc. of ACL
2006, Sydney, Australia.
I. Mani, D. House, G. Klein, L. Hirschman, T. Firmin,
and B. Sundheim. 1999. The TIPSTER SUMMAC
text summarization evaluation. In Proc. of EACL
1999, Bergen, Norway, pages 77?85.
I. Mani. 2001a. Automatic Summarization. John Ben-
jamin, Amsterdam, NL.
I. Mani. 2001b. Summarization evaluation: An
overview. In Proc. of the NTCIR Workshop 2 Meeting
on Evaluation of Chinese and Japanese Text Retrieval
and Text Summarization, Tokyo, Japan, pages 77?85.
G. Murray, G. Carenini, and R. Ng. 2010. Generating
and validating abstracts of meeting conversations: a
user study. In Proc. of INLG 2010, Dublin, Ireland.
M. Porter, J-P. Onnela, and P. Mucha. 2009. Communi-
ties in networks. Notices of the American Mathemati-
cal Society, 56:1082?1097.
D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Ex-
periments in single and multi-document summariza-
tion using MEAD. In Proc. of DUC 2001, New Or-
leans, LA, USA.
W.M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66:846?850.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proceedings of SIGdial Workshop
on Discourse and Dialogue, Cambridge, MA, USA,
pages 97?100.
S. Teufel and M. Moens. 1997. Sentence extraction as a
classification task. In Proc. of ACL 1997, Workshop on
Intelligent and Scalable Text Summarization, Madrid,
Spain, pages 58?65.
18
