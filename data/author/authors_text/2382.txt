Proceedings of the 5th Workshop on Important Unresolved Matters, pages 73?80,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Arabic to French Sentence Alignment: Exploration of A Cross-
language Information Retrieval Approach 
Nasredine Semmar 
CEA, LIST 
Laboratoire d?ing?nierie de la connais-
sance multim?dia multilingue 
18 route du Panorama 
BP6, FONTENAY AUX ROSES, F- 
92265 France 
nasredine.semmar@cea.fr 
Christian Fluhr 
CEA, LIST 
Service R?alite virtuelle, Cognitique et 
Interfaces 
18 route du Panorama 
BP6, FONTENAY AUX ROSES, F- 
92265 France 
christian.fluhr@cea.fr 
 
 
Abstract 
Sentence alignment consists in estimating 
which sentence or sentences in the source 
language correspond with which sentence 
or sentences in a target language. We pre-
sent in this paper a new approach to align-
ing sentences from a parallel corpus based 
on a cross-language information retrieval 
system. This approach consists in building 
a database of sentences of the target text 
and considering each sentence of the 
source text as a "query" to that database. 
The cross-language information retrieval 
system is a weighted Boolean search en-
gine based on a deep linguistic analysis of 
the query and the documents to be indexed. 
This system is composed of a multilingual 
linguistic analyzer, a statistical analyzer, a 
reformulator, a comparator and a search 
engine. The multilingual linguistic analyzer 
includes a morphological analyzer, a part-
of-speech tagger and a syntactic analyzer. 
The linguistic analyzer processes both 
documents to be indexed and queries to 
produce a set of normalized lemmas, a set 
of named entities and a set of nominal 
compounds with their morpho-syntactic 
tags. The statistical analyzer computes for 
documents to be indexed concept weights 
based on concept database frequencies. The 
comparator computes intersections between 
queries and documents and provides a rele-
vance weight for each intersection. Before 
this comparison, the reformulator expands 
queries during the search. The expansion is 
used to infer from the original query words 
other words expressing the same concepts. 
The search engine retrieves the ranked, 
relevant documents from the indexes ac-
cording to the corresponding reformulated 
query and then merges the results obtained 
for each language, taking into account the 
original words of the query and their 
weights in order to score the documents. 
The sentence aligner has been evaluated on 
the MD corpus of the ARCADE II project 
which is composed of news articles from 
the French newspaper "Le Monde Diplo-
matique". The part of the corpus used in 
evaluation consists of the same subset of 
sentences in Arabic and French. Arabic 
sentences are aligned to their French coun-
terparts. Results showed that alignment has 
correct precision and recall even when the 
corpus is not completely parallel (changes 
in sentence order or missing sentences). 
1 Introduction 
Sentence alignment consists in mapping sentences 
of the source language with their translations in the 
target language. Automatic sentence alignment 
approaches face two kinds of difficulties: robust-
ness and accuracy. A number of automatic sen-
tence alignment techniques have been proposed 
(Kay and R?scheisen, 1993; Gale and Church, 
1991; Brown et al, 1991; Debili and Samouda, 
1992; Papageorgiou et al, 1994; Gaussier, 1995; 
Melamed, 1996; Fluhr et al, 2000). 
73
The method proposed in (Kay and R?scheisen, 
1993) is based on the assumption that in order for 
the sentences in a translation to correspond, the 
words in them must correspond. In other words, all 
necessary information (and in particular, lexical 
mapping) is derived from the to-be-aligned texts 
themselves. 
In (Gale and Church, 1991) and (Brown et al, 
1991), the authors start from the fact that the length 
of a source text sentence is highly correlated with 
the length of its target text translation: short sen-
tences tend to have short translations, and long 
sentences tend to have long translations. 
The method proposed in (Debili and Sammouda, 
1992) is based on the preliminary alignment of 
words using a conventional bilingual lexicon and 
the method described in (Papageorgiou et al, 1994) 
added grammatical labeling based on the assump-
tion that the same parts of speech tend to be em-
ployed in the translation. 
In this paper, we present a sentence aligner 
which is based on a cross-language information 
retrieval approach and combines different informa-
tion sources (bilingual lexicon, sentence length and 
sentence position). This sentence aligner was first 
developed for aligning French-English parallel text. 
It is now ported to Arabic-French and Arabic-
English language pairs. 
We present in section 2 the main components of 
the cross-language search engine, in particular, we 
will focus on the linguistic processing. In section 3, 
the prototype of our sentence aligner is described. 
We discuss in section 4 results obtained after align-
ing sentences of the MD (Monde Diplomatique) 
corpus of the ARCADE II project. Section 5 con-
cludes our study and presents our future work. 
2 The Cross-language Search Engine 
Information retrieval consists to find all relevant 
documents for a user query in a collection of 
documents. These documents are ordered by the 
probability of being relevant to the user's query. 
The highest ranked document is considered to be 
the most likely relevant document. Cross-language 
information retrieval consists in providing a query 
in one language and searching documents in 
different languages (Grefenstette, 1998). The 
cross-lingual search engine is a weighted Boolean 
search engine based on a deep linguistic analysis of 
the query and the documents to be indexed 
(Besan?on et al, 2003). It is composed of a 
linguistic analyzer, a statistical analyzer, a 
reformulator and a comparator (Figure 1): 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. The cross-language search engine 
2.1 Linguistic Analysis 
The linguistic analyzer produces a set of normal-
ized lemmas, a set of named entities and a set of 
nominal compounds. It is composed of several lin-
guistic resources and processing modules. 
 
Each language has its proper linguistic resources 
which are generally composed of: 
? A full form dictionary, containing for each 
word form its possible part-of-speech tags 
Linguistic analysis 
Comparator 
Statistic 
analysis 
Reformulation 
General 
lexicons 
Reformulation 
lexicons 
Search engine database (Indexed 
documents) 
Documents to be 
indexed 
Queries 
Documents 
grouped in rele-
vant classes 
74
and linguistic features (gender, number, etc). 
For languages such as Arabic which pre-
sents agglutination of articles, prepositions 
and conjunctions at the beginning of the 
word as well as pronouns at the ending of 
the word, we added two other dictionaries 
for proclitics and enclitics in order to split 
the input words into proclitics, simple forms 
and enclitics. 
? A monolingual reformulation dictionary 
used in query expansion for expanding 
original query words to other words express-
ing the same concepts (synonyms, hypo-
nyms, etc.). 
? Bilingual dictionaries used in cross-
language querying. 
? A set of rules for tokenizing words. 
? A set of  part-of-speech n-grams (bigrams 
and trigrams from hand-tagged corpora) that 
are used for part-of-speech tagging. 
? A set of rules for shallow parsing of sen-
tences, extracting compounds from the input 
text. 
? A set of rules for the identification of 
named entities: gazetteers and contextual 
rules that use special triggers to identify 
named entities and their type. 
The processing modules are common for all the 
languages with some variations for some specific 
languages: 
? A Tokenizer which separates the input 
stream into a graph of words. This separa-
tion is achieved by an automaton devel-
oped for each language and a set of seg-
mentation rules. 
? A Morphological analyzer which searches 
each word in a general dictionary (Debili 
and Zouari, 1985). If this word is found, it 
will be associated with its lemma and all 
its morpho-syntactic tags. If the word is 
not found in the general dictionary, it is 
given a default set of morpho-syntactic 
tags based on its typography. For Arabic, 
we added to the morphological analyzer a 
new processing step: a Clitic stemmer 
(Larkey et al, 2002) which splits aggluti-
nated words into proclitics, simple forms 
and enclitics. If the simple form computed 
by the clitic stemmer does not exist in the 
general dictionary, re-write rules are ap-
plied (Darwish, 2002). For example, con-
sider the token ?? (with their ballon) 
and the included clitics ??? (with) and ??? 
(their), the computed simple form ???? 
does not exist in the general dictionary but 
after applying one of the dozen re-write 
rules, the modified simple form ???? 
(ballon) is found in the general dictionary 
and the input token is segmented as: 
 = ? + ?? + ?. 
? An Idiomatic Expressions recognizer 
which detects idiomatic expressions and 
considers them as single words for the rest 
of the processing. Idiomatic expressions 
are phrases or compound nouns that are 
listed in a specific dictionary. The detec-
tion of idiomatic expressions is performed 
by applying a set of rules that are triggered 
on specific words and tested on left and 
right contexts of the trigger. These rules 
can recognize contiguous expressions as 
the "white house" in English, la "maison 
blanche" in French or " ?????? ?" in Ara-
bic. Non-contiguous expressions such as 
phrasal verbs in English: "switch?on" or 
"tomber vaguement dans les pommes" in 
French are recognized too. 
? A Part-Of-Speech (POS) tagger which 
searches valid paths through all the possi-
ble tags paths using attested trigrams and 
bigrams sequences. The trigram and bi-
gram matrices are generated from a manu-
ally annotated training corpus (Grefen-
stette et al, 2005). They are extracted from 
a hand-tagged corpora of 13 200 words for 
Arabic and 25 000 words for French. If no 
continuous trigram full path is found, the 
POS tagger tries to use bigrams at the 
points where the trigrams were not found 
in the matrix. The accuracy of the part-of-
speech tagger is around 91% for Arabic 
and 94% for French. 
? A Syntactic analyzer which is used to split 
word graph into nominal and verbal chain 
and recognize dependency relations (espe-
cially those within compounds) by using a 
set of syntactic rules. We developed a set 
of dependency relations to link nouns to 
75
other nouns, a noun with a proper noun, a 
proper noun with the post nominal adjec-
tive and a noun with a post nominal adjec-
tive. These relations are restricted to the 
same nominal chain and are used to com-
pute compound words. For example, in the 
nominal chain ? ??